Zhang et al. Journal of Biomedical Semantics  (2016) 7:21 
DOI 10.1186/s13326-016-0063-3ERRATUM Open AccessErratum to: Extracting drug-enzyme relation
from literature as evidence for drug drug
interaction
Yaoyun Zhang1, Heng-Yi Wu2, Jingcheng Du1, Jun Xu1, Jingqi Wang1, Cui Tao1, Lang Li2 and Hua Xu1*Following the publication of the original article [1] it
was brought to our attention that an important acknow-
ledgement from co-author Jingcheng Du was missing
from the manuscript. The authors would like to apolo-
gise for this oversight and now take the opportunity to
gratefully acknowledge the support from the UTHealth
Innovation for Cancer Prevention Research Training
Program Pre-doctoral Fellowship (Cancer Prevention
and Research Institute of Texas grant # RP140103).
Author details
1School of Biomedical Informatics, University of Texas Health Science Center
at Houston, Houston, TX, USA. 2School of Medicine, Indiana University,
Indianapolis, IN, USA.
Received: 5 April 2016 Accepted: 5 April 2016
Reference
1. Zhang Y et al. Extracting drug-enzyme relation from literature as evidence
for drug drug interaction. J Biomed Semantics. 2016;7:11.* Correspondence: hua.xu@uth.tmc.edu
1School of Biomedical Informatics, University of Texas Health Science Center
at Houston, Houston, TX, USA
Full list of author information is available at the end of the article
© 2016 Zhang et al. Open Access This article
International License (http://creativecommons
reproduction in any medium, provided you g
the Creative Commons license, and indicate if
(http://creativecommons.org/publicdomain/ze  We accept pre-submission inquiries 
  Our selector tool helps you to find the most relevant journal
  We provide round the clock customer support 
  Convenient online submission
  Thorough peer review
  Inclusion in PubMed and all major indexing services 
  Maximum visibility for your research
Submit your manuscript at
www.biomedcentral.com/submit
Submit your next manuscript to BioMed Central 
and we will help you at every step:is distributed under the terms of the Creative Commons Attribution 4.0
.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
ive appropriate credit to the original author(s) and the source, provide a link to
changes were made. The Creative Commons Public Domain Dedication waiver
ro/1.0/) applies to the data made available in this article, unless otherwise stated.

Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 
DOI 10.1186/s13326-016-0076-y
RESEARCH Open Access
Functional coherence metrics in protein
families
Hugo P. Bastos1, Lisete Sousa2, Luka A. Clarke3 and Francisco M. Couto1*
Abstract
Background: Biological sequences, such as proteins, have been provided with annotations that assign functional
information. These functional annotations are associations of proteins (or other biological sequences) with descriptors
characterizing their biological roles. However, not all proteins are fully (or even at all) annotated. This annotation
incompleteness limits our ability to make sound assertions about the functional coherence within sets of proteins.
Annotation incompleteness is a problematic issue when measuring semantic functional similarity of biological
sequences since they can only capture a limited amount of all the semantic aspects the sequences may encompass.
Methods: Instead of relying uniquely on single (reductive) metrics, this work proposes a comprehensive approach
for assessing functional coherence within protein sets. The approach entails using visualization and term enrichment
techniques anchored in specific domain knowledge, such as a protein family. For that purpose we evaluate two novel
functional coherence metrics, mUI and mGIC that combine aspects of semantic similarity measures and term
enrichment.
Results: These metrics were used to effectively capture and measure the local similarity cores within protein sets.
Hence, these metrics coupled with visualization tools allow an improved grasp on three important functional
annotation aspects: completeness, agreement and coherence.
Conclusions: Measuring the functional similarity between proteins based on their annotations is a non trivial task.
Several metrics exist but due both to characteristics intrinsic to the nature of graphs and extrinsic natures related to
the process of annotation each measure can only capture certain functional annotation aspects of proteins. Hence,
when trying to measure the functional coherence of a set of proteins a single metric is too reductive. Therefore, it is
valuable to be aware of how each employed similarity metric works and what similarity aspects it can best capture.
Here we test the behaviour and resilience of some similarity metrics.
Background
Over the last two decades functional annotation systems
have been providing annotations for numerous proteins
as well as other gene products. One of the most com-
mon steps used in functional annotation is the use of
sequence alignment algorithms to compare sequences and
find homologies from which functions can be extrapo-
lated. Usually, lists of proteins (or other gene products)
result from the output of many high-throughput tech-
nologies. Therefore, not only is it important to identify
common functions in those sets of proteins but also to
quantify how functionally related the proteins are in order
*Correspondence: fcouto@di.fc.ul.pt
1LaSIGE, Faculdade de Ciências, Universidade de Lisboa, Lisboa, Portugal
Full list of author information is available at the end of the article
to increase understanding of the involvement of biological
systems [1, 2].
The GeneOntology (GO) project aims to provide gener-
ically consistent descriptions for the molecular phenom-
ena in which gene products are involved [3]. For over a
decade the increasing popularity and consequent growth
of GO has led to its adoption and prevalent use in
annotation projects. Consequently, this pervasiveness has
enabled andmotivated the development of several seman-
tic similarity metrics [46]. Semantic similarity can be
defined as the quantity that reflects the closeness inmean-
ing of two concepts in an ontology. However, the semantic
similarity between two proteins, which can be annotated
with several GO terms is commonly called functional
similarity since it is the functional annotation terms that
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 2 of 11
are being compared. More recently, several metrics focus-
ing specifically on measuring the functional cohesiveness
of a set of proteins (or gene products) through their
annotations have been developed. These metrics for the
assessment of functional coherence using annotations are
commonly based on the previously developed groupwise
semantic similarity approaches.
One of those metrics, GS2 [7], uses a set-based approach
and was developed with computational efficiency in mind
to measure gene set functional similarity based on GO
terms. The GS2 algorithm ranks annotation terms using
a simple gene counting method and then compares each
gene with the remaining genes with respect to the dis-
tribution of functional annotations. This simple measure
can only capture similarity trends within gene sets and can
not precisely assess similarity. Despite that, GS2 has per-
formed well when compared with the semantic similarity
pairwise measure of [8].
On the other hand, another set of three different met-
rics: average seed degree, total length and relative seed
degree were developed by [9], for the assessment of func-
tional coherence in gene sets based on the topological
properties of GO-derived graphs. The procedure leading
to these metrics relies on building GO subgraphs that
subsume each gene set annotation (for each GO aspect),
whereas each node is a GO term and each edge is an is_a
relationship between terms. Subsequently, those graphs
are further enriched by adding genes, as a new type of
node, associated to the original GO nodes, and additional
new edges are created between GO terms whenever these
share gene annotations. The original term-to-term edges
are weighted using the Information Content [10] differ-
ence between both terms while the new edges created
after addition of the gene nodes to the graph are statis-
tically weighted based on the total number of edges in
the graph and the number of supporting genes for each
particular edge. Hence, this approach handles the issue
at hand both from an annotation enrichment perspective
and an annotation relationship perspective. Steiner trees
are then extracted from the graphs and the sum of all
edge lengths is minimized for all possible subgraphs. The
aforementioned three metrics are then applied to these
trees. The average seed degree averages, for a full tree,
the counts of the number of genes associated to the seed
terms thus reflecting a global measure of enrichment. On
the other hand the total length metric reflects the over-
all relatedness of functions by performing the sum of the
length of all edges in a tree. The relative seed degree met-
ric combines the aspects described above as a ratio. The
methodology performs well, but like other GO-evaluation
methodologies, its metrics are dependent on the gene
annotation state.
The GO-based functional dissimilarity (GFD) metric
[11] approaches the problem of functional coherence in
gene sets by considering that each gene can encode several
proteins with different functions. In this metric, for each
gene set, only the most common and specific function is
chosen as being the most globally cohesive function. In
this approach, genes are represented as sets to which a
simple counting edge-based measure ratio is applied and
that aims at equating both gene relatedness and specificity.
The actual GFD is then the minimum of dissimilarity
possible for all representations of a given set of genes.
Like the previous metrics this one also depends on the
completeness of the annotations used in order to pro-
vide accuratemeasurements. Furthermore, by considering
only the most common and specific function in a gene
set the authors are effectively discarding potential non-
related functions that would cause noise, however at the
cost of disregarding multi-functional associations in gene
sets.
Furthermore, and despite not being exactly a system for
measuring functional coherence in gene sets, RuleGO [12]
provides a service that statistically compares and char-
acterizes two disjointed gene sets. Underneath it runs a
rule-based system that incrementally iterates the list of
GO terms annotating the two input gene sets and verifies
at each step if a new co-occurrence rule can be created.
Much like the typical gene enrichment systems, this sys-
tem also performs over-representation tests on the rules
created and only rules corresponding to a p-value below
a given statistical significance threshold (after multiple
testing correction) are considered. This process results
in multi-attribute rules containing annotation terms and
respective support indexes and evaluation parameters that
can be used in the characterization of the disjointed gene
sets. In this methodology rules are evaluated by length
(number of genes in a rule premise) representing support,
by depth (normalized sum of the GO graph levels where
terms in the rule appear) representing specificity and by
an additional quality measure.
A different approach is taken by [13] where functional
coherence in gene sets is assessed with the help of the
biological literature. Here, term-by-genematrices are con-
structed with entries derived from weighted frequencies
of the terms across a collection of abstracts (biologi-
cal literature). The genes are then represented as vec-
tors and the similarity between them is calculated as the
cosine of the vector angles. Thus, a pair of genes would
have a cosine score of 1.0 if they shared the exact same
abstracts in the collection. Gene sets in this method were
deemed functionally coherent when cosine values above a
given threshold (0.6) were often found with significances
measured by a statistical test (Fishers exact test). This
threshold was chosen based on the distribution of sim-
ilarity cosine scores in 1,000 random gene sets. Hence,
functional coherence here is derived essentially from the
supporting literature, thus making the method sensitive to
Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 3 of 11
the quality of the document corpus used. Regardless, the
method was used to obtain results similar to those pro-
duced by another literature-based functional coherence
assessing method [14].
Since functional annotation quality is paramount, [15]
developed a system to provide an annotation confidence
score for genome annotations. The system operates on
the basis of a genome comparison approach whereby
annotations in a target genome are scored in compari-
son with a reference genome. The gene alignments across
genomes are made via the BLAST tool with adjustments
for expected number of genes (different organisms have
different gene counts) and phylogenetic distance (closer
genomes typically share more genes than distant ones).
However, actual annotation similarity is derived from
free-text annotations which are converted into word vec-
tors that enable the calculation of a simple cosine simi-
larity measure. Both sequence similarity and annotation
similarity are combined into a single metric by applying
statistical techniques.
Despite the existence of these types of metrics the pro-
tein annotation landscape is often very heterogeneous in
terms of quality, specificity and completeness. Annota-
tion quality is related with the annotation method and
source used, e.g. defined by the different GO evidence
codes associated to each annotation. Annotation speci-
ficity relates to how specific or general an annotation
term is, and when in a protein set there is a clear dis-
proportion between general term annotations and specific
annotations, that set can be said to suffer from annotation
incompleteness.
In this work we concern ourselves mostly with the
aspects of annotation completeness and specificity. Given
that functional similarity is derived from semantic sim-
ilarity approaches over the annotation terms, it is also
relevant to define the concept of annotation agreement as
a measure of annotation homogeneity for a given set of
proteins. This metric, will naively measure the coherence
of a given set based on the fraction of shared annotation
terms between all proteins in the set, and thus will be
highly susceptible to the lack of annotation completeness.
We use this measure as a baseline whereas we introduce
other metrics to characterize the state of known func-
tional similarity of a given set and gauge the potential
state of annotation incompleteness. Hence, in this work
functional coherence is defined as a measure of functional
closeness (similarity) among all proteins in a set given the
current functional annotations within that protein set.
Methods
A functional annotation is defined as a pairing between
a gene product (protein) identifier and a term providing
some functional description. In this study, only themolec-
ular function term annotations from GO were considered
because the aim of this work lies closer to studying
one-dimensional annotation (as proposed by [16]) at the
molecular functional level in enzymes. Ideally, the func-
tional annotations over a given protein set should allow us
to infer biological relationships within the set. In order to
achieve that, it is convenient to have metrics that enable
us to compare how similar (or dissimilar) annotations
are within a given protein set. However, considering the
GO DAG structure it becomes apparent that measur-
ing functional relatedness via annotation is not a trivial
matter. Therefore, in order to help make such assertions
regarding functional relatedness, three main annotation
aspects were considered: completeness, agreement and
coherence.
Completeness
Any set of functionally related proteins, in which not all
proteins are annotated to the same specificity level, can
be considered to incur in a form of annotation incom-
pleteness. Figure 1a) illustrates such a situation. For a
hypothetical set of one hundred proteins, only one of the
hypothetical annotation terms (besides the root) anno-
tates all the proteins in that set. As the nodes get further
away from the root term, it can be seen that the num-
ber of annotations dwindles until it reaches the leaf terms.
And while any given protein does not need to have its
most specific function represented by a leaf term, it is
unlikely that a very generic term (such as a direct child of
the root term) is a full descriptor of its activity. However,
it is not trivial to determine this kind of incompleteness,
and only after determining or predicting new functional
activities can we definitively say that any given protein
(or set) was incompletely annotated. Thus, in this work
we limit the definition of completeness to the minimal
set of annotations evenly distributed among the pro-
teins in a set that characterizes the unifying functions of
that set.
This kind of annotation incompleteness can derive
from the fact that different protein annotation methods
are used, which provide different degrees of annotation
confidence. Therefore, annotation heterogeneity is cre-
ated accordingly to the annotation confidence level given
by each annotation method. For instance, a majority
of the automatic annotation methods typically create
more generic annotations. On the other hand, man-
ual curation is more likely to lead to more highly spe-
cific annotations. Additionally, the inherent research bias
towards more intensively studied model organisms and
biological processes can also help further this state of
incompleteness.
Agreement
Annotation agreement can be defined as the fraction of
annotations that are shared in a set of proteins. Hence,
Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 4 of 11
Fig. 1 Hypothetical GO graph. Terms are represented by nodes where the number within is the number of proteins (of a given set of 100)
annotated to that term. There are three situations represented: a annotation incompleteness, b annotation agreement and c annotation coherence
the annotation agreement of a given protein set (S) can be
computed using Eq. 1 as shown below:
annotation agreement (S) =
( t?
i=1
xi
)
× tN (1)
with xi as the number of annotations for a term i, N as the
total number of proteins annotated and t the total num-
ber of distinct annotation terms. Therefore, the greater
the amount of shared annotations the greater is the anno-
tation agreement. Figure 1b) illustrates a hypothetical full
annotation agreement situation. In this situation, each one
of the one hundred proteins is annotated to the same
exact annotation term set and thus that hypothetical set
achieves maximum or total annotation agreement. How-
ever, this is a naive metric that is also overly sensitive
to annotation incompleteness and even small amounts of
noise.
Coherence
Naturally, a set of proteins having a total annotation agree-
ment is also functionally similar, to the extent of its most
specific annotation terms. On the other hand, functional
similarity may not need to be so strictly defined. Addition-
ally, due to the above mentioned incompleteness issue and
the multi-functional nature of proteins, when measuring
functional similarity through annotation, it may be useful
to consider just some of the annotations as being func-
tionally characteristic of a given protein set. Furthermore,
for the purposes of this work, the concept of annotation
coherence is further refined and defined as the fraction
of shared annotations that define the core of the func-
tional activities that is common and most relevant and
thus able to characterize a given protein set, as a func-
tional cohesive group. Figure 1c) illustrates a hypothetical
full annotation coherence situation, where the grey shaded
nodes represent the functionally more relevant terms, or
the central functional cohesiveness of that set. However,
a single metric is too reductive in assessing these (and
other) different aspects of annotation that can dictate the
functional coherence of the annotation space in protein
sets. Therefore, in this work, we use a set of metrics and
respective interpretation strategies relating to these three
aspects of annotation described above in order to explore
protein (enzyme) annotation spaces.
Methodology
When it comes to capturing the relationship between
functional and sequence similarity, the different semantic
similarity metrics often present a similar behaviour, with
the main distinction among them being their resolution.
A comparison of several GO-based semantic similarity
metrics [17], found the graph-basedmeasure simGIC con-
sistently showing a high resolution (and providing about
19-44% increased resolution over the metric it derives
from, the simUI metric). In the work presented here, both
the simUI [18] and the simGIC [19] metrics are used
for assessing functional coherence and establishing sim-
ilarity baselines. Both metrics are pairwise, and as such
calculate the similarity between protein pairs through
Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 5 of 11
their extended set of annotations (direct annotations and
ancestral terms). Therefore, for a pair of proteins A and
B with their extended GO term annotation sets being
GO(A) and GO(B), respectively, simUI is computed by
dividing the number of terms in the intersection of GO(A)
with GO(B) by the number of terms in their union as
shown by Eq. 2.
simUI (A,B) = COUNTt?{GO(A)?GO(B)}COUNTt?{GO(A)?GO(B)} (2)
Equation 3 is used to compute simGIC which hence
is obtained by summing the Information Content (IC)
of each term in the intersection of GO(A) with GO(B)
divided by the sum of the IC of each term in their union.
simGIC (A,B) =
?
t?{GO(A)?GO(B)} IC(t)?
t?{GO(A)?GO(B)} IC(t)
(3)
As previously mentioned, in the [11] methodology, only
the most common and specific function of a set is cho-
sen as the most globally cohesive function. In this work
it is also assumed that not all functional annotations in
any given protein set (family) should characterize that
set. On the other hand, considering the frequent multi-
functional nature of proteins, in this work, a set of anno-
tation terms are selected in each protein set or family
as being its functional characteristic core. Therefore, the
strategy employed in this work to isolate the functional
characteristic cores in protein families was to resort to
term enrichment analysis. In particular, a Python imple-
mentation of the ubiquitous term-for-term enrichment
approach was developed. Sincemost of the study sets used
here are relatively small, and with several terms having low
expected frequencies (up to five expected observations)
the Fisher exact test was used to determine enrichment.
Hence, for each annotation term t in a given protein
set a 2x2 contingency table was generated according to
Table 1 with N being the number of proteins in the set,
M the number of proteins in all considered sets, nt the
number of proteins annotated with term t in the set and
mt the number of proteins annotated with term t in all
considered sets (mt). The statistical evidence of enrich-
ment was then postulated on the basis of the p-values
obtained from the Fisher exact test being smaller than the
chosen statistical significance (alpha). It should be noted
that on the term-for-term approach the graph nature of
GO will lead to a statistical dependency issue. That is,
for a given term annotating a certain number of pro-
teins, at least that same number of proteins or more will
also be annotated by the parental terms. Among the sev-
eral strategies available to mitigate this issue, here, the
Topology-based Elimination (Elim) strategy [20] was used.
The strategy consists in targeting significant leaves in
an annotation graph and iteratively subtracting the pro-
teins annotated there from parent terms up to the root
Table 1 Fisher exact tests 2x2 contingency table
Set Background
annotated nt mt-nt
not Annotated N-nt (M-N)-(mt-nt)
term. After all terms are processed new p-values are
computed for each term. Thus, this mitigates the sta-
tistical dependencies between nodes by downplaying the
statistical significance (and thus importance) of ancestor
nodes. This is a desired effect, since (for a similar level of
annotation quality) a more specific annotation is prefer-
able to a general annotation. Therefore, the Elim method
favours leaf terms found to be significant and at the
same time removes proteins annotated to significant child
terms from the parent terms annotation counts, which in
turn attenuates the childrens influence on the parental
terms. Additionally, it should be noted that the computed
p-values for the GO terms under this strategy are condi-
tioned on their children terms, and thus not independent.
Therefore, direct application of the multiple testing the-
ory is not possible. It is then preferable to interpret the
returned p-values as corrected or not affected by multiple
testing [20].
Coherence resilience assays
In order to test our approach Polysaccaride Lyase (PL)
families of the CAZy [21] database were used as a study
case. The protein (module) families in this database
are organized into 5 different classes: Glycoside Hydro-
lases (GH), GlycosylTransferases (GT), Carbohydrate
Esterases (CE), Polysaccharide Lyases (PL) and Carbo-
hydrate Binding-Modules (CBM). The CAZy database
version (c7-2011) that we used in our analysis has about
138,000 distinct UniProt identifiers distributed through
the families in these classes as shown in Table 2. The per-
formed assessments were limited to using the UniProt
identifiers in those families because of their direct map-
ping to GO term annotations. Thus, for the annotation
mapping we have used the GOA [22] annotation files
(version 2013-03). Within the CAZy database the PL
class is the one that is better characterized by the Gly-
cobiology community, in part due to its more tractable
Table 2 Number of protein UniProt identifiers (size) in each of
the classes in the CAZy database (ver. c7-2011)
Size
GH 70227
GT 55461
CBM 10907
CE 8110
PL 1766
Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 6 of 11
dimensions, as can be seen in Table 3. For this reason
we also have elected to perform our assays using this
class of families. We have run the coherence resilience
assays that we describe below only for families PL1 to
P12, PL16, PL17 and PL22 because these are the only
ones that met the minimal size requirement for our
assaying.
In order to perform our assays we subjected each of
these families (sets) to a degeneration procedure as illus-
trated by Fig. 2 where x% proteins in an original protein
set are replaced by random proteins. In our assay these
protein replacements were obtained randomly from the
remainder of the CAZy families. The degeneration pro-
cedure was applied in discrete levels of 10% protein
replacement (from 0% up to 100% protein replacement)
to each of the sets. Hence, each original protein family
(0% replacement) would gradually turn into a complete
random set (100% replacement). Consequently, for each
family the functional similarity is expected to degrade
progressively as the percentage of random replacement
(noise) rises. Subsequently, we have used these gradual
degeneration sets to assay the behaviour of each of the
Agreement, simUI [18], simGIC [4] and GS2 [7] metrics
along with two novel hybrid metrics, mUI and mGIC that
we introduce further ahead. For each of the discrete levels
of degeneration (noise) one hundred iterations were run
per family. During each iteration, both the original family
and the noise source were randomly sampled for the pro-
teins to keep and the replacement proteins, respectively.
The similarity was computed at the end of each itera-
tion for each of the assayed metrics and then averaged
for the total one hundred iterations. For all the assayed
metrics (simUI, simGIC, mUI and mGIC), the global set
results were obtained by averaging all the term pairwise
results within each protein set. The resulting average
Table 3 List of the protein families belonging to the PL class in
the CAZy database and their respective size (in number of
UniProt identifiers)
Family Size Family Size
PL1 491 PL12 80
PL2 34 PL13 7
PL3 229 PL14 38
PL4 45 PL15 10
PL5 37 PL16 22
PL6 24 PL17 33
PL7 82 PL18 5
PL8 184 PL20 6
PL9 148 PL21 9
PL10 84 PL22 42
PL11 84 unassigned 80
scores are shown in Fig. 3 as plots of similarity (functional
coherence) as a function of the percentage of randomly
replaced proteins.
Hybrid metrics
For this work two novel functional coherence metrics,
mUI and mGIC were developed. They are based on
the combination of semantic similarity metrics simUI
and simGIC and a term-for-term enrichment analysis as
described by the following algorithm:
1. INIT annotationGraph and
annotationGraph
2. FOR each term IN annotationGraph
3. EXECUTE enrichment analysis of term
4. IF term enriched
5. annotationGraph <- term
6. ENDFOR
7. mUI <- compute simUI of
annotationGraph
8. mGIC <- compute simGIC of
annotationGraph
The annotation graph for a protein set (family) being
measured is generated (line 1). For each term in the anno-
tation graph (line 2) enrichment analysis using a term-
for-term (with Elim adjustment) strategy is performed as
previously described. If a term is found to be statisti-
cally enriched (line 4) it is added to a derived annotation
graph (line 5). When both annotation graphs are pro-
cessed (line 6) the simUI and simGIC are applied to the
shadow graph (annotationGraph) resulting in the values
for the mUI andmGICmetrics, respectively (line 7 and 8).
Results and discussion
From the analysis of Fig. 3 it can be seen, as expected, that
the similarity reported by each metric generally decreases
as noise (in the form of random proteins) is increas-
ingly added (replacing the original proteins) in each of
the tested PL families. In this study, each of considered
metrics is scaled on a [0, 1] theoretical range. The aim of
our protein family degeneration assays is to observe two
main aspects for each of the metrics, noise resilience and
resolution. With noise resilience we check by how much
the reported values can vary given the same amount of
noise. As for resolution we register the difference between
the maximum and minimum values it can actually report
during our assays.
The Agreement metric is the least noise resilient met-
ric, as can be seen by both the generally low values it
reports and the steep declines after adding small amounts
of noise to family sets with previously high agreement.
Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 7 of 11
Fig. 2 Protein set degeneration procedure. For each set (family) a chosen percentage of the set original proteins is replaced with proteins drawn
randomly from outside the set
This property is most evident in mono-functional fam-
ilies like PL5, PL16 and PL17 and also PL12 where the
introduction of 10% random proteins produces a sharp
decline in the reported values. This occurs because this
naive metric only equates the average of annotation term
frequencies in each protein family (or set). Thismetric was
chosen and used as the overall baseline.
The simUI and its derivative simGIC, as expected, have
a similar behaviour because simGIC is a IC-weighted
version of simUI. Furthermore, in the obtained results
(Additional file 1) it is noticeable that simGIC presents
a greater resolution than simUI (average range of 0.57
against a range of 0.46, respectively, as can be com-
puted from Table 4), a behaviour that was also previously
reported by [17] in their assessment of semantic similar-
ity metrics. In contrast, the GS2 metric has the smallest
resolution (for the tested sets) of all the tested metrics
showing an average range of 0.18. In addition, to offering
a smaller range of values (and a thus lower resolution) it
is important to notice that reported values for this metric
fall within the 0.75-1.0 range of similarity. Given that it is
expected for protein (enzyme) families to have function-
ally similar proteins it would also be expected (and opti-
mal) that these families would display higher coherence.
However, when the unadulterated families are considered
some of them do not provide the necessary annotations
supporting such high global set functional coherence val-
ues, especially when considering values produced from
the 100% randomized sets.
The mUI and mGIC (such as the metrics they are
derived from) also display, as expected, similar behaviours
to each other. Their results measure the enrichment con-
tribution relative to the original semantic similarity met-
rics. In fact, for most of the tested PL families and their
respective degenerate sets the reported values are very
similar. However, unlike the other tested metrics mUI and
mGIC are resilient to noise (replacement with random
proteins). That is evident from the gradual curves in Fig. 3
which in most families plateau until higher levels of ran-
domization and typically only fall abruptly after addition
of 90% random proteins. This resilience to noise is con-
ferred by the term enrichment step which pre-selects only
the subset of proteins that are annotated with the terms
found to be statistically significant by the enrichment pro-
cedure. Thus, this is an important factor to consider when
analysing the results provided by these two metrics. As
they were engineered to capture local (subset) functional
coherence, for a comprehensive evaluation they should
only be used in an analysis that also simultaneously con-
siders the annotation coverage within the analysed set.
This also explains the observed peaks at high noise lev-
els in some of the families (PL2, PL6, PL9, PL11) where
a small number of terms annotates a small subset of pro-
teins and thus creates a local similarity effect. That is, at
high levels of random protein replacement the original
families are greatly degenerated because they lose the pro-
teins that were characteristic for the identity of that family
while, on the other hand, randomly gaining less related
proteins. Hence, if a couple of random proteins being
introduced happen to be very similar in terms of anno-
tations and those terms are also found to be statistically
enriched, then a new similarity core is introduced which
Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 8 of 11
Fig. 3 Plots of the average similarity as measured by six different metrics. For the first eight PL protein families (from the CAZy database) and their
derived sets. These sets were made by replacing the original proteins with increasing amounts (of 10% increments; 100 iterations) of random
proteins (taken from the CAZy database)
Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 9 of 11
Table 4 Difference between maximum and minimum values
reported for each tested metric (Agreement, simUI, simGIC, mUI,
mGIC, GS2) against each PL family and iterations of derived
respective sets created by insertion of increasing amounts of
random proteins (from CAZy) into the original families
Metrics PL1 PL2 PL3 PL4 PL5
Agreement 0.122 0.391 0.260 0.368 0.874
simUI 0.298 0.497 0.620 0.376 0.650
simGIC 0.356 0.539 0.825 0.458 0.853
mUI 0.139 0.214 0.671 0.353 0.801
mGIC 0.147 0.216 0.672 0.343 0.802
GS2 0.137 0.224 0.238 0.177 0.246
Metrics PL6 PL7 PL8 PL9 PL10
Agreement 0.405 0.201 0.180 0.058 0.178
simUI 0.386 0.432 0.548 0.207 0.368
simGIC 0.429 0.542 0.660 0.27 0 0.484
mUI 0.469 0.501 0.329 0.368 0.564
mGIC 0.474 0.505 0.285 0.372 0.559
GS2 0.175 0.129 0.224 0.080 0.146
Metrics PL11 PL12 PL16 PL17 PL22
Agreement 0.229 0.771 0.831 0.869 0.400
simUI 0.108 0.644 0.613 0.649 0.443
simGIC 0.122 0.838 0.829 0.853 0.521
mUI 0.378 0.744 0.903 0.831 0.494
mGIC 0.373 0.741 0.905 0.831 0.501
GS2 0.054 0.247 0.211 0.248 0.191
results in the appearance of those peaks of high similar-
ity. However, for this work this behaviour is advantageous
because the underlying assumption is that each protein
family shares core annotations that define the group role
of that set of proteins. Thus, by using a term enrichment
technique the purpose is to target and select these core
annotation terms. The proteins annotated by these iden-
tified core annotation terms can then, for instance, be
used for annotation extension within that set as previ-
ously proposed [23]. Thus, according to that proposal, for
an hypothetical partially annotated protein set (with an
expected degree of functional relatedness) themUI/mGIC
metrics can be used to identify the functional core of
that set while reporting its functional similarity. If that
core, reports a high similarity value and also provides
enough statistical power (number of associated protein
sequences) it can be used to create, for instance, a Hidden
Markov Model profile model. Subsequently, that model
can potentially be used as a classifier in order to extend
annotations from the core to the sub-annotated sequences
in the original measured protein set.
Defining a completeness state and quantitatively mea-
suring it is a challenging task considering the complex-
ity in generalizing rules needed to detect it. Instead we
approach it only qualitatively by analysing each different
protein set, case-by-case by relying on domain knowl-
edge (confirmed and expected functional associations)
and then making empirical assertions about the state of
annotation completeness of each protein set. For that pur-
pose we use GRYFUN [24], a web application that we have
previously developed. This application allows for anno-
tation visualization coupled with statistical assessment
(term-by-term enrichment) and is used to produce anno-
tation graphs like the one shown in Fig. 4. The graph
portrayed in Fig. 4 subsumes all the GO terms (from the
molecular function ontology branch) annotating a set of
PL10 family proteins. Unlike the typical GO graph where
the edges point towards their parent terms, here the edges
point towards their children and have widths proportional
to the number of proteins annotated to each successive
child term. The purpose is to convey the annotation flow
and easily be able to identify annotation bottlenecks, or
terms where annotation might have stopped despite the
expectation that more proteins in that set could have been
annotated to children terms of these bottlenecks.
For the case of the PL10 family set portrayed in Fig. 4
the bottleneck annotation is on the term lyase activity.
Domain knowledge indicates that this term should anno-
tate each protein in this family (e.g. the PL10 family is part
of the Polysaccharide Lyases). However, this annotation
term is relatively generic and considering the proportion
of proteins not annotated with children of this term (as
can be easily seen from the graph) it is fair to assume
substantial annotation incompleteness. Additionally, con-
sidering the plot in Fig. 3 that represents the degeneration
of the PL10 set, it can be seen that the values for mUI
and mGIC actually increase along with the degeneration
of the set. As previously explained the enrichment process
of the mUI/mGIC algorithm considers only a protein sub-
set of the target set beingmeasured. Hence, it is important
to consider other metrics (for instance the parent metrics
simUI/simGIC) in tandem with these novel metrics for a
global assessment of functional coherence in a set. Nev-
ertheless, these novel metrics allow the identification of
core activities which can potentially be extended to more
sequences within the original set.
Conclusions
Measuring the functional similarity between proteins
based on their annotations is a non trivial task. Sev-
eral metrics exist but due to characteristics both intrinsic
to the nature of graphs and extrinsic natures related to
the process of annotation each measure can only capture
certain functional annotation aspects of proteins. Hence,
when trying to measure the functional coherence of a set
Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 10 of 11
Fig. 4 GRYFUN annotation graph. Annotation of GO molecular function ontology graph generated by the GRYFUN web application for a set of
proteins from the PL10 family
of proteins a single metric is too reductive. Therefore, it
is valuable to be aware of how each employed similarity
metric works and what similarity aspects it can best cap-
ture. Here we test the behaviour and resilience of some
similarity metrics.
Additionally, we propose a comprehensive approach
at determining functional coherence in protein sets
(families) based not only on metrics but also statistics
(term enrichment) and visualization coupled with domain
knowledge-based empirical assessments.
Furthermore, we propose two novel metrics mUI
and mGIC that combine two of the above mentioned
approaches, semantic similarity metrics and term enrich-
ment. The goal is to capture protein subsets within fam-
ilies (or other functionally related sets) that characterize
that family (or set), which can subsequently be used for
annotation extension for potentially sub-annotated pro-
teins within the same family (or set).
The proposed approach is modular and can be inte-
grated with other annotation methodologies mostly as a
pre-processing step. In the future, we will be implement-
ing both mUI and mGIC (along with other) metrics into
our web application GRYFUN. This will more easily cap-
ture the annotation functional cores in protein sets and
pipeline them to a custom annotation extension module
based on HMM profiles that we are currently developing.
Additional file
Additional file 1: Average similarity as measured by six different metrics
for each of the discrete levels of noise. (XLS 50 kb)
Acknowledgements
This work was supported by the Portuguese Fundação para a Ciência e
Tecnologia through a PhD research grant [PhD grant ref. SFRH/BD/48035/
2008 to H.P.B] and three research centers Strategic Projects funding [project
UID/MAT/00006/2013 (CEAUL) to L.S. & UID/MULTI/04046/2013 (BioISI) to
L.A.C. & UID/CEC/00408/2013 (LaSIGE) to H.P.B. and F.M.C.] and by the
European Commission [BiobankCloud project under the Seventh Framework
Programme grant #317871 to F.M.C.]. Furthermore we like to thank Faculdade
de Ciências da Universidade de Lisboa for the tuition support for H.P.B.
Authors contributions
HPB implemented all metrics and performed the evaluation, and the
remainder authors supervised the work. All authors read and approved the
final manuscript.
Competing interests
The authors declare that they have no competing interests.
Author details
1LaSIGE, Faculdade de Ciências, Universidade de Lisboa, Lisboa, Portugal.
2CEAUL, Departamento de Estatística e Investigação Operacional, Faculdade
de Ciências, Universidade de Lisboa, 1749-016 Lisboa, Portugal. 3BioISI -
Biosystems & Integrative Sciences Institute, Faculdade de Ciências,
Universidade de Lisboa, 1749-016 Lisboa, Portugal.
Received: 8 September 2015 Accepted: 17 May 2016
Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 11 of 11
Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 
DOI 10.1186/s13326-016-0072-2
SOFTWARE Open Access
PhenoImageShare: an image annotation
and query infrastructure
Solomon Adebayo1, Kenneth McLeod2, Ilinca Tudose3*, David Osumi-Sutherland3, Tony Burdett3,
Richard Baldock1, Albert Burger2 and Helen Parkinson3
Abstract
Background: High throughput imaging is now available to many groups and it is possible to generate a large
quantity of high quality images quickly. Managing this data, consistently annotating it, or making it available to the
community are all challenges that come with these methods.
Results: PhenoImageShare provides an ontology-enabled lightweight image data query, annotation service and a
single point of access backed by a Solr server for programmatic access to an integrated image collection enabling
improved community access. PhenoImageShare also provides an easy to use online image annotation tool with
functionality to draw regions of interest on images and to annotate them with terms from an autosuggest-enabled
ontology-lookup widget. The provenance of each image, and annotation, is kept and links to original resources are
provided. The semantic and intuitive search interface is species and imaging technology neutral. PhenoImageShare
now provides access to annotation for over 100,000 images for 2 species.
Conclusion: The PhenoImageShare platform provides underlying infrastructure for both programmatic access and
user-facing tools for biologists enabling the query and annotation of federated images. PhenoImageShare is
accessible online at http://www.phenoimageshare.org.
Keywords: Image annotation, Genotype-phenotype associations, Semantic annotation
Introduction
As reference genomes and large-scale programs to gen-
erate model organism mutants and knock-outs are com-
pleted, there have been parallel and complementary
efforts from projects such as the International Mouse
Phenotyping Consortium (IMPC) and the Asian Mouse
Phenotyping Consortium (AMPC) to establish and codify
phenotype with genomic coverage [1]. Current pheno-
typing efforts typically deliver a variety of images with
annotations describing the phenotype on display. These
are then stored in independent databases associated with
the primary data. These databases may be searched indi-
vidually, albeit there is no mechanism for integration,
cross-query or analysis, especially with respect to human
abnormality and disease phenotypes. We have developed
PhenoImageShare (PhIS) to address this problem. PhIS
*Correspondence: tudose@ebi.ac.uk
3European Bioinformatics Institute (EMBL-EBI), European Molecular Biology
Laboratory, Wellcome Trust Genome Campus, CB10 1SD Hinxton, UK
Full list of author information is available at the end of the article
is a cross-browser, cross-repository platform enabling
semantic discovery, phenotypic browsing and annota-
tion of federated phenotypic images. PhIS provides a
centralised repository that stores a limited set of meta-
data including links to the originating resource, and the
annotations generated through the use of PhIS. As such,
the complete PhIS system is a federated resource that
includes the central PhIS database and the repositories of
the underlying image sources.
Resources such as OMERO [2] allow users to store their
images, but do not provide ontology-enabled tools. Fur-
ther, images are often siloed by imaging methodology or
domain [3] and access to multiple different image repos-
itories may require bespoke development against multi-
ple modes of programmatic access which may evolve as
resources change. PhIS achieves this level of integration by
using a lightweight annotation document structure, which
can then be exposed through standard query engines such
as Solr [4]. PhIS is species and imaging technology neutral
and currently provides access to 117,982 images federated
© 2016 Adebayo et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 Page 2 of 10
from four different data resources with 53,000 regions
of interest (ROI) associated to anatomy or phenotype
ontology term annotations. These can be accessed via the
web GUI or programmatically via web services. To date
PhIS is populated with images from Drosophila and three
different mouse projects.
Related work
Whilst no existing service or tool is directly comparable
to PhIS a number of products may be considered simi-
lar. The Yale Image Finder (YIF) [5] offers an interface
to query over 1.5 million images from open access jour-
nals. A user has a few options to restrict or loosen the
search to image description, whole article or abstract.
NCI Visuals Online [6] provides access to about 3000
images with the possibility to search the image descrip-
tions making use of common text-search functionality
such as quotations for exact matches, term exclusion,
multiple keywords, defaults to case insensitive search,
stemming, et cetera. These are useful resources that
address a different set of needs from PhIS. Ontology-
backed search (i.e. semantic search) is not supported,
although an advanced text search exists. YIF does not
support the display of Regions Of Interest (ROI), online
annotation, image submission or resource integration via
an API.
The Semantic Enrichment of Biomedical Images (SEBI)
[7] semantically enriches image meta-data from YIF
and enables search over that meta-data. The SEBI plat-
form is based upon a number of different modules.
Collectively these modules facilitate automatic annota-
tion (i.e., semantic enrichment) by using Semantic Auto-
mated Discovery and Integration (SADI) [8] enabled
services to pull related information from other resources.
When automatic annotation fails, there is provision for
crowd-based annotation. The generated data is stored
within a triplestore called iCryus [9] that can be queried
through a SPARQL endpoint or navigated via a RDF
browser. Both SEBI and iCyrus focus on DNA/protein
sequence images rather than the phenotypic images
found within PhIS. Another difference is the approach
to annotation creation. SEBI/iCyrus take the meta-data
associated with an image and extend it by using other
services available on the Semantic Web. PhIS operates at
a different level, helping to create and publish human-
expert-generated annotations. A SEBI/iCyrus-like plat-
form for phenotype images would be complementary to
PhIS. An iCyrus-like phenotoype tool would pull image
data from PhIS in same way that iCyrus pulls data
from YIF.
Another framework is proposed by Kurtz et al. [10], with
support for annotation suggestion and semantic annota-
tions, with focus on similarity metrics but with no link to
an application.
Wang et al. [11] have addressed the need for an
ontology-assisted image-annotation tool and have pro-
duced software that supports RadLex [12] backed annota-
tion of radiology images.While the project was successful,
its scope is restricted to radiology annotations and is not
accessible online to external users. It does not aim to offer
further integration with other resources through any API.
A set of commercial desktop applications such as Osirix
[13] and Amira [14] offer segmentation assistance and
plain-text annotations, either integrated or through plu-
gins. AISO [15] is a desktop tool that was developed
for plant image annotation and supports ontology use.
AISO is restricted to the plant ontology, which it accesses
through its own web service. OMERO offers annotation
support both on the web client and the desktop applica-
tion but no ontologies are integrated. The OMERO server
is not an online service but a server distribution so every
group or institution needs to host its own server. Phe-
noImageShare is a complementary resource that can be
used in association with OMERO hosted images.
There is a wide variety of public image portals [3]
with some focusing on phenotype images: IMPC portal,
EMAGE [16, 17], The Cancer Genome Atlas [18] to men-
tion a few. All of these focus on species- or project-specific
images and image submission is not open to the outside
user. IMPC and EMAGE support ontology annotation but
have limited or no support for region of interest (ROI) dis-
play on the original image, however EMAGE does support
spatial mapping onto a standard atlas for the purposes of
spatial comparison and query.
We have identified the need for a public and easy to use
web service that can federate cross-species, cross-project
images, with open image submission and powerful seman-
tic search. There is also a clear need for an online image
annotation tool with support of ontology terms and differ-
ent ontologies. To the best of our knowledge such a tool
does not exist.
Methods
The PhIS platform consists of three main software lay-
ers: the User Interface (UI), the integration layer and
the backend services. The UI components provide an
intuitive Graphical User Interface (GUI) to query cross-
platform image meta-data. The GUI also provides a basic
ontology-enabled annotation service to allow image own-
ers, and third parties to annotate images using ontologies
for their own use and also for sharing images and anno-
tations between consortia and collaborators. The inte-
gration layer is the one which consolidates access to the
different backend services into one access point, used by
the GUI. The backend services provide API methods to
query and annotate the data as well as a data importmech-
anism. The architecture described is also represented in
Fig. 1.
Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 Page 3 of 10
Fig. 1 PhIS architecture. The different software components part of the PhIS platform. The arrows describe the collaboration of the components and
the direction of the function calls. For example the UI uses the methods provided by the IQS but the IQS alone cannot trigger any actions in the UI.
The annotation submission API needs to be secured in order to keep the integrity of the database but the query API is not restricted
Image discovery infrastructure
The query API offers a simple REST-like interface to
access the data in multiple ways. Approximate search
methods are provided with the purpose of image discov-
ery but also specific interfaces for example search by id
are available. The API provides an endpoint for query-
suggestion options on an input string. This was designed
to follow the following order: 1) exact matches, 2) phrases
that start with the given string as a word, 3) phrases that
start with the given string as part of the first word, 4)
exact match in a word in the phrase, other than at the
start of it, 5) matches where another word in the phrase
starts with the given string.To illustrate this through an
example, for the string eye, the suggestions will come in
the following order: eye (Case 1), eye hemorrhage (Case
2), eyelids fail to open (Case 3), TS20 eye, TS21 eye,
left eye, right eye, narrow eye opening, abnormal eye
development (Case 4), abnormal eyelid aperture (Case
5). We have implemented this after a series of formative
sessions with users. We achieve this sorting by apply-
ing different boost factors to Solr text fields tokenized
in different ways. Text matching is case insensitive in all
cases.
The database, XML schema (XSD) and Solr represen-
tations share a common high-level schema such that the
information storage is split three ways: image informa-
tion, channel information and ROI/annotation informa-
tion. This is represented in a simplified way in Fig. 2.
Provenance information is an important part of the
schema. PhIS currently implements a pragmatic sim-
ple model, covering image/annotation creator, annota-
tion editor, image and annotation creation/last edit date,
repository or resource of origin or publication if one is
provided for the image.
The API offers JSON responses, which is the stan-
dard for modern APIs, is straight forward to consume
by the UI, has the advantage of human readability and
is also the best fit for our Solr-backed infrastructure. As
the API gains traction and users require other formats,
new response types can be evaluated and provided. The
approach used to create the datamodel for the Annotation
and Image Markup project [20] has been applied to PhIS.
Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 Page 4 of 10
Fig. 2 PhIS schema. The image entity is the core meta-data object
and stores generic, immutable meta-data such as imaging procedure
and details, data source, credits and sample information, genotype or
sample preparation techniques. The channel entity stores visualization
information, such as tags or labels and genomic information when
needed, such as for expression images. The ROI entity holds both the
coordinates, within the given image, and the annotation values
The implementation is in Java and the query functionality
is primarily via Solr. Solr is built upon Lucene [19], which
is an open source indexing and search engine. Lucene-
based technology powers a number of existing image
repositories, e.g., OMERO and Yale Image Finder both use
Lucene to search for images. The authors have experience
of using Solr to build image repositories from their work
on the IMPC portal. Solr offers fast, flexible, robust, open
source search solutions with scaling options such as Solr
cloud. Through a varied array of features Solr enabled us
to build a rich search and navigation experience with little
resources.
Spatial annotations
Many phenotypes occur in a particular spatial loca-
tion, which can be described using either an anatomy
(ontology) term or by being mapped onto a biomedical
atlas [20]. Currently, PhIS only supports anatomy-based
descriptions, with atlas-based descriptions targeted for a
future release. Anatomy-based descriptions are simpler to
use, but less precise.
Should a PhIS submission be missing a spatial descrip-
tion, it may be possible to infer an appropriate anatomy
term from a supplied phenotype term. For example, given
the phenotype term cataract the anatomy term eye can
be inferred. This inference relies upon the bridge ontolo-
gies provided by the Monarch initiative [21]. Inferred
spatial annotations are pre-computed and stored within
Solr allowing them to be queried by a user.
User interface
PhIS delivers its functionality through a highly respon-
sive and easy-to-use web interface built upon standard
technologies such as Python, Django, JavaScript, Asyn-
chronous JavaScript (AJAX), Bootstrap [22] and Flat-UI
[23]. Portability, reusability and responsiveness are at the
core of GUI design considerations.
Throughout the development of PhIS a series of for-
mative evaluation sessions have been used to determine
and then prioritise requirements. This included the cre-
ation of a series of tasks against which the functionality
of the search capability can be tested. Additionally the
search functionality has undergone a small scale sum-
mative evaluation. Feedback from this has been inte-
grated within the current beta release. Development
of the annotation tool started after the search func-
tionality, and so the annotation tool is still undergoing
an iterative process of formative evaluation and devel-
opment. A comprehensive summative evaluation that
tests the complete workflow (search and annotation)
will be undertaken when the annotation tool reaches
maturity.
There are four distinct elements to the PhIS GUI, and
this section discusses each one in the order a user encoun-
ters them in a typical workflow.
Landing page
When arriving at www.phenoimageshare.org a user is
greeted by a visual summary of the data held within
the PhIS repository that sits below a search box. The
visual summary consists of three pie charts that collec-
tively quantify and classify the types of images stored.
The three dimensions visualised are imaging method,
sample type (mutant vs. wild type) and image type
(expression vs. phenotype). Clicking on a block within
one of the charts performs a query. For example, click-
ing on the macroscopy block will display all the images
that have meta-data indicating they are macroscopy
images.
Alternatively, the user can use the search box to query
PhIS. A query may consist of free-text, gene or allele
symbols, anatomical or phenotypic terms from standard
ontologies. An query-suggestion facility integrated within
the search box provides the user with a drop-down list
of terms predicted from the set of existing annotations.
Search results are displayed in real-time on a dedicated
page.
Search interface
This page (see Fig. 3) displays a list of images that meet the
users query criteria, and provides the ability to filter that
list or navigate through it. For each image in the list a brief
summary of the images meta-data is displayed alongside
a thumbnail.
Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 Page 5 of 10
Fig. 3 Query interface with ontology-based search and faceting functionality. Search results for a phenotype (abnormal epidermis stratum basale
morphology) (a) is filtered by development stage (postnatal) (b) and confocal microscopy imaging method (c). Example query and help on using
facets (d) are also provided
To reduce the images returned by a query the user can
apply a range of predefined filters or a faceted search. Both
the filters and the search are facet-based and meta-data
driven. Nested checkboxes allow the user to reduce the
images displayed using filters including imaging method,
stage, species and resource (i.e., the online resource from
which PhIS obtained the image). Faceted search is avail-
able for anatomy, gene/allele and phenotype. If the user
searches for heart in the anatomy facet search box, only
those images from the original query with an anatomy
annotation featuring the term heart will be displayed.
The filters and faceted search can be combined to deliver
powerful customised queries.
To find an image of interest the user can either adjust
the filters/search or (s)he can undertake a completely new
query using the main search box (now in the menu bar).
When the user identifies an image of interest in the search
results, clicking on that image reveals more information in
the Image Interface page.
Image interface
In the Image Interface (see Fig. 4) a single image is dis-
played alongside a more comprehensive set of meta-data
for the image. Meta-data shown includes data captured
from the source (e.g., genotype, phenotype and prove-
nance) and annotations generated by users of PhIS. The
meta-data contained within PhIS is a subset of the data
available at the originating resource, thus PhIS provides a
link back to the originating resource enabling the user to
investigate further.
Image annotations can be exported in a variety of for-
mats, with new formats to be added. The user can add
their own annotation to this image using the PhIS annota-
tion tool.
Annotation interface
The annotation interface allows the users to semantically
annotate an image. Annotations can apply to the whole
image or to a region of interest within the image. ROIs are
indicated by the user drawing a rectangle upon the image.
This rectangle is treated as part of the annotation and is
stored within PhIS for later reproduction. When creating
an annotation, the user has the ability to select a series
of ontology terms obtained through the BioPortal widget
[24]. All annotations submitted through the PhIS inter-
face will appear on the website and become searchable
instantly.
Currently the annotation tool only supports low resolu-
tion images; however, it has been implemented with the
goal of displaying high resolution images via OpenSead-
ragon [25]. OpenSeadragon is an extensible JavaScript
library providing high quality zooming functionality that
will enable PhIS to support high resolution images.
Because it works well with OpenSeadragon, and required
minimal extension, FabricJS [26] provides the function-
ality for drawing ROIs on the images. Other possible
options (e.g., Annotorious [27] or AnnotatorJS [28]) were
rejected because their support for OpenSeadragon was
too immature at the time of testing.
Figure 5 presents a screenshot of the Annotation Inter-
face in use.
Data and ontologies
To make data available through PhenoImageShare batch
submissions are generated in XML format by the
resources wishing to submit data to PhIS. Our data
releases are versioned and the current one is listed in
the lower right corner of the page, which links to more
details about each release. Old releases can be accessed
Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 Page 6 of 10
Fig. 4 Detail view, showing meta-data, provenance and annotations associated with one of the images returned in the search shown in Fig. 2
programmatically, through the API. Data sources provide
PhenoImageShare with up-to-date XML exports of their
data, following the PhIS schema (XSD). Producing the
XMLwill vary based on the amount of data exported while
the processing on the PhIS server only takes a few min-
utes. However, we do data releases so there might be a few
weeks lag between themoment the export is ready and the
moment the data is published live. It is up to the data sub-
mitter how often updated XML exports are submitted.We
then perform XML and semantic validation, followed by
data enrichment before indexing. This includes addition
of synonyms and ancestor relationships to the Solr index
allowing an improved semantic search. Some examples of
semantic validation include checking if the term provided
as an anatomy annotation comes from an anatomy ontol-
ogy or checking if the label provided with the ontology
id matches. Ontology curators sometimes update the pre-
ferred labels and databases have trouble keeping up with
that. As part of our import process we select the most
up to date label and index the old one as a synonym. If
the ontology term was deprecated and a replacement one
is suggested we use the replacement term. In order to
Fig. 5 Annotation interface in edit mode. Context menu (a) is available to user via right-click on selected terms from an ontology (b). User appends
selected ontology terms to the drawn region of interest (c)
Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 Page 7 of 10
achieve this semantic manipulation of the data we make
extensive use of the OWL API [29] and the Elk reasoner
[30] to precompute relations that we then index in Solr.
This approach offers more restricted semantic capabilities
than RDF/SPARQL, but it is faster to set up and covers our
use-case needs, both in terms of semantic and text-based
search capabilities.
Data
The current PhIS release contains data from four sources,
presented with more details in Table 1.
 The TRACER database [31] contains imaging for
embryos carrying a transgenic insertion generated by
the Sleeping Beauty transposon-based system.
 Images from the Wellcome Trust Sanger Institute,
generated as part of KOMP2 [32] focus on single
gene knock-outs. Genotype, anatomy and phenotype
annotations are provided for these data.
 The EMAGE database provides embryo gene
expression images.
 Virtual Fly Brain (VFB) [33]. The VFB dataset offers
an interesting use-case for cross species integration,
but also adds value to our anatomy coverage with its
focus on neuroanatomy and gene expression in the
adult Drosophila melanogaster brain.
Ontologies
To add semantic value (i.e. synonyms, hierarchical rela-
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2016) 7:33 
DOI 10.1186/s13326-016-0078-9
RESEARCH Open Access
The Proteasix Ontology
Mercedes Arguello Casteleiro1, Julie Klein2 and Robert Stevens1*
Abstract
Background: The Proteasix Ontology (PxO) is an ontology that supports the Proteasix tool; an open-source
peptide-centric tool that can be used to predict automatically and in a large-scale fashion in silico the proteases
involved in the generation of proteolytic cleavage fragments (peptides)
Methods: The PxO re-uses parts of the Protein Ontology, the three Gene Ontology sub-ontologies, the Chemical
Entities of Biological Interest Ontology, the Sequence Ontology and bespoke extensions to the PxO in support of a
series of roles: 1. To describe the known proteases and their target cleaveage sites. 2. To enable the description of
proteolytic cleaveage fragments as the outputs of observed and predicted proteolysis. 3. To use knowledge about the
function, species and cellular location of a protease and protein substrate to support the prioritisation of proteases in
observed and predicted proteolysis.
Results: The PxO is designed to describe the biological underpinnings of the generation of peptides. The
peptide-centric PxO seeks to support the Proteasix tool by separating domain knowledge from the operational
knowledge used in protease prediction by Proteasix and to support the confirmation of its analyses and results.
Availability: The Proteasix Ontology may be found at: http://bioportal.bioontology.org/ontologies/PXO. This
ontology is free and open for use by everyone.
Keywords: Ontology, Proteasix, Protease prediction, Peptide, Cleaveage site, Open biomedical ontologies
Background
Proteases are enzymes that catalyze peptide bond cleav-
age and this activity can lead to the generation of protein
cleavage fragments or peptides. Proteases have a wide
spectrum of specificity [1]. The human genome encodes
over 550 different proteases, participating in many dif-
ferent biological processes, including protein degradation,
immunity response, regeneration or cell division and are
involved in diseases such as cancer, inflammation and
cardiovascular disease [2, 3].
Body fluids (e.g. serum, urine, cerebrospinal fluid)
contain thousands of protein fragments and disease-
associated peptides. The proteolyticmechanisms that lead
to the generation of these fragments may be associated
with diseases, and are not well described in the literature.
Further insight into the proteases implicated in peptide
generation may help in understanding some diseases.
*Correspondence: Robert.Stevens@manchester.ac.uk
1School of Computer Science, University of Manchester, Oxford Road, M13
9PL Manchester, UK
Full list of author information is available at the end of the article
The particular function and specificity of each protease
are defined by their binding to a characteristic amino
acid motif that forms a cleavage site in the protein tar-
get [4]. Knowledge about proteases and their substrates
and cleavage sites is scattered across publications and
databases. These different resources do not permit cleav-
age site information to be retrieved from peptide sequence
input automatically and thus elucidating the proteases
implied in peptide production is difficult.
The Proteasix tool [5, 6], is an open-source peptide-
centric tool that can be used to predict automatically the
proteases involved in the generation of proteolytic cleav-
age fragments (peptides). Proteasix is a tool that uses
protease/cleavage sites (CS) associations established by
either observations or predictions to suggest the proteases
implicated in the generation of a peptide. Proteasix does
this by using the N- and C-terminal sequences of pep-
tides that are reconstructed using information from the
UniProt knowledge base [7] to identify the possible pro-
teases that were involved in their generation [5]. Obser-
vations of protease/CS combinations were extracted from
© 2016 Arguello Casteleiro et al.Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2016) 7:33 Page 2 of 7
CutDB [8], UniProt and the literature. When a previous
observation has not been established, Proteasix calcu-
lates the probability of protease/CS association by using
the MEROPS [9] and BRENDA [10] databases. Pro-
teases exhibit varying binding affinities for amino-acid
sequences, ranging from strict restriction to one or few
critical amino-acids in given positions, to generic bind-
ing with little discrimination between different amino
acids [5].
The predictions currently made by Proteasix are agnos-
tic as to the taxon of the organism whence the peptides
come, the cellular location of the predicted proteases and
the proteins theymay cleave. Also, the function of the pro-
teases, e.g., whether they are an endo- or exo-peptidase
is not taken into account. This is the kind of knowledge
an ontology is able to provide. Thus the new version of
Proteasix uses the Proteasix Ontology (PxO) to make this
knowledge available to its algorithm.We go on to describe
the PxO and its role in Proteasix.
Competency questions for the PxO
The PxO is written in theWebOntology Language (OWL)
[11] using the Protégé [12] version 5.0.0 beta 17 editor.
In creating PxO we wished to undertake as little de novo
ontology development as possible and to take advantage
of the work already done in annotating gene products with
the Gene Ontology (GO) [13]. This implied a strategy of
re-using relevant Open biomedical Ontologies Consor-
tium [14] (OBO) ontologies where possible, together with
relevant annotations. The choice of which of the OBO to
use was driven by a set of competency questions (CQ)
that the PxO should fulfil. Once chosen, relevant por-
tions of the ontologies were taken and extended in a way
that accommodated the CQ, making appropriate commit-
ments to the ontology used. The resulting PxO was then
evaluated against the CQ.
To obtain the observed and predicted proteases respon-
sible for the generation of peptides, the PxO needs to
answer the following CQ:
1. What are the known protease and their target cleav-
age sites (observed and/or predicted)?
2. For a given peptide and protein from which it was
derived, what are the cleavage sites that led to its pro-
duction and is it the product of observed or predicted
proteolysis?
3. What are the function, species and cellular location
for both proteases and their substrate proteins?
4. For a given protease, what are its cleavage site speci-
ficity?
5. Given an amino acid, what are its biochemical prop-
erties?
6. For a protease predicted to have generated a peptide,
what are its function and the processes in which it is
known to participate?
The Additional file 1 provides the ELK reasoner times
and shows the SPARQL SELECT queries for the CQ
and the execution times for the CQ using JENA
ARQ [15].
Reuse of ontologies fromOBO
To enable these competencies to be answered the PxO re-
uses parts of some of the OBO; PxO uses the OWL [11]
versions. After downloading the OWL files, a selection
of class names (without deprecated classes); class expres-
sions; class definitions; and annotation assertions were
extracted. Where only a portion of the source ontology
was required to support the CQ in PxO, we program-
matically extracted a top-module [16]. A top-module is
used as in the PxO only a restricted query supporting
a CQ needs to be answered, rather than a query that
necessitates all entailments from a signature to be pre-
served. The following OBO or their parts were used in
PxO:
1. The Protein Ontology (PRO) [17]  Reuse
of Protein(PR:000000001) and proteolytic
cleavage product(PR:000018264) that are both
subclasses of PROs amino acid chain (PR:000
018263). In order to follow the PRO annotation
guidelines [18], the relationships participates
in; located in; and has function were
substituted with their Relationship Ontology
equivalents. The use of the PRO supports all the
CQ.
2. Relationship Ontology (RO) [19]  Where possi-
ble, PxO uses object properties from RO. For PxO,
this includes has_function, has_location,
participates_in and only_in_taxon.
3. the three Gene Ontology (GO) sub-ontologies [20] 
First, a class name extraction was performed based on
the three GO namespaces cellular component;
molecularfunction; andbiological process.
As an example of usage in the PxO, the GO
class peptidase activity (GO:0008233) was
used to define protease molecular function, while
proteolysis (GO:0006508) was used to describe
the biological process of peptide production. Use of
the GO supports CQ 1, 3 4 and 6.
4. Chemical Entities of Biological Interest Ontology
(ChEBI) [21]  Reuse of chemical entity
(CHEBI: 24431) that has as subclass molecular
entity(CHEBI: 23367). PROs amino acid chain
(PR:000018263) and amino acid were made
subclasses of ChEBIs molecular entity(CHEBI:
23367). The twenty amino acids are also taken from
ChEBI. Some amino acids are interchangeable at
a certain CS position, for they may have identical
biochemical properties. This supports the answering
of CQ 5.
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2016) 7:33 Page 3 of 7
5. Phenotypic Quality Ontology (PATO) [22]  Classes
from PATO were reused to describe the prop-
erties of the amino acids. Hence, classes such as
electric charge (PATO:0002193), polarity
(PATO:0002182) as well as their superclasses like
molecular quality (PATO:0002182) and
subclasses such as negative charge (PATO:
0002196) were extracted. The twenty amino acids
from ChEBI are classified taken PATOs descendants
from molecular quality and side chain
structure, which is outside of PATO. This helps to
answer CQ 5.
6. the Sequence Ontology (SO) [23]  Cleavage
site regions and C- and N-terminus of polypep-
tide sequences were described using polypeptide
region (SO:0000839) to describe Cleavage site
region. Moreover, the key classes to link proteins
from Uniprot with gene names were described using
gene (SO:0000704) along with its subclass protein
coding gene (SO:0001217). Based on superking-
dom and subclasses, i.e. upper-levels of the UniPro-
tKB Taxonomy, the gene names are classified, and
thereby, obtaining a hierarchy with three levels. These
classes were used to support the CQ 1, 2, and 4.
7. The PRO proteins are organised based on taxon
organism, and therefore, new classes under the PRO
proteinclass were created according to the upper-
levels of the UniProtKB Taxonomy [24]. These classes
were used to support CQ 3.
8. GALEN ontology [25]  A medical ontology outside
of OBO, which can be downloaded from BioPortal
[26]. Reuse of the class KnowledgeStatus and the
relationship hasKnowledgeStatus to represent
observed or predicted proteolysis. To describe the
level of confidence associated with a predicted
proteolysis, the relationship hasConfidence
LevelStatus the class ConfidenceLevel
Status were also extracted. these classes were used
to support CQ 1.
PxO axioms and axiom patterns
Peptide and protein: A peptide, also known as
proteolytic cleavage productin PRO, is
described in the following way in PxO (all OWL frag-
ments are represented using Manchester OWL Syntax
[27]):
Class :  p r o t e o l y t i c c l e a v a g e product 
SubClassOf :
 amino ac i d chain  ,
 output of  some p r o t e o l y s i s ,
 d e r i v e s from  some p ro t e i n
Knowledge patterns are representations which capture
recurring structure within and across ontologies [28]. And
therefore, knowledge patterns (patterns for short) can
be seen as generalisations where entities are replaced
by variables [29]. The above pattern conteins two vari-
ables ?Peptide and ?Protein. When the pattern is instan-
ciated, the variables will be replaced with entities. For
example, for peptide with ID 1023927 the variable
?Protein will be replaced with the parent protein from
which the peptide is derived, which is PROs Collagen
alpha-1(I) chain (PR:P02452). It should be noted
that a pattern (a.k.a. axiom pattern) does not neces-
sarily coincide with the notion of ontology design
pattern (see [29]). A pattern can also represent a set
of OWL axioms. And thus an ontologys class expres-
sions or definitions can be easily obtained instanciating
patterns.
The description of proteins in the PxO follows guide-
lines for the PRO [18] and it uses the RO object
properties has function to relate a protein to
its GO molecular_function, has location to
relate a protein to its location in a GO cellular
component and participates in to relate a
protein to the GO biological process in
which it may participate. Proteins are thus described in
the PxO with the following axioms:
Class : p r o t e i n
SubClassOf :
 amino ac i d chain  ,
 l o c a t e d in  some ce l lu l a r_component ,
 p a r t i c i p a t e s in  some b i o l o g i c a l _ p r o c e s s ,
 has func t ion  some molecu l a r _ func t i on
The aim in PxO is to describe protein types taken
from Uniprot described by terms from the Gene Ontol-
ogy according to PRO guidelines. Given the PRO protein
Collagen alpha-1(I) chain (PR:P02452), the fol-
lowing two axioms are made by instanciating the above
pattern
Class :  Co l l agen alpha ?1( I ) chain 
SubClassOf :
 l o c a t e d in  some  e x t r a c e l l u l a r reg ion  ,
 l o c a t e d in  some  e x t r a c e l l u l a r space 
UniProtKB/Swiss-Prot [30] contain more than one hun-
dred thousand protein records for metazoa (i.e. mul-
ticellular animals) that were reviewed, and manually
annotated. Some of these proteins have isoforms, i.e.
alternatives to the canonical sequence. The PxO is
released currently with the PxO metazoa that contains
139 720 OWL protein classes (UniProtKB SwissProt and
Isoform sequences), 4 591 OWL organism taxons, and
with 89 846 OWL gene classes. Each type of OWL Class
(protein, gene, and organism taxon) is generated using its
own axiom pattern.
Both proteins and peptides have N-terminus- and C-
terminus regions. Thus, axioms were introduced to refine
PROs amino acid chain:
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2016) 7:33 Page 4 of 7
Class :  amino ac i d chain 
SubClassOf :
 mo lecu la r e n t i t y  ,
h a s _pa r t some C?terminus reg ion  ,
h a s _pa r t some N?terminus reg ion  ,
h a s _pa r t some po l ypep t i d e_ r eg i on ,
on l y_ in_ t axon some organism
Cleavage sites: On the one hand, a cleavage site (CS) is
part of a protein. On the other hand, a protein may have
one or more CS. Therefore, two patterns were created.
PxO is released currently with 16 273 OWL CS classes
for known cleavage sites, which are associated with 5 084
distinct protein classes.
Protease: An equivalence axiom allows any protein
with a GO annotation for peptidase activity(GO:
0008233) or one of its children to be recognised as a pro-
tease by Proteasix. The axioms for representing proteases
are as follows:
Class : P r o t e a s e
EquivalentTo :
p r o t e i n and (  has func t ion 
some  p e p t i d a s e a c t i v i t y  )
SubClassOf :
 i npu t of  some p r o t e o l y s i s
In the same vein, it is straight-forward to define a
class such as exopeptidaseby using the GO anno-
tation for exopeptidase activity (GO:0008238).
Definitions for endopeptidase, aminopeptidase
and carboxypeptidase are easily made by exploiting
the GOs catalytic activity hierarchy.
Proteolysis: Taking the GOs proteolysis, it is feasi-
ble to create additional axioms to describe the biological
process that has input participants of a substrate protein,
a protease and has output participants proteolytic
cleavage fragment
Class : p r o t e o l y s i s
SubClassOf :
 p r o t e i n metabo l i c process  ,
 has input  some Protease ,
 has output  some  p r o t e o l y t i c
c l e a v a g e product  ,
 has input  some ( p r o t e i n and
( h a s_pa r t some  C leavage s i t e reg ion  ) )
In the PxO there is a clear distinction between:
a) observations of protease/CS combinations extracted
from the literature (e.g. CutDB [8]); and b) predic-
tion of cleavage based on proteases cleavage site speci-
ficity from MEROPS [9] or exopeptidases cleavage site
annotation assertions with the catalytic activity from
BRENDA [10], which captures how likely it is for an
amino acid to be present or absent in a certain posi-
tion close to the CS. To represent this dichotomy,
two classes were introduced: observed proteolysis
and predicted proteolysis. To create their class
definitions, the status of the proteolysis is indicated.
Observed proteolysis is defined as:
Class :  Observed p r o t e o l y s i s 
EquivalentTo :
p r o t e o l y s i s and ( hasKnowledgeSta tus
some  Observed s t a t u s  )
In the PxO there are 20 229 observed proteo
lysis and 329 predicted proteolysis created
from two patterns.
Annotation assertionsprovide the means to
associate aditional information with an entity, like an
exact synonym (oboInOwl:hasExactSynonym), a database
cross reference (oboInOwl:hasDbXref ), or a definition
(IAO:0000115). When a protein taken from the UniPro-
tKB has a MEROPS specificity matrix, annotation asser-
tion axioms are used in PxO to represent this data.
However, the probability for a protease to cleave a protein
substrate is calculated outside of the PxO, although using
the annotation properties and values in the PxO.
PxO in use
There are two methods to find the protease classes in the
PxO: 1) use an automated reasoner like ELK [31] to infer
which proteins are proteases (see the Protease defined
class in the previous section); or 2) use the SPARQL 1.1
query language [32] to create a SELECT query (Q0) that
retrieves the OWL protein classes with a GO assertion
for peptidase activity (GO:0008233) or any of its
children. Likewise, using DL queries with ELK or SPARQL
SELECT queries, the proteins that are endopeptidase,
aminopeptidase or carboxypeptidase can be
obtained.
The essence of the Proteasix algorithmis given
below, which exploits the PxO ontology, and uses the
competency questions CQ. The algorithm assumes that
protease cleavage of substrate proteins is directed by short
amino acid motifs, from two to eight amino acids of
the type (Pn . . . )P1 - P1(. . . Pn), with the scissile bond
between P1 and P1 residues [5]. Residues towards the
N-terminus of the substrate are on the non-prime side
and numbered as P1 P2 P3 P4 and so on; while residues
towards the C-terminus are on the prime side and num-
bered as P1 P2 P3 P4 and so on [33].
STEP 1: User input  For each peptide, the end-user
provides: a) the peptide identifier; b) the UniProt Acces-
sion Number (AC) or identifier (ID) of the parent protein
from which the peptide is derived; c) the start amino acid
position with respect to the parent proteins sequence, i.e.
P1 of the N-terminus CS; and d) the end amino acid posi-
tion with respect to the parent proteins sequence, i.e. P1
of the C-terminus CS.
STEP 2: Reconstruct N- and C-terminus CS  Each
OWL protein class created from the UniProtKB (Swiss-
Prot/TrEMBL) has among others the following anno-
tation properties: oboInOwl:id for AC; oboInOwl:
hasAlternativeId for ID; and PxO:hasSequence
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2016) 7:33 Page 5 of 7
where the amino acid sequence (sequence for short) of the
protein is stored. SPARQL query CQ2-2 in the Additional
file 1 exemplifies how to obtain the amino acid sequence
for protein P02768 (PR:P02768). Outside of PxO, and
using the protein sequence, the peptide sequence for an
input peptide is extracted, and the N- and C-terminus are
reconstructed, i.e. eight amino acids or fewer if close to
the beginning or end of the proteins sequence. The out-
put of this step is the creation for each input peptide of an
OWL peptide class along with an OWL N-terminus class
and an OWL C-terminus class.
STEP 3: Observed cleavage  Using the class expres-
sion polypeptide region SubClassOf part
of some amino acid chainOWL CS classes
and OWL protein classes are linked, and therefore, this
class expression represents that a CS is part of a pro-
tein. An OWL CS class also has annotation properties
for storing the CS sequence and the P1 and P1 values.
SPARQL query CQ2-1 in the Additional file 1 illustrates
how to retrieve for protein P02768 (PR:P02768) the
observed CS regions where the P1 value is 25. To make
the retrieval process more efficient, firstly, for each
peptide, the sequence of its corresponding OWL N-
terminus class and OWL C-terminus class are matched
against the OWL CS class sequence. If successful, a
more detailed match is triggered. A successful outcome
of this step is an instantiation of the axiom pattern
?peptide SubClassOf output of some
(?proteolysis and (hasKnowledgeStatus
some Observed status)).
STEP 4: Predicted cleavage  For the OWL N-
terminus and C-terminus classes with sequences that
remain unmatched after the previous step, a protein cleav-
age prediction is attempted. SPARQL query CQ4 in the
additional file can be generalised by replacing the PROs
protease P08253 (PR:P08253) for a parameter ?C, and
thus, the results of the query will be the set of proteases
for which prediction can be undertaken by exploiting the
MEROPS cleavage site specificity matrix. The probability
calculations are outside of PxO. Firstly, the probability of
cleavage is estimated from the proteases MEROPS speci-
ficity matrix [9] using a log-likelihood. If the probability
is above the 99th percentile of the population distribution
of all possible sequences, then the sequence is taken as
statistically matched. A confidence level is then assigned
to the matching, using levels from a simulation distribu-
tion of thematching step. Secondly, for the sequences that
obtained a low/medium confidence level prediction or still
have no prediction, BRENDA [10] exopeptidases catalytic
information is used and a second prediction is attempted,
assuming that after endopeptidase cleavage, and exopep-
tidase cuts the free extremity (i.e. C- or N- terminus
CS). A successful outcome of this step is an instantia-
tion of the axiom pattern ?peptide SubClassOf output
of  some (?proteolysis and (hasKnowledgeStatus some Pre-
dicted status)).
Further validation of the observed and predicted prote-
olysis (step 3 and 4) is accomplished by checking in PxO
that: a) If the source organism for the protease and the
substrate are the same; and b) if both the protease and the
substrate are co-located.
Positive example: In the additional file, there is
an SPARQL SELECT query (CQ3-2) that investi-
gates whether the two above-mentioned conditions are
met for PROs substrate Serum albumin (PR:P02768)
and PROs protease 72 kDa type IV collagenase
(PR:P08253). The query indicates that both protease
and substrate protein may come from the same taxon
Homo sapiens (NCBI:9606) and may have the fol-
lowing common co-locations: nucleus(GO:0005634);
extracellular region(GO:0005576);
extracellular space(GO:0005615). This is a pos-
itive corroboration of co-location. Indeed, there is evi-
dence that the cleavage is observed.
Negative example: Reusing the SPARQL SELECT
query (CQ3-2) with different protease Neutrophil
elastase(PR:P08246) and substrate protein
ATP-binding cassette sub-family A
member 6(PR:Q8N139). The query indicates that both
protease and substrate protein may come from the
same taxon Homo sapiens (NCBI:9606), however
no common co-locations are found. This reinforces the
low confidence level status assigned to the prediction
obtained.
Discussion
The first version of Proteasix was agnostic as to species,
location and function of the proteases and their sub-
strate proteins. The PxO allows knowledge of the domain
to be added to the Proteasix algorithm. The PxO allows
Proteasix to add semantics to its data such that the algo-
rithm can check proteases for their function and both
protease and substrate for species and location, as well as
making the data reliably queriable. The aim here is two-
fold: first, separating operational knowledge from domain
knowledge; this will enable update and expansion of the
knowledge component with relative ease. Second, the aim
is to allow Proteasix and human users to check the valid-
ity of Proteasix results. These results may not be improved
due to the use of PxO, but may be interpreted with more
confidence.
The bulk of the PxO has been developed through re-
use of other ontologies and these were ontologies mainly
from the OBO Consortium. In doing so we have commit-
ted to the ontological viewpoint of those ontologies. We
have not substantially changed those ontologies, except to
extend, as it makes interoperability with other OBO based
applications harder, as well as update and maintenance
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2016) 7:33 Page 6 of 7
more difficult. This commitment does, however, come at
a potential cost. Like any commitment, making one com-
mitment excludes others. The OBO, like most ontologies,
are not without their controversies. The GO, in particular,
has long had its critics both on ontological and logi-
cal grounds [34, 35] There is debate about whether the
molecular function in the GO is ontologically a function
or a finer grained process than theGOs biological process
[34]. The GO has also been criticised for inconsistency in
its modelling and lack of constraints that allow automated
reasoning to be applied more effectively [35, 36]. In the
PxO we have taken these aspects into account with Pro-
teasixs application needs and resources, and have decided
that commiting to the OBO is an appropriate choice; this
decision will be kept under review.
There is much further work to be done in the PxO. Cur-
rently, we only incorporate cannonical and isoform infor-
mation from the UniProtKB, which also contains much
information about sequence variants. Inclusion of this
may improve the analysis done by Proteasix. The Gene
Ontology annotations used in describing proteins can be
accompanied with evidence codes [13], and therefore, tak-
ing into account with what confidence annotations are
made may also improve the utility of the PxO in Pro-
teasix. Another line of work is to map the cleavage to
the peptidase family instead of mapping the cleavage to
an individual enzyme. A protein is typically represented
as having many functions, in many locations and being
involved in many biological processes. At present which
functions, in which process and in which location is not
represented. As this kind of representation emerges it will
be adopted in PxO and may contribute to the accuracy of
predictions made in Proteasix.
The PxO has its limitations in addition to those indi-
cated by the future work. The PxO, like most ontologies,
is limited by the state of knowledge in its domain, which
for PxO is large. That confirmatory information on a pro-
tease protein interaction is not found does not mean it
cannot occur. The knowledge in the literature is much
greater than that in ontological form. Nevertheless, if PxO
can help give confidence to Proteasixs predictions then it
is a help.
A further limitation comes in the ability to predict
cleaveage sites in Proteasix. Proteases exhibit varying
binding affinities for amino-acid sequences, ranging from
strict restriction to one or few critical amino-acids in
given positions, to generic binding with little discrim-
ination between different amino acids. The MEROPS
database [9] lists such information. When available,
MEROPS specificity weight matrices were added to PxO.
The MEROPS specificity matrix shows how frequently
each amino acid occurred at each position in the cleavage
site. Matrices were further transformed into Probabil-
ity Matrices, by dividing the number of occurrences for
each amino acid in each position with the total num-
ber of observations. It has been acknowledged that to be
able to study peptidase specificity and make predictions
about where in a protein cleavage might occur, at least 40
cleavages in substrates are required [33] and/or a mini-
mal 10 times enrichment for one amino acid should be
observed in at least one position of the CS. Hence, the
availability of MEROPS peptidase specificity data is a hard
limitation of the current approach, which is a limitation
inherent to experimental science. However, the predic-
tions of Proteasix will improve as the body of evidence
increases.
This research work was done for the sysVASC project,
and so the emphasis is on Metazoa, where the organisms
human, mouse, and rat are the main focus. Despite the
large amount of work still to do in the PxO, the PxO nev-
ertheless is a rich ontology supporting peptide analysis
that has been enabled by re-using the ontologies pro-
duced by the OBO Consortium; the PxO has partitioned
these ontologies based on the task they need to support
and enriched them axiomatically on the same basis. As a
result Proteasix can better support the prediction of pro-
teases implicated in the production of peptides and the
consequent elucidation of biological mechanisms.
Additional file
Additional file 1: PxO Metazoa ontology: Ontology metrics; ELK reasoner
times; and SPARQL queries execution times. (PDF 141 kb)
Acknowledgements
This work was supported by a grant from the European Union Seventh
Framework Programme (FP7/2007-2013) for the sysVASC project under grant
agreement number 603288 and by Pretreat H2020-MSCA-RISE-2015 (grant
agreement number 690966). Original development of Proteasix was
supported by the grant ProteasiX FP7-PEOPLE-2011-IEF (300582), Pretreat
H2020-MSCA-RISE-2015 (690966) and the Fondation du Rein sous égide de la
Fondation pour la Recherche Médicale et ses partenaires, grant number
GENZYME 2014 FDR-SdN/FRM. The work reported here was also supported by
the EPSRC project:WhatIf: Answering "What if..." questions for Ontology
Authoring, EPSRC reference EP/J014176/1.
Authors contributions
All authors contributed to the development of the ontology and the writing of
the paper. All authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Author details
1School of Computer Science, University of Manchester, Oxford Road, M13 9PL
Manchester, UK. 2Institut National de la Sante et de la Recherche Medicale
(INSERM), U1048, 24105 Toulouse, France.
Received: 14 December 2015 Accepted: 19 May 2016
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:31 
DOI 10.1186/s13326-016-0082-0ERRATUM Open AccessErratum to: Towards exergaming commons:
composing the exergame ontology for
publishing open game data
Giorgos Bamparopoulos1, Evdokimos Konstantinidis1, Charalampos Bratsas2 and Panagiotis D. Bamidis1*After publication of the original article [1] it was
brought to our attention that an acknowledgement was
missing from the article. The authors would therefore
like to add the following acknowledgement, and offer
their apologies that this was missed out in the original
publication:
This work was supported in part by the European
Unions Seventh Framework Programme (Project USEFIL,
GA 288532; http://www.usefil.eu), as well as the LLM Care
(www.llmcare.gr) self-funded initiative that emerged as the
business exploitation of the Long Lasting Memories (LLM
Project) (www.longlastingmemories.eu) originally funded
by the ICT-CIP-PSP Programme.
Author details
1Medical Physics Laboratory, Medical School, Faculty of Health Sciences,
Aristotle University of Thessaloniki, Thessaloniki, Greece. 2Mathematics
Department, Aristotle University of Thessaloniki, Thessaloniki, Greece.
Received: 24 May 2016 Accepted: 24 May 2016
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 
DOI 10.1186/s13326-016-0064-2
RESEARCH Open Access
OmniSearch: a semantic search system
based on the Ontology for MIcroRNA Target
(OMIT) for microRNA-target gene interaction
data
Jingshan Huang1*, Fernando Gutierrez2, Harrison J. Strachan1, Dejing Dou2, Weili Huang3, Barry Smith4,
Judith A. Blake5, Karen Eilbeck6, Darren A. Natale7, Yu Lin8, Bin Wu9, Nisansa de Silva2, Xiaowei Wang10,
Zixing Liu11, Glen M. Borchert12, Ming Tan11 and Alan Ruttenberg13
Abstract
As a special class of non-coding RNAs (ncRNAs), microRNAs (miRNAs) perform important roles in numerous biological
and pathological processes. The realization of miRNA functions depends largely on how miRNAs regulate specific
target genes. It is therefore critical to identify, analyze, and cross-reference miRNA-target interactions to better explore
and delineate miRNA functions. Semantic technologies can help in this regard. We previously developed a miRNA
domain-specific application ontology, Ontology for MIcroRNA Target (OMIT), whose goal was to serve as a foundation
for semantic annotation, data integration, and semantic search in the miRNA field. In this paper we describe our
continuing effort to develop the OMIT, and demonstrate its use within a semantic search system, OmniSearch,
designed to facilitate knowledge capture of miRNA-target interaction data. Important changes in the current version
OMIT are summarized as: (1) following a modularized ontology design (with 2559 terms imported from the NCRO
ontology); (2) encoding all 1884 human miRNAs (vs. 300 in previous versions); and (3) setting up a GitHub project site
along with an issue tracker for more effective community collaboration on the ontology development. The OMIT
ontology is free and open to all users, accessible at: http://purl.obolibrary.org/obo/omit.owl. The OmniSearch system
is also free and open to all users, accessible at: http://omnisearch.soc.southalabama.edu/index.php/Software.
Keywords: microRNA, Non-coding RNA, Target gene, Biomedical ontology, Ontology development, Data annotation,
Data integration, Semantic search, SPARQL query
Introduction
microRNAs (miRNAs) are a type of non-coding RNA
(ncRNA) with important biological, biomedical, and clin-
ical impact. Prior research [1, 2] indicates that miRNAs
perform significant roles in both biological and patholog-
ical processes, thus affecting the control and regulation of
various human diseases. miRNAs realize critical functions
via binding to their respective target genes. The ability
to identify and analyze miRNA-target interactions in an
*Correspondence: huang@southalabama.edu
1School of Computing, University of South Alabama, Mobile, Alabama
36688-0002, USA
Full list of author information is available at the end of the article
effective manner is thus a key step in the understanding
and delineation of miRNA functions.
The conventional method by which the users of data
(e.g., biologists, bioinformaticians, and clinical investiga-
tors) determine miRNA functions involves:
 Searching for biologically validated miRNA targets,
for example, by querying the PubMed database [3];
and
 Finding additional potential miRNA targets, for
example, by initiating inquiries on various prediction
databases or websites such as miRDB [4], TargetScan
[5], and miRanda [6].
© 2016 Huang et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 2 of 17
Unfortunately, both steps currently require significant
manual effort because the relevant data sources are
both syntactically and semantically heterogeneous  that
is, the meaning of seemingly similar data from differ-
ent sources may be quite different and thus open to
misinterpretation. It is therefore challenging for users
to identify and establish possible links among original
data sources. As a result, conventional miRNA knowl-
edge discovery and acquisition methodologies are time-
consuming, labor-intensive, error-prone, and sensitive to
limitations in the prior knowledge of different end users.
These barriers are exacerbated by the need to obtain
additional information for each and every miRNA tar-
get (whether validated or putative) using existing data
sources and analysis tools, including but not limited to:
the DAVID Bioinformatics Resources (DAVID) [7], NCBI
Gene [8], the Medical Subject Headings (MeSH) Database
[9], the HUGO Gene Nomenclature Committee (HGNC)
Database [10], and NCBI Nucleotide [11].
Emerging semantic technologies can help in address-
ing the aforementioned challenges. The core of cur-
rent semantic technologies include specifications such as
the resource description framework (RDF), RDF Schema
(RDFS), and Web Ontology Language (OWL), all of
which are intended to provide a formal description of
classes of entities of different types and of the relations
between them in such a way as to enable automatic rea-
soning (inference). Semantic technologies can be applied
to miRNA knowledge acquisition by transforming data
obtained from heterogeneous miRNA-related databases
into a common framework by utilizing a single format
(such as RDF) and aligning the data through use of anno-
tations from common, formally defined ontologies. By
means of this transformation we can use the SPARQL
Protocol (SPARQL) [11] to query the enhanced data auto-
matically.
In previous research [1217], we investigated the con-
struction of an application ontology for the miRNA field,
named Ontology for MIcroRNA Target (OMIT), the first
ontology to formally encode miRNA domain knowledge.
By providing a standardized metadata model to establish
miRNA data connections among heterogeneous sources,
the OMIT is able to fill two gaps: the lack of common
data elements and the lack of data exchange standards for
miRNA research, especially with regard to miRNA-target
interactions.
We describe two major scientific contributions in this
paper: (1) recent improvements to the OMIT ontology
and (2) a semantic search system, which is built upon the
ontology and enables the capture of miRNA-target inter-
action data in a way leading to more effective miRNA
knowledge acquisition.
The remainder of this paper is organized as follows.
Related work Section summarizes state-of-the-art
research in biomedical ontologies and semantic search,
respectively. OMIT reconstruction Section reports
our efforts on reconstructing the OMIT ontology.
OmniSearch: an OMIT-based semantic search system
Section describes technical details of OmniSearch,
an OMIT-based semantic search system. Results and
discussion Section reports our experimental results.
Finally, Conclusions Section summarizes the major
points and presents ideas for future research.
Related work
Related work in biomedical ontologies
The use of ontologies to describe, define, and inte-
grate biological entities has long been embraced by the
biological, biomedical, and clinical research commu-
nities. Here we briefly describe some representative
bio-ontologies included in both the Open Biological
and Biomedical Ontologies (OBO) Library [18] and the
National Center for Biomedical Ontology (NCBO) Bio-
Portal [19] that are pertinent to the development of this
project.
The Gene Ontology (GO) [20] is by far the most
successful and widely used ontology for biological
data description. It consists of three independent sub-
ontologies: biological processes, molecular functions, and
cellular components, which describe these aspects of gene
products: both protein and RNA. The GO has been widely
utilized to annotate gene products of model organisms.
By the time of writing this paper, there were GO annota-
tions for 36 organisms including Homo sapiens available
for download.
The Sequence Ontology (SO) [21] is an ontology to cap-
ture genomic features and the relationships that obtain
between them. This ontology contains the features neces-
sary to annotate a genome with structural features such as
gene models and also the terms necessary for the anno-
tation of genomic variants. SO terms define the kinds of
and parts of ncRNA features, and these terms are used
to identify these features and their location in genomic
sequence.
The PRotein Ontology (PRO) [22] is a comprehensive
description of the forms of protein, including isoforms,
modifications, and the relationships between them. Pro-
teins are functional entities in many processes eventually
impacted by the regulatory effect of ncRNAs (e.g., miRNA
bindings). The PRO provides an ontological representa-
tion of proteins with a particular focus on human proteins
and disease-related variants thereof.
The RNA Ontology (RNAO) [23] is a candidate OBO
foundry reference ontology to catalogue the molecu-
lar entities composing primary, secondary, and tertiary
components of RNA. The goal of this project is to
enable integration and computation over diverse RNA
datasets.
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 3 of 17
Related work in semantic search
Semantic search is a research field that intends to improve
the access to contents by considering the semantics
behind the search process [24]. In other words, semantic
search goes beyond conventional, keyword-based search
by considering the contextual meaning of words, the
intent of the user, and the nature of the search space.
In general, semantic search requires the use of struc-
tured knowledge, such as ontologies, in the modeling and
interpretation of queries. Ontologies can help improve
the search by query expansion. One main idea in many
semantic search systems (e.g., [2529]) is, the original
set of query keywords can be expanded by drawing on
synonyms and other relationships (e.g., subclass and part-
hood) that are not part of the query. For example, in
the work by Chauhan et al. [29], the original query was
first expanded by considering synonyms, then terms with
high semantic similarity were chosen from the ontology
to be integrated to the search query, and the seman-
tic similarity used for the query expansion was com-
puted by the distance among concepts in the ontology,
the position in the hierarchy, and the number of upper
classes.
Another way to implement semantic search is to use
ontologies to translate keyword-based search into formal
semantic queries. For example, Tran et al. [24] used
a set of models (mental, user, system, and query) to
capture information, such as thought entities, language
primitives, knowledge representation (KR) primitives,
and query elements. These models were then com-
bined with a set of assumptions to redefine original
queries, filling the gap between terms with structural
information from an ontology. That is, each term
within the query was considered a property of another
term.
OMIT reconstruction
Modularized ontology design
The OMIT ontology consists of the following modules:
 omit.owl  Defines all OMIT-specific terms and
relations, for example, prediction_from_miRDB and
gene_context_score_in_TargetScan.
 bfo.owl  Imports upper-level terms from the Basic
Formal Ontology (BFO) [30], for example, generically
dependent continuant and material entity.
 ro-imports.owl  Imports common relations (shared
across different ontologies) from the Relation
Ontology (RO) [31], for example, has participant and
regulates.
 ncro.owl  Imports ncRNA-related terms and
relations from the Non-coding RNA Ontology
(NCRO) [32], for example, miRNA_target_gene and
miRNA_gene_family.
 go-imports.owl  Imports gene product terms from
the GO, for example, RNA binding and regulation of
biological process.
 so-imports.owl  Imports sequence structural
feature terms from the SO, for example,
biological_region and insertion_site.
 obi-imports.owl  Imports life-science and clinical
investigation terms from the Ontology for Biomedical
Investigations (OBI) [33], for example, cultured cell
population and organism.
 chebi-imports.owl  Imports molecular entity
(especially small chemical compounds) terms from
the Chemical Entities of Biological Interest Ontology
(ChEBI) [34], for example, ribonucleic acid and
ribosomal RNA.
 iao-imports.owl  Imports information entity terms
from the Information Artifact Ontology (IAO) [35],
for example, information content entity.
 clo-imports.owl  Imports cell line-relevant terms
from the Cell Line Ontology (CLO) [36], for example,
cell line.
 pr-imports.owl  Imports protein-related entity
terms from the PRO, for example, amino acid chain
and protein.
 uberon-imports.owl  Imports cross-species
anatomy terms from the Uberon multi-species
anatomy ontology (UBERON) [37], for example,
anatomical structure and organ.
 doid-imports.owl  Imports disease terms from the
Human Disease Ontology (DOID) [38], for example,
disease of cellular proliferation and cancer.
Note that:
(1) Orthogonality among different ontologies is one
of the important practices proposed by the OBO
Foundry Initiative, and has been widely accepted in
the bio-ontology community. As a result, to achieve
better orthogonality, it is a common practice to reuse
contents defined in relevant, existing ontologies.
(2) The OMIT ontology directly imported the NCRO
ontology (a comprehensive ncRNA domain
ontology), which in turn, directly imported other
ontologies in the above list. Therefore, the OMIT
ontology itself includes two OWL files: omit.owl
and ncro.owl. All other OWL files,
go-imports.owl and so-imports.owl for example,
are shown as indirectly imported in Protégé.
(3) Ontology concepts are referred to as classes in
Protégé and terms in OBO Edit, respectively.
Therefore, classes and terms are interchangeably
used throughout the whole paper.
Table 1 lists a subset of important terms and relations
imported into the OMIT.
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 4 of 17
Table 1 A subset of imported terms and relations
Imported term or relation Source ontology Original ID
RO:part of Relation Ontology BFO_0000050
RO:participates in Relation Ontology RO_0000056
RO:has participant Relation Ontology RO_0000057
BFO:entity Basic Formal Ontology BFO_0000001
BFO:continuant Basic Formal Ontology BFO_0000002
BFO:independent continuant Basic Formal Ontology BFO_0000004
BFO:occurrent Basic Formal Ontology BFO_0000003
BFO:material entity Basic Formal Ontology BFO_0000040
CHEBI:molecular entity Chemical Entities of Biological Interest Ontology CHEBI_23367
CHEBI:ribonucleic acid Chemical Entities of Biological Interest Ontology CHEBI_23367
CHEBI:ribosomal RNA Chemical Entities of Biological Interest Ontology CHEBI_18111
CHEBI:small nuclear RNA Chemical Entities of Biological Interest Ontology CHEBI_74035
CHEBI:transfer RNA Chemical Entities of Biological Interest Ontology CHEBI_17843
NCRO:human_miRNA Non-coding RNA Ontology NCRO_0000810
NCRO:hsa-miR-125b-1-3p Non-coding RNA Ontology NCRO_0003283
NCRO:hsa-miR-125b-2-3p Non-coding RNA Ontology NCRO_0003284
NCRO:hsa-miR-125b-5p Non-coding RNA Ontology NCRO_0003282
NCRO:miRNA_target_gene Non-coding RNA Ontology NCRO_0000025
NCRO:miRNA_and_target_gene_binding Non-coding RNA Ontology NCRO_0000003
NCRO:protein_miRNA_promoter_binding Non-coding RNA Ontology NCRO_0000011
IAO:information content entity Information Artifact Ontology IAO_0000030
IAO:measurement datum Information Artifact Ontology IAO_0000109
 The format for the left column (Imported Term or
Relation) is PREFIX:human-readable label, for
example, NCRO:miRNA_target_gene and RO:part of.
 The format for the right column (Original ID) is
PREFIX_unique identifier, for example,
NCRO_0000025 and BFO_0000001.
Ontology core design
The core design of the OMIT ontology is shown in Fig. 1.
Compared with earlier versions, the current version con-
tains many important new terms and relations, and some
of which are listed in Tables 2 and 3, respectively.
 Both terms and relations are represented in the
format of PREFIX:label in Fig. 1.
 For the purpose of better readability, labels rather
than identifiers are used in Tables 2 and 3.
 Relations in Table 3 were either defined in or
imported into the OMIT, which can be easily
distinguished from each other by different prefixes
used in the first column.
OmniSearch: an OMIT-based semantic search
system
Based on the OMIT ontology, we developed a semantic
search system:OmniSearch. First, the OmniSearch system
will conduct semantic annotation on various sources that
were originally heterogeneous in their semantics; follow-
ing that, OMIT-annotated data will then be integrated
into a unified and consistent data layer in RDF; and finally,
complex semantic queries will be performed to provide
meaningful results and clues to system end users (e.g.,
biologists, bioinformaticians, and clinical investigators).
Data sources used
Data sources used in the OmniSearch system include
three miRNA target prediction databases (miRDB, Tar-
getScan, and miRanda), as well as PubMed, NCBI Gene,
GO, RNA Central, DAVID, HGNC, and MeSH term
databases. These sources contain both structured data
(database instances) and unstructured data (free text), and
are semantically heterogeneous among each other.
Software architecture
TheOmniSearch system consists of several softwaremod-
ules: semantic annotation, data integration, and semantic
search.
Semantic data annotation is the process of tagging
source files with predefined ontological metadata like
names, entities, attributes, definitions, and descriptions.
The annotation provides original data with extrametadata
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 5 of 17
Fig. 1 The design of core terms and relations in the OMIT ontology (both terms and relations are represented in the format of PREFIX:label)
Table 2 A subset of new OMIT terms
OMIT new term Direct parent term Human-readable explanation
computationally_asserted_evidence IAO:information content entity Evidence obtained from some
computational methods.
information_from_miRNA_ OMIT:computationally_asserted_evidence Records obtained from various
target_prediction_database miRNA target prediction databases.
prediction_from_miRDB OMIT:information_from_miRNA_ Records specifically obtained
target_prediction_database from the miRDB database.
prediction_from_TargetScan OMIT:information_from_miRNA_ Records specifically obtained
target_prediction_database from the TargetScan database.
prediction_from_miRanda OMIT:information_from_miRNA_ Records specifically obtained
target_prediction_database from the miRanda database.
target_score_in_miRDB IAO:measurement datum The score of some specific
miRNA-target binding prediction
from the miRDB database.
gene_context_score_in_TargetScan IAO:measurement datum The context score of some specific
miRNA-target binding prediction
from the TargetScan database.
mirSVR_score_in_miRanda IAO:measurement datum The mirSVR score of some specific
miRNA-target binding prediction
from the miRanda database.
information_from_NCBI_gene IAO:information content entity Records obtained from NCBI Gene
according to gene IDs or gene symbols.
information_from_NCBI_nucleotide IAO:information content entity Records obtained from NCBI Nucleotide
according to GenBank Accession numbers.
information_from_PubMed IAO:information content entity Records obtained from the PubMed
database according to PMIDs.
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 6 of 17
Table 3 A subset of new OMIT relations
New relation Domain Range Human-readable explanation
OMIT:miRNA_target_ NCRO:miRNA_and_ OMIT:computationally_ Specific miRNA-target binding
assumption_ target_gene_binding asserted_evidence prediction is based on some
based_on computationally asserted evidence.
OMIT:is_quality_ IAO:measurement datum OMIT:computationally_ A piece of measurement datum
measurement_of asserted_evidence (e.g., the target score in miRDB)
is a quality measurement of
computationally asserted evidence.
OMIT:is_gene_ NCRO:miRNA_target_gene OMIT:target_protein A miRNA target gene
template_of_protein serves as a template
of relevant protein.
RO:has participant OMIT:prediction_from_miRDB SO:miRNA Each miRNA-target binding
prediction record has one
miRNA as a participant.
RO:has participant OMIT:prediction_from_miRDB NCRO:miRNA_target_gene Each miRNA-target binding
prediction record has one
target as a participant.
RO:part of OMIT:target_score_in_miRDB OMIT:prediction_from_miRDB Each miRNA-target binding
prediction record from
miRDB contains one score.
Each record from NCBI
RO:part of OMIT:PubMed_summary_ OMIT:information_from_NCBI_gene Gene contains one or
in_NCBI_gene more PubMed summaries.
information formally defined in the OMIT ontology. The
output of semantic data annotation is a collection of
RDF triples (from both free text and database instances).
These triples will be accumulated into a centralized RDF
repository: OmniStore.
We used Python scripts to conduct automated seman-
tic annotation and data integration. As an example, Fig. 2
shows the flowchart of our programs to annotate miRDB
data. We explain below the detailed steps. One miRDB
file, the miRNA data file, contains two columns con-
sisting of miRNA names and their associated interna-
tionalized resource identifiers (IRIs). Another miRDB file,
the gene data file, contains four columns consisting
of miRNA names, gene IDs, gene symbols, and target
scores.
 Step One: As each miRNA name and its associated
IRI were read in from the miRNA data file, they were
placed into a dictionary where the miRNA name is
the key and the IRI is the value.
 Step Two: All lines were read in from the gene data
file, and each line was converted into a total of four
RDF triples. (1) The first triple was generated to
represent a newly created instance of the
prediction_from_miRDB class, namely, instance_i,
and a new OMIT IRI was assigned to instance_i. (2)
Next, the miRNA name read from the same line was
used to retrieve its corresponding IRI from the
dictionary (generated in Step One). The second triple
then connected this retrieved IRI with instance_i. (3)
Two more triples were generated to connect
instance_i with the corresponding gene ID and target
score read in from the same line, respectively.
 Step Three: Finally, all generated RDF triples were
written into a Turtle file.
Note that:
(1) Semantic annotation and data integration
Section exhibits some example triples resulted from
the above-mentioned annotation process.
(2) Mappings between database schemas and
ontological entities were defined in the OMIT
ontology and can be reused or modified in the future,
when needed.
(3) Due to our automated annotation and integration
techniques, only minimum effort will be required to
integrate a new resource in the future.
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 7 of 17
Fig. 2 Semantic annotation and data integration flowchart in the OmniSearch system
Because all semantic tags are to be generated from the
global metadata model defined in the OMIT ontology, the
RDF triple repository will provide a unified view over orig-
inal data sources at semantic level. Consequently, complex
semantic queries will be enabled. To implement semantic
search, we made use of Apache HTTP server [39], PHP:
Hypertext Preprocessor (PHP) server [40], and Apache
Jena Fuseki server [41]. The overall software architec-
ture is demonstrated in Fig. 3, with the following working
protocol:
 Query parameters are sent from the clients browser
to the Apache server through Ajax requests.
 SPARQL queries are dynamically generated by the
Apache server using these query parameters, which
are then sent to the Apache Jena Fuseki server.
 JSON objects, containing the requested information,
are retrieved from the RDF triple store (installed on
the Apache Jena Fuseki server) after running the
dynamically generated SPARQL queries.
 These JSON objects are returned to the Apache
server, which are used to generate either (1) a list of
miRNAs and/or MeSH terms or (2) the HTML
Markup for the search result table.
 Finally, the Apache server sends the obtained data, or
an error message if the search fails, back to the
clients browser as a JSON object.
User interface design
The OmniSearch is a Web-based search system that is
free and open to all users, accessible at: http://omnisearch.
soc.southalabama.edu/index.php/Software. As shown in
Fig. 4, the main components of the graphic user inter-
face (GUI) are: two search criteria boxes, a search result
table, a pagination control, a set of result viewing filters,
a result download tool, and DAVID analysis functionality.
More discussion on our friendly user interface design can
be found in Search results and discussion Section.
Results and discussion
The significantly refactored OMIT ontology
The updated version of the OMIT ontology contains
a total of 3169 terms and 46 relations (besides a total
of 5515 is_a relations). Note that out of 46 relations
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 8 of 17
Fig. 3 Semantic search architecture in the OmniSearch system
mentioned here, there are 5 data properties, and the
rest are object properties. Also note that these terms
and relations include both OMIT-specific ones and those
imported ones1.
Compared with the previous versions [1217], impor-
tant changes in the current version OMIT ontology are
summarized as follows.
 As discussed earlier in Modularized ontology design
Section, we have followed amodularized ontology
design in this new version, which will significantly
further facilitate the ontology maintenance and
update. In particular, a total of 2559 terms in the
updated OMIT have been imported from the NCRO
ontology [32]. Because the NCRO is a comprehensive
domain ontology in the ncRNA field, following the
NCRO hierarchy will enhance the interoperability
between the OMIT and future ontologies to be
developed in other ncRNA sub-domains.
 In the previous versions of OMIT, around 300
human miRNAs were included. In the current
version, all 1884 miRNAs appearing in humans
have been encoded, along with the information about
the gene family group of each and every miRNA.
According to miRBase [42], there are a total of 320
different gene family groups. This information can be
highly valuable because the fact that two or more
miRNAs of interest indeed belong to the same gene
family group can provide biologists,
bioinformaticians, and clinical investigators with
critical clues in constructing new hypothesis.
 In our previous investigations, we established a
dedicated project website [43], as well as entries in
both the OBO Library [44] and the NCBO BioPortal
[45]. To further disseminate the ontology, and, to
gather feedback from community in a more effective
manner, we have recently created a GitHub project
site (https://github.com/OmniSearch/omit) for this
new version OMIT ontology. We have also
established a tracker [46] for an enhanced
mechanism in handling the discussion among
groups to further improve the ontology. New
concepts, definitions, and their locations in the
OMIT can now be proposed, debated, and approved
(or rejected) by an open group of individuals through
this tracker.
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 9 of 17
Fig. 4 GUI design in the OmniSearch system
Semantic annotation and data integration
Experimental setup
The OmniStore RDF repository is housed on a server with
the following configuration: Intel(R) Core(TM) i7-3632
QM CPU @ 2.80 GHz 2.80 GHz; 32.00 GB memory; and
Windows Server 8 Operating System.
Semantic annotation and data integration results
OmniStore contains a total of 6,136,514 RDF triples, and
the file size of OmniStore is 369 MB. All triples are rep-
resented in RDF 1.1 Turtle: Terse RDF Triple Language
format [47], for example:
<http://purl.obolibrary.org/obo/OMIT_
0015037>
rdfs:subClassOf
<http://purl.obolibrary.org/obo/NCRO_
0000025> .
<http://purl.obolibrary.org/obo/OMIT_
0015037>
rdfs:label
"IRF4" .
<http://purl.obolibrary.org/obo/OMIT_
0995324>
rdf:type
<http://purl.obolibrary.org/obo/OMIT_
0000020> .
<http://purl.obolibrary.org/obo/OMIT_
0995324>
<http://purl.obolibrary.org/obo/RO_
0000057>
<http://purl.obolibrary.org/obo/OMIT_
0015037> .
<http://purl.obolibrary.org/obo/OMIT_
0995324>
<http://purl.obolibrary.org/obo/RO_
0000057>
<http://purl.obolibrary.org/obo/OMIT_
0050688> .
<http://purl.obolibrary.org/obo/OMIT_
0995324>
<http://purl.obolibrary.org/obo/OMIT_
0000108>
100 .
The semantics of the above six example triples is: IRF4
(OMIT_0015037) is a subclass of the miRNA_target_gene
class (NCRO_0000025); one miRDB database record
(OMIT_0995324), which is an instance of the pre-
diction_from_miRDB class (OMIT_0000020), indicates
that IRF4 is a predicted target of the miRNA hsa-
miR-125b-5p (OMIT_0050688); and the prediction score
(OMIT_0000108) is 100.
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 10 of 17
Semantic search
We use one example in this section to demonstrate in
detail how the OmniSearch system assists in end users
knowledge acquisition.
Experimental setup
Semantic search was conducted on a personal computer
(PC) with the following configuration: Intel(R) Core(TM)
i7-3632 QM CPU @ 2.50 GHz 2.50 GHz; 16.00 GB mem-
ory; and Windows 10 64-bit Operating System.
SPARQL query statements
The SPARQL statements to generate the miRNA and
MeSH term lists in the two search boxes are as fol-
lows, where the PHP variable $type is used to determine
whether the client is requesting a miRNA or MeSH term,
and the PHP variable $input contains either a partial or
exact miRNA or MeSH term. Note that each line of the
query statement has a detailed explanation right above it
(the line starting with a pound sign #).
# prefix declarations
PREFIX rdfs:<http://www.w3.org/2000/01/rdf-
schema#>
# result clause
SELECT ?label
# query pattern
WHERE {
# get IRI of either human_miRNA or
MeSH_Term as parent
?parent rdfs:label $type .
# get all children of parent
?child rdfs:subClassOf ?parent .
# get label for each child
?child rdfs:label ?label .
# filter results to only include label
that match the user input
FILTER REGEX(LCASE(?label), LCASE
($input))
}
# order the result by label
ORDER BY ?label
Suppose that the question of interest is: What is the
role of hsa-miR-125b-5p in cancer drug resistance? The
SPARQL statements are as follows. Similarly, all query
statements have a detailed explanation.
# prefix declarations
PREFIX rdfs: <http://www.w3.org/2000/01/
rdf-schema#>
PREFIX rdf: <http://www.w3.org/1999/02/22-
rdf-syntax-ns#>
PREFIX obo: <http://purl.obolibrary.org/
obo/>
# result clause
SELECT ?gene_symbol
# group the gene ids together
(GROUP_CONCAT(DISTINCT ?g_id;
SEPARATOR=",") AS ?gene_id)
# assign mdb_score to mirdb_score if
bound, otherwise assign 0
(MAX(COALESCE((?mdb_score),
?mdb_score,0)) AS ?mirdb_score)
# assign ts_score to targetscan_score
if bound, otherwise assign 0
(MAX(COALESCE(?ts_score), ?ts_score, 0))
AS ?targetscan_score)
# assign absolute value of mrnd_score
to miranda_score if bound, otherwise
assign 0
(MAX(COALESCE(ABS(?mrnd_score), 0)) AS
?miranda_score)
# concatenate and group all pubmed ids
together, separated by a comma
(GROUP_CONCAT(DISTINCT ?pmid;
SEPARATOR=",") AS ?pubmed_ids)
# query pattern
WHERE {
# get microRNA IRI with label
"hsa-miR-125b-5p"
?mirna rdfs:label "hsa-miR-125b-5p" .
# get prediction that has_human_miRNA
microRNA IRI
?prediction obo:OMIT_0000159 ?mirna .
# get target where prediction has_
miRNA_target_gene
?prediction obo:OMIT_0000160 ?target .
# get gene symbol label of target
?target rdfs:label ?gene_symbol .
# get target gene_id as g_id
?target obo:OMIT_0000109 ?g_id .
OPTIONAL {
# get prediction of type
prediction_from_TargetScan
?prediction rdf:type obo:
OMIT_0000019 .
# get prediction score as ts_score
?prediction obo:OMIT_0000108
?ts_score
}.
OPTIONAL {
# get prediction of type
prediction_from_miRDB
?prediction rdf:type obo:OMIT_
0000020 .
# get prediction score as mdb_score
?prediction obo:OMIT_0000108 ?mdb_
score
}.
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 11 of 17
OPTIONAL {
# get prediction of type
prediction_from_miRanda
?prediction rdf:type obo:OMIT_
0000021 .
# get prediction score as mrnd_
score
?prediction obo:OMIT_0000108 ?mrnd_
score
}.
OPTIONAL {
# get MeSH Term IRI for "drug
resistance"
?mesh_term rdfs:label "drug
resistance" .
### exact match ###
# get pubmed id associated with
the target gene
?target obo:OMIT_0000151
?pubmed_id .
# get pubmed id associated with
the mesh term
?mesh_term obo:OMIT_0000151
?pubmed_id
### narrower match ###
# get each successive child of mesh
term
#?child rdfs:subClassOf* ?mesh_
term .
# get pubmed id associated with
the target gene
#?target obo:OMIT_0000151
?pubmed_id .
# get pubmed id associated with
the child mesh term
#?child obo:OMIT_0000151
?pubmed_id
### broader match ###
# get each successive parent mesh
term
#?mesh_term rdfs:subClassOf*
?parent .
# restric parent to be a subclass
of MeSH_Term
#?parent rdfs:subClassOf obo:OMIT_
0000110 .
# get pubmed id associated with
the target gene
#?target obo:OMIT_0000151
?pubmed_id .
# get pubmed id associated with
the parent mesh term
#?parent obo:OMIT_0000151
?pubmed_id
}.
}
# group the results by gene symbol
GROUP BY ?gene_symbol
# order the results by mirdb score then by
targetscan score
ORDER BY DESC(?mirdb_score)DESC
(?targetscan_score) DESC(?miranda_score)
Search results and discussion
Corresponding to the aforementioned question of inter-
est, Fig. 5 demonstrates the search results from a query
on hsa-miR-125b-5p along with a MeSH-term filter drug
resistance.
 The Candidate Targets column contains all targets
predicted by at least one target prediction database.
The user can choose a prediction database and sort
all targets by the scores, in descending order, from
the selected database.
 The Predicted By column shows that each target is
predicted by which database(s), along with the Web
link(s) to these database(s).
 The Publications column links to all PubMed
publications that are relevant to the search and
filtering criteria. In this example, the criteria for any
line are: the predicted target on that line, the miRNA
hsa-miR-125b-5p, and the MeSH-term filter drug
resistance.
 The GO Annotations column connects to GO
annotation results of each predicted target and the
miRNA hsa-miR-125b-5p, respectively.
 Pathway analysis through DAVID can be performed
on selected targets, either using the checkboxes to the
left of the table or clicking the Select All Targets
checkbox. Additionally, the user can select the desired
tool to perform such analysis, Gene Functional
Classification, Functional Annotation Clustering,
Functional Annotation Summary, and so forth.
 The whole result table can be downloaded in two
different formats (tab-delimited text or CSV format);
the user is also able to download only the predicted
targets (selected ones or all).
We examined the search results demonstrated in this
example, and our observations are summarized below.
1. Effective querying and accurate search results.
 Potential targets from all three miRNA target
prediction databases (miRDB, TargetScan, and
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 12 of 17
Fig. 5 Search results for the question of What is the role of hsa-miR-125b-5p in cancer drug resistance?
miRanda) were correctly retrieved. There were
476 and 924 targets from miRDB and
TargetScan, respectively; and there were 323
common targets. Consequently, a total of 1077
distinct targets were retrieved in the table when
the Predicted by Any Database filter was
chosen. Note that the miRanda database did not
contain prediction results for the miRNA
hsa-miR-125b-5p; therefore, no results
appeared in the table when the display filter was
set to Predicted by All Databases. In fact, this
observation further verified the effectiveness of
the OmniSearch system.
 Relevant papers according to the search criteria
were successfully retrieved. For example, two
publications (PMID: 2497002 and 22808086)
were retrieved for the predicted target LIN28A,
supporting the conclusion that Lin28A
contributes to cancer drug resistance; and three
publications (PMID: 21823019, 24643683, and
19463775) were retrieved for the predicted
target BAK1, supporting another conclusion
that BAK1 has an important role in cancer drug
response and drug resistance.
 RNA Central annotations and GO annotations
were correctly obtained. In this example query,
a total of five sequences regarding the miRNA
hsa-miR-125b-5p were retrieved from RNA
Central annotations, and GO annotations for all
predicted targets were retrieved as well. For
example, a total of 117 GO annotations
(GO_REF:0000038, GO_REF:0000033, and so
forth) were retrieved regarding a potential
target, BAK1.
 Based on the above knowledge returned in the
OmniSearch GUI, regarding the example
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 13 of 17
question of What is the role of
hsa-miR-125b-5p in cancer drug resistance?
end users obtained the following answer: It is
reasonable to speculate that expression of
the miRNA hsa-miR-125b-5p contributes to
cancer drug resistance, possibly through its
suppression of expression for target genes
BAK1 and/or LIN28A.
Discussion:
(1) miRDB, TargetScan, and miRanda databases have
quite different meanings among each other in terms
of their database entities. Due to the underlying
OMIT and the formally defined semantics in the
ontology, the OmniSearch system was able to
effectively integrate the prediction results from all
three databases. Note that conventional,
database-oriented techniques can also implement
such integration; however, inflexible, ad-hoc
hard-coding will be required.
(2) To retrieve a correct set of relevant papers
requires accessing numerous heterogeneous data
sources such as NCBI Gene, PubMed, HGNC, and
MeSH. Without the common data elements defined
in the OMIT and the thereafter semantic
technologies including semantic annotation and data
integration, it would have been extremely challenging
to effectively integrate data from these sources, which
is the case in database-oriented search and querying.
(3) As discussed earlier in OMIT
reconstruction Section, the OMIT is closely
connected with the GO by importing a set of GO
terms. Compared with data integration based on
traditional, relational databases, our approach has
further facilitated integrating data about GO
annotations.
2. More efficient querying process.
 One-stop visit rather than accessing different
data sources separately, resulting in about 60 %
of time saved for end users.
 DAVID analysis was performed in a more
efficient manner due to the target gene list
automatically generated by the system. resulting
in about 50 % of time saved for end users.
 It was easier to compare different prediction
results among miRDB, TargetScan, and
miRanda databases, resulting in about 60 % of
time saved for end users.
 The above percentages of saved time were
calculated as follows: We asked the
aforementioned domain experts to perform a
given set of queries using their conventional
methods; next, they performed the same set of
queries through the OmniSearch GUI; and
finally, the saved time for all domain experts
were averaged. Greater details on the system
time and saved time for end users are contained
in Table 4.
 Applying the MeSH-term filter resulted in a
much smaller number of relevant publications
returned. For example, 50 vs. 16 for the target
ABCC5, 13 vs. 2 for the target DPH2, and 31 vs.
3 for the target FOXQ1. More examples are
demonstrated in Table 5.
Discussion:
(1) The reduced time spent by users was due to both
data integration and the more accurate semantics
defined in the ontology.
(2) In an non-ontology software system, to filtering
on MeSH terms almost unavoidably results in
hard-coding some ad-hoc searching rules in source
code. On the contrary, semantics-oriented systems,
such as OmniSearch, can well handle this issue in a
more efficient manner. By decoupling domain
knowledge from source code, ontologies and software
applications can be developed independently, leading
to more flexible software development.
(3) Based on the is_a relation, the OmniSearch
system can perform logic reasoning over the
ontology concept hierarchy (that is, both broader and
narrower terms of the ontology term of interest),
thus greatly improving the flexibility of search and
query capability. For example, after a MeSH term is
chosen by users, they are able to search the exact
MeSH term, or its broader terms (i.e., ancestor
terms) and narrower terms (i.e., offspring terms)
defined in the ontology. Such results would not have
been obtained without semantic technologies
because systems based on relational databases are
not able to perform any logical reasoning. Of course,
users can still manually perform numerous queries
and then obtain similar results as obtained from our
system. However, such manual querying is
significantly more time-consuming and
labor-intensive, and more importantly, error-prone.
(4) Cross-referencing among miRDB, TargetScan,
and miRanda prediction results was made much
easier because relevant database entities have already
been formally defined in the OMIT. In other words,
unambiguous semantics was accurately encoded with
common data elements provided by the ontology,
resulting in successful data sharing and exchanging
among heterogeneous data sources.
(5) We asked the aforementioned domain experts to
verify the accuracy of MeSH-term filtering. Because
all returned publications contained the
corresponding MeSH term, the Precision measure
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 14 of 17
Table 4 The system time and saved time for end users
Query
First search Second search System time User time Percentage of saved Percentage of saved Percentage of saved
criterion criterion (seconds) (seconds) time for end users time on DAVID analysis time on result comparison
1 hsa-miR-1231 cell movement 2.51 10 62 % 55 % 61 %
2 hsa-miR-1288-5p cell proliferation 2.89 9 61 % 51 % 62 %
3 hsa-miR-143-3p mitosis 5.54 10 61 % 52 % 60 %
4 hsa-miR-192-5p leukemic infiltration 2.24 8 53 % 53 % 59 %
5 hsa-miR-216a-5p drug resistance, 4.09 11 65 % 55 % 62 %
multiple
6 hsa-miR-29c-3p recurrence 8.99 11 68 % 53 % 63 %
7 hsa-miR-3155a dna cleavage 1.21 6 53 % 47 % 55 %
8 hsa-miR-320b drug resistance 17.59 18 73 % 51 % 66 %
9 hsa-miR-3622a-5p entosis 0.30 6 51 % 43 % 57 %
10 hsa-miR-371b-5p mitochondrial 3.89 12 66 % 59 % 64 %
dynamics
11 hsa-miR-3934-5p dna methylation 0.93 8 61 % 45 % 59 %
12 hsa-miR-4263 mutagenesis 1.65 6 52 % 46 % 56 %
13 hsa-miR-4431 mitochondrial 0.17 6 53 % 47 % 55 %
degradation
14 hsa-miR-4505 cell transformation, 4.25 10 63 % 55 % 61 %
neoplastic
15 hsa-miR-4648 cell polarity 0.71 6 52 % 45 % 57 %
16 hsa-miR-4700-3p neoplasm regression, 1.56 7 53 % 51 % 59 %
spontaneous
17 hsa-miR-4756-5p endocytosis 3.76 10 67 % 53 % 62 %
18 hsa-miR-4802-3p drug resistance, 1.67 7 55 % 47 % 59 %
microbial
19 hsa-miR-501-3p insulin resistance 1.78 8 57 % 43 % 61 %
20 hsa-miR-520a-3p ubiquitination 13.31 17 75 % 55 % 65 %
Average   3.95 9.30 60.05 % 50.30 % 60.15 %
was evaluated as 100 %. As for the Recall measure, it
took a much longer time to evaluate because we
needed to identify all publications that were
incorrectly filtered out by the system. For example,
there were three (one, resp.) publications relevant to
CSNK2A1 (DVL3, resp.) that should not have been
filtered out. More such examples are demonstrated
in Table 6. Overall, an average Recall of 73 % was
achieved, meaning that while a user is able to obtain
desired knowledge in a much more efficient manner
(by reading significantly less publications, as shown in
Table 5), the potential information lost is rather low.
3. Friendly user interface.
 For both search boxes, a list of partially
matching terms were presented in a drop-down
box as users typed in the box. Users were also
allowed to not to type in anything, in which case
all terms will be presented.
 The Rows Per Page drop-down and pagination
control helped users to easily navigate among all
predicted targets.
 A set of display filters were designed to allow
users to conveniently and freely customize their
preferred way to view retrieved results from
various facets. For example, results can be
sorted by the prediction score from any selected
prediction database; users can choose to view
only results that have publication evidence, or
does not have such evidence, or both; and so
forth.
 Flexible download options were provided, and
all downloaded documents had self-explanatory,
meaningful file names that contain the search
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 15 of 17
Table 5 Reduced number of publications after applying the
MeSH-term filter drug resistance
Target gene Original number Number of papers Percentage
symbol of papers after MeSH filtering reduced
ABCC5 50 16 68 %
DPH2 13 2 85 %
FOXQ1 31 3 90 %
CIAPIN1 43 4 91 %
SLC38A9 12 1 92 %
MCL1 452 31 93 %
MKNK2 30 2 93 %
BAG4 32 2 94 %
ARID3B 18 1 94 %
HSPB2 79 4 95 %
THEMIS2 20 1 95 %
BAK1 266 11 96 %
SULT4A1 27 1 96 %
FUT4 57 2 96 %
GPC6 29 1 97 %
DDX54 29 1 97 %
MBD1 58 2 97 %
PRDM1 118 4 97 %
DTNB 30 1 97 %
LIN28A 91 3 97 %
SIRT7 33 1 97 %
ZBTB7A 67 2 97 %
NCOR2 240 7 97 %
TTPA 35 1 97 %
MAP3K10 35 1 97 %
SGPL1 36 1 97 %
MYO18A 36 1 97 %
EIF4EBP1 217 6 97 %
LIMK1 109 3 97 %
TP53INP1 37 1 97 %
CYTH1 39 1 97 %
SLC7A1 41 1 98 %
date, Query_Results_for_hsa-miR-125b-5p-
2015-12-05.csv and
Target_List_for_hsa-miR-125b-5p-2015-12-
05.txt for example.
Conclusions
As a special class of ncRNAs, miRNAs have been demon-
strated to play important roles in various biological and
pathological processes. Because miRNAs realize their
functions by regulating respective targets, it is critical to
Table 6 An example set of publications correctly/incorrectly
filtered by drug resistance
Gene symbol Total number of
publications
without applying
the drug resistance
filter
Total number
of publications
that contain the
MeSH term drug
resistance
Total number of
incorrectly filtered
publications
IRF4 130 3 0
ARID3B 18 1 0
SGPL1 36 1 0
ESRRA 131 3 0
PAFAH1B1 129 1 0
ETS1 287 5 0
TTPA 35 1 0
DVL3 60 1 1
THEMIS2 20 1 0
VTCN1 66 1 0
WDR5 128 1 0
ETV6 198 4 0
TAZ 74 1 0
IL6R 300 1 0
DPH2 13 2 0
BTG2 84 1 0
CYP24A1 146 2 0
LIN28A 91 3 0
TRPS1 69 1 0
CSNK2A1 619 5 3
TP53INP1 37 1 0
GPC6 29 1 0
DICER1 291 3 0
identify and analyze miRNA-target interaction data to
better explore and delineate miRNA functions. Semantic
technologies and domain ontologies have been utilized to
overcome limitations of conventional miRNA knowledge
acquisitionmethods. To this end, we followed the research
direction identified in our previous investigations regard-
ing the establishment of common data elements and data
exchange standards in the miRNA research. Specifically,
our major scientific contributions in this paper are:
 We have significantly improved the OMIT ontology
by: (1) following a modularized ontology design; (2)
encoding all 1884 human miRNAs; and (3) setting up
a GitHub project site along with an issue tracker for
more effective community collaboration on the
ontology development. The up-to-date ontology file is
accessible at: http://purl.obolibrary.org/obo/omit.owl.
 Based upon the OMIT, we built the OmniSearch
semantic search system, accessible at: http://
Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 16 of 17
omnisearch.soc.southalabama.edu/index.php/
Software. Our experimental results demonstrated
promising performance of OmniSearch.
Consequently, more effective, more efficient
miRNA-related knowledge capture has been
achieved.
Finally, some research directions are envisioned as fol-
lows for our future work.
(1) To investigate a new set of filters to perform a wider
scope of ontology reasoning. For example, potential filters
can be developed according to different miRNA cate-
gories such as: oncogenic or tumor-suppressive miRNAs;
individual tissues and/or cell lines in which miRNAs are
expressed; and the gene family group to which miRNAs
belong.
(2) To verify the consistency of contents retrieved
from different data resources is another important future
research topic. It is not trivial to resolve conflicting facts
among different sources.
(3) It would be terrific for users to have more flexible
options in further exploiting the semantics of the domain.
Note that to construct more flexible queries will involve
natural language processing (NLP) techniques, which are
beyond the scope of this paper. Nevertheless, such an
interesting topic can be considered in the future.
Endnote
1There are 103 and 18 OMIT-specific terms and
relations, respectively.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
All authors performed requirements analysis. HJS contributed to GUI
development. JH, AR, YL, HJS, and FG contributed to ontology development,
term definition, and annotation examples. All authors read and approved the
final manuscript.
Acknowledgements
Funding for Huang, J was provided in part by the National Cancer Institute
(NCI) at the National Institutes of Health (NIH), under the Award Number
U01CA180982. Funding for Borchert, GM was provided in part by Natural
Science Foundation (NSF) CAREER grant 1350064 (GMB) awarded by Division
of Molecular and Cellular Biosciences (with co-funding provided by the NSF
EPSCoR program) and in part by the Abraham A. Mitchell Cancer Research
Fund. The views contained in this paper are solely the responsibility of the
authors and do not represent the official views, either expressed or implied, of
the NIH, NSF, the U.S. Government, or the Abraham A. Mitchell Cancer
Research Fund.
Author details
1School of Computing, University of South Alabama, Mobile, Alabama
36688-0002, USA. 2Computer and Information Science Department, University
of Oregon, Eugene, Oregon 97403-1202, USA. 3Miracle Query, Inc., Eugene,
Oregon 97403-1202, USA. 4Department of Philosophy, University at Buffalo,
Buffalo, New York 14260-4150, USA. 5Genome Informatics, The Jackson
Laboratory, Bar Harbor, Maine 04609-1523, USA. 6Department of Biomedical
Informatics, University of Utah, Salt Lake City, Utah 84112-5775, USA.
7Department of Biochemistry and Molecular and Cellular Biology, Georgetown
University Medical Center, Washington D.C. 20007-1485, USA. 8Center for
Computational Science, University of Miami, Miami, Florida 33146-2960, U.S.A.
9Department of Microbiology and Immunology, First Affiliated Hospital,
Kunming Medical University, Kunming, Yunnan 650032, China. 10Department
of Radiation Oncology, Washington University School of Medicine, St. Louis,
Missouri 63110-0001, USA. 11Mitchell Cancer Institute, University of South
Alabama, Mobile, Alabama 36604-1405, USA. 12Department of Biology,
University of South Alabama, Mobile, Alabama 36688-0002, USA. 13School of
Dental Medicine, University at Buffalo, Buffalo, New York 14214-8006, USA.
Received: 15 December 2015 Accepted: 12 April 2016
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 
DOI 10.1186/s13326-016-0096-7
RESEARCH Open Access
Gene Ontology synonym generation rules
lead to increased performance in biomedical
concept recognition
Christopher S. Funk1*, K. Bretonnel Cohen1, Lawrence E. Hunter1 and Karin M. Verspoor2,3
Abstract
Background: Gene Ontology (GO) terms represent the standard for annotation and representation of molecular
functions, biological processes and cellular compartments, but a large gap exists between the way concepts are
represented in the ontology and how they are expressed in natural language text. The construction of highly specific
GO terms is formulaic, consisting of parts and pieces from more simple terms.
Results: We present two different types of manually generated rules to help capture the variation of how GO terms
can appear in natural language text. The first set of rules takes into account the compositional nature of GO and
recursively decomposes the terms into their smallest constituent parts. The second set of rules generates derivational
variations of these smaller terms and compositionally combines all generated variants to form the original term. By
applying both types of rules, new synonyms are generated for two-thirds of all GO terms and an increase in F-measure
performance for recognition of GO on the CRAFT corpus from 0.498 to 0.636 is observed. Additionally, we evaluated
the combination of both types of rules over one million full text documents from Elsevier; manual validation and error
analysis show we are able to recognize GO concepts with reasonable accuracy (88 %) based on random sampling of
annotations.
Conclusions: In this work we present a set of simple synonym generation rules that utilize the highly compositional
and formulaic nature of the Gene Ontology concepts. We illustrate how the generated synonyms aid in improving
recognition of GO concepts on two different biomedical corpora. We discuss other applications of our rules for GO
ontology quality assurance, explore the issue of overgeneration, and provide examples of how similar methodologies
could be applied to other biomedical terminologies. Additionally, we provide all generated synonyms for use by the
text-mining community.
Keywords: Biomedical concept recognition, Named entity recognition, Text-mining, Gene ontology
Background
The Gene Ontology (GO) represents the standard by
which we refer to functions and processes that genes/gene
products participate in. Due to its importance in biology
and the exponential growth in the biomedical literature
over the past years, there has been much effort in utiliz-
ing GO for text mining tasks [1, 2]. Performance on these
recognition tasks is lacking; it has been previously seen
*Correspondence: christopher.funk@ucdenver.edu
1Computational Bioscience, University of Colorado School of Medicine, Aurora
CO 80045, USA
Full list of author information is available at the end of the article
that there is a large gap between the way concepts are rep-
resented in the ontology and the many different ways they
are expressed in natural text [35].
There are twomain applications of biomedical literature
mining where improved recognition of Gene Ontology
can improve downstream performance. 1) It is well known
that manual curation can no longer keep up with the
annotation of gene and protein function [6]. Automatic
annotation is not our direct goal, but utilizing automatic
methods to highlight functions could provide input to
curators to help speed up manual curation. The more
accurate automated methods become, the more useful
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 2 of 16
their application becomes inmanual curation. 2) Themin-
ing of GO concepts from large collections of biomedical
literature has been shown to be useful for biomedical dis-
covery, for example, pharmacogenomic gene prediction
[7] and protein function prediction [8, 9]. Providing these
discovery algorithms with not only cleaner, but more data,
could increase the ability their accuracy of prediction and
generalizability.
Identification of gene ontology concepts in unstructured
text
There are two main methods of identifying GO con-
cepts within unstructured text, dictionary lookup and
pattern/similarity based measures. Unfortunately, there
have been very few evaluations assessing the ability to
recognize and normalize Gene Ontology concepts from
the literature; this is mostly due to lack of gold-standard
annotations.
There are sub-tasks within the BioCreative I and IV
[2, 10] community challenges that involve similar, but
more involved, tasks to GO term recognition  relat-
ing relevant GO concepts given protein-document pairs.
While the methods utilized for this specific tasks are
beyond the scope of this work, some systems utilize these
corpora to evaluate their ability to identify GO concepts
on unstructured text. Ruch et al. [11] implement pat-
tern based matching on a 5 token window and a vector
space indexing model. Their GO pattern based matching
reports highest average precision of 0.07 while their index-
ing model reports highest precision at recall = 0 (0.15) on
the BioCreative I corpus. Gaudan et al. [12] utilize proxim-
ity, specificity, and similarity to calculate the score of GO
term t appearing in zone z. They report average precision
and recall of 0.34 for the terms at rank 1 on the BioCreative
I corpus. A more recent system, GOCat [13], combines
semantic similarity and a machine learning based k-NN
algorithm to return the most similar k GO concepts in
some text. On the Biocreative I corpus, GOCat reports
0.56 precision at recall 0.20 (F-measure = 0.29). A pitfall
of these types of algorithms is they do not identify the
exact span of text that matched the GO concept. They
only specify that the concept could be present within this
sentence or document.
Dictionary based methods identify the exact span of
text that corresponds to the GO concept. Previous work
evaluated concept recognition systems utilizing the Col-
orado Richly Annotated Full Text Corpus (CRAFT). Funk
et al. [14] evaluated three prominent dictionary-based sys-
tems (MetaMap, NCBO Annotator, and ConceptMapper)
and found Cellular Component was able to be recog-
nized the best (F-measure 0.77). The more complex terms
from Biological Process (F-measure 0.42) and Molecular
Function (F-measure 0.14) were much more difficult to
recognize in text. Campos et al. present a framework
called Neji and compare it against Whatizit on the
CRAFT corpus [15]; they find similar best performance
(Cellular Component 0.70, Biological Process/Molecular
Function 0.35). Other work explored the impact of case
sensitivity and information gain on concepts recogni-
tion and report performance in the same range as what
has previously been published (Cellular Component 0.78,
Biological Process/Molecular Function 0.40) [16]. Since
all previously publishedmethods utilized dictionary based
systems and report similar performance, there is a need
for more sophisticated methods of utilizing the informa-
tion contained within the Gene Ontology. For further
progress to be made, the gap between concept representa-
tion and their expression in literature needs to be reduced,
which serves as major motivation for the work presented
in this manuscript.
There have been efforts to increase the ability to recog-
nize biomedical concepts through enumerating variability
in terms through generation, rewriting, and suppression
rules. Tsuruoka et al. [17] generate spelling and punctu-
ation variants based upon probabilistic generation rules
learned from 84,000 MEDLINE abstracts. These types
of rules help to capture the surface variability within
concepts, such as type I, Type I, type i, etc. Hettne
et al. [18] implement rewriting and suppression rules for
to reduce the variability in UMLS concepts. For identifica-
tion of terms, they remove leading parentheses/brackets
and filter out some semantic types. Additionally, the sup-
press certain terms that should not be matched on, i.e.
only EC numbers or those that contain dosages. While the
rules presented here do not specifically utilize the meth-
ods described above, the same underlying principles are
incorporated.
Compositionality of the gene ontology
The structure of concepts from the Gene Ontology has
been noted by many to be compositional [1921]. A term
such as GO:1900122 - positive regulation of receptor
binding contains another concept GO:0005102 - recep-
tor binding; not only do the strings overlap, but the terms
are also connected by relationships within the ontology.
Ogren et al. explore more in detail terms as proper sub-
string of other terms [19]. Additionally, previous work
examined the compositionality of the GO and employed
finite state automata (FSA) to represent sets of GO terms
[20]. An abstracted FSA described in that work can be
seen in Fig. 1. This example shows how terms can be
decomposed into smaller parts and how many different
terms share similar compositional structure. While using
regular expressions are useful for simple terms, there are
more complex concepts that require more sophisticated
decomposition.
To facilitate generation of meaning (cross-product def-
initions) and consistency within the ontology, a system
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 3 of 16
Fig. 1 Finite state automata representing activation, proliferation, and differentiation GO terms. An abstracted FSA adapted from a figure in Ogren
et al. [20] that shows how a particular term can be decomposed into its smaller components; where cell type can be any specific type of cell
called Obol [22] was developed. This work involved
analyzing the structure of terms through the creation
of grammars to decompose and understand the for-
mal language underlying the GO. An example grammar
describing the positive regulation of a molecular function
term follows: process(P that positively_regulates(F)) ?
[positive],regulation(P),[of ],molecular_function(F). These
grammars serve as templates for the decompositional
rules utilized in this work. Recently, GO has been moving
away from pre-computed term, towards post-computed
on-the-fly creation of terms for annotations using cross-
products [23]. Additionally, TermGenie [24] was devel-
oped, using a pattern-based approach, to automatically
generate new terms and place them appropriately within
the Gene Ontology. This work dealt with the analysis and
generation of new terms for curation, but no work has
been focused on synonym generation.
There has been previous work using the compositional
nature and common syntactic patterns within the Gene
Ontology itself to automatically generate lexical elemen-
tary synonym sets [25]. This method generates a total
of 921 sets of synonyms with a majority being generated
from 13 terms; 80 % of the sets refer to orthographic
{synthase, sythetase}, chemical products {gallate, gallic
acid}, or Latin inflection {flagella, flagellum}. We believe
this method is complementary to what we present here.
In this work, we manually created these sets, along with
many more, through analysis of Gene Ontology annota-
tions in unstructured text. Additionally we go beyond and
incorporate derivational variants, i.e. flagella?flagellar,
which have been shown to be very useful for capturing
the natural language text of concepts. We were currently
unable to find them publicly available, but if we should, the
synonym sets by Hamon et al. could be seamlessly inte-
grated within the synonym generation rules we present
here.
Other work takes advantage of the structure of the Gene
Ontology and relationships between GO terms to show
that these properties can aid in the creation of lexical
semantic relationships for use in natural language pro-
cessing applications [26]. Besides compositionality, previ-
ous work tries to identify GO terms that express similar
semantics that use distinct linguistic conventions [27].
They find, in general, that concepts from the Gene Ontol-
ogy are very consistent in their representation (there are
some exceptions but these are quality issues that the
consortium would like to avoid or fix). The consistency
of term representation along with the underlying com-
positional structure suggests the effective generation of
synonyms for many terms using only a small number of
rules.
Current synonyms are not sufficient for text-mining
The identification of Gene Ontology terms is more dif-
ficult than many other types of named entities such as
genes, proteins, or species mainly due to the length [14]
and complexity of the concepts. To help illustrate this, we
examined all variations of the GO term GO:0006900 -
membrane budding within the CRAFT corpus. The entry
for this concept in the ontology file is seen below. Like
most other terms, the concept name appears as a noun
and the entry contains a few synonyms (Table 1).
There were eight varying expressions of membrane
budding in all of CRAFT, five of which are contained
Table 1 Example ontology entry for the concept membrane budding
id: GO:0006900
name: membrane budding
namespace: biological_process
def: "The evagination of a membrane resulting in formation of a vesicle."
synonym: "membrane evagination" EXACT
synonym: "nonselective vesicle assembly" RELATED
synonym: "vesicle biosynthesis" EXACT
synonym: "vesicle formation" EXACT
is_a: GO:0016044 ! membrane organization and biosynthesis
relationship: part_of GO:0016192 ! vesicle-mediated transport
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 4 of 16
within a single article about expression and localization
of Annexin A7 (PMID:12925238). In Table 2 we list the
annotations along with sentential context. We find that
using exact matching and context from the ontology file,
the first two examples can be identified, but the others
cannot. This one example illustrates that a rather simple
term can be expressed in natural language text in many
different ways, that convey identical semantic meaning.
Objectives of this work
We hypothesize that due to the highly formalized and
compositional nature of the Gene Ontology [19], a small
number of generation rules can help to automatically gen-
erate synonyms for current and novel GO concepts, dif-
ferentiated from GO synonyms as generated synonyms.
Additionally, we hypothesize that the variation captured
in these generated synonyms will allow for better recog-
nition of Gene Ontology concepts from the biomedical
literature. We are aware that our method might overgen-
erate like Blaschke et al. [28], but we also hypothesize that
those generated synonyms probably will not be found in
the biomedical literature, and therefore, will not hinder
performance.
In this work, we present 18 manually created rules
to facilitate generation of synonyms from the entirety
of the Gene Ontology. We evaluate these automatically
generated synonyms both intrinsically, on a gold stan-
dard corpus, and extrinsically, through manual valida-
tion of annotations from a large literature collection.
We show that these automatically generated synonyms
increase recognition of GO concepts over any published
results and illustrate the accuracy and impact the gen-
erated synonyms have at a large scale. Additionally, we
show that the principles behind the rules generalizes to
novel GO concepts. It is the goal to generate and release
these generated synonyms for the larger biomedical nat-
ural language processing community. Currently, we do
not suggest that all generated synonyms be considered
for addition to GO, but filtering and classification meth-
ods could be employed to suggest the most accurate
generated terms as synonyms. Not only does this work
apply to the two tasks mentioned above, but it also adds
the ability to generate synonyms for newly created GO
concepts.
Table 2 Examples of the membrane budding concept within a
single document
Lipid rafts play a key role inmembrane budding. . .
Having excluded a direct role in vesicle formation. . .
. . . involvement of annexin A7 in budding of vesicles
. . . Ca2+-mediated vesiculation process was not impaired
Red blood cells which lack the ability to vesiculate cause. . .
Methods
Methodological overview
The main idea behind our method is made up of three
different steps:
1. Recursively decompose each Gene Ontology term to
its constituent terms
2. Generate derivational variants for each of these
constituent terms
3. Recombine all forms of all constituent terms (the
constituent term itself, the generated derivational
variants, and current synonyms of constituent term
in the Gene Ontology) using differing syntactic and
lexical rules
This methodology is made possible due to the highly
formalized and compositional nature of the Gene
Ontology.
Returning to the membrane budding example pre-
sented above (Table 2), we illustrate the methodology
behind creation and application of our rules. By analyz-
ing the different ways membrane budding is expressed
in CRAFT, we find that a majority of the annotations
are phrased around the end product, the vesicle. To help
recognize these (currently) un-recognizable annotations
there are two steps that should be done: 1) reorder words
and change the syntax (budding of vesicles) and 2) gen-
erate derivational variants of vesicle (vesiculation and
vesiculate). We developed two classes of rules that inter-
act seamlessly to generate these types of synonym varia-
tion. The first we designate recursive syntactic and the
second derivational variant, which are discussed imme-
diately below. Each of our rules was manually created
through the analysis of the differences between concept
annotations within the gold standard CRAFT corpus and
the Gene Ontology itself, along with discussions with an
ontologist and biologist about how they most frequently
express certain concepts. A more in-depth example is
presented within the description of the individual rules.
Recursive syntactic rules
The recursive syntactic rules perform step 1 & 3 outlined
in the Methodological overview section. The recursive
rules, step 1, were developed through studying the gram-
mars used in Obol [22], utilizing the dependency parse of
the Gene Ontology terms from ClearNLP [29], and exam-
ining common formalizations within Gene Ontology con-
cepts. These represent semi-frozen expressions as anchors
to identify the constituent terms. The lexical and syntac-
tic recombination rules, step 3, were derived by studying
the transformations required to get from Gene Ontology
term to the gold standard annotations that appear in
CRAFT. Additionally, there were many discussions with
biologists on the variation in terminology in which they
could express the same concept.
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 5 of 16
We identified 11 cases when terms can be broken down
into smaller composite terms; we acknowledge that there
are more, but choose to focus on the ones that affected the
majority of concepts. Over 55 % (14,221 out of 25,471) of
the Gene Ontology concepts can be decomposed using at
least one of these 11 different cases. Through our analy-
sis we have developed an ordering for rule application, to
generate the most possible synonyms. The 11 cases, the
order, and examples applied are presented in Table 3; for
full enumeration and further explanation of all rules see
Additional file 1.
Derivational variant rules
Once the original term is broken down to its constituent
components, step 1, through the recursive syntactic rules
presented above, we can apply derivational variant gen-
eration rules, step 2. The goal of this step is to generate
synonyms that reflect the broader range of variability
that occurs in natural language text expression of Gene
Ontology concepts.We incorporate two open source tools
to generate the derivational variants, WordNet [30] and
Lexical Variant Generator [31]. There are a total of seven
different specific cases when we apply these derivational
generation rules (Table 4). These rules were developed
by examining the transformations needed to create the
text spans annotated in the CRAFT gold standard from
the information contained within the GO. For example,
for single word terms we would generate both verb and
adjective forms of the noun concept, if they exist, which
would then both be incorporated compositionally within
the more complex concepts. For additional explanation
and full enumeration of the rules see Additional file 1.
Example of rules applied
In Fig. 2 we walk through all three steps of the syn-
onym generation process with the concept GO:00507678
- negative regulation of neurogenesis.
1. It is decomposed into 2 constituent terms: 1)
negative regulation of and 2) another GO concept
 GO:0022008 - neurogenesis. Since it cannot be
decomposed any further, we begin generating
synonyms for both of these composite parts.
2. Derivational variants for the term neurogenesis are
generated utilizing the single word term rule. There
are three different forms of neurogenesis, the term
itself, the adjective form exists in WordNet [30] or
can be generated through LVG (lexical variant
Table 3 Recursive syntactic rules order, constituent terms, and example generated synonyms
Order Rule GO term Constituent terms Generated synonyms
1 via or involved in
terms
GO:0002679 - respiratory burst
involved in defense response
respiratory burst, defense
response
defense response associated respiratory
burst
2 regulation of terms GO:0030513 - positive regulation
of BMP signaling pathway
BMP signaling pathway positive regulation of BMP receptor
pathway, up-regulation of BMP receptor
signaling
3 response to terms GO:0034263 - autophagy in
response to ER overload
autophagy, ER overload ER overload responsible for autophagy,
autophagy response to ER overload
4 signaling terms GO:0035329 - hippo signaling hippo hippo signaling pathway, signaling of
hippo
5 biosynthetic process
terms
GO:0042095 - interferon-gamma
biosynthetic process
interferon-gama interferon-gamma biosynthesis,
production of interferon-gamma
6 metabolic process
terms
GO:0042120 - alginic acid
metabolic process
alginic acid metabolism of alginic acid, alginic acid
metabolism
7 catabolic process
terms
GO:0042190 - vanillin catabolic
process
vanillin vanillin degradation, breakdown of
vanillin
8 binding terms GO:0042314 - bacteriochlorophyll
binding
bacteriochlorophyll binding of bacteriochlorophyll, bacteri-
ochlorophyll bound
9 transport terms GO:0042876 - aldarate transmem-
brane transporter activity
aldarate, transmembrane transportation of aldarate across the
membrane, transporting aldarate
transmembrane
10 differentiation terms GO:0043158 - heterocyst differenti-
ation
heterocyst heterocyst cell differentiation, differenti-
ation into heterocyst
11 activity terms GO:0043492 - ATPase activity, cou-
pled to movement of substances
ATPase, coupled to move-
ment of substances
ATPase, coupled to movement of
substances, coupled to movement of
substances activity of ATPase
While these examples show only one rule applied at once, each constituent term identified recursively goes through each rule in the order outlined to determine the most
basic constituent terms, which will get derivational variations (discussed in next paragraph) and then combinatorially re-combined into generated synonyms of the original
term
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 6 of 16
Table 4 Individual derivational variant generation rules
Order Rule Rule defined GO terms Example derivations
1 Single word terms 1 {NN} ? {JJ} 1 GO:0043066 - negative regu-
lation of apoptosis
1 apoptotic down regulation
2 {NN} ? {VB} 2 GO:0023040 - signaling via
ionic flux
2 signaled via ionic flux
2 Double word terms 1 {NN_1 NN_2} ? {NN_1},
{VB_2 NN_1}, {JJ_1 NN_2},
{NN_1 JJ_2}
1 GO:0048666 - neuron devel-
opment
1 neural development, neurotic
development, neuronal develop-
ment
2 {JJ_1 NN_2} ? {JJ_1}, {JJ_1
JJ_2}
2 GO:0005576 - chromosomal
region
chromosomal, chromosome
region
3 Triple word terms 1 {NN_1 NN_2 NN_3}? {NN_1
NN_3}, {NN_3 NN_1}, {VB_3}
1 GO:0052386 - cell wall thick-
ening
1 thickened wall, cell
thickening, thickens cell wall
4 cell part terms Introduce and re-order cell part
terms
GO:0035452 - extrinsic com-
ponent of plastid membrane
peripheral to plastid membrane,
extrinsic to plastid membrane
5 sensory
perception terms
Introduce variants of the sense
- sensory perception of {NN}
GO:0050909 - sensory percep-
tion of taste
gustory, gustation
6 transcription,
X-dependent
terms
Introduce variants of transcrip-
tion
GO:0006410 - transcription,
RNA-templated
RNA-dependent reverse tran-
scription, RNA-dependent
RT
7 X strand annealing
activity terms
Introduce variants of anneal-
ing
GO:0033592 - RNA strand
annealing activity
RNA hybridization, hybridize
The seven patterns that we generate derivational variants are presented along with examples of each. While these are presented individually, all derivational and recursive
syntactic (presented in Table 3) interact at each step. The examples provided are single GO terms, but any of the constituent terms produced through the above steps will go
through all derivational rules, if possible. The bolded words in the GO Term and Synonyms generated column represent the impact of the rule. The Penn Treebank
part-of-speech (POS) tags are utilized below: NN = noun, VB = verb, JJ = adjective. All varying forms were converted to the basic POS tag, e.g. NNS = plural noun and were
converted to NN
generator) [31], and the current synonyms found
within the Gene Ontology.
3. There are no derivational variants of negative
regulation of, but there are syntactic and lexical
synonymous expressions enumerated in the
regulation of terms rule. To generate synonyms of
the original concept, the three forms of
neurogenesis are combinatorially combined with
the 12 different synonymous expressions of negative
regulation of to form 36 synonyms for the original
term; the Gene Ontology currently only has 4
synonyms for this concept.
ConceptMapper
ConceptMapper (CM) is an open source highly con-
figurable dictionary lookup tool created for identifying
named entities in text. CM is part of the Apache UIMA
Sandbox [32] and is available at http://uima.apache.org/d/
uima-addons-current/ConceptMapper. Version 2.3.1 was
used for these experiments.
The first step in the CM pipeline is to convert the GO
ontologies to the required XML dictionary format. The
document text is then provided and tokenized. All tokens
within a span, in this case a sentence, are looked up
in the dictionary using a configurable lookup algorithm.
The lookup algorithm has the ability to reorder words,
insert gaps, ignore words, identify all or only longest
match, etc. For each branch of GO we used the highest
performing parameter combination previously identified
[14]. Additional file 2 provides a summation of the dif-
ferent type of ConceptMapper parameters and shows the
exact parameter combinations used for recognition of
each sub-branch of the Gene Ontology.
Concept recognition pipeline and baselines
The baseline for GO recognition was established in pre-
vious work [14] through parameter analysis of three dif-
ferent concept recognition systems. The top performing
system, ConceptMapper (CM), is used for the following
test because it produced the highest F-measures on 7 out
of 8 ontologies in the CRAFT corpus. CM takes an obo file
and converts it to an xml dictionary, which is used to rec-
ognize concepts in free text. In analyzing the results there
are two different baselines that were provided. Both base-
lines use the same ConceptMapper parameters settings
but differ in the way the dictionary was created:
 B1, a CM dictionary containing only information
within the ontology obo file.
 B2, a CM dictionary that deletes the word activity
from molecular function terms containing that word
(for example, for term GO:0016787 - hydrolase
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 7 of 16
Fig. 2 Three steps of synonym generation applied. A single GO concept broken down into its composite parts (bolded and underlined), synonyms
generated for each part (text underneath the part), then combination of all synonyms from all composite parts to form complete synonym of the
original concept
activity a synonym of hydrolase is added). This
addresses a known property of molecular function
terms formalization that aims to separate of the
protein and the function of the protein.
For the intrinsic evaluation pipeline on the CRAFT cor-
pus, we use the version of GO used to annotate CRAFT
from November 2007. We are aware of the great number
of changes made, but this was purposefully done to keep
the concepts available to the dictionary the same that were
available to the annotators when they marked up the gold
standard. To show that the rules created are able to gen-
eralize and apply to the many new concepts added to the
Gene Ontology added since 2007, for the extrinsic evalu-
ation on large collection we use an updated version of the
GO from 9/25/2015.
Evaluation corpora
There are two different corpora utilized in evaluation of
our generated synonyms.
CRAFT corpus
The gold standard used is the Colorado Richly Annotated
Full-Text (CRAFT) Corpus [33, 34] version 1.0 released
October 19th, 2012. The full CRAFT corpus consists of 97
completely manually annotated biomedical journal arti-
cles, while the public release set, which consists of 67
documents, was used for this evaluation. CRAFT includes
over 100,000 concept annotations from eight different
biomedical ontologies. Even though the collection is small
relative to the size of PubMed, there is no other cor-
pus that has text-level annotations of Gene Ontology
concepts.
Large literature collection
To test generalization and for further analysis of the
impact our concept recognition can have, we utilized
a large collection of one million full-text articles from
Elsevier. This is a collection of full-text documents from
a wide variety of biomedical Elsevier journals that was
delivered to the University of Colorado for internal
analysis.
Evaluation of generated synonyms
To evaluate the synonyms given we use the same pipelines
described in Funk et al. [14]. Synonyms are generated
by each method and then only those that are unique
(both within the generated synonyms and GO itself )
are inserted into a temporary obo file. The temporary
obo file is then used to create an xml dictionary used
by ConceptMapper [35] for concept recognition. The
CRAFT corpus is used as the gold standard and preci-
sion, recall, and macro-averaged F-measure are reported
for each branch of the GO.We provide counts of concepts
along with changes from the evaluation on the large scale
corpus.
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 8 of 16
Results and discussion
Application of gene ontology synonym rules
To explore the impact that our rules had on the recogni-
tion of concepts from the biomedical literature, we applied
our synonym generation rules to two different version of
the Gene Ontology and compared the concepts identified
before/after application on two different biomedical cor-
pora. For evaluation on the CRAFT corpus, we applied
our rules to the CRAFT annotated version of GO con-
taining 25,471 concepts; our 18 rules generated 291,031
synonyms for 16,800 concepts (66 % of all concepts).
Because the CRAFT version is from 2007, we applied
our methodology to a more recent version of GO from
September 2015. On this recent version, our rules gener-
ated ?1.5 million unique, but unconfirmed, synonyms for
66 % of all GO concepts (27,610 out of 41,852). Only a few
rules, 18, can have wide applicability to a majority of the
concepts in the Gene Ontology due to the concepts being
highly formalized and exhibiting a compositional nature.
While our rules appear to overgenerate, the main focus of
this work is to improve recognition of GO concepts from
the biomedical literature; we expect overgeneration to not
decrease performance because a majority of the generated
synonyms will not be seen in the biomedical literature.
An easy method of reducing the overgeneration would
be to only include the generated synonyms that currently
appear in all of MEDLINE or through an exact Google
search.
Not only do the introduced rules generate new
synonyms, but are also able to recreate 67 % of all
synonyms (68,174 out of 101,615) from all concepts
on the 2015 version. This illustrates the usefulness
of our presented methodology for not only synonym
generation for ontology curation and enhance-
ment. We now focus on how introducing variation
through synonym generation aids in identification
of Gene Ontology concepts from the biomedical
literature.
Synonym evaluation on a gold standard
The overall results for all methods performance on
CRAFT can be seen in Table 5 with more detailed anal-
ysis of each method following. More details about how
we evaluated performance of each method can be seen in
Evaluation of generated synonyms.
Besides the rules presented, there are a number of man-
ually curated external mappings from Gene Ontology
concepts to other data sources such as UniProt [36], the
Brenda database [37], andWikipedia [38]. To test the use-
fulness of these mappings as sources of synonyms, we
imputed synonyms for the Gene Ontology concept from
synonyms of the linked concept in the respective data
source. Overall, we find that external ontological map-
pings introduce significantly more errors than correctly
recognized concepts and are not suggested to be useful,
in their current form, as a whole, for concept recognition
(methods and detailed analysis of each data source can be
seen in Additional file 3).
Overall, the best results are obtained by using both
syntactic recursive and derivational rules; an increase in
F-measure of 0.112 is seen (0.610 vs 0.498). This per-
formance gain is the result of a large increase in recall
(0.225) with only a modest decrease in precision (0.049).
Examining the overall performance we find that all meth-
ods perform better than B1, while all but the external
synonyms perform better than B2. Overall, all genera-
tion methods increase recall with a decrease in precision,
which is to be expected when adding synonyms. We now
discuss the impact of synonyms generated through both
classes of rules.
Performance impact of generated synonyms
The Gene Ontology is broken down into three sub-
ontologies, Cellular Component (CC), Biological Process
(BP), and Molecular Function (MF). Terms from each
sub-ontology have differing biological meaning and tex-
tual characteristics  some rules are more applicable
to one sub-ontology than another, so we evaluate them
separately. We apply only the recursive syntactic rules
(Steps 1 & 3, described in Methodological overview)
to all concepts within the Gene Ontology and evalu-
ate on the full-text CRAFT corpus using our dictionary
based lookup system ConceptMapper; performance can
be seen in Table 6. For Cellular Component, only a
few new synonyms are generated, which is not surpris-
ing, because concepts from this branch normally do not
Table 5 Micro-averaged results for each synonym generation method on the CRAFT corpus
Method TP FP FN Precision Recall F-measure
Baseline (B1) 10,778 6,280 18,669 0.632 0.366 0.464
Baseline (B2) 12,217 7,367 17,230 0.624 0.415 0.498
All external synonyms 12,747 11,682 16,704 0.522 0.433 0.473
Recursive syntactic rules 12,411 7,587 17,036 0.621 0.422 0.502
Recursive syntactic and derivational rules 18,611 10,507 10,836 0.639 0.632 0.636
Bold highlighting indicates the method that produces the highest F-measure
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 9 of 16
Table 6 Performance of manual Gene Ontology rules on the CRAFT corpus
Method Generated synonyms Affected terms TP FP FN P R F
Cellular Component (CC)
Baseline (B1) X X 5,532 452 2822 0.925 0.662 0.772
Baseline (B2) X X 5,532 452 2822 0.925 0.662 0.772
Syntactic recursion rules 23 21 5,532 452 2,822 0.925 0.662 0.772
Both rules 4,083 724 6,585 969 1,769 0.872 0.788 0.828
Molecular Function (MF)
Baseline (B1) X X 337 146 3,843 0.698 0.081 0.145
Baseline (B2) X X 1,772 964 2,408 0.648 0.424 0.512
Syntactic recursion rules 11,637 7,353 1,759 977 2,421 0.643 0.421 0.509
Both rules 14,413 7,401 2,422 1,074 1,758 0.693 0.579 0.631
Biological Process (BP)
Baseline (B1) X X 4,909 5,682 12,004 0.464 0.290 0.357
Baseline (B2) X X 4,913 5,951 12,000 0.452 0.291 0.354
Syntactic recursion rules 182,617 6,847 5,120 6,158 11,793 0.454 0.303 0.363
Both rules 272,535 8,675 9,604 8,464 7,309 0.532 0.568 0.549
All Gene Ontology
Baseline (B1) X X 10,778 6,280 18,669 0.632 0.366 0.464
Baseline (B2) X X 12,217 7,367 17,230 0.624 0.415 0.498
Syntactic recursion rules 194,277 14,221 12,411 7,588 17,036 0.621 0.422 0.502
Both rules 291,031 16,800 18,611 10,507 10,836 0.640 0.632 0.636
Bold highlighting indicates where the generated synonyms have a positive effect on the performance
appear compositional in nature. These new CC have no
impact when compared to the baselines.
Eighty six percent (7,353 out of 8,543) of terms within
Molecular Function had at least one new synonym gen-
erated by the recursive syntactic rules. Unexpectedly,
performance on MF slightly decreases. The performance
on Biological Process slightly increases with the addition
of recursive syntactic rules. BP sees the largest increase
in the number of new synonyms generated, with over
180,000 new synonyms for 46 % (6,847 out of 14,767) of
BP concepts. The syntactic recursive rules are most help-
ful in generating Biological Process synonyms that match
instances within CRAFT. For example, 74 more correct
instances of GO:0016055 - Wnt receptor signaling path-
way, expressed in the gold standard as Wnt signaling
and Wnt signaling pathway, are able to be identified.
These are generated through the signaling terms rule
which found that both the words receptor and path-
way were uninformative.
MF and BP share similarities in the kinds of errors
introduced: a true positive (TP) in the baseline is con-
verted to a false positive (FP) and false negative(s) (FN)
because a longer term is identified through one of the gen-
erated synonyms (one ConceptMapper parameter used
specifies that only the longest match is returned). It is pos-
sible that these are missing annotations within the gold
standard. For example, one of the generated synonyms for
GO:0019838 - growth factor binding is binding growth
factor. In the corpus, bound growth factor is annotated
with both GO:0005488 - binding and GO:0008083 -
growth factor activity. With our generated synonyms
added to the dictionary, the same text span is only anno-
tated with the more specific GO:0019838 - growth factor
binding which results in the removal of two true positives
and the introduction of one false positive, thus reducing
overall performance, but possibly increasing the accuracy
of annotations. If this is a wide-spread issue, changing the
parameters for our dictionary lookup will allow it to find
all concepts, which would identify all three annotations
instead of only the longest one.
Overall, despite the decrease in performance of Molec-
ular Function terms, the recursive syntactic rules slightly
improve concept recognition of the Gene Ontology on the
CRAFT corpus over baseline 2 (?200 more TPs and?200
more FPs introduced). Because the CRAFT corpus con-
tains only a small portion of the whole GO (1,108) and
these rules only account for reordering of tokens within
GO, we did not expect to see a large increase in concept
recognition performance.
When we apply both the recursive syntactic and deriva-
tional rules (Steps 1, 2 & 3, described in Methodological
Overview) to all concepts and evaluate on the full-text
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 10 of 16
CRAFT corpus we see improvements for all branches
(Table 6). (The derivational rules cannot be evaluated on
their own due to an implementation dependency to the
recursive syntactic rules. The derivational rules assume
that all concepts passed in will already be decomposed
into their smallest GO components. The real power comes
when combining both rules because variation is being
introduced in only parts of the longer GO concepts.) Each
branch has different properties and when evaluated indi-
vidually, we see an increase in F-measure for all. This
increase is due to a large gain in recall (up to 0.27). For
both Biological Process and Molecular Function, preci-
sion also increases, while precision slightly decreases for
Cellular Component. When performance is aggregated
over all branches of the Gene Ontology, an increase in
F-measure of 0.14 (0.498 vs. 0.636) is seen; this comes
from both an increase in recall (0.22) and precision (0.02).
Our rules introduce ?291,000 generated synonyms which
cover 66 % (16,800 out of 25,471) of all terms within GO.
Analysis of generated synonyms
Now we explore which generated synonyms contribute
the most to the increase in performance seen on the
gold standard corpus. The top 5 concepts that impact
these performance numbers are presented in Table 7.
For Cellular Component, the most helpful synonym
generated immunoglobulin?antibody is seen many
times within CRAFT and is contained within the dou-
ble word rule. The other four are generated using the
single word rule, specifically converting from the noun
form from the ontology to the adjective form. Through
examining Molecular Function terms, it became clear
that annealing was missing synonymous representation
within the Gene Ontology; within the annealing rule we
add a synonym of hybridization. Two of the next most
helpful synonyms are due to excluding low information
containing words and derivational variations. It should
be noted that within Molecular Function an even larger
increase in performance is seen between baseline 1 and
2 (Table 6), which takes into account the many activity
terms. These types of synonyms are also accounted for
in our rules and are compositionally combined into other
terms. For Biological Process we observe that the most
helpful synonyms are generated using the double word and
single word derivational rules.We also find that generating
different lexical forms of both single word concepts and
within longer terms helps to introduce many true positive
annotations.
From examining the top most helpful synonyms, we
provide evidence that the derivational synonyms improve
performance on a manually annotated corpus through
the introduction of more linguistic variability, which
decreases the gap between concepts in the ontology and
their expression in natural language text. Overall, the top
Table 7 The top 5 derivational synonyms that improve performance on the CRAFT corpus
GO ID Term name TP FP FN Generated synonyms
Cellular Component
GO:0019814 Immunoglobulin complex +548 +0 ?548 Antibody, antibodies
GO:0005634 Nucleus +218 +35 ?218 Nuclear, nucleated
GO:0005739 Mitochondrion +135 +0 ?135 Mitochondrial
GO:0031982 Vesicle +11 +3 ?11 Vesicular
GO:0005856 Cytoskeleton +15 +0 ?15 Cytoskeletal
Molecular Function
GO:0000739 DNA strand annealing activity +327 +1 ?327 Hybridized, hybridization, annealing, annealed
GO:0033592 RNA strand annealing activity +327 +1 ?327 Hybridized, hybridization, annealing, annealed
GO:0031386 Protein tag +6 +79 ?6 Tag
GO:0005179 Hormone activity +1 +0 ?1 Hormonal
GO:0043495 Protein anchor +1 +10 ?1 Anchor
Biological Process
GO:0010467 Gene expression +2235 +361 ?2235 Expression, expressed, expressing
GO:0007608 Sensory perception of smell +445 +1 ?445 Olfactory
GO:0008283 Cell proliferation +97 +71 ?97 Cellular proliferation, proliferative
GO:0007126 Meiosis +93 +2 ?93 Meiotic, meiotically
GO:0006915 Apoptosis +173 +2 ?173 Apoptotic
The GO terms that increase performance the most on CRAFT are along with the change () in number of true positives (TP), false positives (FP), and false negatives (FN) from
the baseline B2 (activity removed baseline). The generated synonyms that result in this increase are shown under Generated synonyms
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 11 of 16
generated synonyms that improve performance do not
take into accountmuch of the compositional nature of GO
terms. We believe this is due to two aspects; 1) The anno-
tation guidelines used to define what constitutes a correct
mention of a GO concept in CRAFT [39] and 2) CRAFT
is only a small representation of what is contained within
the entire biomedical literature. This small representation
is due to the paper content (only mouse papers resulting
in functional annotation of at least one protein), small cor-
pus size, and appearance of only a small subsection of the
Gene Ontology. To further evaluate the synonyms gener-
ated by our rules without the aforementioned drawbacks,
in the next section, we explore the impact our rules make
on a large collection of the biomedical literature.
Evaluation of generated synonyms on a large full text
collection
We evaluated the impact of synonyms generated by both
recursive syntactic and derivational variant rules have on
the ability to recognize GO concepts within a large collec-
tion of one million full text documents. Unlike the previ-
ous evaluation, these documents do not have any manual
annotation or markup of Gene Ontology concepts, so we
are unable to calculate precision/recall/F-measure. How-
ever, we can calculate descriptive statistics and perform
manual evaluation of a random sample of the differences
in annotations produced when our rules are applied. For
these we used a version of GO from September 2015.
Applying our rules generates ?1.5 million new synonyms
for 66 % of all GO concepts (27,610 out of 41,852).
Since one of the primary focuses of the Gene Ontology
is functional annotation of proteins, we imparted some of
that knowledge into the large scale analysis by calculat-
ing information content of each concept with respect to
the experimental UniProt GOA annotations [40]. We cal-
culated the information content (IC) described in Resnik
et al. [41]. The IC scores range from 0-12.25; a lower score
corresponds to a term that many proteins are annotated
with and should appear many times in the literature while
a high scoring term is more specific and might have only
one or two annotations in GOA. For example, a com-
mon term such as GO:0005488 - binding has a score of
0.80 while a more informative term GO:0086047 - mem-
brane depolarization during Purkinje myocyte cell action
potential has a score of 12.25. A score of undefined
corresponds to a concept that is not currently annotated
to any protein with GOA. It is our hypothesis that the
most informative terms (higher IC) would be more dif-
ficult to identify in text and that our rules, described
above, would help increase the frequency at which we can
recognize correct mentions of these highly informative
terms.
Statistics for both the concepts recognized using the
ontology (baseline 2 presented above) and rules applied
along with the differences broken down by information
content can be seen in Table 8. Utilizing only the informa-
tion contained within the Gene Ontology, and accounting
for activity terms, ?97 million mentions of ?12,000
unique GO concepts are identified. After generation of
synonyms by both the recursive syntactic and derivational
Table 8 Statistics of annotations produced on the large literature collection by information content
Baseline B2 With generated synonyms Impact of synonyms
IC # Terms # Annotations # Terms # Annotations New concepts New annotations Change
Undefined 3,548 16,929,911 4,303 23,653,066 755 6,723,155 +39.7 %
[0,1) 7 3,202,114 7 3,177,333 0 ?24, 781 ?0.1 %
[1,2) 16 2,655,365 17 2,801,431 1 146,066 +0.1 %
[2,3) 43 7,332,003 44 8,016,573 1 684,570 +0.1 %
[3,4) 94 4,474,422 101 5,188,968 7 714,546 +0.2 %
[4,5) 178 4,185,438 191 9,340,757 13 5,155,319 +123.8 %
[5,6) 354 13,547,423 373 22,284,670 19 8,737,247 +64.4 %
[6,7) 666 9,533,940 715 12,060,499 49 2,526,559 +26.3 %
[7,8) 1,044 18,354,299 1,154 21,251,834 110 2,897,535 +16.8 %
[8,9) 1,465 7,932,937 1,648 15,316,476 183 7,383,539 +92.4 %
[9,10) 1,551 4,813,153 1,813 7,671,601 262 2,858,448 +58.3 %
[10,11) 1,396 2,390,061 1,690 4,291,831 294 1,901,770 +79.1 %
[11,12) 942 1,246,758 1,162 2,279,005 220 1,032,247 +83.3 %
[12,13) 732 578,501 953 1,257,956 221 679,455 +117.2 %
Total 12,036 97,176,325 14,171 138,592,000 2,135 41,415,675 +42.5 %
Shows the number of unique terms and total number of annotations produced through baseline B2, both derivational and syntactic recursive rules applied, and the impact
the rules have overall. The change is percent change in total annotations
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 12 of 16
rules, ?138 million mentions of ?14,100 unique GO
concepts are identified. In summation, our rules aid in the
recognition of?41million more mentions for all GO con-
cepts (?42 % increase) along with the ability to recognize
?2,000 unique GO concepts (?18 % increase) that are not
previously identified using the ontology alone. There were
a total of ?2.5 million mentions associated with the 2,135
unique concepts that were only found when the synonym
generation rules were applied. The other ?39 million new
mentions are associated with the ?12,000 concepts both
dictionaries recognize.
The biggest increase in number of annotations and
concepts identified can be seen in those concepts with
undefined and higher information content (IC > 8). This
shows that the our syntactic and derivational rules suc-
cessfully introduce variation that allow the more specific
and information containing concepts to be recognized
either at all or more frequently. While we do not find
much change in annotations produced on the lower infor-
mation content concepts, we do see a negative change in
annotations produced for some of the low information
containing concepts. This is due to our rules generating
synonyms that can help to identify more specific con-
cepts. For example, GO:0005215 - transporter activity
is found ?75,000 fewer times after the addition of our
generated synonyms due to more specific transporters
being identified. For instance, in the following sentence,
the bold text corresponds to the concept recognized using
the baseline, while the italicized concept is exactly gen-
erated through the use of our rules: The present study
was aimed to evaluate whether intraperitoneal carnitine
(CA), a transporter of fatty acyl-CoA into the mitochon-
dria. . . . (PMID: 17239403). The usefulness of these rules
goes beyond that of just improving our recognition of con-
cepts from test as identification ofmore informational GO
concepts has been shown to increase performance on the
protein function prediction task [8, 9].
Examining the overall numbers of concepts and men-
tions recognized provides insights into how useful the
synonyms generated are for recognition of GO concepts
from the biomedical literature. Since most mentions iden-
tified using only the ontology information were also found
when the rules were applied, this indicates that our rules
aid in identification of many new concepts along with new
mentions of concepts, thus leading to an overall increase
in recall. We saw in evaluation on CRAFT that both preci-
sion and recall were increased; we explore throughmanual
validation the accuracy of concepts identified utilizing
the generated synonyms on a large scale in the following
section.
Manual validation of gene ontologymentions
Although we found an improvement in performance
on the CRAFT corpus and on the larger corpus a
significant number of additional concepts and mentions
were identified through our synonym generation rules,
we are hesitant to reach any further conclusions with-
out some manual validation of the accuracy of these
generated synonyms. There are too many concepts and
annotations produced to manually validate them all, so
we performed validation of a randomly distributed subset
of concepts and instances of those concepts within text.
For cases where the validity of the term was unclear from
the matched term text alone we went back to the origi-
nal paper and viewed the annotation in sentential context.
For a baseline of performance, we validated a random
sample of 1 % of baseline concepts (125 concepts with ?
1,200 randomly sampled mentions) from each IC range
and a random sample of 10 % of all new concepts (217
terms with ?1,450 randomly sampled mentions) recog-
nized through our rules; these results are presented in
Table 9. We find that overall accuracy is very high (0.94)
for the concepts recognized only utilizing the ontology
information. A majority of these text spans identified
are exact, or very near, matches to the official onto-
logical name or one current synonyms. The only vari-
ation introduced is through a stemmer or lemmatizer
used in the concept recognition pipeline (see Additional
file 2 for more details). The annotations produced when
using synonyms generated through our rules do not have
as high of accuracy (0.74) but still produce reasonable
results.
Earlier, we hypothesized that overgeneration of syn-
onyms would not hinder performance because synonyms
that contain incorrect syntactic format or those that are
not lexically sound, would not appear within the text
we are searching. While performing manual evaluation
of annotations produced, we noted that a majority of
the errors came from three scenarios: 1) naive stem-
ming introducing incorrect concepts (60 %), 2) incorrect
level of specificity due to information loss (25 %), and
3) inclusion of incorrect punctuation (15 %). A detailed
error analysis along with strategies to correct them is
presented in Additional file 4. Based upon these results,
we do not believe that the 1.5 million new synonyms
generated introduce many false positives from overgener-
ation. While we see a decrease in accuracy in annotations
returned from text when we include the synonyms gener-
ated by our rules, we do not attribute the decrease entirely
to the synonyms themselves, as over half of the errors are
due to interaction of synonyms and the stemmer utilized
for dictionary lookup (Additional file 4). An interesting
observation is that sometimes generating a phrase or syn-
onym that initially appears incorrect can actually aid in
recognition. An example is the different adjective forms
of protein; most would use the form proteinaceous,
but another form is generated through Lexical Variant
Generator (LVG), protenic. This appears multiple times
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 13 of 16
Table 9 Results of manual inspection of random samples of annotations
Baseline B2 With rules Overall
IC # Terms # Annotations Accuracy # Terms # Annotations Accuracy Accuracy
Undefined 35 231 0.98 75 363 0.70 0.81
[0,1) 1 15 0.20 0 0 0.00 0.20
[1,2) 1 15 1.00 1 4 1.00 1.00
[2,3) 1 15 1.00 1 4 1.00 1.00
[3,4) 1 4 1.00 1 1 0.00 0.80
[4,5) 2 30 0.60 2 24 0.88 0.72
[5,6) 4 60 0.97 2 13 0.23 0.84
[6,7) 7 79 0.99 5 41 0.49 0.82
[7,8) 10 136 0.89 11 116 0.65 0.78
[8,9) 15 197 0.98 19 163 0.83 0.91
[9,10) 16 175 0.97 26 205 0.79 0.87
[10,11) 14 119 0.83 30 217 0.80 0.81
[11,12) 10 103 0.97 22 141 0.77 0.86
[12,13) 8 93 0.98 22 156 0.72 0.82
Total 125 1272 0.94 217 1448 0.74 0.83
Accuracy, calculated via manual review of textual annotations for correctness, of random subsets of concepts recognized from the large literature collections. We sampled
1 % of concepts, with up to 15 randomly sampled specific text spans per concept, from concepts identified using baseline B2. We sampled 10 % of concepts, with up to 15
randomly sampled text spans per concept, from the new concepts recognized through the presented synonym generation rules. Overall accuracy is calculated by combining
annotations of the same IC from baseline and with our rules
within articles translated into English, for example, the
concept GO:0042735 - protein body is seen within the
following sentence The activity is exhibited through a
protenic body of NBCF. . .  (PMID: 1982217).
The impact of supercomputing on concept recognition
tasks
We ran the our concept recognition pipeline over the large
full text collection on the Pando supercomputer located
at the University of Colorado, Boulder campus. It has 60
 64 core systems with 512GB each along with 4  48
core systems with 1TB ram each, for a total of 4,032 com-
pute nodes. We utilized a quarter of the machine and
ran our pipeline over 1,000 directories with 1,000 full
text documents in each. We were able to produce GO
annotations for all one million documents in around 10
minutes. Granted, no components are particularly com-
plicated. They consist of a sentence splitter, tokenizer,
stemmer/lemmatizer, followed by dictionary lookup, but
we have performed similar tasks on a large memory
machine, with 32 cores and the complete task has taken
34 weeks. Given that Pubmed consists of over 24 mil-
lion publications, if it was possible to obtain all documents
and performance is linear to the number of documents,
we could recognize GO concepts from the entirety of the
biomedical literature in around 4 hrs. More complex and
time consuming tasks, such as relation extraction, will
take longer but will still be on the order of days or weeks
utilizing the power of a supercomputer, since these tasks
are embarrassingly parallel.
Generalization to other biomedical ontologies
The synonym generation methodology presented here,
of breaking down complex concepts into their most
constituent parts, generating synonym for the parts,
then recursively combining to form synonyms of the
original concept is one that can generalize to many
other ontologies or standardized terminologies. The Gene
Ontology contains very complex and lengthy worded
concepts; the rules required to implement composi-
tional synonyms in other ontologies might not need
as many syntactic and derivational rules as we present
here. Besides GO we can envision similar method-
ologies easily applied to Human Phenotype Ontology
(HPO), Chemical Entities of Biological Interest (ChEBI),
SNOMED, and International Classification of Diseases 10
(ICD10).
One example, within the Human Phenotype Ontology
(HPO) [42], there is a high level HPO term that cor-
responds to phenotypic abnormality. There are just
over 1,000 terms (?10 % of all HPO concepts) that
are descendants of phenotypic abnormality that can
be decomposed into: abnormality of [the] other con-
cept (e.g. HP:0000818 - abnormality of endocrine sys-
tem). Not only can we add syntactic rules to reorder
words, semantic synonyms of abnormality, such as
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 14 of 16
malformation or deformity, can be added to express
the concepts in similar ways. There are many other
concepts that could benefit from recursively generating
synonyms as the HPO appears to have compositional
characteristics as well. There could also be subsets of
rules depending on the context; recognizing concepts
in doctors notes or electronic medical record will be
expressed differently than those within the biomedical
literature.
Conclusions
In this work, we present a set of simple language gen-
eration rules to automatically generate synonyms for
concepts in the Gene Ontology. These rules take into
account the compositional nature of GO terms along
with manually created syntactic and derivational vari-
ants derived from discussions with biologists, ontolo-
gists, and through analyzing Gene Ontology concepts as
they are expressed within the literature. The 18 hand-
crafted rules automatically generate over ?1.5 million
new synonyms for ?66 % of all concepts within the Gene
Ontology. We acknowledge the approach overgenerates
synonyms, but we find that many generated synonyms
do not appear within biomedical text, thus not hindering
performance.
We argue that current synonyms in structured ontolo-
gies are insufficient for text-mining due to the vast
degree of variability of expression within natural lan-
guage text; our methods do not propose to solve this
problem, but make a step in the right direction. This
claim is supported through the examination of spe-
cific examples of concept variation in biomedical text
and an empirical evaluation of the overlap of cur-
rent GO synonyms and their expression in the CRAFT
corpus.
We evaluate our synonym generation rules both intrin-
sically and extrinsically. Utilizing the CRAFT corpus for
intrinsic evaluation, we evaluate three different sources
of automatically generated synonyms 1) external ontology
mappings, 2) recursive syntactic rules and 3) derivational
variant rules. External mappings introduced too many
false positives and are currently not recommended for use.
The recursive syntactic rules added ?194,000 new syn-
onyms but did not significantly affect performance. Using
a combination of recursive syntactic rules and derivational
variant rules ?300,000 new synonyms were generated,
resulting in an increase in F-measure performance of 0.14,
mostly due to greatly increased recall. This illustrates the
importance of derivational variants for capturing natural
expression.
Our rules were extrinsically evaluated on a large col-
lection of one million full text documents. The rules
aid in the recognition of ?2,000 more unique concepts
and increase the frequency in which all concepts are
identified by 41 % over the baseline (Table 9), using
only current information contained within the Gene
Ontology. Specifically, the synonyms generated aid in
the recognition of more complex and informative con-
cepts. Manual validation of random samples conclude
accuracy is not as high as desirable (74 %). An error
analysis produced concrete next steps to increase the
accuracy; simply removing one generation sub-rule, and
filtering mentions with unmatched punctuation, increases
accuracy of a random sample of 217 newly recognized
concepts (?1,450 mentions) to 83 %. Overall, man-
ual analysis of 342 concepts (?2,700 mentions) leads
to an accuracy of 88 % (Additional file 4). We find
that our rules increase the ability to recognize con-
cepts from the Gene Ontology within the biomedical
literature.
Even though we chose a specific dictionary based-
system, ConceptMapper, to evaluate our rules, the gen-
erated synonyms can also be useful for many other
applications. Any other dictionary based system can sup-
plement its dictionary with the generated synonyms.
Additionally, any machine learning or statistical based
methods will be able to utilize the synonyms we gener-
ate to try to normalize the span of text identified as a
specific entity type to an ontological identifier; this will
provide a richer feature representation for target con-
cepts. In addition, we provide examples of how these
rules could generalize to other biomedical ontologies and
discuss the impact of supercomputing on scaling this
work.
Not only have our rules proven to be helpful for recogni-
tion of GO concepts, but there are also other applications
separate from the evaluated task. They could be used to
identify inconsistencies within the current Gene Ontol-
ogy synonyms. Concepts that share similar patterns, i.e.
regulation of X, should all contain synonyms that corre-
spond to a certain syntactic pattern. While performing
this work we identified a few concepts that should con-
tain synonyms but do not, illustrating the usefulness of
the presented rules for ontology quality assurance as orig-
inally outlined in Verspoor et al. [27]. Additionally, a
certain conservative subset of our rules could easily be
incorporated into TermGenie [24], a web application that
automatically generates new ontology terms. Our rules
would be of help to generate synonyms of the auto-
matically generated concepts. It is our desire to submit
the good synonyms identified within the text to the
Gene Ontology Consortium for curation into the ontol-
ogy. Additionally, there could possibly be a text mining
synonym category added or we can deposit them, for the
time being, within a larger application such as Freebase
[43]. We would like other people to be able to use our
synonyms for text mining so we provide the full list as
Additional file 5.
Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 15 of 16
Additional files
Additional file 1: Full enumeration and explanation of each manually
created rule. (PDF 363 kb)
Additional file 2: Outline of ConceptMapper parameters used for each
branch of the Gene Ontology. (PDF 106 kb)
Additional file 3: Analysis of using external ontological mappings as
synonyms. (PDF 165 kb)
Additional file 4: Detailed error analysis of manually reviewed concepts
from large scale evaluation. (PDF 108 kb)
Additional file 5: The list of novel synonyms generated by this method.
(TXT 129024 kb)
Acknowledgements
This work was funded by NIH grant 2T15LM009451 to LEH. The S10
supercomputer resides on the University of Colorado Boulder campus and
was obtained through NIH grant 1S10OD012300-01.
Availability of data andmaterials
The list of novel unique automatically generated synonyms can be found in
Additional file 5.
The CRAFT corpus is available at http://bionlp-corpora.sourceforge.net/
CRAFT/.
The large literature Elsevier collection is unavailable for public distribution. We
are aware of the difficulties associated with obtaining or even accessing full-
text documents from subscription journal publishers in a machine readable
format, such as XML. This collection of articles was procured negotiating a
licensing deal mediated through on campus librarians. A possible solution and
lessons learned from that process is described in Fox et al. [44].
Authors contributions
CSF developed and evaluated synonym generation rules. LEH, KBC, and LEH
contributed to the design of methods and offered supervision at every step.
All authors read and approved the manuscript.
Competing interests
The authors declare that they have no competing interests.
Author details
1Computational Bioscience, University of Colorado School of Medicine, Aurora
CO 80045, USA. 2Department of Computing and Information Systems,
University of Melbourne, Parkville, Melbourne 3010, Australia. 3Health and
Biomedical Informatics Centre, University of Melbourne, Parkville, Melbourne
3010, Australia.
Received: 10 April 2015 Accepted: 5 August 2016
Slater et al. Journal of Biomedical Semantics  (2016) 7:49 
DOI 10.1186/s13326-016-0090-0
RESEARCH Open Access
Using AberOWL for fast and scalable
reasoning over BioPortal ontologies
Luke Slater1*, Georgios V. Gkoutos1,2, Paul N. Schofield3 and Robert Hoehndorf4
Abstract
Background: Reasoning over biomedical ontologies using their OWL semantics has traditionally been a challenging
task due to the high theoretical complexity of OWL-based automated reasoning. As a consequence, ontology
repositories, as well as most other tools utilizing ontologies, either provide access to ontologies without use of
automated reasoning, or limit the number of ontologies for which automated reasoning-based access is provided.
Methods: We apply the AberOWL infrastructure to provide automated reasoning-based access to all accessible and
consistent ontologies in BioPortal (368 ontologies). We perform an extensive performance evaluation to determine
query times, both for queries of different complexity and for queries that are performed in parallel over the ontologies.
Results and conclusions: We demonstrate that, with the exception of a few ontologies, even complex and parallel
queries can now be answered in milliseconds, therefore allowing automated reasoning to be used on a large scale, to
run in parallel, and with rapid response times.
Keywords: Ontology, Reasoning, Scalable reasoning, Description logics, OWL, AberOWL
Background
Major ontology repositories such as BioPortal [1], Onto-
Bee [2], or the Ontology Lookup Service [3], have existed
for a number of years, and currently contain several hun-
dred ontologies. They enable ontology creators and main-
tainers to publish their ontology releases and make them
available to the wider community.
Besides the hosting functionality that such reposito-
ries offer, they usually also provide certain web-based
features for browsing, comparing, visualising and process-
ing ontologies. One particularly useful feature, currently
missing from themajor ontology repositories, is the ability
to provide online access to reasoning services simultane-
ously over many ontologies. Such a feature would enable
the use of semantics and deductive inference when pro-
cessing data characterized by the ontologies these reposi-
tories contain [4].
For example, there is an increasing amount of RDF
[5] data becoming available through public SPARQL [6]
*Correspondence: lxs511@bham.ac.uk
1College of Medical and Dental Sciences, Institute of Cancer and Genomic
Sciences, Centre for Computational Biology, University of Birmingham, B15 2TT
Birmingham, United Kingdom
Full list of author information is available at the end of the article
endpoints [710], which utilise ontologies to annotate
entities, and access to reasoning over ontologies will allow
combined queries over knowledge contained in ontologies
and the data accessible through the SPARQL endpoints
[4].
However, enabling automated reasoning over multiple
ontologies is a challenging task, since automated reason-
ing can be highly complex and costly in terms of time
and memory consumption [11]. In particular, ontologies
formulated using the Web Ontology Language (OWL)
[12] can utilize statements based on highly expressive
description logics [13], and therefore queries that utilize
automated reasoning cannot, in general, be guaranteed to
finish in a reasonable amount of time.
Prior work on large-scale automated reasoning over
biomedical ontologies has often focused on the set of
ontologies in Bioportal, as it is one of the largest collec-
tions of ontologies freely available. To enable inferences
over this set of ontologies, modularization techniques
have been applied [14] using the notion of locality-based
modules, and demonstrated that, for most ontologies and
applications, relatively small modules can be extracted
over which queries can be answered more efficiently.
Other work has focused on predicting the performance of
reasoners when applied to the set of BioPortal ontologies
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Slater et al. Journal of Biomedical Semantics  (2016) 7:49 Page 2 of 6
[15], and have demonstrated that performance of particu-
lar reasoners can reliably be predicted; at the same time,
the authors have conducted an extensive evaluation of
the average classification time for each ontology. Further
approaches apply RDFS reasoning [16] to provide lim-
ited, yet fast, inference capabilities in answering queries
over Bioportals set of ontologies through a SPARQL inter-
face [17, 18]. Alternatively, systems such as OntoQuery
[19] provide web-based access to ontologies through auto-
mated reasoning but limit the number of ontologies. The
performance of OntoQuery has been found to be compa-
rable to the performance of reasoning over ontologies in
tools such as Protege [19].
The AberOWL [4] system is an ontology repository
which aims to allow access to multiple ontologies through
automated reasoning, utilizing their OWL semantics.
AberOWL mitigates the complexity challenges by using
a reasoner which supports only a subset of OWL (i.e.,
the OWL EL profile [20]), ignoring ontology axioms and
queries that do not fall within this subset. This enables
the provision of polynomial-time reasoning, which is suf-
ficiently fast for many practical uses, even when applied
to large ontologies [21]. However, thus far, the AberOWL
software has only been applied to a few, manually selected
ontologies, and therefore does not have a similar domain
coverage to other ontology repositories, nor does it cater
for reasoning over large sets of ontologies, such as the
ones provided by the BioPortal ontology dataset (Biopor-
tal contains, as of 9 March 2015, 428 ontologies consisting
of 6,668,991 classes).
Here, we apply the AberOWL framework to reason
over the majority of the ontologies available in Biopor-
tal. We evaluate the performance of querying ontologies
with AberOWL, utilizing 337 ontologies from BioPortal.
We evaluate AberOWLs ability to perform different types
of queries as well as assess its scalability in performing
queries that are executed in parallel. We demonstrate that
the AberOWL framework makes it possible to provide, at
least, light-weight description logic reasoning over most
of the freely accessible ontologies contained in BioPortal,
with a relatively lowmemory footprint and high scalability
with respect to the number of queries executed in paral-
lel, using only a single medium-sized server as hardware
to provide these services. Furthermore, we identify several
ontologies for which the performance of reasoning-based
queries is significantly worse than the majority of the
other ontologies tested, and discuss potential explanations
and solutions.
Methods
Selection of ontologies
We selected all ontologies contained in BioPortal as can-
didate ontologies, and attempted to download the current
versions of all the ontologies for which a download link
was provided by BioPortal. A summary of the results is
presented in Table 1.
Out of a total of 427 ontologies listed by Bioportal,
only 368 could be directly downloaded and processed by
AberOWL. Reasons for failure to load ontologies include
the absence of a download link for listed ontologies,
ontologies that are only available in proprietary data for-
mats (e.g., some of the ontologies and vocabularies are
provided in custom representation languages as part of
the Unified Medical Language Systems [22]), or license
restrictions which prevents their inclusion in public ontol-
ogy repositories (e.g., SNOMED CT). Thirty nine ontolo-
gies were not obtainable. Furthermore, 17 ontologies that
could be downloaded were not parseable with the OWL
API, indicating a problem in the file format used to dis-
tribute the ontology. Three ontologies were inconsistent
at the reasoning stage. Whilst several ontologies also
included unobtainable ontologies as imports, we included
these ontologies in our analysis, utilizing only the classes
and axioms that were accessible. As AberOWL currently
relies on the use of labels to construct queries, we further
removed 31 ontologies that did not include any class labels
from our test set.
Overall, we use a set of 337 ontologies in our experi-
ments consisting of 3,466,912 classes and 6,997,872 logical
axioms (of which 12,721 are axioms involving relations,
i.e., RBox axioms). In comparison, BioPortal currently (9
March 2015) includes a total of 6,668,991 classes.
Use of the AberOWL reasoning infrastructure
AberOWL [4] is an ontology repository and query service
built on the OWLAPI [23] library, which allows access
to a number of ontologies through automated reason-
ing. In particular, AberOWL allows users or software
applications to query a single ontology within AberOWL
using Manchester OWL Syntax [24], using the class and
property labels as short-form identifiers for classes. Mul-
tiple queries to single ontologies can be performed at the
same time, and AberOWL also provides functionality to
perform a query on all ontologies within the repository.
Table 1 Summary of Ontologies used in our test
Total 427
Loadable 368
Used 337
Unobtainable 39
Non-parseable 17
Inconsistent 3
No Labels 31
The loadable ontologies are the ones obtained from BioPortal which could be
parsed using the OWL API and which were found to be consistent when classified
with the ELK reasoner. We exclude 31 ontologies that do not contain any labels
from our analysis
Slater et al. Journal of Biomedical Semantics  (2016) 7:49 Page 3 of 6
AberOWL exposes this functionality over the Internet
through a JSON API as well as a web interface avail-
able on http://aber-owl.net. To answer queries, AberOWL
utilizes the ELK reasoner [25, 26], a highly optimized
reasoner that supports the OWL-EL profile. Ontologies
which are not OWL-EL are processed by the reasoner
by means of ignoring all non-EL axioms, although as of
2013 50.7 % of ontologies in Bioportal were natively using
OWL-EL [27].
We extended the AberOWL framework to obtain a list
of ontologies from the Bioportal repository, periodically
checking for new ontologies as well as for new versions
of existing ontologies. As a result, our testing version of
AberOWL maintains a mirror of the accessible ontolo-
gies available in BioPortal. Furthermore, similarly to the
functionality provided by BioPortal, a record of older ver-
sions of ontologies is kept within AberOWL, so that, in the
future, the semantic difference between ontology versions
can be computed.
In addition, we expanded the AberOWL software to
count and provide statistics about:
 the ontologies which failed to load, with associated
error messages;
 axioms, axiom types, and number of classes per
ontology; and
 axioms, axiom types, and number of classes over all
ontologies contained within AberOWL.
For each query of AberOWL, we also record the query
execution time within AberOWL and pass this informa-
tion back to the client along with the result-set of the
query. Thus, the figures presented here do not include the
time required to transmit the response.
All information is available through AberOWLs JSON
API http://aber-owl.net/help, and the source code is freely
available at https://github.com/bio-ontology-research-
group/AberOWL.
Experimental setup
In order to evaluate the performance of querying single
and multiple ontologies in AberOWL, queries of different
complexity were randomly generated and executed. Since
the ELK reasoner utilises a cache for answering queries
that have already been computed, each of the generated
queries consisted of a new class expression. The follow-
ing types of class expressions were used in the generated
queries (for randomly generated class labels A, B, and
relation R):
 Primitive class: A
 Conjunctive query: A and B
 Existential query: R some A
 Conjunctive existential query: A and R some B
Three hundred random queries for each of these types
were generated for each ontology that was tested (1200
queries in total per ontology). Each set of the 300 random
queries generated, were subsequently split into three sets,
each of which contained 100 class expressions. The ran-
dom class expressions contained in the resulting sets were
then utilised to perform superclass (100 queries), equiv-
alent (100 queries) and subclass (100 queries) queries,
and the response time of the AberOWL framework was
recorded for each of the queries.
We further test the scalability of answering the queries
by performing the queries in parallel. For this purpose, we
perform three separate tests, to query AberOWLwith one
query at once, 100 queries in parallel, and 1,000 queries in
parallel.
In our test, we record the response time of each query,
based on the statistics provided by the AberOWL server;
in particular, response time does not include network
latency. All tests are performed on a server with 128 GB
memory and two Intel Xeon E5-2680v2 10-core 2.8 GHz
CPUs with hyper-threading activated (resulting in 40 vir-
tual cores). The ELK reasoner underlying AberOWL is
permitted to use all available (i.e., all 40) cores to perform
classification and respond to each query.
Results and discussion
On average, when performing a single query over
AberOWL, query results are returned in 10.8 ms (stan-
dard deviation: 48.0 ms). The time required to answer
a query using AberOWL correlates linearly with the
number of logical axioms in the ontologies (Pearson cor-
relation, ? = 0.33), and also strongly correlates with
the number of queries performed in parallel (Pearson
correlation, ? = 0.82).
Figure 1 shows the query times for the ontologies
based on the type of query, and Fig. 2 shows the query
times based on different number of queries run in par-
allel. The maximum observed memory consumption for
the AberOWL server while performing these tests was
66.1 GB.
We observe several ontologies for which query times
are significantly higher than for the other ontologies. The
most prevalent outlier is theNCI Thesaurus [28] for which
average query time is 600 ms when performing a single
query over AberOWL. Previous analysis of NCI The-
saurus has identified axioms which heavily impact the
performance of classification for the ontology using mul-
tiple description logic reasoners [29]. The same analysis
has also shown that it is possible to significantly improve
reasoning time by adding inferred axioms to the ontol-
ogy. To test whether this would also allow us to improve
reasoning time over the NCI Thesaurus in AberOWL and
using the ELK reasoner, we apply the Elvira modulariza-
tion software [21], using the HermiT reasoner to classify
Slater et al. Journal of Biomedical Semantics  (2016) 7:49 Page 4 of 6
a
b
c
d
Fig. 1 Query times as a function of the number of logical axioms in
the ontologies, separated by the type of query
a
b
c
Fig. 2 Query times as function of the number of logical axioms in the
ontologies, separated by the number of queries executed in parallel
Slater et al. Journal of Biomedical Semantics  (2016) 7:49 Page 5 of 6
the NCI Thesaurus to add all inferred axioms that fall into
the OWL-EL profile to the ontology, as opposed to ELKs
approach of ignoring non-EL axioms during classification.
We then repeat our experiments. Figure 3 shows the dif-
ferent reasoning times for NCI Thesaurus before and after
processing with Elvira. Query time reduces from 703 ms
(standard deviation: 689 ms) before processing with Elvira
to 51 ms (standard deviation: 42 ms) after processing with
Elvira, demonstrating that adding inferred axioms and
removing axioms that do not fall in the OWL-EL profile
can be used to improve query time.
a
b
Fig. 3 Query times over the NCI Thesaurus
Another outlier with regard to average query time is the
Natural Products Ontology (NATPRO, http://bioportal.
bioontology.org/ontologies/NATPRO). However, as NAT-
PRO is expressed in OWL-Full, it cannot be reliably clas-
sified with a Description Logic reasoner, and therefore
we could not apply the same approach to improve the
performance of responding to queries.
Future work
The performance of using automated reasoning for query-
ing ontologies relies heavily on the type of reasoner used.
We have used the ELK [25, 26] reasoner in our evalua-
tion; however, it is possible to substitute ELK with any
other OWLAPI-compatible reasoner. In particular, novel
reasoners such as Konklude [30], which outperform ELK
in many tasks [31], may provide further improvements in
performance and scalability.
We identified several ontologies as leading to per-
formance problems, i.e., they are outliers during query
time testing. For these ontologies, including the Natural
Products Ontology (NATPRO), and, to a lesser degree,
the Drug Ontology (DRON) [32], similar culprit-finding
analysis methods may be applied as have previously been
applied for the NCI Thesaurus [29]. These methods may
also allow the ontology maintainers to identifying possi-
ble modifications to their ontologies that would result in
better reasoner performance.
Conclusion
We have demonstrated that it is feasible to reason over
most of the ontologies available in BioPortal in real time,
and that queries over these ontologies can be answered
quickly, in real-time, and using only standard server hard-
ware. We further tested the performance of answering
queries in parallel, and show that, for themajority of cases,
even highly parallel access allows quick response times
which scale linearly with the number of queries.
We have also identified a number of ontologies for
which performance of automated reasoning, at least when
using AberOWL and the ELK reasoner, is significantly
worse, which renders them particularly problematic for
applications that carry heavy parallel loads. At least for
some of these ontologies, pre-processing them using tools
such as Elvira [21] can mitigate these problems.
The ability to reason over a very large number of ontolo-
gies, such as all the ontologies in BioPortal, opens up the
possibility to frequently use reasoning not only locally
when making changes to a single ontology, but also mon-
itor  in real time  the consequences that a change
may have on other ontologies, in particular on ontolo-
gies that may import the ontology which is being changed.
Using automated reasoning over all ontologies within
a domain therefore has the potential to increase inter-
operability between ontologies and associated data by
Slater et al. Journal of Biomedical Semantics  (2016) 7:49 Page 6 of 6
verifying mutual consistency and enabling queries across
multiple ontologies, and our results show that such a
system can now be implemented with available software
tools and commonly used server hardware.
Acknowledgements
This paper is an extended version of a conference paper by the same name,
presented at ICBO 2015.
Authors contributions
RH and LS conceived of the study. LS implemented the software and
performed the experiments. All authors evaluated the results. RH, PS, GVG
supervised the research. All authors contributed to the manuscript. All authors
read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Author details
1College of Medical and Dental Sciences, Institute of Cancer and Genomic
Sciences, Centre for Computational Biology, University of Birmingham, B15 2TT
Birmingham, United Kingdom. 2Institute of Translational Medicine, University
Hospitals Birmingham NHS Foundation Trust, B15 2TT Birmingham, United
Kingdom. 3Department of Physiology, Development and Neuroscience,
University of Cambridge, Downing Street, CB2 3EG England, United Kingdom.
4Computational Bioscience Research Center, Computer, Electrical and
Mathematical Sciences & Engineering Division, King Abdullah University of
Science and Technology, 4700 KAUST, 23955-6900 Thuwal, Saudi Arabia.
Received: 3 February 2016 Accepted: 8 July 2016
SHORT REPORT Open Access
Publication of nuclear magnetic resonance
experimental data with semantic web
technology and the application thereof to
biomedical research of proteins
Masashi Yokochi1, Naohiro Kobayashi1, Eldon L. Ulrich2, Akira R. Kinjo1, Takeshi Iwata1, Yannis E. Ioannidis3,
Miron Livny4, John L. Markley2, Haruki Nakamura1, Chojiro Kojima1 and Toshimichi Fujiwara1*
Abstract
Background: The nuclear magnetic resonance (NMR) spectroscopic data for biological macromolecules archived at
the BioMagResBank (BMRB) provide a rich resource of biophysical information at atomic resolution. The NMR data
archived in NMR-STAR ASCII format have been implemented in a relational database. However, it is still fairly
difficult for users to retrieve data from the NMR-STAR files or the relational database in association with data from
other biological databases.
Findings: To enhance the interoperability of the BMRB database, we present a full conversion of BMRB entries to
two standard structured data formats, XML and RDF, as common open representations of the NMR-STAR data.
Moreover, a SPARQL endpoint has been deployed. The described case study demonstrates that a simple query of
the SPARQL endpoints of the BMRB, UniProt, and Online Mendelian Inheritance in Man (OMIM), can be used in
NMR and structure-based analysis of proteins combined with information of single nucleotide polymorphisms
(SNPs) and their phenotypes.
Conclusions: We have developed BMRB/XML and BMRB/RDF and demonstrate their use in performing a federated
SPARQL query linking the BMRB to other databases through standard semantic web technologies. This will facilitate
data exchange across diverse information resources.
Keywords: NMR, BMRB, Database, XML, RDF
Findings
Background
The BioMagResBank (BMRB; http://www.bmrb.wisc.edu/)
is a worldwide data repository for experimental and de-
rived data gathered from nuclear magnetic resonance
(NMR) spectroscopic studies of biological molecules [1].
For more than 15 years, the BMRB has used and devel-
oped the NMR-STAR format based on the STAR format
specifications [24] for data archiving and data exchange
(see Fig. 1a). The NMR-STAR Dictionary, acting as ontol-
ogy for NMR-STAR data, continues to be improved and
expanded to keep up with the needs of the biomolecular
NMR community. The most important parameter
archived in the BMRB is assigned chemical shifts, which
can be used directly to determine protein secondary struc-
ture and to assist in the determination of their solution
structures, to identify interactions of small molecules with
target proteins for drug discovery, and to characterize
protein-protein interactions.
As of 2015, the BMRB archive contains more than
10,000 entries, and ~800 new entries are being added
each year. This growing biological data archive has
raised researchers interest in integrating the diverse
biological information resources to form new hypotheses
for understanding biological functions and phenomena.
The best approach for enhancing interoperability of the
* Correspondence: tfjwr@protein.osaka-u.ac.jp
1Institute for Protein Research, Osaka University, 3-2 Yamadaoka, Suita, Osaka
565-0871, Japan
Full list of author information is available at the end of the article
© 2016 Yokochi et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Yokochi et al. Journal of Biomedical Semantics  (2016) 7:16 
DOI 10.1186/s13326-016-0057-1
Fig. 1 The sequential data conversion of a BMRB entry from NMR-STAR format. a Part of a BMRB entry in NMR-STAR format. Parts of the entry
have been converted to b XML format and c RDF format. d Schematic representation of linked external information resources, where shorter distances
from the BMRB represent closer relationships with the BMRB
Yokochi et al. Journal of Biomedical Semantics  (2016) 7:16 Page 2 of 4
BMRB would be to convert the archive into standard
web formats, XML and RDF, using a data structure that
corresponds closely to the NMR-STAR ontology
described by an XML schema and OWL.
Methods
We have extended the NMR-STAR Dictionary to accom-
modate the derived data repositories on BMRB, such as
LACS validation reports [5], structural annotations using
PACSY [6] and Protein Blocks [7], etc., followed by transla-
tion of the dictionary to an XML schema [8] (BMRB/XML
Schema), using the PDBx/mmCIF Dictionary Suite de-
veloped by the Research Collaboratory for Structural
Bioinformatics (RCSB) Protein Data Bank (PDB)
(http://sw-tools.rcsb.org/). To automate the XML con-
version of BMRB entries, we have developed a software
suite (the BMRBxTool) that generates XML documents
and validates their format and data consistency according
to the BMRB/XML schema. EXtensible Stylesheet Lan-
guage (XSL) transformation [9] was applied to generate
BMRB entries in RDF format from the corresponding XML
documents. Along with the RDF, we also provide its
ontology written in RDF/RDFS/OWL syntax as BMRB/
OWL [10, 11], which is a direct translation of the BMRB/
XML schema. To bridge different data models between the
XML tree and the RDF directed graph, we have introduced
abstract OWL classes and RDF properties to the ontology
[12]. For the data conversion from XML to RDF in accord-
ance with the principles of Linked Data [13] and recom-
mendations widely accepted by biological database
community [14], we have developed a software suite called
BMRBoTool. We have tested as many as thirty SPARQL
queries to show how NMR experimental data can be re-
trieved. In a case study to demonstrate a federated SPARQL
query, we performed a search for phenotypes annotated
with information for SNPs from the human genome by in-
tegrating three SPARQL endpoints: the BMRB, UniProt,
and OMIM [see Additional file 1 for details].
Results and discussion
On our portal site (http://bmrbpub.protein.osaka-u.ac.jp/,
hereafter abbreviated as ~/), we have archived the
BMRB/XML (~/archive/xml/), as shown in Fig. 1b. The
BMRB/RDF derived from the reduced version of the
Fig. 2 Federated search using multiple SPARQL endpoints of the BMRB, UniProt, and OMIM. a An example of a SPARQL query begins from the
BMRB entry. b Results performed by the SPARQL query for BMRB entry 4280, showing the BMRB ID, mutation, OMIM ID, dbSNP ID, secondary structure,
and SASA. c Ribbon models of NMR structure (PDB: 1QK9) of MECP2. The side-chains (space-filling model) for the mutation (E137 and A140) exposed
to solvent are responsible for X-linked mental retardation [18]
Yokochi et al. Journal of Biomedical Semantics  (2016) 7:16 Page 3 of 4
BMRB/XML (lacking bulky information such as atomic
coordinates and NMR restraints) has also been archived
(~/archive/rdf/) as shown in Fig. 1c. A bulk download ser-
vice is available using the rsync protocol that helps users
mirror the latest data collections, which are updated peri-
odically. A schematic RDF graph of linked databases is il-
lustrated in Fig. 1d. Owing to the machine readability of
the XML format, the BMRB/XML provides users with an
excellent starting point to develop new tools for use in
biochemistry and structural biology. The BMRB/RDF and
associated web services enable the integration of the
BMRB archive with other biological databases, which may
facilitate flexible data exchange and knowledge discovery.
Furthermore, we provide a SPARQL endpoint for query-
ing the BMRB/RDF (~/search/rdf) in the same way as
Bio2RDF [15] does. The RDF query language, SPARQL
[16], realizes data exchange between databases in a con-
cise syntax. The key feature of SPARQL is its capability of
joining remote SPARQL endpoints in what is called a fed-
erated SPARQL query [17]. We confirmed the feasibility
of such a query (see Fig. 2a), by demonstrating search and
classification of SNPs in an associated BMRB entry. In the
case study, we successfully collected residues in a BMRB
entry whose sequences correspond to SNPs annotated by
OMIM, as shown in Fig. 2b. The search results (Fig. 2c)
were represented by structural parameters archived in the
BMRB [see also Additional file 1 for methods applied and
all results, chapter 3], allowing users to infer biological
relationships between the phenotype annotated SNPs and
structural features in human proteins (see Fig. 2c). The re-
sults show that the BMRB/RDF offers a promising ap-
proach for integrating biophysical information derived
from biological NMR spectroscopy with other bioinfor-
matics resources of interest in biological and medical
science research.
Additional file
Additional file 1: Further detail of the BMRB/XML, BMRB/RDF and web
services including SPARQL endpoint. Complete results of the SPARQL
query (Fig. 2a) and many other SPARQL query examples are also
available. (PDF 6814 kb)
Abbreviations
BMRB: BioMagResBank; MECP2: methyl-CpG-binding protein 2; NMR: nuclear
magnetic resonance; OMIM: Online Mendelian Inheritance in Man; OWL: web
ontology language; RDF: resource description framework; RDFS: RDF schema;
SASA: solvent accessible surface area; SNP: single nucleotide polymorphism;
SPARQL: SPARQL Protocol and RDF Query Language; XML: eXtensible
Markup Language.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MY performed research, data analysis and wrote the paper with NK. TF was
principal investigator for this project, contributed to research design and feedback
on the manuscript. All authors read and approved the final version of manuscript.
Acknowledgements
This work was supported by National Bioscience Database Center (NBDC) of
Japan Science and Technology (JST); and the United States National Library
of Medicine [LM05799] and National Institute of General Medical Sciences
[GM109046].
Author details
1Institute for Protein Research, Osaka University, 3-2 Yamadaoka, Suita, Osaka
565-0871, Japan. 2Department of Biochemistry, University of Wisconsin-Madison,
Madison, WI 53706, USA. 3Department of Informatics & Telecommunications,
University of Athens, Athens, Greece. 4Department of Computer Sciences,
University of Wisconsin-Madison, Madison, WI 53706, USA.
Received: 11 July 2015 Accepted: 18 March 2016
RESEARCH Open Access
Diagnosis, misdiagnosis, lucky guess,
hearsay, and more: an ontological analysis
William R. Hogan1* and Werner Ceusters2
Abstract
Background: Disease and diagnosis have been the subject of much ontological inquiry. However, the insights
gained therein have not yet been well enough applied to the study, management, and improvement of data
quality in electronic health records (EHR) and administrative systems. Data in these systems suffer from workarounds
clinicians are forced to apply due to limitations in the current state-of-the art in system design which ignore the various
types of entities that diagnoses as information content entities can be and are about. This leads to difficulties in
distinguishing amongst diagnostic assertions misdiagnosis from correct diagnosis, and the former from coincidentally
correct statements about disease.
Methods: We applied recent advances in the ontological understanding of the aboutness relation to the problem
of diagnosis and disease as defined by the Ontology for General Medical Science. We created six scenarios that
we analyzed using the method of Referent Tracking to identify all the entities and their relationships which must
be present for each scenario to hold true. We discovered deficiencies in existing ontological definitions and proposed
revisions of them to account for the improved understanding that resulted from our analysis.
Results: Our key result is that a diagnosis is an information content entity (ICE) whose concretization(s) are
typically about a configuration in which there exists a disease that inheres in an organism and instantiates a
certain type (e.g., hypertension). Misdiagnoses are ICEs whose concretizations succeed in aboutness on the
level of reference for individual entities and types (the organism and the disease), but fail in aboutness on the
level of compound expression (i.e., there is no configuration that corresponds in total with what is asserted).
Provenance of diagnoses as concretizations is critical to distinguishing them from lucky guesses, hearsay, and
justified layperson belief.
Conclusions: Recent improvements in our understanding of aboutness significantly improved our understanding of
the ontology of diagnosis and related information content entities, which in turn opens new perspectives for the
implementation of data capture methods in EHR and other systems to allow diagnostic assertions to be captured with
less ambiguity.
Keywords: Biomedical ontology, Referent tracking, Disease, Diagnosis, Information content entity, Representation,
Ontological realism
Background
As administrative, clinical, and patient-reported data are
increasingly shared and reused, especially for patient
care [14] and research [1, 57], several issues with
these dataincluding diagnosis dataare of increasing
concern. The issue that appears to be of greatest concern
is data error and the implications thereof for making deci-
sions and conclusions based on them [813]. Although
Shapiro et al., in a report for the Office of the National
Coordinator for Health Information Technology, do not
cite error as a concern for including patient-generated
health data into the electronic health record (EHR) [14],
there are known errors with patient self reporting espe-
cially in research [1522]. A second issue of concern is
data provenance [10, 23], i.e. information about who cre-
ated the data, in what setting, how, when, for what
* Correspondence: hoganwr@ufl.edu
1University of Florida, 2004 Mowry Rd, P.O. Box 100219, Gainesville, FL
32610-0219, USA
Full list of author information is available at the end of the article
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 
DOI 10.1186/s13326-016-0098-5
purpose, and so on. For example, Johnson et al. noted that
the provenance of symptom data was essential to using
those data correctly to determine whether a colonoscopy
was a screening vs. diagnostic procedure [23].
Data error and data provenance are closely related.
For example, Hersh et al. note that data recorded in
billing workflows for financial purposes are less accur-
ate than clinical data [10]. Thus, timing, method, and
purpose of recording data at a minimumall aspects of
provenanceare intertwined with accuracy. Further-
more, a key result of the Johnson et al. study is that
Researchers who do not consider data provenance risk
compiling data that are systematically incomplete or
incorrect [23].
An ontological account of data error and data proven-
ance can identify crucial distinctions. For example, there
are significant differences among (1) a measured weight
that is off because the scale was not properly tared, (2) a
rough weight of 70 kg entered in an emergency when
the patient cannot be weighed, and (3) a weight meas-
urement entered on the wrong patient. Detecting and
accounting for these differences and their causesespe-
cially the aspects of provenance that influence themis
necessary to inform strategies to study, cope with, and
improve data error when using pre-existing EHR data
for research.
Additionally, a recent review article on the methods
for assessing quality of EHR data for clinical research
found that: Most of the studies included in this review
presented assessment methodologies that were developed
with a minimal empirical or theoretical basis [24]. It
concluded with a call for moving away from ad hoc ap-
proaches to data quality assessment, to formal, validated
approaches. Although error is only one aspect of data
quality (fitness for purpose and completeness are two
others), a formal ontological understanding of data error
could play a role in more formalized methods for data
quality assessment.
In this work, we apply Smith and Ceusters recent
ontological account of incorrect information (i.e., error)
[25] to diagnosis data in administrative systems, EHRs,
and patient-reported information. Their account holds
that a statement such as a diagnostic assertion can suc-
ceed or fail in aboutness on at least two levels: (1) the
level of denoting single entities and/or types (i.e., the
level of reference) and (2) the level of veridical repre-
sentation of a configuration of multiple entities and/or
types (i.e., the level of compound expression).
To succeed on the second level (compound expression),
the information content entity (ICE) must be correct about
all particulars, their relationships, and their instantiations
of types that it mentions. Failure on a single particular, re-
lation, or instantiation causes the ICE to fail at the second
level while still potentially succeeding at the first level. For
example, if Mrs. Jones has type 1 diabetes mellitus, then
the sentence Mrs. Jones suffers from type 2 diabetes melli-
tus fails in aboutness on the level of compound expression
because it misstates one thing: her disease does not instan-
tiate type 2 diabetes mellitus. However, despite this failure
the sentence is nevertheless still about Mrs. Jones, about
her disease, and about type 2 diabetes mellitus on the level
of reference, because indeed it mentions those three en-
tities. It is therefore, per Smith and Ceusters, an ICE that is
about something even though it is a misdiagnosis.
Prior ontological work on the aboutness of clinical
statements like diagnoses has been constrained by the
view that an ICE is about nothing (or is perhaps not
even an ICE at all) if it fails on the level of compound
expression. Martínez Costa and Schulz, for example, use
the universal quantifier when relating an information
entity to a clinical situation to avoid asserting the exist-
ence of an entity the existence of which cannot be guaran-
teed [26]. For an ICE such as suspected heart failure they
want to avoid the implication that there is some instance
of heart failure that it is about. Because they cannot guar-
antee the existence of some heart failure, they use universal
quantification to say if it is about anything, then it is about
an instance of heart failure. Researchers working in areas
other than diagnosis have encountered similar issues. For
example, Hastings et al. note that chemical graphs and dia-
grams are not always about types of molecules that exist
[27]. They, too, used the workaround of replacing existen-
tial quantification with universal quantification to avoid
asserting that every chemical graph/diagram is about some
type of molecule that exists (level of compound expres-
sion), while still allowing such graphs and diagrams to be
subtypes of information content entity.
In our own, previous ontological analysis of diagnosis,
using the methodology of referent tracking, we identified
what entities must exist or must have existed for a par-
ticular diagnostic statement to hold true [28, 29]. A key
result of this work is that a diagnosis is minimally about
both the patient and the type of disease that is asserted
to exist. In addition, building on previous work on the
Ontology for General Medical Science (OGMS), the
foundations of which were laid down in Scheuermann
et al. [30], we noted that for a diagnosis to exist (at
least in medicine and under the assumption that the
diagnosis was made lege artis), there must also have
existed a diagnostic process, a person who carried out
that process, and a clinical picture which was used as
input into that process.
The hypothesis for the work described here was that
applying Smith and Ceusters results to disease and diagno-
sis, in combination with prior work on the ontology of dis-
ease and diagnosis (including provenance of the latter),
could address limitations encountered in previous onto-
logical work on disease and diagnosis and improve our
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 2 of 15
representations of them in support of studying, coping
with, and reducing ambiguity in the generation of diagnos-
tic statements and error in the interpretation thereof.
Methods
To test this hypothesis, we analyzed a set of scenarios
that we created and that involve correct and incorrect
diagnoses, lucky guesses, and justified layperson belief in
the existence of a disease of a certain type. The goal was
to explore whether, and if so how, a realism-based ac-
count of information can deal successfully not only with
diagnostic statements asserting the ideal case of a cor-
rect diagnosis, but also with deviations from the ideal.
Materials
In our analysis we used as input (1) Smith and Ceusters
work on aboutness and their definitions of representation,
mental quality, cognitive representation, and information
quality entity (Table 1), (2) definitions of disease, disorder,
and diagnosis from the Ontology for General Medical
Science (Table 2), and (3) our prior work on analysis of
diagnostic statements [27, 28].
Smith and Ceusters stressed that the relation of
aboutness includes any portion of reality, rather than
being limited to just a single particular or a single uni-
versal. A portion of reality (POR) can be a particular, a
universal, a relation, or a configuration. A configuration
is a combination of particulars and/or universals and
certain relation(s) that hold among them.
A representation, then, that is intended to be about a
POR but fails in its aboutness because it misrepresents
that POR in some way, is misinformation. The sentence
Bob Dylan was in the Beatles fails to represent not
because Bob Dylan or the Beatles did not exist, but
because such a configuration involving Bob Dylan and
the Beatles in the way as expressed, never existed. The
sentence fails in aboutness on the level of compound
expression, but nevertheless is about Bob Dylan and the
Beatles individually (on the level of reference) and thus
is still an information content entity.
Smith and Ceusters [25] deal more fully with the issue
of what it means that a representation is intended to be
about some entity. Here, we highlight that it follows the
doctrine of the primacy of the intentional [31], where
our written and verbal expressions are to be understood
on the basis of the cognitive acts that generated them.
That is, a sentence is about that to which its author was
directing his or her thoughts when she wrote it.
In addition to Smith and Ceusters work, we also founded
our ontological analysis on the Ontology for General
Medical Science or OGMS [30]. This work distinguishes
disease, disorder, and diagnosis, and we used definitions
from OGMS as starting points for our analysis (Table 2).
Note that in OGMS, a diagnosis refers to the existence of a
disease of a given type. In clinical medicine, however, diag-
noses also refer to (1) disease courses (e.g., acute hepatitis
vs. chronic hepatitis), (2) disorders (e.g., fractures and tu-
mors), and (3) the absence of any disease (i.e., a conclusion
that a person is healthy also is a diagnosis). It was not our
goal to address this issue in this work, as it was not our goal
to refine the OGMS definition of diagnosis.
The scenarios
All the scenarios have in common a particular patient,
Mr. Adam Jones, who suffers from type 2 diabetes mellitus.
Thus in every scenario, there exists Mr. Jones, his disease,
the type Type 2 diabetes mellitus, the configuration of these
three entities (which includes the bearer of and instance
of relationships), and the placement in space and time of
this configuration (Fig. 1).
Table 1 Definitions based on Smith and Ceusters [25]
Term Definition
INFORMATION CONTENT ENTITY An ENTITY which is (1) GENERICALLY DEPENDENT on (2) some MATERIAL ENTITY and which is
(3) concretized by a QUALITY (a) inhering in the MATERIAL ENTITY and (b) that is_about some
PORTION OF REALITY
INFORMATION QUALITY ENTITY A REPRESENTATION that is the concretization of some INFORMATION CONTENT ENTITY
REPRESENTATION A QUALITY which is_about or is intended to be about a PORTION OF REALITY
MENTAL QUALITY A QUALITY which specifically depends on an ANATOMICAL STRUCTURE in the cognitive system
of an organism
COGNITIVE REPRESENTATION A REPRESENTATION which is a MENTAL QUALITY
Relation Explanation
x is_about y x refers to or is cognitively directed towards y. Domain: representations; Range: portions of reality
x concretizes y x is a QUALITY and y is a GENERICALLY DEPENDENT CONTINUANT (GDC) and for some MATERIAL ENTITY z,
x specifically_depends_on z at t and y generically_depends_on z at t, and if y migrates from bearer z to another
bearer w then a copy of x will be created in w.
x is_conformant_to y =def. x is an INFORMATION QUALITY ENTITY and y is a COGNITIVE REPRESENTATION and there is some
GDC g such that x concretizes g and y concretizes g.
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 3 of 15
Scenario 1: correct diagnosis by physician (ideal case)
Dr. Anne Smith sees Mr. Jones in the office. She takes a
history and physical, performs certain laboratory testing,
and based on her analysis of the findings, correctly con-
cludes that Mr. Jones has type 2 diabetes mellitus. She
subsequently writes her diagnosis in the patients med-
ical record.
Scenario 2: subsequent correct diagnosis by physician using
first diagnosis
A second doctor, Dr. John Brown, sees Mr. Jones in
the office at some later date. Mr. Jones has released
his records from Dr. Smith to Dr. Brown, who subse-
quently sees Dr. Smiths diagnosis prior to seeing Mr.
Jones. He uses that diagnosis plus his own findings to
infer a new clinical picture of Mr. Jones, which he subse-
quently uses to make another correct diagnosis of Mr.
Jones disease. He writes his diagnosis in Mr. Jones med-
ical record.
Scenario 3: incorrect diagnosis by physician
Mr. Jones is traveling on vacation, when he falls ill.
He sees Dr. Jane Miller who does not have any of his
past records available, and thus she is not aware of
the previous diagnoses of Drs. Smith or Brown. She
infers a new clinical picture of Mr. Jones, and based
on it incorrectly concludes that Mr. Jones has type 1
diabetes mellitus (as opposed to type 2). She records
a diagnosis of type 1 diabetes mellitus in her medical
record for for Mr. Jones.
Scenario #4: coincidentally correct conclusion by layperson
(lucky guess)
A friend of Mr. Jones is a seer. Mr. Jones asks his friend
what is in his future. Having no prior knowledge of Mr.
Jones medical conditions, the seer concludes based on
Mr. Jones horoscope and the position of the moon that
he has type 2 diabetes mellitus. He subsequently predicts
that Mr. Jones will be hospitalized for his diabetes and
miss his daughters wedding.
Scenario #5: laypersons justifiable conclusion
Mr. Jones daughter, upon learning of her fathers type 2
diabetes mellitus, adds this information into her letter to
her brother, writing Dad has type 2 diabetes mellitus.
Table 2 Key definitions from OGMS used in the analysis
Term Definition
DISEASE A DISPOSITION (i) to undergo PATHOLOGICAL PROCESSes that (ii) exists in an ORGANISM because of one or
more DISORDERs in that ORGANISM.
DISORDER A causally relatively isolated combination of physical components that is (a) clinically abnormal and (b) maximal,
in the sense that it is not a part of some larger such combination.
DIAGNOSIS A conclusion of an interpretive PROCESS that has as input a CLINICAL PICTURE of a given patient and as output
an assertion (diagnostic statement) to the effect that the patient has a DISEASE of such and such a type.
DIAGNOSTIC PROCESS An interpretive PROCESS that has as input a CLINICAL PICTURE of a given patient and as output an assertion to
the effect that the patient has a DISEASE of a certain type.
PATHOLOGICAL PROCESS A bodily PROCESS that is a manifestation of a DISORDER.
PHENOTYPE A bodily feature or combination of bodily features of an organism determined by the interaction of the genetic
make-up of the organism and its environment.
CLINICAL PHENOTYPE A clinically abnormal PHENOTYPE.
CLINICAL PICTURE A representation of a CLINICAL PHENOTYPE that is inferred from the combination of laboratory, image and clinical
findings about a given patient.
CLINICAL FINDING A REPRESENTATION that is either the output of a clinical history taking or a physical examination or an image
finding, or some combination thereof.
MANIFESTATION OF DISEASE A QUALITY of a patient that is (a) a deviation from clinical normality that exists in virtue of the realization of a
disease and (b) is observable.
CLINICAL HISTORY TAKING An interview in which a clinician elicits a clinical history from a patient or from a third party who is authorized
to make health care decisions on behalf of the patient.
CLINICAL HISTORY A series of statements representing health-relevant features of a patient.
Fig. 1 The configuration of Mr. Jones, his disease, and type 2
diabetes mellitus
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 4 of 15
Scenario #6: correct diagnosis by computer-based expert
system
A medical student is seeing Mr. Jones in the clinic. He
performs a history and physical, and types his findings
into a diagnostic expert system. The diagnostic expert
system infers based on these findings that Mr. Jones has
type 2 diabetes mellitus. The medical student writes this
diagnosis in Mr. Jones medical record.
The analysis
Our analysis follows the method of Referent Tracking,
which we have found to be a stringent test of ontologies
and their definitions [28]. This approach proceeds in
three main steps. First, we systematically identify all the
relevant particulars that must exist for the scenario to be
true, regardless of whether the scenario explicitly men-
tions them or only implies their existence. We assign each
particular an instance unique identifier (IUI), of the form
IUI-n, where n is any integer. Second, we identify for
each particular the type it instantiates and the temporal
interval during which it exists (and assign an identifier of
the form tn to that interval). Lastly, we identify the rela-
tionships that hold between the particulars as well as all
relevant relations particulars have to universals other than
instantiation, including situations where a particular lacks
a given relation to any instance of a certain type (for ex-
ample, a statement that a patient has had no cough in the
last two weeks means that the patient does not stand in
the agent_of relation to any instance of the type Coughing
event, indexed temporally to the two-week interval) [32].
This approach identifies problems in ontologies and their
definitions in two major ways. First, it identifies problems
that occur when the scenario explicitly rules out the exist-
ence of a particular whose existence is implied by an onto-
logical definition (and vice versa). Second, it helps identify
exceptions to existing definitions and situations that should
not fall under a definition but are erroneously captured by
it. Definitions in ontologies can subsequently be adjusted to
avoid the errors so identified.
Although our approach is to identify particulars im-
plied by sentences in natural language, the ontological
analysis of language and the mechanism(s) by which it
makes implicit reference to certain entities is not the
focus of this work. Therefore, we convert a sentence like
Mr. Jones has type 2 diabetes mellitus to Referent
Tracking Tuples (e.g., as in Tables 3, 4, 5, 6 and 7) and it
is these tuples in which inhere representations that are
the objects of our analysis.
To simplify our analysis somewhat, we wrote scenarios
under which humans record diagnoses on paper. However,
concretization of ICEs also occurs by pixels on monitors,
binary switches in memory and processor chips, and mag-
netic fields on hard disks. But a detailed account of these
concretizations and transformations among them is not
central to our analysis of what is a diagnosis. Our analysis
can be extended to these concretizations without modifica-
tion of the method.
Results and discussion
In each scenario, Mr. Jones (IUI-1) and his disease (IUI-2)
exist, the latter inhering in the former (Table 3). Further-
more, his disease is an instance of the type type 2 diabetes
mellitus at any moment in time during which a diagnosis
is formulated in any of the scenarios. Mr. Jones (IUI-1) ex-
ists through a certain period of time (t1) of which we do
not know the exact beginning or end. We use temporal
identifiers of the form tn to clearly distinguish such iden-
tifiers from IUIs: where IUIs are always intended to be glo-
bally and singularly unique, distinct temporal identifiers
may denote a unique period of time which is also denoted
by another temporal identifier. We also assign an identifier
to the time interval during which his disease (IUI-2) exists
(t2). Diseases usually begin to exist after the organism
does, but in the case of congenital genetic diseases, the
two intervals might be coextensive. Also, we assume that
disease IUI-2 existed at the time of diagnosing, but we
recognize that diagnosing a disease thousands of years
after it existed is possible, such as in the case of archaeolo-
gists recent diagnosis of Tutankhamuns malaria [33].
Note that the configuration of organism, disease, and
disease type is anchored at a particular location in space-
time, as is the diagnosis. But note also that the diagnosis
additionally has an implicit or explicit reference to the
location of the configuration in spacetime. To be a correct
diagnosis, this reference must also be correct (it has to
refer to some part, not necessarily the entirety of space-
time, occupied by the configuration). Thus, for example,
to say that Tutankhamun had malaria in 1000 C.E. or
today is incorrect, as it would be to say that Mr. Jones had
type 2 diabetes mellitus before his parents were born.
Scenario 1: correct diagnosis
In this scenario, numerous PORs in addition to Mr. Jones
and his disease must exist and stand in certain relation-
ships to each other (Tables 4, 5 and 6). Before Dr. Smith
(IUI-3) writes (IUI-13) her diagnosis (IUI-8), there is a
cognitive representation (IUI-6) that is concretized in
some anatomical part (IUI-5) of her cognitive system
(IUI-4). Note that we follow Ceusters and Smith [34] in
asserting that all anatomical entities in which cognitive
representations inhere are part of a persons cognitive sys-
tem (that is, any entity used in cognition, including the
bearing of cognitive representations, are necessarily within
a persons cognitive system) at least during the temporal
interval that the cognitive representation exists. If, for
example, it would be the case that some white blood cell
flowing through some brain capillary would through some
of its molecules take part in the concretization of a
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 5 of 15
cognitive representation, then that white blood cell would
be part of the cognitive system at least during the exist-
ence of that concretization. It would not anymore be part
of the cognitive system once it continues its journey
through the body without participating in thought forma-
tion. Additionally, Ceusters and Smith take the position
(which we also follow) that the cognitive system is not ne-
cessarily strictly limited to the brain or even to the entire
neurological system of a person: the current state-of-the-
art of neuroscience is yet searching for answers to ques-
tions such as what is it in which cognitive representations
inhere? but until it reaches such answers, we remain in
our representations agnostic.
IUI-9 denotes the sentence Dr. Smith wrote, as it ex-
ists on the particular piece of paper she used to write it
on: The patient has type 2 diabetes mellitus. This writ-
ten statement on paper (IUI-9) bears an information
quality entity (IQE, IUI-10) that concretizes her diagno-
sis (IUI-8). The cognitive representation (IUI-6) and IQE
(IUI-10) that concretize the diagnosis are both about the
configuration (IUI-7) (the level of compound expres-
sion), as well as about Mr. Jones, Mr. Jones disease, and
the universal Type 2 diabetes mellitus individually (the
level of reference). The cognitive representation (IUI-6)
and the diagnosis (IUI-8) are the output of Dr. Smiths
diagnostic process (IUI-11), which had as input Dr.
Smiths clinical picture (IUI-12) of Mr. Jones. Because
the cognitive representation and IQE concretize the
same ICE, the latter is conformant to the former (see
Table 1).
A correct diagnosis is thus fundamentally an information
content entity that is concretized by a representation that
stands in an is_about relation to the configuration of an or-
ganism, its disease, the relation of inherence between the
disease and the organism, a type that the disease instanti-
ates, and the instantiation relation of the disease to that
type, all within a given portion of spacetime (Fig. 2). Fur-
thermore, diagnoses are additionally differentiated from
Table 4 The entities in Scenario 1
IUI Entity Existence period Type Notes
IUI-3 Dr. Anne Smith t3 Human being
IUI-4 Cognitive system of IUI-3 t4
IUI-5 An anatomical entity that is
part of IUI-4
t5 Anatomical entity Which anatomical entity and its lifetime cannot be easily
specified given current state of neuroscience.
IUI-6 Quality that inheres in IUI-5
and is about IUI-7
t6 Cognitive representation
IUI-7 The POR that is truth-maker
for IUI-8
t7 Configuration Mr. Jones, his disease, their relationship, and diseases instantiation
IUI-8 Dr. Smiths diagnosis t8 Diagnosis ICE concretized by IUI-6 and IUI-10
IUI-9 That which is written down on
paper and forms the sentence.
t9 Material entity I conclude therefore that Mr. Jones has type 2 diabetes mellitus.
IUI-10 IQE that inheres in IUI-9. t10 Information quality entity The sentence began to exist as soon as ink was laid down
on paper, but the IQE did not begin to exist until the
sentence was finished.
IUI-11 Dr. Smiths interpretive process occupies t11 Diagnostic process Dr. Smiths diagnostic process that led to her diagnosis IUI-8
IUI-12 The clinical picture input into IUI-11 t12 Clinical picture Dr. Smiths clinical picture as ascertained prior to t6
IUI-13 Dr. Smith writing her diagnosis
in the note
occupies t13 Process
Table 3 Referent tracking tuples true in every scenario
IUI Entity Existence period Type Notes
IUI-1 Mr. Adam Jones t1  the period during
which IUI-1 exists
Material Entity
IUI-2 IUI-1s disease t2 Disposition
Relationships among particulars
IUI-2 inheres in IUI-1 at t2
IUI-2 instance of UUI-1 at t2 UUI-1 is a universal unique identifier that denotes type 2 diabetes mellitus.
We assume that if something is at any time of its existence an instance of
type 2 DM, it is instance of type 2 DM at all times it exists.
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 6 of 15
other ICEs by the fact that they are generated by a diag-
nostic process that has a clinical picture as input. We
expand further on what constitutes a clinical picture in
the next scenario, Scenario 2, as well as revisit the diag-
nostic process briefly in Scenario 4, although it was not
our objective in this work to develop a fuller account of
this process.
Note that it is trivial to state that the particular disease
inhering in the organism is an instance of entity or even
disease. Thus, there is an expectation that a diagnosis be
Table 5 Additional temporal entities in Scenario 1
Temporal identifier Description Notes
t14 The interval during which the anatomical entity (IUI-5)
is part of the cognitive system (IUI-4)
This interval is not easily specified given the current state of
neuroscience. It could be different than t3 and t4.
t15 The interval during which the clinical picture (IUI-12) is
used in the interpretive process (IUI-11)
Could be shorter than t11
t16 The point in time at which the cognitive representation
(IUI-6) and diagnosis (IUI-8) begin to exist
t16 ends t11. Because the ICE does not exist until the cognitive
representationits first concretizationexists, this is also the
point in time at which the diagnosis begins to exist.
t17 The interval during which the cognitive representation
(IUI-6) participates in the writing process (IUI-13)
t18 The interval during which the diagnosis (IUI-8) participates
in the writing process (IUI-13)
It is possible that the original cognitive representation (IUI-6) gets
copied elsewhere in the brain for reasoning and thus that the ICE
continues to participate after the initial cognitive representation
t19 The interval during which that which is written on paper
(IUI-10) begins to exist until it exists in full
The writing process begins earlier than the time at which the
sentence begins to exist: the author starts the process with getting
a pen and paper, any preparation necessary (clicking the pen), etc.
Table 6 Relationships among particulars in Scenario 1
IUI Relation IUI When relation holds in reality Notes
IUI-4 part of IUI-3 at t4
IUI-5 part of IUI-4 at t14 All anatomical components in which the cognitive representation
inheres are part of the cognitive system. We do not assume the
cognitive system is limited to the brain, as the state of neuroscience
does not permit such an assumption.
IUI-6 inheres in IUI-5 at t6
IUI-6 is about IUI-7 at t6 The cognitive representation stands in aboutness to IUI-7 as long as
it exists
IUI-6 is about IUI-1 at t6 It is also about Mr. Jones
IUI-6 is about IUI-2 at t6 And about Mr. Jones disease
IUI-6 is about UUI-1 at t6 And about Type 2 diabetes mellitus
IUI-6 concretizes IUI-8 at t6 It also concretizes the diagnosis
IUI-10 inheres in IUI-9 at t9 The IQE inheres in the sentence on paper
IUI-10 is about IUI-7 at t10 The IQE stands in aboutness to IUI-7
IUI-10 is about IUI-1 at t10 It is also about Mr. Jones
IUI-10 is about IUI-2 at t10 And about Mr. Jones disease
IUI-10 is about UUI-1 at t10 And about Type 2 diabetes mellitus
IUI-10 concretizes IUI-8 at t10
IUI-10 is conformant to IUI-6 at t10 Is conformant to the cognitive representation as long as it exists
IUI-3 agent in IUI-11 at t11
IUI-12 input into IUI-11 at t15 Clinical picture input into IUI-11
IUI-6 output of IUI-11 at t16 Cognitive representation output from IUI-11
IUI-8 output of IUI-11 at t16 Both the diagnosis and its concretization are outputs of IUI-11
IUI-8 input into IUI-13 at t17 The diagnosis is input into writing
IUI-6 input into IUI-13 at t18 As is its cognitive representation
IUI-10 output of IUI-13 at t19 The sentence is output of writing
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 7 of 15
as precise (the most specific type) as possible and at a
minimal level of granularity that is relevant to treat the pa-
tient appropriately and to provide a reasonable prognosis.
Scenario 2: second diagnosis
The second physician, Dr. Brown, makes a second diag-
nosis at a later point in time, using the first diagnosis
in addition to clinical and possibly other findings to
infer a new clinical picture of Mr. Jones. With the ex-
ception of the configuration of Mr. Jones/his disease/
type 2 diabetes mellitus (IUI-7), there is a one-to-one
correspondence of PORs as in Scenario 1, numbered
IUI-23 through IUI-33 (Additional file 1: Tables S1-S3).
That is, there is no IUI-27 because the configuration is the
same POR across scenarios. Similarly, there is no IUI-21
or IUI-22 because IUI-1 and IUI-2 already identify Mr.
Jones and his disease, respectively, uniquely.
In this scenario, Dr. Brown (IUI-23) makes a new diag-
nosis (IUI-28), concretized both by his cognitive repre-
sentation (IUI-26) in some part (IUI-25) of his cognitive
system (IUI-24) and by the IQE (IUI-30) inhering in the
sentence in his note (IUI-29). Dr. Smiths previous diag-
nosis (IUI-8) can be viewed as either (view1) being in
the aggregate of things that Dr. Brown uses to infer his
clinical picture (IUI-32) that serves as input into his
diagnostic process (IUI-31), or (view2) as something
which serves as extra inputalongside his clinical pic-
turefor the diagnostic process. The cognitive represen-
tation and the IQE are about the configuration (IUI-7)
as well as Mr. Jones (IUI-1), his disease (IUI-2), and type
2 diabetes mellitus (UUI-1).
The current definition of clinical picture in OGMS
(see Table 2) seems to conflict with view1 about this sce-
nario, because the definition seems to exclude using a
Table 7 Relationships of representations to portions of reality in Scenario 3: Incorrect diagnosis
Relationships among particulars Notes
IUI-46 is about IUI-1 at t46 Dr. Jane Millers cognitive representation is about Mr. Jones
IUI-46 is about IUI-2 at t46 And Mr. Jones disease
IUI-46 is about UUI-2 at t46 And Type 1 diabetes mellitus (denoted by UUI-2)
IUI-50 is about IUI-1 at t50 Likewise with the IQE inhering in the ink on paper
IUI-50 is about IUI-2 at t50
IUI-50 is about UUI-2 at t50
IUI-46 is misrepresentation of IUI-7 at t46 But the cognitive representation is a misrepresentation of the configuration,
i.e., it is intended to be about the configuration but fails on the level of
compound expression
IUI-50 is misrepresentation of IUI-7 at t50 The same is true of the IQE
Fig. 2 Diagram of diagnostic process, its inputs, a correct diagnosis, its concretization, and the configuration that that the concretization is about
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 8 of 15
past diagnosis to infer a clinical picture. Although the
current OGMS definition of clinical picture is inclusive
of clinical findings, diagnosis as currently defined is not
an explicit subtype of clinical finding in OGMS. Further-
more, it is common for clinicians to elicit a previous
providers past diagnosis from the patient or the patients
caregiver during an interview (for example, if Mr. Jones
in scenario #2 would have said: Dr. Smith says I have
type 2 diabetes mellitus). But the current OGMS defin-
ition of clinical history (Table 2) conflicts with this possi-
bility. It refers to health-relevant features of a patient, but
features as elucidated by OGMS include only qualities,
processes, and physical components of the organismnot
dispositions of which disease is a subtype. Therefore, a
representation of a disease such as a diagnosis is currently
excluded from the OGMS definition of clinical history.
We also note that the OGMS definition of clinical pic-
ture is ambiguous in that it is not clear whether it requires
that laboratory and image findings must always be used to
infer a clinical picture, or that they are the only entities that
can be used. Regardless, it would be a mistake to do so,
because diagnoses can and frequently are made from symp-
tom findings alone. Laboratory and image findings are not
necessary components of a clinical picture in reality. Note
that a clinical picture can comprise findings of a single
type (laboratory alone, pathology image alone, radiology
image alone, physical exam finding alone), or even a single
finding instance (e.g. Reed-Sternberg cells for a diagnosis of
Hodgkins lymphoma). All these issues are compounded by
the fact that the term clinical picture itself is not intuitive.
Given that clinical history taking elicits past diagnoses
routinely in clinical medicine, we propose modifying the
definition of clinical history to accommodate this reality
(bolded sections represent changes to the definition):
clinical history = def.  A series of statements representing
one or more health-relevant features of a patient,
possibly complemented by representations of diseases
and configurations.
Note that the definition already allowsunder the
broader heading of featurerepresentations of disorders
(kinds of physical component) and disease courses (kinds
of process). Thus, the definition already accommodates
these aspects of clinical histories. We also allow the state-
ments to represent configurations, in line with Smith and
Ceusters [2]. These configurations might or might not in-
clude various relevant types (for example, The patient
has not participated in any instance of vomiting in the last
two weeks.). Finally, note that by using the word repre-
senting, the definition also accommodates per Smith and
Ceusters [2] that some statements might fail in aboutness
despite their intention to be about such features. In other
words, some statements in the clinical picture might be
wrong: for example, a statement that the patient has a dis-
ease or pain that she does not in fact have.
To clarify that laboratory and imaging findings are not
always required inputs into the diagnostic process, and
to capture realistic scenarios compatible with view2 (for
example, Dr. Brown reads Dr. Smiths note in the chart),
we also propose a modified definition of clinical picture
(changes in bold):
clinical picture = def.  A representation of a clinical
phenotype that is inferred from a combination of, for
example, diagnoses and laboratory, image, and clinical
findings about a given patient.
These changes to the definitions of clinical history and
clinical picture now properly capture situations where
past diagnoses are elicited from the patient and/or her
caregiver during a clinical history taking: these diagnoses
are now clinical findings in the clinical history that was
generated by the clinical history taking (see the definition
of clinical finding in Table 2).
Scenario 3: Misdiagnosis
The third physician, Dr. Miller, misdiagnoses Mr. Jones
type 2 diabetes mellitus as type 1 diabetes mellitus (Fig. 3).
Per Smith and Ceusters, because the misdiagnosis is still
about Mr. Jones, his disease, the relationship between
them, and the type type 1 diabetes mellitus on the level
of reference, it is an information content entity. However,
it fails to be about the configuration IUI-7 as a whole on
the level of compound expression.
Again, in this scenario there exist PORs in one-to-one
correspondence (except the configuration and its compo-
nents) numbered IUI-43 through IUI-53 (Additional file 2:
Tables S4-S6). Dr. Miller (IUI-43) writes (IUI-53) his mis-
diagnosis (IUI-48) in Mr. Jones chart, and the IQE (IUI-50)
inhering in the ink (IUI-49) is conformant to his cognitive
representation (IUI-46), and both are abouton the level
of referenceMr. Jones, his disease, and type 1 diabetes
mellitus. But neither one is about the configuration (IUI-7).
To capture the relation both (1) between the cognitive rep-
resentation and the configuration and (2) between the IQE
and the configuration, we define a new relation:
is-misrepresentation-of: domain: representation, range:
portion of reality.
Def: x is-misrepresentation of y iif x is a representation
and x is intended to be about y and it is not the case
that x is about y.
Then we assert that the representations (IUI-46 and
IUI-50) are misrepresentations of the configuration
(Table 7 and Additional file 2: Table S6). Note that our
definition precludes the cognitive representation (IUI-46)
and IQE (IUI-50) being about any configuration other
than IUI-7, because they are not intended to be about, for
example, the configuration of the sun, earth, and moon at
a particular date and time.
Note that asserting the incorrect disease type is not
the only way to make a misdiagnosis. There are at least
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 9 of 15
six possibilities where a diagnosis fails to be about a con-
figuration on the level of compound expression (Table 8).
If a representation fails on the level of reference, it also
fails on the level of compound expressions, because a
configuration cannot consist of that which does not exist.
These six possibilities could also exist in combination, but
if the 2nd, 3rd, and 4th possibilities are all present (for ex-
ample, Ron Weasley has spattergroit), then there is not a
diagnosis, or even any information content entity at all,
because the representation is not about anything even on
the level of reference. Of course, if the organism itself does
not exist, then there cannot be a clinical picture inferred,
and thus it would not be a diagnosis or misdiagnosis,
although it could still be an ICE if it is about a really-existing
disease type (for example, James Bond has influenza).
Also, as medical knowledge evolves, the profession comes
to understand that certain types of disease thought to exist
in fact do not. Thus past diagnoses of dropsy and con-
sumption we now understand to be misdiagnoses.
Despite searching the extensive literature on diagnostic
error, we could not find any studies that looked at what
percentages of misdiagnoses fall into these categories. We
conjecture based on our past clinical expertise and experi-
ence that asserting the incorrect disease type is the most
common mistake among those in Table 8, but confirm-
ation or rejection of this conjecture requires study.
Scenario 4: the lucky guess
In this scenario, a layperson (the seerIUI 63) correctly
concluded coincidentally that Mr. Jones had type 2 diabetes
Fig. 3 Misdiagnosis of type of disease. The diagnosis is individually about the patient, the disease, and the incorrectly diagnosed disease type Y,
but it is not about the configuration of patient, disease, and disease type X
Table 8 Six possibilities for a diagnosis failing in aboutness on the level of compound expressions
Problem Where it fails first Description
Noninstantation, asserted type exists Level of compound expression Disease instantiates a different type than the stated type,
but the stated type exists
Noninstantation, asserted type does not exist Level of reference Disease instantiates a different type than stated, while the
stated type of disease does not exist
Disease nonexistence Level of reference The disease instance does not exist
Organism nonexistence Level of reference The organism instance does not exist. In this case, there
could not be a clinical picture properly inferred and thus
it is not a misdiagnosis although it could still be an ICE.
Disease non-inherence Level of compound expression The disease inheres in a different organism than the one
stated. For example, the doctor mistakenly ascribes
Mr. Johnsons hypertension to his twin.
Configuration is not located in that part of
spacetime where the diagnosis says it is located.
Level of compound expression A diagnosis of type 2 diabetes mellitus 5 years ago is wrong
because the patient didnt have the disease at that time,
even though the patient has type 2 diabetes today. Also,
a diagnosis that the patient has an upper respiratory tract
infection today when in reality the infection resolved
two weeks ago.
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 10 of 15
mellitus based on the position of the moon and Mr. Jones
horoscope (Additional file 3: Tables S7-S9). It would be
wrong to say the seers reasoning (IUI-71) constituted a
diagnostic process. To avoid coincidentally correct state-
ments from qualifying as diagnoses, we additionally require
as input into the diagnostic process cognitive representa-
tions of the disease type and the types instantiated by the
sequalae, signs, symptoms, and any clinical, laboratory, or
imaging findings or phenotypes of the instances of this
disease type. Note that this is a minimal requirement:
clinicians often additionally include in their diagnostic
reasoning cognitive representations of other disease
types and associated PORs when considering alternative
possibilities for the disease type.
This view is based on the extensive literature on clin-
ical reasoning processes, especially diagnosis (for a re-
view, see Norman [35]). This research has established
the use of representations, called knowledge structures,
in the diagnostic process. The nature and form of these
representations evolves as clinical expertise develops
[36], and we note that the differences in diagnostic pro-
cesses that result could result in a typology of diagnostic
processes in OGMS.
Because the seer had no cognitive representations of
type 2 diabetes mellitus, let alone used them as input
into his reasoning, his conclusion (IUI-68), although
an ICE, is not a diagnosis. Similarly, if a physician makes
a lucky guess based not on his cognitive representations
of the stated disease type but instead by flipping a coin
or some such, that too would not be a diagnosis.
To Table 3 we add an aggregate of cognitive represen-
tations of disease types and associated entities as input
into the diagnostic process (Table 9).
We propose to redefine diagnostic process as follows:
Diagnostic process = def. An interpretive PROCESS
that has as input (1) a CLINICAL PICTURE of a given
patient AND (2) an aggregate of REPRESENTATIONs
of at least one type of disease and at least one type of
phenotype whose instances are associated with instances
of that disease, and as output an assertion to the effect
that the patient has a DISEASE of a certain type.
Scenario 5: laypersons justifiable conclusion
Mr. Jones daughter wrote a sentence in her letter to her
brother based on reading Dr. Smiths progress note say-
ing that that her father has type 2 diabetes mellitus
(Additional file 4: Tables S10-S12). Of course, the daugh-
ter has not made a diagnosis. She is communicating to
her brother what she believes to be the case.
Had she merely written Dr. Smith says and then cop-
ied Dr. Smiths sentence word for word into her letter,
then her writing would concretize Dr. Smiths diagnosis
(IUI-8). This is the case of hearsay (so-and-so said it
was the case that).
As Smith and Ceusters showed, however, the same sen-
tence written by two different people does not guarantee
they concretize the same ICE. ICEs are further differenti-
ated by the provenance of their concretizations, including
who created them and when, and to what POR they intend
to be about. In their example, two people writing the sen-
tence Barack Obama has never been President of the United
Statesone before and one after Obamas inauguration as
Presidentgenerate two different ICEs. The one written
after fails on the level of compound expressions but not on
the level of reference, whereas the one written before suc-
ceeds on both levels (it remains true that at the time when
the sentence was written, he had never been President).
We therefore distinguish between a human (1) merely
copying a representation, in which case the copy con-
cretizes the same ICE as the original text and (2) creat-
ing her own cognitive representation of the PORwhich
involves forming a belief that the POR really existed as
representedand then subsequently creating an IQE
that is conformant to the cognitive representation. In
the former case, a new ICE does not come into being. It
does not even require in the cognitive system of the
copier any representation of the POR that the original
representation is about (as in the case of copying German
text that one does not understand at all). In the latter case,
by contrast, a new ICE does come into being.
In Scenario 5, the daughter did not merely repeat Dr.
Smiths diagnosis. She communicated to her brother her
belief about her fathers disease. She deliberately chose not
Table 9 Additional tuples required to distinguish diagnosing from a lucky guess
IUI Entity Lifetime Type Notes
IUI-14 The aggregate of Dr. Smiths cognitive
representations of various disease types
and their associated types of phenotypes
including type 2 diabetes mellitus that he
used in the diagnostic process
t20 Aggregate of cognitive
representations
Relationships among particulars
IUI-14 input into IUI-11 at t21 t21 refers to the temporal interval during
which IUI-14 participated in the reasoning
process. It could start at the same time as
t11 or after t11, and end at the same time
as or before t11.
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 11 of 15
to merely convey Dr. Smiths diagnosis, but rather her be-
lief that her father has type 2 diabetes mellitus. She heard
the opinion of an expert, in whom she had trust. Based on
(1) her observations of her father, (2) Dr. Smiths diagno-
sis, and (3) her trust in Dr. Smith, she reached the conclu-
sion herself that her father suffers from type 2 diabetes
mellitus. Because she did not begin with a clinical picture
and her own cognitive representations of type 2 diabetes
mellitus, her conclusion is not a diagnosis.
However, consider the scenario where she is given the
clinical picture and has enough knowledge to arrive at a
conclusion, which could be the case either if she were a
physician or somehow other acquired or were given the
necessary knowledge: it is analagous to Scenario #6,
where she takes the place of the expert system (see ana-
lysis of that scenario below). Thus, here in Scenario #5 it
is important to note that she did not reason from a clin-
ical picture to the diagnosis.
In this scenario, therefore, the daughter has created a
new ICE (IUI-88) that is not a diagnosis. She has con-
cretized it in the sentence (IUI-89) in her letter.
Scenario 6: diagnosis by non-human
The diagnostic decision support system has made a diag-
nosis (or misdiagnosis depending on whether it is correct),
because it (1) takes as input a clinical picture and repre-
sentations of the relevant disease type and one or more
types of phenotypes with which it is associated; (2) partici-
pates in a process of making a conclusion based on this
input; and (3) outputs from this process a statement about
a configuration involving an organism, a disease, and a
disease type.
In this case, there are no cognitive representations. In
their place are digital representations on hard drives,
memory chips, and central processing units. If we as-
sume the system generates a sentence and prints it on
paper, then we have an analagous IQE to the written
diagnosis of the physician and ICE of the sister.
Nothing in our proposed definitions conflicts with this
scenario. Replacing Dr. Smith and associated representa-
tions and diagnostic process with various components of
the computer and its digital representations as well as
inferential process (which is an instance of diagnostic
process) is straightforward.
Returning briefly to a point made in Scenario #5, Mr.
Jones daughter could follow the exact same algorithm(s)
of the diagnostic expert system using the exact same
clinical picture as input, and she would arrive at (or
make) a diagnosis, in contrast to scenario #5 where her
conclusion was an ICE but not a diagnosis.
Conclusions
We applied Smith and Ceusters results on aboutness
[25] to diagnosis in order to develop an account of
diagnosis, misdiagnosis, lucky guesses, hearsay, a layper-
sons justified belief about disease configurations, and a
diagnosis made by an expert system. Our key result is
that a correct diagnosis, as defined by OGMS, is about a
configuration of an organism, its disease, and the type
the disease instantiates (level of compound expression)
in a specified portion of spacetime. A misdiagnosis by
contrast is a misrepresentation of this configuration.
Nevertheless, both diagnosis and misdiagnosis are still
aboutat the level of individual referencethe organism
and (when they exist) a disease instance and a disease
type. Also, they are both the output of a diagnostic
process, which differentiates them from lucky guess and
hearsay as well as the misinformation-based counterparts
to lucky guess and hearsay. We also carefully represented
the inputs and outputs of this process.
We identified several subtypes of misdiagnosis (e.g.,
wrong disease subtype, wrong patient, wrong temporal
placement) that have not been differentiated in the
literature on diagnostic error, to our knowledge. Studying
the incidence and causes of these subtypes might advance
the study of diagnostic error and strategies to reduce it.
Note that as we have defined it, misdiagnosis does not
refer to the diagnostic errors of absent diagnosis (failing to
diagnose a disease at all, let alone incorrectly) and delayed
diagnosis. Lastly, we note that the current literature on
diagnostic error, per a 2016 Institute of Medicine report,
does not lend itself to generating reliable estimates of inci-
dence of diagnostic error per se, let alone any subtype of
such error [37].
Although misdiagnoses involving non-existence of certain
entities might at first seem to be of minor importance, we
highlight two cases where non-existence is relevant. First,
in the case where the type of disease does not exist (con-
sider past diagnoses of dropsy), it could well be that our
understanding of disease decades from now is much more
advanced, and what we think are types of disease today in
fact are not. So just as with past diagnoses of dropsy, it
could be that todays diagnoses of schizophrenia are mis-
diagnoses merely by referring to a type that does not exist.
Second, in the case where the instance of disease does not
exist, we consider two scenarios. The first scenario involves
past diagnoses of mental illness where neither the instance
nor the type exists. For example, past diagnoses of runaway
slaves as having drapetomania involved neither a really
existing instance nor a really existing type of disease. The
second scenario involves patients with hypochondria or
who are malingering. They feign a condition for which the
unassuming practitioner mistakenly asserts the existence of
an instance and the instantiation of a type.
Our results and typology of misdiagnosis could serve
as the beginnings of a formal framework for studying
diagnostic error as a component of data quality in EHRs
and research data collections, in response to the call by
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 12 of 15
Weiskopf and Weng for more formal, generalizable, and
validated methods for assessing data quality [24]. Applying
Ceusters detailed typology of mistakes in ontology (e.g.,
asserting a type that does not exist) [38] and referent
tracking systems (e.g., assigning an identifier but there is
no corresponding particular that it identifies, assigning
one identifier to two particulars, assigning two identifiers
to one particular, etc.) [39] to diagnosis could build on our
work here to build out such a framework. It remains fu-
ture work to do so.
The provenance of the ICE and its concretizations are
critical: lucky guesses, hearsay, and laypersons conclusions
about disease (when not arrived at through a diagnostic
process using a clinical picture and cognitive representa-
tions of the associated type(s) of disease as input) do not
constitute diagnoses and therefore are different types of
ICE than diagnoses. Provenance also includes which find-
ings and other information constituted the clinical picture
used in the diagnostic process. Our analysis of the scenarios
identified past diagnoses as important input into the diag-
nostic process, leading to proposed redefinitions of clinical
history, clinical picture, and diagnostic process for OGMS.
Smith and Ceusters results on aboutness and our ex-
tension of them here to diagnosis reduce the need for
the workarounds reported by Martínez Costa and
Schulz [26] and Hastings et al. [27] It is perfectly legit-
imate to relate suspected heart failure finding to con-
gestive heart failure with an existential quantifier: if an
instance of this type is not about a really-existing con-
figuration of patientdiseaseheart failure, it is still an
ICE that is individually about the patient, her condition,
and the type heart failure on the level of reference. In
OWL, we could assert:
Suspected heart failure ICE - > ICE and (is about
SOME Organism)
Suspected heart failure ICE - > ICE and (is about
SOME Condition)
In more expressive formalisms including first-order
logic, we could also assert that it is about the type heart
failure, where Type, Instance_of , and Is_about are
predicates in what follows, where the universal quantifi-
cation applies to the ICE, not what it is about:
Type(heart_failure)
Type(suspected_heart_failure_ICE)
?x (Instance_of(x, suspected_heart_failure_ICE) - >
Is_about(x, heart_failure))
Similarly, chemical graphs and diagrams are ICEs about
individual types of atoms such as carbon, oxygen, hydro-
gen, and so on, even when they fail to be about any type
of configuration (e.g., molecule) of such atoms. However,
because they are typically not about any instances, proper
existential quantification in OWL is not possible. How-
ever, we could relate in first-order logic the diagram of
octaazacubane (a hypothetical molecule which would be
comprised of eight nitrogen atoms arranged in a cubic
structure) to the nitrogen type of atom using existential
quantification (again where the universal quantification in
what follows applies to the ICE and not what it is about):
Type(nitrogen_atom)
Type(octaazacubane _diagram)
?x (Instance_of(x, octaazacubane_diagram) - >
Is_about(x, nitrogen_atom))
It is therefore not required to use universal quantification
over the range of things that an ICE is about, when relating
ICEs to those entities they are about, to avoid failure of
aboutness on the level of compound expression. This result
is qualified by the constraints of representational formal-
isms such as OWL that prevent directly asserting aboutness
to types. Schulz et al. describe workarounds in OWL to
asserting aboutness to types, that may be of benefit in some
use cases [40].
The use of universal quantification actually introduces
problems when we account for aboutness on the level of
individual reference. For example, if we leave the suspected
heart failure finding of Martínez Costa and Schulz as being
only about congestive heart failure, then it would result in a
contradiction to say that it is about some organism. Like-
wise for condition. So use of the universal quantifier pre-
cludes aboutness on the level of individual reference, in
direct conflict with the results of Smith and Ceusters on
misinformation.
Although it was not the primary or even secondary
goal of the present work, other advantages of our ap-
proach with respect to inference are easy to derive. First,
in our approach with explicit representation of the disease
in addition to the diagnosis, we can infer all instances of
Type 1 diabetes mellitus that have been misdiagnosed as
Type 2 diabetes mellitus at some point in time, in first
order logic minimally and possibly in OWL with work-
arounds. Generalizing slightly, we can query for all condi-
tions that have been misdiagnosed as Type 2 diabetes
mellitus. Using a typology of organisms, we can find in the
veterinary domain all diagnoses and/or misdiagnoses of a
certain type of disease in organisms of a certain type: for
example, misdiagnoses of foot and mouth disease in cattle.
Having no ability to create an aboutness relation from a
misdiagnosis, or more generally an incorrect clinical state-
ment, to the organism it is about (due to the contradic-
tions that will result as pointed out above) or even to
anything in reality at all, the universal quantifier approach
of Martínez Costa and Schulz would require substantial
revision to make these inferences.
Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 13 of 15
In the realm of chemical diagrams, our approach en-
ables one to query for all chemical diagrams that depict
nitrogen atoms or certain chemical groups (e.g., hy-
droxyl group and benzene rings), including the diagrams
that are not about any existing type of molecule. The
universal quantifier approach in Hastings et al., by con-
trast, would require significant revision to return dia-
grams that depict nitrogen, hydroxyl groups, benzene
rings, and so on, but are not about any existing type of
molecule. In depth exploration of the effects of our rep-
resentation on inference remains future work, as it is
not our primary interest here.
Our analysis also identified problems with, and sug-
gested improvements to, the definitions of core terms
from the Ontology for General Medical Science includ-
ing diagnostic process and clinical picture. This result
is consistent with our past work, where we have found
the method of referent tracking analysis to be a stringent
test of definitions in ontologies.
This work is limited by the fact that we did not conduct
further ontological analysis of the diagnostic process be-
yond OGMS and beyond what our scenarios required, as
this was not the purpose of the present work. We do note
that our requirement for including cognitive representa-
tions of disease types as input into the diagnostic process
is based on this literature, however. Engaging experts in
the study of clinical reasoning in future work to develop a
typology of diagnostic processes has the potential to result
in a corresponding typology of diagnoses.
Future work includes (1) an account of differential diag-
nosis, where a clinician or expert system generates a list of
likely types of disease for further investigation to identify
the actual type the organisms disease instantiates; (2) pro-
posing to the OGMS community to clarify the definitions
of clinical history, clinical picture, and diagnostic process
as suggested here, and to expand the definition of diagno-
sis to include disorders, disease courses, and absence of
disease (i.e., healthy); (3) extending our analysis as re-
ported here to this expanded definition of diagnosis; (4)
conducting deeper ontological analysis of the diagnostic
process, in coordination with experts in the study of clin-
ical reasoning; and (5) more fully exploring the effects of
our representations on logical inference beyond some
readily evident advantages discussed here.
Additional files
Additional file 1: Table S1. Entities in Scenario 2: Second correct
diagnosis. Table S2. Additional temporal entities in Scenario 2: Second
correct diagnosis. Table S3. Relationships among particulars in Scenario 2:
Second correct diagnosis. (DOCX 88 kb)
Additional file 2: Table S4. Entities in Scenario 3: Incorrect diagnosis.
Table S5. Additional temporal entities in Scenario 3: Incorrect diagnosis.
Table S6. Relationships among particulars in Scenario 3: Incorrect diagnosis.
(DOCX 88 kb)
Additional file 3: Table S7. Entities in Scenario 4: Laypersons unjustified
inference. Table S8. Additional temporal entities in Scenario 4: Laypersons
unjustified inference. Table S9. Relationships among particulars in Scenario
4: Laypersons unjustified inference. (DOCX 95 kb)
Additional file 4: Table S10. Entities in Scenario 5: Laypersons justified
inference. Table S11. Additional temporal entities in Scenario 5: Laypersons
justified inference. Table S12. Relationships among particulars in Scenario 5:
Laypersons justified inference. (DOCX 92 kb)
Abbreviations
BFO: Basic formal ontology; GDC: Generically dependent continuant;
IAO: Information artifact ontology; ICE: Information content entity;
IQE: Information quality entity; OGMS: Ontology for General Medical Science;
POR: Portion of reality; RT: Referent tracking; RTT: Referent tracking tuple
Acknowledgments
This work was suppoted in part by the NIH/NCATS Clinical and Translational
Science Award to the University of Florida UL1TR001427.
Authors contributions
The authors contributed equally to the ontological analysis and development
of results. Author WRH created the first version of the manuscript. Both authors
had full access to all materials and analysis and participated in revising the
manuscript. Both authors approved the final version of the manuscript.
Competing interests
The authors declare that they have no competing interests.
Author details
1University of Florida, 2004 Mowry Rd, P.O. Box 100219, Gainesville, FL
32610-0219, USA. 2Department of Biomedical Informatics, University at
Buffalo, 77 Goodell street, 5th floor, Buffalo, NY 14203, USA.
Received: 1 October 2015 Accepted: 6 September 2016
Han et al. Journal of Biomedical Semantics  (2016) 7:22 
DOI 10.1186/s13326-016-0059-z
RESEARCH Open Access
Active learning for ontological event
extraction incorporating named entity
recognition and unknown word handling
Xu Han1, Jung-jae Kim2* and Chee Keong Kwoh1
Abstract
Background: Biomedical text mining may target various kinds of valuable information embedded in the literature,
but a critical obstacle to the extension of the mining targets is the cost of manual construction of labeled data, which
are required for state-of-the-art supervised learning systems. Active learning is to choose the most informative
documents for the supervised learning in order to reduce the amount of required manual annotations. Previous works
of active learning, however, focused on the tasks of entity recognition and protein-protein interactions, but not on
event extraction tasks for multiple event types. They also did not consider the evidence of event participants, which
might be a clue for the presence of events in unlabeled documents. Moreover, the confidence scores of events
produced by event extraction systems are not reliable for ranking documents in terms of informativity for supervised
learning. We here propose a novel committee-based active learning method that supports multi-event extraction
tasks and employs a new statistical method for informativity estimation instead of using the confidence scores from
event extraction systems.
Methods: Our method is based on a committee of two systems as follows: We first employ an event extraction
system to filter potential false negatives among unlabeled documents, from which the system does not extract any
event. We then develop a statistical method to rank the potential false negatives of unlabeled documents 1) by using
a language model that measures the probabilities of the expression of multiple events in documents and 2) by using
a named entity recognition system that locates the named entities that can be event arguments (e.g. proteins). The
proposed method further deals with unknown words in test data by using word similarity measures. We also apply
our active learning method for the task of named entity recognition.
Results and conclusion: We evaluate the proposed method against the BioNLP Shared Tasks datasets, and show
that our method can achieve better performance than such previous methods as entropy and Gibbs error based
methods and a conventional committee-based method. We also show that the incorporation of named entity
recognition into the active learning for event extraction and the unknown word handling further improve the active
learning method. In addition, the adaptation of the active learning method into named entity recognition tasks also
improves the document selection for manual annotation of named entities.
Keywords: Active learning, Biomedical natural language processing, Information extraction
*Correspondence: jjkim@i2r.a-star.edu.sg
2Data Analytics Department, Institute for Infocomm Research, 1 Fusionopolis
Way, 138632 Singapore, Singapore
Full list of author information is available at the end of the article
© 2016 Han et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International
License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any
medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons
license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.
org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 2 of 18
Background
A common framework of information extraction systems
is supervised learning, which requires training data that
are annotated with information to be extracted. Such
training data are usually manually annotated, where the
annotation process is time-consuming and expensive. On
the other hand, in biomedical domain, recent research
efforts on information extraction are extending from
focusing on a single event type such as protein-protein
interaction (PPI) [1] and gene regulation [2] to simulta-
neously targeting more complicated, multiple biological
events defined in ontologies [3], which makes the man-
ual annotation more difficult. There is thus the need of
reducing the amount of annotated data that are required
for training event extraction systems.
Active learning is the research topic of choosing infor-
mative documents for manual annotation such that the
would-be annotations on the documents may promote
the training of supervised learning systems more effec-
tively than the other documents [4]. It has been studied
in many natural language processing applications, such
as word sense disambiguation [5], named entity recog-
nition [68], speech summarization [9] and sentiment
classification. Its existing works can be roughly classified
into two approaches: uncertainty-based approach [10] and
committee-based approach [11]. The uncertainty-based
approach is to label the most uncertain samples by using
an uncertainty scheme such as entropy [12]. It has been
shown, however, that the uncertainty-based approachmay
have worse performance than random selection [1315].
In the biomedical information extraction, the
uncertainty-based approach of active learning has been
applied to the task of extracting PPIs. For instance, [16]
proposed an uncertainty sampling-based approach of
active learning, and [17] proposed maximum uncertainty
based and density based sample selection strategies.
While the extraction of PPI is concerned with a single
event type of PPI, however, recent biomedical event
extraction tasks [18] involve multiple event types,
even hundreds of event types in the case of the Gene
Regulation Ontology (GRO) task of BioNLP-ST13 [19].
The committee-based approach, based on a committee
of classifiers, selects the documents whose classifications
have the greatest disagreements among the classifiers
and passes them to human experts for annotation. This
approach, however, has several issues in adaptation for
event extraction tasks. First, event extraction (e.g. PPI
extraction, gene regulation identification) is different from
many other applications of active learning, which are in
essence document classification tasks. Event extraction is
to locate not only event keywords (e.g. bind, regulates),
but also event participants (e.g. gene/protein, disease)
within documents and to identify pre-defined relations
between them (e.g. subject-verb-object). Thus, even if the
event extraction systems produce confidence scores for its
resultant events, the confidence scores do not correspond
to the probability of how likely a document expresses
an event type: in other words, how likely a document
belongs to an event type class, which should be the goal
of classifiers of the committee-based approach for event
extraction. Second, previous classifiers for the committee-
based approach may miss some details of events including
event participants. For example, the keyword expression
may mislead a classifier to predict that the document with
the keyword expresses gene expression event, although
the document does not contain any gene name.
Our target tasks of event extraction for active learning
in this paper are those introduced in BioNLP-ST13 [20],
which involve multiple, complicated event types. Cur-
rently, there is only one event extraction system available
for all the tasks, called TEES [21], and we need an addi-
tional classifier to follow the committee-based approach.
We thus propose as an additional classifier a novel
statistical method for informativity estimation, which pre-
dicts how likely a text expresses any event concept of a
given ontology. The method is based on a language model
for co-occurrences between n-grams and event concepts.
Furthermore, it independently estimates the presence of
event participants in a text and the probabilities of out-of-
vocabulary words and combines them with the prediction
of event concept in the text. We collectively estimate the
informativity of a text for all the concepts in a given
ontology, similarly to the uncertainty-based approach
of [2224].
We also present a revised committee-based approach of
active learning for event extraction, which combines the
statistical method with the TEES system as follows: Since
the confidence scores of the TEES system are not reli-
able for active learning, we take TEES outputs as binary,
that is, whether the system extracts any instance of a con-
cept from a text or not. The disagreement between the
TEES system and the statistical model is captured when,
given a text (T) and an event concept (C), the TEES system
does not extract any instance of C in T, but the probabilis-
tic model predicts a high probability of C in T. In other
words, the TEES system is used for filtering potential false
positives, and the probabilistic model for ranking them.
We further adapt our active learning method and the
statistical method for event concept detection to named
entity recognition, including gene name recognition. We
show that our method can improve active learning for
named entity recognition as well, when tested against the
BioCreative and CoNLL datasets.
Methods
We formalize the general workflow of active learning as
follows: At the start of round t, let U (t?1) be the pool of
unlabeled documents and let L(t?1) be the pool of labeled
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 3 of 18
documents, where t starts from 1. In round t, we select
the most informative document x(t) from U , manually
label it, and add it to L. If the label y(t) is assigned to
the document x(t) by the oracle, the labeled and unlabeled
document sets are updated as follows:
L(t) = L(t?1) ?{(x(t), y(t))} U (t) = U (t?1)\x(t)
(1)
Such process is iterated until a certain stopping criteria
is met, such as when U = ? and after a pre-defined num-
ber of rounds. It also can be done in a batch mode, where
a group of documents are selected at each round for the
manual labeling.
Active learning method for event extraction
As explained above, our active learning method follows
the committee-based approach. As the committee, we
employ two classifiers: A classifier based on an event
extraction system called TEES and a statistical classifier
based on language modeling (see the next section for
details). The TEES [21] is a state-of-the-art biomedical
event extraction system based on support vector machine,
and was the only system that participated in all the tasks of
BioNLP-ST13, showing the best performance in many of
the tasks [25]. The TEES system produces the confidence
score of each event it extracts. However, we do not use
the score for active learning because the confidence score
does not indicate the probability of the event in the doc-
ument. We also assume that if the TEES system extracts
an event (E) from a document (D), D is not informative
for E, because true positives are already not informative
and because the correction (i.e. labeling) of false positives
might not be useful for training event extraction systems
where event descriptions are scarce, and thus there are far
more negative examples than positive examples. In other
words, the primary goal of our active learning method is
to correct more false negatives, that is, to annotate the
true events not extracted by the existing system. Figure 1
depicts the workflow of the proposed method.
Our method works iteratively as follows: In round t, we
train the TEES system and the statistical classifier based
onL(t?1). Wemeasure the informativity of each unlabeled
document among U (t?1) and choose the top documents
as feed for manual annotation. We measure the informa-
tivity score of a document at the sentence level, that is, the
average of the informativity scores of all the sentences in
the document, as illustrated in (2).
x?Informativity = argmaxx I? t (x) I? t (x) =
1
||x||
Sk?x?
I(Sk)
(2)
Document (D) Concept/relation (C)
Does TEES 
annotate C on D?
D is informative 
for C
Unlabeled corpus Ontology
No Yes
Is D likely to 
express C?
Yes No
D is not 
informative for C
Fig. 1 Overview of proposed active learning method. The integration
of underlying system into active learning method. For event
extraction task, if the underlying event extraction system (TEES) can
recognize the concept (C) in the given document (D), the D is not
considered as informative
? t indicates the current models of the TEES system and
the statistical classifier at round t, but we will omit it for
simplicity.
The informativity of a sentence (Sk) is measured for the
event concept set E , which contains all event defined in
a given ontology, as expressed in (3). The informativity
score for event concept set is denoted as I(Sk , E). In fact,
the BioNLP-ST13 tasks include not only events, but also
relations. A key difference between events and relations is
that an event always involves an event keyword (e.g. reg-
ulates for GeneRegulation), but a relation does not have
any keyword (e.g. partOf). For simplicity, wemention only
events in the paper, while ourmethod involves both events
and relations in the same way.
I(Sk) = I(Sk , E) (3)
Informativity for event concept set
The informativity of a sentence for event concept set is
calculated as the sum of the informativity scores of the
sentence for all the event as follows:
I(Sk , E) =
Ei?E?
I(Sk ,Ei) (4)
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 4 of 18
As explained earlier, we treat a sentence as non-
informative for an event if the event extraction system
TEES can extract any instance of the event from the
sentence. Otherwise, the informativity score is estimated
as the probability of the concept given the sentence as
follows:
I(Sk ,Ei)=
{
0 if Ei is recognized in Sk by the TEESmodel at round t
p(Ei|Sk) otherwise
(5)
p(Ei|Sk) can be converted into (6) using the Bayes
theorem.
p(Ei|Sk) = p(Ei)p(Sk|Ei)p(Sk) (6)
The P(Ei) is estimated using the maximum-likelihood
estimation (MLE) based on the statistics of event annota-
tions in the training data.
As for P(Sk|Ei), we score the correlation between the
sentence Sk and the event Ei with a real value scoring func-
tion Z (see below for details) and use the softmax function
to represent it as a probabilistic value, shown in (7).
p(Sk |Ei) = ?(Z(Sk : Ei)) = 1?
Ej?E exp(Z(Sk : Ej))
exp(Z(Sk : Ei))
(7)
We use two types of units to approximately represent
the sentence Sk : n-grams (NG) and predicate-argument
relations (PAS) produced by the Enju parser [26]. A sen-
tence is represented as a bag of elements of a unit, for
example, a bag of all n-grams or a bag of all predicate-
argument relations from the sentence.
A. Using N-gram feature for probability estimation If
we use the bag of n-gram model, the score Z(Sk : Ei)
is measured using the average of the correlation score
between the n-gram (NG) contained in the sentence with
the event, expressed in (8), where len(Sk) is the normaliza-
tion factor and is calculated as the word count of sentence
Sk .
Z(Sk : Ei) = 1len(Sk)
NGj?Sk?
p(NGj|Ei) (8)
While the probability between the n-gram and event
p(NGj|Ei) is also calculated using a correlation score
W (NGj,Ei) between the n-gram and the event, together
with the softmax function, shown in (9).
p(NGj|Ei) = ?(W (NGj,Ei))
= 1?
NGl?U exp(W (NGl,Ei))
exp(W (NGj,Ei))
(9)
The correlation scoreW (NGj,Ei) is calculated using one
of the following three methods: 1) Yates chi-square test,
2) relative risk, and 3) odds ratio [27]. For the calculation
of the three methods, a 2×2 table is constructed for each
pair of an N-gram and an event at the level of sentences,
as shown in Table 1. For example, a indicates the number
of sentences that contain the N-gram NGj and express the
event Ei.
Based on the 2×2 table, the three methods of Yates
chi-square test, relative risk, and odds ratio calculate the
correlation score for the pair as shown in the formulas
(10), (11), and (12), respectively.
W (NGj,Ei) = N(|ad ? bc| ? N/2)
2
NSNFNANB
(10)
W (NGj,Ei) = a/(a + b)c/(c + d) (11)
W (NGj,Ei) = a/bc/d (12)
B. Using predicate-argument relation for probability
estimation Similarly for the bag of predicate-argument
relation model, the score Z(Sk : Ei) is calculated with the
average of the correlation scores between the event and
the predicate-argument relations from the sentence, as in
(13).
Z(Sk : Ei) = 1len(Sk)
PASj?Sk?
p(PASj|Ei) (13)
Additional features of active learning
We introduce two additional features of our active learn-
ing method: Incorporation of event participants and deal-
ing with out-of-vocabulary words.
Incorporation of event participants
The absence of event participants should negatively affect
the prediction of events. To reflect this observation, we
utilized a gene name recognition system, called Gimli [28],
in order to recognize gene/protein names, since most of
the BioNLP shared tasks involve genes and proteins (e.g.
gene expression, gene regulation, phosphorylation). We
incorporate the results of the Gimli system into our active
learning method as follows:
I(Sk) = I(Sk , E ,NG) + I(Sk ,N ) (14)
Table 1 Numbers of sentences for the calculation of correlation
score between Ei and NGj
Express event Ei Not express event Ei Total
Contain N-gram NGj a b NA
Not contain N-gram NGj c d NB
Total NS NF N
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 5 of 18
I(Sk ,N ) = ? × T (15)
T indicates the number of gene/protein names pre-
dicted in a sentence Sk .
In fact, the Gimli system can be replaced with other
named entity recognition systems for tasks whose event
participants are other than gene/protein. Since the event
extraction tasks for evaluating our active learning method
(i.e. BioNLP shared tasks) are mainly about gene/protein,
we do not replace the Gimli system when evaluating the
incorporation of event participants. When we apply our
active learning method for the tasks of named entity
recognition (NER), however, we will evaluate it against
two NER systems (i.e. Gimli, Stanford NER system) (see
for details Sections Active learning method for NER task
in Page 8, Datasets and employed systems in Page 11,
and Evaluation of active learning method for NER task in
Page 19).
Dealing with OOV issue with word similarity
When we use the n-gram features, there is Out-of-
Vocabulary (OOV) issue, such that some n-grams in the
test dataset may not appear in the training dataset. To
tackle this issue, we adopt the word2vec system, which is
an unsupervised method for representing each word as a
vector in a latent semantic model and for measuring word
similarity [29], as follows: Consider an n-gram NGout that
does not occur in the training dataset. We use word2vec
to find the top-k n-grams NGin that are closest to NGout ,
where the word similarity score between NGout and each
NGin is designated as Sim(NGout ,NGin). We then recal-
culate the correlation scoring function W (NGout ,Ei) as
shown in Formula (16). Note that since word2vec can only
handle unigrams, and also since unigrams show the best
performance in our experiments of parameter optimiza-
tion (see the next section), we only deal with unknown
unigrams in this method. The word similarity scores are
trained a priori using the whole set of MEDLINE abstracts
released in April 2014.
WOOV (NGout ,Ei)=
NGin?TrainingDataset?
top?k
W(NGin,Ei)× SimNGout ,NGin
(16)
We denote the n-gram-based informativity of sentence
calculated using the updated correlation scoring function
(16) as I(Sk ,NGOOV ). For example, when the correlation
scoring function in (9) is updated, the resultant informa-
tivity in (4) is denoted as I(Sk , E ,NGOOV ).
Linear combination of n-gram and predicate-structure
relation features
While we choose either n-grams or predicate-argument
relations as features, we also tested the linear combination
of the two feature sets for our active learning method, as
follows:
I(Sk) = ? × I(Sk ,NGOOV ) + ? × I(Sk ,PAS) + ? × I(Sk ,N )
= ? × I(Sk , E ,NGOOV ) + ? × I(Sk , E ,PAS) + ? × I(Sk ,N )
(17)
Table 2 illustrates the calculation of informativity scores
in pseudo codes.
Active learning method for NER task
We also adapt our active learning method to named entity
recognition (NER), considering the ontology concepts of
named entities (e.g. Gene, Disease) instead of events (e.g.
PPI, gene regulation). Themethod for named entity recog-
nition estimates informativity, or the likelihood of a text
expressing any named entities.
Similar to Eq. (2), the informativity estimation in the
NER task is expressed in (18).
x?Infomativity = argmaxx I? t (x) I? t (x) =
1
||x||
Sk?x?
I(Sk)
(18)
? t indicates the current model of a given NER system
and the statistical classifier at round t, but we will omit
it for simplicity. We evaluate our method with two NER
systems of Gimli for biomedical domain and Stanford
Table 2 Proposed algorithm of active learning with TEES
Input: labeled document pool L, unlabeled document pool U, batch size b
// Initialization
ER0 = the set of events/relations annotated on L
Learn a TEES modelM0 from ER0
i = 0 // the index of the current round
// Active Learning Loop
while U is not empty:
i += 1
for each document Dij in U:
Document informativity score I(Dij) = 0
for each sentence Sk in Dij :
ApplyMi?1 to Sk and collect the resultant events/relations set ERSk
for each event/relation er s.t. er /? ERsk :
I(Dij) += informativity score I(Sk , er)
I(Dij) = I(Dij) / sizeOf(Dij)
Rank Dij in U based on I(Dij) and select the top b documents,
designated as B
Remove B from U, add B to L, and add the annotations on B to ERi?1,
designated as ERi
Learn a new modelMi from ERi
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 6 of 18
NER system for general domain (see Section Results and
discussion for details of evaluation), one system at a time.
The informativity of a sentence for named entity set is
calculated as the sum of the informativity scores of the
sentence for all the named entities as follows:
I(Sk) = I(Sk ,N ) =
Ni?N?
I(Sk ,Ni) (19)
Similar to the active learning method for event extrac-
tion, we treat a sentence as non-informative for an named
entity if the NER system can recognize any instance of the
named entity from the sentence. Otherwise, the informa-
tivity score is estimated as the probability of the named
entity given the sentence as follows:
I(Sk ,Ni)=
{
0 ifNi is recognized in Sk by the NER system at round t
p(Ni|Sk) otherwise
(20)
The probability p(Ni|Sk) is calculated as follows:
p(Ni|Sk) = p(Ni)p(Sk|Ni)p(Sk) (21)
Similarly to the estimation for event, p(Ni) is estimated
using the maximum-likelihood estimation (MLE) based
on the statistics of named entities in the training data. For
the calculation of p(Sk|Ni), we follow similar steps as in
(7), using n-grams (i.e. Formula (8)), but not using PAS (i.e.
Formula (13)).
Comparison with related works
In this section, we describe the previous methods of active
learning that we compare with our proposed methods for
event extraction in the evaluation experiments.
A. Conventional committee-based method The com-
mittee based active learning, based on a committee of
classifiers, selects the documents whose classifications
have the greatest disagreements among the classifiers and
passes them to human experts for annotation, expressed
as follows:
x?Committee = argmaxx D? (Y |x) (22)
D? (Y |x) is the disagreements among the classifiers for
a document x under the model ? , and the Y is the whole
label set. We use the summation of disagreement over the
sentence Sk contained in the document x.
D? (Y |x) =
Sk?x?
D(Y |Sk) (23)
For each sentence, we measure the collective disagree-
ment over the whole event concept set E defined in the
ontology by using the sum of all disagreement for all
event Ei.
D(Y |Sk) =
Ei?E?
D(Ei|Sk) (24)
The disagreement D(Ei|Sk) is calculated using the abso-
lute value of the differences of the probability produced
by the classifiers, named the aforementioned informativity
estimation method and the TEES event extraction system.
D(Ei|Sk) = |pInformativity(Ei|Sk) ? pTEES(Ei|Sk)| (25)
The pTEES(Ei|Sk) is the probability estimated from the
TEES system, and pInformativity(Ei|Sk) is from the infor-
mativity estimation using statistical method, which is cal-
culated in Eq. (6). Note that while p(Ei|Sk) in Eq. (5) is
estimated using Eq. (6) only for the sentences from which
no Ei is recognized by the TEES, the same informativity
probability in Eq. (25) is estimated for all the sentences of
unlabeled documents.
However, as the TEES is a support vector machine
(SVM) based system and do not produce probabilistic out-
put, we use the confidence the SVM classifier has in its
decision for a event prediction as follows:
pTEES(Ei|Sk) =?(C(Ei|Sk)) = 1?
Ej?E exp(C(Ej|Sk))
exp(C(Ei|Sk))
(26)
C(Ei|Sk) is the confidence for the classifier.
The confidence is calculated using the difference-2 of
the distance from the separating hyperplane, produced by
the SVM classifier. It is shown to have best performance
in active learning [30, 31], and the calculation is expressed
as follows:
mmax = argmax
m
dist(m, Sk)
n = argmax
n=mmax
dist(n, Sk)
C(Ei|Sk) = dist(mmax, Sk) ? dist(n, Sk)
(27)
The dist(m, Sk) is the distance of the predicted label m
in such sentence Sk .
Similarly in adapting to the NER task, for each sen-
tence, we measure the collective disagreement over the
whole named entity concept setN by using the sum of all
disagreement for all named entity Ni.
D(Y |Sk) =
Ni?N?
D(Ni|Sk) (28)
The disagreementD(Ni|Sk) is calculated using the abso-
lute value of the differences of the probability produced
by the classifiers, named the aforementioned informativity
estimation method and the NER system.
D(Ni|Sk) = |pInformativity(Ni|Sk) ? pNER(Ni|Sk)| (29)
The pNER(Ni|Sk) is the marginal probability provided by
the Conditional Random Field (CRF) model from the NER
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 7 of 18
system, and pInformativity(Ni|Sk) is from the informativity
estimation using statistical method.
B. Entropy based active learning method Entropy is
the most common measure for uncertainty, which indi-
cates a variables average information content. The docu-
ment selection of entropy-based methods is formalized as
follows:
x?Entropy = argmaxx H? (Y |x) (30)
The H? (Y |x) is the entropy of a document x under the
model ? and the Y is the whole label set. We use the sum-
mation of entropy over the sentence Sk contained in the
document x.
H? (Y |x) =
Sk?x?
H(Y |Sk) (31)
For each sentence Sk , we use the aforementioned bag
of n-gram method, and estimate H(Y |Sk) as the average
entropy of each n-gram NGj in Sk , as follows:
H(Y |Sk) = 1len(Sk)
NGj?Sk?
H(Y |NGj) (32)
We estimate the collective entropy over the whole event
concept set E defined in the ontology as the summation of
the entropy for all event Ei.
H(Y |NGj) =
Ei?E?
H(Ei|NGj) (33)
H(Ei|NGj) is calculated by using the Weka package for
the calculation of entropy [32].
C. Gibbs error based active learning method Gibbs
error criterion is shown to be effective for active learning
[33], which selects documents that maximize the Gibbs
error, as follows:
x?Gibbs = argmaxx HGibbs(?) (34)
Similarly to the entropy-based method implementation,
we calculate the collective Gibbs error as follows:
HGibbs(?)=
Sk?x?
HGibbs(Y |Sk)=
Sk?x? 1
len(Sk)
NGj?Sk? Ei?E?
HGibbs(Ei|NGj)
(35)
For the calculation of HGibbs(Ei|NGj), we use the con-
ditional probability of p(Ei|NGj), defined as follows [33],
where p(Ei|NGj) is estimated using the proposed method
as shown in (9):
HGibbs(Ei|NGj) = 1 ? p(Ei|NGj)2 (36)
Results and discussion
Datasets and employed systems
The BioNLP shared tasks (BioNLP-ST) were organized
to track the progress of information extraction in the
biomedical text mining. In this paper, we used the datasets
of three tasks, namely GRO13 (Gene Regulation Ontol-
ogy) [19], CG13 (Cancer Genetics) [34] and GE13 (Genia
Event Extraction) [35]. Each corpus was manually anno-
tated with an underlying ontology, whose number of
concepts and hierarchy are different from each other. A
comparison between the datasets is given in Table 3. Note
that since the official test datasets for CG and GE tasks are
inaccessible, we instead use parts of their training datasets
as the test datasets, and the statistics of the datasets
include only those accessible documents.
Specifically, we employ the state-of-the-art Stanford
NER [36] system for the CoNLL-2003 [37] dataset, and the
Gimli gene name recognition system [28] for the BioCre-
ative II Gene Mention [38] dataset. Note that in BioCre-
ative task, the named entities are naturally of one class, i.e.,
the Gene/Protein name; while the CoNLL dataset involves
four classes of named entities (i.e. Person, Organization,
Location, Misc).
Evaluation metrics for comparison of active learning
methods
To compare the performance of the different strategies
of sample selection, we plot their performance in each
iteration. Since the difference between some plots is not
obvious, however, we mainly use the evaluation metric of
deficiency for comparison [39, 40], defined as follows:
Defn(AL,REF) =
?n
t=1(accn(REF) ? acct(AL))?n
t=1(accn(REF) ? acct(REF))
(37)
The acct(C) is the performance of the underlying clas-
sifier C at tth round of learning iteration. AL is an active
learning method, and REF is a baseline method (see below
for details). n refers to the total number of rounds (i.e. 10).
A deficiency value smaller than 1.0 means that the active
learning method is superior to the baseline method, and
in general, a smaller value indicates a better method.
Table 3 Summary of task datasets used in the experiments
Task Corpus size (Dev/Train/Test) Document type No. event concepts No. relations
GRO13 300 (50/150/100) MEDLINE abstract 507 10
CG13 400 (100/200/100) MEDLINE abstract 58 1
GE13 20 (5/10/5) PubMed Central full text 13 20
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 8 of 18
Parameter optimization
We first take a parameter optimization step to determine
the most appropriate parameters for the aforementioned
calculation of informativity scores.
Correlationmeasure and n-gram size
As mentioned above, we considered three correlation
measures to estimate the correlation score between n-
gram and event, including chi-square test, relative risk,
and odds ratio.We also should determine the value of n for
n-grams. To find the optimal solutions for the two tasks,
we carried out a simulation of ontology concept predic-
tion at the sentence level as follows: Given a sentence Si
and Ni ontology concepts manually annotated on the sen-
tence, we predict the top Ni ontology concepts in Si and
compare them with the Ni manually annotated concepts,
measuring the overlap between the two concepts sets.
We select the best combination of co-occurrence analysis
method and n-gram size for the rest of experiments in this
paper.
Using 10-fold cross validation, the average prediction
rate is calculated and reported in Table 4. Each column
corresponds to an n-gram size, and each row to one of the
three co-occurrence analysis methods used for the pre-
diction. Note that when N=2 (i.e. bi-grams), it does not
include unigrams for the calculation. N=1-2 indicates the
mixture of unigrams and bi-grams. This experiment is
carried out using the GRO13 dataset.
As shown in Table 4, for all co-occurrence analysis
methods, the accuracy mostly drops as the length of N-
grams increases. This may happen due to the data sparse-
ness problem for large N-grams. We choose to use chi-
square test and unigrams for the following experiments
based on the results.
Parameter for the incorporation of event participants
The parameter of ? in Eq. (15) is to determine the sig-
nificance of effects of event participants on event con-
cept prediction. We tested our active learning method in
Eq. (14) against the GRO13 dataset with the ? values set
as 0.15, 0.25 and 0.35. We summarize the performance
results in terms of deficiency in Table 5. We choose the
? = 0.25 for the following experiments based on the
results.
Table 4 Parameter optimization results
Calculation
method
N-gram
N = 1 N = 2 N = 3 N = 4 N = 5 N = 1 ? 2
Chi-square 0.507 0.413 0.159 0.036 0.009 0.436
Relative ratio 0.341 0.395 0.307 0.128 0.038 0.361
Odds 0.420 0.395 0.274 0.117 0.035 0.407
The averaged concept prediction accuracy is reported. The best accuracy is
highlighted in boldface
Table 5 Parameter optimization results
Method GRO13
RS_Average 1
? = 0.15 0.716
? = 0.25 0.706
? = 0.35 0.713
The deficiencies of active learning method using different factor against the GRO13
are reported. The best deficiency is highlighted in boldface in this table and also in
the tables below
Parameter for dealing with OOV issue
In dealing with the OOV issue, we choose top-k simi-
lar words for an unknown word, as in Formula (16). In
order to choose the optimal value for k, we use the linear
combination method in Eq. (17) with the other parame-
ters ? = 0.1, ? = 0.1 and ? = 0.8, and test our active
learning method against the GRO13 dataset, as changing
the k value from 5 to 25. We summarize the deficiency of
the active learning method using the different k values in
Table 6. As the result, we choose k=25 for the remaining
experiments.
Evaluation of active learning methods for event extraction
Active learningmethods using informativity estimation
In the following evaluations, we show the learning curves
and deficiencies of the event extraction system TEES
under different sample selection strategies against the
dataset of GRO13, CG13 and GE13 task. The active
learning methods use only the informativity estimation,
but not the additional features such as incorporation of
event participants and dealing with OOV issue, which will
be discussed in the next section.
We compare the proposed active learning method with
other sample selection strategies, including random selec-
tion, and entropy-based [17], and Gibbs error [33] based,
as well as a conventional committee based active learning
methods. We use the random selection as the baseline for
deficiency calculation. Each experiment has ten rounds,
where in each round, 10 % of the original training data
are added for training the TEES system. The initial model
of the TEES system before the first round is trained only
on the development dataset. Note that the test data of
Table 6 Parameter optimization results
Method GRO13
RS_Average 1
LC_(?=0.1,?=0.1,?=0.8), k = 5 0.611
LC_(?=0.1,?=0.1,?=0.8),k = 10 0.600
LC_(?=0.1,?=0.1,?=0.8),k = 15 0.617
LC_(?=0.1,?=0.1,?=0.8),k = 20 0.628
LC_(?=0.1,?=0.1,?=0.8),k = 25 0.563
The deficiencies of active learning method using different factor against the GRO13
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 9 of 18
Table 7 Deficiencies of sample selection methods for event
extraction against the GRO13, CG13 and GE13 datasets
Method GRO13 CG13 GE13
RS_Average 1 1 1
AL(Entropy) 1.017 1.226 0.854
AL(GibbsError) 1.039 0.993 0.850
AL(ConventionalCommittee_PAS) 0.830 0.589 0.439
AL(ConventionalCommittee_Unigram) 0.832 0.788 0.263
AL(Informativity_PAS) 0.845 0.581 0.872
AL(Informativity_Unigram) 0.760 0.768 0.139
each dataset is fixed. The followings are considered for the
selection of additional 10 % training data in each round:
 Random selection: We randomly split the training
data into 10 bins in advance, and during the training
phase in each round, one bin is randomly chosen. We
report the averaged performance of random selection
for ten times (hereafter referred as RS_Average).
 Entropy-based active learning: We calculate the
entropy of each document based on (30), sort
documents by their entropy values and feed from
documents with top values to those with bottom
values as training data. (designated as AL(Entropy))
 Gibbs error based active learning: We calculate the
Gibbs error of each document based on (34), sort
documents by their Gibbs error values and select the
documents with top values as training data.
(designated as AL(GibbsError))
 Proposed active learning: We evaluate the method
using either unigrams (Unigram) or
predicate-argument relations (PAS). The resultant
method is referred as AL(Informativity_Unigram)
and AL(Informativity_PAS), respectively.
 Conventional committee-based active learning: We
evaluate the committee based method based on (22),
using the confidence score produced by TEES. We
estimate the informativity using either unigrams
(Unigram) or predicate-argument relations (PAS)
for the proposed statistical method. The resultant
method is referred as AL(Conventional
Committee_Unigram) and AL(Conventional
Committee_PAS), respectively.
We first apply those methods to the dataset of GRO13
[19] and measure the performance change of the TEES
system with the incremental feed of the training data. We
summarize the deficiency for each method in Table 7.
The proposed active learning methods and the con-
ventional committee-based methods achieve deficiency
value of less than 1, while the entropy and Gibbs error
method achieve a deficiency higher than 1, suggesting
that the entropy and Gibbs error methods do not per-
form better than that of random selection. Particularly,
the AL(Informativity_Unigram) method achieves the best
deficiency of 0.760, while the corresponding conventional
committee based method achieves the performance of
0.832 in AL(ConventionalCommittee_Unigram), which
is an 8.65 % improvement for the informativity based
method over that of conventional committee-based
method. However, when using the PAS model, the
25
27
29
31
33
35
37
39
41
43
45
0 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
F-
m
ea
su
re
 (
%
)
Percentage of selected documents
Learning curve of TEES with active learning of PAS model in GRO13'
RS_average
AL(Entropy)
AL(GibbsError)
AL(ConventionalCommittee
_PAS)
AL(Informativity_PAS)
Fig. 2 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and
random selection against GRO13 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method
(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity
method (Informativity), as well as the random selection (RS), when tested against the GRO13 task dataset. The active learning method uses the
predicate-argument relation (PAS) model
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 10 of 18
25
27
29
31
33
35
37
39
41
43
45
0 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
F-
m
ea
su
re
 (
%
)
Percentage of selected documents
Learning curve of TEES with active learning of unigram model in GRO13' 
RS_average
AL(Entropy)
AL(GibbsError)
AL(ConventionalCommittee
_Unigram)
AL(Informativity_Unigram)
Fig. 3 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and
random selection against GRO13 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method
(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity
method (Informativity), as well as the random selection (RS), when tested against the GRO13 task dataset. The active learning method uses the
unigram model
AL(Informativity_PAS) achieves deficiency of 0.845,
which is 1.78 % worse than that of the committee-based
method, whose deficiency is 0.830. In addition, when
comparing the performance of the methods using the
PAS and unigram, we notice that using the unigram,
the proposed informativity method shows an 10.1 %
improvement over that using PAS model, yet this is not
evident in the committee-based method. The results sug-
gest that the proposed informativity method performs
best when using the unigram model in the GRO13
dataset. We then plot the learning curves for each method
in Figs. 2 and 3. In Fig. 3, the AL(Informativity_Unigram)
32
37
42
47
52
57
0 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
F-
m
ea
su
re
 (
%
)
Percentage of selected documents
Learning curve of TEES with active learning of PAS model in CG13' 
RS_average
AL(Entropy)
AL(GibbsError)
AL(ConventionalCommittee
_PAS)
AL(Informativity_PAS)
Fig. 4 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and
random selection against CG13 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method
(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity
method (Informativity), as well as the random selection (RS), when tested against the CG13 task dataset. The active learning method uses the
predicate-argument relation (PAS) model
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 11 of 18
32
37
42
47
52
57
0 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
F-
m
ea
su
re
 (
%
)
Percentage of selected documents
Learning curve of TEES with active learning of unigram model in CG13' 
RS_average
AL(Entropy)
AL(GibbsError)
AL(ConventionalCommittee
_Unigram)
AL(Informativity_Unigram)
Fig. 5 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method,
random selection against CG13 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method
(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity
method (Informativity), as well as the random selection (RS), when tested against the CG13 task dataset. The active learning method uses the
unigram model
method is consistently performing over the other meth-
ods after 50 % of the documents are selected, which also
explains the results in the comparison of deficiency val-
ues. In addition, in the comparison of average number
of instances per ontological concept provided in [41], the
GRO13 dataset have 13 instances per concept, while such
value for GE13 dataset is 82. This also suggests that in
datasets such as GRO13 whose document annotation
may not be abundant, the active learning method using
the unigram may perform better than the PAS model.
However, the experiment result in the GRO13 dataset
indicates that the proposed informativity based active
learning method with unigram model can show better
performance than the conventional committee-based, the
entropy based and the Gibbs error based active learning
methods.
We then carry out a similar experiment using the CG13
dataset. We summarize the deficiency for each method
40
45
50
55
60
0 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
F-
m
ea
su
re
 (
%
)
Percentage of selected documents
Learning curve of TEES with active learning of PAS model in GE13' 
RS_average
AL(Entropy)
AL(GibbsError)
AL(ConventionalCommittee
_PAS)
AL(Informativity_PAS)
Fig. 6 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and
random selection against GE13 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method
(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity
method (Informativity), as well as the random selection (RS), when tested against the GE13 task dataset. The active learning method uses the
predicate-argument relation (PAS) model
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 12 of 18
40
45
50
55
60
0 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
F-
m
ea
su
re
 (
%
)
Percentage of selected documents
Learning curve of TEES with active learning of unigram model in GE13' 
RS_average
AL(Entropy)
AL(GibbsError)
AL(ConventionalCommittee_
Unigram)
AL(Informativity_Unigram)
Fig. 7 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and
random selection against GE13 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method
(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity
method (Informativity), as well as the random selection (RS), when tested against the GE13 task dataset. The active learning method uses the
unigram model
in the Table 7. In this experiment, the Gibbs error based
approach achieves the deficiency value of less than 1, while
the deficiency for the entropy based method is 1.226.
Comparing the PAS and unigram model, the deficiency
values for PAS model are generally better than those of
unigram model. For instance, in the committee-based
method, the percentage of deficiency difference is 25.3 %.
Similarly in the proposed informativity method, there is a
24.3 % change in the deficiency value. This may suggest
that the PAS model may be more suitable for the CG13
dataset. In addition, while comparing the proposed infor-
mativity method and committee-based method, the infor-
mativity method achieves better deficiency value over the
committee-based method. In terms of deficiency differ-
ence, the improvements are 0.020 and 0.008, for PAS
and unigram feature, respectively, which is a less obvious
improvement for the informativity method. However, this
also suggest that the PAS feature may be more sensitive
than that of unigram in the CG13 dataset. Note that one
of the specialties in CG13 dataset is that only a single
25
30
35
40
45
50
0 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
F-
m
ea
su
re
(%
)
Percentage of selected documents
Integration of prediction of named entities into active learning
AL(Informativity_PAS + NE)
AL(Informativity_PAS)
AL(Informativity_Unigram + NE)
AL(Informativity_Unigram)
AL(ConventionalCommittee_PAS
+ NE)
AL(ConventionalCommittee_PAS)
Fig. 8 Integration of named entity recognition into active learning with PAS and n-grams against GRO13 dataset. The learning curves for the TEES
system under the proposed informativity method using predicate-argument relation (PAS) and unigram model, as well as the conventional
committee (ConventionalCommittee) based active learning method as the benchmark. In contrast, each method is integrated with the output from
the named entity recognition result (NE)
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 13 of 18
Table 8 Deficiencies of active learning methods with and
without integrating the prediction of named entities (NE) against
GRO13 dataset
Method GRO13
RS_Average 1
AL(ConventionalCommittee_PAS) 0.830
AL(ConventionalCommittee_PAS + NE) 0.693
AL(Informativity_PAS) 0.845
AL(Informativity_PAS + NE) 0.589
AL(Informativity_Unigram) 0.760
AL(Informativity_Unigram + NE) 0.706
relation type of Equiv is defined. Equiv is a symmetric
and transitive binary relation to identify entity mentions
as being equivalent in the sense of referring to the same
real-world entity [42]. Such relation is not evaluated in
the GRO13 or GE13 dataset. The better performance
of PAS model over unigram model may due to that the
PAS model is more stable for identification of equiva-
lent entity mentions than the unigram model. The learn-
ing curves for the active learning method are plotted in
Figs. 4 and 5.
We extend the aforementioned active learning meth-
ods to the GE13 dataset, and the Table 7 summarize
the deficiency of the methods. In Table 7, all methods
achieve deficiency values less than the random selection.
The method of Gibbs error based approach achieve the
deficiency of 0.850, while the deficiency for the entropy
method is 0.854. The proposed active learning methods
using the unigram shows a more obvious improvement
than that using PAS. For instance, in the committee-
based method, there is an improvement of 40.1 % for
the unigram model over the PAS model. This may sug-
gest that, against the GE13 dataset, the unigram feature
is more suitable for proposed method than that of the
PAS feature. We notice a more obvious improvement for
the unigram model in the informativity method. Partic-
ularly, the best performing AL(Informativity_Unigram)
achieve a deficiency value of 0.139.While the correspond-
ing committee-based method achieve the deficiency of
0.263 in AL(ConventionalCommittee_Unigram). We plot
the learning curves in Figs. 6 and 7. In the Fig. 7,
the active learning method using unigram generally
shows obvious improvement over the baseline of ran-
dom selection method, yet the active learning method
using PAS show less significant improvement over the
baseline method. This may due to the fact that the
ontology defined in GE13 task is generally less com-
plicated than that in GRO13 and CG13. In addition,
the document annotation in the GE13 dataset may be
abundant, as the average number of instances per onto-
logical concept in GE13 dataset is 82, above six times
more than that of GRO13 dataset [41]. Given the dataset
with less complicated ontological concepts and abun-
dant training data of document annotation, the unigram
model may show obvious improvement for active learning
methods.
Active learningmethods using additional features
Incorporation of event participants We evaluate the
active learning method that is incorporated with the
recognition of gene/protein names for event extraction,
as illustrated in Formula (14). We show the performance
25
30
35
40
45
50
0 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
F-
m
ea
su
re
(%
)
Percentage of selected documents
Integration of word vector into active learning
AL(Informativity_WordVec)
AL(Informativity_Unigram)
Fig. 9 Evaluation of incorporation of the word vector method into active learning with n-grams against GRO13 dataset. The word vector is applied
into the active learning method to solve the out-of-vocabulary (OOV) issue that exists in the unigram model. For the unknown unigram, its score is
replaced by the top-25 most similar known unigrams
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 14 of 18
Table 9 Deficiencies of using word vector to solve the
Out-Of-Vocabulary(OOV) issue for the unigram model
Method GRO13
RS_Average 1
AL(Informativity_Unigram) 0.790
AL(Informativity_WordVec) 0.769
of the TEES system, with active learning method that
is either with or without using the gene/protein names.
Such experiment is carried out using the GRO13 dataset.
The experiment results are plotted in Fig. 8 and we
summarize the deficiency values in the Table 8. In
the Table 8, the incorporation of gene/protein names
shows positive effects towards the active learning method
for event extraction, for both of bag of n-gram or
PAS method. By using the gene/protein names, the
deficiency for the active learning method using PAS
is further improved from 0.845 to 0.589, which is a
30.3 % improvement. Yet in the unigram model of
the informativity method, the improvement is rather
less significant of 7.1 %, which may suggest that some
named entities are already captured as n-grams, thus
redundant.
In addition, we notice similar improvement of the con-
ventional committee-based method by incorporating the
information of event participants into the part of sta-
tistical informativity estimation, from 0.830 (i.e. Con-
ventionalCommittee_PAS) to 0.693 (i.e. Conventional-
Committee_PAS + NE), a 16.5 % improvement. How-
ever, this improvement is significantly less than that for
our proposed method, which may indicate that the con-
fidence scores of the TEES used by the conventional
committee-based method hamper the effects of event
participants.
Dealing with OOV issue with word similarity The
n-gram model is based on the registered n-grams that
occur in the training data, which has the issue of Out-
of-Vocabulary (OOV) words. We solve this by using
the word2vec toolkit to find top-k words that are clos-
est to a given OOV word in the test data and to use
their weights to estimate the weight of the OOV word.
The results of evaluating the word vector incorporation
against the GRO13 dataset are plotted in Fig. 9, and the
deficiency is summarized in Table 9. Note that the experi-
ments about OOV word handling are carried out only for
events, excluding relations, observing that the relations
of the BioNLP-ST13 tasks are little affected by the OOV
issue, since they are not associated with trigger words.
By using the word similarity, the n-gram model method
is further improved, as the deficiency of n-gram model
goes from 0.790 to 0.769, an improvement of 2.66 %.
The rather less significant improvement may suggest that
such OOV issue is rather not prevalent in the GRO13
dataset.
Linear combination of n-gram and predicate-structure
relation features
Lastly, we linearly combine the proposed n-gram and
predicate-structure relation features for the active learn-
ing, as expressed in Eq. (17), and to understand which of
the active learning methods proposed in this paper are
more important towards the overall performance.
We use four weight combinations of (?=0.8, ?=0.1,
?=0.1), (?=0.1, ?=0.8, ?=0.1), and (?=0.1, ?=0.1, ?=0.8),
as well as the equal distribution of weight (?=0.33, ?=0.33,
?=0.33). The method of AL(Informativity_PAS + NE)
Fig. 10 Evaluation of linear combination of active learning methods against GRO13 dataset. The active learning modules are assigned with different
weights and combined linearly. Different weight assignment strategies are compared
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 15 of 18
Table 10 Deficiencies of linear combination of active learning
methods
Method GRO13
RS_Average 1
AL(Informativity_PAS + NE) 0.589
LC_(?=0.33,?=0.33,?=0.33) 0.740
LC_(?=0.8,?=0.1,?=0.1) 0.772
LC_(?=0.1,?=0.8,?=0.1) 0.752
LC_(?=0.1,?=0.1,?=0.8) 0.563
LC_(?=0,?=0,?=1) 0.583
is used as the benchmark, as it is the best perform-
ing method in the previous experiments in the GRO13
dataset. Note that the AL(Informativity_PAS + NE) cor-
responds to the weight combination of (?=0, ?=1, ?=1).
Additionally, we also use the benchmark of only using the
named entity for the active learning, i.e the weight com-
bination of (?=0, ?=0, ?=1), to check if simply using the
total number of recognized named entities be sufficient
for the active learning method.
The results of comparison are plotted in Fig. 10, and
we summarize the deficiency values in Table 10. Overall,
the weight combination of (?=0.1, ?=0.1, ?=0.8) shows
the best performance (deficiency 0.563). Compared to
PAS or unigram-based statistics, the incorporation of
event participants has the most effect on the best per-
formance. Note, however, that the model of using only
the event participants, i.e., the weight combination of
(?=0, ?=0, ?=1), achieves the deficiency of 0.583, higher
than the best deficiency, which indicates that the PAS
or n-gram based statistics are complementary to event
participants.
Evaluation of active learning method for NER task
We apply the active learning method into NER task as
expressed in Eq. (18), and follow the similar experiment
design. Each sample selectionmethod starts with the same
held-out labeled development dataset for model initializa-
tion and a pool of unlabeled training dataset for selection.
In each round, 10 % of the unlabeled documents in the
training dataset are selected by different sample selec-
tion strategies. For evaluation, we report the performance
of NER system trained with the selected training docu-
ment in each round, against the same held-out test dataset
following the official evaluation procedure.
The sample selection strategies are as follows:
 Random selection: We randomly split the training
dataset into 10 bins in advance, one bin is randomly
chosen in each round. Following 10-fold cross
validation, we report the averaged performance in
each round. (hereafter referred to as RS_Average)
 Entropy-based active learning: The entropy of
documents are calculated, and select documents by
their entropy values, from the top to bottom.
(designated as AL(Entropy) )
 Maximum Gibbs Error based active learning: Similar
to the entropy-based method, but uses the Gibbs
error, as introduced in [33]. (designated as
AL(GibbsError) )
 Proposed active learning method using informativity
scoring only: Use the aforementioned system in
0.64
0.69
0.74
0.79
0.84
0 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
F-
m
ea
su
re
Percentage of selected documents
Learning Curve of Gimli with BioCreative dataset 
RS_Average
AL(Entropy)
AL(GibbsError)
AL(ConventionalCommittee)
AL(Informativity)
Fig. 11 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and
random selection against BioCreative dataset. The learning curves for the Gimli system under active learning (AL), using the Gibbs error based
method (Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed
informativity method (Informativity), as well as the random selection (RS), when tested against the BioCreative task dataset
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 16 of 18
Table 11 Deficiencies of sample selection methods against the
BioCreative and CoNLL datasets
Method BioCreative CoNLL
RS_Average 1 1
AL(Entropy) 1.171 0.737
AL(GibbsError) 1.045 0.885
AL(ConventionalCommittee) 0.684 0.763
AL(Informativity) 0.514 0.575
Eq. (18), and selects documents based on their
informativity scores. (designated as
AL(Informativity))
 Conventional committee-based active learning: We
evaluate the committee based method based on (22),
using the confidence score produced by NER system.
The resultant method is referred as AL(Conventional
Committee).
We applied these methods to the BioCreative dataset
and plotted the learning curve of Gimli in Fig. 11,
and summarized their deficiency values in Table 11. In
Fig. 11, the proposed active learning method show steady
improvement over the other methods in most rounds.
Based on the deficiency comparison in Table 11, the
proposed method achieved a deficiency value of 0.514,
while the deficiency for the conventional committee based
method is 0.684.
We carried out similar experiments with the CoNLL
dataset, and the learning curves are plotted in Fig. 12,
and the deficiencies are compared in Table 11. In Fig. 12,
the proposed active learning method outperforms the
other methods; and in terms of deficiency, the proposed
method achieves 0.575 in the deficiency, a nearly 42 %
improvement over the random selection. In contrast, the
benchmark of Entropy and Gibbs error based approaches
also are shows deficiency value of less than 1, yet their
improvement over the random selection is nearly 26 %
and 11 %. The deficiency for the conventional com-
mittee based method is 0.763. The experiment results
in the BioCreative and CoNLL datasets indicate that
the proposed informativity based method can show bet-
ter performance than the conventional committee-based
method, as well as the Entropy and Gibbs error based
methods.
Conclusions
In this study, we proposed a novel active learning method
for ontological event extraction, which is more complex
than the simple PPI extraction. Our method measures
the collective informativity for unlabeled documents,
in terms of the potential likelihood of biological events
unrecognizable for the event extraction system. We eval-
uated the proposed method against the BioNLP Shared
Tasks datasets, and showed that our method can achieve
better performance than other previous methods, includ-
ing entropy and Gibbs error based methods and the
conventional committee-based method. In addition, the
incorporation of named entity recognition into the active
learning for event extraction and the unknown word
handling further improved the active learning method.
Finally, we adapted the active learningmethod into named
entity recognition tasks and showed that the method also
improved the document selection for manual annotation
of named entities.
0.78
0.79
0.8
0.81
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
F-
m
ea
su
re
Percentage of selected documents
Learning Curve of Stanford NER with CoNLL dataset
RS_Average
AL(Entropy)
AL(GibbsError)
AL(ConventionalCommittee)
AL(Informativity)
Fig. 12 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and
random selection against CoNLL dataset. The learning curves for the Gimli system under active learning (AL), using the Gibbs error based method
(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity
method (Informativity), as well as the random selection (RS), when tested against the CoNLL task dataset
Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 17 of 18
Competing interests
The authors declare that they have no competing interests.
Authors contributions
XH conceived the study, designed and implemented the system, carried out
the evaluations and drafted the manuscript. JJK and CKK motivated the study
and revised the manuscript. All authors read and approved the final
manuscript.
Acknowledgements
This research was partially supported by Ministry of Education, Singapore,
grant (MOE2014-T2-2-023).
Author details
1School of Computer Engineering, Nanyang Technological University, 50
Nanyang Avenue, 639798 Singapore, Singapore. 2Data Analytics Department,
Institute for Infocomm Research, 1 Fusionopolis Way, 138632 Singapore,
Singapore.
Received: 21 February 2015 Accepted: 28 March 2016
RESEARCH Open Access
Inferring gene-to-phenotype and gene-to-
disease relationships at Mouse Genome
Informatics: challenges and solutions
Susan M. Bello* , Janan T. Eppig and the MGI Software Group
Abstract
Background: Inferring gene-to-phenotype and gene-to-human disease model relationships from annotated mouse
phenotypes and disease associations is critical when researching gene function and identifying candidate disease
genes. Filtering the various kinds of genotypes to determine which phenotypes are caused by a mutation in a
particular gene can be a laborious and time-consuming process.
Methods: At Mouse Genome Informatics (MGI, www.informatics.jax.org), we have developed a gene annotation
derivation algorithm that computes gene-to-phenotype and gene-to-disease annotations from our existing
corpus of annotations to genotypes. This algorithm differentiates between simple genotypes with causative
mutations in a single gene and more complex genotypes where mutations in multiple genes may contribute
to the phenotype. As part of the process, alleles functioning as tools (e.g., reporters, recombinases) are filtered out.
Results: Using this algorithm derived gene-to-phenotype and gene-to-disease annotations were created for 16,000
and 2100 mouse markers, respectively, starting from over 57,900 and 4800 genotypes with at least one phenotype and
disease annotation, respectively.
Conclusions: Implementation of this algorithm provides consistent and accurate gene annotations across MGI and
provides a vital time-savings relative to manual annotation by curators.
Keywords: Phenotype, Genotype, Disease, Mouse, Annotation
Background
Genetic mutations in mouse models have proven a
valuable tool in investigating gene function and facili-
tating research into human disease. The phenotypes as-
sociated with these mutations in mice occur in the
context of other defined or undefined mutations in
their genome. To determine if a phenotype is caused by
a mutation in a specific gene, providing insight into
gene function, the impact of each allele in the genotype
needs to be evaluated. Doing this manually is a labori-
ous and time-consuming process. Intensely researched
genes may have dozens of alleles each with multiple
genotypes. The mouse gene Pax6 (MGI:97490) alone
has 53 mutant alleles present in some 150 mouse geno-
types with phenotype annotations in Mouse Genome
Informatics (MGI, as of 12/29/2015). Only a fraction of
these reported phenotypes are caused solely by the mu-
tation(s) in Pax6.
MGI (www.informatics.jax.org) provides gold-standard
annotations to describe mouse models in the context of
both the known alleles and strain backgrounds of the
mice [1]. In MGI, phenotype and disease annotations are
ascribed to a genetic representation (allele pairs and
strain background) of the mice that displayed the pheno-
type. Sophisticated genetic engineering techniques have
allowed for the production of multi-genic models with
spatiotemporal control of gene expression and the intro-
duction of multi-color reporters. These increasingly
complex models may include both causative mutations
and non-causative transgenic tools [2]. To relate pheno-
type and disease annotations made to a genotype in
MGI with the gene, genomic marker, or transgene con-
taining the causative mutation, non-causative markers,
such as transgenic tools (e.g., recombinases and* Correspondence: susan.bello@jax.org
Mouse Genome Informatics, The Jackson Laboratory, Bar Harbor, ME 04609, USA
© 2016 Bello and Eppig. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Bello and Eppig Journal of Biomedical Semantics  (2016) 7:14 
DOI 10.1186/s13326-016-0054-4
reporters), need to be computationally excluded from
consideration. For example, mice carrying an inducible
knock-in of a mutant form of mouse Kcnj11 in the
Gt(ROSA)26Sor locus and a transgene expressing cre
recombinase in pancreatic cells, Tg(Ins2-cre)23Herr
(genotype MGI:4430413), are annotated to the Mam-
malian Phenotype ontology (MP) [3] term decreased
insulin secretion (MP:0003059) and are a model of per-
manent neonatal diabetes mellitus (OMIM:606176) [4].
The phenotype and disease annotations are correctly asso-
ciated with Kcnj11. However, the annotations should not
be linked with the cre recombinase transgene or
Gt(ROSA)26Sor since neither directly causes the phe-
notypes or disease displayed by the mice.
MGI is implementing improvements throughout the
database to enhance the ability of users to evaluate the
function of genes. As part of this, phenotype and disease
associations at the level of the gene are now being pre-
sented (see below) in multiple locations in the MGI web-
site. The gene-level associations give users an overview
of the phenotypes and diseases associated with a gene
that can be challenging to decipher from detailed model
annotations. For both phenotypes and disease, creating a
gene-level annotation implies that mutations in this gene
cause the associated phenotype or disease. Therefore,
the gene-level annotations may be useful to identify can-
didate genes for specific phenotypes and/or diseases. To
create these gene-level associations, we have developed
rules to algorithmically identify and computationally
separate causative mutations from transgenic tools in
complex mouse genotypes.
The first and simplest implementation of the rules ex-
cluded all complex genotypes and removed recombinase
and wild-type alleles prior to inferring relationships. The
need to separate causative mutations from transgene
tools can best be illustrated by example. The complex
genotype Apoetm1Unc/ Apoetm1Unc Faslgld/Faslgld on an in-
bred C57BL/6 strain genetic background (MGI:5514345)
is annotated to the human disease Systemic Lupus Ery-
thematosus, SLE (OMIM:152700) [5]. Inferring a causal
relationship between Apoe and/or Fasl and SLE may or
may not be correct, since it is unclear whether one or
both genes are responsible for the observed phenotype.
For complex genotypes such as this one, the algorithm
does not derive any gene annotations. Conversely,
Smotm1Amc/Smotm2Amc Isl1tm1(cre)Sev/Isl1+ mice on a mixed
129 strain genetic background (MGI:3689403) are anno-
tated to the phenotype perinatal lethality (MP:0002081)
[6]. The Isl1 recombinase allele is present to drive dele-
tion of the loxP-flanked Smo allele in the cardiovascular
system; thus, we do not want to associate the perinatal
lethality phenotype with Isl1. As we can clearly identify
the non-causative allele and distill this genotype to
alleles associated to a single gene, we derive a
relationship between the phenotype perinatal lethality
and the gene Smo.
Other databases presenting phenotype and disease an-
notations for model organisms also have to decide when
an annotation to a model can used to infer information
about gene function. For example, the Zebrafish Model
Organism Database (ZFIN, www.zfin.org, [7]) annotates
phenotypes to a fish line that includes the alleles, trans-
genes and/or morpholinos used in an experimental co-
hort. Each allele and morpholino has an asserted
relationship to a gene. Gene level annotations are then
inferred for lines where only 1 asserted gene relationship
exists (Y. Bradford, personal communication). Gene level
annotations are not inferred for fish with more than one
asserted gene relationship or for fish expressing non-
reporter transgenes. This is similar to the early stages of
the MGI algorithm. A key difference between mouse
and zebrafish models, for the purpose of inferring gene
annotations, is the widespread use of knock-in mutations
in mouse where asserting the gene to allele relationship
is less straightforward.
In contrast to the restrictive approach taken by ZFIN
and MGI, the Monarch Initiative (monarchinitiati-
ve.org, [8]), which integrates data from both MGI and
ZFIN as well as many other sources, infers gene anno-
tations for all genes in a model. Thus, in the example
above (Apoetm1Unc/ Apoetm1Unc Faslgld/Faslgld) gene an-
notations would be inferred for both Apoe and Fasl
(M. Brush, personal communication). This approach
maximizes the number of gene-to-phenotype annota-
tions but means the user will need to evaluate the re-
sults to remove false positive associations.
In the current implementation, presented below, the
algorithm we have developed excludes additional trans-
genic tools, accounts for the introduction of expressed
genes in alleles, and deals with multi-genic mutations.
This approach increases the number of derived gene an-
notations, while attempting to reduce both the number
of false positive and false negative annotations. While
the precise implementation would not be of use to other
databases the logic behind the algorithm should be
transferable.
Gene annotation derivation rules
Refinement of the derivation rules to eliminate add-
itional types of transgenic tools has been an iterative
process. Various changes to the MGI database schema
have facilitated the identification and removal of many
types of transgenic tools and non-causative marker asso-
ciations. Throughout this process we have worked to
minimize the number of false positive associations. The
overall goal of these rules is to eliminate transgenic tools
alleles and then infer gene, multi-genic marker, or trans-
gene relationships from genotypes with only a single
Bello and Eppig Journal of Biomedical Semantics  (2016) 7:14 Page 2 of 11
remaining associated locus. Genotypes with multiple as-
sociated loci are not used to infer gene relationships, with a
few exceptions (see below). Recent re-implementation of
these rules in a consistent manner across all MGI products
has improved the gene annotation data quality at the dis-
play level and allowed us to make this data set available for
export.
Details of the annotation derivation rules
In the application of the derivation rules, genotypes are
processed in a step-by-step fashion (see Fig. 1). First,
the number of genetic loci associated with all alleles in
the genotype is determined (Fig. 1, box 1). Genetic loci
include: genes within the mutation region, genes
expressed by the allele, transgene markers, and pheno-
typic markers. For example, the alleles Apptm1Dbo,
Tg(tetO-Notch4*)1Rwng, and Del(7Coro1a-Spn)1Dolm
(MGI:2136847, MGI:4431198, MGI:5569506 respectively)
are associated with one, two, and forty loci, respectively.
The two loci associated with Tg(tetO-Notch4*)1Rwng are
the transgene itself and the expressed mouse gene, Notch4.
The forty loci associated with Del(7Coro1a-Spn)1Dolm
include the deletion region itself (recorded in MGI as
a single, unique genetic marker) and all thirty nine
endogenous mouse genes overlapping the deletion region.
Gene-to-phenotype and gene-to-disease annotations can
then be derived for the genes in nearly all genotypes with
a single associated genetic locus (see docking sites below
for the exception).
For genotypes including more than one locus, such as
those described above, non-causative alleles are identi-
fied and computationally excluded from consideration.
Non-causative allele types in the algorithm include:
transgenic transactivator alleles, transgenic reporter al-
leles, knock-in and transgenic recombinase alleles, and
wild-type alleles. Since many knock-in transactivator and
reporter alleles may also be knock-out alleles that are
causative for a phenotype, only transgenic alleles of these
types are excluded. For recombinase alleles, curation in
MGI distinguishes between conditional genotypes,
where these alleles function as a recombinase, and non-
conditional genotypes, where these alleles may be
causative; therefore, both transgenic and knock-in re-
combinase alleles may be eliminated when the genotype
is conditional. When the genotype is not conditional,
recombinase alleles are retained. For a recombinase or
transactivator allele to be excluded, it must express only
a single gene. In cases where another gene is expressed,
the allele is retained. For example the recombinase al-
lele Tg(Tyr-cre/ERT2)1Lru (MGI:3617509) is excluded
at this stage, so no derived annotation to the transgene
is computed as a result of this allele. But the allele
Tg(Tyr-cre/ERT,-Hras1*,-Trap1a)10BJvde
(MGI:4354013) is retained, as it expresses both Hras1
and Trap1a in addition to cre. Additional rules de-
scribed below address whether and how to derive anno-
tations to those genes. Motifs (ERT2, ERT) designed to
alter the expression of cre are not curated as expressed
genes and are therefore ignored by the algorithm.
After excluding non-causative alleles, the number of
remaining loci is determined for each genotype. Gene-
to-phenotype and gene-to-disease annotations are then
derived for genes and genomic markers in genotypes
with a single remaining locus. For genotypes with more
than one remaining locus, further processing is done to
identify additional cases where gene annotations can be
derived. If the genotype is associated with a single multi-
genic marker (e.g., Del(7Coro1a-Spn)1Dolm) and one or
more affected genes located in the region, then annota-
tions are derived for the multi-genic marker and not for
the individual endogenous genes in the region (Fig. 1,
box 4). Genotypes associated with more than one multi-
genic mutation or with a multi-genic marker and any
markers outside the mutation region are excluded and
Fig. 1 Flow chart for the application of gene annotation derivation
rules. One gene*, annotations are derived only for certain cases of
genotypes containing a single gene. See text for additional details.
Transgene+, gene annotations are made to the transgene and an
endogenous mouse gene
Bello and Eppig Journal of Biomedical Semantics  (2016) 7:14 Page 3 of 11
annotations are not derived for any of the genes or gen-
omic markers involved.
The number of inserted expressed genes is then con-
sidered. Inserted expressed genes are genes that have
been introduced into the mouse genome and the gene
product is expressed in one or more tissues of the
mouse. Genotypes with multiple associated markers and
no inserted expressed genes are eliminated. Genotypes
associated with multiple inserted expressed genes are as-
sociated to the transgenic locus only, if there is a single
transgene associated with the genotype and no add-
itional endogenous genes (Fig. 1, box 6). In this case, it
is assumed that the transgene is expressing all of the
inserted expressed genes and that the transgene as a
whole, not the individual expressed genes, is causative
for the phenotypes or diseases annotated to the geno-
type. For these genotypes, transgene-to-phenotype and
transgene-to-disease annotations are derived. Derived
annotations are not created for the inserted expressed
genes. Other genotypes having more than one inserted
expressed gene are excluded and no gene or transgene
annotations are derived.
Genotypes associated with only a single inserted
expressed gene (Fig. 1, box 7) are divided into two
types: those expressing a mouse gene and those ex-
pressing a non-mouse gene. Genotypes associated with
an expressed non-mouse gene are eliminated. No as-
sumption is made that the phenotypes or diseases dis-
played would also be produced if the orthologous
mouse gene had been used instead. Gene-to-phenotype
and gene-to-disease annotations may be derived for a
transgene and also an endogenous mouse gene in two
cases: 1) if the genotype contains only a single transgene
which carries a single inserted expressed mouse gene
(Fig. 1, box 8); 2) if the transgene, inserted expressed
mouse gene, and the single endogenous gene that is the
same as the inserted expressed mouse gene are associated
with the genotype (Fig. 1, box 9). In both cases annota-
tions are derived for both the endogenous mouse gene
and the transgene (Fig. 1, transgene + ).
Three genes (Gt(ROSA)26Sor, Col1a1, Hprt) are com-
monly used, based on examination of alleles in MGI, as
docking sites in mouse to knock-in expressed genes,
frequently under the control of a heterologous pro-
moter sequence. For example, of the 63 alleles of
Col1a1 in MGI with the attribute inserted expressed
sequence, 55 have a construct inserted in the untrans-
lated region based on the molecular description in MGI
(12/7/15). For genotypes associated with a docking site
and a single expressed mouse gene, gene-to-phenotype
and gene-to-disease annotations are derived for the
expressed gene and not for the docking site. There are
no known phenotypes or diseases ascribed to mutations
in Gt(ROSA)26Sor (MGI:104735, [9]). Therefore, no
derived annotations are created for Gt(ROSA)26Sor, even
when there are no associated expressed genes in MGI.
MGI currently only annotates expressed genes with an
ortholog in mouse; therefore, not all Gt(ROSA)26Sor
alleles with an inserted expressed gene have an as-
sociated expressed gene. For example the allele
Gt(ROSA)26Sortm1(gp80,EGFP)Eces (MGI:5004724) expresses
a gene from the Kaposi sarcoma herpes virus that does
not have an ortholog in mouse. The phenotypes displayed
by mice carrying this allele are the result of expression of
the viral gene but as there is no display in MGI for any
gene-to-phenotype annotations for a viral gene with no
mouse ortholog, no derived annotations are created. In-
sertions in Col1a1 (MGI:88467) and Hprt (MGI:96217)
are typically made without altering normal endogenous
gene function. For Col1a1 and Hprt alleles, annotations
are derived for the inserted expressed gene when one is
present. If no expressed genes are present then
annotations are derived for the docking site gene itself
(Fig. 1, box 10).
The final case where gene annotations are derived is
when the inserted expressed mouse gene is identical to
the endogenous gene (Fig. 1, box 11). No gene annota-
tions are created for any remaining genotypes.
Gene annotation derivation examples
To illustrate the function of the derivation algorithm, four
example genotypes have been overlayed on the flow chart
(Fig. 2). For mice hemizygous for Tg(tetO-Notch4*)1Rwng
and Tg(Tek-tTA)1Rwng (genotype MGI:5502689, Fig. 2a),
the transactivator expressing transgene Tg(Tek-tTA)1Rwng
is excluded from consideration. This leaves 2 remaining
genes, Tg(tetO-Notch4*)1Rwng and Notch4. As this leaves a
single transgene marker and a single expressed mouse
gene, gene level annotions are derived for both the
transgene and the expressed mouse gene. For mice homozy-
gous for Prnptm1Cwe and Tg(Prnp*D177N*M128V)A21Rchi
(genotype MGI:3836994, Fig. 2b) there are no non-causative
alleles to remove. The single transgene in this case expresses
the same mouse gene that is mutated by the allele
Prnptm1Cwe leaving the genotype associated with two genes,
mouse Prnp and Tg(Prnp*D177N*M128V)A21Rchi. As this
fits the requirements for the transgene exception (Fig. 2, box
9) annotations are derived for both the endogenous mouse
gene and the transgene. For mice heterozygous for the
deletion Del(7Coro1a-Spn)1Dolm and hemizygous for
the reporter transgene Tg(Drd2-EGFP)S118Gsat (genotype
MGI:5571091, Fig. 2c), the reporter transgene is excluded
from consideration. As the deletion marker is associated
with the 39 genes in the deletion region, this genotype falls
into the Phenotypic mutation class for purposes of the algo-
rithm. Gene annotations are derived for the deletion marker
but not for the 39 genes in the deletion region (Fig. 2c, box
4). Mice heterozygous for Ewsr1tm2(FLI1*)Sblee and hemizygous
Bello and Eppig Journal of Biomedical Semantics  (2016) 7:14 Page 4 of 11
Fig. 2 (See legend on next page.)
Bello and Eppig Journal of Biomedical Semantics  (2016) 7:14 Page 5 of 11
for Tg(CAG-cre/Esr1*)5Amc (genotype MGI:4429149,
Fig. 2d) illustrate a case where gene annotations are not
derived. While two non-causative alleles are removed by
the algorithm, the cre transgene and wild-type allele of
Ewsr1, after processing is complete there are still two
genes associated with the genotype, Ewsr1 and FLI1. As
the gene knocked into Ewsr1 is not a mouse gene this
genotyope is excluded at box 7 in the flow chart. Even if
the expressed gene had been a mouse gene this genotype
would have been excluded as the expressed gene is not
the same as the mutated endogenous gene.
Output of the rules
Once all genotypes with phenotype or disease annota-
tions have been processed by the derivation rules the set
of derived gene annotations are used throughout MGI,
HMDC and MouseMine. As currently implemented, the
rules result in derived gene-to-phenotype and gene-to-
disease annotations for over 16,000 and 2200 mouse
markers, respectively, starting from over 57,000 and
4800 genotypes with at least one phenotype and disease
annotation, respectively (as of 1/4/2016). Of the over
57,000 genotypes processed, almost 40,000 contain only
mutations in a single marker (Table 1). Gene level anno-
tations could be derived from these genotypes using the
simplest possible rule (only derive annotations when
there is one marker associated with the genotype). Use
of the derivation algorithm allows a further almost 8000
genotypes to be processed and marker level annotations
created. This represents an almost 14 % increase in the
number of genotypes contributing phenotype annota-
tions at the marker level. Of the approximately 18,000
multiple marker genotypes, conditional genotypes and
genotypes involving alleles expressing inserted genes are
two important subsets. Conditional genotypes are
primarily processed by removal of recombinase alleles.
There are currently over 7000 genotypes where a recom-
binase allele is removed (Table 2). The ability to include
special and temporal specific phenotypes in the gene
level annotations enhances the overall picture of gene
function MGI provides to users. There are over 3700 al-
leles (knock-in and transgenes) expressing at least one
inserted sequence involved in nearly 4800 genotypes
currently in MGI (as of 12/28/15). Over 2000 of these al-
leles express a mouse gene and may therefore potentially
contribute to gene level annotations. Incorporation of these
overexpression and misexpression induced phenotypes
improves both the overall picture of gene function and the
relation of mouse models of human disease to genes.
There is a potential for the creation of false positive
and false negative annotations by the derivation algo-
rithm. One possible source of false positive annotations
is the use of expressed gene relationships to identify
when an allele is expressing a transcript that may alter
the phenotype. For example, the gene Col1a1 has 64 tar-
geted alleles with the attribute inserted expressed se-
quence of these 58 have an association to an expressed
gene. Of the remaining 6 alleles, 5 are alleles where an
interfering RNA (RNAi) has been inserted into the gene.
Determining how to represent the relationship between
an RNAi expressing allele and the gene targeted by the
RNAi is one of MGIs future projects. During the devel-
opment of the algorithm the use of the inserted
expressed sequence attribute was still in development
so the presence of an association to an expressed gene
was used. We are reviewing the possibility of changing
the algorithm to use the presence of the inserted
expressed attribute instead of the presence of an
expressed gene association, as this would improve our
handling of these cases.
(See figure on previous page.)
Fig. 2 Overlay of specific genotype examples on the flow chart of the gene annotation derivation rules. a Processing of a genotype that results
in annotations to a transgene and endogenous mouse gene. b Processing of a genotype that fits the transgene exception rule, where the
transgene expresses a mouse gene and the same endogenous mouse gene is mutated in the mice. c Processing of a genotype with a reporter
transgene and phenotypic mutation affecting multiple genes. d Processing of a conditional genotype where no gene annotations can be derived
Table 1 Number of genotype and gene annotations processed
by the derivation algorithm
Genotypes with MP and/
or OMIM annotations
Number of genotypes
(percent of total, 1/4/2016)
Number of
genes
Total 57,920
With Derived Gene
Annotations
47,869 (82.6 %) 16,044
One Marker Genotypes
(Fig. 1, box 1)
39,873 (68.9 %) 14,074
Resolved Multiple Marker
Genotypes
7996 (13.8 %) 3870 (1970
novel markers)
Table 2 Breakdown of resolved multiple marker genotypes.
These numbers only include genotypes with MP or OMIM
annotations that have more than 1 marker
Non-causative alleles in
genotypes
Number of genotypes
processed (as of 1/4/2016)
Number
Alleles
Recombinase allelesa 7,015 936
Reporter transgenes 256 157
Transactivator transgenes 282 84
Wild-type alleles 5,371 1,577
aOnly counting recombinase alleles in conditional genotypes which have
MP/OMIM annotations
Bello and Eppig Journal of Biomedical Semantics  (2016) 7:14 Page 6 of 11
One possible source of false negative annotations is
the limitation of docking site alleles to only Col1a1,
Hprt and Gt(ROSA)26Sor. For example, annotations
from the genotype MGI:5544092 could be associated
with the mouse gene Edn2 if the marker for the inter-
genic insertion site in the allele Igs1tm11(CAG-Bgeo,-Edn2)Nat
was excluded from consideration. Instead of expanding
the list of markers used for docking sites, we are explor-
ing implementation of a Docking Site attribute that
could be applied to specific alleles. This would avoid the
need to modify the algorithm when new docking sites
are encountered but would require back annotation of
existing alleles. Another source of false negative annota-
tions is the use of reporter genes that are a mouse gene
or with an ortholog in mouse. For example, there are 63
knock-in alleles that use the mouse gene Tyr as a coat
color reporter. Other than the pigmentation phenotype,
phenotypes in these mice are the result of the mutated
endogenous locus and not due to the expression of Tyr.
However, using the current algorithm gene annotations
are not derived for any of the annotated phenotypes.
Correcting these would require modifying the algorithm
to both ignore Tyr and teasing apart the phenotypes due
to the reporter from those due to the mutated endogen-
ous locus.
Impact of MGI improvements
The development of these rules has relied heavily on the
implementation of other database improvements in
MGI. For example, the introduction of allele attributes
allowed a distinction to be made between reporter trans-
genes that express only a reporter and transgenes that
express a reporter and some other gene. The attributes
were introduced as part of a restructuring of allele types
into generation method and attributes. Attributes in-
clude both changes to the endogenous gene function
(null/knockout, hypomorph) and characteristics of the
inserted sequence (reporter, recombinase). Some attri-
butes may apply to either the endogenous gene or the
inserted sequence (hypomorph, modified isoform). An
allele may have zero to many attributes but only one
generation method. Certain attributes were then incor-
porated into the rules. These attributes include: reporter,
recombinase, transactivator, and inserted expressed se-
quence. For example, exclusion of a reporter transgene
requires the allele to have the generation method trans-
genic and the attribute reporter but not the attribute
inserted expressed sequence. Therefore, the reporter
transgene Tg(Cspg4-DsRed.T1)1Akik (MGI:3796063)
that has only the attribute reporter is excluded as a
non-causative allele. However, the reporter transgene
Tg(CAG-Bmpr1a*,-lacZ)1Nobs (MGI:5473821) has mul-
tiple attributes including reporter and inserted expressed
sequence and is retained.
The recent introduction of formalized data associa-
tions between transgenic and knock-in alleles and the
genes expressed by these alleles has also been incorpo-
rated into the rules. MGI now annotates alleles express-
ing either a mouse gene or gene with a mouse ortholog
to the gene being expressed. Alleles expressing inserted
genes are then displayed on both the detail page for the
endogenous locus where the insertion occurred and on
the detail page for the mouse gene or mouse ortholog of
the inserted gene being expressed. The rules make use
of these associations to avoid assigning phenotypes to
the endogenous gene in cases where an inserted expressed
gene may be causative. They also allow annotations for
phenotypes and diseases caused by transgenes expressing
a mouse gene to be derived for the expressed mouse
gene. For example, phenotypes for the knock-in allele
Ctnnb1tm1(Nfkbia)Rsu (MGI:3039783) may be the result of
loss of expression of Ctnnb1 or the expression of Nfkbia
and therefore no derived annotations are created. How-
ever, phenotype and disease annotations for the transgene
Tg(Prnp*D177N*M128V)A21Rchi (MGI:3836986) are as-
sumed to be the result of the expression of the mouse
Prnp gene and derived annotations may be created for
both the transgene and the expressed mouse gene.
Use of the derived annotations in MGI
Implementation of the annotation derivation rules de-
scribed here has improved both searching and display of
gene-to-phenotype and gene-to-disease annotations in
MGI. Gene level annotations are used on multiple dis-
plays and by multiple search tools in MGI. These dis-
plays and tools provide users with different ways to
access, group, and filter the data. Regardless of how the
user accesses the data, consistent results sets are now
returned when searching for genes by a phenotype or
disease.
One way a user may access the derived annotations
for a gene or set of genes is using the Human-Mouse:
Disease Connection (HMDC, www.diseasemodels.org,
Fig. 3). In the HMDC, searches for mouse data are re-
stricted to only the derived gene-to-phenotype and
gene-to-disease annotations. In the results, users may
also access the set of genotype annotations used to gen-
erate the gene annotations, but multi-genic genotypes
are excluded from the display. In MGI, the display of a
mouse gene on a disease detail page is based both on
the derived gene-to-disease annotations and on orthol-
ogy relationships to known human disease genes. A
gene that has both a derived gene-to-disease annotation
and is orthologous to a known human disease gene is dis-
played in the human and mouse section of the page.
Those without an orthology relationship but with a de-
rived annotation are shown in the mouse only section. A
similar division is made on the all models page for a
Bello and Eppig Journal of Biomedical Semantics  (2016) 7:14 Page 7 of 11
disease, with multi-genic models that have neither gene
orthologs nor derived annotations shown in the additional
complex models section. The derived gene annotations
are also incorporated into the updated design of the MGI
gene detail page. With this modification, users see a sum-
mary graphic of the types of phenotypes caused by mu-
tations in the gene (Fig. 4). On both the gene detail
page and in the HMDC, gene level annotations are
shown at the MP system level. Users may click through
to see the detailed MP terms and associated allele pairs.
This avoids the problem of displaying conflicting phe-
notypes (i.e., increased vs decreased body weight) at the
gene level. From both locations users can access details
Huang et al. Journal of Biomedical Semantics  (2016) 7:24 
DOI 10.1186/s13326-016-0066-0
RESEARCH Open Access
The Non-Coding RNA Ontology (NCRO): a
comprehensive resource for the unification of
non-coding RNA biology
Jingshan Huang1*, Karen Eilbeck2, Barry Smith3, Judith A. Blake4, Dejing Dou5, Weili Huang6,
Darren A. Natale7, Alan Ruttenberg8, Jun Huan9, Michael T. Zimmermann10, Guoqian Jiang10, Yu Lin11,
Bin Wu12, Harrison J. Strachan1, Yongqun He13, Shaojie Zhang14, Xiaowei Wang15, Zixing Liu16,
Glen M. Borchert17 and Ming Tan16
Abstract
In recent years, sequencing technologies have enabled the identification of a wide range of non-coding RNAs
(ncRNAs). Unfortunately, annotation and integration of ncRNA data has lagged behind their identification. Given the
large quantity of information being obtained in this area, there emerges an urgent need to integrate what is being
discovered by a broad range of relevant communities. To this end, the Non-Coding RNA Ontology (NCRO) is being
developed to provide a systematically structured and precisely defined controlled vocabulary for the domain of
ncRNAs, thereby facilitating the discovery, curation, analysis, exchange, and reasoning of data about structures of
ncRNAs, their molecular and cellular functions, and their impacts upon phenotypes. The goal of NCRO is to serve as a
common resource for annotations of diverse research in a way that will significantly enhance integrative and
comparative analysis of the myriad resources currently housed in disparate sources. It is our belief that the NCRO
ontology can perform an important role in the comprehensive unification of ncRNA biology and, indeed, fill a critical
gap in both the Open Biological and Biomedical Ontologies (OBO) Library and the National Center for Biomedical
Ontology (NCBO) BioPortal. Our initial focus is on the ontological representation of small regulatory ncRNAs, which we
see as the first step in providing a resource for the annotation of data about all forms of ncRNAs. The NCRO ontology
is free and open to all users, accessible at: http://purl.obolibrary.org/obo/ncro.owl.
Keywords: Non-coding RNA, Biomedical ontology, Domain ontology, Reference ontology, Ontology development,
Data annotation
Introduction
It is known that non-coding RNAs (ncRNAs), a special
class of functional RNA molecules, will not be translated
into proteins. The chemical identity and first guesses as
to the role of RNA were discussed by Casperson and
Schultz back in 1939, and the first RNA structure was
reported by Alexander Rich in 1956 [1]. Since then, many
types of ncRNAs have been identified, including the now
well-known transfer RNAs (tRNAs) and ribosomal RNAs
*Correspondence: huang@southalabama.edu
1School of Computing, University of South Alabama, Mobile, Alabama
36688-0002, USA
Full list of author information is available at the end of the article
(rRNAs), in addition to the more recently discovered long
non-coding RNAs (lncRNAs), microRNAs (miRNAs), and
so forth. Many ncRNAs perform important roles in the
realization of a wide range of molecular functions as well
as in affecting many different biological and patholog-
ical processes. As such, interest in ncRNA biology has
grown throughout biomedicine, biomedical informatics,
and clinical sciences. In addition, the fertile area of ncRNA
research has been significantly enhanced in recent years
by new sequencing technologies that have generated con-
tinuously increasing quantities of available data. However,
annotation and integration of data about ncRNAs, the
© 2016 Huang et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Huang et al. Journal of Biomedical Semantics  (2016) 7:24 Page 2 of 12
functions regulated by ncRNAs for example, has lagged
behind their identification, resulting in an urgent need for
effective methodologies to bring together discoveries con-
tinuously deriving from different segments of the ncRNA
research community.
Emerging semantic technologies provide computational
methodologies that promote more precise communica-
tion among scientists, enable more effective information
retrieval and integration across diverse resources, and
extend the power of computational technologies to per-
form data exploration, inference, and mining [27]. In
particular, the sorts of reasoning (inference) enabled by
semantic technologies are not available where we are
confined to traditional relational database systems or text-
based search and query. By placing more emphasis on the
semantics (i.e., the intended meaning) of data, semantic
technologies and domain ontologies enable us to estab-
lish more meaningful connections among original data,
thereby helping to bridge gaps in our knowledge. More-
over, semantic data connections are established in a highly
flexible manner that allows these connections to be much
more easily extended  for example when new sorts of
entities are discovered  than is possible using more
traditional approaches.
Among all successful efforts in applying semantic tech-
nologies in the biomedical domain, the Open Biological
and Biomedical Ontologies (OBO) Library [8] is of spe-
cial importance in that it has served as an umbrella
for different ontologies shared across various biologi-
cal, biomedical, and clinical domains. However, there has
until now existed in the OBO Library no comprehensive
ontologies specifically designed for the ncRNA domain,
although portions of the domain are catalogued in several
orthogonal ontologies. The National Center for Biomedi-
cal Ontology (NCBO) BioPortal [9], a repository of biolog-
ical and biomedical ontologies (short for bio-ontologies),
is another effort in some ways parallel to the OBO Library
but with a broader scope and lower hurdles for admis-
sion. However the BioPortal, too, contains, no compre-
hensive ncRNA ontologies. These observations indicate
that there is an important gap that needs to be filled 
hence the Non-Coding RNA Ontology (NCRO) project.
As the first comprehensive, domain-specific ontology in
the ncRNA field, the NCRO ontology aims to supply
a systematically structured, precisely defined controlled
vocabulary for the ncRNA domain, consisting of a set
of common, standardized terms and relations that will
facilitate the discovery, curation, analysis, exchange, and
reasoning of data about the structures, functions, and
molecular, cellular, organismal, therapeutic, or biotechno-
logical uses of ncRNAs. The NCRO ontology can serve
as a resource for annotating and integrating ncRNA data
produced by diverse communities, thereby significantly
enhancing integrative and comparative analysis of the
myriad resources currently housed in disparate sources.
We believe that the NCRO will help to address a vital
need for the comprehensive unification of ncRNA biol-
ogy. We aim to integrate genomic and sequence-based
annotation with gene expression regulation, secondary
and 3D structure information, protein interactions, and
their inter-relationships. Our initial focus is on the onto-
logical representation of small regulatory ncRNAs, which
we see as the first step in providing a standardized
resource for (1) annotating data about all forms of ncR-
NAs and (2) facilitating knowledge capture in the ncRNA
domain.
The rest of this paper is organized as follows.
Section Related work summarizes state-of-the-art
research in ncRNAs and bio-ontologies; Section
Ontology scope gives an overview of the scope covered
by the NCRO ontology; Section Ontology development
introduces NCRO development principles and proce-
dure; Section NCRO terms, relations, and reasoning
describes NCRO terms and relations, as well as ontology
reasoning; Section Examples in NCRO annotations
presents two examples to demonstrate how NCRO
annotations and ontology reasoning can be performed to
facilitate knowledge capture; finally, Section Conclusions
concludes with future research directions.
Related work
Related work in ncRNA research
Prior research, [1012] for example, has uncovered
numerous ncRNA genes, and recent advances in next gen-
eration sequencing technology have resulted in an even
greater number and faster pace of discovery of ncRNA
genes. In fact, Nature has a whole site dedicated to key
apes in this area [13]. Given the relatively large propor-
tion of the genome dedicated to ncRNA genes, significant
potential exists to explore ncRNAs that may have diverse
biological roles.
Abnormal expression of some ncRNAs is involved
in human disease. For example, alterations of gene-
regulatory ncRNA expression are involved in the
development, progression, and metastases of human can-
cer [14]. When differentially expressed gene-regulatory
ncRNAs play roles in altering target gene expression,
further phenotypic effects can be realized. Differential
expression of such ncRNAs in malignant versus nor-
mal tissue can be exploited as a biomarker used for
diagnosis, prediction of patient outcome, or monitor-
ing the effectiveness of cancer therapeutics. Therefore,
these gene-regulatory ncRNAs are potential therapeu-
tic targets for cancer therapy. In recent years serious
attempts have been made to effectively deliver ncRNA
into tumors in animal models. Some of the attempts have
already shown promising therapeutic efficacy [1517].
Huang et al. Journal of Biomedical Semantics  (2016) 7:24 Page 3 of 12
In RNA interference therapy and drug development,
a first-in-human trial has been conducted in cancer
patients who were administered with lipid nanoparticles
(LNP) formulated siRNA targeting VEGF and
KSP [18].
Aberrant expression of ncRNAs has been associated
with not only cancers but also numerous other dis-
eases, including autism, hearing loss, Alzheimers disease,
Prader-Willi Syndrome, diabetes, and psoriasis [1923].
Tissue-specific miRNAs have been shown to be involved
in cardiovascular, muscular, and neurodegenerative dis-
eases, and pharmaceutical companies are developing new
therapeutic molecules that alter the function or expres-
sion of specific miRNAs for treating these and other
human diseases [24].
Related work in bio-ontologies
There are several pre-existing bio-ontologies that are rel-
evant to the development of an ontology in the domain of
functional non-coding RNA. The RNAOntology (RNAO)
[25] is a reference ontology created to catalogue the
molecular entities composing primary, secondary, and
tertiary components of RNA. The goal of the RNAO
project is to enable integration and analysis of diverse
RNA datasets. The Gene Ontology (GO) [26] is by far the
most successful and widely used bio-ontology, consisting
of three independent sub-ontologies: biological processes,
molecular functions, and cellular components. The GO
has been utilized to annotate both protein and RNA
gene products across multiple organisms. The Sequence
Ontology (SO) [27] is an ontology that is designed to cap-
ture genomic features and the relationships that obtain
between them. This ontology contains the features nec-
essary to annotate a genome sequence with structural
features such as gene models and also the terms nec-
essary for the annotation of the location and extent of
genomic variants. The PRotein Ontology (PRO) [28] has
been developed with a particular focus on human pro-
teins and disease-related variants thereof, providing an
ontological representation of proteins. As proteins are
often the functional entities in the processes impacted
by the regulatory effect of ncRNAs, they are an impor-
tant factor in the understanding of ncRNA. The Ontol-
ogy for MIcroRNA Target (OMIT) [2931] is a miRNA
domain ontology that is being developed as part of the
OmniSearch project. The purpose is to establish standard
metadata in miRNA domain for more effective identifica-
tion of the roles of miRNAs in various human diseases.
The ontology of Chemical Entities of Biological Interest
(ChEBI) [32] provides the terminology and relationships
to describe small molecules.
There are also other bio-ontologies that are in use in
a wider context that are also important for the descrip-
tion of clinical impact of ncRNA. SNOMED CT [33] is
a comprehensive, clinically oriented medical terminology
system, and also a reference standard in the United
States Meaningful Use program that promotes the use
of certified electronic health record (EHR) technol-
ogy to improve quality, safety, and efficiency, as well
as to reduce health disparities [34]. SNOMED CT is
owned andmaintained by the International Health Termi-
nology Standard Development Organization (IHTSDO).
Anatomy description has been unified over multiple
species with the Uberon anatomical Ontology [35]. This
ontology relates taxon-specific anatomies and is fully
integrated with other bio-ontologies such as the GO.
The Human Disease Ontology (DOID) [36] encapsu-
lates the terminology of diseases and provides equivalent
mappings to many related terminologies. The NCI The-
saurus (NCIt) [37] is a reference biomedical ontology
published by the National Cancer Institute (NCI) with
terminology that includes clinical care, translational and
basic research, and public information and administrative
activities.
Additionally, there are ontologies that address the
domain of data collection and are pertinent to the under-
standing of ncRNA. An ontology that covers the domain
of translational research is the Ontology of Biomedical
Investigations (OBI) [38], describing the foundational ter-
minology needed to define experimental processes and
investigation. Moreover, the Information Artifact Ontol-
ogy (IAO) [39] arose as a branch of OBI, to define the
foundational entities of scientific information in the digi-
tal domain.
Note that all bio-ontologies described in this section
except for SNOMED CT are included in both the OBO
Library and NCBO BioPortal. SNOMED CT is included
only in the BioPortal.
Ontology scope
The NCRO ontology will represent:
1. All known subtypes of ncRNA molecules including
those created in living organisms as well as those
engineered or adapted for some purposes (aptamers
for example [40])  this aspect will utilize high-level
terms defined in both the SO and ChEBI, with more
specific terms defined in the NCRO;
2. The structure involved in each ncRNA type,
including sequence and conformation  this aspect
will utilize the RNAO;
3. The functions, dispositions, and roles of ncRNAs, as
well as the processes in which these are realized1 
this aspect will utilize, mostly, the GO, with gaps
specific to ncRNAs filled by the NCRO or other
ontologies;
4. Different clinical phenotypes associated with
expression of normal and/or abnormal ncRNAs 
Huang et al. Journal of Biomedical Semantics  (2016) 7:24 Page 4 of 12
this aspect will utilize the SNOMEDCT, NCIt, and
Human Disease Ontology (DOID); and finally,
5. Various relations that are unique to ncRNAs and
their different components.
The initial focus of our work in building the ontol-
ogy is on small regulatory ncRNAs. Nevertheless, we
have designed an overarching framework of high-level
terms for other ncRNAs, such as: circular RNA (circRNA),
lncRNA, rRNA, small interfering RNA (siRNA), small
nuclear RNA (snRNA), and tRNA. These high-level terms,
all of which are direct child terms of the term ncRNA,
serve as placeholders: a more detailed hierarchy under-
neath each term, along with relevant relations, will be
developed at a later project stage.
Ontology development
Development principles
In the development pipeline for the NCRO ontology,
we have observed a set of practices proposed by the
OBO Foundry Initiative [41, 42]. Above all, the ontol-
ogy should be: freely available; expressed in a standard
language; documented for successive versions; orthogonal
to existing ontologies; including natural language specifi-
cations; developed collaboratively; and used by multiple
researchers.
Compliance with established upper-level ontologies
All NCRO terms descend from terms defined in the Basic
Formal Ontology (BFO) v2.0 [43]. The BFO is a small,
upper-level ontology that is designed for use in sup-
porting information retrieval, analysis, and integration in
scientific and other domains. Because the BFO is a well-
established upper ontology adopted by all OBO ontolo-
gies, our strategy to make the NCRO a BFO-compliant
ontology will set the stage for interoperability between
the NCRO ontology and other currently existing OBO
ontologies.
As for relations, besides those defined in the NCRO,
we have also used a set of well-defined relations in the
Relation Ontology (RO) [44, 45], such as: part of, par-
ticipates in, and precedes, all of which relate different
types defined in the BFO. Greater details of various rela-
tions can be found in Section NCRO terms and relations
and Table 3.
Strategy for orthogonality
Out of the set of OBO Foundry principles, orthogo-
nality is of special importance in defining the novelty
of the NCRO ontology. Our strategy to abide by this
principle is that we have imported and reused extant
terms wherever possible, focusing especially on terms
from OBO ontologies, SO, GO, PRO, and ChEBI for
example. Such terms have been imported with their
original identifier information using internationalized
resource identifiers (IRIs)/uniform resource identifiers
(URIs). This strategy helps us to achieve the maximum
possible orthogonality. Table 1 demonstrates a subset of
imported terms. More details can be found in Section
NCRO terms, relations, and reasoning, where per-
centages of imported terms from various existing bio-
ontologies are calculated.
The NCRO team and domain expertise
The NCRO team members come from a wide variety of
communities, covering computer science, ontology engi-
neering, wet-lab biological research, biomedical informat-
ics, and clinical sciences. The wide scope of participants
will provide (1) the necessary expertise in ontology devel-
opment and ontology-based reasoning and (2) the ncRNA
domain knowledge including expertise in ncRNA-relevant
phenotype. It will also help to ensure (3) a diversity of
communities eager to adopt the NCRO ontology for use
in representing and annotating ncRNA data.
Dynamic ontology construction procedure
The NCRO development is from the top down (start-
ing with more general terms), progressively utilizing
the ncRNA domain knowledge provided by the cellu-
lar biologists and clinical investigators in the project
Table 1 A subset of terms imported into the NCRO ontology
Imported term Source Ontology Original ID
miRNA Sequence Ontology SO:0000276
ncRNA Sequence Ontology SO:0000655
small_regulatory_ncRNA Sequence Ontology SO:0000370
gene Sequence Ontology SO:0000704
promoter Sequence Ontology SO:0000167
binding Gene Ontology GO:0005488
transcription, Gene Ontology GO:0006351
DNA-templated
translation Gene Ontology GO:0006412
metabolic_process Gene Ontology GO:0008152
protein PRotein Ontology PR:000000001
organism Ontology for OBI:0100026
Biomedical Investigations
cell Gene Ontology GO:0005623
cell line Cell Line Ontology CLO:0000031
molecular entity Chemical Entities of CHEBI:23367
Biological Interest Ontology
organ Uber Anatomy Ontology UBERON:0000062
tissue Uber Anatomy Ontology UBERON:0000479
disease Human Disease Ontology DOID:4
Huang et al. Journal of Biomedical Semantics  (2016) 7:24 Page 5 of 12
team. Lower levels of the ontology were then further
developed on the basis of a thorough analysis of repre-
sentative ncRNA-related databases (Table 2). Moreover,
an iterative procedure, including a series of interviews,
exchanges of documents, refinements, and related doc-
umentations, is being followed to make the NCRO a
dynamic ontology. In addition to a dedicated project
website [46], we have utilized GitHub [47] to further
assist the management and version control of the ontol-
ogy during both design and implementation, including
an established issue tracker [48] to facilitate discus-
sion among the members of an open group of investi-
gators, so that OBO Foundry principles can be better
followed.
Naming conventions
Each NCRO term has a unique identifier consisting
of a prefix and seven digit numerical string, as in:
NCRO_0000001. On the other hand, each NCRO term is
also assigned a human-readable label. We have followed
a set of OBO Foundry naming conventions [49] to design
such labels. Specifically:
 Labels are written in lower cases except for commonly
accepted acronyms such as RNA and ncRNA.
 Hypens are kept as is if they are commonly used in, or
easily understood by, the ncRNA community, as in:
hsa-miR-125b.
For greater readability, we italicize all relations through-
out this paper, whether they are defined in the NCRO or
imported from the RO and BFO.
Ontology languages and development tools
We have chosen both the Web Ontology Language
(OWL) [50] and OBO formats to describe the ontol-
ogy: both are widely accepted in OBO Foundry com-
munity and the former is recommended by the World
Wide Web Consortium (W3C). A first version of the
ontology was authored in OBO-Edit [51] and trans-
lated to OWL by the ROBOT tool [52]; then the
OWL version has been subsequently edited. Moving for-
ward, our focus will be placed on editing and releas-
ing the OWL version to take advantage of OWL-
specific features such as availability of ontology reasoners
and triple stores, as well as enhanced annotation
expressivity.
NCRO terms, relations, and reasoning
NCRO terms and relations
The current version NCRO (http://purl.obolibrary.org/
obo/ncro.owl) is our first production release. There
are a total of 3,078 terms and 27 relations (besides
a total of 5,394 is_a relations). Terms break down as
follows: 82.68% were defined in the NCRO ontology
itself, and the rest were imported from extant ontolo-
gies: BFO (1.14%), GO (8.67%), SO (6.50%), PRO
(0.10%), CHEBI (0.29%), OBI (0.13%), IAO (0.06%),
DOID (0.13%), CLO (0.06%), and UBERON (0.16%).
As for relations, many (55.56%) were imported from
the RO, and the rest (51.03%) were defined in the
NCRO.
Orthogonality among different ontologies has been
widely accepted in the bio-ontology community. To
achieve better orthogonality, it is a common practice to
reuse contents defined in relevant, existing ontologies.
This is our motivation to import terms and relations
from extant ontologies, as demonstrated above. On the
other hand, it is not trivial to obtain 100% orthogo-
nality, because ontologies are continuously being devel-
oped for good reasons within specific domains and by
different groups. As a result, given the holistic nature
of biology, along with the fact that different applica-
tions most likely have adopted different development
methodologies and have focused on various emphases,
Table 2 A list of ncRNA-related databases
Database name Brief introduction Web link
Ensembl ncRNA A database of ncRNA annotations. http://www.ensembl.org/info/
genome/genebuild/ncrna.html
GENCODE A database for annotation of gene features. http://www.gencodegenes.org
lncRNAdb A reference database for functional lncRNAs. http://www.lncrnadb.org
lncRNAtor A Web portal encompassing lncRNA data. http://lncrnator.ewha.ac.kr
miRBase A database of miRNA sequences and annotation. http://www.mirbase.org/
NDB A database of experimentally determined nucleic acids. http://ndbserver.rutgers.edu
NONCODE A database of ncRNAs except for tRNAs and rRNAs. http://www.noncode.org/
NRED An ncRNA expression database. http://nred.matticklab.com/cgi-bin/ncrnadb.pl
Rfam A database of a collection of RNA families. http://rfam.xfam.org
RMDB Chemical Mapping Data of RNA Sequences. https://rmdb.stanford.edu
Huang et al. Journal of Biomedical Semantics  (2016) 7:24 Page 6 of 12
there will inevitably be some overlaps among ontolo-
gies regarding their covered terms and/or relations. For
example, the term analgesic_treatment defined in the
NCRO2 is similar with the term analgesic treatment
defined in the Malaria Ontology3. Such overlaps may
have negative impacts on logical inferencing if ontol-
ogy reasoning is performed across relevant ontologies.
Whereas it is not realistic, if not impossible at all, to
obtain pure (i.e., 100%) orthogonality, one effective way
Read et al. Journal of Biomedical Semantics  (2016) 7:30 
DOI 10.1186/s13326-016-0071-3
RESEARCH Open Access
The BioHub Knowledge Base: Ontology and
Repository for Sustainable Biosourcing
Warren J. Read2, George Demetriou1, Goran Nenadic3, Noel Ruddock2, Robert Stevens1* and Jerry Winter2
Abstract
Background: The motivation for the BioHub project is to create an Integrated Knowledge Management System
(IKMS) that will enable chemists to source ingredients from bio-renewables, rather than from non-sustainable sources
such as fossil oil and its derivatives.
Method: The BioHubKB is the data repository of the IKMS; it employs Semantic Web technologies, especially OWL, to
host data about chemical transformations, bio-renewable feedstocks, co-product streams and their chemical
components. Access to this knowledge base is provided to other modules within the IKMS through a set of RESTful
web services, driven by SPARQL queries to a Sesame back-end. The BioHubKB re-uses several bio-ontologies and
bespoke extensions, primarily for chemical feedstocks and products, to form its knowledge organisation schema.
Results: Parts of plants form feedstocks, while various processes generate co-product streams that contain certain
chemicals. Both chemicals and transformations are associated with certain qualities, which the BioHubKB also
attempts to capture. Of immediate commercial and industrial importance is to estimate the cost of particular sets of
chemical transformations (leading to candidate surfactants) performed in sequence, and these costs too are captured.
Data are sourced from companies internal knowledge and document stores, and from the publicly available
literature. Both text analytics and manual curation play their part in populating the ontology. We describe the
prototype IKMS, the BioHubKB and the services that it supports for the IKMS.
Availability: The BioHubKB can be found via http://biohub.cs.manchester.ac.uk/ontology/biohub-kb.owl.
Keywords: Ontology, Repository, Environmental sustainability, Chemistry
Background
The aim of the BioHub project is to develop an Inte-
grated Knowledge Management System (IKMS) that will
enable chemists to source ingredients for chemical engi-
neering processes from biorenewables rather than sourc-
ing from non-renewable fossil feedstocks. An important
component of the IKMS is the BioHub Knowledge Base
(BioHubKB) which is an RDF store that uses an ontology
written in the Web Ontology Language (OWL) [1] as a
schema to organise knowledge about biorenewables; their
component chemicals will be used by the IKMS to seed
the exploration of possible chemical ingredients through
application of cheminformatics algorithms [2].
*Correspondence: Robert.Stevens@manchester.ac.uk
1School of Computer Science, University of Manchester, Oxford Road,
M13 9PL Manchester, UK
Full list of author information is available at the end of the article
One class of compounds of particular interest that
drives the BioHub project are surfactants, which form
the principal active ingredients in many personal care
and household cleaning products. Thus the impetus of
the BioHub project is to build an informatics infrastruc-
ture to support the development of surfactants and other
chemicals, from sustainable agricultural feedstocks and
co-product streams.
The use of such agricultural streams as chemical feed-
stocks is intended to obviate the need for sourcing
chemicals from non-renewable fossil feedstocks, and to
avoid the concomitant environmental costs associated
with their extraction, refinement and use. The BioHub
project seeks to facilitate the move from fossil fuel to
biorenewable feedstocks. The process of exploring the
sourcing of ingredients from biorenewables among the
project partners is currently ad hoc, relying on read-
ing public literature and proprietory documentation on
© 2016 Read et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International
License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any
medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons
license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.
org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Read et al. Journal of Biomedical Semantics  (2016) 7:30 Page 2 of 7
chemical analyses, which are sometimes decades old.
Gathering such knowledge into a central resource should
facilitate the sourcing of novel ingredients. The project
is a collaboration between Unilever and several other
commercial and academic partner organisations. The
commercial partners are motivated both to move to bio-
renewables and to exploit more effectively their own
materials streams.
The first use case in developing this prototype BioHub
is to take co-product streams from the processing of sugar
beet and identify the ingredients and transformations that
can be chained together to generate good candidate sur-
factants. To make such a scenario work, the IKMS needs
to enable chemists to describe the properties of a class
of molecules they wish to derive from chemicals in bio-
renewables. The IKMS will calculate a model of this class
of chemicals by enumerating and selecting the possibilities
with starting point chemicals [3] from knowledge in the
BioHubKB about feedstocks and co-product streams and
their component chemicals together with the chemical
transformations in which they may participate.
The IKMS process takes chemicals, their properties and
the transformations in which they may be involved, and
supplies those data to a chemical enumeration routine.
The BioHubs hosted, web-based UI proposes sequences
of candidate processes leading to candidate molecules,
with both cost and chemical property prediction. In this
paper we describe the BioHubs knowledgebase, the Bio-
HubKB, and its role in the IKMS.
The IKMS allows a chemist to specify the available co-
product streams, the allowable transformations, and the
specific desired chemical properties, before the chemical
enumeration pipeline is run. A chemist might, for exam-
ple, choose wool wax and rape oil as allowable source
streams; s/he might go on to specify ozonolysis and ester
hydrolysis as allowable transformations, with surfactancy
as the desired property, e.g. a range of allowable surfac-
tancy values as measured by critical micelle concentration
(CMC). The input for the enumeration pipeline comes
from the BioHubKB.
Figure 1 shows an overview of the IKMS. The compo-
nents of the IKMS are:
IKMS user interface: A web GUI that gives the chemist
the ability to specify the application-specific require-
ment for a set of candidate molecules in suitable
quantitative terms. Here the chemist may also select
the allowable source streams of seed chemicals for
the enumeration pipeline, as well as restricting the
set of allowable transformations if necessary.
IKMS enumeration pipeline: A set of routines that
runs iteratively, which in each iteration computes
the allowable transformations and products for all
theoretically possible chemical reactions, based on
a known set of substrates. These are then scored
according to user-defined design criteria, Pareto-
ranked and a subset chosen as inputs to the next
iteration. In the first instance, the pipeline uses
the known molecular constituents of co-product
streams to seed the enumeration. Products of these
reactions form the substrates for the next generation,
and so on.
BioHubKB: An RDF store with a schema formed
from ontologies describing feedstocks, co-product
streams, chemicals and transformations of those
chemicals. A set of Web services offers methods to
the enumeration pipeline to recover chemicals and
the transformations in which they may participate.
Some example competencies [4] in the form of queries
that the BioHubKB needs to support are:
1. Return a list of chemicals originating from a specified
co-product stream, or set of co-product streams.
2. Return a list of chemicals that can undergo a single
transformation, or at least one transformation from a
supplied list.
3. Return a list of chemicals originating from among a
supplied list of 1 or more co-product streams that are
capable of undergoing 1 or more transformations
among a separate supplied list.
4. Return the complete list of available chemical
transformations.
5. Return the complete list of available co-product
streams.
6. Return a list of co-product streams that are
themselves a direct or downstream output of a
specified upstream co-product stream (e.g. wool
wax).
As well as being a repository for biorenewable feedstock
data, the BioHub also contains curated details of indus-
trially available chemical transformations. The BioHub
application will provide chemists with query access to
possible chemical products of chains of such transforma-
tions, starting from bio-sourced chemicals and thereafter
using successive rounds of products as possible substrates
for subsequent transformations. Queries specify desired
chemical properties (e.g. relating to surfactancy) that
are evaluated by one of several selectable computational
chemical models; the model is also used as a generational
filter to prevent the combinatorial explosion of chemical
species.
The BioHubKB
The BioHubKB is an application ontology [5], where
an ontology is created to fit a particular task model; in
this case the production of surfactants, that addresses the
competencies and scenario outlined above.
Read et al. Journal of Biomedical Semantics  (2016) 7:30 Page 3 of 7
Fig. 1 The BioHub IKMS
Where practicalities of scope, content and cognitive
complexity permit, we have re-used extant, community
based ontologies made by the Open Biomedical Ontolo-
gies consortium [6]. It re-uses (i) ChEBI [7] to describe
its chemicals, and (ii) the Relations Ontology (RO) [8]
for many of the relationships as well as some role hier-
archy needed by the BioHubO. The Plant Ontology
(PO) [9] is used to describe the parts of plants whence
various feedstocks come. As well as plants, the IKMS
will use animal based feedstocks meaning the BioHubO
will need to be extended to animal species and the
generic animal anatomy Uberon [10] wil be appropriately
extended.
The design of the BioHub ontology was based on
data sources that were made available to the devel-
opers by domain experts. Such sources included (i)
corporate reports about chemical experiments and indus-
trial processes (British Sugar), and (ii) internal spread-
sheets/databases about specific chemicals and their
potential sources and transformations (Unilever). The
transformations for extracting streams and chemicals
from sugar beet are well documented in the literature.
Nevertheless, the developers paid particular attention to
internal data used to describe the derivation of certain
chemicals from sugar-beet for the development of the
ontological structure of the sugar-beet representation. In
addition, several key points in the ontology were discussed
and clarified during consultation meetings with experts.
Finally, external ontology resources, such as ChEBI, were
used primarily to populate the BioHub ontology. In pop-
ulating the BioHubKB, chemicals in input data needed to
be mapped to chemicals in ChEBI and the representa-
tion of chemical transformations; this was accomplished
using SMILES strings [11] present in ChEBI, plus SMILES
and SMIRKS strings [12] from our in-house developed
representations.
The BioHubKB is authored in the Web Ontology Lan-
guage (OWL) and classes etc. are denoted in this paper
by the typeface Class name. Figure 2 shows the domain
general classes of the BioHubKB, indicating its scope.
We have the class Substance that is a superclass
for distinct chemical entities and mixtures, slurries and
soups of chemical entities. We use ChEBI for distinct
chemicals, but chemical engineering involves soups and
slurries of mixtures of chemicals with various physical
properties. The latter are covered by a class Melange,
that is a superclass for continuous phase mixtures, plus
soups/slurries which include components that co-exist in
different phases. Formulation is a Melange of con-
tinuous phase. Stream is a Melange and is a superclass
of Feedstock, Product stream and Coproduct
stream.
Read et al. Journal of Biomedical Semantics  (2016) 7:30 Page 4 of 7
Fig. 2 The domain general classes covering the scope of the BioHub ontology
Stream is a material stream that may be a completely
unprocessed agricultural feedstock, or the output of a
downstream process or series of processes (such as sepa-
ration) applied to such a feedstock or its derivatives.
A Feedstock is an unprocessed, raw agricultural
Stream such as Sugar beet feedstock (the freshly
harvested plants themselves). Sugar beet, once harvested
and entered into the processing is playing the role
of a feedstock; the Sugar beet feedstock is sourced
from Whole sugar beet (within the Plant Ontol-
ogy). Feedstock may be a somewhat arbitrary role
assigned to some raw materials. A sugar beet feedstock
is itself separated (by cutting) into the sugar beet root
and the sugar beet leaves, a co-product for further
processing.
A Coproduct stream is a stream of materials
obtained as a co-product of processing an enterprises
principal product(s). For example, Syrup is a Product
stream derived from the Sugar beet feedstock
and the Coproduct stream that is co-derived is
Sugar beet pulp; this goes on for further refine-
ment and produces (an)other Product stream(s) and
Coproduct stream(s) until distinct chemical entities
are produced. Each stream has its components of interest
described (see below) down to chemical constituents and
their proportions; for instance product stream Syrup has
a large proportion of sucrose. Figure 3 shows how sugar
beet and some of its co-product streams are represented
as streams. The labels used for these streams are the ones
used by the domain specialists with whom the BioHubO
was created.
The BioHubKB classes below show sugar beet as a feed-
stock and some derivations into a product such as syrup
and co-product streams such as sugar beet pulp.
Class: Sugar beet feedstock
SubClassOf:
Feedstock,
has source type some Whole sugar beet
Class: Sugar beet leaves
SubClassOf:
Coproduct stream,
derivesFrom some Sugar beet feedstock,
derivesFrom only (Sugar beet feedstock)
Class: Syrup
SubClassOf:
Product stream,
derivesFrom some Sugar beet feedstock,
derivesFrom only (Sugar beet feedstock)
Class: Beet pulp
SubClassOf:
Coproduct stream,
derivesFrom some Sugar beet feedstock,
derivesFrom only (Sugar beet feedstock)
Class: 6-Kestotriose
SubClassOf:
Chemical entity,
derivesFrom some Beet pulp
A SourceType is the type of source (typically
extracted from a type of animal, vegetable, or mineral)
Read et al. Journal of Biomedical Semantics  (2016) 7:30 Page 5 of 7
Fig. 3 The entities and relationships that represent the sugar beet feedstock and some of its co-product streams. The arrows joining the streams
represent Transformation processes such as Separation, Hydrolysis and Fermentation
whence a Substance derives. An OrganismalPart is
an identifier assigned in the BioHubKB, to link a species
ID, and an anatomical entity ID from Uberon or the
Plant Ontology. For instance, Whole sugar beet is
an organismal part whence Sugar beet feedstock
comes (see above).
Transformation is a chemical Process used
and/or made available industrially. For example, a trans-
formation such as Separation will take Beet pulp
and separate it into Cellulose, Hemicellulose and
Pectin (subclasses of Coproduct stream), as well
as Sugar mixtures (a Product stream). The class
Separation can be further specialised to more pre-
cise separations, but the current representation is that
required by our domain experts and the application needs.
Transformation also describes chemical processes
such as Acetylation. Among a transformations anno-
tations are SMIRKS strings that capture the reaction
transforms. The SMIRKS annotations are used by the
IKMSs enumerator to evaluate which molecules (either
from the source streams or the products from a previous
iteration of the enumerator itself ) constitute valid sub-
strates for each transformation, based on their SMILES
strings.
The class Participant expresses a ternary relation
among a Substance, a Transformation and a Role;
There is always an Input, an Output, or both, in rela-
tion to a Transformation. A MelangeComponent
describes a part of a Melange, associated with a
Substance (i.e. either a ChemicalEntity or another,
nested Melange) and a proportion, expressed as a simple
percentage. This enables BioHubKB to describe the yield
of transformations.
Class: Participant
SubClassOf:
(input of some Transformation
or output of some Transformation),
has role some chemical role,
is associated with some Substance
A Role is a class imported from OBO RO, with various
re-used subclasses from the same, e.g. Buffer, Solvent
and Catalyst, in addition to other subclasses defined
for the BioHubKB. These are chemical Substrate,
which is an input of some Transformation, and
chemical Product, which is an output from some
Transformation. RawMaterial acts as input
Stream to some Process. OutputStream acts as
output Stream from Process.
A MeasurementProcess is a Process involving
the quantitive measurement of some chemical property
such as may relate to its surfactancy. A Predictive-
ModellingProcess is a process utilising a chemin-
formatics model to predict the attributes of a particular
molecule. An Evaluation is a result generated by
a Process, typically a MeasurementProcess or
PredictiveModellingProcess.
An Organisation is an entity typically having an
ownership relation to a Process or Substance.
The class Quality is used to represent a quality asso-
ciated with an Evaluation, e.g. surface interfacial ten-
sion. QualityValue is a reifying class for associating
unit and number with a Quality, for an Evaluation.
For a given transformation of a co-product stream a use-
ful aspect for the IKMS is the percentage yield. We are
interested in yield as a factor of the overall cost calcula-
tion associated with a Process, as applied to particular
Read et al. Journal of Biomedical Semantics  (2016) 7:30 Page 6 of 7
inputs and outputs. Yield is modelled as a Quality,
with an associated QualityValue, which in turn has
an associated unit (percentage in this case) and unit-
less number. Each QualityValue is associated with an
Evaluation, which in the case of yield is the output of a
MeasurementProcess (itself a subclass of Process).
A user of the IKMS will describe the class of chemi-
cals to be generated through selection of physicochem-
ical properties of that class. To take a familiar example,
soaps with potential for use in laundry applications are
a subclass of anionic surfactants which may be expected
to exhibit characteristic traits, reflected in characteris-
tic ranges in qualities like surface interfacial tension and
CMC, as well as typically being easily derived from cer-
tain types of agricultural co-products. The modelling of
physicochemical properties is difficult. The issues in doing
so include the conditions in which a quality wasmeasured,
the devices used to do the measurement and the units for
the measurement. There are several ontologies that could
be used to capture some aspects of such measurements
[13, 14], but pragmatic considerations led to a somewhat
simplistic axiom pattern.
Actual feedstocks, products, co-products, etc. are rep-
resented as classes below the BioHubO general domain
ontology to form the bulk of the BioHubKB. The con-
tent in the BioHubKB comes from spreadsheets supplied
by our partners. We have a process to generate the Bio-
HubKB using Tawny-OWL [15]. Spreadsheet worksheets
are exported into comma-separated-variables (CSV) files,
then read by Java code (via the OpenCSV library);
the data then populate a Tawny-OWL script skeleton
whose template format is defined by another Java library
(FreeMarker). The populated template is then translated
into OWL from Tawny-OWL.
The BioHub ontology was developed as described and
then evaluated by project partners. We used a simple
mechanism for evaluation whereby a diagram of the
derivation of streams from feedstocks was presented
along with a diagram of the gross structure of the ontol-
ogy. These diagrams were associated with a simple survey
[16]. The evaluation confirmed the overall representation
in the ontology, but provided some useful extensions, in
the form of missing steps, to the derivation paths. For
example, instead of Ferulic acid being derived from
Beet pulp, it was changed to reflect the true stepwise
derivation of Ferulic acid, directly from Pectins.
There were eight changes of this type. All proposed
changes were made to the ontology.
Querying the BioHubKB
The BioHubKB is typically queried automatically (via
web services) by other components of the IKMS. One
example is the IKMS user interface, which pre-populates
its selection drop-downs (e.g. lists of streams and
transformations) prior to any direct user interaction,
based on the current content of the BioHubKB; another
is the enumeration tool, which selects its initial feed
molecules from those listed in the BioHubKB as belong-
ing to the user-selected streams, and amenable to a set
of user-selected transformations. However, the user (i.e.
the chemist) him- or herself will also have the option to
issue direct queries to the BioHubKB, such as, Return a
list of 3 chemicals found in the beet pulp stream, picked at
random.
Discussion
The BioHubKB is a development of an application ontol-
ogy that re-uses several OBO ontologies as well as being
a de novo ontology that satisfies a series of competencies.
It is used by a cheminformatics enumeration pipeline that
generates a set of candidate chemicals based on a model
specified by a user. If one of these candidate molecules is
transformed from a chemical in the BioHubKB, then the
BioHubKB contains the associated information about the
feedstock whence the chemical came, such as the organ-
isation that supplies that feedstock, its cost and the set
of transformations the chemical underwent to yield the
predicted chemical.
The current IKMS and its BioHubKB is a prototype
to see if the approach is feasible. Hence there is much
potential work to be done. The BioHubKB development
has so far concentrated on sugar beet as a biorenew-
able; the other feedstocks are numerousoil seed rape,
as well as many other agricultural feedstocks, both plant
and animal, and their co-products. Further development
is also needed in the descriptions of chemical proper-
ties and their models used in the processing within the
IKMS; incorporating some of the Ontology for Biomedical
Investigations (OBI) [13] may help in this respect.
The goal of the IKMS that the BioHubKB supports is
to facilitate the use of bio-renewable feedstocks in the
chemical manufacturing process. This is necessarily a
knowledge-driven process, a job for which Semantic Web
technologies appear to be suited. That the bio-ontologies
community has produced a range of ontologies that can
be slotted into an artefact such as the BioHubKB, despite
the scenario in which the IKMS is deployed not being
traditional for the bio-ontology community, is a sign of
the maturity and wider applicability of the communitys
ontologies. This supports a model of a collection of ref-
erence ontologies that can be refactored and repurposed
into application ontologies in a broad range of settings.
Ultimately, the BioHub has an auxiliary potential to act
as a marketplace for sustainable production, enabling
chemists from diverse backgrounds and organisations
to source bio-derived chemicals with multiple potential
applications (including, but not limited to, surfactancy)
from pre-existing industrial processing operations.
Read et al. Journal of Biomedical Semantics  (2016) 7:30 Page 7 of 7
Competing interests
The authors declare that they have no competing interests.
Authors contributions
All authors contributed to the development of the ontology and the paper.
The ontology effort was, however, led by WR. All authors read and approved
the final manuscript.
Acknowledgements
The BioHub project is funded by awards from TSB (Innovate UK) grants
EP/L505808/1/TSB 101508 and TS/L001950/1/3TSB 101717. The partners in
these projects are: Unilever Research Port Sunlight, British Sugar Ltd, Croda,
University of Liverpool, University of Manchester, University of Sheffield and
Cybula Ltd. We would like to thank Matthew White for his helpful comments
on the sugar beet feedstock relationships. We thank Dr Mercedes Argüello
Casteleiro for producing the ontology diagram. We thank the ChEBI team for
adding chemicals to ChEBI in a most helpful manner.
Author details
1School of Computer Science, University of Manchester, Oxford Road,
M13 9PL Manchester, UK. 2Unilever Research Port Sunlight, Bebington, CH62
4ZD Wirral, UK. 3Manchester Institute of Biotechnology, Princess St,
Manchester M1 7DN, UK.
Received: 3 December 2015 Accepted: 3 May 2016
RESEARCH Open Access
NeuroRDF: semantic integration of highly
curated data to prioritize biomarker
candidates in Alzheimer's disease
Anandhi Iyappan1,2, Shweta Bagewadi Kawalia1,2*, Tamara Raschka1,3, Martin Hofmann-Apitius1,2
and Philipp Senger1
Abstract
Background: Neurodegenerative diseases are incurable and debilitating indications with huge social and
economic impact, where much is still to be learnt about the underlying molecular events. Mechanistic disease
models could offer a knowledge framework to help decipher the complex interactions that occur at molecular
and cellular levels. This motivates the need for the development of an approach integrating highly curated and
heterogeneous data into a disease model of different regulatory data layers. Although several disease models
exist, they often do not consider the quality of underlying data. Moreover, even with the current advancements in
semantic web technology, we still do not have cure for complex diseases like Alzheimers disease. One of the key
reasons accountable for this could be the increasing gap between generated data and the derived knowledge.
Results: In this paper, we describe an approach, called as NeuroRDF, to develop an integrative framework for modeling
curated knowledge in the area of complex neurodegenerative diseases. The core of this strategy lies in the usage of well
curated and context specific data for integration into one single semantic web-based framework, RDF. This increases the
probability of the derived knowledge to be novel and reliable in a specific disease context. This infrastructure integrates
highly curated data from databases (Bind, IntAct, etc.), literature (PubMed), and gene expression resources (such as GEO
and ArrayExpress). We illustrate the effectiveness of our approach by asking real-world biomedical questions that link
these resources to prioritize the plausible biomarker candidates. Among the 13 prioritized candidate genes, we identified
MIF to be a potential emerging candidate due to its role as a pro-inflammatory cytokine. We additionally report on the
effort and challenges faced during generation of such an indication-specific knowledge base comprising of curated and
quality-controlled data.
Conclusion: Although many alternative approaches have been proposed and practiced for modeling diseases, the
semantic web technology is a flexible and well established solution for harmonized aggregation. The benefit of this
work, to use high quality and context specific data, becomes apparent in speculating previously unattended biomarker
candidates around a well-known mechanism, further leveraged for experimental investigations.
Keywords: RDF, Semantic web, Data integration, Data curation, Data harmonization, Disease modeling,
Neurodegenerative diseases, Alzheimer's disease
* Correspondence: shweta.bagewadi@scai.fraunhofer.de
Equal contributors
1Department of Bioinformatics, Fraunhofer Institute for Algorithms and
Scientific Computing (SCAI), Schloss Birlinghoven, 53754 Sankt Augustin,
Germany
2Bonn-Aachen International Center for Information Technology, Rheinische
Friedrich-Wilhelms-Universität Bonn, 53113 Bonn, Germany
Full list of author information is available at the end of the article
© 2016 Kawalia et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 
DOI 10.1186/s13326-016-0079-8
Background
Alzheimer's disease (AD), the most prominent neurode-
generative disease (NDD), has become a global threat to
the aging society, affecting nearly 115 million people by
2050 [1]. The imperfect understanding of the AD
etiology has created a large gap in translating the pre-
clinical findings into clinical trials dominantly observed
in high drug attrition rates [2]. Early diagnosis and pre-
ventive interventions could facilitate substantial reduc-
tion in the number of affected cases to 9 million by 2050
[3, 4]. Particularly, reliable biological markers of disease
and disease progression could assist in early diagnosis
and treatments catered to the patient [5]. In this direc-
tion, considerable global research efforts have been dedi-
cated to investigate molecular players underlying AD
pathogenic events, contributing to an ever-growing
wealth of disparate data. Refinement of this information
into actionable knowledge representations requires a
good interoperable and formalized framework, capable
of inferring potential biomarkers across different facets
of the molecular physiology. Additionally, in silico
disease models that integrate complementary data from
various resources are capable of recapitulating key
mechanisms for a given condition [68].
Among others, most widely used data integration
strategies include data warehousing (e. g., Pathway Com-
mons [9]), data centralization (e. g., UniProt [10], IntAct
[11]), and federated databases (e. g., BioMart [12]). An
example of a data integration framework is tranSMART
[13], which consists of a data warehouse covering vari-
ous types of data and related data mining applications
required for translational research and biomarker
discovery workflows. Such a harmonized aggregation of
heterogeneous data sources facilitates interpretation over
a large knowledge space [14].
However, one fundamental challenge with most of
these integration approaches is to cope with the variabil-
ity and heterogeneity in content, language, and formats
of incoming data from different source repositories.
Moreover, regular updates of these data resources are
necessary to keep up with newly added information and
to avoid incompleteness. The inaccessibility to the inte-
grated data resources, due to altered database structure
or change in the naming conventions is unavoidable
[15]. Semantic web technologies have overcome the
above described challenges up to an extent by revolu-
tionizing the lossless exchange of data and formalizing
the data format into a computable knowledge [16],
calling it smart data" [17]. The capability of using rich
formal descriptions for data and its standardized map-
ping allows complex querying in a more efficient way
without information loss.
Resource Description Framework (RDF) is the World
Wide Web Consortium (W3C) proposed standard for
semantic integration and modeling of data. RDF uses the
syntax of Extensible Markup Language (XML) and im-
poses structural constraints to represent the meta-data
as a set of triples containing directed edges. One big ad-
vantage lies in the usage of common namespaces across
the different data domains encoded as Unified Resource
Identifiers (URIs). Initiatives such as Identifiers.org [18]
provide persistent official identifiers in the biomedical
domain, allowing sustained interlinking between distinct
data resources. This allows high levels of seamless inter-
operability between data sources and the capability to
access and map against additional related data unam-
biguously, called data federation. On the contrary, large
efforts are still needed during an initial definition of the
ontologies to build the schema for data representation.
Semantics in life sciences
The idea of semantic web prevails in various domains,
including life sciences. Recently, "The Monarch Initia-
tive" [19] has taken the semantic route to enable reason-
ing over genotype-phenotype equivalence within and
across species. They leverage on ontologies to link exter-
nal curated data resources for generating new hypothesis
and prioritizing candidates/variants based on the pheno-
typic similarity. Stevens et al. [20] launched TAMBIS,
multi-data application tool, which allows biologists to for-
mulate complex molecular biology questions to databases
such as Swiss-Prot [21], Enzyme [22], CATH [23], BLAST
[24], and Prosite [25] through well-defined semantics.
Among the early users of RDF, Lindemann et al. [26]
applied it to centralize and flexibly access the heteroge-
neous and varying quality of medical data obtained from
several clinical partners. The importance of semantic
mining in the life science domain was brought to lime-
light by the Bio2RDF project [27], which demonstrated
the possibility of querying life science knowledgebases
by linking public bioinformatics databases and providing
public SPARQL endpoints. Subsequently, Linking Open
Drug Data (LODD) [16] demonstrated linking drug data
information from DrugBank [28] and clinical trials
resources. Chem2Bio2RDF [29] demonstrates the poten-
tial usage of the above two mentioned RDF repositories
in the field of chemoinformatics.
Observing the immense advantage of linked open data,
several major publicly available life science databases
such as UniProt, DisGeNet [30], Protein Data Bank
Japan (PDBj) [17], and EBI resources such as Gene Ex-
pression Atlas [31], ChEMBL [32], BioModels [33],
Reactome [34], and BioSamples [35], have made their
data available in the form of RDF. Thus, the RDF plat-
form has been increasingly adopted as a standard for
data exchange. Amidst prime users of RDF in elucidating
disease pathophysiology, Shin et al. [36] demonstrated
systematic querying of linked experimental data to
Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 2 of 15
explore the effect of genes that are regulated by volatile
organic compounds in human blood. Qu et al. [6]
showed semantic framework capability in drug re-
purposing by proposing Tamoxifen, an FDA approved
drug for Breast Cancer, as a candidate drug for Systemic
Lupus Erythematosus. The above reported association
has already been tested in mice by Sthoeger et al. [37],
showing a leverage of semantic web in a real world sce-
nario. Furthermore, Willighagen et al. [38] presented the
linkage of several RDF technologies in molecular che-
moinformatics and proteochemometrics.
To our knowledge, there has been very limited applica-
tion of semantic web approaches to the research of neuro-
degenerative diseases. Linked Brain Data (LBD) [39] is an
upcoming initiative which focusses on understanding the
brain functionality by integrating resources such as gen-
omic, proteomic, anatomical and biochemical resources
with respect to neuroscience. Using such a multi-level
knowledgebase, they aim to understand the association
between cognitive functions and brain diseases. Lam et al.
[40] made the first attempt to develop an e-Neuroscience
data integration framework, AlzPharm [41]. They ex-
tracted AD-related drug information from BrainPharm
[42] to be further integrated with manually inferred
hypotheses from the scientific literature and published ar-
ticles (SWAN [43]). They demonstrated the usage of such
a model by clustering AD drugs based on their molecular
targets and to filter publications (claims and hypotheses)
specific to Donepezil effect on treatment of AD. Although
AlzPharm made use of manually inferred hypothesis, they
lack the validation of their findings with experimental data
such as gene expression and pathways.
Motivation
Despite the current advancements in semantic web tech-
nology, we still do not have cure for complex diseases like
AD. One of the key reasons accountable for this could be
the increasing gap between generated data and the derived
knowledge. In order to increase the probability of the
derived knowledge to be novel, data quality and data
reliability is highly desirable. Moreover, the contextual
specificity of the data is of paramount importance.
Compared to relational database management system
(RDBMS) technologies, in RDF the relations have expli-
cit meaning (expressiveness) in a given context and are
directly accessible; allowing the user to extract meaning-
ful knowledge from the data as opposed to an unstated
structured data. In addition, RDF structures are more
adaptive and flexible, allowing fluidity in the data rela-
tionships. This overcomes the fragility of RDBMS; where
if the underlying representation of the keys and flat table
changes, the tentacled connections are lost. Moreover,
triples from RDF can be transformed into RDBMS struc-
ture and vice-versa. One another advantage of RDF is its
graph representation that enables us to better explore
relations through network topological characteristics
such as relatedness, network perturbation, centrality, in-
fluence, etc. The usage of automated reasoners have
largely been beneficial to understand the semantics and
to expand the associated relations [44].
In this paper we propose NeuroRDF, an approach harnes-
sing the potential of RDF as a framework for modeling neu-
rodegenerative diseases to enable a close, biologically
sensitive integration of well curated, complementary, and
multi-faceted data. The fundamental principle of this strat-
egy is to take advantage of semantics to develop a context
specific, multi-layered in silico disease model, represented
as a formalized, and computationally processable domain
knowledge. A fine-grained analysis of the metadata from
various data resources empowers the user to ask more fo-
cused questions around a hypothesized pathomechanism
involving previously neglected or hidden candidates, further
leveraged for experimental investigations. Considerable ef-
forts have been invested to process and manually curate
huge amounts of data that is required to build such a
knowledge base around a specific indication. This includes
for example the in-depth assessment of the respective
phenotype, the type of tissue used in an experiment, and in-
formation around the donor of the tissue like gender, age,
and possible comorbidities. Querying such a highly curated
and focused knowledgebase increases the chances of unrav-
eling novel hypothesis, which could have been lost over
time or pave way to newly emerging knowledge.
We used SPARQL to traverse each of these knowledge
graphs (derived from distinct resources) in an integrative
manner, allowing highly disease specific analysis of the
underlying data. Using this approach, we demonstrate an
example on how to prioritize novel candidates in AD
mechanism.
Methods
The developed generic semantic web-based workflow in-
tegrating heterogeneous data resources is outlined in
Fig. 1. This multi-layered model integrates data from
various public resources such as databases, literature,
and gene expression information. The harmonization of
heterogeneous data to build RDF models was achieved
by using several data/file parsers. The workflow also
includes a pre-processing step to monitor the quality of
each incoming data type for specificity.
Data collection and resources
This subsection depicts briefly the different data re-
sources integrated into the NeuroRDF.
Database-derived interactions for healthy brain
A closer look into the healthy human brain interac-
tions could improve identification of the dysregulated
Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 3 of 15
mechanisms which further surges the plausibility of
identifying AD drugs in clinical trails [45, 46]. How-
ever, the mainstream AD research is biased towards
the well known disrupted events such as APP, and tau
rather than recognizing their role in normal brain
functions [47].
Several publicly available databases provide protein-
protein interactions (PPIs) and microRNA-target inter-
actions (MTIs), which can be derived using multiple
sources and methodologies. For instance, Human Pro-
tein Reference Database [48], Molecular INTeraction
database [49], and miRTarBase [50] focus on experimentally
verified interactions that are manually mined from litera-
ture by expert biologists. In addition to literature-derived
information, Biomolecular Interaction Network Database
[51] centralizes interactions from high-throughput tech-
nologies. Few other databases such as STRING [52], and
miRWalk [53] also provide predicted interactions.
However, none of these databases mine interactions
specific to a given context (for example AD pathology
or normal physiology).
A lot of published healthy state PPIs are not directly
measured in human cells but in artificial conditions such
as human cell lines, human genes transfected into yeast
cells, etc., missing out on the biological plausibility in
humans and context specificity [54]. Hence, considerable
effort by Bossi and Lehner [55] was invested to verify
the tissue specificity of PPI interactions from 21 data-
bases (including a few above mentioned) using human
gene expression data. Furthermore, this additional action
to ensure validity of the interactions in normal state aids
improved prediction of genes in disease state [56]. In
that direction, our group has extracted a subset of these
experimentally confirmed PPIs belonging to healthy
brain physiology [57]. Currently, the healthy brain PPI
network contains 7,192 genes and 45,001 PPIs.
Extracting AD-specific interactions from literature
The bridging factor between researchers and scientific
accomplishments are published as texts, warehoused in
large repositories like PubMed [58]. These biomedical
articles are the major information source of functional
factors such as proteins, genes, microRNAs (miRNAs),
etc. However, their functional descriptions are scattered
as unstructured text in literature [59]. Text-mining
methods could help us mine these articles and retrieve
the associated relations/evidence for a given context.
Since proteins are the chief players in almost all bio-
logical processes and miRNAs have been established in
the last decade as important regulators of gene expres-
sion, we focus our current research on MTIs and PPIs.
In order to harvest AD-specific knowledge from the
literature, we used our in-house state-of-the-art named
entity recognition (NER) system ProMiner [60] and the
semantic search engine SCAIView [61]. Identification of
genes/proteins and disease mentions was accomplished
using dictionaries. The disease dictionary was built using
MeSH [62], MedDRA [63], and Allie [64] databases.
Currently, it contains 4,729 concepts and 64,776 syno-
nyms [65], which are normalized to MeSH names.
Human Genes/Proteins dictionary [60] was compiled
from three different resources: SwissProt, EntrezGene
[66], and HGNC [67]. Currently, this dictionary consists
of 36,312 entries and 515,191 synonyms. All the identi-
fied gene/protein names were normalized to HUGO
gene symbols for maintaining homogeneity across all
Fig. 1 Overall workflow of NeuroRDF. The workflow illustrates the collection of data from various resources such as databases, and literature,
followed by steps taken to pre-process and prune the collected data. These high-quality data are represented semantically as RDF models and
stored in a triplestore. The stored knowledge can later be queried for biologically interesting questions
Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 4 of 15
data resources and also for future comparisons and
visualizations.
To identify MTIs from MEDLINE abstracts, we applied
our previously developed approach [65]. Here we ex-
tracted novel miRNA mentions using a regular expression.
These mentions were normalized to miRBase database
identifiers [68]. In addition, relation dictionary containing
the major classes of relationship terms between miRNAs
and their target genes/proteins was also developed. A tri-
occurrence based approach was used to extract the MTIs
(co-occurring with a relation term) at the sentence level.
Using the above-mentioned dictionaries, our group
previously harvested AD specific PPIs from MEDLINE
abstracts and full text articles [69]. Here we used the
interaction terms compiled by Thomas et al. [70]. A
state-of-the-art machine learning based approach [71]
was applied to retain true pairs of PPIs in a given sen-
tence. Both approaches have been optimized for recall.
Hence, the obtained relations have been manually fil-
tered for false positives. After manual inspection, 339
PPIs for 301 proteins and 99 MTIs for 36 microRNAs
that are specific to AD were obtained. Articles published
in languages other than English could lead to increased
information content, however a dedicated approach to
harvest them is needed. Moreover, separate parsers are
needed. Thus, for this work we extracted interactions
from the biomedical literature in English.
AD gene expression data
A standard approach to test any generated hypothesis is
to assess the gene expression of the involved candidates
between affected and healthy patients or in the absence of
human data we fall back to animal models or derived cell
cultures [7275]. High-throughput technologies such as
microarray, RNA-seq provide potential to measure gene
expression simultaneously for different experimental/bio-
logical conditions. These studies are assembled in widely
adopted public archives: The NCBI Gene Expression
Omnibus (GEO) [76] and ArrayExpress [77].
For querying AD-specific gene expression data, we used
previously developed database, NeuroTransDB [78], which
contains highly curated meta-data information for eligible
AD studies. It assembles studies from public resources
namely, GEO and ArrayExpress, using a keyword based
search approach. Among the 45 prioritized AD human
studies, we filtered for microarray studies that measure
gene expression in brain tissue extracted from both AD
and healthy patients. In addition, availability of raw data
was a mandate for applying uniform pre-processing. In
total, we obtained eight microarray studies to be integrated
in NeuroRDF: GSE12685, GSE1297, GSE28146, GSE5281,
E-MEXP-2280, GSE44768, GSE44770, and GSE44771.
To assess the quality of the arrays we applied ArrayQuali-
tyMetrics [79] package. The selected studies (independent
of the platform type) were pre-processed using Bioconduc-
tor (Version 3.0) packages in R [80], by applying similar
methods for maintaining consistency by reducing variance.
All studies conducted on Affymetrix chips were normalized
by robust multi-array average method (rma) [81]. Similarly,
package limma [82] was applied on Rosetta/Merck Human
44 k 1.1 microarray chip. All the chips were normalized for
background correction and quantile normalization. The
normalized intensity values were log2-transformed
and duplicate probes were averaged. To identify the
differentially expressed genes between healthy and
Alzheimers patients we used limma package by ap-
plying Benjamini and Hochberg's method to control
for false discovery rate (adjusted p-value ? 0.05).
Data curation
Although the current text-mining methods have started
to leverage expert curators to extract PPIs, MTIs, etc.
from text, the extracted information are still prone to
false positives [83]. Moreover, it is not straightforward to
use these systems for retrieval of context-specific triples
due to technological limitations [84]. Hence, the meticu-
lousness of the identified triples to occur in a certain cell
type, disease state, or events captured in AD-specific
documents is not guaranteed. Thus, the need for manual
verification is unavoidable, especially when considering
the full text articles. The previously published test cor-
pus used for evaluating the constructed AD PPI network
contained AD-specific PPIs extracted by the machine
learning approach from 200 full text articles [69]. Man-
ual inspection by the authors resulted in retaining PPIs
from 38 articles that are truly specific to AD, thus dis-
carding 81 % of the originally retrieved articles. Similarly,
we retained only 68 abstracts from 250 articles (27 %)
that were retrieved using our tri-occurrence based ap-
proach for AD MTIs [65]. Thus, we can conclude that
only about 2030 % of the (relation extraction based)
extracted PPIs and MTIs are truly relevant to AD, point-
ing out the need of manual curation.
Similarly, in our recent publication [78], we have
highlighted the key issues related to retrieval and reusabil-
ity of the datasets from public transcriptomics archives,
such as GEO and ArrayExpress. We showed that a simple
keyword based search not necessarily asserts the specifi-
city of the retrieved datasets to the queried disease or
organism. When manually inspected, we reported nearly
20 % of these retrieved studies to be irrelevant for AD
query. In addition, basic metadata annotations such as
age, gender, etc., which strongly contributes to the differ-
ential estimates, were observed to be incomplete. Brazma
et al. [85] had earlier reported that not all the data submit-
ted to GEO or ArrayExpress are MIAME compliant [86].
We additionally noticed these missing annotations being
scattered as unstructured prose in database webpages,
Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 5 of 15
publications, supplementary material, figures, etc., leading
to a steep increase in the needed curation effort. Al-
though the published research articles are rich in anno-
tations, a large number of experiments have missing
citations [87], which have to be added manually. More-
over, inconsistencies between the information stored in
the archives and in the associated publications were
also noted. On an average, about 30 min to 2 h of cur-
ation effort was needed to retrieve pertinent informa-
tion for a single dataset. The outcome of this work
resulted in a highly curated metadata database, Neuro-
TransDB, which is used in this work for extracting rele-
vant AD gene expression studies.
Generation of RDF models
RDF data model
RDF allows the generation of models for processed data
that exchanges information on the Web [82]. The RDF
data model stores all the relationships between different
entities as triples (subject-predicate-object). In RDF
terminology, the subject, the predicate and the object
are known as resources and are represented by a
unique Uniform Resource Identifiers (URIs)" in order
to support global data exchange. Literals are constant
values such as numbers and strings mapped to the re-
sources. Literals can only be used as objects but never
as subjects or predicates.
RDF schemas
We constructed the RDF schemata by abiding the stand-
ard RDF graph notation where an ellipse represents
Resource, an arrow for Property, and rectangle for Literal.
In all the RDF schemas, we have maintained a common
resource representation for the Gene" namespace adapted
from Bio2RDF that maps to the NCBI gene database. For
the namespaces with no available ontologies, we created
an internal namespace, called SCAI". Some of the proper-
ties were described using URIs from Dubin Core Metadata
Element [88].
Four separate schemas (for each data resource) have
been generated that are centered on genes for interoper-
ability, associating each gene product to its official gene
symbol. In the AD PPI schema (see Fig. 2), proteins and
their interactions were represented using the Uniprot
Core Ontology [89]. Supporting literature evidence were
adapted to URIs from Bio2RDF namespaces. The article
resource was linked to its PubMed ID, sentence in which
the interaction has been mentioned, and the associated
journal. Experimental evidence that validates the given
interaction (if any) were mapped to BioPax [90], MGED
[91], ONTOAD [92], and SCAI namespaces. In the MTI
models (see Fig. 3), literature, genes, and proteins name-
spaces were adapted similarly to the PPIs. To represent
the miRNAs, we applied the Bio2RDF namespace that
links it to miRBase database [93].
For the PPI schema encoding the healthy state, as seen
in Fig. 4, we used the same ontologies as in case of AD
PPI. Additional interaction evidence such as brain
region, reference database, experimental evidence, and
literature information were described using Core, BioPax,
and Bio2RDF namespaces.
The microarray schema has two branches that are
linked to the experiment: sample details and differential
expression analysis. The majority of the resources and
properties are linked to URIs from EBI's Atlas (atlas) [94]
and MGED [91] namespaces, cf. Fig. 5. Gene expression
experiments could contain several samples that are
measured in different conditions. A detailed description of
Fig. 2 Schematic representation of the Diseased PPIs in RDF. The figure describes AD specific PPI interactions along with supporting evidence
mined from literature
Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 6 of 15
each sample is needed for accurate analysis. Thus, we
associated each sample to its meta-data annotations,
namely age, gender, organism, organism part, platform,
and phenotype. Organism under investigation is mapped
to NCBI Taxonomy URIs [95]. The factor value of each
sample, i.e., the phenotype information, is described using
the EFO ontology [96]. Each platform array is made up of
multiple probes that may represent a gene. To be able to
retain the expression values for individual probes, we
linked the probe ID resource to platform. However, for
better reasoning, quantitative values retrieved from
statistical analysis are linked to genes and not to probes.
The meta-analysis results, derived from limma [82], such
as differential expression value of a gene and its associated
p-value are all linked to the gene symbols.
Construction, validation and storage of RDF models
We modeled all the triples (represented in the schemas)
using the Apache Jena API [97]. Resources, and Proper-
ties as Java classes were created from the ontologies
using the corresponding in-built methods in the API
and with the help of Schemagen [98].
In order to check for the correctness of our generated
RDF models, we made use of the online service RDF
validator [99]. By using such a service, we verified the
models using their graph and triples representation.
Fig. 4 Schematic representation of Healthy PPIs in RDF. The figure represents PPIs of healthy subjects extracted from
literature and PPI specific databases. The schema also contains meta-information about these PPIs
Fig. 3 Schematic representation of MiRNA-target interactions in RDF. The figure encapsulates miRNA mentions along with their corresponding
gene identifier from literature
Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 7 of 15
Triple stores, such as Virtuoso [100], provides an op-
portunity to store individual or integrated RDF models
in one endpoint. Taking advantage of this, we stored all
the generated RDF models as individual graphs in a sin-
gle Virtuoso instance. Using common URIs (e.g., Gene"
identifier) as the connecting link between these models,
it is possible to traverse through them integratively.
Data mining and analysis
In RDF, all the stored triples are accessible using a com-
mon query language, SPARQL Protocol and RDF Query
Language (SPARQL) [101]. We generated a Java library
with embedded SPARQL queries to ask our endpoint
and the underlying networks biologically relevant ques-
tions. Queries were generated from individual models,
which were further integrated as nested queries to
traverse different graphs. Each query uses the common
Gene URI namespace (which is common across all
models) to pass on the results used to the next nested
query. One possibility to visualize the query results is
the SemScape Cytoscape [102], to represent the return
values as (sub-) graphs again.
Results and discussions
NeuroRDF covers a wide range of curated AD related
data resources, stored as four separate RDF models in a
single Virtuoso endpoint. It tries to address the main
concepts (complementary) that contributes significantly
to unraveling AD pathology.
Differentially expressed genes
For the eight selected microarray datasets, gene expres-
sion analysis was performed between healthy and
diseased patients. Among these, GSE1297, GSE28146,
and E-MEXP-2280 resulted in no differential genes for
adjusted p-value cutoff 0.05. From the remaining studies,
only genes that exhibited a log2 fold change of > 1.5
were selected for analysis. In total, GSE5281 resulted in
4,278 genes under p-value cutoff and 2 up-, and 48 down-
regulated genes for the defined fold change cutoff. Simi-
larly, GSE44770 provided 254 differentially expressed
genes, among which 16 up- and 11 down-regulated were
selected further. In case of GSE44771, we obtained 335
differential genes that contain 11 up and 11 down-
regulated genes that show > 1.5 log2 fold change. For both,
GSE12685 and GSE44768, we obtained 1 and 51 genes
under the p-value cut-off. However, there were no genes
that had log2 fold change of >1.5. The list of all the
differentially expressed genes that were selected for fur-
ther analysis is provided in Additional file 1.
RDF models
Table 1 summarizes the content of the generated triple
store by providing some statistics of all integrated
networks. In total, there are 8353 unique triples in AD
PPI, 1,204,194, 667 unique triples in Healthy PPI, and
20,454 unique triples in gene expression RDF models
(Additional file 2). The number of unique predicates
(relations) for AD and healthy PPIs are 11, whereas
for MTI there are 5 and the gene expression model
Fig. 5 Schematic representation of Gene Expression Data in RDF. This figure represents gene expression data obtained from public resources
such as GEO and ArrayExpress
Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 8 of 15
consists of 16. The number of entities present in
these models range from 300 to 78,852 (cf. Table 1).
In case of the gene expression data, to avoid large
triples we excluded the gene expression values of in-
dividual probes and included information only related
to differential expression. Uploading and querying these
models was not computationally expensive due to lower
set of predicates and relatively small file size.
Prioritization of AD candidates
To illustrate the potential of NeuroRDF approach and to
determine novel AD candidates from the high quality
integrated data, we exploit the underlying biological
association between the different data resources and
identify the previously unknown information.
Our prioritization criteria was based on the notion
that every data resource brings with it a piece of missing
biological information which is needed to understand
the mechanism of a certain candidate. We tried to
associate this distributed information by systematically
addressing the following questions:
(1)Whether candidates in the diseased network tend to
be associated with normal physiology. If yes, what
are the common players that could help us in the
differential estimates (called as causal candidates);
(2)Which microRNAs regulate the selected causal
candidates that could give insights into their post-
transcriptional dysregulation;
(3)Have any of the selected causal candidates assessed
for their level of differential expression in an
unbiased data source (e. g., gene expression data);
(4)How strong is the influence of the neighboring
genes on the casual candidates. This is based on the
assumption that strong candidates tend be
surrounded by dysregulated genes and have an
influence on the candidate itself;
(5)Is there any functional relatedness between the
causal candidates and their neighbors;
To answer these questions, we generated a set of
SPARQL queries. Figure 6 is an example SPARQL query
syntax used to obtain miRNAs that regulate the genes in
the AD networks. Similar querying has been applied to
build a system of faceted searches for the above de-
scribed questions. Firstly, we identified common genes
between the healthy and AD PPI networks. This query
resulted in 230 intersecting genes. Looking into the
MTIs, we found 13 of these genes to be regulated by at
least one microRNA (cf. Table 2). Among these 13
genes, 9 were observed to be differentially expressed:
APP, BACE1, ADAM10, IL1B, MAPK3, DLG4, LRP1
PTGS2, and TGFB1. Except for APLP2, and IL6, all the
other genes contained differentially expressed neighbors
either in AD or in healthy PPIs. There were no miRNAs
that were common to these 13 genes.
Sub-networks from the AD and healthy PPIs were ex-
tracted to investigate the prioritized candidates (see
Figs. 7 and 8). As observed from Fig. 8, for healthy PPIs
there was one larger sub-network (containing APP,
ADAM10, BACE1, MIF, MAPT, and LRP1) and a
smaller one containing two genes (PTGS2, and IL1B).
On the other hand, for diseased PPIs in Fig. 7, there
were two large sub-networks containing four (STAT4,
JUN, MAPK3, and STMN2) and five genes (APP, LRP1,
BACE1, DLG4, and TGFB1). The third sub-network was
made up of two genes (MAPT, and TUBA4A). Among
the prioritized candidates, APLP2 and IL6 had no com-
mon links to other prioritized candidates. Thus, they
were discarded for further analysis.
Relevance of prioritized AD candidates
The remarkability of complementing wet lab research
using the predictability and reproducibility of measured
outcomes is one of the core reasons why researchers are
Table 1 Statistics of generated RDF models stored in Virtuoso
endpoint
Models No. of triples No. of entries No. of
properties
Size (mb)
Alzheimers disease
PPI
8353 19900 11 0.894
Healthy State PPI 1204194 78852 11 99.102
MTI 667 300 5 0.095
Microarray 20454 9477 16 303.5
Fig. 6 Example SPARQL query for information retrieval from NeuroRDF. SPARQL query as seen in the figure retrieves the miRNAs for a given gene
Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 9 of 15
more inclined to the field of bioinformatics. Therefore,
in silico validation of predicted candidates for its rele-
vancy is of utmost importance. In this direction, we pin-
point the relevance of our prioritized candidates through
a literature survey.
AD established candidates
Although there are no FDA approved biomarkers for AD,
researchers focus on some of the key candidates that are
hypothesized to be involved in AD. In the current NDD
research practice, APP has been established as the widely
used biomarker candidate. The classical pathological hall-
mark of AD is formation of amyloid-beta aggregates (lead-
ing to plaques) in brain. This is reported to be caused by
faulty proteolytic processing of APP that releases amyloid-
beta [103]. Another hallmark of AD is tau pathology
(MAPT gene), regulated by amyloid-beta. Hyperphosphor-
ylation of tau causes accumulation of neurofibrillary tan-
gles due to the disrupted functioning of axonal transport
[5]. However, it is also interesting to note the paradigm
shift in AD research due to recently failed drug trails that
focused mostly around these hypotheses [2]. Never-
theless, several neuroscientists still believe in the po-
tential of APP and the tau hypothesis for elucidation
of the underlying pathomechanism. As observed from
our generated sub-networks, our largest sub-network
was established around the APP gene.
When compared to APP, BACE1 has not been so fre-
quently studied. However these genes often fall into the
"most interesting gene zone" as far as AD is concerned
since it is involved in the formation of amyloid-beta.
BACE1 is the major enzyme (beta secretase) involved in
the cleaving of APP at beta site and generating soluble
amyloid-beta [104]. However, increased BACE1 activity
has been reported to be associated with amyloid-beta ag-
gregation in AD patients [105]. Bu et al. have detailed out
the evidence that LRP1 is a receptor for APOE, a contrib-
uting factor to AD [106]. Furthermore, in 1993, Strittmat-
ter, Roses and colleagues [107] have identified APOE4 as
the major risk for late-onset AD. TGFB1 polymorphism
has been widely associated with an increased risk of late-
onset AD. Deficiency in TGFB1 signaling leads to neuro-
fibrillary tangle formation increasing the advancement of
mild cognitive impairment patients to AD, by increasing
the depressive symptoms [108]. DLG4 is a post-synaptic
scaffolding protein that interacts with postsynaptic recep-
tors such as NMDA receptors for efficient postsynaptic
response [109]. However, its impairment has largely con-
tributed to the synaptic degeneration in AD. Mutations in
ADAM10 gene have been associated to late-onset AD.
ADAM10 enzyme has alpha-secretase activity to cleave
amyloid-beta, however BACE1 competes with ADAM10
for cleavage. Thus, its decreased expression has been
implicated in AD pathogenesis [110].
Table 2 Prioritized AD candidate genes
Intersected genes
between healthy
and AD PPI
MiRNAs Differentially expressed
neighbors
Number of
literature articles
for intersected
genes
Healthy PPI AD PPI
APP MIR101-1, ADAM10, TGFB1,
MIR106A, MAPT, BACE1,
MIR106B, MIF, LRP1
MIR124-1, BACE1, 24550
MIR137, LRP1
MIR153-1,
MIR181-C,
MIR29A,
MIR520C,
MIR19-1
BACE1 MIR107,
MIR124-1, APP,
MIR145, APP LRP1 1883
MIR298,
MIR29A,
MIR29B1,
MIR328,
MIR9-1
ADAM10 MIR451,
MIR144,
MIR1306, APP - 231
MIR107,
MIR103
IL1B MIR146A,
MIR155 PTGS2 - 1099
MAPK3 MIR15A, - STMN2, 276
MIR155 JUN
MAPT MIR16-1, APP TUBA4A 3367
MIR132
APLP2 MIR153-1 - - 134
DLG4 MIR485 - LRP1 151
IL6 MIR27B - - 748
JUN MIR144 - STAT4, 142
MAPK3
LRP1 MIR205 APP DLG4, 305
APP,
BACE1
PTGS2 MIR146A IL1B - 474
TGFB1 MIR155 - APP 276
This table summarizes the literature based evidences of intersected genes
between healthy and AD PPI and their corresponding miRNAs and
differentially expressed genes
Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 10 of 15
AD emerging candidates
To identify emerging knowledge in the context of AD, we
performed an individual gene analysis using SCAIView for
publications in PubMed. Here, we measured the co-
occurrence of the causal genes (including its differential
neighbors) and AD over a period of last 10 years, see Fig. 9.
Since the number of articles for the APP gene was rela-
tively too high each year, we normalized the number of lit-
erature evidence of other candidates using the APP gene's
article count for that year. Hence, the normalized range
for the literature distribution is between 0 and 1, where 1
is the highest number of articles for that year (the APP
gene). Please refer to Additional file 3 for details of the lit-
erature counts. Inspecting literature evidence, we found
that all the prioritized causal candidates have been studied
in the context of AD. Moreover, among their differentially
expressed neighbors, STMN2 (8 articles), MAPK4 (1 art-
icle), TUBA4A (2 articles), and MIF (15 articles) contained
fewer articles related to AD. Among these genes, STMN2
and MIF have been recently studied in the context of AD,
whereas, MAPK4, STMN2, and TUBA4A were implicated
in AD nearly two decades before but failed to establish as
robust biomarker candidates.
MIF's role in AD
Macrophage Migration Inhibitory Factor (MIF) has for
long been known to participate in tumor proliferation
due to its pro-inflammatory cytokine functionality [111].
In general, MIF acts as a key regulator of inflammatory
activities such as innate and adaptive immunity [112].
Apart from that, it is also known to play a significant
role as an anti-apoptotic factor of neutrophils as well as
macrophages [113].
The MIF gene has been well studied in cancer and
inflammation. However, recent studies are emerging
around a plausible role of MIF in neurodegenerative dis-
eases, in particular AD. Moreover, Flex et al. [114] have
earlier reported that MIF polymorphisms are not linked
Fig. 7 Extracted sub-networks from AD PPIs network. This figure symbolizes the diseased sub-graphs that were generated using prioritized
candidates and their differentially expressed neighbors
Fig. 8 Extracted sub-networks from healthy PPIs network. This figure
symbolizes the healthy sub-graphs that were generated using
prioritized candidates and their differentially expressed neighbors
Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 11 of 15
to AD, but confirmed its complex immune and inflam-
matory activities. Although, APP and tau have been as-
sociated to play a key role in the pathophysiology of AD,
many researchers strongly believe in the role of inflam-
matory processes subsidizing to the pathology of AD.
This stems from the fact that activated microglial cells
discharge immunoregulatory cytokines which result in
various side-effects such as neuronal dysfunction and in-
hibition of hippocampal neurogenesis [115]. MIF is one
such pro-inflammatory cytokine which is known to bind
with amyloid-beta protein and enhance the plaque re-
moval and neuronal debris from the brain during normal
conditions [116]. Also, MIF has been identified to play a
role in neuronal survival by inhibiting the activation of
ERK-1/MAP kinases [117] (regulatory role in cell prolif-
eration and glucocorticoid action) as well as its ability to
surpass the p53 mediated apoptosis [118]. Although, the
precise molecular function of MIF in the context of AD
is unknown, it is known to play a role in inflammatory
processes around the plaque formation. MIF is also
highly expressed in the neurons of rat hippocampus, one
of the primary regions to be affected by AD [117]. Bryan
et al. [119] also report on the abnormal expression of
MIF in both microglia and in the hippocampal neurons
in human. This all makes MIF a plausible biomarker for
inflammatory responses in AD.
Conclusion
NeuroRDF approach has been designed to identify new
knowledge through semantic mining. The proposed inte-
grative approach takes advantage of the RDF technology
to integrate well-curated data from various sources
within a specific indication area. From our perspective,
it is necessary to focus on one indication or at least a
group of indications to build such a knowledge base for
precise modeling and analysis due to the high curation
effort one has to spend in order to reach the necessary
details. We showed how to harmonize three major het-
erogeneous resources (databases, gene expression data,
and literature) used in the research area to generate
hypotheses for underlying disease mechanisms. This ap-
proach supports identification of novel insights without
compromising over quality. Furthermore, new data re-
sources can be included without altering the overall
framework. The usage of well-accepted ontologies pro-
vides the advantage for further integration of external re-
sources and databases (e.g., federated queries). Using
such an approach, we were able to prioritize MIF gene
as an emerging candidate due to its role in inflammatory
processes implicated in AD pathogenesis.
The advantage of using an RDF schema is that it is
highly supportive for data interoperability. Although this
work represents the usage of the RDF schema specific
for AD, we have also extended the same to other disease
models such as Parkinson's and Epilepsy. However, the
curated data and the generated hypothesis for these two
diseases will be released in future under the terms of a
Neuroallianz agreement [120]. Also, these resources are
constantly kept up-to-date as they are transferred to
various upcoming projects such as AETIONOMY [121].
Additional files
Additional file 1: List of differentially expressed genes. This file contains
the list of differentially expressed genes (for each dataset used) that fall
under the adjusted p-value cutoff of 0.05. The differential expression
analysis was performed using limma package in R statistical environment.
The file is provided in an Excel format. (XLSX 68 kb)
0
0.05
0.1
0.15
0.2
2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016
BACE1
ADAM10
IL1B
MAPK3
MAPT
APLP2
DLG4
IL6
JUN
LRP1
PTGS2
TGFB1
MIF
STMN2
TUBA4A
STAT4
N
or
m
al
iz
ed
 c
ou
nt
 o
f l
ite
ra
tu
re
 e
vi
de
nc
es
Year
Fig. 9 Statistics of the literature evidence for emerging candidate genes in the speculated sub-networks
Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 12 of 15
Additional file 2: The developed RDF models and the SPARQL queries
used are made available at: http://www.scai.fraunhofer.de/en/business-
research-areas/bioinformatics/downloads/neurordf.html. (ZIP 178 kb)
Additional file 3: Detailed count of literature evidences for prioritized
candidates. This file contains the detailed count of number of evidences
available for each prioritized candidate for each year since 2005 in
context of Alzheimer's disease. These statistics were retrieved using
SCAIView knowledge discovery tool (as of 18 May, 2016). (XLSX 35 kb)
Acknowledgement
We are grateful to Matthew Page, Translational Bioinformatics, UCB Pharma for
providing his valuable inputs during the design of the project and reviewing
the manuscript. We are thankful to Erfan Younesi and Ashutosh Malhotra for
providing the healthy state PPI and AD-PPI network respectively for this work.
We also want to thank Christian Ebeling for his support in building the resources
for gene expression data. We would like to acknowledge the Semantic Mining
in Biomedicine (SMBM2014) conference organizers, participants, and
reviewers for inspiring discussions during the conference. The authors express
gratitude to the SMBM2014 conference organizers for providing an opportunity
to submit to the Journal of Biomedical Semantics an extended version of the
initially published conference proceeding paper.
Funding
This study was funded by a grant from the German Federal Ministry for
Education and Research (BMBF) within the BioPharma initiative "Neuroallianz".
Authors contributions
AI, SBK, PS, and MHA conceived and designed the overall research strategy
required for data integration. PS is the scientific supervisor to this work. SBK
and AI are the main contributors to manuscript writing. TR contributed to
the analysis of gene expression data. PS, and MHA reviewed the content.
All authors read and approved the final version of the manuscript.
Competing interests
The authors declare that they have no competing interests.
Declarations
The underlying principles of this article have been previously published in
Proceedings of the 6th International Symposium on Semantic Mining in
Biomedicine (SMBM2014), Aveiro, Portugal, 2014.
Author details
1Department of Bioinformatics, Fraunhofer Institute for Algorithms and
Scientific Computing (SCAI), Schloss Birlinghoven, 53754 Sankt Augustin,
Germany. 2Bonn-Aachen International Center for Information Technology,
Rheinische Friedrich-Wilhelms-Universität Bonn, 53113 Bonn, Germany.
3University of Applied Sciences Koblenz, RheinAhrCampus,
Joseph-Rovan-Allee 2, 53424 Remagen, Germany.
Received: 1 March 2015 Accepted: 23 May 2016
Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 
DOI 10.1186/s13326-016-0070-4
RESEARCH Open Access
Filtering large-scale event collections
using a combination of supervised and
unsupervised learning for event trigger
classification
Farrokh Mehryary1,2*, Suwisa Kaewphan1,2,3, Kai Hakala1,2 and Filip Ginter1
Abstract
Background: Biomedical event extraction is one of the key tasks in biomedical text mining, supporting various
applications such as database curation and hypothesis generation. Several systems, some of which have been applied
at a large scale, have been introduced to solve this task.
Past studies have shown that the identification of the phrases describing biological processes, also known as trigger
detection, is a crucial part of event extraction, and notable overall performance gains can be obtained by solely
focusing on this sub-task. In this paper we propose a novel approach for filtering falsely identified triggers from
large-scale event databases, thus improving the quality of knowledge extraction.
Methods: Our method relies on state-of-the-art word embeddings, event statistics gathered from the whole
biomedical literature, and both supervised and unsupervised machine learning techniques. We focus on EVEX, an
event database covering the whole PubMed and PubMed Central Open Access literature containing more than 40
million extracted events. The top most frequent EVEX trigger words are hierarchically clustered, and the resulting
cluster tree is pruned to identify words that can never act as triggers regardless of their context. For rarely occurring
trigger words we introduce a supervised approach trained on the combination of trigger word classification produced
by the unsupervised clustering method and manual annotation.
Results: The method is evaluated on the official test set of BioNLP Shared Task on Event Extraction. The evaluation
shows that the method can be used to improve the performance of the state-of-the-art event extraction systems. This
successful effort also translates into removing 1,338,075 of potentially incorrect events from EVEX, thus greatly
improving the quality of the data. The method is not solely bound to the EVEX resource and can be thus used to
improve the quality of any event extraction system or database.
Availability: The data and source code for this work are available at: http://bionlp-www.utu.fi/trigger-clustering/.
Keywords: BioNLP, Event extraction, Trigger detection, Word embeddings
*Correspondence: farmeh@utu.fi
1Department of Information Technology, University of Turku, Turku, Finland
2The University of Turku Graduate School (UTUGS), University of Turku, Turku,
Finland
Full list of author information is available at the end of the article
© 2016 Mehryary et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 2 of 13
Background
The overwhelming amount of biomedical literature
published annually makes it difficult for life science
researchers to acquire and maintain a broad view of
the field, crossing the boundaries of organism-centered
research communities and gathering all of the informa-
tion that would be relevant for their research. Modern
natural language processing (NLP) techniques strive to
assist the researchers with scanning the available literature
and aggregating the information found within, automati-
cally normalizing the variability of natural language state-
ments. The applications of NLP in life sciences range from
automated database curation and content visualization to
hypothesis generation and offer intriguing challenges for
both NLP and life science communities [13].
As a response to the need for advanced literature min-
ing techniques for the biomedical domain, the BioNLP
(Biomedical Natural Language Processing) community
of researches has emerged. The primary focus of the
majority of research within the BioNLP community is
to improve information retrieval (IR) and information
extraction (IE) in the domain.
In this paper we focus on the task of event extraction, a
task that has received much attention in BioNLP recently.
Event extraction constitutes the identification of biolog-
ical processes and interactions described in biomedical
literature, and their representation as a set of recur-
sive event structures. In its original form, introduced in
the 2009 BioNLP Shared Task on Event Extraction (ST)
[4], the task focused on gene and protein interactions,
such as RNA transcription, regulatory control and post-
translational modifications. In subsequent Shared Tasks,
while the overall setting remained unchanged, the task
has been broadened to cover a large number of additional
biological domains and event types [5, 6].
More specifically, event extraction involves detecting
mentions of the relevant named entities which are typi-
cally genes and gene products (GGPs), the type of their
interaction from a small vocabulary of possible types, the
trigger expression in the text which states the event, and
the roles of the participants in the event, e.g. regulator
or regulatee. One of the distinguishing features of events
is that they can recursively act as participants of other
events, forming recursive tree structures which precisely
encode the factual statements in the text, but are a chal-
lenging extraction target. An example of an event is shown
in Fig. 1.
A number of event extraction systems have been intro-
duced as the result of the series of BioNLP Shared
Tasks. Most of these systems focus solely on the
immediate textual context of the event candidates, but
recently approaches benefiting from bibliome-wide data,
either through self-training or post-processing steps, have
been introduced as well [79]. Unfortunately the recent
advancements in this field have been modest, reflecting
the complexity of the task. As an example, the best per-
forming system in ST09, TEES (Turku Event Extraction
System) [10], has remained a state-of-the-art approach,
winning also several categories in the later Shared Tasks,
although the performance of the system has not increased
substantially during these years.
Several event extraction systems have been applied at
a large scale, extracting millions of events from massive
text corpora [11, 12]. These large corpora, typically the
totality of PubMed abstracts and PubMed Central full-
text articles, contain a number of documents which are
partly or entirely out-of-domain for these systems, being
unlike the carefully selected data from narrow biological
domains on which the systems have been trained. Fac-
ing documents from such previously unseen domains, the
systems often produce suboptimal output, making what
seems to a human like trivial mistakes. Tuning the per-
formance of these systems in the general domain requires
further effort.
Here we focus on EVEX [11], an event database cov-
ering the whole PubMed and PubMed Central Open
Access (PMC-OA) literature, produced using the afore-
mentioned TEES system. Already a casual inspection of
EVEX reveals occasional occurrences of obviously incor-
rect events especially in out-of-domain documents. Previ-
ously, Van Landeghem et al. [13] have studied the output
of the event extraction systems on general domain data
in further detail. Their analysis resulted in a set of rules
that can be used to remove or correct erroneous events.
Although applying this method produced only an increase
of 0.02pp in F-score when evaluated on the official Shared
Task data, the consequences on large-scale resources such
as EVEX are significant: hundreds of thousands of false
events can be excluded, thus greatly improving the quality
of the extracted data. This is because the official Shared
Task test data does not contain the out-of-domain docu-
ments found in the corpora used to build EVEX and many
of the error types made by the system will not be seen in
the test set output.
Van Landeghem et al. point out that a large portion
of the false event predictions originate from the trigger
detection phase, i.e. false positive identification of the tex-
tual spans expressing the biological processes underlying
the events. These, in turn, lead to the generation of false
positive events by the system. Here it is important to
take into consideration that the top-ranking event extrac-
tion systems are based on machine learning and do not
uniquely rely on a list of possible safe trigger words,
which would result in an excessively low recall. Instead,
any word can become a trigger word, which occasion-
ally leads to wildly incorrect predictions. These, in turn,
are easily spotted by the users of the event databases and
decrease the perceived credibility of the resources.
Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 3 of 13
Fig. 1 Visualization of a specific event occurrence. Genes and gene products (GGPs) are marked, as well as the trigger words that refer to specific
event types. Finally, arrows denote the roles of each argument in the event (e.g. Theme or Cause). (Adapted from [23])
In this paper, we thus focus specifically on the event
triggers in the EVEX event database, with the objective
of automatically identifying and removing those that are
obviously incorrect. To solve this problem, we introduce
a novel approach based on word embeddings, bibliome-
wide statistics and both supervised and unsupervised
machine learning techniques.
Since our method relies on bibliome-wide statistics that
should be gathered from a large-scale biomedical event
database, it serves as a post-processing step in event
extraction pipeline to filter out incorrect events from that
database, after the events are extracted.
Method
In this section, we first introduce the data that is used in
this study and then propose a 6-step method to achieve
the aforementioned objectives.
In the first five steps of our method, we focus on the
top most frequent trigger words which account for 97.1%
of all events in EVEX. In steps 1, 2, 3 and 4 we per-
form hierarchical clustering of these trigger words and
build, analyze and prune the resulting binary tree to cat-
egorize these triggers as correct/incorrect. In step 5, we
refine these two sets using manual annotation. Finally in
step 6, we build a predictive model based on support vec-
tor machines (SVM) to classify the triggers as correct or
incorrect.
Data
This study is based on the EVEX resource [11] contain-
ing 40,190,858 events of 24 different types such as binding,
positive-regulation, negative-regulation, and phosphoryla-
tion. These events are extracted using the TEES system
[14] from 6,392,824 PubMed abstracts and 383,808 PMC-
OA full-text articles that were published up to 2012 and
which contain at least one gene/gene-product mention.
The EVEX resource can be downloaded and browsed
online at www.evexdb.org.
Trained on the ST-data sets, TEES extracts events based
on the recognition of an occurrence of a trigger word in
the underlying sentence. An event is thus representing
the link between the event trigger word and participat-
ing argument GGPs. However, one textual span can act
as a trigger for multiple events with varying arguments as
illustrated in Fig. 2.
In addition, a single unique trigger word, such asmodify,
may have a number of occurrences in the data, acting as a
trigger for many events. It is important to note that these
events may be of different types. For instance the trigger
word expression acts as a trigger for both gene-expression
and transcription events, depending on the context.
Throughout this paper, we refer to the total number of
extracted events from a trigger as trigger frequency and
to the actual occurrence count of the trigger in the corpus
as trigger occurrence count. Clearly, trigger frequency is
greater or equal to trigger occurrence count since one trig-
ger occurrence can be associated withmultiple events. For
example, the frequency of expression is 3,909,759, while
its occurrence count is 2,736,782. It should be highlighted
that the aim of this study is to increase the precision of
extracted events, thus the focus is on the trigger frequency,
i.e. the number of incorrect events that are finally removed
from EVEX, when a particular trigger is identified as
incorrect.
In total, there are 137,146 unique event triggers (exclud-
ing obviously incorrect trigger words that are purely num-
bers and those which contain unicode special characters).
Different trigger words have different frequency in the
system ranging from 1 to 3,909,759.
As expected, the vast majority of events in EVEX corre-
spond to a small number of highly frequent trigger words,
as shown in Table 1. For example, there are only 3,391 trig-
ger words with frequency above 300 (i.e. corresponding to
at least 300 event occurrences), but these words account
for 97.1% of all events in EVEX. Consequently, when the
aim is to increase the precision of the events in EVEX by
recognizing incorrect trigger words and eliminating them,
the focus should be centered on highly frequent trigger
words instead of the rare ones. Accordingly, we decided
to concentrate on these 3,391 top most frequent trigger
words. Limiting ourselves to the top most frequent trig-
ger words allows manual inspection of the hierarchical
clustering tree discussed in the following sections.
Among the trigger words, we will target those which
are obviously incorrect, regardless of their context. These
could be for example, gene/protein/chemical names,
author names or any other words such as hospital, univer-
sity, research, diagram, box, clarify, investigate, visualiza-
tion, knowledge, one or please. The main objective of this
study is thus to develop a method that can categorize the
Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 4 of 13
Fig. 2 Example sentence with multiple events sharing a single trigger. Two event occurrences extracted from the same trigger word recognized
trigger words so as to eliminate the obviously incorrect
trigger words, thus increasing the precision of the event
extraction systems without impacting their recall.
Another interesting aspect when studying the trigger
words is to build a general overview of all of the trig-
ger words according to the 24 different event types and
to study whether there exist groups/sub-groups of related
trigger words which would allow us to define subtypes of
the 24 event types. Of specific interest will be studying the
groups/sub-groups before and after eliminating incorrect
trigger words.
Hierarchical clustering of topmost frequent trigger words
In the first step, we induce a vector space representa-
tion for the trigger words, and hierarchically cluster the
triggers based on this representation. Cosine similarity is
used as the clustering metric with the Wards variance
minimization algorithm defining the distances between
newly formed clusters. To build the vector space repre-
sentations, we use the word2vec method of distributional
semantics introduced by Mikolov et al. [15] and previ-
ously applied in the biomedical domain by Pyysalo et al.
[16]. The word2vec method comprises a simplified neu-
ral network model with a linear projection layer and a
hierarchical soft-max output prediction layer. The input
Table 1 Distribution of triggers and their associated event
percentages in the EVEX database
Trigger word frequency EVEX events coverage Number of trigger
(at least) percentage words
100 98.4 6339
200 97.6 4263
300 97.1 3391
400 96.6 2880
500 96.3 2538
layer has the width of the vocabulary, while the projection
layer has the desired dimensionality of the vector space
representation. Upon training, the weight matrix between
the input and the projection layer constitutes the word
vector space embeddings. The network can be trained
in several different regimes, but in this work we use the
skip-gram architecture, whereby the network is trained to
predict nearby context words, given a single focus word at
the center of a sliding window context.
We train the word2vec model on the lower-cased texts
from the EVEX resource, i.e. all abstracts and full-text arti-
cles in which at least one GGP was identified. All GGP
mentions in the texts are replaced with the ggp place-
holder and all numbers with the num placeholder to
densify the text.
An initial experiment in hierarchical clustering of the
top 100 most frequent trigger words revealed that on one
hand many coarse/fine grained sub-clusters were formed
in a way that each sub-cluster contained trigger words
with biologically similar meaning. Many sub-clusters
could be clearly associated with a unique event type.
On the other hand, many trigger words were clustered
together incorrectly, especially for the common positive-
regulation and negative-regulation types (e.g. increase and
decrease) because they have a high similarity in the vector
space representation.
To address this issue, we add trigger/event type asso-
ciation information as additional dimensions to the word
vectors, thereby affecting the clustering to more closely
conform to the event types. To obtain reliable event type
distribution for the trigger words, we use the BioNLP
Shared Task 2011 (ST11) training and development sets
[5]. Out of the 1,447 unique trigger words in this data,
995 are single-token trigger words and of these, 828 are
actually among the top 3,391 most frequent EVEX trig-
ger words. For these 828 triggers, we append a normalized
event type distribution vector to their word2vec-based
Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 5 of 13
vectors (the vectors for the remaining 2,563 triggers
for which a reliable event type information could not
be obtained are simply padded with 24 zeroes). Re-
clustering with the modified vectors, we notice that
positive-regulation and negative-regulation trigger words
are no longer clustered together, obtainingmoremeaning-
ful clusters with regard to the task at hand.
Event type vectors of sub-clusters
In this step, event type distribution vectors for all nodes of
the binary cluster tree are calculated. For each leaf of the
tree (i.e., a trigger word), its corresponding trigger/event
type vector is calculated based on the occurrence counts
of its respective events in EVEX, and for each interme-
diate node of the tree (i.e., a sub-cluster), its respective
event type vector is calculated by adding trigger/event
type vectors of all triggers that belong to it.
Using this information, it is possible to inspect how
the tree is organized and whether and how its different
branches represent different event types. For example, by
checking which element in a sub-clusters event type vec-
tor has the maximum value, we can tell what is the event
type that this sub-cluster is mostly associated with and
the level of purity of that cluster. For example, while one
sub-cluster can be 98% binding and is thus to a large
extent pure, another cluster can be 43% gene_expression
and cannot be assigned a single predominant type.
Identifying possibly incorrect trigger words
Focusing on 3,391 top most frequent trigger words, in this
step we prepare a list of safe or supposedly correct trigger
words and regard the remaining triggers as possibly incor-
rect. This is necessary for pruning the tree and finding the
list of incorrect trigger words in the next step.
As stated in Section Hierarchical clustering of top most
frequent trigger words, by analyzing the ST11 training
and development sets, we obtain a list of 995 unique
single-token trigger words. Some of these triggers are
overlapping with EVEX triggers. However, our list con-
tains many other trigger words that can not be directly
found in the ST11 sets, but variations of them or vari-
ations of their parts can. For instance, processing and
co-regulation are in the EVEX-based list, while processed
and regulation are in the ST11 sets.
We therefore process BioNLP ST09 [4], ST11 [5], and
ST13 [6], training and development sets, to obtain a set
of all single-token ST-trigger words. This trigger set, here-
after ST-set, contains 1,092 trigger words. Then we per-
form the following preprocessing steps on every trigger
word in both EVEX and ST-set.
1. Remove any punctuation or special characters from
the beginning of the trigger word, retaining the rest
of the word as the trigger word. For example,
-stimulated is transformed into stimulated.
2. We split each trigger word based on occurrences of
the following characters: {-, . , _ , /}. For example,
co-express is divided into co and express, and
similarly cross-reacts is divided into cross and reacts.
3. For every trigger word, each of its split parts is saved
if all of the following conditions are met:
(1) The part is longer than one character.
(2) The part is not a number.
(3) The part is not in the following stop list: {32p,
auto, beta, cis, co, cross, de, double, down,
mono, non, out, poly, post, re, self, trans,
under}. We obtained this list experimentally
by careful examination of the ST-set.
4. Finally, we lemmatize all the trigger words, and all of
their parts, using the BioLemmatizer tool [17] which
is specifically developed for the biomedical domain,
and record all the produced lemmas for each trigger
word.
After the preprocessing, 977 EVEX trigger words that
can directly be found in the ST-set are regarded as safe.
The rest of the triggers are regarded as safe if their exact
form, or one of their parts, or one of the lemmas of their
parts can be found in the ST-set, or ST-set words parts or
part lemmas. Otherwise, the trigger word is regarded as
possibly incorrect.
Performing the aforementioned approach resulted in
identification of 506 trigger words which were added to
the list of safe triggers, totaling to a list of 1,483 safe trigger
words. The 1,908 remaining triggers are regarded as pos-
sibly incorrect. Table 2 shows some example words from
EVEX triggers in our list that are matched against ST-set
trigger words, parts, or lemmas.
As discussed earlier, we do not save parts of the EVEX
trigger words if they belong to our stop list. The stop
list comprises the prefix parts obtained by splitting ST-set
trigger words, which are not themselves ST-set trigger
words. For example, cross-link is a ST-set trigger word, but
cross itself is not a stand-alone ST-set trigger, therefore
Table 2 Examples of matching EVEX trigger words against
Shared Task exact trigger words or their corresponding
parts/lemmas
EVEX trigger word ST11-trigger word/Part/Lemma
co-transcribed transcribed
calcium-induced induced
co-immunoprecipitates immunoprecipitate
downregulating downregulate
recognise recognize
preceding precede
analyzing analyse
Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 6 of 13
cross is included in the stop list. As a contrary example,
up-regulation is a ST-set trigger word, however we did not
include up in the stop list because up itself is a ST-set trig-
ger word. We perform such approach because we do not
want any EVEX trigger word like re-X or cross-X (X can be
any word) to be matched against ST-set words, parts and
lemmas, just because it has a re or cross as a prefix.
Pruning the tree
Pruning the tree is done using the list of possibly incorrect
trigger words in four steps:
1. If a trigger word exists in the list of possibly incorrect
trigger words, its corresponding leaf is marked as
unsafe, otherwise it is marked as safe.
2. If all of the children of an intermediate node are
marked as unsafe, this node (sub-cluster as a whole)
is marked as unsafe as well, otherwise it is marked as
safe.
3. All of the descendants of the intermediate nodes that
are marked as unsafe, are deleted from the tree. The
respective trigger words of the deleted leaves are
subsequently added to the list of incorrect trigger
words.
4. After tree pruning, the trigger words of all leaves that
remain in the tree, are marked as safe and regarded
correct.
After applying the aforementioned tree pruning algo-
rithm, we obtain a set of correct and a set of incorrect top
most frequent EVEX trigger words.
There is one important aspect in the pruning algorithm.
Since the tree is binary, not all of the trigger words that
are in the list of possibly incorrect triggers were finally
regarded as incorrect trigger words, because if such a trig-
ger word was clustered near a safe trigger word (i.e., had a
very small cosine distance to a safe trigger word in the fea-
ture space), it was not considered as an incorrect trigger
word and remained in the tree. This helps us to identify
more correct trigger words.
For example, co-localization which is an EVEX trig-
ger word is also a Shared Task trigger word, so it had
been marked as safe in the matching step, however
colocalization (another EVEX trigger word) originally had
been regarded as possibly incorrect, because our match-
ing procedure could not have matched this trigger word
(or its lemma) against any ST-set trigger word or part or
lemma. However, because these two words are extremely
similar in the vector space representation, they clustered
together in the binary cluster tree. Consequently, since an
unsafe trigger was clustered with a safe trigger, that whole
sub-cluster was regarded as safe and remained in the tree,
so colocalization finally is regarded as a correct trigger
word. To summarize, the tree pruning algorithm causes
deletions to be propagated to the upper level nodes of the
tree only if all of the participating leaves are recognized as
incorrect.
After pruning, event type vectors for all intermediate
nodes of the tree are recalculated so that we can compare
the tree before and after pruning.
Refining correct and incorrect trigger sets
The output of the tree pruning step are the correct and
incorrect trigger words sets, into which the top most fre-
quent EVEX trigger words are assigned. As discussed
in Section Results, our unsupervised method (steps 1-4)
increases the precision and F-score of event extraction
systems, however it causes a comparatively small drop
of recall. This means that some of the correct trigger
words are erroneously included in the incorrect trigger
set, thus deleting their corresponding events from EVEX
consequently decreases the recall of that event extraction
system. As our objective is to increase the precision with-
out decreasing the recall, i.e. we try to avoid removing
correct events from EVEX at any cost, we address the issue
using manual annotation to refine the results.
Manual annotation of triggers
A list of 3,391 trigger words was prepared by extract-
ing the trigger words with frequency of at least 300 from
EVEX. As discussed in Section Identifying possibly incor-
rect trigger words, 977 of EVEX topmost frequent triggers
overlap with the ST-set. We assume these triggers to be
correct and provided the 2,414 remaining trigger words to
an annotator with prior experience in biomedical domain
annotation.
The annotator performed the manual annotation by
deciding for each trigger whether it is correct or incor-
rect. On one hand, a trigger is correct if its occurrence can
lead to the extraction of one or more of the 24 Shared
Task event types, i.e. the given trigger word can repre-
sent at least one of the ST event types in some context,
although in another context they might still be invalid. On
the other hand, an incorrect trigger cannot express Shared
Task events in any context. The annotator was allowed
to use any available resources, such as NCBI [18], Gene
Ontology [19] and KEGG [20] databases, to support the
annotation.
The annotation of the top most frequent EVEX triggers
resulted in three categories:
 2083 triggers were annotated as correct.
 577 triggers were annotated as incorrect.
 731 triggers were not annotated and remained
undecided.
In a closer look at the annotation data, the most
common correct triggers are the words specifically
used in biomedical domains such as gene expression,
regulation and transcription to state the events. The
Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 7 of 13
incorrect triggers are mostly biomedical entities such as
genes, proteins and chemicals. While the majority of the
triggers (2660/3391, 78.44%) can be annotated, the anno-
tator was unable to make a decision for 21.56% of the
triggers. Most of these undecided triggers are multiple-
meaning words used in both biomedical and generic
domains such as conserved, deletion, and develop-
ment. Thus it is possible to construct hypothetical sen-
tences where these words are valid triggers, but the anno-
tator was not able to find any evidence supporting the
use of these words as triggers from the existing literature.
While going through all the sentences would be an ideal
solution to resolve this issue, it is impossible in practice
due to the vast amount of the data.
As this evaluation was conducted by a single annota-
tor, we have not assessed the inter-annotator agreement
(IAA) for this task. To our knowledge, the organizers of
the BioNLP Shared Task have not reported the IAA for
the GE data set either. For the EPI data set (Epigenet-
ics and Post-translational Modifications) the organizers
report agreement level of 82% measured in F-score [21].
This evaluation, however, measures the annotation of the
full event structures and no direct conclusions can be
made for the trigger annotations.
Aggregating unsupervisedmethod results withmanual
annotation results
In this step, we aggregate the results from tree prun-
ing (Section Pruning the tree) and manual annotation.
We naturally prioritize the manual annotation, i.e., in the
aggregated data a trigger remains correct or incorrect if
labeled as such in the manual annotation. The undecided
triggers are assigned using the tree pruning method. As a
result, the final set is comprised of 2,242 correct triggers
and 1,149 incorrect triggers.
Classification of low-frequency event triggers
In the previous steps, the focus was on assigning a label for
top most frequent trigger words (those with frequency of
at least 300) which account for 97.1% of all EVEX events.
However, this demanding manual annotation method can
not be applied to the huge number of triggers with lower
frequency that exist in EVEX. To address this problem,
we use support vector machines (SVM) to classify the
low-frequency triggers (i.e., triggers with frequency below
300). As the training data, we use the aggregated trigger
set from the previous section, assigning correct and incor-
rect triggers as positive and negative examples, respec-
tively. Our training set totals 3,391 training examples,
consisting of 2,242 positive and 1,149 negative examples.
We optimize the model using grid-search combined with
cross-validation.
Prior to building the model, we considered two impor-
tant aspects which should be highlighted here. First, we
prefer a conservative predictive model which tends to
have a very high positive recall, because if the classi-
fier mispredicts a correct trigger as incorrect, all of its
respective events mistakenly will be deleted from the
output of event extraction systems which is very unde-
sirable and that can also have a huge adverse effect on
the recall of events. Conversely, if the classifier mistak-
enly predicts an incorrect trigger as correct, its respective
events will remain in the output of event extraction sys-
tems, and in general we prefer to tolerate false events
instead of deleting correctly extracted events. Because
of this reason, and because our training set is imbal-
anced, we give weight 10 to the positive class and weight
1 to the negative class during classifier training. These
weights are set experimentally during the grid search and
classifier optimization. In addition, during optimization,
instead of optimizing against F1-score we optimize against
F2-score, because F2-score weights recall higher than
precision.
Second, from the point of view of event extrac-
tion systems, the respective events of the triggers are
more important than the trigger words themselves. For
instance, misclassifying a correct trigger with frequency
of 200 will translate into removing 200 correct events,
comparing it with the removal of a correct trigger with fre-
quency of only 1. Consequently, we consider the precision
and recall of respective events (not the triggers) and adjust
the parameter optimization and training accordingly:
 During optimization, instead of optimizing against
the F2-score of triggers, i.e., calculating F2-score
based on the counts of true-positives (TP),
true-negatives (TN), false-positives (FP) and
false-negatives(FN), we optimize against F2-score of
trigger frequency, i.e., calculating F2-score based on
the sum of frequencies of TP, TN, FP and FN.
 We give a weight to each training example by
calculating the logarithm of its frequency.
Thus the training examples with higher weights, i.e.
higher event frequency, will be regarded more
important than lower weight examples, those with
lower event frequency. In other words, classifier will
be penalized more on misclassifying the frequent
trigger words than lesser ones during training and
k-fold cross-validations. As a result, the classifier is
trained towards better performance on more
frequent triggers while we intentionally do not give
the trigger frequency as a feature to the classifier.
Below is the set of features used by the classifier.
1. word2vec-based vector for each trigger, which is
exactly the same vector discussed in
Section Hierarchical clustering of top most frequent
trigger words.
Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 8 of 13
2. If the trigger word (or its lemma or its parts or
lemmas of its parts) can be matched against ST-set
words/parts/lemmas (according to
Section Identifying possibly incorrect trigger words)
this feature is 1, otherwise it is zero.
3. The value for this feature is calculated as following:
feature_value = occurrence count of trigger / X
where X is the sum of occurrences of the word in all
PubMed abstracts and PMC-OA full text articles
published up to 2013, regardless of being recognized
as a trigger word in the underlying sentences or not.
For many incorrect trigger words, this feature will
have a very low value. For example, for an incorrect
trigger word like hospital which is in EVEX (not in
training set), the value will be (928 / 1,266,408) =
0.0007.
4. For this feature, we first extract the set of single-token
triggers with length of more than 6 characters in the
ST-set, introduced in Section Identifying possibly
incorrect trigger words. Then, for each training
example we calculate the feature value as following:
feature_value = length(trigger word) / (Y + 1)
where Y is the minimum edit distance (Levenshtein
distance) of the trigger word to the words in the
previously created set. For all training examples that
belong to the ST-set, we assume Y to be zero.
The longer the trigger word, and the smaller its
minimum edit distance, the higher will be the value
of this feature.
This feature is beneficial for example in the case of a
misspelled trigger (e.g., phosphoryalation instead of
phosphorylation), which is not recognized correctly
by our matching protocol discussed in
Section Identifying possibly incorrect trigger words.
5. Number of alphabetic characters divided by the
length of the trigger word.
We perform a grid-search combined with 5-fold cross-
validation to optimize the classifier and find the best
hyper-parameters for the model (kernel type, C value, and
the gamma parameter for RBF-kernel) against the F2-
score of trigger event frequency. Subsequently, we train
the classifier using the best parameter values on all avail-
able training examples.
Results
In this section, we discuss the results in four parts. First,
in Section Evaluation of event filtering, we evaluate the
impact of trigger pruning on event extraction systems.
We then evaluate our predictive model and investigate
the effect of event filtering on the EVEX resource in
Sections Evaluation of low-frequency trigger classification
and Evaluation of event removal on the EVEX resource.
Finally, in Section Tree organization before/after pruning
we examine the trigger cluster tree organization before
and after the pruning.
Evaluation of event filtering
Evaluationmethod
We evaluate the impact of trigger pruning on event extrac-
tion using the official test sets of the BioNLP ST11 and
GENIA Event Extraction (GE) Shared Tasks (ST13). As
the basis we consider the outputs of the TEES system
entry [10, 14] in 2011 (3rd place) and in 2013 (2nd place)
GE tasks and, for the 2013 Shared Task, also the winning
EVEX entry [7]. We prune the outputs of these systems
by removing events whose trigger words are identified as
incorrect using the aforementioned algorithm and eval-
uate the resulting pruned set of events using the official
evaluation services of the respective Shared Task on the
held-out test sets. The results are shown in Table 3.
It should be highlighted that naturally the magnitude of
the F-score improvements is modest, as the top-ranking
systems are well optimized and major improvements have
been difficult to achieve regardless of the approach. Note
also that a filtering approach such as the one proposed in
this paper cannot increase recall because it is unable to
produce new events. Our main focus thus is on improv-
ing the precision while trying to retain the recall, aiming
to increase the credibility of large-scale event extraction
systems in general.
Evaluation of unsupervisedmethod
In this section, we investigate the effect of removing
triggers from event extraction systems using the set of
incorrect trigger words obtained from our unsupervised
method in Section Pruning the tree.
As shown in Table 3, in all three instances (compar-
ing our unsupervised method against the TEES systems
predictions on tasks 2011 and 2013, and the EVEX sys-
tems predictions on task 2013), we see an improvement
in both precision and F-score with a relatively small drop
in recall. Especially for the ST13, the pruned TEES sys-
tem (+0.23pp F-score over TEES) matches in performance
with the winning 2013 EVEX system. Since the EVEX sys-
tem was also based on TEES, it is interesting to note that
we have matched these improvements using a different
approach. Finally, the pruned EVEX system (+0.18pp F-
score over the EVEX entry) establishes a new top score on
the task.
Evaluation ofmanual annotationmethod
In this section we investigate the effects on event extrac-
tion if we rely ourmethod solely on themanual annotation
results. We remove events from those three aforemen-
tioned event extraction system outputs, using the set of
trigger words that were annotated as incorrect by the
human annotator.
Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 9 of 13
Table 3 Performance comparison of the different pruning approaches and the baseline methods (TEES/EVEX) on the official BioNLP
Shared Task GE data sets
Predictions Precision Recall F1-score
TEES-2011 (Shared Task 2011)
Original TEES 61.76 48.78 54.51
Pruned-TEES (Unsupervised Method) 62.39 48.75 54.74
Pruned-TEES (Manual Annotation Method) 62.04 48.78 54.62
Pruned-TEES (Aggregation Method) 62.26 48.78 54.70
Pruned-TEES (Aggregation Method + SVM) 62.27 48.78 54.71
TEES-2013 (Shared Task 2013)
Original TEES 56.32 46.17 50.74
Pruned-TEES (Unsupervised Method) 57.13 46.02 50.97
Pruned-TEES (Manual Annotation Method) 56.63 46.17 50.87
Pruned-TEES (Aggregation Method) 56.97 46.17 51.00
Pruned-TEES (Aggregation Method + SVM) 57.01 46.17 51.02
EVEX-2013 (Shared Task 2013)
Original EVEX 58.03 45.44 50.97
Pruned-EVEX (Unsupervised Method) 58.77 45.29 51.15
Pruned-EVEX (Manual Annotation Method) 58.32 45.44 51.08
Pruned-EVEX (Aggregation Method) 58.66 45.44 51.21
Pruned-EVEX (Aggregation Method + SVM) 58.71 45.44 51.23
As shown in Table 3, in all three instances (compar-
ingmanual annotation method against the TEES systems
predictions on tasks 2011 and 2013, and the EVEX sys-
tems predictions on task 2013), manual annotation retains
the recall, which is obviously a better result than our unsu-
pervised method. However in all three instances, its pre-
cision and F-score is less than the precision and F-score of
our unsupervised method.
The preserved recall suggest that our annotation
strongly agrees with the ST annotation guidelines. How-
ever, the higher precision of the unsupervised pruning
strategy shows that some cases not clear for a human
annotator, can be classified with this method.
This is exactly what we had anticipated. As precise
annotation was not possible for many trigger words,
we have 731 undecided top most frequent triggers, and
many incorrect trigger words might actually be among
them.
To summarize, the manual annotation has produced an
almost pure but incomplete set of incorrect trigger words.
In comparison to original event extraction system perfor-
mances, our manual annotation method does increase the
precision and F-score while retaining the recall, but its
precision and F-score are not as high as our unsupervised
method.
Evaluation of aggregationmethod
As shown in the previous sections, our unsupervised
method increases the precision and F-score, but slightly
drops the recall, whereas the manual annotation alone
retains recall with lesser increase in precision. In this
section, we investigate the effect of event filtering using
the set of incorrect triggers obtained from the aggregation
method discussed in Section Aggregating unsupervised
method results with manual annotation results.
As shown in Table 3, in comparison with the TEES
performance on ST11 and ST13, and the EVEX perfor-
mance on ST13, the aggregationmethod retains the recall
and increases the precision and F-score. Interestingly, in
all three cases, in comparison with manual annotation
method it has a higher precision and F-score. Conse-
quently, we conclude that our unsupervised method is
indeed able to find incorrect trigger words elusive to the
human annotator.
If we compare the aggregation method performance
with our unsupervised method performance, we notice
that in all three instances, it does have a higher recall
and in two cases also higher F-score. In one case the
unsupervised method alone reaches the highest F-score.
This might be due to trigger words that we have anno-
tated as correct, but are used in wrong event types by the
underlying even extraction system, thus resulting in lower
precision.
As a conclusion, while all of our methods establish
new top scores on 2013 tasks, the aggregation method is
the best among them. It retains the recall, increases the
precision and has the best F-score in two cases out of
three.
Evaluation of low-frequency trigger classification
As stated in Section Classification of low-frequency event
triggers, we use all 3,391 top most EVEX frequent triggers
Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 10 of 13
to train the classifier and aim to apply it on those triggers
with frequency below 300.
Similar to the previous evaluations, we first evaluate
the classifier performance against the Shared Task test
sets as an end-to-end system together with the aggrega-
tion method. For this aim, we apply the trained classifier
to predict labels for EVEX triggers with frequency below
300. This results in identification of 16,674 negative (sup-
posed to be incorrect) triggers with total frequency of
232,748 respective events in EVEX. The rest of the triggers
were predicted as correct. Then, we prune the output of
event extraction systems using these recognized incorrect
triggers and incorrect triggers obtained by aggregation
method.
Results for this experiment are shown in Table 3. Com-
parison of this method (Aggregation Method + SVM
entries in the table) against our aggregation method, the
previously best approach, shows slight increase in both
precision and F-score in all three cases while retaining the
same recall. Thus, the classifier is able to recognize some
previously undetected incorrect trigger words, giving us
the most complete set of incorrect trigger words.
As this processing step focuses specifically on low-
frequency (rare) triggers, unlikely to be found in the
carefully selected Shared Task data sets, the performance
improvement is small, as anticipated. However, we expect
the outcome to be more significant in large-scale event
extraction and to show this we conduct another evaluation
based on the EVEX resource.
In the second evaluation we form an evaluation set by
randomly selecting 700 words from the triggers with fre-
quency less than 300 in EVEX and use the same manual
annotation procedure discussed in Section Manual anno-
tation of triggers to divide them into positive (correct) and
negative (incorrect) sets.
The annotation resulted in 363 correct and 233 incor-
rect triggers. For 104 triggers our annotator was unable
to assign a label. Even though, in terms of our annotation
protocol, the triggers are divided into three independent
classes, for simplicity we exclude the 104 undecidable trig-
ger words from our test set and use only the 596 remaining
words.
The performance evaluation results against the test set
are shown in three different tables.
 Table 4 shows the counts and respective event
frequencies of true-positives, true-negatives,
false-positives and false-negatives.
 Table 5 shows the performance in terms of
classification of triggers. Precision, recall and
F2-score in this table are calculated based on the
counts of the predicted triggers.
 Table 6 shows the performance in terms of
classification of events. Precision, recall and F2-score
Table 4 Trigger/event classification performance, measured on
the EVEX test set: The first column (Count) shows prediction
results based on the counts of trigger words (test set examples).
The second column (Sum of frequency) shows the number of
respective events of those triggers in the EVEX database. For
instance, the first row (True-Positive) shows that the classifier has
correctly predicted 352 test set trigger words to be correct
triggers, while these words account for 4,602 extracted events in
the EVEX resource
Count
Sum of frequency
(Number of events)
True-Positive 352 4602
True-Negative 99 679
False-Positive 134 850
False-Negative 11 115
Total 596 6246
in this table are calculated based on the event
frequencies of the predicted triggers (i.e., based on
the sum of frequencies of TP, TN, FP and FN).
As mentioned in Section Classification of low-fre-
quency event triggers, from the event extraction point
of view, the event frequencies are more important than
the unique trigger words themselves. Thus, results listed
in Table 6 are the most relevant for examining the per-
formance of the classifier. As this is also the evaluation
metric the classifier hyper-parameters were optimized
against, the numbers in Table 6 are generally higher than
in Table 5.
We can see that the classifier achieves recall of 0.98
for the positive class, i.e. the correct triggers as shown
in Table 6. This result suggests that we have succeeded
in our goal of preserving as much of the true events as
possible. Besides, the classifier also reaches recall of 0.44
for the incorrect triggers, i.e. we are able to detect and
exclude almost half of the events with false triggers in this
evaluation set.
Table 5 Trigger classification performance on the EVEX resource
based on trigger counts (test set examples). The prediction
measures in this table are calculated based on the values in the
first column of Table 4. This table shows how well the classifier is
able to classify and distinguish between correct and incorrect
trigger words. The last column (Support) shows that there are
363 correct and 233 incorrect trigger words in the test set, i.e, 596
in total
Precision Recall F2-score Support
Negative (incorrect) 0.90 0.42 0.48 233
Positive (correct) 0.72 0.97 0.91 363
Weighted averages, total 0.79 0.76 0.74 596
Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 11 of 13
Table 6 Classification performance on the EVEX resource based
on the respective event counts in the EVEX database. This table
shows how well the classifier will perform the prediction,
preserving correct and eliminating incorrect respective events
from the EVEX database. The prediction measures in this table are
calculated based on the values in the second column of Table 4.
The last column (Support) shows that there are 1,529 incorrect
and 4,717 correct corresponding events in the EVEX database
(6,246 in total) which are extracted based on those 596 trigger
words in the test set
Precision Recall F2-score Support
Negative (incorrect) 0.86 0.44 0.49 1529
Positive (correct) 0.84 0.98 0.95 4717
Weighted averages, total 0.85 0.77 0.77 6246
Evaluation of event removal on the EVEX resource
In this section we investigate the impact of removing
events from the EVEX resource based on all trigger words
recognized as incorrect.
Even though our manual annotation or aggregation
methods are able to preserve the recall when evaluated
against official predictions of Shared Task test sets, it
is not guaranteed that the same performance will be
achieved when applying them on a large-scale resource
such as EVEX. In fact there might be correct triggers
which are not present in ST11 or ST13 test sets, but are
mistakenly labeled as incorrect by the human annotator,
our unsupervised method or the classifier. Consequently,
in the evaluation against official Shared Task test sets,
we do not delete these triggers and do not detect any
drop in recall. However based on our evaluation results,
we are optimistic that most of the correct events will be
preserved if the method is applied on the EVEX resource.
To investigate the impact of event removal on EVEX,
for top most frequent triggers (accounting for 97.1% of
all EVEX events), we rely on our aggregation method
which had the best performance. The aggregation method
resulted in labeling 1,149 triggers as incorrect and these
account for 1,105,327 events in EVEX.
For the rest of EVEX triggers (low frequency triggers
accounting for 2.9% of all EVEX events), we use the
classifier. However, the classifier could not be applied to
48,960 triggers with 122,344 respective events (0.3% of all
EVEX events). These words have less than 5 occurrences
in the corpus used for training the word2vec model, and
thus do not have a corresponding vector representation,
required by the classifier. Applying the classifier on the
rest of low frequency triggers (accounting for 2.6% of all
EVEX events) resulted in identification of 16,674 incorrect
triggers with 232,748 events in EVEX.
Consequently, in total we have been able to identify
17,823 expected to be incorrect triggers in the whole
EVEX resource with 1,338,075 events which constitutes
3.3% of all events in EVEX.
Tree organization before/after pruning
In this section we address two questions. First, how the
resulting binary cluster tree differs before and after the
pruning, and second, whether we can define new event
subtypes based on the organization of sub-clusters in dif-
ferent branches of the tree. For these aims, we visualize the
tree before and after pruning up to the depth of 9 using
the Dendroscope software [22]. We label every intermedi-
ate node of the tree with its mostly associated event type
and the level of purity of that sub-cluster (see Additional
file 1 for diagrams of the tree).
As expected, in both trees we notice that trigger words
of same event types are clustered together, to some
extent. By considering the length of the shortest path
in tree as a basic distant measure, we observe that sub-
clusters of similar/related event types are closer in the
tree, while sub-clusters of different event types are located
far. For instance, triggers for expressing different types
of post-translational modifications events (e.g., phos-
phorylation, DNA-methylation, glycosylation, acety-
lation) are clustered together, far from trigger words
for expressing positive/negative regulation or binding.
Similarly, sub-clusters of gene-expression, transcrip-
tion and localization trigger event types are close in the
tree.We observe that before pruning the tree, sub-clusters
are not pure. For example, many trigger words for pos-
itive regulation events are often clustered together with
the ones for negative regulation events. By removing the
sub-clusters of purely incorrect triggers, i.e. pruning, the
sub-clusters in the middle levels of the tree become purer
and are for the most part, associated with the same event
type which signifies the possibility of identifying some of
the event subtypes.
We thus continue the analysis on the associated events
in the sub-tree anticipating to recognize the patterns.
However, to our surprise, there is no clear signal in the
sub-clusters that would signify any of the subtypes. As a
result, we thus do not pursue further analysis on the trees.
To conclude, our pruning algorithm yields a meaningful
tree which can distinguish different event types into sub-
clusters, however, the resulting clusters could not be used
to identify event subtypes.
Conclusions
In this paper, we propose a method which can be used
for identification of incorrect trigger words and remov-
ing incorrect events from the output of large-scale event
extraction systems.
Our unsupervised method achieves a modest improve-
ment over the winning system of the BioNLP 2013 Shared
Task on GENIA event extraction and establishes a new
top score on the task. The aggregation of manual annota-
tion results with our unsupervisedmethod results, further
increases the precision and F-score of the unsupervised
Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 12 of 13
method. Besides, the unsupervised method decreases the
original recall when evaluated against official predictions
of Shared Task test sets, while our aggregation method
retains it.
Because the highly demanding manual annotation is not
possible for all EVEX trigger words, we build a SVMclassi-
fier for predicting incorrect triggers among low-frequency
EVEX triggers. While having 0.98 positive recall which
translates to preserving a huge proportion of correct
events, the classifier has 0.44 negative recall, meaning that
it is able to identify about half of the incorrect events.
Combining the results of our aggregation method with
incorrect trigger words identified by applying the classifier
on all low frequency EVEX triggers, results in recog-
nition of 17,823 expected to be incorrect triggers with
1,338,075 respective events which constitutes about 3.3%
of all events in EVEX resource.
In this paper we have only discussed the identification
of the incorrect triggers and the outcome of removing
these triggers from a large-scale event resource. Although
our evaluation shows only minimal drop in recall, bluntly
removing the corresponding events might have unwanted
effects. As the EVEX resource ranks the events shown
to the users based on a scoring system derived from
the TEES classification confidence, we would thus as a
future work like to investigate how to incorporate these
new findings in the ranking. This would let us, instead
of completely abolishing the likely incorrect events, only
to decrease their scoring and conserve them for those
use cases that demand extremely high recall, but can
overcome the noise in the data.
Another direction is to investigate the different event
types in more detail. We hope this study will give us a
better insight of whether the method can be adapted to
also correctmistyped events, thus increasing the precision
even further. For instance, it is possible that a detected
regulation trigger should in fact be classified as positive-
regulation, a subtype of regulation, but the used trigger
detector has not been able to make this distinction. By
observing how the given trigger word is located in the
hierarchical cluster tree, these errors could be possibly
corrected.
As the distributional semantics research is progress-
ing towards better representations of phrases and larger
text sections in addition to word-level embeddings, it
might be possible in the future to instead of judging
the trigger words globally, to focus only on certain types
of contexts giving us the ability to make more precise
decisions.
Availability of supporting data
The source code and data sets supporting the results
of this article are available at the Turku BioNLP group
website at: http://bionlp-www.utu.fi/trigger-clustering/.
Additional file
Additional file 1: This tar file contains images of the binary cluster tree,
before and after the pruning. The HowToInterpretTreeDiagrams.txt file
describes how the diagrams should be interpreted. (TAR 1290 kb)
Competing interests
The authors declare that they have no competing interests.
Authors contributions
The method is designed, fine-tuned, and implemented by FM, with input from
all authors. SK did all manual annotations, while KH prepared an annotation
framework. All authors equally contributed in analyzing the results and
drafting the manuscript. The work is carried out under the supervision of FG.
All authors read and approved the final manuscript.
Acknowledgments
We would like to thank Sofie Van Landeghem, Ghent University, for initiating
the ideas for this project and her valuable suggestions. We also kindly thank
Jari Björne, University of Turku, for his help during the project.
Author details
1Department of Information Technology, University of Turku, Turku, Finland.
2The University of Turku Graduate School (UTUGS), University of Turku, Turku,
Finland. 3Turku Centre for Computer Science (TUCS), Turku, Finland.
Received: 28 February 2015 Accepted: 1 May 2016
RESEARCH Open Access
The Apollo Structured Vocabulary: an OWL2
ontology of phenomena in infectious
disease epidemiology and population
biology for use in epidemic simulation
William R. Hogan1*, Michael M. Wagner2, Mathias Brochhausen3, John Levander4, Shawn T. Brown5,
Nicholas Millett6, Jay DePasse5 and Josh Hanna7
Abstract
Background: We developed the Apollo Structured Vocabulary (Apollo-SV)an OWL2 ontology of phenomena in
infectious disease epidemiology and population biologyas part of a project whose goal is to increase the use of
epidemic simulators in public health practice. Apollo-SV defines a terminology for use in simulator configuration.
Apollo-SV is the product of an ontological analysis of the domain of infectious disease epidemiology, with particular
attention to the inputs and outputs of nine simulators.
Results: Apollo-SV contains 802 classes for representing the inputs and outputs of simulators, of which
approximately half are new and half are imported from existing ontologies. The most important Apollo-SV class for
users of simulators is infectious disease scenario, which is a representation of an ecosystem at simulator time zero
that has at least one infection process (a class) affecting at least one population (also a class). Other important
classes represent ecosystem elements (e.g., households), ecosystem processes (e.g., infection acquisition and
infectious disease), censuses of ecosystem elements (e.g., censuses of populations), and infectious disease control
measures.
In the larger project, which created an end-user application that can send the same infectious disease scenario to
multiple simulators, Apollo-SV serves as the controlled terminology and strongly influences the design of the message
syntax used to represent an infectious disease scenario. As we added simulators for different pathogens (e.g., malaria
and dengue), the core classes of Apollo-SV have remained stable, suggesting that our conceptualization of the
information required by simulators is sound.
Despite adhering to the OBO Foundry principle of orthogonality, we could not reuse Infectious Disease Ontology
classes as the basis for infectious disease scenarios. We thus defined new classes in Apollo-SV for host, pathogen,
infection, infectious disease, colonization, and infection acquisition. Unlike IDO, our ontological analysis extended to
existing mathematical models of key biological phenomena studied by infectious disease epidemiology and
population biology.
(Continued on next page)
* Correspondence: hoganwr@ufl.edu
1University of Florida, P.O. Box 100219, 2004 Mowry Rd, Gainesville, FL
32610-0219, USA
Full list of author information is available at the end of the article
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 
DOI 10.1186/s13326-016-0092-y
(Continued from previous page)
Conclusion: Our ontological analysis as expressed in Apollo-SV was instrumental in developing a simulator-
independent representation of infectious disease scenarios that can be run on multiple epidemic simulators. Our
experience suggests the importance of extending ontological analysis of a domain to include existing mathematical
models of the phenomena studied by the domain. Apollo-SV is freely available at: http://purl.obolibrary.org/obo/
apollo_sv.owl.
Keywords: Disease transmission model, Epidemic simulator, Epidemic simulation, Biomedical ontology, Infectious
disease epidemiology, Population biology, Infection
Abbreviations: Apollo-SV, Apollo structured vocabulary; BFO, Basic formal ontology; DTM, Disease transmission model;
EO, Epidemiology ontology; GO, Gene ontology; IDO, Infectious disease ontology; MIREOT, Minimum information to
reference an external ontology term; OBO, Open biological and biomedical Ontologies; OGMS, Ontology for general
medical science; OWL 2, Web ontology language version 2; OWL DL, OWL description logic; PURL, Permanent Uniform
resource locator; SimPHO, Simulation modeling of population health ontology; UAL, Unique apollo label;
XML, eXtensible markup language; XSD, XML schema document
Background
The science and practice of infectious disease epidemi-
ology, like climate science, is increasingly reliant on
computational simulation [1], which is performed by
software applications known as epidemic simulators.
The simulators require information about pathogens,
host populations, rates of infection transmission, inter-
ventions, and the disease outcomes of infections [2].
Using this configuration informationwhich we refer
to as an infectious disease scenarioa simulators
algorithm computes the progression of one or more
infections in one or more populations over time, under
zero or more interventions. The result of this computa-
tionthe output of the simulatoris information on
which decision makers can base policy or decisions
about disease control.
The goal of our research for the past 4 years has been
to increase the accessibility and ease of use of simulators
to promote progress in the field of infectious disease epi-
demiology [3]. A key focus has been reducing the time
and effort required to locate a simulator, access it,
understand its characteristics, create an infectious dis-
ease scenario to configure it, run it, and analyze its out-
put. As an example of the effort required, Halloran et al.
spent 6 months creating a comparative study of three
simulators [4]. Most of the effort was expended on
representing the same scenario in the different configur-
ation representations and then converting results into a
common representation for comparisons. As an example
of the syntatic and semantic differences among simula-
tor configurations, to configure the FRED simulator ver-
sion 2.0.1 [5] to simulate the closing of schools1 3 days
after some event occurs (such as influenza incidence
reaching a particular threshold) one would place
school_closure_delay = 3 in its configuration file,
whereas for FluTE version 1.15 [6] one would place
responsedelay = 3 in its configuration file (unlike
FRED, this setting would also affect other interventions
such as vaccination).
To address this problem, we are developing a common
representation for simulator configuration and output that
is capable of representing the configurations and output
of infectious disease simulators [3]. We use an XML
Schema Document (XSD) as our primary representation
because the XSD language enabled us to represent the
probabilistic, mathematical, and other non-ontological
knowledge required for and generated by simulation. We
inform the design of the XSD representation by formal
ontological analysis of the domain of infectious disease
epidemiology, with particular attention to the inputs and
outputs of nine simulators. Our goal was for the XSD to
have the capability to represent the configuration and out-
puts of not only these nine simulators, but also other
existing and future simulators. We represent the results of
this analysis in an OWL ontologycalled the Apollo
Structured Vocabulary or Apollo-SV.
Apollo-SV and XSD together can be understood as a
hybrid approach to knowledge representation and
reasoning as defined by Davis et al. in their seminal
paper on knowledge representation [7]. In particular,
Apollo-SV (1) controls the terminology used in the XSD,
(2) is a source of human-readable definitions of the
terms for users of the XSD, and (3) serves as a record of
the ontological commitments made by the developers of
the XSD.
Our hypothesis was that it is feasible to develop a
common representation for the configuration and out-
put of simulators that are diverse both in their internal
representations and in the pathogens, modes of trans-
mission, geography, and interventions that they model.
We previously reported our initial versions of the
XSD and Apollo-SV (versions 1.0), as well as our
creation of a set of Web services to transmit a com-
mon configuration to two simulators [3]. We use
Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 2 of 12
configurations compliant with the XSD to invoke sim-
ulators as part of these Web Services, but generated
the OWL2 representationApollo-SVas our core
ontology.
In this paper, we describe new results from our subse-
quent ontological analyses of additional simulators and
our updated understanding of simulator configurations
that we incorporated into Apollo-SV version 3.0.1.
Methods
Our method for the development of the common repre-
sentation was formal ontological analysis with rapid
implementation of the representation to configure simu-
lators and feedback from the results of implementation
into further analysis.
The next sections discuss our style of ontology
development, the application in which the ontology is
used, and the procedures and principles we followed in
constructing the OWL ontology, Apollo-SV.
Gene Ontology Style of ontology development
We developed Apollo-SV using what we refer to as the
Gene Ontology style of ontology development and
testingor GO style for short. GO style is a method for
ontology development that emphasizes participation of
subject matter experts and frequent and early feedback to
ontology developers generated from using the ontology in
software applications. We adopted GO style because it was
successful for the Gene Ontology and because our
community of developers and users was similar in many
respects.
A key strength of GO stylewhich the Gene Ontology
Consortium cites as a factor in its successis that a com-
munity of scientists, ontologists, artificial intelligence
experts, and software developers all contribute in an egali-
tarian fashion to the ontology and its applications [8]. The
team developing Apollo-SV comprises experts in infec-
tious disease epidemiology, simulator and other software
development, disease surveillance, medicine, biomedical
informatics, medical terminologies, ontological engineer-
ing, artificial intelligence, and formal logic (the last one in
the list helps to ensure that OWL2 axioms that define
classes are correct). All these individuals have been
actively engaged in development and review of Apollo-SV,
and their feedback guides design decisions.
A second strength of the GO style of ontology devel-
opment is its emphasis on early use of the ontology in
applications, which identifies issues and generates rapid
feed back into ontology development [9]. We discuss the
application of Apollo-SV in the next section. Additional
elements of the style, that have subsequently been
adopted by the Open Biological and Biomedical Ontol-
ogies (OBO) Foundry as principles of ontology develop-
ment, include creating textual definitions for each class
and making the ontology publicly and freely available for
community use, review, and input [810]. We discuss
how we implemented these additional elements of the
style, as well as additional OBO Foundry principles, in
the section following application.
The application in which the ontology is used
As stated previously, Apollo-SV serves as the repository
for definitions and standard terminology for the Apollo
XSD. The Apollo XSD in turn is used in a set of Web
services.
The Web services, called the Apollo Web Services,
allow a publicly available, Web-based, end-user applica-
tion to access multiple epidemic simulators through
requests to a single Broker service (Fig. 1). In Fig. 1, the
Simple End User Application (SEUA) [11] creates an
infectious disease scenario for simulation, encoded in an
XML document that conforms to the Apollo XSD syntax
[12], which in turn uses terminology defined by Apollo-
SV. The SEUA invokes the runSimulation() method of
the Broker service with the XML-encoded infectious dis-
ease scenario. The Broker service subsequently invokes
the Translator service, which translates the infectious
disease scenario into the native terminology and syntax
of the requested simulator(s). The SEUA polls the
Broker service for the current status of the simulator
until the status returned is COMPLETED. The SEUA
then invokes various visualization services on the simu-
lator output to display epidemic curves and maps in the
interface.
By standardizing the terminology in the Web services,
Apollo-SV helps to ensure that the SEUA end user and
the simulators understand the XML-encoded infectious
disease scenario to mean the same thing. Towards that
end, the SEUA displays the textual definitions of classes
in Apollo-SV to help the end user specify her infectious
disease scenario accurately and precisely.
Beginning with the earliest development of Apollo-SV,
exposing the terminology and definitions from Apollo-SV
to subject matter experts, developers, and others in the
SEUA was a significant source of critical feedback that led
to additional ontological analysis as well as refinements of
the terminology and definitions.
Procedures and principles of Apollo-SV construction
We encode the results of our ontological analyses in
OWL2. Our process proceeds concurrently with devel-
opment of the Apollo XSD, and issues discovered in
constructing either the OWL or the XSD are fed back
into the analysis.
We conducted a formal ontological analysis of seven
additional simulatorstheir configuration files, output
files, documentation (including any user guides), and
journal and conference papers that either described or
Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 3 of 12
used them. As part of this process, we reviewed terms
that we extracted from these sources with the developers
of the simulators to identify relevant but missing terms,
to discover synonymy among terms, and to detect and
resolve ambiguity. Of the seven additional simulators,
four are presently connected to the Apollo Web
Services.
We wrote a textual definition for every class that we
create, in keeping with the GO style and OBO Foundry
principles. We also created an elucidation annotation for
classes in Apollo-SV because formal ontological textual
definitions are sometimes not accessible to domain ex-
perts. The elucidation restates the definition in language
more familiar to subject matter experts, while still refer-
ring to the same type of entities as the definition.
Also in accordance with the GO style of ontology de-
velopment, we made Apollo-SV publicly available at
[13], a permanent URL (PURL), to allow external
scientific review, comments, and requests for additions
as well as to encourage adoption of Apollo-SV. We en-
sured that Apollo-SV is easily accessible for browsing
and download at the Web-based Ontobee portal [14],
analogous to Gene Ontology browsers (the GO itself is
viewable on Ontobee). The issue tracker is located at
the Apollo GitHub site [15]. The PURL to the develop-
ment version of Apollo-SV is at [16].
Because the Gene Ontology has full membership sta-
tus in the OBO Foundrya special status conferred on
ontologies that conform to the OBO Foundry principles,
we also followed the principles of the OBO Foundry in
addition to openness and textual definitions [17, 18]. Per
those principles, we release it in a common format,
OWL2 [19].
We also adopted the Foundry principle of orthogonal-
ity, which stipulates that ontology developers reuse pre-
existing ontological representations into Apollo-SV
when and where appropriate.
We employed two methods for ontology reuse. The
first method is the OWL2 ontology-import mechanism.
This method inserts into the target ontology all classes
and object properties of the imported ontology. How-
ever, bulk inclusion of large ontologies is often impracti-
cal and can degrade the usability of the target ontology.
Therefore, the second method we used is the Minimum
Information to Reference an External Ontology Term
(MIREOT) methodology [20]. Using a MIREOT Protégé
plugin that we developed [21], we import selected clas-
ses, individuals, and properties from certain ontologies
into Apollo-SV.
We hypothesized that we would be able to reuse pre-
existing ontologies or significant portions of them in de-
veloping Apollo-SV. In particular, we anticipated reusing
substantial portions of the Infectious Disease Ontology
(IDO) [22]. IDO is an OBO ontology (but not a full
member of the Foundry) that represents infections,
infectious diseases, pathogens, and hosts from the per-
spectives of infectious disease as a medical subspecialty
and infectious disease research.
We adhered to OBO Foundry naming conventions
[23]. We edited our terms to (1) avoid connectives (and,
or), (2) prefer singular nouns, (3) avoid the use of nega-
tions, and (4) avoid catch-all terms such as Unknown x.
Fig. 1 The relationships of Apollo components and epidemic simulators. Apollo-SV defines the terminology used in Apollo XSD, which specifies the
message syntax for the Web services. The SEUA calls the Broker service to configure simulators (messages passed along blue arrows) and to access
simulator output (messages passed along red arrows). The Translator service translates Apollo messages to/from native simulator input/output. Purple
ovals represent Apollo standards; blue ovals represent Apollo-developed software that use the Apollo Web services; and red ovals represent entities
interacting with Apollo
Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 4 of 12
To help link the OWL file to the XSD, we created a
Unique Apollo Label (UAL) annotation for classes in
Apollo-SV. The UAL is the exact XSD type or element
name to which the class in Apollo-SV corresponds, for ex-
ample, InfectiousDisease and BasicReproductionNumber.
Although not required by OBO Foundry principles, we
imported Basic Formal Ontology (BFO) version 1.1 [24]
into Apollo-SV as its upper ontology as do many other
Foundry ontologies. The main reasons were (1) to main-
tain the semantics of BFO-based ontologies and their
components that we reused and (2) to ensure that new
classes and their associated axioms in Apollo-SV did not
introduce inconsistencies to those semantics.
We created description logic axioms according to the
syntax and semantics inherent in OWL2 for classes in
Apollo-SV (e.g., Figs. 2,3, 4 and 5). When possible, these
axioms provide both necessary and sufficient criteria for
class membership. Many axioms, however, define only
necessary criteria, most often because the description
logic semantics of OWL2 were insufficiently expressive
to encode both the necessary and sufficient criteria of
the class.
Results
Apollo-SV version 3.0.1 comprises 868 classes, of which
802 were required for describing simulator configuration
and output. The remaining 66 classes are extraneous
imported classes resulting from OWL2-based imports of
ontologies in toto. Of the 802 classes, we created 397
(49.5 %) new classes, of which 117 classes have necessary
and sufficient criteria. We imported 118 (14.7 %) classes
via the methodology of Minimum Information to Refer-
ence and External Ontology Term or MIREOT (Table 1),
and imported 287 (35.8 %) via OWL2-based import. The
ontology comprises a total of 1180 logical axioms.
High level classes in Apollo-SV
The most important Apollo-SV class for users of sim-
ulators is infectious disease scenario, which represents
an ecosystem at simulator time zero with at least one
infection process (a class) affecting at least one popu-
lation (also a class). The infectious disease scenario
includes information about the infection process and
its acquisition by a host organism (e.g., transmission
probabilities and the durations of infectious and latent
periods). It can also include information about
planned or ongoing interventions to control infection
(such as vaccination control measures). Representing
ecosystems, populations, and censuses thus expanded
the scope of Apollo-SV to population biology
(Table 2). Including population biology subsequently
influenced our definitions of key terms in infectious
disease epidemiology.
Fig. 2 Representation of the equivalent class axiom for infection in Apollo-SV. Boxes represent named classes, boxes with curved bases represent
anonymous classes, arrows represent object properties. In the boxes is the rdfs:label and the namespace of the source ontology, if different from
Apollo-SV. Each arrow is labeled with the rdfs:label of the property it represents
Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 5 of 12
Classes representing the infections, infection acquisi-
tions, hosts, pathogens, and infectious diseases in an
ecosystem are foundational in Apollo-SV. The reason is
that the essential prediction of simulators is how many
infections will occur given an infectious disease scenario.
Nearly everything else that simulators predict are events
that revolve around infection. They either (1) occur
downstream of infection (such as disease outcomes in-
cluding symptoms and death), (2) influence the proba-
bilty of acquiring an infection (such as going to work or
school or being vaccinated), or (3) occur as part of an in-
fectious disease control strategy to prevent infection ac-
quisition (such as school closure or quarantine). Also,
because one simulator that we analyzed predicts
colonization of hosts by pathogens and the processes by
which hosts acquire colonizations, it was also important
to represent colonization and how it differs from infec-
tion (see below).
Foundational classes where reuse of IDO was not possible
We now describe a set of foundational classes we cre-
ated in Apollo-SV after attempting unsuccessfully to re-
use IDO classes and their definitions. We also discuss
the reasons why these classes and definitions were
unworkable.
Infection
Apollo-SV defines infection as: A reproduction of a
pathogen organism of a particular biological taxon in a
tissue of a host organism from another taxon (Fig. 2).
From the perspective of population biology, an infection
is merely a process by which one species reproduces,
surviving from generation to generation, utilizing the re-
sources of a host species. It is the normal biology of the
pathogen species.
Infection is distinguished from other types of pathogen
reproduction in a hostnamely colonization (defined
below)by violation of the integrity of tissue in the host
through tissue invasion. This tissue invasion may
occurand subsequently endwithout causing any
symptoms or permanent ill effects on the host. Thus, in-
fection does not equate to disease, and we carefully dis-
tinguish between infection and infectious disease.
Epidemic simulators represent infection as a process
because infectious disease epidemiologists define infec-
tion as a process. For example, [25, 26] define infection
as the invasion of a host organisms tissue by pathogens,
the multiplication of those pathogens, and the reaction
of the hosts tissue(s) to the pathogens and the toxins
they produce. Further reinforcing the fact that infection
is a process is the fact that simulators represent periods
Fig. 3 Representation of the equivalent class axiom for host in Apollo-SV. The graphical representation is analogous to Fig. 2
Fig. 4 Representation of the equivalent class axiom for pathogen in Apollo-SV. The graphical representation is analogous to Fig. 2
Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 6 of 12
of (or ontologically speaking, occurrent parts of ) the in-
fection: the latent period and the infectious period.
Before we created a class for infection in Apollo-SV,
we reviewed IDO for a class that represents the process
of infection, whether labeled as infection or with some
other term.
We found that IDO defines infection as a physical thing,
or material entity in the terminology of Basic Formal
Ontology (BFO). Specifically, it defines infection as: A part
of an extended organism that itself has as part a popula-
tion of one or more infectious agents and that is (1) clinic-
ally abnormal in virtue of the presence of this infectious
agent population, or (2) has a disposition to bring clinical
abnormality to immunocompetent organisms of the same
Species [sic] as the host (the organism corresponding to the
extended organism) through transmission of a member or
offspring of a member of the infectious agent population.
Given that epidemic simulators and the relevant basic
sciences on which they are founded recognize infection as
a process, we needed to create a new class in Apollo-SV
to represent it. The lack of a representation of the process
of infection in IDO is surprising because IDOs definitions
of its classes host role and infectious agent role require a
process to realize them. This process would presumably
be infection.
Colonization
Apollo-SV defines colonization as: A reproduction of a
pathogen of a particular biological taxon inside or on
the surface (e.g., skin, mucosal membrane) of a host
organism of another taxon, without invasion of any
tissues of the host. We required this class to represent
the input of the Regional Healthcare Ecosystem Analyst
[27] simulator, which models the spread of methicillin-
resistant Staphylococcus aureus (MRSA). MRSA, as well
as methicillin-sensitive varieties of S. aureus, typically
colonize the nasal mucosa and skin of humans, living
on these surfaces but not invading them. If a human
Fig. 5 Representation of the equivalent class axiom for infectious
disease in Apollo-SV. The graphical representation is analogous
to Fig. 2
Table 1 Re-use of classes and object properties from
pre-existing ontologies in Apollo-SV via MIREOT
Ontology Classes Object
Properties
Total
Uberon 7 1 8
Ontology of Medically Related Social
Entities
26 7 33
Gene Ontology 13 0 13
Ontology for General Medical Science 11 0 11
Ontology of Biomedical Investigations 21 6 27
Infectious Disease Ontology 3 7 10
The Drug Ontology 1 0 1
FlyBase Controlled Vocabulary 2 0 2
Vaccine Ontology 4 0 4
Drug-drug Interaction Evidence
Ontology
1 0 1
Unit Ontology 5 0 5
Phenotypic Quality Ontology 3 0 3
Totals 97 21 118
Table 2 Classes in Apollo-SV by domain
Domain Classes in Apollo-SV
Infectious disease
epidemiology
Infection Infection acquisition
Pathogen Host
Latent period Infectious period
Contaminated thing Contamination
acquisition
Contamination
Infectious disease scenario Basic reproduction
number
Transmission coefficient Transmission
probability
Disease transmission
model
Infectious disease
control strategy
Susceptible population Exposed population
Infectious population Resistant population
Population biology Ecosystem Biotic ecosystem
Abiotic ecosystem Community
Population Population census
Population infection and
immunity census
Abiotic ecosystem
census
Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 7 of 12
host subsequently becomes immunocompromised or
suffers a breach of the integrity of these surfaces, this
colonization may extend to infection. Colonization is
an important epidemiological process because an indi-
vidual may acquire colonization from another MRSA
colonized host.
IDO defines colonization as An establishment of
localization in host process in which an organism estab-
lishes itself in a host. The latter part of the definition is
more general than the former (assuming that there are
other types of establishment besides localization) and
thus does not differentiate this IDO class from its parent
in IDO. We did not consider it further.
Host
Apollo-SV defines host as: An organism of a particu-
lar biological taxon that is the site of reproduction of
an organism of a different taxon (Fig. 3). This defin-
ition accomodates the host undergoing infection and/
or colonization. We note that our use of site of in
this definition has a precise meaning as specified in
the Relation Ontology, where site of is a synonym for
the contains process relation, which relates an inde-
pendent continuant and a process, in which the
process takes place entirely within the independent
continuant.
We could not reuse IDOs definition of host, which is:
An organism bearing a host role. To understand this
IDO definition, it is necessary to review two additional
IDO definitions:
1. Host role: A role borne by an organism in virtue of
the fact that its extended organism contains a
material entity other than the organism.
2. Extended organism: An object aggregate consisting of
an organism and all material entities located within
the organism, overlapping the organism, or occupying
sites formed in part by the organism.
Under these definitions, any organism that has an arti-
ficial joint, a penny in its gut, or an arrow through its
chest is a host. Classifying a person with a prosthetic
knee as a host is counterintuitive and not in keeping
with how host is defined in population biology or infec-
tious disease epidemiology (or in clinical medicine). Fur-
thermore, the definition is based on IDOs view of
infection as a material entity and does not account for
the process of infection.
Pathogen
Apollo-SV defines pathogen as: An organism of a particular
biological taxon that is the bearer of a disposition that is
realized as its reproduction in the tissue of an organism of a
different biological taxon (Fig. 4). Thus Apollo-SV defines a
pathogen as an organism that has the capability to repro-
duce inside the tissue of a host organism of another
biological taxon. Note that this definition is inclusive of
organisms like MRSA involved in colonization: the organ-
ism still has the potential to invade tissue and establish
infection and thus meets the definition.
Once again, we had intended to reuse IDO. However,
IDO defines pathogen as: A material entity with a patho-
genic disposition. Again, this definition requires add-
itional IDO definitions to clarify its meaning:
1. Pathogenic disposition: A disposition to initiate
processes that result in a disorder.
2. Disorder: A material entity which is clinically
abnormal and part of an extended organism.
Disorders are the physical basis of disease.
Thus, per IDO any material that causes injury is a
pathogen, including the endotoxin of Clostridium diffi-
cile or an overdose of acetaminophen. This definition is
not how infectious disease epidemiology uses the term
pathogen. IDO does have a class infectious agent as a
subtype to pathogen that refers specifically to organisms
that can enter into a host and cause disease. The IDO
definition of infectious agent, however, relies on IDOs
definitions of infection and infectious disorder as material
entities. To be consistent with infection as a process, we
created the above definition of pathogen in Apollo-SV.
Infectious Disease
Apollo-SV defines infectious disease as: A disease that
inheres in a host and is realized as a disease course that
is causally preceded by an infection (Fig. 5). This means
that the infection occurs first and creates abnormalities
in the host that result in disease.
This definition is compatible with the OBO Foundry
definition of disease in the Ontology of General Medical
Science (OGMS) [28]. We thus were able to reuse the
OGMS definition of disease, in keeping with the Foun-
dry principle of orthogonality. Note that the disease
inheres only in the host. From the pathogens perspec-
tive, there is no clinical abnormality (which is a neces-
sary condition to meet the definition of disease in
OGMS) as infection is normal biology of pathogens.
IDOs definition of infectious disease is incompatible
with our definition of infection as process.
Infection Acquisition
Apollo-SV defines infection acquisition as: The biological
process of a pathogen of a particular biological taxon en-
tering (the tissues of the body of ) a susceptible host or-
ganism of another taxon and reproducing using host
resources. A susceptible host can acquire an infection
from one of at least three routes:
Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 8 of 12
1. From another host organism (of the same or
different species) that is infectious, which we
represent in Apollo-SV as the class Infection
acquisition from infectious host.
2. From some object or its surface that is contaminated
with the pathogen, which we represent in Apollo-SV
as the class Infection acquisition from contaminated
thing.
3. From self colonization with the pathogen, which we
represent in Apollo-SV as the class Infection acquisi-
tion from self colonization.
Note that we chose to define infection acquistion in-
stead of transmission or transmission process. One rea-
son was our insight that ontologically it is only the
second, susceptible host that undergoes change during
the process, and the term infection acquisition describes
this change better than the term transmission. Another
reason is that we needed to represent the acquisition of
infections from contaminated things and from self-
colonization with a pathogen. In both cases, transmis-
sion from host to host is indirect (mediated through
contaminated surfaces and objects and through acquis-
tion of colonization, respectively).
As with other key terms, IDO lacked an adequate class
and definition for the process of infection acquisition.
IDO imports transmission process and its two definitions
from the Transmission Ontology:
1. A process that is the means during which the
pathogen is transmitted directly or indirectly from its
natural reservoir, a susceptible host or source to a
new host.
2. Suggested definition: A process by which a pathogen
passes from one host organism to a second host
organism of the same Species [sic].
Beginning with the second definition (which for some
reason the Transmission Ontology labels as a suggested
definition), it erroneously restricts transmission to
occur only between two hosts of the same species. It is
thus not usable in infectious disease epidemiology or
any other science that studies cross-species transmission,
which frequently occurs in zoonoses and diseases like
foot and mouth disease.
The first definition has two major problems. The first
problem is circularity, defining transmission process in
terms of a pathogen being transmitted, with no defin-
ition of transmitted. The definition also excludes infec-
tion acquisitions from contaminated objects and self
colonization and refers to the undefined terms natural
reservoir and source.
The second problem is an ontological one. It attributes
to one process the property of being the means by which
something else happens. For example, assume droplet
spread of infection from one host to another by a
sneeze. This definition equates the sneeze with the
transmission process. That is, it says that only the sneeze
exists, but it also has the property of having transmitted
the pathogen. However, equating the sneeze to the
transmission process is nonsensical because for example,
droplets can remain airborne and infectious for hours.
Thus the pathogen may not reach (or be transmitted to)
another host until long after the sneeze is over. The
sneeze cannot therefore be the transmission process. In
reality, there are two distinct processes: the sneeze and
the subsequent acquisition of an infection by the second
host.
Testing Apollo-SV and its ontological commitments in
software
We created a capability to configure six simulators: using
the SEUA, an end user creates an infectious disease sce-
nario that conforms to the XSD and then submits it to the
simulators via Web services. The SEUA then retrieves the
output of the simulators and displays it on maps and
graphs. This capability was the end product of iterative,
concurrent development of Apollo-SV and the XSD ac-
cording to our analysis of the simulators, which included
feedback from implementation in the Web services and
SEUA. In addition, the SEUA displays textual definitions
of Apollo-SV classes to the end user. Feedback on these
definitions was fed back into ontology development which
resulted in ontology changes including improved defini-
tions. We are piloting a 7th simulator whose unique
ontological commitments are reflected in Apollo-SV and
the XSD, but are still undergoing refinement. The six
configurable simulators are (1) a compartmental model
developed by authors MMW, NEM, and JDL (disease
agnostic); (2) the FRED model developed by the University
of Pittsburgh Public Health Dynamics Laboratory in
collaboration with the Pittsburgh Supercomputing Center
Public Health Applications group and the School of
Computer Science at Carnegie Mellon University (influ-
enza A in humans); (3) the FluTE model developed by the
University of Washington and Fred Hutchinson Cancer
Research Center in Seattle (influenza A in humans), (4) a
compartmental model of anthrax developed by authors
MMW, NEM, and JDL, (5) the Computational Arthopod
Agents (CLARA) dengue model developed by the
Pittsburgh Supercomputing Center Public Health Applica-
tions group [29], and (6) an ebola model by Bellan et al.
[30] These simulators are diverse in terms of underlying
model (compartment vs. agent-based), disease (influenza,
anthrax, ebola, and dengue), transmission (vector and per-
son to person), and geography, both in terms of granular-
ity (tract vs. county vs. entire nation) and scale (from a
single state or nation to the entire globe).
Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 9 of 12
Discussion
We developed and implemented a common representa-
tion for simulator configuration and output and used it
in an application that constructs and sends infectious
disease scenarios to six different epidemic simulators.
Our success in representing the inputs of a diverse sam-
ple of simulators lends support to our hypothesis that a
common representation is feasible. Early usage of the
ontology and exposure of its definitions to subject mat-
ter experts in software resulted in ontology improve-
ments, most notably in the definitions of the core
classes of Apollo-SV that we discussed here. This result
is consistent with those of other ontology development
efforts.
The ontological analysis we used to create the com-
mon representation identified abstractions that spanned
simulators diverse in their core mathematical founda-
tions (compartmental vs. agent based), pathogens, routes
of transmission, geographical scope (single city or
county vs. entire world), and interventions. The key
abstractions were that the input of a simulator was an
infectious disease scenario and that the scenario was
properly understood as a representation of an ecosystem
at a particular time, which corresponded to simulator
time zero. We note that there is nothing specific to
infectious disease in this conceptualization, which
suggests that the ontology could be applied to simula-
tion of other ecological phenomena.
A novel aspect of our method was its focus on the
ontological analysis of epidemic simulators. This focus
quickly brought into view the key biological phenomena
being simulated and their fundamental nature. Addition-
ally, simulatorsbeing mathematical modelsmake
explicit ontological commitments about the core entities
involved in infections and their acquisition, which led us
to confront the issues involved in representing them
from the outset. It is worth noting that simulators used
in epidemiology are often rigorously vetted through peer
review of simulator-based research, as well as peer re-
view of the simulators themselves. A final advantage of
our focus on simulators is that they make a relatively
small number of ontological commitments, which
allowed us to devote sufficient time to them, while still
being able to implement an application that continously
tested whether the evolving representation could config-
ure an expanding set of simulators. We expect that
ontological analysis of any domain for which mathemat-
ical models exist would benefit from a focus on the
models. For example, for human physiology there is an
extensive library of mathematical models that are the
focus of the Human Physiome project [31].
Prior work on the use of ontologies for modeling and
simulation identified a distinction between so-called
referential and methodological ontologies [32]. The
former correspond with domain ontologies: a represen-
tation of the phenomena simulated. The latter corres-
pond with application ontologies: a representation of
simulators, how they work, and parameters that specify
their operations. Apollo-SV is both a domain (a.k.a. ref-
erential) and an application (a.k.a. methodological)
ontology in the field of infectious disease epidemiology.
We were surprised that we were unable to reuse clas-
ses from IDO for infection, pathogen, host, colonization,
infectious disease, and transmission process. We conjec-
ture that IDOs ontological analysis may have begun with
a disease focus and worked from there to the nature of
infection, whereas we began with a biological science
perspective. Our focus differed fundamentally from
IDOs concentration on how the terms are used in clin-
ical medicine. In particular, our focus led us to a require-
ment to represent the process of infection, including key
parts of this process such as the infectious period, as op-
posed to the steady-state, material-entity view of IDO.
We note however that our definitions of infection,
pathogen, host, and infectious disease do not conflict with
how these phenomena are understood by clinical medi-
cine and thus could be reused without difficulty by ontol-
ogies that support clinical applications. In fact, in the case
of zoonoses and infections that result from a prior process
of colonization, our representations are a marked
improvement because our definition of infection acquisi-
tion permits cross-species transmission and infections
resulting from self colonization, whereas IDOs definition
of transmission process does not. Also, our definition of
host and pathogen are more consistent with their usage by
infectious disease specialists.
We also could not reuse other prior work on ontol-
ogies that have overlap with Apollo-SV. This work
includes the Epidemiology Ontology (EO) [33] and the
Ontology for Simulation Modeling of Population Health
(SimPHO) [34]. EOlike Apollo-SVstrives to meet
Foundry principles [33]. However it, like IDO, also de-
fines infection as a material entity. It erroneously defines
infection acquisition as occuring only in humans and
does not axiomatize its classes. Okhmatovskaia et al. do
not define for SimPHO [34] any of the terms in Table 1.
Further comparison is not possible because SimPHO is
not publicly available for review/reuse.2
Given that simulator configurations require represent-
ing several kinds of knowledge including probabilistic
and mathematical knowledge, it was not possible to use
an OWL2 representation in the Web services to config-
ure simulators. At present the application that creates
infectious disease scenarios does not invoke any
description-logic reasoning supported by the axioms in
Apollo-SV. Nevertheless, we found it advantageous to
create the OWL2 representation and reuse it at the
lower level of information representation of XSD.
Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 10 of 12
However, in other work, our OWL2 representation (i.e.,
Apollo-SV) supports reasoning in our ontology-based
catalog of infectious disease epidemiology (OBC.ide),
which is a catalog of datasets, publications, grey litera-
ture, and simulators [35]. The OBC.ide search interface
makes use of multiple OWL2 reasoning capabilities
including the is a hierarchy, transitive roles such as
part of, and role chaining. Adaptation of Apollo-SV to
this purpose required no re-axiomatization of the classes
discussed here.
Our future plans include expanding Apollo-SV and the
XSD to cover additional simulators and types of infor-
mation used in infectious disease epidemiology.
Conclusions
Apollo-SV captures the output of our ontological analysis
of the entities in reality represented by epidemic simulator
configuration and output. It also supplies the standardized
terminology used in epidemic simulator configuration and
output, which also includes an XSD-based syntax and
database schema. We validated Apollo-SV through use in
a simple end-user application that enables analysts to spe-
cify an infectious disease scenario and submit it to one or
more of six simulators. Our analysis of biologically-
grounded epidemic simulators and our process of testing
the ontology in software led to scientifically accurate defi-
nitions that we have found to be reusable across diverse
simulators to date. When available, mathematical models
of natural phenomena like epidemics are potentially useful
starting points for ontology development.
Endnotes
1Closing schools is one infectious disease control strat-
egy that simulators study for the control of influenza
epidemics.
2We are unable to find any remaining links to Sim-
PHO, and past links while we were doing the work were
broken at the time.
Acknowledgments
This work was supported by award R01GM101151 from the National Institute
for General Medical Sciences (NIGMS) and awards UL1 TR000064 and
UL1TR001427 from the National Center for Advancing Translational Sciences
(NCATS). This paper does not represent the views of NIGMS or NCATS. This
work used the Protégé resource, which is supported by grant GM10331601
from NIGMS.
Authors contributions
Authors MMW, WRH, and MB conducted the ontological analysis. Authors
WRH, MB, and JH created and curated the Web Ontology Language
implementation of the analysis. Author MMW created and curated the XML
Schema Document (XSD) implementation of the analysis. Authors JL and NM
built the Apollo Web services based on the XSD and developed the Simple
End User Application (SEUA). Author NM incorporated Apollo-SV ontology
definitions and elucidations into the SEUA. Authors STB and JD validated the
ontological analysis and implementations, provided feedback, and developed
the connection of the Web services to the FRED and CLARA simulators. They
also provided substantial insight into the FRED and CLARA simulators.
Authors WRH, MMW, and MB developed the first manuscript draft. All other
authors had significant input into the manuscript. All authors have reviewed
and approved the final manuscript text.
Competing interests
The authors declare that they have no competing interests.
Author details
1University of Florida, P.O. Box 100219, 2004 Mowry Rd, Gainesville, FL
32610-0219, USA. 2University of Pittsburgh, 5607 Baum Boulevard, Room 434,
Pittsburgh, PA 15206, USA. 3University of Arkansas for Medical Sciences, 4301
W. Markham St. Slot #782, Little Rock, AR 72205, USA. 4University of
Pittsburgh, 5607 Baum Boulevard, Room 434G, Pittsburgh, PA 15206, USA.
5Pittsburgh Supercomputing Center, 300 S. Craig St., Pittsburgh, PA 15213,
USA. 6University of Pittsburgh, 5607 Baum Boulevard, Room 435 J, Pittsburgh,
PA 15206, USA. 7University of Florida, P.O. Box 100212, Gainesville, FL
32610-0212, USA.
Received: 26 June 2015 Accepted: 10 August 2016
SHORT REPORT Open Access
Thematic issue of the Second combined
Bio-ontologies and Phenotypes Workshop
Karin Verspoor1* , Anika Oellrich2, Nigel Collier3, Tudor Groza4,5, Philippe Rocca-Serra6, Larisa Soldatova7,
Michel Dumontier8 and Nigam Shah8
Abstract
This special issue covers selected papers from the 18th Bio-Ontologies Special Interest Group meeting and Phenotype
Day, which took place at the Intelligent Systems for Molecular Biology (ISMB) conference in Dublin in 2015. The papers
presented in this collection range from descriptions of software tools supporting ontology development and
annotation of objects with ontology terms, to applications of text mining for structured relation extraction involving
diseases and phenotypes, to detailed proposals for new ontologies and mapping of existing ontologies. Together, the
papers consider a range of representational issues in bio-ontology development, and demonstrate the applicability of
bio-ontologies to support biological and clinical knowledge-based decision making and analysis.
The full set of papers in the Thematic Issue is available at http://www.biomedcentral.com/collections/sig.
Introduction
This special issue originates from the papers presented
in a 2-day meeting, combining the Bio-Ontologies SIG
(Special Interest Group) meeting with a phenotype-
focused Phenotype Day, held at the Intelligent Systems
for Molecular Biology (ISMB) conference in Dublin,
Ireland in 2015. This was the second combined event,
following on from a successful event in 2014 [1]. The
papers that feature in this special issue are a selection of
submissions to the meeting that were extended and sub-
mitted for consideration by the journal. All papers were
substantially revised from the original SIG meeting pa-
pers, and underwent the standard journal peer review
process.
The Bio-Ontologies meeting, as in years past, invited
presentation and discussion of research across a broad
scope, encompassing the organization and dissemination
of knowledge in biomedicine and any aspect of applica-
tion of ontologies in life sciences research. There were
14 submissions to the Bio-Ontologies SIG meeting, and
12 were accepted to the workshop, including six short
papers and six flash updates. Of these, four were
extended for this special issue.
Phenotype Day brought together researchers from a large
number of different domains to discuss aspects of represen-
tation, integration and dissemination of phenotype data
across domains and disciplines. There were 15 submissions
in total to Phenotype Day, with 11 accepted to the workshop
(one full paper, six short papers, one position paper, three
posters). Of these, five appear in this special issue.
Summary of selected papers
In correspondence with the broad selection of topics rep-
resented at the Bio-Ontologies Special Interest Group
meeting, the papers selected for this special issue also
cover this range of topics. The areas of interest included
not only the creation, further development and integration
of existing ontologies, but also their application in phe-
nomics research. Application areas include the representa-
tion and integration of model organism databases as well
as text mining of the scientific literature or clinically
relevant documents, such as electronic health records.
Summary of papers from bio-ontologies
Two papers in this special issue address issues core to the
construction of ontologies for management of biological
information. Vita et al. [2] directly propose an extension
to a previously-proposed Major Histocompatibility Com-
plex (MHC) Restriction ontology called MaHCO [3], con-
structed with the assistance of ontology design patterns,
* Correspondence: karin.verspoor@unimelb.edu.au
1Department of Computing and Information Systems, The University of
Melbourne, Melbourne, VIC, Australia
Full list of author information is available at the end of the article
© The Author(s). 2016 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Verspoor et al. Journal of Biomedical Semantics  (2016) 7:66 
DOI 10.1186/s13326-016-0108-7
while Jupp et al. [4] introduce a web-based tool to ease
authoring of ontologies with built-in support for enforcing
precisely such design patterns.
Specifically, the ontology proposed by Vita et al. [2]
aims to enable representation of MHC molecules in the
Immune Epitope DataBase (IEDB), in terms of their rela-
tion to immunological experiments. These molecules
play an important role in the adaptive immune system,
and because of their wide variation and broad relevance,
pose a challenge to knowledge representation. The
enriched MHC ontology enables logical querying of
MHC molecules, in terms of a protein complex of two
chains, and includes the details of their locus, haplotype
and/or serotype, as well as the haplotype of the host spe-
cies. Finally, the experimental evidence for the MHC re-
striction is also modelled. The authors have provided
users of the IEDB the capability to search complex rela-
tionships among MHC genes and MHC restrictions, in
terms of standard ontology identifiers wherever possible.
In their software article, Jupp et al. [4] introduce the
Webulous application suite, including an add-on appli-
cation for Google Sheets that allows population of ontol-
ogy design templates with content, and demonstrate it
with a case study using the Experimental Factor Ontol-
ogy (EFO). This software allows addition of ontology
content in bulk, while ensuring consistency of that con-
tent. It includes access to BioPortal services [5] that
allow users to search for existing ontology terms to fa-
cilitate ontology integration and reuse. The templates
themselves allow automatic creation of relations or as-
sertions from data entered into a spreadsheet, using con-
sistent transformation of the data to OWL axioms. In
short, it supports large-scale ontology development with
the assistance of domain experts who may not them-
selves be ontology experts.
Webulous is used to create terms in EFO for the work
described by Sarntivijai et al. [6]. In the context of the
Centre for Therapeutic Target Validation (CTTV), they
aim to represent disease-phenotype associations, with
the objective of linking rare and common diseases to en-
able identification of potential therapeutic (drug) targets.
A particular representational challenge tackled in this re-
search is to capture phenotypes that are only sometimes
associated to a disease, to reflect that not all relevant
phenotypes will be present in every presentation. This is
done through the use of a generic association model
OBAN (Open Biomedical AssociatioN) which allows
qualification of association with evidence and, eventu-
ally, frequency. The authors describe the use of text
mining of the literature to identify candidate disease-
phenotype associations that are curated and transformed
into the OBAN model using EFO.
Leung and Dumontier [7] similarly apply text mining
in the context of disease associations, in their case
considering drug-disease associations as extracted from
drug structured product labels. The identified associa-
tions are compared to the clinical practice guidelines,
with the finding that there is not a large overlap between
the disease indications for drugs in their structured la-
bels, and the indications for those same drugs in clinical
practice guidelines. The authors did find that using taxo-
nomic relationships among drugs did improve the over-
lap, but a substantial gap remained. The study raises
concerns about the inconsistent evidence between these
drug-related information sources and has implications
for clinical decision making in evidence-based practice.
Summary of papers from Phenotype Day
Bello and colleagues report in Inferring Gene-to-
Phenotype and Gene-to-Disease Relationships at Mouse
Genome Informatics: Challenges and Solutions [8] on an
algorithm for the assignment of gene-phenotype and
gene-disease association from the existing genotype-
phenotype links contained in the Mouse Genome Inform-
atics (MGI) database. The algorithm has been applied to
the existing wealth of data in this database to the effect
that 2100 mouse markers could be linked to human dis-
ease and 16,000 mouse markers could be linked to pheno-
types. The resulting gene-phenotype and gene-disease
associations are provided as part of the databases web
pages and can be downloaded by interested parties.
In Interoperability between phenotypes in research
and healthcare terminologies - Investigating partial map-
pings between HPO and SNOMED CT [9], Dhombres
and colleagues report about their investigations to deter-
mine partial alignments between both the Human
Phenotype Ontology (HPO) and SNOMED CT using
modifier terms and HPOs subsumption relations. Using
the suggested approach, the authors identified partial
mappings for 92% of the investigated HPO terms. 30%
out of these 92% partial mappings correspond to equiva-
lence statements, while the remaining 60% follow a
next-best approach to allow for traversing between both
ontologies.
Mowery et al. in Extracting a Stroke Phenotype Risk
Factor from Veteran Health Administration Clinical Re-
ports: An Information Content Analysis conducted ex-
periments to investigate the report of a stenosis
phenotype in relation to stroke in radiology and text in-
tegration utility notes [10]. These notes were gathered
from the Veteran Health Administration electronic
health records. The authors analyse sections and pure
textual representations in both types of records using
pyConText. The results show that there are differences
in the performance of stenosis identification and the
location of reporting for both types of note. Yet the
authors conclude that pyConText can still be used to fil-
ter chart reports into significant and no/insignificant
Verspoor et al. Journal of Biomedical Semantics  (2016) 7:66 Page 2 of 4
stenosis findings for the data from the Veteran Health
Administration, facilitating further studies on effective-
ness of stroke prevention.
Tudose et al. present in PhenoImageShare: An image
annotation and query infrastructure [11] a phenotype
annotation infrastructure for image data. Images are
imported from four different resources, leveraging ontol-
ogy annotations from the original repository. Further-
more, images can be manually annotated using a variety
of ontologies, such as UBERON or the Mammalian
Phenotype Ontology (MP). The annotation service is in-
dependent from species and image data. PhenoImage-
Share holds to date ~118 k images (retrieved from
mouse and fly databases) associated to anatomical or
phenotype concepts (so called regions of interest). The
phenotype image data can be accessed either via a web
interface or an API.
The manuscript Reporting phenotypes in mouse
models when considering body size as a potential con-
founder by Oellrich and colleagues [12] investigates the
challenges surrounding confounding variables in experi-
mental studies associating genotypes and phenotypes.
The authors provide a case study based on the experi-
mental results released by the International Mouse Phe-
notyping Consortium (IMPC) and further discuss the
limitations of current ontological representation to re-
port on confounding effects. The authors conclude that
further discussion is needed within the community to
derive a community-approved representation and dis-
semination of confounders in genotype-phenotype asso-
ciation studies.
Conclusions
The storage, retrieval, and analysis of ever-growing bio-
logical information is complicated by the complexity and
diversity of that information. To the extent that
consistency in representation of this information can be
achieved through the use of a common terminology and
validated relationships, and that strategies for modeling
rare, confounded, or highly contextual relationships can
be developed, it is possible to make progress on making
that information findable and available for further inves-
tigation. The papers in this thematic issue contribute to
those goals, both by addressing the foundational issues
of representational expressivity as well as consistency of
use of bio-ontologies, and by demonstrating the applica-
tion of such structured representations to support infer-
ence from biological or biomedical data, on tasks
ranging from determination of stroke risk factors and
predictions of novel gene-disease associations. The pa-
pers raise some important challenges yet add to the body
of research that establishes the promise of improved bio-
medical information access and analysis.
Acknowledgements
We thank the International Society for Computational Biology (ISCB) for
hosting the Bio-Ontologies SIG and Phenotype Day at the 2015 Intelligent
Systems for Molecular Biology (ISMB 2015) meeting in Dublin, Ireland.
Funding
Not applicable.
Availability of data and materials
Not applicable.
Authors contributions
NS, MD, PRS and LS were the organizers of the Bio-Ontologies 2015 SIG meeting at
ISMB 2015; NC, TG, AO, and KV were the organizers of the Phenotype Day at the
Bio-Ontologies 2015 SIG meeting at ISMB 2015. KV and AO were the Guest Editors
of the Thematic Issue and wrote the overview of the Thematic Issue papers. All
authors read and approved the manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Author details
1Department of Computing and Information Systems, The University of
Melbourne, Melbourne, VIC, Australia. 2MRC Social, Genetic & Developmental
Psychiatry Centre (SGDP), Kings College London, London SE5 8AF, UK. 3The
Language Technology Lab, Department of Theoretical and Applied
Linguistics, University of Cambridge, Cambridge, UK. 4Centre for Clinical
Genomics, Garvan Institute of Medical Research, Sydney, NSW, Australia. 5St
Vincents Clinical School, Faculty of Medicine, University of New South Wales,
Sydney, Australia. 6University of Oxford e-Research Centre, 7 Keble Road, OX1
3QG Oxford, UK. 7Brunel University London, London, UK. 8Stanford University,
Stanford, CA, USA.
Received: 29 October 2016 Accepted: 18 November 2016
RESEARCH Open Access
miRiaD: A Text Mining Tool for Detecting
Associations of microRNAs with Diseases
Samir Gupta1*, Karen E. Ross2, Catalina O. Tudor1,2, Cathy H. Wu1,2, Carl J. Schmidt3 and K. Vijay-Shanker1
Abstract
Background: MicroRNAs are increasingly being appreciated as critical players in human diseases, and questions
concerning the role of microRNAs arise in many areas of biomedical research. There are several manually curated
databases of microRNA-disease associations gathered from the biomedical literature; however, it is difficult for
curators of these databases to keep up with the explosion of publications in the microRNA-disease field. Moreover,
automated literature mining tools that assist manual curation of microRNA-disease associations currently capture
only one microRNA property (expression) in the context of one disease (cancer). Thus, there is a clear need to
develop more sophisticated automated literature mining tools that capture a variety of microRNA properties and
relations in the context of multiple diseases to provide researchers with fast access to the most recent published
information and to streamline and accelerate manual curation.
Methods: We have developed miRiaD (microRNAs in association with Disease), a text-mining tool that
automatically extracts associations between microRNAs and diseases from the literature. These associations are
often not directly linked, and the intermediate relations are often highly informative for the biomedical researcher.
Thus, miRiaD extracts the miR-disease pairs together with an explanation for their association. We also developed a
procedure that assigns scores to sentences, marking their informativeness, based on the microRNA-disease relation
observed within the sentence.
Results: miRiaD was applied to the entire Medline corpus, identifying 8301 PMIDs with miR-disease associations.
These abstracts and the miR-disease associations are available for browsing at http://biotm.cis.udel.edu/miRiaD. We
evaluated the recall and precision of miRiaD with respect to information of high interest to public microRNA-
disease database curators (expression and target gene associations), obtaining a recall of 88.4690.78. When we
expanded the evaluation to include sentences with a wide range of microRNA-disease information that may be of
interest to biomedical researchers, miRiaD also performed very well with a F-score of 89.4. The informativeness
ranking of sentences was evaluated in terms of nDCG (0.977) and correlation metrics (0.678-0.727) when compared
to an annotators ranked list.
Conclusions: miRiaD, a high performance system that can capture a wide variety of microRNA-disease related
information, extends beyond the scope of existing microRNA-disease resources. It can be incorporated into manual
curation pipelines and serve as a resource for biomedical researchers interested in the role of microRNAs in disease.
In our ongoing work we are developing an improved miRiaD web interface that will facilitate complex queries
about microRNA-disease relationships, such as In what diseases does microRNA regulation of apoptosis play a
role? or Is there overlap in the sets of genes targeted by microRNAs in different types of dementia?.
Keywords: MicroRNA, Disease, Associations, Text-mining, Relation extraction, Natural language processing
* Correspondence: sgupta@udel.edu
1Department of Computer and Information Sciences, University of Delaware,
Newark, DE 19711, USA
Full list of author information is available at the end of the article
© 2016 Gupta et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 
DOI 10.1186/s13326-015-0044-y
Background
MicroRNAs (miRs) are a class of small non-coding RNAs
encoded in the genomes of animals, plants, and protozoa.
In general, miRs negatively regulate gene expression by
base pairing with sequences in the 3-untranslated region
of mRNAs, which either inhibits their translation or trig-
gers their cleavage. Thousands of miRs have been identi-
fied in mammals, and they have been implicated in the
control of a wide range of biological processes [1].
miRs are increasingly being appreciated as critical
players in human disease. The role of miRs in cancer is
very well established, with a wealth of studies demonstrat-
ing the participation of miRs in multiple cancer-related
processes in diverse tissue types [2]. miRs have also been
linked to many other diseases, including cardiovascular
disease [3], diabetes [4], neurological disease [5] and liver
[6] and intestinal [7] disorders.
Although at a mechanistic level miRs influence disease
through their effects on the expression of their target
genes, in the scientific literature miRs are associated
with diseases through a variety of relationships. In some
cases, miRs are directly associated with the disease itself
or with a feature or outcome of the disease, such as ag-
gressiveness [8], invasiveness [9], or patient survival [10].
In other cases, miRs are identified as biomarkers [11] or
therapeutic targets [12] for a disease. miRs can also be
linked to biological processes that are, in turn, con-
nected to the disease. This category includes miR-gene
targeting events, as well as regulation of processes such
apoptosis [13], metastasis [14], or cholesterol transport
[15] by miRs. Finally, in some cases, it is the state of the
miR (e.g., over- or under-expression [16]) that is associ-
ated with the disease.
There are currently several high-quality databases
that capture miR-disease associations and some of the
above relations, including miR2Disease [17], miRCan-
cer [18] and the Human microRNA Disease Database
(HMDD; [19]). These resources are literature based
and support searches for miR or disease of interest.
miR2Disease and miRCancer provide information on
miR expression in disease, and miR2Disease addition-
ally covers miR target genes. HMDD documents miRs
that are potential biomarkers and provides several ana-
lysis tools, such as miR set enrichment analysis. miR2-
Disease and HMDD are manually curated; thus they
are limited by the time-consuming nature of manual
curation and have difficulty keeping up with the explo-
sion of publications in the miR-disease field. In 2014
alone, using the PubMed query miRNA[TIAB] OR
microRNA[TIAB] OR miR[TIAB], we obtained around
5100 PubMed citations, which was a 120 % increase
compared to 2013 and a 160 % increase compared to
2012. Additionally, we identified 19,402 abstracts (as of
February 2015) that mention miRs and of these, we
estimate 15,171 abstracts also mention disease terms
(as detected by PubTator [20]).
Automated literature mining tools could help
streamline and accelerate the curation process as well
as provide researchers with fast access to the most re-
cent published information; however, currently, such
tools are limited and have not been widely adopted.
Most of the miR-related literature mining tools avail-
able focus on extraction of miR-target gene relations
without regard to disease, and rely on relatively simple
text mining techniques, such as co-occurrence of miR
and disease in the same sentence or abstract. These in-
clude miRSel [21] and the tools used by the miR-target
databases miRWalk [22], TarBase [23], and miRTarBase
[24]. miRCancer [18], is one of the few resources that
uses a rule-based system to identify disease-relevant
miR information in literature, but is limited to detect-
ing miR expression associations in cancer.
In this work, we present miRiaD (microRNAs in asso-
ciation with Disease), which automatically extracts from
the biomedical literature associations between miRs and
diseases together with any intermediate relations that
bridge the association, thereby capturing myriad ways in
which a miR can be associated with a disease. In general
miRiaD connects a miR or an aspect of a miR (e.g., dif-
ferential expression, methylated state) to a disease or a
disease aspect (e.g., outcome or therapy) through some
relations (e.g., involvement, regulation, is-a). For example,
miRiaD can extract a miRs involvement in the outcome
of a disease, or its role as a biomarker or therapeutic target
for a disease. Additionally, miRiaD can extract the involve-
ment of a miR in some cellular process that is highly
related to a disease, thus (indirectly) linking the miR with
the disease. These links between a miR and a disease
through cellular processes or target gene, which we refer
to as linking entity, are often implicit but highly inform-
ative to researchers studying disease mechanisms. Our
previous work, STEM [25], extracted relations between
two entities, namely a miR and process/function terms.
miRiaD extends upon the previous work by allowing more
type of entities to be linked (e.g. disease with its outcome,
miR with a disease outcome etc.).
We have applied miRiaD to the entire set of Medline ab-
stracts, and we provide a web interface through which the
results can be searched using PubMed-like queries. Details
about our miR-disease association extraction approach are
presented in the Methods section; screenshots and details
about the interface are provided in the Results and Discus-
sion section. In conjunction with miRiaD, we developed a
procedure for ranking sentences containing miR and dis-
ease mentions according to their informativeness, which
we envision can be used in the future to guide how miR-
iaD results will be presented to the user. The details of this
approach are given in the Methods section.
Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 2 of 15
miRiaD was evaluated with two potential user commu-
nitiesmiR-disease database curators and biomedical re-
searchersin mind. To address the needs of curators, the
recall and precision of miRiaD was evaluated with re-
spect miR-target and miR expression information, which
are the two types of information curated by miR2Di-
sease, the most comprehensive database for miR-disease
associations; this evaluations achieved recall results be-
tween 88.4690.78 %. For biomedical researchers, who
are potentially interested in the full range of possible
connections between miRs and disease, we evaluated
miRiaD with respect to a variety of sentences in which
miRs and diseases co-occur, resulting in F-scores of
89.4 %. Finally, an evaluation of our informativeness
ranking system accomplished an nDCG of 0.9815, as
well as correlations of 0.6780.727, when compared to
an annotators ranked list. Details about the experimen-
tal setup and the evaluations are given in the Results
and Discussion section.
Methods: Approach and Implementation
In developing the miRiaD system, we attempted to cap-
ture the variety of ways in which connections between a
miR and a disease are stated in text. Figure 1 schematic-
ally depicts these relationships. First, both miRs and dis-
eases are often associated with descriptive information
or properties, which we will collectively refer to as as-
pects. Examples of miR aspects include expression level
and state (e.g., hyper-methylated or mutated); examples
of disease aspects include outcome/stage, biomarker, or
therapy. For convenience, we will refer to a miR or its
aspects as a miR entity (e.g. mir-9, overexpressed mir-9,
hypermethylation of mir-9) and likewise refer to a dis-
ease or its aspects as a disease entity (e.g. gastric cancer,
biomarker for gastric cancer). In some sentences, a miR
entity may be directly related to a disease entity. In other
cases, a miR entity may regulate a target gene or be in-
volved in a biological process that is in turn implicitly
linked to a disease entity. Even more complex associa-
tions are possible; for example, a miR may regulate a
gene that is involved in a biological process that is ultim-
ately relevant to a disease. These relationships may be
expressed in text using a variety of phrases and not all
phrases are applicable to all types of relationships. An
association between a miR and a process or disease is
likely to be described using relations such as involved
in or has a role in, whereas an association between a
miR and biomarker is likely to be expressed using an
is-a relation. A formal description of the patterns,
the list of triggers and the types relations between the
different pairs is provided in Additional file 1.
miRiaD identifies specific relations (i) between a miR and
its aspect, (ii) between a disease and its aspect and (iii) be-
tween a miR entity and a disease entity. For example, in the
sentence Downregulation of mir-26a is associated with
tumor metastasis in osteosarcoma., miRiaD will detect
the connection between mir-26a and its aspect, downregu-
lation; between the disease osteosarcoma and its aspect,
tumor metastasis; and between miR entity miR-26a down-
regulation and the disease entity of tumor metastasis in
osteosarcoma.
miRiaD also detects multi-step connections where the
connection between the miR entity and disease entity is
mediated through another entity or process. We call this
extra entity or process a linking entity. Consider the sen-
tence, MicroRNA-9 promotes tumor metastasis via
repressing E-cadherin in esophageal squamous cell carcin-
oma. Typically, as in this sentence, the miR entity regu-
lates or is involved with the linking entity (E-Cadherin).
This regulation in turn can be connected to the disease
Fig. 1 miR-disease associations extracted by miRiaD. miRs or their aspects (state or expression levels) can be directly associated with diseases or
disease aspects (outcome, stage, biomarker/therapy) through a variety of relations; association can also be bridged by a linking entity such as a
target gene or biological process
Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 3 of 15
entity (tumor metastasis of esophageal squamous cell
carcinoma). However, it is quite common for the connec-
tion between the linking entity and the disease entity to be
left unstated with an understanding of the implicit con-
nection requiring additional domain knowledge.
miRiaD currently extracts information from Medline
abstracts. After the abstracts are retrieved the abstract
text and title are extracted. The text is split into individ-
ual sentences using a tool developed in-house. miRiaD
extracts the connection between a miR and disease,
through the detection of the direct relations between
miR entities and disease entities, or through detection of
multiple relations involving linking entities. miRiaD uses
the presence of certain lexico-syntactic dependency
structures in a sentence to detect these semantic rela-
tions. Thus, the basic steps of the miRiaD system in-
clude (i) Detecting miR/disease entities and linking
entities; (ii) preliminary syntactic processing; (iii) identi-
fying syntactic dependencies between miRs and co-
occurring terms; and (iv) assigning semantic relations
between miRs and co-occurring terms. These steps are
described below and also shown in Fig. 2. Finally, we
also describe a method to score sentences based on their
informativeness in describing miR-disease connections.
Detecting miR/disease entities and linking entities
miR entity
A miR entity can be a miR in isolation or together with one
of its aspects (expression, mutation, methylation). Although
miRs are mentioned in text in a variety of ways (e.g., miR-1,
microRNA1, miRNA-1, let-1, etc.), they follow a well-
established naming convention. miR mentions consists of a
prefix (miR, MIR, miRNA, microRNA) followed by a
unique identifying number, which is assigned based on se-
quence similarity. This number may be followed by a suffix
such as -a, -1, -3p or -5p, and/or a prefix that de-
notes the species may be included. miRiaD detects such
miR mentions by using simple regular expressions.
miR aspects usually describe the abundance or prop-
erties of a miR. We detect the former by searching for
noun phrases headed by trigger words such as level,
expression and regulation as well as their variants.
For the latter, we consider mutation terms such as
mutation, variants or polymorphism as well as
nominalized forms of common events such as methy-
lation. Of course these terms are only candidates to be
miR aspects and are treated as such only after we de-
tect their syntactic relation to a miR.
Disease entity
A disease entity can be disease in isolation or in combin-
ation with one of its aspects (diagnostic, treatment, out-
come). We detect disease mentions using Pubtator [20]
database, which includes disease mentions tagged in
Medline abstracts by DNorm [26]. The disease mentions
are normalized to Medic concept IDs. We process only
those abstracts which have a disease mention and thus
recall for disease mention detection is important for the
miRiaD system performance. The DNorm [26] system
reports a micro-averaged precision, recall and f-measure
of 0.803, 0.763 and 0.762.
We did some additional analysis for PubTator disease
detection by randomly selecting 200 abstracts from the
miR2Disease database and checking whether the disease
annotated by the miR2Disease curators was detected by
Pubtator. A miR2Disease abstract can be annotated with
a disease which is not mentioned in the abstract but
mentioned in the full length article. Thus we selected
only those miR2Disease abstracts where the annotated
disease either was mentioned in the title or the abstract.
While checking if the annotated disease matched one of
the disease mentions detected by Pubtator, we allowed for
synonym matches (breast cancer with breast carcinoma).
Pubtator picked the exact name, a synonym or part of the
annotated disease name in 100 % of the abstracts. How-
ever in ten cases, it picked only part of the name, despite
Fig. 2 miRiaD Pipeline. The steps of the miRiaD pipeline are illustrated with numbered grey blocks. External tools used throughout the pipeline
are shown in bold and italic font
Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 4 of 15
the fact that there was ample evidence of the full
name mention. For example, in a sentence that con-
tained the phrase Oral Squamous Cell Carcinoma
(OSCC) , PubTator only detected Squamous Cell
Carcinoma.
Similar to miR aspect detection, we locate disease as-
pects by looking for certain trigger words or and their
textual variations. Commonly occurring disease aspect
terms include disease stage or outcome terms/phrases
like clinical outcome, disease free survival (dfs), over-
all survival (os), metastasis, sensitivity, prognosis,
tumorgenicity, invasion, and progression. Diagnosis-
related terms include biomarker, marker, predictor,
profiler, prognostic, diagnosis, indicator, and their
textual variations; and finally treatment-related triggers in-
cluding therapeutic, treatment, target, therapy, and
their textual variations. As with miR aspects, a candidate
phrase is considered to be a disease aspect only after the
syntactic dependency (as described later) with a disease
mention is established. Several examples of miR entities
(let-7i and low miR-335 levels) and disease entities
(colorectal cancer metastasis, overall survival, and re-
lapse-free survival) are highlighted in the following
sentence fragments: let-7i is associated with colorectal
cancer metastasis and low miR-335 levels in EOC
were associated with shorter overall survival and relapse-
free survival
Linking entity
Linking entities are cellular processes or target genes
through which the miRs association with the disease can
be explained. We use PubTator, which uses GenNorm
[27] to identify gene mentions in a sentence. In order to
detect cellular processes, we follow the method adopted
in eGIFT [28], which uses dictionary look-ups or mor-
phological derivatives e.g., terms/phrases with -sion,
?tion, ?sis, ?or, ?er, ?ment suffixes. Candidate linking
entity phrases are excluded if they are determined to be
a miR/disease aspect phrase. Additionally syntactic de-
pendency (as discussed later in this section) needs to be
established for the candidate phrase to be considered as
a linking phrase. In the sentence fragment mir-320a
down-regulation mediates bladder carcinoma invasion
by targeting itgb3 the linking entity is highlighted.
Preliminary Syntactic Processing
The miRiaD approach attempts to identify a relation be-
tween a miR entity and a disease entity or linking entity
by first identifying syntactic dependencies between
phrases of that sentence. Two steps facilitate the detec-
tion of such syntactic dependencies: chunking and sim-
plification. Note, to reduce overhead for the chunking
and simplification step, we filter out sentences that do
not contain a miR or a disease mention. Chunking is the
task of identifying and grouping words in a sentence into
constituents (noun groups, verb groups etc.) called
chunks. Sentences are tagged with part-of-speech
(POS) tags using the Genia Tagger [29]. We further
chunk the words based on syntactically related POS tags
to form noun phrases (NPs), verb groups (VGs) and
prepositional phrases (PPs).
After chunking, we use iSimp [30], which simplifies a
variety of complex syntax structures in a sentence into a
relatively small number of simple patterns, thus facilitating
the identification of syntactic dependencies and relation
extraction. iSimp [30] identifies syntactic constructs, such
as appositives, relative and reduced relative clauses, con-
junctions, and parenthetical elements. These syntactic
constructs are used to form simple sentences from a com-
plex sentence. For example, consider the following
sentence:
We have profiled four miRNAs, miR-21, miR-210,
miR-155, and miR-196a, all implicated in the
development of pancreatic cancer with either proven
or predicted target genes involved in critical cancer-
associated cellular pathways. (PMID 19,723,895)
We illustrate here how miRiaD identifies the relation
between miR-21 and the disease entity pancreatic can-
cer development. Automatically detecting a relationship
between miR-21 and pancreatic cancer in this sentence
might be difficult by just trying to match basic patterns
and rules. Additional information of no immediate use
occurs between the two mentions, preventing the pat-
terns from detecting a relationship. However, by using
simplification constructs identified by iSimp, we can
generate a simple sentence from the original sentence
that states the relationship in a straightforward way:
Mir-21 is implicated in the development of pancreatic
cancer. iSimp tags the following syntactic constructs:
(i) A conjunction in the form of a list of elements
(miR-21, miR-210, miR-155, and miR-196a),
(ii) A conjunction (proven or predicted),
(iii)The appositive construct involving the two noun
phrases four miRNAs and miR-21, miR-210,
miR-155, and miR-196a,
(iv)The reduced relative clause all implicated in the
development of pancreatic cancer with either proven
or predicted target genes, which modifies the noun
phrase four miRNAs
(v)Another reduced relative clause involved in critical
cancer-associated cellular pathways, which modifies
the noun phrase target genes
Using constructs (i) and (iii) we can replace four
miRNAs by miR-21 in the construct (iv) and thus
Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 5 of 15
generate miR-21 is implicated in the development of
pancreatic cancer . Additional simplifications are
also generated for the remaining elements of the list
(miR-210, miR-155, or miR-196a). Note that the
chunking information is carried through when generat-
ing simplified sentences from iSimp constructs.
Identifying Syntactic Dependencies
miRiaD extends our previous system [25], which was built
to detect semantic relations, such involvement, regula-
tion, state, attribute and association, between miRs
and biological processes or functional aspects of miRs.. Like
STEM, to capture the above semantic relations we detect
these semantic relations via the detection of the following
lexico-syntactic dependencies. 1) agent-theme of predicate
(e.g., miR-9 regulates cell proliferation), and 2) noun
modification (e.g., miR-1 overexpression, metastasis of
gastric cancer). We detect these lexico-syntactic depend-
encies after chunking and sentence simplification.
Agent-theme of predicate
We are interested in extracting the relations of involve-
ment and regulation where a miR entity is the agent
and a process (for example) is the theme. These often
have a syntactic dependency counterpart (subject and
object of a verb). Consider the example sentence miR-9
regulates cell proliferation. Here the regulation rela-
tion is represented by the verb regulate and the agent
(mir-9) and theme (cell proliferation) are the subject and
object of this verb. Typically the predicates are simple
verbs, and the arguments are noun phrases appearing to
the left and to the right of the predicate if the clause is
in active voice. However, with the use of chunking, it is
possible to extract relationships from more complex
sentences as well. Consider the sentence mir-9 is
known to directly regulate cell proliferation. Notice by
chunking this sentence we will get two verb groups (is
known and to directly regulate). The predicate verb
(regulate) is the head verb of the second of the two
consecutive verb groups and thus the noun phrase be-
fore this predicate is still miR-9. In the sentence ex-
pression of mir-9 regulates proliferation of U87 cells.
the agent and the theme are not base noun phrases, but
are larger noun phrases that have attached prepositional
phrases, which are identified by chunking.
The voice of the predicate verb is important in deter-
mining the role of the arguments. For example, in the
sentence apoptosis is regulated by miR-9, miR-9 is
the agent performing the action regulates and apop-
tosis is the theme being regulated. This is the reason
why, rather than using syntactic notions of subject and
object, we refer to more thematic notions of agent and
theme of a predicate, abstracting away from word order.
There are also cases in which the predicate is
expressed in a nominalized form. For example, consider
the fragment miR-9 regulation of cell proliferation,
where the predicate is the nominalized form of the verb
regulates. We extend our agent-theme extraction rules
to handle such cases by following the rules for
nominalization from iXtractR [31].
Predicates need not necessarily be single words. In the
sentence miR-146a may play a role in cell prolifera-
tion, the predicate spans multiple words. Phrases like,
is crucial for, is important for, plays a role in are
some examples of multi-word predicates that we con-
sider. We defer to the next part of this section for a dis-
cussion of the words and phrases that constitute the
predicates we are interested in.
In some sentences, an argument (agent/theme) can
be shared by two predicates. For example, consider the
sentence miR-126 was able to inhibit laryngeal squa-
mous cell carcinoma partly by suppressing Camsap1
expression. Here, miR-126 is the agent for both in-
hibit and suppressing predicates. Because it is awk-
ward to mention the agent each time one of its
predicates is used, the agent is mentioned only once, in
the beginning, and omitted in the second case (i.e., for
the verb suppress). We call the cases in which the ar-
gument is omitted and inferred from an earlier men-
tion the null-argument agent-theme of predicate.
Sentences with null-arguments have clauses separated
by prepositions to + a simple verb, via or through
+ nominal form of a verb, or by + a verb ending in
ing. For example, Tumor suppressive miR-1 in-
duces apoptosis through direct inhibition of SRSF9 in
bladder cancer is a sentence containing a null-
argument after through + nominal form of a verb.
The null argument rules that identify the agent for the
second predicate were taken from RLIMS-P [32] and
iXtractR [31].
Our rules are limited to some simple patterns corre-
sponding to simple syntactic structures in order to be
precise. Sentence simplification allows us to handle more
complex cases without making the patterns more in-
volved. Consider the sentence breast cancer metastasis
suppressor 1 up-regulates mir-146, which suppresses
breast cancer metastasis, which contains a relative
clause. Without simplification our rules would incor-
rectly extract [which (agent), suppresses (predicate),
breast cancer metastasis (theme)]. iSimp detects the
relative clause and generates a simplified sentence mir-
146 suppresses breast cancer metastasis, thus enabling
us to extract the correct relation [miR-146 (agent), sup-
presses (predicate), breast cancer (theme)]. As discussed
in the pre-processing step the other simplification con-
structs which iSimp handles include appositives, con-
junctions, reduced relative clauses and parentheticals.
Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 6 of 15
Noun modification
We detect state or attribute relations that are in a
noun modification syntactic relationship. Consider the
subject noun phrase in the sentence mir-21 overexpres-
sion is associated with glioblastoma. The entity miR-21
acts a modifier of a noun, overexpression, which is a
nominalized form of a stative verb. In this case the head
modifier relation indicates the state of entity miR-9.
Additionally the head modifier dependency can be stated
with a prepositional attachment (e.g., metastasis of gastric
cancer). In this work, we only consider these two forms of
noun modification syntactic dependency.
Assigning Semantic Relations
Having captured the syntactic dependency structures, we
look at the predicate for the identification of the seman-
tic relation between a miR entity and a disease entity or
a linking entity. In our previous work we observed that
the semantic relations that frequently connect a miR en-
tity with a related term (disease entity or linking entity)
fall into several categories, namely involvement, regu-
lation, is-a, association found-in and state. Each
category encompasses a number of trigger words that are
commonly found in text. The predicate detected in the
pattern used to detect the syntactic dependency can be
used to assign the relation to the appropriate category. For
example, if the predicate found in the sentence is is in-
volved or is implicated, then the relation is categorized
as an involvement relation. As noted earlier, the verb
group containing the predicate can contain additional
words modifying the predicate (e.g., directly targets or
was able to inhibit). In these situations, the head of the
final verb group is used when matching the trigger words
(e.g., targets or inhibit). Example trigger words and
sentences are given below for each of the four categories.
Although trigger words are provided in present tense in
the examples below, the reader should assume all of their
textual variations (tense and nominalized forms).
Involvement relations
For detection of the involvement relation, we expect
the use of the agent-predicate-theme dependency struc-
ture. Further, we expect the miR entity and the disease
entity or a linking entity to be picked as the agent and
theme respectively. A range of words or multi-word trig-
gers can be used as the predicate including: is involved in,
is implicated in, is required for, is needed for, play a role
in, is necessary/sufficient/crucial/etc. for, is dependent on,
participates in, contributes to, influences, fosters, affects,
allows, initiates, etc. The main verbs in these triggers can
appear in different tense forms and the verb or noun can
be modified, as in the sentence miR-21 may play a critical
role in chronic myelogenous leukemia.
Regulation relations
Regulation relations are similar to involvement relations
except for the list of trigger words/phrases that can serve
in the predicate position. Based on our previous work,
the regulation trigger words we use include: regulates,
promotes, induces, elevates, targets, enhances, increases,
decreases, raises, up/down-regulates, modulates, causes,
results, interacts, blocks, mediates, etc. In this category,
we also encounter cases in which the predicate is
expressed in a nominalized form. An example sentence
is restoration of mirna-143 (mir-143) regulates cox-2
and inhibits cell proliferation of pancreatic cancer cells.
Here the association between the miR-143 and the dis-
ease pancreatic cancer is via two linking entities: gene
(cox-2) and cellular process (cell proliferation).
Association relations
To find sentences where a miR entity is associated with
a process, linking entity, a disease or a disease outcome,
we consider sentences with agent-predicate-theme de-
pendencies with the following trigger words or phrases:
is associated with, correlated with, linked to, etc.
For example, in the sentence reduced circulating mir-150
levels are associated with poor survival in pulmonary ar-
terial hypertension. a miR entity (expression of mir-150)
is linked to a disease entity (poor survival in pulmonary
arterial hypertension).
Is-a relations
Relations that link two entities via the is_a relation are
normally of agent-predicate-theme type. We expect the
miR entity and the disease entity or linking entity to be
picked as the agent and theme respectively, with be as
trigger. However we include a wider range of trigger
words or phrases such as:. is, are, acts as, functions as,
serves as, etc. In the example sentence Plasma miR-601
and miR-760 can potentially serve as promising non-
invasive biomarkers for the early detection of colorectal
cancer both miR-601 and miR-760 are in a is_a relation
with non_invasive biomarkers as indicated by the trig-
ger phrase serve as. Note that sentence simplification
makes it possible to link both miR to the theme non-in-
vasive biomarkers.
Found_in
For detection of the found_in relation, we expect the
use of the agent-predicate-theme dependency structure.
Further, we expect the miR entity and the disease to be
picked as the agent and theme respectively. These rela-
tions indicate that an aspect of the miR (expression,
states like mutation, hyper-methylation) is found in the
disease. There are two classes of triggers used to detect
such relations. First set of triggers include words or
multi-word triggers like: is found in, is detected in,
Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 7 of 15
is increased in etc. In these cases the miR entity will in-
dicate the specific aspect of the miR (e.g. high level of mir-
155 was found in gallbladder cancer). In other cases the
aspect of the miR is inferred from the predicate trigger (e.g.
miR-155 was overexpressed in gallbladder cancer). The
triggers to detect such cases include: overexpressed in,
highly expressed in, upregulated in, mutated in etc..
State relations
These relations are used to describe relations between a
miR and its aspect or between disease and its aspect. In
these sentences, the term describes the state in which
the miR (or its promoter) is observed (e.g., overex-
pressed, methylated) or an outcome, treatment, or diag-
nostic property of the disease. Here we use the noun
modification syntactic dependency, where miR aspect or
disease aspect is the head noun and the miR or disease,
respectively, is the modifier. For miR aspects we use trig-
ger words such as methylation, expression, silencing, and
knockdown while for disease aspects we use words/
phrases such as level, biomarker, or disease free survival.
An example sentence is Overexpression of the miR-200b
is associated with hepatocellular carcinoma cell migration
through the epithelial to mesenchymal transition.
Multiple predicate triggers In some sentences there
may be multiple predicate triggers appearing between an
agent and a theme. For example in the sentence fragment
miR-9 is involved in the regulation of apoptosis, the
two triggers involved in and regulation connect the
miR entity (mir-9) and the linking entity (apoptosis). In
such cases we can expect that one of the triggers will be
semantically more specific. Typically, we have found that
the more specific predicate is of the type regulation and
the more general predicate is of the type is-a or involve-
ment. Hence we modify the output to include only the
more specific relation. For example we first extract the
tuple [miR-9, is involved in, regulation of apoptosis] and
resolve it to [miR-9, regulates, apoptosis], categorizing it
as a regulation relation.
Assigning scores to sentences
As we have seen from the above sections, there are
many different ways in which a miR entity can be con-
nected to a disease entity in a sentence through various
linking entities and semantic relations. Biologists might
be interested in sentences containing some categories of
relation more than others, and might prefer sentences
mentioning certain types of semantic relations more
than others. We wanted to see if scoring sentences in
terms of linking entities, semantic relations, and other
factors (such as hedging), might help rank the sentences
in a specific way that is preferred by biologists. Such a
scoring system could potentially be used to prioritize the
most relevant sentences when presenting miRiaD results
to users.
Our assumption is that biologists might want to see
sentences mentioning a miR-disease relationship that is
explained via a target gene or a process. Equally import-
ant might also be sentences describing the miR-disease
relationship via a sequence of biological steps connected
by words such as contributes, results in, causes,
supports and their textual variations, as well as the
conjunction and. Less informative are sentences that
describe the miR-disease relationship in terms of expres-
sion level of the miR. We assign a score between 1 and
3 based on how informative the sentence is (highly in-
formative, informative, somewhat informative), using
empiricallyderived rules.
Sentences are assigned to the highly informative
(3 points) category if they contain some explanation
of the connection between the miR and the disease.
We use two indicators for detecting the explana-
tory component. The first is the detection of null argu-
ment sentences. For example, in the sentence miR-137
functions as a tumor suppressor by targeting CtBP1 to
inhibit epithelial-mesenchymal transition and inducing
apoptosis of melanoma cells the target gene aspect of the
miR explains its functionality as a tumor suppressor. The
second indicator of the explanatory component is the
presence of at least two semantic relations, which form
a sequence of events/process/outcome/diagnostic/treat-
ment.. One such example is Treatment of gastric cells
with dihydroartemisinin (DHA) increased miR-15b and
miR-16 expression, caused a downregulation of Bcl-2,
resulting in apoptosis of gastric cancer cells. In
addition we also assign sentences to the highly inform-
ative category if they contain target gene information
together with some other relation (e.g., UBASH3B is a
functional target of anti-invasive miR200a that is down-
regulated in triple negative breast cancer).
Sentences are assigned to the informative (2 points)
category if they contain both diagnostic and treatment
disease aspects, a linking entity regulation process, or a
treatment aspect of a disease in the theme (e.g., miR is a
therapeutic target for a disease). Some example sen-
tences in this category include: miR-23b is epigeneti-
cally down-regulated and restoration of miR-23b can
effectively suppress cell growth in glioma stem cells or
miR-139-5p is a potential biomarker for early diagnosis
and prognosis and is a therapeutic target for esophageal
squamous cell carcinoma (ESCC).
Sentences are assigned as somewhat informative
(1 point) if they contain an altered expression rela-
tion, an involvement relation, or a diagnostic aspect.
Some examples include: High miR-199a expression
is associated with liver fibrosis or miR-125b could
be an important prognostic indicator for colorectal
Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 8 of 15
cancer patients. All sentences that were not
assigned highly informative or informative scores are
also considered to be somewhat informative.
A sentence can be upgraded from somewhat inform-
ative to informative or informative to highly in-
formative if it contains an outcome aspect. A sentence
can also be downgraded if it contains a diagnostic/treat-
ment aspect that was obtained through a co-occurrence
relation (i.e., not part of the relation itself but co-
occurring in the sentence). Additionally, hedging or other
evidence of doubt will lower the score of the sentence by
one point. The triggers used here for doubt were might,
could, suggest, propose, and their lexical variations.
Evaluation: Overview and Experimental Setup
We evaluated miRiaD with respect to the needs of two po-
tential user communities: miR-disease database curators
and biomedical researchers studying disease mechanisms.
Study 1: Evaluation of miRiaD for assistance with manual
curation
Database curators can benefit from automated text min-
ing tools that make manual curation more efficient by
quickly identifying relevant information in the literature.
Because miR2Disease is the most comprehensive data-
base aiming to manually curate miR-disease relation-
ships, we focused our first study on the ability of
miRiaD to retrieve the types of information annotated
by miR2Disease, namely miR expression and target in-
formation in the context of disease. Initially, we tested
the recall of miRiaD using sentences from abstracts that
were cited as evidence in miR2Disease entries. By gath-
ering the evaluation set from the database itself, we are
guaranteed to have sentences that are of interest to
miR2Disease curators; however, because all of the sen-
tences are by definition positive, we cannot use them to
assess precision. Therefore, we also evaluated the recall
and precision miRiaD using a set of randomly selected
Medline abstracts not found in the miR2Disease database.
These abstracts were disease-focused and mentioned a
miR, but only a subset had target gene or miR expression
information, so we could test the ability of miRiaD to reject
papers that are not relevant for curation by miR2Disease.
We downloaded the entire mirRDisease database,
which contained 3273 entries at the time (January 2015).
Each entry in the downloaded file contained a miR, a
disease, a title of an article, and the year in which the
article was published. Because no PMIDs were provided
with the downloadable database, we looked up the
PMID corresponding to each entry by matching the title
and the year of publication in the Medline 2015 corpus.
We filtered out the entries for which the miR and the
disease mentions could not be found together in the ab-
stract or the title of the article. This resulted in a total of
486 entries in the miR2Disease database that we could
use for our study.
For these entries, we retrieved the title and the ab-
stract of each referenced article from the Medline 2015
corpus. We also obtained the descriptions accompanying
each miR-disease-article entry from the miR2Disease on-
line database. These descriptions consist of sentences
taken from the referenced article that support the miR-
gene relation in the entry. Because we have so far only
applied miRiaD to abstracts, we used Perls StringSimi-
larity Module, which is based on the algorithm described
in [33], followed by manual verification to detect entries
in which the description text was taken from the ab-
stract. Of these entries, we randomly chose 100 to test
the recall of the miRiaD system.
miR2Disease clearly mentions that their manual anno-
tations include miRNA expression patterns in the dis-
ease state, detection methods for miRNA expression,
and experimentally verified miRNA target gene(s).
Therefore, we manually reviewed each description, and
marked each sentence as containing expression and/or
target information. Ninety descriptions were marked as
containing expression information, and 58 descriptions
were marked as containing target information.
Next, we selected randomly selected 100 additional
Medline abstracts not found in the miR2Disease database.
The abstracts were selected to be focused on diseases (i.e.,
a disease is mentioned in the title, the first or the last sen-
tence of the abstract, or three or more times throughout
the abstracts, as defined in the work by Tudor et al. [28]).
One biologist annotator marked the abstracts in terms of
genes targeted by the miR and miR expression informa-
tion for the miR. There were 52 abstracts marked as con-
taining expression information and 48 abstracts marked
as containing target information.
Study 2: Evaluation for general extraction of miR-disease
associations
For our second study, we evaluated miRiaD with respect
to the extraction of a wide range of relations that appear
in text connecting a miR to a disease that may be of
interest to biomedical researchers. As above, we con-
ducted the study in two parts: first, we selected a set
which is highly likely to contain such relevant relation
between a miR entity and a disease or linking entity and
assessed recall and precision. Next we assessed precision
and recall of relations for randomly selected sentences
from Medline abstracts. For the first set, we gathered
Gene Reference into Function (GeneRIF) sentences from
the EntrezGene entries for miRs. GeneRIFs are used to
annotate EntrezGene entries and consist of short sen-
tences or phrases with literature citations describing the
function of the gene/miRNA [34]. These GeneRIF sen-
tences may or may not be direct quotes from the
Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 9 of 15
abstract and may be rephrased by the annotator. We
chose these sentences because of their rich variety of
miR-disease associations, which included relationships
between miRs and miR aspects (expression) and diseases
and disease aspects (outcome, biomarker, therapy), as
well as linking entities (target, process). Importantly, for
this study, we were not testing the ability of miRiaD to
detect appropriate sentences for EntrezGene annota-
tions; we were simply using the EntrezGene sentences as
a convenient source of the types of relations we designed
miRiaD to detect. The randomly chosen sentences men-
tioned both a miR and a disease but were both positive
and negative for miR-disease associations. The annota-
tors were asked to mark all the relations indicating a
miR-disease association, such we can assess miRiaDs
ability to detect such relations.
We downloaded the GeneRIF sentences from the Entrez-
Gene database (ftp://ftp.ncbi.nih.gov/gene/GeneRIF). The
file contained a total of 946,742 entries at the time of down-
load (January 2015). Each entry in the downloaded file con-
tains a taxonomy ID, a gene ID, a PMID list, the GeneRIF
text, and the timestamp of the last update. Using the gene
ID field, we extracted all the entries where the gene ID was
that of a miR. Additionally, we looked in the GeneRIF text
of these entries for the mention of at least one disease using
the PubTator database [20] for the detection of disease
mentions. This resulted in a set of 8476 GeneRIF entries.
A set of 100 entries from all 8476 GeneRIF entries was
randomly selected and presented to a second annotator.
The annotator was asked to mark the GeneRIF sen-
tences as relevant if they contained a relationship be-
tween a miR and a disease mentioned within or not
relevant otherwise. Of the 100 sentences, 97 were
marked as relevant and 3 were marked as not relevant.
In addition, the annotator marked all of the relations in
the sentence indicating an association between a miR
and a disease, including miR and disease aspects and
linking entities. Because multiple miRs, diseases, and
types of relationships could be found within the same
sentence, the annotations yielded a total of 175 relations.
We also randomly selected a second set of 100 sen-
tences containing miR and disease mentions from Med-
line abstracts. As discussed earlier, the reason for
selecting an additional 100 sentences was to construct a
less biased evaluation set that included some negative
sentences. Ninety-two sentences were marked as rele-
vant and contained a total of 159 relations; 8 sentences
were marked as not relevant.
Study 3: Evaluation of the miRiaD sentence informativeness
ranking
Finally, we evaluated our informativeness ranking approach
using a set of sentences containing miR and disease men-
tions that were manually scored for informativeness by a
biologist. We selected 100 random sentences from the 8476
GeneRIF sentences that we obtained in the previous evalu-
ation, plus 100 sentences randomly selected from Medline
abstracts. We asked an annotator to mark each sentence
with scores on a three-point Likert scale, depending on
how informative the sentence might be to a researcher in-
terested in the disease, with a score of 1 indicating some-
what informative, a score of 2 indicating informative,
and a score of 3 indicating highly informative. The anno-
tator marked 58 sentences as highly informative, 87 sen-
tences as informative, and 55 sentences as somewhat
informative.
Results and Discussion
Study 1: Evaluation of miRiaD for assistance with manual
curation
miRiaD was applied to the sentences in 100 abstracts cited
by miR2Disease and 100 randomly selected Medline ab-
stracts (200 abstracts total) as described in Methods. The
results of this evaluation are shown in Table 1. For
the abstracts from miR2Disease, we obtained a recall
of 90 % (81 true positives (TP) and 9 false negatives
(FN)) for expression information and 89.6 % (52 TP
and 6 FN) for target information. For the randomly
selected abstracts, we got very similar results: recall
of 92.3 % for expression information and 83.3 % for
target information. When we combined the two sets
of abstracts, we obtained recalls of 90.78 % and
88.46 % for expression and target information re-
spectively. Using the randomly selected abstracts, we
were able to assess precision as well as recall. For ex-
pression information we obtained precision of 92.3 %
and f-score of 92.3 % (48 TP, 4 FN and 4 false posi-
tives (FP)); for target information we obtained preci-
sion of 97.5 % and f-score of 89.8 % for target
information (40 TP, 8 FN and 1 FP).
Looking at the false negatives, we observed that most
of the errors were introduced by highly complex sen-
tences, for which the iSimp tool could not generate use-
ful simplified sentences. As a result, the syntactic and
Table 1 Evaluation of miRiaD for assistance with manual curation
TP FN TN FP Recall Precision F-score
miR2Disease based set
Expression 81 9 - - 90.0 - -
Gene target 52 6 - - 89.6 - -
Randomly chosen set
Expression 48 4 46 4 92.3 92.3 92.3
Gene target 40 8 52 1 83.3 97.5 89.8
Combined
Expression 129 13 - - 90.78 - -
Gene target 92 14 - - 88.46 - -
Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 10 of 15
lexical patterns could not be matched. An example is the
following sentence: Among 15 upregulated target genes
of the miR-30 miRNA, four genes known to be
expressed and/or functional in podocytes were identi-
fied, including receptor for advanced glycation end prod-
uct, vimentin, heat-shock protein 20, and immediate
early response 3 (PMID 18776119). The four genes were
not identified as targets of miR-30 due to the inability to
link them to the four genes mention, and subsequently
to the 15 upregulated target genes mention. Another
class of false negatives involves relations missed due to the
presence of anaphora (generally a pronoun referring to an
entity). For example in the sentence fragment miR-
181a, a small non-coding RNA believed to induce apop-
tosis by repressing its target gene, BCL-2, the word its
refers to miR-181a. We currently do not perform pro-
noun resolution and hence miRiaD misses the target rela-
tion with BCL-2. Looking at false positives, we observed
most of errors were introduced due incorrect identifica-
tion of the simplification constructs. In the sentence the
protein inhibitor of activated STAT3 (PIAS3) was con-
firmed as a direct miR-21 target, STAT3 was detected as
the parenthetical for PIAS3 and tagged as a target for
miR-21 in addition to PIAS3.
Study 2: Evaluation for general extraction of miR-disease
associations
For the second study, 100 GeneRIF sentences from
EntrezGene entries for miRNA and 100 sentences with
miR and disease mentions randomly chosen from Med-
line abstracts were processed by miRiaD (see Methods).
The results of this evaluation are shown in Table 2. miR-
iaD was able to identify relevant relations in 91 sen-
tences from the GeneRIF set and 93 sentences from the
Medline set. For calculation of TP, FN and FP we
matched the relations annotated by the annotators with
the relations extracted by miRiaD. This yielded recall of
84.0 %, precision of 94.8 %, and f-score of 89.1 % with
respect to GeneRIF sentences (147 TP, 28 FN and 8 FP)
and recall of 84.2 %, precision of 96.4 %, and f-score of
89.8 % with respect to the additional Medline sentences
(134 TP, 25 FN and 5 TP). As in the previous evaluation
we do not see a significant difference in performance
between the two evaluation sets. The combined f-score
for all 200 sentences was 89.4 %.
Looking at the false negatives in this evaluation re-
vealed an additional type of error. We observed certain
syntactic dependencies between events that should be
captured, but which are not. For example, the following
sentence contains a temporal relation, which is triggered
by the word after: These data indicate for the first
time a mechanism involving STAT1/2 upregulation
under the transcriptional control of INF-alpha signaling
after knockdown of miR-221/222 cluster in U251 glioma
cells (PMID 20428775). The relationship [miR-221/222,
negatively regulates, STAT1] could not be extracted
because of the systems inability to detect the temporal
relation. As in or previous evaluation, we observed false
negatives due to anaphora. For example, in the sentence
fragment the tumor suppressor activity of miR-124
could be partly due to its inhibitory effects on glioma
stem-like traits and invasiveness, miRiaD misses the
inhibitory relations between mir-124 and stem-like
traits and invasiveness.
Most of false positives errors in sentences had an in-
volvement relation between a miR and a disease in
addition to a more informative relation. For example in
the sentence this study further extends the biological
role of miR-92b in non-small cell lung cancer A549 cells
and for the first time identifies PTEN as a novel target
of miR-92b, miRiaD extracted an involvement relation
between miR-92b and non-small cell lung cancer that
was not marked as relevant by the annotator. Instead,
the annotator only marked the relation between mir-
92b and the target gene PTEN as relevant. This type
of errors will constitute the grounds for future work.
Study 3: Evaluation of the miRiaD sentence
informativeness ranking
This evaluation was conducted on 100 sentences ran-
domly selected from GeneRIF set used for the previous
evaluation 100 sentences randomly selected from Medline
abstracts, which were marked for their informativeness on
a three-point scale by an annotator (see Methods). miRiaD
ranked 57 sentences as highly informative, 97 sentences
as informative, and 48 sentences as somewhat inform-
ative. The comparisons between the annotators scores
and the miRiaD scores are shown in Table 3. Of the sen-
tences ranked as highly informative by the system, most
of them (45/57) were also ranked as highly informative
by the annotator; the remaining 12 sentences were ranked
as informative by the annotator. Looking at the diagonal
of the table, we observe that the majority of the scores in
all three categories were in agreement. The average score
difference on the 3-point scale (absolute value) between
annotator score and the miRiaD system score is 0.29.
Various correlation measures were computed, where a
value of 1 is total positive correlation, 0 is no correlation,
and ?1 is total negative correlation. First, we computed
Table 2 Evaluation for general extraction of miR-disease
associations
TP FN TN FP Recall Precision F-score
GeneRIF based Set 147 28 1 8 84.0 94.8 89.1
Randomly chosen set 134 25 7 5 84.2 96.4 89.8
Combined 281 53 8 13 84.1 95.5 89.4
Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 11 of 15
the Pearson product-moment correlation coefficient,
which is widely used as a measure of the degree of linear
dependence between two variables (0.727). Second, we
computed the Kendall tau rank correlation coefficient,
which measures the association between two measured
quantities by looking at the similarity of the orderings of
the data when ranked by each of the quantities (0.678). Fi-
nally, we computed the Spearmans rank correlation coef-
ficient, which assesses how well the relationship between
two variables can be described using a monotonic func-
tion, and got a value of 0.724, which indicates a very
strong positive relationship [35]. The p-values for these
correlation scores are very low, indicating that the null hy-
pothesis of no correlation is extremely unlikely. To test
against a stronger baseline than no correlation, we ran-
domly reordered the annotated scores 10,000 times and
calculated the Pearson correlation coefficient. The mean
correlation was 0.544, and none of the random correlation
values were above the reported correlation value 0.727.
We also computed the normalized discounted cumula-
tive gain (nDCG) [36, 37], which compares the ordering
of a list by a system (e.g., miRiaD) with the perfect or-
dering of that list by gold standard (e.g., our annotator).
Because the sentences were given scores between 1 and
3, and not actual ranks, multiple possible orderings are
possible when displaying the sentences marked with 3
first, then the sentences marked with 2 second, and
finally the sentences marked with 1 at the bottom. For
this reason, we considered 10,000 different possible or-
derings. The mean of the nDCGs scores obtained this
way was 0.977, with the lowest being 0.950 and the me-
dian being 0.978. The high nDCG values could be due
to the strong agreement between miRiaD and the anno-
tator for the sentences marked as highly informative.
Browsing the results online
We have developed a preliminary website for interactive
query of miRiaD miR-disease association extraction. The
interface accepts PubMed-like queries as input, thus
supporting queries like a miR name, or a disease name,
or any biological concept. For example, a user interested
in miR-9 and breast cancer can submit a query such
as mir-9 AND breast cancer. To ensure that all of the
abstracts passed to miRiaD contain a miR mention, we re-
strict the user query by appending the AND operator
and miRNA keywords connected by the OR operator,
i.e., query AND (miRNA[TIAB] OR microRNA[TIAB] OR
miR[TIAB]). The system then submits the query to
PubMed, which returns all the PMIDs satisfying the query.
miRiaD processes this list of PMIDs and displays the trip-
lets < miR,Disease,Text Evidence/PMID>. The interface is
available at the URL: http://biotm.cis.udel.edu/miRiaD.
Figure 3ad provides screenshots of the interface.
Figure 3a. shows the search form where the user sub-
mits his/her query. Figure 3b. shows the triplet view in
the table after submitting the query mir-9 AND
breast cancer. There are three columns in the triplet
view, namely: the miR, Disease and Text Evidence.
Note that the results also contains triplets for other
miR (mir-151, miR-200) and other diseases (hepatocel-
lular carcinoma, Hodgkins lymphoma etc.). This is due
to the fact that miRiaD processses the entire abstract
for each PMID returned by the submitted query and
some abstracts may contain mentions of multiple miR
and/or diseases. Results can be filtered to include only
those where mir-9 is the miR and breast cancer is the dis-
ease using the drop-down menus above each column as
shown in Fig. 3c. Finally, clicking on the PMID in the Text
Evidence column takes the user to the abstract (Fig. 3d.),
where sentences indicating the miR-disease association are
underlined and the respective miRs and diseases are
highlighted in bold.
Browsing the highlighted sentences in the 13 PMIDs
shown in Fig. 3c reveals that miR-9 has been associated
with a number of breast cancer phenotypes including me-
tastasis, invasiveness, aggressiveness, cell motility, and
poor prognosis. miR-9 targeting of E-cadherin (CDH1)
has been implicated in promotion of cell motility and in-
vasiveness. Variations in miR-9 expression related to miR-
9 promoter hypermethylation have also been observed in
the disease. As a consequence of these findings, miR-9 has
been suggested as both a potential biomarker and thera-
peutic target. Interestingly, one study (PMID: 22761433)
reported that miR-9 targeting of the mitochondrial en-
zyme, MTHFD2, mediated a tumor suppressive effect in
breast cancer, and in contrast to other studies, found that
miR-9 inhibited invasion. More careful consideration of
the contextual information in these articles may help to
resolve this apparent contradiction. This small example il-
lustrates the wide variety of miRNA-disease information
that can be easily obtained using miRiaD.
Conclusion
In this paper, we have presented miRiaD, a relation ex-
traction system that automatically extracts associations
between miRs and diseases from the literature. The
Table 3 Evaluation of the miRiaD sentence informativeness
ranking
Annotated Highly
informative
Informative Somewhat
informative
Total
System
Highly informative 45 12 0 57
Informative 11 64 20 97
Somewhat informative 2 11 35 48
Total 58 87 55 200
Row values correspond to the frequency of scores assigned by miRiaD, while
the columns denote the annotator score frequency
Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 12 of 15
miR-disease associations are sometimes indirect, with
miR and disease connected by a linking entity, such as a
target gene or cellular process. Similarly, miRs and dis-
eases might be mentioned in terms of their aspects:
(e.g., state or expression level for miRs or outcomes,
diagnostics/treatments, or therapy information for dis-
eases). Our method, which entails detecting syntactic
dependencies and, subsequently, semantic relations be-
tween miRs (or their aspects) and linking entities in the
context of a disease or disease aspect, has demonstrated
high performance (recall, precision, and F-scores) in
identifying information of interest to both database cu-
rators and biomedical researchers studying the connec-
tions between miRs and disease. Additionally, we have
shown how assigning scores to sentences containing a
miR-disease association, based on syntactic dependencies,
semantic relations, and linking entities, highly correlates
RESEARCH Open Access
Using Semantic Web technologies for the
generation of domain-specific templates to
support clinical study metadata standards
Guoqian Jiang1*, Julie Evans2, Cory M. Endle1, Harold R. Solbrig1 and Christopher G. Chute3
Abstract
Background: The Biomedical Research Integrated Domain Group (BRIDG) model is a formal domain analysis model
for protocol-driven biomedical research, and serves as a semantic foundation for application and message development
in the standards developing organizations (SDOs). The increasing sophistication and complexity of the BRIDG model
requires new approaches to the management and utilization of the underlying semantics to harmonize domain-specific
standards. The objective of this study is to develop and evaluate a Semantic Web-based approach that integrates the
BRIDG model with ISO 21090 data types to generate domain-specific templates to support clinical study metadata
standards development.
Methods: We developed a template generation and visualization system based on an open source Resource
Description Framework (RDF) store backend, a SmartGWT-based web user interface, and a mind map based tool for
the visualization of generated domain-specific templates. We also developed a RESTful Web Service informed by the
Clinical Information Modeling Initiative (CIMI) reference model for access to the generated domain-specific templates.
Results: A preliminary usability study is performed and all reviewers (n = 3) had very positive responses for the
evaluation questions in terms of the usability and the capability of meeting the system requirements (with the
average score of 4.6).
Conclusions: Semantic Web technologies provide a scalable infrastructure and have great potential to enable
computable semantic interoperability of models in the intersection of health care and clinical research.
Keywords: BRIDG, RDF, CIMI, Doman analysis model, Clinical study meta-data standards, Detailed clinical model,
Semantic Web technologies
Introduction
The Biomedical Research Integrated Domain Group
(BRIDG) model is a formal domain analysis model for
protocol-driven biomedical research, and serves as the
semantic foundation for application and message devel-
opment in the standards developing organizations
(SDOs) [1, 2]. The increasing sophistication and com-
plexity of the BRIDG model requires new approaches to
the management and utilization of the underlying se-
mantics to harmonize domain-specific standards.
A typical use case for the BRIDG model comes from
the Clinical Data Interchange Standards Consortium
(CDISC) [3]. CDISC initiated the Shared Health And
Clinical Research Electronic Library (SHARE) project to
build a global, accessible electronic library, which en-
ables standardized data element definitions and richer
metadata to improve biomedical research and its link
with healthcare [4]. In it, CDISC envisioned integrated
domain-specific templates built from the classes and at-
tributes from the BRIDG model and ISO 21090 data
types as a foundation for the definition of research con-
cepts in the therapeutic target areas.
The CDISC SHARE approach to domain-specific tem-
plates has much in common with an international col-
laboration effort initiated by the Clinical Information
* Correspondence: jiang.guoqian@mayo.edu
1Department of Health Sciences Research, Mayo Clinic, 200 First St SW,
Rochester, MN 55905, USA
Full list of author information is available at the end of the article
© 2016 Jiang et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Jiang et al. Journal of Biomedical Semantics  (2016) 7:10 
DOI 10.1186/s13326-016-0053-5
Modeling Initiative (CIMI) [5], an international collab-
oration that is dedicated to providing a common format
for detailed specifications for the representation of
health information content so that semantically inter-
operable information may be created and shared in
health records, messages and documents [6]. While the
domain-specific templates defined in CDISC SHARE are
focused on clinical research and CIMI is more focused on
electronic health records (EHR) and secondary use of
EHR data, we see the semantic interoperability of the two
models as critical for predictable exchange of meaning be-
tween two or more systems in the area of health care and
clinical research. We also believe that the emerging
Semantic Web technologies based on World Wide Web
Consortium (W3C) standards can provide much of the in-
frastructure and tools needed to accomplish this goal.
The W3C standards include the Resource Description
Framework (RDF) and the Web Ontology Language
(OWL) [7, 8], which provide a scalable framework for
semantic data integration, harmonization and sharing.
These technologies are beginning to appear in both clin-
ical research and health care workspaces and have been
leveraged in several notable projects, including the UK
CancerGrid [9], the US caBIG [10] and the National
Center of Biomedical Ontologies (NCBO) [11]. The Se-
mantic Web Health Care and Life Sciences (HCLS) Inter-
est Group has been formed under the auspices of the
W3C to develop, advocate for and support the use of the
Semantic Web technologies across the domains of health
care, life sciences, clinical research and translational medi-
cine [12]. In some of our previous studies, we explored
the use of OWL to represent clinical study metadata
models such as HL7 Detailed Clinical Models (DCMs)
[13] and the ISO/IEC 11179 model [14], and investigated
a Semantic Web representation of the Clinical Element
Model (CEM) for secondary use of the EHR data [15, 16].
The objective of this study is to develop and evaluate a
Semantic Web-based approach that integrates the
BRIDG model with ISO 21090 data types to generate
domain-specific templates to support clinical study
metadata standards development. The main purpose of
the tools developed in this study is to support SDOs
such as CDISC to create information models that can
enable data exchange between clinical care systems
(e.g., in a CIMI model) and clinical trial systems (e.g.,
in a BRIDG model). In it we developed a template gen-
eration and visualization system based on an open
source Resource Description Framework (RDF) store
backend, a SmartGWT-based web user interface, and a
mind map based tool for the visualization of gener-
ated domain-specific templates. We also created a
RESTful Web Service informed by the Clinical Informa-
tion Modeling Initiative (CIMI) reference model for ac-
cess to the generated domain-specific templates. A
preliminary usability study is performed to evaluate the
system in terms of the ease of use and the capability for
meeting the requirements using a selected use case.
Background
BRIDG model
In 2004, CDISC initiated the Biomedical Research Inte-
grated Domain Group (BRIDG) in collaboration with
HL7 and National Cancer Institute (NCI). The collabor-
ation effort developed a domain analysis model that is a
shared view of the dynamic and static semantics for the
domain of protocol-driven research and its associated
regulatory artifacts [1]. The BRIDG model was based
on the HL7 Development Framework. Multiple repre-
sentations of the model were introduced in the BRIDG
3.0 release, including the canonical Unified Modeling
Language (UML)based representation, a HL7 Refer-
ence Information Model (RIM)-based representation
and a ontological representation in a single OWL file.
Figure 1 shows BRIDG multiple-perspective representa-
tions in UML, HL7 RIM and OWL.
CDISC standards development
The mission of CDISC is to develop and support global,
platform-independent data standards that enable infor-
mation system interoperability to improve medical re-
search and related areas of healthcare [17]. Over the
past decade, CDISC has fulfilled its mission by publish-
ing and supporting a suite of standards that enable the
electronic interchange of data throughout the lifecycle of
a clinical research study [18].
Specifically, CDISC has developed standards for use
across the various points in the research study lifecycle:
 Planning: Protocol Representation Model Version 1,
which includes Study Design, Eligibility Criteria and
Clinical Trial Registration
 Data Collection:
o Clinical Data Acquisition Standards
Harmonization (CDASH) for the collection of
data through case report forms
o Operational Data Model (ODM) for the
collection of operational data through electronic
data exchange
o Laboratory Model (LAB) for the collection of
clinical laboratory data through electronic data
exchange
 Data Tabulations
o Study Data Tabulation Model (SDTM) for
submission of human subject data to regulatory
agencies
o Standard for the Exchange of Nonclinical Data
(SEND) for submission of non-human subject data
to regulatory agencies
Jiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 2 of 10
 Statistical Analysis: Analysis Data Model (ADaM)
for submission of statistical analysis data to
regulatory agencies.
Clinical information modeling initiative
The Clinical Information Modeling Initiatives (CIMI)
was officially launched in July, 2011 with more than 23
participating organizations. The initiative was established
to improve the interoperability of healthcare informa-
tion systems through shared implementable clinical in-
formation models [5]. The principles of the CIMI
include 1) CIMI specifications will be freely available to
all. 2) CIMI is committed to making these specifications
available in a number of formats. 3) CIMI is committed
to transparency in its work and product. The goals of
the CIMI include: 1) shared repository of detailed clin-
ical information models; 2) a single formalism; 3) a com-
mon set of base data types; 4) formal bindings of the
models to standard coded terminologies; and 5) reposi-
tory is open and models are free for use at no cost. As of
May 7, 2013, CIMI is finalizing its reference model spe-
cification that consists of a core reference model, a data
value type model and a party model.
Semantic Web technologies
The World Wide Web Consortium (W3C) is the main
international standards organization for the World Wide
Web [7]. Its goal is to develop interoperable technolo-
gies and tools as well as specifications and guidelines to
realize the full potential of the Web. The W3C tools
and specifications that we used in this study include
the Resource Description Framework (RDF) [8], RDF
Schema (RDFS) [19], the Web Ontology Language
(OWL), OWL 2 [20], the Simple Knowledge Organization
System (SKOS) [17], the SPARQL Protocol and RDF
Query Language (SPARQL) [21], and the SPARQL Infer-
ence Notation (SPIN) [22], which is a W3C Member
Submission that can be used to represent SPARQL
rules and constraints on Semantic Web models.
Methods
System requirements
The system requirements for this study were based on a
CDISC SHARE project, in which building domain-
specific templates based on BRIDG model is an essential
process for clinical study metadata standards develop-
ment. These requirements include:
 Selection from multiple BRIDG classes. For
example, describing a measurement on a subject
(such as vital signs like body temperatures) may
include the BRIDG classes Defined Observation,
Defined Observation Result, Performed Observation,
Performed Observation Result and Reference Result.
 Selection of specific attributes from each selected
BRIDG class. The attributes include the inherited
attributes from its parent classes. For example when
selecting attributes based on a BRIDG class Person,
the inherited attributes (e.g., name, birthDate, etc.)
from its parent class Biologic Entity shall be available
for the selection.
 Specification of the subcomponents of the data type
for a specific attribute of a BRIDG class. BRIDG
attributes are associated with ISO 21090 data types,
each of which has multiple components with its
own data type, which may also be a complex. Using
the BRIDG class Person as an example, the attribute
educationLevelCode has the data type CD. CD, in
turn has a set of components including code,
displayName, codeSystem, codeSystemName,
Fig. 1 BRIDG multiple-perspective representations in UML, HL7 RIM and OWL
Jiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 3 of 10
codeSystemVersion, valueSet, etc. Each of which
components has their own data type.
 Selection of attributes from the BRIDG classes that
link to a selected BRIDG class through potential
association relationships. For example, through the
association be reported by, the class Performed
Observation links to a set of BRIDG classes
including Subject, Healthcare Provider, Laboratory,
Device, etc. The attributes from associated classes
are available for building a domain-specific template.
 Provide a standard representation of generated
templates, which is scalable for supporting
downstream development and harmonization of
clinical study metadata standards.
System architecture
Figure 2 shows the system architecture. The system
comprises the following modules: 1) a normalization
pipeline module; 2) a backend module that uses a RDF
store; 3) a frontend module that includes a BRIDG
model browser, a template generation mechanism and a
mind map viewer for generated templates.
Implementation
Materials
BRIDG model in OWL In the release of the BRIDG
version 3.2, an ontological perspective, i.e., OWL repre-
sentation of BRIDG semantics is developed for the
BRIDG model. For this release, the scope of the OWL
contents is limited to the information found in the
BRIDG UML model. In this study, we used the OWL
rendering of the BRIDG model that is publicly available
from the release package of the BRIDG 3.2 [1].
HL7 V3 data types in OWL The HL7 OWL project has
published an initial draft of the Core HL7 V3 in OWL.
The publicly available draft was released on January
2013 and can be downloaded from the HL7 OWL pro-
ject web site [23]. In this study, we use the HL7 OWL
rendering of HL7 V3 data types in place of the ISO
21090 equivalents.
Backend implementation
We started with the 4store, an open source RDF store
developed at Garlik [24]. We then loaded the RDF image
BRIDG model and HL7 V3 data types in OWL into two
separate graphs. We also established a SPARQL end-
point that provides standard query services against the
RDF store backend.
To make all of the inherited attributes and associations
explicit for each BRIDG class, we used Jena ARQ API-
based script [25] that recursively retrieved the attributes
and associations from parent classes of each BRIDG
class and materialized them explicitly using two BRIDG
predicates: bridg:attributeProperty and bridg:association-
Property. We also used a template, spl:Attribute, from
the SPARQL Inference Notation (SPIN) to model the
metadata of each attribute and association, including the
cardinality and a predicate bridg:isInherited indicating
whether the target attribute or association is inherited or
not. Figure 3 shows an example of the flattened repre-
sentation for an association and an attribute of the
BRIDG class Person. Following this, we combined the
namespaces used for the HL7 V3 data types and the
OWL renderings of the BRIDG models.
Frontend implementation
Building a BRIDG model browser and a template
generation mechanism We developed a BRIDG model
browser as a web application based on the SmartGWT
API [26]. SmartGWT is a Google Web Toolkit (GWT)-
based framework that allows users to utilize its compre-
hensive widget library for user interface development.
The browser displays a hierarchical tree of BRIDG classes
(see Fig. 4). For each class, the browser displays a
metadata structure comprising Children, Attributes and
Associations, which streamlined those metadata associ-
ated with each class. We defined a set of SPARQL queries
to retrieve the children, attributes and associations for
each class. Figure 5 shows a SPARQL query to retrieve all
attributes associated with the BRIDG class Person.
If a BRIDG class has children, they will be displayed
under the folder Children. The Attributes folder displays
all inherited and non-inherited attributes and their data
types. Separate icons are used to differentiate which at-
tributes are local vs. inherited. The sub-components are
displayed for complex data types. As an example, the
upper right corner of Fig. 4 shows the sub-components
of the data type CD for the attribute maritalStatusCode.
Fig. 2 A diagram illustrating the system architecture
Jiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 4 of 10
Fig. 3 An example of flattened representation for an association and an attribute of the BRIDG class Person using a SPIN template
Fig. 4 A customized BRIDG model browser with a metadata structure for each class. In the left hand panel, a hierarchical tree of BRIDG classes is
displayed. In the right upper part, it displays nested sub-components and their selection for the data type (i.e., CD) of an attribute Person.maritalStatusCode.
In the right lower part, it displays the associations of the class Person
Jiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 5 of 10
Data type sub-components can be expanded to display
interior data types.
The Associations folder shows inherited and non-
inherited associations with icons representing their in-
heritance status. The associated class will be displayed
and it can be expanded to show its corresponding struc-
ture. The lower right hand of Fig. 4 shows the expansion
of the Associations folder for the class Person.
We also developed a template generation mechanism
by allowing selection of specific elements in the BRIDG
model browser. A target template can be constructed
from the attributes (including data type components)
from one or more BRIDG classes. Based on the system
requirements, a set of rules is applied when users make
their selections. The upper right hand part of Fig. 4
shows the user selecting the data type ST data type of
the CD.displayName component with the full path of
the selected attribute used as the attribute name:
Person.maritalStatusCode.CD.displayName.ST.
A generated template with a set of selected attributes
(including data type components) can be rendered as a
mind map. We use the Freemind browser [27] to dis-
play a target mind map.
A CIMI reference model-based Semantic Web repre-
sentation of generated domain templates We created
a mapping between CDISC standard objects and CIMI
reference model elements. In it a domain-specific tem-
plate corresponds to the CIMI element ENTRY (the lo-
gical root of a single clinical statement within a clinical
session) and the component BRIDG classes and BRIDG
attributes correspond to the CIMI element CLUSTER (a
set of ELEMENTs) and ELEMENT (a type of data ITEM,
which does not itself contain ITEMs) respectively. Using
this mapping, we were able to create a CIMI-complaint
Semantic Web representation for generated BRIDG
domain-specific templates. Figure 6 shows an example
of a CIMI-compliant Semantic Web representation for a
domain-specific template generated from the BRIDG
class AdverseEventSeriousness. As illustrated, we used
the elements from the CIMI reference model, such as
cimi:ENTRY, cimi:CLUSTER, cimi:ELEMENT, and cimi:-
CLUSTER.item. We also used the SPIN template spl:at-
tribute to attach the metadata of each selected attribute
including the cardinality.
We then developed the RESTful Web Service that
provides programmatic and browser access to the CIMI
reference model-based representations of the domain-
specific templates. As an example, the CIMI reference
model-based representation for the AdverseEventSer-
iousness domain in Turtle format is shown in Fig. 6.
Results and discussion
System evaluation
We performed a preliminary evaluation on the system in
terms of the usability and the capability of meeting the
system requirements as described in the Section 3. For
the evaluation design, we created a use case test script
that describes the use case of generating a template
Measurement on a Subject. The target of the use case
is to develop a template that covers 5 BRIDG classes, 20
BRIDG attributes and 5 BRIDG associations. We re-
cruited three reviewers: one reviewer (JE, a co-author)
from CDISC SHARE team who has extensive expertise
on BRIDG model and clinical study metadata standard
development, and two other reviewers who are biomed-
ical informatics researchers. We arranged a teleconfer-
ence meeting and introduced the background of the
project and demonstrated the basic features and usages
of our frontend widgets to them. We made the web ap-
plication accessible to the three reviewers who followed
the test script to build a template for the target use case.
Each reviewer worked individually to complete the test
case. After they completed, the three reviewers are asked
to answer the evaluation questions in a 1-5 scale, in
which 1 stands for Strongly disagree, 2 for disagree, 3
Fig. 5 A SPARQL query to retrieve all attributes associated with the BRIDG class Person
Jiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 6 of 10
for neutral, 4 for agree and 5 for Strongly agree.
The preliminary results indicated that all three reviewers
successfully created the template as described in the test
script. All reviewers had very positive responses for the
evaluation questions in terms of the usability and the
capability of meeting the system requirements (with the
average score of 4.6). The reviewers also provided free-
text feedback on the system. Some of comments include
1) the suggestion to add a search button for users who
look for a particular class and attribute; 2) the suggestion
that the icon used for the folder Children could be mis-
leading and confusing; 3) the issues for displaying
Freemind map in different browsers; 4) the suggestion of
allowing multiple ways to de-select an attribute; 5) the
suggestion of allowing to reload the generated template
for modification; 6) the suggestion of allowing to con-
strain the data type of ANY in a specific data type.
Discussion
In this study, we designed, developed and evaluated a
BRIDG-based domain-specific template generation and
visualization system for supporting clinical study meta-
data standards development. We consider that the sys-
tem and approach developed in this study are significant
Fig. 6 A CIMI-compliant Semantic Web representation in the Turtle format for a domain-specific template generated from the
class AdverseEventSeriousness
Jiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 7 of 10
in both domain specific perspective and technical
perspective.
Domain specific significance
The system requirements were derived directly from a
real-world CDISC SHARE project [4], which demon-
strated that a scalable mechanism for access and modu-
lar use of the BRIDG model elements is essential for
supporting metadata standards development. With the
increasing complexity of the BRIDG model, the BRIDG
development team has made efforts to deal with the
scalability issue. One example is the six subdomain
views, Adverse Event, Common, Protocol Representa-
tion, Regulatory, Statistical Analysis, and Study Conduct,
which help domain experts to navigate subsets of the
domain semantics. In addition, multiple representations
as described in the Background section are used to meet
the requirements from different use cases. In this study,
we focused on the domain-specific template generation
use case and developed a customized BRIDG browser
that enables the standards developer to interact with the
BRIDG model elements. Specifically, we streamlined the
metadata for each BRIDG class using a metadata struc-
ture of Children, Attributes and Associations. The pre-
liminary evaluation demonstrated the positive results in
terms of the ease of use and the capability to meet the
system requirements. In addition, the generated domain-
specific templates can be rendered in a Mind Map view,
which has been widely used in the standards develop-
ment community. Furthermore, we developed a Seman-
tic Web representation informed by CIMI reference
model for the generated domain-specific templates, pro-
viding a modular representation for a specific domain
exposed as a standard RESTful service. This will enable
semantic harmonization with other CIMI-compliant
models, potentially developed from different contexts.
Technical significance
Semantic Web technologies played a critical role in the
system design and development. First, the RDF data
model and the triple store technology enabled data inte-
gration of the BRIDG model and ISO 21090 data type
model. All BRIDG attributes have defined data types
based on ISO 21090. For those complex data types, they
have multiple components. Some of the components of
a complex data type are required for a domain-specific
template. For example, the CD data type has the compo-
nents valueSet and valueSetVersion that can be used for
the valueset binding. Utilizing the Semantic Web OWL/
RDF version of the two models, we were able to seam-
lessly link the data type defined for each BRIDG attri-
bute with their components defined in the ISO 21090
data type model. Note that we unified the namespaces
used for the data types in the two models for the inte-
gration purpose.
Second, the subsumption property, rdfs:subClassOf,
asserted in the OWL/RDF version of the BRIDG model
provides an elegant way to compute and retrieve the
inherited attributes and associations from parent classes
for a BRIDG class. The BRIDG model is authored in the
UML, in which a child class should inherit all asserted
attributes/associations from their parent classes, just as
in object-oriented model. Being able to browse and se-
lect the inherited attributes/associations is one of key
system requirements for domain-specific template gen-
eration. As part of the normalization pipeline, we re-
trieved and materialized all inherited attributes/
associations for each BRIDG class, which streamlined
the metadata of each BRIDG class and made the attri-
bute selection straightforward to users.
Third, a SPARQL endpoint was established to provide
standard SPARQL query services for accessing the con-
tent of the BRIDG model elements. We defined a set of
SPARQL queries to extract the metadata for each
BRIDG class. We found that the normalization pipeline
as we implemented it was very helpful to simplify the
query building. For example, as we materialized the
inherited attributes and associations for each BRIDG
class, building the SPARQL queries for retrieving this
kind of metadata was simplified. In addition, the
SPARQL endpoint based on 4store implementation sup-
ports SPARQL 1.1 update features, which enables the
storage and update of generated domain-specific tem-
plates with their provenance information and provides
potential for future authoring application development.
Fourth, a CIMI-compliant Semantic Web representa-
tion was developed for representing the generated
domain-specific templates and the elements from the
CIMI reference model were used. As we mentioned
above, the CIMI is finalizing its reference model. A Se-
mantic Web representation of the CIMI reference model
and its compliant clinical information models is one of
key tasks envisioned by the CIMI community. We con-
sider that our current efforts in this study would provide
useful experiences and test cases for the CIMI commu-
nity. In addition, we used a SPIN template to represent
the metadata of an attribute in a domain-specific tem-
plate. The SPIN framework is designed to represent the
SPARQL rules and constraints in Semantic Web models.
SPARQL rules are a collection of RDF vocabulary that
builds on the W3C SPARQL standard to let us define
new functions, stored procedures, constraint checking,
and inference rules for Semantic Web models. The rules
are all stored using object-oriented conventions and the
RDF and SPARQL standards. We expect that the SPIN
framework will provide a natural way to represent the
constraints and rules in a CIMI-compliant model and
Jiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 8 of 10
enable an automatic mechanism for model validation
and consistency checking.
Limitations and future study
There are several limitations in the study. First, a more
rigorous evaluation from a panel of domain experts from
broader communities would be helpful in the future.
The system will be iteratively enhanced based on the
feedback from the evaluators. For example, the search
functionality would be helpful to allow users to find a
target class/attribute more quickly. Second, the system
evaluation was limited to the ease of use and the fulfill-
ment of those basic requirements. We have not evalu-
ated the system in terms of the CIMI conformance for
generated domain-specific templates. We are actively
working with the CDISC SHARE and CIMI communi-
ties to review the current prototype representation. One
of main tasks is to develop the mappings between the
ISO 21090 data types used in the BRIDG model and the
data type defined in the CIMI reference model.
Conclusions
In summary, we developed and evaluated a Semantic
Web based approach that integrates the model ele-
ments from both BRIDG model and ISO 21090 model
and enables a domain-specific template generation mech-
anism for supporting clinical study metadata standards de-
velopment. The source code of the application are
available from the project GitHub website at https://
github.com/caCDE-QA/bridgmodel. We demonstrated
that Semantic Web technologies provide a scalable infra-
structure and have great potential to enable computable
semantic interoperability of models in the intersection of
health care and clinical research.
Availability of supporting data
The data set(s) supporting the results of this article
is(are) included within the article (and its additional
file(s)).
Abbreviations
BRIDG: The Biomedical Research Integrated Domain Group; SDO: Standards
developing organizations; RDF: Resource Description Framework; CIMI: The
Clinical Information Modeling Initiative; CDISC: The Clinical Data Interchange
Standards Consortium; SHARE: The Shared Health And Clinical Research
Electronic Library; EHR: Electronic health records; W3C: World Wide Web
Consortium; OWL: The Web Ontology Language; NCBO: National Center for
Biomedical Ontology; HCLS: The Semantic Web Health Care and Life
Sciences; DCMs: Detailed Clinical Models; CEM: Clinical Element Model;
NCI: National Cancer Institute; UML: Unified Modeling Language; RIM: Reference
Information Model; CDASH: Clinical Data Acquisition Standards Harmonization;
ODM: Operational Data Model; LAB: Laboratory Model; SDTM: Study Data
Tabulation Model; SEND: Standard for the Exchange of Nonclinical Data;
ADaM: Analysis Data Model; SKOS: The Simple Knowledge Organization System;
SPIN: The SPARQL Inference Notation; GWT: Google Web Toolkit.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
Conceived and designed the study: GJ, HRS. Developed the system: CME, GJ.
Designed and conducted the system evaluation: JE, GJ. Wrote the paper: GJ,
HRS, JE, CGC. All authors read and approved the final manuscript.
Acknowledgments
The authors thank Dr. Chunhua Weng from Columbia University and Dr. Cui
Tao from Mayo Clinic who participated in the evaluation. The authors also
thank the technical support from Mr. Craig Stancl from Mayo Clinic. The
study is supported in part by the SHARP Area 4: Secondary Use of EHR Data
(90TR000201).
Author details
1Department of Health Sciences Research, Mayo Clinic, 200 First St SW,
Rochester, MN 55905, USA. 2Clinical Data Interchange Standards Consortium
(CDISC), Austin, TX, USA. 3Johns Hopkins University, Baltimore, MD, USA.
Received: 30 May 2014 Accepted: 2 December 2015
Scharm et al. Journal of Biomedical Semantics  (2016) 7:46 
DOI 10.1186/s13326-016-0080-2
RESEARCH Open Access
COMODI: an ontology to characterise
differences in versions of computational
models in biology
Martin Scharm1*, Dagmar Waltemath1, Pedro Mendes2,3 and Olaf Wolkenhauer1,4
Abstract
Background: Open model repositories provide ready-to-reuse computational models of biological systems. Models
within those repositories evolve over time, leading to different model versions. Taken together, the underlying
changes reflect a models provenance and thus can give valuable insights into the studied biology. Currently,
however, changes cannot be semantically interpreted. To improve this situation, we developed an ontology of terms
describing changes in models. The ontology can be used by scientists and within software to characterise model
updates at the level of single changes. When studying or reusing a model, these annotations help with determining
the relevance of a change in a given context.
Methods: Wemanually studied changes in selected models from BioModels and the Physiome Model Repository.
Using the BiVeS tool for difference detection, we then performed an automatic analysis of changes in all models
published in these repositories. The resulting set of concepts led us to define candidate terms for the ontology. In a
final step, we aggregated and classified these terms and built the first version of the ontology.
Results: We present COMODI, an ontology needed because COmputational MOdels DIffer. It empowers users and
software to describe changes in a model on the semantic level. COMODI also enables software to implement
user-specific filter options for the display of model changes. Finally, COMODI is a step towards predicting how a
change in a model influences the simulation results.
Conclusion: COMODI, coupled with our algorithm for difference detection, ensures the transparency of a models
evolution, and it enhances the traceability of updates and error corrections. COMODI is encoded in OWL. It is openly
available at http://comodi.sems.uni-rostock.de/.
Keywords: Ontology, Modelling, Difference detection, SBML, CellML, Version control
Background
Modelling plays an important role in the life sciences.
The multi-disciplinary approach requires scientists to
reuse other work. This task is greatly supported by com-
mon languages to describe model-based results [1]. Com-
putational models of biological systems are frequently
described in XML standard formats such as the Systems
Biology Markup Language (SBML, [2]) or CellML [3].
These formats have several advantages: Models can be
*Correspondence: martin.scharm@uni-rostock.de
Equal contributors
1Department of Systems Biology and Bioinformatics, University of Rostock,
Rostock, Germany
Full list of author information is available at the end of the article
simulated, analysed, and visualised using different soft-
ware tools; models encoded in standard formats may
outlive the tool used to create the model; model exchange
becomes feasible; and models can more easily be shared,
published, and reused. SBML and CellML focus on encod-
ing the biological network, the mathematics, and the
dynamics of the system. This information enables the
technical reuse of model code. However, sustainable
model reuse requires a basic understanding of (i) the
biological background, (ii) the modelled system, and (iii)
possible parametrisations under different conditions. For
this purpose, terms from ontologies and controlled vocab-
ularies can be linked to the model, adding a semantic
layer.
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Scharm et al. Journal of Biomedical Semantics  (2016) 7:46 Page 2 of 8
Bio-ontologies are formal representations for areas of
knowledge [4]. They clarify the intended semantics of
the data, which makes the data more accessible, sharable,
and interoperable [5]. An ontology is a tool to provide
meaning to data, the information of which can then be
subjected to algorithmic processing [6, 7]. For example,
the Gene Ontology [6] provides additional information on
the genomic level, the NCBI Taxonomy [8] provides infor-
mation about the nomenclature of species, and UniProt
[9] provides information about proteins. We believe that
a similar approach should be taken for the semantic
description of differences between versions of a model.
Using the semantic layer to describe changes in a model
allows for storing meaning together with possible implica-
tions of these changes. Changes can then be filtered and
analysed automatically.
Models evolve over time. Both before and after pub-
lication, regular changes lead to different versions of a
model [10]. For example, modellers test different hypothe-
ses, maintainers of databases initially curate the model,
and other scientists later on correct or extend it. Specif-
ically, semantic annotations are added, parameter values
are updated, errors are corrected, models are adopted to
changes in the underlying format, etc. On average, amodel
changes 4.87 times during the first five years after publish-
ing in an open repository1. It is important to track these
changes for a number of reasons. Changes in parametrisa-
tions or on the underlying network may lead to a situation
where the original results are not reproducible anymore.
Furthermore, all contributions to the model code should
be correctly attributed.With respect to simulation results,
change records can help to predict modifications in the
simulation outcome. Finally, a good communication of
model changes increases the trust of scientists wishing to
reuse a model for their own purposes.
In our previous work we developed BiVeS, an algorithm
to identify and communicate the changes between two
versions of a model [11]. BiVeS encodes changes in an
XML file and, thus, tools can visualise and post-process
identified changes. Small changes might be easy to grasp
without the aid of a principled annotation scheme. How-
ever, as the list of changes increases it becomes harder to
grasp their relevance. To address this problem, we present
an ontology to annotate the changes identified with the
BiVeS algorithm. The ontology is sufficiently generic that
it can be used to annotate the differences between com-
putational models, including those encoded in SBML and
CellML.
Results
We developed the COMODI ontology, because COm-
putational MOdels DIffer. COMODI provides terms that
describe changes in models. These terms can be linked
to single differences between two versions of a model,
such as typo corrections in a parameter name. Based
on the resulting set of annotations, differences can be
characterised and classified.
Design considerations and identification of terms
COMODI was developed based on a study of changes
in versions of SBML and CellML models. The models
were retrieved from the respective model repositories.
More specifically, we started our investigation by man-
ually analysing a predefined set of cell cycle models2
from BioModels [12]. We subsequently extended this set
with randomly chosen models from both, BioModels and
the Physiome Model Repository [13]. The single steps of
development are summarised in Fig. 1 and explained in
the following:
1. Using the BiVeS algorithm we identified the
differences between all subsequent versions of each
model and exported the deltas in XML-encoded files.
A delta is a set of operations on entities (nodes or
attributes, respectively) necessary to transform one
document into another [11].
2. Each found difference was manually translated into a
human-readable description and recorded in a wiki
software to share and discuss it with collaborators. In
total, we investigated more than 10000 differences.
3. Afterwards, we manually analysed the verbose
descriptions of changes to understand their effects
on the model and to derive hypotheses and
explanations for a change. For example, the change
of an entity name from Gulcose to Glucose renames
a species and can be considered as the Correction
of a Typo that effects an EntityName.
4. We then grouped the changes into several logical
clusters, according to the derived hypotheses and
explanations of a group of changes. These clusters
are based on our own experiences and on feedback
from domain experts. The knowledge we gained led
to candidate terms for the ontology. We used the
human-readable description as a basis for the term
definitions.
5. In a last step, we designed a first version of the
ontology from the obtained clusters. The ontology
was afterwards extended with concepts stemming
from standard formats (SBML and CellML
terminology, e.g. ParameterSetup) and from the
XML domain (e.g. EntityIdentifier). Thus,
COMODI contains a whole subtree that specifically
focuses on XML encodings.
We quickly identified technically driven properties of
changes. For example, it is easy to determine the type of
a change as BiVeS already distinguishes between inser-
tions, deletions, updates, and moves of entities in XML
Scharm et al. Journal of Biomedical Semantics  (2016) 7:46 Page 3 of 8
Fig. 1 Development process of COMODI. The development process involved five steps with several iterations. First, we used BiVeS to generate the
differences between all subsequent model versions. Second, we converted the formal description of more than 10000 differences into
human-readable descriptions. Third, we manually studied these descriptions and derived hypotheses and explanations for them. Fourth, we
grouped the human-readable descriptions into sets of concepts and derived candidate terms for the ontology. Fifth, we aggregated and classified
these terms and implemented the first version of the ontology in Protégé
documents. Moreover, it is always possible to specify the
XML entity that is subject to a change. It was, however,
more difficult to identify terms describing the reason,
intention, or target of a change. The absence of appro-
priate terms led us to derive new terms from our human
readable description of changes. The initial set of terms
was then shaped in discussions with other researchers.
Throughout the development of COMODI we sought
feedback from experts in the fields, e.g., through per-
sonal communication or poster presentations at meetings.
Finally, we implemented the derived ontology in the Web
Ontology Language (OWL) [14] using Protégé [15].
Ontology organisation and content
COMODI is organised into four branches around the
central concept Change: XmlEntity, Intention,
Reason, Target (cf. Fig. 2). As a running example, we
use the change of a parameter in an imaginary SBML
model. We assume that the parameter changed from 0.5
to 0.8 in the new version of the SBML model.
The subtree rooted by the Change class can be used to
specify the type of a change in more detail. Model enti-
ties may be inserted (Insertion), deleted (Deletion),
updated (Update), or moved (Move). In our example,
the modification of the parameter value from 0.5 to 0.8
corresponds to an update (Update) of an attribute value.
Many models are encoded in XML documents. In
these cases, a change is always applied to a certain
XmlEntity. We distinguish between an XmlNode, an
XmlAttribute, or an XmlText element. The update
of the parameter value in our example is applied to an
XmlAttribute.
Intention and Reason both indicate the purpose
of a change. On the one hand, the Intention spec-
ifies the aim of a change, particularly with respect
to consequences in the future. In our example, the
intention of modifying the parameter value is a
Correction. On the other hand, a Reason specifically
focuses on the cause of a change. In our example, a
MismatchWithPublication caused an update of the
parameter value.
Most prominent is the Target branch. It contains
terms to specify possible targets of a change. COMODI
basically distinguishes between five layers in a model
document, that can be subject to a change:
1. The ModelEncoding corresponds to the formal
encoding of the model document. Terms of this
branch can, for example, be used to describe an
update of the underlying SBML specification.
2. The ModelAnnotation branch corresponds to
the semantic layer of a model document. Terms of
this branch can, for example, be used to capture
changes in the annotations.
3. The ModelDefinition refers to the actual
biological system, for example a reaction network.
Terms of this branch can, for example, be used to
specify the parts of a model that are affected by a
change.
4. The ModelSetup branch can be used to describe
changes in the simulation environment. Terms of
Scharm et al. Journal of Biomedical Semantics  (2016) 7:46 Page 4 of 8
Fig. 2 Structure of the COMODI ontology. Differences between computational models can be annotated with the Change term. Using the
properties appliesTo, hasIntention, hasReason, and affects, the differences can be linked to the terms of the four major branches of
COMODI: XmlEntity, Intention, Reason and Target. All arrows between terms within these five branches indicate an is-a relation, unless
labelled otherwise
this branch can, for example, be used to describe
changes in parameter values.
5. The ModelBehaviour links to the TEDDY
ontology [16]. Thus, it is possible to capture changes
in the dynamics of the system. Such changes may, for
example, affect the stability characteristics.
Finally, different changes might be linked to each other
if they have mutual dependencies. For example, the dele-
tion of a biological reaction triggers the deletion of its
kinetic law. Similarly, the deletion of an XML node (e.g.
an SBML species) triggers the deletion of all its attributes
(e.g. the species initial concentration). Those changes can
be linked using the wasTriggeredBy relationship to
express relations between changes.
COMODI version 2016-03-11 contains a hierarchy of
65 classes and includes five object properties. The object
properties can be used to establish relationships between
members of the Change class and members of the four
main branches of the ontology. We list and explain these
properties in Table 1.
Availability
The COMODI ontology is licensed under the terms
of the Creative Commons Attribution-ShareAlike 4.0
International License3. The OWL encoding of the lat-
est version may be downloaded from http://purl.uni-
rostock.de/comodi/comodi.owl. Additionally, users may
also browse the ontology at http://purl.uni-rostock.
de/comodi/. COMODI is also available at http://
bioportal.bioontology.org/ontologies/COMODI through
BioPortal [17].
Applications
The COMODI ontology is specifically designed for the
annotation of differences between versions of a compu-
tational model in the life sciences. In the following we
show the usefulness of COMODI for annotating changes,
Scharm et al. Journal of Biomedical Semantics  (2016) 7:46 Page 5 of 8
Table 1 List of object properties defined in COMODI
Name Description Domain Range
affects Provides information about the parts in a model that were affected by a change. Change Target
appliesTo Stores information about the entity type in an XML document that was changed. Change XmlEntity
hasIntention Links a change to an intention that was to be achieved by the corresponding change. Change Intention
hasReason Links a change to a reason that made this change necessary. Change Reason
wasTriggeredBy Represents dependencies among changes: A change might trigger further changes. Change Change
predicting the effects of changes on the simulation result,
and filtering versions of a model for specific differences.
Annotation of changes
SBMLmodels typically use parameters to define the kinet-
ics of a process. The corresponding entity in the SBML
document looks as follows:
<parameter name="Km1" value="23.24"
units="molesperlitre" />
Here the value of the parameter Km1 is 23.24
molesperlitre.
Updating the parameter value to23.42 molesperlitre
results in an update of the corresponding XML entity. The
new version of the model contains the following piece of
SBML code:
<parameter name="Km1" value="23.42"
units="molesperlitre" />
BiVeS identifies the difference as an update of the
paramter value. The XML-encoded serialisation provides
the new and the old value of Km1:
<update>
<attribute id="1"
oldPath="/sbml[1]/../parameter[1]"
name="value" newValue="23.42"
oldValue="23.24" [...] />
</update>
Using COMODI the detected update can now be anno-
tated. Some information can directly be inferred and thus
be annotated automatically with BiVeS. For example, we
know that the above is an update and can link it to the
XML entity XmlAttribute. BiVeS is even able to rec-
ognize that this change corresponds to a change of the
ParameterSetup. The combination of several state-
ments using terms of the different branches allows users
to be very specific. COMODI offers terms describing
the reason and the intention of a change. Following the
example from the previous section, the annotation of the
parameter update might look like4:
#1 a comodi:Update ;
comodi:appliesTo comodi:XmlAttribute ;
comodi:affects comodi:ParameterSetup ;
comodi:hasIntention comodi:Correction ;
comodi:hasReason
comodi:MismatchWithPublication.
The full example is included in Additional file 1 and
explained in Additional file 2.
Prediction of the possible effects of a change
The modification of the ParameterSetup also affects
the ModelSetup (cmp. ontology terms in Fig. 2) and
thus potentially influences the simulation results. Simi-
larly, modifications of a FunctionDefinition or the
KineticsDefinition can influence the simulation
outcome. Finally, changes in the network structure (e. g.,
modification of the ReactionNetworkDefinition
by transforming a reactant into a modifier) will not only
affect the simulation outcome, but in addition the visual
representation of the network. For this subset of changes,
modellers should be notified of any modification.
Another case are changes that affect the
ModelEncoding. For example, models are regularly
updated to remain compliant with new versions of format
specifications. These changes are especially relevant for
software tools dealing with model code. As not all tools
feature the full set of SBML constructs [18], for example,
the upgrade of a model may require the use of another
software tool. Thus, changes that result from modifica-
tions of the format specification can be of indirect interest
for modellers. They may not affect the modelled system
but the tools that are needed to interpret and simulate it.
However, other changes may not be as relevant. It can
be helpful to hide them and thereby help users focus on
important changes. For example, the reading and subse-
quent writing of amodel file using different software tools,
such as COPASI [19] or CellDesigner [20], often results
in a re-shuffling of elements within the document. How-
ever, the sequence of certain elements might not matter
to the encoded model. In SBML for example, the order of
parameters defined in the listOfParameters is irrel-
evant for the encoded system, as SBML does not give any
semantic meaning to element orders [21]. Thus, changes
that only affect the order of elements, can be neglected.
Even if BiVeS reports them in its XML serialisation, these
changes can be discarded if annotated with the corre-
sponding COMODI terms. For other types of changes,
the decision whether to neglect a change or not depends
on the user. A new identifier scheme for the semantic
annotations, for example, is relevant to curators and tool
developers, while it is probably irrelevant for the majority
Scharm et al. Journal of Biomedical Semantics  (2016) 7:46 Page 6 of 8
of modellers. However, modellers who based their model
analysis, comparison, or visualisation on semantic anno-
tations need to be notified about this type of change. Here,
COMODI terms need to be evaluated based on the users
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 
DOI 10.1186/s13326-016-0089-6
RESEARCH Open Access
Towards natural language question
generation for the validation of ontologies
and mappings
Asma Ben Abacha1*, Julio Cesar Dos Reis2, Yassine Mrabet1, Cédric Pruski1 and Marcos Da Silveira1
Abstract
Background: The increasing number of open-access ontologies and their key role in several applications such as
decision-support systems highlight the importance of their validation. Human expertise is crucial for the validation of
ontologies from a domain point-of-view. However, the growing number of ontologies and their fast evolution over
time make manual validation challenging.
Methods: We propose a novel semi-automatic approach based on the generation of natural language (NL)
questions to support the validation of ontologies and their evolution. The proposed approach includes the automatic
generation, factorization and ordering of NL questions from medical ontologies. The final validation and correction is
performed by submitting these questions to domain experts and automatically analyzing their feedback. We also
propose a second approach for the validation of mappings impacted by ontology changes. The method exploits the
context of the changes to propose correction alternatives presented as Multiple Choice Questions.
Results: This research provides a question optimization strategy to maximize the validation of ontology entities with
a reduced number of questions. We evaluate our approach for the validation of three medical ontologies. We also
evaluate the feasibility and efficiency of our mappings validation approach in the context of ontology evolution.
These experiments are performed with different versions of SNOMED-CT and ICD9.
Conclusions: The obtained experimental results suggest the feasibility and adequacy of our approach to support the
validation of interconnected and evolving ontologies. Results also suggest that taking into account RDFS and OWL
entailment helps reducing the number of questions and validation time. The application of our approach to validate
mapping evolution also shows the difficulty of adapting mapping evolution over time and highlights the importance
of semi-automatic validation.
Keywords: Ontology validation, Mapping validation, Question generation, Knowledge management
Introduction
An ontology can be defined as a formal, explicit specifi-
cation of a shared conceptualization [1] that can play a
key role in many different applications [2]. In the med-
ical domain, ontologies are becoming popular to repre-
sent clinical knowledge. Several ontologies became a de
facto standard in the domain (e.g., SNOMED CT1, NCI2).
As multiple ontologies can describe the same domain,
*Correspondence: asma.benabacha@gmail.com
1Luxembourg Institute of Science and Technology (LIST), Esch-sur-Alzette,
Luxembourg
Full list of author information is available at the end of the article
semantic mappings are often defined to link ontology ele-
ments that refer to the same real-world entity but belong
to different domain-related ontologies [3]. These links
play a key role for systems interoperability tasks as they
allow them to reconcile data annotated using different
ontologies [4].
New ontologies and mappings are frequently published
and updated on the Web. For instance, Bioportal3 is a
biomedical ontology repository where more than 350 dif-
ferent ontologies are published and maintained. CISMeF4
is another example of how biomedical ontologies can be
used to retrieve relevant information on the Web. The
successful application of ontologies brings new challenges
© 2016 Ben Abacha et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 2 of 15
related to their construction, maintenance and validation.
Natural Language Processing (NLP) techniques have been
extensively used to retrieve and extract information from
textual sources in order to automatically identify con-
cepts, instances, and relations used in a specific domain.
This led to the rapid growth of biomedical ontologies, and
consequently, to an increasing need of content validation.
Erroneous facts can be included in ontologies because
of several factors; including automated concept and rela-
tion extraction methods, disagreements between different
human actors involved in ontology design, and ontol-
ogy evolution. The ontology validation process can target
different aspects depending on the requirements of vali-
dation [5]: e.g., human understanding, logical consistency,
modeling issues, ontology language specification, real-
world representation and semantic applications (a sum-
mary of specific techniques for ontology evaluation can be
found in [6]).
While a wide range of approaches tackled logical con-
sistency, few approaches considered the validation of the
conceptualization itself from a domain point of view. In
this article, we propose a semi-automatic approach for the
conceptual validation of ontologies and their mappings.
By conceptual validation we refer to the assessment of
whether a given fact is true or false with regards to real-
world knowledge: i.e., is the real-world model compliant
with the formal model (ontology)? [7].We propose a semi-
automatic approach to conceptual validation based on the
automatic generation of natural language questions and
the processing of experts answers to these questions. We
propose an optimization method to reduce the number
of questions required to validate a given ontology using
RDFS and OWL entailment [8].
Our second goal is to validate mapping relations
between different ontologies (e.g., equivalentClass, sub-
ClassOf, equivalentProperty, subPropertyOf). We partic-
ularly focus on the context of evolving ontologies and
the validation of automatically-generated mapping adap-
tations.We propose an adapted semi-automatic validation
approach based on the automatic generation of natural
language questions. When the expert invalidates a given
mapping, Multiple-Choice Question (MCQ) are automat-
ically generated to propose correction alternatives from
the ontology itself [9].
Our contributions can be summarized as follows:
 We propose a semi-automatic approach to simplify
the human intervention during the validation of an
existing ontology. The proposed technique
automatically generates well-formulated questions
from the target ontology using pattern-based
methods. The introduced patterns are instantiated
with ontology labels to generate Boolean questions,
which are submitted to domain experts. The answers
returned by the experts are used to prune a subset of
the remaining questions using inverse RDFS
entailment. To this end, the initial question sets are
ranked according to their impact on the remaining
questions following RDFS entailment rules. This
approach can also be applied to subsets of ontologies
corresponding to their evolution (i.e., new and
modified facts).
 We extend our approach to address the problem of
mapping validation. We propose a novel approach to
support human experts to validate recommended
modifications in mappings affected by ontology
evolution. We investigate techniques to generate
MCQ that allow suggesting new decisions in the
mapping modification process. The proposed
approach analyses both the old and the updated
context of concepts to propose alternative choices if
the initial adapted mappings are invalidated by the
experts.
 We experimentally assess our approaches conducting
evaluations using real-world biomedical ontologies
and mappings established between them. We
measure different aspects to observe the quality and
effectiveness of the questions and the defined
approaches. Our results show innovative findings
regarding the way that questions are generated and
their relevance for the validation of ontologies and
mappings.
We present and discuss research work related to ques-
tion generation and the validation of ontologies and map-
pings in Section Background. In Section Methods we
present our approaches for the validation of ontologies
(Section Ontology validation method) and their map-
pings (Section Mapping validation method). Our exper-
iments and results are detailed and discussed in Sections
Experimental evaluation and Discussion and Future
work.
Background
In this section, we review existing techniques related
to ontology evaluation (e.g., ontology verbalization) and
methods related to the validation of ontologies and
mappings. We also present the original aspects of our
approach with regard to related works.
Ontology evaluation criteria
Initial approaches tackling the quality of ontology con-
tents emphasized on statistical aspects such as the num-
ber of classes, the number of properties or the number
of leaf classes [10]. While these numbers reveal some
information about the complexity of an ontology, they
do not cover other aspects of validation as discussed in
Section Introduction.
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 3 of 15
In their work in the PERTOMED project, Baneyx and
Charlet [11] introduced several relevant criteria for the
evaluation of ontology quality at various moments of its
life-time (i.e., construction, evolution and maintenance)
with a particular focus on the biomedical domain and
its specifics. Some of the criteria deal with the struc-
tural and logical aspects of the ontology, while others
tackle the conceptualization of the represented domain.
Among those related to the conceptualization, they dis-
cuss the ontological commitment as essential. They advo-
cate that when designing an ontology, a minimal number
of hypotheses must be assumed to represent the domain.
The authors also stress the usability of the ontology as well
as its ability to fulfill the set of requirements it has been
designed for.
Stvilia [12] defines a model with twelve different crite-
ria to evaluate the quality of an ontology. He also con-
siders statistical criteria which exploit explicitly-defined
ontological elements, including: the number of classes or
properties, subjective aspects like semantics and struc-
tural consistency. He also considers volatility as a new
criterion to evaluate the duration for which an ontology
remains valid by measuring the period of time elapsed
between two successive updates. Therefore he takes into
account, to a certain extent, the evolution of the con-
sidered ontology. As noted by Baneyx and Charlet [11],
this work also considers the usability of an ontology by
counting the number of applications that are using it.
Djedidi and Aufaure [13] proposed an approach to
assess the quality of an OWL ontology at evolution
time. They proposed a set of quality criteria dealing with
complexity, cohesion (e.g., average number of connected
components), conceptualization (e.g., average number of
object properties per classes), abstraction (e.g., maximum
number of classes between the root and the leaves of the
ontology), completeness and comprehension (i.e., number
of annotated classes or individuals). Nevertheless, the pro-
posed approach is clearly dependent on the OWL model,
since the implementation of the metrics relies on OWL
primitives.
Sabou and Fernandez [6] introduced two other dimen-
sions to consider when evaluating ontologies. The first
one consists in finding relevant criteria for the selection
of an existing ontology, instead of creating a new ontol-
ogy from scratch. This makes sense because of the large
number of available ontologies through theWeb. The sec-
ond criterion deals with the modularity of an ontology.
They proposed to evaluate modules that require combina-
tion for a given purpose, or for a particular application, to
decide the relevance of an ontology.
More recently, Rico et al. [14] presented the OntoQual-
itas framework. In this work, no new type of criterion
addressing the quality of an ontology has been introduced,
but the metrics to calculate them have been improved and
refined. The authors also provided a concrete case study
to assess the framework.
Question generation and verbalization of ontology content
Several efforts have addressed the automatic generation of
NL questions5. Most of them focused on the generation of
questions from text (text-to-question task) [15]. The ques-
tion generation process can rely on manual patterns [16]
and/or on statistical techniques [17].
AUTOQUEST [15] stands for one of the first question
generation systems proposed to assess text understand-
ing. More recently, Heilman [17] proposed to generate
WH questions from text, relying on hand-crafted trans-
formation patterns and a statistical ranking model. For a
related e-learning task, Liu et al. [16] proposedG-Asks, an
automatic question generation system. To support learn-
ing through writing, G-Asks generates specific questions
using manual patterns that are associated with different
question types. Mitkov and Ha [18] tackled the generation
of multi-choice questions from instructional documents.
The main proposed process employs NLP techniques for
domain term extraction and shallow parsing. Their work
also defined hand-crafted transformational rules to gener-
ate the questions from declarative sentences with minimal
modification to the original words.
The current volume of dense ontologies requires novel
techniques to guarantee the semantic precision of the
contents. Papasalouros et al. [19] suggested an automatic
approach to generate MCQs from ontologies that remains
independent from linguistic resources and domain-
specific constraints. They proposed ontology-level strate-
gies according to the basic RDFS types and primitives
(e.g., classes, properties and subClassOf axioms). The
approach defined these strategies to produce distractors
in the scope of computer-assisted assessment. However,
they do not explore advanced NL generation methods.
For example, the same stem, Choose the correct sentence"
is used in all questions. Similarly, Cubric and Tosic [20]
examined the same type of ontology-related strategy for e-
assessment, by defining a generic questions ontology and
linking it to domain ontologies.
The MoKI systems designed by Pammer [21] is, to the
best of our knowledge, the only work that addresses the
problem of ontology validation by means of question-
answering techniques. The proposal is to generate ques-
tions from the content of the ontology and submit them to
domain experts in order to get their feedback, leading to
the validation or modification of the underlying ontology.
However, the question generation process does not inte-
grate the fact that domain experts are rarely familiar with
formal ICT languages. The questions generated by MoKI
look very similar to description logic formulas hardly
understandable by experts. Hence, the outcome of the
proposed system still require a substantial intervention
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 4 of 15
of ICT experts to support domain experts through the
validation process.
In another application, Teitsma et al. [22] presented an
ontology-based generation approach for situation deter-
mination. A traffic accident ontology and two databases
on accidents were used to: (i) generate questions from
infons (sets of field values describing the situation; such
as weather or injuries) and (ii) get validation answers
from human observers whowatched video scenes describ-
ing the target situations. They used rule-based logical
optimization techniques (e.g., if the observers validated
rain for weather the system does not ask if the road
is wet). However, this work does not provide details
on the NL level of the generation process, which seems
adhoc with respect to the concepts of the selected domain
ontologies.
Ontology validation
As discussed above, a significant amount of work tack-
led the study and proposition of a set of metrics dealing
with the quality of ontologies. These metrics are based on
measurable properties such as the number of classes, indi-
viduals and properties. While it can be argued that such
metrics can give a relevant evaluation of the quality of
an ontology, they cannot be used to validate the ontology
content, i.e., an ontology with a good quality can contain
more erroneous facts that an ontology with an estimated
lower quality.
Gangemi et al. introduced a model for evaluating and
validating ontologies [23]. Based on a meta-ontology
called O2 and semiotics, the authors propose to evalu-
ate ontologies by considering structural, functional and
usability-profiling measures. The validation is then com-
plemented with an ontology called oQual aiming at pro-
viding necessary criteria to select an ontology to answer a
particular need.
Köhler et al. provided a rule-based methodology (i.e.,
a set of conventions) to establish well-defined labels for
Gene Ontology (GO) concepts [24]. The rules aim at avoid-
ing circular definitions (i.e., term of the label that are also
in the definition of the considered concept) and obscure
language (i.e., labels of concepts must be understood by
non expert persons). Although interesting, this work can
hardly be applied to other ontologies because GO labels
are very domain specific. They are not only built on lin-
guistic aspects, but they use a lot of alphanumeric symbols
to denote gene and proteins.
A similar argument is used by Verspoor et al. [25]. The
authors proposed amethodology to classify medical terms
that denote the same meaning but use different linguis-
tic conventions in order to standardize them. Such kind
of approaches addresses an important aspect which is
the choice of the terminology to describe ontological ele-
ments. However, elements that are implicitly defined (e.g.,
concepts that are logically defined by inference) are not
standardized.
Dimitrova et al. designed the ROO tool for supporting
domain experts designing OWL ontologies [26]. This pro-
vides a controlled language interface and offers systematic
guidance throughout the whole ontology construction
process with an aim of optimizing the quality of the result-
ing ontology. However, nowadays ontologies are rather
built automatically from the textual content of relevant
documents or data [27, 28], or rather slightly modified
by virtue of knowledge evolution. Accordingly domain
experts are mostly involved in the validation phase and
less and less from the beginning of the ontology life cycle.
In their work [29], vor der Bruck and Stenzhorn
described a method to validate ontologies using an
automatic theorem prover and MultiNet axioms. To
this end, the authors focused on the logical structure
and ignored the conceptualization part. Therefore, their
method requires formal ontologies expressed in logic-
based languages, which is not always the case in the
biomedical domain. The system implementing the pro-
posed algorithm, is accompanied with a user friendly
software interface to speed up the fixing of the detected
erroneous axioms facilitating users intervention.
More recently, Poveda-villalón et al. [5] proposed the
OOPS! system. This consists of detecting pre-defined
anomalies or bad practices in ontologies to enhance their
quality. However, the real-world representation dimen-
sion is neglected in this approach, which refers to how
accurately the ontology represents the domain intended
for modelling. This is left to the discretion of domain
experts.
Other families of approaches have been proposed
addressing the validation of the domain-conceptualization
side. Some of them emphasize on user interface devel-
opment to better present large amount of (structured)
data without overwhelming users [30] to support the
validation effort. Other research work promotes the use
of NLP techniques to better involve domain experts in
the ontology validation process [21].
Mapping validation
Similar to ontologies, previous studies have revealed
the real difficulties and importance of validating map-
pings and involving human experts in the process [31].
Mapping revision refers to a method aiming to iden-
tify and repair invalid mappings that can be explored
for mapping validation. Existing techniques may detect
the invalid mappings at ontology evolution time. Meilicke
et al. [32] proposed an automatic mapping debugging
between expressive ontologies eliminating inconsisten-
cies, caused by erroneous mappings, through logical
diagnostic reasoning. Mapping revision still demands
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 5 of 15
logically expressive ontologies. This motivates further
research on alternative methods to validate the adap-
tation of mappings. Similarly, Serpeloni et al. [33]
proposed a semi-automatic process to validate map-
pings through graph algorithms that select instances for
verification.
In order to take human aspects into account, recent
studies have examined the interactive aspect to validate
mappings. Some approaches tackled the design of interac-
tive tools to support the ontology mapping process with
relevant visualizations [34]. Other studies proposed ontol-
ogy alignment and validation based on users in commu-
nity [35] and crowdsourcing [36]. However, these users are
not necessarily experts in the (sub-)domain represented
by the ontology.
Positioning
Although several studies addressed recently the tasks of
ontology validation, mapping validation and question gen-
eration, little attention has been given to the problem of
validating ontology contents (including semantic align-
ments) from the perspective of domain conceptualization.
In this context, our first contribution is a semi-automatic
approach to reduce and simplify the human interven-
tions required to validate ontology contents andmappings
from a domain point of view. Our approach is based
on the generation of boolean questions from the ontol-
ogy. Expert answers to these questions are then processed
automatically to validate and correct the ontology. We
also use the expert feedback incrementally to prune a
subset of the remaining questions using inverse RDFS
entailment.
On the other hand, to the best of our knowledge, there
are no studies that tackle particularly the possibility of
validating ontology mappings via automatic question gen-
eration. We propose a mapping validation approach that
tackles the special characteristics of modifications inmap-
pings over time, taking the involvement of users into
account. More precisely, our second contribution aims
to support human experts during the mapping validation
process.
Methods
We briefly present the preliminary definitions needed
to describe our approach. We first introduce the for-
mal notions of ontology and mapping before defining
the problem of conceptual validation and mapping val-
idation (Section Definitions and problem statement).
In the second part of this section we present our
optimisation approach for the generation of boolean
questions to validate ontologies (Section Ontology
validation method) and our method to search for
correction alternatives for invalid mappings (Section
Mapping validation method).
Definitions and problem statement
An ontology O = (Concepts,Relationships,Attributes)
consists of a set of Concepts interrelated by directed Rela-
tionships. We define a set of concepts of an ontology Ox
at time t as Concepts(Otx) = {Ct1,Ct2, ...,Ctn}. Each concept
C ? Concepts has a unique identifier and is associated
with a set of attributes Attributes(C) = {a1, a2, ..., ap}
(e.g., label, definition, synonym, etc.). A relationship r ?
Relationships interconnects two concepts and has a spe-
cific type, e.g., is_a or part_of .
The context of a concept (CT(Ci)) in the ontology
stands for the union of the sets of super concepts (sup(Ci)),
sub concepts (sub(Ci)) and sibling concepts of Ci (sib(Ci)),
as following:
CT(Ci) = sup(Ci) ? sub(Ci) ? sib(Ci) (1)
where
sup(Ci) = {Ck |Ck ? Concepts(O),Ci  Ck ? Ci = Ck}
sub(Ci) = {Ck |Ck ? Concepts(O),Ck  Ci ? Ci = Ck}
sib(Ci) ={Ck |Ck ?Concepts(O),?a, bs.t.Ca? sup(Ck)andCa? sup(Ci)}
where Ci  Ck means that Ck subsumes Ci.
An ontology mapping MtOA,OB , established at time t,
interlinks a set of given concepts Ca and Cb from two
different ontologiesOA/OB by so-called correspondences:
MtOA,OB={(Cta,Ctb, semTypetab, conf t , statust)|Ca ? Concepts
(OA),Cb ? Concepts(OB), confidence ?[ 0, 1] ,
semanticType ? {?,?,?,?},
status?{"valid","invalid","inactive","handled","to?verify"}}
A correspondence corCA,CB = (CA,CB, confidence,
semanticType) links two concepts CA ? Concepts(OA)
and CB ? Concepts(OB). The confidence value represents
the semantic similarity between CA and CB (indicating
the confidence of their relation [3]). The higher the value,
the more related are both concepts. The semanticType
in corCA,CB refers to the semantic relation connecting CA
and CB. We consider the following types of semantic rela-
tions: unmappable [?], equivalent [?], narrow-to-broad
[?], broad-to-narrow [?] and overlapped [?].
The conceptual validation problem consists of defin-
ing a method to get feedback from domain experts on the
correctness of ontology facts (or mappings) and interpret
their answers to modify the ontology/mapping accord-
ingly. We propose to examine question generation tech-
niques to cope with the conceptual validation problem.
Two issues have to be investigated in particular:
 How to formulate Natural Language (NL) questions
that would lead to expert answers that are both
relevant and computer-interpretable? (clarity
problem)
 How to avoid overwhelming human experts with
unnecessary questions? (optimisation problem)
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 6 of 15
We define the mapping validation problem as follows:
starting from an ontology OA at time t, noted OtA, and
another different ontology OB at time t, noted OtB, a set
of correspondences exist between them MOtA,OtB . In par-
ticular, the investigated problem consists in validating the
correspondences if the ontology OtA evolves to a new ver-
sion Ot?A at time t?. We divide the problem by considering
the evolution of only one ontology at a time (i.e., the target
ontology remains unchanged).
In the following, we define our methods for ontol-
ogy validation (Section Ontology validation method)
and mapping validation (Section Mapping validation
method).
Ontology validation method
Approach overview
We propose a semi-automatic approach based on ques-
tion generation to validate ontologies. Figure 1 describes
our ontology validation system, called SAVANT. The first
step consists of automatically generating a list of boolean
questions from the ontology under validation.
These questions are submitted to domain experts who
provide an agreement decision (Yes/No) and a textual
feedback. The next step consists on interpreting expert
feedback to validate or modify the ontology. The novelty
of our approach relies on the fact that manual interven-
tions are performed only by Health Professionals (HPs),
who will lead the ontology validation process. ICT experts
are required only when the error cannot be solved auto-
matically. This increases the quality of exchanges between
actors and reduce errors and time consumption.
We explore the proposed approach to (i) validate ontolo-
gies constructed automatically from medical texts (e.g.,
clinical guidelines) and also (ii) to re-validate ontologies
(constructed manually or automatically), since medical
knowledge evolves quickly over time.
We focus on validating the following types of ontology
statements:
 A rdfs:subClassOf B (class A is a subclass of B)
 P rdfs:subPropertyOf Q (property P is a sub-property
of Q)
 P rdfs:domain D (D is the domain class for property P)
 P rdfs:range R (R is the range class for property P)
 I rdf:type A (I is an individual of class A)
 I P J (the property P links the individuals I and J)
The proposed approach uses manually constructed pat-
terns for each kind of ontology element as described in the
following section.
Pattern-basedmethod for boolean question generation
We start from the hypothesis that all the elements of a
medical ontology must be validated. This involves validat-
ing concepts (e.g., Substance), relations between concepts
(e.g., administrated for), concept instances (e.g., activated
charcoal is an instance of Manufactured Material), rela-
tions between concept instances (e.g., chest X-ray can be
ordered for Chronic cough) or between concept instances
and literals (e.g., give oral activated charcoal 50 g indi-
cates the dose of the substance to be administrated 50 g).
These ontology elements provide the main keywords of
the question patterns through the labels of concepts, rela-
tions and instances.
We constructed manually question patterns associated
to each type of ontological element (5 different elements
in our preliminary experimentations). A question pattern
Fig. 1 Proposed approach to ontology validation based on the automatic generation of boolean questions
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 7 of 15
consists of a regular textual expression with the appro-
priate gaps [37]. For instance, the pattern Is DOSE of
DRUG well suited for PATIENTS having DISEASE? is
a textual pattern with 4 gaps: DOSE, DRUG, PATIENTS
and DISEASE. This question pattern aims to validate a
drug dose administrated to a patient having a particular
disease. The singular or plural form of the verb in the
expression is determined using the Stanford parser6. Sin-
gular is used by default if the detection is not possible, a
frequent case that occurs because of the heterogeneity of
ontology labels.
Table 1 presents examples of boolean-question patterns.
Question optimization strategy
At this level, our main objective is to investigate a
technique to build relevant questions from formalized
knowledge in order to validate the maximum number of
assertions with the minimum number of questions.
We propose an optimization strategy relying on the
RDFS logical rules to rank the questions according to the
elements that imply the more changes in the ontology.
For instance, if we have the following data:
 hasSuitedAntiobioticsType rdf:subPropertyOf
hasTreatment
 Antibiotics rdfs:subClassOf Treatment
 hasSuitedAntiobioticsType rdfs:range Antibiotics
and the expert invalidates Antibiotics rdfs:subClassOf
Treatment", than the property hasSuitedAntiobioticsType
cannot be declared as a sub-property of hasTreatment
because the hasSuitedAntibioticType relation has not a
common range with the property hasTreatment, which
leads to a formal error regarding the RDFS entailment
rules.
We consider all RDFS entailment rules7. Table 2
presents some inversed forms of these rules to show the
impact of invalidating each one of the target ontology
statements.
This technique enables ranking questions in a manner
that allows to delete some of the remaining questions if
one of the RDFS entailment rules apply. This leads to the
following validation order:
1. A rdfs:subClassOf B
2. P rdfs:domain D and P rdfs:range R
3. P rdfs:subPropertyOf Q
4. I rdf:type A
5. I P J
Answer analysis and ontology update
The second step of our approach refers to the exploitation
of expert feedback to validate or modify the target ontol-
ogy. The ontology under validation might contain con-
cepts, individuals and relations defined between concepts
or individuals.
Feedback consists of two main parts: (i) an assertion
on the correctness of the target knowledge and (ii) a free
textual explanation if provided8. In the scope of this arti-
cle, we take into account ontologies that are formally-valid
(with no inconsistencies) and emphasize the validation of
domain conceptualization.
In this context, Yes answers have no impact on the
ontology. The ontology is modified on the No answers
provided by the domain experts. Invalidating an ontol-
ogy element implies different impacts according to the
element type.
We use the same RDFS entailment rules to update the
ontology. The ontology item invalidated by the expert and
the inferred invalidations are deleted from the ontology,
as well as the questions associated to them.
Mapping validation method
Approach overview
We consider as input a set of adapted correspondences
MOt?A ,OtB . Adaptation here refers to the automatic mapping
adaptation that occurs after the evolution of the source
ontology OtA. We consider that the old mappings are cor-
rect and we want to validate only the new ones. Similarly
to the ontology validation method, our approach to val-
idate mappings relies on the generation of NL questions
from the new mappings. Figure 2 describes our proposed
approach for mapping validation. Figure 3 presents our
method for question generation through a state transition
diagram.
Table 3 presents examples of correspondences between
SNOMED-CT and ICD9. Figure 4 shows examples
of more or less ambiguous correspondences retrieved
between the biomedical ontologies SNOMED-CT and
ICD9. This selection provides concrete examples of the
issues related to the heterogeneity and broadness of some
Table 1 Examples of boolean-questions patterns used for ontology validation
Question pattern Example of instance
Does a(n) CLASS have a PROPERTY Does an effect have a measurement method?
Does a treatment have an administration scheme?
Is CLASS a type of CLASS? Is statistical evidence a type of evidence?
Is SUB-PROP of a CLASS a PROP of the same CLASS? Is primary treatment of a disease a treatment of the same disease?
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 8 of 15
Table 2 Examples of ontology update rules with respect to invalidated elements used for ontology validation
NOT A rdfs:subClassOf B ? NOT A rdfs:subClassOf C s.t. C rdfs:subClassOf B
NOT P rdfs:domain A ? NOT P rdf:subPropertyOf Q s.t. Q rdfs:domain A
NOT I rdf:type A ? NOT <I, P, J> s.t. P rdfs:domain A
NOT <J, P, I> s.t. P rdfs:range A
concept definitions. Dealing with this problem requires to
define flexible answer types to ensure a relevant interac-
tion with the human validators.
STEP 1: Boolean questions
In the first step, our method translates the proposed
adapted correspondences into a NL question using tex-
tual patterns associated to each relation type. Let X be the
source concept label and Y be the target concept label, the
main patterns are as follows:
1. (Is|Are) X <equivalent to> Y?
2. (Is|Are) X <more specific meaning than> Y?
3. (Is|Are) X <less specific than> Y?
4. Do(es) X <partially correspond to> Y?
5. X <cannot be matched with> Y?
These patterns are instantiated with the involved
concepts of a correspondence. We present three instanti-
ation examples from our dataset in the following:
1. Are intestinal diseases equivalent to vascular disorder
of intestines?
2. Does the Trousseau sign partially correspond to
ill-defined and unknown causes of morbidity and
mortality?
3. Is the Eisenmenger Complex more specific than
other congenital malformations of cardiac septa?
STEP 2: multiple choice questions
In the second and main step, negative answers trigger
multiple choice questions (MCQs) that are submitted to
the expert in order to detect alternative correct mappings
between Ct?a (the source concept) and the concept Ct
?
b in
Fig. 2 Proposed approach to validate mapping adaptation based on the automatic generation of boolean questions for new mappings and
multiple choice questions for invalid mappings
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 9 of 15
Fig. 3 The question generation process as a state-transition diagram
target ontology Ot?B . MCQ consists of (i) a problem known
as the stem and (ii) a list of suggested alternatives. In our
approach, we have three categories of stems/questions (cf.
Fig. 3):
1. Revision of Ca. This suggests revising the source
concept by candidate proposals from the new source
ontology Ot?A. This category preserves the semantic
mapping-relation between the source concept Ct?a
and the target concept Ct?b , and proposes candidate
concepts from Ot?A that are semantically close to the
initial source concept Cta. In this MCQ category, we
propose stems of the form:
What concept <semanticType> <target
concept>? corresponds to the revision of the
candidate source concept, where
< semanticType > refers to the type of mapping
relation semanticTypet?ab and
Table 3 Examples of correspondences between SNOMED-CT and ICD9CM
Concept source label semanticType Concept target label
Intestinal diseases equivalent to [?] Vascular disorders of intestine
Nail-Patella syndrome more specific than [?] Congenital malformation syndromes
predominantly involving limbs
Respiratory tract infections less specific than [?] Acute upper respiratory infection, unspecified
Abnormality of gastric inhibitory
peptide secretion (disorder) partially correspond to [?] Bladder
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 10 of 15
Fig. 4 Examples of ambiguous correspondences between SNOMED-CT and ICD9CM
<target concept> consists of the label of the target
concept Ctb. For instance, an instantiation of this stem
pattern is: What concept is more specific than other
restrictive cardiomyopathy? The alternatives for this
stem stand for the top n most semantically-close
concepts to the initial source concept (e.g.,
Cardiomyopath, Restrictive). Section Selection of
alternative concepts and mapping relations presents
the selection of alternative candidate concepts.
2. Revision of mapping relation (MR). This category
of questions proposes revising the type of mapping
relation. More precisely, in case of a negative answer
in the previous MCQ option, our method preserves
the initial concept candidate Ct?a and modifies the
semanticType of the adapted correspondence,
selecting another alternative mapping relation (cf.
Section Selection of alternative concepts and
mapping relations). We propose stems of the form
Choose the correct mapping relation alternative.
The proposed alternatives are declarative sentences
derived from the question patterns.
3. Revision of both. In case of a negative answer in the
previous option, our method revises both candidate
proposals and semantic relation types, aggregating
both option 1 and 2 in a single multiple choice
question. We formulate stems of the form Choose
the correct source concept and relation type
corresponding to the revision of both the candidate
source concept and semanticType of mapping. We
present alternatives for the question generation in 3
columns format, where the first column consists of
the list of selected source concept alternatives, the
second column presents the list of new suggested
types of semantic relations and the third column
contains the target concept.
We present two instantiation examples from our dataset
in the following:
1. Is Other spontaneous pneumothorax more specific
than closed pneumothorax?
Alternative source concepts:
 Iatrogenic pneumothorax
 Secondary spontaneous pneumothorax
2. Is Gastroparesis more specific than Diabetic
Gastroparesis associated 2 diabetes mellitus?
Alternative source concepts:
 Acute dilatation of stomach
 Dyspepsia and other specified disorders of
function of stomach
Selection of alternative concepts andmapping relations
The defined approach based on MCQ (cf. Section STEP
2: multiple choice questions) requires selecting candi-
date concepts and different semanticType relations as
suggested alternatives in question generation to support
mapping validation over time. For this purpose, we pro-
pose an algorithm to select similar concepts to the original
source concept.
Selection of alternative concepts
In the scope of source concept revision, we generate alter-
natives in MCQs by using candidate concepts from the
context of the initial source concept in the ontology. We
aim at combining the answers from these questions to
propose re-adapting correspondences if necessary. For
example, if a given correspondence between source con-
cept Cta and target concept Ctb is adapted, such that a
concept Ct?k ? Concepts(Ot
?
A) replaces the original concept
Cta, this generates an adapted correspondence at time t
?
between Ct?k and C
j?
b ? Concepts(Ot
?
B).
Therefore, we retrieve from the ontological context CT
(cf. Eq. 1) a set of other concepts which differs from
Ct?k , Candidates = {(Ct
?
ai , simi)i ? [ 1..n] }, where Ct
?
i ?
CT(Ct?a ).
Algorithm 1 presents the designed procedure to retrieve
the candidate concepts from the context, given a source
concept Cta of a mapping. The algorithm sorts the
best top n candidate concepts from CT(Ct?a ) using a
similarity measure. We use the the bigram similarity
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 11 of 15
measure following the observations of [38] on its suit-
ability for ontology matching tasks. For two given labels,
Bigram similarity is computed as the euclidean dis-
tance, using all possible bigrams from both labels as
dimensions. In our approach, we compute the similarity
between pairs of comparable attributes that are selected
beforehand as a parameter (e.g., the name and syn-
onym attributes). We denote the similarity function as
simAtt(ati .value, at
?
j .value) between two attribute values
ati .value and at
?
j .value.
Algorithm 1: Find candidate concepts in context
Require: Cta ? Concepts(OtA);CT(Ct
?
a ) ? Ot?A;Ct
?
k ?
Concepts(Ot?A); n ? N;Attributes(Cti )forallCti ?
Concepts(OtA)
Ensure:
Candidates = {(c1, sim1), (c2, sim2), ..., (cn, simn)}
Candidates ? ?;maxSim ? 0;
for all atp ? Attributes(Cta) do
for all Ct?i ? CT(Ct
?
a ) do
if Ct?i = Ct
?
k then
for all at?i ? Attributes(Ct
?
i ) do
s ? simAtt(atp.value, at?i .value);
ifmaxSim < s then
maxSim ? s;
end if
end for
Candidates ?
Candidates ? {(Ct?i ,maxSim)};maxSim ? 0;
end if
end for
end for
return Candidates ? sort(Candidates, n); {Select top
n concepts}
Given all attributes of the original source concept, the
algorithm retrieves all concepts in context CT at time
t?. For all retrieved concepts different from the concept
Ct?k , to which the adapted mapping is associated, the algo-
rithm selects their attributes and calculates the similarity
between the attribute values (between attributes of the
source concept and attributes of concepts inCT). For each
candidate concept, the algorithm keeps the maximal sim-
ilarity value calculated among the attributes. Finally, the
algorithm sorts the top n retrieved candidate concepts
according to the calculated similarity. We use these can-
didates as alternative answers in our MCQ approach, so
they play a central role for the automatic generation of the
questions.
Selection of alternativemapping relations
Revising the semantic relation semanticType in our
question generation method demands retrieving alter-
natives for the second category of proposed MCQ (cf.
Section STEP 2: multiple choice questions). To this end,
we recover a set of semantic relations Relalternatives =
{(semanticTypei)i ? [ 1..n] } where semanticTypei ? {?
,?,?,?} such that semanticTypei = semanticTypet?ab.
We use the Relalternatives to formulate the question in the
revision ofMR category. The alternative relations are pro-
posed from the most precise one to the more general one
(i.e., ?, ?, ?, then ?).
Experimental evaluation
We selected a set of ontologies and mappings and
designed a series of experiments to evaluate the pro-
posed methods. In this article, we considered medical
ontologies and mappings in English language, but our
approaches can be applied to other languages as well.
We present the obtained results in Sections Exper-
iments on ontology validation and Experiments on
mapping validation and discuss our findings in Section
Discussion and Future work.
Experiments on ontology validation
Materials
We tested our ontology validation approach on three
different medical ontologies that cover different aspects
of the medical domains (Treatment-Disease vs. mental
health) and constructed using different methods:
 Caries Ontology (CO). CO was developed manually
by a dentistry expert in our company.
 Disease-Treatment Ontology (DTO). We
constructed an OWL translation of the ontology
proposed by Khoo et al. [39].
 Mental Diseases Ontology (MDO). This ontology is
publicly available.
Results of ontology validation
For the first step of the experiment, Table 4 presents
the number of questions with respect to the num-
ber of classes, properties and instances of each
ontology (DTO, MDO and CO) without question
optimization.
The number of generated questions depends on the
ontology size and shows the importance of question rank-
ing and optimization. The results indicate that the opti-
mization method works better in case of ontologies with
many instances. For the CO ontology, this strategy helps
minimizing the number of submitted questions from 290
to 283 questions with only four NO answers. For theMDO
ontology, our method allows asking 239 questions instead
of 243 with only two NO answers. In case of ontologies
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 12 of 15
Table 4 The number of ontology elements (OE) and the number of generated questions for different medical ontologies without
optimization
Ontology Number of classes Number of properties Number of instances Total number of OE Number of questions
DTO 49 148 0 197 165
MDO 149 76 18 243 243
CO 26 266 13 305 290
with more NO answers (i.e. more invalid elements), the
number of deleted questions will increase.
For the DTO ontology, the concepts have no instances
and all facts were evaluated as correct by the expert, con-
sequently the initial number of questions was conserved.
The ontologies used in these experiments were con-
structed manually and semi-automatically. More experi-
ments should be conducted on automatically constructed
ontologies when available in order to evaluate more accu-
rately the benefits of question optimization.
In the case of ontologies with few invalid elements (few
NO answers), other methods should be used to optimize
the presentation and reduce the time needed to answer
the questions. For example, the following presentation
methods can be studied: (i) question factorization accord-
ing to an ontology element (concept, relation or individ-
ual) and (ii) logical chaining (A hasRelation1With B, B
hasRelation2With C, etc.). Such organization can be effec-
tive in helpingmedical experts understand and answer the
questions more quickly.
Experiments onmapping validation
Materials and experimental procedure
We evaluate the NL quality of the questions generated
automatically to validate mappings. We use two biomed-
ical ontologies SNOMED-CT9 (SCT) and ICD-9-CM10
(ICD9) including different versions of official mappings
established between them.
We aim to investigate to which degree it is possible
to generate NL sentences that can adequately describe
mappings. For this purpose, we evaluate the generated
questions according to three standard measures in NL
generation: grammaticality, fluency and meaning preser-
vation. Since our approach aims to facilitate human inter-
vention in mapping adaptation, we assume that it is
relevant to assess the NL quality of the automatically-
generated questions.
We presented the generated questions to three differ-
ent human assessors who were asked to associate a score
value between 1 and 10 for each dimension and each ques-
tion. Assessors were ontology experts and familiar with
the biomedical domain. We evaluated the approach for
the validation of 20 randomly-selected adapted mappings
generated from the evolution of mappings between SCT
and ICD9.
We measure the Inter-Assessor Agreement (IAA) for
grammaticality, fluency and meaning preservation. IAA
corresponds to the average ? measure defined in [40].
The ? measure indicates how much the assessors agree-
ment is above the probability of an agreement by chance,
and it is commonly used in computational linguistics. In
order to have relevant measures, we define 3 score inter-
vals for grammaticality, fluency andmeaning preservation
which are: [0..3], [4..6], [7..10]. We use these intervals
as categories in the calculation of the ? measure, which
corresponds to:
? = P(a) ? P(c)1 ? P(c) (2)
where P(a) refers to the observed inter-assessor agree-
ment and P(c) is the probability of a chance agreement. ?
values range from -1 to 1 (cf. Table 5 for results).
Results of mapping validation
Table 5 presents the obtained results for the 20 Boolean
questions that are generated for the 20 targeted mappings.
They present the measures of IAA for grammaticality,
fluency and meaning preservation.
Our second focus is to evaluate the usefulness of each
question type. To this end, we count (i) the number of
returned answer-types for the set of 20 questions accord-
ing to the different question types and (ii) the number of
validation/invalidation according to the observed adapted
mappings (examining the evolution of the two official
Table 5 Quality of the NL generated questions for mapping validation and average ? Inter-Assessor Agreement
Grammaticality Fluency Meaning
Min. Max. Avg. Min. Max. Avg. Min. Max. Avg.
Assessor 1 0.4 1 0.775 0.4 1 0.81 0.4 1 0.915
Assessor 2 0.4 1 0.745 0.4 1 0.77 0.4 1 0.86
Assessor 3 0.6 0.9 0.735 0.6 0.9 0.745 0.6 0.9 0.77
Average ? 0.28 0.48 0.45
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 13 of 15
releases mappings in our dataset) (cf. Table 6). In this sec-
ond independent evaluation, we asked the assessors to
find a common agreement on the semantic correctness of
the correspondences.
The overall assessment of the generated initial Boolean
Questions (cf. Section STEP 1: Boolean questions) indi-
cates good values for grammaticality and fluency because
the attained average values are satisfactorily high regard-
ing the used metric. The most important criterion for
the mapping validation, which is meaning preservation,
had the best score by the assessors. The ? Inter-Assessor
Agreement is also relatively high (? is not negative), which
provides a positive test on the reliability of the assessors
ratings. In short-term perspectives, further tests will be
made with a correlation-based approach to have a more
precise view on the levels of agreement.
Table 6 shows the percentage of different answer types
returned during the validation of the adapted mappings.
Results indicate that 80 % of the initial adapted mappings
were validated or led to the validation of another new
mapping (i.e., re-adaptation) (cf. Final output validation
row in table 6), discovered during the validation process.
On the test set of 20 mapping adaptations, only one ques-
tion was rated as ambiguous, due to an incomplete con-
cept label. The low percentage of Yes answers for the initial
Boolean questions indicates that the automatic selection
of mapping adaptations using the approach described in
[31] were insufficient for this dataset. The 3rd type of
question, revising only the mapping relation, allowed to
validate 40 % of the mappings. Revising the source con-
cepts alone fails to validate more mappings, but together
with the revision of the mapping relation it allowed to val-
idate 40 % of the mappings (cf. see 4th type of questions
revision of both in Table 6).
Discussion and Future work
The experiments on ontology validation showed the need
to add other specific types of questions and answer types.
In some observed cases an answer can be YES but for
a specific kind of patients (e.g. Infant) or also NO for a
specific kind of patient, or under a specific condition. In
our experiments the experts answered NO for such cases.
Table 6 Answer types according to question types
Answer type
Yes answers No answers Ambiguous
source/target
Question type
Boolean question 0 % 95 % 5 %
Revision of cs 0 % 95 % 5 %
Revision ofMR 40 % 55 % 5 %
Revision of both 40 % 55 % 5 %
Final output validation 80 % 15 % 5 %
Therefore, it would be interesting to give to the expert the
possibility to specify a contextual element or an additional
condition to their YES/NO answers. A possible solution
can be to integrate factual questions as possible question
type, which will also contribute to enrich the ontology
during the validation process.
On the other hand, even if our optimization method
may allow significantly reducing the number of questions,
it is still challenging to validate very large ontologies with
natural language questions. In this context, advanced con-
tent selection techniques such as summarization can play
an important role. As suggested by Sure et al. [41], a sum-
mary of an ontology might include a couple of top levels in
the ontologys class hierarchy, and also the ontologys hub
concepts (i.e. concepts with the largest number of links).
To validate huge ontologies, we are considering
approaches based on summarization. A possible solu-
tion can be by using the Key Concepts Extraction (KCE)
algorithm which automatically extracts the most repre-
sentative classes of an ontology. More particularly, sum-
marization can be adapted to the validation task by taking
into account several features such as the number of ques-
tions needed to validate a given extract or summary and
the number of key concepts.
On the level of Mapping Validation, the conducted liter-
ature survey indicated that the generation of NL questions
for mapping validation was not investigated. Our proposal
originally evaluated the quality of generated NL questions
to help human experts judge the quality of correspon-
dences under evolution. The proposed method uses the
context of the source concept to select similar concepts as
alternatives in case of invalidated mappings.
In the conducted experiments on mapping validation,
the analysis of quality-deficient Boolean questions pro-
duced by the NL generation system highlighted to two
main error causes:
 The heterogeneity and length of the literal attributes
that led to some inadequacies with the conceived
patterns, e.g., Are other eye disorders more specific
than family history degenerative disorder of macula?
 Errors in the concepts attributes (mainly labels), e.g.
 Is other more specific than mechanical
complication of suprapubic catheter?.
These observations show the importance of evaluating
the linguistic quality of the ontology literals beforehand.
This issue is particularly discussed in [42], where a meta-
model is proposed to link ontology elements to relevant
lexical entries. In the scope of our approach on ques-
tion generation, an enhancement could be to (i) have
several patterns that paraphrase the same mapping rela-
tion and (ii) syntactically parse the generated question to
detect more trivial errors and choose alternative patterns
Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 14 of 15
if needed. According to our experiments, multiple choice
questions with 5 correction alternatives for each invalid
mapping proved to be efficient to locate the correct map-
ping relation and/or mapping target.
Conclusions
The methods for automatic ontology construction have
highlighted the problems of validation of ontologies and
mappings. Research efforts and interests have grownmore
and more, but mostly at a formal level. In this article, we
addressed the problem of ontology validation including
mappings from a conceptual and a semantic point of view.
We proposed novel semi-automatic approaches based
on the generation of questions and answers interpreta-
tion to facilitate the communication with domain experts.
The defined methods generates natural language ques-
tions from medical ontologies and mappings and uses the
answers from the expert to update/correct them.
Our approach implementing automatic methods might
guide domain experts in the validation process. Relying on
domain experts to monitor the validation can lead to var-
ious benefits, ensuring reliable communication between
heterogeneous systems.
We evaluated the proposed methods based on several
real datasets from different releases of biomedical ontolo-
gies and their associated mappings. The achieved results
underscored the feasibility of our general approach for the
validation of ontologies and mappings, and its relevant
role in the completion of the their adequate evolution over
time.
Endnotes
1http://ihtsdo.org/snomed-ct/
2http://ncit.nci.nih.gov/
3http://bioportal.bioontology.org/
4http://www.chu-rouen.fr/cismef/
5An international workshop is also held on question
generation since 2008.
6http://nlp.stanford.edu/software/lex-parser.shtml
7http://www.w3.org/TR/rdf-mt/#RDFSRules
8Free textual explanation is considered for future per-
spectives and is not used in this work.
9www.nlm.nih.gov/research/umls/licensedcontent/
snomedctarchive.html
10www.cdc.gov/nchs/icd/icd9cm.htm
Acknowledgements
We thank the SMBM 2014 program committee members for their feedback
and for the opportunity to extend our original submission [9] in the scope of
this special issue. We would also like to thank our anonymous reviewers for
their insightful comments that helped to improve our paper. This research
work was funded in part by the the São Paulo Research Foundation (FAPESP)
granted to JDR (Grant #2014/14890-0). The opinions expressed in this work do
not necessarily reflect those of the agency that funded one of the authors.
Authors contributions
ABA, CP and MDS designed the proposed approach for ontology validation
using natural language questions. CP constructed the OWL translation of the
Disease-Treatment ontology. ABA designed the approach for question
optimization, implemented and evaluated the question generation system for
ontology validation and wrote the first manuscript. All authors contributed to
the manuscript. ABA and JDR designed the proposed approach for mapping
validation. ABA implemented the system for mapping validation using
boolean questions for new mappings and multiple choice questions with
suggested alternatives for invalid mappings. JDS implemented the generation
of candidate concepts from the context of the initial source concept in the
ontology. ABA, JDR and YM evaluated the mapping validation system. All
authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Author details
1Luxembourg Institute of Science and Technology (LIST), Esch-sur-Alzette,
Luxembourg. 2Institute of Computing, University of Campinas, Campinas,
Brazil.
Received: 5 March 2015 Accepted: 22 June 2016
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 
DOI 10.1186/s13326-016-0095-8
RESEARCH Open Access
Informationandorganization inpublic health
institutes: an ontology-based modeling of the
entities in the reception-analysis-report phases
Giandomenico Pozza1* , Stefano Borgo2, Alessandro Oltramari3, Laura Contalbrigo1
and Stefano Marangon1
Abstract
Background: Ontologies are widely used both in the life sciences and in the management of public and private
companies. Typically, the different offices in an organization develop their own models and related ontologies to
capture specific tasks and goals. Although there might be an overall coordination, the use of distinct ontologies can
jeopardize the integration of data across the organization since data sharing and reusability are sensitive to modeling
choices.
Results: The paper provides a study of the entities that are typically found at the reception, analysis and report
phases in public institutes in the life science domain. Ontological considerations and techniques are introduced and
their implementation exemplified by studying the Istituto Zooprofilattico Sperimentale delle Venezie (IZSVe), a public
veterinarian institute with different geographical locations and several laboratories. Different modeling issues are
discussed like the identification and characterization of the main entities in these phases; the classification of the
(types of) data; the clarification of the contexts and the roles of the involved entities. The study is based on a
foundational ontology and shows how it can be extended to a comprehensive and coherent framework comprising
the different institutes roles, processes and data. In particular, it shows how to use notions lying at the borderline
between ontology and applications, like that of knowledge object. The paper aims to help the modeler to understand
the core viewpoint of the organization and to improve data transparency.
Conclusions: The study shows that the entities at play can be analyzed within a single ontological perspective
allowing us to isolate a single ontological framework for the whole organization. This facilitates the development of
coherent representations of the entities and related data, and fosters the use of integrated software for data
management and reasoning across the company.
Keywords: Ontology, DOLCE, Data transparency, Data integration, Knowledge objects
Abbreviations: A, Responsible agent; ASO, Agentive social object; BPMN, Business process model and notation; D,
Information entity; DOLCE, Descriptive ontology for linguistic and cognitive engineering; GIS, Geographic information
system; GUI, Graphical user interface; IZSVe, Istituto Zooprofilattico Sperimentale delle Venezie; ISO/TC, International
Organization for Standardization/Technical Committee; LARU, Reception and public relation laboratory; LIMS,
Laboratory information management system; LOINC, Logical observation identifiers names and codes; M, Material
entity; MDA, Model driven architecture; NPED, Non-physical endurant; KO, Knowledge object; SNOMED CT,
Systematized nomenclature of medicine - clinical terms; UML, Unified modeling language
*Correspondence: gpozza@izsvenezie.it
Equal contributors
1Istituto Zooprifilattico Sperimentale delle Venezie, Viale dellUniversitá, 10,
35020 Legnaro (PD), Italy
Full list of author information is available at the end of the article
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 2 of 15
Background
Ontology
An ontology is the part of the information system that
explicitly commits it to a certain conceptualization of the
world [1, 2].When described in terms of lexical semantics,
ontologies take the simple form of dictionaries or thesauri;
when described in terms of axioms in a logical language,
we talk about formal ontologies; if logical constraints are
encoded in a computational language, formal ontologies
turn into computational ontologies. Finally, an ontology
that concentrates on general and domain-independent
categories is called foundational.
The applied ontology approach has had a huge impact in
all branches of information science. The techniques devel-
oped in the last twenty years are now exploited by compa-
nies around the world in particular in areas like intelligent
interfaces [3], data access [4] and warehouse [5], semantic
web standard [6] and medicine [7, 8].
In the life sciences, ontological techniques are applied
towards a variety of goals [9] among which the study
and organization of areas like genomics [10], anatomy
[11, 12], plant anatomical and morphological struc-
tures [13], phenotype annotation [14], with important
results also in knowledge modeling, organization, integra-
tion and exploitation [8, 15, 16]. Notwithstanding these
achievements, the application of ontological techniques is
still problematic [17].
Our aim in this paper is to show how ontology can
be applied to understand the organization and the activ-
ity of large institutes woking in the life sciences. We use
a public veterinary organization located in north-eastern
Italy, namely the Istituto Zooprofilattico Sperimentale
delle Venezie [18], to clarify our analysis and exemplify
the application of ontological techniques.
The Istituto Zooprofilattico Sperimentale delle Venezie
The Istituto Zooprofilattico Sperimentale delle Venezie
(IZSVe) is an Italian public veterinary institute deputed to
conduct prevention, control, research and services in the
fields of animal health and food safety. The IZSVe belongs
to the Italian National Health Service, is part of a national
network that consists of nine other public veterinary insti-
tutes, employsmore than 600 people (veterinarians, biolo-
gists, chemists, technicians and administration staff ), and
has eleven geographical locations with groups and labo-
ratories devoted to areas like animal welfare, diagnostic
services, food risk communication, geographic informa-
tion systems (GIS), international cooperation, training,
veterinary biobank. The IZSVe carries out routine tests in
disciplines like diagnostics, virology, parasitology, micro-
biology, molecular biology and chemistry. Its research
activities concentrate on the animal health and food safety
fields aiming to develop new diagnostic techniques as well
as vaccines and vaccination procedures.
Limits of standards and software tools
There are many standards centered in aspects relevant to
the reception, analysis and report phases of a public insti-
tute like IZSVe: the Logical Observation Identifiers Names
and Codes (LOINC) [19], the Systematized Nomencla-
ture Of Medicine Clinical Terms (SNOMED CT) [20],
the ISO/TC 212 Clinical laboratory testing and in vitro
diagnostic test systems and others [21]. Also, there are
several languages and conceptual modeling techniques
that the knowledge engineer can use to develop informa-
tion systems, e.g., the Business Process Modeling Nota-
tion (BPMN) [22] or the Unified Modeling Language
(UML) [23] and its related standards like the Model
Driven Architecture (MDA) [24]. However, all these stan-
dards and modeling systems focus on some elements, e.g.
LOINC, or aspects, e.g. BPMN, of the complex scenar-
ios in public institutes like the IZSVe. Even the UML
language, which is perhaps the most broadly applicable,
assumes that the modeler adopts a viewpoint, i.e., has
an understanding of the scenario. How to reliably under-
stand the domain and, more specifically, the scenario
of interest is a goal explicitly addressed by ontological
analysis.
Regarding the analyses carried out in laboratories,
typically a Laboratory Information Management System
(LIMS) is exploited to manage and coordinate the infor-
mation on the tests and the related set of materials and
procedures. Nonetheless, the management of a large set
of laboratories and tests is quite complicated and in many
cases the LIMS is applied to the subset of data that are
homogeneous across laboratories and activities. This is
the situation at the IZSVe, which offers about 950 types of
tests and runs almost 1.7 million tests per year. It has been
recognized that having a partial management via the LIMS
reduces the possibilities to coordinate, monitor and ana-
lyze the institutes activities and the performance. On the
other hand, the adoption of different LIMS allows to take
into account the specificity of each laboratory. However,
fine tuning each LIMS to a specific case jeopardize the
possibility of an integrated data and process management
system.
These problems arise from a substantial lack of a uni-
fying understanding of the institutions scenarios and of
the role that the different elements (objects, tools, data,
personnel) play in its activities. We study this issue look-
ing at the IZSVes use case. The goal is to develop a global
view for a centralized management system and verify the
contribution of ontological techniques in understanding
complex situations.
The ontological approach is also promising in deal-
ing with the information systems evolution. The IZSVe
adopted a state-of-the-art LIMS ten years ago and, since
then, the system have gone through several updates,
revealing a certain lack of flexibility. It is recognized
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 3 of 15
today that the LIMS is unable to evolve with and adapt
to the companys needs. It is unclear whether the prob-
lem lies in the technical and implementation aspects of
this LIMS or in the initial analysis of the institute. Most
likely, it is a combination of both. In any case, to align
the LIMS and the institutes activities, a deep under-
standing of the institute seems necessary, including its
reasons to exist and its strategies. This information is
about what the institute does and why, and should not
be confused with how the institute does it today or at
any other period of time. These are inherently ontological
distinctions.
Ontological analysis
Some of the most relevant issues posited by the scenario
we study are: (a) to identify and characterize the elements
in the institutions activities; (b) to classify the types of
data that are needed and to identify where they are used;
and (c) to make explicit the context and the role of the
involved entities.
To study the IZSVe scenario we adopt an approach
based on a foundational and formal ontology. The use of a
foundational ontology ensures that the principles are not
constrained by the specific domain we work with which,
in turn, leads to an understanding of the institute inde-
pendent from contingent settings (like the organization of
the institute at some point in time or the set of tests it
makes available). This feature makes the resulting model
more flexible. For example, the introduction or modifi-
cation of other laboratory methodologies and techniques,
which requires important changes in traditional rule-
based systems, is managed in an ontological model via
extensions, e.g. by adding (or dropping) classes and their
descriptions. Furthermore, the use of a logic language
with formal semantics allows to check the consistency
of the system well beyond approaches like BPMN and
UML.
In the rest of the paper we use ontological analysis and
a foundational ontology to study the reception, analysis,
and report phases in public institutes in the life science
domain. The goal is to show how to understand the sce-
nario, how to isolate and distinguish the relevant entities,
to indicate their relationships and which roles they play.
The entities we discuss, like specimens, reports, sample
seals, laboratory tools, data sets, administrative stuff and
so on, will be classified according to their ontological
types.
One important result of this work is the development
of a single framework where it is possible integrating
all the discussed entities, data and roles. Working with
a single model helps also to evaluate the coverage of
the domain, to check the coherence across elements and
phases, and to facilitate maintenance. Finally, as pointed
out earlier, by using a foundational ontology we expect
that the model we obtain will remain valid across time,
provided the essential constraints remain unchanged (e.g.,
in the scenario, the law defining the IZSVes institutional
goals).
Methods
In this part we first introduce the ontological analysis
approach and give some indication on how its applica-
tion can be evaluated. Then, we use aspects of the IZSVe
scenario to introduce the rationale, the basic structure
and some categories of the DOLCE foundational ontology.
Later, in the next section, we will use ontological analysis
to discuss typical modeling issues taken from our sce-
nario, and will use the results of this analysis to expand the
DOLCE ontology to an ontology for public institutes in the
life sciences.
Ontological analysis and its assessment
Generally speaking, it is important to distinguish how
things occur in the activities and what are the expectations
we have about them. Ontological analysis helps to make
subtle, yet crucial, distinctions. For example, ontological
considerations lead to separate the seal of a specimen con-
tainer as a signal of integrity (a role) from the seal as an
artifact (an object). Once we have identified the entities
in a scenario, we need to organize them in a hierarchy
and to establish their relationships (e.g. it is the object-
seal that plays the role-signal of integrity, and stops when
broken), and to ensure that the hierarchy leaves space for
future extensions. After all, a finer analysis may lead to
introduce new entities, and one may want to expand this
very hierarchy on aspects not yet considered like new pro-
cedures, safety regulations, responsibilities or workload
management.
Usually, a model presents a perspective. It can present
the scenario from the perspective of the service user,
from that of the overall organization, or from that of a
sample to be analyzed. Since all the key elements that
characterize these different tasks and views could be orga-
nized within an ontological framework, we aim to show
that it is indeed possible to construct such a general
framework. This framework provides a conceptual sys-
tem that comprises the different perspectives and, thus, is
suitable for tasks as different as data management, qual-
ity assessment and responsibility tracking. Our work is
based on techniques that have been developed from the
90s to guide the development of robust ontologies, see
e.g. [2531].
Being a conceptual tool, ontological analysis is not suit-
able for quantitative evaluation. However, there are differ-
ent qualitative parameters to assess its results, e.g. [32].
We will apply them to evaluate the results we obtain in
studying our guiding scenario (see Sections Devising the
ontology and Framework Evaluation).
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 4 of 15
The IZSVe scenario
All the IZSVe laboratories activities related to sample
receiving, delivering and transferring are coordinated by
a centralized delivery service, the Reception and Public
Relation Laboratory (LARU), located at the headquar-
ters. All the information on the samples and the analyt-
ical processes (like tracking the samples distribution and
managing the analysis procedures) are registered in the
dedicated LIMS, called IZILAB. IZILAB manages also data
unnecessary for the tests, e.g., information about involved
parties (sample deliverer, sample owner, sample collector,
etc.), reason of investigation, submission form number,
breeding or food processing plant where the sample was
collected. Data are collected from different sources from
human operators to automatic access to, e.g, the farm
registry database.
The rest of this section presents a typical IZSVe sce-
nario. To keep the presentation simple, the description
focuses on some interesting parts of the overall sys-
tem. Later we will refer to this scenario as our guiding
example.
A qualified technician, personally or via a delivery ser-
vice, delivers a sample  e.g., a pathological specimen from
alive or dead animals or food for human consumption 
to a IZSVe reception point requesting to test the speci-
men on some characteristics, e.g. microbiological safety.
The reception unit personnel collects the sample, the sub-
mission form with the required analysis and performs
some preliminary check like the presence and integrity
of sample seals; the samples storage temperature (when
needed); the match between the declared number of sam-
ples (or units) and the delivered items; the presence of
the requested analysis in the list of services provided by
IZSVe.
The reception unit may perform more specific checks
to verify, for instance, that: the needed information is
reported in the submission form; the form lists any spe-
cial management constraint (e.g. the analysis might be
requested at a fixed date and time for the participation
of external observers); the sample has been correctly col-
lected and preserved for the requested analysis type (e.g.
storage temperature; timing of the analysis).
After the checks, the sample is registered in the IZILAB
and unique identification labels are physically attached to
the sample container and to the analysis submission form.
A receipt with the identification number is released to the
deliverer. From this point on, the IZSVe is fully responsi-
ble of the sample management and all the data needed for
the IZSVe internal procedures are made accessible to the
operators via the IZILAB.
Next, the sample is stored in a ward (storage room,
cooler, freezer) to be distributed to the laboratories. The
registration data, called batch, is added to the IZILABs
batch-list of the ward (a kind of loading/unloading
register). The sample is then collected by laboratory per-
sonnel or delivered via the IZSVe service. At the lab-
oratory, the administrative and technical staff make a
final assessment on the suitability of the sample for the
requested analysis: the documents and the compliance of
the specimen to the test requirements are verified. Then,
the seals are broken and direct inspection of the sample
content can be done. The laboratory personnel complete
the data via a dedicated interface in IZILAB: the batch
code ensures that data are added to the right record as
well as the consistency of the sample tracking informa-
tion. Some fields are shown in the IZILAB labs reception
GUI - Graphical User Interface -(Fig. 1). The number
of external acceptance and the date of external accep-
tance are needed to coordinate further tests, if any, run
by other IZSVe Laboratories. The flag delivery charge
activates the fields for delivery charges. The flag identi-
fication of the payer indicates the party charged for the
procedure costs. The rules for test assignment provides
information on the acquisition of digital data while the
first/sequence is needed when there are several samples
and/or several analyses are done on the same specimen.
Once the data are uploaded in IZILAB, a working paper
(lab sheet) is prepared where the lab technicians report
the steps of the analysis process. At the end of the anal-
ysis, the administrative assistant uploads into IZILAB the
rough data and the Laboratory Head checks whether fur-
ther tests are needed, e.g. for verification. If s/he decides
for further tests in a different laboratory, the process
(transferring and analysis) is repeated as before but the
batch-list (loading/unloading) is now linked to a wait-
ing acceptance list devoted to samples moving from lab
to lab. Once the analyses are completed and all data
are collected in IZILAB, the report is produced by the
administration and the Laboratory Head digitally signs it.
The file is then made accessible (in full or limited form)
through the web to the parties that have the right to
access it.
Devising the ontology
Research in applied ontology has devised a series of logic-
based relations and categories that furnish the starting
point for entity analysis and classification. The idea is to
explicitly list all the types of entities that are taken to exist
(or at least to be of relevance) in the application domain
and to classify each specific item as belonging to one sin-
gle category. Further constraints help to enforce the right
use of the hierarchy.
In this paper we adopt the foundational ontology
DOLCE [33] with some extensions relative to the cat-
egories of roles and descriptions as presented in [34].
DOLCE is a foundational and formal ontology developed
from cognitive and linguistic considerations and with par-
ticular emphasis on social reality (Fig. 2). Our choice of
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 5 of 15
Fig. 1 GUI at the labs reception. Graphical user interface (GUI) translated, original in Italian
DOLCE relies on a few observations: DOLCEs underly-
ing principles and construction techniques have been well
described [33] and there is evidence that this ontology is
preferred even by non trained users [15], the ontology is
available in different formalisms [35], it is stable and sev-
eral extensions are available, e.g. social roles [36], artifacts
and products [37] and mental states [38]. Furthermore,
the ontology has been verified in terms of ontological and
logical soundness [39, 40].
Here we describe the parts of the ontology structure,
and some of the basic relations, that are needed to under-
stand the material in this paper. The presentation is
minimal, the user can find a complete introduction in
[33, 34]. DOLCE is based on the basic distinction between
objects, events and qualities. Other important categories
are those of descriptions and roles. Objects (DOLCE cate-
gory: Endurant) are entities that are mainly identified by
their relationship with space while they persist in time.
People, samples, laboratory equipments as well as the cav-
ity of a specimen container are classified as objects in
DOLCE. Events (DOLCE category: Perdurant) form a dif-
ferent category; these are things that necessarily happen
in time like the (process of ) delivering a sample or running
a laboratory test. The category Quality gathers individual
properties, i.e. properties associated to a specific object
or a specific event, and that serve to qualify them: each
sample has its own specific weight and temperature, each
instance of a laboratory test has its own duration etc.
Qualities can be simple like weight, temperature and dura-
tion; or complex like price, frequency and speed. The cat-
egory Description collects information entities, like pro-
cedure specifications, and the category Social Concepts
entities like the Italian and the European legal notions of
organization. We anticipate that members of the last two
categories are generally seen as temporal objects (they live
and change in time) but for practical reasons they are here
treated as atemporal entities. Thus, here we ignore that
the society, the laws and the concepts evolve in time. This
choice will become clear in Section Roles and players in
the IZSVe scenario. Descriptions and social concepts are
distinguished in the ontology from their physical supports
and realizations: the EU legislation on the control of the
notifiable disease of livestock (e.g. Foot and Mouth Dis-
ease) is ontologically a concept, thus a different kind of
entity than the document where it is written (which is a
physical object), and the application of the regulation is
still another kind of entity, namely an event. This event is
the instantiation of a procedure, which is a description. A
further class, Role, collects properties that identify a tem-
poral status of an entity usually dependent on some social
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 6 of 15
Particular
(PT)
Abstract
(AB)
Endurant
(ED)
Non-physical
endurant 
(NPED)
Perdurant
(PD)
Eventive
(EV)
Stative
(STV)
Quality
(Q)
Physical
quality (PQ)
Temporal
quality (TQ)
....
Description
(DS)
....
Social
concept
(SC)
Non agentive
ph. obj.
(NAPO)
Agentive 
ph. obj.
(APO)
Agentive
social obj.
(ASO)
Non agentive 
social obj.
(NASO)
....
Role
(RL)
Legal Concept
 (LC)
....
....
Accomplishment
(ACC)
Achievement
(ACH)
State
(ST)
Process
(PRO)
Fig. 2 Category hierarchy of the DOLCE ontology. DOLCE fragment, from [33], with an extension of the social object category (gray boxes). Arrows
represent ISA relationships and dotted arrows chains of ISA
contexts [36, 41, 42]. For example, an individual agent may
play the role of IZSVes Laboratory Technician in some
period of time. Similarly, a physical object can play the role
of a Sample or of a Seal within a IZSVe activity (which pro-
vides the context). A fragment of the DOLCE categories,
with the mentioned extensions, is presented in Fig. 2.
Our goal in the rest of this paper is to expand the
DOLCE framework with new categories tuned to the mod-
eling issues elicited from scenarios like that of IZSVe.
This activity is known as domain adaptation [43] and
aims to cover the notions characterizing the application
domain.
The relations among the categories must also be fixed.
Structural relationships, like subsumption (aka ISA),
instantiation and parthood [29] are already part of the
DOLCE language. Subsumption is the subclass relation: for
instance, the category of Agentive social object (ASO) is
subsumed by the category of Non-physical object (NPED),
Fig. 2. This means that any ASO entity, e.g. a company, is
also an NPED entity. Similarly, the IZSVe category Sub-
analysis is subsumed by the IZSVe category Analysis: any
IZSVe sub-analysis is also a IZSVe analysis. The instanti-
ation relation applies to a class and an individual, it states
that the individual is an instance of the class. As an exam-
ple, the object identified by code PD/A123 is an instance
of the category Sample. Note that, being this latter cate-
gory a subclass of Endurant, the object identified by code
PD/A123 is also an instance of the category Endurant.
This result is a consequence of the formal interaction
between the instantiation and the subsumption relations.
Finally, the parthood relation is used to state that an entity
is part of another entity, e.g. report X of laboratory A can
be part of report Y of laboratory B (say, when the first
reports about a related analysis on the same specimen).
The example needs clarification: report X, in the sense
of a description (a collection of data, thus an information
entity), is part of report Y , also understood as a descrip-
tion. On the other hand, report X in the sense of a piece
of paper (or an electronic file) is part of report Y pro-
vided now we understand even Y in the sense of a piece
of paper (or an electronic file). The ontology tells us that
no other combination of these senses holds. The use of
a formal language, as in DOLCE, helps to keep these two
parthood statements apart and to correctly relate them
the right meaning of the terms. Similarly, parthood can
be used to state that an instantiation (a specific event) of
procedure A (a type of event) is obtained by the (mere-
ological) sum of instances of procedures B1, B2 etc. We
refer the reader to [33] and [29] for further details on these
relations.
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 7 of 15
Results and discussion
In this part we exemplify the use of ontological analy-
sis via the IZSVe scenario that leads to the classification
in Table 1, introduce other relevant notions and then
evaluate the results. We pay particular attention on the
selection and description of the static elements and in
particular on roles and dependencies across entities. The
discussion of events and their interrelationships aims to
highlight the variety of distinctions that can be made.
A comprehensive analysis of events from the ontological
viewpoint is out of the scope of this paper.
Categories and participants: a look at the IZSVe scenario
 Endurant. The objects are grouped via new
subcategories of the DOLCE category Endurant. We
introduce categories of documents like the analysis
result form and the analysis result (an analysis result
form is a document suitable to list the data obtained
from an analysis procedure, the analysis result is the
document filled out with the data), the analysis
request form and the analysis request, the registration
form and the test report. Other objects are person,
organization, agent, laboratory tool, laboratory
material, container, building etc.
 Perdurant. The new types of event are subcategories
of the DOLCE category Perdurant. We focus in
particular on perdurants identifying activities like:
running a laboratory test; disposing, storing,
transferring and delivering a sample; preparing,
signing and delivering a test report; requesting an
analysis; breaking a seal etc. Among these, activities
like running a laboratory test and disposing a sample,
have a clear ending point (these activities are called
accomplishment in DOLCE). Others, like storing a
sample, do not and are called states.
 Quality. Qualities in the scenario are divided into a
variety of subcategories. Beside the usual qualities like
weight, shape, duration, speed etc. we model also
accuracy, reproducibility and repeatability of the tests,
price and priority as individual or relational qualities.
 Description. The category Description is a
subcategory of Non-agentive social object and
collects social entities that are neither agentive nor
material. These are information objects that serve as
classifiers for other entities. Typical examples are
descriptions of a tool or a specimen (descriptions of
endurants, for instance the data associated to a
specimen), descriptions of specific processes or
actions (descriptions of perdurants, for instance a
description of how specimen X was delivered to
laboratory Y ) and descriptions of methods
(descriptions of rules and other constraints, for
Table 1 IZSVe elements in our guiding example
Cooler Analysis report Batch-list
Laboratory equipment Freezer IZILAB
IZSVe Laboratory report Laboratory room
Receipt Sample Sample reception point
Sample label Seal Submission form
Storage room Working sheet
Waiting the acceptance list issue Awarding of the batch number Booking of additional tests
Checking the rough data Checking the sample Colleting the sample
Filling the submission form Delivering samples to the laboratory Delivering the sample to the LARU
Issuing the receipt Editing the batch list Editing the report
Signing the test report Breaking the seals Recording the data
Registrating the sample Requesting the analysis
Storage temperature Accuracy of a test Cost of a test
Priority of a test Duration of a test Integrity of seal
Sample temperature Repeatability of a test Reproducibility of a test
Number of aliquots of a sample Number of steps in a procedure
Working sheet content Analytical method description LARU procedure description
Laboratory procedure description Laboratory result for a sample European (national etc.) procedures
Sample Owner Administrative Assistant Deliverer
Requirer Frontdesk Operator Report Receiver
Head of Laboratory Laboratory Technician Qualified Technician
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 8 of 15
instance the description of the procedure for
specimen delivery).
 Social Concept. This is a subcategory of Non-agentive
social object closely related to the previous (see
Fig. 2). It is populated with information objects that
acquire social relevance. Here we find the content of
official regulations and laws, or the data produced by
an official test (these data, legally binding or not, give
always a description in the sense of the previous
category). Other descriptions with a binding social
value, like the official description of a test procedure,
are in this category. Two subcategories of Social
Concept are particularly relevant to our work: the
Role and the Legal Concept categories.
 Role. We focus on two types of roles: agent roles,
played by agentive entities only (typically humans or
organizations), and functional roles played generically
by non-agentive physical objects. Among the agent
roles there are laboratory technician and laboratory
head, sample deliverer and analysis payer. Among the
functional roles, we have specimen, reagent, seal and
reference material (note the distinction between the
material, typically an amount of matter or an artifact,
and its role in a procedure).
 Legal Concept. This category is here introduced
following studies in the legal domain [44]. It collects
European, national, regional and internal laws and
regulations intended as sets of normative
specifications, that is, not mere descriptions nor
generic social concepts.
 Abstract. The category of abstract entities does not
seem relevant in this context. We mention it just to
remind the reader that it serves to classify entities like
numbers and quality spaces [33].
These are the most relevant categories in our specific
scenario. More specialized categories are discussed later
in this section. Although the overall scenario is really
rich, these categories exemplify all the issues we found
and suffice to clarify the key modeling choices in this
scenario.
The notion of knowledge object
Due to historical, legacy and business factors, organi-
zations may adopt special perspectives which are not
ontologically justified [45]. For instance, the IZSVe con-
siders a type of object that is actually a mix of three
DOLCE ontological types: a material entity (e.g. a sample),
an information entity (the sample information in IZILAB)
and an agent role (e.g. the personnel in charge of the sam-
ple). This mix defines an official sample which is a crucial
element in the institutes legal activities. This situation
is fairly common across companies although the charac-
teristics of these entities may change considerably. Since
these entities are important for the organization and are
not ontologically consistent (an ontological entity cannot
be member of disjoint categories), to model them we use
a specific methodology [46, 47] allowing to introduce a
new type of objects called knowledge objects. The exten-
sion is not necessary for the organizations information
(the ontology suffices for this goal) but helps to include in
the model these special perspectives of the organization.
Let us see how knowledge objects can be modeled by
studying the IZSVe case. Given the above description, an
IZSVe knowledge object KO comprises a material entity
M, an information entity D and an agent role A. Then,
the object KO is the triple (M,D,A) such that at time t,
the element D of KO collects the information that IZSVe
has at that time t on M and the element A is an IZSVe
role responsible of the status of and the changes in M
and D at that time. Typically, the M is a physical object
or a quantity of matter officially delivered to IZSVe for
analysis. The entity D is created when at the time of the
M reception an IZSVe operator creates the data record
about M in the IZILAB. Thus, D is the information object
relative to M stored in the IZILAB database and regu-
larly updated during the IZSVes procedures on M. A is
the person responsible of the service or laboratory that is
managingM.
Since knowledge objects are not part of the ontology,
they should be seen as auxiliary elements of the informa-
tion system. They evolve in agreement with their ontolog-
ical components (M, D and A). A sketch of the changes
that a knowledge object KO undergoes from its status at
the reception time t, given by (M,D,A), to its status at the
time t? in which the laboratory report is written, indicated
by (M?,D?,A?), is shown in Fig. 3. The figure highlights
three temporal points t, t? and t??, and three phases (time
is oriented top-down): specimen check, specimen analysis
and report writing. The solid arrow on the left is marked
by events relative to changes in the material component
M, the solid arrow in the center is marked by events rel-
ative to the information component D, and the one on
the right to the agentive component A. KO, as a whole,
has also its own specific properties. We say that a KO =
(M,D,A) is completewheneverD contains all information
relative toM that are of value for the IZSVes activities and
institutional goals. Furthermore, the procedure to manage
the KO is said to preserve correctness if, given that the data
inD relative to some property ofM at a time t is true, then
at any later time t? D contains only true data relatively to
that property ofM.
Recall that, due to the introduction of knowledge
objects, the model we are building is not purely ontolog-
ical. The reason is that these knowledge objects, being
a combination of elements from disjoint categories, rely
on a combination of properties that is incompatible with
the assumptions of the ontology. In the ontology M, D
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 9 of 15
AGENT ROLE
(A)
INFORMATION
(D)
SPECIMEN
(M)
knowledge object K corresponding 
to (M',D',A') at lab report time
knowledge object K corresponding 
to (M,D,A)  at lab reception time
TIME
t
analysis
procedure
starts
 pre-analysis 
data update
specimen 
check ends
M       D       A  
M'      D'        A'  
t'  
specimen is 
acted upon
 actions and 
data are 
reported
specimen
check
specimen
analysis
report
writing 
t''  
analysis 
procedure 
ends
ad
mi
nis
tra
tiv
e
lab technician
lab technician
Fig. 3 Tracing the changes in IZSVes knowledge objects. Possible changes that a knowledge object undergoes from (M,D, A) to (M? ,D? , A?) within
the IZSVe scenario
and A are separated elements interconnected via depen-
dency relations (D contains information about M, A is
responsible for D and M etc.). The corresponding knowl-
edge object KO is ontologically understood as a set of
cross-categorical constraints. If there is a need to include
knowledge objects within the ontology itself, one can
apply the reification technique presented in, e.g, [30].
Processes in the IZSVe scenario
The categories previously discussed allow to talk about the
organizations activities, the flow of information in the dif-
ferent phases and the participating roles. Ontology can be
extended to model finer information.
For instance, take the flowchart (ontologically this is
a description) of the IZSVe laboratory analysis phase
(Fig. 4), indicating the roles of the lab technician and the
lab head. The figure takes the viewpoint of the sample;
the graph lists the set of actions following the temporal
order and some causality constraint.
This graphical representation is weak from the onto-
logical perspective since it hides distinctions that can be
important for, say, management tasks. Ontology analysis
tells us that a generic event is a bundle of more spe-
cific events: the stable properties (these identify a state
in DOLCE); the dynamic regularities (a process in the
DOLCEs terminology); the evolving conditions that lead
to complete the activity (an accomplishment in DOLCE);
the key transition moments (an achievement in DOLCE).
Thus, ontologically an activity like the data entry into
LIMS (see Fig. 4) comprises several types of events that
we might want to distinguish, for instance: the continu-
ous relationship between the hardware, the software, the
work location, the data and the operator (state); the evolv-
ing input/output interactions between the operator, the
database, the program and the hardware (process); the
steps that lead to the update of the information stored
in the database (accomplishment); and the event of the
instant in which the new data is physically recorded in the
database (achievement).
Roles and players in the IZSVe scenario
The role category is characterized by dynamicity (one can
play a role just temporarily), anti-rigidity (playing or not
a role does not change the entitys ontological status) and
relational dependency (a role depends on external defini-
tions, its context) [33]. Other views exist: [48] separates
the role hierarchy from the category hierarchy so that a
(specific) student depends on a (specific) person but is
itself not a person; [49] sees roles as realizable dependent
entities that fully exist only while they are played; and [42]
takes the relationship role-context as primary.
Notwithstanding the differences, to model the social
reality we need roles. Agent and actors are different
things: an agent is an entity that can act, an actor is
an agent whose acts acquire a social value. There are
also roles played by non-agentive entities: a quantity of
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 10 of 15
Fig. 4 Partial flowchart of the IZSVes scenario - Laboratory. Laboratory technician and head views only
biological material can play the role of a specimen (Fig. 5).
On the other hand, not all the distinctions are equally rel-
evant. For instance, do we need to distinguish between
a specimen container (the artifact) and that very object
when used as a container in a laboratory (the role)? Onto-
logically, these things are kept apart and, yet, in the mod-
eling context the distinction is negligible. Thus, we suggest
to have the two notions in the ontology but organize the
information system in such a way that it uses only one
(typically, the role category). Unfortunately, we lack more
precise guidelines on this issue (see also Section Func-
tional roles).
Agent roles
We separate roles that act for the company, in our scenario
these are called IZSVe internal roles, from the others, here
called IZSVe external roles (non-IZSVe roles, for short),
see Fig. 5.
The internal roles are components of the company
and are played by its personnel: all these roles must
be played by individual agents and are thus marked
by I in Fig. 5. The scenario involves six internal
roles, namely, Health Director, Head of Laboratory, Head
of the Reception Service, Laboratory Technician, Front-
desk Operator and Administrative Assistant. The rela-
tionship of supervision holding among roles is quite
standard in todays social organizations and we do
not discuss it further (see Supervision hierarchy in
Fig. 6).
The non-IZSVe roles are mentioned in the companys
procedures but not structured within its organization,
namely: Sample Owner, Submitter, Payer, Report Receiver,
Deliverer and Requirer (Fig. 5). We find that another role,
here calledQualified Technician, needs to be added to the
list although it is never introduced in the IZSVe scenario.
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 11 of 15
Role
Function
role
Reagent
role
Seal
role
Reference
material 
role
Specimen
role
Sample
Owner
Submitter (I) Payer
Report
Receiver
Deliverer Requirer
Tecnician (I)
Frontdesk
Operator (I)
Head of the 
Reception 
Service (I)
Administrative
Assistant (I)
Laboratory
Technician (I)
Health
Director (I)
Head of the
Laboratory (I)
Agent
role
IZSVe 
Agent role
non-IZSVe 
Agent role
...
...
...
...
Fig. 5 Some roles in the IZSVe scenario. Major roles of the scenario in Section Methods, I indicates individual roles
A Qualified Technician is a role that can be played only
by individuals with a specific degree, e.g. a veterinary title,
and registered in a dedicated public repository. This role
is needed to justify the social power of the Submitter role.
Also, the Submitter and Qualified Technician are the only
non-IZSVe roles that must be performed by an individ-
ual agent. In all remaining cases the player can be an
individual agent or an organization. Four of these roles,
namely Submitter, Report Receiver, Payer and Deliverer,
are the interfaces between IZSVe and the external social
system: the Submitter makes the official analysis request
and thus triggers the IZSVe activity; the Report Receiver is
the receiver of the service report, the Payer is needed for
the economic sustainability of the service, and the Deliv-
erer (which physically delivers the sample to the IZSVe
reception site) takes care of the physical interactions. It
is interesting to note that the Sample Owner role, the
only non-IZSVe role remaining, is not a figure that has a
direct relationship with the IZSVe. This is also seen by the
fact that its involvement is not motivated in the scenario.
These specific cases are due to the institutional goals of
the IZSVe: to collect data on the regional territory and
to anticipate possible problematic situations. When there
is a suspect of potential food contamination or presence
Fig. 6 Constraints on agent roles in the IZSVe scenario. The supervision relation among the internal roles is standard
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 12 of 15
of a disease, knowledge of the Sample Owner allows fast
reactions by the authorities.
Differently from the internal roles, data on non-IZSVe
roles might be missing from IZILAB. This lack of infor-
mation is a factual issue, not a modeling problem. The
IZILAB system requires explicit knowledge only of some
roles: Submitter, Report Receiver and Payer. For exam-
ple, the Deliverer role is marked optional in the IZILAB
GUI. Since the analysis request form is delivered together
with the sample, the information about the Deliverer
was considered marginal. This, however, shows that the
IZILAB designer did not have an integrated view of the
goals/duties of the different roles. As of today, nothing
prevents the (player of the role) Sample Owner to play
the Deliverer role as well. But the two roles may have
conflicting interests: the Submitter trusts the Deliverer to
correctly manage the sample delivery so that it can be cor-
rectly tested. Yet, the Sample Owner could be interested
in altering the sample (e.g. not following storing require-
ments) to prevent the possibility to test it correctly. Our
analysis suggested to better model the roles interrela-
tions so to prevent these cases, possibly enriching existing
guidelines.
Our analysis shows that all IZSVe external roles are
mutually independent with two exceptions. (1) The Sub-
mitter role must also play the Qualified Technician role,
as we have already discussed. (2) The Report Receiver role
must be played by the player of either the Requirer or the
Submitter.
Functional roles
This is the other subcategory of roles we deal with in
Fig. 5. Here it is important to understand the artifactual
and the contextual status of objects. A laboratory tool is
an artifact manufactured to realize some functionality, a
specimen is a quantity of (natural or artificial) material
selected as representative of some substance or object.
A general approach for artifactual entities and their roles
is presented in [42] where one can model a lab container
when used as such or when used as, say, a pencil case.
Indeed, the lab container artifact may play a role for which
it was not produced. Unfortunately, this approach is based
on the notion of context which is hard to model [50].
The study of functional roles in the IZSVe scenario leads
to many subtle distinctions, which may be sensitive to the
granular level of the description [51]. We model the arti-
factual status of the entities via the notion of ontological
artifact [37] and technical artifact [52]. (Alternative onto-
logical views, e.g. [53], could be easily adopted.) Entities
like equipment and laboratory material, are always playing
the intended role in this model since the IZSVe organiza-
tion and its scenario are quite rigid on this. Note however
that where different levels of granularity are needed, one
should not simplify the model in this way. For instance,
we classify specimens, seals and reagents as roles (Fig. 5)
but treat laboratory tools and containers as endurants, see
Section Categories and participants: a look at the IZSVe
scenario.
Of course, a specimen is an artifact in the sense of [37]
since it has been intentionally selected and it has the
(intentionally attributed) capacity to provide information
about the whole material from which it is extracted.
However, in the IZSVe procedures specimens may have
different status. This happens in particular when the
dependencies between the components of the corre-
sponding knowledge object (section The notion of know-
ledge object) are broken, e.g. when a wrong or incorrect
procedure is applied or the responsibility chain is broken.
In these cases, the specimen looses its official or legal
status. To make room for this change, we include both the
specimen as an artifact and the specimen as a role. Sim-
ilarly, when one entity enters in the scenario as a seal, it
does so to guarantee the integrity of the container. Once
the seal is broken, the seal looses its role and thus changes
its status, it is still a seal from the artifactual perspective
but it is not sealing anymore. Similar arguments apply to
the modeling of reference material and of reagent (before
and after their use). In contrast, entities like a registra-
tion form, a test report and a laboratory tool (in the sense
of non-consumable tool like a microscope) maintain their
status throughout the IZSVe activities independently on
what happens. Note that this leaves out from the model
events that destroy the objects functionalities (beyond
malfunctioning).
Framework Evaluation
The evaluation of an ontology developed for applica-
tion scenarios is still largely debated in the literature
[32, 5456]. Among the different criteria listed in [32],
our work aims to: (a) reach an agreement about meanings
of terms in a vocabulary, (b) provide a uniform view to
facilitate data integration across distributed sources, and
(c) develop a formal model that allows automatic verifica-
tion of its own consistency and accuracy. According to the
analysis in [32], the use of foundational ontology increases
coherence and interoperability; the provision of unam-
biguous and formal documentation increases coherence
and clarity; the provision of machine-readable documen-
tation allows for automated data processing, automated
knowledge- and data-integration, semantic integration;
consistency verification helps to detect modeling errors
and increase data coherence. Since we focus on mod-
eling methodologies and ontological analysis in existing
complex scenario, we will concentrate on the conceptual
analysis, reusability and consistency criteria. The model
is in the design phase and has not been implemented.
User feedback is limited and restricted to people that
have been involved in the scenario analysis or interviewed
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 13 of 15
to describe the IZSVe phases. Unfortunately, these data
are limited and not suitable for evaluation methods like
statistical analysis.
Following todays practice [32], we listed five parame-
ters to evaluate the result of the application of ontological
analysis, namely: generality, openess, flexibility, coherence
and consistency.
1. Generality. As discussed in Section Devising the
ontology, we started from an existing ontology,
DOLCE, which has been deployed in domains as
different as engineering, finance, fishery and medical
image analysis. This previous experience indicates
that DOLCE is comprehensive, conceptually sound
and not focused on any particular perspective of the
scenario. Also, other independent evaluations, e.g.
[39], established that DOLCE is comprehensive and
suited for mesoscopic entities, that is, commonsense
entities cast by human reasoning and language, and
that are at the center of our social environment.
In extending the ontology to cover the IZSVe
domain we have followed the DOLCE principles and
construction methodology. This approach ensures
two things: no new restriction is added to the system,
and the different areas (data management, laboratory
conduct, responsibility hierarchy, resource
administration etc.) are or can be included in the
ontology. Since the new categories have an auxiliary
role and do not form partitions, our extension
inherits the generality of DOLCE and preserves it.
2. Openness. The proposed extension of DOLCE
models domain notions like specimen, laboratory
tool and method description. This is obtained by
introducing specialized categories and by populating
them without introducing new partitions or cross
categorial constraints on DOLCE itself. It follows that
further categories can be added at each taxonomical
level of our extension. For instance, a new category
collecting the roles related to some other process
(e.g., contract management) can be added to the
IZSVe roles without having to revise the ontological
system already developed. This design choice ensures
that the system is open to revisions and further
extensions.
3. Flexibility. Flexibility is obtained by balancing
ontological assumptions and formal constraints. Our
extension adds a new set of domain-dependent
properties to characterize the new categories. In
some cases, e.g. for knowledge objects (Section The
notion of knowledge object), we departed from the
DOLCE perspective and applied a methodology to
model entities not classified by the ontology. Our
approach ensures that this new kind of information is
managed in the ontology as a set of requirements or
dependencies. This choice allowed us to mediate
between the strict ontological sieve and the more
permissive attitude one has in applications. Finally,
the constraints introduced via knowledge objects are
local in the sense that they apply to only those
entities that are connected via the notion of
knowledge object. As such, these constraints do not
limit the ontology itself.
4. Coherence. A foundational ontology like DOLCE
provides a unifying view within which one can
identify and classify every entity in the domain of
study. This allowed us to develop a classification of
the entities and roles in the scenario without
committing to a specific perspective, and to
reconstruct the perspective of, say, a lab technician
by looking at its role definition, its associated goals
and the activities it performs. The fact that these
views are obtained by extracting information within a
single formal ontology ensures that these views are
coherent (they logically co-exist and do not
contradict each other) and aligned in the sense that
they relate to the same set of integrated events.
5. Consistency. As observed in Section Devising the
ontology, consistency is ensured at development time
by applying ontological analysis to identify the
needed categories and by modeling them in the
formal language of DOLCE. At run time conceptual
consistency is preserved because of the clear criteria
for classifying the entities in the ontology.
Technically, logical consistency is achieved by the
computable versions of the ontology, like that in the
computational language OWL [6]. There are
software environments for managing OWL
ontologies, such as Protègè [57]. OWL also supports
efficient reasoning so that ontology consistency at
run time can be ensured by state-of-the-art
automatic inference engines, such as Pellet (freely
available for most of the ontology tools [58] and
Racer (highly customizable proprietary system) [59].
Novelties, Impact and Limitations of the study
In this study we showed how to use ontological analysis
to develop an ontological model for public institutes in
the life sciences. Differently from the literature, we fol-
lowed a principled top-down approach by starting from a
foundational ontology and expanding it via an ontological
discussions of the domain. Typically, the opposite is done:
one starts from a domain model and aligns it to a foun-
dational ontology. This standard strategy may improve
the interoperability of the existing models but does not
increase our understanding of the domain nor introduces
more flexibility. Instead, we obtained a model rich of new
distinctions and that can be used to reason from different
perspectives.
Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 14 of 15
Among the advantages of our ontological model, we
recall that it has a rich role hierarchy useful to highlight
conflicting goals and other dependencies, distinguishes
physical objects from their descriptions and their social
status, allows multiple views on single processes and
makes space for modeling hybrid elements as we showed
with knowledge objects.
It is too early to talk about the implementation of this
ontological model in the IZSVe information system. The
number of required changes is considerable also because
ontological models lead to important changes from the
data management viewpoint. This is a relevant limitation
of our work since actual capacities and advantages can be
established only when the system is practically exploited.
At the moment, the gained deep understanding of the
domain is the most important result we can report about.
Future work
The work in this paper concentrated mainly on the study
of objects and roles in the management of samples and
related data from the reception to the release of the anal-
ysis report. This part of the model needs to be better
connected to the analysis of the processes which is still
ongoing.
In the future we need to evaluate which parts of the
new model are more complex to implement and disrup-
tive with respect to the existing information system. Also,
the changes suggested by the new model will likely trigger
new requirements and services, which should be evalu-
ated beforehand. Finally, we need to understand how to
modularize the model in order to reduce development
concerns and to optimize software implementation.
Conclusions
In this paper we presented elements of a wide-range analy-
sis of processes, data and roles in a large public institute in
the life sciences. We showed how to perform an ontologi-
cal analysis of the domain, what distinctions it highlights,
and how to model them in an ontology. This principled
approach led us to define a series of notions that together
cover a large variety of data, procedures and objects; these
are indicative of the complexity of real-life organizations
and of the capacity of ontological models approaches to
model them.
The result of this work is an ontological framework,
technically an extension of DOLCE, which is tuned to the
IZSVe scenario.
Finally, we have also exemplified the use of flexible con-
ceptual techniques, via the notion of knowledge object,
which help to reconstruct the organizations perspective
within the ontological viewpoint.
Acknowledgements
This paper contributes to the IZSVE RC 19/08 project and to the IZSVe RC
16/09 project, funded by the Italian Ministero del Lavoro, della Salute e delle
Politiche sociali.
Funding
Italian Ministero del Lavoro, della Salute e delle Politiche sociali: Projects IZSVe
RC 19/08 and IZSVe RC 16/09.
Availability of data andmaterials
Not applicable.
Authors contributions
Topic and scenario: GP and SM. Performed analysis: SB, GP and LC. Developed
the ontology: SB and AO. Validated the results: SB, GP, LC, SM. Wrote the paper:
SB, GP, AO, SM, LC. All authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Author details
1Istituto Zooprifilattico Sperimentale delle Venezie, Viale dellUniversitá, 10,
35020 Legnaro (PD), Italy. 2Laboratory for Applied Ontology (LOA), ISTC CNR,
Via alla Cascata 56/C - Povo, 38100 Trento, Italy. 3Bosch Research and
Technology Center, 2555 Smallman Street, 15222 Pittsburgh, Pennsylvania,
USA.
Received: 9 August 2016 Accepted: 24 August 2016
Jupp et al. Journal of Biomedical Semantics  (2016) 7:17 
DOI 10.1186/s13326-016-0055-3
SOFTWARE Open Access
Webulous and the Webulous Google
Add-On - a web service and application for
ontology building from templates
Simon Jupp* , Tony Burdett, Danielle Welter, Sirarat Sarntivijai, Helen Parkinson and James Malone
Abstract
Background: Authoring bio-ontologies is a task that has traditionally been undertaken by skilled experts trained in
understanding complex languages such as the Web Ontology Language (OWL), in tools designed for such experts. As
requests for new terms are made, the need for expert ontologists represents a bottleneck in the development
process. Furthermore, the ability to rigorously enforce ontology design patterns in large, collaboratively developed
ontologies is difficult with existing ontology authoring software.
Description: We present Webulous, an application suite for supporting ontology creation by design patterns.
Webulous provides infrastructure to specify templates for populating ontology design patterns that get transformed
into OWL assertions in a target ontology. Webulous provides programmatic access to the template server and a client
application has been developed for Google Sheets that allows templates to be loaded, populated and resubmitted to
the Webulous server for processing.
Conclusions: The development and delivery of ontologies to the community requires software support that goes
beyond the ontology editor. Building ontologies by design patterns and providing simple mechanisms for the
addition of new content helps reduce the overall cost and effort required to develop an ontology. The Webulous
system provides support for this process and is used as part of the development of several ontologies at the European
Bioinformatics Institute.
Keywords: OWL, Ontology, Spreadsheet, Webulous, Google App
Introduction
Like most data resources, ontologies are rarely com-
plete, and healthy ontologies are continually growing and
improving, as the state of knowledge progresses [1, 2].
Typically, authoring ontologies is a task performed by
trained experts, familiar with ontology development prac-
tices and the complexities of languages such as the Web
Ontology Language (OWL). This presents a major bot-
tleneck to the ontology development process as the time
and availability of trained experts is limited and ontol-
ogy development is hard to fund [3]. Tools are now being
developed to simplify the addition of content to ontologies
that are based on populating ontology design patterns via
data entry templates.
*Correspondence: jupp@ebi.ac.uk
European Bioinformatics Institute (EMBL-EBI),European Molecular Biology
Laboratory, Wellcome Trust Genome Campus, Hinxton, Cambridge, UK
Ontology design patterns (ODPs) are commonly used
in ontology development in guiding the ontology devel-
oper in the modeling of knowledge [4, 5]. They also help
in enforcing consistency and best practice in ontology
design whilst reducing arbitrary class descriptions within
an ontology that can lead to both errors and ontologies
that are difficult to maintain. Whilst ODPs can provide
a sound methodological framework, ontology expertise
is still required to establish and apply modeling pat-
terns for real-world entities from a particular domain of
interest [6].
Several tools have been previously developed to sup-
port building OWL ontologies from design pattern tem-
plates [1, 7, 8]. The main aim of these tools is to provide
a simple interface for populating a design pattern that
© 2016 Jupp et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International
License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any
medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons
license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.
org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Jupp et al. Journal of Biomedical Semantics  (2016) 7:17 Page 2 of 8
shields the users from the underlying OWL vocabulary.
These systems help to enforce rigour and adherence to
a design pattern and allow new content to be added
in bulk in a reproducible manner. Although these tools
help in enforcing consistency of ontology development, in
order to truly mediate content contributions from non-
ontology experts, tools that use a familiar paradigm to
domain experts are required. Such tools should enable
non-ontologists to contribute whilst also tackling the
issues of translating input into OWL ontologies.
In this paper we describe the Webulous framework that
provides software for the management of ontology design
patterns and ontology building templates. Webulous is
built around a client/server architecture, where the server
hosts a number of ontology building templates that can
be served to any number of client applications. Data sub-
mitted to the server is translated into OWL assertions
according to design patterns expressed in the Ontology
Pre-Processing Language (OPPL) [9]. We have developed
a client application for Webulous using the Google Sheets
Add-On framework that allows design pattern templates
to be loaded into Google Sheets and submitted back to a
Webulous server for processing. The Webulous client is
aimed at domain experts adding new content to ontolo-
gies and is demonstrated as a term submission tool for the
Experimental Factor Ontology [10].
Results
Webulous provides a public service for the creation and
management of ontology design templates. A Webulous
server can host a number of ontology building templates
that use OPPL statements to translate input data into
OWL axioms. A Webulous template specifies a series of
fields for the input data, and fields can can be restricted
to only allow values from a list of ontology terms. The
Webulous API can be used by client side applications to
automatically build the user interface for a given tem-
plate. Once a user populates a template with data this is
submitted back to a Webulous server where the patterns
are instantiated to create new OWL statements ready for
import into the target ontology.
Google Sheets Add-on
Providing Webulous as a service means that a range of
client-side applications can be developed for populating a
template. We built a Google Sheets Add-On that supports
loadingWebulous templates from a server and submitting
populated templates back to the server for processing. We
chose Google Sheets for their convenient document man-
agement and sharing functionality and for the familiarity
of the spreadsheet format for users.
When a Webulous template is loaded via the Google
Add-On, each template input field represents a column in
the sheet. Columns can be restricted to a set of allowed
ontology terms by using term labels to create data vali-
dation. This data validation provides the user with con-
venient term autocomplete when entering data into a cell
and will alert the user when an invalid term has been
entered. Data submitted from Google Sheets is associated
with the users Google account so the server can notify
both the user and template admin via e-mail once the
template has been processed.
TheWebulous Google Sheets Add-On (Fig. 1) has addi-
tional functionality by allowing users to connect directly
to BioPortal services [11]. The Webulous Add-on pro-
vides a side bar for searching BioPortal for ontology terms
and creating custom ontology-based data validations. The
sidebar allows users to create a validation, which consists
of a restricted set of term labels, and provides a conve-
nient way to create further validations using subclasses of
any particular term.
Application of Webulous
The Experimental Factor Ontology contains descriptions
of experimental variables ranging from diseases, cell
types, cell lines, anatomy, assays, chemical compounds
and phenotypes. It is developed as an application ontol-
ogy that integrates and bridges several external reference
ontologies (such as ChEBI and the Gene Ontology). EFO
enriches these existing ontologies by including additional
axioms that connect terms like diseases to tissues and
anatomical systems; cell lines to cell types, diseases and
tissue; and link common and rare diseases through asso-
ciated anatomical parts and phenotypes. EFO is used to
annotate resources spanning multiple omics including;
transcriptomics data in ArrayExpress [12] and Expression
Atlas [13], genomics data in the NHGRI-EBI GWAS Cat-
alog [14], proteomics data in PRIDE [15] and cell line
data in Encode [16]. EFO is also used by the Centre for
Therapeutic Target Validation (CTTV)1 as their core data
annotation resource.
One of the appealing features of EFO is that many
of the design patterns are well established and applied
consistently across large portions of the ontology. This
use of design patterns makes EFO nicely amenable to
the generation of new content using templates. Prior
to the work presented here most cell lines have been
added to EFO for the ENCODE project using Excel-
based spreadsheets that were processed with Populous
[17]. As more resources adopt EFO there is an increas-
ing pressure on the editors to add new content, much
of which remains in a spreadsheet-based format on
submission.
A dedicated Webulous instance is now running at the
EBI to serve templates for adding new content to EFO2.
This instance currently contains six EFO templates sum-
marised in Table 1. Four of these templates are for
adding new terms to EFO that include new cell lines,
Jupp et al. Journal of Biomedical Semantics  (2016) 7:17 Page 3 of 8
Fig. 1Webulous Google Add-on. Screenshot of the Webulous Google Sheets Add-on showing a ontology-based data validation
diseases, assays or measurement terms. There is a ded-
icated template for adding synonyms to existing terms
and a more general template for adding other types of
RESEARCH Open Access
DermO; an ontology for the description of
dermatologic disease
Hannah M. Fisher1, Robert Hoehndorf2, Bruno S. Bazelato3, Soheil S. Dadras4, Lloyd E. King Jr.5,
Georgios V. Gkoutos3,6,7, John P. Sundberg8 and Paul N. Schofield1,8*
Abstract
Background: There have been repeated initiatives to produce standard nosologies and terminologies for
cutaneous disease, some dedicated to the domain and some part of bigger terminologies such as ICD-10. Recently,
formally structured terminologies, ontologies, have been widely developed in many areas of biomedical research.
Primarily, these address the aim of providing comprehensive working terminologies for domains of knowledge, but
because of the knowledge contained in the relationships between terms they can also be used computationally for
many purposes.
Results: We have developed an ontology of cutaneous disease, constructed manually by domain experts. With
more than 3000 terms, DermO represents the most comprehensive formal dermatological disease terminology
available. The disease entities are categorized in 20 upper level terms, which use a variety of features such as
anatomical location, heritability, affected cell or tissue type, or etiology, as the features for classification, in line with
professional practice and nosology in dermatology. Available in OBO flatfile and OWL 2 formats, it is integrated
semantically with other ontologies and terminologies describing diseases and phenotypes. We demonstrate the
application of DermO to text mining the biomedical literature and in the creation of a network describing the
phenotypic relationships between cutaneous diseases.
Conclusions: DermO is an ontology with broad coverage of the domain of dermatologic disease and we
demonstrate here its utility for text mining and investigation of phenotypic relationships between dermatologic
disorders. We envision that in the future it may be applied to the creation and mining of electronic health records,
clinical training and basic research, as it supports automated inference and reasoning, and for the broader
integration of skin disease information with that from other domains.
Keywords: Dermatology, Ontology, Disease, Dermatopathology
Abbreviations: DermO, dermatology ontology; DO, human disease ontology; GWAS, genome-wide association
studies; HPO, human phenotype ontology; ICD-9, 10, 11, international classification of disease versions 9, 10, and 11;
MeSH, medical subject headings; MPO, mammalian phenotype ontology; NOS, not otherwise specified; OBO, open
biomedical ontologies; OMIM, online mendelian inheritance of man; OWL, web ontology language; PheWAS,
phenome-wide association studies; PNS, Paul N. Schofield; UMLS, unified medical language system.
* Correspondence: pns12@cam.ac.uk
1Dept. of Physiology, Development and Neuroscience, University of
Cambridge, Downing Street, Cambridge CB2 3EG, UK
8The Jackson Laboratory, 600, Main Street, Bar Harbor Maine ME 04609-1500,
USA
Full list of author information is available at the end of the article
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Fisher et al. Journal of Biomedical Semantics  (2016) 7:38 
DOI 10.1186/s13326-016-0085-x
Background
Estimation of the impact of skin disease on population
morbidity and mortality has always been complicated by
the nature of the terminologies used for recording dis-
ease incidence, prevalence and cause of death. However
measured, it remains a highly significant social, eco-
nomic and clinical burden. In the USA, collective preva-
lence estimates for skin disease are greater than those of
obesity, hypertension and cancer [1]. There have been
repeated initiatives to generate structured terminologies
of sufficient granularity to accurately capture skin dis-
ease diagnoses, but the available tools, such as ICD-10,
remain blunt instruments in the face of the pressing
need for precision phenotyping, and are unsuitable for
many types of computation-based research. We have,
with the close involvement of domain experts in derma-
tology, pathology, and genetics, consequently created a
new ontology, DermO, for cutaneous disease.
There have been several previous initiatives to devise
lexicons or structured comprehensive terminologies for
the description of cutaneous disorders, for example the
Dermatologischer Diagnosenkatalog [2]. The most recent
is the DermLex ontology [3, 4], created under the auspices
of the American Academy of Dermatology, with the pur-
pose of providing a definitive nomenclature for clinical
dermatology [5]. This terminology was merged with the
British Association of Dermatologists BAD Index and is
mapped to International Classification of Disease (ICD9-
CM) codes. However, maintenance of DermLex was dis-
continued in 2009. This lexicon covered sporadic and
inherited cutaneous disorders and consisted of 6104 terms
including a nosology, classic signs, therapeutic procedures,
and anatomical distributions. To date DermLex has been
the most comprehensive tool for capturing dermatological
disease information. The Human Disease Ontology (DO)
[6] also contains an integumentary branch of 234 terms,
with skin and adnexal diseases classified as skin, hair, and
nail disease. Likewise, the Human Phenotype Ontology
(HPO) [7] contains a branch of skin and adnexal pheno-
types with terms focused on phenotypic manifestations of
cutaneous disorders. Skin diseases are largely out of the
scope of HPO, which primarily focuses on phenotypes
and does not aim to cover the breadth of cutaneous condi-
tions we envisage. Similarly, the DOs coverage and
organization of the integumentary branch is limited and
may not entirely capture the breadth and diversity of cuta-
neous conditions required for the purposes of patient
stratification, population analysis, cross-species compari-
sons, database query expansion and automated reasoning.
The current revision of ICD, ICD-11, will contain the
most radical revision of dermatology terms since 1948
and so far around 2000 terms have been assigned to the
dermatology chapter (XII) with 20 major subdivisions
[8]. ICD-11 is not currently finalized (June 2016) and
uptake is expected to be gradual (for example the USA
has only recently transitioned to ICD-10 for electronic
health records as of 1.10.15) [9].
The other main resource for dermatological disease
terms is SNOMED-CT. Whilst SNOMED contains
more than 2000 terms related to cutaneous disease the
coverage and structure of the terminology does not
lend itself to detailed clinical description and having a
relatively flat hierarchy does not permit the relation-
ships between diseases to be used computationally in
a useful way. (Discussed in [4]). An additional issue
remains the licensing of SNOMED-CT by the Inter-
national Health Terminology Standards Development
Organization (IHTSDO) that precludes its free use in
some countries.
The main applications for DermO are data capture
and informatic analyses that only recently have become
feasible using electronic health records, and semantic
integration of disease information between species and
across domains of knowledge; these analyses require a
sufficiently rich structure, granularity, and coverage
from the available terminologies. We have therefore cre-
ated a new ontology from the scientific literature with the
close involvement of domain experts in dermatology,
pathology, and genetics, while explicitly maintaining inter-
operability with established ontologies and vocabularies in
the biomedical domain, DermO is organized in a manner
intuitive to dermatologists and dermatopathologists. The
framework adopted for the development of DermO was
based on that of the definitive clinical dermatology text,
Dermatology by Bolognia et al. [10]. The specific aim is to
include all of the currently accepted primary and second-
ary skin diseases, including those caused by systemic dis-
orders, external insult, and the genodermatoses. DermO
was developed with the intention of creating a tool applic-
able to patient care, clinical training and basic research, as
well as to support automated inference and reasoning. It
can be used for patient stratification, genotype/phenotype
studies, and for the broader integration of skin dis-
ease information with that from other domains, such
as model organism phenotypes and pharmacogenomics
for translational science. DermO is freely available on
https://github.com/dermatology-ontology/dermatology.
Methods
Ontology construction
DermO was constructed by domain experts using the
framework of the most recent definitive text on Derma-
tology edited by Bolognia et al. [10]. The approach taken
was to produce a classification familiar to dermatologists,
as the envisaged uses of DermO include both patient diag-
nostic annotations by clinicians and mining of electronic
health records. The formalization of patient information
provides a data resource that can be expanded to integrate
Fisher et al. Journal of Biomedical Semantics  (2016) 7:38 Page 2 of 9
historical and newly generated information from both
human and mouse dermatology, and genetic studies.
The structure adopted for DermO is familiar both for
diagnostic support and patient data recording purposes.
Because of the inclusion of systemic and inherited dis-
eases, DermO also includes diseases that would normally
be found in other branches of a disease ontology, such
as Mendelian monogenic syndromes, and for completeness
we therefore use a degree of polyhierarchy and include
genetic diseases (the genodermatoses) as well as systemic
diseases. For the same reasons the ICD-11 topic advisory
group adopted a similar approach [11].
The ontology was manually constructed using OBO-Edit
[12] and the OWL2 version prepared using Protégé [13].
Consistency was verified using the HermiT reasoner [14],
which detected no inconsistencies and no unsatisfiable
classes, mainly due to the absence of disjointness axioms
in the ontology. The main ontology is available in both the
OBO Flatfile format [15] and the Web Ontology Language
(OWL) [16]. DermO is housed in a Github repository and
is made available via Bioportal (permanent URL: http://
purl.bioontology.org/ontology/DERMO) [17], Aber-OWL
(permanent URL: http://aber-owl.net/ontology/DERMO)
and on the projects website https://github.com/dermatol-
ogy-ontology/dermatology.
Content, relations and mapping to other ontologies
DermO was constructed as a simple ontology with limited
polyhierarchy, and currently contains 3,425 classes (Fig. 1,
Table 1). These are currently mapped to concepts in other
major terminologies and provided with synonyms (Table 2).
Synonyms were sourced from Bolognia et al. [10],
DermnetNZ [18] and from expert curator knowledge.
Class labels and synonyms were initially lexically mapped
to concepts in UMLS [19], ICD-10, MPO [20], HPO [7],
DO [6], OMIM (Online Mendelian Inheritance in Man)
[21], SNOMED-CT [22], Medical Subject Headings
(MeSH) and DermLex [4]. Only exact mappings were in-
Hoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 
DOI 10.1186/s13326-016-0107-8
RESEARCH Open Access
The flora phenotype ontology (FLOPO):
tool for integrating morphological traits and
phenotypes of vascular plants
Robert Hoehndorf1,2* , Mona Alshahrani1,2, Georgios V. Gkoutos3,4,5, George Gosline6, Quentin Groom7,
Thomas Hamann8, Jens Kattge10,11, Sylvia Mota de Oliveira8, Marco Schmidt9, Soraya Sierra8,
Erik Smets8, Rutger A. Vos8 and Claus Weiland9
Abstract
Background: The systematic analysis of a large number of comparable plant trait data can support investigations
into phylogenetics and ecological adaptation, with broad applications in evolutionary biology, agriculture,
conservation, and the functioning of ecosystems. Floras, i.e., books collecting the information on all known plant
species found within a region, are a potentially rich source of such plant trait data. Floras describe plant traits with a
focus on morphology and other traits relevant for species identification in addition to other characteristics of plant
species, such as ecological affinities, distribution, economic value, health applications, traditional uses, and so on.
However, a key limitation in systematically analyzing information in Floras is the lack of a standardized vocabulary for
the described traits as well as the difficulties in extracting structured information from free text.
Results: We have developed the Flora Phenotype Ontology (FLOPO), an ontology for describing traits of plant
species found in Floras. We used the Plant Ontology (PO) and the Phenotype And Trait Ontology (PATO) to extract
entity-quality relationships from digitized taxon descriptions in Floras, and used a formal ontological approach based
on phenotype description patterns and automated reasoning to generate the FLOPO. The resulting ontology consists
of 25,407 classes and is based on the PO and PATO. The classified ontology closely follows the structure of Plant
Ontology in that the primary axis of classification is the observed plant anatomical structure, and more specific traits
are then classified based on parthood and subclass relations between anatomical structures as well as subclass
relations between phenotypic qualities.
Conclusions: The FLOPO is primarily intended as a framework based on which plant traits can be integrated
computationally across all species and higher taxa of flowering plants. Importantly, it is not intended to replace
established vocabularies or ontologies, but rather serve as an overarching framework based on which different
application- and domain-specific ontologies, thesauri and vocabularies of phenotypes observed in flowering plants
can be integrated.
Keywords: Phenotype, Biodiversity, Flora, Botany, Morphological traits
*Correspondence: robert.hoehndorf@kaust.edu.sa
1Computational Bioscience Research Center (CBRC), King Abdullah University
of Science and Technology, 4700 KAUST, 239556900 Thuwal, Kingdom of
Saudi Arabia
2Computer, Electrical and Mathematical Sciences & Engineering Division
(CEMSE), King Abdullah University of Science and Technology, 4700 KAUST,
239556900 Thuwal, Kingdom of Saudi Arabia
Full list of author information is available at the end of the article
© The Author(s). 2016 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Hoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 2 of 11
Background
For hundreds of years, information on plant species found
across the world has been collected in Floras, taxonomic
monographs and annotations to collection material. Flo-
ras are books collecting the information on all known
plant species found within a region. They describe plant
traits with a focus onmorphology and other traits relevant
for species identification in addition to other characteris-
tics of plant species, such as ecological affinities, distribu-
tion, economic value, health applications, traditional uses,
and so on. Floras not only allow identification of plants
found within a region, but also provide a large knowledge
base of the phenotypic diversity found within ecosys-
tems. The systematic analysis of such large-scale trait data
can support investigations into phylogenetics and ecolog-
ical adaptation, with broad applications in evolutionary
biology, conservation, and the functioning of ecosystems.
Moreover, the provision of trait data enables integrated
knowledge discovery for agriculture (i.e. plant breeding)
and phytomedicine. In particular many medicinal plants
are not as comprehensively characterized as food crops
or model plant systems. A comprehensive overview of
the Floras available at the global level is given by [1]. A
key limitation in systematically analyzing information in
Floras is the lack of a standardized vocabulary for the
described traits as well as the difficulties in extracting
structured information from free text.
To facilitate integration and analysis of the information
contained in Floras, we have developed the Flora Pheno-
type Ontology (FLOPO), an ontology for describing traits
of plant species found in Floras. Ontologies provide for-
mal, machine-readable definitions of the vocabulary used
within a knowledge domain [2, 3]. The FLOPO builds on
existing ontologies for morphological structures and phe-
notypic qualities, in particular the Plant Ontology (PO)
[4] and the Phenotype And Trait Ontology (PATO) [5].
We have used these ontologies to extract entity-quality
relationships from digitized taxon descriptions in Floras,
and used a formal ontological approach based on pheno-
type description patterns [6] and automated reasoning to
generate the FLOPO. Phenotype description patterns are
formal statements in the Web Ontology Language (OWL)
[7] that express the content of a phenotype description,
i.e., the features of an organism when it has a particular
phenotype.
The FLOPO allows integration of qualitative trait data
from different sources, including text-based descriptions
of phenotypes, such as those found in Floras and mono-
graphs, image-based representations of plant traits such as
those found in photos and specimen scans (e.g., informa-
tion stored in herbaria), as well as information about traits
and phenotypes in trait databases such as TRY [8] or the
Encyclopedia of Lifes TraitBank [9]. Through its links to
established ontologies, it can also be used to link this data
to data sources from other domains, such as genomics,
macroecology or systems biology.
In our initial use case, our aims were to (1) identify
the traits associated with taxa in Floras, (2) represent the
traits in a semantic form amenable to computational anal-
ysis, (3) link the traits to standard vocabularies of plant
morphology used in related areas of biological research,
(4) and demonstrate that these traits can subsequently
be integrated and compared with traits recorded in other
databases. The FLOPO is freely available under a CC-0
license at http://purl.obolibrary.org/obo/flopo.owl.
Methods
Data sources
Building upon a collaborative prototype developed at the
2014 Biodiversity Data Enrichment Hackathon [10], an
event similar to the popular BioHackathon series [11],
we used several Floras (Flora Malesiana [12], Flore du
Gabon [13, 14], Flore dAfrique Centrale [15], Flore du
Congo Belge et du Ruanda-Urundi [16], and a collection
of Kews African Floras available at http://www.kew.org/
science-conservation/research-data/science-directory/
projects/e-floras, including the Flora Zambesiaca, Flora
of Tropical East Africa, Flora of West Tropical Africa,
Flora of Tropical Africa, Flora Capensis and the Useful
Plants of West Tropical Africa). The Floras were available
in digitized form, with most Floras written in English, and
three in French (Flore dAfrique Centrale, Flore du Congo
Belge et du Ruanda-Urundi, and Flore du Gabon).
We assembled a vocabulary of plant morphological enti-
ties, attributes and attribute values. The terms for this
vocabulary were taken from ontologies that are widely
used in biological research: PO [17] for plant morpho-
logical entities, and PATO [5] for attributes and attribute
values. Each ontology provides one or more English terms
associated with one kind of plant entity (i.e., the labels
and synonyms of classes in the ontologies). To identify
the French terms associated with these entities, we used a
dictionary provided by the Missouri Botanical Garden at
http://www.mobot.org/mobot/glossary/ that was used by
the project partners in the context of the FlorML project
[18]. As result of this step, we obtained two dictionaries
comprised of French and English terms for plant morpho-
logical entities, and attributes and attribute values.
Text processing
Floras are available in different formats, including the
structured XML-based format FlorML [18] as well as
free text in taxonomic databases. In each Flora, we iden-
tified taxon names and identifiers together with com-
plete (textual) taxon descriptions. We then processed
the text using natural language processing (NLP) tools
provided by the Apache Lucene [19] standard analyzer
(basic stemming, stopword removal), applied a sentence
Hoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 3 of 11
identification method to tokenize the text into sentences
(using the OpenNLP toolkit https://opennlp.apache.org/)
and stored the resulting sentences together with their
taxon names and identifiers in a fulltext index using the
Apache Lucene framework.
We then applied the same stemming and stopword
removal steps on the labels and synonyms of the ontol-
ogy classes, and used Lucene to query the full text index
for taxa descriptions in which sentences contain both a
label or synonym of a quality (from PATO) and a label or
synonym from a morphological entity (from PO). When
querying French Floras, we first performed a dictionary-
based translation of the labels, then applied the same pre-
processing as applied to the textual taxa descriptions and
performed the same query. Finally, we used the Stanford
parser [20] to identify whether the quality term stands in
an attributive relationship to the entity term.
As a result, we identified Entity-Quality pairs [5] in
which entity-terms refer to plant morphological entities
(from the Plant Ontology), and quality-terms to attributes
or attribute values (from PATO). For example, from a sen-
tence The flowers are red we identify the entity-quality
pair (Flower, Red), where Flower is taken from the Plant
Ontology (PO:0009046), and Red is taken from PATO
(PATO:0000322). More complex relationships, such as
connectivity between two morphological structures, are
expressed as ternary relations in PATO (requiring the
two connected entities and an additional instance of a
relational quality as arguments), and we ignore them in
our analysis; instead, we introduced placeholders which
state that each structure is related to something without
providing information on the second entity.
To filter the results, we used lexical parsing to determine
whether the sentence expresses an attributive relation-
ship between the quality and the entity we identified. For
example, in the sentence The flowers are red with yel-
low stamens., an attributive relationship exists between
Flower and Red as well as Yellow and Stamen.
As a result of this text processing pipeline, we obtained a
set of 502,693 PATO-based entity-quality descriptions of
traits found in the Floras we analyzed. The entity-quality
based descriptions consist of 20,584 distinct combinations
of morphological structures from PO and qualities from
PATO, using 287 distinct plant morphological structures
and 545 distinct qualities, and are associated with 26,104
taxa.
Ontology generation and automated reasoning
To generate the FLOPO, we use the extracted informa-
tion in phenotype definition patterns [6], i.e., OWL axiom
patterns for defining classes of phenotypes. We mainly
generate three types of classes which we fully define in
OWL: first, we create grouping classes representing the
phenotypes of a plant structure or any of its parts (e.g.,
flower phenotype); second, we create classes for traits (or
characters) of plant structures (e.g., flower color); finally,
we create classes for the values of traits (or character
states) of plant structures (e.g., flower red).
Using OWL, we generate the following classes and
axioms for each entity-quality pair (E,Q):
 E phenotype EquivalentTo: has-part
some ((part-of some E) and
has-quality some quality)
 E Q EquivalentTo: has-part some
(E and has-quality some Q)
 If Q is in the values subset of PATO, we identify
the most specific superclass T of Q that is in PATOs
attribute subset, and generate the axiom E T
EquivalentTo: has-part some (E and
has-quality some T).
For example, for the entity quality pair (flower, red), we
generate
 flower phenotype EquivalentTo:
has-part some ((part-of some flower)
and has-quality some quality)
 flower red EquivalentTo: has-part
some (flower and has-quality some
red)
 flower color EquivalentTo:
has-part some (flower and
has-quality some color)
The intuition behind our axiom patterns is that they
always define a phenotype with respect to what must be
true for a whole organism if the phenotype is present. For
this purpose, we prefix every axiom with a has-part
some restriction.
The use of this prefix pattern allows combining sim-
ple phenotypes (expressed through a single entity-quality
pair) into complex phenotypes (requiring combinations
of entity-quality pairs) through a simple intersection; for
example, to describe the complex phenotype of having
both red flowers and yellow stamens, the flower red and
stamen yellow phenotypes would be intersected to form
the complex phenotype of a whole organism having two
parts, flowers that are red and anthers that are yellow.
Without such a prefix, phenotypes could not easily be
combined in such a way since flower and anthers are
disjoint morphological entities, and red and yellow are
disjoint qualities.
The parthood relation is used in another pattern to
group traits by plant morphological structure as well as
all parts of that morphological structure. In particular,
the part-of relation (the inverse of the has-part
relation) is both reflexive and transitive, and therefore
subclasses of part-of some X include X as well as all
Hoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 4 of 11
classes with instances that necessarily are a part of some
X. For example, subclasses of part-of some flower
include, among others, flower, petal, and androecium, and
using the parthood relation in the definition of the phe-
notype classes will lead to petal phenotype, androecium
phenotype, etc., to become subclasses of flower phenotype.
We do not use this pattern on the level of traits such as
flower color as the traits of the parts will be different from
the trait of the flower (e.g., the flower may be red while its
stamens are yellow).
To distinguish between traits and their values, we use
the distinction between attributes and attribute values in
PATO in which classes are tagged through an annotation
property either as attribute or value.When using theOBO
Flatfile Format [21], these distinctions are expressed as the
attribute slim and value slim of PATO.
Following the generation of the axioms for FLOPO
based on the axiom patterns, we added one further axiom
to the resulting ontology to remove impossible combina-
tions of entity and quality, in particular those in which a
morphological structure in PO is asserted to have a quality
that can only be the quality of processes:
has-part some (owl:Thing and has-quality
some process quality) SubClassOf:
owl:Nothing
The ontology was generated using a Groovy script based
on the OWL API [22] and the Elk reasoner [23]. Source
code for processing Floras and generating the ontology
is freely available (under a BSD-style license) at https://
github.com/flora-phenotype-ontology/flopoontology.
Results
Data-driven generation of the Flora Phenotype Ontology
For creating the Flora Phenotype Ontology we used as pri-
mary use case the traits and phenotypes described in the
Floras listed in the Methods section. Figure 1 provides an
overview of our workflow. Using the Entity-Quality pairs
extracted from the Floras, we developed a data-driven
approach to generate a prototype of an ontology that
would likely be capable of characterizing a large number
of the traits observed in our study. In each Entity-Quality
pair, the entity term directly maps to a morphologi-
cal entity (in the Plant Ontology), and the quality term
maps to an attribute or value (in the PATO ontology).
We aimed to exploit the background knowledge in these
ontologies together with an automated reasoner to gener-
ate an ontology in which each class characterizes a trait
and is associated with at least one taxon in one of the
Floras we processed. Specifically, we aimed to exploit the
information about parthood relations between morpho-
logical structures and biological processes, and the sub-
class relations between qualities, morphological parts and
physiological processes to generate the ontology [6]. We
used a pattern-based approach in which we create axiom
patterns that combine information obtained through our
NLP-based approach with information in the referenced
ontologies. Through the axiom patterns, we achieve:
 structural organization based on anatomical
parthood (e.g., a petal phenotype should become a
subclass of the flower phenotype based on petal
being a part of flower),
 separation of types of trait for each morphological
structure (e.g., a flower color should be separate from
the flower shape), but both should be more closely
related to each other than to root color as both are
flower traits,
 separation and structural organization of attributes
and values (e..g, flower red should become a subclass
of flower color), and
 semantic interoperability with existing ontologies in
the plant domain, including the Plant Ontology and
Trait Ontology.
Flora Phenotype Ontology
The Flora Phenotype Ontology (FLOPO), available at
https://purl.obolibrary.org/obo/flopo.owl, is the result of
classifying the axioms generated from our text-mining
pipeline together with the PATO and PO ontologies. Clas-
sification of an ontology is a reasoning task in which
the axioms within the ontology are used to determine
the most specific sub- and super-class for each class
in the ontology. As all generated axioms are in the
OWL EL profile [24], we used the Elk reasoner [23]
to perform the classification. The resulting ontology,
the FLOPO, consists of 25,407 classes (24,076 classes
unique to the FLOPO, in addition to the classes in
PO and PATO). Each class is assigned a unique IRI
in the namespace http://purl.obolibrary.org/obo/FLOPO_
followed by a unique numerical identifier. For example,
the class flower red has the identifier FLOPO:0007599
when using FLOPO: to refer to the FLOPO namespace,
i.e., FLOPO:0007599 will refer to the IRI http://purl.
obolibrary.org/obo/FLOPO_0007599.
The classified ontology closely follows the structure of
PO in that the primary axis of the classification shows the
observed plant anatomical structure, while more specific
traits are classified based on parthood and subclass rela-
tions between anatomical structures as well as subclass
relations within PATO. Figure 2 shows the upper level of
the FLOPO.
As most classes in the FLOPO are fully defined using
axioms in OWL, it can be queried using either the labels
of a class, the identifier of a class, or semantically using
the axioms that are used to define the class. The lat-
ter kind of query is particularly useful when querying
Hoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 5 of 11
Fig. 1 Overview over the main workflow in generating the FLOPO. After generation of the FLOPO, it can then be used to access plant trait and
phenotype data in different databases
for classes that are not currently contained within the
FLOPO. For example, the FLOPO does not currently
contain a class for the flower being deep pink. Neverthe-
less, a semantic query using the entity-quality pair flower
and deep pink and the axiom patterns we would use
within the FLOPO (has-part some (flower and
has-quality some deep pink), it is possible to
query for the equivalent or direct superclasses of that
description which will return flower pink as the clos-
est matching class.
Following the automatic generation of the FLOPO, we
have also begun to manually add classes to FLOPO based
on user requests and our own use cases. While we aim
to fully define all classes in FLOPO, some classes can-
not be defined without also extending other ontologies
such as PO or PATO. FLOPO currently contains 198man-
ually created classes of which more than 50 % are fully
defined while the others are restricted by subclass axioms
alone.
Evaluation: coverage of traits in Floras and plant databases
To test the coverage of traits in FLOPO, we manually
annotated taxon descriptions from Floras and evaluated
the correctness and coverage of traits in FLOPO. Correct-
ness refers to the creation of nonsensical classes generated
by the automated analysis, while coverage (i.e., recall)
refers to the number of characters in plant descriptions
that have a corresponding FLOPO class.
We have not performed a quantitative assessment of
how many of the classes in FLOPO do not make sense
but did a qualitative analysis instead. Classes such as
xylem vessel member tomentose, peduncle female, and
lower glume subacute are obviously artifacts of the auto-
mated generation and constitute a significant number of
classes in FLOPO. We distinguish two main sources of
these artifacts:
 The parsing of the descriptions failed to correctly
associate entities with attributes. Parsing of
Fig. 2 An overview of the top-level structure of the FLOPO
Hoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 6 of 11
descriptions is difficult, and entities and qualities in
the same sentences may incorrectly be identified as
entity-quality pairs.
 Some labels of qualities in PATO can be used in
another context to refer to completely different
qualities. For example, acute in PATO is a quality of
processes, used to characterize for example diseases
such as acute malaria, while the term acute in a
plant description usually refers to an angle. These
qualities are then propagated up the hierarchy (due to
inheritance in PATO) and yield further non-sensical
classes (such as leaf intensity).
In response to our evaluation, we have manually depre-
cated several classes and added an axiom to prevent the
use of any process qualities in FLOPO. Currently, 564
classes in FLOPO have been deprecated for this reason.
While nonsensical classes clutter the ontology with use-
less classes, they do not prevent the use of the remaining
classes in standardizing the description of traits.
To evaluate coverage of FLOPO, we have performed a
rigorous application of the ontology to eight plant descrip-
tions from several Floras and within a number of taxo-
nomic groups. The detailed results can be found in the
Additional files 1 and 2. We identified between 40 and 85
characters for each taxon, and the coverage of characters
in FLOPO ranged from 48 to 70 %. Simple characters such
as stem diameter are well represented in FLOPO. More
complex characters, however, are often lacking, although
some complex characters such as petiole margin undulate
(often a useful character for identification) are present.
The largest number of missing classes in FLOPO are due
to qualities missing in PATO. Examples of these include
caulescent, chartaceous, and axillary (full list provided as
Additional file 3). While truncate is present in PATO, we
did not match truncated in our text processing method.
Furthermore, some PO classes were also missed due to
missing labels or synonyms and our use of exact matching
in text processing. For example, ovule was not matched
because it corresponds to the class plant ovule in PO
which has no synonym ovule. Any plant organ class miss-
ing in PO leads to an absence of FLOPO classes for that
organ. Additionally, some combinations of PO and PATO
are not identified, sometimes due to the lack of compar-
ative classes (or synonyms) in PATO such as unequal or
longer than.
To further evaluate the coverage of FLOPO, we have
used independent trait data from African Plants  a photo
guide [25] database, an expert-based tool using trait data
for identification purposes. The trait life form and quan-
titative traits such as the number of petals, that do not
fit with the entity-quality terms in FLOPO, have been
excluded beforehand. Out of 80,887 taxon-trait combina-
tions, 44,200 (55 %) could be matched to FLOPO classes.
Out of 88 traits that were used in the African Plants
database, 31 were already present in FLOPO and 57 were
manually created in FLOPO following this evaluation.
The link to genetics: integrating wild-type andmodel
organism phenotypes
Phenotypes are not only collected in a natural context,
but also in the context of model organisms [26]. In many
cases, model organism databases collect abnormal phe-
notypes [26]. These differ from phenotypes observed in
a biodiversity context in the fact that they represent dif-
ferences to a control group. For example, while a flower
red phenotype in a biodiversity context states that the
members of a particular species, or an individual sam-
ple of that species, have red flowers, it may indicate in a
model organism context that, based on some experimen-
tal conditions such as a gene knockout or environmental
alteration, the flowers of the organism are red under the
experimental conditions while the control group has dif-
ferently colored flowers. These experiments can provide
useful information on functional genetics by revealing
the phenotypic effects associated with particular genes
or revealing the mechanisms underlying environmental
adaptation [26, 27].
While the FLOPO is primarily focused on describing
the traits and phenotypes in wild-type plants, its classes
can also be used to characterize divergent phenotypes
as, for example, observed in functional genetics experi-
ments. To test this assumption we used a dataset of formal
phenotype descriptions recorded in mutant models of
Arabidopsis thaliana, maize, barrel medic, rice, soybean,
and tomato [28]. Out of 5,186 phenotype statements con-
tained in the dataset that involve a plant anatomical entity,
315 directly match one of the classes in the FLOPO, while
the others have superclasses in the FLOPO. The low num-
ber of directly matching classes may be a consequence of
the different way in which the phenotypes are recorded;
in a model organism context, phenotype descriptions
include statements such as whole plant increased size
or seed inviable, which are not recorded, or meaningful,
without an explicit group to which phenotypes are com-
pared. Nevertheless, these results show that FLOPO can
be used to combine plant phenotype data from different
databases and domains.
Discussion
Interoperability with plant trait vocabularies
The FLOPO is primarily intended as a framework based
on which plant traits can be integrated computationally
across all species and higher taxa of flowering plants.
Importantly, while FLOPO can be used for annotation
directly, it is not intended to replace established vocab-
ularies or ontologies, but rather serve as an overarching
framework based on which different application- and
Hoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 7 of 11
domain-specific ontologies, thesauri and vocabularies of
phenotypes observed in flowering plants can be inte-
grated. Using the axiom patterns we defined and used
to generate the FLOPO, any ontology-based pheno-
type description using the entity/quality method can
be directly integrated with the FLOPO, and appropri-
ate equivalent classes, sub- and super-classes can be
identified using automated reasoning (either using an
automated reasoner directly or querying through pub-
lic repositories such as AberOWL [29] which provide
reasoning services for ontologies, including the FLOPO).
Additional terminological resources, such as the Plant
Trait Thesaurus [30], the Crop Ontology [31], the Plant
Trait Ontology [32], as well as general and application-
specific plant-related thesauri, can be integrated and
semantically enriched through mappings to the FLOPO.
These mappings can either be established manually by
domain experts or, in some cases, automatically through
mapping of labels.
Multi-modal data sources
We have primarily used a large corpus of plant taxa in
Floras as a source for the FLOPO. However, an increas-
ing number of automated methods is being developed
to detect traits, phenotypes and species from multi-
modal information sources including photographs [33],
herbarium sheets [34, 35], microscopy images [36], or
schematic drawings. The FLOPO can also be utilized to
integrate data obtained from different sources and anal-
ysis approaches. To achieve this goal, analysis methods
that detect morphological traits and phenotypes in plants
would either output FLOPO classes directly, or the output
of these methods would be mapped to FLOPO classes.
As different data sources and analytic approaches have
different error rates and levels of confidence, data sources
that integrate multi-modal information should provide
different kinds of evidence and additional information,
at least the data source (e.g., the collection of which it
is a part), the type of data (e.g., whether it is textual
data, or photographs), the protocol that was applied to
obtain the data, the data extraction method (e.g., image
analysis, text mining), and the environmental conditions
under which the phenotype has been observed. Differ-
ent ontologies and checklists have been developed to
capture these aspects of scientific data collection. For
example, the Provenance Ontology (PROV-O) [37] can
be used to specify the data source and authoring infor-
mation. The Biological Collections Ontology (BCO) [38]
can be used to specify the plant specimens mentioned in
the species treatments and thereby link to geography and
species concepts. The Plant Experimental Assay Ontology
(PEAO) (https://bitbucket.org/PlantExpAssay/ontology/
raw/v0.1/PlantExperimentalAssayOntology.owl) can be
used to specify the assays that were used to process both
the original plants of which phenotypes were recorded
and the protocols used to collect the data. The EDAM
ontology [39] can be used to specify how the data was
extracted, e.g., whether FLOPO classes were assigned
manually or automatically, and if the latter, whichmethods
were used to extract the information. A crucial compo-
nent in any description of observed phenotypes is the
combination of environmental conditions under which
the phenotypes have been observed, and several ontolo-
gies have been established for this purpose. In particular,
the Environment Ontology (EnvO) [40] covers environ-
ments in which organisms are found and can also provide
relevant classes applicable to plant biodiversity. We have
also attempted to annotate the Floras in our study with
classes from EnvO. However, in contrast to plant mor-
phology and phenotypes, in which we can filter lexical
matches by the syntactic relations between the term refer-
ring to a morphological entity and the term referring
to a quality, we find that environmental conditions are
more difficult to identify precisely using purely lexical
approaches. Especially in Floras, environmental descrip-
tions may be context-specific and require prior knowl-
edge of the area. Future research will include develop-
ing and applying dedicated environmental named entity
recognition approaches [41], as well as using additional
plant-specific ontologies such as the Plant Environment
Ontology [4] to precisely identify and characterize envi-
ronmental conditions.
Automatic generation of phenotype ontologies and
comparison
The initial draft of FLOPO was generated from literature
using a pattern-based approach in order to maintain a
balance between trait descriptions that are actually used
to characterize plants and the totality of all descriptions
that are possible when using the PO and PATO ontolo-
gies. The axiom patterns we use in FLOPO are motivated
primarily by the aim to generate an ontology in which
the basic underlying taxonomy follows the distinctions
made in classifyingmorphological structures in plants and
are comprehensible to domain experts using the ontol-
ogy. However, the axioms we use to define traits and
phenotypes are distinctly different from the axioms used
in other phenotype ontologies [42], including the widely
used Mammalian Phenotype Ontology [43] and Human
Phenotype Ontology [44]. The classes we generate are also
not explicitly declared to be subclasses of quality (from
PATO), as in some other ontologies and applications
[42, 45]; while we do not perform an explicit analysis
regarding the ontological state of our classes, the intention
is that our axioms provide a description of a whole organ-
ism and what must be true of it when having a particular
phenotype. These can either be considered as qualities
of a whole organism (and therefore a subclass of PATOs
Hoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 8 of 11
quality class), or equivalently as subclasses of the whole
organism (a material entity) [6].
The pattern-based approach we use is inspired by recent
suggestions to go beyond the quality-centric approach of
defining phenotypes, and instead explicitly characterize
the configurations of the whole organism that has the phe-
notype, including the parts the organism has or lacks,
the processes it participates in or not, the functions is
has, and the qualities is has or lacks [6, 46, 47]. These
approaches have the advantage of explicitly being able to
utilize knowledge from anatomy or physiology ontologies
[6, 46], and have successfully been applied to integrate
a large number of phenotype ontologies [28, 48]. How-
ever, a difference in axiom patterns to other phenotype
ontologies may increase the effort required in integrat-
ing these ontologies with FLOPO. Should it be required
to treat the classes in FLOPO as subclasses of quality,
all our axiom patterns can further be prefixed with
inheres-in some in order to make them subclasses
of quality. These changes can be applied automatically
without changing any of the inferences we describe [6], or
increasing the expressiveness of the language required to
express the axioms (i.e., OWL 2 EL).
Continuing development of the FLOPO and its annotations
We used largely an automated and data-driven process
to generate the FLOPO. As a consequence, the gener-
ated FLOPO contains several artifacts that are a con-
sequence of the text matching process. In particular, it
contains traits that are not relevant or measured, such
as bark surface area, and may lack traits that are dif-
ficult to identify through a lexical approach. Therefore,
after our largely automatic approach, we have already
started to manually improve both correctness and cov-
erage of FLOPO, and we aim to continue the devel-
opment of the FLOPO further with involvement of
domain experts. For this purpose, we provide an issue
tracker (at https://github.com/flora-phenotype-ontology/
flopoontology/issues) in which FLOPO users can request
changes, ask for new classes to be added, and actively
contribute to the further development of the FLOPO.
One instance of a further manual evaluation of the
FLOPO by domain experts is an ongoing study at Natu-
ralis (involving TH, SMO and RV) to extract homologized
traits and their values, i.e., characters and character
states in the context of evolutionary comparative anal-
ysis, for the economically valuable tropical plant family
Piperaceae. In this study, automatically extracted entities
and respective qualities are scrutinized by botanists, and
their fidelity to the entity-quality context in the source
evaluated. This longer-term study will help to further
improve FLOPO.
We also aim to develop semantic annotations of taxa
with the FLOPO. Currently, we are using a custom text
processing pipeline to extract entity-quality pairs with
the primary aim of building a comprehensive ontology.
However, there is an extensive body of research on ana-
lyzing traits and phenotype found in text; in particu-
lar the CharaParser [49, 50] has achieved high accuracy
in extracting formalized character statements from Flo-
ras, and we intend to evaluate its use in the future.
We plan to apply similar methods and make FLOPO-
based annotations of taxa available using Semantic Web
technologies, and link the taxa to their correspond-
ing International Plant Names Index (IPNI) [51] iden-
tifiers to enable interoperability with databases of plant
traits and phenotypes. IPNI provides a service for URNs
(LSID), which we are currently evaluating among other
services like Identifiers.org (URL based) [52] to pub-
lish the taxon annotations as Linked Open Data (see
Fig. 1).
Conclusions
We have developed the Flora Phenotype Ontology
(FLOPO), an ontology of plant traits and phenotypes
found in Floras and monographs. The FLOPO is an ongo-
ing, community-driven project, and is intended both for
data annotation and as a framework based on which
plant traits can be integrated computationally across all
species and higher taxa of flowering plants.The FLOPO
is being used for annotation of traits, in particular
within the African Plants Database [25], and in ongoing
projects for the annotation and integration of plant trait
data.
Additional files
Additional file 1: Annotation of Floras 1. The file contains the manual
annotation of Salacia erecta, Cucumeropsis mannii, Oxalis, and Basella alba.
The file includes identified phenotypes and the mapping to FLOPO. It also
highlights missing classes in PATO or PO. (XLSX 24 kb)
Additional file 2: Annotation of Floras 2. The file contains the manual
annotation of Salacia erecta, Andropogon chinensis, Oxalis, and Anisopappus
chinensis. The file includes identified phenotypes and the mapping to
FLOPO. It also highlights missing classes in PATO or PO. (XLSX 36 kb)
Additional file 3: Missing PATO classes. A list of missing classes in the
PATO ontology identified by our study. (TXT 4 kb)
Additional file 4: Flora descriptions. The original descriptions of Salacia
erecta, Andropogon chinensis, Oxalis, and Anisopappus chinensis based on
which the manual annotation was performed. (PDF 24 kb)
Abbreviations
BCO: Biological collections ontology; EnvO: Environment ontology; FLOPO:
Flora phenotype ontology; IPNI: International plant names index; LSID: Life
science identifier; NLP: Natural language processing; OBO: Open biological
and biomedical ontologies; OWL: Web ontology language; PATO: Phenotype
and trait ontology; PEAO: Plant experimental assay ontology; PO: Plant
ontology; PROV-O: provenance ontology; URN: Uniform resource name
Acknowledgements
The initial draft of the Flora Phenotype Ontology was created at the 2014
Biodiversity Data Enrichment Hackathon (Leiden, the Netherlands).
Hoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 9 of 11
Funding
The initial draft of the Flora Phenotype Ontology was created at the 2014
Biodiversity Data Enrichment Hackathon (Leiden, the Netherlands), which was
sponsored by the the pro-iBiosphere project (Grant Agreement number
312848), funded by the European Commission under the 7th Framework
Programme. Funding for GVG was provided by the National Science
Foundation (Grant Number: IOS-1340112), the BBSRC national capability in
plant phenotyping (Grant Number: BB/J004464/1) and the FP7 European Plant
Phenotyping Network (Grant Agreement No. 284443). Funding for MS and CW
was provided by the Deutsche Forschungsgemeinschaft (DFG) under grant
no. HI 1538/2-2 (GFBio). RH and MA were supported by funding from the King
Abdullah University of Science and Technology.
Availability of data andmaterials
All data and materials are available from https://github.com/flora-phenotype-
ontology/flopoontology. The FLOPO is available from http://purl.obolibrary.
org/obo/flopo.owl. Evaluation results are available as Additional file 1 and
Additional file 2.
Authors contribution
GG, QG, TH, SS, RV, CW, RH implemented the first version of the FLOPO at the
Leiden Hackathon; MA, RH, MS, CW edited, critically revised and updated the
FLOPO; GG, QG performed the manual evaluation; GVG, GG, QG, TH, JK, SMO,
MS, SS, RV, ES evaluated applications of FLOPO to biological data and helped
to revise FLOPO; all authors contributed to writing the manuscript. All authors
read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethical approval and consent to participate
Not applicable.
Author details
1Computational Bioscience Research Center (CBRC), King Abdullah University
of Science and Technology, 4700 KAUST, 239556900 Thuwal, Kingdom of
Saudi Arabia. 2Computer, Electrical and Mathematical Sciences & Engineering
Division (CEMSE), King Abdullah University of Science and Technology, 4700
KAUST, 239556900 Thuwal, Kingdom of Saudi Arabia. 3College of Medical
and Dental Sciences, Institute of Cancer and Genomic Sciences, Centre for
Computational Biology, University of Birmingham, B15 2TT Birmingham,
United Kingdom. 4Institute of Translational Medicine, University Hospitals
Birmingham, NHS Foundation Trust, B15 2TT Birmingham, United Kingdom.
5Institute of Biological, Environmental and Rural Sciences, Aberystwyth
University, SY23 2AX Aberystwyth, United Kingdom. 6Royal Botanical Gardens,
Kew, Richmond, TW9 3AB Surrey, United Kingdom. 7Botanic Garden Meise,
Nieuwelaan 38, 1860 Meise, Belgium. 8Naturalis Biodiversity Center, P.O. Box
9517, 2300 RA Leiden, The Netherlands. 9Senckenberg Biodiversity and
Climate Research Centre (BiK-F), Senckenberganlage 25, 60325 Frankfurt am
Main, Germany. 10Max Planck Institute for Biogeochemistry, Hans Knoell Str.
10, 07745 Jena, Germany. 11German Centre for Integrative Biodiversity
Research (iDiv) Halle-Jena-Leipzig, Deutscher Platz 5e, 04103 Leipzig, Germany.
Received: 2 December 2015 Accepted: 1 November 2016
RESEARCH Open Access
Extracting drug-enzyme relation from
literature as evidence for drug drug
interaction
Yaoyun Zhang1, Heng-Yi Wu2, Jingcheng Du1, Jun Xu1, Jingqi Wang1, Cui Tao1, Lang Li2 and Hua Xu1*
Abstract
Background: Information about drugdrug interactions (DDIs) is crucial for computational applications such as
pharmacovigilance and drug repurposing. However, existing sources of DDIs have the problems of low coverage,
low accuracy and low agreement. One common type of DDIs is related to the mechanism of drug metabolism: a
DDI relation may be caused by different interactions (e.g., substrate, inhibit) between drugs and enzymes in the
drug metabolism process. Thus, information from drug enzyme interactions (DEIs) serves as important supportive
evidence for DDIs. Further, potential DDIs present implicitly could be detected by inference and reasoning based
on DEIs.
Methods: In this article, we propose a hybrid approach to combining machine learning algorithm with trigger
words and syntactic patterns, for DEI relation extraction from biomedical literature. The extracted DEI relations
are used for reasoning to infer potential DDI relations, based on a defined drug-enzyme ontology incorporating
biological knowledge.
Results: Evaluation results demonstrate that the performance of DEI relation extraction is promising, with an F-measure
of 84.97 % on the in vivo dataset and 65.58 % on the in vitro dataset. Further, the inferred DDIs achieved a precision of
83.19 % on the in vivo dataset and 70.94 % on the in vitro dataset, respectively. A further examination showed that the
overlaps between our inferred DDIs and those present in DrugBank were 42.02 % on the in vivo dataset and 19.23 % on
the in vitro dataset, respectively.
Conclusions: This paper proposed an effective approach to extract DEI relations from biomedical literature. Potential
DDIs not present in existing knowledge bases were then inferred based on the extracted DEIs, demonstrating
the capability of the proposed approach to detect DDIs with scientific evidence for pharmacovigilance and
drug repurposing applications.
Keywords: Drug-enzyme interaction, Pharmacokinetic drug-drug interactions, Semantic graph kernel,
Ontology-based inference, Relation extraction, Literature mining
Background
Drugdrug interaction (DDI) is a situation when one
drug alters the effect of another drug in a clinically
meaningful way [1]. It has been demonstrated as one
of the major causes of adverse drug reactions and a
threat to public health [24]. Existing resources of
DDIs include expert-curated knowledge bases such as
DiDB (http://www.druginteractioninfo.org/), DrugBank
(http:// www.drugbank.ca/), and pharmacy clinical sup-
port systems [5]. Significant efforts have been invested to
incorporate DDIs into various data sources. However,
existing sources suffer from the problems of low coverage
[6], low accuracy [7] and low agreement [8].
Under such circumstance, scientific evidence revealing
the mechanism behind the drug interactions are neces-
sary to provide support for reliable DDI information [9].
One common type of DDIs is related to the mechanism
of drug metabolism. For example, suppose drug A is a
* Correspondence: hua.xu@uth.tmc.edu
1School of Biomedical Informatics, University of Texas Health Science Center
at Houston, Houston, TX, USA
Full list of author information is available at the end of the article
© 2016 Zhang et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Zhang et al. Journal of Biomedical Semantics  (2016) 7:11 
DOI 10.1186/s13326-016-0052-6
substrate of enzyme E, i.e., enzyme E is responsible for
the metabolism of drug A. If the enzyme is inhibited or
induced by drug B, the metabolism process of the drug
A may be affected. Thus, the bioavailability of drug A
could be different than expected, potentially causing
adverse effect [10]. Therefore, drug-enzyme interactions
(DEIs) serve as one type of important supportive evi-
dence for DDIs. Besides, DDIs not explicitly stated in
text may be detected by linking and reasoning over DEIs
published in different scientific articles.
Since newly reported DEIs are rapidly accumulating in
the huge archive of scientific literature [11], text mining
techniques are needed to automatically extract DEIs as
supportive scientific evidence for DDIs [6]. One pilot
work in this direction is [10], which tried to extract the
relations between drugs and enzymes based on proper-
ties of drug metabolism; potential DDIs were then
detected by inference and reasoning. In [10], sentences
in PubMed were stored as parse trees in a database, and
SQL queries consisting of keywords and simple syntactic
and semantic constraints were used to extract DEIs.
SemRep [12], a widely used tool to extract relations from
biomedical literature, also uses rule-based methods to
extract DEI relations.
One problem with current DEI extraction methods is
that their performance tend to be poor [10], given that
sentences in scientific literature tend to be long and have
complex structure. Hence, more data-driven, statistical
methods such as machine learning algorithms are neces-
sary to automatically improve the performance. Further-
more, no biological knowledge of concept hierarchies is
involved in the inference process for DDIs currently. For
example, if the drug Delavirdine is an inhibitor of
CYP3A [13], it could be an inhibitor of all enzymes in
the subfamily of CYP3A, such as CYP3A4. Potential
DDIs between Delavirdine and drugs that are substrates
of CYP3A4 could then be inferred. In this way, more
implicit potential DDIs may be identified.
In this article, we propose a hybrid approach to
extracting DEI relations. First, related drug enzyme pairs
are extracted from sentences using the all-path graph
kernel based machine-learning algorithm [14]. Specific
DEI relation types are then assigned according to trigger
words and syntactic patterns. After that, variations of
drug and enzyme names are normalized to remove re-
dundant relations. In the last step, inference rules are
built based on the drug-enzyme ontology and biological
knowledge about mechanisms of drug metabolism and
interaction. Using these inference rules, the extracted
DEI relations are then used for reasoning and inferring
potential DDI relations.
Our approach differs from existing approaches in
two ways. First, we propose a hybrid method to im-
prove the performance of DEI relation extraction.
Second, we establish an ontology-based inference process,
incorporating hierarchical relations between enzymes.
Our evaluation results using the DEI corpus [15] demon-
strates that our proposed approach outperforms SemRep
significantly. Moreover, implicit DDI relations are inferred
with supportive evidence from DEIs, which may contrib-
ute to existing DDI knowledge bases such as DrugBank.
Methods
Two DEI datasets, consisting of in vivo studies and in
vitro studies, were used in this study. Our method
involves three steps. First, related drug-enzyme pairs
were extracted using an all-path graph kernel based
machine-learning model. Different relation types were
then assigned based on the trigger words and syntactic
patterns. Second, variations of drug and enzyme names
were normalized to remove redundant relations. In the
last step, inference rules were built on the basis of
drug-enzyme ontology and biological knowledge about
mechanisms of drug metabolism and interaction.
Using these inference rules, the extracted DEI relations
were used for reasoning about potential DDI relations.
Datasets
The corpus of DEI relations built by Wu, Karnik et al.
[15] was employed in this study. The DEI relations were
manually curated using 428 related abstracts from Med-
Line [15]. Related abstracts were retrieved from MedLine
using the keywords of probe substrate/inhibitor/inducers
for specific metabolism enzymes in queries. The abstracts
for annotation were randomly selected from the search
results. The abstracts in this corpus were categorized into
two datasets for in vivo studies and in vitro studies,
respectively, in order to accommodate the differences
found between them the two study types. Two example
sentences with DEI relations from the in vivo and in vitro
studies are listed in Table 1.
All the drug enzyme pairs that co-occur in one sen-
tence were considered as candidate DEI pairs. The
interaction relations between drug pairs were labeled
as DEI (positive) or NDEI (negative). Table 2 shows
the statistics from the two datasets.
Table 1 Example sentences with drug enzyme relations from
literature
PMID Study type Sentence with drug enzyme interaction
10223773 in vivo Rifampin (INN, rifampicin) is a potent inducer of
CYP3A4 and some other CYP enzymes.
11353758 in vitro Rifalazil-32-hydroxylation in microsomes was
completely inhibited by CYP3A4-specific inhibitors
(fluconazole, ketoconazole, miconazole,
troleandomycin) and drugs metabolized by
CYP3A4 such as cyclosporin A and clarithromycin,
indicating that the enzyme responsible for the
rifalazil-32-hydroxylation is CYP3A4.
Zhang et al. Journal of Biomedical Semantics  (2016) 7:11 Page 2 of 8
Relation extraction
Our relation extraction method consisted of three
steps. First, we represented sentences with dependency-
based syntactic structures. Second, all-path graph ker-
nels describing the syntactic connections within the
sentences were generated from those representations. A
Support Vector Machine (SVM) classifier was trained
based on the graph kernels to generate a predictive
model and to identify if the candidate drug-enzyme pair
was related. In the last step, trigger words and syntactic
patterns of different mechanisms of metabolism, i.e.,
substrate, inhibitor, inducer, were used for specific
DEI relation assignment.
Sentence representation
Sentences with candidate DEI pairs were represented by
the dependency syntactic structure. For generalization,
specific drug/enzyme names in a candidate DEI pair
were replaced with Drug/Enzyme in a preprocessing
step. For example, CYP2C9 and sildenafil in S1 were
replaced with Enzyme1 and Drug1.
Enzyme1
Drug1
S1: CYP2C9 exhibited substantial sildenafil N-
demethylase activity.
Dependency graph of a sentence was constructed
based on its syntactic parse structure. It was a directed
graph that included two types of vertices: a word vertex
containing its lemma and part-of-speech tags (POS), and
a dependency vertex containing the dependency relation
between words. In addition, both types of vertices con-
tained their positions, which differentiated them from
other vertices. Figure 1(a) illustrates the dependency
graph of S1. Since the words connecting the candidate
entities in a syntactic representation are particularly
likely to carry information regarding their relationship
[16], the labels of the vertexes on the shortest undirected
paths connecting drug and enzyme were differenti-
ated from the labels outside the paths using a special tag
IP. Further, the edges were assigned weights; all edges
on the shortest paths received a weight of 0.9 and other
edges received a weight of 0.3 as in [14]. Thus, the short-
est path is emphasized while also considering the other
words outside the path as potentially relevant.
All-path graph kernel
A graph kernel calculates the similarity between two
input graphs by comparing the relations between com-
mon vertices. The weights of the relations are calculated
using all possible paths between each pair of vertices.
Our method follows the all-paths graph kernel proposed
by Airola et al. [14]. The kernel represented the target
pair using graph matrices based on two sub-graphs. The
first sub-graph represented the structure of a sentence
using the dependency graph; the second sub-graph
Table 2 Statistics of drug enzyme relation datasets
Dataset Abstract Sentence Relation pair True pair
in vivo Train 174 2114 1287 326
Test 44 546 364 110
in vitro Train 168 1894 4337 1360
Test 42 475 1262 348
Fig. 1 Illustration of the all-path graph representation. The candidate interaction pair is marked as Enzyme1 and Drug1. The shortest path between
the enzyme and the drug is shown in bold. In the dependency based sub-graph (a), all nodes in the shortest path are specialized using a post-tag (IP).
In the linear order subgraph (b), possible tags are (B)efore, (M)iddle, and (A)fter
Zhang et al. Journal of Biomedical Semantics  (2016) 7:11 Page 3 of 8
represented the word sequence in the sentence, and each
of its word vertices contained its lemma, its relative pos-
ition to the target pair and its POS; all edges received a
weight of 0.9 as in [14] (please see Fig. 1(b)).
Assuming that V represents the set of vertices in the
graph, calculation of the similarity between two graphs
used two types of matrices: edge adjacent matrix A and
label matrix L. The graph is represented with the adja-
cent matrix A ? R|V| × |V| whose rows and columns were
indexed by the vertices, and [A]i,j contains the weight of
the edge connecting vi ? V and vj ? V if such an edge
exists, and 0 otherwise. In addition, the labels were pre-
sented as a label allocation matrix L ? R|I| × |V|, so that
Li,j = 1 if the j-th vertex had the i-th label, and Li,j = 0
otherwise. Using the Neumann Series, a graph matrix G
is calculated as:
G ¼ LT
X?
n¼1A
nL ¼ LT I?Að Þ?1?I L ð1Þ
This matrix sums up the weights of all the paths be-
tween any pair of vertices, where each entry represents
the strength of the relation between a pair of vertices.
Given two instances of graph matrices G? and G?, the
graph kernel K(G',G' ') is defined as follows:
K G 0; ;G
00
 
¼
X Lj j
i¼1
X Lj j
j¼1G
0
ijG
00
ij 0 ð2Þ
Relation type assignment
After recognizing the related drug-enzyme pairs, the rules
generated from trigger words and common syntactic pat-
terns of various mechanisms of drug metabolism were
used to assign specific relations, i.e., isSubstrateOf, isIn-
hibitorOf and isInducerOf. Some rules of each relation
are illustrated in Table 3. For example, the sentence The
metabolism of MDZ, which is specifically metabolized by
CYP3A4 in humans matches the pattern of Drug me-
tabolized by Enzyme, from which the relation that MDZ
is a substrate of CYP3A4 could be identified. The source
code for relation assignment rules can be accessed follow-
ing the link https://sbmi.uth.edu/ccb/resources/dei.htm.
Concept normalization
In the DEI datasets employed in this study, the drug
names were recognized using DrugBank and regular ex-
pressions of various drug metabolites; enzyme names
were recognized using regular expressions of various
forms of enzymes [15]. Many variations of drugs and en-
zymes were annotated in the dataset. For example,
CBZ is an abbreviation of the drug Carbamazepine.
Both P4503A4 and 3A4 were mentions of the en-
zyme CYP3A4. Hence, drug names and enzyme names
were first normalized to reduce relation redundancy
before the reasoning step. Drug names were normal-
ized to concepts in Unified Medical Language System
(UMLS) [17] using MetaMap [18]. Enzyme names
were normalized to CYP450 enzymes, as defined in the
human cytochrome P450 allele nomenclature database,
http://www.cypalleles.ki.se/. The number of extracted
DEIs were reduced accordingly.
Knowledge representation and reasoning
Drug-enzyme ontology definition
To incorporate the knowledge of drug metabolism with
the extracted DEI relations from biological literature, we
created a DEI ontology. There are two classes in DEI
ontology: Drug and Enzyme. Each extracted drug or
enzyme was considered an individual of Drug or Enzyme
respectively. Further, biological knowledge of mecha-
nisms in drug metabolism were represented by object
properties between Drug and Enzyme in the ontology.
As shown in Table 4, five object properties were defined
between Drug and Enzyme. We implemented the DEI
ontology in OWL 2 (Web Ontology Language) [19].
OWL 2 uses description logic to represent formal se-
mantics for semantic inference. OWL API (Application
Programming Interface) was used for the creation and
manipulation of the DEI Ontology [20].
Drug enzyme ontology based inference
After the ontology was populated, we defined property
chain rules to infer new DDI. The following are three
rules that we defined to infer DDI:
Table 3 Trigger words and syntactic patterns of different DEI
relation types
Relation Trigger words & syntactic patterns
isSubstrateOf Drug  mediated/catalyzed/metabolized by EnzymeEnzyme
 responsible for/contribute to Drug metabolismMetabolism
Drug(Enzyme)
isInhibitorOf Drug  an inhibitor of EnzymeEnzyme inhibitor
(Drug)Enzyme inhibit Drug activity
isInducerOf Drug inducedEnzymeDrug  as a potent inducer
of Enzyme
Table 4 Logic facts definition for drug drug interaction
inference
Drug enzyme relation
isSubstrateOf (d, e) Drug d is metabolized by enzyme e
isInhibitorOf (d, e) Drug d inhibits the activity of enzyme e
isInducerOf (d, e) Drug d induces the activity of enzyme e
Enzyme enzyme relation
isAncestorOf (e1, e2) Enzyme e1 is an ancestor of enzyme e2 in
the enzyme family
Drug drug relation
DDI(d1, d2) Drug d1 and drug d2 have an interaction
Zhang et al. Journal of Biomedical Semantics  (2016) 7:11 Page 4 of 8
Rule 1: isSubstrateOf (d1, e) and isInhibitorOf (d2, e)
- >DDI (d1, d2)
Rule 2: isSubstrateOf (d1, e) and isInducerOf (d2, e) - >
DDI (d1, d2)
Rule 3: isSubstrateOf (d1, e1) and isAncestorOf (e1, e2)
- > isSubstrateOf (d1, e2)
Rule 1 and Rule 2 encode the knowledge that if a given
drug d1 is a substrate of enzyme e, and drug d2 is an
inhibitor/inducer of enzyme e, then drug d1 and d2 have
a potential interaction. Rule 3 defines that the isSubstra-
teOf relation can be inherited by a descendant enzyme
from its ancestors. Similar rules of inheritance were
then defined for the other drug-enzyme relations based
on the enzyme hierarchical relations. The reasoner
HermiT was employed for DDI relation inference,
which could check consistency of ontologies, compute
the classification hierarchy, and explain inferences
(Horrocks, et al., 2012). The ontology can be downloaded
from https://sbmi.uth.edu/ontology/files/DEIOntology.owl.
Experiments
Machine learning (ML) algorithm
SVM algorithms are the dominant ML methods (Segura-
Bedmar et al., 2013) among the existing DDI systems.
This study used the sparse version of RLS, also known
as the least squares SVM, to learn the DEI prediction
model based on the all-path graph kernel [14].
Experimental setup
POS-tags and dependency trees of the datasets were
generated by Stanford parser [21]. We used the standard
evaluation measures (Precision, Recall and F- measure)
to evaluate the performance. We evaluated the perform-
ance of our system on each test dataset after training on
the corresponding training dataset. Because our datasets
were imbalanced with much more NDEI relations then
DEI relations, the same candidate drug-enzyme pair
present in multiple instances may be classified as DEI
in one instance and as NDEI in another. In this case,
we treated this candidate DEI pair as a true DEI pair to
enhance the precision. Hence, the performance evalu-
ation of relation extraction was carried out at the entity-
level instead of the sentence level.
The following systematic analyses were conducted
based on the experiments implemented in our study:
(1)Comparison of DEI relation extraction performance
between the all-path graph kernel based model
(GraphKernel) with the model of java simple relation
extraction (JSRE) [22]. JSRE is another state-of-the-
art relation extraction model. It has demonstrated
comparable performance with the all-path graph
kernel based model in protein-protein interaction
relation extraction [14, 23]. Different kernel options
and parameters provided by JSRE were examined by
10-fold cross validation on the training datasets. The
optimal performance of JSRE was used for comparison
in our study, which was achieved by employing
the shallow linguistic context kernel with default
parameters. Further comparison was made with
the existing knowledge base SemMedDB of literature
relations, which was built using the SemRep system
[12]. To select relations between drugs and genes
from SemMedDB, PMIDs were used as one of the
query constraints, to ensure that the selected relations
were within the same publications as the test datasets.
(2)Comparison of generated DDI relations with
DrugBank: for each drug, we looked into the
overlap between the generated DDI relations with
the DrugBank. Specfically, novel DDI relations
generated in our study were examined by checking
their supportive evidence.
Results and discussion
Performance of drug-enzyme relation extraction
Table 5 illustrates the performance of DEI relation extrac-
tion. As can be seen, JSRE obtained higher recall on both
datasets as compared to the GraphKernel (in vivo: 83.67 %
vs. 85.30 %; in vitro: 57.96 % vs. 71.20 %), while its precision
dropped significantly (in vivo: 86.32 % vs. 72.70 %; in vitro:
75.51 % vs. 61.90 %). Overall, GraphKernel outperformed
JSRE on the in vivo dataset (F1: 84.97 % vs. 78.50 %), with a
slightly lower F1 on the in vitro dataset (F1: 65.58 % vs.
66.20 %). The DEI relations extracted by SemRep are of
only two types (INTERACTS_WITH and INHIBITS),
most of which were of the INTERACTS_WITH type
(105/165). Therefore, only the performance with reference
to recognition of related drug-enzyme pairs was compared
between our method and SemRep. As shown in Table 5,
GraphKernel outperformed SemRep significantly (in vivo:
84.97 % vs. 30.53 %; in vitro: 65.58 % vs. 15.32 %). Besides,
the performance of the in vivo study dataset was much
higher than that of the in vitro study dataset. Specifically,
in the in vitro study dataset, the recall was much lower as
compared to the in vivo study dataset (GraphKernel:
84.97 % vs. 65.58 %; SemRep: 30.53 % vs. 15.32 %).
Table 5 Drug enzyme relation extraction performance
Dataset Method P R F1
in vivo GraphKernel 86.32 % 83.67 % 84.97 %
JSRE 72.70 % 85.30 % 78.50 %
SemRep 60.60 % 20.41 % 30.53 %
in vitro GraphKernel 75.51 % 57.96 % 65.58 %
JSRE 61.90 % 71.20 % 66.20 %
SemRep 55.73 % 8.88 % 15.32 %
Zhang et al. Journal of Biomedical Semantics  (2016) 7:11 Page 5 of 8
Table 6 illustrates the performance of our system in
terms of drug-enzyme relation assignment. After drug
and enzyme normalizations, 30 isSubstrateOf, 29 isInhi-
bitorOf and 7 isInducerOf relations were identified in the
in vivo dataset totally; 62 isSubstrateOf, 67 isInhibitorOf
and 5 isInducerOf relations were identified in the in vitro
dataset. As can be seen, the performance for the isSub-
strateOf relation was relatively higher among the three
relations in both datasets (in vivo: 87.48 %; in vitro:
72.79 %). The performance in the in vitro dataset is
much lower than that in the in vivo dataset, since many
of DEI pairs were already lost in the first stage of recog-
nizing related drug-enzyme pairs (Table 5). The ex-
tracted relations were used to populate the DEI ontology
defined in Section 2.4.1. Totally, the current ontology
contains 104 individuals in Drug, 16 individuals in En-
zyme, and 213 triples for drug metabolism, including 81
isSubstrateOf triples, 96 isInhibitorOf triples, 12 isIndu-
cerOf triples, and 24 isAncestorOf triples.
Performance of drug-drug interaction inference
Evaluation results of inferred DDIs are listed in Table 7.
Totally, 181 DDIs were inferred from the in vivo dataset,
and 376 DDIs were inferred from the in vitro dataset, re-
spectively. For comparison, only relations between drugs
present in DrugBank were examined during evaluation.
Totally, 31 drugs and 40 drugs in the in vivo and in vitro
datasets, were present in DrugBank respectively. For the
drugs present both in our corpus and DrugBank, totally
119 DDIs were inferred from the in vivo dataset, of
which 69 DDIs were not included in DrugBank; 234
DDIs were inferred from the in vivo dataset, of which189
DDIs were not included in DrugBank. As illustrated in
Table 7, the overlap between inferred DDIs in this study
and DrugBank was low (in vivo: 42.02 %; in vitro:
19.23 %). However, by manually checking the supportive
evidences, i.e., the underlying DEI relations for those
DDIs, it was verified that the inferred DDIs achieved a
precision of 83.19 % for the in vivo dataset and 70.94 %
for the in vitro dataset, respectively.
Discussion
DEIs are important supportive evidence for DDIs. This
study applied a hybrid approach for DEI relation extraction
from biomedical literature. Reasoning was then conducted
on the extracted DEIs to infer potential DDI relations, by
incorporating biological knowledge into drug-enzyme
ontology. Evaluation results demonstrated the effectiveness
of our approach: potential DDIs were inferred with reliable
precisions (in vivo: 80.30 %; in vitro: 72.09 %), indicating its
capability to detect DDIs with scientific evidence.
The model of GraphKernel obtained much higher pre-
cision and lower recall than JSRE (Table 5). This demon-
strated that GraphKernel and JSRE have advantages of
different aspects on the DEI datasets. One potential
explanation could be the essential kernel difference be-
tween these two models. JSRE only relies on shallow
linguistic features of text, such as tokens, POS and lem-
mas, while GraphKernel combines shallow linguistic
features with more complex structural syntactic features.
Thus, the constraints of JSRE were relatively relaxed
on the text in comparison with GraphKernel, leading
to the high recall of JSRE and the higher precision of
GraphKernel. Overall, GraphKernel outperformed JSRE
significantly on the in vivo dataset (F1: 84.97 % vs.
78.50 %), with a slightly lower F1 on the in vitro dataset
(F1: 65.58 % vs. 66.20 %). This indicates that there is room
for further improvement in the relation extraction from
the in vitro dataset.
As shown in Table 5, our approach outperformed Sem-
Rep significantly in terms of DEI relation extraction. One
possible reason could be that SemRep is a general infor-
mation extraction tool for biomedical literature, which is
not focused on the DEI relation. On the other hand, our
model was trained on the datasets dedicated to DEI rela-
tions. Another possible reason is that instead of using
rule-based methods as in SemRep, our study applied stat-
istical machine-learning model first to recognize related
drug-enzyme pairs to remove false positive DEI relation
pairs and to improve the performance. As an illustration,
in the sentence the possibility of in vivo drug interaction
of azelastine and other drugs that are mainly metabolized
by CYP2D6, the candidate relation pair of azelastine and
CYP2D6 matches the pattern of the isSubstrateOf rela-
tion. However, it is a false positive relation and is
removed in the first step by the statistical model.
Although for the drugs present both in our corpus and
DrugBank, only 42.02 % of inferred DDIs from the in
vivo dataset and 19.23 % from the in vitro dataset are
covered by DrugBank, manual examination demon-
strated that our approach could find potential DDI rela-
tions with supportive evidence. For example, from the
Table 6 Drug enzyme relation assignment performance
Dataset Relation P R F1
in vivo isSubstrateOf 89.34 % 85.71 % 87.48 %
isInhibitorOf 83.33 % 77.42 % 80.27 %
isInducerOf 71.43 % 57.14 % 63.49 %
in vitro isSubstrateOf 80.15 % 66.67 % 72.79 %
isInhibitorOf 73.88 % 55.37 % 63.30 %
isInducerOf 69.74 % 40.00 % 50.84 %
Table 7 Performance of drug drug relation inference
Dataset P R F1 DrugBankOverlap
in vivo 83.19 % 52.84 % 64.63 % 42.02 %
in vitro 70.94 % 42.11 % 52.85 % 19.23 %
Zhang et al. Journal of Biomedical Semantics  (2016) 7:11 Page 6 of 8
sentence  and is probably caused by inhibition of
CYP3A4 -mediated voriconazole metabolism (PMID:
16890574), we identified that the drug voriconazole is a
substrate of CYP3A4; meanwhile, from the sentence 
oxcarbazepine (OXCZ) are well-known inducers of drug
metabolism via CYP3A4 (PMID: 17346248), we identi-
fied the relation that the drug oxcarbazepine is an in-
ducer of CYP3A4. One potential interaction between
voriconazole and oxcarbazepine could then be inferred,
which is not listed in DrugBank. More examples of
inferred DDIs as well as their supportive evidence from
literature are listed in Table 8.
Despite the fact that our proposed method of DEI rela-
tion extraction achieved a F1 of 84.97 % on the in vivo
dataset, the F1 of 65.58 % obtained on the in vitro data-
set is still low. Based on our empirical observation, the
major reason for the performance difference between
these two datasets lied in the essential difference of their
linguistic structures, which originated from the differ-
ence between the in vivo and in vitro studies. In vivo
studies focus on evaluating the effect of an investiga-
tional drug on other drugs, by checking the changes of
pharmacokinetic parameters. Different from in vivo
studies, in vitro studies can qualitatively provide the
mechanisms of a potential DDI based on the observation
of enzyme kinetics parameters. Thus, sentences in the in
vitro dataset contained more drug enzyme interactions;
whereas they were also much complex than those in the
in vivo dataset, with more multiple clauses, long con-
junctive structures and rare patterns. When we looked
into the errors of DEI relation extraction, especially in
the in vitro dataset, we found that the major causes of
false negative instances include conjunctive structures of
drugs/enzymes (e.g., Studies using the CYP3A4 inhibi-
tors ketoconazole, troleandomycin, and erythromycin),
and the rare patterns uncovered by the statistical model
(e.g. Induction of CYP2C9 would explain the increased
systemic elimination of glipizide). On the other hand,
the major causes of false positive instances include the
inability to catch the context information differentiating
between positive and negative relations (e.g., the word
confirm indicates the uncertainty of the DEI relation
in the sentence  to confirm that fluvoxamine inhibits
CYP2C19), and wrong predictions between drugs and
enzymes across multiple clauses, as in the sentence
Greater inhibition was produced by the less selective
CYP3A inhibitors parathion, quinidine, and ketocona-
zole; CYP1A inhibitors were ineffective..
The above problems should be addressed in the future
to further improve the DEI relation extraction perform-
ance. Specifically, additional advanced methods tailored to
the in vitro dataset should be explored, including auto-
matic pattern recognition methods to identify conjunctive
structures of drugs/enzymes, multiple clauses split before
feature extraction, keyword expansion to indicate the
uncertainty (e.g., to determine and was examined).
One limitation of our current work is the size of the
annotated corpus. For practical usage, we plan to apply
our system to all the related articles in PubMed to ob-
tain a more comprehensive list of DEIs and potential
DDIs. Besides, further improvements of our system may
need to be conducted after evaluation on a larger DEI
corpus. In addition to narrative literature text describing
DEIs, tables of DEIs with details of interactions in the
published full text articles are another valuable resource
to obtain such information that we plan to incorporate.
Extracting DEIs from tables is more straightforward and
potentially have more accurate results as compared to
the text. However, in comparison to accessing titles and
abstracts of articles through MedLine, one problem of
tables is that the automatic access to full text is limited.
Actually, these two resources could be complementary
to each other for mining DEIs from biomedical litera-
ture. In our future work, methods of mining tables from
DEI related articles would be explored. Another draw-
back of our current approach for DDI relation inference
is that the information of specific conditions required
for the occurrence of DEIs and DDIs, such as dosages of
Table 8 Examples of inferred drug drug interactions and supportive evidence from literature
Drugs with interaction Enzyme Evidence
Carbamazepine/oxcarbazepine
quinidine
CYP3A4 We performed a study in healthy volunteers to investigate the relative inductive effect of CBZ and OXCZ
on CYP3A4 activity using the metabolism of quinidine as a biomarker reactionWe confirm a clinically
significant inductive effect of both OXCZ and CBZ. (PMID: 17346248)
Lidocaine fluvoxamine CYP1A2 Lidocaine is metabolized by cytochrome P450 3A4 (CYP3A4) and CYP1A2 enzymesWe conclude that
inhibition of CYP1A2 by fluvoxamine considerably reduces the presystemic metabolism of oral lidocaine
(PMID: 16918719)
Quinidine itraconazole CYP3A4 Quinidine is eliminated mainly by CYP3A4-mediated metabolism Itraconazole increases plasma concentrations
of oral quinidine, probably by inhibiting the CYP3A4 isozyme during the first-pass and elimination phases
of quinidine. (PMID: 9390107)
Propofol orphenadrine CYP2B6 Involvement of human liver cytochrome P4502B6 in the metabolism of propofol orphenadrine, a CYP2B6
inhibitor, reduced the rate constant of propofol by liver microsomes by 38 % (P < 0.05) (PMID: 11298076)
Rifalazil fluconazole CYP3A4 Rifalazil-32-hydroxylation in microsomes was completely inhibited by CYP3A4-specific inhibitors (fluconazole,
)  indicating that the enzyme responsible for the rifalazil-32-hydroxylation is CYP3A4. (PMID: 10923859)
Zhang et al. Journal of Biomedical Semantics  (2016) 7:11 Page 7 of 8
drugs, was not considered. Information of such condi-
tions is also very critical for supportive evidence for DDI
relations, which should be taken into consideration in
the next step.
Conclusion
Our study proposes a hybrid approach of combining
machine-learning algorithm with rule-based patterns
to extract DEIs from biomedical literature, from which
potential DDI relations can be inferred by reasoning.
Evaluation results demonstrate that the performance
of DEI relation extraction outperformed SemRep sig-
nificantly, with a F-measure of 84.97 % on the in vivo
dataset and 65.58 % on the in vitro dataset. Moreover,
potential DDIs not present in DrugBank were also
inferred, indicating that this proposed approach could
be used to detect DDIs supported by scientific evidence of
drug metabolism and interaction.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
YZ, HW, LL and HX were responsible for the overall design, development,
and evaluation of this study. LL, HW and YZ developed the annotation
guidelines and annotated the data set used for this study. JD, JX and JW
worked with YZ on the algorithm development. YZ and HX did the bulk
of the writing, JD and CT also contributed to writing and editing of this
manuscript. All authors reviewed the manuscript critically for scientific
content, and all authors gave final approval of the manuscript for publication.
Acknowledgements
This work was supported by Cancer Prevention & Research Institute of Texas
[R1307]; GM10448301, and LM011945.
Author details
1School of Biomedical Informatics, University of Texas Health Science Center
at Houston, Houston, TX, USA. 2School of Medicine, Indiana University,
Indianapolis, IN, USA.
Received: 15 November 2015 Accepted: 11 February 2016
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 
DOI 10.1186/s13326-016-0049-1RESEARCH Open AccessModularising ontology and designing
inference patterns to personalise health
condition assessment: the case of obesity
Aleksandra Sojic*, Walter Terkaj, Giorgia Contini and Marco SaccoAbstract
Background: The public health initiatives for obesity prevention are increasingly exploiting the advantages of smart
technologies that can register various kinds of data related to physical, physiological, and behavioural conditions.
Since individual features and habits vary among people, the design of appropriate intervention strategies for
motivating changes in behavioural patterns towards a healthy lifestyle requires the interpretation and integration of
collected information, while considering individual profiles in a personalised manner. The ontology-based
modelling is recognised as a promising approach in facing the interoperability and integration of heterogeneous
information related to characterisation of personal profiles.
Results: The presented ontology captures individual profiles across several obesity-related knowledge-domains
structured into dedicated modules in order to support inference about health condition, physical features,
behavioural habits associated with a person, and relevant changes over time. The modularisation strategy is
designed to facilitate ontology development, maintenance, and reuse. The domain-specific modules formalised in
the Web Ontology Language (OWL) integrate the domain-specific sets of rules formalised in the Semantic Web
Rule Language (SWRL). The inference rules follow a modelling pattern designed to support personalised assessment
of health condition as age- and gender-specific. The test cases exemplify a personalised assessment of the
obesity-related health conditions for the population of teenagers.
Conclusion: The paper addresses several issues concerning the modelling of normative concepts related to obesity
and depicts how the public health concern impacts classification of teenagers according to their phenotypes. The
modelling choices regarding the ontology-structure are explained in the context of the modelling goal to integrate
multiple knowledge-domains and support reasoning about the individual changes over time. The presented
modularisation pattern enhances reusability of the domain-specific modules across various health care domains.
Keywords: Obesity, Ontology modularisation, Personalised inference, Physical constitution, Physical activity,
Nutritional habits, Healthy lifestyle, Person, TeenagerBackground
Overweight and obesity are estimated to result in the
deaths of about 320 000 people in western Europe every
year [1]. The prevalence of obesity among children and
adolescents motivated public health organisations to
promote a healthy lifestyle by specifically engaging chil-
dren [1, 2] and adolescents [1, 3, 4]. The initial activity
towards the engagement of individuals consists in the* Correspondence: aleksandra.sojic@itia.cnr.it
Institute of Industrial Technologies and Automation (ITIA), National Research
Council (CNR), Via Bassini 15, Milan, Italy
© 2016 Sojic et al. Open Access This article is
International License (http://creativecommons
reproduction in any medium, provided you g
the Creative Commons license, and indicate if
(http://creativecommons.org/publicdomain/zedesign of a scientifically informed strategy, i.e. the de-
velopment of a model that defines the key features asso-
ciated with obesity. Capturing this knowledge is a
complex task since it often requires understanding inter-
twined relations between various phenotypic parameters
and socio-behavioural aspects of lifestyle [5]. Availability
of technological devices that can register data associated
with physical constitution, physiology, behavioural habits
related to physical activity, and nutrition enables the
acquisition of more specific insight into physical char-
acteristics of individual people and their behavioural
patterns [6]. The interpretation and understanding ofdistributed under the terms of the Creative Commons Attribution 4.0
.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
ive appropriate credit to the original author(s) and the source, provide a link to
changes were made. The Creative Commons Public Domain Dedication waiver
ro/1.0/) applies to the data made available in this article, unless otherwise stated.
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 2 of 17the acquired data involves multiple domains of know-
ledge and the analysis of heterogeneous information.
The relevant data need to be collected, organised, and
integrated in order to provide a feedback that is ap-
propriate to a specific personal profile.
The task of representing personal profiles in a model
that integrates diverse kinds of data provided by various
sources motivates the employment of Semantic Web tech-
nologies [7]. In particular, ontologies are recognised as a
convenient approach to deal with complex and heteroge-
neous information across various domains [810], enab-
ling data interoperability [11, 12] and also knowledge
generation via reasoning [8, 9]. Unlike some alternative
modelling approaches (i.e. relational databases), ontology
models incorporate semantics, formalising and explicating
shared understanding of a domain that can be easier to
reuse across various applications (for the comparison of
ontologies and relational databases see [1316]).
Several studies report the use of ontology and seman-
tic technologies to target obesity (e.g. [2, 17, 18]). Scala
et al. [18] present an e-Knowledge platform, based on a
Web Ontology Language (OWL) [19] ontology and
Semantic Web Rule Language (SWRL) [20] rules,
classifying individuals according to the obesity level
and certain medical conditions (Sarcopenia, Hypertension,
Dyslipidemia, Diabetes, Insulin resistance, Metabolic
syndrome). Arash et al. [17] and Addy et al. [2] present
the preliminary stage of an ontology designed to
support a knowledge-based infrastructure, promoting
healthy eating habits and lifestyles. In particular, Addy
et al. [2] aim to support the ontology-based decision
making across multi-stakeholder partnerships (MSPs)
of the Quebec community involved in the manage-
ment of childhood obesity.
The relevant literature addressing the issues related to
obesity mostly refers to specific scenarios, focusing ei-
ther on adults with certain diseases [18] or on children
within a local community context [2, 17].
Since the public health concerns related to obesity [1]
include various scenarios (e.g. diverse social, geopolitical
and age groups, etc.), a generic model that would cover
diverse knowledge-domains and application-contexts re-
lated to obesity would be beneficial as it could exploit
the full potential of ontology-based modelling that goes
beyond single application (see e.g. [11, 21]).
In order to face the need to model complex and di-
verse aspects of obesity-related knowledge, this paper
in particular deals with:
 Formalisation of generic knowledge related to obesity.
 Specialisation of the generic model into a teenager
tailored model.
 Modularisation to support integration of generic and
specific knowledge. Changes of personal features over time.
 Automatic inference of personal health status that is
relevant for obesity assessment and prevention.
The possibility of having both a general and a specific
model is achieved by adopting a modular design strat-
egy. The core ontology module specifies certain generic
classes that are applicable to any human being and a
generic characterisation of individual health conditions.
The domain-specific ontology modules (applicable to
any person) provide obesity-related classifications. The
core module as well as the domain-specific ontology
modules are formalised in OWL (see the following sec-
tions). The modules specifying sets of rules are modelled
in SWRL and explicitly provide reference values that
support inference and classification of personal profiles
for the population of teenagers. In particular, the
ontology design is driven by the need to track changes in
health condition over time (i.e. the issue that was not
addressed by [17, 18]).
Thus, the ontology model presented in this paper
addresses the problem that was only partially addressed in
the models previously described in the literature (i.e.
[2, 17, 18]) as it faces modelling of personal profiles
on a generic level to support specific inference within
a comprehensive ontology model of the obesity-related
knowledge. The developed ontology formalises infor-
mation about obesity-related human features, enables rea-
soning, and enables information flow and interoperability
between the technological tools and platforms employed
to monitor the changes of health status, behaviour, and
nutritional habits of humans in general, and adolescents
in particular.
The development of this ontology was initiated within
the European research project named PEGASO [22]
whose main goal is the enhancement of self-awareness
and motivation of adolescents towards a healthy lifestyle
[3, 23, 24]. Like some other initiatives (e.g. [2, 17]), the
project is driven by the public health concerns aiming at
the decrease of obesity-related risks to health [3, 25]. The
target population is represented by the future adults
whose behavioural habits at an early age can significantly
impact their health status on a life-long horizon [23]. The
project includes several research initiatives and interven-
tional strategies such as the development of serious games
[3, 26] that should promote a healthy lifestyle, the design
of a life companion [24], the use of wearable gadgets
equipped with sensors to monitor health status [3, 24], the
design of mobile applications such as an e-diary used to
record dietary habits, etc. [3, 24].
In the following sections we first outline the theoretical
and practical context that is relevant in explaining and jus-
tifying the decisions taken during the ontology design
phase. We discuss the modular structure of the ontology
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 3 of 17as related to the methodological approach that considers
ontology-design from two perspectives:
(1) task dependent modelling that faces a particular
application scenario and (2) extrapolation of general
modelling patterns that can be used in a broad context
that goes beyond a single application task. We specific-
ally describe an ontology module that captures the phys-
ical domain and classifies health conditions based on the
assessment of body constitution. In the second part of
the paper we present the inference patterns that are used
in the current version of the ontology. In order to exem-
plify the employment of reasoning patterns, we provide
the case of reasoning over a personal assessment of
health condition by combining OWL and SWRL rules.
The concluding remarks outline some advantages of the
modular structure and inference patterns, discussing the
potential of their reuse in other application scenarios.
Methods
The aim of this section is to explain and justify the rep-
resentational choices employed in the ontology design.
The initial step in the ontology development includes a
multi-disciplinary analysis that considers a person as a
dynamic agent who is constantly changing in their
interaction with their environment. Thus, the method-
ology for the ontology design includes a detailed specifi-
cation of the modelling domain(s), goal(s), and context
of current scientific knowledge, i.e. theories used to de-
fine the key concepts relevant for capturing obesity-
related knowledge in a comprehensive manner. Since
the domain problem covers several fields of knowledge
that are related to the problem of obesity, the specific
fields are identified as distinct sub-domains of know-
ledge. The identified fields are later used to structure
knowledge into the dedicated ontology modules, each of
which can exist independently as the modules capture
field-specific aspects of human features that are relevant
for the modelling task and are also applicable to a wide
scope of related scenarios. The methodology is in line
with the tradition that considers ontology as an engineer-
ing artifact that is useful to model some aspects of the
world. In other words, we accept the position that in
Artificial Intelligence Systems, what exists is what can
be represented ([27], p. 908909).
A preliminary study of a cross-disciplinary approach
The preliminary analysis of domain-knowledge, as pre-
sented below, is relevant for (1) the specification of the
ontology goal and scope, (2) the methodology for the
ontology design, and (3) the justification of the represen-
tational choices regarding the study of obesity and its
prevention. The impact of cross-disciplinary studies
on the ontology design are considered in the context
of background knowledge and theories that theontology needs to capture formally. The explication of
the design-rationales aims at reducing opacity of the
developed ontology and increasing its re-usability (c.f.
[28], p. 222).
While dealing with the problem of obesity, its character-
isation and prevention, it is important to consider several
factors such as physical (in)activity, physiological (dys)-
function, (un)healthy eating habits, social and psycho-
logical problems [3]. In some cases, one of these aspects
can be more decisive than the others causing overweight
or obesity, whereas in other cases the overweight-
condition (or a related disease) is the result of a combin-
ation of several factors. In order to identify and, poten-
tially, modify the most relevant factor(s) or a specific habit
of an individual that increases the likelihood of developing
an overweight-condition, it would be optimal to consider
ones current state from the perspective of a comprehen-
sive model that captures the features of a human being as
a whole [3]. Such a model can be understood as an ab-
stract representation that aims at integrating the cross-
disciplinary knowledge of humans in a broad context.
Lafortuna et al. [5], Guarneri et al. [3], Caon et al. [23]
and Carrino et al. [24] carried out multidisciplinary studies
to address the issue of obesity and its prevention via em-
ployment of smart devices and persuasive technologies
[23]. The studies considered intertwined relationships be-
tween human individuals and their environment. In par-
ticular, the results of the studies on physical, physiological,
and behavioural aspects of human phenotypes provided a
comprehensive model, i.e. the so-called Virtual Individual
Model (VIM) [5, 23] that is meant to be a theoretical
framework to deal with obesity prevention. The VIM
identifies the key components that influence the
health status of a person with reference to overweight
and obesity, focusing on adolescents in particular. Since
the VIM captures obesity-related knowledge by common
representational means, e.g. natural language definitions,
tables and graphs (readable to competent human experts),
the presented information was not specified in a formal
language. In other words, the VIM lacks a formal seman-
tics and explicitness that would disambiguate its termino-
logical and ontological assumptions in order to structure
the concepts and relations in a comprehensive and
machine-readable form.
An elaboration of the contents of the VIM led to the
identifications of the key targets of the ontology-model:
(1) capture health conditions of individual teenagers; (2)
detect personal obesity-related risk factors; and (3) opti-
mise the information structure in order to provide a per-
sonalised feedback that can motivate behavioural changes
towards a healthy lifestyle.
After the identification of the ontology goal, the next
step in the ontology design was to partition the modelling
domain by specifying the most relevant fields of knowledge
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 4 of 17that can be formalised as independent sub-domain on-
tologies, i.e. the modules that can be later integrated into
the final ontology model (see the following section).
The identification of knowledge sub-domains started
with the analysis of the VIM that characterises human
individual on three levels:
(i) the physical-physiological level (see Physical Status
in [23], p.1812),
(ii) the nutritional level (see Dietary Habits in [23],
p.1813), and
(iii) the psycho-social level (see Psychological Status in
[23], p.1812).
Each of these three levels of the VIM addressed the prob-
lem of obesity and its prevention from diverse disciplinary
perspectives, thus allowing the domain specialists to
contribute with their expertise to a comprehensive view on
the problem. However, since we aim at characterising the
knowledge sub-domains as distinct, coherent and comple-
mentary segments of knowledge, the characterisation of the
levels (i-iii) is insufficient as it lacks a clear demarcation
criterion necessary for the development of independent
ontology modules. For instance, level (ii) includes the char-
acterisation of dietary habits and as such it partly intersects
with level (iii), which also (from another perspective, i.e.
psychological) aims at targeting behavioural habits, e.g.
fruit and vegetable intake. On the other hand, level (i) deals
with both physical and physiological aspects that are
closely related, but nonetheless (ontologically) distinct. In
addition, the physical description in (i), besides body con-
stitution, also includes the characterisation of habits such
as physical activity that actually represents a behaviour. As
such, the characterisation of physical activity captures fea-
tures that are distinct from those used in the description of
some physical parameters related to body constitution.
For the requirements of an ontologically clean and
coherent model that follows proper classification cri-
teria [11, 29, 30], a more specific distinction of the
modelling-domain and its sub-domains is required.
The task at hand is to specify sub-domains of knowledge
in a way that can support a sustainable ontology de-
velopment, thus following a coherent modularisation
approach. Accordingly, we define and combine the
topic-centred and discipline-oriented demarcation cri-
teria, where by discipline we consider any field of
study that is covered by the current educational system
(see e.g. ISCED: International Standard Classification of
Education [31]). On the other hand, the topic-centred cri-
terion considers not merely a topic of study addressed by
some discipline, but the features of the objects targeted by
some study are also taken into account according to the
OntoClean methodological perspective [29]. Thus, the cri-
terion distinguishes on a meta-level kinds of objects thatare targeted by the study (i.e. meta-topic). The identifica-
tion of a meta-topic can be illustrated by the previous ex-
ample of physical activity and physical features of humans
that can be the topics of study addressed by psychologists,
nutritionists, general practitioner, and so on. As a selected
topic of study might target ontologically distinct objects of
interest, we used a meta-topic characterisation to discrim-
inate between static and dynamic parameters, features
changeable over time vs. rigid features, etc. For instance,
while a living being will necessarily have weight and
height, their values will change over time. Likewise, the
date of birth can be considered as a rigid parameter bound
for a person, while the age of an individual changes over
time. Also the characterisation of physical activity might
be considered as a topic that includes the description of
physical features, but from the meta-topic point of view
the description of the activity captures behaviour and not
some static physical features. The description of physical
features might complement the description of physical ac-
tivity, but the two concepts have different meanings as
they capture diverse aspects of the physical reality. Thus,
the demarcation of the topic of interest was performed ac-
cording to an onto-sensitive approach that was used
jointly with the disciplinary criterion to define the ontol-
ogy modules, as described in the following section.
Ontology modularisation
Ontology modularisation is recognised as an important
topic especially regarding the implementation, mainten-
ance, and reuse of ontology [3234]. Despite the fact
that modularisation plays a significant role in ontology
engineering, there is no universally accepted methodo-
logical approach to modularisation [35, 36]. Some ap-
proaches focus on logical criteria (see e.g. [37]),
whereas others address the issue of modularisation
from a broader perspective (see e.g. [35, 36]) arguing
that the choice of a modularisation technique and
methodological approach actually depends on the par-
ticular requirements defined by the modelling goal and
the application scenario.
Even so, the opportunity to modularise an ontology
already in its early developmental stages provides nu-
merous advantages related to its evolution, maintenance
and reuse.
Concerning the most appropriate modularisation strat-
egy for the task at hand as well as for potential future
applications [28, 32, 38, 39], several criteria were com-
bined to define the modules, as outlined in Fig. 1.
First, the linearisation methodology for robust modu-
lar implementation of ontologies [32] is taken into ac-
count by adopting the following design criteria:
 the ontology modules are identified and separated
from the whole;
Fig. 1 The Multidimensional Modularisation Methodology (MMM): the modularisation dimensions identified to support the ontological coherence
on the theoretical level and sustainability on the application level (i.e. independent development, evolution, and validation)
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 5 of 17 ontology maintenance is enhanced by enabling
independent work on single modules;
 modules can evolve independently and new modules
can be added with minimal side effects;
 the differences between different domain-specific
categories are represented explicitly, thus enabling
both human understanding and formal machine
inference.
Furthermore, the validation criterion ([35], p.69) and the
domain coverage criterion ([35], p.74) are taken into
account in order to enable the independent validation of
defined modules by different experts (See Disciplinary
Perspective criterion in Fig. 1). Besides the fact that
multiple fields of knowledge must be captured and
validated independently, the intended formalisation in
two modelling languages (i.e. OWL and SWRL) requires
the language-specific validation [20] that motivates the
separation of the segments formalised in OWL and the
segments formalised in SWRL (see Formal Specification
criterion in Fig. 1).Fig. 2 Ontology modularisation in the case of the Obesity domain: Applyin
identify specific ontology modulesFinally, the main ontology structure is designed accord-
ing to the Multidimensional Modularisation Methodology
(MMM) (Fig. 1) that identifies the following criteria:
(a) the criterion of disciplinary perspectives; (b) the
meta-topic coherence view, i.e. the criterion used to define
(b1) the specific Topic that should be captured within an
ontology module in an onto-sensitive and coherent man-
ner, thus narrowing down the scope of a disciplinary per-
spective and domain coverage; and (b2) features that can
be either dynamic or static (see the previous section). The
meta-topic view is also used to specify the scope of (c) the
integrative-view criterion that is used to identify common
concepts shared across-domains, thus supporting the in-
ter-module integration (e.g. intersecting the Cross-domain
criterion and Human Health Condition topic  the ex-
ample that will be discussed in the Results section).
Figure 2 illustrates the application of MMM to the
Obesity domain, where criterion (a) was crucial in
defining domain-specific modules (see O1-O5 with
the segments T1-T5 and R1-R5), criterion (c) was particu-
larly relevant in defining cross-domain modules (see T6 ing The Multidimensional Modularisation Methodology (MMM) to
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 6 of 17Fig. 2), whereas criterion (b) had an important role in the
characterisation of all the modules, providing an onto-
logically coherent structure. In addition, the validation
criterion focused on Formal Specification, jointly with (b),
is used to distinguish the ontology segments modelled in
OWL (T1-T6) from the rule segments modelled in SWRL
(R1-R5) (see Figs. 1 and 2). The resulting ontology consists
of the following modules:
(O1) PhysicalStatus that captures physical features of
the human body.
(O2) PhysicalActivity that captures the physical
behaviour and habits.
(O3) PhysiologicalStatus that captures certain
physiological parameters.
(O4) NutritionalBehaviour, capturing nutritional habits
and behaviour.
(O5) ApplicationContext that specifies the contextual
information relevant for potential application
scenarios, e.g. geographical location.
(O6) Common module captures cross-domain
information to support the interoperability
across the modules (O1-O5).
Figure 3 presents the modular architecture designed to
support independent development, validation, use, and evo-
lution of the domain-specific ontology modules. The Com-
mon module (O6), formalised in OWL, consists of a TBox
that provides terminological contents and the most generic
specification of classes and relations that are common for
all other modules (O1-O5). On the other hand, the
domain-specific ontology modules (O1-O5) are composed
of a TBox component formalised in OWL (see T1-T6) and
an RBox component containing only domain-specific
SWRL rules (see R1-R5) specified as extension of the corre-
sponding TBox modules. In particular, the following RBoxFig. 3 The modular architecture supporting independent ontology
development, validation, use, and evolution. The sub-domain ontologies
(O1-O5) are composed of domain-specific TBox and RBox modules.
Module O6 consists of a TBox that is common to all other modulesmodules are used for personalised inference and classifica-
tion of individuals within the target population of teenagers:
(R1) PhysicalStatus Rules - used for personalised
assessment of health conditions as based on the
obesity classification.
(R2) PhysicalActivity Behaviour Rules - used for the
assessment related to behavioural habits, e.g.
sedentariness.
(R3) PhysiologicalStatus Rules - related to the
assessment of health conditions based on
physiological parameters, e.g. metabolism rate.
(R4) NutritionalBehaviour Rules - used for the assessment
of nutritional characterisation of individuals as based
on the food and drink intake, e.g. breakfast skipper.
(R5) ApplicationContext Rules - used for the context-
dependent assessment to characterise conditions
that vary across socio-cultural contexts, e.g.
modifying the assessment based on the information
about geographical location.
These RBoxes define the rules based on current know-
ledge of the relationships between the captured parameters
and the reference values acquired by the team of experts,
the World Health Organisation (WHO) reference tables for
the population of adolescents [40], and the most recent lit-
erature in the domain of interest that is provided in the
ontology annotation. On the other hand, the TBoxes are
modelled to be more stable and population independent.
By keeping TBox and RBox separate, the ontology valid-
ation, maintenance, reusability, and evolution are enhanced.
For example, the rules in the RBox are defined according to
the current state of knowledge that defines cut-off values
used in classifying a teenager as obese, over-weight etc. In
case the state of knowledge changes (or a target population
changes), any change in cut-off values specified in the rules
will not impact the ontology as a whole and modifications
can be made only within the rules that contain the up-
dated values. In addition, separating RBoxes specified in
SWRL from the TBoxes specified in OWL enables inde-
pendent and the language-specific validation [20] of the
OWL and SWRL ontology segments.
Results and discussion
This section focuses on the ontology content. While pre-
senting the modelling patterns, we describe the Common
and PhysicalStatus ontology modules, specifying the most
relevant body features as related to measurements and to
several other classes of health conditions that are used to
define the obesity-related status and potential risk factors.
Capturing normative concepts: assessment of obesity as a
health condition
In general terms, a description of a person (e.g. a teen-
ager) via some structural, functional, and behavioural
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 7 of 17characteristics is actually capturing aspects considered
to be relevant to describe his/her phenotype (that might
be a teenager-specific phenotype). The main focus is on
the phenotypic features describing the class to which a
person belongs as determined by the characterisation of
his/her physical and behavioural features [41]. Thus, we
consider that a persons phenotype belongs to the class
obese based on his/her characteristics, description of
which (despite of individual variations) fits to the de-
scription of an obese phenotype that is typical for every
person of a certain gender and age range. We define typ-
ical features of an obese phenotype in terms of a conven-
tional agreement at the current stage of knowledge. The
reference system that we use to characterise the physical
features of an obese phenotype is provided by the World
Health Organisation [25] and it includes the age- and
gender-specific ranges of values, e.g. body mass index of
teenager (see [40]). Moreover, we treat the description of
body constitution as a specific characterisation of pheno-
type that is associated with health condition.
HealthCondition is conceptualised as the most
general class capable of capturing diverse physical and
behavioural features that describe health-related features of
Person. Accordingly, the most general classes of the
ontology, i.e. the class HealthCondition and the class
Person, are defined together with the most relevant object
and data properties in the Common module (Fig. 4), i.e. the
cross-domain TBox composed of the classes and properties
that are common to all the domain-specific ontology
modules. While subclasses of Person, i.e. Male and
Female, are defined in the Common module, the
subclasses of HealthCondition are specified in the
dedicated domain-specific modules (T1-T5 in Fig. 3).
Since the characterisation of a phenotype as obese is
based on the assessment of ones physical constitution
(weight, height, body mass index, considered in the context
of age and gender), the obesity classification is captured
within the PhysicalStatus module which defines
further the subclasses of HealthCondition as the
hierarchy of PhysicalConstitutionConditionFig. 4 Common module classes. Linking of Person and HealthCondit
integration of domain-specific modules used for the gender-specific assess(see Fig. 6). Likewise, other domain-specific modules define
subclasses of HealthCondition (Fig. 5) in an onto-
sensitive manner. Thus, PhysicalActivityCondition
and its subclasses are defined in PhysicalActivityBehaviour
module, NutritionalCondition classes in Nutritional-
Habits module, PhysiologicalCondition classes in
PhysiologicalStatus module. The class HealthCondition
of the Common module enables the integration of the
sub-domain modules via the subsumption relationship
(c.f. Figs. 3 and 5). On the other hand, each of the domain-
specific modules can be used independently as they consist
of domain-specific hierarchies that describe target domains
in a comprehensive and exhaustive manner (see e.g. Fig. 6).
Besides the hierarchies, each of the modules specifies data
properties that define how some obesity-related health con-
dition is assessed. In this way, each module explicates for-
mally evidence for the assessment of some health condition
as based on the current scientific knowledge.
Consider, for instance, the PhysicalStatus module as
the example on which we illustrate formalisation of the
evidence-based assessment of health condition. Figure 6
presents the hierarchy of the relevant health conditions,
specifying the physical constitution that considers adi-
posity, body fat distribution, body mass, and central
obesity. Each of the conditions is associated with a
specific classification and linked to the reference
values that characterise physical features relative to
gender and age [40]. These classifications are distinct
as they are using diverse criteria to describe a condi-
tion of body constitution.
For instance, the criterion of body mass (provided as
body mass index [40]) in one of the classifications is used
to distinguish people as belonging to one of the following
groups: obese, underweight, overweight or normal weight
[25]. According to the classification that considers fat
distribution, a person may be classified either as android
or as gynoid. Figure 6 depicts the hierarchy of health
condition subclasses based on the description of body
constitution via diverse classificatory criteria. Numbers
(1-5) in Fig. 6 associated with the classes stand for theion via the object property isInHealthCondition enables the
ment of health condition
Fig. 5 Extending HealthCondition with the key subclasses. The subclasses of HealthCondition are captured in the domain-specific ontology
modules (O1-O4); the heterogeneous knowledge-domains relevant for the obesity assessment and prevention are integrated via the subsumption relationship
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 8 of 17classificatory criteria used in PhysicalStatus module to
characterise the associated condition via the following
data properties:
In other words, central obesity can be assessed by provid-
ing information on either (2) or (3) or (4); gynoid andFig. 6 Classifying the obesity-related conditions within the PhysicalStatus m
relevant for the obesity assessment distinguish the classes according to
4) Waist To Height Ratio; and 5) Body Fat Massandroid status is assessed based on (2); obesity status is
assessed as one of the body constitution classes as based on
(1); adiposity status is assessed based on (5).
In addition, the classification is annotated with the
reference sources and relevant scientific literature pro-
viding evidence for the classificatory choices. Finally, the
classification (Fig. 6) captures various types of obesity
sub-classifications that are grouped into one (i.e. Physi-
calStatus module) because all of them satisfy the
common criterion of describing phenotype via character-
isation of physical constitution.odule. The criteria used to specify the conditions of physical constitution
1) Body Mass Index; 2) Waist To Hip Ratio; 3) Waist Circumference;
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 9 of 17Regarding the above mentioned specialisation of the
model to characterise health conditions specifically for the
population of teenagers, the inference rules, together with
the reference values, are defined within the domain-specific
sets of SWRL rules. The following subsections present how
the ontology is used in practice to personalise, and auto-
matically asses, an obesity-related health condition. The
personalised inference is achieved by combining OWL-
TBoxes and SWRL-RBoxes that specify inference rules,
particularly considering the population of teenagers.
Combining OWL and SWRL to personalise obesity
assessment
SWRL is an expressive DL-based rule language that
allows specification of rules expressed in terms of OWL
concepts while enhancing the deductive reasoning
capabilities [20, 42]. A SWRL rule is structured as a con-
ditional, consisting of an antecedent (i.e. body), and a
consequent (i.e. head), as illustrated below with the ex-
amples of SWRL rules (see Ax1 - Ax10). SWRL supports
only the conjunctive form, and it does not support ne-
gated atoms or disjunction [20]. The predicate symbols
of a SWRL atom within a rule can include OWL classes,
properties or data types. The SWRL arguments can also
be OWL individuals, data values or variables. In order to
face the undecidability that might accompany the high
expressivity of SWRL, we follow the recommendation
for the use of DL-safe SWRL rules (see [42], p. 113).
In order to personalise inference about someones health
condition, the classification of individuals is performed by
combining the defined SWRL rules with the OWL declara-
tions that formalise the domain-specific yet generic classifi-
cations of health condition and the facts asserted about
instances of classes Person and HealthCondition.
While the domain-specific modules (see T1-T5 in
Fig. 3) provide the hierarchies that are utilised in the in-
stantiation of health conditions, the Common module
(O1) provides the classes used in the instantiation of
basic personal information, including the link between
the person and their health condition. Precisely, the
Common module (O1) specifies Person and Health-
Condition as two key OWL classes. These two classes
are linked by a restriction involving the object propertyFig. 7 A fragment of the model designed to infer automatically personal hisInHealthCondition (Fig. 7), so that an instance
of Person can be associated with one or more health
conditions. Specification of physical and behavioural fea-
tures to characterise directly health condition and only
indirectly a person (via the relationship isInHealth-
Condition) enables the assignment of diverse health
conditions to a person, thus capturing diverse physical
and behavioural features of an individual while tracking
the evolution of the captured health conditions and
associated features over time (the pattern is extrapo-
lated from an analogous approach [43]). In order to
assess a specific condition it is necessary to declare
the key properties describing a person (see P1) and
the time of the condition assessment (H1) that are
specified in OWLs Manchester syntax [44] as the
following restrictions:
The inference patterns are modelled further as the
rules that classify personal health conditions as belong-
ing to some of the HealthCondition subclasses (see
Fig. 6). In other words, the rules identify which class an
assessed condition actually belongs to (based on (P1),
(H1), and data that characterise the assessed condition,
e.g. BMI specified as 1 in (C1)). The reasoning over the
classes and inference of a certain condition attributed to a
specific person, are performed by means of SWRL rules
and Pellet reasoner [45] that makes use of the assertedealth condition
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 10 of 17facts about (1) physical (structural) and functional (meta-
bolic, etc.) features, (2) gender, and (3) the age of a person.
The asserted functional and structural features as well
as age are directly associated with a health condition.
For example the body mass index (BMI) defined via the
data property isCharacterizedByBodyMassIndex
(see 1 in (C1) and Fig. 6) characterises the health condi-
tion of a person that is assessed at specific age (see
isAssessedAtAge in (H1)). In particular, BMI is used
as the criterion to classify people as being in a health
condition that belongs to one of the BodyMassCondi-
tion subclasses (Fig. 6). However, BMI is not sufficient
to classify a person as being in ObeseCondition or
in OverweightCondition. Associating one of the
PhysicalConstitutionCondition subclasses with
a person requires assertion of the facts that define age and
gender of that person [25, 40].
The gender is defined by instantiating a Person as
belonging to one of its subclasses, i.e. classes Male
and Female within the Common module (Fig. 7).
The age of a person at the time when the health
condition is evaluated is crucial information because the
reference values for the assessment are particularly vari-
able in adolescence when body grows and changes [40]. In
order to capture this variability that can impact on the as-
sessment, the classes Person and HealthCondition
are characterised via the object properties and restrictions
specified in (P1) and (H1), thus enabling the age-specific
assessment of health condition.
Having the data related to the date of birth and time of
assessment, we can apply a rule modelled in SWRL [20] in
order to get an age-value associated with a personal condi-
tion assessment, so that all the needed elaborations can be
performed by a reasoning tool without needing to inter-
face with other applications. The rule is specified as
The age calculation rule (Ax0) utilises the SWRL built-ins
defined for various numeric types [20]. Such a software-
independent age calculation facilitates testing, as shown in
the examples that employ rules to infer specific health con-
dition (see Section on the instantiation, Fig. 9).
The following axioms (Ax1 - Ax10) exemplify the sets
of SWRL rules that are defined according to the domain-
specific criteria used to asses some health condition as
based on age and gender. The axioms labelled with the odd
subscripts are specifying inference-rules for the malepopulation, while the even-subscript axioms define the
rules to classify health conditions associated with female in-
dividuals. The examples are just a fragment of the rule sets
formalised within the PhysicalStatus RBox (see R1).
Generally speaking, whenever the conditions specified in
the antecedent hold, then the conditions specified in the
consequent must also hold [20]. The listed rules are struc-
tured to specify in antecedent (body) a variable p that can
have as its extension some of the instances asserted as
members of the class Person (specified in OWL) within
the Common module. The variable h should have as
its extension members of the class HealthCondition.
Age is represented by the variable age (calculated in a
separate rule, Ax0), while gender is specified as a predicate
(either Male or Female) associated with p. For instance,
Axiom 10 can be interpreted in natural language as a
conditional declaring that for any female person of age
between 13 and 17, who is also in health condition that
is characterised by waist to hip ratio greater or equal
0.85, we can infer that the asserted health condition of
that person is AndroidCondition. In this way the
rules lead to new knowledge, thus expanding the
Knowledge Base with new information that classifies
health conditions associated with persons based on the
information describing particular phenotype.
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 11 of 17The following subsection presents how the above-
introduced classes are instantiated in practice and explains
the reasoning steps that exploit specific information at the
time of assessment, date of birth, specific characterisation
of physical features that are all together used to infer a
personal health condition.
The ontology instantiation and testing
The ontology validation was performed on the test cases
designed to capture diverse profiles of teenagers by instan-
tiating classes Person and HealthCondition and
assigning data values to the instances in a realistic manner
across the domain- specific modules. This section illus-
trates the employment of the ontology in the reasoning
over instances by means of an example of the obesity as-
sessment that provides an explanation of the above-
presented design patterns on the application level.
Figure 8 is an extension of Fig. 7 that exemplifies the
instantiation of the classes Person and HealthCondi-
tion. In particular, we will focus on the instance represent-
ing a boy, named Tom, born in October 2000. By declaring
explicitly Toms birth data, body mass index (BMI) value
and the date of BMI assessment, the ontology enriched
with the rules (Ax0 and Ax5) automatically infers that the
condition TomCondition1 associated with Tom is an
ObeseCondition. This fact presents the new informa-
tion inferred via ontology which ABox previously contained
only the facts about Toms birth data, gender, and the data
used to characterise his health condition (i.e. BMI).
Since the goal of the ontology is to capture changes of
phenotypes associated with a person over time, consider
the scenario in which Tom is assigned two assessments
of his health condition at two different time points:
TomCondition1 (assessed in November 2014), and
TomCondition2 (assessed in November 2015) (see
Fig. 9). Since the reference value for the assessment of an
obese condition changes with age (see the age dependent
reference values in Ax1 - Ax10), given that Toms age
changes while his body mass index stays unchanged, only
TomCondition1 is inferred to be ObeseCondition, while
TomCondition2 is classified as OverWeightCondition.Fig. 8 Depicting the reasoning over instances, e.g. the dashed bright arrow
TomCondition1 as an ObeseConditionThe following specification captures this in OWLs
Manchester syntax [44].
The facts resulting from the reasoning can be saved into
the ontology, thus actually enriching the Knowledge Base.(orange) stands for an inferred relationship, automatically classifying
Fig. 9 A fragment of the ontology with the example of inference to the personal health conditions at different time points; TomCondition1 is
classified as ObeseCondition while TomCondition2 is OverWeightCondition
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 12 of 17The inferred facts can be safely added to the Knowledge
Base because the addition of new instances of the assessed
health conditions will not invalidate the previous in-
ferences, thanks to the adopted modelling pattern.
Moreover, the analysed ontology provides the explanation
that Toms condition TomCondition2 is assessed as
OverWeightCondition in November 2015 based on
BMI. On the other hand, the ontology explains that Toms
ObeseCondition is assessed in November 2014 based
on the BMI that characterises TomCondition1.
The same reasoning can be performed with persons of
different ages and genders. The domain-specific RBoxes
specify the inference rules with the reference values rele-
vant for assessing the obesity-related categories of a per-
sons health condition (characterised by measure of waist
circumference, waist to hip ratio etc. as illustrated e.g. in
Fig. 6) by defining a total of 76 SWRL rules specific for the
population of teenagers. Inference to health condition
based on information about body mass assessment speci-
fies 56 rules; the assessment based on body fat distribu-
tion consists of 4 rules; the inference on the presence of
central obesity is performed via employment of 8 rules,
and adiposity condition assessment also specifies 8 rules.
Besides the 76 rules used to classify obesity-related behav-
ioural, physical, and physiological conditions, several rules
are used to calculate derived parameters, such as body
mass index, age, etc. As a comparison, the work by Scalaet al. [18] contains approx. 40 rules, covering only a frac-
tion of the obesity types.
The ontology testing is performed by instantiation of
the classes that capture physical, physiological, and be-
havioural features of individual people (see Figs. 3, 5,
and 6) and then running the Pellet reasoner [45] to
properly classify the asserted health conditions associ-
ated with the instances of persons. Pellet supports infer-
ence over the DL-Safe rules and reports on possible
errors and misuses of SWRL. The ontology editor
Protégé [46] was employed in the creation of TBoxes
and RBoxes.
The ABoxes are managed as separate modules via the
OntoGUI tool [47] that, inter alia, supports fast instanti-
ation of ontology Tbox (see Fig. 10). The modules (i.e.
libraries generated by OntoGUI) can be used afterwards
as inputs for other ontology based applications. The
control panel of OntoGUI enables the creation and
loading of ontology modules from a repository (either
file-based or triple stores) and provides access to
several functional modules (see [47]), including
Individual Manager (Fig. 10) that is a general purpose
tool for the exploration, generation and char-
acterisation of OWL individuals. The user interface of
the Individual Manager is dynamically reconfigured
whenever an OWL class is selected. The dynamic reconfig-
uration is enhanced with the OntoGUIs capacity to extract
Fig. 10 The OntoGui Individual Manager tool instantiates classes Person and HealthCondition, storing the facts about them into dedicated libraries
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 13 of 17information related to the following axioms defined in the
Tbox ontology:
 Equivalent classes, both defined as single classes or
union of classes;
 Subclasses, both defined as single classes or union of
classes;
 Restrictions of any degree if they involve universal
quantifier, existential quantifier, or cardinality
constraints.
Figure 10 presents the case of instantiation of a
subclass of the class Person and assertion of the key
facts about the instance via the OntoGUI Individual
Manager. The facts are automatically stored in the
dedicated ABoxes.
Integrating the privacy concerns and modelling tasks
The above-presented ontology can capture physical
and behavioural features of individuals related to their
health condition that include sensitive information. Ac-
cordingly, the Knowledge Base (KB) [48, 49] architecture
was designed specifically to support management of per-
sonal information by means of the ABox modularisation
(Figs. 11, 12 and 13). Besides the TBox and RBox modules
(Fig. 3), the KB consists of the ABox modules dedicated tothe assertion of facts about instances that represent indi-
vidual people and their health status. More specifically,
the ABox is partitioned into:
 HealthCondition ABox containing instances that
represent the target health conditions and facts
about them, i.e. criteria used to characterise the
assessed condition;
 Person ABox containing instances that represent
persons and basic facts about them, i.e. gender and
date of birth;
 The Integration ABox that imports Person ABox and
HealthCondition ABox and contains assertions of
the links between the facts stored within the
separate ABoxes.
Accordingly, access to the stored data can be managed
separately on each of the three levels. Any assertion of
the facts about instances requires a concurrent access to
both ABoxes that will only jointly associate some health
status with appropriate instances representing persons.
The partition of ABoxes is motivated by ethical concerns
and it aims to support privacy of the health-related data
also on the modelling level.
Figure 13 depicts the complete KB architecture, including
the relationships between the ontology modules: RBoxes
Fig. 11 The modularisation pattern that enables (re)use of a single
domain-specific ontology module (e.g. characterising body constitution);
the ABox partitioning supports protection of personal data
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 14 of 17import the dedicated TBoxes that in turn provide termin-
ology for the reasoning rules formalised in RBoxes. ABoxes
on the other hand import directly RBoxes and indirectly
domain-specific TBoxes and Common TBox, thus enabling
the merging of sub-domain ontologies into the integrated
and populated ontology. Alternatively, the modular
structure enables independent development, employment,
re-use, and evolution of the domain-specific segments.
For example, Fig. 11 presents only one fragment of the
ontology presented in this paper (O1), i.e. the ontologyFig. 12 The modularisation pattern that enables (re)use of two
domain-specific ontology modules, O1 and O2, integrating them with
the partitioned ABox that supports protection of personal datathat captures obesity-related knowledge focused on phys-
ical constitution (see Fig. 6). If the modelling task changes
to include the information related to physical activity and
integrate it with the model that represents physical
constitution, the relevant modules will be imported and
integrated accordingly (see Fig. 12).
Figures 11, 12 and 13 together exemplify a variety of
the modes to (re)use the ontology-modules, where
particular modules are employed according to the
demands of the modelling task that might be focused
on data collection, data retrieval, and/or reasoning
over some of the obesity-related domains such as
personal body constitution, physical and nutritional
behaviour, etc.
Nonetheless, independently of the modelling task, the
module-integration patterns conserve the links
 between the Common module (O6) and Person ABox;
 between the domain-specific ontology modules
(O1-O5) and the HealthCondition ABox.
In other words, the domain-specific ontology modules
(O1-O5) are used to classify individual health conditions,
while the assertions of the facts about the conditions are
stored in the corresponding HealthCondition ABox. The
integrated ABox brings together personal information
and generic obesity-related knowledge (captured as
the OWL classes (T1-T5) and SWRL reasoning rues
(R1-R5)) as it stores both asserted and inferred facts,
including the links that hold between instances of the
classes Person and HealthCondition.
Conclusions
This paper described the ontology that captures several
obesity-related knowledge-domains: Physical Status
Domain, Physical Activity Behaviour-Domain, Physiological
Status Domain, and Nutritional Habits Domain. The
ontology is designed to support flexible use and reuse
of captured information, the interoperability between
technological devices, the integration of collected informa-
tion, and automated inference about personal status over
time. The modular structure is adopted in order to enable
independent development, maintenance, evolution, and
validation, as well as the integration of diverse domain-
specific modules. The modular design enables the use and
combination of the modules according to the needs of a
particular task, while each of the modules can be used sep-
arately from others and the ontology can be extended with
new modules that can be added at a later stage.
Besides the validation and domain-coverage criteria, the
modularisation methodology included the criterion that
distinguishes disciplinary perspectives, the meta-topic cri-
terion, and the integrative-view criterion. In particular, the
Common ontology module was developed to support the
Fig. 13 The modularisation architecture that integrates multiple domain-ontologies and supports protection of personal information via the
ABox partitioning
Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 15 of 17integration and interoperability between the domain-
specific modules as well as tracking the evolution of per-
sonal health condition over time. The combination of two
formal languages motivated partition of the modules into
TBoxes specified in OWL and RBoxes specified in SWRL,
while the ethical concerns motivated partition of the ABox
into the segments that store separately facts about persons
and those about health conditions.
In particular, the paper illustrated how health conditions
are associated with the physical constitution (i.e. obesity-
related) classes, and are then employed to infer automatic-
ally personal health status as age- and gender-dependent.
The forthcoming task is to perform the ontology test-
ing within a Semantic repository that will be developed
and populated with real data (i.e. instances representing
adolescents and their phenotypic features) acquired
through the pilot studies of the PEGASO project ([23]
p. 19). In terms of databases integration and interoperability,
the activities will include mappings between the
ontology and several task-oriented databases developed to
store the data acquired from wearable devices, nutrition-
related questionnaires etc. The research related to the
exploitation of a Semantic repository (see e.g. [50]) will
have to deal further with the compatibility between the
ontology and available technological solutions (e.g.
Stardog [51]) that add certain modelling constraints
in terms of supported OWL2-profiles [52].
Regarding the interoperability and integration with other
ontologies, future work will examine possible links and
alignments [32, 53] with the relevant phenotype ontologies
[54, 55], the reference terminologies and ontologies
[5662], as well as the foundational ontologies [6366].
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 
DOI 10.1186/s13326-016-0104-y
RESEARCH Open Access
TrhOnt: building an ontology to assist
rehabilitation processes
Idoia Berges*, David Antón, Jesús Bermúdez, Alfredo Goñi and Arantza Illarramendi
Abstract
Background: One of the current research efforts in the area of biomedicine is the representation of knowledge in a
structured way so that reasoning can be performed on it. More precisely, in the field of physiotherapy, information
such as the physiotherapy record of a patient or treatment protocols for specific disorders must be adequately
modeled, because they play a relevant role in the management of the evolutionary recovery process of a patient. In
this scenario, we introduce TRHONT, an application ontology that can assist physiotherapists in the management of
the patients evolution via reasoning supported by semantic technology.
Methods: The ontology was developed following the NeOn Methodology. It integrates knowledge from ontological
(e.g. FMA ontology) and non-ontological resources (e.g. a database of movements, exercises and treatment protocols)
as well as additional physiotherapy-related knowledge.
Results: We demonstrate how the ontology fulfills the purpose of providing a reference model for the representation
of the physiotherapy-related information that is needed for the whole physiotherapy treatment of patients, since they
step for the first time into the physiotherapists office, until they are discharged. More specifically, we present the
results for each of the intended uses of the ontology listed in the document that specifies its requirements, and show
how TRHONT can answer the competency questions defined within that document. Moreover, we detail the main
steps of the process followed to build the TRHONT ontology in order to facilitate its reproducibility in a similar context.
Finally, we show an evaluation of the ontology from different perspectives.
Conclusions: TRHONT has achieved the purpose of allowing for a reasoning process that changes over time
according to the patients state and performance.
Keywords: Ontologies, Knowledge representation, Clinical decision support systems in physiotherapy
Background
Whenever a patient is treated in a physiotherapy unit
some amount of information is generated, which includes
the clinical data relevant to the current situation of the
patient, as well as information regarding their personal
habits and family history. This information composes the
physiotherapy record of a patient and must be adequately
modeled in order to be efficiently consulted. Moreover,
it is important to recognize achievements of goals in
order to manage the evolutionary recovery process of
the patient. For that reason, specific protocols must be
defined for specific disorders and customization of exer-
cises is usually needed depending on the circumstances
*Correspondence: idoia.berges@ehu.eus
University of the Basque Country, UPV/EHU, Paseo Manuel de Lardizabal, 1,
20018 Donostia-San Sebastián, Spain
of each patient. Thus, knowledge about state and context
of patients, disorders, phases of protocols, goals, achieve-
ments, and recommended or contraindicated exercises
depending on the patients state must be properly rep-
resented to assist the design of the treatments and to
support some decisions during their execution.
Undoubtedly, information technologies are playing a
relevant role in the research and improvement of the
healthcare domain [1]. Proof of this fact is the plethora of
works that have been published in this area. Since the pur-
pose of this paper is to present an ontology for physiother-
apy, we will restrict the review of the related literature to
three kinds of works: (1) works which address the develop-
ment of ontologies for different areas related to medicine
other than physiotherapy; (2) works that focus on the field
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 2 of 21
of physiotherapy but use technologies other than ontolo-
gies; (3) works where ontologies for physiotherapy are
presented.
Many solutions in the field of medicine [25] tend
towards the use of semantic technologies such as ontolo-
gies, which can play a relevant role in transforming
information into knowledge that facilitates the work of
the physicians. Thus, a great effort has been made on
the development of ontologies that cover medical knowl-
edge [6]. One example is the Foundational Model of
Anatomy (FMA) [7], whose current release contains over
100,000 classes and properties for the OWL represen-
tation of the phenotypic structure of the human body.
Ontologies have also been used for Clinical Decision Sup-
port (CDS) [8] in health-related fields. In [9] an ontology
for cardiac intensive care units is introduced, which cap-
tures the patients vital parameters and provides experts
with a recommendation regarding the treatment to be
administered. In [10] an ontology-based pervasive health-
care solution is presented with the purpose of delivering
e-health services in self care homes, such as recommen-
dations about daily physical activities, changes of room
temperature, etc. due to the residents conditions. The
ontology developed in [11] uses both recent information
taken at the point of care and past patient data stored in
electronic health records to provide support to three clin-
ical applications: triage of pediatric abdominal pain, triage
of pediatric scrotal pain and postoperative management
of radical prostatectomy. In [12] ontologies are used in
the development of a preoperative assessment system to
recommend preoperative tests to patients while in [13] a
lung cancer ontology for categorizing patients and pro-
ducing guideline-based treatment recommendations is
described.
Considering the field of physiotherapy, software for
physiotherapy and rehabilitationmanaging has been avail-
able, on the one hand, as commercial products for some
years [14, 15]. These systems represent the transition
from a paper-based storage to standardized electronic
records, but while they have been specially focused on
data recording and administrative purposes, they do not
use technologies such as CDS that would allow them to
deepen in the use of the data gathered for assisting phys-
iotherapists in diagnosis, treatment definition and patient
monitoring. On the other hand, there exist proposals such
as Gross et al. [16] that use machine learning techniques
to develop a CDS tool for selecting appropriate rehabili-
tation interventions for injured workers; Hawamdeh et al.
[17] that use resilient backpropagation artificial neural
network algorithm to accurately predict the rehabilita-
tion protocols prescribed by the physicians for patients
with knee osteoarthritis; and finally, Farmer [18], which
presents a CDS system based on a Bayesian belief network
for musculoskeletal disorders of the shoulder.
However, to the best of our knowledge, only few
physiotherapy-related works address problems from the
point of view of semantic technologies. Button et al. [19]
present TRAK, an ontology that models information for
the rehabilitation of knee conditions. It aims to standard-
ize knee rehabilitation terminology and integrate it with
other relevant knowledge sources. Although we acknowl-
edge the usefulness of TRAK, we feel that it does not
take advantage of all the capabilities that semantic tech-
nologies offer, especially with regard to reasoning, which
would require greater detail in the definition of concepts.
In [20] Dogmus et al. introduce REHABROBO-ONTO,
an ontology that represents information about rehabilita-
tion robots. This ontology helps in the process of selecting
the right rehabilitation robots for a particular patient or
a physical therapy, by means of a web interface. How-
ever, this solution does not integrate the description of the
patient report and thus it is just a query tool.
In this paper we present TRHONT (Telerehabilitation
Ontology), an ontology whose goal is to assist physiother-
apists in the following daily tasks:
 Recording and searching information about the items
that compose the physiotherapy record of a patient :
providing a means to represent information regarding
age, symptoms, personal and family history, recovery
goals, results of explorations, etc. in a structured way
allows for reasoning about that information. Notice
that it also facilitates interoperability with Electronic
Health Record (EHR) data recorded in other
institutions which also make use of ontologies [21].
 Defining treatment protocols for a specific disorder,
by selecting the exercises that can be performed in
each phase of the protocol : a treatment protocol is
composed usually of different phases that a patient
must go through until their recovery is completed.
Each of the phases contains exercises whose difficulty
level is in line with one that the phase requires.
Among others, the representation of protocols,
phases, and exercises in an ontology allows for a
reasoning-based selection of suitable exercises for
each phase. Moreover, definitions of new protocols,
phases or exercises can be proven consistent by
means of reasoning.
 Identifying in which phase of a treatment protocol a
patient is at some specific moment: resoning plays a
relevant role in the decision making process related to
the evolution of the patient. Thanks to the ontological
description of the state of the patient, and more
precisely of the results that they have achieved when
performing the exercises, the patient can be classified
in one phase of the treatment protocol. However
notice that this classification is not final: it will evolve
alongside the evolution of the patient in the therapy.
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 3 of 21
 Identifying which exercises are most suitable for a
patient at some specific moment: given all the
information that is known about a patient (their
current state, their personal history, their age, etc.)
the ontology provides a means to identify which of
the exercises are recommended or contraindicated
for them at that specific moment. As a result, the
most suitable exercises for the patient can be
detected and suggested.
The contribution presented in this paper focuses on
the rehabilitation of the glenohumeral joint. Neverthe-
less, the general nature of our method makes it repro-
ducible to model any other body structure deserving
rehabilitation.
Methods
In order to achieve the goal described in the previous
section we implemented an ontology following the NeOn
methodology [22]. The NeOn Methodology framework
presents a set of scenarios for building ontologies and
ontology networks. These scenarios are decomposed into
several processes or activities, and can be combined in
flexible ways to achieve the expected goal.
In our case three of the scenarios proposed by NeOn
(scenarios 1, 2 and 4, see Fig. 1) have been combined
to obtain the current version of the ontology, named
TRHONT, which contains over 2400 classes and proper-
ties to represent:
 The physiotherapy record of a patient.
 Movements, exercises and treatment protocols: An
ontology module named KIRESONT (Kinect
Rehabilitation System Ontology) was developed. This
module is imported by TRHONT.
 A description of a selected part of the human body:
We focused on the glenohumeral joint and the body
parts that are related to it. An ontology module
named GLENONT (Glenohumeral Ontology) was
developed. This module is also imported by TRHONT.
 Other relevant information for the physiotherapeutic
domain.
Fig. 1 Scenarios of NEON used in the development of TRHONT. Scenarios for building ontologies and ontology networks that were used in the
development of TRHONT. Adapted from [22]
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 4 of 21
A detailed account of how each of those scenarios was
applied is presented next.
Scenario 1: from specification to implementation
This scenario is composed of the five core activities to
be performed in the development of any ontology: ontol-
ogy requirements specification, scheduling, conceptual-
ization, formalization and implementation.
 Ontology requirements specification: It produces as
output the Ontology Requirements Specification
Document (ORSD), where information such as the
purpose, the scope and the intended uses of the
ontology is described (Table 1). Special attention
must be paid to the definition of groups of
competency questions, which are the set of questions
that the ontology must be able to answer. In our case,
competency questions related with physiotherapy
records, body parts and treatment protocols were
defined, as well as some general-purpose competency
questions that either fall in more than one of those
categories or do not fall in any of them.
Once the ORSD was generated, we performed a
search for candidate knowledge resources. The
search was performed following the activities defined
in Scenarios 2 and 4 of NeOn, which will be
explained later. The outcome of these activities were
the KIRESONT and GLENONT ontology modules.
 Scheduling: The selected ontology network life cycle
was the Six-Phase Waterfall, described in [22],
because apart from the initiation, design,
implementation and maintenance phases that
4-phase cycles usually include, it integrates a reuse
phase and a re-engineering phase.
 Conceptualization and Formalization: Both activities
were performed together to obtain a formal model of
the ontology, where all the classes and properties that
are needed to answer the competency questions were
described by means of a Description Logic [23] (see
Results section).
 Implementation: The formal model was
implemented in the ontology language OWL 2 DL
[24] using Protégé 5.0.0 [25].
Scenario 2: reusing and re-engineering non-ontological
resources
This scenario was used to select non-ontological
resources that represent information related to joint
movements, rehabilitation exercises and treatment pro-
tocols for disorders of the shoulder, and convert that
information into one ontology. Two processes were car-
ried out: reuse and re-engineering. The reuse process
comprises three activities:
 Search non-ontological resources: Among others, a
document about exercises and treatment protocols
for rehabilitation after shoulder dislocation from the
National Health Service (NHS) was found [26].
Moreover, a database of shoulder movements and
exercises from a Kinect-based telerehabilitation
system [27] was considered, as well as a set of
treatment protocols for several shoulder-related
disorders provided by expert physiotherapists. We
restrict the description of the remaining activities to
these resources.
 Assess the set of candidate non-ontological resources:
We performed the assessment keeping in mind the
intended uses of the target ontology (Table 2). In the
case of resources that contain movements the quality
of their description was assessed (i.e. does the
movement indicate the initial and final position?
Does it indicate the ROM?). In the case of resources
that contain exercises, the quality of the description
and the easiness to identify single movements within
those exercises was evaluated. Finally, concerning
resources that contain treatment protocols, we took
into account the number of disorders that were
considered, as well as the existence of phases in those
protocols and conditions to classify patients in phases.
 Select the most appropriate non-ontological
resources: We selected the database of the
Kinect-based telerehabilitation system as a resource
for movements and exercises, due to the richness of
their descriptions, which provide great information
for our reasoning purposes. Moreover, we selected
the pool of treatment protocols provided by expert
physiotherapists since it covers a wide range of
disorders with definition of phases and their
conditions (Fig. 2).
After the reuse process, the re-engineering process was
carried out to obtain an ontology from the gathered infor-
mation. Three activities were performed:
 Non-ontological resource reverse engineering: the
resources were analyzed to identify their underlying
components. In the case of movements, their name,
type (flexion, extension, internal/external rotation,
(horizontal) abduction, (horizontal) adduction),
range of motion, plane (frontal, sagittal, transverse),
initial/final posture, execution and affected body
location were identified. It was also detected that in
some cases a single movement is composed of more
than one submovement that take place
simultaneously but with different values for the {type,
ROM, location} triplet. In the case of exercises their
name and sequence of movements were considered.
As for treatment protocols, their name, related
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 5 of 21
Table 1 Excerpt of the Ontology Requirements Specification Document defined for our ontology
1. Purpose
The purpose of the TrhOnt ontology is to provide a reference model for the representation of the physiotherapy-related information
that is needed for the whole physiotherapy treatment of a patient, since they step for the first time into the physiotherapists office,
until they are discharged.
2. Scope
The ontology will focus on physiotherapy issues related to the glenohumeral joint.
3. Implementation language
The ontology has to be implemented in a formalism that allows classification of classes and realization between instances and classes.
4. Intended Users
 User 1: Physiotherapists
5. Intended uses
 Use 1: To record and search information about the items that compose the physiotherapy
record of a patient.
 Use 2: To help the process of defining general treatment protocols for a specific disorder, by
selecting the exercises that must be performed in each phase of the protocol.
 Use 3 To help the process of identifying in which phase of a treatment protocol a patient is at
some specific moment.
 Use 4: To identify which exercises are most suitable for a patient at some specific moment
given all the information that it is known about him.
6. Ontology requirements
(6.a) Non-functional requirements (not applicable)
(6.b) Functional requirements: Groups of competency questions
 CQG1: Physiotherapy record-related competency questions:
?CQ1.1: What is the patients age?
?CQ1.2: Which health issue does the patient report?
?CQ1.3: Which are the patients recovery goals?
?CQ1.4: How much pain does the patient report on the Visual Analogue Scale (VAS)?
?CQ1.5: Which results are obtained from the exploration of the joint movement of the
patient?
?CQ1.6: What is the physiotherapy diagnostics of the patient?
?CQ1.7: Which is the family and personal past history of the patient?
? . . .
 CQG2: Body-related competency questions:
?CQ2.1: Which are the body parts that compose a more general body part?
?CQ2.2: Which is the laterality of a specific body part?
? . . .
 CQG3: Treatment protocol-related competency questions:
?CQ3.1: Which is the type of a movement?
?CQ3.2: Which body part does a movement refer to?
?CQ3.3: Which range of movement does a movement cover?
?CQ3.4: Which movements compose an exercise?
?CQ3.5: Which exercises compose a phase of a treatment protocol?
?CQ3.6: Which are the conditions that an exercise must fulfill to be a candidate exercise for
a phase of a treatment protocol?
?. . .
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 6 of 21
Table 1 Excerpt of the Ontology Requirements Specification Document defined for our ontology (Continued)
 CQG4: General competency questions:
?CQ4.1: Which are the conditions that a patient must fulfill in order to be in a phase of a
treatment protocol?
?CQ4.2: Which phase is a patient in?
?CQ4.3: Which exercises are recommended for a patient at some specific moment?
?CQ4.4: Which exercises are contraindicated for a patient at some specific moment?
?CQ4.5: Which exercises do patients usually perform badly?
? . . .
7. Pre-glossary of terms
Patient, goal, joint, movement, exercise,. . .
disorder, sequence of phases (which are made up of a
collection of exercises), conditions of the phases,
number of repetitions of each exercise and number of
times the whole phase must be repeated in the same
session were identified.
 Non-ontological resource transformation: A
conceptual model relating the underlying
components identified in the previous activity was
generated.
 Ontology forward engineering: A formal model
expressed in a Description Logic was generated from
the conceptual model and later implemented in
OWL 2 DL using Protégé (see Results section). The
resulting ontology module, KIRESONT, was the
outcome of this scenario.
Scenario 4: reusing and re-engineering ontological
resources
This scenario was used to select ontological resources that
represent the glenohumeral joint and related body parts.
As in the previous scenario, reuse and re-engineering were
performed. More specifically, four activities were carried
out in the reuse process:
 Ontology search: The search for an ontology that
covered only the glenohumeral joint and its related
body parts was unsuccessful, so we expanded the
search to ontologies that cover the whole human
body. Two candidate ontologies were selected:
OpenGALEN [28] and FMA [7].
 Ontology assessment: The assessment was
performed taking into account five criteria: Coverage,
Understandability effort, Integration effort, Reuse
economic cost and Reliability. We restricted the
assessment to the Human Anatomy extension of
OpenGALEN. In the case of FMA, version 4.0 was
assessed (Table 3).
 Ontology comparison: Both ontologies cover the
domain of the glenohumeral joint to an appropriate
extent. Moreover, we think that the hierarchy and
nomenclature used in FMA are much clearer than
those in OpenGALEN, which reduces the expected
man-hours of work and thus the reuse economic cost.
Since an implementation of both ontologies in OWL
exists, both of them are suitable for OWL reasoners.
However FMA includes unsatisfiable classes [29, 30],
as opposed to OpenGALEN, although the literature
has proved that fully satisfiable modules can be
obtained from it [31]. Both ontologies are considered
reliable since they were developed by reputable
institutions and have been used in multiple projects
throughout the years [3235].
 Ontology selection: Given the need of involving a
physiotherapist for pruning the ontology, we opted
for selecting the FMA due to its clarity, always
keeping in mind that we would need to check the
Table 2 Summarized assessment of candidate non-ontological resources
NHS document Database Kinect-based system General treatment protocols
Movements: Quality of description   
Exercises: Quality of description   
Exercises: Easiness to identify movements X  
Protocols: Number of disorders 1  10
Protocols: Phases   
Protocols: Transition conditions X  
A tick () indicates that the resource fulfils the requirement, an X that the resource does not fulfill it, and a hyphen () that the requirement does not apply to that resource
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 7 of 21
Fig. 2 Example of movement and treatment protocol. Example of movement and excerpt of treatment protocol from the selected non-ontological
resources
satisfiability of the glenohumeral joint module once
extracted.
After the reuse process, the re-engineering process was
carried out to obtain the glenohumeral joint module.
More precisely, two activities were performed:
 Ontology re-specification: The scope of the FMA
ontology (with over 104,000 classes and 170
properties) was modified to consider just the
glenohumeral joint and its related classes.
 Ontology re-conceptualization: We pruned the
FMA ontology with the help of a module extractor
[36, 37] and a physiotherapist to obtain the
glenohumeral joint module, used to represent the
concepts about rehabilitation processes of shoulder
pathologies. The outcome was the GLENONT
ontology module (with 2054 classes and 23
properties). The module extractor works selecting
concepts that are logically connected to a list of
concepts passed as an argument. This way we
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 8 of 21
Table 3 Summarized assessment of candidate ontological resources
Requirements OpenGALEN FMA
Coverage It must cover at least the glenohumeral joint and
its related body parts at great detail
 
Understandability effort Pruning supported by a physiotherapist will be
needed to obtain a module about the gleno-
humeral joint. Thus the structure of the ontology
in ontology development tools such as Protégé
must be easy to understand
Too many classes defined at
the top level, it makes it difficult
to understand the actual hier-
archy. Many classes have very
long names, which are difficult
to read.

Integration effort It should be easy to integrate the candidate
ontology with the ontology being developed.
Moreover its implementation must adapt to the
reasoner being used, and be logically satisfiable.
In our case it is sufficient if the glenohumeral joint
module is satisfiable.
 It includes unsatisfiable classes,
but it is known that satisfiable
modules can be obtained from
it [19, 31]
Reuse economic cost It refers to the cost of accessing and using the
ontology, including licensing costs.
30 man-hours. No licensing
fees.
20 man-hours. No licensing
fees.
Reliability The candidate ontology should come from
reliable sources
 
obtained a module of classes and properties
composed of elements connected between them.
In our case we performed an upper hierarchy
extraction using GlenoHumeral Joint" as the only
argument for the extraction process. A concept
selected this way will always be connected with
some other hierarchically or by a property.
Then we performed a clean-up process to remove
those concepts that were clearly not related with
upper limbs (e.g. toe, ankle, pelvis). After that, we
applied another round of the module extractor to
remove orphan" terms that might be left after the
removal. Finally, this new module was presented
to a physiotherapist that checked it manually, and
validated its content removing those terms that
were considered inadequate for the representation
of upper limb pathologies in rehabilitation. This
module proved to be free of unsatisfiable classes.
We also incorporated UMLS (Unifided Medical
Language System [38]) codes for those FMA
classes that had an equivalent class in UMLS. This
resulted in 272 classes from GLENONT for which
alignment axioms appeared in the UMLS
repository, allowing interoperability with other
sources that use this terminology.
Results
We developed a new application ontology, named
TRHONT, which imports both KIRESONT andGLENONT
ontology modules, and contains other physiotherapy-
related information that will be presented next. The
resulting ontology covers the four intended uses men-
tioned in the ORSD (see Scenario 1 in Methodssection),
which are related to the competency questions listed in
that same document.
Results for intended use 1
In this intended use the ontology is regarded as a means to
record and search information about the items that com-
pose the physiotherapy record of a patient. It must be able
to answer the competency questions in groups CQG1 and
CQG2 (see Table 1).
The core class is PhysiotherapyRecord. Each
Patient is related to their physiotherapy record(s),
which is composed of a set of answers.
Patient  ?hasRecord.
PhysiotherapyRecord
PhysiotherapyRecord  ?hasAnswer.Answer
For each of the competency questions of CQG1 a repre-
sentation of its answer was defined within the physiother-
apy record. For example class CA1.4 is used to represent
the answer to CQ1.4: How much pain does the patient
report on the Visual Analogue Scale (VAS)?, and includes
the necessary properties (hasVASvalue) to store the
patients response as well as restrictions in its type and/or
value (double[? 0.0,? 10.0]). When needed, other
classes related to the terms in the competency questions
were defined to representmore complicated concepts (e.g.
MovementExploration).
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 9 of 21
CA1.1
? Answer  ?hasAge.integer[?0]
CA1.4
?Answer  ?hasVASvalue.double[?0.0,?10.0]
CA1.5
? ?hasMovementExploration.
MovementExploration
MovementExploration
 ?hasMovementType.MovementType
?hasLocation.Joint  ?hasROMvalue.
double  ?hasPain.boolean
MovementType
? Flexion unionsq Extension unionsq ExtRotationunionsq
IntRotation unionsq Abduction unionsq Adductionunionsq
HorizAbduction unionsq HorizAdduction
CA1.7
? ?hasPastHistory.
FamilyOrPersonalPastHistoryItem
FamilyOrPersonalPastHistoryItem
? PathologicalCondition  ?hasPatient.
(Self unionsq Relative)  ?hasIntensity.
Intensity  ?hasTimespan.Timespan
DislocationOfLeftGlenohumeralJoint
 PathologicalCondition
Recorded answers about a specific patient are repre-
sented as individuals of classes in the ontology. Hence, the
information about patient with ID patient2015 seen in
Fig. 3 is transformed, among others, into the following set
of triples:
Competency questions in CQG2 can be answered by
means of the GLENONT part of the ontology that was
created in Scenario 4. For example, one relevant prop-
erty in that ontology is constitutional_part, used
to describe meronymy relationships between body parts.
In Fig. 4 we show a snapshot of the class Glenohumer-
alJoint. It illustrates the description of this joint and how
the relations with other body parts and anatomical struc-
tures are represented (e.g. constitutional_part,
constitutional_part_of, nerve_supply).
Results for intended use 2
In the second intended use the ontology is seen as a means
to help the process of defining general treatment proto-
cols for a specific disorder. It should help in the selection
of the exercises that must be performed in each of the
phases of the protocol and it must be able to answer
the competency questions in group CQG3 (see Table 1).
These requirements are covered by the definitions of the
KIRESONT ontology module, which was created from
non-ontological resources in Scenario 2.
Representation ofmovements, exercises and treatment
protocols
A Movement is represented by its initial and final pos-
tures, and is composed of one or more Submovements
that take place simultaneously within that movement.
Simultaneity is needed for movements that occur in
more than one anatomical plane (e.g. diagonals) or which
require the movement of two joints at the same time (e.g.
both right and left glenohumeral joints).
?patient2015 rdf:type Patient?
?patient2015 hasRecord record2015?
?record2015 rdf:type PhysiotherapyRecord?
?record2015 hasAnswer ca1.4?
?ca1.4 rdf:type CA1.4?
?ca1.4 hasVASvalue 0.0?
?record2015 hasAnswer ca1.5?
?ca1.5 rdf:type CA1.5?
?ca1.5 hasMovementExploration movexp1?
?movexp1 rdf:type MovementExploration?
?movexp1 hasMovementType flexion?
?movexp1 hasLocation leftGlenoJoint2015?
?leftGlenoJoint2015 rdf:type GlenohumeralJoint?
?movexp1 hasROMvalue 80?
?movexp1 hasPain false?
?record2015 hasAnswer ca1.7?
?ca1.7 rdf:type CA1.7?
?ca1.7 hasPastHistory phi1?
?phi1 rdf:type DislocationOfLeftGlenohumeralJoint?
?phi1 hasPatient self?
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 10 of 21
Fig. 3 Patient record. Excerpt of the patient record of patient patient2015
Movement ? ?hasComponent.Submovement
For each Submovement its Joint, MovementType
and ROM are represented, which for example can be used
to answer competency questions CQ3.1 to CQ3.3
Submovement  ?hasLocation.Joint 
?hasMovementType.
MovementType  ?hasROMmin.
integer  ?hasROMmax.integer
Mov2.1.5d and Mov2.2.1z are examples of classes
of movements with one and more submovements respec-
tively.
Mov2.1.5d
? Movement  ?hasInitialPosture.
value{Arms on the sides}
?hasFinalPosture.value{Arm remains
separated...}?hasComponent.
(Submovement  ?hasLocation.
GlenohumeralJoint  ?hasMovementType.
Abduction  ?hasROMmin.value{0}
?hasROMmax.value{90})
Mov2.1.5d
 ?hasName.value{Abduction of the
shoulder at 90 degrees}
Mov2.2.1z
? Movement  ?hasInitialPosture.
Fig. 4 Glenohumeral joint. Glenohumeral Joint class description in Protégé
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 11 of 21
value{The initial posture for...}
?hasFinalPosture.value{Arm flexed
and adducted...}  ?hasComponent.
(Submovement  ?hasLocation.
GlenohumeralJoint  ?hasMovementType.
Flexion  ?hasROMmin.value{0}
?hasROMmax.value{180}) ?hasComponent.
(Submovement  ?hasLocation.
GlenohumeralJoint  ?hasMovementType.
Adduction  ?hasROMmin.value{0}
?hasROMmax.value{50})  ?hasComponent.
(Submovement  ?hasLocation.
GlenohumeralJoint  ?hasMovementType.
ExtRotation  ?hasROMmin.value{0}
?hasROMmax.value{90})
Mov2.2.1z
 ?hasName.value{Diagonal of flexion,
adduction and external rotation}
An Exercise is represented as a sequence of move-
ments. Thus, every exercise must have an initial move-
ment, which can be followed by another movement, and
so on (This serves to answer CQ3.4). For example, in the
case of Exer2.1.5d, this exercise is composed of two
movements. The initial movement belongs to the class
Mov2.1.5d, while the second one belongs to the class
Mov2.1.5d_inv.
Exercise ? ?hasMovement.Movement
Exer2.1.5d ? Exercise  ?hasMovement.
(Mov2.1.5d  ?hasMovNum.
value{1})  ?hasMovement.
(Mov2.1.5d_inv  ?hasMovNum.
value{2})  =2 hasMovement.
Movement
A treatment protocol is represented as a sequence
of phases. Each phase contains a sequence of exer-
cises to be performed during that phase, as well as
the conditions that indicate when a patient is in that
phase. Actually, those conditions are the key for the
conceptualization. They specify the Range Of Motion
(ROM) that patients may achieve and the pain they may
report during the performance of the exercises. Next,
the representation of the treatment protocol for lim-
ited flexion of the glenohumeral joint shown in Fig. 2 is
presented:
TreatmentProtFlexGlenoJ
? TreatmentProtocol  ?hasPhase.
(Phase1FlexGlenoJ  ?hasPhaseNum.
value{1})  ?hasPhase.
(Phase2FlexGlenoJ  ?hasPhaseNum.
value{2})  ...
?hasPhase.(Phase5FlexGlenoJ
?hasPhaseNum.value{5})
Phase2FlexGlenoJ
? Phase
?hasExercise.(Exer2.1.1a ?hasExerNum.
value{1})  ?hasExercise.(Exer2.1.1b
?hasExerNum.value{2})  ...
?hasExercise.(Exer2.1.6a  ?hasExerNum.
value{15})  ?hasSeries.value{4}
?hasConditions.Cond2FlexGlenoJ
Cond2FlexGlenoJ
? ?ROMFlex.double[<90.0]  ?ROMExt.
double[<25.0]  ?ROMAbdu.double
[<90.0]  ?ROMAddu.double[<27.0]
?ROMIntRot.double[<45.0]  ?ROMExtRot.
double[<55.0]  ?hasVASvalue.
double[<3.0]
It should be noticed that the set of classes ofmovements,
exercises and protocols in KIRESONT can be extended by
physiotherapists. Currently we are developing a graphi-
cal tool for this purpose (see Context of use of TrhOnt in
Discussion section).
Selection of the exercises to be performed during a phase
Whenever a physiotherapist creates a general treatment
protocol, they can rely on ontology-based reasoning to
select the exercises for each phase. Once the num-
ber of phases of the protocol has been defined along-
side the patient assessment conditions of each phase,
new class descriptions capturing the notion of candi-
date exercises for each phase are automatically gener-
ated and included in the ontology. For example, class
CandExe2FlexGlenoJ describes the candidate exer-
cises for phase 2 of the protocol for patients with
limited flexion of the glenohumeral joint. A candidate
exercise for this phase must be composed of at least
one movement that is allowed in this phase, and more
importantly, all its movements must also be allowed in
this phase.
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 12 of 21
CandExe2FlexGlenoJ
? Exercise  ?hasMovement.
Phase2FlexGlenoJAllowedMov
?hasMovement.Phase2Flex
GlenoJAllowedMov
Phase2FlexGlenoJAllowedMov
? Movement  (MovAbduGJLessEqual90unionsq
MovAdduGJLessEqual27unionsq
MovExtGJLessEqual25unionsq
MovExtRotGJLessEqual55unionsq
MovFlexGJLessEqual90unionsq
MovIntRotGJLessEqual45)
A movement is allowed in a phase if it complies
with the conditions of the phase. As can be seen in
the definition of CandExe2FlexGlenoJ, the move-
ments allowed in phase 2 of the protocol for the lim-
ited flexion of the glenohumeral join must belong to the
classes MovAbduGJLessEqual90, MovAdduGJLess
Equal27, MovExtGJLessEqual25, MovExtRotGJ
LessEqual55, MovFlexGJLessEqual90 or MovInt
RotGJLessEqual45. For example, MovFlexGJLess
Equal90 represents those movements of flexion of the
glenohumeral joint with a ROM lower or equal to 90°.
MovFlexGJLessEqual90
? Movement  ?hasComponent.(Submovement
 ?hasLocation.GlenohumeralJoint
?hasMovementType.Flexion  ?hasROMmax.
double[?90.0])
Specific movements (e.g. Mov2.1.5d, Mov2.2.1z)
are properly classified as subclasses of these sort of class
descriptions (e.g. Mov2.1.5d is classified as subclass
of MovAbduGJLessEqual90), and moreover, exercises
get classified as subclasses of the corresponding candi-
date classes (e.g CandExe2FlexGlenoJ), depending on
the movements they include. More precisely, any exercise
class that only contains movements that are subclasses
of Phase2FlexGlenoJAllowedMov is classified as a
subclass of CandExe2FlexGlenoJ, and will be pre-
sented to the physiotherapist on demand of exercises
for phase 2 of the selected protocol. This happens, for
instance, with Exer2.1.5d.
If they decide to select that exercise class for the proto-
col definition, the following new axiom is created:
Exer2.1.5d  Exe2FlexGlenoJ
Now, Exer2.1.5d will not only be a subclass of
the class for representing candidate exercises for phase
2 (CandExe2FlexGlenoJ), but also a subclass of
the class for representing proper exercises for phase 2
(Exe2FlexGlenoJ).
Classes for representing candidate exercises for other
phases are defined likewise:
CandExe3FlexGlenoJ
? Exercise  ?hasMovement.
Phase3FlexGlenoJAllowedMov
?hasMovement.
Phase3FlexGlenoJAllowedMov
Phase3FlexGlenoJAllowedMov
? Movement (MovAbduGJLessEqual144unionsq
MovAdduGJLessEqual36unionsq
MovExtGJLessEqual40unionsq
MovExtRotGJLessEqual88unionsq
MovFlexGJLessEqual144unionsq
MovIntRotGJLessEqual72unionsq
MovHorAbduGJLessEqual32unionsq
MovHorAdduGJLessEqual112)
Notice thatone of the classes (CandExe3FlexGlenoJ)
subsumes the other (CandExe2FlexGlenoJ), meaning
that all the exercises classified in CandExe2FlexGlenoJ
are also members of CandExe3FlexGlenoJ. This
is considered conceptually correct by physiotherapists,
because at any point they should be able to select milder
exercises, in order, for example, to warm the joint up.
Results for intended use 3
The third intended use gives response to some of the
competency questions defined in CQG4 (see Table 1).
The ontology is used as an artifact to help the process
of identifying in which phase of a treatment protocol
a patient is at some specific moment. This is done by
taking into account the results of the movement capa-
bility explorations of the patient at that time. Analo-
gously to the previous intended use 2, the classification
is guided by the conditions specified in the phases of the
protocols. In this case, conditions regarding the ROM
and the pain are considered. Then, one ontology class
is automatically created for each phase of each proto-
col based on the associated conditions. For example,
the definitions of the classes Patient2FlexGlenoJ
and Patient3FlexGlenoJ that can be seen next rep-
resent those patients who are respectively in phase 2
and 3 of the protocol to treat the limited flexion of
the shoulder.
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 13 of 21
Patient2FlexGlenoJ
? Patient  ?hasRecord.
(PhysiotherapyRecord  ?hasAnswer.
(CA1.4  ?hasVASvalue.double[<3.0])
?hasAnswer.(CA1.5
?hasMovementExploration.
(MovExploFlexGJLessThan90unionsq
MovExploExtGJLessThan25unionsq
MovExploAbduGJLessThan90unionsq
MovExploAdduGJLessThan27unionsq
MovExploIntRotGJLessThan45unionsq
MovExploExtRotGJLessThan55)))
Patient3FlexGlenoJ
? Patient  ?hasRecord.
(PhysiotherapyRecord  ?hasAnswer.
(CA1.5  ?hasMovementExploration.
((MovExploFlexGJBetween90And143unionsq
MovExploExtGJBetween25And39unionsq
MovExploAbduGJBetween90And143unionsq
MovExploAdduGJBetween27And35unionsq
MovExploIntRotGJBetween45And71unionsq
MovExploExtRotGJBetween55And87unionsq
MovExploHorAbduGJLessThan32unionsq
MovExploHorAdduGJLessThan112)
?hasPain.value{false})))
Definitions of the classes with the prefix MovExplo*
refer to one type of movement exploration asses-
sed in a patient. For instance the definition of
MovExploFlexGJLessThan90 describes an explo-
ration of the flexion of the shoulder where the ROM
achieved by the patient is below 90°.
MovExploFlexGJLessThan90
? MovementExploration  ?hasLocation.
GlenohumeralJoint  ?hasMovementType.
Flexion  ?hasROMmax.double[<90.0]
The other explorations are defined likewise. Thus,
whenever a patient presents a movement exploration that
satisfies the definition of any of the MovExplo* classes in
Patient2FlexGlenoJ and reports a VAS value lower
than 3.0, the patient will be classified as belonging to the
class Patient2FlexGlenoJ.
For instance, considering the set of the triples about
patient patient2015 presented in Results for intended
use 1 section, patient patient2015 would be clas-
sified as a Patient2FlexGlenoJ, because they have
reported a VAS value of 0.0 (<3.0) and there exists in their
current physiotherapy record a movement exploration of
flexion of the glenohumeral joint where they achieved
a ROM of 80° (which satisfies conditions of the class
MovExploFlexGJLessThan90). Notice that the clas-
sification of the patient evolves alongside their evolution
in the therapy: if after being in phase 2 and performing
the exercises recommended for that phase the aforemen-
tioned ROM increases to 100° and the patient reports no
pain when performing those exercises, some triple asser-
tions are deleted and some others added. As a result, the
patient would no longer be classified as a patient of phase
2, but as a patient of phase 3 (see previous definition for
Patient3FlexGlenoJ).
Results for intended use 4
In the last intended use, the ontology is regarded as a
means to identify which exercises are most suitable for a
patient at some specific moment given all the information
that is known about them. Three cases are considered:
 Recommended exercises due to the physical state of
the patient: This is done by taking into account the
results of the movement explorations of the patient at
that time. For example, if the movement explorations
of patient2015 indicate that they are in phase 2
(intended use 3) then they have as recommended
exercises those for the patients in phase 2 (that group
of patients is represented by class
Patient2FlexGlenoJ). Then, the following
axiom represents that knowledge:
Patient2FlexGlenoJ  ?recommended.
Exer2FlexGlenoJ
Notice that the exercises of a certain phase were
inferred as shown in the intended use 2.
 Recommended/Contraindicated exercises due to
general physiotherapy knowledge: Some domain
specific axioms have been added to the ontology to
represent general physiotherapy knowledge such as
People with a personal past history of dislocation of
glenohumeral joint should not perform exercises that
contain abduction movements with a ROM greater
than 80° (e.g. class axioms for the left glenohumeral
joint are shown in the following).
PatientPastDislocationLeftGlenoJ
? Patient  ?hasRecord.
(PhysiotherapyRecord
?hasAnswer.(CA1.7
?hasPastHistory.
(DislocationOfLeftGlenoJ 
?hasPatient.Self)))
PatientPastDislocationLeftGlenoJ
 ?contraindicated.
ExerAbduLeftGlenoJGreaterThan80
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 14 of 21
 Recommended/Contraindicated exercises for a
specific patient: The physiotherapist can specify at
any time that an exercise is recommended/
contraindicated for a specific patient. For example
patient2015 should not perform exercises that
contain extension movements.
Patient2015 ? {patient2015}
Patient2015  ?contraindicated.
ExerExtension
ExerExtension ? Exercise  ?hasMovement.
MovExtension
MovExtension ? Movement  ?hasComponent.
(Submovement
?hasMovementType.Extension)
Object properties recommended and contraindicated
have been created to represent this knowledge.
The most suitable exercises for a patient p will be
represented by the named classes Xp such that Xp ?
RecommendedFor(p) but Xp 
? ContraindicatedFor(p)
where
RecommendedFor(p) = {Z| namedClass(Z) ?
?Y ?C(namedClass(Y ) ?
namedClass(C) ? Z  Y ?
p?C ? C  ?recommended.Y )}
ContraindicatedFor(p) = {Z| namedClass(Z) ?
?Y ?C(namedClass(Y ) ?
namedClass(C) ? Z  Y ?
p?C ?C?contraindicated.Y )}
Evaluation
In this section we present a threefold evaluation of our
ontology. First, we show the results of checking our ontol-
ogy using the OntOlogy Pitfall Scanner OOPS! [39] in
order to diagnose potential design errors. Then, an eval-
uation of the ontology using criteria related to ontology
quality is presented. Finally, a list of several ontology
metrics regarding its size and composition is shown.
Detection of potential pitfalls
OOPS! evaluates an ontology against a catalogue of 41
potential pitfalls classified in three levels (critical, impor-
tant, minor). We performed an evaluation of our ontol-
ogy and corrected the reported pitfalls. As a result, we
obtained the fixed current version of the ontology. How-
ever we feel the need to introduce here some of the pitfalls
that were related to the GLENONT module, because these
piftalls are also present in FMA ontology. Tables 4 and 5
respectively present the critical and important pitfalls ini-
tially reported by OOPS!. For each pitfall we indicate its
code, description, where in the ontology it appears, the
reason why the pitfall is flagged and other useful informa-
tion that is needed to understand it, its implications and
how we corrected it.
Ontology quality
Next we evaluate our ontology against several quality cri-
teria described in [40], which are presented as part of a
common framework for aspects of ontology evaluation.
Accuracy: The definitions and descriptions in the ontol-
ogy agree with the experts knowledge about the
domain. The GLENONT ontology module was
obtained from the well-known FMA ontology. The
KIRESONT module and the information regarding
patients were developed using actual physiotherapy
records and recovery protocols, and with the support
of physiotherapists.
Adaptability: We have opted for implementing the
ontology in several modules that are related to each
other via import clauses. The file GlenOnt.owl1
contains the GLENONT ontology module. The
KIRESONT ontology module has been divided into
two files: KiReSOntFM.owl2, which contains generic
classes and properties for describing movements,
exercises and protocols, and KiReSOnt.owl3, which
contains the descriptions of specific movements,
exercises and protocols (e.g. Mov2.1.5d). Finally,
the main file TrhOnt.owl4, incorporates the patient
record, general axioms about physiotherapy and the
relations to the other files. This choice enhances
extensibility and reusability, and makes the ontology
be easily adaptable to several contexts. Moreover, we
provide a merged file5 with all the resources.
Clarity: All the terms in the ontology have been
given a non-ambiguous label or description using
rdfs:label or rdfs:comment, so that the
ontology communicates effectively the intended
meaning of those terms. For example, class CA1.4
has been described as Answer to the question How
much pain does the patient report on the Visual Ana-
logue Scale (VAS)?, while class Mov2.1.5e has
been described as Movement of abduction of the
shoulder at 90 degrees.
Completeness: This feature measures whether the ontol-
ogy can answer all the questions that it should be able
to answer, that is, how well the ontology represents
the domain it models. Those questions were speci-
fied in the ORSD and it has been checked carefully
that all of them can be answered.
Computational efficiency: It must be admitted that the
GLENONT module as a whole is still too big for
some of our purposes. Current DL reasoners are not
able to handle it in what we consider reasonable
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 15 of 21
Table 4 Critical pitfalls
Code P28: Defining wrong symmetric relationships
Description A relationship is defined as symmetric when the relationship is not
necessarily symmetric.
Appears in SymmetricProperty(continuous_with)
Reason the domain of continuous_with is different from the range of
continuous_with (Material Anatomical Entity vs.
Physical Anatomical Entity).
Other useful information subClassOf(Material Anatomical Entity,
Physical Anatomical Entity)
Implications Let Material Anatomical Entity(x), Physical
Anatomical Entity(y), continuous_with(x,y). Due to
SymmetricProperty(continuous_with), the reasoner infers
that Physical Anatomical Entity(x) and Material
Anatomical Entity(y)
Correction Change the domain of continuous_with to Material
Anatomical Entity.
Code P05: Defining wrong inverse relationships
Description Two relationships are defined as inverse relationships when they are not
necessarily inverse.
Appears in inverseOf(continuous_with,continuous_with)
Reason the domain of continuous_with is different from the range of
continuous_with (Material Anatomical Entity vs.
Physical Anatomical Entity).
Implications Let Material Anatomical Entity(x), Physical
Anatomical Entity(y), continuous_with(x,y). Due
to inverseOf(continuous_with,continuous_with), the
reasoner infers that Physical Anatomical Entity(x) and
Material Anatomical Entity(y)
Correction This pitfall corrects itself as a result of correcting pitfall P25 (see Table 5)
time6. However, we must distinguish two uses of
GLENONT: when the physiotherapist is defining new
movements, exercises or protocols they would have
the whole GLENONT module at their disposal, so
that they can choose from awide range of body parts,
because in this case the purpose of GLENONT is
annotation and not reasoning. Thus, computational
efficiency will not be an issue in this case. Once
the definitions have been made, the handful of the
classes of GLENONT that are used within them can
be used as seeds in the module extractor in order
to obtain on the spot a lighter module to be used
in those moments where reasoning is necessary (e.g
when asking for the exercises that are recommended
for a patient). We are currently working on a tool
that given a set of treatment protocols, automatically
reduces the number of terms in the GLENONT mod-
ule by using the module extractor and the terms used
to describe those protocols, in such a way that no
semantic loss is involved.
Conciseness: Since the development of the ontology was
made with the help of physiotherapists, we asume
that the ontology does not contain irrelevant terms
with regard to the domain that is being covered.
Moveover, checking our ontology with OOPS! has
discarded the presence of redundant terms (see pit-
fall P30 in Table 5).
Consistency: Reasoning was performed on the ontology
using Fact++. No inconsistencies were found.
Ontology metrics
In Table 6 a summary of ontology metrics obtained from
the Protégé development framework can be found. These
metrics are related to the size of the ontology and its
components.
Discussion
TRHONT is an application ontology that can assist physio-
therapists in themanagement of the patients evolution via
reasoning supported by semantic technology. We can find
in the literature many ontologies (e.g. [4244]) that have
been built with the purpose of supporting a precise and
comprehensive semantic annotation of resources. How-
ever, TRHONT goes an step further and it also provides a
framework where the reasoning process takes a relevant
role.
TRHONT contains terms from the well-known FMA
ontology, that covers the whole anatomical structure of
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 16 of 21
Table 5 Important pitfalls
Code P11: Missing domain or range in properties
Description Object and/or datatype properties without domain or range (or none of them) are
included in the ontology.
Appears in For example: http://purl.org/sig/ont/fma/part
Reason All the cases refer to meronimy relations that can be applied to any of the classes of the
ontology.
Other useful
information
subClassOf(Physical Anatomical Entity, Material
Anatomical Entity)
Implications None.
Correction We chose not to change anything, since it is not an error per se, just an implication of the
current domain.
Code P25: Defining a relationship as inverse to itself.
Description A relationship is defined as inverse of itself.
Appears in inverseOf(continuous_with, continuous_with),
inverseOf(articulates_with, articulates_with)
Reason This relationship could have been defined as owl:SymmetricProperty instead.
Correction Remove both inverseOf axioms. SymmetricProperty(continuous_with)
and SymmetricProperty(articulates_with) already existed in the ontology.
Code P26: Defining inverse relationships for a symmetric one.
Description A symmetric object property is defined as inverse of another object property.
Appears in inverseOf(continuous_with, continuous_with),
SymmetricProperty(continuous_with), inverseOf(articulates_
with, articulates_with), SymmetricProperty(articulates_with)
Correction This pitfall corrects itself as a result of correcting pitfall P25.
Code P24: Using recursive definitions.
Description An ontology element is used in its own definition.
Appears in continuous_with, articulates_with,Frontal part of head
Other useful
information
Frontal part of head ? attributed_part.(?(related_part.
Frontal part of head)  (1 partition.{Partition}))
Correction The problems concerning continuous_with and articulates_with correct
themselves as a result of correcting pitfall P25. Moreover, we feel that the aforementioned
axiom involving Frontal part of head is correct, so we chose not to change it.
Code P34: Untyped class.
Description An ontology element is used as a class without having been explicitly declared as such
using the primitives owl:Class or rdfs:Class.
Appears in Anatomical entity
Correction owl:Class(Anatomical entity) added to the ontology.
Code P30: Equivalent classes not explicitly declared.
Description Missing the definition of equivalent classes (owl:equivalentClass) in case of
duplicated concepts.
Appears in Cheek vs. Face, Ear vs. Pinna, Mouth vs. Lip, Limb vs. Arm
Reason The names of both classes appear in a common synset (set of synonyms) in WordNet [45].
Correction None. We checked each of the suggestions by looking up in WordNet the synsets where
each pair appears. For example, Cheek and Face appear in a synset with terms such as
Boldness, Nerve and Brass, refering to impudent aggressiveness, not the body part, which is
the meaning intended in the ontology. Same applies to the other pairs. Thus, we did not
change anything in the ontology
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 17 of 21
Table 6 Ontology metrics
Metrics
Axiom 28181
Logical axiom count 6161
Class count 2351
Object property count 65
Data property count 35
Individual count 134
DL expressivity ALCROIQ(D)
Class axioms
SubClassOf 4982
EquivalentClasses 216
DisjointClasses 617
GCI count 0
Hidden GCI count 199
Object property axioms
SubObjectPropertyOf 7
EquivalentObjectProperties 0
InverseObjectProperties 5
DisjointObjectProperties 0
FunctionalObjectProperty 0
InverseFunctionalObjectProperty 0
TransitiveObjectProperty 0
SymmetricObjectProperty 2
AsymmetricObjectProperty 0
ReflexiveObjectProperties 0
IrreflexiveObjectProperty 0
ObjectPropertyDomain 49
ObjectPropertyRange 52
SubPropertyChainOf 3
Data property axioms
SubDataPropertyOf 0
EquivalentDataProperties 0
DisjointDataProperties 0
FunctionalDataProperty 9
DataPropertyDomain 29
DataPropertyRange 35
Individual axioms
ClassAssertion 143
ObjectPropertyAssertion 10
DataPropertyAssertion 2
NegativeObjectPropertyAssertion 0
NegativeDataPropertyAssertion 0
SameIndividual 0
DifferentIndividuals 0
Annotation axioms
AnnotationAssertion 19402
AnnotationPropertyDomain 0
AnnotationPropertyRangeOf 0
the human body, as well as terms that describe the phys-
iotherapy records of patients, and also movements of
body parts, exercises for physiotherapy and treatment
protocols. The selection of terms was made with the
support of physiotherapists, and their descriptions turn
TRHONT into an actionable tool for physioterapists in
their daily work. Moreover, UMLS codes have been also
incorporated to some terms imported from the FMA
ontology, favouring in this way interoperability with other
sources that use this terminology. TRHONT is still in
active development, and we expect it to grow consider-
ably, in the area of physiotherapy; however, in the current
state it contains enough terms for supporting open contri-
butions from professionals desiring to populate the ontol-
ogy with more therapy elements. That is to say, TRHONT
is ready to be used.
Next, some of the decisions made during the develop-
ment of TRHONT are presented. Moreover, the scope and
context of use of the ontology are also discussed.
Decisions in the development of TRHONT
During the development process, different alternatives
have been considered, and we present in the following
some of the choices we made:
Representation of the physiotherapy record
The core class in the representation of the physiother-
apy record of a patient is PhysiotherapyRecord. This
class is related to Answer, whose subclasses (e.g CA1.1)
represent the answers to competency questions about
the physiotherapy record (see Results for intended use
1 section). This representation facilitates the extensibility
of the model, since the addition of new competency ques-
tions related to the physiotherapy record will not interfere
with the current ones.
Storage of patient information
As we indicated in Results for intended use 1 section,
recorded answers about a specific patient are represented
as individuals of classes in the ontology. However, having
all the information about all the patients always stored in
the assertional box of the ontology would take an unnec-
essary toll in the efficiency of any EHR system that uses
the ontology. Thus, we envisage a use of the ontology
where the patient information and the terminology part
of the ontology are kept separate. Whenever a physiother-
apist is treating a patient, only the information of that
patient is loaded onto the ontology, and then discharged
when the interaction is over. Moreover, specific classes
such as Patient2015 (see Results for intended use
4 section) are also temporary. They will be created when
reasoning about the recommended/contraindicated exer-
cises for a patient and deleted once the reasoning process
is over.
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 18 of 21
Representation ofmovements, exercises and treatment
protocols
In order to select the right descriptions of those core terms
in the physioterapy treatments, we collaborated closely
with physiotherapists. The idea was to get descriptions
that mimic their conception of the elements related to
the treatments. Therefore, a movement is represented
by its initial and final postures, and is composed of
one or more submovements that take place simultane-
ously within that movement. An exercise is represented
as a sequence of movements, and a treatment proto-
col is represented as a sequence of phases. Each phase
contains a sequence of exercises to be performed dur-
ing that phase, as well as the conditions that indicate
when a patient is in that phase. The sequential nature of
the descriptions allows checking proper concatenation of
them.
Scope of TRHONT
Although in this paper we have focused on the rehabilita-
tion of the glenohumeral joint, the exposedmethod can be
easily reproduced in order to cover other body structures
subject to rehabilitation. The same module extractor that
was used as a basis in the creation of GLENONT can be
used to perform hierarchical extractions of concepts using
other body structures as argument for the extraction pro-
cess. Other parts of TRHONT (e.g. the patient record) will
not be affected by a change in the selected body structure.
The scaffolding of the phases, exercises and movements
can be reused. Specific information to that body structure
will have to be added (e.g. if instead of the glenohumeral
joint the ankle is selected, then specific exercises for the
recovery of the ankle will have to be defined).
Context of use of TRHONT
TRHONT is a nuclear part of a telerehabilitation sys-
tem called KiReS, a Kinect-based system which covers,
on the one hand, the needs of physiotherapists in the
process of creating, designing, managing and assigning
physiotherapy treatment protocols, as well as evaluating
the performance of patients in those protocols; and, on
the other hand, the needs of the patients, by providing
them an intuitive and encouraging interface for perform-
ing exercises, which also gives useful feedback to enhance
the rehabilitation process.
Most technical details related to the creation of classes
and the behavior of TRHONT are encapsulated in KiReS.
Its interface handles aspects concerning, among others,
the creation of exercises and the design of protocols. In
order to be able to design adequate protocols for the
patients, first of all, exercises must be created in KiReS.
Postures and movements, basic components of the exer-
cises, are recorded by being performed in front of Kinect.
Later, they are combined to create exercises. The inter-
face provides step by step assistance in this process.
Next, we present two snapshots that show respectively the
appearance of the interface for recording movements and
creating exercises.
In Fig. 5 the interface for recording new movements is
shown. The definition of a movement requires, at least,
to assign a name that identifies the movement and to
select the initial and final postures that the movement will
Fig. 5Movement recording. Interface for recording new movements in KiReS
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 19 of 21
have. Then, the physiotherapist can visualize in the inter-
face two avatars that show these postures and proceed to
record the transition between the postures that best mim-
ics the optimal execution of the movement. After reaching
the final posture, a recording player tool is available and
the physiotherapists can replay the movement and decide
whether to store it or not.
Moreover, exercises are created by a combination of
one or more movements. The only restriction when com-
bining movements is that the final posture of a move-
ment must match the initial posture of the next one. The
interface for creating exercises (see Fig. 6) allows physio-
therapists to define the composition of exercises. In the
left side of the interface the name and description of the
exercise can be filled. The right side of the interface is
divided into three areas. In the top area physiotherapists
can restrict the search for movements by indicating cer-
tain conditions about them, such as the type of movement,
the specific joint or the ROM. The search is performed
over the list of movements stored in the system and by
means of DL Queries that are transparent to the users.
The area in the middle shows the results of the search
(e.g. Mov2.1.5a, Mov2.1.5b, ...). Finally, the last area
shows the movements selected by physiotherapists (e.g.
Mov2.1.5d). When selecting a movement the system
checks whether the final posture of the previous move-
mentmatches the initial of the new one. If posturesmatch,
then the movement is added to the exercise. Once this is
done the exercise will be stored in the system and will be
available to be added to a treatment protocol.
Currently we are developing a new functionality for
KiReS that will allow physiotherapists to create their
own treatment protocols for their patients, guided by the
ontology, using the exercises that have been stored with
the aforementioned interfaces.
Movements, exercises and treatment protocols created
in this way are represented internally as classes of the
TRHONT ontology. This representation is generated auto-
matically at the same time that the physiotherapists create
them with KiReS.
Furthermore, once treatment protocols are assigned to
patients, those patients are monitored at the same time
they are performing the exercises for each phase of treat-
ment (see Fig. 7. More technical details in [27]). All
captured data are recorded in the KiReS database and
asserted as facts in the TRHONT ontology. Due to a rea-
soning process (see Results for intended use 3 section),
the physiotherapists can see if the patients have overcome
each phase of the recommended treatment and therefore
decide whether to end the rehabilitation process or to
assign new exercises to the patients.
Conclusions
Semantic technologies have been widely used in several
medical fields in order to facilitate the work of physicians.
In this paper we have presented an ontology for phys-
iotherapists from two different perspectives. On the one
hand, from the point of view of its creation, by showing
how it has been created by integrating information from
different resources: pre-existing ontologies, databases of
movements, exercises and treatment protocols, experts
knowledge, patient records, etc. On the other hand, from
the perspective of its usage and the relevant informa-
tion that it provides for the physiotherapists via reasoning
Fig. 6 Creation of exercises. Interface for creating new exercises in KiReS
Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 20 of 21
Fig. 7 Execution of exercises. Inferface for patients when executing exercises in KiReS
processes. This information includes recommended exer-
cises depending on the physical state of the patient, the
patients past history, etc. That is, information that can
improve rehabilitation processes.
In summary, TRHONT conceptualizes medical knowl-
edge (in order to deal with aspects related to physiother-
apy), process knowledge (in order to describe treatment
protocols), and instrumental knowledge (in order to
describe treatment elements such as exercises). It has been
designed to assist physiotherapists in several daily tasks
such as recording and searching information in the phys-
iotherapy record of a patient, defining treatment protocols
by selecting suitable exercises for each phase of a proto-
col, determining the current state of a patient and showing
their evolution, by identifying which phase of a protocol
the patient is in, and detecting which exercises are most
suitable for a patient at some specific moment taking into
account all the information that it is known about them.
TRHONT has been developed in such a way that it can be
easily extended, e.g. by adding new competency questions
to the physiotherapy record of the patient, or by adding
new exercises or treatment protocols. Moreover, it is also
reusable, since it has been implemented in several mod-
ules that could be used for other purposes. The work has
been completed with a threefold evaluation of the ontol-
ogy centered on piftfall detection, quality assessment and
ontology metrics.
Endnotes
1Available from http://bdi.si.ehu.es/bdi/ontologies/
GlenOnt.
2Available from http://bdi.si.ehu.es/bdi/ontologies/
KiReSOntFM.
3Available from http://bdi.si.ehu.es/bdi/ontologies/
KiReSOnt.
4Available from http://bdi.si.ehu.es/bdi/ontologies/
TrhOnt.
5Available from http://bdi.si.ehu.es/bdi/ontologies/
MergedTrhOnt.
6More precisely, it took the FaCT++ reasoner [41] about
1.5 min to classify TRHONT in an Intel(R) Core(TM) i7-
4610M CPU @ 3.00GHz with 8GB of RAM when the
GLENONT module was used as a whole.
Acknowledgements
Authors thank Dr. Jon Torres and Dr. Jesús Seco for their help with the
physiotherapy-related aspects. Authors thank Dr. María Poveda-Villalón for her
help with OOPS!. This work was supported by the Spanish Ministry of
Economy and Competitiveness [grant number FEDER/TIN2013-46238-C4-1-R]
and by the Basque Country Government [grant number IT797-13].
Authors contributions
All authors participated in the definition of the process and in the discussion
of relevant aspects. DA and IB perfomed the analysis of ontological and
non-ontological resources. The extraction of the GLENONT ontology module
was made by DA and AG. IB and JB designed the ontology. IB was in charge of
the implementation of the ontology. DA, IB, JB and AI wrote the manuscript.
All authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Received: 28 June 2016 Accepted: 20 September 2016
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 
DOI 10.1186/s13326-016-0105-x
RESEARCH Open Access
Dione: An OWL representation of
ICD-10-CM for classifying patients diseases
María del Mar Roldán-García1,2*, María Jesús García-Godoy1,2 and José F. Aldana-Montes1,2
Abstract
Background: Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT) has been designed as standard
clinical terminology for annotating Electronic Health Records (EHRs). EHRs textual information is used to classify
patients diseases into an International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM)
category (usually by an expert). Improving the accuracy of classification is the main purpose of using ontologies and
OWL representations at the core of classification systems. In the last few years some ontologies and OWL
representations for representing ICD-10-CM categories have been developed. However, they were not designed to be
the basis for an automatic classification tool nor do they model ICD-10-CM inclusion terms as Web Ontology
Language (OWL) axioms, which enables automatic classification. In this context we have developed Dione, an OWL
representation of ICD-10-CM.
Results: Dione is the first OWL representation of ICD-10-CM, which is logically consistent, whose axioms define the
ICD-10-CM inclusion terms by means of a methodology based on SNOMED CT/ICD-10-CM mappings. The ICD-10-CM
exclusions are handled with these mappings. Dione currently contains 391,669 classes, 391,720 entity annotation
axioms and 11,795 owl:equivalentClass axioms which have been constructed using 104,646 relationships extracted
from the SNOMED CT/ICD-10-CM and BioPortal mappings included in Dione using the owl:intersectionOf and the
owl:someValuesFrom statements. The resulting OWL representation has been classified and its consistency tested
with the ELK reasoner. We have also taken three clinical records from the Virgen de la Victoria Hospital (Málaga, Spain)
which have been manually annotated using SNOMED CT. These annotations have been included as instances to be
classified by the reasoner. The classified instances show that Dione could be a promising ICD-10-CM OWL
representation to support the classification of patients diseases.
Conclusions: Dione is a first step towards the automatic classification of patients diseases by using SNOMED CT
annotations embedded in Electronic Health Records (EHRs). The purpose of Dione is to standardise and formalise a
medical terminology, thereby enabling new kinds of tools and new sets of functionalities to be developed. This in turn
assists health specialists by providing classified information from EHRs and enables the automatic annotation of
patients diseases with ICD-10-CM codes.
Keywords: ICD-10-CM, SNOMED CT, Ontologies, Automatic classification
*Correspondence: mmar@lcc.uma.es
1Ada Byron Research Center, University of Malaga, Ampliación del Campus de
Teatinos, Málaga, Spain
2IBIMA Instituto de Investigación Biomédica de Málaga, University of Malaga,
Málaga, Spain
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 2 of 16
Introduction
The International Classification of Diseases, 10th Revi-
sion (ICD-10) [1] is a standard diagnostic tool for health
management, epidemiology and clinical purposes. ICD-
10 comprises Chapters I to XXII which cover diseases,
a variety of signs and symptoms, abnormal findings,
complaints, social circumstances and external causes of
injuries and diseases. ICD-10-CM corresponds to the
tenth version, clinical modifications, which is the current
ICD version. This medical classification standard, main-
tained and published by the World Health Organisation
(WHO) is used to classify diseases and health problems
that have been recorded on death certificates and in other
records. The accuracy of this classification is a very impor-
tant issue because it is used, for example, to set capitation
rates and allocate resources to medical centers. It is also
used by medical and health services researchers to deter-
mine the case fatality and morbidity rates. Furthermore,
ICD-10-CM has beenmandatory in the U.S. and therefore
in regular and routine use since October 1, 2015.
The concept of ontologies has been widely used in
numerous real-word applications domains from Health
Care and Life Science to Finance and Government. The
majority of current ontologies are expressed in the well-
known Web Ontology Language (OWL) [2]. Semantic
Reasoners such as Pellet [3], ELK [4], KAON2 [5] and
RacerPro [6] are all widely used to develop ontology-based
automatic classification systems. Improving the accuracy
of classification is the main purpose of using ontologies
and OWL representations as the basis for a classifica-
tion system. The literature review refers to several OWL
ontologies for representing ICD-10-CM categories that
have been developed. However, they were never intended
to be the basis for an automatic classification tool nor do
they model ICD-10-CM inclusion terms as OWL axioms,
which enables this type of automatic classification.
Systematized Nomenclature of Medicine - Clinical
Terms (SNOMED CT) [7] is a broad clinical terminol-
ogy which covers a wide range of disciplines, clinical
specialties and requirements. One of the main aims of
SNOMED CT is for it to be used as the standard termi-
nology in Electronic Health Records (EHRs) systems. The
use of SNOMED CT enables providers and EHR to use a
common language. Therefore, EHRs are annotated using
several SNOMEDCT concepts and relationships between
SNOMED CT concepts, which codify patients informa-
tion such as previous diseases, affected part of the body,
and symptoms. Figure 1 shows an excerpt from an EHR
where SNOMED CT codes are embedded.
SNOMED CT/ICD-10-CM alignments (mappings)
have been established and described by Unified Medical
Language System (UMLS) [8]. These mappings have a car-
dinality of many to many. This means that one SNOMED
CT concept can be mapped with many ICD-10-CM target
categories and vice versa. When a SNOMED CT concept
is not mapped to an ICD-10-CM category it means that
it is not classifiable or is awaiting editorial review. One or
more SNOMED CT source concepts can also be mapped
with the same ICD-10-CM target category. In addition
to these mappings provided by UMLS, BioPortals users
have also defined SNOMED CT/ICD-10-CM mappings
[9]. In this case, the Bioportals mappings have the same
cardinality as the mappings provided by UMLS.
The current situation is that EHRs are annotated using
SNOMED CT concepts and textual information from
these records is then used to classify the patients dis-
eases into an ICD-10-CM category (usually by an expert).
Interestingly, SNOMED CT/ICD-10-CM mappings can
be exploited to define the ICD-10-CM inclusion terms
based on the SNOMED standard definition of patients
medical evidence, affected part of the body and symp-
toms and their relationships, connecting the standard
annotations in EHRs with an ICD-10-CM category.
Themain hypothesis of the work presented here is: (H1)
It is possible to code, as OWL axioms, the ICD-10-CM
inclusion terms obtained from SNOMED CT/ICD-10-
CM mappings and use these OWL axioms to build
an OWL representation of the ICD-10-CM diseases.
As a result (H2) we obtain a useful OWL represen-
tation, which can be used as the basis for a seman-
tic classification system. This system will then use the
set of SNOMED CT concepts and relationships between
SNOMED CT concepts, taken from EHRs, as input to
automatically classify patients diseases into an ICD-10-
CM category. The objective of using this OWL rep-
resentation is to improve the accuracy of the manual
annotation. The accuracy of the classification is partic-
ularly relevant at a time when diagnoses codes, such as
the ICD-10-CM codes, can significantly affect the total
funding that a hospital may receive for patients admitted
[10]. For example, in the United States, diagnosis-related
groups (DRGs) based on ICD codes are the basis for
hospital reimbursement for acute-care stays of Medicare
beneficiaries [11]. Another fact is that health services
researchers use the ICD codes to study risk-adjusted,
cross-sectional, and temporal variations in access to
care, quality of care, costs of care, and effectiveness of
care [12].
This has motivated us to develop an OWL represen-
tation to help find an automated approach to classify
patients diseases in a medical context. Inclusion terms
for each ICD-10-CM category are formalised as OWL
axioms by exploiting SNOMED CT/ICD-10-CM map-
pings. These mappings allow SNOMED CT concepts and
relationships to be used to define the inclusion terms.
The resulting OWL representation called Dione1, which
includes the ICD-10-CMChapters I to XIV, is as complete
as the available SNOMED CT/ICD-10-CM mappings
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 3 of 16
Fig. 1 SNOMED CT codes embedded in an Electronic Medical Record [44]. EHRs are annotated using SNOMED CT concepts (in parentheses). For
example, the SNOMED CT concepts 248152002 and 267036007 correspond to Female and Shortness of Breath, respectively
allow. The exclusions proposed by ICD-10-CMare already
handled by the mappings.
The main contributions of this paper are summarised as
follows:
 We have defined an algorithm which, starting from
an ICD-10-CM category code, is able to obtain its
corresponding SNOMED CT concept and all its
relationships.
 The relationships obtained are considered to be the
inclusion terms for the ICD-10 category. Therefore,
we have also defined an algorithm for translating
these relationships to OWL axioms.
 To test H1 we have developed Dione, an OWL
representation of ICD-10-CM, specifically designed
to classified patients diseases by exploiting OWLs
(Description Logic) reasoning capabilities. Dione
represents the ICD-10-CM categories as classes and
the inclusion terms as OWL axioms related to the
class by means of the owl:equivalentClass statement.
Classes representing ICD-10-CM categories as well
as classes representing SNOMED CT concepts are
organised as several hierarchies.
 To test H2, Diones consistency has been checked
and information from real clinical records has been
classified to show Diones applicability through three
clinical use cases from the Virgen de la Victoria
Hospital (Málaga, Spain).
Background
There have been some attempts made to construct OWL
models using biomedical classifications like SNOMEDCT
and ICD-10. For ICD, the work developed by [13] was the
first attempt to model the ICD-9 ontology, an older ver-
sion of the current ICD-10. In [14], the authors proposed
the first formal representation of the ICD-10 based on
three logical layers of the GALEN Core Reference Model
(CRM) terminology system [15]. They used a description
logic-like language called GRAIL [16] which allows classes
to be inferred with the semantics of role propagation and
links a more detailed description of a diagnosis to a more
abstract class. The ICD-10 ontology presented in [14]
contains only ICD-10 categories and their definitions. The
hierarchical relationship of the ICD-10 is not represented
and the ICD-10 category definitions are limited to three
concepts defined by a multi-axial conceptual system that
includes the anatomy, the morphology and the etiology.
However, it has to be said that the methodology adopted
by the authors to formalise the ICD-10 has some limi-
tations: first, only two ICD-10 chapters are represented;
second, not all the ICD terms are represented using
GALEN and finally, the ontology was not loaded into
an OWL reasoner and therefore, the formal consistency
was neither checked nor classified. Given these problems,
the authors presented a DOLCE-based formal represen-
tation [17]. DOLCE is a descriptive upper-level ontol-
ogy designed for ontology cleaning and interoperability.
In this formal representation of the ICD-10, anatomi-
cal entities were taken from the Foundational Model of
Anatomy (FMA) [18], morphological abnormalities and
procedures were taken from SNOMED CT, the organ-
isms used were from the biological taxonomy and the
chemical objects were taken from the International Union
of Pure and Applied Chemistry nomenclature (IUPAC).
Despite these improvements over the previous version
of the GALEN-based ICD-10 representation, some prob-
lems have yet to be solved. For example, not elsewhere
classified diseases are modeled as logical exclusions of
elsewhere classified ICD categories from the appropriate
parent concepts. This solution does not provide any infor-
mation for a system which aims to automatically classify
a patients disease d. The doctor should assert or the sys-
tem should infer that d is an instance of the negation
of a class. Due to the OWA (Open World Assumption)
semantics of OWL, if d is not an instance of class C, the
reasoner cannot infer that d is an instance of ¬C. Further-
more, the ontology has not been checked or classified by a
reasoner.
The last approach to represent ICD-10 in OWL was
developed in [19]. In this study, an ontology was cre-
ated based on two super-classes, the icd10:Entry and
the icd10:Modifier which contain ICD-10 codes from
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 4 of 16
the WHO and the German Institute for Medical Docu-
mentation and Information (German: Deutsches Institut
für Medizinische Dokumentation und Information) [20],
respectively. The general structure of the ICD-10 ontol-
ogy includes Chapters I to XXI; classes are represented
by an URL which consists of a name space and the ICD-
10 code name, and their relations are established with
owl:subClassOf axioms. The ICD-10 exclusions are han-
dled with the owl:disjointWith axiom. This approach does
not provide information as to in which class the dis-
eases should be classified. If the same disease is classified
in both classes, the reasoner infers that the ontology
is inconsistent, but is unable to distinguish the correct
class. Furthermore, to solve the problem of exclusions
that are shared with multiple exclusions, the authors
proposed the inclusion of icd10:hasExcludes that links
to a icd10:ICDdescription (with a rdf:type and rdf:label
predicates) which has an icd10:concernsClass property.
As icd10:concernClass can involve other ICD-10 cate-
gories, the ontology requires an OWL-full expressivity.
The inclusions are modelled in the same way as the
exclusions. The OWL-full properties presented in the
ontology invalidate it for use by reasoners and thus, it
is not possible to check the ontologys consistency or
classify it.
According to the literature review, there have been
several attempts to model ICD-10 in OWL. However,
these studies have various weaknesses which can be sum-
marised as follows: 1) some difficulties in correctly han-
dling inclusions and exclusions in an OWL representation
of ICD-10; 2) the lack of a validation process using an
OWL reasoner to check the consistency of the ontology.
This step is very important in ontology development and
testing [21] because an ontology can be used by OWL
reasoners without human supervision. If an ontology is
inconsistent, the reasoning may lead to erroneous con-
clusions; 3) no application of these OWL representations
to real clinical use cases to show how they can sup-
port a clinician in decision making and 4) the reviewed
work uses ICD-9 and/or ICD-10. In this paper, we have
worked with ICD-10-CM. Although the intention is to
replace ICD-9-CMwith ICD-10-CM, it has been reported
in [22] that for reasons such as the complexity of ICD-
10-CM and the costs of migrating from one system to the
other would explain why the ICD-9-CM version is still
in use [22]. The Center for Disease Control and Preven-
tion (CDC) encourages the use of the ICD-10-CM version
because of the improvements that it has over ICD-9-CM
and ICD-10 [23]. These improvements include the addi-
tion of information relevant to ambulatory and managed
care encounters; expanded injury codes; the creation of
combination diagnosis and symptom codes to reduce the
number of codes needed to fully describe a condition; the
addition of sixth and seventh characters; incorporation of
common fourth and fifth digit subclassifications; laterality
and greater specificity in code assignment [23].
Methods
Formalising the ICD-10-CM categories in OWL
For the construction of Dione, we focused on three basic
issues that are critical when modelling an ontology or
an OWL representation, and are specified in the Ontol-
ogy 101 development process methodology [24]. First, a
selection of the concepts used to cover the objectives to
be accomplished in the health domain; second, the organ-
isation of all concepts in a hierarchy and third, a semantic
formalisation of these concepts using a knowledge rep-
resentation language such as the description logic (DL)
formalism.
In order to select the terms for each concept, an XML
file containing the ICD-10-CM categories in the English
version was downloaded from the Centers for Disease
Control and Prevention (CDC) website that stores all
the ICD versions [25]. ICD-10-CM consists of chap-
ters that are sub-divided into homogeneous blocks of
three-character categories (a capital letter and two ara-
bic numerals). These categories are sub-divided by means
of four-character categories (a capital letter and three
arabic numerals) and these are further divided into five-
character categories. The file with the ICD-10-CM cate-
gories was parsed to output a tree with parent and child
nodes (Additional file 1). The upper-level and lower-level
nodes of the tree generated from the XML file corre-
spond to the ICD-10-CM upper and lower levels, which
involve blocks of three-, four- and five-character cate-
gories. For the semantic formalisation, the hierarchy tree
from the output file, which includes Chapters I to XIV, was
encoded in OWL using the OWL API library [26]. Using
the information from ICD-10-CM about blocks and their
categories, the OWL hierarchy was modelled establishing
the Diseases category as super-class because Chapters I
to XIV, are related to diseases. All Dione classes are iden-
tified by an URI, which consists of a namespace and a
special term which corresponds to the name of each ICD-
10-CM category code. The Diseases category includes
the following ICD-10-CM categories: A00-B99 (Certain
infectious and parasitic diseases), C00-D49 (Neoplasms),
D50-D89 (Diseases of the blood and blood-forming organs
and certain disorders involving the immune mechanism),
E00-E89 (Endocrine, nutritional and metabolic diseases),
F01-F99 (Mental, Behavioural and Neurodevelopmental
disorders), G00-G99 (Diseases of the nervous system),
H00-H59 (Diseases of the eye and adnexa), H60-H95 (Dis-
eases of the ear and mastoid process), I00-I99 (Diseases
of the circulatory system), J00-J99 (Diseases of the respi-
ratory system), K00-K95 (Diseases of the digestive system),
L00-L99 (Diseases of the skin and subcutaneous tissue),
M00-M99 (Diseases of the musculoskeletal system and
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 5 of 16
connective tissue) and N00-N99 (Diseases of the genitouri-
nary system).
In this hierarchy, theDiseases class includes the ICD-10-
CM disease classification, the classes inside each disease
category are related to their upper-level category using
the is-a relationship; these relationships are formalised
in Dione with the owl:subClassOf axiom. Therefore, a
branch of the disease ICD-10-CM hierarchy was created
(Additional file 2). Figure 2 shows part of the Diseases
hierarchy, specifically the A00-B99 (Certain infectious and
parasitic diseases) branch and its subclasses. For example,
A00-A09 (Intestinal infectious diseases) has as subclasses
A00.0 (Cholera due to Vibrio cholerae 01, biovar cholerae),
A00.1 (Cholera due to Vibrio cholerae 01, biovar eltor)
and A00.9 (Cholera, unspecified). An English label with
the name of each category was included for each class
of the ICD-10-CM disease classification hierarchy, using
the rdf:label statement. Once the Dione hierarchy had
been constructed, the next step consisted in including the
owl:equivalentClass axioms for each Dione class to model
the ICD-10-CM inclusion terms. Therefore, in order to
complete the semantic formalisation of the concepts of
the OWL hierarchy, the SNOMED CT/ICD-10-CMmap-
pings were used to construct the owl:equivalentClass
axioms for each class. Where an SNOMED CT/ICD-10-
CMmapping was not available, we completed Dione with
the inferred mappings provided by the BioPortal website.
These steps are fully explained in the following sections.
Inclusion of the OWL axioms
The files related to SNOMED CT were downloaded from
the National Institutes of Health (NIH) webpage [27].
The SNOMED CT concepts, the descriptions, the is-
a relationships and the other relationships that repre-
sent other concept associations were stored in an Oracle
11g Database. The mapping files between ICD-10-CM
and SNOMED CT (also known as the map) were
downloaded from UMLS [8]. The SNOMED CT/ICD-
10-CM mappings were also extracted and stored in the
database. To generate the inclusion terms to be included
as owl:equivalentClass axioms for each OWL class of
the Dione hierarchy, an automatic process was imple-
mented (Additional file 3) based on the SNOMED CT
relationships that provide information about ICD-10-CM
inclusion terms (see Table 2), namely, the SNOMED CT
relationships that we found in EHRs which provided
information for classifying patients diseases. Therefore,
relationships representing historical attributes such as
maybe a (ambiguous concept), was a (erroneous con-
cept), moved to (moved to elsewhere concept), and
other relationships with qualifier values such as Episod-
icities etc. have not been considered. In total, there are
12 SNOMED CT relationships that are used to define the
ICD-10-CM categories. For example, the SNOMED CT
Fig. 2 Part of the A00_B99 category hierarchy included in the Diseases
category displayed on the Protegé software interface. A00_B99 is a
subclass of the main class Diseases. According to the ICD-10-CM
structure, A00_A09 is a subclass of A00_B99 and A00_0, A00_1 and
A00_9 are subclasses of A00_A09. According to the semantics of
OWL, if x is an instance of A00_9, it is also an instance of A00_B99 and
therefore, it is also an instance of A00_A09
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 6 of 16
causative-agent relationship, which identifies the direct
causative agent of a disease which can be an organism,
substance or physical force, that is represented by the
caused-by-agent Dione object property. Another example
is the associated-morphology relationship, which speci-
fies the changes that are seen at the cellular level or in
tissues, caused by a disease, that is represented by has-
Associated-Morphology Dione object property. Table 1
shows materialised examples of such relationships.
Once the Dione properties had been identified, the
relationships between two SNOMED CT concepts (one
of which maps to the ICD-10-CM code included in
Dione ICD-10-CM hierarchy) were extracted and mod-
elled by means of the owl:equivalentClass restriction of
Dione classes and owl:intersectionOf statement in order to
model the ICD-10-CM inclusion terms. The ICD-10-CM
category was defined with the following properties: the
SNOMED CT relationship (the object property in Dione),
the owl:someValuesFrom restriction and the SNOMED
CT concept. In order to illustrate this, a good use case
is the I10 (Essential primary hypertension) ICD-10-CM
category:
I10 (Essential primary hypertension) is mapped to these
two SNOMED CT concepts:
 Hypertensive episode (disorder) (62275004)
 Complication of systemic hypertensive disorder
(disorder) (449759005)
According to the ICD-10-CM guidelines, the I10 cat-
egory (Essential primary hypertension) has the inclusion
terms High Blood pressure and Hypertension, which
could be considered as a set of symptoms from a given
patient. The SNOMED CT structure indicates that the
SNOMED CT concept Hypertensive episode (disorder)
(62275004) is related to the concept Finding of increased
blood pressure (finding) (24184005) by the relation-
ship Has definitional manifestation (363705008). In the
same way, the SNOMED-CT concept Complication of
systemic hypertensive disorder (disorder) (449759005)
is related to the concept Hypertensive disorder, sys-
temic arterial disorder (38341003) by the relationship
Associated with (47429007). The SNOMED CT con-
cepts Finding of increased blood pressure (finding)
(24184005) and Hypertensive disorder, systemic arterial
disorder (38341003) are equal to High Blood pressure
and Hypertension, respectively. These two inclusion
terms have been modelled in Dione, by means of the
owl:someValuesFrom statement (?), as follows:
? hasDefinitionalManifestation.241840052
? associatedWith.38341003.
To associate the inclusion terms in a conjunctive form,
the owl:intersectionOf () statement was used. Finally, the
conjunction was associated with the class I10 through
an owl:equivalentClass (?) axiom. Therefore, the defini-
tion of the I10 class including the inclusion terms is the
following (Fig. 3 also shows it graphically):
I10 ? ? hasDefinitionalManifestation.24184005
 ? associatedWith.38341003
Figure 4 describes how the inclusion terms of I10 (Essen-
tial primary hypertension) have been modelled in Dione.
It is also worth noting that some SNOMED CT relation-
ship names to define the Dione properties were changed
to avoid any ambiguity in Dione. For example, the Has
definitional manifestation relationship was renamed as
hasDefinitionalManifestation object property to avoid
gaps between words as displayed in the first and second
columns of Table 2.
The rest of the relationships of the SNOMED CT con-
cepts mapped with I10 (Essential primary hypertension)
were also included as OWL axioms to complete the defi-
nition of the class (see Fig. 5).
Dione is composed of several branches: 1) the ICD-
10-CM disease hierarchy, which comprises Chapters I
and XIV, including the concepts that are the domain
of the relationships included and 2) the SNOMED CT
imported concept hierarchies (and their annotations in
rdf:label), which comprise those SNOMED CT concepts
taken from the relationships whose ranges include these
concepts. These imported concept hierarchies were cre-
ated in OWL using the is-a relationship, as described in
Phase I (Additional file 4). Like the previous example of
how the inclusions are modelled for I10 class, the I10 has
associatedWith, hasDefinitionalManifestation and affects
properties identified in the owl:equivalentClass restric-
tion. These properties have a range that corresponds to a
SNOMED CT concept. For example, affects and hasDef-
initionalManifestation have SNOMED CT concepts such
as Systemic circulatory system structure (body struc-
ture) and Finding of increased blood pressure (finding)
Table 1 SNOMED relationships examples. Examples of SNOMED CT concept related to SNOMED CT through causative-agent and
associated-morphology relationships
SNOMED CT concept SNOMED CT relationship SNOMED CT concept
Cholera-non-01 group vibrio, disorder causative-agent Vibrio cholerae, non-O1, an organism
Bullous pyoderma, disorder associated-morphology Chronic superficial ulcer, a morphologic abnormality
Bold data are SNOMED relationship
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 7 of 16
Fig. 3 OWL definition of the Class I10. ICD-10-CM category I10 inclusion terms are modelled as owl:someValuesFrom statement (?) and their
intersection are defined as equivalent of the class I10 by means of the owl:equivalentClass () axiom. The Description Logic syntax of OWL is used
as range, respectively. These concepts are included in the
Dione SNOMED CT imported hierarchy as shown in
Fig. 6.
Completion with BioPortal mappings
Once Dione had been developed, we have determined
the number of classes defined with the relationships
from the SNOMED CT/ICD-10-CM mappings pro-
vided by UMLS (Additional file 5). Those classes that
did not involve either an inherited or non-inherited
axiom (in the case of a superclass of the ICD-10-
CM disease branch of Dione) were defined using the
SNOMED CT/ICD-10-CM mappings from the BioPortal
website [9]. To do this, the new mappings between the
SNOMED CT concepts and the ICD-10-CM categories
were extracted from the BioPortal API [28] and inferred
to avoid duplicate SNOMED CT/ICD-10-CM mappings
from NIH. Following the methodology described in the
previous subsections, the new mappings were stored in
the database. The new OWL statements for the inclu-
sion terms of the ICD-10-CM categories without any
axioms were generated and included in Dione. Thus,
we obtained the most complete Dione version possi-
ble with the resources available, given that some classes
Fig. 4 Description of the process to model I10 inclusion terms in Dione. From the SNOMED CT/ICD-10-CM mappings and by exploiting the
SNOMED CT relationships the ICD-10-CM inclusion terms are modelled. This figure shows there exists a SNOMED CT/ICD-10-CMmapping for each
ICD-10-CM inclusion term
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 8 of 16
Table 2 SNOMED CT relationships that were used to define the Dione classes
Dione object property SNOMED CT relationship Description Number of uses in Dione
Affects Finding site The part of the body affected by a condition 10,812
After-Of After Represents a sequence of events where a clinical
finding occurs after another
491
Associated-With Associated with Represents a clinically relevant association between
concepts without either asserting or excluding a
causal or sequential relationship between the two
469
Caused-By-Agent Causative agent Identifies the direct causative agent of a disease (e.g.,
an organism)
1,701
Due-To Due to Relates a clinical finding directly to a cause such as
another clinical finding or a procedure
504
Has-Associated-Finding Associated finding Links concepts in the situation with explicit context
hierarchy to their related clinical finding
66
Has-Associated-Morphology Associated morphology Specifies the morphologic changes seen at the
tissue or cellular level that are characteristic features
of a disease
7,634
Has-Definitional-Manifestation Has definitional manifestation Links disorders to the manifestations (observations)
that define them
818
Has-Occurrence Occurrence Refers to a specific period of life during which a
condition first presents
520
Has-Pathological-Process Pathological process Provides information about the underlying
pathological process for a disorder, but only when the
results of that process are not structural and
cannot be represented by the associatedmorphology
relationship
1,867
Interprets Interprets Refers to the entity being evaluated or interpreted,
when an evaluation, interpretation or judgment is
intrinsic to the meaning of a concept
425
IsPartOf Part of Represents a sequence of events where a clinical
finding occurs after another clinical finding or a
procedure
11
The first and second columns include the selected SNOMED CT relationships with their Dione and original names. The third column is a description obtained from the
SNOMED CT User guide [45] and the fourth column includes the number of times that these relationships were used in Dione
were not defined (statistics are presented in the Results
section).
Completing the definition of Dione classes
As we have mentioned, we could not find SNOMED
CT/ICD-10-CM mappings for all ICD-10-CM categories.
This means that we could not include OWL statements
that model inclusion terms, for all Dione classes. In some
cases where we did find a mapping, the SNOMED CT
concept which the ICD-10-CM category was mapped to
did not have relationships that could be translated into
OWL statements.
This incompleteness of Dione means that a patients dis-
ease can be classified into ICD-10-CM categories belong-
ing to different chapters. In order to partially solve this
problem and taking into account that the main objec-
tive of Dione is to classify patients diseases using the
SNOMED CT annotations embedded in EHRs, we have
implemented a first process to include the axioms defin-
ing a class in the definition of its subclasses (Additional
file 6). Then we have completed a second process to
include the owl:someValuesFrom statement from the
definition of sibling subclasses in the definition of their
Fig. 5 Complete definition of the class I10. In addition to the SNOMED CT/ICD-10-CM mappings representing ICD-10-CM inclusion terms we model
the rest of mappings as owl:someValuesFrom statement (?) in order to complete the definition of the class. ? affects.113257007 models that I10
affects 113257007 (Structure of cardiovascular system), ? affects.51840005 models that I10 affects 51840005 (Systemic circulatory system structure),
? associatedWith.38341003 models that I10 is associated with 38341003 (Hypertensive disorder, systemic arterial) and ?
hasDefinitionalManifestation.24184005 models that I10 has definitional manifestation 24184005 (Finding of increased blood pressure)
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 9 of 16
Fig. 6 Structure of the ICD-10-CM disease hierarchy and the owl:equivalenClass axioms identified and included. The range of the properties in Dione
corresponds to the SNOMED CT concepts imported in the SNOMED CT hierarchy
superclass (Additional file 7). Before this process, Dione
defined:
K58 (Irritable bowel syndrome) ? ?
affects.113276009 (Intestinal structure)
E73_0 (Congenital lactase deficiency) ? ?
affects.113276009 (Intestinal structure)  ?
hasOccurrence.255399007 (Congenital)
Therefore, if a patients disease was classified in E73_0
(Congenital lactase deficiency), it was also classified in
K58 (Irritable bowel syndrome), because E73_0 was a
subclass of k58. The problem here was that we found a
mapping but we could not find more OWL statements
to define K58 from SNOMED CT relationships. How-
ever, we did observe that K58 has two subclasses (K58_0
and K58_9) and both subclasses share ? affects.71854001
(Colon structure) in their definitions. Therefore, we were
able to add this OWL statement to K58 and now K58 is
defined as:
K58 (Irritable bowel syndrome) ?
?affects.113276009 (Intestinal structure) 
?affects.71854001 (Colon structure)
With this new definition E73_0 is not a subclass of K58.
Dione consistency and classification
For Dione classification, we used the ELK reasoner with
the OWL API (Additional file 8). After some attempts
to apply Dione classification with reasoner systems such
as Fact++ [29], Hermit [30], Pellet [3], TrOWL [31],
RacerPro [6] and CEL [32], it was found that the ELK
reasoner [4] was the only reasoner able to classify Dione
while simultaneously checking that Dione was consistent.
Fact++, Pellet, RacerPro and CEL failed due to an out-
of-memory error (heap space set to 12 GB). TrOWL and
Hermit failed due to a timeout after 48 h. The experiments
were performed on a PC Intel(R) Core (TM) i7-2600 CPU
with 3.39 GHz and 16 GB of RAM and took 2781 s.
Results and discussion
Level of completion of Dione
Dione has been built based on the ICD-10-CM terms
(2014 release) provided by the CDC [25] and SNOMED
CT terms from UMLS (March 2013 release). Dione con-
tains 391,669 classes, 391,720 entity annotation axioms
and 19,797 owl:equivalentClass axioms which were con-
structed with 104,646 relationships extracted from the
SNOMED CT/ICD-10-CM and Bioportal mappings and
included in Dione using the owl:intersectionOf and the
owl:someValuesFrom constructs.
The current version of Dione has 21,616 classes
for modelling ICD-10-CM categories. After using the
SNOMED CT/ICD-10-CM mappings from UMLS
metathesaurus, the percentage of classes with axioms was
93 %. So, to provide a more complete version of Dione,
we extracted a set of ICD-10-CM/SNOMED CT map-
pings from BioPortal. These BioPortal mappings include
one-to-one mappings and one-to-many mappings. An
example of the second case is the ICD-10-CM category
A93.8 (Other specified arthropod-borne viral fevers)
which is mapped to three SNOMED CT concepts. This
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 10 of 16
concept points to the SNOMED CT concept [X] Other
specified arthropod-borne viral fevers, Piry virus dis-
ease and [X] Other specified viral hemorrhagic fevers.
The relationships from these SNOMED CT concepts for
both types of mappings were extracted and included in
Dione to define the ICD-10-CM categories. Therefore,
with the new mappings from BioPortal included in Dione
as OWL statement for defining ICD-10-CM categories,
the percentage of Dione classes with axioms is 93,3 %
(with an average of 4,8 axioms per class of the disease
hierarchy, affects being the most used Dione object
property as shown in Table 2). Dione currently contains
391,669 classes; 21,616 classes (5,5 %) taken from the
ICD-10-CM hierarchy and 370,053 classes (94,5 %) taken
from the SNOMED CT imported hierarchies. As we have
mentioned in the Methods section, this version of Dione
is still incomplete, and we could still obtain incorrect
inferences from the reasoner, for example, when we dont
have a definition for a specific class. In addition, in some
cases we could not find an OWL statement to distinguish
between subclasses. Therefore, some classes are inferred
to be equivalent to their descendants. Figure 7 presents
an extract of Dione which shows how Dione looks when
classes have complete definitions. In this example, A00
(Cholera) has two subclasses A00_0 (Cholera due to
Vibrio cholerae 01, biovar cholerae) and A00_1 (Cholera
due to Vibrio cholerae 01, biovar eltor). The definition of
A00 is included in the definition of its subclasses because
they are both Cholera. However, the definition of A00_0
and A00_1 includes a new owl:someValuesOf statement
to distinguish between the different types of Cholera. If
Fig. 7 Dione classes and subclasses modeling Cholera. This is an
extract of Dione which shows how Dione looks when classes have
complete definitions. The definition of A00_0 and A00_1 includes an
OWL statement to distinguish between the different types of Cholera
we are to solve all the incorrect inferences we will have
to find new SNOMED CT/ICD-10-CM mappings as well
as align Dione with other medical terminologies and/or
ontologies so as to complete the definition of the classes.
It may also be possible to obtain information from new
mappings established and reviewed by the scientific
community (UMLS and Bioportal) or from an expert (i.e.
doctor) to complete the definition of some classes.
Validation of Dione axioms
The Dione axioms have been included using the
SNOMED CT/ICD-10-CM mappings, which has been
constructed by a collaborative community of trained ter-
minology specialists (closely following the methodology
of SNOMED CT to ICD-10 Crossmap project). These
final mappings are published only if they have been estab-
lished as identical by a group of experts and pass a final
review. In the case that SNOMED CT/ICD-10-CM map-
pings are not available for certain ICD-10-CM category,
BioPortal provides SNOMED CT/ICD-10-CM mappings
that have been previously inferred by the BioPortal algo-
rithm and/or included (and validated) by the BioPortal
user community [33, 34]. The SNOMED CT/ICD-10-
CM mappings that are equal to the UMLS mappings
have been removed to avoid duplicate ICD-10-CM
inclusions.
As the percentage of Dione classes with axioms is 93,3 %,
it is worth noting that we could have manually completed
the mappings to generate the axioms for defining those
classes which do not have any axiom, either inherited from
parent classes or defined. However, we prefer to release
the first version of Dione using only those mappings that
are available and widely accepted by the scientific com-
munity. We have called this current version Dione V0.933.
As new mappings are created, new axioms will be used to
complete the current version of Dione.
Applicability of Dione in clinical use cases
As Dione is logically consistent, we have used it together
with the ELK reasoner to classify clinical records. Clini-
cal record information is codified by means of the Dione
object property assertions which use SNOMED CT con-
cepts. The objective of these use cases is to show how
Dione can assist health specialists by providing ICD-10-
CM classified information. We have chosen three clinical
records from the Virgen de la Victoria Hospital (Málaga,
Spain).
The first clinical record from the Hematology depart-
ment describes the case of a 61-year-old man, heavy
smoker, who presented with obesity, no fever, severe
hypoventilation, arrhythmia and signs of hypersensibility.
The results of the blood test show a low level of platelets
and the electrocardiogram (ECG) determines an auric-
ular fibrillation. The hematologic and main diagnoses
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 11 of 16
are thrombocytopenia and atrial fibrillation, respectively.
According to ICD-10-CM guidelines, the hematologic
diagnosis code D69.6 corresponds to Thrombocytope-
nia, unspecified which was diagnosed by the health spe-
cialist. The ICD-10-CM category that corresponds to the
main diagnosis is I48. For the hematologic diagnosis per-
formed by the reasoner, instances with object property
assertions such as interprets Platelet count, hasDefin-
tionalManifestation Platelet count below reference range
(finding) and hasPathologicalProcess Hypersensitivity
process (qualifier value) were created and included in
Dione (Fig. 8). The ELK reasoner classified the informa-
tion in the Dione D69_6 class, which corresponds to the
correct ICD-10-CM category. In the case of themain diag-
nosis, we have included object property assertions such
as affects Atrial structure (body structure), affects Car-
diac conducting system structure (body structure) and
Structure of cardiovascular system (body structure). All
this information was classified in the I48 class, which
corresponds to the ICD-10-CM category of Atrial fibril-
lation and flutter. The list of inferred classes provided
by the reasoner also contains other diseases that involve
atrial dysfunction because the lack of definitions to dis-
tinguish between classes in the same ICD-10-CM chapter
and few ones belonging to a different chapter, because
the lack of class definitions (see Level of completion of
Dione sub-section). This first use case is an example
of how information from two complementary diagnoses
of a given patient can be included in Dione and reasoned
by ELK.
The second clinical record from the department of Gen-
eral Clinical Surgery and Digestive Apparatus describes a
53-year-old man who presented with pain, no fever and
swelling of the perianal area that is identified by the clin-
ician as an abscess. The blood test revealed a high level
of leukocytes. The main diagnosis was an Ischiorectal
abscess that corresponds to the K61.3 class. The object
property assertions to be included in Dione and classi-
fied by the ELK reasoner are: affects Anorectal struc-
ture (body structure), hasAssociatedMorphology Abscess
(morphologic abnormality) and hasPathologicalProcess
Infectious process (qualifier value) (Fig. 9). The rea-
soner classified this information in the ICD-10-CM K61
class, which corresponds to the Abscess of anal and rec-
tal regions. This class includes a further five classes: K61.0
(Anal abscess), K61.1 (Rectal abscess), K61.2 (Anorectal
abscess), K61.3 (Ischiorectal abscess) and K61.4 (Intras-
phincteric abscess). Our objective is to provide the health
specialist with an as accurate as possible ICD-10-CM
code taken into account the information obtained from
the EHR. This use case is an example of how addi-
tional annotations by the health specialist on the patients
clinical record can lead to a more exact diagnosis with
the proposed approach. For example, if the clinician had
annotated Ischiorectal fossa structure on the clinical
record, an additional individual with the object property
assertions affects some Ischiorectal fossa structure (body
structure) would have been included in Dione and there-
fore, the information would have been classified in a more
specific ICD-10-CM class, like K61.3.
Fig. 8 Representation of the object property assertions for the first use case. Blue and black arrows represent the object property expressions that
relate individual Disease to the rest of individuals that correspond to SNOMED CT codes which have been manually annotated from the
hematologic and main diagnoses
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 12 of 16
Fig. 9 Representation of the object property assertions for the second use case. The black arrows represent the object property expressions that
relate individual Disease to the rest of individuals that correspond to SNOMED-CT codes. The SNOMED CT codes have been manually annotated
from the medical report
The third clinical record from the department of Inter-
nal Medicine and Nephrology describes the case of a
77-year-old woman who underwent surgery to remove
renal carcinoma. The patient did not undergo chemother-
apy treatment and received the usual treatment with
Bisoprolol and acetylsalicylic acid. For the next few
months following the surgery, she lost weight. The clin-
icians exploration through an abdominal computerised
tomography revealed several neoplasms in lung, liver,
in the retroperitoneal space and also some retroperi-
toneal adenopathies. The diagnosis was multiple neo-
plasms that had spread from the primary renal cancer.
The diagnosis involves several ICD-10-CM categories,
which are included in the class C78 (Secondary malig-
nant neoplasm of respiratory and digestive organs) such
as C78.0 (Secondarymalignant neoplasm of lung), C78.3
(Secondary malignant neoplasm of other and unspeci-
fied respiratory organs) and C78.80 (Secondary malig-
nant neoplasm of unspecified digestive organ). The
object property assertions that have been defined are
hasAssociatedMorphology Neoplasm metastasic (mor-
phologic abnormality), affects Structure of retroperi-
toneal lymph node (body structure), Lung structure
(body structure), Liver structure (body structure), Peri-
toneum (serous membrane) structure (body structure),
Abdominal lymph node structure (body structure) and
Retroperitoneal structure (body structure) (Fig. 10). The
disease instance was classified in the following ICD-10-
CM codes: C78, C78.30 (Secondary malignant neoplasm
of unspecified respiratory organ) and C78.80. This third
use case describes a second diagnosis of a patient who
had originally suffered from renal cell carcinoma. The
diagnosis by the health specialist involves several ICD-
10-CM categories and the information extracted from the
clinical record includes information from different body
structures. Therefore, according to the results obtained in
this use case and the others, Dione has been able to pro-
vide a list of classified instances to help doctors classify
clinical information from medical records.
Advantages of formalising the ICD-10-CM categories in
OWL
Two possible approaches can be applied to formally define
the ICD-10-CMdisease terms: 1) build anOWL represen-
tation to be reasoned by the OWL reasoners, as proposed
in this paper and 2) establish mappings between ICD-10-
CM with other terminologies and ontologies and thereby,
apply indirect reasoning [35]. This second approach has
been adopted by UMLS with the SNOMED CT to ICD-
10-CM map. This map has been applied in the I-MAGIC
algorithm [36], a Java program in which these mappings
are applied using the mapping rules. The BioPortal web-
site has also adopted the same methodology and provides
mappings between ICD-10-CM terms and other ontolo-
gies. However, it is has to be noted that this second
approach has some drawbacks:
1. There are two types of SNOMED CT/ICD-10-CM
mappings: one-to-one and one-to-many mappings.
This means that not every SNOMED CT concept
can be mapped to only one ICD-10-CM categories
with an identical meaning. Rather it can be mapped
to more than one ICD-10-CM categories with several
meanings.
2. This approach does not allow direct reasoning based
on hierarchy relationship of ICD-10-CM categories
established by owl:subClassOf axiom, the axioms
included in owl:equivalentClass axioms to define the
ICD-10-CM categories and the type of object
properties that are established in the OWL model.
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 13 of 16
Fig. 10 Representation of the object property assertions for the third use case. The black arrows represent the object property expressions that
relate individual Disease to the rest of individuals that correspond to SNOMED CT codes. The SNOMED CT codes have been manually annotated
from the medical report
For these reasons, we have built an OWL hierarchy with
ICD-10-CM categories and used the SNOMED CT/ICD-
10-CM mappings to model the ICD-10-CM inclusion
terms. According to the applicability of Dione to real clin-
ical use cases demonstrated in the Results section, this
approach provides users with a direct OWL reasoning
over a set of instances from one or more actual problems
(Electronic Health Records) proposed by the physician to
infer new relationships and provide a new approach to the
classification problem.
Comparison with other OWL ICDmodels
The representation from a clinical terminology to an
OWL model can cause semantic inconsistencies. Accord-
ing to the reviewed literature (Background section), there
is a lack of consistency checking in the proposed ICD-10-
CM formal representations and therefore, ABox and TBox
classifications3 have not been done. In this approach,
Dione has been validated by the ELK reasoner, which was
found to be the only reasoner able to classify it, after
testing all reasoners that have been successful in clas-
sifying large and widely-used real-world ontologies like
SNOMED CT [37]. The ELK reasoner returns that Dione
is consistent and performs TBox and ABox classifications.
In order to carry out the ABox classification of Dione, a set
of instances from clinical use cases taken from the Virgen
de la Victoria Hospital (Málaga, Spain) has been included
in Dione and classified to ICD-10-CM categories as is fully
explained in the Results section.
In the Background section, we highlighted some limita-
tions of existing work in the literature. In this section, we
discuss the formal representation in OWL of ICD-10-CM
proposed by [19] given that it is an ICD-10-CM represen-
tation that has improved upon other approaches. In this
approach, the authors created an ICD-10-CM hierarchy
with owl:subClassOf handling the ICD-10-CM exclusions
with owl:disjointWith axioms. We consider that such an
approach is limited given that only using owl:disjointWith
could result in a lack of information, because if the same
diseases is classified in two disjoint classes, the reasoner
infers that the ontology is inconsistent, being unable to
distinguish the correct class. Furthermore, the exclusions
that are shared withmultiple exclusions are modelled with
OWL-Full, making it impossible to validate and classify
the ontology with a reasoner. The inclusion terms are
modelled in the same way as the exclusions. In our case,
the semantic Disease hierarchy of Dione is constructed
with owl:subClassOf axioms (using the ICD-10-CM con-
cepts which have not been used in other approaches in the
literature) and the inclusion terms are modelled from the
information extracted from SNOMED CT/ICD-10-CM
mappings. The approach adopted solves the problem of
integrating features fromOWL-Full. As mentioned, in the
case of dealing with the exclusions, the mappings include
an exhaustive mapping of the low-level descendants of
those SNOMEDCT concepts that could lead to a different
ICD-10-CM category given ICD-10-CM exclusions and
other rules.
Conclusions
This paper has presented the implementation process of
Dione, an OWL representation of ICD-10-CM, which
uses SNOMED CT/ICD-10-CM mappings to formalise
the ICD-10-CM diseases categories and their inclusion
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 14 of 16
terms. Themain hypothesis guiding us is: (H1) It is possi-
ble to code, as OWL axioms, the ICD-10-CM inclusion
terms obtained from SNOMED CT/ICD-10-CM map-
pings and use these OWL axioms to build an OWL
representation of the ICD-10-CM diseases. Therefore,
we have used an automatic process to build a hierarchy
tree with ICD-10-CM disease categories and their axioms
using the owl:equivalentClass axiom. The main objective
of our approach has been to build a model that can be
used by a reasoner. Therefore, we have also shown that
Dione is consistent and a TBox classification has been
carried out by the ELK reasoner. It is worth noting that
the automatisation of the ICD-10-CM disease categories
is important given that new mappings are continuously
being added to complete Dione, whose initial version is
released with this paper. In its current version, we have
not been able to find validated SNOMED CT/ICD-10-
CM mappings for each ICD-10-CM category and so did
not have correct results in several cases. Therefore, our
first objective is to complete the definition of all classes.
We plan to add more mappings from other ontologies
which will relate Dione with more biological information
from different areas that involve personalised medicine.
BioPortal provides ontologies mapped to ICD-10-CM cat-
egories such as the National Drug Data File (with 2,857
ICD-10-CM mappings) [38], OMIM (with 3,921 ICD-
10-CM mappings) [39], the Human Phenotype Ontology
(with 1,370 ICD-10-CM mappings) [40] and the Regu-
lation of Transcription Ontology (with 61 ICD-10-CM
mappings) [41, 42]. Using the approach presented in this
paper, the mappings can be extracted from BioPortal and
stored in a database. The axioms to be included in Dione
using owl:equivalentClass can be defined with an affect
object property or also with the axioms that are defined
in the ICD-10-CM mapped class of the target ontology. It
may also be possible to obtain information from an expert
(i.e. doctor) to complete the definition of some classes.
As a secondary hypothesis, we (H2) obtain a useful
OWL representation which can be used as the basis
for a semantic classification system. This enables a set
of SNOMED CT concepts and their relationships, taken
from EHRs, as input to automatically classify patients dis-
eases into an ICD-10-CM category. This has been tested
by including object property assertions in Dione from
the Virgen de la Victoria Hospitals (Málaga, Spain) clin-
ical records which have been classified into ICD-10-CM
categories showing the applicability of the OWL repre-
sentation. After completing Dione, we plan to measure
the accuracy of the classification and to study new ways
to improve it. As the development of Dione is ongoing,
further work will include looking at how Dione reason-
ing can assist experts in providing classified information
for ICD-10-CM disease categories from actual use cases
of patient records. Dione should be supported in the
future by a semi-automatic EHR annotation tool, proba-
bly based on text mining and natural language processing
techniques. Our intention is to use an end-user interface
where the information processed can be displayed in such
a way as to make it easily understandable to specialists
in the field. This application could provide functionali-
ties which allow users to make specific OWL queries such
as: Find a disease which affects a body structure (find-
ing site) like Systematic circulatory system structure (body
structure) AND hasDefinitionManifestation Finding of
increase blood pressure (finding). These kinds of queries
can be used to retrieve an ICD-10-CM disease category
that has the same definition in Dione and also, to automat-
ically generate a list of possible ICD-10-CM categories in
which the instances from real clinical records can be clas-
sified. Finally, we will study howDione could be combined
with other techniques such as SWRL (Semantic Web Rule
Language) rules [43] and probabilistic databases in order
to develop a diagnostic assistance tool.
Endnotes
1Dione is available at http://www.khaos.uma.es/dione
2According to the semantics of OWL, this represents
an anonymous class. It has an object property hasDef-
initionalManifestation. At least one value for hasDefi-
nitionalManifestation must be an instance of 24184005
Finding of increased blood pressure (finding).
3 TBox and ABox are known as terminological and
assertion components, respectively. TBox statements
describe the set of Dione classes and properties and ABox
statements are used to describe the instances associated
with those classes and properties.
Additional files
Additional file 1: Creating the ICD-10-CM categories tree. PDF file
containing the algorithm for parsing the XML-file containing the ICD-10-CM
categories to output a tree with parent and child nodes. (PDF 82 kb)
Additional file 2: Representing ICD-10-CM categories in OWL. PDF file
containing the algorithm for creating the ICD-10-CM representation in
OWL. (PDF 73 kb)
Additional file 3: Including OWL axioms in Dione. PDF file containing the
algorithm for obtaining axioms from SNOMED CT/ICD-10-CM mappings
and for adding these axioms to Dione. (PDF 73 kb)
Additional file 4: Imported hierarchies. PDF file containing the algorithm
for creating SNOMED CT imported hierarchies and for including these
hierarchies in Dione. (PDF 73 kb)
Additional file 5: Dione level of completeness. PDF file containing the
algorithm for recursively counting the number of classes defined using
relationships from the SNOMED CT/ICD-10-CM mappings. (PDF 72 kb)
Additional file 6: Classification of Dione. PDF file containing the
algorithm for including the axioms defining a class in the definition of its
subclasses. (PDF 68 kb)
Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 15 of 16
Additional file 7: Classification of Dione. PDF file containing the algorithm
for including the owl:someValuesFrom statement from the definition of
sibling subclasses in the definition of their superclass. (PDF 68 kb)
Additional file 8: Classification of Dione. PDF file containing the
algorithm for classifying Dione using the ELK reasoner through the OWL
API. (PDF 72 kb)
Abbreviations
CDC: Center for disease control and prevention; CRM: Core reference model;
DRGs: Diagnosis-related groups; EHR: Electronic health record; FMA:
Foundational model of anatomy; ICD-10: International classification of diseases,
tenth revision; ICD-10-CM: International classification of diseases, tenth
revision, clinical modification; IUPAC: International union of pure and applied
chemistry; NIH: National institutes of health OWL: Web ontology language;
SNOMED CT: Systematized nomenclature of medicine - clinical terms; UMLS:
Unified medical language system; WHO: World health organisation
Acknowledgements
The authors would like to thank the Admissions and Medical Documentation
Service at the Virgen de la Victoria Hospital (Málaga, Spain) for providing us
with the necessary clinical reports to demonstrate the viability of Dione.
Funding
This work was partially funded by grants TIN2014-58304-R (Ministerio de
Economía y Competitividad), P11-TIC-7529 and P12-TIC-1519 (Plan Andaluz de
Investigación, Desarrollo e Innovación).
Authors contributions
MMRG designed the methodology for creating the Dione inclusion terms as
OWL axioms from the SNOMED CT/ICD-10-CM mappings, implemented the
relational database containing the SNOMED CT concepts and relationships
and designed the algorithms for implementing Dione. MJGG contributed to
the design of the algorithms, implemented the algorithms and the OWL
representation and validated Dione. MMRG and MJGG designed and
implemented the use cases for testing the applicability of Dione and wrote
the manuscript. Finally, JFAM is the director of the research group. He devised
and supervised the work and collaborated in writing the manuscript. All
authors discussed, read and approved both Dione and the manuscript.
Competing interests
The authors declare that they have no competing interests.
Received: 1 June 2016 Accepted: 21 September 2016
RESEARCH Open Access
The Ontology of Biological and Clinical
Statistics (OBCS) for standardized and
reproducible statistical analysis
Jie Zheng1*, Marcelline R. Harris2, Anna Maria Masci3, Yu Lin4, Alfred Hero5, Barry Smith6 and Yongqun He4*
Abstract
Background: Statistics play a critical role in biological and clinical research. However, most reports of scientific
results in the published literature make it difficult for the reader to reproduce the statistical analyses performed in
achieving those results because they provide inadequate documentation of the statistical tests and algorithms
applied. The Ontology of Biological and Clinical Statistics (OBCS) is put forward here as a step towards solving this
problem.
Results: The terms in OBCS including data collection, data transformation in statistics, data visualization,
statistical data analysis, and drawing a conclusion based on data, cover the major types of statistical processes
used in basic biological research and clinical outcome studies. OBCS is aligned with the Basic Formal Ontology
(BFO) and extends the Ontology of Biomedical Investigations (OBI), an OBO (Open Biological and Biomedical
Ontologies) Foundry ontology supported by over 20 research communities. Currently, OBCS comprehends 878
terms, representing 20 BFO classes, 403 OBI classes, 229 OBCS specific classes, and 122 classes imported from ten
other OBO ontologies.
We discuss two examples illustrating how the ontology is being applied. In the first (biological) use case, we
describe how OBCS was applied to represent the high throughput microarray data analysis of immunological
transcriptional profiles in human subjects vaccinated with an influenza vaccine. In the second (clinical outcomes)
use case, we applied OBCS to represent the processing of electronic health care data to determine the associations
between hospital staffing levels and patient mortality. Our case studies were designed to show how OBCS can be
used for the consistent representation of statistical analysis pipelines under two different research paradigms. Other
ongoing projects using OBCS for statistical data processing are also discussed.
The OBCS source code and documentation are available at: https://github.com/obcs/obcs.
Conclusions: The Ontology of Biological and Clinical Statistics (OBCS) is a community-based open source ontology
in the domain of biological and clinical statistics. OBCS is a timely ontology that represents statistics-related terms
and their relations in a rigorous fashion, facilitates standard data analysis and integration, and supports reproducible
biological and clinical research.
Keywords: OBCS, Biological statistics, Clinical outcomes statistics, Standardization, Statistical analysis, Data
integration
* Correspondence: jiezheng@upenn.edu; yongqunh@med.umich.edu
1Department of Genetics, University of Pennsylvania Perelman School of
Medicine, Philadelphia, PA 19104, USA
4Department of Microbiology and Immunology, Unit for Laboratory Animal
Medicine, University of Michigan Medical School, Ann Arbor, MI 48109, USA
Full list of author information is available at the end of the article
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 
DOI 10.1186/s13326-016-0100-2
Background
The movement to advance reproducibility of research ad-
vocates the use of open data, standard operating proce-
dures, and reproducibility of methods used in both
computation [1] and statistics [2]. To support such
reproducibility it is imperative that standard metadata for-
mats be used to describe how results were generated. Con-
sider, for example, the case of ImmPort (the Immunology
Database and Analysis Portal; https://immport.niaid.nih.-
gov/), which is the worlds largest repository of public-
domain de-identified clinical trial data related to immun-
ology [3, 4]. All data derived from clinical trials funded by
the Division of Allergy, Immunology and Transplantation
(DAIT) of the National Institute of Allergy and Infectious
Diseases are required to be published on the ImmPort por-
tal. In addition, the ImmPort portal includes data obtained
from the work of the Human Immunology Project Consor-
tium (HIPC, www.immuneprofiling.org/) as well as rele-
vant data from a number of external sources such as the
Gates Foundation. ImmPort currently contains data sets
from 211 studies with 34,801 subjects and 1054 experi-
ments, including complete clinical and mechanistic study
data all of which are publicly available for download in a
deidentified form. To facilitate data import and processing,
ImmPort has created templates for data representation and
documented standard operating procedures for working
with imported data. To facilitate discovery and usability of
these data to external users, and also to address the goal of
reproducibility, ImmPort is seeking wherever possible to
draw on publicly available ontologies such as the Cell
Ontology (CL) [5] and the Protein Ontology (PRO) [6] as
sources for the terms used in these templates.
If the information-driven research documented in re-
sources like ImmPort is to be reproducible, however, then
the research results contained in the ImmPort and similar
repositories must also be annotated using ontology terms
which specify the protocols and methods used in data
generation and analysis. The OBI has been created to this
end, and the OBCS follows in the footsteps of OBI by pro-
viding terms and formal definitions representing the stat-
istical methods used in biological and clinical research.
OBCS will thereby not merely allow researchers to repro-
duce statistical analyses in order to validate the conclu-
sions reached by study authors but also allow new
possibilities for discovery of and for comparison between
studies. We believe that it will also serve as impetus for
the creation of new sorts of software tools supporting
more advanced use of statistics in complex analysis and
meta-analysis of large and heterogeneous data sets.
Ontologies are human- and computer-interpretable
representations of the types of entities existing in spe-
cific scientific domains and of the relations between
these types. Since the creation of the first version of the
Gene Ontology (GO) in 1998 [7], many influential
ontology resources have been created, most recently fol-
lowing the principles of the Open Biological and Bio-
medical Ontologies (OBO) Foundry [8]. Ontologies built
in accordance with OBO Foundry principles are de-
signed to allow not only consistent classification, com-
parison and integration across heterogeneous datasets,
but also automatic reasoning with the data annotated
with their terms. This is achieved in part through the
adoption of Basic Formal Ontology (BFO) [9] as a com-
mon top-level ontology, and also through employment
of a common set of logically defined relations. OBI is a
prominent example of ontology that aligns with BFO.
OBI provides a set of logically defined terms covering a
broad range of investigation processes, experimental
conditions, types of equipment and documentation, and
data analysis methods performed on data generated from
the experiments [10, 11]. OBO ontologies were initially
successful in the representation of model organism re-
search data and have subsequently been influential in
health-related data standardization and processing [12].
Ontobee, the default OBO ontology linked server, incor-
porates over 100 OBO ontologies and provides the facility
to track how ontology terms are reused in multiple ontol-
ogies [13].
OBCS has its origin in a 2010 study of the ANOVA
(ANalysis Of VAriance) meta-analysis of vaccine protec-
tion assays [14], which led to the addition into OBI of
statistical terms related to ANOVA and survival rate
analysis. Later, an OBI statistics branch was generated to
identify and fill gaps in OBIs representation of statistics
[15]. The OBCS resulted directly from these efforts.
OBCS extends OBI, and the two ontologies share over-
lapping development groups.
Methods
OBCS development
OBCS is a community-based ontology of statistical tools
and methods used in biological and clinical investigations
that follows OBO Foundry principles in providing the pos-
sibility for enhanced representation and analysis of data
generated through complex statistical procedures.
OBCS is expressed using the W3C standard Web Onto-
logy Language (OWL2) (http://www.w3.org/TR/owl-guide/).
The meta-data schema of OBCS is implemented using
OWL annotation properties defined in the Information
Artifact Ontology (IAO, http://purl.obolibrary.org/obo/iao),
which is widely used by OBO Foundry and other ontologies
(https://github.com/information-artifact-ontology/IAO/wiki/
OntologyMetadata). In what follows, we use single
quotes to represent terms from ontologies (including
OBCS), and italics to represent object properties (also
known as relations) such as is_a and part_of. The Pro-
tégé OWL editor (http://protege.stanford.edu/) was used
for ontology editing and the Hermit reasoner (http://
Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 2 of 13
hermit-reasoner.com/) for consistency checking and infer-
encing. OBCS-specific terms were generated with IDs using
the prefix OBCS_ followed by auto-generated seven-digit
numbers.
OBCS is released under Creative Commons Attribution
(CC BY) 3.0 License, and the OBCS source code is freely
available at: https://github.com/obcs/obcs and also on the
Ontobee [13] (http://www.ontobee.org/ontology/OBCS) and
NCBO BioPortal (http://purl.bioontology.org/ontology/
OBCS) websites. The summary information of ontology
terms in OBCS based on term types and resources can be
found here: http://www.ontobee.org/ontostat/OBCS.
The RDF triples for the OBCS ontology have been saved
in the He group Triple store [13], which allows easy re-
trieval of related OBCS contents using Semantic Web
SPARQL technology. OBCS can be queried from the Onto-
bees SPARQL query endpoint (http://www.ontobee.org/
sparql) [13].
A combination of top-down and bottom-up methods was
used in OBCS development. The top-down approach was ini-
tiated by extending OBCS from the latest version of OBI
(http://purl.obolibrary.org/obo/obi/2015-12-07/obi.owl) using
the BFO 2.0 classes-only version (http://purl.obolibrary.org/
obo/bfo/2014-05-03/classes-only.owl) and the middle tier
ontology IAO (http://purl.obolibrary.org/obo/iao/2015-02-23/
iao.owl) [11]. OBCS also reuses ontological models (as de-
scribed in the Results section) developed in OBI and IAO.
The remaining parts of the ontology were then populated on
the basis of a survey of statistics workflows, which led to the
identification of a number of statistics-related terms not in-
cluded in other ontologies. These terms were supplemented
through the prototype statistics data analysis workflow pro-
posed in [14] and through the study of specific use cases de-
scribed below.
This bottom-up strategy was combined with a top-down
approach involving definition terms through downward
migration from higher-level ontology classes. For example,
the Robust Multi-Array Average (RMA) normalization is a
commonly used microarray data normalization method
[16] and the corresponding term robust multi-array aver-
age normalization (OBCS_0000140) was generated as a
subclass of OBI normalization data transformation. Popu-
lating the ontology in this way provided a simple strategy
for creating definitions of the terms introduced using the
method of specific difference [17] since the genus (and its
definition) were inherited from OBI it was necessary for
purposes of OBCS to supply only the differentia.
Reusing existing ontology resources
OBCS imports the subset of OBI consisting of all
statistics-related terms and associated parent terms using
the Ontodog tool [18]. The Excel input data used for this
purpose is available at: https://github.com/obcs/obcs/raw/
master/docs/OBI_statisitics_subset.xlsx. To ensure valid
reasoning, Ontodog was set to import all the terms used
in the expression of the ontological axioms relating to
each imported OBI term. For example, when Ontodog
fetched the OBI term log-log curve fitting, the axiom:
log-log curve fitting: achieves planned objective
some curve fitting objective was also retrieved,
together with the terms achieves planned objective
and curve fitting objective.
To eliminate redundancy and ensure orthogonality,
terms already defined in OBO Foundry ontologies were
reused in accordance with the Minimum Information to
Reference an External Ontology Term (MIREOT) guide-
line [19]. OntoFox, a software program implementing
and extending MIREOT [20], was used to extract indi-
vidual terms from external ontologies using this strategy.
Driving use cases for OBCS development
The first of two use cases driving OBCS development con-
cerns a study of the systems biology of influenza vaccin-
ation described in [21], relating to a transcription profiling
by array experiment which has as its objective the identifi-
cation of the gene expression profiles in human study sub-
jects after influenza vaccine administration. Human blood
specimens were used in this study, so that both biological
and clinical domains were involved. The second use case
concerns the study of clinical outcomes of nursing services
data with the aim of investigating statistical associations
between variable levels of nurse staffing and inpatient mor-
tality [22]. Using observational data collected from a clin-
ical setting, a Cox proportional hazards model estimation
was conducted to draw the conclusion that understaffed
shifts were significantly associated with increased inpatient
mortality [22]. Transcription profiling by array experi-
ment and Cox proportional hazards model estimation
are among the OBCS terms deriving from these use cases.
Results
OBCS overview and high level hierarchy structure
The latest release of OBCS contains a total of 878 terms,
including 780 classes, 42 object properties, 25 annota-
tion properties, and 6 datatype properties. Among these
780 classes, 229 classes are unique to OBCS, 403 were
imported from OBI. The remaining terms were imported
from various OBO ontologies, such as BFO (20 classes),
IAO (51 classes), the Statistics Ontology (STATO)
(http://stato-ontology.org/) (36 classes), the Phenotypic
Quality Ontology (PATO) (10 classes) [23] (Table 1).
Figure 1 illustrates the top-level hierarchical structure
and some key ontology terms of OBCS, showing terms
from both the continuant and occurrent branches of
BFO [9]. The continuant branch represents entities (e.g.,
material entity) which endure through time; the
Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 3 of 13
occurrent branch represents entities such as process
which occur in time. Many key continuant terms in
OBCS are classified under IAOs information content
entity, including: data item, probability distribution, as
well as directive information entities such as protocols
and algorithms (Fig. 1). OBCS includes 178 subclasses
under the branch of IAO data item (IAO_0000027).
Major occurrent terms in OBCS relate to different
types of planned process, including: data collection, data
operation, data visualization, and drawing a conclusion
based on data. The term data operation (OBI_0200000;
a new alternative label for the imported OBI term data
transformation) is defined as A planned process that
produces output data from input data. Data operation
satisfies two logical axioms:
has_specified_input only/some data item
has_specified_output only/some data item
Two important child terms of data operation are: data
transformation in statistics and statistical data analysis
(Fig. 1). A data transformation in statistics converts a
data set to another data set by applying a deterministic
mathematic function to each data item in the input data
set. For example, the OBI term logarithmic transform-
ation (OBI_0200094) represents the kind of process that
transforms input data to output data by applying a loga-
rithm function with a given base. In this case, the data
transformation process concretizes realize the mathemat-
ical function. A key subclass under data transformation in
statistics is normalization data transformation, which in-
cludes various normalization processes such as robust
multi-array average normalization (RMA) [16].
General design pattern used in the OBCS representation
of statistical studies
OBCS is designed to represent all aspects of a statistical
study. A general statistical study workflow is represented
Table 1 Summary of ontology terms in OBCS as of June 4, 2016
Ontology Names Classes Object properties Datatype properties Annotation properties Instance Total
OBCS 229 2 0 1 1 233
OBI (Ontology for Biomedical Investigations) 403 9 2 3 4 421
IAO (Information Artifact Ontology) 51 8 4 13 18 94
STATO (Statistics Ontology) 36 0 0 0 0 36
BFO (Basic Formal Ontology) 20 6 0 2 0 28
PATO (Phenotypic Quality Ontology) 10 0 0 0 0 10
RO (Relation Ontology) 0 17 0 1 1 19
Other ontologiesa 31 0 0 4 1 42
Total 780 42 6 25 25 878
athe name and statistics of other ontologies used in OBCS can be found on the Ontobee website: http://www.ontobee.org/ontostat/OBCS
Fig. 1 The top level OBCS hierarchical structure and key ontology terms. The terms shown in boxes with the prefix OBCS: in bold font are
OBCS-specific terms, and the other terms are imported from existing ontologies including BFO, IAO and OBI
Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 4 of 13
in Fig. 2, which shows five major processes including: data
collection, data transformation in statistics, statistical data
analysis, data visualization, and drawing a conclusion
based on data. These are all subtypes of OBI planned
process (Fig. 1) which comprehends two major subtypes
of data item (Fig. 3), namely: measurement datum and
derived data from statistical analysis. The latter is further
divided into: derived data from descriptive statistical ana-
lysis (e.g., median and mode) and derived data from in-
ferential statistical analysis (e.g., p-value).
OBCS defines many different types of data directly
under data item, and then provides logical axioms that
Fig. 2 Semantic representation of statistical data analysis studies using OBCS. The boxes highlighted in red represent key planned processes in
OBCS, and the terms in black boxes represent different information content entities
Fig. 3 Illustration of selected OBCS terms under data item and statistical data analysis and their hierarchies. a Illustration of part of the asserted
hierarchical structure for the OBCS branch of data item. Note that there is no subclass under derived data from statistical analysis. b The inferred
hierarchical structure after using the reasoner HermiT 1.3.8. After the reasoning, the derived data from statistical analysis has two direct subclasses
derived data from descriptive statistical analysis and derived data from inferential statistical analysis. c Illustration of a part of the statistical data
analysis branch in OBCS. Note that many OBCS terms under these branches are not shown, and these selected terms are used for demonstration.
These screenshots came from the OBCS display using the Protégé OWL editor
Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 5 of 13
can be used to infer a data type under a particular class
based on a statistical data analysis (Fig. 3). For example,
derived data from inferential statistical analysis is de-
fined by an equivalence class axiom as:
data item and (is_specified_output_of some
inferential statistical data analysis)
Given this equivalence class, every data item sub-
class (e.g., specificity) having the axiom of is_specified_out-
put_of some inferential statistical data analysis will be
inferred to be a subclass of derived data from inferential
statistical analysis.
Next we will introduce how OBCS represents each of
the five major processes. Figure 4 represents the major
parts of the OBCS data collection branch, which compre-
hends 12 subclasses corresponding to different approaches
to data collection, such as from experiment, literature, ob-
servation, by online extraction, or by sampling. Online ex-
traction can be performed from online databases or
through a web crawler. Sampling can be achieved through
survey or censoring. Data collection may face difficulties
of different sorts. For example, data fields may be incom-
mensurable, data may be missing, incompatibilities may
arise due to different types of study design, and we may
have only partial information concerning data provenance,
and so forth. To address these factors OBCS includes
terms such as generation of missing data, which repre-
sents a planned process that generates possible values of
missing data. OBCS also includes the term processing in-
compatible data that represents a data transformation
process that attempts to transforms incompatible data
into data that are compatible. Different methods can be
used to support these processes.
After data are collected, the data often need to be reor-
ganized or processed by different data transformation pro-
cesses that transform the data into a format suitable for
statistical data analysis. A typical method is to transform
a list of data by associating data items with probability
values yielding one or other type of probability distribu-
tion, for example, normal (or called Gaussian) distribu-
tion (Fig. 5). Such a distribution follows a specific
probability density function, a term that is also included
in OBCS. Normalization data transformation is a com-
monly used data transformation in statistics that adjusts
values measured on different scales to a notionally com-
mon scale and makes variables comparable to each other.
OBCS defines 34 different types of normalization
methods. Other data processing methods applied before
statistical data analysis  for example permutation, sort-
ing, or data partitioning  are also represented in OBCS.
Figure 1 represents are two types of statistical data
analysis methods called descriptive and inferential, re-
spectively. A descriptive statistical data analysis quanti-
tatively summarizes a feature of a collection of data. It
includes the application of many statistical operations
that summarize a data set in terms of its arithmetic
mean, standard deviation, empirical probability distribu-
tion, and so on. An inferential statistical data analysis,
in contrast, infers properties of a collection of data
through analysis of data. An inferential statistical ana-
lysis includes testing hypotheses and deriving estimates.
These methods can be performed on multiple data sets
and thereby generate a p-value, R-squared value, likeli-
hood ratio, or other quantitative confidence value
(Fig. 3c). In total, OBCS now includes 12 types of de-
scriptive statistical analysis methods and 92 types of in-
ferential statistical analysis methods.
Fig. 4 Illustration of logical axioms and subclasses of data collection in OBCS. This is a screenshot of Protégé OWL editor display of OBCS. As
shown here, this term is defined by an equivalent axiom, four subclass axioms, and one inherited subclass axiom. There are 12 subclasses under
data collection
Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 6 of 13
OBCS defines a statistical variable, including independent
and dependent variable, as a directive information entity
that is about a data item and can only be realized in a stat-
istical analysis. Without a statistical analysis, a statistical
variable does not exist. For example, a clinical study may
have collected data about age, sex, weight, and whether dia-
betes. Age data item is about age quality. A statistician can
specify that an independent variable (for instance an age-
independent variable) is about an age data item and a
dependent variable is about whether a given individual has
diabetes, and then test whether age significantly affects the
likelihood of the occurrence of diabetes in a human popula-
tion. The age data item is about the age quality of a human
subject. A scalar measurement datum includes two parts:
data value and data unit. If a human subject is 20 years old,
the age data item can be represented as:
((has value value 20) and (has measurement unit
label some year)) and is about some age.
The age data item will vary from subject to subject,
which is the reason why we can get an age data set in a
study. For example, if a clinical study includes three hu-
man subjects whose ages are 20, 40, and 50 years, then
the three age data items form an age data set.
One or multiple data sets can be visualized as an image or
graph by performing a process of data visualization (Fig. 2).
Two logical axioms are defined for data visualization:
has_specified_input only/some data item
has_specified_output only/some (graph or image)
Currently, OBCS includes four subclasses of data
visualization: clustered data visualization, gene list
visualization, classified data visualization, and back-
ground corrected data visualization. To support data
visualization, OBCS imports 25 terms from the graph
branch in the STATO ontology.
Based on the results of a statistic data analysis, we can
draw a conclusion based on data (Fig. 2). The descrip-
tive statistical analysis results (such as median and
mode) describe the features of a data set. The result of
an inferential statistical data analysis, such as a p-value
(a type of quantitative confidence value), is used to help
us to draw a conclusion either accepting or rejecting a
hypothesis. One important quantitative confidence
value is R-squared value, which is often used for analyz-
ing prediction problems. The class draw a conclusion
based on data also includes subclasses corresponding to
different sorts of biological feature conclusions, such as
comparative phenotypic assessment and assigning gene
property based on phenotypic assessment.
Next we focus on the OBCS modeling of the two use
cases introduced in the Methods section above.
OBCS statistical representation of a systems vaccinology
use case
The first use case, which is of a sort typically found in high
throughput biomarker analysis, is a study selected from
the field of systems vaccinology [21]. Each of the twenty
eight enrolled human subjects was vaccinated once with
Fluarix, a trivalent inactivated influenza vaccine (TIV). At
days 0 (baseline), 3 and 7 post vaccination, blood speci-
mens were collected from human subjects and from these
Fig. 5 OBCS modeling of statistical terms related to normal distribution. The normal (or Gaussian) distribution is a continuous probability
distribution of a numerical data set that follows the normal distribution probability density function. The formula of the density function is at the
bottom of the figure and included in OBCS as a mathematical formula annotation. A normal distribution transformation is able to transform a
data set to normally distributed data set
Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 7 of 13
samples peripheral blood mononuclear cells (PBMCs)
were prepared. RNA extracts were then prepared from
the PBMCs and used in a transcription profiling assay
examining expression of a large number of genes using
Affymetrix microarray technology. The basic investigation
procedure can be presented using OBI. For the statistical
steps however the additional resources of OBCS are
needed. To illustrate how OBCS is used for annotation or
tagging of statistics workflows we single out one human
subject (subject X) (Fig. 6). The initial statistical data ana-
lysis step is data collection from experiment. Such a data
collection process has specified output the raw gene ex-
pression data at different time points post vaccination.
After the raw data collection, the gene expression results
for individual genes such as TNFRSF17 (tumor necrosis
factor receptor superfamily, member 17) at days 0 (base-
line) and 7 post vaccination were normalized using the
Robust Multi-array Average (RMA) method [16]. The
RMA (OBCS_0000140) statistical method has the follow-
ing two asserted axioms (Fig. 2b):
RMA: has_part some background correction data
transformation
RMA: has_part some quantile transformation
The above RMA data normalization process ensures
that all Affymetrix microarray data from the study can
be analyzed. Normalized gene expression values for any
given gene (for example TNFRSF17) across all subjects
at different time points can then be used in a range of
statistical tests, including ANOVA, signal-2-noise ana-
lysis (S2N) and significance analysis of microarrays
(SAM). The p-values for these tests are then reported,
the p-value of <0.05 obtained for all 3 tests indicating
that a null hypothesis that the two groups (baseline Day
0 vs Day 7) have the equal means of gene expression in-
tensities can be rejected. From this we can draw a con-
clusion that the gene in question is significantly
regulated in study subjects consequent to influenza vac-
cination (Fig. 6).
Note that for simplicity some important experimental
factors (such as sex, age, and microarray format) and
statistical results (for example fold change) are not rep-
resented in the Figure. These factors can be represented
using OBO Foundry-related ontologies such as PATO
and the Experimental Factor Ontology (EFO) [24]. Rep-
resentation of these factors can be incorporated in a
statistical analysis using the OBCS approach. The Figure
provides, we believe, a good illustration of how OBI and
OBCS are used to annotate the sorts of biostatistics
studies commonly found in high throughput molecular
assay analysis and biomarker analysis.
For users to explore and better understand how OBCS
can help biomedical data annotation and analysis, we
provided an example data set and supplemental
Fig. 6 Ontological representation of an influenza microarray study. In this example, each of 28 human subjects was vaccinated once with an
influenza vaccine [21]. At day 0, 3, and 7 post vaccination, human blood samples were extracted, peripheral blood mononuclear cell (PBMCs)
were isolated from the blood samples, and RNAs were prepared from PBMCs. An Affymetrix microarray experiment was then conducted using
the RNA samples. After RMA data normalization, the gene expression data from different groups (separated on the basis of time) were used for
three types of statistical tests (ANOVA, S2N and SAM). All the boxes represent instances, labelled by the class names of these instances. All the
relations are italicized
Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 8 of 13
materials available for downloading at https://github.-
com/obcs/obcs/wiki/OBCS-example. The example con-
tains an Ontodog-generated OBCS subset including all
the terms, relations, and axioms introduced in the Fig. 6
use case. We created instances of the OBCS subset clas-
ses to present transcriptional expressions of 3 genes
(TNFRST17, MAPK1, and CNOT2) in the PMBCs col-
lected from three individuals at day 0, 3, and 7 after an
administration of TIV. This example shows how data
items (e.g., gene expression intensities) can be grouped
together to form a data set (e.g., the set of expression in-
tensities of the genes in a microarray chip for one sam-
ple), and how data sets themselves can be grouped
together to form a data matrix, which is data set of
higher order (for example a set of gene expression data
obtained from applying multiple microarray chips to
multiple samples). As shown in Fig. 6, planned pro-
cesses link the data sets collected from experiments
to the conclusion based on data drawn from statis-
tical data analysis.
OBCS representation of a clinical outcomes research use case
The second use case is focused on medical informat-
ics analysis in clinical research. In a study of clinical
outcomes of nursing services [22], data were obtained
from electronic medical records (EMR) and trans-
formed to data with standard measurement units.
This study examined the effect of variable levels of
nurse staffing on inpatient mortality. The statistical
analysis of these variables is represented ontologically
in OBCS (Fig. 7). On a shift-by-shift basis, the unit
on which each patient was located was identified and
unit characteristics and staffing data for the shift were
merged with outcomes-relevant patient data. This
process resulted in 3,227,457 separate records, with
information for each patient relating to each shift
during the period in which they were hospitalized.
The records included measures of patient-level and
unit-level characteristics, nurse staffing, other shift-
specific measures, and patient mortality. In this study,
the initial statistical data analysis step is data collec-
tion from an observation, rather than from an experi-
ment. Covariates were constructed to adjust for
factors not controlled in the study and yet still affect-
ing the dependent variable, which is in this case mor-
tality. Such a process has specified output the
difference between targeted and actual staffing levels
at different time points during hospital stay. The rela-
tive risk of increased mortality was then estimated using
Cox proportional hazards models. Reported statistical re-
sults included p-value, mean, standard deviation, confi-
dence interval, and standardized mortality ratio. The
obtained p-value of <0.05 indicates that a null hypothesis
of equal average of mortality ratios between those with
and without exposure to understaffed shifts was rejected.
Therefore, we can draw a conclusion that understaffed
shifts were significantly associated with increased mortal-
ity (Fig. 7).
Fig. 7 Ontological representation of a clinical study. This use case study analyzed the effects of variable levels of nurse staffing on inpatient
mortality [22]. The data was obtained from eMedical records and then transformed. Cox proportional hazards model estimation (a type of survival
analysis) was performed to identify the effect of hospital unit shift rates (independent variable) on the patient mortality rate (dependent variable).
A p-value of <0.01 was used to draw a conclusion of the statistical significance between these two variables
Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 9 of 13
OBCS data query
OBCS can be queried using SPARQL, a Resource De-
scription Framework (RDF) query language for retrieving
ontology data stored in the RDF format [25]. Figure 8
shows how a simple SPARQL query can be used to iden-
tify the number of methods under the OBCS class stat-
istical data analysis (OBCS_0000001). As shown in
Figs. 1 and 3, there are two types of statistical data ana-
lysis: inferential and descriptive. The SPARQL query can
recursively search all the layers of the two branches and
identify the total number of subclasses in each branch.
Discussion
There is a critical need to standardize data representa-
tion, including standardization and formal representa-
tion of statistical methods. In this paper, we have
introduced Ontology of Biological and Clinical Statistics
(OBCS), focusing on the introduction of high level sta-
tistics terms in OBCS and on how OBCS can be used in
combination with OBI for the ontological representation
of statistics-related biological and clinical data process-
ing. OBCS provides a timely source of statistical terms
and semantics in various areas of biological and clinical
statistics. OBCS provides a consensus-based rigorously
curated representation of the steps involved in statistics
pipelines in different domains in biological and clinical
fields, thereby supporting reproducibility of research.
The current OBCS development team is composed of
researchers from a number of complementary back-
grounds. Jie Zheng (PhD) is an experienced ontology de-
veloper and biomedical researcher. Marcelline Harris
(PhD, RN) is a domain expert in clinical statistics. Alfred
Hero (PhD, with appointment in the Department of Sta-
tistics at the University of Michigan) is a domain expert
in statistics and biostatistics. Anna Maria Masci (PhD) is
an immunologist who is well trained in ontology. Dr Yu
Lin (MD, PhD) is experienced in both clinical and bio-
medical informatics and ontology development. Barry
Smith (PhD) is a co-creator of the BFO ontology and of
the OBO Foundry. Yongqun He (DVM, PhD) is an
ontology developer and a domain expert in vaccine and
immunology research as well as computer science.
OBCS and OBI are closely related. OBCS extends OBI
by focusing on data collection, normalization and statistical
analysis performed on data. Some core terms in OBCS are
taken from OBI but the term coverage in OBCS includes
terms relating not only to data generated through experi-
ments but also to data from other resources such as survey
studies, text mining, clinical observations, online databases,
and so on. While many statistical data analysis methods
and related terms, including ANOVA and p-value, are
used quite generally, there are also statistical methods and
terms that are applied only in certain specific domains.
The RMA [16] and GSEA (gene set enrichment analysis)
[26] methods, for example, are used only in relation to bio-
logical data. Since OBCS originated from and inherits its
coverage domain from OBI, it places its emphasis on bio-
medical statistics. However, OBCSs coverage goes beyond
that of OBI, since it contains terms relating to statistical
methods commonly used in clinical fields. The major pur-
pose of OBCS is to provide a standardized representation
of statistical processes, methods and algorithms across
both the biological and clinical science. OBCS can serve as
an integrative metadata platform to support statistical data
representation, analysis, and data integration and facilitate
statistical validation, reproducibility, and discovery of pub-
lished research involving statistical analysis.
Several ontologies have been developed that contain
terms related to statistics [2729]. Above all, the STATO
(http://stato-ontology.org/) was recently announced.
While OBCS is focused on biological and clinical statis-
tics, STATO aims to cover a broader scope and to
Fig. 8 SPARQL query of the number of statistic data analysis methods in OBCS. This SPARQL query was performed using the Ontobee SPARQL
query website (http://www.ontobee.org/sparql/). This query found 108 statistical data analysis methods in OBCS. These terms are all under the
OBCS term statistical data analysis (OBCS_0000001)
Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 10 of 13
include also natural science domains outside the life sci-
ences. OBCS and STATO thus focus on different aspects
of statistics. After STATO was officially released, we
reused STATO terms in OBCS wherever applicable. We
have imported many terms with a focus especially on
the graph branch of STATO. To deal with the residual
sets of terms common to both OBCS and STATO, we
added the STATO-OBCS mapping information using
the oboInOwl:database_cross_reference annotation
property in OBCS. STATO terms contain many logical
axioms, which can be used for generating queries such
as those listed at http://stato-ontology.org/. Such imple-
mentations are very helpful will be incorporated within
OBCS wherever possible. On the other hand, the major
driving use case for OBCS is to serve annotation of bio-
medical data in a way that promotes reproducibility  and
eventual automation  of statistical data analyses for the
whole sets of biomedical experimental data [30]. As a re-
sult, OBCS focuses more than STATO on providing de-
tailed data analysis pipelines, as illustrated in Figs. 2, 5, 6,
and 7, to support systematic statistical data analyses. We
are collaborating with STATO to achieve a consensus div-
ision of development effort in the future.
The OntoDM ontology [27], developed with a focus
on data mining, also has some statistical components, as
does the Hypothesis and Law Ontology (HELO), which
focuses on representing probabilistic scientific know-
ledge and hypothesis evaluation [28]. In addition, the
Ontology of Clinical Research (OCRe) incorporates
some statistics terms used in clinical studies [29]. How-
ever, OCRe, in contrast to all the aforementioned ontol-
ogies, does not provide definitions for its terms, and it is
not aligned with BFO or OBI. After careful comparison
and examination, we found that none of these ontologies
focuses on the sort of comprehensive representations of
biological and clinical statistics that meets our needs in
formally representing the statistical tools and methods
used in data collection, organization, analysis, presenta-
tion and interpretation. OBCS is the first ontology that
systematically represents the five major processes in stat-
istical studies (Fig. 2), and lays out general design pat-
terns for representing statistical distributions (e.g.,
normal distribution as shown in Fig. 5) and related
terms. OBCS also includes many statistics terms unavail-
able in other ontologies.
Our two use cases lie at opposite ends of the trans-
lational science continuum from T0 (basic biomedical
research) to T4 (translation to population). These use
cases demonstrate the usage of OBCS in basic and
translational biomedical research. We are developing
OBCS applications addressing other points on this
continuum, including T1 (basic to clinical translation),
T2 (demonstrating efficacy), T3 (translation to prac-
tice) [31, 32].
In addition to the two use cases detailed in this manu-
script, OBCS is currently being used for the standardized
representation and annotation of genomics data analysis in
the Beta Cell Genomics Database (http://www.betacell.org/
gbco/). The OBCS design pattern and strategies are consist-
ent with previous research of using the combination of OBI
[10] and the Vaccine Ontology [33, 34] to support the stat-
istical meta-analysis of variables contributing to the effects
of protective immunity [14, 35]. OBCS is being tested in
the work of the ImmPort project team for standard data
representation and statistical data analysis, and it is being
evaluated also for its ability to support statistical software
tools such as RImmPort [4] and Python for Population
Genomics (PyPop; http://www.pypop.org/) developed to
promote reproducible and automated analysis of biological
and clinical data. Since OBI, VO, PATO, and other
OBO ontologies provide many terms to represent fea-
tures of biological and clinical studies, OBCS can use
the corresponding terms when representing the corre-
sponding experimental variables (such as vaccine,
gender, age, mortality) in a statistical analysis.
In the future, OBCS will be further developed to in-
clude new statistical methods and inference procedures,
support data integration, and be applied to more re-
sources. The OBCS development is driven primarily by
use cases. Our current use cases have been focused on
vaccinology, immunology, flow cytometry, microarray,
and clinical nursing scenarios. Many more statistics-
specific questions and cases will still be generated and
studied in these areas. In addition to standard statistical
data representation and integration, OBCS will be useful
also in supporting more consistent extraction of data,
thereby allowing new kinds of search (for example: for
all data derived using a specific type of analysis or a spe-
cific type of variable). Many statistical methods have
been implemented in different software programs such
as many statistical programs available in BioConductor
[36] and RImmPort [4]. We plan to relate OBCS statis-
tical analysis methods to corresponding software pro-
grams by reusing the Software Ontology (SWO) terms
[37]. Such linkage between OBCS statistical methods
and software programs can support standard statistical
method representation and software integration, repro-
ducible data analysis, and interoperable communications
between different software programs. We believe that
biomedical and clinical databases and software programs
targeting big data analysis will benefit considerably from
the standardized definitions and logical representations
of statistics terms and relations [30] of the sort which
OBCS provides.
Conclusion
The Ontology of Biological and Clinical Statistics
(OBCS) is a community-based open source ontology in
Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 11 of 13
the domain of biological and clinical statistics. We pre-
sented the rationale, history, scope, contents, top level
hierarchy, and a general design pattern of OBCS. Sec-
ond, we provided detailed accounts of the main branches
of OBCS, including data item, statistical data analysis,
and data collection. Third, the OBCS approach to stat-
istical terms related to normal distribution is presented,
and it is shown how this approach can be generalized to
other statistical distributions. Fourth, two OBCS use
case are studied and presented, demonstrating how
OBCS can be applied to the ontological representation
of real statistical studies. Lastly, a SPARQL query ex-
ample (Fig. 8) is provided to demonstrate how to quickly
query OBCS information stored in an RDF triple store.
Overall, we believe that OBCS is a timely ontology able
to represent statistics-related terms and their relations in
a rigorous fashion, facilitate standard data analysis and
integration, and support reproducible biological and
clinical research.
Acknowledgments
We thank Drs. Jennifer Fostel, Bjoern Peters, Alan Ruttenberg, Larisa N. Soldatova,
Christian J. Stoeckert Jr., and the OBI Consortium for their valuable discussions and
feedback.
Funding
This work was supported by NIH grants R01AI081062 (YH and YL) and
5R01GM93132 (JZ), NIAID contracts HHSN272201200028C and NIGMS and
NCATS grants R01GM080646 and 1UL1TR001412 (BS) and NIH U01CA157703
and DOD W81XWH-15-1-0467 (AMM), and ARO grant W911NF-15-1-0479
(AH) and DARPA grant W911NF-15-1-0161 (AH). The article-processing charge
was paid by a discretionary fund from Dr. Robert Dysko, the director of the
Unit for Laboratory Animal Medicine (ULAM) in the University of Michigan.
Authors contributions
JZ and YH are primary developers of OBCS, generated initial use case models
and all figures, and prepared the first draft of the manuscript. MRH added
and annotated many terms associated with clinical statistics and supported
the clinical use case modeling. AMM supported the modeling and analysis of
the influenza microarray use case. YL participated in many discussions and
provided input in ontology development. AH provided many comments and
suggestions as a domain expert in statistics and biostatistics. BS provided
valuable suggestions on ontological matters including alignment with BFO.
YH also initiated the OBIstat project that was a prototype of OBCS. All
authors edited and approved the manuscript.
Competing interests
The authors declare that they have no competing interests.
Author details
1Department of Genetics, University of Pennsylvania Perelman School of
Medicine, Philadelphia, PA 19104, USA. 2Division of Systems Leadership and
Effectiveness Science, University of Michigan School of Nursing, Ann Arbor,
MI 48109, USA. 3Department of Biostatistics and Bioinformatics, Duke Medical
Center, Duke University, Durham, NC 27710, USA. 4Department of
Microbiology and Immunology, Unit for Laboratory Animal Medicine,
University of Michigan Medical School, Ann Arbor, MI 48109, USA.
5Department of Electrical Engineering and Computer Science, Department of
Biomedical Engineering, and Department of Statistics, Michigan Institute of
Data Science, University of Michigan, Ann Arbor, MI 48109, USA.
6Department of Philosophy and National Center for Ontological Research,
University at Buffalo, Buffalo, NY 14203, USA.
Received: 7 January 2016 Accepted: 6 September 2016
DATABASE Open Access
The cellular microscopy phenotype
ontology
Simon Jupp1*, James Malone1, Tony Burdett1, Jean-Karim Heriche2, Eleanor Williams3, Jan Ellenberg2,
Helen Parkinson1 and Gabriella Rustici1
Abstract
Background: Phenotypic data derived from high content screening is currently annotated using free-text, thus
preventing the integration of independent datasets, including those generated in different biological domains, such
as cell lines, mouse and human tissues.
Description: We present the Cellular Microscopy Phenotype Ontology (CMPO), a species neutral ontology for
describing phenotypic observations relating to the whole cell, cellular components, cellular processes and cell
populations. CMPO is compatible with related ontology efforts, allowing for future cross-species integration of
phenotypic data. CMPO was developed following a curator-driven approach where phenotype data were
annotated by expert biologists following the Entity-Quality (EQ) pattern. These EQs were subsequently transformed
into new CMPO terms following an established post composition process.
Conclusion: CMPO is currently being utilized to annotate phenotypes associated with high content screening
datasets stored in several image repositories including the Image Data Repository (IDR), MitoSys project database
and the Cellular Phenotype Database to facilitate data browsing and discoverability.
Keywords: Ontology, OWL, Cellular phenotype, Imaging
Introduction
Recent advances in imaging techniques make the study
of complex biological systems feasible, particularly at the
cellular level, complementing existing omics ap-
proaches, most notably genomics and proteomics, by re-
solving and quantifying spatio-temporal processes with
single cell resolution [1]. High content screening (HCS)
is an imaging based multi-parametric approach that al-
lows the study of living cells. HCS is used in biological
research and drug profiling, to identify substances, such
as small molecules or RNA interference (RNAi) re-
agents, which can alter the phenotype of a cell. It can
also be used to look at the effect of knocking out genes
completely, or to determine protein localization by
modifying genes to produce tagged proteins that can be
visualized. Phenotypes may include morphological
changes of a whole cell, or any of its cellular compo-
nents, as well as alteration of cellular processes.
Correlative analysis of cellular phenotypes, specific to
individual genes, with morphological imaging data from
diseased tissue specimens (both human and mouse tis-
sues) allow us to link phenotypic data to associated
image annotations and metadata, leading to a powerful
predictor of disease biomarkers as well as drug targets.
For example, when a certain cellular phenotype, like mi-
totic delay or multi-nucleated cells, observed in cells
after gene knockdown experiments, is also observed in
cells of a cancer tissue, this could give us an indication
of which gene(s) might be involved in the aetiology of
the disease, in that specific tissue. Knowledge of the
functional implications of somatic tumor mutations can
thus be used to design more targeted drug therapies.
Data derived from live cell imaging is typically associ-
ated with rich metadata, including genetic information,
and can be more easily interpreted and linked to under-
lying molecular mechanisms. As we move to higher or-
ganisms, such as mouse and human, the degree of
metadata available decreases (e.g. no genetic information
* Correspondence: jupp@ebi.ac.uk
1European Bioinformatics Institute (EMBL-EBI), European Molecular Biology
Laboratory, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD,
UK
Full list of author information is available at the end of the article
© 2016 Jupp et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Jupp et al. Journal of Biomedical Semantics  (2016) 7:28 
DOI 10.1186/s13326-016-0074-0
is available for diseased human tissues), alongside the
feasibility of assays that can be carried out in such or-
ganisms (e.g. genetic engineering is only possible in cell
lines and mouse models). Taking this into consideration,
it becomes evident that integrating imaging datasets
from different biological domains could greatly advance
our understanding of the molecular mechanisms under-
lying specific diseases.
Due to its late arrival on the omics scene, the im-
aging field has not yet achieved the same degree of
standardization that other high-throughput approaches
have already reached [1], thus hampering integration of
image data with current biological knowledge. Standards
are needed for describing, formatting, archiving and ex-
changing image data and associated metadata, including
suitable nomenclatures and a minimal set of information
for describing an imaging experiment. This is crucial to
enable the establishment of databases and public reposi-
tories for image data and allow for the integration of in-
dependent datasets.
The use of ontologies to annotate data in the life sci-
ences is now well established and provides a means for
the semantic integration of independent datasets. Des-
pite the availability of several species-specific ontologies
for describing cellular phenotypes (e.g. the Fission Yeast
Phenotype Ontology), there isnt an appropriate infra-
structure in place to support the large-scale annotation
and integration of phenotypes across species and differ-
ent biological domains.
As part of the BioMedBridges project,1 efforts are un-
derway to integrate biological imaging datasets provided
by emerging biomedical sciences research infrastruc-
tures, including Euro-BioImaging,2 for the provision of
cellular image data; Infrafrontier,3 for mouse tissue
image data, and BBMRI/EATRIS,4 for human tissue
image data. Such infrastructures are generating a wealth
of imaging data that can only be made interoperable
through consistent annotation with appropriate
ontologies.
There has been much work published on the develop-
ment of cross-species phenotype ontologies and their
benefits [2]. To date ontologies describing phenotypes
exist for a host of species including mammalian pheno-
types (MP; [3]), Ascomycetes (APO; [4]), S. pombe
(FYPO; [5]) and C. elegans (WPO; [6]). There are also
well established ontology design patterns for modeling
phenotypes in a species and domain independent man-
ner that utilise the Phenotype and Trait Ontology
(PATO) [7]. These phenotypic descriptions are based
around the Entity-Quality model (EQ) that refers to de-
scribing a phenotype in terms of an Entity (E), from one
of many given reference ontologies, such as Gene Ontol-
ogy (GO, [8]) and an associated Quality (Q), from PATO
[9]. For example, a large nucleus phenotype could be
expressed in EQ using the entity term nucleus
[GO:0005634] from GOs cellular component and the
quality term increased size [PATO:0000586] from
PATO. This model has been adopted by a range of
model organism databases for the annotation of various
phenotypes ranging from disease, anatomical and cellu-
lar phenotypes [10].
Ontology languages, such as the Web Ontology Lan-
guage (OWL), allow us to express logical definitions for
classes that describe class membership based on quanti-
fied relationships to other classes. The Basic Formal
Ontology (BFO) defines the inheres in [BFO:0000023]
relationship that can be used to capture the relationship
between qualities, which in BFO are specifically
dependent continuants, and the bearer of those qualities,
which are typically independent continuants. For ex-
ample, in order to logically define a large nucleus
phenotype we say that the quality of increased size
inheres in the bearer, which in this case would be the
nucleus. We can express this relationship logically in
OWL using existential quantification to assert that the
class of all large nucleus phenotype is equivalent to the
class of things that have an increased size quality that
inheres in a nucleus. We could go on to further de-
scribe another class of phenotypes, such as a more gen-
eral nuclear size phenotype and by virtue of the fact
that increased size is a subclass of a more general size
quality, use an OWL reasoner to automatically classify
large nucleus phenotype as a subclass of nucleus size
phenotype. Highly scalable reasoners, such as ELK [11],
have made it practical for ontology engineers to fully ex-
ploit this expressivity when working with large ontol-
ogies. In the case of building phenotype ontologies, it
means we can now build logical class definitions for a
large number of phenotypes following the EQ pattern,
and let the reasoner do the work to classify those pheno-
types and infer equivalence across different phenotype
ontologies.
A previous effort to develop a species neutral cellu-
lar phenotype ontology (CPO) was undertaken by
Hoehndorf et al. [12]. The CPO was automatically
generated and includes logical class definitions com-
posed from GO and PATO terms. Whilst in principle
this is a reasonable approach, in practice the resulting
ontology was difficult to work with and did not pro-
vide a good vocabulary for data annotation. The size
of the ontology coupled with limitations in standard
ontology authoring software made it impractical to
extend and maintain this ontology whilst keeping in
sync with GO and PATO via the automatic gener-
ation process. The size and automatic label creation
strategy also made it difficult for the biocurators to
find terms for annotating data. It would have been a
considerable amount of effort to manually clean the
Jupp et al. Journal of Biomedical Semantics  (2016) 7:28 Page 2 of 8
CPO to make it fit for purpose as a general annota-
tion vocabulary for imaging datasets.
Our approach was therefore to build CMPO from the
available data, using a post-composition approach where
phenotypes were manually annotated with ontology
terms that were later used to compose new stable
phenotype terms in the ontology. These new terms were
annotated with appropriate meta-data, such as synonyms
and definitions that reflect how the terms are used in
the data and literature.
Results
As of release 1.9 CMPO contains 361 phenotype terms.
CMPO provides a root class called cellular phenotype
which is further divided into five major sub-types,
namely; cell process phenotype, cellular component
phenotype, molecular component phenotype, single cell
phenotype and cell population phenotype. (Fig. 1). Each
of these categories represents a different level of granu-
larity for which we see phenotype descriptions in the
data. Every effort is made to ensure that each CMPO
term has an equivalence axiom that describes the term
using an OWL class expression. We strive to avoid
asserting subclass axioms between named phenotype
classes and instead use a reasoner to infer classification
using logically defined classes.
Cell process phenotype
The cell process phenotypes aim to capture phenotypic
descriptions at the level of cellular processes. Using the
Manchester OWL syntax (MOS) notation5 we can ex-
press a CMPO cell process phenotype as being logically
equivalent to the anonymous OWL class has_part some
(process quality and (inheres_in some biological_pro-
cess)), where the process quality comes from PATO and
the biological_process term is from GO. In some cases,
such as the CMPO mitotic process phenotype, we
would like to capture all phenotypes that inhere either
the GO mitotic cell cycle or part of the GO mitotic cell
cycle. Whilst OWL provides the vocabulary for union
(OR) operators in OWL class descriptions, this would
take CMPO outside of the OWL-EL6 sublanguage. In
order to keep CMPO within EL and have the ability to
compute desirable subclass relations, we used two separ-
ate equivalence class axioms e.g. mitotic process pheno-
type is equivalent to has_part some (process quality
and (inheres_in some (part_of some mitotic cell cycle)))
and equivalent to has_part some (process quality and
(inheres_in some mitotic cell cycle)).
There are also cases where phenotype descriptions at-
tempt to capture the absence of a process e.g. absence of
mitotic chromosome decondensation phenotype. Whilst
PATO contains a quality called lacking processual parts,
it would be incorrect to assert that absence of mitotic
chromosome decondensation is a quality that inheres in
the mitotic chromosome decondensation process itself.
To deal with such cases we make use of the BFO ontol-
ogy specifically depends on at all times [BFO:0000070]
(also referred to as s depends on or towards) relation,
that can be used to relate a relational quality or dispos-
ition to a relevant entity. For absence of mitotic chromo-
some decondensation phenotype we describe it as a
lacking processual parts quality that inheres in the cell
cycle as a whole, where the lacking processual parts
quality specifically depends on the mitotic chromosome
decondensation phenotype. The fully qualified equivalent
class description for this phenotype is has_part some
(lacking processual parts and (towards some mitotic
chromosome decondensation) and (inheres_in some cell
cycle)). A similar pattern is used throughout CMPO to
deal with cases of phenotypes where the phenotype de-
scribes the absence of a particular entity.
HCS data often includes phenotypes relating to pro-
tein localisation in the cell. CMPO aims to describe
Fig. 1 Visualisation of the top-level terms in the CMPO phenotype
ontology showing cell process, single cell, cellular component and
molecular component phenotypes
Jupp et al. Journal of Biomedical Semantics  (2016) 7:28 Page 3 of 8
protein localisation phenotypes in terms of the protein
localisation process that is occurring along with details
of the protein complex being transported and in some
cases the target and end location of the protein. Using
the transports or maintain localization of  and has tar-
get end location object properties from the OBO Rela-
tion Ontology we describe a complex phenotype as
equivalent to has_part some (occurrence quality and
(inheres_in some (protein localization and (transports
or maintains localization of  some polypeptide) and (has
target end location some cellular_component)))). The
GO provides good coverage of protein localisation pro-
cesses that CMPO has utilised to develop a branch of
protein localisation phenotypes relating to various cellu-
lar components and the CMPO design pattern is con-
sistent with the pattern used in the OWL edition of GO.
Cellular component phenotypes
All cellular component phenotypes are logically de-
scribed as any quality (non processual quality) that
inheres in any cellular component from GO e.g. in
MOS notation has_part some (quality and (inheres_in
some cellular_component)). Typically these observa-
tions relate to the morphology or position of a par-
ticular component in a cell. In order to drive all the
necessary inference to infer subclasses of a general
term such as nuclear phenotype we describe these
terms using three equivalence class axioms to capture
qualities of the nucleus, nuclear parts, and any qual-
ities that11 are towards the nucleus.
Molecular component phenotypes
The molecular component phenotype branch describes
phenotypes at the level of molecules in the cell. All mo-
lecular component phenotypes are logically equivalent to
has_part some (quality and (inheres_in some molecular
entity)) where the molecular entity is a bio-molecule
from the ChEBI ontology [13]. To date this branch of
the CMPO only contains phenotypes terms relating to
the shape of DNA molecules within the cell.
Single cell phenotypes
Single cell phenotypes in CMPO describe phenotypes
that are observed at the level of the whole cell. Single
cell phenotypes are described as logically equivalent
to has_part some (quality and (inheres_in somecell in
vitro)) where cell in vitro is imported from the Cell
Ontology [14]. The single cell phenotypes are further
classified in terms of cellular component number,
whole cell morphology, cell movement, cell nucleation
and cell viability.
Cell population
CMPO describes a cell population phenotype as a collec-
tion of qualities that inhere in a population of cells. We
distinguish between qualities of the population as a
whole and qualities of individual cells within the popula-
tion using the Relation Ontology bearer of  relationship.
For example, CMPO describes a fewer mitotic meta-
phase cells phenotype as a has fewer part of type quality
that inheres in a population that bears a mitotic meta-
phase phenotype. In MOS we can define fewer mitotic
metaphase cells as equivalent to cell population that
has_part some (has fewer parts of type and (bearer of 
some mitotic metaphase phenotype)).
CMPO annotation properties
CMPO follows many standard conventions from the OBO
foundry for ontology term metadata. Every CMPO term
must have an rdfs:label and definition using the Informa-
tion Artifact Ontology (IAO) definition [IAO:0000115]
predicate. In cases of phenotype terms that could be
traced back to a source publication or dataset, we used
the definition source [IAO:0000119] predicate from IAO
to link the term to the publication. The standard set of
OBO synonym properties are also used to capture exact,
broad and narrow synonyms for a term. The source
CMPO OWL file imports the full GO and PATO ontology
to support development of the ontology and to drive the
inference. Finally we define a CMPO slim so that we can
easily extract a simplified version of CMPO for a release
of the ontology that exclude all the PATO and GO terms.
CMPO availability
The CMPO homepage (http://www.ebi.ac.uk/cmpo) pro-
vides access to the ontology and issue tracker for sub-
mitting new term requests. The source ontology for
CMPO is hosted on GitHub7 and it is also available from
the NCBO BioPortal [15] and the EMBL-EBIs Ontology
Lookup Service (OLS).8
Applications of CMPO
In the context of the BioMedBridges project, we want to
demonstrate the power of interoperability of large-scale
image data sets from different biological scales to enable
drug target and biomarker discovery for human diseases,
focusing on cancer as an example.
CMPO is being used to annotate mitotic phenotypes
observed in live human cells, as well as cellular pheno-
types from tissue microarrays of diseased tissues from
both human patients and mouse models. Analysing
phenotypic correlations between cellular and tissue data
sets, and linking imaging data with molecular data, in-
cluding the cancer genome sequence and expression
data, will allow for in silico validation of the predictions
and prioritization of biomarkers for validation in clinical
Jupp et al. Journal of Biomedical Semantics  (2016) 7:28 Page 4 of 8
research. In particular, we focus on genes with a function
in controlling cell cycle and cell division, as well as inva-
sive behaviour, for which comprehensive molecular and
cellular datasets are available.
CMPO is currently being utilized to annotate pheno-
types associated with HCS datasets stored in the Image
Data Repository (IDR), a next generation repository cur-
rently being developed to: (i) provide easy access to ref-
erence image data linked to peer-reviewed publications
and support browsing, search and visualization of image
data and metadata; (ii) facilitate the establishment and
adoption of data standards to enable interoperability of
image data; (iii) link such data to other biomolecular
data resources (e.g. genomics databases, structural data-
bases and functional annotation) and (iv) build a compu-
tational resource to support the re-analysis of image
data and the development of new computational tools.
IDR is built upon established, actively developed open
source platforms and applications, including the
OMERO software for visualization, management and
analysis of biological microscope images [16]. The
OMERO API is currently being extended to explicitly
support ontological annotations and access CMPO
through OLS to look up of additional information and
subsumption queries [17]. Since CMPO has been applied
to annotate phenotypes associated with IDR data (Fig. 2),
50 new phenotype terms have been added to the ontol-
ogy. CMPO has also been integrated into the MitoSys
project database9 and the Cellular Phenotype Database
[18] to facilitate data browsing and discoverability. Work
is in progress to add a functionality for ontology based
browsing in CellCognition [19].
Method
Eleven imaging datasets were initially sourced to collect
a set of candidate phenotypic descriptions for manual
ontology annotation [2030]. Our approach was to an-
notate the phenotypes with terms from GO and PATO
to generate EQ based annotations that would be later
post-composed to form new CMPO terms. We devel-
oped a simple Web application called Phenotator for the
data providers to submit and annotate their phenotypes
with an EQ. The Phenotator is built using services from
the NCBO BioPortal [15] to generate simple drop down
menus and autocomplete search functionality to guide
the users in generating EQs with appropriate terms
(Fig. 1). Phenotator has a feature to export the collected
EQ annotations as an OWL file containing new terms
that are logically defined according to the SUBQ pat-
tern,10 which can be expressed in Manchester OWL syn-
tax as (has_part some (<Quality> and inheres_in some
<Entity>)). One hundred twenty-seven phenotype
Fig. 2 Screenshot of the Image Data Repository showing image meta-data that include phenotype annotation to CMPO term decreased
duration of mitotic prophase [CMPO:0000329]
Jupp et al. Journal of Biomedical Semantics  (2016) 7:28 Page 5 of 8
descriptions from the original 11 datasets were entered
into Phenotator, together with 41 phenotypes collected
from cell migration assays (Z. Kam, personal communi-
cation) and 193 phenotypes from the GenomeRNAi
database [31]. The domain experts entered EQ based de-
scriptions for a total of 201 phenotypes.
The EQs were exported from Phenotator as an
OWL file and loaded into the Protege OWL ontology
editor. The generated OWL file imported the full
Gene Ontology and PATO and the ELK reasoner was
used to compute an automatic classification of the
post-composed EQ terms. The biological curators and
ontology experts were able to use this classification to
both verify the collected EQs and inform the organ-
isation of the upper level of the ontology so that the
terms were classified into useful categories. After sev-
eral iterations of this process, the post-composed
terms were assigned permanent CMPO identifiers and
relevant metadata for each term was collected in
preparation for the initial release.
CMPO accepts new terms requests via the CMPO
website and also accepts more structured term requests
via the Webulous application. Webulous provides a ser-
vice for specifying ontology term creation templates.
These templates can be loaded into tools such as Google
Sheets using the Webulous Google Sheets Add-on,11 so
that users can submit batch requests of new terms to
CMPO. The CMPO Webulous templates have been used
by the Image Data Repository (IDR) curators as a mech-
anism for adding new terms to CMPO for both cellular
process and cellular protein localisation phenotypes.
CMPO releases are managed using a continuous inte-
gration server and the OBO ontology release tool
(Oort).12 CMPO is released as four files: a single OWL
file that contains all axioms and the full GO and PATO
import; a single Mireoted13 version of CMPO with only
relevant GO and PATO terms, and two simple versions
that only contain CMPO terms that are available in
OWL or OBO format. All files are made public via the
CMPO website and the CMPO GitHub repository.14
Discussion
CMPO follows established best-practices from the Open
Biomedical Ontology community and can provide a way
to bridge low-level cellular phenotype data across spe-
cies. Merging CMPO with other post-composed pheno-
type ontologies, such as FYPO, and classifying these
together using a reasoner shows that equivalent terms
can be inferred. Some manual intervention is required to
harmonise the URIs used for some of the relationships
and many terms dont merge as expected because the
OWL version of FYPO doesnt use the SUBQ pattern
used in CMPO. Best practices for the translations of EQ
annotations into OWL statements are still emerging and
inconsistent use of common OBO relationships and lack
of shared design patterns suggest that there is still some
work to be done to integrate the various cellular pheno-
type ontologies.
Most cellular phenotype ontologies contain terms for
describing features such as cell size, shape and morph-
ology that are often observations that can be considered
subjective or are only valid in the context of a particular
assay. For instance, nuclei are not bright unto them-
selves, but we have data where the phenotype has been
recorded as bright nuclei in response to a particular
treatment. CMPO currently includes terms such as
bright nuclear body and increased cell size, however,
these terms are unlikely to have a shared meaning across
independent datasets. We believe having these terms in
the ontology is important as they represent the vocabu-
lary of the domain, but their use without additional con-
text may be of less value for data integration. Ontologies
for describing types of microscopy assays already exist
and should be used in combination with ontologies like
CMPO in order to provide a meaningful annotation,
however, best practices and tooling to support this kind
of structured data annotations are still lacking.
Despite the generality of the ontology building meth-
odology applied, several challenges remain, including the
lack of common design patterns that curators could con-
sistently use when creating new terms, in the pre-
composition phase. The need for common design pat-
terns can be illustrated with an example from CMPO for
the creation of an increased cytoplasmic actin pheno-
type term. This term was initially problematic to anno-
tate with a basic EQ because no term for cytosolic actin
existed in GO. The curators initially used a close ap-
proximation which was EQ(actin filament, present in
greater number in organism), but the fact that the actin
is localised to the cytosol is lost in the EQ. To increase
the expressivity of the annotation in Phenotator a third
column was added to capture additional modifiers to the
EQ resulting in annotations emerging like EQE2 (actin
filament, localised, cytosol). There are other ways that
one might consider describing this phenotype such as
EQE2 (cytosol, has extra parts of type, actin filament).
Guidelines and tooling that help with guiding the cura-
tors to create a good EQ annotation are therefore
needed to resolve ambiguities and develop a consistent
strategy for creating new ontology terms.
Pattern-based tooling to rapidly generate new terms
are emerging and these could nicely complement exist-
ing tooling that are primarily aimed at annotating phe-
notypes with EQs such as Phenotator and Phenote.
Phenotator and Phenote do little to guide the annotator
to make a correct EQ annotation and the translation of
these annotations to OWL typically only allows for a
basic SUBQ pattern. Tools like TermGenie [32] and
Jupp et al. Journal of Biomedical Semantics  (2016) 7:28 Page 6 of 8
Webulous offer greater flexibility for post composing
terms, as they are not restricted to EQ alone and can
use more expressive design patterns for the translation
of input data into OWL.
Conclusion
CMPO is a species neutral ontology for describing cellu-
lar phenotypes that has been established according to
the best practices from the Open Biomedical Ontology
community. This allows CMPO to be developed inde-
pendently from other phenotype ontologies, but to also
remain interoperable via inference derived from the use
of logical class descriptions. This interoperability will
allow future integration of data annotated with species-
specific vocabularies with imaging data annotated with
CMPO.
We are committed to the continued development of
CMPO and the use of CMPO in tools such as CellCog-
nition and resources such as the IDR, Mitosys and Mito-
check. We are developing better tools to support
building ontologies from design patterns that allow us to
engage the imaging user community in the future devel-
opment of CMPO. Beyond the benefits in browsing and
searching phenotypic data, CMPO also enables new data
analysis. For example, by replacing free-text annotations,
CMPO makes automatic evaluation of phenotypic simi-
larity possible and allows systematic exploration of the
links between gene function and loss of function pheno-
types across experiments thus facilitating the conversion
of phenotypic annotations to functional annotations.
Additional work to harmonise the various cellular phe-
notypes ontologies with CMPO will provide new possi-
bilities for integration and analysis of this kind of data
across species.
Endnotes
1http://www.biomedbridges.eu/
2http://www.eurobioimaging.eu/
3https://www.infrafrontier.eu/
4http://bbmri-eric.eu/
5http://www.w3.org/TR/owl2-manchester-syntax/
6http://www.w3.org/TR/owl2-profiles/#OWL_2_EL
7https://github.com/EBISPOT/CMPO
8http://www.ebi.ac.uk/ols
9http://www.mitosys.org/
10https://github.com/obophenotype/upheno/blob/mas-
ter/docs/OWLAxiomatization.md
11https://chrome.google.com/webstore
12https://github.com/owlcollab/owltools
13http://obi-ontology.org/page/MIREOT
14https://github.com/EBISPOT/CMPO
Competing interests
The authors declare that they have no competing interests.
Authors contributions
CMPO was conceived by SJ, JK, JM and GR. SJ, GR and JM designed the
ontology and were responsible for much of its content. The Phenotator and
Webulous software was developed by SJ and TB. Many of the phenotypes
were annotated with ontology terms by GR, EW and JK. All authors read and
approved the final manuscript.
Acknowledgements
The authors would like to acknowledge the following for their contributions
to CMPO; Johan Lundin, Zvi Kam, Bran Herpers, Beate Neumann, Thomas
Walter, Claudia Lukas, Frauke Neff, Jennifer L. Rohn,. Funding for CMPO
development was provided by EU-FP7-BioMedBridges project, grant agree-
ment no. 284209. A subset of the phenotypic data used to build CMPO was
provided by the EU-FP7-Systems Microscopy NoE, grant agreement no.
258068. IDR development is supported by the BBSRC for bioscience big data
infrastructure, award BB/M018423/1.
Author details
1European Bioinformatics Institute (EMBL-EBI), European Molecular Biology
Laboratory, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD,
UK. 2European Molecular Biology Laboratory, Meyerhofstrasse 1, 69117
Heidelberg, Germany. 3Centre for Gene Regulation and Expression, University
of Dundee, Dundee DD1 5EH, UK.
Received: 16 December 2015 Accepted: 10 May 2016
Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 
DOI 10.1186/s13326-016-0083-z
RESEARCH Open Access
A corpus of potentially contradictory
research claims from cardiovascular research
abstracts
Abdulaziz Alamri* and Mark Stevenson
Abstract
Background: Research literature in biomedicine and related fields contains a huge number of claims, such as the
effectiveness of treatments. These claims are not always consistent and may even contradict each other. Being able to
identify contradictory claims is important for those who rely on the biomedical literature. Automated methods to
identify and resolve them are required to cope with the amount of information available. However, research in this
area has been hampered by a lack of suitable resources. We describe a methodology to develop a corpus which
addresses this gap by providing examples of potentially contradictory claims and demonstrate how it can be applied
to identify these claims from Medline abstracts related to the topic of cardiovascular disease.
Methods: A set of systematic reviews concerned with four topics in cardiovascular disease were identified from
Medline and analysed to determine whether the abstracts they reviewed contained contradictory research claims. For
each review, annotators were asked to analyse these abstracts to identify claims within them that answered the
question addressed in the review. The annotators were also asked to indicate how the claim related to that question
and the type of the claim.
Results: A total of 259 abstracts associated with 24 systematic reviews were used to form the corpus. Agreement
between the annotators was high, suggesting that the information they provided is reliable.
Conclusions: The paper describes a methodology for constructing a corpus containing contradictory research
claims from the biomedical literature. The corpus is made available to enable further research into this area and
support the development of automated approaches to contradiction identification.
Keywords: Contradictory claims, Natural language processing
Background
The research literature in medicine is vast and increas-
ing rapidly. These papers contain a massive amount of
information, including claims about the research ques-
tion being addressed. However, papers may not come to
the same conclusion about a particular research question
and claims in different papers may even contradict one
another. Contradictory claims make it difficult to under-
stand the current state of knowledge about a research
question. Systematic reviews aim to avoid this problem by
evaluating and assessing the evidence related to a particu-
lar research question, including contradictory claims, and
*Correspondence: adalamri1@sheffield.ac.uk
1Department of Computer Science,The University of Sheffield, Sheffield, UK
presenting it in a summarised format. However, these are
not available for all research questions and are also limited
by the evidence that was available when the review was
written.
Tools that support the automatic identification of con-
tradictory claims would be useful for those that rely on
biomedical literature. They could be used to highlight
research claims that are contradicted by other research
findings, assist in the creation of systematic reviews [1]
and literature surveillance systems [2]. They would also be
useful for automatic textmining applications which gener-
ally accept claimsmade within research literature as prima
face correct. Despite this there has been little exploration
into this problem. The work that has been carried out [3]
focused on descriptions of molecular events in a corpus
© 2016 Alamri and Stevenson. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 2 of 9
mainly generated from the events in the BioNLP09 corpus
[4], and was restricted to a single indicator of contradic-
tion, the use of negation. Outside the biomedical domain
there have only been a few attempts to study the problem
of contradiction identification independently of the more
general problem of textual inference [57].
One of the reasons for this limited progress is a lack of
suitable resources that can be used to develop and test
approaches. Developing these resources is not straight-
forward given the volume of research that has been
published and the difficulty of identifying contradictory
claims within them. This paper presents an approach
to developing a corpus containing examples of poten-
tially contradictory research claims which are identified
bymaking use of information found in systematic reviews.
The corpus is designed to include a wider range of topics
from the biomedical literature and wider range of linguis-
tic phenomena that can be used to indicate contradiction
(e.g. negation, use of antonyms and adjective polarity)
than previous work.
Reviewing the published biomedical literature to iden-
tify the best available information related to biomedical
questions is standard practice [2]. However, the litera-
ture may contain findings that contradict one another
and investigators have shown a tendency to reproduce the
findings of original research with contradictory claims.
Consequently, editors and publishers attracted by these
results tend to publish them faster than those with less sig-
nificant findings, dubbed the Proteus phenomenon [8]. A
good example of contradictions between research claims
that have appeared in the biomedical research literature
concerns the relation between aspirin and heart attack
prevention.
Aspirin has been widely used as a pain killer and
an effective drug for preventing blood clots. A conflict
on its benefits started when doctors began prescrib-
ing a daily dosage to protect heart attack victims from
further attacks. At that time there was no biomedical
research to prove that this was effective. An attempt
to investigate [9] found that aspirin was significantly
beneficial in preventing heart attacks. However, a sub-
sequent trial was less confident about that because it
found little difference between the fatality rate of people
who never used, seldom used or often used aspirin [10].
Another study [11] was compatible with that result as it
failed to show the preventative role of aspirin on heart
attacks. The first team, who found a significant benefit
of aspirin on the heart, conducted another experiment
[12] and reported a positive results that supported their
first claim. The contradictions between aspirin research
claims lasted 20 years, until researchers finally concluded
that aspirin reduces the risk of non-fatal heart attacks,
but its effects on other problems such as stroke are still
unclear [13].
Other research topics such as the effectiveness of mam-
mographies for discovering breast cancer or whether the
Dalkon Shield caused pelvic infections are also rich with
contradictory claims [13].
Methods
This section discusses some of the key concepts used in
this work, beginning with the definition of contradiction
and followed by the types of claim.
Defining contradiction
Contradiction has been defined as the existence of two
or more incompatible propositions that describe the same
fact [14]. In another words, two fragments of text, T1
and T2, are contradictory when they assert information
about the same fact that cannot both be true at the same
time. The problem of contradiction has previously been
explored within work on textual entailment [5, 7, 15]
where a common approach is to consider T1 and T2 to be
contradictory when one of them entails the negation of
the other. De Marneffe et al. [6] used a looser definition
intended to be less restrictive: two fragments of text are
contradictory when they are extremely unlikely to be true
at the same time.
There has been some previous exploration of the prob-
lem of identifying contradictions in biomedical docu-
ments [3]. Contradiction was defined as two texts that
describe events sharing certain attributes (e.g. theme,
cause and anatomical location) but with different polarity.
That work was restricted to statements about a very spe-
cific type of information (chemical interactions) and one
way of expressing contradiction (negation).
This work also focusses on biomedical documents but
uses a less restrictive definition of contradiction. Two
texts, T1 and T2, are said to contradict when, for a given
fact F , information inferred about F from T1 is unlikely to
be true at the same time as information about F inferred
from T2.
This definition of contradiction is based on inferences
from statements being unlikely to be true at the same time
rather than being logically inconsistent. This approach
avoids the definition of contradiction being overly restric-
tive and has been used by previous work [6]. Research
findings in scientific documents are often expressed cau-
tiously, e.g. using hedges [16], reducing the chances of
statements being logically inconsistent with one another.
Nevertheless, researchers are often interested in obtaining
as much information as possible about a research ques-
tion of interest and are likely to be interested in statements
which are unlikely to be simultaneously true.
The language used in biomedical documents tends to
involve complex sentence structures with multiple facts
described within the same sentence and it is there-
fore important to consider contradiction relative to a
Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 3 of 9
particular research question or fact. For example, sen-
tences (2) and (4) in Table 1 would be considered contra-
dictory in relation to some facts but not others. Sentence
(2) states that fish intake does not prevent heart failure
without providing information about the types of fish or
population groups the assertion applies to. Sentence (4)
asserts that fish intake does prevent heart failure for a
particular population group (older adults) and types of
fish (tuna, broiled or baked). The sentences would not
be considered contradictory relative to the fact consump-
tion of fried fish prevents heart failure, since both suggest
that it does not. However, they would be considered con-
tradictory if the fact being considered was eating tuna
prevent heart attack in older adults since sentence (4)
suggests that it does while sentence (2) suggests that it
does not.
It is possible that contextual information may affect
whether pairs of statements are considered contradic-
tions (e.g. there would be no contradictions between sen-
tences (2) and (4) if sentence (2) only applied to teenagers
and fried fish). However, they are considered in isolation
and do not take account of their context. This approach
ensures that the problem does not become intractable
and is common with other work on contradiction
detection [6].
Claim definition and types
The identification of claims, and the contradictions
between them, is made more complex by the range of
different types of claim that can occur in biomedical liter-
ature and various typologies have been proposed.We now
define research claim and discuss the types used in the
corpus.
A research claim can be defined as the summary of
the main points presented in a research argument; these
points can either introduce new knowledge to readers or
update their knowledge on a topic [17]. The claim con-
tains themost important piece of information that authors
want to communicate. It represents the research findings
or outcomes. In biomedical literature claims tend to sum-
marize the authors findings and occur at the end of the
study [17].
Blake [18] identified five types of claims: explicit,
implicit, correlations, observations and comparisons. This
typology was formulated based on the availability of cer-
tain information (facets): two concepts, a change and
the basis of the claim. Although the typology provides a
framework of how a biomedical claim can automatically
be analysed, it was not clear how a judgmental claim such
as effectiveness of a drug or a technique can be analyzed,
for example, sentence (6) in Table 2.
We use another framework [17], which has been
constructed from a general perspective rather than specif-
ically for the biomedical domain. The framework con-
sists of four types of claim: factual, recommendation,
evaluative and causal. Causal and evaluative claims are the
most relevant types for our corpus. Factual claims tend
to be generally accepted information and these claims are
most commonly found in background sections rather than
the conclusion. Recommendation claims often provide
recommendations of courses of action supported by the
main research claim rather than providing any new infor-
mation. We describe causal and evaluative claims using
examples from the biomedical literature.
Evaluative claims occur when an author expresses a
judgment about the value of a biomedical concept (e.g.
drug, procedure, equipment, gene, protein). This type of
claim is often used as an interpretation of evidence pre-
sented in the research. It is usually expressed by either
making a statement about the properties of a concept
Table 1 Claims extracted from the abstracts described in Table 3
Claim PMID Value Type
1 In this large, population-based sample of African-
American and white adults, whole-grain intake was
associated with lower HF risk, whereas intake of eggs
and high-fat dairy were associated with greater HF risk
after adjustment for several confounders
18954578 YS CAUS
2 Our findings do not support a major role for fish intake
in the prevention of heart failure
19789394 NO CAUS
3 Moderate consumption of fatty fish (1-2 servings per
week) and marine omega-3 fatty acids were associated
with a lower rate of first HF hospitalization or death in
this population
20332801 YS CAUS
4 Among older adults, consumption of tuna or other
broiled or baked fish, but not fried fish, is associated
with lower incidence of CHF
15963403 YS CAUS
5 Increased baked/broiled fish intake may lower HF risk,
whereas increased fried fish intake may increase HF risk
in postmenopausal women
21610249 YS CAUS
Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 4 of 9
Table 2 Claims typology examples
Claim PMID Type
6 Combined clopidogrel and aspirin overcome single drug resistances, are safe for
bleeding
22942294 Judgemental
7 Aspirin plus clopidogrel is more effective in venous graft patency than aspirin alone in
the short term after CABG, but further, long-term study is needed
21050973 Comparative
8 Although a bedtime dose of doxazosin can significantly lower the blood pressure, it
can also increase left ventricular diameter, thus increasing the risk of congestive heart
failure.
18551024 Excitatory
9 Routine use of postoperative aspirin after coronary artery bypass grafting (CABG)
reduces graft failure and cardiovascular events
21146675 Inhibitory
10 In the Spanish Mediterranean area, the presence of antigens B-15 and DQ3 would be
associated with advanced DCM
10198739 Neutral
(judgment), e.g. sentence (6), or comparing the concept
with another, e.g. sentence (7).
Causal claims suggest a relationship between two con-
cepts and assert that one concept influences the other.
Hashimoto et al. [19] described three types of influences:
excitatory, inhibitory and neutral. Excitatory influence
indicates a direct activation or enhancement, e.g sentence
(8) shows the doxazosin had an excitatory influence on
left ventricular diameter. An inhibitory influence is the
opposite of excitatory and indicates direct deactivation or
suppression. For example, sentence (9) is a casual claims
which asserts that Routine use of postoperative aspirin has
an inhibitory effect on graft failure and cardiovascular
events. The final type of causal claim, neutral, is nei-
ther excitatory nor inhibitory. For example, sentence (10)
asserts a relationship between presence of antigens B-15
and DQ3 and advanced DCM (Dilated Cardiomyopathy)
but doesnt explicitly state whether it is excitatory or
inhibitory.
Corpus construction stages
Corpus data collection
The corpus was created using research abstracts of studies
considered in systematic reviews related to cardiovascular
diseases. Cardiovascular diseases have been reported as
a major contributor to world mortality and their causes
are commonly explored in research papers [20]. Given the
volume of research published on the topic we expect to
find some contradictory findings.
Four types of cardiovascular disease were chosen: Car-
diomyopathy, Coronary artery, Hypertensive and Heart
failure. The Pubmed search engine was used to retrieve
systematic reviews associated with these types. For exam-
ple, the query Cardiomyopathy[title] AND
meta-analysis[title] was used to search for
systematic reviews discussing cardiomyopathy disease,
and the same procedure were applied on the other types.
Themodifier [title]was used to ensure that the search
keywords occurred within the title of the article.
Systematic reviews were used since they gather find-
ings from multiple studies related to a defined research
question and summarise their results using statistical
meta-analysis. Results of the meta-analysis are often pre-
sented using a diagram called a forest plot [21] which
represents the findings of a set of studies. In each sys-
tematic review, the forest plot diagram was examined to
determine whether it suggested contradictions between
the studies included. If any potential contradictions were
identified then all the studies included within the sys-
tematic review were included in the corpus. For exam-
ple, Fig. 1 shows a forest plot in which a single study,
(Comstock & Webster, 1969), favours the placebo and
consequently this study may contain a claim regarding
the effectiveness of the treatment which contradicts those
made in the other studies. Although, the difference may
not be significant and may not even be reflected in the
abstract description, the forest plot diagram is still a good
indication that the details discussed in this review may
contain contradictory claims.
Table 3 shows a list of abstracts titles; title (11) refers to
the systematic review that was collected from MEDLINE;
and titles (12-16) are the studies used in that review. These
studies were included as candidate datasets since the
diagram of the systematic review showed disagreement
between at least one of theirs findings.
Question formulation
Biomedical literature contains claims with complex struc-
ture, often expressing multiple facts in the same sentence.
This may confuse the annotators when annotating con-
tradictory claims in the dataset. To avoid this issue and
ensure that annotations correspond with the definition of
contradiction being used, a common question shared by
claims needs to be identified to ensure that the annotators
focus on the same fact when identifying contradictions.
As an attempt to achieve that goal, an annotator with
an advanced degree in medicine was asked to use the
titles of each systematic review and the studies abstracts
Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 5 of 9
Fig. 1 The diagram represents the outcome of studies exploring the effectiveness of a vaccine (BCG) for preventing tuberculosis. Studies that favour
the vaccine are shown on the left side of the vertical column while those that favour the placebo are shown on the right side. This diagram shows
one study that favoured the placebo (Comstock & Webster, 1969) and two for which no statistically significant difference between the vaccine and
placebo could be identified, (TPT Madras, 1980) and (Comstock et al.,1976). The vaccine was favoured in all other studies. The dataset was retrieved
from metafor [25], an R package for conducting meta-analyses
included, as information to formulate a suitable question
for the group of studies in that review. The annotator
was asked to formulate closed questions (i.e. ones that
could be answered as either yes or no) written in sim-
ple present tense. He was also asked to ensure that the
questions were compiled with the PICO standard [22] to
include information about the patient problem or popu-
lation (P), Intervention (I), comparison (C) and outcomes
(O). For example, the question In patients with chronic
heart disease (P), does bone marrow stem cell transplanta-
tion or injection (I), compared to none (C), improve cardiac
function (O)?.
This approach will enable the annotators to measure
the assertion values of claims with respect to the ques-
tion. Thus, when two claims provides different assertion
value or conclusion to a question, they are considered
potentially contradictory.
Corpus annotation
The final stage of corpus construction was to identify
and annotate the claims in each abstract. Two annotators
were recruited. Each annotator had native-level English
fluency, an advanced degree in a field related to medicine
and was employed in a medical role (one in an academic
department and another in a hospital). Both were familiar
with biomedical research literature and evidence-based
medical research. The annotators were asked to carry out
three tasks: choose a claim, annotate the claim with an
assertion value (YS/NO) with respect to the question for-
mulated for the review group, and annotate the claim
type (CAUS/EVAL) according the claim types described
earlier.
The first task required examining each study abstract
and, considering the question that had been formulated
from the systematic review, identifying the claim sentence
Table 3 A systematic review title and the titles of its associated studies
Title PMID
11 Review Fish consumption and incidence of heart failure a meta-analysis of prospective
cohort studies
23489806
12 Study Incident heart failure is associated with lower whole-grain intake and greater
high-fat dairy and egg intake in the Atherosclerosis Risk in Communities (ARIC)
study
18954578
13 Study Intake of very long chain n-3 fatty acids from fish and the incidence of heart
failure: the Rotterdam Study
19789394
14 Study Fatty fish, marine omega-3 fatty acids and incidence of heart failure 20332801
15 Study Fish intake and risk of incident heart failure 15963403
16 Study Fish intake and the risk of incident heart failure: the Womens Health Initiative 21610249
Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 6 of 9
within each abstract that could answer that question. If
the abstract contained multiple candidate claim sentences
then annotators were asked to select one that corresponds
to the overall abstract finding (as represented in the for-
est diagram) and best describes the contribution of the
research relative to the question. After the claim had been
identified annotators were asked to mark it as either YS
(to indicate the claim was an affirmative answer to the
question) or NO (to indicate that it was not). Finally, the
annotators were asked to identify the type of the claim (i.e.
causal or evaluative). After each annotation phase the two
annotators met to resolve disagreements and decided on
the final annotation.
Results and discussion
Examination of forest plot diagrams lead to the identifi-
cation of 40 suitable systematic reviews and a question
was formulated for each. A total of 397 studies were
mentioned in these reviews. These studies were retrieved
and annotators asked to identify a claim in each, decide
whether this claim agreed with the question that had been
formulated and determine the claim type. 19 of the studies
were excluded since the annotators were unable to iden-
tify a claim that provided a clear answer to the question.
No contradictions were identified for 16 of the system-
atic reviews (i.e. the annotators judged all of the claims in
the studies associated with the review as either agreeing
or disagreeing with the question). These reviews and the
studies associated with them were also excluded.
The final corpus consists of 259 studies used within
the 24 systematic reviews that were not excluded. Table 4
shows the number of studies associated with each sys-
tematic review and their distribution across the assertion
values (YS and NO). The questions formulated for each
systematic review are shown in Table 5. The corpus is
formatted in XML as shown in Fig. 2.
The annotators were asked to complete three tasks:
identify a single claim within each abstract, determine
whether that claim agreed with the research question or
not, and annotate the claim type (CAUS/EVAL).
Inter-annotation agreement for the claim identification
task was 92%. The main reason for disagreement was
cases where there were multiple claims in the same study
abstract that potentially answer the question formulated
for the systematic review. For example, Table 6 shows
two sentences, (17) and (18), extracted from an abstract
that potentially answer the question In women with pre-
eclampsia, is polymorphism in angiotensin gene associated
with pre-eclampsia?. In such cases the annotators were
asked to prefer sentences in the conclusion sections of the
abstracts.
Agreement for the second task, determining whether
the claim agreed with the question or not, was very
high (97%). The disagreements that did occur arose from
Table 4 Claims classes and type distribution among the groups
Assertion Type
Topic Review-PMID #Abstracts YS NO CAU EVA
Cardiomyopathy
22498326 4 3 1 2 2
23623290 9 7 2 3 6
21556773 15 12 3 12 3
Coronary artery
24035160 5 3 2 2 3
24135644 20 13 7 13 7
24036021 4 2 2 4 0
24212980 20 12 8 14 6
24039708 18 11 7 17 1
24172075 7 2 5 0 7
24090581 8 4 4 2 6
24040766 5 4 1 5 0
Heart failure
23489806 4 3 1 4 0
23181122 5 4 1 5 0
23962886 15 13 2 14 1
24163234 29 22 7 13 16
24165432 6 4 2 2 4
23219304 10 6 4 5 5
21521728 11 7 4 6 5
Hypertensive
23602289 17 14 3 10 7
22795718 14 13 1 9 5
23435582 6 4 2 5 1
22854636 5 3 2 2 3
23223091 7 6 1 6 1
22086840 15 7 8 10 5
TOTAL 259 180 79 165 94
claims that did not provide a conclusive answer to the rel-
evant question. This problem was generally avoided by
formulating a question for each systematic review but in
some cases multiple inferences can be derived from the
same claim. For example, the question In the elderly, is n-
3 fatty acid from fish intake associated with reduction in
risk of developing heart failure? asked about the associa-
tion of n-3 fatty acid from fish and the risk of developing
heart failure but did not specify the type of fish. Table 7
shows multiple inferences derived from claims (4) and (5)
in Table 1 (inferences (4a) and (4b) from claim (4) and
inferences (5a) and (5b) from claim (5)). These inferences
differ in their agreement with the question. In such situa-
tions annotators were asked to choose the inference that is
the best fit for the question. In this case, inference (4a) was
used for claim (4) since it is more general than the alter-
native (4b). Similarly, inference (5a) was used for claim (5)
rather than (5b) since the second referred to a restricted
population (postmenopausal women).
Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 7 of 9
Table 5 A list of the 24 questions formulated for the final corpus
Review-PMID Question
22498326 In patients with HCM, does using imaging technique, compared to conventional techniques, serve as a predictor for
adverse prognosis?
23623290 In patients with chronic heart disease, does Bone marrow Stem cell transplantation or injection, compared to none,
improve cardiac function?
21556773 In patients with dilated cardiomyopathy, are HLA genes associated with development of Dilated Cardiomyopathy?
24040766 In Han Chinese population, is SNP T-778C of apolipoprotein M associated with risk of developing Diabetes or stroke?
24212980 In patients undergoing coronary bypass surgery, does Aspirin usage, compared to no aspirin, cause bleeding?
24035160 In patients undergoing choronary artery bypass, does the combination of aspirin and clopidogrel, compared to aspirin
alone, prevent graft occlusion or improve patency?
24172075 In patients undergoing coronary by pass surgery, is Off-pump, compared to conventional on pump coronary artery
bypass grafting, more beneficial?
24135644 In patients with choronary artery disease, is mutation or polymorphisms in endothelial nitric oxide synthase gene
associated with CAD or MI or ACS development?
24036021 In patients with atherosclerotic plaque or myocardial infaction, does ?463G or ?463A polymorphism in MPO gene
influence MI or CAD development?
24039708 In patients with coronary artery disease (CAD), is C242T polymorphism of P22(PHOX) gene associated in development
of CAD?
24090581 In patients with coronary artery diseases, does combining CABD and CEA, compared with CABG or CEA alone, reduce
morbidity?
24165432 In elderly patients with CHF, does physical exercise or cardiac rehabilitation, compared to no exercise, improve cardiac
function?
23962886 In patients with heart failure, do statin drugs treatment, compared to non statin drug, treatment improve cardiac
function or prevent cardiac morbidity?
23219304 In patients with renal or cardiovascular disease, does treatment with ACE inhibitors, compared with placebo, improve
renal function or protect against cardiovascular incidents respectively?
23181122 In the elderies, is n-3 fatty acid from fish intake associated with reduction in risk of developing heart failure?
23489806 In the elderlies, does omega 3 acid from fatty fish intake, comparedwith no consumption, reduce the risk of developing
heart failure?
24163234 In patients with CHF, does care giving or teleguidiance-telecare, compared to usual care, reduce morbidity?
21521728 In patients with advanced diabetes, does treatment with antihypertensives, compared with placebo, improve renal
function or protect againct cardiovascular incidents?
22854636 In patients with hypertension, does revascularisation, compared with medical therapy, improve blood pressure?
22795718 In patients with hypertension, does treatment with ACE inhibitors, compared to placebo, reduce risk of cardiovascular
event or improve blood pressure?
23602289 In patients with hypertesion or hypercholesterolemia, does statin drugs, compared to placebo, reduce blood pressure
or lipid levels?
23435582 In women with pre-eclampsia, does treatment with L Arginine, compared to placebo, reduce blood pressure or pre-
eclampsia?
22086840 In women with pre-eclampsia, is Polymorphism in angiotensin gene associated with pre-eclampsia?
23223091 In women with pre-eclampsia, is mutation in renin-angiotensin gene associated with pre-eclampsia?
Lower agreement (86%) was obtained for the final task,
annotation of claim type. The main cause of disagree-
ment were claims that could potentially be simultane-
ously interpreted as causal or evaluative. For example,
the claim These results suggest that HLA-DR4 antigen
may be a genetic marker for susceptibility to dilated car-
diomyopathy can be considered a causal claim since it
describes an association relation between the two con-
cepts HLA-DR4 antigen and dilated cardiomyopathy. But
it can also be evaluative since the author is evaluating the
effectiveness of that gene as a genetic marker. Annotators
were reminded that evaluative claims should express a
judgement, which is not the case here and it was conse-
quently annotated as a causal claim.
The high inter-annotator agreement figures indicate
that the annotation tasks are well-defined and that the
annotations are reliable and form a sound basis for future
studies. Although agreement for the claim type identifica-
tion task is lower than the others, the informationmay still
be useful for further exploration.
Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 8 of 9
Fig. 2 Examples of formatted claims
Automatic identification of contradictory claims is a dif-
ficult problem and a number of challenges were identified
during the construction of our corpus. Claims tend to
appear at the end of abstracts and consequently authors
often use shorter forms such as acronyms, for exam-
ple Our observations indicate a significant relationship
between p22phox C242T and PARP-1 Val762Ala poly-
morphisms, CAD and its severity, but not with occur-
rence of MI in T2DM individuals with significant coro-
nary stenoses. This complicates the process of identifying
claims, particularly since acronyms are often ambiguous
in biomedical text [23, 24].
Identifying connections between statements is also
complicated by authors use of alternative terms. For
example, statin, atorvastatin and rosuvastatin were all
used to refer to drugs that lowers cholesterol levels in
studies included in the corpus.
The corpus developed in this research could be used as a
resource for researchers to explore the problems of identi-
fying, analysing and resolving contradictory claims made
in the biomedical literature. For example, it could be used
to build a machine learning system that discriminates
between claims that agree or disagree with a query, where
contradiction occurs between them when they provide
different answers to the same query. Moreover, the con-
struction methodology described in this paper could be
applied to construct other corpora containing potentially
contradictory claims.
Conclusions
The contradictory claims found in biomedical literature
present a challenge to evidence-based evaluation into
the effectiveness of approaches. Automatic identification,
analysis and resolution of these claims would be useful for
those that rely on this literature.
This paper described the development of a corpus con-
taining contradictory claims found within Medline. Sys-
tematic reviews were used to identify studies that contain
contradictory statements regarding particular research
questions. Claims within the studies were identified and
annotated. Analysis shows that the agreement between
annotators is reliable, suggesting that the information in
the corpus will be useful for those who wish to explore this
problem. The corpus construction methodology could be
applied to other topics in the biomedical domain.
The corpus can be accessed via: http://staffwww.
dcs.shef.ac.uk/people/M.Stevenson/resources/bio_
contradictions/.
Table 6 Potential answers to a formulated question from the same abstract
Sentence PMID Value Type
17 The frequency of T allele of angiotensinogen T174M
gene was slightly increased, but not significantly, in
preeclampsia (0.11) than in controls (0.07)
15082899 YS CAUS
18 In conclusion, a molecular variant of ACE, but not
angiotensinogen, gene is associatedwith preeclampsia
in Korean women
15082899 YS CAUS
Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 9 of 9
Table 7 Multiple inferences derived from two claims
Inference PMID Value
4a consumption of tuna or other broiled or baked fish is associated with lower incidence of CHF 15963403 YS
4b fried fish is not associated with lower incidence of CHF 15963403 NO
5a Increased baked/broiled fish intake may lower HF risk 21610249 YS
5b Increased fried fish intake may increase HF risk in postmenopausal women 21610249 NO
Acknowledgements
The authors thank Adams Aminat and Yomi Yusuf for annotating the corpus
and the anonymous reviewers for their feedback.
Funding
MS was funded by the Engineering and Physical Sciences Research Council
(EP/J008427/1).
Authors contributions
AA carried out the research, data analysis and wrote the initial draft of the
paper. Both authors revised the paper and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Received: 23 May 2015 Accepted: 26 May 2016
Mutowo et al. Journal of Biomedical Semantics  (2016) 7:59 
DOI 10.1186/s13326-016-0102-0DATABASE Open AccessA drug target slim: using gene ontology
and gene ontology annotations to navigate
protein-ligand target space in ChEMBL
Prudence Mutowo1* , A. Patrícia Bento1, Nathan Dedman1, Anna Gaulton1, Anne Hersey1, Jane Lomax2
and John P. Overington1Abstract
Background: The process of discovering new drugs is a lengthy, time-consuming and expensive process.
Modern day drug discovery relies heavily on the rapid identification of novel targets, usually proteins that
can be modulated by small molecule drugs to cure or minimise the effects of a disease. Of the 20,000
proteins currently reported as comprising the human proteome, just under a quarter of these can potentially
be modulated by known small molecules Storing information in curated, actively maintained drug discovery
databases can help researchers access current drug discovery information quickly. However with the increase
in the amount of data generated from both experimental and in silico efforts, databases can become very large
very quickly and information retrieval from them can become a challenge. The development of database tools
that facilitate rapid information retrieval is important to keep up with the growth of databases.
Description: We have developed a Gene Ontology-based navigation tool (Gene Ontology Tree) to help users
retrieve biological information to single protein targets in the ChEMBL drug discovery database. 99 % of single
protein targets in ChEMBL have at least one GO annotation associated with them. There are 12,500 GO terms
associated to 6200 protein targets in the ChEMBL database resulting in a total of 140,000 annotations. The slim
we have created, the ChEMBL protein target slim allows broad categorisation of the biology of 90 % of the
protein targets using just 300 high level, informative GO terms.
We used the GO slim method of assigning fewer higher level GO groupings to numerous very specific lower level
terms derived from the GOA to describe a set of GO terms relevant to proteins in ChEMBL. We then used the slim
created to provide a web based tool that allows a quick and easy navigation of protein target space. Terms from the
GO are used to capture information on protein molecular function, biological process and subcellular localisations.
The ChEMBL database also provides compound information for small molecules that have been tested for their
effects on these protein targets. The ChEMBL protein target slim provides a means of firstly describing the biology
of protein drug targets and secondly allows users to easily establish a connection between biological and chemical
information regarding drugs and drug targets in ChEMBL.
The ChEMBL protein target slim is available as a browsable Gene Ontology Tree on the ChEMBL site under
the browse targets tab (https://www.ebi.ac.uk/chembl/target/browser). A ChEMBL protein target slim OBO file
containing the GO slim terms pertinent to ChEMBL is available from the GOC website (http://geneontology.
org/page/go-slim-and-subset-guide).
(Continued on next page)* Correspondence: prudence@ebi.ac.uk
1European Molecular Biology Laboratory, European Bioinformatics Institute
(EMBL-EBI), Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD,
UK
Full list of author information is available at the end of the article
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Mutowo et al. Journal of Biomedical Semantics  (2016) 7:59 Page 2 of 7(Continued from previous page)
Conclusions: We have created a protein target navigation tool based on the ChEMBL protein target slim.
The ChEMBL protein target slim provides a way of browsing protein targets in ChEMBL using high level
GO terms that describe the molecular functions, processes and subcellular localisations of protein drug targets
in drug discovery. The tool also allows user to establish a link between ontological groupings representing
protein target biology to relevant compound information in ChEMBL. We have demonstrated by the use of
a simple example how the ChEMBL protein target slim can be used to link biological processes with drug
information based on the information in the ChEMBL database. The tool has potential to aid in areas of
drug discovery such as drug repurposing studies or drug-disease-protein pathways.
Keywords: Ontologies, Bioinformatics, Drug discovery, Database, Biology, ProteinBackground
The use of small molecules in alleviating symptoms in a
disease state is generally evaluated against a protein tar-
get [1]. The human proteome is reported to have around
20,000 proteins [2] with literature sources reporting a great
variation in the number of proteins deemed to be druggable
[3, 4]. The biology of the druggable proteome (alternatively
the druggable genome) is usually described in terms of dis-
tinct and well-studied protein families. Protein family classi-
fication can be used together with protein-centric bio-
ontologies to better understand the characteristics of a drug
target of interest.
ChEMBL is a manually curated, freely available resource
containing bioactive ligands with drug-like properties as
well as quantitative bioassay results and the biological
targets of these molecules [5]. Biological targets reported
in ChEMBL assays include nucleic acids, cell-lines, tissues,
subcellular-fractions, whole organisms and proteins. Pro-
tein targets are the largest portion of targets in ChEMBL.
This target group is further divided into single protein
targets, protein complexes, protein families, and protein-
protein interactions. Compound information to ChEMBL
protein targets is obtained by manual curation of selected
published medicinal chemistry literature and data depo-
sitions from data sharing partnerships.
There has been a steady increase of data in ChEMBL
over time. There has been a fourfold increase in the
total numbers of assays from the first ChEMBL release
(ChEMBL1) to the current release (ChEMBL21) which
has 1.2 million assays. The number of single protein
targets doubled with the first release having 3222 single
protein targets to just over 6000 in the current release.
Standardising protein information in ChEMBL is useful to
facilitate ease of protein target retrieval. Single protein
targets in the database are cross-referenced to a variety of
protein property descriptors including Gene Ontology
(GO) based annotations. GO is a set of concepts, struc-
tured as a graph or tree, that provide a controlled and
concise way of capturing the processes, molecular func-
tions and subcellular localisations of gene products in thiscase proteins [6]. An annotation is an evidence-based as-
sertion created to capture biological information about a
protein [7]. GO annotations to protein targets in ChEMBL
are obtained from the GO consortium database [8]. These
annotations provide useful insight into the biology of
proteins in drug discovery. GO annotations vary in their
information content depending on the specificity of the
term used in annotation. Some annotations contain very
specific and fine-grained information about a protein
while others contain broad, high level information. Com-
paring, grouping or searching through protein targets
annotated at different levels of GO information content
can be time consuming and challenging. GO slims are
often used to allow comparison of protein information
captured at different levels of the GO.
A GO slim is a high-level subset of the GO created by
collapsing specific terms and mapping them to their
higher level parent terms using the parentchild hierar-
chies inherent in the GO. GO slimming allows for a repre-
sentation of biological information by using high level
terms that provide a broad overview of the biology [8].
GO slims are typically generated for specific organism or
particular areas of scientific interest and have been used
to aid visualisation, exploration and summarization of
GO functional data [9, 10]. We have created a ChEMBL
protein target slim to allow users to easily access the
biological information to targets with GO annotation.
Construction and content
Creating the ChEMBL protein target slim
We created the ChEMBL target slim by retrieving all rele-
vant annotations to single protein targets in ChEMBL
using the QuickGO tool [11]. QuickGO is a web browser
for GO terms and annotations. GO terms in QuickGO are
identified by an alpha numeric identifier, a term definition
and relationships established between a specific term and
other terms in GO. Annotations retrieved from QuickGO
are obtained from the Gene Ontology consortium GO
database and are created by consortium members. We
retrieved all annotations to the protein set across all
Table 1 Proteins mapped to GO slim terms per species
Species Proteins targets mapped to slim
Homo sapiens 3254
Rattus norvegicus 899
Mus musculus 828
Bos taurus 194
Sus scrofa 98
Escherichia coli K-12 74
Oryctolagus cuniculus 74
Mycobacterium tuberculosis 73
Saccharomyces cerevisiae S288c 70
Staphylococcus aureus 50
Mutowo et al. Journal of Biomedical Semantics  (2016) 7:59 Page 3 of 7evidence codes. The output was downloaded as a Gene
Association File (GAF) which contains protein accessions
and GO term information.
We used the generic GO term mapper tool [12] to
identify an initial high level set of GO terms represen-
ting the annotation information for our protein target
set. The GO term mapper tool uses the map2slim algo-
rithm in count mode to identify high level term parent
terms to terms in the annotation set. The slim terms
suggested by the algorithm are grouped according to the
number of proteins whose initial annotation has been
mapped to a higher level GO term. GO terms with a
high number of proteins mapped are incorporated in the
slim while terms with no proteins are removed. We
selected the Generic GO slim (version 1.2) as the refe-
rence slim for the selection of slim terms. This slim is
not species specific.
From the output the term mapping we manually
inspected and customised the slim to the ChEMBL pro-
tein target set as follows:
GO term refinement
We identified fine grained annotations that could be
mapped to higher level terms. One of the considerations
made in this exercise was the information content of the
higher level terms. An example being proteins annotated
to granular protein binding terms like GO:0017124 SH3
domain binding were not mapped up to the higher level
parent GO:0005515 protein binding due to loss of infor-
mation content.
GO term selection
We assessed the number of accessions not mapped to
any term in the initial reference generic GO slim with a
view to customising the slim terms to reflect the biology
of the protein annotations in the set. We manually
added terms to the GO slim to address this.
We removed terms from the generic GO slim that did
not have any annotations in the ChEMBL protein set.
This addition, removal and term refinement was done
in several rounds of term-to-accession-mapped inspec-
tion until we obtained a set of slim terms providing a
good coverage (in this case 90 %) of the protein targets
in ChEMBL.
Results
The resultant ChEMBL protein target slim generated con-
tains a total of 300 high level GO terms representing the
biology of 5600 protein targets in ChEMBL in the three
areas of GO. In total, proteins from 532 different species
are mapped to terms in the GO slim. The top ten species
(in terms of number of proteins) in the current ChEMBL
release are shown in Table 1.Utility
Based on these slim term categories, we created a GO-
based navigation tool which is available on the ChEMBL
website. This tool termed the Gene Ontology tree can
be found by clicking on the Browse Targets tab on the
ChEMBL home page and selecting the radio button next
to the tree name.
The two key functionalities of the GO tree are:
a. Protein target browsing by GO categories
The ChEMBL protein target slim in the form of a
navigational GO tree also allows users to establish
which processes or functions proteins are involved
in by selecting the process and function nodes. The
cellular component node provides a quick overview
of the subcellular locations of protein targets as well
as a link out to small molecules interaction with
targets in a selected localisation (Fig. 1, Panel 1a).
The numbers affiliated with each GO category on
the tree allow a rapid assessment of which areas of
biology have high proteins giving an indication of
target prioritisation in drug discovery endeavours.
b. Searching for proteins and related compound
information
The tree has a search functionality that allows users
to search the database for protein information using
a specific biological key word or phrase to retrieve
all proteins targets annotated to that term as well
as the compounds and bioactivities to the selected
subset. Figure 1 shows how to use the tree to search
for all proteins involved in response to toxic
substance. By using a key phrase toxic substance in
the search box, the GO slim allows retrieval of all
proteins annotated to GO:0009636 response to toxic
substance (Fig. 1, Panel 1a). The tree shows 432
protein targets annotated to this term as well as
showing the more specific child terms of
GO:0009636 which are GO:0046677 response to
antibiotic. Right clicking on the GO term grouping
Fig. 1 Searching the ChEMBL database using the GO tree to retrieve all proteins involved in response to toxic substance and their related
compound and bioactivity information. Panel a shows the biological process node of the GO tree with a 'toxic substance' keyword search. Panel
b shows the search output of the list of proteins annotated with the 'toxic substance' GO term
Mutowo et al. Journal of Biomedical Semantics  (2016) 7:59 Page 4 of 7information leads the user to a page containing all
the protein targets annotated to that term as well
as the compounds tested against them and the
bioactivities reported for the assays (Fig. 1, panel
1b). A link exits to the QuickGO webpage to view
the definition of the GO term of interest.Case study- using Gene Ontology and drug ATC
information to further establish links between
biological and chemical space in ChEMBL
Another useful application of the biological groupings
created by the GO slim is the ability to provide insight
into the biology of a group of proteins that are targets of
drugs used for specific indications. The ChEMBL data
base contains information to small molecules important
in drug discovery. Small molecule to protein information
links are primarily established by considering the bio-
assay that the two entities are reported together in. In
addition, for FDA approved drugs, targets responsible
for their efficacy (mechanism of action) are assignedmanually. These high level drug classification categories
were combined with the higher level GO classification
categories for their targets to show the mechanism of
action information displayed in Fig. 3.
The database also uses World Health Organisation ana-
tomical therapeutic (ATC code) classifiers that describe
the mechanism of action of a drug to group drugs in
higher level categories. For examples drugs used against
parasitic infections are grouped as anti-protozoals with an
WHO ATC code of P01.
Figure 3 is a simple example of selecting a drug clas-
sification of interest from the ATC classification. We
retrieved all drugs from ChEMBL that are used as Anti-
neoplastic and Immunomodulating agents (WHO ATC
classification L [13]), and the curated mechanism of
action information and readily retrieving the biology of
the targets of these drugs by using the GO slim catego-
ries that describe their assigned efficacy targets.
The targets of all therapeutic drugs in release 20 of
ChEMBL consists of 1179 individual proteins. Of this
number, 196 proteins are targets of drugs in the ATC L
Mutowo et al. Journal of Biomedical Semantics  (2016) 7:59 Page 5 of 7class [13]. We used the ChEMBL slim to navigate the
biological processes that these proteins are involved in.
Figure 2 shows a venn diagram [14] of the 5 GO biological
process categories for these proteins as cell death, cell
motility, cell morphogenesis, cell proliferation and cell
death. The number of drugs that modulate the proteins in
each of the groupings are shown in the venn diagram sets.
It is immediately apparent that 24 drugs have targets rep-
resented in all 5 categories of biological grouping. Using
the drug mechanism of action information in ChEMBL
we probed this set of 24 drugs on mechanism and the two
main mechanism of actions shown are protein kinase
inhibitor action and growth factor receptor inhibition
(Fig. 3).
Considering higher level drug and target classification
can give broad insight into the biology for protein tar-
gets of drugs in the same classification.
Discussion
The existence of large numbers of protein annotations
in the GO database provides a useful resource for com-
putational querying of protein sets annotated in this
way. The fact that not all GO annotations are made to
the same level of term specificity can make it proble-
matic to query protein sets annotated with this way. GO
slims are a useful of compressing annotation information
to obtain a broad but informative overview of protein
biology. Grouping proteins into biological categories
using a GO slim approach comes with the caveat that
its is possible for a single protein to being represented
in more than one category due to the multi-functionalFig. 2 Number of drugs used as Antineoplastic and Immunomodulating A
generated using the ChEMBL slimnature of certain proteins. Similarly some proteins that
have yet to be annotated or protein whose annotations are
yet to be deposited in a database may not feature in such
grouping systems until such a time when the annotation is
created and incorporated in the database. The same can
be said of drug information. Some drugs have been known
to have protein targets from more than one biological
grouping due to the drug having multiple targets and/or
different mechanisms of action. However with the bio-
logical information captured using GO to protein targets
in ChEMBL, the GO slim still provides a quick and useful
way of navigating protein target space and related small
molecule information.
Conclusion
We have created a protein target navigational tool using
the ChEMBL protein target slim specifically designed
for browsing drug discovery protein targets. This tool
provides a rapid way of searching for biological informa-
tion to proteins in a large database. The tool also allows
for a rapid overview of the biology of protein target space.
Besides providing information on the biological process
and molecular functions of protein targets the navigation
tree also readily provides an overview of protein target
subcellular localisation.
The slim is freely available for use and is updated regu-
larly to reflect changes in both the GO and ChEMBL
protein target space. We anticipate the slim will be a use-
ful tool for other researchers and tool developers wishing
to display, explore and summarize GO data in the area of
drug discovery.gents (ATC Class L) targeting proteins in 5 biological process categories
Fig. 3 Mechanism of action for drugs at intersection of protein GO categories
Mutowo et al. Journal of Biomedical Semantics  (2016) 7:59 Page 6 of 7Availability and requirements
The ChEMBL drug target slim is freely available from the
ChEMBL website https://www.ebi.ac.uk/chembl/target/
browser [15]. The GO terms slim terms used for the slim
classes are available from the Gene Ontology Consortium
together with the other GO slims [16]. The ChEMBL data
is made available on a Creative Commons Attribution-
Share Alike 3.0 Unported License.
Abbreviations
GAF: Gene association file; GO: Gene ontology; WHO ATC: World Health
Organisation Anatomical Therapeutic Chemical Classification
Acknowledgements
We would like to thank the GO consortium for agreeing to host the slim on
their website. Special thanks go to David Osumi-Sutherland, Paola Roncaglia,
and Chris Mungall for specialist ontology advice and assisting with the
formatting of the ontology files.
Funding
This work was supported by the EMBL member states and Wellcome Trust
Strategic award [WT086151/Z/08/Z].
Authors contributions
PM created the ChEMBL GO slim, carried out protein target analysis and
drafted the manuscript. APB integrated the WHO ATC classification for drugs
and informed the data analysis for the drugs. AG integrated the biology and
chemistry concept of the slim and created tables in the database to house
the slim terms and generated scripts for their updates and continued
maintenance as well as helping to draft the manuscript. AH conductedchemistry analysis on the representation of the compound data and how
this is best represented in the context of the slim and helped to draft the
manuscript. JL provided input into the ontology terms and in the interative
selection of appropriate terms for the slim. ND created the ChEMBL target
slim visual on the ChEMBL website. JPO conceived the idea of a method of
target navigation incorporating Gene Ontology and the design of the
concept regarding the user needs and database capabilities and helped to
draft the manuscript. All authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Author details
1European Molecular Biology Laboratory, European Bioinformatics Institute
(EMBL-EBI), Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD,
UK. 2Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus,
Hinxton, Cambridge CB10 1SA, UK.
Received: 2 December 2015 Accepted: 16 September 2016
RESEARCH Open Access
Towards exergaming commons: composing
the exergame ontology for publishing open
game data
Giorgos Bamparopoulos1, Evdokimos Konstantinidis1, Charalampos Bratsas2 and Panagiotis D. Bamidis1*
Abstract
Background: It has been shown that exergames have multiple benefits for physical, mental and cognitive health.
Only recently, however, researchers have started considering them as health monitoring tools, through collection
and analysis of game metrics data. In light of this and initiatives like the Quantified Self, there is an emerging need
to open the data produced by health games and their associated metrics in order for them to be evaluated by the
research community in an attempt to quantify their potential health, cognitive and physiological benefits.
Methods: We have developed an ontology that describes exergames using the Web Ontology Language (OWL); it
is available at http://purl.org/net/exergame/ns#. After an investigation of key components of exergames, relevant
ontologies were incorporated, while necessary classes and properties were defined to model these components. A
JavaScript framework was also developed in order to apply the ontology to online exergames. Finally, a SPARQL
Endpoint is provided to enable open data access to potential clients through the web.
Results: Exergame components include details for players, game sessions, as well as, data produced during these
game-playing sessions. The description of the game includes elements such as goals, game controllers and
presentation hardware used; what is more, concepts from already existing ontologies are reused/repurposed. Game
sessions include information related to the player, the date and venue where the game was played, as well as, the
results/scores that were produced/achieved. These games are subsequently played by 14 users in multiple game
sessions and the results derived from these sessions are published in a triplestore as open data.
Conclusions: We model concepts related to exergames by providing a standardized structure for reference and
comparison. This is the first work that publishes data from actual exergame sessions on the web, facilitating the
integration and analysis of the data, while allowing open data access through the web in an effort to enable the
concept of Open Trials for Active and Healthy Ageing.
Keywords: Serious games, Exergames, Ontology, Linked open data, Exergame commons, Active and healthy
ageing, Open clinical trials
Introduction
Continuous monitoring through self-tracking tools such
as serious games may result in better health assessments.
These games produce large amounts of data, however, if
they are published in a non-standardized way
, the capacity to exploit data richness, by combing and
integrating patient-related datasets will be destroyed,
which may weaken its assessment capabilities. Moreover,
large volumes of data are nowadays collected from appli-
cations that monitor posture and body movements. The
difficulty of assessing physical activity and/or sedentary
behavior can be addressed by describing the data in a
standardized format. A recent study has presented the
ontology of physical activity (OPA) in order to provide a
formal description of physical activity [1]. The ontology
of physical exercise (OPE) was developed towards a
similar purpose, in an attempt to describe the physical
exercise in terms of functional movements, muscles
* Correspondence: bamidis@med.auth.gr
1Medical Physics Laboratory, Medical School, Faculty of Health Sciences,
Aristotle University of Thessaloniki, Thessaloniki, Greece
Full list of author information is available at the end of the article
© 2016 Bamparopoulos et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 
DOI 10.1186/s13326-016-0046-4
involved, as well as, hardware and monitoring devices
that are used [2].
In recent years, there has been an increased interest by
the game industry in game controllers that enable user
interaction through body movements such as Wii Balance
Board and Microsoft Kinect. These technological advances
have led to the development of the so called exergames
[3], a special kind of serious games, which combine
gaming and exercise resulting in maintenance and im-
provement of physical status, focusing on large muscle
groups. Serious games in general have an impact on
users such as a change in knowledge, behavior, physical
state, cognitive function as well as health and mental
well-being [4]. Moreover, they may form a part of the
health care system in the foreseeable future, as they en-
able focusing on the patient needs through personalized
interventions demanded by recent focus on active and
healthy ageing [4]. Konstantinidis et al. [5] presented an
exergaming platform which was used in the Long Lasting
Memories project [68]. In the latter papers, it was im-
plied that the perceived, by the elderly users, usefulness of
the physical exercise through exergames contributes to
their motivation and their subsequent adherence to the
exercise protocol. In accordance with other studies, it was
also reported that exergames can positively impact many
health areas, such as balance, gait [9], motion control [10],
quality of life [5], mood and sociability [11, 12], self-
esteem and reduced risk for depression [13]. Regular exer-
cise at home could help to prevent diseases aggravated by
a sedentary life imposed by conditions such as cardiovas-
cular diseases and stroke [14]. Furthermore, health bene-
fits from exercises could contribute to the reduction of
health care costs for insurance companies and the public
health system as well [15].
Only recently some studies started investigating and
presenting exergames as health monitoring tools,
through collection and analysis of the data produced by
their metrics [16]. Such metrics are quantitative mea-
sures of the characteristics of game objects, used to
record a players performance and behavior. They
range from simple measures such as rating and total
game time to more complex ones, which are calculated
by combining various variables/features [4]. More spe-
cifically, metrics of balance games have been studied in
the context of fall prediction [17, 18], while other stud-
ies have found significant correlations between re-
sponse time and the risk of falling [19]. Besides this,
measurements such as grip strength, heart rate, weight
and speed, measured during exergaming sessions, have
been proposed as potential indexes of frailty [20]. The
use of game metrics as monitoring and assessment tools is
followed by low cost development, time saving [16] and
real time recording and users performance visualization
of [15, 16]. Finally, game metrics are considered as reliable
determinants towards exploitation for personalized game
difficulty adjustment [21, 22].
Moreover, the unobtrusive monitoring attitude of
games contributes to elimination of user anxiety which
is apparent in conventional clinical cognitive assessment,
as users do not realize the fact that they are being tested;
they are, therefore, absorbed by the game [23]. Accord-
ing to literature, assessments performed using game
metrics and algorithms could be equally or even more
effective and reliable than those performed in clinical
settings [15, 20]. Furthermore, the role of the serious
games in-game metrics in early detection of cognitive or
physical decline symptoms has recently gained the inter-
est of researchers [2426].
A point that deserves attention, however, is the fact that
the lack of experience in using new technologies can cause
high mental and emotional stress as well as cognitive over-
load. Moreover, it is difficult to correlate user interactivity
in a game with game metrics as well as with clinical out-
comes, since any two users playing the same game may in-
deed have pretty different experiences [4].
However, understanding and further exploiting game
data derived from game metrics can be based on stan-
dards and knowledge sharing, among people involved in
the design and development of exergames, as well as, its
user deployment [15].
The majority of existing games are designed for rec-
reational purposes aimed at a typical healthy person,
making it particularly challenging for elderly users [5].
Therefore, the development of exergames that target
specific populations [27, 28] such as people with de-
mentia or cognitive decline and memory or vision im-
pairments is of paramount importance [15, 29]. In
light of this, some researchers have developed user-
tailored games by combining game data with physical
exercise protocols [30] (planned, structured and repeti-
tive motion in order to maintain or improve physical
fitness and body capacity [31]) or even just physical ac-
tivity (body movement produced by the contraction of
skeletal muscles).
Following this concept, a noteworthy number of ontol-
ogies covering different domains of this type of serious
games have emerged over the last years. Ontologies de-
scribing games and gameplay achievements [32], game
development [33], multiplayer entities [34], graphics
[35], physical activity [1] and physical exercise [2] are
publicly available. However, the fact that exergames have
just started being considered as therapeutic interven-
tions [15, 36], along with the immature exploitation of
their associated performance metrics, calls for an exer-
game ontology combining all the relevant ontologies and
further developing and describing recently introduced
concepts. Needless to say that the number of papers pre-
senting game metrics as a tool of cognitive and physical
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 2 of 15
assessment in correlation with clinical assessment tests
is rather limited. The current piece of work presents the
first ontology that describes exergames and puts em-
phasis on detailed performance metrics. Built on top of
others, after a scrutiny of the exergame literature, the
proposed ontology focuses on the semantic description
of games and game sessions and combines the concepts
of game and exercise. More specifically, the ontology in-
troduces non-gaming concepts such as physical exercise,
muscles involved and necessary equipment for games.
This study suggests a unified model for concepts related
to exergames. To that extent, it introduces a well-
structured model for exergames description and open
game metrics data publishing, thereby incorporating
game-related concepts such as game components, ses-
sions and results. Consequently, it aims to facilitate
monitoring and understanding of health-related situa-
tions though the establishment of common vocabularies
and data exchange standards through exergames. The
proposed ontology was adopted and integrated in an
exergaming platform which was then piloted by fourteen
(14) elderly users who participated in a number of game
sessions. The acquired game results were automatically
converted to RDF triples and published on the web as
open data, accessible through a SPARQL Endpoint.
The remainder of this paper is structured as follows.
The background section enumerates the existing ontol-
ogies in the domain of games as well as some exergam-
ing paradigms that already highlight the value of their
in-game metrics and performance indicators. In the
methods section the development tools and processes
are presented. The results section presents details of key
concepts composing the exergame ontology. An exer-
gaming platform realizing and utilizing the proposed
ontology is demonstrated in the same section along with
information on accessing the RDF triples, derived from
the actual game sessions, through a SPARQL endpoint.
At the end of the paper light is shed on the importance
of this work towards contributing to healthcare thereby
putting emphasis on monitoring and assessing the
players status. Finally, the research limitations and chal-
lenges of this work are discussed along with further en-
visaged work.
Background
Similarly to the need for describing diseases in standardized
ways by using suitable vocabularies such as International
Classification of Diseases (ICD) [37] and Systematized
Nomenclature of Medicine (SNOMED) [38], there seems
to be an emerging need to allow standardized descriptions
of user interactions with exergames. An early effort along
these lines was the Game Ontology Project (GOP) [32],
which was one of the first attempts toward the semantic
description of games; it provides a structured model for the
game and establishes relationships among its elements.
GOP consists of five main components, namely, the user
interface, rules, goals, entities and entity manipulations. As
far as game metrics are concerned, GOP includes metrics
such as score, time and success level. The Game Content
Model (GCM) is another ontology developed for serious
games aiming at game development facilitation by non-
specialists in the context of role-playing and simulation
games [33]. GCM creators have spotted that the GOP de-
scribed game concepts from the end user perspective and
did not include notions that describe the game environ-
ment, its structure and its events. Thus, Chan and Yuen
[39], based on the GOP, developed the Digital Game Ontol-
ogy (DGO) which expressed events and concepts related to
games, such as users actions and production details, while
they used the Semantic Web Rule Language (SWRL) to
model game rules. They also highlighted the need for a
standard format for data recorded by the game since that
could contribute to the understanding of user-game inter-
action. Moreover, Mepham and Gardner [34] developed an
ontology for online multiplayer games; this ontology de-
scribed the common elements of games such as players
and game objects. Roman et al. [40] suggested an ontology
for role-playing games, including elements such as charac-
ters, story and resources. Furthermore, an ontology consist-
ing of three main components comprised of graphics,
configuration entities and multimedia has been presented
in the domain of mobile video games [35].
Semantic web technologies can transform games from
isolated standalone programs to distributed and inter-
connected systems enabling the combination of user
data with other datasets to produce useful knowledge
[34]. Moreover, the use of ontologies and SWRL rules
modelling could be deemed as an artificial intelligence
methodology [40]. Furthermore, game ontologies pro-
vide a structured vocabulary that facilitates modelling
and representation of design specifications as well as
maintenance and development of games [41]. In con-
temporary projects with large groups from all over the
world, knowledge management, which can be further fa-
cilitated by game and other semantics web technologies
[42, 43], could be vital in game development.
According to recent literature, there are no available
methods established for evaluating the data generated by
game metrics [44]. Recently, exergames were attributed
a role in unobtrusive health monitoring. However, the
evaluation of game algorithms, such as those calculating
expenditure and heart rate through game controllers or
assessing physical and cognitive status, requires re-
searchers to compare the data generated by these games
with measurements from calibrated and certified devices
(calorimeters, heart rate monitors, etc) or clinically valid
assessment tests. For instance, studies have shown that
calculation of energy expenditure through games has
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 3 of 15
moderate correlation with corresponding measurements
from external devices [16], while game metrics have mod-
erate correlation with cognitive assessment tests [32].
However, in a typical scenario nowadays, game data
are usually the very property of gaming companies
thereby resulting in inability to verify any potential clinical
outcomes of serious games. If only exergaming metrics
were proved reliable and valid, exergames could replace
external devices or even ideally account as additional
measurement methods and information providers [16]. It
is, therefore, imperative that there is a need to open this
kind of data. One could appreciate that the incorporation
of common models, specifications and data interoperabil-
ity standards through a commons approach may result in
a more effective collaboration among researchers and as-
sist in the design and development of such games as well
as knowledge sharing. Exergame commons, comprising
shared knowledge, standards and tools, along with other
initiatives such as open data commons [45] which may fa-
cilitate publishing of game metrics data, would promote
collaborative and community-driven research thereby
maximizing the return on investment [46].
Methods
Ontology editing tools and resources
The ontology was implemented using standard semantic
web technologies namely the Resource Description
Framework (RDF), RDF Schema (RDFS) and Web
Ontology Language (OWL), while some parts of the
ontology were developed using Protégé Desktop 5.0
beta ontology editor [47]. Protégé is an open-source
platform that provides tools to develop ontologies and
knowledge-based applications.
Ontology development process
During the design and development process, we
followed the steps outlined in the work of Noy &
McGuinness [48]. Firstly, the field of exergames as well
as the potentials uses of the ontology were carefully ex-
amined. The author team has been designing and devel-
oping exergames for more than seven years so far. Such
games have been tested in large-scale pilots with over
200 participants, facilitating a deep understanding of
player-game interactions [5, 36]. Extended analysis of
data has been conducted deriving different game met-
rics, but also revealing their likely correlations with clin-
ical assessment tools. These findings led them also to
suggest some exergaming design guidelines for stealth
assessment of cognitive and physical status [49]. In
addition, the authors have already investigated the con-
tribution of the exergame results to decision support
systems [50] and the use of new, accessible to seniors
technologies for the development of exergames [51]. In
order to obtain a deep understanding of all aspects and
concrete entities comprising exergames themselves, as
well as, exergame sessions, the author team has orga-
nized meetings, where multidisciplinary researchers
such as exergame developers, psychologists and physical
trainers/sport scientists, have attempted to collabora-
tively define key exergame elements to be recorded.
Then, a detailed search in literature as well as in reposi-
tories of biomedical ontologies such as BioPortal [52],
Ontobee [53] and AberOWL [54] was performed in
order to find relevant ontologies. Various relevant on-
tologies were identified both in research papers as well
as in biomedical repositories. The most suitable ones
were incorporated in the proposed exergame ontology:
i) FOAF: this ontology is used for the description of
players since it is one of the most popular ontologies for
the representation of people profiles; ii) OPE: this ontol-
ogy was found at then BioPortal repository and is used
for the representation of physical exercises. The exer-
game ontology utilizes OPE for the semantic description
of the exercises involved in games; iii) NCI Thesaurus
(NCIt) for the description of muscles involved in exer-
cises: this ontology was found at the BioPortal reposi-
tory and is used for the description of muscles that are
not included in OPE; iv) Quantity, Unit, Dimension
and Type collection of ontologies (QUDT) for defining
the units of measuring of game metrics; such metrics
are used in measuring of physiological variables like
heart rate and blood pressure, while others focus on
physical dimensions such as time, degrees, etc. To be of
utility when analyzing the performance of the person
doing the exercise, these metrics have both value and
unit of measurement. QUDT was selected for the ex-
pression of the measurements; v) vCard for information
about the site where a game session is carried out and
vi) SKOS for the development of a vocabulary of com-
mon terms for exergames. Afterwards, a list of game-
related concepts was developed and the class hierarchy
was defined. Since there are many ways to develop an
ontology and model the concepts related to a particular
domain, we have defined a straightforward class hier-
archy, aiming to provide an easy-to-use model which
can be effortlessly integrated to exergames. Subse-
quently, the properties that represent relationships be-
tween classes were identified and the property
restrictions were defined. In this step, difficulties en-
countered linked with game metrics representation, since
it was needed to model an n-array relationship. For this
reason, an auxiliary class was developed to associate game
results, metrics and their values. Moreover, in order to let
end-users have several options when it comes to property
ranges, restrictions were kept to a minimum. Finally, in-
stances of classes were developed, which facilitate the con-
nection of different data sets by referencing to same
concepts. The development of the ontology was a
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 4 of 15
repetitive process where the multidisciplinary team con-
tributed to each stage.
Availability
The developed exergame ontology is available through the
OCLC Persistent uniform resource locator (PURL) http://
purl.org/net/exergame/ns#. This Uniform Resource Iden-
tifier (URI) is dereferenceable, serving both RDF docu-
ment and ontology documentation in HTML format,
depending on the request. The documentation is gener-
ated automatically using the Live OWL Documentation
Environment (LODE) [55]. In oder to facilitate the usage
of the ontology, apart from the RDF/XML format of the
ontology, this methodology enables end-users to retrieve a
description of the resource through their browser at the
same URI, in line with repositories of biomedical ontol-
ogies such as Ontobee and BioPortal providing an inter-
face for all ontology terms.
Results
The purpose of this study is to investigate and provide a
structured description of the key concepts related to exer-
games such as components of games, game sessions and
game results, composing the exergame ontology. Since the
nature of the data produced by different games were
highly diverse and heterogeneous, special attention was
given to the design of the structure of game results to-
wards a universal model for all exergames. All classes and
properties of the exergame ontology are depicted in Fig. 1.
Konstantinidis et al. [5], proposed an architecture
scheme of exergames, by defining the fundamental
building blocks of an exergame. Aligning with their
paradigm, the exergame ontology incorporates classes
that refer to most of the layers and components of that
architecture (Fig. 2). In this study, the intention was to
cover exergames from a user monitoring point of view,
thereby focusing on game sessions, players and game re-
sults. At a later stage, further game-related components
Fig. 1 Exergame ontology overview. A graphical representation of the ontology. All classes and properties that do not have a prefix belong to
exergame ontology
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 5 of 15
would be incorporated in the ontology, such as the game
engines used for its development as well as event man-
agement details.
In the sections below, each concept of the developed
ontology is presented separately.
Game
For the description of a game as an entity we use con-
cepts from GOP and GCM including goals, game con-
trollers and presentation hardware. However, taking into
account the particularities of exergames, the ontologies
were extended to cover new concepts such as physical
exercise and auxiliary equipment (e.g. weights and fit-
ness springs). For each game, an instance of class exer-
game:Game is created and then all sessions refer to this
game by its URI.
Goals
This section includes game goals, exercise goals and
game metrics. Game goals refer to the objectives and
conditions that must be fulfilled in order for the player
to succeed in the game. They could be either high level
objectives such as victory or success at some level, or
lower level objectives such as avoiding an obstacle. As
each game incorporates a physical exercise, one of the
main objectives is the proper execution of this exercise
which is referred to as an exercise goal. Game metrics
are shared among the games and thus instances of the
class exergame:GoalMetric, that could be referenced by
all games, should be developed. A list of instances which
was developed in this study is available at http://pur-
l.org/net/exergame/metric#. Moreover, each game metric
should have a unit of measurement which is an instance
of the class exergame:Unit, an equivalent class of qud-
t:Unit. Some instances of the qudt:Unit can be found at
the QUDT Units Ontology [56].
Player
Refers to the user playing the game and it is connected
with game sessions and their results. The FOAF ontology
is used to describe each player through the foaf:Person
class.
Interface
Interface is the shared boundary between user and com-
puter and refers to how the player interacts with the game
as well as how the game is presented to the player. It con-
sists of the presentation hardware and the game control-
lers. The former represents the device on which the game
is presented (e.g. tablet, computer, smart phone, video
game console, etc.). The latter stands for the devices
which provide input to a game in order to control an ob-
ject or a character in it (e.g. keyboard, joystick, steering
wheel, etc.). The game controllers include also contem-
porary controllers which detect motion and users pos-
tures and gestures (e.g. Kinect, Wii Balance Board and
Wii Remote) as well as other recently introduced devices
which record brain activity such as NeuroSky MindWave
Fig. 2 Exergame architecture. An illustration of exergames components along with their semantic description. Dark blocks refer to concepts that
can be modeled using the exergame ontology. In each of these blocks, the classes being used to model these concepts are included inside
brackets. The remaining (light) blocks will be incorporated to the ontology in a following version.
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 6 of 15
and Emotiv. Instances of the class exergame:GameCon-
troller were developed for most modern game control-
lers and are available at http://purl.org/net/exergame/
controller#. These instances were connected through
the owl:sameAs property with the corresponding re-
sources of DBpedia, which is the core of linked data.
Due to the small size of game controllers alongside the
low rate of new emerged controllers, the links between
these instances and DBpedia were done manually, since
the use of automatic tools was not worth the effort. If
the number of instances increases dramatically, employ-
ing automatic and semi-automatic semantic linkage and
integration frameworks such as SILK [57] will be of
paramount importance.
Physical exercise
Each exercise is described using the OPE ontology in-
cluding the muscles involved, the equipment used dur-
ing the exercise as well as possible health benefits, like
improved fitness and clinical outcomes. Physical exer-
cises like biceps and leg extension are represented as
instances of the class exergame:Exercise.
Game session
Game session refers to the period in which a player is
playing a game. It begins when the game starts and ends
either with the end of the game or when for some rea-
son the game stops. The session, apart from the start
and end time, is associated with concepts like the player,
the game itself, the site and the data produced by game
metrics.
Game results
A game result includes all data derived from game metrics
in one session of a game. Each game may have more than
one metrics and more than one iterations (referring to the
corresponding physical exercise iteration). Each game ses-
sion is associated with an instance of the class exerga-
me:Result and this instance is linked with a set of game
metrics along with their values and units of measurement.
In RDF and OWL, a property is a binary relation used to
link two instances or an instance to a literal value. For this
reason, in order to define an n-array relationship [58] that
includes an instance of the class exergame:Result, in-
stances of the classes exergame:GoalMetric and exerga-
me:Unit as well as an RDF literal which represents the
metrics value, the auxiliary intermediate class exergame:-
MetricRelation is used, which represents this relationship.
In case that a game consists of one metric and iteration,
the sessions result includes an instance of exergame:Me-
tricRelation which is linked to a metric, a unit of measure-
ment and a value. In the event of more iterations, the
property exergame:metricValue links to a sequence of
metric values using rdf:Seq.
Site
The site refers to the venue where a game is conducted in-
cluding the address and the type of it (e.g. Ecologically
valid Active and Healthy Ageing Living Lab area - Medical
Physics Laboratory of the Aristotle University of Thessalo-
niki). Venue details may include street address, email ad-
dress, telephone numbers, coordinates etc. For these
details, many well-known ontologies could be used such
as vCard, but the range of the property exergame:siteAd-
dress was left open thereby allowing for any other form.
For example, a vCard instance can be used as its value
and besides address it can provide site coordinates in
the form of geographical coordinates (geo URI) using
vCard:hasgeo property. As far as site category is con-
cerned, instances of the class exergame:SiteType were
developed, which represent specific categories such as
home, research laboratory and senior center (http://pur-
l.org/net/exergame/sitetype#).
Vocabulary for common concepts
To date, there is no common vocabulary for terms re-
lated to exergames that uses semantic web technologies.
Therefore, in order to support the reuse of exergame
concepts, the exergame ontology incorporates the exer-
game:concept property which links instances to the con-
cept they represent. Moreover, a vocabulary of such
terms was developed using the SKOS ontology; this is
now available at http://purl.org/net/exergame/concept#.
For example, in the case that an exergame includes a
time-related metric such as reaction time, the instance
that represents this metric could connect with the con-
cept of time, which is an instance of skos:Concept. This
vocabulary is intended to be scalable and form an inte-
gral part of exergame commons, since it can be viewed
as a more general dictionary, just to cover for those re-
searchers for which the already defined concepts do not
fit their own needs. As an illustration, when an instance
of a class (e.g. an instance that represents a game metric
which calculates the distance covered by the player) con-
cerns a concrete concept which is not included in the
aforementioned vocabulary, it can be extended by devel-
oping an instance of skos:Concept. Therefore, the exist-
ence of a rich vocabulary is on the exergame
communitys best interest since it facilitates the linkage
of two distinct instances that refer to a common con-
cept. As a result, game results derived from distinct
exergames can be analyzed together or compared to
each other, based on their common characteristics.
Applications
This study provides opportunities to capitalize on open
exergame data for active and healthy ageing. Publishing
game results as open data will enable the exergame com-
munity to test these data, validate their algorithms,
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 7 of 15
combine game results with other datasets and produce
new knowledge. The task of adapting the ontology to
an exergame is pretty simple, regardless the technology
used; In fact, each game should be semantically de-
scribed only once. Regarding game sessions, all infor-
mation, including game results, can effortlessly be
semantically enriched and stored to a triplestore as
well. On the other hand, in most games the results are
stored in a database. Hence, already stored game results
can be converted to RDF utilizing a language for ex-
pressing mappings from relational databases to RDF
datasets, such as R2RML [59]. As far as privacy is con-
cerned, any information about patients that might re-
veal their identity should not be stored. For this reason,
a plain unique identifier that corresponds to a particu-
lar patient may be attached to game sessions data.
Integration in HTML5 exergames
Exergame ontology was integrated in HTML5 games on
webFitForAll (wFFA) web platform (Konstantinidis EI,
Bamparopoulos G, Bamidis PD: Moving Real Exergaming
Engines on the Web: The webFitForAll case study in an ac-
tive and healthy ageing living lab environment, submitted.).
wFFA is an exergaming platform developed to support e-
health applications incorporating various game controllers
such as Wii Balanceboard, Wii Remote, Microsoft Kinect
and NeuroSky MindWave through the transparent Con-
troller Application Communication (CAC) Framework
[60]. wFFA contributes to fitness maintenance and well-
being of elderly people through specifically designed
games/interfaces with simple graphics, which belong to
three categories. In the first category, there are games
that incorporate physical exercises such as stretching,
resistance and weight lifting, aiming at power increase
of the upper and lower limbs [5]. These games com-
prise a predetermined number of iterations and include
metrics such as success, reaction time and goal time.
Moreover, there are games such as Apple, Hiking,
Fishing and Golf which are related to physical activ-
ities in conjunction with light cognitive tasks, such as
walking, balance and reaction. The metrics of these
games are total time and players score, which is calcu-
lated differently for each game (number of apples in the
basket, number of steps). Finally, the last category
includes interfaces which facilitate the health measure-
ments collection, including but not limited to weight
and blood pressure. These measures are treated in the
same way as game metrics.
Integration architecture
The wFFA is based on a JavaScript library which is re-
sponsible for several tasks such as coordination of games
and exercise protocols and consists of three main com-
ponents: the initializing component, the processing
component and the output component. For the purposes
of this study, specific extensions were developed for each
component in order to support the semantic description
of players, games and sessions as well as to enable
games data storage to a triplestore. One of the main
tasks of the initializing component is the game control-
lers management and the exercises difficulty adjustment.
In this study, an extension was developed that initializes
game metrics in order to record players behavior.
The processing component accounts for the proper
function of the game and consists of a set of subcompo-
nents, being executed during a game session, including
common game tasks such as feedback control. We ex-
tended this component in order to enable data collection
from game metrics resulting in a detailed record of users
actions. Furthermore a preprocessing component was de-
veloped to gather all information about the game session
by utilizing the users unique identifier to retrieve the URI
of the instance of class exergame:Player that represents
them, which was stored automatically when the users ac-
count was created. Any information that could potentially
disclose the user identity is omitted. As far as the site is
concerned, an HTML form was developed extending the
users settings to include information about the site (a set
of predefined site categories are provided as options).
Once this form is completed, site details are added to the
session. Finally, the start date and time are added to the
session, which is almost ready to be saved.
The output component is activated at the end of each
game and is responsible for storing the result of a game
session in a relational MySQL database (through a Rest
API). In this study, a subcomponent was developed that
processes the data from game metrics, describes them se-
mantically according to the ontology and adds them in the
game session along with the end date and time. As long as
all the information about the game session is converted to
RDF triples, it is uploaded in a Sesame triplestore using
Ajax. All triples concerning games, exercises, game sessions
and patients are stored in a single graph in the triplestore.
Description of game SideRaises
In the SideRaises game, players start by holding hand
weights straight down at their sides. Then they have to
raise both arms to the side at shoulder height and after
they hold this position for a few seconds, they lower
their hands and continue with the next iteration. De-
pending on their physical condition, users can use differ-
ent weights. The game consists of the following metrics:
success, reaction time and goal time and it utilizes a
Kinect as a game controller and a smart TV as a presen-
tation hardware. The corresponding exercise involves
the deltoid, the infraspinatus, teres minor and major and
the latissimus dorsi muscles. For this example, the fol-
lowing prefixes are used:
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 8 of 15
@prefix : <http://www.fitforall.gr/resources#>.
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-
syntax-ns#>.
@prefix rdfs: <http://www.w3.org/2000/01/rdf-
schema#>.
@prefix exergame: <http://purl.com/net/exergame/ns#.
@prefix exergame-metric: <http://purl.org/net/exergame/
metric#>.
@prefix exergame-gamecontroller: <http://purl.org/net/
exergame/controller#http://purl.org/net/exergame/
controller#>.
@prefix ope: <http://www.semanticweb.org/ontologies/
2013/2/OPE.owl#>.
@prefix nci: <http://ncicb.nci.nih.gov/xml/owl/EVS/
Thesaurus.owl#>.
The exercise and muscles involved are described first.
In the case that a muscle is not involved in OPE, it is de-
fined as a subclass of nci:Muscle.
Thereafter, the :SideraisesGame is defined alongside its
metrics, game controller and presentation hardware:
Piloting with users in a living lab
A number of elderly volunteers were visiting a specially
designed environment in the laboratory of Medical Physics
at Aristotle University of Thessaloniki [60] daily and were
playing a set of exergames. This environment, facilitating
also the trials of USEFIL project [6165], had two sepa-
rated areas, a furnished one with sofa, table and chairs to
resemble a small living room and an area with a sink that
resembles a bathroom. This configuration was designed to
create an ecologically valid experimentation environment.
Participants
Fourteen elderly people (3 males and 11 females, average
age 73.4 years old) who had no previous experience with
technology participated in this study. On average, each
elderly person visited the specially designed environment
for 6 days in total and played/interacted with 7 different
games each time. The total time of all game sessions for
all users was about 35 h.
Game data
The game metrics data which was produced during
sessions of the aforementioned games were converted
to RDF by the game framework and were uploaded on
a triplestore as open data. In order to save the results
of game sessions, there are about 30,000 records in
the triplestore corresponding to 35 h of gaming. All
data are accessible from a SPARQL endpoint that is
available at http://www.fitforall.gr/sparql, where quer-
ies can be made using the GET or POST method. In
Table 1, data from a game session of SideRaises
game are depicted. For instance, the SPARQL query
for getting all values from the metric that measures
goal time (exergame-metric:GoalTime) during the 3rd
iteration of SideRaises game, along with the corre-
sponding player and date of game session, is the fol-
lowing:
Discussion
In this study, an ontology for exergames was developed by
fusing concepts from gaming and physical exercise into a
single structure. As existing gaming ontologies target
mainly at the game design and development including con-
cepts such as game scenario and game objects [32, 33, 39],
they do not provide a comprehensive model that describes
the data produced in a game session. As a consequence, a
standard structure that facilitates publishing of game data
on the web was developed. This was achieved by modelling
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 9 of 15
the concept of the game session, which in combination
with the game, the player and the game results, formed a
universal model for the semantic description of the playing
process. This structure could then be effortlessly integrated
into other existing ontologies.
Another neat contribution of this work though, is that
of opening the game data emerging from the aforemen-
tioned game sessions. For the first time, such data were
described semantically in a machine-readable format
using W3C standards and some instances are linked to
related resources from DBpedia. Complying with the
highest level in the 5-star open data scheme introduced
by Tim Berners-Lee [66], this dataset is, to the best of
the authors knowledge, the worlds first available open
dataset of exergames metrics described semantically by
means of Linked Open Data (LOD). Furthermore, open-
ing game data in an RDF format is in researchers, care-
givers and patients best interests, as game analytics and
data visualization techniques can be subsequently ap-
plied, enabling them to monitor patients performance
and retrieve visual signs of early detection and deterior-
ation. Further exploitation of this dataset, along with
other semantically annotated monitoring sources of the
project (gait analysis, cognitive assessment tests, etc.),
will open new avenues in Active and Healthy Ageing
[67] as well as health monitoring per se as it will provide
new insights of the exergames role as assessment tools.
This is in line with the emerging trend of knowledge ex-
traction from multimodal sources, towards knowledge
understanding by combining information with different
granularity ranging from muscles and exercices to cogni-
tive functionality and health status [68].
Contribution to healthcare
This work aims at semantically enriching exergaming
data, thereby enabling semantic processing in the field
of exergames. Semantic description of game results
alongside open access enables the biomedical semantics
community to develop automatic processing, analysis
and visualization tools for these data. Such tools are
much meaningful in the biomedical domain since fur-
ther research and study of the produced information will
hopefully contribute to a better understanding of any pa-
tients health state and condition. Abiding to the holistic
definition of health by WHO [69] as well as current
ehealth emphasis on personalized, citizen driven elec-
tronic health record systems (EHRs) [70, 71], it would
be reasonable to envision expansion and exploitation of
herein presented achievements within future EHR sys-
tems. This may be merely attempted by incorporating or
even better semantically linking a persons EHR with
exergaming behavior and performance data. The advan-
tages of such an EHR approach promoting joint exploit-
ation of heterogeneous information sources [72], would
Table 1 RDF triples produced during a session of SideRaises game
Subject Predicate Object
a_:SideRaisesGameSession rdf:type exergame:GameSession
_:SideRaisesGameSession exergame:result _: SideRaisesGameResult
_:SideRaisesGameSession exergame:site <http://www.fitforall.gr/resources/MedicalPhysicsLaboratoryAuth>
_:SideRaisesGameSession exergame:startDateTime 2014-09-22T18:09:39 02:00
_:SideRaisesGameSession exergame:endDateTime 2014-09-22T18:09:39 02:00
_:SideRaisesGameSession exergame:player <http://www.fitforall.gr/resources/player162>
_:SideRaisesGameSession exergame:game <http://www.fitforall.gr/resources/SideRaisesGame>
_:SideRaisesGameResult rdf:type exergame:Result
_:SideRaisesGameResult exergame:metricRelation _:MetricRelation1
_:SideRaisesGameResult exergame:metricRelation _:MetricRelation2
_:SideRaisesGameResult exergame:metricRelation _:MetricRelation3
_:MetricRelation1 exergame:metricName <http://purl.com/net/exergame/metric# Success>
_:MetricRelation1 exergame:metricValue _:node197jv5qonx686
_:node197jv5qonx686 rdf:type rdf:Seq
_:node197jv5qonx686 rdf:_1 TRUE
_:node197jv5qonx686 rdf:_2 TRUE
_:node197jv5qonx686 rdf:_3 FALSE
_:node197jv5qonx686 rdf:_4 TRUE
_:node197jv5qonx686 rdf:_5 FALSE
_:node197jv5qonx686 rdf:_6 TRUE
aFor brevity, _:genid-6569e0b8e8774a22b73a515a98c0f963- is abbreviated as _:
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 10 of 15
be twofold. First, the EHR information would give a bet-
ter understanding of the exergaming information and its
value in the context of the general health. Secondly, the
EHR information, decoded to exergaming aspects, will
lead to automatic, design user-tailored exercise proto-
cols, taking into account any possible conditions and
limitations.
The need for evaluation of exergames
Several studies highlight the importance of exergames as
monitoring and evaluation tools which is achieved
through the collection and analysis of data produced by
their metrics [9, 10, 16] and the gaming environment
[73]. However, the absence of large volumes of research
that evaluates these data hinders the full integration of
exergames in healthcare systems [44]. This work en-
hances the prospects for further research on evaluating
exergames and opens up new ways for open linked game
data, thereby facilitating their analysis, verification and
understanding by the scientific community.
The need for comparison of games metrics and algorithms
to external monitoring devices
As mentioned previously in this document, some serious
games integrate interfaces performing various measure-
ments through game controllers such as energy expend-
iture. Hence, there is a need to compare their results
with data from corresponding measurements from exter-
nal devices as well as self-monitoring devices in terms of
quantified self. To this end, several studies indicate that
these measurements should be published as open data
enabling researchers to compare them and verify their
reliability [16]. In the exergame ontology, these measure-
ments are treated as they were data produced by game
metrics and described using the same format, laying the
basis for publication of such data.
Real-time data analysis
In the context of collecting data in home environments,
there is an increasing interest in automated analysis. Data
produced in such environments can be analyzed in real
time, reducing the costs of health services [74]. Health
professionals can provide advice to patients based on deci-
sion support systems that consume these data automatic-
ally, thereby facilitating personalized healthcare [75, 76].
Decentralized information
Moreover, the need to store data in a central database is
reduced since patients can play games at their homes
which results in the development of a decentralized
knowledge base. For instance, information concerning
games, exercises and game results could be stored in dif-
ferent locations and recovered through queries over
multiple SPARQL Endpoints. Linking patient data from
multiple sources may result in a better understanding of
health-related problems and even in acquisition of new
knowledge. The latter can only be achieved though,
under the condition that the data will be open. By open-
ing access to medical databases we can save valuable
time facilitating early detection of diseases [77]. For in-
stance, datasets from open public health initiative [78],
in which federal health agencies publish health-related
data along with population data can be linked with data
produced by exergames. Moreover, they can be com-
bined with or even extend patient personal health re-
cords derived from well-known personal health record
systems such as Microsoft HealthVault, Dossia and
World Medical Card [79].
Unobtrusive monitoring
Nowadays, a noteworthy number of research efforts is car-
ried out on early detection of cognitive decline utilizing
sensors at home [8082], paying extra attention in pre-
serving a sense of unobtrusiveness. Serious games have
just been considered as potential unobtrusive monitoring
sources [30]. Therefore, ontologies like the one proposed
in this paper will further facilitate the evolution of the ser-
ious games into considerable sources of information.
Challenges
Big data
The advent of embedding processors to everyday de-
vices, converting them to smart objects, opens the road
towards ubiquitous computing [83]. Quantified-self, in-
cluding data about diet, mood, physical activity, sleep
quality etc., is leveraged from such technological ad-
vances, as they facilitate acquisition of daily living data.
Smart sensors, self-reporting via mobile applications and
other devices are employed to the quantified-self
realization [84], resulting in large amounts of data. Big
data has been receiving an increased attention in bio-
medical and healthcare applications, paving research
from hypothesis-driven to data-driven. Therefore, there
is an increasing demand in new analysis techniques de-
velopment, following the size, variety and complexity of
these data hindering information retrieval [85, 86]. Big
data are either formed as structured data, such as those
produced by games, or unstructured information. The
unstructured data constitute more than 80 percent of
the available data [86]. Considering the ever increasing
use of serious games, the data produced is expected to
scale exponentially. Taking into account the rapid in-
crease in the volume of these data, standards for the ex-
change and linkage should be developed by health
professionals, so as to facilitate data interoperability and
integration on different platforms [75]. Furthermore,
special attention should be paid to the storage and
distribution limitations as well as the development of
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 11 of 15
techniques, tools and infrastructures for data processing
and analysis [74].
Privacy
Despite the large number of positive statements about the
need for transparency on information about clinical trials
over many years [87, 88], as far as privacy is concerned, dis-
closing users personal data should be carefully handled
[15]. In alignment with the patient commons, it must be
ensured that any information system used in storage would
protect them from unauthorized access, since such data are
sensitive while ownership issues are not always clear [74].
In the realm of data openness, special attention should be
paid when publishing game results on the web to ensure
the protection of privacy [15]. In this study, information
about game sessions was published obscuring any in-
formation that may reveal whose data are included in
each session. Additionally, the protocol followed by the
pilots of this work has been approved by the Bioethics
Committee of the Medical School of the Aristotle
University of Thessaloniki (No.93/26-6-2014). However,
continuous identification of users to different sources is a
risk that increases with the volume of data [89]. In
addition, apart from the raw data, origin, type and acquisi-
tion method should be part of the metadata schema. On
the other hand, sharing data that is incomplete and not
described properly may have an adverse effect, thereby
leading to incorrect assumptions [90].
Weaknesses and limitations
Although the power of data openness and sharing is ac-
knowledged, there are some concerns in the scientific
community about publishing data on the web. More spe-
cifically, many researchers are reluctant to open their
data as they are afraid that others will not follow the
same approach or that they could not be able to control
data access [90]. In recent years, there have been efforts
contributing to this direction by reinforcing data access
and exchange policies. In 2010, U.S. Presidents Council
of Advisors on Science and Technology (PCAST) issued
a report [91] about the potential of health information
technology to improve healthcare, underlining the need
for the adaptation of a universal exchange language for
healthcare information. Other studies state that RDF can
be served as a Universal Healthcare Exchange Language,
since it is format independent and all existing vocabular-
ies such as Health Level 7 (HL7) and Clinical Document
Architecture (CDA) can be mapped to RDF [92].
With regard to the exergame ontology, a limitation is
that this study focused mainly on the high-level concepts
description such as game controllers, goals, game metrics
and presentation hardware, while existing ontologies offer
a more detailed description of a game itself (e.g. game sce-
nario and game entities). Moreover, the ontology includes
only structured exercises, which is going to be addressed
in the authors future work by incorporating physical ac-
tivity as well. Furthermore, possible weaknesses of users
such as vision and movement problems as well as mental
disorders are not part of the players description. For in-
stance, the results produced during a game session may
vary between a healthy user and a user with mobility prob-
lems. The authors intend to study corresponding ontol-
ogies and integrate them in the exergame ontology.
Finally, this work included a small number of real pa-
tients and exercises focusing on establishing the basis
correctly and studying the feasibility of the employed in-
frastructure. Performance and scalability evaluation with
a large number of seniors, participating in a complete
protocol (in terms of exercises) will take place within a
newly funded research project (www.uncap.eu) where
this infrastructure will be further exploited.
Future work
Linking game results with other datasets
The next decade will be the beginning of integrated gam-
ing, when game data and social network data will be
linked with other datasets [4] such as electronic health re-
cords [44]. An attempt to this direction was made by
Aetna, a US insurance company that developed an open
platform facilitating data integration from various sources
such as medical records [84]. Game data can be combined
with data produced by smart phones in the context of util-
izing sensors such as accelerometers and GPS towards
producing large amounts of location specific data con-
cerning physical activity and physical exercise [86]. Apart
from fitness-related data, information from other sources
can also be combined with games such as data about diet,
sleep quality and allergies. The need for such data integra-
tion has led to the emergence of Data as a Service (DaaS)
platforms such as linked life data that provides access to
data from various biomedical databases through a single
SPARQL Endpoint [93]. With the aid of these technolo-
gies, anonymous data can be effortlessly extracted and uti-
lized by pharmaceutical and insurance companies as well
as agencies on health related information to improve drug
design and health protocols altogether. Towards this dir-
ection, we aim to semantically describe all exergames that
are part of the web FitForall platform. Additionally and
apart from the instances developed in this work represent-
ing contemporary game controllers that are already linked
to DBpedia, other concrete entities will be linked to exist-
ing datasets as well. More specifically, game metrics, exer-
cises involved in games as well as ailments for which
exercises might be employed as treatments or preventative
measures will be linked to DBpedia and relevant datasets
which utilize biomedical ontologies such as SNOMED
and ICD. Moreover, when the amount of RDF triples
representing game sessions and results grows, the number
Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 12 of 15
of links with other LOD datasets will be increased as well
and as a result, exergame-related datasets could then be-
come part of the LOD cloud.
Game analytics and semantic processing tools
Apart from linking game results with other datasets, there
is an increasing interest in game analytics in the field of
game research. Players today have a wide range of ages,
backgrounds, intellectual abilities and motivations. The
need for understanding players behavior and experience
during the game has led to integration of game analytics
into design and development process [94]. Data from
game metrics and players behavior are collected and ana-
lyzed throughout the game, revealing valuable information
that range from details about the players profile to design-
related information such as software bugs and design
problems [95]. Game analytics constitute an important
field of business intelligence for the game industry, while
the combination of game data with other information sets
such as market reports and quality assurance systems facil-
itates knowledge management, marketing and decision
making. The semantic description of games, facilitates
searching and retrieval of their data and enables the devel-
opment of knowledge extraction and data mining tools
through automatic processes and semantic web crawlers,
opening the way for serious game and exergame analytics.
Personalized exercise protocols
Today, the rapidly increasing gaming population, regard-
less of age and background, has led to the maximization
of the need for customized games [27]. In the context of
physical activity and exercise, recommendation systems
for exergames may be developed, utilizing automated
reasoning. In this manner, personalized exercise proto-
cols can be designed, taking into account several factors
such as game sessions data, biological factors as well as
possible disabilities.
Conclusions
In this work, a unified model for the semantic representa-
tion of exergames is proposed, while actual data from game
sessions are published as open data on the web. Aligning
with a commons approach, this study aims to be the outset
of an exergame commons initiative, which will establish
shared standards across the researchers and facilitate exer-
game research. Last but not least, this exergame commons
and open exergame data initiative is one of the very first ef-
forts to enable the concept of Open Trials (an initiative that
will aggregate information from a wide variety of existing
sources in order to provide a comprehensive picture of the
data and documents related to all trials of medicines and
other treatments around the world) for health monitoring,
in general, as well as, Active and Healthy Ageing more
specifically.
Abbreviations
CAC: Controller application communication; CDA: Clinical document
architecture; DGO: Digital game ontology; EHR: Electronic health record;
FOAF: Friend of a friend; GCM: Game content model; GOP: Game ontology
project; HL7: Health level 7; ICD: International classification of diseases;
LOD: Linked open data; LODE: Live OWL documentation environment;
MMSE: Mini-mental state examination; OPA: Ontology of physical activity;
OPE: Ontology of physical exercise; OWL: Web ontology language;
PCAST: Presidents Council of Advisors on Science and Technology;
PURL: Persistent uniform; resource locator; RDF: Resource description framework;
RDFS: Resource description framework schema; SNOMED: Systematized
nomenclature of medicine; SWRL: Semantic web rule language; URI: Uniform
resource identifier; wFFA: Web fitforall.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
GB contributed to the ontology conceptualization, design, development
and integration to HTML5 games, was involved in the data collection as
well as the literature review and had primary responsibility for writing the
manuscript. EK contributed to the ontology conceptualization and design,
was involved in the data collection, literature review and writing the
manuscript. CB contributed to the ontology conceptualization. PDB
contributed to the ontology conceptualization, directed the design as well
as the literature review and had primary responsibility for writing and
reviewing the manuscript. He is the guarantor of this manuscript. All authors
read and approved the final manuscript.
Author details
1Medical Physics Laboratory, Medical School, Faculty of Health Sciences,
Aristotle University of Thessaloniki, Thessaloniki, Greece. 2Mathematics
Department, Aristotle University of Thessaloniki, Thessaloniki, Greece.
Received: 2 July 2015 Accepted: 25 January 2016
RESEARCH Open Access
Normalizing acronyms and abbreviations to
aid patient understanding of clinical texts:
ShARe/CLEF eHealth Challenge 2013, Task 2
Danielle L. Mowery1*, Brett R. South1, Lee Christensen1, Jianwei Leng1, Laura-Maria Peltonen2, Sanna Salanterä2,
Hanna Suominen3, David Martinez4,5, Sumithra Velupillai6, Noémie Elhadad7, Guergana Savova8,
Sameer Pradhan8 and Wendy W. Chapman1
Abstract
Background: The ShARe/CLEF eHealth challenge lab aims to stimulate development of natural language
processing and information retrieval technologies to aid patients in understanding their clinical reports. In clinical
text, acronyms and abbreviations, also referenced as short forms, can be difficult for patients to understand. For one
of three shared tasks in 2013 (Task 2), we generated a reference standard of clinical short forms normalized to the
Unified Medical Language System. This reference standard can be used to improve patient understanding by
linking to web sources with lay descriptions of annotated short forms or by substituting short forms with a more
simplified, lay term.
Methods: In this study, we evaluate 1) accuracy of participating systems normalizing short forms compared to a
majority sense baseline approach, 2) performance of participants systems for short forms with variable majority
sense distributions, and 3) report the accuracy of participating systems normalizing shared normalized concepts
between the test set and the Consumer Health Vocabulary, a vocabulary of lay medical terms.
Results: The best systems submitted by the five participating teams performed with accuracies ranging from 43 to
72 %. A majority sense baseline approach achieved the second best performance. The performance of participating
systems for normalizing short forms with two or more senses with low ambiguity (majority sense greater than
80 %) ranged from 52 to 78 % accuracy, with two or more senses with moderate ambiguity (majority sense
between 50 and 80 %) ranged from 23 to 57 % accuracy, and with two or more senses with high ambiguity
(majority sense less than 50 %) ranged from 2 to 45 % accuracy. With respect to the ShARe test set, 69 % of short
form annotations contained common concept unique identifiers with the Consumer Health Vocabulary. For these
2594 possible annotations, the performance of participating systems ranged from 50 to 75 % accuracy.
Conclusion: Short form normalization continues to be a challenging problem. Short form normalization systems
perform with moderate to reasonable accuracies. The Consumer Health Vocabulary could enrich its knowledge base
with missed concept unique identifiers from the ShARe test set to further support patient understanding of
unfamiliar medical terms.
Keywords: Natural language processing, Acronyms, Abbreviations, Consumer health information, Unified Medical
Language System
* Correspondence: danielle.mowery@utah.edu
1Department of Biomedical Informatics, University of Utah, Salt Lake City, UT,
USA
Full list of author information is available at the end of the article
© 2016 Mowery et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 
DOI 10.1186/s13326-016-0084-y
Background
International healthcare policies aim to improve patients
access to their clinical record and involvement in their
healthcare delivery, e.g. in the United States [1], in
Australia [2], and in Finland [2]. These policies have mo-
tivated healthcare organizations to adopt patient-
centered approaches e.g., the United States Open Notes
project [3], some resulting in modest benefits and min-
imal risks [48].
Patient access to easy-to-understand, simple text in
clinical reports is also stipulated in several countries by
law. For instance, regulations in the United States [9]
and European Union [10] state that patients should have
access to their clinical information upon request [11].
United Kingdom guidelines describe best practices for
patient access [12]. Laws and statutes in Sweden [13]
and Finland [14] state that clinical notes must be explicit
and comprehensive, including only well known, accepted
concepts and abbreviations.
Automated tools for text simplification can help clini-
cians comply with regulations and improve information
readability for patients. For instance, statistical approaches
can identify, reduce, and disambiguate unfamiliar con-
cepts. Specifically, unsupervised methods and statistical
associations can automatically learn unfamiliar terms,
identify potential semantic equivalents, and present lay
terms or definitions [1517]. Text simplification architec-
tures can analyze, transform, and regenerate sentences for
patients e.g., simplifying Wall Street Journal sentences for
Aphasia patients [18]. In the biomedical domain, one text
simplification tool reduces the semantic complexity of
sentences conveying health content in biomedical articles
by substituting unfamiliar medical concepts with syno-
nyms or related terms, and the syntactic complexity by
dividing longer sentences into shorter constructions [19].
In the clinical domain, a prototype translator reduces
the semantic complexity of clinical texts by replacing
abbreviations and other terms with consumer-friendly
terms from the Consumer Health Vocabulary and ex-
planatory phrases [20].
Making annotated corpora available to the natural lan-
guage processing community through shared tasks can fur-
ther stimulate development of technologies in this research
area [21]. Like the Message Understanding Conference
(MUC) [22], Text REtrieval Conference (TREC) [23, 24],
Genome Information Acquisition (GENIA) [25, 26], and
Informatics for Integrating Biology and the Bedside (i2b2)
challenges [2731], the 2013 Shared Annotated Resources/
Conference and Labs of the Evaluation Forum (ShARe/
CLEF) eHealth Challenge evaluated participant natural
language processing systems against a manually-generated
reference standard [32]. The 2013 ShARe/CLEF eHealth
Challenge took initial steps toward facilitating patient
understanding of clinical reports by identifying and
normalizing mentions of diseases and disorders to a
standardized vocabulary (Task 1) [33], by normalizing
acronyms and abbreviations (Task 2) [34], and by re-
trieving documents from health and medicine websites
for addressing patient-centric questions about diseases
and disorders documented in clinical notes (Task 3)
[35]. This paper describes studies related to Task 2.
We review acronym and abbreviation recognition in
the context of text normalization. We are motivated by
the need for creating an annotated corpus of acronyms
and abbreviations to encourage the development of nat-
ural language processing tools that improve patient un-
derstanding and readability of clinical texts.
Text processing for acronyms and abbreviations
Conceptually disambiguating the meaning of a word or
phrase from clinical text often involves mapping to a
standardized vocabulary [36]. For example, natural lan-
guage processing tools that normalize words and phrases
to Unified Medical Language System (UMLS) concept
unique identifiers (CUIs) include IndexFinder [37],
KnowledgeMap [38], MetaMap [39], Medical Language
Extraction and Encoding System (MedLEE) [40] and
clinical Text Analysis and Knowledge Extraction System
(cTAKES) [41]. Acronyms and abbreviations are short-
ened words used to represent one or more concepts
[42]. Acronyms are formed from the first letters of
words in a meaningful phrase (BP = Blood Pressure) and
can be pronounced as words (CABG = Coronary Artery
Bypass Graft, pronounced cabbage) or letter-by-letter
(TIA = Transient Ischemic Attack, pronounced T-I-A).
Abbreviations are shortened derivations of a word or
phrase (myocard infarc =myocardial infarction) and are
generally pronounced as their expanded forms (myocard
infarc, pronounced myocardial infarction). We will refer
to acronyms and abbreviations throughout the manu-
script as short forms for brevity and to convey a mixture
of both acronyms and abbreviations, including their lex-
ical variants from the clinical text.
Accurate short form detection methods may handle
various linguistic characteristics and phenomena associ-
ated with short form usage in text. Short forms are doc-
umented using different orthographic constructions
including varied letter case (CAD vs cad). Punctuation
can be applied to acronyms to represent one concept
(b.i.d. means twice a day) or list many related concepts
(m/r/g represents three heart sound concepts - mur-
murs, rubs, gallops). Syntactically, short forms may be
conveyed using both singular and plural forms (TIA/
TIAs) as well as possessives (Pts). Syntactically, a short
form can conceptually represent different long forms of
the same concept and semantically, short forms may be
polysemous, having different, but related word senses
(LV can stand for an adjectival phrase like left
Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 2 of 13
ventricular or a noun phrase left ventricle). Short forms
may be homonymous, having different, unrelated word
senses within and across report genres (LV can stand
for both left ventricle and low volume in an echocardio-
gram report, but would more likely stand for lumbar
vertebrae in a radiology report). In fact, short form am-
biguity can lead to unintended medical errors; therefore,
many short form are banned from clinical document
usage by the Joint Commission, in Do not use List: Ab-
breviations, Acronyms, and Symbols [43]. Short forms
can occur with misspellings (myocrad infrac should be
spelled myocard infarc) and can be concatenated with
other words (70 year oldM = 70 year old M). We de-
veloped an annotation schema and guidelines for human
annotators that addressed the annotation of such exam-
ples from clinical text (described under Methods
Annotation Schema).
Text normalization
In general, text normalization, which may include short
form (mention-level) boundary detection, word sense
disambiguation, and named entity and event recognition,
can be an important processing step for some clinical in-
formation extraction approaches. For example, in order
to extract a disease and disorder mention and link it to
information to help a patients understanding of the un-
familiar medical concept e.g., such as Abdominal tender-
ness from ABD: Tender, a natural language processing
system would need to 1) detect ABD as a short form, 2)
disambiguate ABD as Abdomen not Aged, Blind and
Disabled, 3) normalize ABD to a concept in a controlled
vocabulary (e.g., C0562238: Examination of the Abdo-
men), 4) post-coordinate ABD with the adjacent finding
tenderness (e.g., C0234233: Tenderness) to define an
event (e.g. C0232498: Abdominal tenderness), and finally
5) link it to a web-based information resource like Medline
Plus. For the purposes of our assessment, we have focused
on short form disambiguation (2) and normalization (3).
Acronym and abbreviation detection and normalization
Early and ongoing work on aspects of short form detection
and normalization focused on developing resources in the
biomedical literature domain, in particular, MEDLINE ab-
stracts. A common and reasonable, baseline approach to
detecting and normalizing biomedical short forms is
exploiting short form  long form patterns [4446]. This
method is advantageous because most short forms demon-
strate no or low ambiguity and can be mapped to the most
frequent sense usage. Furthermore, few short forms dem-
onstrate moderate to high ambiguity due to polysemous
and/or homonymous usage.
Researchers have also developed more sophisticated,
high performing biomedical short form disambiguation
modules by training supervised models and evaluating
against MEDLINE corpora, e.g., Medstract Gold Stand-
ard Evaluation corpus (support vector machines:
98 % F1-measure [47], logistic regression: 81 % F1-
measure [48]) and semi-supervised (AbbRE): 91 % F1-
measure [49]). Further resources  databases and
tools  for disambiguating biomedical short forms in-
clude Acronym Resolving General Heuristic (ARGH),
Stanford Biomedical Abbreviation Database, AcroMed,
and Simple and Robust Abbreviation Dictionary (SaRAD)
[48, 50]. However, few resources exist for short form rec-
ognition from clinical texts.
Indeed, a comparison study of state-of-the-art clinical
text normalization tools suggests that clinical short
forms detection and normalization is still in its early
stages [51]. This study determines that clinical short
forms normalization tools generally demonstrate low to
moderate performance - clinical Text Analysis and
Knowledge Extraction System (F1-measure: 21 %), Meta-
Map (F1-measure: 35 %), and Medical Language Extrac-
tion and Encoding System (F1-measure: 71 %) [51].
Natural language processing systems can perform with
low normalization scores due to multiple senses for a
short form. The study suggests that the reason that the
Medical Language Extraction and Encoding System out-
performs MetaMap and clinical Text Analysis and
Knowledge Extraction System for disambiguating am-
biguous short form is due to its highly integrated clinical
sense inventories [51]. Natural language processing re-
searchers have successfully produced sense inventories
and automated disambiguation modules using rule-
based and machine learning-based approaches [52, 53].
For instance, a short form sense inventory was generated
using regular expressions and morphological heuristics
from 352,267 clinical notes and the most frequent short
forms were manually mapped to three vocabularies 
Stedmans Medical Abbreviations, Acronyms & Symbols,
the Unified Medical Language System, and Another
Database of Abbreviations in Medline (ADAM) [52].
Such sense inventories were developed using features
generated from the Internet, Medline, and Mayo clinical
notes to train decision tree and maximum entropy clas-
sifiers for eight short forms [53]. Both decision tree
(94 %) and maximum entropy (96 %) classifiers demon-
strated more accurate short form classification than a
majority sense baseline (71 %). Disambiguation modules
focus on ambiguous word-senses of clinical short forms
[54]. One disambiguation module uses a support vector
machine trained with 125 samples that achieved high ac-
curacy (over 90 %) for the 50 most frequent short forms
with varied senses from a dataset of 604,944 clinical
notes. In addition to support vector machines, decision
trees and naïve bayes classifiers are able to disambiguate
short forms with high accuracy (exceeding 90 %) using
part-of-speech, unigram, and bigram features [55]. Semi-
Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 3 of 13
supervised (Specialist Lexicon Release of Abbreviations
and Acronyms (LRABR) with multi-class support vector
machine) and unsupervised (hierarchical clustering) ap-
proaches have also demonstrated moderate to excellent
disambiguation performance [56]. Although rule-based
and machine learning-based approaches can disambigu-
ate short forms with multiple senses from a subset of
data, more work can be done addressing a larger subset
of short forms and report types. To enable further pro-
gress in this area, we have developed a corpus annotated
with clinical short forms linked to normalized values.
With recent patient-centered initiatives, the focus of
the 2013 ShARe/CLEF eHealth Challenge was to facilitate
development of natural language processing applications
that could be used to help patients understand the content
of a clinical report, and Task 2 focused on normalization
of short forms. We describe the performance of the par-
ticipating systems at automatically normalizing clinical
short forms to the Unified Medical Language System
compared to a majority sense baseline, evaluate the per-
formance of participating systems according to short
form terms with variable majority sense distributions,
and assess each participating systems performance for
concepts shared between the ShARe test corpus and a
vocabulary containing simplified health terms. The
study extends the overview of all three tasks [32] and
organizers working notes on Task 2 [34, 57] by focus-
ing on Task 2 in significantly greater depth, focusing on
1) the difficulty of handling multiple short form senses,
and 2) the utility of each participating system with re-
spect to a vocabulary containing simplified health terms
for potentially supporting patient understanding of
clinical text.
Methods
In this section, we describe the short form schema, data-
set, shared task, sense categorization, and short form
coverage using the Consumer Health Vocabulary.
Annotation schema
We developed our annotation schema and guidelines
using a top-down and bottom-up methodology. We ap-
plied top-down knowledge of text normalization by
starting with an annotation approach focusing on clin-
ical short forms described in [51, 58]. We added rules
based on guidelines from Task 1 developed for disease
and disorder annotation and refined these rules through
feedback provided by a panel of four natural language
processing experts (WWC, SP, NE, and GS) to develop
an initial schema and guidelines. Annotation by two bio-
medical informatics students (DLM and BRS) on ten re-
ports provided a bottom-up approach to validate these
rules and clarify instructions through examples in the
guidelines. For example, we applied a top-down rule
derived from the Task 1 guidelines to exclude modifying
information like negation, history, and change in the
concept description (e.g., no eff ). After annotating ten
reports, we refined this rule with a bottom-up approach
to include anatomic locations, sidedness, and structures
within the short form span boundaries (e.g., bilat pleur
eff ) based on the data. We included sections, diseases
and disorders, signs and symptoms, diagnoses, proce-
dures, devices, gender, healthcare unit names. We
excluded medications, lab results, measurement units,
non-medical short forms, severities, and salutations. An-
notators were also provided Task 1 disease and disorder
annotations to help annotate short forms and interpret
the annotation rules. For instance, annotators were pro-
vided the Task 1 annotation C0232498: Abdominal ten-
derness for the finding ABD: Tender. and encouraged
to use this knowledge to assign ABD as Abdomen ra-
ther than Aged, Blind and Disabled. Similar to Task 1,
annotators were instructed to assign the label CUI-less
to a short form span when no appropriate concept de-
scription existed in the vocabulary. For Task 2, annota-
tors mapped short form spans to the Unified Medical
Language System. The final schema contained inclusion
and exclusion rules for 1) identifying the character spans
of short form terms in the corpus (boundary detection)
and 2) normalizing short forms to CUIs from the Uni-
fied Medical Language System 2012 using an applica-
tion program interface call within an annotation tool
(extensible Human Oracle Suite of Tools - eHOST).
The final guidelines can be viewed in detail on the
ShARe website [59].
Dataset
For this IRB-approved study, we leveraged the ShARe
corpus, a subset of de-identified discharge summary,
electrocardiogram, echocardiogram, and radiology re-
ports from about 30,000 ICU patients provided by the
Multiparameter Intelligent Monitoring in Intensive Care
(MIMIC) II database [60]. As part of ShARe/CLEF
eHealth Challenge Task 1 [59], 298 clinical reports were
split into training (n = 199 reports) and test (n = 99
reports) sets and annotated for disease and disorder
mentions and their Systematized Nomenclature Of
MEDicine Clinical Terms (SNOMED CT) codes by two
professional medical coders. We maintained these splits
and provided the Task 1 corpus to Task 2 annotators to
annotate clinical short forms along with their normal-
ized values. We achieved high inter-annotator agree-
ment of 91 % for the test dataset between annotations
that were reviewed and adjudicated by a biomedical
informaticist and a respiratory therapist. We further
characterize the corpus development and inter-annotator
agreement in [3234].
Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 4 of 13
ShARe/CLEF eHealth challenge shared task 2 - participating
teams
The annotated ShARe corpus was released as part of the
2013 ShARe/CLEF eHealth Evaluation Challenge [61].
Two training sets were provided containing short form
spans and CUIs. Participants were instructed to develop
a natural language processing normalization system to
predict the CUI for each provided short form span in the
test dataset. In summary, five teams  UTHealthCCB
[62], LIMSI [63], TeamHealthLanguageLABS [64], THCIB
[65], and WVU [66] - submitted systems for Task 2. Four
teams approached this task using machine learning-based
methods: three teams built conditional random field clas-
sifiers [63, 64, 66] and one team used support vector ma-
chines [62]. The teams used a variety of features including
lexical, morphological, and structural features from the
Unified Medical Language System, Systematized Nomen-
clature Of MEDicine Clinical Terms, clinical Text Ana-
lysis and Knowledge Extraction System, and gazetteers.
One team built a rule-based system combining clinical
Text Analysis and Knowledge Extraction System and rules
developed from the training data [65]. Specifically, the five
participating teams developed the following short form
normalization solutions:
? UTHealthCCB [62] applied one of four different
sense tagging methods based on short form
characteristics of frequency (high or low) and
ambiguity (present or not): 1) a trained support vector
machine mapped high frequency and ambiguous short
forms, 2) a majority sense method mapped high
frequency and unambiguous short forms, 3) a vector
space model mapped all low frequency short forms,
and 4) a Unified Medical Language System
Terminology Services Application Programming
Interface mapped any unseen short forms.
? LIMSI [63] applied clinical Text Analysis and
Knowledge Extraction System and MetaMap to extract
features - lexical and morphological (unigrams, short
form terms, and token characteristics), syntactic
(unigrams and bigrams part of speech), document
(report and section types), semantic (MetaMap
semantic type and CUI), and Wikipedia (semantic
category) features. Many features included a context
window of 1-3 tokens. These features were used to
train a linear-chain conditional random field classifier
using Wapiti.
? TeamHealthLanguageLABS [64] trained a linear-
chain conditional random field classifier using context
(bigram window), lexical (Lexicon Management System
terms), grammatical (lemma, part of speech and
chunk), ring-fence (complex and compound short
forms) and Systematized Nomenclature Of MEDicine
Clinical Terms (terms, concept id and category)
features to identify short forms. A sequence of gazet-
teers applied the optimal CUI mapping based on pos-
sible expansions, usage frequency, and token contexts.
? THCIB [65] developed a rule-based system combin-
ing clinical Text Analysis and Knowledge Extraction
System with custom short form and full name diction-
aries developed from the training set as well as the
STANDS4 online medical dictionary.
? WVU [66] trained a linear-chain conditional random
field algorithm from the Factorie toolkit using a dic-
tionary of short forms generated from the training data,
Unified Medical Language System data sets, and gen-
eral websites.
System evaluation metrics
We compared each participating system predictions
against the short form annotations in the test set using
accuracy defined as the count of correct short forms
divided by the total count of the reference standard
short form annotations [67]. A system short form was
correct if the assigned CUI matched the reference stand-
ard CUI. Participating teams were allowed to submit two
systems each.
Majority sense baseline
From the training data, we developed a majority sense
baseline classifier, as this approach has been successful
in other biomedical short form studies [4446]. Based
on the training data annotations, for which each short
form annotation contains the short form span offset,
term, and Systematized Nomenclature Of MEDicine
Clinical Terms CUI, we generated a majority sense dic-
tionary using frequency counts for each CUI associated
with a unique short form term (converted to lower-
case). The dictionary was structured as a list sorted first
by CUI frequency and the most frequent CUI value was
selected. For example, the short form term ca contains
2 unique CUI labels representing C0006826: Malignant
Neoplasms: 5 or C0443758: Carbohydrate antigen: 1. If
we observed ca as the short form term in the test set,
we selected the most frequent CUI value for ca
C0006826 as the normalization value (based on the fre-
quency of the training set annotations); otherwise the
short form term was assigned CUI-less. If the CUI were
equally probable, we randomly selected the CUI to be
used in the sense dictionary. For example, for the short
form term lle we randomly selected C0239340 from
the following CUI list: [C0230416: Left lower extremity:
1, C0239340: Edema of lower limbs: 1]. We compared
the majority sense baseline and participant system accur-
acy scores for statistical significant differences using ran-
dom shuffling [68].
Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 5 of 13
Sense prediction evaluation
We report the proportion of annotations from the test
set for which a short form term has one unique sense
versus two or more senses (CUI normalization values or
CUI-less). Applying a discretization method [55], we re-
port the majority sense distributions annotated for each
of the top ten most frequent short form terms contain-
ing two or more senses and variable distributions across
value sets. Furthermore, we assessed each participating
teams system performance according to the sense dis-
tribution categories below, which were defined to
characterize the ambiguity of short form terms in the
test dataset [55]:
? no ambiguity: short form terms with 1 unique sense
? low ambiguity: short form terms with > = 2 senses,
majority sense >80 %
? moderate ambiguity: short form terms with > =2
senses, majority sense 5080 %
? high ambiguity: short form terms with > = 2 senses,
majority sense <50 %
Consumer health vocabulary coverage
We evaluated the coverage of short form concepts and
annotations from the ShARe corpus against a vocabulary
of simplified, consumer-friendly terms, the Consumer
Health Vocabulary [69] developed by Zeng and col-
leagues [66]. The Consumer Health Vocabulary provides
lay terms for clinical concepts and contains a mapping
to Unified Medical Language System preferred terms for
each Consumer Health Vocabulary term. We queried
each Unified Medical Language System CUI against the
Consumer Health Vocabulary concept terms flat file
from the Consumer Health Vocabulary website [70] to
determine how frequently the preferred term was the
same both in the Unified Medical Language System and
the Consumer Health Vocabulary, and how frequently
they differed.
For the test set, we report the prevalence of unique
short form CUIs in the ShARe corpus and Consumer
Health Vocabulary. We report the proportion of the
Consumer Health Vocabulary concepts that provide a
different preferred name than the preferred name in the
Unified Medical Language System as mapped in the
Consumer Health Vocabulary resource. For example, the
patient-friendly term CT scan may be preferred over the
clinical-friendly preferred term X-Ray Computed Tom-
ography. From the test set, we also evaluated the cover-
age of short forms found in each vocabulary using recall,
with true positives (TP) defined as a ShARe short form
occurring in the vocabulary and false negatives (FN) de-
fined as a ShARe short form not occurring in the vo-
cabulary. Of annotations represented by CUIs shared by
both the test set and the Consumer Health Vocabulary,
we evaluated how well each participants system com-
pleted the normalization task using accuracy, with a TP
defined as an short form correctly normalized to a
shared ShARe/Consumer Health Vocabulary CUI and a
FN defined as an short form missed or incorrectly
normalized to a shared ShARe/Consumer Health
Vocabulary CUI.
Results
We characterized the ShARe short form corpus, assessed
participants systems, reported majority sense distribu-
tions for the most prevalent terms, assessed participants
systems for each majority sense distribution category,
evaluated the coverage of short form concepts against
the Consumer Health Vocabulary, and evaluated how
well each participants system could normalize short
forms with shared CUIs between the test set and the
Consumer Health Vocabulary.
Test corpus
On the test set of 99 clinical texts, we observed 3774
short form annotations, 603 unique terms, and 707
unique normalization values (CUIs and CUI-less). Six
percent (221/3774) of short form annotations were
assigned CUI-less.
ShARe/CLEF eHealth challenge shared task 2 - system
performances
Results for the participating systems and the majority
sense baseline for normalizing short forms in the test set
are summarized in Table 1. Although there were only
3774 observations in the test set, a total of 4892 unique
annotations were submitted among participating teams.
As a result of creating end-to-end systems (i.e. also pre-
dicting short form spans), several teams were missing an-
notations  from 163 (LIMSI.1) to 1415 (TeamWVU.1).
UTHealthCCB had the highest accuracy (71.9). We com-
pared the performance of the majority sense baseline
against the performance of the top-performing system,
UTHealthCCB.B.1. The majority sense baseline achieved
an accuracy of 69.6. about 3 percentage points lower than
the UTHealthCCB.B.1 system. However, the majority
sense baseline outperformed the second ranked system
from the same team, UTHealthCCB.B.2.
Sense prediction evaluation
We observed that 603 unique terms from a total of 2095
(55 %) short form annotations in the test data have no
ambiguity (1 unique sense); 135 unique terms from 1679
(45 %) annotations have two or more normalization
values (CUI or CUI-less). Of the short forms with two
or more normalization values, 47 unique terms, from
971 (26 %) annotations, have low ambiguity (equal or
greater than 80 % majority sense); 80 unique terms, from
Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 6 of 13
641 (17 %) annotations, have moderate ambiguity (50 to
80 % majority sense); and 8 unique terms from 67 (2 %)
annotations, have high ambiguity (less than 50 % major-
ity sense). In Table 2, we enumerate the top ten most
frequent short form terms and their majority sense
distributions for cases when two or more senses are
observed according to ambiguity classes.
In terms of overall system performance,
UTHealthCCB.B.1 achieved the highest accuracy across
all sense categories (Fig. 1). The performance of partici-
pating systems for normalizing short form terms with no
ambiguity compared to low ambiguity short form terms
ranged from a slight increase in accuracy of 1.17 to 1.19
points. The performance of participating systems for
normalizing low ambiguity short form terms compared
to high ambiguity short form terms ranged from a de-
crease in accuracy of 31.4 to 55.9 points.
Consumer health vocabulary coverage
The Consumer Health Vocabulary consists of 158,519
terms and 57,819 unique CUIs. The ShARe/CLEF short
form test set consists of 860 unique terms and 706
unique Unified Medical Language System CUIs. We ob-
served 66 % (466/707) of unique CUIs from the ShARe
test set in the Consumer Health Vocabulary. Of the
shared CUIs, 54 % (250/466) had a Consumer Health
Vocabulary preferred term. For instance, C0027051:
Myocardial Infarction occurs with a Consumer Health
Vocabulary preferred name heart attack. We determined
that 52 % (129/250) of the shared CUIs have a Con-
sumer Health Vocabulary preferred term (patient-
friendly name) that differed from the Unified Medical
Language System preferred term (clinically-friendly
name). For instance, C0013516: Echocardiography has a
Consumer Health Vocabulary preferred term of heart
ultrasound and Unified Medical Language System pre-
ferred name of echocardiography. Two thousand five
hundred ninety four of the 3774 (69 %) annotations con-
tained common CUIs between the Consumer Health Vo-
cabulary and the ShARe test set. For these possible
annotations, UTHealthCCB had the highest accuracy
(75.0), followed by the majority sense baseline (73.2),
and THCIB.B.1 (73.1) (Table 3).
Discussion
We characterized the ShARe short form corpus, assessed
participants systems, reported majority sense distribu-
tions for the most prevalent terms, assessed participants
systems for each majority sense distribution category,
evaluated the coverage of short form concepts against
the Consumer Health Vocabulary, and assessed how well
each participants system could normalize short forms
with shared CUIs between the ShARe test set and the
Consumer Health Vocabulary.
Test corpus
We estimated that around 81 % of the short form anno-
tations represent terms with none or low ambiguity (ei-
ther one unique sense or two senses with a majority
sense over 80 %); in contrast, about 19 % of the short
form annotations are moderately to highly ambiguous
(two senses with a majority sense between 50 and 80 %,
or two senses with a majority sense less than 50 %). For
example, the term trach had two senses with a majority
sense less than 80 %, requiring word sense disambigu-
ation. For instance, in now s/p trach, trach represents
a Therapeutic or Preventative Procedure - C0040590:
Tracheostomy Procedure. In Assess for trach place-
ment, trach represents a Medical Device - C0184159:
Tracheostomy Tube. In the case of these polysemous
(different, but related) senses, predicting C0040590:
Tracheostomy Procedure instead of C0184159: Trache-
ostomy Tube may not necessarily result in a misunder-
standing of the text by a patient due to level of shared
concept similarity. In the case of the following hom-
onymous (different and unrelated) sense example, PT
can represent C0949766: Physical therapy or C0030705:
Patients. In such a case, it would be more important for
a system to accurately select the correct sense for patient
understanding of clinical text due to the lack of concept
similarity.
Table 1 aParticipant system performances from [32, 34] compared against a majority sense baseline performance
Short form normalization system Unique predictions by the system Annotations comparable with reference standard Accuracy
aUTHealthCCB.B.1 3,774 3,774 71.9*
Majority Sense Baseline 3,774 3,774 69.6
aUTHealthCCB.B.2 3,774 3,774 68.3
aLIMSI.1 3,896 3,611 66.4
aTHCIB.B.1 3,774 3,774 65.7*
aTeamHealthLanguageLABS 2,987 2,633 46.7*
aWVU.1 3,068 2,359 42.6
*Indicates that the difference in accuracy is statistically significant with the system immediately below (p < 0.01)
Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 7 of 13
Table 2 Top 10 most frequent lexical variants with two or more senses according to distribution type
Short form term Total count Senses according to concept unique identifiers Distribution of senses
Low ambiguity
pt 137 C0030705: Patients 89 %
C0949766: Physical therapy procedure 4 %
C0086835: Structure of the posterior tibial artery 4 %
3 more senses 8 %
ct 82 C0040405: X-Ray computed tomography 95 %
C1274037: Cardiothoracic surgery 2 %
C0008034: Thoracic drain 2 %
1 more sense 1 %
m 62 C0024554: Male gender 81 %
C0018808: Heart murmur 16 %
C0026591: Mother 2 %
1 more sense 2 %
ekg 41 C0013798: Electrocardiogram 98 %
C1623258: Electrocardiographic procedure 2 %
f 37 C0015780: Female 92 %
C0015967: Fever 5 %
CUI-less 3 %
cath 33 C0007430: Catheterization 97 %
C0085590: Catheter 3 %
lad 33 C0226032: Anterior descending branch of left coronary artery 85 %
C0497156: Lymphadenopathy 15 %
pcp 31 C0033131: Primary care physicians 84 %
C0032305: Pneumonia, Pneumocystis carinii 16 %
cad 31 C1956346: Coronary artery disease 97 %
C0010068: Coronary heart disease 3 %
abd 29 C0562238: Examination of abdomen 90 %
C0000726: Abdominal 10 %
Moderate ambiguity
bp 53 C1271104: Blood pressure finding 68 %
C0005823: Blood pressure 32 %
r 43 C0205090: Right 58 %
C0232267: Pericardial rub 23 %
C0035508: Rhonchi 11 %
2 more senses 9 %
hr 40 C0577802: Finding of heart rate 68 %
C0018810: Heart rate 33 %
neuro 34 C0027853: Neurologic examination 79 %
C0205494: Neurologic (qualifier value) 6 %
C0221571: Nervous system problem 6 %
3 more senses 9 %
pod 28 CUI-less 79 %
C0032790: Postoperative period 21 %
ra 26 C2709070: On room air 62 %
Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 8 of 13
ShARe/CLEF eHealth challenge shared task 2  system
performances
We evaluated participants system performance for nor-
malizing acronyms/abbreviations to Unified Medical
Language System CUIs on the test set (Table 1).
Compared to the majority sense baseline results, only
the highest performing system by UTHealthCCB.1
showed improvement. Our majority sense baseline ap-
proach results (~70 % accuracy) are comparative to pre-
viously reported clinical majority sense baseline results
Table 2 Top 10 most frequent lexical variants with two or more senses according to distribution type (Continued)
C0225844: Right sided atrium 35 %
C0456165: Right atrial pressure 4 %
bs 26 C0232693: Bowel sounds 77 %
C0035234: Respiratory Sounds 23 %
pa 19 C1996865: Postero-anterior 53 %
C0034052: Pulmonary artery structure 37 %
C0428642: Pulmonary artery pressure 11 %
rrr 18 C0232185: Cardiac rhythm AND/OR rate finding 67 %
C0232188: Normal heart right 28 %
C0513693: Monitor rate, rhythm, depth, and effort of respirations 6 %
mr 18 C0026266: Mitral valve insufficiency 78 %
C0024485: Magnetic resonance imaging 22 %
High ambiguity
c 25 C0010520: Cyanosis of skin 32 %
C0149651: Clubbing 32 %
C0205064: Cervical 24 %
2 more senses 12 %
trach 9 C0040590: Tracheostomy procedure 33 %
C0184159: Tracheostomy tube 33 %
C0040591: Tracheotomy procedure 11 %
2 more senses 22 %
meds 9 C0013227: Pharmaceutical preparations 44 %
C0025118: Medicine 33 %
C0033081: Drug prescriptions 22 %
cont 8 C0549178: Continuous 38 %
CUI-less 38 %
C0584669: Recommendation to continue with treatment 13 %
1 more sense 13 %
v 6 C0042963: Vomiting 33 %
C0348013: Venous 33 %
C2228490: Examination of trigeminal nerve 33 %
d/c 3 C0030685: Patient discharge 33 %
C1444662: Discontinued 33 %
C1548175: On discharge 33 %
pos 3 C0205531: Oral route 33 %
C0518037: Oral food intake 33 %
C1446409: Positive 33 %
cvp 3 C0199666: Measurement of central venous pressure 33 %
C0428640: Central venous pressure 33 %
C1321771: Central venous pressure finding 33 %
Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 9 of 13
(71 % accuracy) [53]. On the training set, THCIB reports
20 % of the short forms from a sentence input could not
be mapped to CUIs using clinical Text Analysis and
Knowledge Extraction System. We believe this demon-
strates that out-of-the-box text normalization systems
will perform moderately for normalizing short forms.
Many participants incorporated clinical Text Analysis
and Knowledge Extraction System pre-processing, con-
ditional random field, and custom dictionaries from
training data and online resources to develop their
systems.
Systems with post-processing, sense disambiguation,
and machine learning trained with natural language pro-
cessing features can outperform a baseline short form
normalization system. The system by UTHealthCCB
used a hybrid approach incorporating rule-based and
machine learning techniques and achieved an accuracy
of 72 % which suggests short form normalization con-
tinues to be a challenging natural language processing
research problem. Some teams developed an end-to-end
system including short form boundary detection and
normalization. This reason accounts for some variation
in the number of predictions by participating systems.
Sense prediction evaluation
We observed that of most short form terms with no or
low ambiguity, over 80 % could be normalized with rea-
sonable accuracies by participants systems. In contrast,
short form terms with moderate or high ambiguity could
be normalized with low to modest accuracy by partici-
pants systems (Fig. 1). This trend was consistent for all
participating systems and approaches. This finding is not
surprising, as we would expect some reduction in per-
formance due to ambiguity.
Consumer health vocabulary coverage
Of the 3774 ShARe/CLEF short form annotations, we
observed most (94 %) short form annotations map to a
CUI in the Unified Medical Language System i.e., only
about 6 % of short form annotations were CUI-less,
demonstrating excellent short form coverage. Over half
(66 %) of the unique Unified Medical Language System
CUIs in the test corpus also occurred in the Consumer
Health Vocabulary implying that a substantial portion of
short form concepts (34 %) could be considered for
addition to the Consumer Health Vocabulary. About
52 % of the shared CUIs had a Consumer Health Vo-
cabulary patient-friendly preferred name that differed
from the Unified Medical Language System. In these
cases, a patient-friendly alternative may be offered to a
patient to improve understanding of clinical text. In con-
trast, some shared CUIs (48 %) have a Consumer Health
Vocabulary preferred term that matched the Unified
Medical Language System preferred term. In these cases
a patient-friendly alternative may not be necessary. In
Fig. 1 Accuracies of participating systems and Majority Sense Baseline for each majority sense distribution category
Table 3 Accuracy of normalizing short forms with concept
unique identifiers shared between the ShARe test set and the
Consumer Health Vocabulary
Short form normalization system Accuracy
UTHealthCCB.B.1 75.0
Majority Sense Baseline 73.2
THCIB.B.1 73.1
UTHealthCCB.B.2 70.4
LIMSI.1 69.6
TeamHealthLanguageLABS 50.9
WVU.1 50.1
Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 10 of 13
future work, we plan to identify patient-friendly pre-
ferred terms for short form CUIs from the test corpus
that did not occur in the Consumer Health Vocabulary
and propose them for inclusion.
In terms of normalizing short forms with common
CUIs between the Consumer Health Vocabulary and the
ShARe test set, participant systems demonstrated mod-
erate to reasonable accuracy suggesting promising re-
sults for supporting patient understanding of clinical
text by replacing these concepts with a more lay term or
linking these terms to web resources.
Conclusion
We completed the 2013 ShARe/CLEF eHealth Challenge
with the focus on creating resources that could be lever-
aged to develop technologies to aid patients understand-
ing of his or her electronic medical record. For Task 2,
we developed a reference standard for short form
normalization with high inter-annotator agreement, add-
ing an additional meta-data layer to the openly available
ShARe corpus [48]. The natural language processing
community demonstrated that a short form normalizer
could be created with reasonably high accuracy; how-
ever, more work needs to be done to resolve short forms
with moderate to high ambiguity. We demonstrated that
more concepts could be added to the Consumer Health
Vocabulary to support patient understanding of short
forms used in clinical reports.
Abbreviations
ADAM, Another Database of Abbreviations in Medline; ARGH, Acronym
Resolving General Heuristic; CLEF, Conference & Labs of the Evaluation
Forum; cTAKES, clinical Text Analysis and Knowledge Extraction System; CUI,
Concept Unique Identifier; eHOST, extensible Human Oracle Suite of Tools;
GENIA, Genome Information Acquisition; i2b2, Informatics for Integrating
Biology and the Bedside; MedLEE, Medical Language Extraction and
Encoding System; MIMIC, Multiparameter Intelligent Monitoring in Intensive
Care II database; SNOMED CT, Systematized Nomenclature Of MEDicine
Clinical Terms; MUC, Message Understanding Conference; SaRAD, Simple and
Robust Abbreviation Dictionary; ShARe, Shared Annotated Resources; TREC,
Text REtrieval Conference
Acknowledgments
We extend our gratitude to our funding sources, natural language
processing experts, and annotators for their invaluable contributions. We
thank Ken Pierce for working with us to complete task 2 data requests from
Physionet.org and Qing Zeng for making the Consumer Health Vocabulary
available to the community. We appreciate the useful feedback and
suggestions from our anonymous reviewers. This work was partially funded
by NICTA, which was supported by the Australian Government through the
Department of Communications and the Australian Research Council
through the ICT Center of Excellence Program, the CLEF Initiative, European
Science Foundation (ESF) project ELIAS, Khresmoi project, funded by the
European Union Seventh Framework Programme (FP7/2007-2013) under
grant agreement no 257528, ShARe project funded by the US National
Institutes of Health (R01GM090187), US Department of Veterans Affairs (VA)
Consortium for Healthcare Informatics Research (CHIR), US Office of the
National Coordinator of Healthcare Technology, Strategic Health IT Advanced
Research Projects (SHARP) 90TR0002, Vårdal Foundation (Sweden), Academy
of Finland (140323), and National Library of Medicine 5T15LM007059.
Authors contributions
WWC, BRS, DLM, SV, HS, NE, SP, and GS defined the task, DLM, WWC, BRS led
the overall task, BRS, DLM, L-MP, and SS led the annotation effort, HS, SV,
and SS co-chaired the lab, BRS and JL developed the annotation infrastruc-
ture, DLM, BRS, LC, and DM processed and distributed the corpus, and DLM,
DM and WWC led result evaluations. DLM, SV, BRS, L-MP, and WWC wrote
the manuscript then all coauthors reviewed and significantly extended the
final version. All authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Availability of supporting data
Following submission of human subjects training and a data use agreement,
the corpus and annotations can be downloaded from Physionet.org. The
data access protocol can be found on the 2013 ShARe/CLEF eHealth
Challenge website: https://sites.google.com/site/shareclefehealth/data under
Obtaining Datasets (Tasks 1 and 2).
Author details
1Department of Biomedical Informatics, University of Utah, Salt Lake City, UT,
USA. 2Nursing Science, University of Turku, and Turku University Hospital,
Turku, Finland. 3Data61, CSIRO, The Australian National University, University
of Canberra, and University of Turku, Locked Bag 8001, Canberra 2601, ACT,
Australia. 4MedWhat.com, San Francisco, CA, USA. 5University of Melbourne,
Parkville, VIC, Australia. 6Department of Computer and Systems Sciences
(DSV), Stockholm University, Stockholm, Sweden. 7Department of Biomedical
Informatics, Columbia University, New York, NY, USA. 8Boston Childrens
Hospital, Harvard Medical School, Boston, MA, USA.
Received: 28 August 2014 Accepted: 1 June 2016
RESEARCH Open Access
Extending gene ontology in the context of
extracellular RNA and vesicle
communication
Kei-Hoi Cheung1,2,21*, Shivakumar Keerthikumar3,21, Paola Roncaglia4,22, Sai Lakshmi Subramanian5,21,
Matthew E. Roth5,21, Monisha Samuel3, Sushma Anand3, Lahiru Gangoda3, Stephen Gould6,21,23,
Roger Alexander7,21, David Galas7,21, Mark B. Gerstein8,9,10,21, Andrew F. Hill3,24, Robert R. Kitchen8,21, Jan Lötvall11,24,
Tushar Patel12,21, Dena C. Procaccini13,21, Peter Quesenberry14,21,24, Joel Rozowsky8,10,21, Robert L. Raffai15,21,
Aleksandra Shypitsyna4,22, Andrew I. Su16,21, Clotilde Théry17,24, Kasey Vickers18,21, Marca H.M. Wauben19,24,
Suresh Mathivanan3,21,24, Aleksandar Milosavljevic5,21 and Louise C. Laurent20,21*
Abstract
Background: To address the lack of standard terminology to describe extracellular RNA (exRNA) data/metadata, we have
launched an inter-community effort to extend the Gene Ontology (GO) with subcellular structure concepts relevant to
the exRNA domain. By extending GO in this manner, the exRNA data/metadata will be more easily annotated and
queried because it will be based on a shared set of terms and relationships relevant to extracellular research.
Methods: By following a consensus-building process, we have worked with several academic societies/consortia,
including ERCC, ISEV, and ASEMV, to identify and approve a set of exRNA and extracellular vesicle-related terms and
relationships that have been incorporated into GO. In addition, we have initiated an ongoing process of extractions of
gene product annotations associated with these terms from Vesiclepedia and ExoCarta, conversion of the extracted
annotations to Gene Association File (GAF) format for batch submission to GO, and curation of the submitted annotations
by the GO Consortium. As a use case, we have incorporated some of the GO terms into annotations of samples from the
exRNA Atlas and implemented a faceted search interface based on such annotations.
Results: We have added 7 new terms and modified 9 existing terms (along with their synonyms and relationships) to
GO. Additionally, 18,695 unique coding gene products (mRNAs and proteins) and 963 unique non-coding gene products
(ncRNAs) which are associated with the terms: extracellular vesicle, extracellular exosome, apoptotic body, and
microvesicle were extracted from ExoCarta and Vesiclepedia. These annotations are currently being processed for
submission to GO.
Conclusions: As an inter-community effort, we have made a substantial update to GO in the exRNA context. We have
also demonstrated the utility of some of the new GO terms for sample annotation and metadata search.
Keywords: Ontology, Extracellular RNA, Extracellular vesicle, Metadata, Faceted search, Atlas
* Correspondence: kei.cheung@yale.edu; `llaurent@ucsd.edu
1Department of Emergency Medicine, Yale Center for Medical Informatics,
Yale University School of Medicine, New Haven, CT, USA
20Department of Reproductive Medicine, University of California, San Diego,
La Jolla, CA, USA
Full list of author information is available at the end of the article
© 2016 Cheung et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Cheung et al. Journal of Biomedical Semantics  (2016) 7:19 
DOI 10.1186/s13326-016-0061-5
Background
Extracellular RNAs (exRNAs) are broadly defined as
RNAs that are present in the acellular portions of bio-
fluids, such as body fluids (blood, cerebrospinal fluid,
bile, lymph, vitreous humour, amniotic fluid, ascites,
pleural, pericardial and peritoneal fluids, etc.), secretions
(saliva, urine, sweat, tears, milk, seminal fluid, etc.), and
cell and tissue culture supernatants. RNA sequencing
analyses of exRNAs demonstrate that they represent al-
most the entire range of cellular RNA species, including
rRNAs, tRNAs, mRNAs, miRNAs, piRNAs, lncRNAs,
and circular RNAs ([15]). However, the profiles of cel-
lular and exRNAs are not identical, as some cellular
RNAs appear to be highly enriched in the exRNA frac-
tion, while others appear to be significantly underrepre-
sented, and still others lie between these extremes of
enrichment or exclusion.
Among the many reasons cells might release exRNAs,
perhaps the most intriguing possibility is that exRNAs
might contribute to intercellular communication. Export
of exRNAs may also be used to eliminate undesired
RNAs from the originating cell. Finally, some exRNAs
might be generated in a nonspecific manner, either by
living cells (e.g. by a bulk flow process) or as a conse-
quence of cell death (reviewed in [6]).
ExRNAs appear to be universally associated with carrier
vehicles, likely due to the rapid degradation of unpro-
tected RNAs in biofluids ([4, 710]). The biochemical
properties of these carriers are likely to be the primary de-
terminant of the types and specific identities of exRNAs
that are secreted from cells, as well as their stability in the
extracellular milieu. The first exRNA carriers identified
were RNA viruses, which carry not only viral RNAs, but
also varying levels of host-encoded RNAs. For example,
retroviruses typically carry two host tRNAs and sub-
stoichiometric levels of host mRNAs from the cell in
addition to two copies of the viral RNA genome and key
viral proteins ([1116]). In fact, retrovirally infected cells
produce more empty virus-like particles (VLPs) than in-
fectious virions. These VLPs have failed to encapsulate the
viral RNA genome, but can carry as much as 10 kb of
host-encoded RNA ([17]). More recently, it has been dis-
covered that virally uninfected cells also release RNAs into
the extracellular space, and that these exRNAs are associ-
ated with extracellular vesicles (EVs), lipoproteins (LPPs,
most commonly HDLs ([10, 18]), LDLs ([18, 19])), and
ribonucleoprotein particles (RNPs, most commonly
Ago2-containing RNPs ([9, 20]). While the biogenic
mechanisms underlying the release of exRNA-containing
EVs, LPPs, and RNPs are still being investigated, it is clear
that they are not generated by a mechanism that is com-
mon to all of them.
Evidence for EVs and exRNA was first provided more
than seventy-five years ago by Albert Claudes observation
that uninfected chick and mammalian cells release RNA-
containing vesicles ([21]). However, these non-viral EVs
remained largely uninvestigated for decades. EVs re-
entered the literature in the late 1960s with descriptions
of calcifying matrix vesicles released by chondrocytes
([22]) and vesicular dust released by platelets ([23]).
These and other such vesicles are now commonly referred
to as exosomes, a term coined by Trams et al. in 1981
([24]) to refer to secreted vesicles that may serve a
physiologic function, including both small vesicles of
~100 nm in diameter, and larger vesicles of ~600 nm
diameter or greater.
This first definition of the term exosome has been
subsequently overlooked at least twice, first in 1987 by
investigators studying the vesicular secretion of the
transferrin receptor, who adopted a more restrictive def-
inition of the term, conflating it with a delayed mode of
vesicle secretion in which the vesicles bud at endosome
membranes to create a multivesicular body (MVB),
followed later by MVB fusion with the plasma mem-
brane to release the vesicles into the extracellular space
([25]). In 1997, investigators studying an RNA-degrading
protein complex adopted the term exosome, this time
for an entirely unrelated intracellular biochemical entity
([26]). Not surprisingly, other investigators have come to
different conclusions about which definition holds scien-
tific precedent, resulting in variable use of the term exo-
some in different laboratories. EV-related nomenclatures
are further complicated by the common use of additional
terms for secreted vesicles that are variably associated with
different biophysical properties or biogenesis pathways, as
wells as terms based on the cell type or tissue of origin.
The former include ectosomes (which refer to EVs that
are produced by budding from the plasma membrane
([27, 28]) and microvesicles (which are frequently oper-
ationally defined as EVs that pellet at moderate centrifuga-
tion speeds (~10,00020,000 xg) [29]), while the latter
include prostasomes ([30]), epididymosomes ([31]),
immunosomes ([32, 33]), oncosomes ([34]), and platelet
dust ([23]). There are even some vesicle names that refer
to observed biological activities (e.g. tolerosomes ([35])
and calcifying matrix vesicles ([22]).
The International Society for Extracellular Vesicles
(ISEV) has previously attempted to clarify the nomencla-
ture in this field. Its primary achievements have been to
(1) introduce the term extracellular vesicle (EV) as a
general term intended to encompass all secreted vesicles,
and encourage its broad acceptance, and (2) encourage
the use of broad-definition terms until a more compre-
hensive understanding of the biogenesis and molecular
compositions of different types of vesicles is developed.
Given the inconsistent vesicle nomenclatures prior to
these ISEV efforts, these were major advances. However,
some inconsistencies still persist. For example, some
Cheung et al. Journal of Biomedical Semantics  (2016) 7:19 Page 2 of 9
investigators use the term microvesicle for larger EVs
(>200 nm diameter) and exosome for smaller EVs
(~30200 nm diameter), while other investigators reject
these size-based definitions and adopt a set of biogenic
definitions in which microvesicle describes vesicles that
bud from the plasma membrane while exosome de-
scribes vesicles that bud into endosomes and are se-
creted only later upon MVB fusion with the plasma
membrane.
Therefore, investigators are currently forced to make a
choice between these competing definitions, the first be-
ing practical but mechanistically barren, while the latter
being impractical but mechanistically appealing. Unfor-
tunately, there is as yet no unambiguous way to distin-
guish between vesicles that bud from the plasma
membrane versus those that bud at the endosome mem-
brane based on either biophysical properties or molecu-
lar content. This lack of standard nomenclatures creates
i) a problem for individual researchers to annotate/share
their data in an unambiguous manner and ii) a barrier to
productive interactions with the broader research com-
munity, most prominently the biomedical, genomics,
and computational biology communities. To address
such issues, we initiated our metadata standard efforts
within the Metadata Working Group (MWG) of the
Extracellular RNA Communication Consortium (ERCC)
funded by the National Institutes of Health (NIH). As
part of these efforts, MWG matched metadata terms to
existing biomedical ontologies and identified the
exRNA-relevant terms that were absent from major on-
tologies such as the Gene Ontology (GO).
Use of ontologies in metadata annotation
As described in [36], the MWG of the ERCC has devel-
oped the data and metadata standards for annotating
exRNA profiling data for submission to the Data Man-
agement and Resource Repository (DMRR) of the ERCC.
Particularly, a process has been established to submit ex-
periment data to DMRR along with metadata in stand-
ard machine-readable formats (using Linked Data
technologies). The standards cover metadata about do-
nors, biosamples, experiments, studies, and analysis
steps. Such metadata enable targeted selection of sam-
ples of interest (e.g. specific health condition of the
donor, biofluid or cell/tissue type, library preparation
method, and sequencing assay) for integrative analyses.
The metadata also helps organize the data for efficient
interactive as well as programmatic access (e.g. REST
Application Programming Interfaces (APIs)).
The MWG identified existing biomedical ontologies
accessible through the NCBO BioPortal [37] as a source
of commonly accepted terms for annotating exRNA
datasets. Such ontology-based metadata annotation does
not only allow semantic retrieval/query of data in ERCC
data sources, it also allows DMRR data to be integrated
with other data sources with metadata annotated using
the same ontologies. For ontologies to be useful for bio-
logical applications, it is critical that the relevant ontol-
ogies contain meaningful and broadly accepted terms, as
well as ensuring that the relationships between the in-
cluded terms be both accurate and accepted by the per-
tinent scientific community. For an ontology such as
GO which is frequently used for functional enrichment
analysis of genomic datasets, it is also important that
terms be associated with specific gene products (coding
and non-coding RNAs and proteins) in an empirically
supported manner.
The Gene Ontology (GO) Consortium (GOC; http://
www.geneontology.org) is a community-based bioinfor-
matics effort. This Consortium develops, maintains and
extends two interconnected resources: the Gene Ontol-
ogy itself, and a database of GO annotations that associ-
ate specific gene products with concepts in the Gene
Ontology. As of March 2016, there are >42,000 GO
terms for describing concepts relevant to gene product
function in a species-independent manner, providing not
only comprehensive coverage of biological concepts but
also community-wide agreement on how those should
be used to describe gene functions across all organisms.
The GO is organized into three aspects [38]: these are
graph structures comprised of classes for molecular
functions, the biological processes these contribute to,
the cellular locations where these occur (cellular compo-
nents), and the relationships connecting these classes. A
GO annotation describes the association between a
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 
DOI 10.1186/s13326-016-0093-x
RESEARCH Open Access
Expansion of medical vocabularies using
distributional semantics on Japanese patient
blogs
Magnus Ahltorp1, Maria Skeppstedt2*, Shiho Kitajima3, Aron Henriksson4, Rafal Rzepka3 and Kenji Araki3
Abstract
Background: Research on medical vocabulary expansion from large corpora has primarily been conducted using
text written in English or similar languages, due to a limited availability of large biomedical corpora in most languages.
Medical vocabularies are, however, essential also for text mining from corpora written in other languages than English
and belonging to a variety of medical genres. The aim of this study was therefore to evaluate medical vocabulary
expansion using a corpus very different from those previously used, in terms of grammar and orthographics, as well as
in terms of text genre. This was carried out by applying a method based on distributional semantics to the task of
extracting medical vocabulary terms from a large corpus of Japanese patient blogs.
Methods: Distributional properties of terms were modelled with random indexing, followed by agglomerative
hierarchical clustering of 3×100 seed terms from existing vocabularies, belonging to three semantic categories:
Medical Finding, Pharmaceutical Drug and Body Part. By automatically extracting unknown terms close to the
centroids of the created clusters, candidates for new terms to include in the vocabulary were suggested. The method
was evaluated for its ability to retrieve the remaining n terms in existing medical vocabularies.
Results: Removing case particles and using a context window size of 1 + 1 was a successful strategy for Medical
Finding and Pharmaceutical Drug, while retaining case particles and using a window size of 8 + 8 was better for Body
Part. For a 10n long candidate list, the use of different cluster sizes affected the result for Pharmaceutical Drug, while
the effect was only marginal for the other two categories. For a list of top n candidates for Body Part, however, clusters
with a size of up to two terms were slightly more useful than larger clusters. For Pharmaceutical Drug, the best
settings resulted in a recall of 25 % for a candidate list of top n terms and a recall of 68 % for top 10n. For a candidate
list of top 10n candidates, the second best results were obtained for Medical Finding: a recall of 58 %, compared to
46 % for Body Part. Only taking the top n candidates into account, however, resulted in a recall of 23 % for Body Part,
compared to 16 % for Medical Finding.
Conclusions: Different settings for corpus pre-processing, window sizes and cluster sizes were suitable for different
semantic categories and for different lengths of candidate lists, showing the need to adapt parameters, not only to
the language and text genre used, but also to the semantic category for which the vocabulary is to be expanded. The
results show, however, that the investigated choices for pre-processing and parameter settings were successful, and
that a Japanese blog corpus, which in many ways differs from those used in previous studies, can be a useful resource
for medical vocabulary expansion.
Keywords: Japanese language processing, Medical vocabulary expansion, Distributional semantics, Random
indexing, Agglomerative hierarchical clustering
*Correspondence: maria@gavagai.se
2Department of Computer Science, Linnaeus University/Gavagai,
Växjö/Stockholm, Sweden
Full list of author information is available at the end of the article
© 2016 Ahltorp et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 2 of 18
Introduction
The ability to recognise named entities in text, and then
to map these to concepts in medical ontologies, is key
for most systems that rely on medical language pro-
cessing, such as information extraction systems for syn-
dromic surveillance [1], automatic detection of adverse
drug events [24] and co-morbidity analyses [5]. As a
result, much research has been conducted, and many sys-
tems developed, with the aim of improving these building
blocks of medical information processing systems.
There are a number of systems for performing mapping
to specific vocabulary concepts, for instance MetaMap
[6], IndexFinder [7], MedLEE [8] and SAPHIRE [9]  all
of these map entities to concepts in the Unified Medical
Language System (UMLS) [10]. Although mapping sys-
tems typically employ techniques for handling abbrevia-
tions, misspellings, inflections and word order differences
[11, 12], the availability of a comprehensive vocabulary
is perhaps the most important prerequisite for perform-
ing high-quality concept mapping. Extensive vocabularies
are also essential for named entity recognition approaches
that rely on vocabulary mapping [13, 14] as well as for
medical text simplification [15]. Vocabularies have, more-
over, also been shown to be useful for generating features
to be used when training machine learning models to
recognise named entities [16].
There are a number of extensive medical vocabular-
ies, many of which are included in the UMLS Metathe-
saurus [10]. These vocabularies are, however, not available
in all languages; in languages for which they are avail-
able, they are often less extensive than resources for
English. Although also less extensive vocabularies have
been shown useful for medical text mining [17], limita-
tions in the vocabularies used can lead to decreased per-
formance. Previous studies on applying Swedish UMLS
resources (which are less extensive than resources for
English) for detecting entities in Swedish clinical text
showed, for instance, a recall of 55 % for detecting Dis-
orders, 33 % for detecting Findings, 80 % for detect-
ing Body Parts [18], and a recall of 74 % for detecting
Pharmaceutical Drugs [19]. Controlled medical vocab-
ularies are, moreover, typically focused on terms from
the professional medical language, despite the important
applications of text mining from social media, such as
syndromic surveillance [20, 21] or detection of adverse
drug reactions [22]. The fact that terms used by laymen to
describe medical concepts often differ from those used by
health professionals [15, 23] render the controlled medical
vocabularies less useful for text mining from social media
[22, 24] as well as for applications such as medical text
simplification [25].
To improve the performance of concept mapping,
entity recognition and medical text simplification, exist-
ing vocabularies thus need to be expanded and adapted
to the text domain in which such systems are to be
employed. To manually expand and adapt vocabularies
to variations in language between different text genres
and over time is, however, expensive and time-consuming.
Methods that can support this process are therefore valu-
able, for instance methods for semi-automatic vocabulary
expansion.
Many previously explored methods for medical vocab-
ulary expansion [2628] rely on terms and abbreviations
being explicitly defined or classified in the text. There are,
however, many medical text genres in which the terms
used are only very rarely explained to the reader, e.g.,
the narrative text of health records and the above men-
tioned social media texts. For such genres, other strategies
for vocabulary expansion are required. One strategy is to
use existing vocabularies as a starting point, and search
for additional terms that occur in similar contexts as the
existing vocabulary terms. This can be motivated by the
distributional hypothesis, which states that words which
occur in similar contexts often have similar meanings [29].
For most languages, there is a limited availability of large
medical/biomedical corpora that can be used for research.
As a result, research on medical vocabulary expansion,
using distributional semantics methods developed for
large corpora, e.g., random indexing or word2vec, has typ-
ically focused on English vocabularies [3034], and on
vocabularies for relatively similar languages, e.g., Swedish
[3537]. Less work has been carried out on vocabulary
expansion using languages not related to English, or on
medical text genres with laymen authors.
Outside of the medical domain, the variability of stud-
ied languages is larger, and distributional semantics has
been applied to e.g., Japanese text, in studies using con-
text in the form of noun-verb and noun-noun dependen-
cies [38, 39]. Research on the adaption of distributional
semantics to Japanese corpora, using context informa-
tion in the form of several neighbouring words, has,
however, not always been successful [40]. This indicates
that, although distributional semantics methods are often
claimed to be language independent, there might be lan-
guages for which adaptions of standard configurations and
pre-processing methods are required. One difficulty asso-
ciated with Japanese emerges from the fact that white
space is normally not used ([41], p. 17). The standard
approach for segmentation used in distributional seman-
tics, in which the white-space segmented word forms the
basic semantic unit (with special handling of punctuation
marks and sometimes also of abbreviations), can therefore
not be employed. Grammatical differences between lan-
guages could pose another challenge and might entail that
the pre-processing choices that are most optimal for, e.g.,
English are not necessarily suitable for, e.g., Japanese.
In this study, we aim to investigate the possibility of
expandingmedical vocabularies by applying distributional
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 3 of 18
semantics on a medical corpus that differs from corpora
used in previous studies. We use a corpus that differs in
terms of language as well as text genre, namely a corpus
of Japanese patient blog texts. We focus on three seman-
tic categories that are highly relevant for  just to name a
few the aforementioned tasks of syndromic surveillance
and detection of adverse drug events: the semantic cat-
egories Medical Finding, Pharmaceutical Drug and Body
Part. We will compare different pre-processing strate-
gies and parameter settings with the aim of investigating
whether standard pre-processing and parameter settings
need to be adapted to (1) the Japanese language, (2) the
text genre, or (3) to the semantic category.
Background
There are a number of methods designed to support the
creation or expansion of vocabularies that, more specif-
ically, categorise words into semantic categories [42] or
identify hyponyms of selected terms [43]. With the exis-
tence of semantic categories, (semi-)automatic vocabulary
expansion can be seen as a word classification task, in
which it should be determined whether an unknown word
belongs to a certain semantic category or not. When cat-
egories have not been defined, however, word clustering
can instead be considered as a means to discover poten-
tial semantic categories [44]. As there is extensive work
on the definition of semantic categories within the med-
ical domain, e.g., within UMLS [10], the first approach
is typically taken for medical vocabulary expansion, i.e.,
classification of unknown words into pre-defined medical
categories.
The material for creating or expanding a vocabulary
could be an existing vocabulary, for instance when iden-
tifying synonym candidates by searching for similar term
descriptions in a lexicon [45], or when translating a vocab-
ulary from one language into another [46]. An alternative
to using information from existing vocabularies is to use
corpora for extracting term candidates for inclusion in a
vocabulary. A frequently used approach is to find text pat-
terns in which terms used are explained to the reader.
From the text pattern term1 also known as term2 [47],
for instance, it could be deduced that term1 and term2 are
synonyms, while the text pattern term1 such as term2
could be used for extracting terms of a given semantic cat-
egory [42]. These patterns can be either manually crafted
or automatically extracted from the corpus. A related
approach is to, instead of using explicit language patterns,
rely on frequent co-occurrences in a large corpus between,
e.g., terms and their hypernyms [48]. Another type of
pattern-based vocabulary extraction from corpora is the
construction of patterns for terms consisting of words
with certain syntactic or semantic relations, e.g., automat-
ically constructed patterns for extracting specific types
of noun-verb pairs [49]. These corpora-based methods
have been applied in the biomedical domain for synonym
extraction [27, 47, 50] and for extracting terms of a cer-
tain semantic category [28, 48]. In the latter study, terms
belonging to the semantic category Disease were automat-
ically extracted by first extracting all sentences containing
the word disease and thereafter using a known set of
disease terms to train an SVM classifier to detect terms
belonging to this semantic category. A precision of 38 %
and a recall of 45 %were achieved when applying the auto-
matically constructed vocabulary for vocabulary-based
named entity recognition of diseases.
As mentioned in the introduction, it is also possible to
extract terms on the basis of the contexts in which they
typically occur [44]. This approach is more suitable to
some types of corpora, e.g., blog posts or health record
narratives, since it does not depend on hyper/hyponomy
being explicitly stated in the text. Instead, it exploits the
fact that words that frequently occur in similar contexts
are likely to have similar meanings. Representation mod-
els for such term co-occurrence can, for instance, be
probabilistic, such as in Brown clustering [51], or spatial
models [52], in which term co-occurrences are given a
geometric representation in the form of a vector space.
Here, semantic similarity is based on geometric proxim-
ity. Zhang and Elhadad [32] have explored the approach
of using distributional semantics for vocabulary expansion
in the biomedical domain [32]. They constructed signa-
ture vectors for noun phrase chunks in a corpus based
on the words included in the chunks, the surrounding
context words (two words to the left and right, respec-
tively, of the noun phrase chunk), as well on their inverse
document frequency. As seed words, they used the terms
of the relevant semantic categories available in UMLS.
Cosine similarity was then measured between the noun
phrase chunks in the corpus and the average of the seed
terms signature vectors. All noun phrase chunks with a
similarity to the average signature vectors above a cer-
tain threshold were considered as candidates for new
terms belonging to the investigated semantic categories.
For detecting named entities of the categories Medical
Problem, precision scores of between 27 % and 28 % and
recall scores of between 31 % and 34 % were achieved
when applying their method on the three different clinical
sub-corpora within the 2010 i2b2/VA challenge.
Most research on the expansion of medical vocabularies
using distributional semantics in large corpora has been
performed on English, but similar research using smaller
corpora has been carried out on a larger variety of lan-
guages. From a French 85000 token coronary diseases
corpus, for instance, unknown tokens with dependency
relationships similar to known medical vocabulary terms
have been automatically extracted [53, 54]. The same cor-
pus has been used for evaluating techniques for building
word space models specifically adapted to small corpora
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 4 of 18
by generalising and normalising distributional contexts
[55]. The techniques were evaluated for the extent to
which the neighbours in the semantic word space had
a semantic relation in existing medical resources, e.g.,
relations of synonymy and co-hyponymy.
Random indexing is another spatial model of distribu-
tional semantics that can be used to build a semantic
space. This method was originally proposed by Kanerva
et al. [56] to deal with the performance problems, in
terms of computational cost, that were associated with
the commonly used models of distributional semantics
at the time (latent semantic analysis/indexing). Due to
its computational efficiency, random indexing remains a
popular method when building distributional semantic
models from very large corpora, e.g., large web corpora
[57] orMedline abstracts [58]. There are, however, a num-
ber of other methods available, e.g., word2vec [59] and
GloVe [60] that are also often used for creating spatial
distributional semantics models from large corpora.
The efficiency of random indexing is achieved by cir-
cumventing the need to perform dimensionality reduction
on the original term-by-contextmatrix (where the context
can be defined as, e.g., a document or the surround-
ing words), which is an important component of many
other models of distributional semantics [57]. Instead, a
semantic space with a smaller, predefined dimensionality
is created from the beginning. This is achieved by first
assigning to each context feature (e.g., each unique term),
a sparse, random vector of the required dimensionality.
These vectors, called index vectors, are generated by ran-
domly distributing a small set of non-zero (+1 and ?1)
values, with the remaining elements set to 0. If the dimen-
sionality is sufficiently large in relation to the number of
context features, the index vectors will, with a high prob-
ability, be nearly orthogonal to each other. The index vec-
tors are only used for building the semantic space, which,
instead, is composed of semantic vectors. The semantic
vector of a term, of the same dimensionality as the index
vectors, is obtained by adding up all the index vectors of
the terms with which it co-occurs in a predefined context
 typically a symmetric window of surrounding words.
The similarity between terms can, e.g., be expressed by the
cosine similarity between their vectors, i.e., the cosine of
the angle (? ) between the semantic vectors u and v. This
is computed as follows ([61], pp. 127134):
cos(?) = u · v?u??v? =
n?
i=1
ui · vi
?
n?
i=1
(ui)2 ·
?
n?
i=1
(vi)2
With random indexing and a window-based context
definition, it is possible to create different types of seman-
tic vectors. If the index vectors of the surrounding words
are added to the target terms semantic vector as-is, the
resulting semantic vectors are known as context vectors.
This, however, entirely ignores word order within the con-
text window. There is also a version of random indexing,
sometimes called random permutation, in which this is
taken into account. It does so by rotating the elements in
the index vector one step to the left or right, depending
on if the context term appears to the left or right of the
target term. When the permuting of index vectors is per-
formed in this manner, the semantic vectors are denoted
direction vectors. Random permutation spaces with direc-
tion vectors have been shown to better detect synonymy
than random indexing spaces with context vectors [62].
There is a previous study, in which a random index-
ing space constructed from Swedish medical journal text
was used for expanding a Swedish medical vocabulary
list, consisting of MeSH terms denoting Medical Find-
ing and Pharmaceutical Drug [36]. Using a set of 91 seed
terms for each of the two semantic categories, it was
possible to extract 53 % of the 90 expected Medical Find-
ings and 88 % of the 90 expected Pharmaceutical Drugs
among the top 1000 retrieved terms. The manual eval-
uation of precision showed results of 80 % for top 50
and 68 % for top 100 for Medical Finding and 64 % for
top 50 and 47 % for top 100 for Pharmaceutical Drug. In
that study, as well as in the previously mentioned study
in which distributional semantics was used for expanding
medical vocabularies [32], terms belonging to semantic
categories given by currently available vocabularies (e.g.,
the categories Medical Finding and the Pharamceutical
Drug) were treated as belonging to one distributionally
similar category of terms. In both studies, the criterion
for ranking unknown words as potential candidates was,
therefore, based on similarity to all of (or to the average
of) the seed terms from one of the categories in the exist-
ing vocabularies. This is not, however, necessarily a good
strategy since there might be a number of distributional
sub-clusters within each semantic category of the exist-
ing vocabularies. If such sub-clusters are positioned at
large distances from each other in the semantic space, this
might have the effect that words that are not part of these
sub-clusters, but close to two or more clusters, will incor-
rectly receive a higher ranking than words that are close to
the centroids of the sub-clusters. This was shown to be the
case in a study using distributional semantics for expand-
ing a Swedish vocabulary of cue terms for uncertainty
and negation [63]. The strategy of first clustering the seed
terms used into more distributionally similar subsets and
thereafter using similarity to the centroids of these subsets
as the criterion for ranking unknown words outperformed
the strategy of treating the seed terms used as one single
distributionally similar category of terms.
We will take the possibility into account that this might
be the case also for the three semantic categories that
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 5 of 18
we investigate here, i.e., Medical Finding, Pharmaceuti-
cal Drug and Body Part, and therefore study the effect
of dividing seed terms of these categories into smaller,
more distributionally similar, subsets before using them
for vocabulary expansion.
Materials
Two types of materials were used: a blog corpus and
existing Japanese vocabularies.
Corpus
The corpus used is a Japanese blog corpus from the
TOBYO site, which collects blogs written by patients
and/or their relatives [17]. After a first normalising step
described below, the corpus contained 270 million char-
acters and after pre-processing, also described below, it
contained 50 million semantic units (2.5 million unique).
Vocabularies
Since the semantic spaces were evaluated for their abil-
ity to expand vocabularies with terms belonging to the
three semantic categories Medical Finding, Pharmaceu-
tical Drug and Body Part, Japanese terms belonging to
these categories were gathered from existing vocabularies.
These were then used both as seed terms and as evaluation
data.
For Medical Finding, the following terms were used:
MeSH terms classified under the nodes Diseases (C)
and Mental disorders (F03) [64], all MedDRA/J terms
except those classified as investigations, social circum-
stances and surgical and medical procedures [65, 66], as
well as terms from the Byomei diagnosis list [67]. For
Pharmaceutical Drug, the following terms were used:
MeSH terms classified under the node Chemicals and
Drugs (D) and pharmaceutical brand names available
at the TOBYO site [68]. For Body Parts, the following
terms were used: MeSH terms under the node Anatomy
(A) except those under the sub-nodes Plant Structures
(A18), Fungal Structures (A19), Bacterial Structures (A20)
and Viral Structures (A21), as well as terms from a lan-
guage education web page listing body parts in Japanese
[69]. The Japanese translations of MeSH and Med-
DRA were obtained from the U.S. National Library of
Medicine.1
Vocabulary terms occurring more than 50 times in
the segmented corpus as a semantic unit in the context
of a sentence were included in the set of terms used.
Terms in existing vocabularies that were segmented into
several semantic units were therefore excluded, as were
infrequent terms, due to the weak statistical foundation
for their context vectors. The number of terms in each
semantic category, before and after the frequency filtering,
is shown in Table 1.
Table 1 Vocabulary size
Medical finding Pharmaceutical drug Body part
(# semantic (# semantic (# semantic
units) units) units)
All terms in used
vocabularies
77350 27912 2960
More than 50
occurrences in
the segmented
corpus as a
semantic unit in
the context of at
least one other
semantic unit
753 276 214
Methods
In addition to not being able to employ the standard
segmentation approach of white space tokenisation, we
identified a number of grammatical differences between
Japanese and languages similar to English that might be
relevant when constructing distributional semantics mod-
els. A morphologic normalisation in the form of a total
lemmatisation is sometimes performed on corpora used
for distributional semantics. Japanese is, however, highly
agglutinative ([70], p. 297), with the possibility to add sev-
eral suffixes to verbs and to one of the two Japanese
adjective types, the verbal adjectives ([41], p. 45). The suf-
fixes are, for instance, used for expressing negation ([41],
p. 54), desire ([41], p. 111) or level of politeness/formality
([41], pp. 8183). Full lemmatisation could therefore result
in severe loss of information. In addition, distributional
semantics studies on Germanic languages have shown
that employing a small context window of co-occurring
words (typically 12 preceding and following words) is
most suitable when building models for word similarity
[62]. This is not necessarily the case for languages with
another sentence structure, such as Japanese. The basic
word order of Japanese (SOV) is different from the word
order of English, and it is also relatively free since the func-
tion of a word (e.g. whether it is a topic, subject or object)
is indicated by case particles ([41], pp. 35-38). Therefore,
another context window size might be more appropri-
ate for Japanese. Also the stop word filtering, often used
for e.g., English vocabulary extraction [62], might have to
be adapted to Japanese, possibly retaining the frequently
occurring case particles.
Previously performed experiments
We have previously performed preliminary experiments
using random indexing for extracting Medical Find-
ings, Pharmaceutical Drugs and Body Parts from a
Japanese blog corpus [71]. In these experiments, we
compared three different corpus pre-processing versions
to investigate the identified grammatical differences. In
the first pre-processing version, the corpus was fully
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 6 of 18
lemmatised and stop word filtering was performed by
removing all semantic units except verbs, adjectives and
nouns/pronouns. In the second version, parts of the infor-
mation contained in the suffixes, which potentially has a
large impact on the semantics of the surrounding seman-
tic units, were retained. This included polarity (negation
or affirmation), grammatical mood and voice, while e.g.,
formality level and tense were excluded. In the third ver-
sion, case particles were also retained to study if this
could compensate for the relatively free word order of
Japanese. We also experimented with different context
window sizes, constructing distributional semantic spaces
with four different window sizes for each pre-processing
version, using a context window of 1 + 1, 2 + 2, 4 + 4 and
8 + 8 surrounding semantic units.
The results from these initial experiments showed that
the optimal window size and pre-processing technique
was dependent on which semantic category was tar-
geted. For extracting Medical Findings and Pharmaceuti-
cal Drugs, the two versions in which case particles were
removed outperformed the version in which they were
retained, while a context window size of 1 + 1 was opti-
mal. For Body Parts, on the other hand, better results
were obtained when case particles were retained. Varia-
tion in context window size had hardly any effect on the
results for this semantic category when case particles were
retained, but marginally better recall was obtained with a
window size of 8 + 8.
Aim of the performed experiments
The previously performed experiments revealed substan-
tial differences in the optimal pre-processing choices for
the investigated semantic categories: removing case parti-
cles and using the smallest context window was the most
suitable for Medical Finding and Pharmaceutical Drug,
whereas retaining case particles and using the largest con-
text window was the most suitable for Body Part. In the
previous study, however, a very simple method for lever-
aging the position of the seed terms in the semantic space
was used [36], in which a summed similarity to a semantic
category for every term in the random indexing space was
calculated. The calculation was carried out by summing
the cosine of the angle (?u¯,s¯) between the context vector of
the semantic unit (u¯) and the context vector of each term
(s¯) in the set of seed terms (S) of the semantic category in
question.
summedsimilarity(u¯) =
?
s¯? S
cos(?u¯,s¯)
It was thus not taken into consideration that each one
of the three investigated semantic categories might in
reality consist of a number of smaller distributionally sim-
ilar sub-clusters, and that a more successful strategy for
expanding a vocabulary might be to use proximity to these
sub-clusters rather than proximity to all seed terms.
In addition, in the previously performed experiments,
only a small vocabulary resource was used for evaluat-
ing the large semantic category Medical Finding, and the
evaluation was only performed for two randomly selected
sets of seed terms, which might make the results diffi-
cult to generalise for seed terms of the three investigated
categories in general.
Given the results and the limitations of the preliminary
experiment, two hypotheses were posed:
 Dividing the seed terms into sub-clusters and using
similarity to their centroids as the ranking criterion is
a more successful strategy than using the method of
summed similarity to all seed terms.
 The appropriate choice of corpus pre-processing
techniques and context window size depends on the
semantic category of interest.
Performed experiments
To investigate the posed hypotheses, an experiment was
carried out in the following four steps: 1) Pre-processing
of the corpus for random indexing; 2) construction of
a semantic space using random indexing; 3) hierarchical
clustering of vectors in the semantic space that corre-
spond to seed words and production of ranked lists of
terms according to their proximity to centroids of the con-
structed clusters (one list per maximum cluster size); 4)
automatic evaluation of recall of the terms in the produced
lists against a reference standard.
Pre-processing of the corpus for random indexing
Japanese is written using three sets of characters: kanji,
the logographic characters borrowed from Chinese writ-
ing, are used for lexical morphemes; hiragana, one of
the two syllabic character sets, are used for grammatical
morphemes, both grammatical morphemes as individual
words and as inflections; and katakana, the other syllabic
character set, is used for loan words of non-Chinese origin
([72], pp. 184192). There are, however, also some lexical
morphemes for which there is no kanji and that, therefore,
are written using hiragana, as well as morphemes that are
commonly written using hiragana, despite a possibility to
use a kanji character. The use of logographic characters
often makes the morphological boundaries in compounds
more evident than what is the case in a phonetic writ-
ing system. This has been used in previous studies, in
which the morphological compounds of medical terms in
languages with phonetic writing systems have been deter-
mined by mapping them to their corresponding Japanese
term [73]. In contrast tomany other writing systems, how-
ever, word boundaries are not marked by white space in
Japanese ([72], pp. 184192). This has the effect that white
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 7 of 18
space tokenisation cannot be applied, which is a standard
tokenisation method used in many previous distributional
semantics studies.
The absence of white space was addressed by apply-
ing a number of steps for creating a segmented version
of the corpus. A basic pre-processing was first carried
out by removing smileys and sentences solely contain-
ing Latin characters, as well as normalising the hiragana
and katakana characters by transforming half-width forms
into the corresponding full-width form. Thereafter, the
segmented version of the corpus was built in the following
two steps: (a) applying the dependency parser CaboCha to
the corpus [74] and (b) applying the semantic role labeller
ASA [75] to the parsed corpus.
Two different versions of the tokenised corpus were
then created to investigate the two different pre-
processing strategies that had been the most successful
in the previously performed experiments. In the first ver-
sion, only semantic units classified by CaboCha as either
a verb (not including helper verbs or copula), an adjec-
tive (including verbal adjectives, adjectival nouns and
adverbial derivations of adjectives) or a noun (includ-
ing pronouns) were retained; all other semantic units
were removed. In addition, all verbs and verbal adjectives
were fully lemmatised (as nouns, pronouns and adjecti-
val nouns are not inflected in Japanese, they cannot be
lemmatised). In the second version of the corpus, case par-
ticles were retained, in addition to verbs, adjectives and
nouns/pronouns. Verb and verbal adjective inflections
indicating polarity (negation/affirmation), grammati-
cal mood (subjunctive/imperative/optative/interrogative/
indicative) and voice (passive/causative/potential/active)
were also retained in this version of the corpus.
Apart from constructing semantic units based on the
constituent and morpheme boundary information given
by CaboCha, ASA also provides the inflection type infor-
mation used in the pre-processing. The output of seman-
tic role labels, which is also given by ASA was, however,
not used.
Construction of a semantic space using random indexing
For the corpus version in which case particles were
removed, a random indexing space with a window size of
1 + 1 was created, and for the version with retained case
particles, a random indexing space with a window size of
8 + 8 was created. The choices were based on the win-
dow sizes that were the most successful for the different
pre-processing techniques in the previously performed
experiments [71].
The parameter settings of the constructed spaces were
generally based on standard settings for random indexing
[62], i.e., using random permutation with 2000 dimen-
sional direction vectors, with 10 non-zero elements in the
index vectors. All semantic units in the context windows
used were given equal weight.
In initial tests using the blog corpus for constructing
random indexing spaces, index vectors corresponding to
sentence boundaries were added to the semantic vec-
tor of a semantic unit whenever it occurred near the
beginning or end of a sentence  a setting which has
previously been used for, e.g., Swedish medical jour-
nal text [36]. This, however, led to semantic spaces in
which a majority of the context vectors were positioned
within close proximity to each other in the constructed
space, probably since blog texts to a larger extent than,
e.g., medical scientific text, consist of sentences with
very few semantic units. These index vectors indicating
sentence beginning and sentence ending were therefore
removed, leading to a better distribution of the context
vectors.
Clustering of seed terms
The general approach for generating a list of candidate
terms for possible inclusion in the vocabulary was to use
a set of seed terms for ranking all words in the corpus not
in the seed term set (the unknown terms) according to
their proximity to the seed terms in the semantic space.
For each one of the three investigated semantic categories,
the following was carried out.
Agglomerative, hierarchical clustering ([76], p. 700) was
applied to the direction vectors of the seed terms. This
was carried out by first assigning each vector its own
cluster. The pairwise distances between clusters were
thereafter calculated, and the two clusters with the largest
cosine similarity between their respective centroids were
then merged into a new cluster. This hierarchical cluster-
ing process was iteratively repeated until all the vectors
of all seed terms of the semantic category formed a single
cluster.
The cluster sizes that were most appropriate for the
task were determined by successively moving upwards in
the tree of the created clusters. First, clusters consisting
of a maximum of one term were used for creating the
ranked list of unknown terms (i.e., the case in which each
seed term was treated as its own cluster). Then, clusters
consisting of a maximum of two terms were used and
the maximum cluster size was then successively increased
until the root of the tree was reached. In each step in this
process, a ranked list of candidate terms was created by
ordering the unknown terms according to cosine similarly
to their most closely located cluster centroid. That is, a
similarity score was computed for each unknown term in
the corpus (u¯) by calculating the cosine of the angle (?u¯,c¯)
between the unknown term and each of the centroids (c¯)
in the set of constructed centroids (C) and returning the
largest cosine similarity score. The unknown terms were
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 8 of 18
subsequently ordered according to decreasing similarity
score.
clustersimilarity(u¯) = max
c¯?C
cos(?u¯,c¯)
As a final step, in order to enable comparison to the
method from the previously performed experiment, the
summed similarity method was also used for ranking the
unknown terms.
Evaluation
The existing vocabulary terms in each semantic category
were divided into two sets: one set of 100 seed terms
and one set of evaluation terms, comprising the remain-
ing terms (n terms). A situation was thus simulated, in
which there is 100 terms in an existing vocabulary for each
semantic category, while the remaining n terms represent
new terms that ought to be added to the vocabulary. Recall
for retrieving the terms in the evaluation set was mea-
sured for the top n, 2n. . . 10n candidates in the constructed
lists. This simulates e.g., a medical terminologist manually
scanning the top n, 2n. . . 10n candidate terms in search of
new terms to add to a medical vocabulary.
To make the results less dependent on which terms
were selected for the set of seed terms and which were
used in the reference standard, a bootstrap resampling
[77] approach was taken, in which the experiment was
repeated 500 times, each time with a new random selec-
tion of seed terms. The final results were obtained by
averaging the recall values obtained for each resampling.
A very crude baseline was also calculated in order to give
an idea of how well the method performs in comparison
with a list of randomly extracted semantic units from the
corpus. When listing all semantic units that occur more
than 50 times in the corpus in a random order, on average
every xth item in the list would be a semantic unit from the
reference standard. The top t terms in the list would thus
on average contain t?(1/x) terms from the reference stan-
dard. There are a total of 43050 semantic units that occur
more than 50 times in the corpus, and if n is the number
of semantic units in the reference standard for a given cat-
egory, then x is 43050/n. Thereby, t?(1/x) is t?(n/43050),
and the top n semantic units in this list would therefore on
average contain n ? (n/43050) of the semantic units in the
reference standard, the top 2n terms 2n ? (n/43050), and
so on.
In addition to the automatic calculation of recall against
the reference standard, an analysis was performed of fac-
tors influencing whether a reference standard term was
often or seldom included as a highly ranked candidate.
A manual analysis was also carried out to get an under-
standing of what terms were suggested as highly-ranked
candidates, apart from those included in the reference
standard. This manual analysis was carried out for a list
of unique terms from the top 100 candidates in each
fold, when using the best settings for each of the three
categories.
Results
The results, presented in Fig. 1, provide a good basis for
accepting the second hypothesis, i.e., that a larger win-
dow size and retained case particles and inflections is a
more suitable setting for retrieving term candidates for the
semantic category Body Part, while a small window size
and the removal of case particles and inflections is more
suitable for the semantic categories Pharmaceutical Drug
and Medical Finding. The best average recall values for a
window size of 1 + 1 and a window size of 8 + 8 are also
shown in Table 2 for top n candidates and in Table 3 for
top 10n candidates. A 95 % confidence interval is given by
the 2.5 %- and 97.5 %-percentiles of the 500 recall values
obtained by bootstrap resampling [77].
The largest difference between the two settings was
observed for the category Pharmaceutical Drug: the
semantic space with a window size of 1 + 1 resulted in a
maximum recall average of 25 % for top n and a maximum
recall average of 68 % for top 10n, while the correspond-
ing scores for the semantic space with a context window of
8+ 8 were 11 % and 32 %, respectively. Likewise, for Med-
ical Finding, better results were achieved with the 1 + 1
space at each of the ten measurement points; however, the
difference is smaller than for Pharmaceutical Drug, with
maximum average recall values of 16 % for top n and 58 %
for top 10n for the 1 + 1 space versus 12 % and 45 % for
the 8 + 8 space. For Body Part, the reverse results were
observed, i.e., that better results were achieved with the
8+8 space at each measurement point, with 23 % recall at
top n and 46 % recall at top 10n versus 16 % recall and 37 %
recall with the 1+1 space. At the top 10n level for window
size 1+1 for Pharmaceutical Drug and especially for Body
Part, the results had a larger variance than for the other
results shown in Tables 2 and 3, indicating that the results
for these categories and settings were more dependent on
what terms were used as seed terms.
When comparing the results of the three categories, it
can also be concluded that, for a candidate list of 10n
candidates, the evaluated method is most successful for
the category Pharmaceutical Drug, followed by Medical
Finding, for which the maximum recall average is 10 per-
centage points lower, while it is yet another 13 percentage
points lower for Body Part. For the top n candidates, on
the other hand, the best results were achieved for Phar-
maceutical Drug and Body Part, with slightly lower results
for Medical Finding.
With respect to the first hypothesis, i.e., that a cluster-
ing approach would be more successful than the simple
summed similarity method, the results are less evident,
also when focusing on the context window size and pre-
processing strategy that was the most successful for each
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 9 of 18
Fig. 1 Recall for retrieving semantic units belonging to the three investigated semantic categories
Table 2 Best results for top n. For window sizes 1 + 1 and 8 + 8
%Window size 1 + 1 Best strategy Average 2.5 % percentile 97.5 % percentile Variance
Medical Finding summed similarity 16.3% 13.9 % 19.2 % 0.0002
Pharmaceutical Drug summed similarity 24.8% 20.1 % 29.4 % 0.0006
Body Part cluster level 83 16.2 % 7.5 % 21.9 % 0.0011
Window size 8 + 8 Best strategy Average 2.5 % percentile 97.5 % percentile Variance
Medical Finding cluster level 12 12.1 % 9.1 % 14.9 % 0.0002
Pharmaceutical Drug cluster level 1 11.1 % 7.3 % 14.7 % 0.0004
Body Part luster level 2 23.1% 18.4 % 28.1 % 0.0006
The best results are shown in bold face
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 10 of 18
Table 3 Best results for top 10n. For window sizes 1 + 1 and 8 + 8
Window size 1 + 1 Best strategy Average 2.5 % percentile 97.5 % percentile Variance
Medical Finding cluster level 100 58.1% 55.3 % 60.8 % 0.0002
Pharmaceutical Drug cluster level 14 67.9% 56.2 % 77.4 % 0.0029
Body Part cluster level 73 36.6 % 20.6 % 46.5 % 0.0036
Window size 8 + 8 Best strategy Average 2.5 % percentile 97.5 % percentile Variance
Medical Finding cluster level 20 44.9 % 40.5 % 49.7 % 0.0006
Pharmaceutical Drug cluster level 2 32.2 % 26.6 % 37.9 % 0.0010
Body Part cluster level 83 45.7% 38.6 % 51.8 % 0.0011
The best results are shown in bold face
category. To use the centroid of all seed terms was more
successful for retrieving Body Part terms than to use the
summed similarity method. There was, however, a very
small difference between different cluster levels for this
semantic category, except for the top n and 2n candidates,
for which it was slightly more successful to use proximity
to centroids of seed term clusters of size 2 as the rank-
ing criterion. Likewise, for the category Medical Finding,
there were no large differences between different cluster
levels, but minimally better results were achieved when
either using the centroid of all seed terms or the summed
similarity method, depending on how many candidate
terms were taken into account. The category Pharmaceu-
tical Drug was the only category for which the results
varied for different cluster levels for a longer candidate
list. Using the centroids of clusters with a maximum size
of 14 gave, on average, the best results with a candidate
list of top 10n and 9n. No clear conclusions can, how-
ever, be drawn for this category either, since either the
summed similarity method or using the centroid of all
seed terms wasmore successful whenmeasuring the recall
for a shorter candidate list. Especially for top 2n and top
3n, using proximity to the centroid of all terms outper-
formed the use of proximity to clusters up to a size of 90
seed terms.
It could also be observed that the curves have different
forms depending on the two explored window sizes/pre-
processing choices. For the 8+ 8 space, all curves are very
flat, while for the 1 + 1 space, the use of different cluster
levels has a bigger effect on the results.
Analysis of retrieved entities
To investigate patterns for which terms were and were not
retrieved by the evaluated methods, statistics of the pro-
portion of times a term was retrieved when it appeared in
the reference standard used were gathered. The best set-
tings for each of the three studied categories were used,
i.e., the setting that resulted in the best recall for amajority
of the ten points of measurement. The results, visualised
in Fig. 2, show that the distribution of retrieved terms
among the top 10n candidate terms is highly skewed for all
three investigated entity categories. Regardless of which
set of seed terms is used, a large proportion of the terms
are found in more than 95 % of the cases, while another
large proportion is found in less than 5 % of the cases.
For these two distinct groups of terms those that were
found in less than 5 % of the cases and those that were
found in more than 95 % of the cases  the frequency
of the terms in the TOBYO corpus was investigated and
is visualised in Fig. 3. For the category Medical Finding,
and even more clearly for Body Part, terms that occur
relatively infrequently in the corpus are overrepresented
among those that were retrieved in fewer than 5 % of
the cases. For Body Part, these infrequently occurring
terms were more typical for the specialised language of
medical professionals than the more frequently occurring
Fig. 2 This illustrates how often a term is found when used as
reference standard term. The first stack shows the number of terms
that are correctly retrieved between 0 % and 5 % of the times they are
used in the reference standard, the second stack shows the number
of terms retrieved between 5 % and 10 % of the times, and so on. The
statistics are shown for top 10n candidate terms (using cluster level
100 and fully lemmatised and stop word filtered corpus for Medical
Finding and Pharmaceutical Drug and cluster level 34 with the corpus
retaining more information for Body Part)
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 11 of 18
Fig. 3 This illustrates the frequency of the terms in the TOBYO corpus for two opposite groups of terms used as evaluation data; those terms that
were found in less than 5 % of the cases they were used as a reference standard term and those that were used in more than 95 % of the cases they
were used as a reference standard term
terms. Terms such as tongue , waist/hips , finger
and throat , which are likely to occur naturally
in laymen text, were often retrieved. Many more tech-
nical terms, such as cranial nerve , pancreatic
duct and nasal cavity , were, on the other
hand, never retrieved. For the category Medical Finding,
however, no such evident difference between laymen lan-
guage and professional language could be found between
frequently and infrequently retrieved terms. The visuali-
sation shows that increasing the frequency cut-off (which
was 50 occurrences in the corpus) by another 10 or 20
occurrences would have resulted in a higher recall for the
evaluation method applied, especially for Body Part, for
which no infrequent terms were found.
For the category Pharmaceutical Drug (which contained
terms under the MeSH node Chemicals and Drugs), there
was also a trend of infrequently occurring terms being
overrepresented among those retrieved in less than 5 %
of the cases. The trend was, however, not as evident
as for the other two categories. A brief inspection indi-
cated that terms denoting chemicals often referred to in
non-medical domains were more frequent among terms
that were rarely found than among those that were often
found. A manual classification of the terms used for eval-
uating the category Pharmaceutical Drug was therefore
performed, in which the termswere classified according to
the three groups: a) Terms often used for denoting a typical
pharmaceutical drug, b) Terms often used when referring to
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 12 of 18
concepts not related to pharmaceutical drugs and c) Terms
often used for pharmaceutical drugs as well as for concepts
not related to pharmaceutical drugs. The classification,
which was performed by an annotator without knowledge
of which terms were often or rarely retrieved, showed that
among terms found in more than 95 % of the cases, 91 %
belonged to the class Terms often used for denoting a typ-
ical pharmaceutical drug, while the same figure for terms
found in less than 5 % of the cases was 34 %.
Examples of terms that were rarely found are dia-
monds , vehicular emission and table
salt , while cough suppressants , diclofenac
and analgesic drugs are examples of
terms often found. As the patient blogs are not solely
focused on medical issues, it is very likely that there are
many terms under the MeSH node Chemicals and Drugs
that occur frequently in the corpus, without referring to
concepts related to pharmaceutical drugs.
Analysis of highly ranked candidate terms not included in
the reference standards
For producing lists of highly ranked terms not included
in the reference standards used, the following was carried
out for each one of the three semantic categories: The top
100 terms in the candidate list (produced using the best
settings according to the criteria described above) were
gathered from each one of the 500 folds, resulting in a
list of 50000 items. The list was then reduced to include
only one unique occurrence of each term (as most terms
were on top 100 lists produced from many of the folds,
most terms occurred several times in the gathered list).
In addition, terms included in existing vocabularies were
removed from the list.
This resulted in a list of 313 terms for Medical Finding
(397 when including those found in existing vocabularies),
94 terms for Pharmaceutical Drug (143 including terms
from vocabularies) and 407 terms for Body Part (485
including terms from vocabularies). The very large reduc-
tion of the lists when only keeping unique terms, shows
the consistency of candidate terms generated between dif-
ferent folds. The lists of unique terms were then manually
analysed by searching for categories of terms.
Around a fifth of the candidate terms for Medical Find-
ing could be classified as belonging to this category.
Among them were terms describing states of mind, most
of them negative, e.g., depression , anxiety/fear ,
lack of sleep and and worry/pain ;
compound terms, e.g., blood vessel+pain=vascular pain
and tear+eyes=teary eyes ; terms consisting of
orthographic variants of those found in vocabularies, e.g.,
breast cancer and ; English and German loan
words, e.g., panic , trauma and complex
. Some of these might be possible to include
in a medical vocabulary of more formal terms, while
others would be typical to a resource of laymen terms. A
type of expressions that might be considered too infor-
mal for the professional language are the double-form
onomatopoetic words that were found among the new
Medical Finding terms, e.g, dizzy , completely
exhausted/weak and , worn-out/shabby
. Despite their informal nature, they might, how-
ever, still be useful, for instance when mining for descrip-
tions of patient reactions to drugs, and there are examples
of such terms in the existing vocabularies used in the
study, e.g., the term feel sick/irritated .
It is difficult to draw the exact line for when a described
state of mind should be considered a Medical Finding;
as a result, an additional 4 % of the candidate terms
were not considered Medical Findings, but more general
descriptions of state of mind, e.g., loneliness ,
cowardice , as well as hypernyms to terms describing
states of mind, e.g., feeling/mode and feeling/emotion
. Among the candidates not categorised as Medical
Findings, there were also about the same amount of terms
describing some kind of level or change of state, e.g.,
change of mood , change and , half price
and development , of which somemight be used
for describing that there is a medically relevant change in
the patient.
Around a tenth of the candidates described general
phenomena, most of them phenomena that at least in
some contexts could be described as negative, e.g., bad
habit/peculiarity , uproar/disturbance ,
trouble and problem/question . Although
some of these terms are likely to be semantically close
to Medical Findings due to frequently occurring in sim-
ilar negative contexts, there were also terms that typ-
ically occur in the context of descriptions of Medi-
cal Findings, e.g., the term custom/habit  that,
for instance, is a component in the expression life-style
disease  and the term condition , for
instance used in an expression such as be in a good state
of health . These kinds of terms might be
too general to include in a vocabulary of professional
language, but might be the expressions used by patients
when describing their health and therefore an important
resource for medical text mining from laymen texts.
The candidate terms for the category Pharmaceutical
Drug could easily be divided into two main groups. The
first group (53 %) were terms denoting pharmaceuticals;
of which 28 % were trade names, e.g., Tamiflu
and an abbreviated form of Elental ; 50 % were
specifications of types of pharmaceuticals, e.g., painkiller
, antipyretic , vitamin pills and
sleeping pills and ; and finally 22 % were
orthographic and other versions of words for pharmaceu-
ticals, e.g., honorific form , written with hiragana and
katakana and , a misspelling/pre-processing
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 13 of 18
error , as well as versions of their physical forms, e.g.,
pill and capsule .
The second group of Pharmaceutical Drug candidate
terms (46 %) were drinks, e.g., coffee ,milk
and tea , including three drink-related words gulp-
ing/gulp down and and straw .
Apart from the two groups pharmaceuticals and drinks,
there was only one additional term, endoscope .
The verb used in Japanese for taking a medicine is the
same word used for drinking , which explains the
large group of drinks in the candidate list.
The candidate terms for Body Part had a larger seman-
tic diversity than those for Pharmaceutical Drug. Around
17 % of the terms denoted body parts/structures/fluids,
e.g., gums , cartilage , digestive organs
and blood. Of these terms 10 % were specifications of
body parts, e.g., right shoulder and left arm ;
23 % were orthographic variants or honorific forms, e.g.,
head written in katakana and stomach written in
hiragana .
The largest group (20 %) among the candidate terms
for Body Part were, however, terms describing persons,
many of them family members or persons associated with
health care, e.g., baby , twins , physician
, patient in polite form , nurse and
Japanese person . Three other evident groups were
Medical Findings (7 %), computer related terms (4 %) and
animals (3 %), e.g., wound, , infection , personal
computer ,monitor , fish and dog .
There were also terms semantically related to body parts,
e.g., those denoting things that are physically close to or
can be worn on the body, such as pillow , wristwatch
, trousers or smile/laughter , as well as
things that have a physical form similar to body parts, e.g.,
container/vessel , balloon and pump .
Discussion
We have experimented with different pre-processing and
parameter settings for expanding a vocabulary using dis-
tributional semantics on Japanese text, more specifically
Japanese patient blog text and the three semantic cate-
gories Medical Finding, Pharmaceutical Drug and Body
Part. As mentioned above, previous research on medi-
cal vocabulary expansion has mainly been conducted on
English or similar languages, and also often on text gen-
res in which the vocabulary used is sometimes defined or
explained to the reader, which is not normally the case in
the blog genre.
Quality of the results compared to those of previous studies
Results obtained in previous studies are difficult to com-
pare directly to those obtained here, as results are heav-
ily dependent on, e.g., the evaluation strategies used. In
previous approaches, in which automatically expanded
vocabularies have been used for named entity recogni-
tion, the category Disease was recognised with a preci-
sion of 38 % and a recall of 45 % [28] and the category
Medical Problem with precisions values between 27 %
and 28 % and recall values between 31 % and 34 %
[32]. Although these studies take a more indirect and
application-oriented approach to evaluation than the one
taken here, the results indicate that fully automatic gen-
eration of medical vocabularies is a difficult task, also for
English, for which vocabulary expansion from corpora has
been studied more thoroughly.
For the vocabulary extraction from the French coronary
diseases corpus, Diseases/Diagnoses had a precision of
17 %, while Chemicals, Drugs and Biological Products had
a precision of 38 % for the 24 and 8 new terms, respec-
tively, that were assigned those semantic categories [54].
Some of the results obtained here for vocabulary expan-
sion could therefore be described asmore than acceptable;
for instance, being able to expand a hypothetical seed
vocabulary of 100 terms with at least 119 new Phar-
maceutical Drugs by manually scanning a list of 1760
candidate terms (i.e., top 10n candidate), and at least 203
new Medical Findings by scanning through a list of 1959
candidates (i.e., top 3n candidates). Moreover, for the cat-
egory Pharmaceutical Drug, scanning through the list of
1760 candidates resulted in 68 % of the terms in a current
vocabulary being retrieved, which is a relatively high cov-
erage for a relatively low-cost effort. For Medical Finding,
a maximum recall of 58 % was achieved for the top 10n
candidates. Although this would require the more labo-
rious task of scanning through 6530 candidates, it could
be motivated by the fact that the Medical Finding cat-
egory contains many vocabulary terms (as can be seen
when comparing the categories in Table 1). Scanning a list
of 1140 (top 10n) candidates for the category Body Part
resulted in at least 52 new Body Part terms being added
to the vocabulary, which comprises 46 % of the expected
terms, i.e., a lower result than for the other two categories.
It should be noted that the number of new terms that
would be retrieved at different lengths of the candidate
list are described as at least x number of terms, since
the candidate lists also contain instances of terms that are
not yet included in available vocabularies, as was shown
by the manual analysis of the top 100 candidate terms.
In addition, the recall values would also be higher if a
larger cut-off value for term frequency would be used, as
low-frequency terms were overrepresented among terms
not found. Therefore, the explored methods have an even
larger potential than indicated by the recall scores.
The results previously obtained for extracting Med-
ical Findings and Pharmaceutical Drugs from Swedish
medical journal text [36] are slightly more comparable,
as a seed term set of approximately the same size was
used, and since the results were measured for a number
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 14 of 18
of candidates close to 10n. When directly comparing
the recall results, it can be concluded that higher recall
was achieved for extracting Pharmaceutical Drugs from
Swedish text (88 % compared to a top 10n recall of 68 %
achieved here), while we here achieved a slightly higher
recall for the category Medical Finding (58 % compared
to the previously achieved recall of 53 %). Although, as
previously mentioned, the results cannot be directly com-
pared, they show that there are no dramatic differences
stemming from the fact that we here expanded a vocab-
ulary using distributional semantics on a Japanese blog
corpus instead of using a corpus of a more formal text
genre written in a Germanic language.
Adaptions to Japanese text and to the blog genre
The quality of the results show that the general strate-
gies for pre-processing the corpus were successful, i.e.,
using CaboCha and ASA, as well as removing all function
words/all function words but case particles.
Vocabulary terms had to occur in the corpus as inde-
pendent semantic units more than 50 times to be used
as seed/evaluation terms, and it is likely that some of
the many terms that were excluded from vocabulary lists
used were excluded because they were multi-word terms
and, thereby, had been segmented into multiple seman-
tic units by CaboCha/ASA. This is, however, not specific
to the use of CaboCha/ASA for creating semantic units,
but is an even larger problem when creating seman-
tic spaces built on corpora pre-processed with white-
space tokenisation. Therefore, many of the vocabulary
terms that were tokenised as independent semantic units
by CaboCha/ASA, and also suggested as candidates for
vocabulary expansion, would not have been suggested in
the similar study conducted in Swedish [36], as they would
have been divided into two different semantic units by the
white-space tokeniser. (For instance diabetes mellitus type
2, Japanese: 2 , Swedish: Typ 2-diabetes). The dif-
ference is even larger between CaboCha/ASA tokenised
Japanese and white-space tokenised English, as compound
words are less frequent in English than in Swedish. There-
fore, e.g., skin diseases (Japanese: , Swedish: hud-
sjukdomar) would be correctly tokenised for this purpose
in Swedish and in CaboCha/ASA-tokenised Japanese, but
not in English. This is typically dealt with by creating n-
grams [30], or by compositional distributional semantics
[78].
The removal of smileys and of the index vectors indi-
cating sentence beginning and end were the only specific
adaptions in the pre-processing and parameter settings
that were performed for the blog text genre.Which vocab-
ulary terms were used for the evaluation were, however,
indirectly governed by the choice of text genre, as some
of the terms in medical vocabularies are more frequent in
the language used by patients. For the semantic category
Body Part, terms occurring rarely in the corpus were very
overrepresented among those that were seldom retrieved.
These terms were also termsmore typical for the language
used by medical professionals than for the language used
by patients, which functions as a reminder that vocabu-
lary expansion from patient blogs can, and should, only
aim at expandingmedical vocabularies with terms that are
included in the language used by patients.
Differences between the three studied semantic categories
Apart from the more general conclusion that the pre-
processing used was successful for Japanese text and for
the genre of patient blogs, the most important conclu-
sion that can be drawn from the conducted experiments
is that different settings for expanding a vocabulary might
be suitable for different semantic categories. The most
suitable settings might also depend on the number of can-
didate terms that, e.g., a medical terminologist is willing to
scan through. To construct sub-clusters of seed terms had,
for instance, no (or very marginal) effect when extract-
ing Medical Findings, or when using the best settings for
extracting Body Parts from 10n candidate terms. When
only looking at the n or 2n top candidates for Body Part,
on the other hand, to use proximity to centroids of clusters
with amaximum size of two seed terms in each cluster was
most successful. To use proximity to centroids of clusters
with a maximum size of 14 seed terms or to the centroid
of all 100 seed terms, were successful ranking strategies
when extracting Pharmaceutical Drugs from a term list
of 10n candidates, while there was a dip in performance
when using the centroid of clusters with a maximum size
of 90 terms.
In addition, retaining case particles/inflections and
using a large context window size was most successful
when extracting Body Parts, while removing case parti-
cles/inflections and using a small context window was
better for Medical Finding and Pharmaceutical Drug. The
reason why different parameter settings are optimal for
different semantic categories could be that Medical Find-
ing and Pharmaceutical Drug are possible to distinguish
from other semantic categories given neighbouring lexi-
cal words, such as the verb take a medicine/drink
for pharmaceuticals, while adding additional information
just adds extra noise. Terms from the category Body Parts,
on the other hand, seem to occur in lexical contexts sim-
ilar to those of terms belonging to other large semantic
categories, e.g., terms describing persons and animals.
Retaining more grammatical information, as well as using
a larger context window, could, however, be a strategy that
better distinguishes Body Parts from these other semantic
categories.
In short, parameter settings need to be adapted to
each semantic category for which the vocabulary is to be
expanded. In a realistic setting, the aim would be to find
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 15 of 18
themaximumnumber of new relevant terms of a semantic
category given the currently available terms, rather than,
as in this study, evaluate the suitability of a method for
expanding a vocabulary, independently of which terms are
already included in the vocabulary. All available vocabu-
lary terms belonging to the semantic category of interest
would in such a case be used as seed terms, and the most
suitable parameter settings given this particular seed set
should then instead be determined, e.g., by leave-one-out
cross validation.
Implications and future work
There are a number of English medical corpora in which
entities of the three semantic categories explored in this
study have been manually annotated [7981]. In some of
these studies, the laborious annotation effort has been
facilitated by automatic pre-annotation built on the exten-
sive medical vocabulary resources that are available for
English [82]. For a language (or genre) with less exten-
sive vocabulary resources, on the other hand, vocabulary-
based pre-annotation might not be as useful. The results
of our study show, however, that distributional seman-
tics can be leveraged for semi-automatically extending an
existing, small vocabulary. This might enable high-quality
vocabulary-based pre-annotation also for a language with
limited vocabulary resources. We believe that scanning
through a list of candidate terms and determining which
of these belong to a certain semantic category is faster
than scanning text for these entities, especially since the
results of our study show the potential for creating lists
with a higher density of the relevant term candidates than
would be the case when scanning through text. The results
show that these methods, previously applied on English
and languages similar to English, can also be success-
fully applied to text written in a very different language.
Since the methods work for both language types, despite
the existence of important orthographic and grammati-
cal differences, we believe that there is a high potential
for these methods to work well also on several other lan-
guages. We therefore hope that the implication of our
study will be that medical annotation projects on corpora
from languages with less extensive vocabulary resources
will include an initial step in which medical terminologies
are semi-automatically expanded by methods similar to
those explored here. This would enable vocabulary-based
pre-annotation also for such annotation projects.
Our future plans include the employment of this strat-
egy for annotating the three investigated semantic cate-
gories in a subset of the TOBYO patient blog corpus. We
aim to use all available vocabulary terms as seed terms,
and determine what settings are most suitable for this
larger seed term set by employing leave-one-out cross-
validation. By applying distributional semantics on patient
blogs, we aim to expand existing vocabularies with more
terms that are typical to the language used by patients.2
To gather lists of terms belonging to certain semantic
categories is sufficient for named entity recognition, but
vocabulary lists are not enough to be able to perform con-
cept mapping of detected entities. Future work, therefore,
also includes strategies for positioning the gathered terms
within the hierarchy of a vocabulary, either as a synonym
to an existing vocabulary concept or as a new indepen-
dent concept that is to be positioned as a hyponym to one
of the existing vocabulary concepts. Distributional seman-
tics could be applied also for this task, as has been shown
in previous research [37].
Another future plan could be to automate the pro-
cess of optimising the parameter settings for different
semantic categories. Alternatively, it might be useful to
explore if there are methods whose performance is more
robust to different choices for pre-processing and param-
eter settings, e.g., ensembles of semantic spaces, wherein
the constituent semantic spaces are built with different
parameter settings. Semantic space ensembles have been
shown often to lead to better predictive performance than
the use of any of the constituent semantic spaces on a
range of tasks, includingmedical synonym extraction [83].
Semantic space ensembles also make it possible to com-
bine different types of corpora in an effective manner
[37]; in future work, it would be interesting to combine a
blog corpus with corpora from other genres, for instance
biomedical and clinical corpora but also other corpora in
which layman terminology is likely to be used. Finally, a
manual curation of the seed term set, before using it for
expanding the vocabulary, might be worth exploring, e.g.,
by removing seed words from existing vocabularies that
are atypical for the semantic category in question.
Conclusions
We have studied different pre-processing strategies and
parameter settings for expanding a medical vocabulary
with terms belonging to the three semantic categories
Medical Finding, Pharmaceutical Drug and Body Part. A
scenario was simulated, in which vocabulary lists of 100
terms of each semantic category would be available, and in
which a medical terminologist, for instance, would man-
ually scan through a list of candidate terms in search for
new terms to add to the vocabulary lists of the investigated
semantic categories. The candidate lists were produced by
applying random indexing on a Japanese patient blog cor-
pus, and recall against existing Japanese medical vocabu-
laries was measured for a scenario in which the medical
terminologist would scan through the top n to the top
10n terms in the generated candidate lists, where n is the
number of terms in existing vocabularies of the category
in question, i.e., the number of terms that the evaluated
system should aim to find.
Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 16 of 18
It could be concluded that different settings for expand-
ing a vocabulary were suitable for different semantic cat-
egories. Retaining case particles in the pre-processing of
the corpus and using a large context window size wasmost
successful for expanding the list of Body Part terms, while
removing case particles and using a small context window
was better for the categories Medical Finding and Phar-
maceutical Drug. To use proximity to centroids of clusters
with a maximum size of 14 seed terms or to the centroid
of all 100 seed terms were slightly better ranking strategies
when extracting Pharmaceutical Drugs from a term list of
10n candidates than to use the centroid of clusters with
a maximum size of 90 terms. For the other two investi-
gated categories, however, different cluster sizes only had
a very marginal effect on the results for the best pre-
processing settings, when looking at recall for a list of the
top 10n candidates. The most suitable settings, however,
also depended on the number of candidate terms that the
simulated medical terminologist would be willing to scan
through. For instance, when comparing recall for the top
n candidates for Body Part, proximity to centroids of clus-
ters with amaximum size of two seed terms in each cluster
was slightly more successful than using the centroid of
larger clusters.
The best pre-processing, context window size and clus-
tering settings resulted in the best average recall values
for Pharmaceutical Drug, for which a recall of 25 % was
achieved for top n candidates and a recall of 68 % for top
10n. For a candidate list of top 10n candidates, the sec-
ond best category was Medical Finding, for which a recall
of 16 % was achieved for top n and a recall of 58 % for
top 10n candidates. When only taking the top n candi-
dates into account, however, results for Body Part were
better than for Medical Finding with a recall of 23 %. For
the top 10n candidates, on the other hand, the strategy
was least successful for Body Part, with a recall of 46 %. A
medical terminologist would thereby, for instance, be able
to expand the hypothetical, small vocabulary with at least
203 new Medical Findings and 119 new Pharmaceutical
Drugs by scanning through 1700-2000 candidate terms.
These results demonstrate that the pre-processing and
parameter settings applied were successful. They also
show the potential in using large corpora for semi-
automatic medical vocabulary expansion, not only those
comprising formal biomedical texts written in English or
similar languages  which have been used in previous
studies  but also texts that differ in style and language,
such as the Japanese patient blog texts that have been used
here.
We hope that the results of this study will inspire
an increased use of semi-automatic medical vocabulary
expansion methods, for medical vocabularies in a larger
range of languages, as well as for vocabularies that are
adapted to a wider variety of medical text genres. This, in
turn, might widen the types of texts to which important
medical text mining applications can be applied; appli-
cations such as syndromic surveillance or detection of
adverse drug reactions.
Endnotes
1MeSH: https://www.nlm.nih.gov/research/umls/source
releasedocs/2008AB/MSHJPN/mrsab.html MedDRA/J:
https://www.nlm.nih.gov/research/umls/sourcereleasedocs/
current/MDRJPN/sourcerepresentation.html.
2The lists of analysed terms (top 100 terms for each of
the 500 folds), that were not included in current vocab-
ularies can be found at: http://people.dsv.su.se/~mariask/
resources/japanese_vocabulary/.
Acknowledgements
The authors would like to thank the main funder, Scandinavia-Japan Sasakawa
Foundation, for supporting this work. We would also like to thank the Swedish
Foundation for Strategic Research, as well as the anonymous reviewers.
Authors contributions
MA designed and implemented the functionality for carrying out corpus
pre-processing, construction of random indexing spaces and agglomerative
clustering, while MS designed and implemented the functionality for carrying
out the evaluation. SK was responsible for the manual analysis of the results,
and, together with MS and RR, selected and gathered vocabulary to use for
evaluation. AH provided input on how to build the random indexing spaces
and RR on how to apply the pre-processing tools. KA provided input on the
overall design of the study, including choice of materials. The manuscript was
written jointly by MA, MS and AH, with comments and translations from the
other authors. All authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Declarations
Publication of this study was funded by a grant from the Scandinavia-Japan
Sasakawa Foundation. AHs contribution to the study was supported by the
project High-Performance Data Mining for Drug Effect Detection at Stockholm
University, funded by Swedish Foundation for Strategic Research under grant
IIS11-0053.
Author details
1Stockholm, Sweden. 2Department of Computer Science, Linnaeus
University/Gavagai, Växjö/Stockholm, Sweden. 3Graduate School of
Information Science and Technology, Hokkaido University, Sapporo, Japan.
4Department of Computer and Systems Sciences (DSV), Stockholm University,
Stockholm, Sweden.
Received: 1 March 2015 Accepted: 15 August 2016
RESEARCH Open Access
Interoperability between phenotypes
in research and healthcare
terminologiesInvestigating partial
mappings between HPO and SNOMED CT
Ferdinand Dhombres and Olivier Bodenreider*
Abstract
Background: Identifying partial mappings between two terminologies is of special importance when one
terminology is finer-grained than the other, as is the case for the Human Phenotype Ontology (HPO), mainly used
for research purposes, and SNOMED CT, mainly used in healthcare.
Objectives: To investigate and contrast lexical and logical approaches to deriving partial mappings between HPO
and SNOMED CT.
Methods: 1) Lexical approachWe identify modifiers in HPO terms and attempt to map demodified terms to
SNOMED CT through UMLS; 2) Logical approachWe leverage subsumption relations in HPO to infer partial
mappings to SNOMED CT; 3) ComparisonWe analyze the specific contribution of each approach and
evaluate the quality of the partial mappings through manual review.
Results: There are 7358 HPO concepts with no complete mapping to SNOMED CT. We identified partial
mappings lexically for 33 % of them and logically for 82 %. We identified partial mappings both lexically and
logically for 27 %. The clinical relevance of the partial mappings (for a cohort selection use case) is 49 % for
lexical mappings and 67 % for logical mappings.
Conclusions: Through complete and partial mappings, 92 % of the 10,454 HPO concepts can be mapped
to SNOMED CT (30 % complete and 62 % partial). Equivalence mappings between HPO and SNOMED CT
allow for interoperability between data described using these two systems. However, due to differences in
focus and granularity, equivalence is only possible for 30 % of HPO classes. In the remaining cases, partial
mappings provide a next-best approach for traversing between the two systems. Both lexical and logical
mapping techniques produce mappings that cannot be generated by the other technique, suggesting
that the two techniques are complementary to each other. Finally, this work demonstrates interesting
properties (both lexical and logical) of HPO and SNOMED CT and illustrates some limitations of mapping
through UMLS.
Keywords: Partial mapping, Human phenotype, Ontology, Standard terminologies, Interoperability
* Correspondence: olivier@nlm.nih.gov
National Library of Medicine, National Institutes of Health, Bethesda, MD, USA
© 2016 Dhombres and Bodenreider. Open Access This article is distributed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,
and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link
to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication
waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise
stated.
Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 
DOI 10.1186/s13326-016-0047-3
Introduction
In parallel to the deep sequencing effort enabled by Next
Generation Sequencing technologies, a need for deep
phenotyping has emerged [1]. Clinical phenotypes can
be recorded in reference to multiple terminologies, in-
cluding the Human Phenotype Ontology (HPO), mainly
used for research purposes, and the Standardized
Nomenclature of Medicine Clinical Terms (SNOMED
CT), mainly used in healthcare. The interoperability of
phenotypes between datasets (including electronic health
record data) annotated with different terminologies is
critical to translational research [2] and rests on the
interoperability between the corresponding terminolo-
gies. For example, electronic health record (EHR) data
coded with SNOMED CT are increasingly used as a
resource for cohort selection (e.g., for selecting patients
exhibiting a specific phenotype defined in reference to
HPO). In this case, a mapping between SNOMED CT
and HPO is key to bridging between datasets annotated
to different terminologies.
The interoperability between HPO and SNOMED CT
can be addressed in several complementary ways, through
complete or partial mappings. Moreover, these two types
of mappings can be obtained lexically (through the lexical
properties of phenotype names) or logically (through the
logical definitions and the hierarchical arrangement of
phenotype concepts).
Complete lexical mappings identify exact and normal-
ized matches between existing (pre-coordinated) terms
in HPO and SNOMED CT and denote equivalent rela-
tions between the corresponding concepts. In previous
work, we showed that only 30 % of HPO concepts could
map to pre-coordinated SNOMED CT concepts [3]. For
example, Multicystic dysplastic kidney [HP:0000003]
maps to Multicystic renal dysplasia [SCTID:204962002]
(through synonymy).
Complete logical mappings. Since both HPO and
SNOMED CT are developed using description logics, it is
be possible to compare the logical definitions of pheno-
type concepts between the two terminologies. However,
given the differences in modeling choices in HPO and
SNOMED CT, few matches would be expected. Instead,
in previous work, we analyzed the logical definitions of
existing phenotype concepts in SNOMED CT and created
patterns (post-coordinated expressions) from these defi-
nitions that could be applied to HPO phenotypes not rep-
resented in SNOMED CT as pre-coordinated concepts.
Through this approach, 1617 additional mappings could
be identified between HPO and SNOMED CT [4]. For
example, Aplastic clavicle [HP:0006660] would be equiva-
lent to the following post-coordinated expression in
SNOMED CT: Disease and (Role group some ((Associated
morphology some Hypoplasia) and (Occurrence some
Congenital) and (Finding site some Clavicle))).
Partial lexical mappings identify matches similar to
complete lexical mappings, but allow some words of the
HPO terms to be omitted in the mapping to SNOMED
CT. Such mappings denote subsumption (subclass)
relations between the more specific HPO concept and
the more general SNOMED CT concept mapped to.
For example, Bilateral renal atrophy [HP:0012586]
maps to the more general concept Atrophy of kidney
[SCTID:197659005] (ignoring the modifier bilateral).
Leveraging the compositional features of HPO terms for
mapping purposes had already been suggested by [5].
Partial logical mappings identify a subclass relation
between one fine-grained HPO concept and a more general
SNOMED CT concept, when an ancestor of the source
HPO concept is equivalent to some SNOMED CT concept.
For example, the concept Oral cleft [HP:0000202] is in sub-
class relation to Abnormality of the mouth [HP:0000153] in
HPO, and Abnormality of the mouth is equivalent to the
SNOMED CT concept Congenital anomaly of mouth
(disorder) [SCTID:128334002] through a complete lexical
mapping. Therefore, a partial logical mapping (denoting a
subClassOf relationship) can be inferred between Oral cleft
[HP:0000202] and Congenital anomaly of mouth (disorder)
[SCTID:128334002].
The objective of this paper is to investigate and
contrast lexical (based on lexico-syntactic properties
of clinical phenotype terms) and logical (based on
subsumption relations between phenotype concepts)
approaches to deriving partial mappings between HPO
and SNOMED CT.
Background
In this section, we introduce the resources used in this
investigation (HPO, SNOMED CT and the UMLS). We
briefly review related work on partial mappings and
present the specific contribution of our work.
Resources
HPO. The Human Phenotype Ontology (HPO) is an
ontology of phenotypic abnormalities developed collab-
oratively and used for the annotation of databases such as
OMIM (Online Mendelian inheritance in Man) and
Orphanet (knowledge base about rare diseases) [6]. The
version of HPO used in this investigation is the (stable)
OWL version downloaded on January 21, 2015 (build
#1337) from the HPO website (http://www.human-
phenotype-ontology.org/). It contains 10,589 classes
(concepts) and 16,807 names (terms) for phenotypes,
including 6218 exact synonyms in addition to one
preferred term for each class.
SNOMED CT is developed by the International Health
Terminology Standard Development Organization
(IHTSDO) [7]. It is the worlds largest clinical termin-
ology and provides broad coverage of clinical medicine,
Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 2 of 13
including diseases and phenotypes. SNOMED CT in-
cludes pre-coordinated concepts (with their terms) and
supports post-coordination, i.e., the principled creation
of expressions (logical definitions) for new concepts. The
U.S. edition of SNOMED CT dated March 2015 used in
this work includes about 300,000 active concepts, of
which 103,748 correspond to clinical findings.
UMLS. The Unified Medical Language System (UMLS)
is a terminology integration system developed by the U.S.
National Library of Medicine [8]. The UMLS Metathe-
saurus integrates many standard biomedical terminolo-
gies, including SNOMED CT. Although the version of
UMLS available at the time of this investigation does not
yet integrate HPO, it is expected to provide a reasonable
coverage of phenotypes through its source vocabularies.
In the UMLS Metathesaurus, synonymous terms from
various sources are assigned the same concept unique
identifier, creating a mapping among these source vocabu-
laries. Terminology services provided by the UMLS
support the lexical mapping of terms to UMLS concepts.
We used the 2015AA version of the UMLS.
Related work
Ontology matching
The general framework of this investigation is that of
ontology matching. More specifically, we investigate
different mappings techniques between the classes of
two medical ontologies. Considering the matching tech-
niques classification of Euzenat et al. [9], our approach
falls under schema matching approaches, as it only relies
on schema-level information. (Concepts in biomedical
terminologies and ontologies represent classes, while the
corresponding instances are found in EHR systems). Sev-
eral techniques have been developed for schema matching
and these approaches can be combined [10, 11]. Most
relevant to our work are matching techniques that lever-
age the structural (i.e., the subsumption hierarchy of an
ontology) and the lexical (i.e., the terms used as labels for
the classes of an ontology) characteristics of the ontol-
ogies [12]. Establishing equivalence mappings is the
most common approach to making two ontologies
interoperable. However, partial mappings can advanta-
geously extend interoperability when one ontology is
finer-grained than the other [13].
Most ontology matching techniques have been de-
veloped for and applied to broad, ambiguous domains
(e.g., the Semantic Web as a whole) and may not be as
efficient when applied to specialized, less ambiguous
domains, such as biomedicine. For example, when the
ontologies to be matched cover different domains (e.g.,
DBpedia), bootstrapping the mappings with unsuper-
vised filters to delimit the target domain can improve
the quality of the resulting mappings [14]. However,
while the improvement was significant for particularly
ambiguous datasets, the domain filter did not improve
(and could even decrease) the mapping quality for
extremely specialized and unambiguous datasets, such as
the subdomain Pathological Function in the UMLS
[14]. Along the same lines, the BLOOMS system is an
interesting solution for Linked Open Data (LOD) schema
alignment, but has not been evaluated on LOD datasets
from the life sciences domain [15].
In the next paragraphs, we review some relevant
related work conducted in the in the medical domain on
partial lexical mappings and partial logical mappings.
Partial lexical mappings
Particularly relevant to this investigation where we
attempt to find partial lexical mappings for HPO
concepts in SNOMED CT by removing some of modi-
fiers that specialize phenotype terms in HPO is work
done on the compositional aspects of biomedical terms.
Terminologies, such as the Gene Ontology, have been
shown to be highly compositional [16, 17] in that some
of their more complex terms are derived from simpler
terms by addition of modifiers. Moreover, it has been
reported that the compositional structure of Gene
Ontology terms impacts its usage [18] and can support
automatic ontology extension [19]. Similarly, the com-
positional structure of SNOMED terms has been
exploited for assessing the consistency of its hierarchical
structure [20]. Recent work based on the compositional-
ity of phenotype terms investigated skeletal abnormal-
ities [21] and clinical phenotypes across species [22].
However in the latter study, the Entity-Quality decom-
position strategy yielded better results on the Mamma-
lian Phenotype Ontology than on HPO. Also of interest
is the work involving partial mappings by Mili?i? et al.
[23] in the context of mapping the rare diseases of the
Orphanet terminology to the UMLS. Partial lexical map-
pings leveraging increasingly aggressive normalization of
Orphanet terms were used to rank candidate mappings
for comprehensive expert curation.
Partial logical mappings
We are not using supervised machine learning approaches
in order to discover new partial mappings, as was done in
[13]. Instead, we use existing equivalence relations be-
tween HPO and SNOMED CT and subsumption relations
asserted in HPO to infer partial logical mappings. The
resulting partial mappings denote a subclass relation
between a fine-grained HPO concept and a more general
SNOMED CT concept. A similar approach was used in a
different domain to map adverse drug events (ADEs)
between SNOMED CT and MedDRA. In this investiga-
tion, the fine-grained concepts in SNOMED CT were
mapped to more general concepts in MedDRA through
partial logical mappings [24].
Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 3 of 13
Specific contribution
The specific contribution of this work is not to propose
new mapping techniques. Rather, we leverage existing
techniques to extend the mapping of clinical pheno-
types from HPO to SNOMED CT. More specifically,
we leverage the lexico-syntactic properties of HPO
terms and the logical structure of HPO to derive partial
mappings. Moreover, we contrast the contribution of
lexical and logical approaches to the development of
partial mappings.
Methods
Our investigation of partial mapping can be summa-
rized as follows. We extracted phenotype concepts
(along with their terms) from HPO and SNOMED
CT. We identified complete lexical mappings between
the two resources. We leveraged the lexico-syntactic
properties of phenotype terms to derived partial lexical
mappings, and the subsumption hierarchy of phenotype
concepts to derive partial logical mappings. Finally, we
analyzed the specific contribution of each approach and
evaluated the quality of the partial mappings through
manual review.
Extracting phenotypes terms
From HPO, we selected the concept Phenotypic abnormal-
ity [HP:0000118] and all its descendants with their corre-
sponding terms (preferred terms and synonyms). In order
to restrict SNOMED CT to phenotypes and disorders, we
selected the concept Clinical Findings [SCTID:404684003]
and all its descendants, along with their terms (referred to
as descriptions in SNOMED CT).
Identifying complete lexical mappings
Although the focus of this investigation is on partial
mappings, we rely on complete lexical mappings (denot-
ing equivalence relations) for two reasons. Partial map-
pings are primarily useful for those concepts for which
no complete mapping exists, and the complete lexical
mappings are key to identifying partial logical mappings.
To identify equivalent mappings between HPO and
SNOMED CT concepts, we mapped each original
phenotype term (preferred term or synonym) from
HPO to the clinical findings of SNOMED CT lexically
through UMLS synonymy, as previously described in
[3]. For example, the HPO concept Abnormality of the
mouth [HP:0000153] has a complete lexical mapping
to the SNOMED CT concept Congenital anomaly of
mouth (disorder) [SCTID:128334002], as indicated by
the UMLS Concept Mouth Abnormalities [C0026633]
in which Abnormality of the mouth and Congenital
anomaly of mouth (disorder) are synonyms. (The
issue of congenitality will be addressed in the Dis-
cussion section.)
Deriving partial lexical mappings
To derive partial lexical mappings, we identified mo-
difiers in phenotype terms (through lexico-syntactic
analysis), and we performed increasingly aggressive
demodification of HPO terms until the demodified HPO
terms could be mapped to SNOMED CT (Fig. 1).
Identifying modifiers through lexico-syntactic analysis
In order to identify modifiers in HPO terms (preferred
terms and synonyms), we performed a lexico-syntactic
analysis (shallow parsing) of these terms using the
minimal commitment parser available as part of
natural language processing tool SemRep [25]. For
example, the HPO term Bilateral renal atrophy
[HP:0012586] is analyzed as two adjectival modifiers,
Bilateral and renal, followed by the head noun
atrophy. Its lexico-syntactic profile would therefore be
recorded as [MOD-MOD-HEAD].
More specifically, we focused on terms with a
[MOD]*[HEAD] profile (i.e., one or more adjectival or
noun modifiers followed by a head noun). We also con-
sidered terms containing one prepositional attachment,
in which we treated each element of the prepositional
phrase as a modifier (of the main head noun) for the
purpose of this analysis. Complex terms with multiple
prepositional attachments were ignored, because their
analysis requires more sophisticated parsing techniques.
Demodifying phenotype terms
Since our intuition is that modifiers in specialized HPO
terms prevent mapping to the more general terms found in
SNOMED CT, we attempted to remove the modifiers iden-
tified in HPO terms through lexico-syntactic analysis and
to map the demodified terms to SNOMED CT through the
UMLS, thereby creating a partial lexical mapping of the
original HPO term to SNOMED CT. In practice, we itera-
tively removed all combinations of modifiers from an
original HPO term (preferred term or synonym), in increas-
ing order of aggressiveness, i.e., first removing one modifier
at the time, then, two modifiers, etc. until only the head
noun remained. For example, after removing the modifier
bilateral from the HPO term Bilateral renal atrophy
[HP:0012586], the demodified term renal atrophy mapped
to SNOMED CT through the UMLS. Note that from this
term, where the head noun atrophy is modified by bilateral
and renal, we generated the following three demodified
terms. By removing one modifier (level-1), we obtained
bilateral atrophy and renal atrophy. After removing both
modifiers (level-2), we generated atrophy. As an example
of term with a prepositional attachment, Congenital ab-
sence of uvula [HP:0010292] has for lexico-syntactic profile
[MOD HEAD][PREP HEAD]. Except for the head noun of
the main noun phrase (absence), all the other lexical items
are treated as modifiers (congenital, of, and uvula).
Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 4 of 13
Mapping demodified terms through UMLS
We attempted a complete lexical mapping of the
demodified HPO terms to SNOMED CT through the
UMLS, as was done for the original HPO terms in
[3]. Note that the complete mapping of a demodified
term corresponds to the partial mapping of the
original term prior to demodification. In order to
select the closest mappings, we only recorded the
mapping for the less demodified term(s). For ex-
ample, there is no complete mapping to SNOMED
CT for Bilateral renal atrophy [HP:0012586], but a
level-1 partial mapping is found to Atrophy of
kidney [SCTID:197659005] after removing one modi-
fier, bilateral.
Deriving partial logical mappings
To derive partial logical mappings, we mapped HPO
concepts to equivalent SNOMED CT concepts and we
inferred partial logical mappings from the subsumption
relations of HPO (Fig. 2).
Most HPO concepts have no complete lexical map-
ping (i.e., no equivalence relation) to SNOMED CT.
For these concepts, we attempted a partial logical
mapping. In practice, when an equivalent mapping to
SNOMED CT was found among the ancestors of a
given HPO concept, we inferred a partial logical map-
ping between this HPO concept and the SNOMED
CT concept(s) equivalent to its ancestor. More specif-
ically, if several ancestors of the HPO concepts have
equivalence relations to SNOMED CT, we only record
as partial logical mappings those ancestors that are
the closest to the source HPO concept.
For example, the HPO concept Oral cleft [HP:0000202]
has no complete lexical mapping in SNOMED CT. This
concept is a subclass of Abnormality of the mouth
[HP:0000153], which has an equivalent relation to
the concept Congenital anomaly of mouth (disorder)
[128334002] in SNOMED CT. Therefore, a partial
logical mapping denoting a subclass relation is inferred
between Oral cleft [HP:0000202] and Congenital anomaly
of mouth (disorder) [128334002]. This logical mapping is
deemed level-1 because it is based on an equivalent
mapping of a direct ancestor (i.e., parent concept). In the
case of Short upper lip [HP:0000188], the resulting partial
logical mapping was deemed level-3 because its closest
ancestor achieving a complete mapping was three
levels above the source HPO concept (Short upper lip
[HP:0000188] is a subclass of Abnormality of upper
lip [HP:0000177], which is a subclass of Abnormality
of the lip [HP:0000159], which is a subclass of Abnor-
mality of the mouth [HP:0000153]).
Evaluation
Quantitative evaluation
We quantified the number of complete lexical mappings
and the number of partial mappings (lexical partial
mappings and logical partial mappings) between HPO
concepts and SNOMED CT concepts. The analysis was
stratified by level of demodification for the partial lexical
mappings and by level of subsumption for the partial
logical mappings. Then we analyzed the overlap between
partial lexical and logical mappings, as well as the
combined coverage of HPO concepts provided by both
types of partial mappings.
SNOMED CT
Clinical finding
terms
Lexico-syntactic profiles 
for HPO
Demodified HPO terms
SemRep
HPO
Phenotype
terms
Partial lexical mapping
through UMLS
Complete lexical mapping
through UMLS
remove modifiers
Fig. 1 Identifying partial lexical mappings between HPO and SNOMED CT
Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 5 of 13
Qualitative evaluation
We evaluated the quality of the partial mappings by
manual review of a random subset of 10 % of the partial
lexical mappings. Additionally, we evaluated a sample of
the partial logical mappings consisting of 25 mappings per
level in the subsumption hierarchy. One of the authors
(FD), a physician, tagged the partial mappings as onto-
logically valid if they were consistent with a subclass rela-
tion. For example, the mapping of Bilateral renal atrophy
[HP:0012586] to Atrophy of kidney [SCTID:197659005] is
ontologically valid. In contrast, the mapping of Abnor-
mality of the paranasal sinuses [HP:0000245] to Con-
genital malformation (disorder) [SCTID:276654001] is
not ontologically valid, because some subclasses of
Abnormality of the paranasal sinuses (e.g., Sinusitis
[HP:0000246]) are obviously not necessarily of con-
genital origin. (We will come back to this issue in the
Discussion section).
Additionally, ontologically valid mappings were evalu-
ated for clinical relevance from the perspective of cohort
selection. In practice, the mappings were tagged as
clinically relevant if they were clinically useful for
building a cohort of patients exhibiting a particular
phenotype, i.e., for selecting medical records describing
the clinical phenotypes of such patients. For example,
the mapping of Bilateral renal atrophy [HP:0012586] to
Atrophy of kidney [SCTID:197659005] is deemed clinic-
ally useful, because it would be relatively easy to select
patients with Bilateral renal atrophy from patients with
Atrophy of kidney. In contrast, the mapping of Abnormal
respiratory motile cilium morphology [HP:0005938] to
Morphologic finding [SCTID:72724002] is not deemed
clinically useful, because few patient records annotated
with Morphologic finding would actually correspond to
cases of Abnormal respiratory motile cilium morphology.
In other words, this metric of clinical relevance attempts
to assess whether the partial mappings are close
enough for a specific use case, here cohort selection.
Results
In this section, we present the results for each step of
our approach to establishing partial lexical and logical
mappings. We also provide an extended example to
illustrate our mapping approach.
Extracting phenotypes terms
From HPO, we selected 10,454 concepts specifically
representing phenotypic abnormalities (10,454 preferred
terms and 6158 synonyms). From SNOMED CT, we
selected 103,748 concepts for clinical findings (103,748
fully specified names and 167,491 synonyms).
Identifying complete lexical mappings
Of the 10,454 phenotype concepts in HPO, we identified
a complete lexical mapping to clinical findings in
SNOMED CT for (at least one term of the) 3096 HPO
concepts (30 %). This proportion is consistent with our
prior findings ([3]). We used the remaining 7358
concepts (10,631 terms) for identifying partial mappings
lexically and logically.
Deriving partial lexical mappings
Identifying modifiers through lexico-syntactic analysis
The lexico-syntactic analysis of the 10,631 HPO terms
produced 494 distinct lexico-syntactic profiles, the most
frequent of which being [MOD-HEAD] (23 %). The list
of the 10 most frequent lexico-syntactic profiles
(accounting for 65 % of the HPO terms) is shown in
Table 1. A total of 6959 HPO terms had lexico-syntactic
profiles amenable to demodification, corresponding to
35 distinct lexico-syntactic profiles. Of note, 218 HPO
terms consisting of a single head noun ([HEAD]), were of
course not amenable to demodification. The remaining
3454 HPO terms are complex terms and were not con-
sidered for demodification.
A total of 2864 distinct modifiers extracted from these
HPO terms were associated with 1838 distinct head
HPO
Phenotype
concept
HPO
Phenotype
ancestor concept
Partial logical 
mapping
Complete lexical mapping
through UMLS
SNOMED CT
Clinical finding
concept
Subclassof
relation in HPO
Fig. 2 Identifying partial logical mappings between HPO and SNOMED CT
Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 6 of 13
nouns. The number of modifiers per term ranged from 1
to 8 (median = 2). The most frequent head nouns were
abnormality, hypoplasia, epiphyses, ossification, atrophy,
phalanx, aplasia, phalanges, EEG and sclerosis. Exclud-
ing prepositions, the most frequent modifiers were
abnormal, increased, absent, hypoplastic and decreased.
Demodifying phenotype terms
The demodification process resulted in the creation of
23,936 demodified terms from the 6959 original terms.
Mapping demodified terms through UMLS
Of the 7358 HPO concepts with no complete mapping to
SNOMED CT, we identified a partial lexical mapping for
(at least one term of the) 2464 HPO concepts (33 %). A
majority of the partial mappings occurred at level 1 (i.e.,
after removing a single modifier). An analysis of the lowest
level at which the mapping occurred is presented in Fig. 3.
Among the modifiers, metabolism, progressive, recurrent,
generalized, abnormal, bilateral, morphology, distal, unilat-
eral, epiphysis and congenital are the most frequently
removed when a mapping was found. The most frequent
profiles involved in these mappings were [MOD-HEAD]
(e.g., Fasciculiform cataract [HP:0010926]), [MOD-MOD-
HEAD] (e.g., Bilateral renal atrophy [HP:0012586]),
[HEAD][PREP-DET-HEAD] (e.g., Osteosclerosis of the clav-
icle [HP:0100923]), and [HEAD][PREP-MOD-HEAD] (e.g.,
Abnormality of glutamine metabolism [HP:0010903]).
Deriving partial logical mappings
Of the 7358 HPO concepts with no complete map-
ping to SNOMED CT, we inferred a partial logical
mapping for 6009 HPO concepts (82 %). The partial
logical mappings were distributed across 10 levels of
subsumption. The first level represented 2106 (35 %)
of the partial logical mappings, and the first 4 levels
represented 5197 (86 %) of all the partial logical
mappings (Fig. 4).
Evaluation
Quantitative evaluation
Of the 10,454 phenotype concepts in HPO, we identi-
fied complete mappings for 3096 (30 %), partial lex-
ical mappings for 2464 (24 %), and partial logical
mappings for 6009 (57 %). As shown in Fig. 5, we
identified partial mappings, lexical or logical, for 6474
HPO concepts (62 %).
Qualitative evaluation
In our randomly selected evaluation subset of 247 partial
lexical mappings, 62 % were ontologically valid and 49 %
were both ontologically valid and clinically relevant. As
shown in Table 2, the quality of these mappings is higher
for the first level of demodification.
Of the 125 logical mappings randomly selected among
concepts with no lexical partial mappings, 71 % were
ontologically valid and 67 % were both ontologically
valid and clinically relevant. As shown in Table 3, the
quality of the mappings is relatively consistent across the
first 4 levels of logical mappings.
Extended example
To illustrate the main steps of our partial mapping
approach, we consider the HPO concept Recurrent bron-
chitis [HP:0002837], for which there is no complete
lexical mapping to SNOMED CT.
Partial lexical mapping
The lexico-syntactic profile of this term is [MOD-HEAD],
in which the head noun bronchitis is modified by the
adjective Recurrent. We demodified this term by removing
its sole modifier, Recurrent, resulting in the bare head
noun, bronchitis. According to the UMLS, bronchitis is
equivalent to three SNOMED CT concepts, Bronchitis
(disorder) [SCTID:32398004], Acute bronchitis (dis-
order) [SCTID:10509002], and Acute tracheobronchitis
(disorder) [SCTID:35301006]. Therefore, we identified
Table 1 Most frequent lexico-syntactic profiles of the 10,631 HPO terms not involved in a complete lexical mapping
Lexico-syntactic profile Terms (%) Examples of HPO terms
[MODHEAD] 2478 (23 %) Oral cleft, Aplastic clavicles, Abnormal philtrum
[MODMODHEAD] 1811 (17 %) Asymmetric limb shortening, Multicystic kidney dysplasia
[HEAD] [PREPDETHEAD] 536 (5 %) Abnormality of the philtrum, Polydactyly of the foot
[MODMODMODHEAD] 478 (4 %) Small proximal femoral epiphyses, Increased cup disc ratio
[HEAD] [PREPMODHEAD] 386 (4 %) Delay in motor development, Abnormality of renal excretion
[MODHEAD] [PREPHEAD] 321 (3 %) Hypertensive disorder of pregnancy, Coronal cleft of vertebrae
[HEAD] [PREPHEAD] 259 (2 %) Abnormality of upper lip, Tremor at rest, Tetralogy of Fallot
[HEAD] 218 (2 %) Gastroschisis, Polydactyly, Pre-eclampsia
[HEAD] [PREPDETMODHEAD] 209 (2 %) Abnormality of the paralabial region, Fragmentation of the metacarpal epiphyses
[MODHEAD] [PREPDETHEAD] 202 (2 %) Downturned corners of the mouth, IgA deposition in the glomerulus
top 10 6898 (65 %)
Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 7 of 13
Fig. 4 Complete and partial logical mappings between HPO and SNOMED CT
Fig. 3 Complete and partial lexical mappings between HPO and SNOMED CT
Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 8 of 13
a level-1 partial lexical mapping for Recurrent bron-
chitis [HP:0002837] to three target concepts in
SNOMED CT.
Partial logical mapping
The concept Recurrent bronchitis [HP:0002837] has
three direct ancestors in the subsumption hierarchy of
HPO, Abnormality of the bronchi [HP:0002109], Bron-
chitis [HP:0012387] and Recurrent upper respiratory
tract infections [HP:0002788]. According to the UMLS,
the concept Abnormality of the bronchi [HP:0002109]
has no equivalent in SNOMED CT. The concept
Bronchitis [HP:0012387] is equivalent to the same three
concepts identified as a mapping for the demodified
term bronchitis. Finally, the concept Recurrent upper
respiratory tract infections [HP:0002788] is equivalent to
two SNOMED CT concepts: Upper respiratory infection
(disorder) [SCTID:54150009] and Recurrent upper re-
spiratory tract infection (disorder) [SCTID:195708003].
Therefore, we inferred a partial logical mapping for Re-
current bronchitis [HP:0002837] to five target SNOMED
CT concepts, three from Bronchitis [HP:0012387] and
two from Recurrent upper respiratory tract infections
[HP:0002788]. Of note, since a partial mapping was
found through a direct ancestor of Recurrent bronchitis
[HP:0002837], we did not explore its more distant
ancestors.
Overall
A partial mapping to SNOMED CT can be derived for
the HPO concept Recurrent bronchitis [HP:0002837]
both lexically and logically, at the first level (of demodifi-
cation or subsumption) in both cases. Moreover, all the
target concepts from the lexical mapping were also iden-
tified by the logical mapping, which also identified two
additional target concepts.
Discussion
Enhanced mapping of phenotype concepts between HPO
and SNOMED CT
In addition to the 30 % of HPO concepts that can be
mapped to SNOMED CT through complete lexical map-
ping (through UMLS), we assessed that 62 % of all HPO
concepts have a partial lexical or logical mapping to
SNOMED CT, bringing to 92 % the proportion of HPO
concepts mapped to SNOMED CT with an equivalent or
subclass relation (Fig. 5). Partial mapping techniques signifi-
cantly increase the rate of mapping for phenotype concepts
between HPO and SNOMED CT, which confirms our intu-
ition that HPO concepts tend to be more specialized than
phenotype concepts in SNOMED CT, where they can often
be mapped to more general phenotype concepts.
Relative contribution of the partial lexical and logical
mapping approaches
Overall
Unsurprisingly, the partial logical mapping approach is
far more productive that the partial lexical mapping
approach. More specifically, of the 7358 HPO concepts
with no complete mapping to SNOMED CT, the propor-
tion of partial mappings obtained is 82 % for the logical
approach vs. 33 % for the lexical approach.
By level
Lexical and logical mappings also differ in the level at
which the mapping occurs. A majority of the partial
Table 2 Qualitative evaluation of the partial lexical mappings
ontologically valid mappings clinically relevant mappings
level yes no total (in proportion of the ontologically valid mappings)
1 130 68 % 60 32 % 25 103 54 %
2 18 40 % 27 60 % 25 14 31 %
3+ 5 42 % 7 58 % 25 4 33 %
all 153 62 % 94 38 % 125 121 49 %
Table 3 Qualitative evaluation of the partial logical mappings, with no lexical mapping
ontologically valid mappings clinically relevant mappings
level yes no total (in proportion of the ontologically valid mappings)
1 22 88 % 3 12 % 25 20 80 %
2 19 76 % 6 24 % 25 17 68 %
3 15 60 % 10 40 % 25 15 60 %
4 18 72 % 7 28 % 25 17 68 %
5+ 15 60 % 10 40 % 25 15 60 %
all 89 71 % 36 29 % 125 84 67 %
Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 9 of 13
lexical mappings (95 %) occur after removing one or
two modifiers (Fig. 3), while the partial logical map-
pings are distributed across a larger number of levels
of subsumption (Fig. 4), with only 54 % of the map-
pings occurring over the first two levels. Although
the levels for the lexical approach (i.e., number of
modifiers removed) and for the logical approach (i.e.,
number of edges in the concept hierarchy) cannot be
directly compared, this difference indicates that the
lexical mappings are generally closer in meaning to
the source HPO concept compared to the logical
mappings.
Overlap between partial lexical and logical mappings
The overlap between the lexical and logical approaches
to partial mapping is limited. As shown in Fig. 5, of the
6474 HPO concepts for which a partial mapping to
SNOMED CT was identified, 1999 (31 %) were common
to both approaches. In other words, the lexical approach
only generated 456 mappings (7 %) that could not be
derived logically.
For example, Severe periodontitis [HP:0000166] maps to
Periodontitis (disorder) [SCTID:41565005] both lexically (at
level 1) and logically (also at level 1). In contrast, Vitamin
B8 deficiency [HP:0100506] maps to Vitamin deficiency
(disorder) [SCTID:85670002] only through lexical mapping,
and Small face [HP:0000274] maps to Dysmorphic facies
(finding) [SCTID:248200007] only through logical mapping.
Of note, the overlapping partial mappings identified
through lexical and logical approaches for a given source
HPO concept are not always the same. For example,
Median cleft lip [HP:0000161] maps to Cleft lip (disorder)
[SCTID:80281008] lexically (at level 1) and to Congenital
anomaly of mouth (disorder) [SCTID:128334002] logically
(at level 3). As suggested by its closest proximity, the
lexical mapping is more meaningful. One strategy for
selecting between lexical and logical mappings for a given
HPO concept when the mappings are different would be
to give precedence to the mapping with the lowest level. A
detailed comparison of the levels at which the map-
pings occur between the lexical and logical approaches
is presented in Table 4.
Fig. 5 Partial logical mappings between HPO and SNOMED CT
Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 10 of 13
Qualitative aspects
As mentioned earlier, the quality of the partial logical
mappings tends to be higher than that of the partial
lexical mappings (71 % vs. 62 % for ontological validity
and 67 % vs. 49 % for clinical relevance).
Failure analysis
We investigated some of the cases where no partial
mappings could be found and present the main reasons
for failure.
Lexical partial mappings
Reasons for failure to derive a partial lexical mapping
include terms with a head noun outside the domain of
disorders, complex lexico-syntactic patterns not proc-
essed in this investigation, and complex lexical items
identified as HEAD.
 Head noun outside the domain of disorders. For
example, the HPO concept Hypoplastic sacrum
[HP:0004590] is demodified to sacrum, for which
cannot find a mapping to phenotypes in
SNOMED CT, because sacrum is an anatomical
entity. (In previous work, we have addressed this
issue through the creation of post-coordinated
expression [4].)
 Complex lexico-syntactic patterns. For example,
Complete duplication of the proximal phalanx of
the 5th toe [HP:0100415] has for lexico-syntactic
pattern [MOD-HEAD][PREP-DET-MOD-HEAD]
[PREP-DET-MOD-HEAD]. We ignored noun
phrases with multiple prepositional attachments
from our processing and were therefore unable
to identify a partial lexical mapping for this
concept.
 Complex lexical items identified as HEAD. For
example, Pyruvate dehydrogenase complex deficiency
[HP:0002928] is a complex lexical item, which
prevents it from being demodified.
Logical partial mappings
The main reasons for failure to derive a partial logical
mapping is that none of the ancestors of the HPO
source concept have an equivalent mapping to
SNOMED CT through the UMLS. For example, none of
the 10 ancestors of the HPO concept Absent sternal ossi-
fication [HP:0006628] has an equivalence to SNOMED
CT. The limitations of the UMLS as a source of eq-
uivalence mappings between HPO and SNOMED CT
directly impact our partial logical mapping approach,
albeit in a relatively small way, since a partial logical
mapping can be derived for 82 % of the HPO concepts
(for which there is no equivalent mapping).
Impact of implicit congenitality on the quality of the
partial mappings
Congenitality tends to be expressed explicitly in SNOMED
CT concepts, while it is often implicit in HPO concepts.
For example, the HPO concept Renal hypoplasia
[HP:0000089] is equivalent to Congenital hypoplasia
of kidney (disorder) [SCTID:32659003] in SNOMED
CT according to the UMLS. Here, congenitality is implied
in HPO, because hypoplasia is always a congenital condi-
tion. In other cases, however, an HPO concept without
mention of congenitality is mapped to a SNOMED CT
concept with explicit mention of congenitality through the
UMLS. For example, according to the UMLS, Abnormal-
ity of the mouth [HP:0000153] is equivalent to Congenital
anomaly of mouth (disorder) [SCTID:128334002], which
is not always true since not all mouth conditions
occur congenitally. The conflation between congenital
and non-congenital (or not-always-congenital) entities
within the same UMLS concept can lead to incorrect
partial mappings.
Partial lexical mappings
As mentioned earlier, the mapping of Abnormality of the
paranasal sinuses [HP:0000245] to Congenital malforma-
tion (disorder) [SCTID:276654001] is inaccurate, because
Table 4 Comparison of the level of the partial mappings in the lexical and logical approaches
Partial logical mapping No logical
level 1 level 2 level 3 level 4 level 5 level? 6 mapping total
level 1 1041 314 135 77 36 29 263 1895
Partial level 2 137 55 58 23 9 3 163 448
lexical level 3 13 20 8 17 5 0 34 97
mappings level 4 4 6 2 3 0 0 5 20
level 5 0 0 1 1 0 0 0 2
level? 6 0 0 0 2 0 0 0 2
No lexical mapping 911 729 809 831 582 148 884 4894
total 2106 1124 1013 954 632 180 1349 7358
Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 11 of 13
Sinusitis [HP:0000246], a subclass of Abnormality of the
paranasal sinuses, is not necessarily of congenital origin.
The problem here is the equivalence provided by the
UMLS between anomaly and Congenital malformation
(disorder) through the UMLS concept Congenital Abnor-
mality [UMLS:C0000768].
Partial logical mappings
The mapping of Abnormal calcification of the carpal
bones [HP:0009164] to Congenital anomaly of the hand
(disorder) [SCTID:34111000] is inaccurate, because some
calcifications can be acquired. The problem here is the
equivalence provided by the UMLS between Abnormality
of the hand, an ancestor of Abnormal calcification of the
carpal bones, and Congenital anomaly of the hand
(disorder) [SCTID:34111000] through the UMLS concept
Congenital Hand Deformities [UMLS:C0018566].
Impact
The mapping of HPO concepts without mention of
congenitality to SNOMED CT concepts with mention of
congenitality is the main raison for creating partial
logical mappings that are not ontologically valid. Since
many HPO terms are demodified to the head noun
Abnormality (mapped to Congenital malformation), this
issue also has a profound impact on the quality of the
partial lexical mappings. Furthermore, we estimated that
the partial mappings would gain in clinical relevance
(+11 % for partial lexical mappings and +2 % for
partial logical mappings) if the issue of congenitality
was addressed. This issue is of particular importance
at a time when HPO intends to represent phe-
notypes not only for genetic diseases, but also for
common diseases [26].
Limitations and future work
One of the limitations of this work is that the mappings
were investigated from the perspective of the source
(HPO) rather than the target (SNOMED CT). More
specifically, we report results in terms of proportion of
the HPO concepts mapped to SNOMED CT without
investigating the SNOMED CT concepts mapped to or
the mappings themselves (i.e., the HPO-SNOMED CT
concept pairs). Investigating the perspective of the target
was beyond the scope of this work, but should be the
object of future research.
Our partial lexical mapping approach only considers a
limited number of lexico-syntactic profiles for the gener-
ation of demodified terms. Moreover, some of the lexical
items characterized as HEAD by our shallow parser
actually correspond to complex items, some of which
could be amenable to demodification (e.g., cortical
cataract from the HPO concept Posterior cortical cata-
ract [HP:0010924] is identified as a single lexical item,
but could be decomposed into the modifier cortical and
the head noun cataract). However, further refinement of
the lexical processes is unlikely to dramatically increase
the performance of the partial lexical mapping approach.
The equivalence between HPO and SNOMED CT
concepts derived through the UMLS is a key component
of our partial logical approach. While SNOMED CT is
fully integrated in the UMLS, HPO was not at the time
of this investigation and we had to rely on the lexical
tools provided by the UMLS to derive this mapping.
HPO is now integrated in the UMLS (as of version
2015AB) and this curated mapping is likely to provide
better equivalences between HPO and SNOMED CT
concepts, which will be highly beneficial to our partial
logical mapping approach.
Conclusions
Through complete and partial mappings, 92 % of the
10,454 HPO concepts can be mapped to SNOMED
CT (30 % complete and 62 % partial). Equivalence
mappings between HPO and SNOMED CT allow for
interoperability between data described using these
two systems. However, due to differences in focus
and granularity, equivalence is only possible for 30 %
of HPO classes. In the remaining cases, partial map-
pings provide a next-best approach for traversing
between the two systems. Both lexical and logical
mapping techniques produce mappings that cannot
be generated by the other technique, suggested that
the two techniques are complementary to each other. The
clinical relevance of the partial mappings (for a cohort
selection use case) is 49 % for lexical mappings and 67 %
for logical mappings. Finally, this work demonstrates
interesting properties (both lexical and logical) of HPO
and SNOMED CT and illustrates some limitations of
mapping through UMLS.
Abbreviations
HPO: Human Phenotype Ontology; UMLS: Unified Medical Language System;
EHR: Electronic health records; LOD: Linked open data.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
The two authors conceived and designed the study, performed the
experiments and analyzed and interpreted the results. All authors
contributed to the redaction of the manuscript. All authors read and
approved the final manuscript.
Acknowledgments
This work was supported in part by the Intramural Research Program of the
NIH, National Library of Medicine, the French Gynecology and Obstetrics
Association (Collège National des Gynécologues et Obstétriciens Français), and
the Philippe Foundation.
Received: 4 November 2015 Accepted: 2 February 2016
Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 12 of 13
RESEARCH Open Access
The environment ontology in 2016:
bridging domains with increased scope,
semantic density, and interoperation
Pier Luigi Buttigieg1*, Evangelos Pafilis2, Suzanna E. Lewis3, Mark P. Schildhauer4, Ramona L. Walls5
and Christopher J. Mungall3
Abstract
Background: The Environment Ontology (ENVO; http://www.environmentontology.org/), first described in 2013, is a
resource and research target for the semantically controlled description of environmental entities. The ontology's
initial aim was the representation of the biomes, environmental features, and environmental materials pertinent to
genomic and microbiome-related investigations. However, the need for environmental semantics is common to a
multitude of fields, and ENVO's use has steadily grown since its initial description. We have thus expanded,
enhanced, and generalised the ontology to support its increasingly diverse applications.
Methods: We have updated our development suite to promote expressivity, consistency, and speed: we now
develop ENVO in the Web Ontology Language (OWL) and employ templating methods to accelerate class creation.
We have also taken steps to better align ENVO with the Open Biological and Biomedical Ontologies (OBO) Foundry
principles and interoperate with existing OBO ontologies. Further, we applied text-mining approaches to extract
habitat information from the Encyclopedia of Life and automatically create experimental habitat classes within ENVO.
Results: Relative to its state in 2013, ENVO's content, scope, and implementation have been enhanced and much
of its existing content revised for improved semantic representation. ENVO now offers representations of habitats,
environmental processes, anthropogenic environments, and entities relevant to environmental health initiatives
and the global Sustainable Development Agenda for 2030. Several branches of ENVO have been used to incubate
and seed new ontologies in previously unrepresented domains such as food and agronomy. The current release
version of the ontology, in OWL format, is available at http://purl.obolibrary.org/obo/envo.owl.
Conclusions: ENVO has been shaped into an ontology which bridges multiple domains including biomedicine,
natural and anthropogenic ecology, omics, and socioeconomic development. Through continued interactions
with our users and partners, particularly those performing data archiving and sythesis, we anticipate that ENVOs
growth will accelerate in 2017. As always, we invite further contributions and collaboration to advance the
semantic representation of the environment, ranging from geographic features and environmental materials,
across habitats and ecosystems, to everyday objects in household settings.
Keywords: Environmental semantics, Habitat, Ecosystem, Ontology, Anthropogenic environment, Indoor
environment, Sustainable development
* Correspondence: pier.buttigieg@awi.de
1Alfred Wegener Institut, Helmholtz Zentrum für Polar- und
Meeresforschung, Am Handelshafen 12, 27570 Bremerhaven, Germany
Full list of author information is available at the end of the article
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Buttigieg et al. Journal of Biomedical Semantics  (2016) 7:57 
DOI 10.1186/s13326-016-0097-6
Background
An environment includes the natural or anthropogenic
systems which can surround a living or non-living entity.
This broad definition encompasses an enormous diver-
sity of entities and scales, thus presenting numerous
challenges for constructing ontologies and standards.
Previously, we described the Environment Ontology
(ENVO; [1]), a community-driven project which repre-
sents environmental entities including biomes, environ-
mental features, and environmental materials. At that
time, our focus was primarily on representing the envi-
ronments associated with metagenomic samples: our
goal was to provide a vocabulary with which to charac-
terise sequenced environmental samples, together with
an ontological structure to facilitate search, advanced
querying, and inference in support of the aims of the
Genomics Standards Consortium (GSC; [2]). This pre-
vious version of the ontology contained a variety of
classes for describing a sample along three primary
axes: the biome or ecosystem within which an entity of
interest (usually an organism or community) is embed-
ded; the environmental features that are in the vicinity
of and have a strong causal influence on the entity; and
the environmental material that is the substance sur-
rounding or partially surrounding the entity. Although
the use case is primarily microbial, the approach can
encompass larger organisms  for example, a killer
whale in a neritic epipelagic zone biome, present in an
ecosystem defined by a marine subtidal rocky reef, and
surrounded by coastal water. We also described the
dynamic nature of the ontology, and the process for
community extension of the ontology.
New challenges
In the time since our initial publication, we have oriented
ENVOs development to a suite of emerging challenges
extending our original and core case of describing samples
of environmental and biomedical importance (e.g. [35]).
On the one hand, sequencing projects are targeting ever
more diverse environments such as city transit systems
[6] and also phenomena such as soil compaction in for-
est ecosystems [7]. This has driven new requests from
adopters such as MG-RAST [8] and the iMicrobe pro-
ject (http://imicrobe.us/) which has annotated some
2813 environmental metagenomic samples with ENVO
terms (see http://data.imicrobe.us/ and [9]). On the other
hand, we have encountered a number of entirely new use
cases in areas such as ecology and biodiversity science.
Both of these fronts have, at times, required the expan-
sion of existing branches in the ontology and, at others,
required the creation of either entirely new branches,
or the refactoring of existing branches. This increase in
scope also presented challenges and opportunities in
terms of how the ontology should be interwoven with
other ontologies in the OBO Foundry and Library
(http://obofoundry.org/) [10].
In this update, we describe how we have extended
and in some cases broken apart ENVO to meet the
above challenges. We also describe how these efforts
have connected ENVO to a broader movement to
further extend OBO-aligned semantics into the realm
of ecology and biodiversity science [1113], centred on
co-development with ecologically themed ontologies
such as the Population and Community Ontology
(PCO) and Bio-collections Ontology (BCO) [14]. These
efforts have been catalysed by several workshops and
meetings e.g. [4] which have greatly supported ENVO
in contending with entities such as habitats, environ-
mental processes, and environmental dispositions while
orienting its content to address issues of global
importance.
Expanding usage and coordination
Along with its scope, the use of ENVO is also growing
and supporting data annotation, searching of datasets,
and the mobilisation of sample data. For example, the
journal Scientific Data (Nature Publishing Group; ISSN
2052?4463) now uses ENVO classes to annotate its Data
Descriptor articles [15], allowing articles to be browsed with
faceted interfaces (http://scientificdata.isa-explorer.org), and
PANGAEA, a data publisher for Earth and environmen-
tal science, is continuing to use the ontology to enrich
its metadata and data archives (http://www.pangaea.de).
Parallel efforts such as those convened by the Global
Biodiversity Information Facility (GBIF) have moved to
enhance the widely used Darwin Core (DwC; http://
rs.tdwg.org/dwc/; [16]) glossary by using ENVO in habi-
tat descriptions [17]. Other users have begun to explore
ENVOs potential in data analysis [18] and in contrib-
uting to semantically aware biodiversity informatics
(e.g. [19, 20]). Further, synthesis centres such as the
National Centre for Ecological Analysis and Synthesis
(NCEAS; Santa Barbara, USA; http://nceas.ucsb.edu/) and
the Centre de synthèse et danalyse sur la biodiversité
(CESAB; Aix-en-Provence, France; http://cesab.org/) have
engaged with us to explore further possibilities for usage
and provide advice on coordination and community needs
linked to projects such as the Data Observation Network
for Earth (DataONE; www.dataone.org). Indeed, it is the
diverse needs of these communities, as well as those of
more recent partners (see Results and Discussion), which
have compelled ENVO to develop with generalisability
and versatility in mind, as is appropriate for a domain
or reference ontology. Representations of microscale
environments co-exist and interoperate with those of
planetary-scale systems and are being further harmo-
nised as the ontology grows in scope.
Buttigieg et al. Journal of Biomedical Semantics  (2016) 7:57 Page 2 of 12
An overview of this update
Below, we describe the updates made to improve
ENVOs ability to maintain coherence while meeting the
needs of its diversifying user base and implementation
partners. Our Methods section describes key technical
updates while our Results focus on content-level changes.
The first section of our results describes ENVOs in-
creased expressivity, acquired through transitioning to a
more powerful development language. The second section
describes the addition of processes to ENVOs content,
which has widened ENVOs range of application and
enriched the relationships between its classes. Building on
its updated expressivity, the third section describes how
ENVO distinguishes between environments and habitats
and how thousands of habitats linked to species de-
scriptions have been represented using text-mining ap-
proaches. Departing from the natural setting, the fourth
and fifth sections describes the increased efforts made
in representing anthropogenic or anthropised environ-
ments and how these changes relate to the monitoring
of policy objectives and global development. Finally, we
comment on how ENVO intends to handle its rapidly
growing scope while maintaining expert-guided repre-
sentations. From a wider perspective, we believe these up-
dates represent multi-stakeholder convergence on the goal
of integrating data through environmental contextualisa-
tion across the biosphere.
Technical note
As a technical note, the reader is advised that OBO Library
ontologies are assigned unique acronyms or initialisations,
such as BFO or ENVO, that serve as shorthand identifiers
for that ontology. In the following text, ontology classes
(or, synonymously, terms), are written in italics and are
taken from ENVO unless otherwise marked through
the provision of an appropriate ontology prefix, as in
PATO:laminar. The unique shorthand fragment of each
terms Permanent Uniform Resource Locator (PURL), e.g.
ENVO_00002297 for environmental feature, will be
included on first mention of any class, in which case
the redundant namespace prefix shall be omitted. Full
PURLs are of the form: http://purl.obolibrary.org/obo/
ENVO_00002297, and are resolved to OWL as well as
to human-readable web pages via OntoBee [21].
Methods
The development of ENVO is now conducted using
Protégé (http://protege.stanford.edu), rather than OBO
Edit [22], allowing more expressivity through the Web
Ontology Language (OWL). For global interoperability, we
preferentially use relations from the Relations Ontology
(RO; [23]) and the Basic Formal Ontology (BFO; [24]) to
connect these classes. Additional relations are present, but
will be incorporated into RO pending an open discussion
and vetting process. The ontology is still released in both
OBO and OWL formats and a number of custom exports
have been made upon request (e.g. flat, character delimited
formats suitable for import into relational databases, table-
oriented analysis software, or network visualisation and
analysis solutions). We continue to maintain obsoleted
terms and link them to their replacements (where
available) in a machine readable way to support auto-
mated updating of user implementations.
As with most other OBO Library ontologies, ENVOs re-
pository has been moved to its own GitHub organization
(https://github.com/EnvironmentOntology). This change
does not affect downstream users who consume the
ontology using standard permanent URLs; however, it
does provide a better mechanism for stakeholders to
become involved with the development of the ontology
through, for example, an improved issue tracker [25].
Further, it allows easier reference to previous versions
of the ontology for backwards compatibility.
OWLTools (https://github.com/owlcollab/owltools) and
ROBOT [26] (https://github.com/ontodev/robot/) are
currently being used for release management, and for the
import of classes from other OBO Foundry and Library
ontologies in alignment with the Minimum Information
to Reference an External Ontology Term (MIREOT; [27])
guidelines. These import procedures are primarily used to
express environments that are dependent on entities
defined outside of ENVO. For example, environments de-
fined by anatomical entities and chemical entities are
expressed using classes from ontologies such as the Uber
Anatomy Ontology (UBERON; [28]) and the Chemical
Entities of Biological Interest Ontology (CHEBI; [29])
to prevent duplicating existing, well-developed seman-
tics relevant to terms such as xylene contaminated
soil [ENVO_00002146] and axilla skin environment
[ENVO_08000001].
We have created a TermGenie instance (http://envo.
termgenie.org/) [30] that allows for web-based addition
of new terms that conform to a pre-defined template, or
following a free-form pattern. We are also documenting
our design patterns (ODPs) using the emerging dead
simple owl design patterns standard (https://github.com/
dosumis/dead_simple_owl_design_patterns) and are using
these patterns to generate small portions of the ontology.
Further, we have begun to use the results of text-mining
approaches, noted in [1], discussed below, and docu-
mented by Pafilis et al. [31], to automatically generate
experimental classes which, upon curation, can be inte-
grated into the core ontology.
Results and discussion
ENVO now includes some 2159 classes primarily repre-
senting biomes, geographic features, and environmental
materials, along with 18,791 axioms (logical statements)
Buttigieg et al. Journal of Biomedical Semantics  (2016) 7:57 Page 3 of 12
defining, interconnecting, and interrelating them. This
contrasts with 1644 classes and 14,542 axioms present
when ENVOs original description was published. The
growth of the ontology was primarily driven by the needs
of the omics community using the Minimal Information
about any (x) Sequence (MIxS; [32]) checklist and its
extensions such as MIxS for the Built Environment
(MIxS-BE; [33]). These needs were communicated through
individual requests for new classes and requests coordi-
nated through, for example, curation efforts of organisa-
tions such as the European Nucleotide Archive (ENA)
(e.g. [34]). More currently, the bulk of the changes to
ENVOs content have been motivated by the ontologys
growing adoption and engagement with new user com-
munities as well as the need to integrate their varying
approaches to describing environments.
Increases in semantic density and expressivity
As we are now developing ENVO using the expressivity
of OWL (see Methods), we have increased the variety and
density of linkages between many of ENVOs classes as
well as the detail in their logical definitions. This increased
semantic density offers more flexibility when using the
ontology for querying, inference, and semantically en-
hanced analysis. To illustrate the increased expressivity,
an oasis [ENVO_01001304] (Fig. 1) is represented as a
subclass of vegetated area [ENVO_01001305] which has,
as a part, some spring [ENVO_00000027] and is partially
surrounded by a portion of either rock [ENVO_00001995],
sand [ENVO_01000017], or soil [ENVO_00001998]
which, itself, is arid [ENVO_01000230]. This repre-
sentation has several facets which involve type hier-
archy (i.e. class and subclass relationships), parthood,
and adjacency, and which define key properties of
one or more of the classes involved. Practically, users
and machine agents can now identify an oasis (and any
data that has been associated with that class) by any one
of these routes such as querying for a vegetated area that
is surrounded by arid environmental materials or which
has a spring as a necessary part.
The increased axiomatisation described above has also
improved our ability to represent semantically problematic
classes such as hydrographic feature [ENVO_00000012]
and marine pelagic feature [ENVO_01000044]. The is-
sues with these somewhat artificial or convenience group-
ings are discussed in [1]; in brief, their membership is
dictated more through convention than physical or forma-
tive similarities, often adding ambiguity and confounding
search and inference. For example, one is correct in
asserting that a lighthouse, a lake, and a coral reef are
hydrographic features due to the nautical conventions
of hydrography; however, these entities are substantially
different from one another and much better distributed
in hierarchies true to their physical attributes and/or
the processes of their formation. With ENVOs greater
semantic flexibility, the varied criteria for including a
class in one of these convenience groupings can be more
precisely defined and classes which satisfy these criteria
can be interlinked through automated inference: the action
of reasoning software which can use logical statements to
infer relationships and hierarchies which were not
asserted by a human. For example, any class which has
been asserted to be adjacent to some water body or par-
tially surrounded by some water will be inferred to be a
subclass of hydrographic feature. Similarly, marine pela-
gic feature would be populated by any entity which has
been asserted to be part of  some marine water body or
'composed primarily of' some sea water. Similarly, many
subclasses of environmental material [ENVO_00010483]
are now placed in inferred hierarchies using various sub-
classes of quality [PATO_0000001] such as quality of a
solid [PATO_0001546], quality of a gas [PATO_0001547],
and quality of a liquid [PATO_0001548]. Such assertions
provide a way to construct and populate classes like
Fig. 1 Illustrative example of ENVOs improved semantic expression with OWL axioms. An oasis is a vegetated area which has, as a part, a spring
and is surrounded by an arid portion of soil, rock, or sand
Buttigieg et al. Journal of Biomedical Semantics  (2016) 7:57 Page 4 of 12
solid or liquid through inference, avoiding asserted
multiple inheritance while simultaneously preserving clear
representations based on multiple criteria.
As illustrated above, the flexibility that comes with in-
creased axiomatisation is an important step in support-
ing multiple, varying classifications of environmental
entities in an integrated fashion. We will leverage these
capacities to disentangle the semantics of environmental
entities across user groups which use different defini-
tions for syntactically similar terms. The hundreds of of-
ficial and operational definitions of forest [35], which
can influence critical decisions in conservation and sus-
tainable land use [36, 37], will be one of our first targets
in this process. We anticipate that ENVO will host mul-
tiple classes representing the different entities typically
gathered under one label, using synonym lists and cross-
Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 
DOI 10.1186/s13326-016-0106-9
RESEARCH Open Access
TNM-O: ontology support for staging of
malignant tumours
Martin Boeker1* , Fábio França1,2, Peter Bronsert3 and Stefan Schulz4
Abstract
Background: Objectives of this work are to (1) present an ontological framework for the TNM classification system,
(2) exemplify this framework by an ontology for colon and rectum tumours, and (3) evaluate this ontology by
assigning TNM classes to real world pathology data.
Methods: The TNM ontology uses the Foundational Model of Anatomy for anatomical entities and BioTopLite 2 as a
domain top-level ontology. General rules for the TNM classification system and the specific TNM classification for
colorectal tumours were axiomatised in description logic. Case-based information was collected from tumour
documentation practice in the Comprehensive Cancer Centre of a large university hospital. Based on the ontology, a
module was developed that classifies pathology data.
Results: TNM was represented as an information artefact, which consists of single representational units. Corresponding
to every representational unit, tumours and tumour aggregates were defined. Tumour aggregates consist of the
primary tumour and, if existing, of infiltrated regional lymph nodes and distant metastases. TNM codes depend on the
location and certain qualities of the primary tumour (T), the infiltrated regional lymph nodes (N) and the existence of
distant metastases (M). Tumour data from clinical and pathological documentation were successfully classified with
the ontology.
Conclusion: A first version of the TNM Ontology represents the TNM system for the description of the anatomical
extent of malignant tumours. The present work demonstrates its representational power and completeness as well as
its applicability for classification of instance data.
Keywords: TNM classification, Tumour classification, Tumour staging, Anatomical extent, TNM ontology,
Description logic
Background
Clinical and pathological staging of malignant tumours is
one of the most important procedures in the diagnosis
of cancer for prognosis assessment and treatment plan-
ning. The staging procedure compiles several clinical and
pathological parameters such as the location and the size
of the primary tumour, the location and the number of
the infiltrated regional lymph nodes, and the existence of
distant metastases.
A prerequisite for an evidence-based cancer treat-
ment is a correct and unambiguous cancer diagnosis.
*Correspondence: martin.boeker@uniklinik-freiburg.de
1Institute for Medical Biometry and Statistics, Medical Center  University of
Freiburg, Faculty of Medicine, Stefan-Meier-Str. 26, 79104 Freiburg i. Br.,
Germany
Full list of author information is available at the end of the article
Interdisciplinary expert groups, e.g. from clinical
medicine, imaging, and pathology, have been working in
close cooperation to establish criteria for precise tumour
diagnoses [1]. One of the most challenging tasks in
clinical oncology is to correctly classify and code clinical
findings, using a multitude of available coding systems.
By far, the most important coding system for tumour
staging is the Tumour-Node-Metastasis (TNM) classifica-
tion [2] formalignant tumours, published by the Union for
International Cancer Control (UICC)1. Besides a growing
number of reliable biomarkers, TNM classification and
staging are the most important information for the ther-
apy planning for patients with colorectal cancer [35] and
other solid tumours (e.g. cancer of the head and neck
[6] or breast tumours [7]), except cancers of the central
© The Author(s). 2016 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 2 of 11
nervous system. In addition, the TNM classification
system is important in cancer research for a cor-
rect description and classification of the anatomical
extent of a given tumour. This is not only rele-
vant for cancer epidemiology but also in fundamen-
tal tumour research (e.g. the dataset descriptions for
researchers of the Surveillance, Epidemiology, and End
Results Program (SEER) of the National Cancer Insti-
tute2 and predefined results using TNM stratified
data3).
The TNM coding procedure requires advanced skills,
encompassing both experience in tumour documentation
and in-depth domain knowledge. The criteria for classifi-
cation of the different primary tumour locations differ to
the same extent as the underlying diseases. As a conse-
quence, even expert coders and physicians for one organ
systemmight encounter difficulties in the correct applica-
tion or interpretation of TNM in a different organ system.
Several combinations of tumour findings are difficult to
encode due to ambiguous or overlapping criteria (non-
disjoint definitions) or non-exhaustive definitions, which
often result in cases where no TNM code or more than
one TNM code is applicable to a given tumour state. A
variety of problems with TNM coding has been described
for different tumour locations. Main issues that arise in
the practice of TNM coding derive from overly com-
plex definitions of the underlying medical situation, which
then result in interpretation problems even for experts
[810]. The required in-depth knowledge of the domain,
together with specific competences needed for TNM cod-
ing, result in poor coding completeness and quality, espe-
cially with the clinical staging in outpatients [11, 12].
Given the importance of TNM staging for the individual
patient, deviation rates of about 20% for clinical coding
and 10% for pathological coding can be interpreted as
very high [13].
The complexity of TNM is mainly due to the develop-
ment of the TNM classification as an evolutionary pro-
cess [14], which has been constantly incorporating huge
amount of new scientific insights in tumour prognosis and
the dependency of therapeutic effects on tumour stage.
Controlled by medical experts, TNMs underlying struc-
ture has become more and more complex over the years.
Experts in different fields of oncology have demanded a
change in TNM maintenance, to address the increasing
complexity, the detachment from clinical practice, and the
resources needed for documentation [15, 16]. Therefore,
standardisation of tumour classification and staging is an
urgent requirement for improvement of tumour docu-
mentation in primary documentation, clinical studies and
cancer registries [11, 1720].
Despite its importance and formal precision, to the
knowledge of the authors, no formal representation
of the complete TNM is available so far. Formal,
i.e. computable representations would have several advan-
tages over TNMs current publication as a textbook.
An initial attempt to represent staging of lung tumours
and glioma tumours was not continued [21, 22]. More
recently, a description logics based (DL) approach was
presented [23].
One of the major requirements a formal representa-
tion of TNM could satisfy is the automatic classifica-
tion of instance data obtained from clinical databases
or mined from textual reports [2426]. Consecutively,
instance data classification could inform higher order pro-
cesses such as clinical documentation systems. Instance
data on pathological or clinical conditions are collected
during routine health care processes in pathology or other
clinical information systems. Users could be supported
by automatic encoding of instance data to TNM in real
time or in spatially and temporally disseminated settings
(e.g. in tumour documentation). For intelligent documen-
tation systems in clinical oncology and pathology, a TNM
ontology could be deployed as part of the knowledge
base supporting the coding of tumour-related findings
and the interpretation of TNM codes. In such systems a
TNM ontology could enable automated reasoning based
in description logics, which would timely detect logical
inconsistencies and complexity related coding problems
in databases and textual reports. In integrated clinical
decisions support systems (DSS) TNM could be deployed
to inform users about guideline-conformant treatment
[27]. A further advantage of a formal approach would be
the enhanced support for development and refinement of
TNM.With a taxonomic backbone and axiomatic descrip-
tions, the current complex natural language descriptions
could be converted into computable structures. This
would help decompose the descriptions into all their
defining criteria, which in turn could facilitate the detec-
tion of coding errors, inconsistencies, and ambiguities in
definitions [28, 29].
Description logics is the method of choice for a for-
malization of TNM [30]. Advanced retrieval and query-
ing tools would be additional benefits that come with
a logical representation following principles of Applied
Ontology [31]. For these use cases, a formalised TNM
version could constitute a unified source on which a vari-
ety of clinical documentation and analysis tools could
be based. In addition, such a resource could be mapped
to other DL-based clinical ontologies, especially to
SNOMED CT.
With this work, we propose to close the gap of a miss-
ing formal representation by outlining and prototyping
the TNM ontology (TNM-O). Following up on initial
attempts in the breast cancer domain [32], the objectives
of this work are (1) to present an ontological framework
for the TNM classification system, (2) to implement a
TNM ontology, describing colon and rectum tumours
Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 3 of 11
based on this framework, and (3) to evaluate this ontology
using a tool for classifying pathology data.
The TNM classification
The canonical description of the TNM classification
based on the anatomic extent of disease (EOD) is pub-
lished by the UICC and the AJCC [2, 33]. The UICC
published the first edition of the TNM coding system
in 1968. Since then, the system has undergone sev-
eral revisions, with the 7th edition published in 2009.
The AJCC has recently announced the release of the
8th edition of the TNM classification for the beginning
of 20174. The part of the new version for lung can-
cer is already in use with its important changes satis-
fying urgent medical requirements [34]. The objectives
of the TNM coding system are six-fold. It supports
treatment planning, prediction of outcomes (prognosis),
evaluation of treatment results, exchange of information
between different participants in health care processes,
continuing research in malignant diseases, and cancer
control [2, 14].
The core TNM classification uses three descriptors: T
(tumour), N (metastasis in regional lymph nodes), and M
(distant metastasis). The extent of the disease is indicated
by integer values resp. character modifiers: TX (Tumour
cannot be assessed), T0 (No evidence of primary tumour),
T1-4 (increasing size or local extent), Tis (Carcinoma in
situ); NX (Regional lymph nodes cannot be assessed), N0
(No regional lymph node metastasis), N1-3 (Increasing
involvement of regional lymph nodes); M0 (No distant
metastasis), M1 (Distant metastasis). For some entities
further subdivisions of the categories are possible indi-
cated by lower case characters (e.g. N2a and N2b).
The specific medical denotation for the different
descriptors is dependent on the localisation of the tumour,
designated by the ICD-O localisation code5. It is not
possible to list all single regions addressed by the TNM
classification here (for a current list see [2]). However, the
TNM classification is not available for all body regions
or systemic malignancies (e.g. C70-C72 Tumours of the
Central Nervous System, C33 Trachea, C42, and C77
Tumours of haematopoietic and lymphoid tissues). For
most of these malignancies the anatomical extent is either
not determinable (systemic malignancies e.g. leukaemia)
or the tumours have no metastasis (e.g. CNS tumours).
The World Health Organisation (WHO) has published
the 3rd edition of International Classification of Diseases
for Oncology (ICD-O) in 2003. As an extension of the
International Classification of Diseases (ICD-10) [35] for
tumour diseases, the ICD-O is a dual classification system
for the tumour morphology and the tumour localisation
[36]. ICD-O is widely used in clinical medicine, tumour
documentation, and research to encode tumour morphol-
ogy and tumour localisation.
With an additional modifier, the TNM classification
is divided into the pre-treatment clinical (indicated as
cTNM) and post-surgical pathological (pTNM) classifica-
tion. pTNM codes can only be assigned to the disease after
pathological assessment following surgery and is the most
important diagnostic item for following (adjuvant) radio-
or chemotherapy or their combination. The results from
the clinical assessment have to be accurately discerned
from the pathological assessment due to their different
meanings and evidence levels.
Besides the already complex semantics of the main
numeric TNM codes, a series of additional symbols exists,
which might have largely different meanings in the differ-
ent tumour locations. Prefixes, suffixes, and certainty fac-
tors increase the confusion, e.g. for carcinoma in situ the
suffix is has to be used (Tis). As TNMallows putting an
X wherever the information about the clinical or patho-
logical situation is incomplete or inaccurate, incomplete
code assignments become widespread (e.g. MX for no
statement on metastases possible). In this work only the
classes with the descriptors T, N, andMwith themodifiers
c and p are represented (for a full list see Table 1).
Table 1 TNM classification descriptors and additional modifiers
Descriptor Values Meaning
T 0-4, is, X Extent of the primary tumour
N 0-3, X Extent of metastasis in regional
lymph nodes
M 0-1 Existence of distant metastasis
Prefix to T, N, M p, c Clinical (pre-therapeutical) or
pathological (post-surgical
assessment)
Suffix to pNn (mi) Micrometastasis (< 0.2 cm)
Suffix to pNn (sn) Sentinel lymph node metastasis
Suffix to pN0 or pM0 (i+), (mol+) Isolated tumour cells, positive
findings
G X, 1-4 Histopathological grading
Suffix to T (m) Multiple primary tumours at a
single side
Prefix to c/ p y Assessment during multimodal
therapy
Prefix to c/ p r Recurrent tumour
Prefix to c/ p a Assessment during autopsy
L X, 0-1 Lymphatic invasion
V X, 0-2 Venous invasion
Pn X, 0-1 Perineural invasion
C 1-5 Validity of the assessment, can
follow each of T, N, M
R X, 0-2 Residual tumour
Depending on the organ of the primary tumour, T, N, and M values can be further
subdivided into levels a-c, e.g. N1a-c, N2a-c, and M1a-b in colorectal tumours
Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 4 of 11
pTNM codes are grouped into stages which are based
on the prognosis of the patients. Stages are designated by
the roman numerals I-IV and further subdivided into sub-
stages described by capital letters A-C. TNM staging has
been subject to frequent changes during the history of
the TNM classification, according to scientific and medi-
cal progress [34]. The mapping of the TNM classification
for colon and rectum tumours to stages for version 7 is
provided in [2, 4].
Methods
TNM-O, the TNM ontology presented here, uses the
Foundational Model of Anatomy [37] for anatomical enti-
ties, together with BioTopLite 2 (BTL2) as a domain
top-level ontology [38, 39]. Tailored for the biomedical
domain and based on description logics [30], BTL2 pro-
vides upper-level types both for general categories like
Material object, Process, Information object, Quality etc.,
as well as constraints on all of them, using a set of sixteen
canonical relations, partly derived from the OBO Rela-
tion Ontology (RO) [40]. They constrain each category by
means of a set of general class axioms. BTL2 also contains
other axioms such as relationship chains, existential and
value restrictions. Thus, the building of domain ontolo-
gies under BTL2 heavily constrains the freedom of the
ontology engineer, which is fully intended as it guaran-
tees a higher predictability of the outcomes of the domain
ontology production under BTL2.
The design of BTL2 is top-level agnostic and has been
influenced both by the Basic Formal Ontology (BFO
and BFO2) and the Descriptive Ontology for Linguis-
tic and Social Engineering (DOLCE) which is discussed
in more detail in [39]. BTL2 is especially appropriate
as domain top-level for TNM-O because it provides a
lean, yet exhaustive ontological framework for the repre-
sentation of clinical documentation artefacts. Moreover,
it is fully axiomatised using RO (see above) so that it
is interoperable with other ontologies in the biomedical
domain.
The development of TNM-O is an ongoing process.
For this study, colorectal cancer was chosen as use case
for several reasons. It is the third most common cancer
worldwide and accounts for 9% of all cancer incidence
[41, 42], affecting more than one million humans in 2002.
Treatment of cancer patients and research on causes of
cancer are main goals of worldwide cancer control pro-
grams6. In prior work, the TNM classification for breast
tumours (ICD-O C50) had been formally represented
[32]. The selection of breast and colorectal tumours was
motivated both by their paramount medical importance
and their complexity in TNM, where both follow non-
trivial medical classification principles, especially for the
cN and pN classifications. Demonstrating the appropri-
ateness and feasibility of TNM-O for these two tumour
locations provides a good support for the general applica-
bility of the approach.
The general rules of the TNM classification and the spe-
cific TNM classification for tumours of the colon and the
rectum (ICD-O topography chapters C18  C21, for ICD-
O morphology codes see Table 2) were represented as
described [2, 43].
A classifying tool for individuals (instances) derived
from pathology reports was developed employing the
OWL API (version 4.0.1)7 and the HermIT DL reasoner
(version 1.3.8)8. It classifies breast tumour and colorectal
tumour data based on the corresponding TNM ontolo-
gies. It reads either tabular input data from files or
processes data from manual entry via a graphical user
interface.
The objective of TNM-O is not to re-design an exist-
ing tumour classification into a new system. At the cur-
rent level of development, TNM-O is the result of an
ontological analysis of what has been developed by the
medical community over a long period, followed by its
translation into a formal language, incorporating onto-
logical principles, in order to improve the development,
maintenance, and application of the TNM classification
system.
In the following two sections, we describe (1) the TNM
classification in detail as foundation of what has to be rep-
resented by TNM-O, (2) how the TNM classification arte-
facts are represented by information artefacts of TNM-O,
(3) how these information artefacts are related to the
actual tumour entities, and (4) how the patho-anatomical
reality of tumour disease is constructed in terms of what
is required for the TNM classification.
Design of the TNM-O
The relation between the artefacts of the TNM
classification and the actual tumour diseases is denota-
tional: the T code denotes the extent (size, infiltration)
of the primary tumour, the N code the extent of regional
Table 2 ICD-O 3 morphology codes for tumours of the colon
and the rectum
Type ICO-O 3 morphology
Adenocarcinoma 8140/3
Mucinous adenocarcinoma 8480/3
Signet-ring cell carcinoma 8490/3
Small cell carcinoma 8041/3
Squamous cell carcinoma 8070/3
Adenosquamous carcinoma 8560/3
Medullary carcinoma 8510/3
Undifferentiated carcinoma 8020/3
Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 5 of 11
lymph node metastases, and the M code the existence of
distant metastases. For TNM-O, we adopted an approach
which is compliant with the Information Artefact Ontol-
ogy from the OBO Foundry and recently published
work on the aboutness relation [44, 45]. In TNM-O,
coding artefacts of the TNM classification i.e. the
classes of the classification are represented by subclasses
of btl2:InformationObject as RepresentationalArtefact.
Information reported on individual patients, e.g. as TNM-
codes in patient records are thus individuals of these
classes. Individuals from subclasses of InformationObject
are related by btl2:represents to individuals of classes
about the current disease state (AnatomicalStructure).
The inverse relation is btl2:isRepresentedBy connects
material or processual entities with the respective
TNM-artefact.
As the TNM classification is compositional, the individ-
ual classes of the three descriptors can be independently
combined to a joint code. Classes are only dependent on
the location of the primary tumour and additional mod-
ifiers c or p: e.g. cN1 for colon cancer has a different
meaning than cN1 for breast cancer, and cT1 has a differ-
ent meaning than pT1 for all locations where these codes
are available). This characteristic is conserved in TNM-
O. The class RepresentationalUnit is a superclass of organ
specific classes separated in a clinical and a pathological
branch.
For representing anatomical structure, TNM-O uses
content from the Foundational Model of Anatomy,
restricted to cancer-related anatomy as referred to by
the TNM classification. All primary tumours individuals
and metastases are then related to individuals anatomi-
cal entities by the relation btl2:locatedIn, thus providing
them with an exact topography and extent. The extent of
primary tumours cannot only be described by their local-
isation (i.e. occupying space or infiltrating through layers
of an organ) but can be further characterised by qualities,
e.g. tumour size or infiltration patterns. These qualities
are dependent on the localisation of the primary tumour
and can substantially differ between them.
What makes a lymph node a regional lymph node
depends on its proximity to a primary organ. An axillary
lymph node is a regional lymph node of the breast gland
but not of the colon. For all relevant organs, these regional
lymph node groups are to be defined. Moreover, the for-
malisation of infiltrated regional lymph nodes depends on
the aggregate of a localised primary tumour together with
some metastasis in a regional lymph node of that organ in
which the primary tumour is located. Thus, an infiltrated
axillary lymph node is a regional lymph node metastasis
for a breast tumour, but certainly not for a colon cancer.
Distant metastases are, by definition, those located in a
tumour aggregate that is not a regional lymph node of the
primary tumour.
Classification of pathology data
We computationally classified data describing the extent
of 291 colorectal cancer specimens into TNM, docu-
mented at the Institute of Surgical Pathology, Medical
Center  University of Freiburg using a pathology infor-
mation system. This data were re-coded as RDF-OWL
instance data and classified into classes of TNM-O by
an application based on the OWL API using an OWL
classifier9. Automatic classification was solely based on
axioms defined in the colorectal TNM-O version 7 (TNM-
O_colon_7.owl). The complete set of criteria is shown in
Table 3.
For comparison of the ontology-based TNM classi-
fication with a manual expert TNM classification, the
data were manually classified by a pathologist into TNM
version 7.
Results
TNM-O is designed as a modular system of independent
ontologies under BTL2. For every organ or organ system
based module of the TNM classification system, TNM-O
Table 3 Criteria of TNM version 7 for colorectal cancers. All TNM codes can be inferred from this criteria. The exact wording of the
textual definitions of the TNM in version 7 is diverging. Exact count of infiltrated organs in distant metastasis is omitted
Criterion btl2 superclass Value
Primary tumour extension MaterialObject Epithelium, Submucosa, Lamina propria, Subserosa, Adventitia, VisceralPeritoneum
Primary tumour growth pattern Quality Infiltrative, Confined
Primary tumour epistemology Quality NoAssessment, NoEvidence
Regional LN number Quality Cardinality1, Cardinality2or3, Cardinality4to6, Cardinality7orMore
Regional LN epistemology Quality NoAssessment, NoEvidence
Distant Mx location MaterialObject Peritoneum
Distant Mx/no. of organs Quality Cardinality1, Cardinality2orMore
Distant Mx epistemology Quality NoEvidence
http://cancerstaging.blogspot.de/2005/02/colon-and-rectum.html
Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 6 of 11
provides a set of specific ontologies. The TNM connect-
ing ontology serves as a hub to import BTL2 as well as
the organ and organ system specific TNM ontologies (see
Table 4). With the modular architecture only those mod-
ules are included that are needed by a tumour-specific
application.
The hub TNM Ontology for all tumours can be
downloaded from http://purl.org/tnmo/TNM-O.owl. The
ontologies for breast tumours and colorectal tumours
are named according to Table 4 and can be downloaded
from the same site. They need to be loaded in the hub
ontology.
Without inclusion of BTL2, the TNM hub ontology
has the description logic expressivity of ALC (for a
short introduction to the DL nomenclature see [46]
section Description Logic Nomenclature). It consists of
79 axioms, 38 logical axioms, and 39 classes. It includes
35 subClassOf and one EquivalentTo axioms. Most of
the classes are proxy classes to BTL2. Inclusion of BTL2
changes the DL expressivity to SRI.
The TNM ontology for colorectal tumours has the
description logic expressivity of ALC. For TNM version
7.0 (version 6.0 in brackets), it consists of 366 (357)
axioms, 198 (199) logical axioms, and 161 (149) classes. It
includes 123 (160) subClassOf, 57 (18) EquivalentTo and
18 (18) DisjointClasses axioms.
Representational units in the TNM-Ontology
The representation of the TNM system is decomposed
into the representational units T, N, and M, together
with the location of the primary tumour. Thus, for every
existing code Tn, Nn, and Mn in combination with a spe-
cific organ there exists one TNM-O:RepresentationalUnit
which is an btl2:InformationObject. E.g. every TNM
code for colorectal cancer is represented by a separate
class. Axioms using the relation btl2:isRepresentedBy
introduce possible TNM values for subclasses of Primary-
Tumour or TumourAggregate. This is done by connecting
Table 4 Modular structure of TNM-O. Codes in clinical
documentation and cancer registries follow TNM versions,
because the meaning of codes and stages may change between
versions. The modular structure is designed to include versions
for every available TNM encoded entity (tumour location) so that
the intended meaning is preserved according to the version
used for coding
Name Description
BTL2 Upper domain level ontology
TNM-O TNM-O central connecting ontology
TNM-O_breast_7 TNM-O for breast cancer (TNM version 7) in: [32]
TNM-O_colorectal_6 TNM-O for colorectal cancer (TNM version 6)
TNM-O_colorectal_7 TNM-O for colorectal cancer (TNM version 7)
these values via the universal quantifier ONLY (role
restriction). In all of these cases, the clause or (not Repre-
sentationalUnitInTNMClassification) allows other values
that are not TNM representational units. In the remaining
text, the namespace of the TNM ontology is suppressed
for clarity:
TumourOfColonAndRectumWith7OrMoreMetastaticRegional-
LymphNodes subClassOf
TumourAggregate and
btl2:isRepresentedBy only
(ColonRectumTNM_pN2b or ColonRectumTNM_N2b
or (not RepresentationalUnitInTNMClassification))
Representation of the primary tumour
The primary tumour is represented as PrimaryTumour, a
subclass of MalignantAnatomicalStructure. The tumour char-
acteristics relevant for the representational unit T of the
TNM classification system are represented as location and
qualities of PrimaryTumour. For colorectal tumours, the
exact localization of the tumour in the gut wall, the qual-
ity of the tumour confinement with respect to neighbour-
ing organs (confined or invasive), the quality of the assess-
ment (no assessment, no evidence or carcinoma in situ), are
important:
InvasiveTumourOfSubmucosaOfColonAndRectum
EquivalentTo ColonAndRectumTumour and
(btl2:isBearerOf some (Confinement and
(btl2:projectsOnto some Invasive))) and
(btl2:isIncludedIn some
SubmucosaOfLargeIntestine)
The specific tumour defined as subclass of PrimaryTumour
above is directly related to the corresponding representational
unit as introduced in the section above.
InvasiveTumourOfSubmucosaOfColonAndRectum
subClassOf
btl2:isRepresentedBy some
(ColonRectumTNM_T1 or
ColonRectumTNM_pT1) and
btl2:isRepresentedBy only
(ColonRectumTNM_T1 or
ColonRectumTNM_pT1 or
(not RepresentationalUnitInTNMClassification))
Representation of regional lymph nodes
The most complex part of the TNM classification of many pri-
mary tumour locations is the interpretation of the axisN, which
describes the extent of infiltration of regional lymph nodes by
the primary tumour. The anatomy of lymph nodes draining the
colon and rectum was modelled according to clinical anatom-
ical conventions. Metastatic regional lymph nodes can exactly
be located by the exact subclass of infiltrated regional lymph
node:
MetastaticLymphNodeOfColonAndRectumTumour
EquivalentTo LymphNode and
(btl2:hasPart some
MetastasisOfColonAndRectumTumour)
Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 7 of 11
MetastaticRegionalLymphNodeOfColonAndRectumTumour
EquivalentTo
MetastaticLymphNodeOfColonAndRectumTumour and
ColonAndRectumRegionalLymphNode
To define regional lymph node metastases of colorectal
cancers, the aggregate of primary tumour and infiltrated
lymph nodes around the colon and rectum (TumourAggre-
gate) has to be considered as one (composite) entity. The
representational unit N of the TNM classification of col-
orectal cancers depends on the count of metastatic regional
lymph nodes and the presence of subserosal tumour deposits
without regional lymph node metastases. The count of
metastatic lymph nodes is represented by subclasses of
CardinalityValueRegion:
TumourOfColonAndRectumWith2or3MetastaticRegional-
LymphNodes EquivalentTo
TumourOfColonAndRectumWith1to3MetastaticRegional-
LymphNodes and
(btl2:isBearerOf some
(Cardinality and
(btl2:projectsOnto some
Cardinality2or3) and
(btl2:projectsOnto only
Cardinality2or3)))
Representation of distant metastases
For the representational unitM of the TNM classification sys-
tem the existence and number of distant metastases are eval-
uated. The definition of distant metastases excludes regional
lymph nodes as their localisation:
DistantMetastasisOfColonAndRectumTumour EquivalentTo
MetastasisOfColonAndRectumTumour and
(not (btl2:isIncludedIn some
ColonAndRectumRegionalLymphNode))
TumourOfColonAndRectumWithDistantMetastasis
EquivalentTo
TumourOfColonAndRectumAggregate and
(btl2:hasPart some
DistantMetastasisOfColonAndRectumTumour)
TumourOfMammaryGlandWithDistantMetastasis
subClassOf
(btl2:isRepresentedBy only
(MammaryGlandTNM_M1 or
MammaryGlandTNM_pM1 or
(not RepresentationalUnitInTNMClassification))
Classification of pathology data
All instance data of 291 samples of colorectal cancer
could be classified into classes of TNM-O on colorectal
cancer. A posteriori comparison of the automatic classifi-
cation results with a manual TNM coding based on the
same findings from the pathology database by an experi-
enced pathologist showed 100% agreement. Table 5 shows
15 exemplary tabular instance data rows and the cor-
responding manual and automatic classification results.
Figures 1 and 2 shows an example of an RDF-OWL
instance which corresponds with rows 6 and 8 of Table 5.
For clarity, the RDF example focuses on TNM N, other
details on tumour invasion and distant metastasis were
left out. All automatic classification results are based on
Table 5 TNM relevant tabular data, manual expert TNM classification (subscript P), and ontology-based automatic TNM classification
(subscript O)
Invasion of rLN tp rLN TD/ Sat. dMT ip dMT TP NP MP TO NO MO
Subserosa 31 0 no 0 no pT3 pN0 M0 pT3 pN0 M0
Muscular layer 13 0 no 0 no pT2 pN0 M0 pT2 pN0 M0
Subserosa 19 0 no 0 no pT3 pN0 M0 pT3 pN0 M0
Submucosa 18 0 no 0 no pT1 pN0 M0 pT1 pN0 M0
Muscular layer 11 0 no 0 no pT2 pN0 M0 pT2 pN0 M0
Visc. peritoneum 19 2 no 0 no pT4a pN1b M0 pT4a pN1b M0
Subserosa 20 0 yes 0 no pT3 pN1c M0 pT3 pN1c M0
Subserosa 14 2 no 0 no pT3 pN1b M0 pT3 pN1b M0
Muscular layer 14 0 no 0 no pT2 pN0 M0 pT2 pN0 M0
Subserosa 24 4 no 0 no pT3 pN2a M0 pT3 pN2a M0
Other 16 6 no 0 no pT4b pN2a M0 pT4b pN2a M0
Subserosa 17 0 no 0 no pT3 pN0 M0 pT3 pN0 M0
Visc. peritoneum 40 29 no 0 no pT4a pN2b M0 pT4a pN2b M0
Subserosa 15 0 no 0 no pT3 pN0 M0 pT3 pN0 M0
Visc. peritoneum 24 15 no 1 no pT4a pN2b M1a pT4a pN2b M1a
rLN: Number of regional lymph nodes inspected; tp rLN: Number of tumour-positive regional lymph nodes, TD/ Sat.: Tumour deposits/ satellites; MT: Number of distant
metastases; ip MT: Intra-peritoneal metastases
Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 8 of 11
Fig. 1 N1b representational unit of TNM-O for colorectal tumours. Graph of the patho-anatomical structures represented by an N1b representational
unit of the TNM-O for colorectal tumours version 7 (TNM-O_colorectal_7.owl). T and M representational units are unspecified
Fig. 2 RDF-OWL instance of a tumour aggregate and corresponding OWL classes. Graph of an RDF instance of a tumour aggregate as created from
tabular data according to TNM-O for colorectal tumours version 7 (TNM-O_colorectal_7.owl). RDF instances data are depicted with a purple
diamond. RDF instance for T and M classification are omitted. Instances of this type are classified as TNM N1b
Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 9 of 11
TNM-O, TNM-O_colorectal_7 and RDF-OWL instance
data.
Discussion
TNM is a globally accepted system to describe the
anatomical extent of malignant tumours [2, 14]. Although
TNM is of high importance for tumour staging, to the
knowledge of the authors, there exists no comprehensive
formal representation of TNM so far. With this work,
the authors provide a first version of a TNM ontology
(TNM-O) and a prototypical implementation of TNM for
colorectal cancers. Further, this work shows that TNM-O
classifies instance data.
Over time, TNM has developed into a coding system,
which had to accommodate both the pragmatics of coding
and representational accuracy. The literature on ambi-
guities and difficulties of TNM in practice is abundant.
The discussion of TNM for breast tumours illustrates
the dilemma of its maintainers [8, 47, 48]. They had to
account for the rapid progression of scientific knowledge
on tumours and to keep it usable at the same time: new
versions of TNM are already outdated when compared
with new scientific insights. On the other hand, TNM has
become increasingly complex, with a negative impact on
its usability by both expert and non-expert documentation
staff and physicians.
Encoding clinical conditions using TNM as well as the
selection of the right treatment according to TNM codes
is daily routine in oncology. In order to assist in these
difficult and time consuming decision processes, sev-
eral systems have been proposed, usually based on text
extraction from pathology reports and machine learn-
ing algorithms [2426]. The accuracy of these approaches
was relatively low [24]. Here, we present an ontology,
which classifies instance data with 100% accuracy in
an experimental setting based on structured data. We
hypothesise that DL based classification using TNM-O
could also improve the results from automated informa-
tion extraction from unstructured data as done in the
above mentioned approaches. Such systems could also be
made available in intelligent documentation systems in
the form of embedded decision support systems, which
could help to choose the right codes for a clinical condi-
tion and/ or the right guideline compliant treatment for
a given code (describing a clinical condition). Further-
more, we think that with an ontology the curation of the
TNM itself could be improved. Based on a taxonomic
and axiomatic description, the detection of coding errors,
inconsistencies, and ambiguities in definitions could be
facilitated [28, 29]. A formal description logic based
axiomatisation allows the use of specific reasoning tools
to check for inconsistencies during the ontology engi-
neering process, which would indicate conflicting axioms.
Redundancies or wrong hierarchical dependencies is
detected by checking the inferred class hierarchy after DL
classification.
This study is limited as far as we provide here a
first version of the TNM Ontology (TNM-O), limited
to mammary gland [32] and colorectal tumours. As
these two tumour entities are the most complex and
best represented ones in TNM, the current version
is already sufficiently complete and stable to be used
as a blueprint for TNM-O extensions to other organ
systems.
Due to the nature of the domain and the rich top-level
ontology employed, the computational resources needed
to classify the ontology are considerable. In order to
alleviate performance issues, TNM-O will be provided as
modules for different organ systems. Thus, the users can
import only the modules of interest into their application
context.
Future research should evaluate the presented prototype
ontology (i) by implementing further tumour locations,
and (ii) by systematic application in clinical classification
and retrieval scenarios. We will provide the formalization
of TNM for other primary tumour locations in a modu-
lar way, so that users can select which part of the TNM-O
they would like to use. In this way, we hope to reduce the
computational resources already needed to a minimum.
Conclusion
We presented a first version of an ontology (TNM-O) that
represents the TNM tumour classification system. The
present work demonstrates its representational power and
completeness as well as its applicability for classification
of instance data. This work provides a foundation for an
exhaustive TNM ontology.
Endnotes
1 http://www.uicc.org
2 http://seer.cancer.gov/seerstat/databases/ssf/
3 http://seer.cancer.gov/csr/1975_2013/sections.html
4 https://cancerstaging.org/About/news/Pages/8th-
Edition-Publication-Date-Announced.aspx
5 http://codes.iarc.fr/usingicdo.php
6 http://www.who.int/cancer/modules/en/
7 http://owlapi.sourceforge.net/
8 http://hermit-reasoner.com/
9 http://owlapi.sourceforge.net/
Acknowledgements
The article processing charge was funded by the German Research
Foundation (DFG) and the Albert Ludwigs University Freiburg in the funding
programme Open Access Publishing.
Authors contributions
MB and SS designed the structure of TNM-O. FF implemented TNM-O for
colorectal cancer, developed the module structure of TNM-O and curated
TNM-O for breast cancer. PB and MB designed the classification study on
Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 10 of 11
pathology data for which PB provided the pathology dataset and evaluated
the classification results. The manuscript was primarily drafted by MB and SS,
and edited and approved for publication by all authors.
Competing interests
The authors declare that they have no competing interests.
Author details
1Institute for Medical Biometry and Statistics, Medical Center  University of
Freiburg, Faculty of Medicine, Stefan-Meier-Str. 26, 79104 Freiburg i. Br.,
Germany. 2Department of Informatics, University of Minho, Campus de
Gualtar, 4710-057 Braga, Portugal. 3Tumorbank Comprehensive Cancer Center
Freiburg and Center for Surgical Pathology, Medical Center  University of
Freiburg, Faculty of Medicine, Breisacher Straße 115a, 79106 Freiburg i. Br.,
Germany. 4Institute of Medical Computer Sciences, Statistics and
Documentation, Medical University of Graz, Auenbruggerplatz 2, 8036 Graz,
Austria.
Received: 2 February 2016 Accepted: 25 October 2016
Baek and Park Journal of Biomedical Semantics  (2016) 7:55 
DOI 10.1186/s13326-016-0094-9
RESEARCH Open Access
Making adjustments to event annotations
for improved biological event extraction
Seung-Cheol Baek1,2* and Jong C. Park1
Abstract
Background: Current state-of-the-art approaches to biological event extraction train statistical models in a
supervised manner on corpora annotated with event triggers and event-argument relations. Inspecting such corpora,
we observe that there is ambiguity in the span of event triggers (e.g., transcriptional activity vs. transcriptional),
leading to inconsistencies across event trigger annotations. Such inconsistencies make it quite likely that similar
phrases are annotated with different spans of event triggers, suggesting the possibility that a statistical learning
algorithm misses an opportunity for generalizing from such event triggers.
Methods: We anticipate that adjustments to the span of event triggers to reduce these inconsistencies would
meaningfully improve the present performance of event extraction systems. In this study, we look into this possibility
with the corpora provided by the 2009 BioNLP shared task as a proof of concept. We propose an Informed
Expectation-Maximization (EM) algorithm, which trains models using the EM algorithm with a posterior regularization
technique, which consults the gold-standard event trigger annotations in a form of constraints. We further propose
four constraints on the possible event trigger annotations to be explored by the EM algorithm.
Results: The algorithm is shown to outperform the state-of-the-art algorithm on the development corpus in a
statistically significant manner and on the test corpus by a narrow margin.
Conclusions: The analysis of the annotations generated by the algorithm shows that there are various types of
ambiguity in event annotations, even though they could be small in number.
Background
Current state-of-the-art approaches to biological event
extraction train statistical models in a supervised learn-
ing manner on annotated corpora, where event triggers, or
the expressions indicative of events, and event-argument
relations, or relations between events and their partici-
pant, are annotated (e.g., [1, 2]). The readers are referred
to [3] if the tasks are not familiar. Inspecting such corpora,
we observed some cases where there is residual ambi-
guity in the span of event triggers (e.g., transcriptional
activity vs. transcriptional). Because of the ambiguity,
these gold-standard corpora would manifest inconsisten-
cies across the span of event triggers. That is, there would
be similar phrases where the span of their counterparts of
event triggers is differently annotated, and as a result, such
*Correspondence: scbaek@nlp.kaist.ac.kr
1Department of Computer Science, KAIST, 291 Daehak-ro, Daejeon, Republic
of Korea
2Agency for Defense Development, Daejeon, Republic of Korea
event triggers are syntactically characterized in a differ-
ent way, suggesting a possibility that a statistical learning
algorithm is hard to generalize from such event triggers
that are similar, but differently annotated in a training cor-
pus. We anticipate that adjustments to event annotations
to reduce such inconsistencies would lead to a meaning-
fully improved performance of even the state-of-the-art
event extraction systems. In this study, we look into this
possibility with the corpora provided by the 2009 BioNLP
shared task [3]. We note that this paper reports an exten-
sion of our previous work [4] with detailed discussions and
more experimental results.
For example, consider sentence (1) from the training
corpora, where the annotated event triggers are set in
bold-face.
© 2016 Baek and Park. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 2 of 12
(1) ... express either decreased or increased numbers
of VDR. (PMID:9783909)
The phrases decreased and increased numbers are
annotated as event triggers of Negative and Positive Reg-
ulation events, respectively, that take a Gene Expression
event with the event trigger express. These annota-
tions are justifiable with respect to the meaning of these
phrases, but there are alternatives, including one where
the phrase increased becomes the trigger of the Positive
Regulation event. Despite the semantic similarity between
these two events, their event-argument relations to the
Gene Expression event are syntactically different (Fig. 1),
in that the event trigger decreased is the adjectival mod-
ifier (AMOD) of the direct object (DOBJ) of the phrase
express, while the event trigger increased numbers is
the direct object (DOBJ) of the phrase express. However,
if these event triggers are slightly adjusted, for example
by dropping the word numbers from the event trig-
ger increased numbers, these event triggers and event-
argument relations will come to have similar to share the
similarity also in syntactic characteristics with respect to
phrasal categories and shortest dependency paths. The
inconsistencies would provide a valuable opportunity for
improving the performance of event extraction, but the
current state-of-the-art approaches have not seriously
addressed them yet.
Note that one may still find that sentence (1) indicates
a Regulation event, not these Positive and Negative Reg-
ulation events, but we can leave the identification of the
Regulation event to an inference engine that would be
deployed after event extraction systems, since the exact
nature of an event can be inferred from the disjunction
of these Positive and Negative Regulation events indicated
by the syntactic construction either A or B.
The only reported effort would be to normalize multi-
word event triggers into single-word event triggers with
the help of the Head-Word rule, or a rule of taking the
syntactic head word of an event trigger (e.g., [2, 5]), even
though the rule often makes an apparently bad choice, as
in the example above, where it picks out the constituent
word numbers from the event trigger increased num-
bers, which does not have any meaning relevant to Pos-
itive Regulation events, and furthermore, is inconsistent
with the event trigger decreased.
In this paper, as a proof-of-concept study, we
examine the benefits of reducing inconsistencies
across event annotations as follows. First, we use the
Expectation-Maximization (EM) algorithm with Viterbi
approximation, where latent variables encode events.
Our experimental results show that the unmodified EM
algorithm is defeated by our baseline algorithm, which
is a learning algorithm that successfully trained state-
of-the-art event extraction systems [2], in part because
Fig. 1 Dependency Graphs of Example Sentences. The graphs are basic Stanford dependency analyses by the Charniak-Johnson parser with a
self-trained biomedical parsing model. In (1) and (4), dashed arrows indicate inferred dependency relations based on conjunctions. In (3), dashed
arrows indicate that corresponding dependency relations are naturally expected dependency relations, but are missed in the analysis generated by
the parser
Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 3 of 12
the EM algorithm adjusts the models to extract similar
but unintended events. To overcome this problem, we
use a posterior regulation technique of consulting the
gold-standard annotations in the form of constraints.
We come up with four constraints on the possible event
annotations to be explored by the EM algorithm. The
resulting algorithm, to be called the informed EM algo-
rithm, turns out to outperform our baseline algorithm
on the development corpus in a statistically significant
manner (p?value =9.59 E-12) and on the test corpus by
a narrow margin (51.6 % vs. 51.3 %). Thus, we found it
beneficial to make proper adjustments to event trigger
annotations. An analysis of the annotations generated
by the algorithm shows that there are various types of
ambiguity in event annotations including ambiguity in
the span of event triggers, even though the algorithm
finds only a small number of such cases.
To the best of our knowledge, this would be the first
study where adjustments to the gold-standard annotations
are made, even though there are NLP studies on a similar
use of posterior regularization techniques including the
one by Okita and colleagues [6], where partial annotations
of alignment links are incorporated as prior knowledge
into the word alignment process.
The rest of this paper is organized as follows. This
Background section ends with the following short subsec-
tion Biological Event Extraction Task, which defines the
task of biological event extraction. The Methods section
develops our statistical models and learning algorithms.
The Results section presents and analyzes experimental
results. The Conclusion section presents possible future
research directions and concludes this paper.
Biological event extraction task
As a case study, we addressed the event extraction task
as defined in the 2009 BioNLP shared task 1 [3], which
was later renamed as GENIA Event Task 1 and extended
to cover full papers in the 2011 BioNLP shared task [7],
where biological events are used to refer to the changes
of a state of one or more biological macromolecules.
The task is to extract structured information on events
from sentences in the biological literature, which con-
sists of their event type and participants encoded with
a controlled vocabulary that consists of nine event type
terms (e.g., Gene Expression) and two role type terms (i.e.,
THEME and CAUSE).
The nine event types are divided into three groups
according to their participants. The first group is plain
protein-taking events that must take a single protein as
THEME (e.g., Gene Expression). The second one ismulti-
ple protein-taking events, or events that take one or more
proteins as THEME (e.g., Binding events). The third one
is event-taking events that must take a single protein and
event as THEME and may take a single protein and event
as CAUSE (e.g., Positive Regulation and Negative Reg-
ulation). The events of the first group may be viewed
as binary relations between event triggers and protein
mentions, but those of the last two groups are differ-
ent from binary relations, in that multiple protein-taking
events take more than one argument and event-taking
events allow nested event structures. Thus, the extrac-
tion of events poses challenges other than those of the
extraction of binary relations, which have been exten-
sively studied in the biomedical information extraction
community.
Methods
Following Björne and colleagues [5], we viewed the event
extraction task as constructing directed graphs, where
event triggers and event-argument relations are encoded
with labeled nodes and edges, respectively. We con-
structed these directed graphs with the help of various
resources including syntactic analyses. In this section, we
first describe these resources used in our event extraction
system and then develop graph representations, statistical
models and learning algorithms, in this order.
Resources
We used lexical and syntactic analyses to encode tokens
and the relation between tokens into statistical mod-
els. As for lexical analyses, we used the baseforms and
part-of-speech (POS) tags of the tokens included in the
analyses by the Enju parser, which are available in the
official website of BioNLP shared tasks (http://weaver.
nlplab.org/~bionlp-st/BioNLP-ST/downloads/support-
downloads.html). As for syntactic analyses, we use basic
Stanford dependency analyses by the Enju parser with the
GENIA model [8] together with those by the Charniak-
Johnson parser [9] with a self-trained biomedical parsing
model [10], since the Enju parser fails to generate analyses
for a few sentences. These syntactic analyses are also
available in the official website of BioNLP shared tasks.
As for protein mentions, we used their gold-standard
annotations available on the official website of BioNLP
shared tasks, which were given to the participants in
the BioNLP shared tasks. The annotations contain multi-
word protein mentions. Since most of them correspond to
syntactic units (i.e., single words and phrases), we can eas-
ily combine tokens in multi-word protein mentions into
single tokens and redirect their dependency relations.
Following Miwa and colleagues [1] and Kilicoglu and
Bergler [11], we developed an event trigger lexicon for each
event type for the purpose of identifying apparently incor-
rect candidates for event triggers as follows. Constituent
words within annotated event triggers in the training cor-
pus are scanned one by one. Each scanned constituent
word is put into the lexicon that corresponds to the
type of events anchored at the event trigger. When the
Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 4 of 12
constituent word contains hyphens, it is split by hyphens
and the resulting components of the word are put into
the lexicon together with the original constituent word.
In a similar manner, we also constructed the stemmed
version of each event trigger lexicon using Porter
Stemmer.
The automatically constructed lexicons would contain
a number of entries not helpful for checking if a token
is part of an event trigger. To identify and remove such
entries, we computed the reliability score Rw,e of each
entry w in the lexicon for each event type e, as defined by
Kilicoglu and Berger [11]:
Rw,e = Cw,eCw (1)
where Cw,e is the number of times the entry w appears
within the event trigger of events of type e and Cw is the
number of times the entry w appears. Finally, we removed
entries with reliability scores below 1 %.
After this removal, these lexicons still have either a part
or the whole of 98 % of the annotated occurrences of event
triggers in the training corpus, and can be used to identify
candidate pairs of words w and event types e indicating
that w might be part of an event trigger for event type e,
where around 11 % of them are actually part of annotated
event triggers.
Graph representations
Let us consider how to encode multi-word event trig-
gers. We came up with the following four possible forms
of multi-word event triggers and manually searched the
training corpus for cases corresponding to each possibil-
ity with the help of syntactic analyses by the Charniak-
Johnson parser [9] with a self-trained biomedical parsing
model [10], as shown in Fig. 1.
The first is that some event triggers are inherently multi-
word expressions, as exemplified in (2), where words
within the bold-faced event trigger negative regulatory
of a Negative Regulation Event fully describe the nature of
the event only together each other:
(2) ... contains a novel negative regulatory element ...
(PMID:10359895)
Second, some words in multi-word event triggers are
adjacent to one another, but have no dependency rela-
tions among them, suggesting that at least the first and last
words of each event trigger should be marked. Returning
to sentence (2), the two words negative and regulatory
are adjacent to each other and have no dependency rela-
tions between them in the generated dependency graph.
The third is that some words within multi-word event
triggers are not consecutive to one another but have
dependency relations among them, suggesting that depen-
dency relations combining words within event triggers
should be encoded. As an example, consider sentence (3),
where the bold-faced word expression is annotated as
the trigger of Transcription and Gene Expression events,
which produce the mRNAs and proteins of the gene
E-selectin, respectively.
(3) ...mRNA and surface expression of E-selectin. ...
(PMID:10202027)
Our intuition is that the word expression in combi-
nation with the word mRNA describes the nature of
Transcription events more fully than the word expression
alone, but only that the words and and surface appear
in-between. That is, the words mRNA and expression in
sentence (3) are not consecutive, but have a dependency
relation NN between them.
Fourth, some words in multi-word event triggers might
not be consecutive to one another and might not have
dependency relations among them either. The effort to
manually find such a case was not successful but we found
a similar case. In sentence (4), the words positive and
regulatory indicate together the presence of Positive Reg-
ulation events (not annotated on the training corpus), but
these two words are not consecutive to each other and are
not linked to each other through dependency relations in
the generated dependency graph where these two words
have the dependency relation AMOD to elements.
(4) ... several positive and negative regulatory elements.
... (PMID:1429562)
Since these four different types of multi-word event trig-
gers would make it complicated to represent the span of
event triggers in the graphs, and since our focus here is
not on exactly identifying the span of event triggers, we
mark only single words within event triggers and encode
the context of these marked words into statistical models
to exploit other words within the span of event triggers in
sensing the presence of the event triggers including them.
For example, we may mark the word regulatory as the
anchor word of the event trigger negative regulatory in
sentence (2) and encode its contextual features including
the fact that the word regulatory is adjacent to the word
negative.
One natural candidate for words to be marked is the
constituent words of an event trigger that we can use
to encode syntactic relations between the event trigger
and other words since we need them anyway, but this
decision did not help to uniquely determine which word
should be marked. Another conceivable decision, to be
pursued in this article, is that a marked word can be used
in describing as many syntactic relations between event
triggers and participants as possible so that it is possi-
ble to easily find regularities in these syntactic relations
only from a small number of instances. Henceforth, we
Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 5 of 12
will call such words meeting these decisions and being
marked as anchor words. Of course, the choice of anchor
words would be dependent on the way for describing
syntactic relations between words and the training cor-
pus, but there are predictable characteristics of anchor
words.
First, when an event trigger corresponds to a phrase
(e.g., the first and third observations above), the natural
choice for the anchor word of the event trigger would be
the head word of the phrase, since the dependency paths
between the head word and words outside the phrase
do not have other constituent words in the event trig-
ger so that the located dependency paths can be used for
different event triggers with the same head words. As a
result, in sentence (3), expression is preferable to mRNA.
Second, when an event trigger does not correspond to
a phrase (e.g., the second and fourth observations above),
the natural choice for the anchor word of the event trig-
ger would be the word frequently occurring in various
event triggers for the same event type. Since seven Positive
Regulation event triggers contain positive in the train-
ing corpus but only one Positive Regulation event trigger
(positive regulatory role) contains regulatory, positive
is preferable to regulatory in sentence (4).
Now let us define the desirable output labels of tokens in
the training corpus for trigger identification. All the words
except for anchor words will be given the label negative.
Anchor words will be labeled with more than one event
type, since some event triggers indicating two different
types of events share an anchor word as shown in sentence
(3), where the word express would be preferable anchor
words for Transcription and Gene Expression events.
When turning to the label of edges, a question arises
whether edges can be labeled with more than one role
type, that is, whether an event takes a protein or another
event both as THEME and CAUSE. To answer this ques-
tion, we constructed graphs for sentences in the training
corpus of 800 annotated abstracts with the Head-Word
rule. There are only six edges labeled with more than
one role (of around 8,200 edges labeled with one or more
roles), suggesting that they are likely to be annotation
noise. As a result, we allow edges to be labeled with at
most one role type.
The issues we have discussed so far are also relevant to
relations, but there are issues specific to events, includ-
ing the one that graphs with cycles and loops may lead
to an infinite number of event-taking events with distinct
event participants. As an example, consider Fig. 2, where
the word in bold-face is the annotated event trigger of
a Gene Expression event and a Positive Regulation event
that takes the Gene Expression event as THEME. It would
be straightforward to derive these Gene Expression and
Positive Regulation events from the graph. The problem is
that there is no principled way to rule out another Positive
Fig. 2 Event Graph with a Loop
Regulation event with the derived Positive Regulation
event as THEME.
One way is to disallow graphs with cycles and loops.
Constructing graphs for the training sentences, we could
discard graphs with cycles and remove loops with some
exceptions, since some of the loops would be justifiable.
Upon analyzing such loops, however, we came up with
a possible explanation for their presence, which is that
the annotators might have failed to find the appropriate
type for some events in sentences in the limited controlled
vocabulary and would have attempted to use the com-
bination of more than one component event to present
the event (merged events). In Fig. 2, Gene Expression and
Positive Regulation events with the event trigger overex-
pressed exemplify such merged events. Most of the other
loops would be due to words hyphenating protein men-
tions and event triggers (e.g., IFNgamma-induced). We
identified the pairs of Gene Expression and Positive Reg-
ulation events making loops and then replace them with
single merged events.
Finally, we point out two differences between our graph
representation and the widely used one proposed by
Björne and colleagues [5]. One is that their represen-
tation allows only predefined labels of combined event
types (e.g., Gene Expression/Positive Regulation), but that
our representation allows any possible labels of combined
event types. Another is that they do not use merged
events, while we evaluate the consequences of these dif-
ferences, as shown in the Results and Discussion section.
Statistical model
Given a sentence x = (x1 . . . xn), we constructed graph
representations of events by finding the most reliable
assignment of labels in a complete directed graph with the
words as nodes and removing edges labeled as irrelevant
from the graph. We measured the reliability of assign-
ments of labels in terms of output scores of a modified
version of a state-of-the-art model proposed by Riedel and
McCallum [2], since their model does not allow words
withmore than one event type. They proposed threemod-
els ranging from the simplest one, Model 1, to the most
complex one, Model 3. Model 3 was ranked the second
in the GENIA Event subtask of the 2011 BioNLP shared
task and its variant was ranked the first [12]. However,
Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 6 of 12
we developed our model from Model 1 for convenience
of experiments, since Model 3 was reported to be much
slower than Model 1 in training and predicting.
Given an assignment L, our model M first checks if the
assignment L satisfies the following two conditions. One
is that identified anchor words (i.e., constituent words of
event triggers) have at least one edge labeled with THEME
starting from them. Another is that all edges labeled with a
role type come from identified anchor words. If the assign-
ment satisfies the conditions, our model assigns scores
Mi,e(Li,e|x) to all the pairs of an event type e and a word
xi (i.e., vertices) and scores Mi,j(Li,j|x) to all the pairs of
words xi and xj (i.e., edges) and take the sum of these
scores as the scoreM(L) of the assignment L as follows:
M(L) =
?
(i,e)
Mi,e(Li,e|x) +
?
i,j
Mi,j(Li,j|x) (2)
where Li,e takes on a value of either positive or nega-
tive, while Li,j takes on a value of either THEME, CAUSE
or negative. Now the extraction of events can be viewed
as finding the assignment with the highest score. To find
the optimal assignment for a given sentence, we use a
modified version of the dynamic programming algorithm
proposed by Riedel and McCallum [2]. One may suppose
that valid assignments should satisfy other constraints,
such as the one that the edge labeled with role types goes
to either anchor words or protein mentions. However,
such constraints make it hard to efficiently find the opti-
mal assignment of graphs. For this reason, the system first
finds the optimal assignment without such constraints,
and if the resulting assignment does not contain any
cycles, we attempt to refine the assignment so that it satis-
fies all the constraints. For example, the label negative is
reassigned to all the incoming edges of a word other than
the identified anchor words and protein mentions. When
the resulting assignment has a cycle, it does not generate
any events for the input sentence.
We scored pairs of a word xi and an event type e using a
weight vector we as follows:
Mi,e(positive|x) = we · (xi), (3a)
Mi,e(negative|x) = ?we · (xi), (3b)
where (xi) is the feature vector of words xi. To define
the feature vectors of words, we used their lexical and lin-
ear/syntactic contextual information. Lexical information
about words is encoded with their surface form, base-
form, POS tag and the reliability scores of the entries
derivable from them in each event trigger lexicon. The
reliability scores are encoded as real-valued features.
The linear contextual features of a word (e.g., the word
decreased in sentence (1)) include center-marked n-
grams of words centered around the word (n = 2-4)
and made out of pairs of baseforms and POS tags (e.g.,
decrease:VBN) and a special symbol PROTEIN for pro-
tein mentions. For example, the center-marked trigram
either:CC [decrease:VBN] or:CC is used as a feature for
the word decreased in sentence (1). They also include
the distance from the word to proteins (e.g., Protein-
Distance:5 for the word decreased and the protein VDR
in sentence (1)) and the distance from the word to poten-
tial anchor words within the sentences relative to them
(e.g., Trigger-Distance:2 for the word decreased and the
word increased in sentence (1)). The distances of protein
mentions are encoded as binary features (i.e., taking either
0 or 1), but features for the distances of potential anchor
words take on the maximal reliability score of the corre-
sponding entries in the lexicons. As syntactic contextual
features, we encoded their syntactic governors and mod-
ifiers (e.g., number:NNS-MOD(amod)-decrease:VBN
and numbers:NNS-GOV(dobj)-express:VBP for the
word numbers in sentence (1)). Note that these contex-
tual features are intended to exploit words other than
anchor words in sensing the event triggers including them.
We also scored the label L of a word pair (xi, xj) using a
weight vector wL as follows:
Mi,j(L|x) = wL · (xi, xj), (4)
where (xi, xj) is the feature vector of a word pair (xi, xj).
To define the feature vector of a word pair (xi, xj), we
used the following features based on the features used
in [1]. Our feature vector consists of the lexical and
linear/syntactic contextual features of each of them as
defined above, the length of the shortest paths between
them and various representations of substructures of
paths between them as defined below. First, from a short-
est path, we generated the token sequence of the pairs
of baseforms and POS tags (e.g., decrease:VBN num-
ber:NNS express:VBP for the word decreased and the
word express in sentence (1), the dependency sequence of
pairs of the types and directions of dependency relations
(e.g., MOD(amod) GOV(dobj) for the word decreased
and the word express in sentence (1), and the token-
dependency sequence of the pairs of baseforms and POS
tags and pairs of the types and directions of depen-
dency relations (e.g., decrease:VBN MOD(amod) num-
ber:NNS GOV(dobj) express:VBP the word decreased
and the word express in sentence (1). From the result-
ing sequences, we generated n-grams of these sequences
(n = 2-4).
For efficiency, we assign the label negative to those
words that do not contain any entry in our event trig-
ger lexicons. We also assigned the label negative to edges
other than those edges whose starting word contains any
entry in the lexicons and whose ending word either refers
to a protein or contains an entry in the lexicons for
events that take proteins. Since about 98 % of the anno-
tated occurrences in the training corpus contain an entry
Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 7 of 12
in the lexicons, this does not incur a large performance
penalty but greatly reduces the size and complexity of the
problem.
Learning algorithms
As a baseline algorithm, we used the online prediction-
based Passive-Aggressive algorithm [13] with the cost
function of penalizing false negative event triggers and
edges 3.8 times more heavily than false positive ones as
in [2], since the algorithm with this setting successfully
trained Model 1 in [2], the one similar to our statistical
model. The pseudo-code of this algorithm is shown in
Fig. 3. It begins with an initial model with all weights set
to 0 (line 1). It takes several passes over the training cor-
pus D = ((x1, y1), ..., (xN , yN )), where xi and yi are the i-th
sentence and the gold-standard graphs that are automat-
ically derived from the gold-standard event annotations
using the Head-Word rule, respectively (line 2). Given a
sentence xj, it constructs a graph y (line 6) with the help
of the current interimmodelmt,i?1. When it makes a mis-
take (i.e., the predicted graph y is not the same as the
gold-standard graph yj), we constructed the next interim
model mt,i, whose output score of yj goes beyond that of
yj by at least the cost c incurred by the mistake, by making
a few modifications to the current interim model mt,i?1
to keep the knowledge learned so far as much as possible.
We take a total of 20 passes over the training corpus, sav-
ing the average Mt of the interim models weight vectors
after each pass (line 15), since the average Mt of interim
weight vectors is less likely to over-fit to the training cor-
pus than the individual interim weight vectors as shown
by Collins [14]. Here, the total number of passes, that is,
20, was arbitrarily chosen, but it turns out that the number
is sufficiently big for learning a statistical model.
Fig. 3 Baseline algorithm
With a modified version of the baseline algorithm as the
M step, we developed the Informed EM algorithm, or the
EM algorithm with a posterior regularization technique
as shown in Fig. 4, where sentences x and event annota-
tions z are observed and assignments y of labels to words
and word pairs are missing. Since it would be intractable
to enumerate all the possible assignments producing the
gold-standard event annotations z, we use the Viterbi
approximation to the EM algorithm under the unreason-
able assumption that the most probable assignment has
a remarkably higher probability than the second probable
assignment. This case may also have the counterpart of
the Inside-Outside algorithm, or the efficient implemen-
tation of the EM algorithm widely used in learning PCFGs
in an unsupervised manner, but we leave the design of
such an algorithm for future research. To incorporate
the gold-standard annotations into the EM algorithm, we
Fig. 4 Informed EM algorithms
Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 8 of 12
impose constraints on possible assignments, which are
derived from the gold-standard annotations.
Now we describe the pseudo-code of this algorithm as
shown in Fig. 4. We constructed the adjusted annota-
tion set D?, where the adjusted graphs yi are initially their
corresponding gold-standard graphs (line 1). It takes sev-
eral rounds (line 6), but behaves like the conventional
EM algorithm of alternatively applying the E and M steps
after the first five rounds (line 7). Here, the number of
rounds for initialization, that is, five, was arbitrarily cho-
sen. Since the EM algorithm may converge models into
local optima, we need to take care of initial models with
which the EM algorithm begins. During the first five
rounds, we trained the model by applying only the M
step in a supervised learning manner similar to that of
the baseline algorithm, since the resulting model would
be closer to the true model, if it exists, than randomly
constructed models. In the E step, it predicts a graph y
for a sentence xi with the current interim model Mt (line
8). It sets the adjusted graph yi to the prediction y if the
prediction y is not matched with the current adjusted
graph yi and satisfies predefined constraints (lines 10 and
11). To enforce models to predict anchor words other
than the head words of the annotated event triggers,
we modify the cost function to penalize errors for sen-
tences with updated graphs 10 times more severely than
for the others as in domain adaptation studies (e.g., [15])
(lines 24-26).
We came up with the following constraints. One is the
basic constraint that the adjusted graph should encode
the same event types and argument types as the gold-
standard graphs. For example, if a Positive Regulation
event with a Gene Expression event as THEME appears
in the gold-standard annotations, this constraint requires
that one or more Positive Regulation and Gene Expres-
sion events appear in the adjusted graphs and that the
Positive Regulation events should take a Gene Expres-
sion event, but does not take care of their event trig-
gers. Another is the confidence constraint such that the
percentage difference in output scores between can-
didate graphs y for next adjusted graphs and current
adjusted graphs yj should be equal to or greater than
the confidence constraint constant ?. To reflect the gold-
standard annotations more faithfully, we come up with
the non-overlapping constraint (NOC for short) that two
event triggers with the same event type in gold-standard
graphs cannot be mapped into a single word in their
corresponding adjusted graphs. For example, consider
sentence (5).
(5) The c-jun mRNA, which is constitutively expressed
in human peripheral-blood monocytes at relatively
high levels, was also slightly augmented ...
(PMID:1313226)
The words high levels and augmented indicate the
presence of two distinct Positive Regulation events with
the same Gene Expression event with the event trigger
expressed as THEME. Since they indicate the presence
of events of the same type with the same participants,
those assignments where only one of the words is labeled
with Positive Regulation violate the non-overlapping
constraint, but not the first two constraints.
Since there might be cases when the event triggers of
more than one event of the same type can be merged
without any problems but when the non-overlapping con-
straint prohibits any merging, we came up with a more
relaxed constraint, or the distance constraint that the dis-
tance between event triggers in candidate graphs y for
next adjusted graphs and event triggers with the same
event type in current adjusted graphs yj (e.g., the dis-
tance between levels and augmented is four) should be
equal to or less than the distance constraint constant ? .
In sentence (5), those graphs without any one of the event
triggers of these two Positive Regulation events would also
violate the distance constraint with ? ? 3.
Results and Discussion
We used the baseline and informed EM algorithms to
train our statistical models and evaluated the models on
the development corpus with respect to standard evalua-
tion metrics, such as recall, precision and F-score.
Evaluation of proposed graph representations
To measure the consequence of the substitution of single
merged events for Positive Regulation and Gene Expres-
sion events sharing single words, we reconstructed train-
ing event annotations by converting the gold-standard
annotations into graphs and the resulting graphs into
event annotations, since events that cannot be encoded in
graphs cannot survive in the reconstruction process. The
substitution may decrease the number of events removed
after the reconstruction process but may increase the
number of incorrect events generated by the graph-
to-event conversion algorithm after the reconstruction
process, leading to changes of the F1-score of the recon-
struction event annotations on the original event anno-
tations. We measured the F1-score of the reconstructed
event annotations twice, one with the substitution and
another without the substitution. We found that the sub-
stitution leads to an increase in the F1-score of the
reconstructed event annotations by 1.13 % points, and in
particular for Positive Regulation events by 3.14 % points,
though the F1-score for Gene Expression events is slightly
decreased by 0.06 % points.
We also measured the consequences of allowing words
with more than one event type. We used the baseline
algorithm to train multi-label and single-label statistical
Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 9 of 12
models. We found that the multi-label models outper-
form the single-label models most of the time as shown
in Fig. 5. To evaluate the statistical significance of the
superiority of the multi-label models over the single-label
models, we carried out the one-tailed paired Students t-
test for the pairs of the two points with the same x value.
The reason for the use of the one-tailed test, but not
the two-tailed test, is that only one direction (multi-label
models scores > single-labeled models scores) is consid-
ered to be against the null hypothesis that the multi-label
models are not superior to the single-labeled models.
According to the test, the superiority of the multi-label
models over the single-label models is shown statisti-
cally significant with a p-value of 0.0013. After the first
five rounds, the more rounds we took to train models
the lower performance the resulting models showed. This
would be because the models trained by taking many
rounds are likely to over-fit to the training corpus. We
suggest to stop the learning process of models when
the models performance on a held-out corpus starts to
decrease.
Table 1 shows the summary of the performance of the
models of each type. Since the Informed EM algorithm
applies the E step after the first five rounds, to be fair with
the Informed EM algorithm, we calculate averages and
sample standard deviations of the F-scores of the models
trained by taking more than five passes.
The single-label models are in fact our implementa-
tion of Model 1 of Riedel and McCallum [2], which
was reported to have the F1-score of 56.2 % for the
development corpus, and the best has a similar F-
score of 55.1 %, where the difference may be due
to implementation details regarding the feature vector
construction.
Table 1 Performance of multi-label and single-label statistical
models. These models are trained using the baseline algorithm
Single-label (R/P/F) Multi-label (R/P/F)
BEST 46.8/67.0/55.1 47.3/67.7/55.7
AVG. 46.2/66.6/54.6 46.6/67.1/55.0
(STD.) (0.36/0.41/0.32) (0.23/0.21/0.30)
Evaluation of the informed EM algorithm
To examine the effect of the posterior regulation, we first
use the Informed algorithm without any constraints (the
pure EM algorithm) to train models. It is again unsur-
prising that the more rounds we took to train models the
lower performance the resulting models showed as shown
in Fig. 6. As a result, the best one is the model it took six
passes to train, which shows a recall of 47.12 %, a precision
of 67.04 % and an F-score of 55.34 %.
At the first E step, more than a thousand of adjusted
graphs were updated and at subsequent E steps, fewer
than half a hundred graphs were updated, suggesting that
the models are converging (the total number of sentences
is about seven thousands) and the pure EM algorithm
would have trained models to predict similar but unin-
tended graphs.
We evaluated our Informed EM algorithm with various
constraint sets, all of which include the basic constraint,
as shown in Tables 2 and 3. The comparison between
Table 1 on the one hand and Tables 2 and 3 on the
other shows that most models outperformmodels trained
by the baseline algorithm in terms of both the best and
averaged F-scores. To assess the statistical significance of
their superiority over the models trained with the base-
line algorithm, we calculate p-values with respect to the
Fig. 5 Comparison Between Multi-Label and Single-Label Statistical Models. Each point (x, y) indicates that a model trained by taking x rounds has
an F-score of y. These models are trained using the baseline algorithm
Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 10 of 12
Fig. 6 Comparison Between the Baseline and Pure EM Algorithm. Each point (x, y) indicates that a model trained by taking x rounds has an F-score
of y
one-tailed paired Students t-test for the pairs of models
trained by taking the same number of rounds, as shown in
Table 4.
We analyzed the effect of the choice of constraints on
the performance of models. The high confidence con-
straint constant ? reduces the number of updates in the
adjusted graphs, making the resulting models similar to
models trained by the baseline algorithm as shown in
Table 5. The distance constraint (? = 2) reduces the num-
ber of updates in the adjusted annotation set and for most
times increases the best F-scores but not the averaged F-
scores. The non-overlapping constraint also reduces the
number of updates but not always increases the best and
averaged F-scores. Note that, even though our best model
is the model we trained with the non-overlapping con-
straint, the best combination of constraints would be with
Table 2 Best performance of informed EM models
? = ? = 2 (R/P/F) ? = 100 (R/P/F)
Without NOC
0.1 48.0/68.2/56.3 47.6/68.3/56.1
0.2 47.6/68.6/56.2 47.4/68.5/56.0
0.3 47.7/68.8/56.3 47.3/67.5/55.7
0.4 47.1/67.8/55.6 47.6/67.7/55.9
With NOC
0.1 47.3/68.9/56.1 47.5/68.1/55.9
0.2 47.3/68.0/55.8 47.5/69.3/56.4
0.3 48.1/68.9/56.7 47.2/68.1/55.8
0.4 46.8/68.9/55.8 47.3/67.7/55.7
The best figures are set in bold-face
the ? value of 0.3 and the ? value of 2 and without the
non-overlapping constraint as indicated in Table 4.
Finally, we chose the best baseline model (a multi-
labeled model) and best proposed model (?=0.3, ?=0.2,
no use of non-overlapping constraint) in terms of the per-
formance on the development corpus and evaluated them
Table 3 Average performance of informed EM models
? = ? = 2 (R/P/F) ? = 100 (R/P/F)
Without NOC
0.1 47.9/66.8/55.8 47.3/67.7/55.7
(0.27/0.56/0.31) (0.22/0.30/0.23)
0.2 47.1/68.0/55.7 47.1/68.1/55.7
(0.35/0.86/0.42) (0.22/0.21/0.16)
0.3 47.4/67.9/55.8 47.3/66.8/55.4
(0.18/0.39/0.23) (0.13/0.22/0.13)
0.4 46.7/67.5/55.2 47.0/67.7/55.5
(0.38/0.52/0.21) (0.35/0.23/0.30)
With NOC
0.1 46.9/68.0/55.5 47.1/67.6/55.5
(0.23/0.39/0.26) (0.15/0.23/0.16)
0.2 47.1/67.6/55.5 47.2/68.3/55.8
(0.22/0.29/0.20) (0.22/0.65/0.35)
0.3 47.6/68.0/56.0 47.0/67.1/55.3
(0.38/0.45/0.40) (0.27/0.36/0.29)
0.4 46.5/68.4/55.4 47.1/67.6/55.5
(0.33/0.72/0.42) (0.24/0.39/0.22)
The best figures are set in bold-face and the sample standard deviations are
bracketed
Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 11 of 12
Table 4 p-values for informed EM models
? = ? = 2 (w.o/w NOC) ? = 100 (w.o/w NOC)
0.1 3.32E-09/1.86E-04 1.03E-06/4.47E-06
0.2 9.98E-07/1.21E-08 3.58E-09/1.05E-08
0.3 9.59E-12/3.93E-09 4.38E-06/2.95E-03
0.4 4.37E-02/1.19E-04 2.50E-08/6.70E-07
The best figures are set in bold-face
on the BioNLP09 test corpus. We found that the best
baseline model has a recall of 42.2 % and a precision of
65.5 % and F-score of 51.3 %, while the best proposed
model has a recall of 42.2 % and a precision of 66.4 % and
F-score of 51.6 %, suggesting that the proposed models
slightly outperform the best baseline model.
Analysis of adjusted graphs
We observed updates of shifting themark of anchor words
from empty words into content words (e.g., activity
vs. -binding in the noun phrase DNA-binding activity
(PMID:9115366)) and from words distant from the partic-
ipants of the anchored events into words closer to them
(e.g., simulates vs. activation in the phrase simulates the
activation of (PMID:8557975)). There were also updates
of labeling more than one words as the event trigger of
an identical event (e.g., results and increases in a phrase
starting with results in increases of (PMID:2121746)).
Unexpectedly, we found that sets of edges were updated
more often than the position of anchor words. Some
edges were copied and redirected (e.g., copies of all edges
coming from results are attached to increase in the pre-
ceding example), leading to new events whose presence
makes sense, where some of them have corresponding
existing ones and some of them are completely new. For
example, a Regulation event of granulocyte-macrophage
colony-stimulating factor was created on sentence (6) with
an annotated Regulation event of the Expression event of
the protein.
(6) Regulation of granulocyte-macrophage colony-
stimulating factor and E-selectin expression in
endothelial cells by cyclosporin A and the T-cell
transcription factor NFAT. (PMID:7545467)
Table 5 Updated graphs for informed EM models
? = ? = 2 (w.o/w NOC) ? = 100 (w.o/w NOC)
0.1 72/47 98/50
0.2 34/18 46/31
0.3 16/11 25/15
0.4 9/8 9/5
Some edges not used in deriving events from the graphs
are removed, leading to the removal of events that seem to
be inferred. For example, sentence (7) below has an anno-
tated Positive Regulation event of H2 receptors, which was
removed by an update. The rationale behind this annota-
tion is that a sensible way of usingH2 receptors to increase
cAMP and c-fos expression is to activate H2 receptors.
(7) Histamine transiently increased cAMP and c-fos
expression throughH2 receptors. (PMID:9187264)
Of course, there are inexplicable updates. Table 6 shows
the distribution of types of 16 updates occurring in learn-
ing the best proposed model (?=0.3, ?=0.2, no use of
non-overlapping constraint). It shows that there are vari-
ous types of ambiguity, even though the algorithm finds a
small number of each type of cases.
Conclusion
In this study, we looked into the possibility that adjust-
ments to the annotated span of event triggers to reduce
inconsistencies across them lead to an improved per-
formance of event extraction systems. In order to make
adjustments automatically in favor of statistical models,
we developed an Informed EM algorithm, or the EM
algorithm with a posterior regularization technique that
exploits the gold-standard event trigger annotations in
the form of constraints. The algorithm (the best F-score=
56.7 %) is shown to outperform our baseline algo-
rithm (the best F-score=55.7 %) on the BioNLP09
development corpus in a statistically significant manner
(p-value=9.59E-12), indicating that proper adjustments
of event trigger annotations would be beneficial. Our
algorithm (F-score=51.6 %) is also shown to slightly out-
perform our baseline algorithm (F-score=51.3 %) on the
BioNLP09 test corpus. The annotations generated by the
algorithm indicate that there are various types of ambigu-
ity in event annotations including ambiguity in the span
of event triggers, even though the algorithm finds only a
small number of such cases.
However, there are still remaining issues. First, we use
the Viterbi approximation to the EM algorithm, whose
soundness is not well grounded. We anticipate that there
Table 6 Distribution of types of 16 updates
Description Count
Adding events similar to existing ones 7
Adding missing but reasonable events 4
Shifting the mark of anchor words 2
Removing duplicated and inferred events 2
Wrongly adding an incorrect event 1
Total 16
Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 12 of 12
would be a counterpart of the Inside-Outside algorithm,
an efficient implementation of the EM algorithm used
in learning PCFGs. Second, we fixed the parameters ?
and ? as confidence and distance constraint constants,
respectively, during training models. However, Smith and
Eisner [16] show that it would be beneficial for the EM
algorithm guided by prior knowledge to soften the con-
straints, as model parameters are converging. We antici-
pate that such update scheduling would also be beneficial
for the informed EM algorithm. Third, we applied this
approach only to the 2009 BioNLP shared task. Since this
approach is not specific to this task, there is a possibil-
ity of applying this approach successfully to similar tasks,
such as Infectious Disease (ID) and Epigenetic and Post-
translational Modification (EPI) tasks defined in the 2011
Bio-NLP shared task. We plan to address these issues in
the future.
Acknowledgements
This work was supported by the National Research Foundation of Korea(NRF)
grant fundedby the KoreaGovernment (NSIP)(No. NRF-2014R1A2A1A11052310).
Competing interest
Both authors declare that they have no competing interests.
Authors contributions
SCB implemented the project and wrote a draft, while JCP designed the
project and revised the manuscript. Both authors read and approved the final
manuscript.
Received: 2 June 2013 Accepted: 15 August 2016
RESEARCH Open Access
Overlap in drug-disease associations
between clinical practice guidelines and
drug structured product label indications
Tiffany I. Leung1* and Michel Dumontier2
Abstract
Background: Clinical practice guidelines (CPGs) recommend pharmacologic treatments for clinical conditions, and
drug structured product labels (SPLs) summarize approved treatment indications. Both resources are intended to
promote evidence-based medical practices and guide clinicians prescribing decisions. However, it is unclear how
well CPG recommendations about pharmacologic therapies match SPL indications for recommended drugs. In this
study, we perform text mining of CPG summaries to examine drug-disease associations in CPG recommendations
and in SPL treatment indications for 15 common chronic conditions.
Methods: We constructed an initial text corpus of guideline summaries from the National Guideline Clearinghouse
(NGC) from a set of manually selected ICD-9 codes for each of the 15 conditions. We obtained 377 relevant guideline
summaries and their Major Recommendations section, which excludes guidelines for pediatric patients, pregnant or
breastfeeding women, or for medical diagnoses not meeting inclusion criteria. A vocabulary of drug terms was derived
from five medical taxonomies. We used named entity recognition, in combination with dictionary-based and
ontology-based methods, to identify drug term occurrences in the text corpus and construct drug-disease
associations. The ATC (Anatomical Therapeutic Chemical Classification) was utilized to perform drug name and
drug class matching to construct the drug-disease associations from CPGs. We then obtained drug-disease
associations from SPLs using conditions mentioned in their Indications section in SIDER. The primary outcomes
were the frequency of drug-disease associations in CPGs and SPLs, and the frequency of overlap between the
two sets of drug-disease associations, with and without using taxonomic information from ATC.
Results: Without taxonomic information, we identified 1444 drug-disease associations across CPGs and SPLs for
15 common chronic conditions. Of these, 195 drug-disease associations overlapped between CPGs and SPLs,
917 associations occurred in CPGs only and 332 associations occurred in SPLs only. With taxonomic information,
859 unique drug-disease associations were identified, of which 152 of these drug-disease associations overlapped
between CPGs and SPLs, 541 associations occurred in CPGs only, and 166 associations occurred in SPLs only.
Conclusions: Our results suggest that CPG-recommended pharmacologic therapies and SPL indications do not
overlap frequently when identifying drug-disease associations using named entity recognition, although incorporating
taxonomic relationships between drug names and drug classes into the approach improves the overlap. This has
important implications in practice because conflicting or inconsistent evidence may complicate clinical decision
making and implementation or measurement of best practices.
Keywords: Clinical practice guidelines, Drug labeling, Drug therapy, Information storage and retrieval, Chronic disease
* Correspondence: tileung@stanford.edu
1Division of General Medical Disciplines, Stanford University, Stanford, CA, USA
Full list of author information is available at the end of the article
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Leung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 
DOI 10.1186/s13326-016-0081-1
Background
Clinical practice guidelines provide recommendations
intended to optimize patient care that are informed by
a systematic review of evidence and an assessment of
the benefits and harms of alternative care options [1].
Increasingly, guideline developing organizations are ex-
pected to produce guidelines based upon a systematic
review of evidence relevant to the scope of the guide-
line; for example, guidelines typically are limited in
scope to a single condition, and possibly even to a subdo-
main of that disease, e.g. screening, prevention, or treat-
ment. High-quality CPGs constitute one of the highest
levels of application of evidence-based medicine, based on
comprehensive searches and appraisal of the literature, in-
cluding systematic reviews if available [2]. The U.S. Food
and Drug Administration (FDA) provides drug structured
product labels (SPLs) for every approved drug. SPLs in-
clude structured information such as drug indications,
contraindications, and adverse effects. Such labeling is
based on data from clinical trials, and evidence about drug
effectiveness for specific indications or conditions may be
provided. Both CPGs and SPLs are each produced using
different and rigorous methodologies, but with common
intents of promoting evidence-based medical practices
and guiding clinician prescribing decisions. As systematic
reviews which form the evidence base for CPG recom-
mendations depend upon well-designed clinical trials
and studies of drugs clinical effectiveness, and SPLs are
produced using clinical trials on drug effectiveness, it
follows that the evidence base for CPG recommenda-
tions and SPL indications should support similar pre-
scribing practices.
Text mining of biomedical texts is increasingly per-
formed to extract associations from otherwise machine-
inaccessible text. Electronic health record documents, such
as clinical notes and discharge summaries, published scien-
tific literature, and SPLs are all well-established corpora for
text and natural language processing. Dictionary-based
named entity recognition (NER) systems and machine
learning approaches have been applied to identify entities,
including drugs and diseases, in such texts, [37] however,
to our knowledge, text mining of clinical practice guide-
lines for these entities had not been done until recently. In
a previous study, we applied dictionary-based NER as a
text mining method to identify disease co-mentions for
common comorbid chronic conditions in chronic disease
clinical practice guidelines [8]. We focused on 15 common
chronic conditions, including obesity, and 14 of the 15
most prevalent chronic conditions among Medicare benefi-
ciaries: hypertension, diabetes mellitus, hyperlipidemia,
stroke, asthma, atrial fibrillation, Alzheimers dementia and
senile dementias, osteoporosis, chronic obstructive pul-
monary disease, depression, chronic kidney disease, heart
failure, arthritis, and ischemic heart disease [9]. In that
study, ontologies from Stanford Universitys National Cen-
ter for Biomedical Ontologies were compiled into a com-
prehensive dictionary of disease concepts for the NER task.
Initial evaluation yielded reasonable precision and recall
with this approach. While annotating biomedical or clinical
text is not a novel concept, the current study uniquely ex-
amines clinical practice guidelines, a text corpus not previ-
ously studied with this approach. Additionally, the current
study aims to demonstrate proof-of-concept of evaluating
drug-disease associations in clinical practice guidelines.
In previous unpublished work, we constructed a diction-
ary of drug concepts to use in the NER task of mining
pharmacologic treatment recommendations in CPGs. Drug
concepts were utilized as a flat list without utilizing avail-
able taxonomic information available [10]. We focused on
the same 15 chronic conditions to examine how well CPG
recommendations about pharmacologic treatment options
for the chronic conditions match with SPLs that include
one of these conditions as a treatment indication. We
found that our recall was low because we did not account
for drug classes in the drug-disease associations. We
reasoned that differences in the language and structure
of CPGs and SPLs may contribute to differences in identi-
fied drug-disease associations in CPGs and SPLs. For ex-
ample, a CPG on heart failure may recommend using an
angiotensin-converting enzyme inhibitor, a drug class ra-
ther than a specific drug. However, a SPL for a specific
drug, such as lisinopril, would specify heart failure as an in-
dication. In this case, angiotensin-converting enzyme inhibi-
tor-heart failure in a CPG drug-disease association should
also match a similar drug-disease association in SPLs, such
as lisinopril-heart failure. Simple term recognition methods
have an advantage of scaling well to larger datasets with lit-
tle to no impact on accuracy, compared to advanced nat-
ural language processing methods [11]. Given CPG text has
not previously been mined before, it was reasonable to
apply NER as the initial method for CPG text processing,
although the approach has been applied to clinical notes,
biomedical literature, and SPLS previously [37].
In this work, we use terminologies with structured hier-
archies to improve our approach. We utilized the parent-
child relationships from the taxonomic structure of a drug
classification to find class-based matches for ontologically
related terms. To accomplish this, we selected one ontol-
ogy that had the highest precision when drug names were
manually reviewed in a subset of guideline recommenda-
tions in the text corpus. We hypothesize that drug-disease
associations in CPG recommendations should overlap
with drug-disease associations in SPL treatment indica-
tions when drug classes and drug names are matched using
taxonomic relationships. This would suggest that FDA-
approved indications for drugs and guideline-recommended
pharmacologic therapies for certain conditions reinforce
similar evidence-based drug prescribing.
Leung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 2 of 10
Methods
First, we constructed a text corpus containing guideline
summaries relevant to the 15 chronic conditions of inter-
est. Then, we created a comprehensive vocabulary of
terms for the chronic conditions and performed named
entity recognition to identify drug names and drug classes
in the text corpus. Next, we performed an evaluation of
the method of constructing CPG drug-disease associa-
tions. Finally, we compared the overlap between the two
sets of drug-disease associations for each chronic condi-
tion (Fig. 1). All files used and produced during this study
will be available for download at https://github.com/
tileung/DrugsInCPGs.
To construct drug-disease associations from the text of
CPG recommendations and SPL treatment indications,
we apply text mining methods using an expanded set of
drug and disease names from multiple terminological
resources with taxonomic structure, such as NDF-RT
(National Drug Formulary  Reference Terminology) and
MESH (Medical Subject Headings), and a data source on
drugs, SIDER. A drug-disease association in a CPG is de-
fined as the occurrence of a drug name mention at least
one time in a guidelines recommendations. A drug-dis-
ease association in a SPL is defined as the occurrence of a
chronic condition mention at least one time within the
Indications section of a SPL.
Data sources
We used data and resources from multiple publicly
available data sources: (1) guideline summaries from the
National Guideline Clearinghouse, (2) drug product label
and indication data from SIDER, (3) chronic disease data
definitions from the Medicare Chronic Conditions Data
Warehouse, and (4) disease and drug ontologies from the
National Center for Biomedical Ontology and ABER-Owl
Repository [12].
National guideline clearinghouse
The National Guideline Clearinghouse (NGC), first devel-
oped in 1997, identifies published CPGs that meet inclu-
sion criteria and summarizes their highlights across 54
guideline attributes, such as Guideline Title, Major Rec-
ommendations, and Target Population [13, 14]. For each
guideline, the Major Recommendations section includes
summarized key recommendations as indexed by the Na-
tional Guideline Clearinghouse. Each guideline summary
is also tagged with Unified Medical Language System
(UMLS) Metathesaurus concepts, identifying major areas
of clinical medicine or health care addressed in the guide-
line [15]. The NGC then indexes the guideline summaries
on a publicly accessible website for retrieval in multiple
formats, including XML and HTML. In June 2014, the
NGC implemented a new set of inclusion criteria for
guidelines included in the NGC repository [1]. As of
September 2015, the NGC featured more than 2400
guideline summaries. NGC guideline summaries, in com-
bination with a comprehensive drug vocabulary con-
structed in this study, were the source of drug-disease
associations in CPGs.
SIDER
SIDER is a publicly available resource that interprets and
extracts information from text and tables from FDA-
approved drugs SPLs, identifying side effects and medical
conditions in the SPLs using UMLS concepts [15]. Each
SPL contains a structured section on Indications, which
specifies diseases or clinical conditions for which the drug
is FDA-approved for use. SIDER 2 was the source of drug-
disease associations in SPLs in this study.
Medicare chronic conditions data warehouse
The Centers for Medicare and Medicaid Services provides
a research database, the Chronic Conditions Data Ware-
house (CCW), of Medicare beneficiaries chronic disease
care. Chronic conditions are defined by ICD-9 codes in
the CCW data dictionary available since 2010 [16].
BioPortal
The National Center for Biomedical Ontology (NCBO)
[17], based at Stanford University, provides online tools
for accessing and integrating ontological resources, in-
cluding BioPortal, a repository of biomedical ontologies.
Fig. 1 Pipeline for generating and comparing drug-disease associations in clinical practice guidelines and structured product labels
Leung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 3 of 10
BioPortal contained more than 460 biomedical ontologies
as of September 2015. ATC (Anatomical Therapeutic
Chemical Classification) was included and obtained from
Bioportal because this ontology contains high-level drug
classes as well as related drug formulations and ingredi-
ents. For similar reasons, NDF-RT was also included, and
was obtained directly from the National Library of Medi-
cine. In NDF-RT, certain parent classes and their children
were included, specifically, Chemical/Ingredient, External
Pharmacologic Class, VA Product, Mechanism of Action,
and Therapeutic Categories.
Aber-OWL repository
Aber-OWL is a framework that consists of an ontology re-
pository, as well as web services that enable ontology-based
semantic access to biomedical knowledge [12]. Specifically,
additional ontologies and their semantic knowledge were
obtained from Aber-OWL, including MESH (Medical
Subject Headings), NCIT (National Cancer Institute
Thesaurus), and CHEBI (Chemical Entities of Biological
Interest Ontology), in order to further expand the drug
vocabulary. Only subsets of these ontologies were re-
trieved. For instance, we restricted the set of MESH terms
to subclasses of organic chemicals, chemical actions and
uses, pharmaceutical preparations and polycyclic com-
pounds. For NCIT, we restricted to drug, food, chemical
or biomedical material. Finally, for CHEBI we restricted
the classes to those under role and organic molecule.
Guideline recommendations text corpus
A corpus of guideline summaries was obtained from the
website of the National Guideline Clearinghouse. Previ-
ously, guideline summaries were obtained from the NGC
website in XML format [8], however, an updated text cor-
pus of guideline summaries was constructed in September
2015 because the NGC updated inclusion criteria for
guideline summaries. We obtained 445 ICD-9 codes from
Medicare CCW data dictionary to identify 14 of the com-
mon chronic conditions of interest [9], and added three
additional ICD-9 codes for the 15th condition, obesity.
The 448 ICD-9 codes representing concepts for the 15
chronic conditions were then mapped to UMLS concept
unique identifiers (CUIs). Using the NGC RSS feed, avail-
able in XML format, a total of 2472 guideline documents
were identified. The mapped CUIs for the 15 chronic con-
ditions were used to identify UMLS concepts tagged to
each guideline summary and identified relevant guideline
summaries for retrieval and build the text corpus. Initially,
505 relevant guideline documents were identified. Manual
review of the retrieved guideline documents revealed that
three were expert commentaries, a different type of NGC
summary, and these were excluded. Guideline summaries
were also excluded from the text corpus if the target pa-
tient population for the guideline was exclusively pediatric
patients, pregnant or breastfeeding women, or for a med-
ical diagnosis that was not among the 15 common chronic
conditions. Additionally, 17 guideline summaries were ex-
cluded because they were not available in XML format
from the NGC website. After exclusion criteria, 377 NGC
relevant guideline summaries remained for inclusion
(Fig. 2). We extracted the Major Recommendations sec-
tion from each guideline summary in order to build the
text corpus because this section would be the most likely
of all sections in the summaries to contain pharmacologic
recommendations.
Text mining for drug names
We constructed a comprehensive drug vocabulary of
97,079 drug names from five ontologies. We performed
named entity recognition of these drug names in each of
the 377 guideline summaries and identified 1986 unique
drug names in the text. We compared the drug-disease
associations in CPGs with the drug-disease associations
in SPLs. We also examined the overlap between the two
sets of drug-disease associations for each chronic condi-
tion. To evaluate the approach, a subset of five heart fail-
ure guideline summaries were manually annotated with
drug names and drug classes to build a reference standard,
as there is no existing set of annotated CPGs to perform
this evaluation. Additionally, CPGs may have representa-
tions of drug names or drug classes that may not appear
in alternative clinical or biomedical texts. The approach
was evaluated for each of the five ontologies in order to
identify one ontology with the highest precision. The
selected ontology was then applied to map drug names
and drug classes using the existing taxonomic relation-
ships and generate the final set of drug-disease associa-
tions in CPGs.
Results
We identified 1986 unique drug names in the corpus of
377 clinical practice guideline summaries from the Na-
tional Guideline Clearinghouse. We found 1109 unique
drug-disease associations in CPGs. We identified 533
SPLs with an indication for one of the 15 chronic condi-
tions. We obtained 449 unique drug-disease associations
from the SPLs.
Evaluation
To evaluate the approach of identifying drug names in
CPG text, one annotator with medical expertise (TL)
manually annotated drug names and drug classes in five
guideline summaries for heart failure to construct a ref-
erence standard using TextAE, a text annotation client
[18]. To guide manual annotation, occurrences of drug
names and drug classes that can be prescribed were an-
notated. For example, in one guideline summary, For
patients with systolic dysfunction (EF <40 %) who have
Leung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 4 of 10
no contraindications: Angiotensin-converting enzyme
(ACE) inhibitors for all patientsDigoxin only for pa-
tients who remain symptomatic despite diuretics, ACE
inhibitors and beta blockers or for those in atrial fibrilla-
tion needing rate control, [19] the concepts Angioten-
sin-converting enzyme (ACE) inhibitors and ACE
inhibitors were annotated drug classes and considered
synonymous, digoxin was a drug name, and diuretics
and beta blockers were drug class names. From the
guidelines annotated manually, a total of 178 annota-
tions of drug-disease associations were obtained. We
compared the manual annotations in the reference
standard with the annotations collected from applying
the constructed drug vocabulary from each of the five
terminologies: ATC, CHEBI, MESH, NCIT, and NDF-
RT. Precision, recall, and F-measure were calculated for
each terminology in order to identify the most appropri-
ate terminology for the NER task performed on the clin-
ical practice guideline corpus. For this task, a high
precision is desirable, where identified drug names and
classes using one of the terminologies is more predictive
of a true positive identification of a drug name or class
in the text. Of the five terminologies, ATC yielded the
highest precision of 0.75 and recall of 0.47 (Table 1). For
this reason, the taxonomic relationships in ATC were
used to perform drug class and drug name matching to
produce drug-disease associations in CPGs.
Drug name and drug class matching
We utilized ATC (Anatomical Therapeutic Chemical
Classification), a commonly used drug classification pro-
duced by the World Health Organization [20], to identify
matches of different drug names to their parent classes.
For drugs included in the set of drug-disease associations
in SPLs, ATC codes were applied to the drug names if
available by using the World Health Organizations index
of ATC codes. The most specific, or descendent, drug
name was used to construct drug-disease associations in
CPGs. For example, eplerenone (C03DA04) - ischemic
heart disease was a drug-disease association identified in
both SPLs and CPGs; however, aldosterone antagonists
(C03DA) - ischemic heart disease was a drug-disease asso-
ciation in CPGs only and not in SPLs. In this case, these
were considered overlapping, or identical, drug-disease
associations in CPGs and SPLs because eplerenone is a
descendent of the class of aldosterone antagonists. In
another example, pneumococcal vaccines (J07AL) - heart
failure was a drug-disease association in CPGs, and no
additional descendants of the class of pneumococcal
vaccines were identified. In this case, this was included as
a drug-disease association in CPGs only. Additionally,
drug names that were descendants of the same parent
class were considered similar. For example, eplerenone
(C03DA04) - heart failure and spironolactone (C03DA01)
- heart failure are similar because both descendants of the
class of aldosterone antagonists (C03DA).
In previous work [10], without matching drug names
and drug classes, there was minimal overlap between
Fig. 2 Inclusion diagram for guideline summaries from the National Guideline Clearinghouse
Table 1 Evaluation metrics for each drug terminology identifying
drug names and drug classes in guideline summaries
Precision Recall F-measure
ATC 0.75 0.47 0.58
MESH 0.5 0.02 0.04
NCIT 0.31 0.32 0.32
NDF-RT 0.25 0.1 0.14
CHEBI 0.25 0.02 0.04
Leung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 5 of 10
drug-disease associations in CPGs and SPLs for all the
chronic conditions (Fig. 3). Without taxonomic informa-
tion, we identified 1444 drug-disease associations across
CPGs and SPLs for 15 common chronic conditions. Of
these, 195 drug-disease associations overlapped between
CPGs and SPLs, 917 associations occurred in CPGs only
and 332 associations occurred in SPLs only. After matching
using taxonomic information, 859 unique drug-disease as-
sociations were identified across CPGs and SPLs. Of these,
152 of these drug-disease associations overlapped between
CPGs and SPLs. This means that CPGs mentioned 541
drug-disease associations that were not also mentioned in
SPLs across all conditions; conversely, SPLs mentioned 166
drug-disease associations that were not also mentioned in
CPGs across all conditions. The frequency of drug-disease
associations in CPGs, SPLs, or both varies depending on
which chronic disease guidelines are of interest (Fig. 4).
Discussion
Our results suggest that guideline-recommended pharma-
cologic therapies and drug product label indications are
reasonably well-matched when taxonomic relationships be-
tween drug names and drug classes are incorporated into
the text mining approach. Our approach, using both drug
names and drug classes, produced superior results over
our previous work in which taxonomic relationships were
not incorporated into the text mining approach, which
resulted in a larger number of mismatches between
guideline-recommended pharmacologic therapies and drug
product label indications. Overall, the current study dem-
onstrated proof-of-concept that NER, in combination with
taxonomic information, can be helpful in identifying drug-
disease associations in clinical practice guidelines. It is pos-
sible that existing NER and natural language processing
systems could be similarly applied out-of-the-box to
the text corpus [11]. In clinical practice, knowledge of
whether there is consistent and clear medical evidence
in both CPGs and SPLs to support certain prescribing
practices is informative in the medical-decision making
process and personalization of care to the individual pa-
tient. Additionally, areas of consistency in the medical evi-
dence in CPGs and SPLs is supportive in the application
of such knowledge into best prescribing practices that
could be incorporated into clinical information and deci-
sion support systems, which is the highest level of applica-
tion of evidence-based medicine [2].
Overlapping drug-disease associations
In the approach described here, 541 drug-disease associa-
tions were identified in both CPGs and SPLs. An increased
overlap between two sources was obtained when informa-
tion about parent-child relationships between drug classes
and drug names was used into the text mining approach.
Additionally, the text corpus in this work reflects the most
recent NGC guideline summaries as of September 2015,
and exclusion criteria were applied to ensure that relevant
chronic condition guidelines were included in the study.
As of June 2014, the NGC implemented a new set of inclu-
sion criteria to ensure that accepted guidelines provide
adequate documentation of their process for systematic re-
view of literature as the basis for the recommendations.
This resulted in the retirement of existing guidelines in the
NGC repository that no longer met the required criteria.
Exclusion criteria appropriately ensured that the included
guidelines summaries were applicable to the 15 selected
highly prevalent chronic conditions in the Medicare popu-
lation. As a result of these updates, a corpus of 377 guide-
line summaries was included.
Fig. 3 Overlap between drug-disease associations between the two sets of drug-disease associations for each chronic condition, without using
taxonomic information on drug names and drug classes
Leung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 6 of 10
CPG drug-disease associations not in SPLs
Our results demonstrate that CPGs contain drug-disease
associations that are not also identified in SPLs. One pos-
sible explanation is that the natural language of CPGs and
SPLs inherently differ, where CPGs may recommend a
drug class for a particular condition rather than a specific
drug. Additionally, NGC guideline summaries may origin-
ate from guidelines produced by professional societies or
organizations worldwide, contributing to CPG recommen-
dations for similar drugs of the same drug class. We pri-
marily addressed this limitation by utilizing hierarchical
relationships to better match drug-disease associations.
However, there remain non-overlapping drug-disease as-
sociations. Another possible explanation may be that some
CPGs may recommend off-label drug prescribing, which
would by definition not be found in SPLs. For example,
one CPG recommendation for diabetic neuropathy man-
agement states to offer a trial of duloxetine, gabapentin
or pregabalin if a trial of tricyclic drug does not provide ef-
fective pain relief [21]. However, gabapentin does not
have a FDA-approved indication for use in diabetic per-
ipheral neuropathy, while duloxetine and pregabalin do
have such an indication in their SPLs. Further, it is pos-
sible that CPGs may consider utilizing the best evidence
available from less robust studies in making recommenda-
tions, even though they are intended to be based on
systematic reviews of best evidence, which may not be
available. In such cases, CPGs typically also convey the
strength of evidence for the recommendation. This may
mean that CPGs might weakly recommend a certain drug
in the treatment of a chronic condition. In contrast, such
a process does not exist in the production of SPLs and a
SPL for a FDA-approved drug would not suggest prescrib-
ing a drug without data on its safety and efficacy from
clinical trials. Weak recommendations in CPGs and off-
label indications of drugs in CPGs may present opportun-
ities for post-marketing surveillance of the drug for the
suggested prescribing practices or may be opportunities
for further study towards drug repurposing.
SPL drug-disease associations not in CPGs
Our results suggest that there are indications from SPLs
that are not mentioned in CPGs, even though hierarch-
ical information improved the overlap. One possible ex-
planation is that accurate identification of drug-disease
associations in SPLs is necessary. Manual validation of
the drug-disease associations in SPLs identified from
SIDER would ensure that there is in fact an FDA-approved
indication for one of the chronic conditions in each SPL.
Another possible explanation is that there may be a delay
in integrating the evidence from FDA-approved treatment
indications into CPG recommendations. Guideline devel-
opment can be a prolonged and labor-intensive process,
during which new evidence may become available before a
guidelines finalization and approval. This would require
further investigation, and may also present important op-
portunities to streamline the process of implementing
medical evidence on the efficacy of newly approved drugs
into CPG updates and best practices in clinical medicine.
Limitations
Our approach is not without limitations. First, a primary
objective of this study was to demonstrate proof-of-concept
that NER, in combination with taxonomic information, can
be helpful in identifying drug-disease associations in clinical
practice guidelines. Out-of-the-box NER and natural lan-
guage processing systems may perform as well as or better
than the current approach. Additionally, the evaluation of
Fig. 4 Overlap between drug-disease associations between the two sets of drug-disease associations for each chronic condition, using taxonomic
information on drug names and drug classes
Leung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 7 of 10
the CPG text mining method could be more robust 
including a larger manually annotated set of guideline
summaries, for a variety of chronic conditions and by at
least two annotators. Annotating a larger set of guidelines
for the evaluation, rather than focusing solely, on heart fail-
ure, facilitate a more robust evaluation. Second, utilizing
existing annotated clinical or biomedical corpora [4, 22]
would allow evaluation of the approach against existing
NER tools.
Additional limitations include the guideline repository
used to construct the text corpus. Because we utilized
the NGC-formulated Major Recommendation section of
guideline summaries, these may not contain all the drug
mentions found in the full-text documents as published
by the original developers. However, if recommendations
were made on pharmacologic treatments, then these
would likely be identified in the NGC guideline summar-
ies. Accessing and text mining corresponding full text arti-
cles will properly assess whether significant differences
exist. Additionally, the National Guideline Clearinghouse
is the largest available guideline repository that also has a
well-indexed, structured, and selective set of guidelines for
inclusion. While the current approach was designed to
facilitate the process of examining a large corpus of guide-
lines, after NGC inclusion as well as application of inclu-
sion criteria for this investigation, a relatively small set of
guideline summaries remained. Further improvements of
the text mining approach may be necessary to ensure ac-
curate information retrieval from the small set of guideline
summaries.
Examining drug names and drug classes occurring in
text may not be adequate alone. Co-reference resolution,
in which a named entity may reference the entity of
interest, is a common challenge in text mining and nat-
ural language processing tasks and also may impact the
current findings. Additionally, we do not extract a pre-
cise relationship between a drug mention in a disease
CPG. Relation extraction of the context of drug name or
class occurrence in the text may better inform the drug-
disease association identified [23]. Here, we performed
an initial assessment of drug-disease associations in
CPGs, and additional methods may better disambiguate
the meaning and context of each drug mention in the
text. Initial manual examination of a random sample of
20 drug-disease associations yielded seven types of indi-
cation relationships (true positives), five other types of
relationships (false positives), and two drug-disease associ-
ation misclassifications. These findings can inform future
work improving the text mining approach. Expanding the
review to 30 drug-disease associations yielded a higher fre-
quency of all of the indication relationships but did not
change the numbers of types of relationships identified on
the initial review. Of the 30 drug-disease associations,
there were two drug misclassifications where the context
of a drug mention in CPG text was not as a prescribable
drug (stroke-oxygen and diabetes mellitus-glucose). Of the
remaining 28 drug-disease associations and their source
guideline summaries, nine overlapped with drug-disease
associations in SPLs. Of the drug-disease associations
identified, there were 27 occurrences of seven types of in-
dication relationships, including having an indication: (1)
for only the primary disease, (2) for a patient characteristic
present with the primary disease, (3) in a specific clinical
setting, (4) in combination with another drug for the pri-
mary disease, (5) as alternative or non-first-line therapy,
(6) for prevention of a comorbid condition, and, most
frequently, (7) for the primary disease when another co-
morbid condition was also present. These relationships
represent true positives of drug-disease associations in
CPGs. Additional relationships included: drug causes the
disease as an adverse effect, drug has a contraindication
for the primary disease, drug necessitates additional moni-
toring requirements in the setting of the disease, and no
recommendation can be made about the indication of a
drug for a disease. These relationships represent false pos-
itives of drug-disease associations. The risk of false posi-
tives may be mitigated by additional text processing
depending on the relationship extracted, for example, for
contraindications, examining the Contraindications sec-
tion of a SPL may be a useful task. Among the remaining
20 drug-disease associations in CPGs that did not overlap
with SPLs, four of the indication relationships were repre-
sented (for only the primary disease, in combination with
another drug for the primary disease, for prevention of a
comorbid condition, and for the primary disease when an-
other comorbid condition was also present). In some
cases, the indication was off-label for the primary disease,
for example, one guideline on atrial fibrillation states,
Where oral anticoagulants are unavailable, clinicians
might offer a combination of aspirin and clopidogrel [24].
In this case, clopidogrel has FDA-approved indications
only for acute coronary syndrome and recent myocardial
infarction, stroke or established peripheral arterial disease
[25]. Detailed and thorough manual review of a larger set
of drug-disease associations would provide additional
insight about the types of relationships between drugs and
diseases in CPGs, and also would be informative in improv-
ing the text mining approach. Although labor-intensive to
perform a detailed review manually, this would be an im-
portant contribution to this field, as clinical practice guide-
lines have only recently been used as a corpus for text
mining.
Finally, SIDER is a database of curated drug-side effect
pairs as the primary database, but may include false posi-
tives when examined for drug-disease associations in the
indications sections of SPLs. At the time of revised sub-
mission of this manuscript for publication, a newer ver-
sion of SIDER was published that determines its accuracy
Leung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 8 of 10
against other resources and estimates adverse event mis-
classification [26]. Utilizing the latest dataset of SPLs from
SIDER 4 may facilitate more accurate identification of
drug-disease associations from SPLs for future work.
Conclusions
Our work offers a first look at the overlap in CPG and
SPL content with respect to drug-disease associations.
Our results suggest that CPG-recommended pharmaco-
logic therapies and SPL indications do not overlap fre-
quently when identifying drug-disease associations using
named entity recognition, although incorporating taxo-
nomic relationships between drug names and drug classes
into the approach improves the overlap. Mismatches be-
tween guideline-recommended pharmacologic therapy
and FDA-approved drug indications may have a number
of implications, including presenting practical challenges
in evidence-based clinical practice, such as adding com-
plexity to clinical decision making and implementation or
measurement of best practices. Further evaluation and im-
provement of our methods may be necessary, including
examining the relationship between a chronic condition
and a drug in a guideline or drug label. Additionally, man-
ual annotation of a larger reference standard or use of
existing annotated biomedical or clinical corpora will be
relevant to evaluate our approach and how well it per-
forms for each chronic condition. Finally, a detailed man-
ual review of areas where drug-disease associations do not
overlap between CPGs and SPLs would be informative,
potentially guiding opportunities for further investigation
about areas of uncertainty in drug prescribing and their
indications. This study is a first step towards further un-
derstanding of CPGs and SPLs as congruent resources for
evidence-based clinical practice.
Acknowledgements
We thank the Bio-Ontologies Special Interest Group for their recognition of this
project with an Outstanding Presentation Prize at the groups workshop at the
23rd Annual International Conference on Intelligent Systems for Molecular Biology.
We thank our anonymous reviewers for their thorough and insightful comments
that improved the quality of the manuscript.
Availability of data and materials
All files used and produced during this study will be available for download
at https://github.com/tileung/DrugsInCPGs.
Authors contributions
TL carried out text corpus construction and design of the study, performed
programming for text mining drug-disease associations in guidelines, evaluated
the method, and drafted the manuscript. MD carried out vocabulary construction,
generated queries to obtain drug-disease associations from structured product
labels, and helped draft the manuscript. Both authors read and approved the final
manuscript.
Authors information
TL is a Clinical Assistant Professor and practicing general internist in
Stanford Primary Care and MD is an Associate Professor of Medicine in
Stanford Department of Medicine.
Competing interests
The authors declare that they have no competing interests.
Author details
1Division of General Medical Disciplines, Stanford University, Stanford, CA, USA.
2Stanford Center for Biomedical Informatics Research, Stanford University,
Stanford, CA, USA.
Received: 1 November 2015 Accepted: 24 May 2016
Conway et al. Journal of Biomedical Semantics  (2016) 7:5 
DOI 10.1186/s13326-015-0043-z
SOFTWARE Open Access
Developing a web-based SKOS editor
Mike Conway1*, Artem Khojoyan2, Fariba Fana3, William Scuba1, Melissa Castine1,
Danielle Mowery1, Wendy Chapman1 and Simon Jupp4
Abstract
Background: The Simple Knowledge Organization System (SKOS) was introduced to the wider research community
by a 2005 World Wide Web Consortium (W3C) working draft, and further developed and refined in a 2009 W3C
recommendation. Since then, SKOS has become the de facto standard for representing and sharing thesauri, lexicons,
vocabularies, taxonomies, and classification schemes. In this paper, we describe the development of a web-based,
free, open-source SKOS editor built for the development, curation, and management of small to medium-sized
lexicons for health-related Natural Language Processing (NLP).
Results: The web-based SKOS editor allows users to create, curate, version, manage, and visualise SKOS resources.
We tested the system against five widely-used, publicly-available SKOS vocabularies of various sizes and found that
the editor is suitable for the development and management of small to medium-size lexicons. Qualitative testing has
focussed on using the editor to develop lexical resources to drive NLP applications in two domains. First, developing a
lexicon to support an Electronic Health Record-based NLP system for the automatic identification of pneumonia
symptoms. Second, creating a taxonomy of lexical cues associated with Diagnostic and Statistical Manual of Mental
Disorders (DSM-5) diagnoses with the goal of facilitating the automatic identification of symptoms associated with
depression from short, informal texts.
Conclusions: The SKOS editor we have developed is  to the best of our knowledge the first free, open-source,
web-based, SKOS editor capable of creating, curating, versioning, managing, and visualising SKOS lexicons.
Background
The Simple Knowledge Organization System (SKOS)
standard was introduced to the wider community by a
2005 World Wide Web Consortium (W3C) working draft
[1], and further developed and refined in a 2009W3C rec-
ommendation [2, 3]1. Since then, SKOS has become the de
facto standard for representing thesauri, lexicons, vocab-
ularies, taxonomies, and classification schemes, both as
a useful data format in its own right, and as a means
for sharing resources on the semantic web. In this paper,
we describe the development of a web-based, free, open-
source SKOS editor suitable for the creation and curation
of knowledge organization systems in general, and health-
related lexicons designed to support clinical Natural
Language Processing (NLP) in particular.
SKOS is a flexible standard designed to represent and
encode a wide number of different types of knowledge
*Correspondence: mike.conway@utah.edu
1Department of Biomedical Informatics, University of Utah, 421 Wakara Way,
Salt Lake City, UT 84108, United States
Full list of author information is available at the end of the article
organization systems, including vocabularies, thesauri,
and classification systems. The standard is widely used
by governments [4] (e.g. United Kingdom Public Sector
Vocabularies, French National Library Subject Headings,
United States Library of Congress Subject Headings),
scientific bodies (e.g. International Virtual Observatory
Alliance Astronomy Vocabulary,NASA vocabularies,The-
saurus for the Social Sciences), and non-governmental
organisations (e.g. Wikipedia categories, UNESCO The-
saurus, General Multilingual Environmental Thesaurus).
In contrast to its sibling World Wide Web Consortium
semantic web standard, the Web Ontology Language
(OWL), SKOS follows the principle of minimal ontolog-
ical commitment [3]. That is, SKOS concepts and rela-
tions are lightly specified, using thesaurus-style relations
like broader rather than logically formalised relations
commonly used in OWL (e.g. IS_A).
SKOS models consist of concept schemes which serve
as containers for concepts. Concepts can be related
© 2016 Conway et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Conway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 2 of 9
together in various ways to create a hierarchical struc-
ture. The most important of these semantic relations
are:
 skos:broader can be read as has broader
concept. For instance, the relation PHOTOPHOBIA
skos:broader VISIONPROBLEM, asserts that
PHOTOPHOBIA has broader concept
VISIONPROBLEM.
 skos:narrower which can be read as has
narrower concept. For instance, the relation
VISIONPROBLEM
skos:narrower PHOTOPHOBIA, asserts that
VISIONPROBLEM has narrower concept
PHOTOPHOBIA.
 skos:related can be read as is related to. For
instance, the relation PHOTOPHOBIA
skos:related
DIPLOPIA, asserts that PHOTOPHOBIA is related to
DIPLOPIA.
Each SKOS concept can be associated with several types
of lexical labels:
 skos:prefLabel (preferred label ) provides a
mechanism to link a preferred label to a concept. The
prefLabel is the primary means of referring to a
concept. Only one prefLabel per language should be
assigned to each concept. For example, the SKOS
concept FEVER could have the skos:prefLabel
fever@en (note that @en refers to English
language).
 skos:altLabel (alternative label ) provides a
mechanism to specify synonyms or near-synonyms
for a given concept. For example, the concept FEVER
could have the skos:altLabel febrile@en.
This relation is especially useful for specifying
synonymous terms necessary for NLP.
 skos:hiddenLabel (hidden label ) provides a
mechanism to specify non-standard synonymous
terms (e.g. misspellings, typographical errors). For
example, the concept FEVER could have the
skos:hiddenLabel feber@en. Hidden labels
are particularly useful for encoding common
misspellings necessary for NLP systems.
In addition to the semantic relations and lexical labels
described above, SKOS also provides facilities to add addi-
tional metadata to concepts and map SKOS concepts to
external vocabularies.
Given its lightweight semantics, SKOS is particularly
suitable as a basis for the development and sharing of
vocabularies to support NLP tasks. A key part of the work-
flow in developing some NLP systems  in particular
NLP systems designed to process health-related text  is
the development of custom lexicons, including common
abbreviations, synonyms (including slang terms), and
truncations [58].
Since its inception in 2005, significant effort has been
expended on the development of software tools for the
SKOS standard, in particular in editing and viewing SKOS
vocabularies. Whilst OWL editors, such as Protégé2, can
be used to create and edit SKOS, they require a user
to understand SKOS in terms of OWL; an unnecessary
overhead for a user simply interested in creating SKOS.
Furthermore, a major requirement for a SKOS editing
tool is the ability to visualise and navigate SKOS con-
cept scheme broader/narrower hierarchies, a functional-
ity that is unlikely be supported by generic OWL and
RDF (Resource Description Framework) tools. Notable
examples of SKOS aware tools include a SKOS Appli-
cation Programming Interface (API) and editing module
[9] for Protégé 43 (the Protégé SKOS Editor), PoolParty,
an online SKOS editing and manipulation tool [10], and
SKOS functionality built into the TopBraid Composer
RDF editing platform [11]4, all of which facilitate the
creation, development, and utilisation of SKOS vocab-
ularies. However, to the best of our knowledge, until
now no free, open-source, web-based SKOS editor has
been available to the research community (note that Pool-
Party, although web-based, is a commercial product). In
this paper, we present a web-based SKOS editing tool
that is suitable for developing and modifying the health-
related lexicons necessary for large-scale information
extraction from clinical notes and other health-related
text, yet is also general purpose enough for any small-to-
medium-sized SKOS vocabulary development or curation
project.
Implementation
A key advantage of using a web-based editor, is that it
can be used anywhere, on any machine, without com-
plex user installation. Given that our target users are
clinicians, public health workers, and domain experts 
i.e. those with little or no experience of semantic web
languages  rather than informatics professionals, ease
of use is an important requirement. We took the deci-
sion to simplify the editors user interface as much as
possible, hiding some of the general OWL/RDF func-
tionality available in tools like Protégé and TopBraid
Composer.
Considerable effort was expended on designing the user
interface (a screenshot of the system is shown in Fig. 1
showing a SKOS thesaurus designed to drive a NLP sys-
tem for the automatic identification of biosurveillance-
relevant symptoms from Electronic Health Records
(EHRs) [12]). After some experimentation, we adopted
an interface that consists of three panes, from left to
right:
Conway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 3 of 9
Fig. 1 Screenshot of the system interface showing a biosurveillance lexical resource, with a new concept pop-up
 CONCEPT PANE: An editable taxonomic hierarchy of
SKOS concepts representing skos:broader and
skos:narrower relations, which the user can click
on to expand and collapse the tree
 RELATIONS PANE: An editable list of relations
between concepts, particularly the skos:related,
skos:broader, and skos:narrower relations
 LINGUISTICS PANE: An editable list of lexical items
related to each SKOS concept (e.g.
skos:prefLabel, skos:altLabel,
skos:hiddenLabel)
We identified six core functionalities necessary for the
editor, partially based on the requirements identified
by [9]:
 Create, edit, and delete SKOS entities
 Assert SKOS relationships between SKOS concepts
(e.g. broader/narrower)
 Assert and edit skos:prefLabel,
skos:altLabel, and skos:hiddenLabel data
properties
 Visualise broader and narrower relationships in a
browsable hierarchical tree
 Support for SKOS documentation properties
 Provide alternative renderings (e.g. multilingual
prefLabels) within the editor
Additionally, our editor provides versioning, and a Wiz-
ard tool to expedite the SKOS concept hierarchy creation
process.
In building our web-based SKOS Editor, we relied heav-
ily on existing OWL, SKOS and RDF tooling, in particular,
the SKOS API [9] (developed by author Jupp) and the
OWL API [13]. The system is a Liferay portlet application
that uses a standard Model-View-Controller architecture
implemented using the following technologies:
 Business (Model) Layer: Java SKOS API and OWL
API
 Presentation (View) Layer: JavaScript/JSP/JQuery
Libraries provides a rich web 2.0 user interface
connected to the middle layer via AJAX calls
 Controller/Middle Layer: The Liferay Portlet
application using the JSR 286 Portlet framework
connects the presentation layer to the SKOS API, as
well as providing user management, authorisation,
and authentication.
Conway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 4 of 9
AMySQL database is used to save files and file versions,
as well as user specific settings. The application is a Single
Page Application, with all server/client communication
based on Ajax calls using a JQuery library (client-side) and
Liferay portlet (server-side).
A screenshot of the system interface is shown in Fig. 1
and a diagram representing the system architecture is
shown in Fig. 2.
Results and discussion
The web-based SKOS editor allows a user to upload a
SKOS file from their local machine for editing, load a
SKOS file from a URL, create a SKOS file ab initio,
and download an edited SKOS file to a local machine.
Furthermore, the editor supports versioning of SKOS
files, and provides a GUI-based Wizard to expedite
the creation of concept hierarchies. The Wizard allows
a user to input a plain-text tab indented concept hier-
archy, a functionality that has been shown in our qual-
itative user testing to expedite the hierarchy creation
process (see Fig. 3 and Additional file 1). The tool takes
its inspiration from the Protégé SKOS editor developed
by author Jupp, and supports core SKOS functional-
ities. In the Concept Pane, SKOS concept schemes
and concepts can be created and manipulated with a
hierarchical tree structure. The Relation Pane shows
hierarchical relations defined in the concept pane, and
allows these relations to be modified, including the addi-
tion of non-hierarchical relations between concepts. The
Linguistics Pane allows lexical information  prefLa-
bels, hiddenLabels, altLabels  to be associated with each
concept.
While there have been attempts at developing best
practices for SKOS thesauri development (e.g. [3, 14])
considerable heterogeneity exists between different SKOS
resources [15]. We built an editor that is designed to han-
dle even those SKOS resources that do not adhere to
suggested best practice (e.g. the thesauri has more than
one prefLabel for a specified language, or a SKOS concept
exists outside a Concept Scheme).
Loading and editing sample SKOS vocabularies
In order to demonstrate and test the capacities of the
SKOS editor, we tested the performance of the editor in
executing some key editing functions. To test the editor,
we used an Apple MacBook with 16GB of memory and
the Firefox web browser (version 32).We chose five widely
used SKOS resources:
Fig. 2 Flowchart describing system functionality
Conway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 5 of 9
Fig. 3 Concept creation Wizard designed to expedite the creation of SKOS concept hierarchies
 STW for Economics Thesaurus is used for indexing
economics research papers [16]
 New York Times (NYT) Subject Descriptions is used
to index NYT news stories [17]
 United Kingdom Archive Thesaurus is a general
purpose subject heading thesaurus developed by the
UK government [18]
 Australian Curriculum Thesaurus, a resource
developed by the Australian government for
managing educational resources [19]
 The UNESCO United Nations Educational,
Scientific and Cultural Organization  Thesaurus
provides general subject terms across the fields of
education, culture, natural science, social and human
sciences, communication, and information [20]
Table 1 shows the capabilities of the editor in edit-
ing large thesauri, where it can be seen that the 5.1 MB
UNESCO Thesaurus took six seconds to load into the
tool. However, larger thesauri  e.g. the UK Archive The-
saurus at 9.4 MB  do not load quickly due to limitations
within the Liferay web framework. The tool is primarily
designed for developing relatively small, linguistically-
oriented vocabularies. In addition to testing whether
Table 1 General functioning evaluation
THESAURUS SIZE #CONCEPTS LOADING TIME HIERARCHY1 PREFLABEL2 SAVE3
STW Thesaurus 15MB 6584 40 sec Y Y Y
NYT Subj Descriptions 1.4 MB 499 3 sec Y Y Y
UK Archive Thesaurus 9.4 MB 13,976 120 sec Y Y Y
Aus. Curr. Framework 312 KB 170 0.5 sec Y Y Y
UNESCO Thesaurus 5.1 MB 44408 6 sec Y Y Y
1Is the SKOS broader/narrower hierarchy rendered correctly? [Y or N]
2Are the prefLabels rendered correctly? [Y or N]
3Can the file be successfully edited, saved, then reopened? [Y or N]
Conway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 6 of 9
various existing SKOS vocabularies could be loaded into
the tool and rendered correctly, for each of the SKOS the-
sauri evaluated, we tested basic editing functionality (e.g.
whether a new concept could be created and inserted into
the existing thesauri, whether concepts could be deleted).
The results of this evaluation are shown in Table 2. Note
that even very large vocabularies (e.g. STW Thesaurus)
could be edited successfully using the tool.
Qualitative evaluation
Our qualitative evaluation of the SKOS editor centred
on two use cases. For the first use case, an experienced
knowledge engineer (author Castine) used the SKOS edi-
tor to build a lexical resource to drive an EHR-oriented
NLP algorithm based on the Centers for Disease Control
pneumonia definition (see Fig. 4 for a screenshot of the
resulting SKOS resource). The pneumonia resource took
a total of 40 min to build using the Web SKOS Editor, as
opposed to the Protégé SKOS Editor Plug-in, which took
45 min. Note that the knowledge engineer did not use
the Wizard concept creation functionality, a tool which
we believe is likely to expedite the concept hierarchy
creation process substantially. For the second use case,
an experienced NLP researcher (author Mowery) used
the tool to develop a resource designed to map lexical
cues to Diagnostic and Statistical Manual of Mental Dis-
orders (DSM-5) diagnoses with the goal of facilitating
the automatic identification of symptoms associated with
depression from short, informal texts [21] (see Fig. 5 for
a screenshot of the SKOS resource creation process). The
depression resource took less than one hour to create,
and it was reported that the Wizard greatly expedited the
concept creation process. However, several enhancements
were suggested, including the development of an auto-
save feature, and the ability to configure default values for
language labels (for example, default to English  @en
labels).
Limitations
While the SKOS editor is suitable for building and
curating special purpose SKOS vocabularies to run
bespoke clinical NLP systems, it does have several limita-
tions:
 It is not suitable for editing very large SKOS
vocabularies
 As the tool is built around the SKOS API [9], some
language features outside core SKOS [1] are not
supported (e.g. skos:closeMatch,
skos:relatedMatch).
Future directions
Our long-term goal is to integrate the SKOS editor as a
lexicon development and management module within a
comprehensive platform for developing clinical NLP algo-
rithms. As part of this long term goal  and informed by
the comments and suggestions of our early users  we
plan three major system enhancements:
 In the medium term, we plan to add multi-user
functionality and collaborative editing to the system.
 We plan to include the ability to search other
vocabularies  in particular the UMLS (Unified
Medical Language System) [22]  from within the
editor interface in order to expedite the synonym
identification process.
 We plan to extend the current documentation and
tutorial material
Conclusions
The SKOS editor we have developed is  to the best of
our knowledge  the first free, open-source, online, SKOS
editor capable of creating, curating, versioning, and man-
aging SKOS vocabularies. The editor is free to use5 and
the source code is available under an Apache Version 2.0
License.
Availability and requirements
Project name:Web-based SKOS editor
Project home page: An instantiation of the tool is
available at http://blulab2.chpc.utah.edu:8080/web/guest/
skos. Source code is released under an open-source license
Table 2 Editing functioning evaluation
THESAURUS NEW CONCEPT1 DELETE NEW CONCEPT2 ADD/EDIT PREFLABEL3 ADD TOPCONCEPT4
STW Thesaurus Y Y Y Y
NYT Subj Descriptions Y Y Y Y
UK Archive Thesaurus Y Y Y Y
Aus. Curr. Framework Y Y Y Y
UNESCO Thesaurus Y Y Y Y
1Can a new SKOS concept be created (Y/N)?
2Can a SKOS concept be deleted (Y/N)?
3Can a SKOS PrefLabel be added and edited (Y/N)?
4Can a SKOS Concept be added and edited (Y/N)?
Conway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 7 of 9
Fig. 4 Pneumonia lexical resource based on Centers for Disease Control definition
Fig. 5 Building a depression lexicon  entering a preferred label
Conway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 8 of 9
and can be found at the University of Utahs Biomedical
Language Understanding Lab GitHub page https://github.
com/Blulab-Utah
Operating system:Multi-platform, browser-based
Programming languages: Java, JavaScript
Other requirements: No other requirements
License: Apache 2.0 License
Anyrestrictionstouse by non-academics:No restrictions
Endnotes
1Note that additional SKOS tutorial material is
DATABASE Open Access
The Cell Ontology 2016: enhanced content,
modularization, and ontology
interoperability
Alexander D. Diehl1*, Terrence F. Meehan2, Yvonne M. Bradford3, Matthew H. Brush4, Wasila M. Dahdul5,6,
David S. Dougall7, Yongqun He8, David Osumi-Sutherland2, Alan Ruttenberg9, Sirarat Sarntivijai2, Ceri E. Van Slyke3,
Nicole A. Vasilevsky4, Melissa A. Haendel4, Judith A. Blake10 and Christopher J. Mungall11
Abstract
Background: The Cell Ontology (CL) is an OBO Foundry candidate ontology covering the domain of canonical,
natural biological cell types. Since its inception in 2005, the CL has undergone multiple rounds of revision and
expansion, most notably in its representation of hematopoietic cells. For in vivo cells, the CL focuses on vertebrates
but provides general classes that can be used for other metazoans, which can be subtyped in species-specific
ontologies.
Construction and content: Recent work on the CL has focused on extending the representation of various cell
types, and developing new modules in the CL itself, and in related ontologies in coordination with the CL. For
example, the Kidney and Urinary Pathway Ontology was used as a template to populate the CL with additional cell
types. In addition, subtypes of the class cell in vitro have received improved definitions and labels to provide for
modularity with the representation of cells in the Cell Line Ontology and Reagent Ontology. Recent changes in the
ontology development methodology for CL include a switch from OBO to OWL for the primary encoding of the
ontology, and an increasing reliance on logical definitions for improved reasoning.
Utility and discussion: The CL is now mandated as a metadata standard for large functional genomics and
transcriptomics projects, and is used extensively for annotation, querying, and analyses of cell type specific data in
sequencing consortia such as FANTOM5 and ENCODE, as well as for the NIAID ImmPort database and the Cell
Image Library. The CL is also a vital component used in the modular construction of other biomedical
ontologiesfor example, the Gene Ontology and the cross-species anatomy ontology, Uberon, use CL to support
the consistent representation of cell types across different levels of anatomical granularity, such as tissues and
organs.
Conclusions: The ongoing improvements to the CL make it a valuable resource to both the OBO Foundry
community and the wider scientific community, and we continue to experience increased interest in the CL both
among developers and within the user community.
* Correspondence: addiehl@buffalo.edu
1Department of Neurology, University at Buffalo School of Medicine and
Biomedical Sciences, Buffalo, NY 14203, USA
Full list of author information is available at the end of the article
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Diehl et al. Journal of Biomedical Semantics  (2016) 7:44 
DOI 10.1186/s13326-016-0088-7
Background
The Cell Ontology (CL) was initially developed in 2004
with the goal of representing knowledge about in vivo
and in vitro cell types [1]. Cells are a fundamental unit
of biology, and most other entities in biology have direct
relationships to identifiable cell types, for example par-
ticular proteins being produced by unique cell types, tis-
sues and organs containing specific combinations of cell
types, or biological processes being dependent on par-
ticular cell types. Cells therefore are an obvious set of
entities to represent ontologically, and provide a useful
pole for organizing and driving data acquisition and ana-
lysis in biology.
The content in the CL is populated via gradual and en
masse class additions, most notably through several rounds
of improvements to representation of hematopoietic cells
in the ontology [24]. Originally, the CL was designed to
include cell types from all major model organisms includ-
ing both plants and animals [1]. However, as a result of
community interest and severe resource limitations, con-
tinuing development of the CL currently focuses primarily
on vertebrate cell types. The CL provides general classes
that can be used for other metazoans (muscle cell, neuron),
and the ontology can be extended in species-specific
ontologies.
The CL is built according to the principles established
by the OBO Foundry [5] and is the designated candidate
ontology for metazoan cell types within the Foundry.
The domain and content of CL is intended to be orthog-
onal to other Foundry ontologies to allow for the con-
struction of compositional classes via logical definitions,
as exemplified by the Gene Ontology (GO) [3, 68].
Work on the CL over the past several years has re-
sulted in many improvements in the ontologys structure
and content. As described below, cooperation among a
number of working groups has resulted in a modular ap-
proach to improving the CL, and continued enhance-
ment of logical definitions in the CL have increased its
integration and interoperability with other ontologies as
well as enhancing its utility for data analysis.
Construction and content
Editorial management of the CL
The CL is maintained primarily by a small group of editors
(ADD, YB, MH, DOS, CVS, NV, CJM), working in con-
junction with interested parties from the ontology commu-
nity. The editors use biweekly teleconferences to discuss
significant issues related to CL ontology development.
Because the CL has not been directly funded in recent
years, most efforts are contributed as part of other pro-
jects and reflect the cooperative efforts of ontology de-
velopers and users based in different communities,
such as the Gene Ontology Consortium [8, 9], the Im-
munology Database and Analysis Portal (ImmPort)
[10], the Human Immunology Project Consortium
(HIPC) [11], the Phenoscape project [12, 13], the Monarch
Initiative [14], and model organism databases such as the
Zebrafish Model Organism database (ZFIN) [15] and
Mouse Genome Informatics (MGI) [16]. Consequently
term creation occurs at an uneven pace, based on requests
and editor availability. Over the past few years, we have
received approximately 35 term requests per month.
Most requests are accommodated in 13 months. The
CL is released on an ad hoc basis, with new releases 56
times per year.
We welcome involvement of the community on par-
ticular domain specific developments, as has been done
with kidney cell types (see below) and with immune cell
types through our continuing collaboration with HIPC.
Collaboration with the larger biological and biomedical
community occurs both through our issue tracker and
through direct contacts with any of the editors.
Cell types in CL
As of June 2016, The CL contains approximately 2,200
classes, compared with 1534 at the time of our last report
[3].The relative distribution of number of cell types among
categories remains relatively constant, with one of the most
well-represented being the hematopoietic cell branch, as
described in [24], currently totaling 575 classes. Although
the size of this branch has remained relatively constant, the
content is continually refined and improved. For example,
many of the original hematopoietic cell definitions are be-
ing reviewed and generalized to be applicable beyond
mouse and human.
One area of expansion has been kidney cell subtypes,
resulting from collaboration with the Kidney and Urinary
Pathway Ontology (KUPO) project [17] as well as the Gene
Ontology [18]. This has resulted in the addition of 125 new
classes to represent kidney cell subtypes.
Over 400 cell types were added by generalizing
human-specific classes from the Foundation Model of
Anatomy (FMA) [19, 20]many of these were compos-
itional classes that we enhanced by adding both textual
definitions and logical definitions connecting to Uberon.
An example is epithelial cell of thyroid gland
(CL:0002257, FMA:0002257), logically defined as endo-
epithelial cell and (part of some thymus) [20].
New and revised skeletal cell types
Work on the Vertebrate Skeletal Anatomy Ontology
(VSAO), a unified ontology for the representation of
skeletal cells, tissues, biological processes, organs, and
subdivisions of the skeletal system [21], resulted in mod-
ifications to 13 existing cell types in the CL to ensure
that the classes applied across vertebrates, and the
addition of 18 new cell types. New relationships between
cell types and skeletal tissues were also added, in
Diehl et al. Journal of Biomedical Semantics  (2016) 7:44 Page 2 of 10
addition to developmental relationships between skeletal
cell types. These improvements enable broader queries
on skeletal diversity across different biological scales.
Improvements in the representation of skeletal tissues,
organs, and subdivisions of the skeletal system have
since been incorporated from VSAO into the Uberon
multi-species anatomy ontology [22], and the logical def-
initions of associated cells to refer to the Uberon classes.
Extending the CL to encompass vertebrate diversity
An ongoing challenge in developing the CL is to in-
crease the number and granularity of cell types repre-
sented for well-studied species such as mouse and
human, while providing high level classes needed for the
representation of cell types in non-mammalian verte-
brates. To ensure that CL classes are applicable to non-
mammalian vertebrates two courses of action have been
necessary: 1) add non-mammalian classes to the CL; 2)
ensure that general cell type definitions do not uninten-
tionally exclude certain organisms. Examples of non-
mammalian cell types that have been recently added to
the CL include the pigmented cells iridoblast
(CL:0005001) and xanthoblast (CL:0005002) [23], and
the Kolmer-Agduhr neuron (CL:0005007) [24]. Ensur-
ing that classes are applicable across species is a multifa-
ceted problem and includes optimizing of cell type
definitions, as well as (ideally) crafting class hierarchies
that incorporate non-mammalian cell types from
inception. Cell type definitions can unintentionally ex-
clude non-mammalian vertebrates by including mamma-
lian specific anatomical structures or by including
species-specific proteins in the logical definition. At the
same time, highly specified cell types for particular taxa
are needed to enable querying of complex data using the
CL. By adding less specific intermediate classes with in-
clusive definitions, such as multi-ciliated epithelial cell
(CL:0005012), the CL can be used by a wide variety of
model organism databases and evolutionary biologists
for data annotation, while serving the needs of sophisti-
cated bioinformatics analyses focused on cell types of
medical interest.
Improved delineation of content and coordination with
other ontologies
The primary focus of CL is to describe in vivo cell types
[3], and while the priority of CL curators has been on in
vivo cell types over the past few years, the ontology does
in fact include a branch for in vitro cells. In order to
clarify the representation of the domain of all cell types,
representatives of the CL, Cell Line Ontology (CLO)
[25], Reagent Ontology (ReO) [26], the Gene Ontology
[9], and Ontology for Biomedical Investigations (OBI)
[27], have agreed that the root class cell (CL:0000000)
in CL should be regarded as the root of all cell type clas-
ses in OBO Foundry ontologies (Fig. 1), and is equiva-
lent to the GO class cell (GO:0005623). As a result,
cell
mortal cell line cellimmortal cell line cell
cell line cell primary cultured cell
cultured cell
experimentally modified cell in vitro
experimentally modified cell cell in vitro native cell
(inferred)
Fig. 1 High level cell types in CL and related ontologies. The hierarchy of high-level cell types is shown. CL nodes: green, ReO: blue, CLO: orange
Diehl et al. Journal of Biomedical Semantics  (2016) 7:44 Page 3 of 10
changes were made to the upper level classes, to allow
for a modular approach that represents in vivo and ex
vivo cells types more accurately. Two of the children of
the root class cell are cell in vitro (CL:0001034), and
native cell (CL:0000003) (which was formerly known as
cell in vivo). The definition for native cell reads as
follows,
A cell that is found in a natural setting, which
includes multicellular organism cells 'in vivo' (i.e. part
of an organism), and unicellular organisms 'in
environment' (i.e. part of a natural environment).
This definition reflects the fact that while cells of
multicellular organisms are naturally considered in vivo
in their native state, single celled organisms often inhabit
environments that are not part of another organism, and
thus are not in vivo in that sense. The naturally occur-
ring in vivo cell types of multicellular organisms are
therefore properly considered subtypes of native cell.
Another agreed upon change in CL is that the classes
cell line cell, immortal cell line cell, and mortal cell line
cell were deprecated (i.e., made obsolete) in CL and re-
placed with equivalent classes from CLO (see discussion
below and Sarntivijai et al. [25] for additional details). As
CLO specifically represents cell line cells, it seemed ap-
propriate for CLO to contain its own root class and
high-level cell type classes, and for the CLO developers
to assume editorial control for these classes. Where
needed, these three CLO classes were imported into CL
using the MIREOT method [28, 29] to support existing
annotations to these classes, and users of these classes,
primarily MGI [16], were informed well in advance of
these changes.
Similarly, ReO [26] contains the class experimentally
modified cell (Fig. 1) and a variety of related classes
such as genetically modified cell and experimentally
modified multicellular organism cell in vivo. These cell
type classes most commonly denote reagents of some
type and fall outside of the domain of the CL proper,
and clearly are within the domain of ReO.
Plant cell types and insect cell types are now han-
dled independently of the CL as separate modules.
The Plant Ontology (PO) has recently undergone new
developments and the PO team has taken responsibil-
ity for curation of all plant cell type classes [30]. Con-
sequently, all plant cell type classes in CL have been
made obsolete. These plant cell types classes in the CL
were already duplicates of existing PO classes, and
were thus redundant and confusing to users. PO cell
type classes may be imported into an extended version
of CL as an OWL import in the future, retaining their
PO IDs. [31]. A similar process is already used to cre-
ate a pan-metazoan version of CL as part of the
Uberon release process [32]; this will be extended to
include Viridiplantae.
While the CL continues to represent a number of high
level insect cell types, the Drosophila Anatomy Ontology
(FBbt) contains cell types for many insect cell types not
represented in CL, particularly insect neurons [3335].
Similarly, the Zebrafish Anatomy Ontology (ZFA) also
contains neuron types not represented in CL [36]. Going
forward, the general approach is that non-mammalian
species-specific cell types will be represented as is_a
children of the appropriate CL parent in the species-
specific anatomy ontology when such an ontology exists.
The CL will continue to maintain general cell types for
representation of non-mammalian cells where no separ-
ate resource or ontology exists and will remain the prin-
cipal ontology for the representation of mammalian cell
types.
As described above, the root class cell (CL:0000000)
in CL is declared to be logically equivalent to the GO
class cell (GO:0005623), within the Cell Ontology.
While this arrangement mostly works for practical use
of the CL, a long class proposal has been to deprecate
cell (CL:0000000) and simply make the GO class cell
(GO:0005623) to be the root of the Cell Ontology. How-
ever, there are still some minor differences in the way
the two classes are defined, and questions about whether
the Gene Ontology with its orientation to describing
normal or physiological biology should provide the CL
root node cell, whose subtypes include tumor cell types,
cell line cell types, and other experimentally modified
cell types. This issue awaits additional discussion with
the Gene Ontology Consortium and other interested
parties.
Natural Language and Logical Definitions in CL
The proportion of classes with natural language defi-
nitions has remained relatively constant, with a cover-
age of 82 % in both 2011 and the present. We still aim
to boost this proportion to have 100 % coverage. The
last five years have seen general improvements in lo-
gical axiomatizationin 2011 we reported the number
of classes with defining equivalence axioms (logical
definitions) to be 340, this number has increased to
1534, added through both manual and automated
methods [3, 20].
The set of ontologies imported into the CL to provide
logical definitions remains constant, and consists of
Uberon [22, 37], Protein Ontology (PRO) [38], GO [9],
the Chemical Entities of Biological Interest (ChEBI)
ontology [39], and the Phenotypic And Trait Ontology
(PATO) [40]. Some classes make use of a variety of clas-
ses in the same axiom, such as T-helper 1 cell, which in-
cludes a mix of relations to both PRO classes and GO
classes (Fig. 2).
Diehl et al. Journal of Biomedical Semantics  (2016) 7:44 Page 4 of 10
Improvements to nervous system cell types
In order to improve the representation of neurons and re-
lated cell types, we adopted the relations and methods ori-
ginally developed from the Drosophila Anatomy Ontology
[34, 35]. These include synapsed_to and has_synaptic_ter-
minal_in, used to capture connectivity of neurons to each
other and larger anatomical structures. We aim to coalesce
with other neuron-specific vocabularies and ontologies, in
particularly those that were part of the Neuroscience Infor-
mation Framework (NIF) Standard suite of ontologies [41].
The analogous task has already been performed for neuron
parts [42], and the gross neuroanatomical structure subset
of NIFSTD has been incorporated into Uberon. As an ini-
tial task, we have aligned the contents of NIF-Cell with the
CL by matching up identical or similar classes in the two
hierarchies to identify gaps in both ontologies and differ-
ences in the ontologies structures. We will then define
standard patterns for neuronal cell types, and import miss-
ing neuron cell types from NIF. In order to synchronize
with the corresponding Neurolex wiki system, we have de-
veloped an approach for translating the Neurolex semantic
wiki into OWL [43].
Recent improvements in CL development methodology
The CL was originally developed using the OBO-Format
and the OBO-Edit ontology editor [1, 44], without any
automated quality control, release pipeline or automated
procedures for building the ontology. We previously re-
ported on improvements to this methodology, specific-
ally leveraging the OWL2 ontology language [45] and
associated tooling such as OWL reasoners, and the Pro-
tégé 5.x editor [20, 46].
We have made further changes and improvements to
the ontology engineering framework we use. Previously,
the editors version (source code) for the ontology was
in OBO-Format, necessitating a conversion to OWL step
prior to reasoning and debugging in Protégé. We have
since switched the editors version to be in OWL, simpli-
fying the procedure for working with the OWL stack of
tools (note that we still produce editions in OBO-
Format along with every release, as many bioinformatics
tools still rely on this format). This switch also gives us
greater flexibility for expressing concepts using the
richer constructs available in the OWL language.
We have also implemented a TermGenie [47] instance,
available at cl.termgenie.org. This provides a wider com-
munity of users a web frontend for instant provisioning
of new classes, either conforming to pre-defined tem-
plates (i.e. design patterns), or templateless free-form ad-
ditions. Currently the only design pattern implemented
is a simple part-whole template for the addition of clas-
ses like epithelial cell of forearm. One of the main users
Fig. 2 Logical definition for T-helper 1 cell (CL:0000545). The logical definition for the cell type T-helper 1 cell as presented by the Protégé
ontology editor. The logical definition uses imported classes the Protein Ontology (T-box transcription factor TBX21, PR:000001835; C-X-C
chemokine receptor type 3, PR:000001207; and C-C chemokine receptor type 6, PR:000001202) and the Gene Ontology (interferon-gamma
production, GO:0032609). Note some anonymous ancestor classes are not shown due to space considerations
Diehl et al. Journal of Biomedical Semantics  (2016) 7:44 Page 5 of 10
of the TermGenie instance has been the curators of the
ENCODE project (see below).
We make use of the Jenkins Continuous Integration sys-
tem, as developed and implemented by the Gene Ontology
Consortium, for quality control and validation [8, 48]. This
system alerts the editorial team if changes are made that
inadvertently introduce logical, terminological, or struc-
tural errors into the ontology (for example, a cell that is lo-
cated in two disconnected locations, or two cell classes
that share the same name). We are in the process of
switching to Travis-CI as this provides more direct integra-
tion with the GitHub system, where we manage the ontol-
ogy. This system is also used to generate releases, creating
a package of ontology files in OWL2 and OBO formats
that are pre-reasoned and in some cases simplified for leg-
acy use for systems that do not support logical definitions
(See Table 1 for listing of available CL files).
In the time since we last published on CL, we have mi-
grated the source repository we use to manage the
ontology on two occasions. We originally migrated from
SourceForge to GoogleCode; some time later, Google an-
nounced the retirement of GoogleCode, so we then
followed many other ontologies and migrated to GitHub,
where the source now resides [49]. Note however that
most users of the CL do not interact with GitHub dir-
ectly, and retrieve the ontology from the URLs provided
in Table 1. Class requests and other inquiries for the
ontology developers should be made through the CL
issue tracker [50]. We have deprecated the older issue
trackers on SourceForge and GoogleCode, and we mi-
grated the tickets on these systems to GitHub.
While this migration process caused some disruption,
this is compensated by efficiencies afforded by the
GitHub systemfor example, the ability to link edits on
the ontology to tickets. The GitHub release mechanism
also works well for ontology releases. One feature we
hope to deploy this year is the ability to move to a
GitHub-flow style of development, allowing external edi-
tors the ability to make pull requests on the ontology,
with complete validation being performed by Travis.
Utility and discussion
Use of CL classes in development of other ontologies
Cells are central to understanding biology from the mo-
lecular to the organismal level, and the CL is increasingly
useful as a tool for representing and organizing cell types
and data related to cell types in a variety of projects. As the
designated ontology for the representation of cells in the
OBO Foundry, the CL is used in a number of ontologies
for the development of compositional classes via logical
definitions. Gene Ontology developers have long employed
the principle of cross-product class development, in
which two classes from different ontologies are combined
to make a more expressive pre-composed (or compos-
itional) ontology class [68]. The class neuron differenti-
ation (GO:0030182), for instance has the logical axiom
'cell differentiation' and (results in acquisition of features of
some neuron), where neuron is a CL class. As GO devel-
opers continue to implement logical definitions for cross-
product classes, they have increasingly needed new cell
types in CL for use in these logical definitions. In order to
facilitate this process, GO ontology developers have been
trained in CL ontology editing as well and are now making
direct contributions to the CL. An extended version of the
GO that includes a subset of the CL together with linking
axioms is available [51].
Development of the Cell Line Ontology (CLO) has ref-
erenced CL cell types and the hierarchy of the CL [25].
In CLO, all cell line cells are under the CLO class cell
line cell, which is a child of CL cultured cell. Initially,
CLO listed over 30,000 cell line cells immediately under
the parent class cell line cell. To better identify the rela-
tions among different cell line cells, CLO generated
many intermediate cell line cell classes (e.g., immortal
epidermal cell line cell and immortal keratinocyte cell
line cell) based on a basic relation design that a CLO
cell line cell is derived from a CL cell, for example,
CLO immortal epidermal cell line cell derives_from
some epidermal cell, and CLO immortal keratinocyte
cell line cell derives_from some keratinocyte. In CL, the
class epidermal cell is a parent of keratinocyte. Based
on this CL hierarchical definition, CLO automatically in-
cludes a logical definition that immortal epidermal cell
line cell is a parent of immortal keratinocyte cell line
cell. In total over 130 CL classes were imported to CLO
with the hierarchy of the CL informing CLO structure.
These newly generated CL-matched CLO classes were
then used as parent classes for the over 30,000 cell line
cells in CLO to layout an improved hierarchy of the cell
line cells [25].
Interactions between different ontologies in the scope
of biological cells can become complicated as we imple-
ment a thorough and precise representation of know-
ledge in this domain. As described above, CLOs cell
line cell is a subclass of CLs experimentally modified
Table 1 CL Ontology Files
File pre-
reasoned
with external
ontology classes
PURL
cl-edit.owl no yes N/A
cl.owl yes yes http://purl.obolibrary.org/obo/
cl.owl
cl.obo yes yes http://purl.obolibrary.org/obo/
cl.obo
cl-basic.owl yes no http://purl.obolibrary.org/obo/
cl-basic.owl
cl-basic.obo yes no http://purl.obolibrary.org/obo/
cl-basic.obo
Diehl et al. Journal of Biomedical Semantics  (2016) 7:44 Page 6 of 10
cell in vitro where experimentally modified cell in vitro
is inferred as a subclass of ReOs experimentally modi-
fied cell. The correct relationships among these related
classes are only seen when all have been loaded into
Protégé and a reasoner has been run. This degree of
interrelatedness and complexity is becoming more com-
mon in bio-ontology practice, and demonstrates the
needs for effective communication within the commu-
nity. Being the center of interactions in situation like
this, the CL has acted as the facilitating moderator of
this kind of communication.
CL development allows for modular development of
species-specific extensions. These extensions enable the
creation of very granular cell types defined in ways that
are unique to a particular species or limited to a subset
of species. However, many cell types can be generically
defined across species, and the CL provides the appro-
priate OBO ontology for their representation. In order
to allow for comparison and integration of cell type spe-
cific data between species, species-specific cell types
should always be subtypes of generic CL cell types.
While development of modular extensions to CL is en-
couraged, the well-developed hierarchy of classes in the
CL provides a valuable resource for data annotators
working in species who do not have time or resources to
develop CL extensions.
As examples of this methodology, developers of
species-specific anatomy ontologies such as the Zebra-
fish Anatomical Ontology (ZFA) [36] and the Xenopus
Anatomy Ontology (XAO) [52] have extended the CL by
incorporating species-specific cell classes as is_a chil-
dren of CL classes in their ontologies. This strategy al-
lows ontologists to make species-specific classes that are
is_a children of the appropriate CL class for use in data
annotation at model organism databases. The integration
of the CL with the species-specific ontologies also allows
the CL classes to be used in phenotype and expression
annotations at ZFIN [15] and expression annotations at
Xenbase [53].
As ontologies such as the Infectious Disease Ontology
(IDO) [54] or the Neurological Disease Ontology [55]
are developed, CL classes are being used to represent in-
formation such as viral tropism or neurons affected in
Parkinsons disease. As with the GO, there is communi-
cation between developers of related biomedical ontol-
ogies that contribute to the development of both. The
CL is also a component of the Experimental Factor
Ontology (EFO), used to provide descriptions of experi-
mental variables in databases at the European Bioinfor-
matics Institute [56].
The CL is also being used far more extensively in the
GO, in particular the GO has added a way to provide
additional cellular context to gene associations using a
mechanism called annotation extensions [57]. These
cross-ontology linkages are used by a number of model
organism databases in GO annotation and visible in
AmiGOfor example, the page for neuron includes
GO annotations for neuronal parts [58].
Use of the CL as Metadata in ENCODE and FANTOM5
Projects
Two major projects studying gene expression have uti-
lized the CL as part of their data analysis pipelines. The
Encyclopedia of DNA Elements (ENCODE) Consortium,
which is funded and organized by National Human Gen-
ome Research Institute (NHGRI), aims to discover and
define the functional elements encoded in the human
genome [59]. ENCODE investigators are utilizing a pri-
oritized set of various cell types to complete annotations
about genes and their RNA transcripts and transcrip-
tional regulatory regions and have developed data stan-
dards that utilizes the CL, among other ontologies, to
describe the metadata for cell types used and experimen-
tal assays [60, 61].
The value of the CL for data integration and analyses
was adeptly demonstrated in a recent series of notable
papers from the FANTOM5 Consortium, which relied
in part on the CL for large-scale data analyses of tran-
scriptional start sites [62], enhancers [63], and waves of
transcription in differentiating cell types [64]. The FAN-
TOM5 Consortium utilized the CL as a component of
the FANTOM Sample Ontology, in combination with
Uberon, the Disease Ontology and the EFO to identify
cellular, tissue, disease sources and experimental modifi-
cations for the samples used in transcriptional analyses
[65]. By relying on the ontological hierarchy provided by
the CL, the FANTOM5 Consortium was able to classify
transcription patterns associated with individual cell
types, groupings of related cell types, and cell lineages
during differentiation [62, 64].
Use of the CL in other non-ontology projects
The CL is being used as metadata in a variety of non-
ontology projects, such as The Cell: An Image Library
[66], CELLPEDIA [67], Phenoscape [13], LINCS [68],
the Human Immunology Project Consortium (HIPC)
[11], and ImmPort [10]. The HIPC and ImmPort pro-
jects are National Institute of Allergy and Infectious
Diseases (NIAID) sponsored programs to collect and
organize data from immunology experiments performed
by NIAID supported investigators in order to facilitate
secondary usage [10]. In support of these projects, the
CL is being used both as a controlled vocabulary of cell
types for use as metadata, and as part of an analytical
pipeline for analyzing high-dimensional flow cytometry
and mass cytometry data (e.g. CyTOF) [69] submitted to
the ImmPort data repository. Developers of the CL have
already incorporated novel B cell types discovered via
Diehl et al. Journal of Biomedical Semantics  (2016) 7:44 Page 7 of 10
high-dimensional flow cytometry [70], such as IgG-posi-
tive double negative memory B cell (CL:0002103) and
IgD-negative CD38-positive IgG memory B cell
(CL:0002107). The CyTOF method is yielding informa-
tion about even more granular cell types [71]. In order
to facilitate the analysis of data generated in high-
dimensional flow cytometry or CyTOF, the flowCL soft-
ware package matches cell populations identified via
automated gating algorithms against existing cell types
in the CL based on their combinations of markers, or
immunophenotypes [72, 73].
Conclusion
Through cooperative efforts between the Cell Ontology
editors and various stakeholders, ongoing development
of the CL has ensured that it continues to be a valuable
resource for users and developers of related ontologies.
Use of the CL by a broad range of third party efforts, in-
cluding the high visibility ENCODE and FANTOM5
projects, as a source of metadata and for data integration
and analysis shows the value of the CL to the wider sci-
entific community. As big data collection and analysis
continues to grow in importance as a source of bio-
logical discovery, we expect the CL will be of key utility
in organizing and understanding these data. We invite
community feedback and participation to continue the
improvements to the CL.
Availability and requirements
Like all OBO library ontologies, the CL is available from
a standard purl [http://purl.obolibrary.org/obo/cl.owl].
The main URL for the project
[http://cellontology.org/] and links to various browsers
are available from the main OBO Library page for CL
[http://obofoundry.org/ontology/cl.html].
Acknowledgements
We would kindly thank Barry Smith, Lindsay Cowell, Anna Maria Masci,
Richard Scheuermann, Jose Mejino, David Hill, Terry Hayamizu, Morgan
Hightshoe, Wade Valleau, Jane Lomax, Paola Roncaglia, Tanya Berardini,
Heiko Dietze, Maryann Martone, Stephan Larson, Gordon Shepherd, Jyl
Boline, Mihail Bota, Giorgio Ascoli, Paul Katz, Robert Burgess, Patrick Ray,
Jonathan Bona, Paula Mabee, Laurel Cooper, Ramona Walls, Pankaj Jaiswal,
Darren Natale, Cathy Wu, Cecilia Arighi, Alistair Forrest, Hideya Kawaji, Helen
Parkison, Simon Jupp, Robert Stevens, Ryan Brinkmann, Melanie Courtot,
Raphael Gottardo, Cliburn Chan, Jie Zheng, Shai Shen-Orr, and Yannick
Pouliot, for discussions about and contributions to the Cell Ontology project.
ADD, TFM, CJM, and JAB were supported by NHGRI grants HG002273-09Z
and HG002273 for portions of this work. CJMs work was supported by the
Director, Office of Science, Office of Basic Energy Sciences, of the U.S.
Department of Energy under Contract No. DE-AC02-05CH11231. ADD and
AR are supported by NIGMS grant 2R01GM080646-06 and NIAID contract
HHSN272201200028C for portions of this work. YMB and CVS are supported
by NIH HG002659 for portions of this work. MAH, MHB, and NAV are supported
for portion of this work by 1R24OD011883 from the NIH Office of the Director.
WD is supported by NSF grants DBI-0641025, DBI-1062404, and DBI-1062542, and
by the National Evolutionary Synthesis Center under NSF EF-0423641 and NSF
EF-0905606 for portions of this work. YH was supported by NIH 1R01AI081062.
Any opinions, findings, and conclusions or recommendations expressed in this
material are those of the author(s) and do not necessarily reflect the views of the
National Science Foundation or the National Institutes of Health. We gratefully
acknowledge the support of the International Neuroinformatics Coordinating
Facility for portions of this work.
Authors' contributions
All authors have contributed to CL ontology development through
discussions of key issues and/or contributions of ontology classes. CJM
developed the CL production system. The CL project is managed by ADD,
AR, MAH, JAB, and CJM. ADD wrote the manuscript with contributions from
YMB, WMD, YH, CVS, DOS, MAH, NV, SS, TFM, and CJM. All authors read and
approved the manuscript.
Competing interests
The authors declare that they have no competing interests.
Author details
1Department of Neurology, University at Buffalo School of Medicine and
Biomedical Sciences, Buffalo, NY 14203, USA. 2European Molecular Biology
Laboratory, European Bioinformatics Institute, Hinxton, Cambridge CB10 1SD,
UK. 3ZFIN, the Zebrafish Model Organism Database, 5291 University of
Oregon, Eugene, OR 97403, USA. 4Ontology Development Group, Library,
Oregon Health and Science University, Portland, Oregon 97239, USA.
5Department of Biology, University of South Dakota, Vermillion, SD 57069,
USA. 6National Evolutionary Synthesis Center, Durham, NC 27705, USA.
7Southwestern Medical Center, University of Texas, Dallas, TX 75235, USA.
8Unit for Laboratory Animal Medicine, University of Michigan Medical School,
Ann Arbor, MI 48109, USA. 9Oral Diagnostics Sciences, University at Buffalo
School of Dental Medicine, Buffalo, NY 14210, USA. 10The Jackson Laboratory,
Bar Harbor, ME 04609, USA. 11Genomics Division, Lawrence Berkeley National
Laboratory, Berkeley, CA 94720, USA.
Received: 15 April 2016 Accepted: 23 June 2016
Hicks et al. Journal of Biomedical Semantics  (2016) 7:47 
DOI 10.1186/s13326-016-0087-8
RESEARCH Open Access
The ontology of medically related social
entities: recent developments
Amanda Hicks1*, Josh Hanna1, Daniel Welch1, Mathias Brochhausen2 and William R. Hogan1
Abstract
Background: The Ontology of Medically Related Social Entities (OMRSE) was initially developed in 2011 to provide a
framework for modeling demographic data in Resource Description Framework/Web Ontology Language. It is built
upon the Basic Formal Ontology and conforms to Open Biomedical Ontologies Foundrys best practices.
Description: We report recent development of OMRSE which includes representations of organizations, roles,
facilities, demographic data, enrollment in insurance plans, and data about socio-economic indicators.
Conclusions: OMRSEs coverage has been expanding in recent years to include a wide variety of classes and has
been useful in several biomedical applications.
Keywords: Ontology, OMRSE, Social entities, Social roles, Organizations
Background
The Ontology of Medically Related Social Entities
(OMRSE) [1] is a realist representation of medically
related social entities. We initially developed OMRSE to
cover demographics data and common roles of people
in healthcare encounters for reuse in the context of the
OBO Foundry [1]. We created a framework for defining
gender roles, legal roles, healthcare provider roles, health-
care organization roles, and patient roles inWebOntology
Language (OWL), one of the accepted languages for the
OBO Foundry and a standard for the Semantic Web. We
have since developed this ontology by adding more spe-
cific classes and creating frameworks for additional topics
to facilitate uses arising out of projects related to epi-
demic modeling, the organizational structure of trauma
systems, and common health care data models. OMRSE
is a middle level ontology in the sense described in [2].
It is designed to bridge the gap between the upper ontol-
ogy, Basic Formal Ontology (BFO), and more specific
domain ontologies as well as provide classes for reuse in
application ontologies. While we acknowledge that the
*Correspondence: aehicks@ufl.edu
1Department of Health Outcomes and Policy, University of Florida, Gainesville,
FL; USA
Full list of author information is available at the end of the article
demarcation between middle and domain level ontolo-
gies is not crisp [2], OMRSE contains mid-level classes
such as employee role, smoker role, and party to a mar-
riage contract that span multiple sub-domains and can be
reused in both more specific domain ontologies as well as
application ontologies.
Applications
OMRSE classes are reused in several application ontolo-
gies. It is available to the wider biomedical community
through OntoBee [3] and NCBO Bioportal [4].
The Apollo [5] and MIDAS projects reuse OMRSE
classes in Apollo-SV to produce synthetic ecosystems for
agent based epidemic modeling. The CAFÉ Project reuses
OMRSE classes in the Ontology of Organizational Struc-
tures of Trauma centers and Trauma systems (OOSTT)
(http://purl.obolibrary.org/obo/oostt.owl). OOSTT is an
OWL representation of organizational structures (organi-
zations, committees, roles, etc.) specific to trauma centers
or trauma systems. It is used to compare the organiza-
tional structure of trauma centers and trauma systems.
OMRSE classes are also being used to create a seman-
tic representation of the PCORnet Common Data Model
(CDM).
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Hicks et al. Journal of Biomedical Semantics  (2016) 7:47 Page 2 of 4
Construction and content
In keeping with the OBO Foundry principles [1], OMRSE
reuses classes from other ontologies including the Basic
Formal Ontology (BFO) [6], NCBI Taxonomy [7], the
Information Artifact Ontology [8], and the Document
Acts Ontology (D-acts) [9]. We searched OntoBee or
Bioportal for existing classes in BFO-based ontologies
before adding new ones to OMRSE. We participate in dis-
cussion about BFO and other OBO ontologies to ensure
compatibility with other OBO/BFO ontologies. The addi-
tions that we report in this paper have been developed
with investigators on these projects to ensure accurate
and useful semantics of new classes. The recent devel-
opments in OMRSE can be classified into eight content
areas described in the subsections below. In addition to
introducing new content, we have also updated OMRSE
to be compatible with the Basic Formal Ontology 2.0
(BFO2).
Organizations and the roles they create
Some roles, such as employee and student roles, exist only
in relation to organizations. To represent the relation-
ships between these roles and organizations we leveraged
the work in D-Acts, an OWL ontology built accord-
ing to the OBO Foundry principles and using BFO. D-
acts is based on works by Reinach [10] and Smith [11]
explaining how social acts create new entities. The ontol-
ogy represents social acts (such as signing a contract
or enrolling as a student), the socio-legal entities those
social acts create (such as rights and obligations or a stu-
dent role), and the object properties relating these kinds
of entities [10].
Demographic data
The major developments for representing demographic
data consist in modeling social identities (race and ethnic-
ity) and marital status. Social identities differ from other
demographic data since the referent of identities is onto-
logically unclear. For considerations of space and clarity,
we reserve a discussion of social identities for a future
manuscript. At present details can be found in [12] and
https://www.youtube.com/watch?v=-pcQUNWtnVk.
Marital status
Many coding schemes have several values for marital
status (HL7 has nine), but we model marital status as
binary. In clinical settings marital status is recorded to
document whether a patient has a spouse who can make
decisions on his or her behalf; either they have a spouse
to make decisions on their behalf or they do not. We cur-
rently have no use case for knowing a person has never
been married nor for capturing what process resulted
in the end of a marriage (i.e., death of the spouse or a
divorce).
Typology of trauma patients
We worked with a team of trauma experts who identified
and reviewed definitions for trauma patients for the The
CAFÉ project (1R01 GM111324). Current classes include
injured patient role, burn patient role, and trauma patient
role. A typology of burn patients was also defined. We
plan to add these classes once there are suitable classes for
types of burns (e.g., thermal vs. chemical burns) in another
OBO ontology.
Health care facilities
We developed a typology of twelve types of health care
facilities that are referred to in the PCORnet CDMs
discharge status field in the encounters Table [13] (e.g.,
hospital facility, urgent care facility, nursing home facil-
ity). We distinguish the types of facilities based on their
functions
Health care provider roles
Wehave distinguished health care provider roles along the
lines of what kind of entity can bear that role. Accord-
ingly, we have health care provider organization role and
subclasses that inhere in organizations. These subclasses
are, hospital role, integrated delivery network, and physi-
cian practice. We also have health care provider role as a
subclass of human health care role. These include nurse
role, physiatrist role, physician role, and US physician
role.
Smoking statuses
OMRSE captures smoking status using smoker roles.
Smoker role is defined as a role that inheres in an human
being and is realized by habitually smoking tobacco prod-
ucts. The subclasses heavy smoker role and light smoker
role are defined in terms of number of cigarettes habitu-
ally smoked per day. Further distinctions based on smoke
exposure and source can be added as applications require
them.
Enrollment in an insurance plan
Modeling enrollment in an insurance plan requires mod-
eling three different types of entities: (1) insurance poli-
cies, (2) the roles involved in an insurance policy, and (3)
enrollment dates.
Insurance policies
Insurance policies come into existence through docu-
ment acts. In technical terms, they are the specified
output of document acts. The document acts involve
two parties (1) a group of persons (the insured par-
ties) and (2) the organization that issues the plan. The
organization and the primary insured persons on the
policy are parties to a legal agreement (an insurance
policy).
Hicks et al. Journal of Biomedical Semantics  (2016) 7:47 Page 3 of 4
Roles
There are two types of roles introduced to model insur-
ance policies: those that inhere in the insurance company
and those that inhere in the insured. More details about
how these roles are modeled are available at http://ncor.
buffalo.edu/2016/Hicks.pptx.
U.S. Census households and housing units
Households and housing units are pivotal for representing
U.S. Census data and epidemic modeling. OMRSE
represents the distinction between households, which are
collections of people, and housing units and asserts that
housing units are individuated by their residence func-
tions.
Socio-economic data
Although we do not directly represent socio-economic
status, with the exception of employee role and insurance
enrollment information, we do represent data that are
about socio-economic status. These terms are included to
facilitate the annotation of data sets that contain infor-
mation about employment status, care plans, income, and
other socio-economic indicators.
BFO 2.0 Conversion
The only modification to OMRSE that was required to
complete this conversion was to import version 2015-10-
07 of the Relation Ontology [14].
Utility and discussion
Validation
We have generated competency questions to validate
the following representations: (1) employee roles, stu-
dent roles, and household data, and (2) health care
organizations and the typology of patients. Compe-
tency questions and queries for (1) are freely available
at www.github.com/ufbmi/socid and http://tinyurl.com/
syneco-queries respectively. Competency questions have
been developed for OOST, but OOST is still under devel-
opment and has not yet been validated. Table 1 has sample
competency questions.
Limitations
The typologies that we mention here are not exhaustive.
For instance, the typologies of health care provider roles
and patient roles are relatively sparse compared to the
myriad of roles that a provider or patient might bear. This
is the result of our use case driven approach. We maintain
an active mailing list and issue tracker to manage requests
to the ontology.
Stating that an individual person is single, i.e. lacks a
party to a marriage contract role, in OWL is challeng-
ing but also necessary to support modeling demographic
data in a manner that is compliant with OBO Foundry
Table 1 Sample competency questions
Topic Ontology Question
Demographics PCORowl Who has identified as Asian
according to both OMB and
PCORnet CDM guidelines?
Marital status/households MIDAS Who are the married
householders according to the
U.S. Census?
Roles and organization CAFÉ How many anesthesiologists
does institution x have on staff?
principles. Two approaches are (1) using negative object
property axioms and (2) defining a class single person as
a person who is not the bearer of a party to a marriage
contract role. The former is often not supported by com-
mon reasoners and the latter would lead to a proliferation
of absence classes for every case where some individ-
ual lacks a relationship to some class of entities. The
analogous problem is still outstanding for non-smoker,
for example. A detailed description of this problem goes
beyond the scope of reporting updates, but we will address
this in future work and are currently investigating the pros
and cons of these approaches, as well as attempting to
come up with additional approaches.
Conclusion
OMRSE is an ontology designated for representing med-
ically related social entities in a manner that is consistent
with BFO and OBO Foundry ontologies. Its coverage has
been expanding in recent years to include a wide vari-
ety of classes.. and has been useful in several biomedical
applications.
Availability and requirements
OMRSE is free and open to all users (https://github.
com/ufbmi/OMRSE). There is a Google Group for dis-
cussing the project at http://groups.google.com/group/
omrse-discuss.
Acknowledgments
Part of the research reported in this publication was supported by the National
Institute of General Medical Sciences of the National Institutes of Health under
award number 1R01GM111324. This work was also supported in part by the
NIH/NCATS Clinical and Translational Science Awards to the University of
Florida UL1 TR000064 and the University of Arkansas for Medical Sciences UL1
TR000039 and the James and Esther King Foundation Biomedical Research
Program 4KB16. The authors would like to thank the CAFÉ Domain Expert
panel, namely Jane Ball, Stephen M. Bowman, Robert T. Maxson, Rosemary
Nabaweesi, Rohit Pradhan, Nels D. Sanddal, Mihael E. Tudoreanu, and Robert J.
Winchell.
Authors contributions
AH manages OMRSE. She contributed to each of the developments discussed
in this paper. She drafted the main sections of this manuscript. DW
contributed to the development of OMRSE, in particular to the classes related
to U.S. Census data, households, and housing units. JH contributed to the
Hicks et al. Journal of Biomedical Semantics  (2016) 7:47 Page 4 of 4
representations of race and ethnicity, marital status, smoking status, insurance
policies, U.S. Census households and housing units, and socio-economic data.
MB contributed to the classes related to the CAFÉ project. WH was the original
creator of OMRSE in support of demographics applications, and has
subsequently participated in its development around synthetic ecosystems
and common data model support including insurance enrollment. All authors
read and reviewed the manuscript.
Competing interests
The authors have no competing interests to declare.
Author details
1Department of Health Outcomes and Policy, University of Florida, Gainesville,
FL; USA. 2Department of Biomedical Informatics, University of Arkansas for
Medical Science, Little Rock, AR; USA.
Received: 3 December 2015 Accepted: 21 June 2016
RESEARCH Open Access
Extracting a stroke phenotype risk factor
from Veteran Health Administration clinical
reports: an information content analysis
Danielle L. Mowery1,2*, Brian E. Chapman1,2, Mike Conway1, Brett R. South1,2, Erin Madden3, Salomeh Keyhani3
and Wendy W. Chapman1,2
Abstract
Background: In the United States, 795,000 people suffer strokes each year; 1015 % of these strokes can be
attributed to stenosis caused by plaque in the carotid artery, a major stroke phenotype risk factor. Studies
comparing treatments for the management of asymptomatic carotid stenosis are challenging for at least two
reasons: 1) administrative billing codes (i.e., Current Procedural Terminology (CPT) codes) that identify carotid
images do not denote which neurovascular arteries are affected and 2) the majority of the image reports are
negative for carotid stenosis. Studies that rely on manual chart abstraction can be labor-intensive, expensive, and
time-consuming. Natural Language Processing (NLP) can expedite the process of manual chart abstraction by
automatically filtering reports with no/insignificant carotid stenosis findings and flagging reports with significant
carotid stenosis findings; thus, potentially reducing effort, costs, and time.
Methods: In this pilot study, we conducted an information content analysis of carotid stenosis mentions in terms
of their report location (Sections), report formats (structures) and linguistic descriptions (expressions) from Veteran
Health Administration free-text reports. We assessed an NLP algorithm, pyConTexts, ability to discern reports with
significant carotid stenosis findings from reports with no/insignificant carotid stenosis findings given these three
document composition factors for two report types: radiology (RAD) and text integration utility (TIU) notes.
Results: We observed that most carotid mentions are recorded in prose using categorical expressions, within the
Findings and Impression sections for RAD reports and within neither of these designated sections for TIU notes. For
RAD reports, pyConText performed with high sensitivity (88 %), specificity (84 %), and negative predictive value
(95 %) and reasonable positive predictive value (70 %). For TIU notes, pyConText performed with high specificity
(87 %) and negative predictive value (92 %), reasonable sensitivity (73 %), and moderate positive predictive value
(58 %). pyConText performed with the highest sensitivity processing the full report rather than the Findings or
Impressions independently.
Conclusion: We conclude that pyConText can reduce chart review efforts by filtering reports with no/insignificant
carotid stenosis findings and flagging reports with significant carotid stenosis findings from the Veteran Health
Administration electronic health record, and hence has utility for expediting a comparative effectiveness study of
treatment strategies for stroke prevention.
Keywords: Natural language processing, Stroke, Phenotype, Information extraction
* Correspondence: danielle.mowery@utah.edu
1Department of Biomedical Informatics, University of Utah, Salt Lake City, UT,
USA
2IDEAS Center, Veteran Affair Health Care System, Salt Lake City, UT, USA
Full list of author information is available at the end of the article
© 2016 Mowery et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Mowery et al. Journal of Biomedical Semantics  (2016) 7:26 
DOI 10.1186/s13326-016-0065-1
Background
In biomedicine, we define a disease or mutant phenotype
experienced by an individual as observations caused by
interactions between the environment and his/her gen-
ome that differ from the expected, normal wild type.
Over the last several years, the biomedical community
has begun to leverage informatics and electronic health
record (EHR) data to define and identify phenotypes for
genetic analyses using genome-wide (GWAS) and
phenotype-wide (PheWAS) association studies [1, 2]. For
instance, PheKB is a knowledgebase that contains phe-
notypes defined using EHR data and subsequently vali-
dated within one or more institutions. This catalogue of
phenotypes was primarily generated by the Electronic
Medical Records and Genomics (eMERGE) network, a
United States (US) National Human Genome Research
Institute-funded consortium, but is also supplemented by
the informatics community at large (https://phekb.org/
phenotypes) [35]. Similarly, the Strategic Health IT Re-
search Program for secondary use of EHRs (SHARPn),
funded by the US Office of the National Coordinator for
Health Information Technology, aims to transform het-
erogeneous EHR data from various sites into a standard-
ized form to support high-throughput phenotyping [6].
Phenotyping with electronic health record data
Several phenotypes have been the foci of informatics
studies including cancer, diabetes, heart failure, rheuma-
toid arthritis, drug side effects, cataract, pneumonia,
asthma, peripheral artery disease, and hypertension [7].
EHRs provide a groundbreaking opportunity to define
and identify these complex phenotypes leveraging data
elements from the longitudinal patient record. Specific-
ally, patient phenotypes are often inferred from both
structured EHR data elements (e.g., administrative bill-
ing codes, vital signs, medications, laboratory values
from data fields including dropdown lists and check-
boxes) and unstructured EHR data elements (e.g., symp-
toms, signs, histories, and diagnoses within clinical notes
including progress notes and discharge summaries).
These heterogeneous data elements are then mapped to
logical representations used to classify a patient into one
or more phenotypes [8]. Outstanding challenges remain
for next-generation phenotyping of EHR data including
the need for approaches that address data complexity,
inaccuracy, coverage, and biases [9].
Natural language processing
Traditionally, International Classification of Disease
(ICD-9) billing codes have been leveraged to identify
phenotype risk factors with variable results. Inaccurate
performance can result from poor granularity within
code descriptions and documentation of risk factors in
patient clinical texts [10, 11]. Natural language
processing (NLP) may improve risk factor detection by
identifying missed risk factor mentions (improving sensi-
tivity) and filtering spurious risk factor mentions (improv-
ing positive predictive value) from these clinical texts.
However, extracting risk factors associated with pheno-
types from clinical texts can be challenging due to the
usage of variable lexical expressions (e.g., occlusion, re-
duced arterial diameters), ambiguous abbreviations (PAD
can stand for peripheral artery disease or pain and dis-
tress), spelling errors (diabetes misspelled as dia-
beetes), and telegraphic constructions (e.g., PHx: HTN
means past history of hypertension) within clinical texts.
Furthermore, multiple mentions of the same risk factor
can be recorded within and across reports. This informa-
tion might be integrated with structured data elements re-
quiring logic to classify a patient with a phenotype. The
success of an algorithm is often defined by performance
metrics of sensitivity (or recall), positive predictive value
(or precision), negative predictive value, and specificity by
comparing the predicted phenotype from the system/algo-
rithm against the coded phenotype from a domain expert
[12].
Extracting stroke risk factors using natural language
processing
NLP has been applied and, at times, integrated with
structured data to successfully identify several stroke risk
factors such as peripheral artery disease [5, 13], diabetes
[4, 14], heart failure [15], and hypertension [16] as part
of large, coordinated research projects. Specifically,
Savova et al. extended the Clinical Text Analysis and
Knowledge Extraction System to extract and classify
positive, negative, probable, and unknown mentions of
peripheral artery disease (PAD) [13]. Kullo et al. then
leveraged this system to encode casecontrol status,
comorbidities, and cardiovascular risk factors from the
EHR for a GWAS study of PAD cases and controls for
the eMERGE project [5]. Wilke et al. applied the
FreePharma system to extract medication histories and
combine them with diagnoses and laboratory results
to identify a diabetes mellitus cohort as part of the
Marshfield Clinic Personalized Medicine Research Project
(PMRP) [14]. Kho et al. extracted diagnoses, medications,
and laboratory results leveraging NLP to encode variables
from unstructured fields for various sites to identify
type 2 diabetes cases and controls for a multi-institutional
GWAS study also as part of the eMERGE project [4].
Garvin et al. extracted left ventricular ejection fraction as
an indicator for heart failure using the Unstructured Infor-
mation Management Architecture (UIMA) as part of a
Translational Use Case Project and quality improve-
ment project within the Veteran Affairs (VA) Consor-
tium for Healthcare Informatics Research (CHIR) [15].
Finally, Thompson et al. translated the nine algorithms
Mowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 2 of 12
for phenotypes including hypertension developed from
the eMERGE project into the Quality Data Model
(QDM) to support EHR-based quality measures [16].
Although NLP has addressed many stroke-associated
risk factors for genotype-phenotype and other studies,
few studies have leveraged NLP to identify these risk
factors specifically for stroke prevention research. Fur-
thermore, to our knowledge, no NLP study has targeted
significant carotid stenosis - a known risk factor for
stroke. Our long-term goal is to develop a comprehen-
sive stroke phenotyping framework that extracts predic-
tors of stroke subtypes e.g., ischemic or hemorrhagic as
well as their precise endotypes e.g., ischemic stroke endo-
types of cardiac embolism, large artery atherosclerosis, or
lacunar infarction, other uncommon causes, from the
EHR powered by NLP. Our short-term goal is to develop
an NLP algorithm for a National Institute of Health
(NIH)-sponsored comparative effectiveness study of
ischemic stroke prevention treatments that automatic-
ally filters carotid reports for patients exhibiting no/
insignificant carotid stenosis of the internal or com-
mon carotid arteries from chart review. In this pilot
study, we completed a qualitative and quantitative
study of where and how mentions of carotid stenosis
findings occur in radiology reports and how this
affects an NLP algorithms performance.
Methods
In this Institute Review Board (IRB or Ethics committee)
and Veteran Affairs (VA) approved pilot study, we aimed
to conduct an information content analysis of a major
predictor of stroke, significant stenosis of the internal or
common carotid arteries, for a sample of free-text re-
ports from the Veteran Health Administration. Our goal
is to automatically distinguish reports denoting one or
more sides of significant stenosis (defined as greater
than 50 %, moderate, or severe stenosis) from reports
denoting no/insignificant stenosis (defined as negated,
ruled out, mild, less than 50 % stenosis) from both of
the internal or common carotid arteries. In this study,
we conducted an information content analysis of carotid
stenosis findings with respect to three aspects of docu-
ment composition - location (Sections), format (struc-
tures), and descriptions (expressions). We assessed the
performance of pyConText, an NLP algorithm, at auto-
matically extracting and encoding stenosis findings given
these three document constituents.
Dataset
We selected all reports from the VA EHR for patients
with an administratively documented carotid image pro-
cedure code (CPT code) restricted to those within ?1 to
+9 days of the procedure code date and that contained a
carotid term (carot, ica, lica, rica, or cca). In our
previous study, we leveraged 418 randomly sampled VA
radiology reports for developing our NLP algorithm,
pyConText, to identify mention-level stenosis findings
[17]. We extended this previous study by randomly
selecting a new set of reports to classify document-level
stenosis based on identified mention-level carotid stenosis
findings. This dataset consists of 598 radiology reports
(RAD: mainly ultrasound reports) and 598 text integration
utility notes (TIU: mainly progress notes, carotid duplex
exams, and carotid triplex exams) (see Fig. 1). Because
much of our algorithm development was completed
during our previous study [17, 18] and the prevalence of
stenosis positive reports is low, we chose a larger testing
set for each report type. We also chose to maintain
the natural distribution to give us a better sense of
whether pyConText could correctly retain stenosis
positive reports (high sensitivity) and to extrapolate
the potential chart review savings from filtering
Fig. 1 Sample texts by report type. Each text contains fictional, but realistic information
Mowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 3 of 12
stenosis negative reports (high negative predictive
value). The dataset was randomly split into two sets:
200 development reports (100 RAD and 100 TIU
notes) for algorithm knowledge base development
[18] and 996 testing reports (498 RAD and 498 TIU
notes) for information content analysis and algorithm
evaluation. For the information content analysis, three
research associates (domain experts) each independ-
ently and manually annotated the dataset for Sections,
structures, and expressions as well as classified the
report at the document-level as stenosis positive (if
the report contained one or more mention of signifi-
cant carotid stenosis) or stenosis negative (if the re-
port contained only mentions of no/insignificant
carotid stenosis). For the algorithm evaluation, the
RAD reports were extracted from the VA EHR as two
separate parts, Findings and Impressions. For the TIU
reports, we parsed the Findings and Impressions
using regular expressions written as a python script.
We assessed pyConTexts performance when provided
the Findings only, Impressions only, and the full
report.
Information content assessment
We aimed to characterize mentions of carotid sten-
osis findings according to Sections, structures, and
expression types. Each report could have zero, one,
or more relevant carotid stenosis findings recorded
with zero, one, or more Sections, structures, and
expression types.
Sections
RAD and TIU reports can be structured using
canonical sections e.g., Indication, Findings, and
Impression sections. We evaluated information con-
tent in the Findings (including Comments) versus
Impressions (including Interpretations and Conclusions)
sections [19].
Structures
VA notes can be generated using narrative or boilerplate
templates in which the contents are saved as unstruc-
tured or semi-structured texts, respectively. For example,
findings may be present in a variety of structures includ-
ing: prose, lists, tables, headings, and other (Table 1). We
evaluated information content according to these struc-
ture types [20].
Expressions
We have identified three types of expressions describing
carotid stenosis findings: category, range, or exact. We
characterized the information content according to these
expression types [21] (Table 2).
pyConText algorithm
pyConText is a regular expression-based and rule-based
system that extends the NegEx [22] and ConText [23]
algorithms. NLP developers can train pyConText to
identify critical findings and their contexts by defining
regular expressions for these targeted findings and their
desired modifiers within its knowledge base, respectively
[24]. These modifiers can be used to filter spurious
finding mentions that would otherwise generate false
positives if generating a cohort based on simple keyword
search. For example, a negation modifier can reduce
false positives by filtering denied findings e.g., no
carotid stenosis. Furthermore, a severity modifier may
reduce false positives by filtering insignificant findings
e.g., slight carotid stenosis. In a previous study, pyCon-
Text identified pulmonary embolism from computed
tomography pulmonary angiograms by filtering spurious
mentions using modifiers of certainty, temporality, and
quality with high sensitivity (98 %) and positive predict-
ive value (83 %). The pyConText pipeline is composed
of three main parts: named entity recognition, assertion
detection, and document-level classification.
Named entity recognition and assertion detection
Specifically, we adapted pyConTexts knowledge base of
findings and modifiers to filter no/insignificant carotid
stenosis findings using regular expressions. These ex-
pressions contain lexical variants including synonyms,
acronyms, abbreviations, and quantifications commonly
documented in clinical text to represent carotid stenosis
findings, semantic modifiers of severity, neurovascular
anatomy, and sidedness, and linguistic modifiers of exist-
ence, temporality, and exam [25]. In Fig. 2, we provide
the schema representing findings and each modifier as
well as the possible normalized values. We represent
these mentions and their normalized values using the
following syntax: finding/modifier(lexical variant: nor-
malized value). For example, in Fig. 3, Moderate plaqueTable 1 Structure types with example sentences
Example sentence
Prose 3045 % stenosis in the right ICA.
List 1. Both ICAs are occluded.
Table 95 % RICA 50 % LICA 75 % LECA
Heading Right: ICA: stenosis >70 %.
Other Any structures not listed above
Table 2 Expression types with example sentences
Example sentence
Category severe stenosis
Range stenosis ranging from 40 to 70 %
Exact 60 % stenosis
Mowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 4 of 12
in the right ICA is encoded as finding(plaque: carotid
disease), severity(Moderate: critical value), neurovascular
anatomy(ICA: internal carotid artery), sidedness(right:
right), and existence(default: definite existence) using the
knowledge base. pyConText leverages these normalized
modifier values to determine whether a mention of a
carotid finding(carotid disease) in the neurovascular
anatomy(internal carotid artery, common carotid artery,
carotid bulb or carotid bifurcation) represents no signifi-
cant stenosis (stenosis with existence: definite negated
existence), insignificant stenosis (stenosis with severity:
non-critical value e.g., values less than 50 % stenosis), or
significant stenosis (stenosis with severity: critical values
e.g., values equal or greater than 50 % stenosis).
Document classification
For document-level classification, if either side or both
sides of the internal or common carotid artery are deter-
mined to have significant stenosis, pyConText classifies
the reports as stenosis positive; otherwise, it classifies it
as stenosis negative. For RAD report example 1, in Fig. 3,
the report would be classified as stenosis positive
because two mentions of significant stenosis in the right
internal carotid artery were identified. Figure 4 depicts
RAD report example 1 fully processed by pyConText.
pyConText evaluation
pyConText applies a simple processing approach of
segmenting and tokenizing sentences to process reports.
The algorithm does not make use of Sections and
structures. Therefore, we quantified how frequently
complex document composition - Sections, structures,
and expressions - are utilized to report carotid stenosis
findings to gauge whether document decomposition pro-
cessing such as section or structure tagging is needed to
accurately extract findings. We evaluated the frequency
of errors by Sections, structures, and expressions by
comparing the predicted report classifications by pyCon-
Text to those generated by our domain experts.
Specifically, we defined a true positive when a report is
correctly classified by pyConText as stenosis positive
and a true negative when a report is correctly classified
by pyConText as stenosis negative. In contrast, we
defined a false positive when a report is spuriously
classified by pyConText as stenosis positive and a false
negative when a report is spuriously classified by pyCon-
Text as stenosis negative [12]. We assessed pyConTexts
performance by each Section and the full report using
standard performance metrics of sensitivity, positive
predictive value (PPV), specificity, and negative predictive
value (NPV) as follows:
1. sensitivity ¼ true positivetrue positiveþfalse negative
2. positive predictive value ¼ true positivetrue positiveþfalse positive
3. specificity ¼ true negativetrue negativeþfalse positive
4. negative predictive value ¼ true negativetrue negativeþfalse negative
Results
Our testing set was comprised of 498 radiology reports
(RAD) ultrasounds and 498 TIU notes. At the
document-level, for RAD reports, 353 (71 %) were
stenosis negative and 145 (29 %) were stenosis positive;
Fig. 2 Schema representing findings as well as semantic and linguistic modifiers and their possible normalized value sets
Mowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 5 of 12
for TIU reports, 396 (80 %) were stenosis negative and
102 (20 %) were stenosis positive. The RAD training set
distribution of 68 % stenosis negative and 32 % stenosis
positive was comparable to the RAD testing set distribu-
tion. The TIU training set distribution of 87 % stenosis
negative and 13 % stenosis positive reports differed
slightly from the RAD testing set distribution.
Information content assessment
Of the 498 RAD reports, we observed most carotid
mentions occur within the Impressions (488), are re-
corded using prose (706), and are expressed as categor-
ical expressions (713). Carotid mentions occurred often
within both Findings and Impressions (359) (Table 3).
In contrast, of the 498 TIU reports, we observed that
most carotid mentions did not occur in either the Find-
ings or Impressions (286). However, similarly to RAD
reports, carotid mentions were recorded using prose
(294), and were expressed as categorical expressions
(344) (Table 3).
Fig. 4 The resulting RAD report example 1 processed by pyConText
from Fig. 3
Fig. 3 Illustration of pyConTexts pipeline encoding a sentence and classifying the document from Fig. 1 RAD report example 1. Some modifiers
e.g., temporality and exam are not displayed for brevity. Blue mentions indicate templated mentions classified as no/insignificant stenosis; red
mentions indicate templated mentions classified as significant stenosis
Mowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 6 of 12
For RAD reports, within Findings, most carotid men-
tions were recorded as prose (306) followed by headings
(66); within Impressions, most carotid mentions were
recorded as prose (352) followed by lists (127) (Table 4).
In contrast, for TIU reports, within Findings, most
carotid mentions were recorded as headings (43)
followed by tables (33); as Impressions, most carotid
mentions were recorded as prose (88) followed by headings
(48) (Table 4).
For RAD reports, of the carotid mentions reported
within both Finding and Impression (n = 359 reports;
379 paired mentions), there was repetition of structure
types between sections (239 paired mentions, 63 %)
(diagonals in Table 5). In cases where a different struc-
ture was used between sections (140 paired mentions,
37 %), the most frequent cases were Finding: prose/Im-
pression: list, and Finding: heading/Impression: prose
(discordants in Table 5). For TIU reports, of the carotid
mentions reported within both Finding and Impression
(n = 67 reports; 53 paired mentions), there was repetition
of structure types between sections (22 paired mentions,
41 %) (diagonals in Table 5). In cases where a different
structure was used between sections (31 paired mentions,
59 %), the most frequent cases were Finding: table/Im-
pression: prose followed by Finding: heading/Impression:
list and Finding: heading/Impression: heading (discordants
in Table 5).
For RAD reports, both Findings and Impressions,
most carotid mentions were expressed as category (330
and 381, respectively) followed by range (73 and 178,
respectively) (Table 6). We observed similar trends for
TIU reports: category (73 and 116, respectively)
followed by range (59 and 110, respectively) (Table 6).
For RAD reports, of the carotid mentions reported
within both Findings and Impressions (n = 359 reports;
526 paired mentions), there was repetition of expression
types between sections (345 paired mentions, 66 %)
(diagonals in Table 7). In the cases where a different
expression type was used between sections (181 paired
Table 3 According to report type, overall frequency of at
least one carotid mention within sections, types of structures
for all carotid mentions, and types of expressions for all
carotid mentions
Information type Information subtype Report types
RAD TIU
Sections
Findings Total 368 106
Impressions Total 488 173
Findings Only 9 39
Impressions Only 129 106
Both 359 67
Neither/Not Applicable 1 286
Structures
Prose 706 294
List 256 76
Table 0 36
Heading 46 152
Other 2 6
Expressions
Category 713 344
Range 254 314
Exact 48 19
Findings Total = Findings only + Both; Impressions Total = Impressions
only + Both. Neither = report has Findings and Impressions, but does not contain
carotid mentions; Not Applicable = report does not have Findings
and Impressions
Table 4 Structure type usage according to sections and report
type
Prose List Table Heading Other
RAD
Findings 306 3 0 66 3
Impressions 352 127 0 22 0
TIU
Findings 25 6 33 43 0
Impressions 88 21 13 48 0
Table 5 Structure type usage between Findings (rows) and
Impressions (columns) for repetitive mentions by report type
Prose List Table Heading Other
RAD
Prose 233 (61 %) 73 (19 %) 0 (0 %) 1 (<1 %) 0 (0 %)
List 1 (<1 %) 1 (<1 %) 0 (0 %) 0 (0 %) 0 (0 %)
Table 0 (0 %) 0 (0 %) 0 (0 %) 0 (0 %) 0 (0 %)
Heading 35 (9 %) 27 (7 %) 0 (0 %) 5 (1 %) 0 (0 %)
Other 2 (<1 %) 1 (<1 %) 0 (0 %) 0 (0 %) 0 (0 %)
TIU
Prose 12 (23 %) 4 (7 %) 0 (0 %) 3 (6 %) 0 (0 %)
List 0 (0 %) 0 (0 %) 0 (0 %) 0 (0 %) 0 (0 %)
Table 15 (28 %) 0 (0 %) 1 (2 %) 0 (0 %) 0 (0 %)
Heading 0 (0 %) 9 (17 %) 0 (0 %) 9 (17 %) 0 (0 %)
Other 0 (0 %) 0 (0 %) 0 (0 %) 0 (0 %) 0 (0 %)
Table 6 Expression type usage by sections and report type
Category Range Exact
RAD
Findings 330 73 25
Impressions 381 178 23
TIU
Findings 73 59 8
Impressions 116 110 5
Mowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 7 of 12
mentions, 34 %), the most frequent cases were Finding:
category/Impression: range and Finding: range/Impres-
sion: category (discordants in Table 7). For TIU reports,
of the carotid finding mentions reported within both
Findings and Impressions (n = 67 reports; 105 paired
mentions), there was repetition of expression types
between sections (45 paired mentions, 43 %) (diagonals
in Table 7). Similar to RAD reports, in the cases where a
different expression type was used between sections (60
paired mentions, 57 %), the most frequent cases were
Finding: category/Impression: range and Finding:
range/Impression: category (discordants in Table 7).
pyConText evaluation
For RAD reports, pyConText achieved the highest posi-
tive predictive value (80 %) and specificity (93 %) when
provided Impressions only (Table 8). However, the
algorithm performed with lower sensitivity (74 %) and
negative predictive value (90 %) compared to perform-
ance when provided the full report performing with
higher sensitivity (88 %) and negative predictive value
(95 %). For TIU reports, we observed a similar trend.
pyConText achieved the highest positive predictive value
(76 %) and specificity (98 %) when provided Impressions
only, but higher sensitivity (73 %) and negative predict-
ive value (92 %) when provided the full report (Table 8).
For RAD reports, given the full report (including Find-
ings and Impressions), pyConText generated 128 true
and 56 false positive, and 297 true and 17 false negatives.
The 73 reports were misclassified due to non-mutually
exclusive errors of 96 prose, 42 list, 0 table, 12 headings,
and 0 other. These non-mutually exclusive errors were
the result of missed cues or erroneous scoping for 91
category, 50 range, and 16 exact expressions. In terms
of locality of errors, 53 mentions were in both section
types, 1 mention was in Findings only, 19 mentions were
in Impressions only, and 0 mentions were in neither
section. For TIU reports, given the full report (including
Findings and Impressions), pyConText generated 74 true
and 53 false positive, and 343 true and 28 false negatives.
The 81 reports were misclassified due to non-mutually
exclusive errors of 58 prose, 10 list, 8 table, 50 headings,
and 0 others. These non-mutually exclusive errors were
the result of missed cues or erroneous scoping for 74
category, 85 range, and 2 exact expressions. In terms of
locality of errors, 14 mentions were in both sections, five
mentions were in Findings only, 21 mentions were in Im-
pressions only, and 41 mentions were in neither section.
Discussion
We conducted a pilot study evaluating information
content of internal or common carotid finding mentions
in terms of Section, structure, and expression usage. We
also assessed pyConTexts performance given these
three factors.
Information content assessment
For RAD reports, most carotid mentions occurred in
both Impressions and Findings with a substantial por-
tion occurring in both sections. Overall mentions were
recorded mainly as prose structure using category ex-
pressions. When carotid mentions were reported in
Findings and Impressions, they were most often encoded
in prose. For these cases, pyConTexts simple text pro-
cessing can accurately extract most of these mentions.
In many cases, carotid mentions are repeated between
Finding and Impressions, mainly as prose. In the case of
discordant structure usage, this redundancy can be a
processing advantage. Specifically, one of the most
frequent cases was Finding: heading/Impression: prose.
Therefore, if given the full report, pyConText can still
correctly extract carotid mentions from the Impressions
when it incorrectly extracts mentions from the Findings
due to more complex structures like headings. Most
mentions were found in Impressions composed mainly
using expressions of category. In cases of repetitive
descriptions between Findings and Impressions, most
are Finding: category/Impression: category and men-
tions with discordant structure usage were Finding: cat-
egory/Impression: range. These observations suggest
Table 7 Expression type usage between Findings (rows) and
Impressions (columns) for repetitive mentions by report type
Category Range Exact
RAD
Category 278 (53 %) 108 (20 %) 14 (3 %)
Range 35 (7 %) 53 (10 %) 2 (<1 %)
Exact 16 (3 %) 6 (1 %) 14 (3 %)
TIU
Category 30 (29 %) 23 (22 %) 1 (<1 %)
Range 26 (25 %) 13 (12 %) 3 (3 %)
Exact 3 (3 %) 4 (4 %) 2 (2 %)
Table 8 pyConText performance according to report type
Sensitivity PPV Specificity NPV
RAD
Findings 57 67 88 83
Impressions 74 80 93 90
Full report 88 70 84 95
TIU
Findings 60 55 88 89
Impressions 19 76 98 82
Full report 73 58 87 92
For each metric and report type, the highest metric value is bolded
Mowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 8 of 12
that most severity descriptions can be extracted lever-
aging lexical, qualitative (e.g., severe) regular expres-
sions rather than quantitative (e.g., 7099 %) regular
expressions.
For TIU reports, in contrast to RAD reports, most
carotid mentions occurred in neither Findings nor
Impressions, suggesting localized processing of reports
for extracting carotid mentions would be suboptimal.
In the few cases when carotid mentions were reported
in Findings, they were most often headings followed
by table structures. Similar to RAD reports, carotid
mentions were reported in Impressions using prose,
but also using headings, suggesting that complex docu-
ment processing could be useful. Additionally, most
mentions were found in Impressions composed mainly
using expressions of category and exhibited the similar
distributions of repetitive expression descriptions between
Findings and Impressions.
For both RAD and TIU reports, we observed several
mentions with two or more expressions or structures.
For example, 55 % moderate ICA stenosis contains
two expressions: exact (55 %) and category (moderate).
pyConText evaluation
We aimed to optimize the number of flagged positive
cases for review (high sensitivity), while minimizing the
loss of positive cases due to filtering (high negative
predictive value); therefore, we conclude that pyConText
performed best with the full report rather than with only
the Finding or Impression sections. We hypothesize that
providing pyConText with the full report resulted in the
highest sensitivity because carotid mentions occurred
with variable prevalence within Findings and Impres-
sions (RAD) or within neither section type (TIU).
Error analysis
A detailed error analysis of pyConTexts outputs revealed
several areas of improvement to reduce false positives and
negatives. For each error described, we provide an
example and potential solution to boost performance
within pyConTexts processing pipeline.
Error 1: For both RAD and TIU reports, some false
positives were due to missing category or range expres-
sions for semantic modifiers. For instance, in Example 1,
although we had small as a non-critical value for sever-
ity and moderate as a critical value for severity, we did
not have small to moderate in our knowledge base due
to mixing of quality (small) and quantity (moderate)
descriptors. In these cases, our domain experts used the
lower bound (small) to classify the severity value and
assert the carotid mention as insignificant stenosis.
However, pyConText did not recognize this as a range
expression and the upper bound (moderate) was
incorrectly used to classify the severity value and assert
the finding as significant stenosis.
Example 1. small to moderate amount of calcified
plague in the left carotid bulb.
Potential solution 1: To improve assertion detection,
we can add missed cues and expand upon existing regu-
lar expressions for the severity modifier. We could also
add a rule that classifies ranges by the lowest bound for
a severity value range by selecting the non-critical value
over the critical value.
Error 2: In some cases, false positives were due to
missing lexical variants for linguistic modifiers. In
Example 2, we did not have a regular expression for
fails to demonstrate for existence: definite negated
existence; therefore, the algorithm classified the finding
as significant stenosis.
Example 2. examination of carotid arteries fails to
demonstrate significant stenosis.
Potential solution 2: To improve assertion detection,
again, we can add missed cues and expand upon existing
regular expressions to identify linguistic modifiers from
the text.
Error 3: Sometimes, the expressions were correct, but
spuriously attributed to flow velocities that were not
used to assert stenosis findings as in Example 3.
Example 3. diameter reduction.. cca with velocity of
82.
Potential solution 3: To improve assertion detection
and scope, we could have created another modifier
velocity to correctly scope the severity modifier and filter
this mention from classification.
Error 4: Our results suggest that we achieved lower
performance for TIU reports than RAD reports due to
more frequent usage of complex document structures
such headings and tables rather than less complex
document structures of prose and lists. In Example 4,
ICA was correctly attributed to Left 40 % stenosis,
but not associated to Right 30 % stenosis.
Example 4. ICA: Left 40 % stenosis. Right 30 %
stenosis.
Potential solution 4: To improve assertion detection
and scope, we could boost pyConTexts performance by
integrating outputs from a section tagger to identify
mentions of neurovascular anatomy from headings/sub-
headings and associate them to all subsequent sentences
within that section with relevant findings.
Error 5: In few examples, the algorithm generated a
false negative due to its failure to identify co-referred
findings of plaque. For Example 5, we observed two
consecutive, long sentences. The first sentence contains
a finding and neurovascular anatomy, but the second
sentence contains its severity modifier. To link the
severity in the second sentence to the finding and its
neurovascular anatomy in the first sentence, we would
Mowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 9 of 12
need to resolve that the finding plaque in the second
sentence co-refers to the finding plaque in the first
sentence and merge their templates.
Example 5. ..calcified plaque in the left ica 
data are consistent with between 50 and 80 % stenosis
by plaque.
Potential solution 5: To improve named entity rec-
ognition and assertion detection, we could handle co-
reference, by identifying co-referring expressions and
either merging or resolving conflicting values for each
finding template.
Error 6: Not all failures resulted in a document mis-
classification. In Example 6, the finding is not given, but
implied by the checkbox and associated modifiers of
sidedness, neurovascular anatomy, and severity so
pyConText did not extract a stenosis finding. However,
if this statement represented a significant stenosis mention,
a false negative would have resulted.
Example 6. Left ICA [x]: 015 %.
Potential solution 6: To improve named entity recogni-
tion and assertion detection, we could integrate outputs
from document decomposition software [26] that readily
identifies checkbox and question/answer constructs
based on characters within the text. We could leverage
these patterns to predict when and how these constructs
should be used to extract assertions and correctly assert
their scope when a finding is not explicitly mentioned.
Error 7: Similarly, although pyConText did not classify
a finding mention in one sentence due to a missing
modifier, it was able to identify and extract a finding
mention from another sentence to correctly classify the
report. In Example 7, pyConText does not find a neuro-
vascular anatomy modifier for the second sentence, so it
ignores it, but correctly classifies the report by correctly
extracting information from the first sentence.
Example 7. Right ICA occluded 1) occlusion on
the right.
Potential solution 7: To improve document classifica-
tion, we could classify sentences without a neurovascular
anatomy modifier, but this strategy would have caused a
significant increase in the number of false positives when
the mention represents an irrelevant neurovascular anat-
omy such as the external carotid artery, increasing the
number of reports for chart review by abstractors.
Error 8: Finally, false positives could be attributed to a
lack of topical context. In Example 8, the sentence does
not contain an actual finding, but rather guidelines for
classifying mentions as significant stenosis.
Example 8. definitions: 7099 % = significant stenosis
Potential solution 8: To improve document classifica-
tion, we could exclude extracted findings and assertions
detected from all sentences that occur in the context of
known guidelines e.g., documented NASCET legends by
filtering these mention with a semantic modifier
guidelines and regular expressions with guideline-
associated keywords like definitions, legend or
NASCET.
Although many of these solutions could prove useful,
they may add significantly to pyConTexts processing
time and complexity. For this study, it was only neces-
sary to identify about 6,000 Veterans for cohort inclu-
sion; therefore, we applied the system to the greater set
of patient records based on these results. Because our
goal is to retain as many stenosis positive cases as pos-
sible while filtering as many stenosis negative cases as
possible, we provided pyConText the full report rather
than only processing Impressions. To date, we have
encoded over 150,000 RAD and 200,000 TIU reports.
Given these results, we estimate that we have reduced the
chart review task for study abstractors to about 85,000
(~25 %) of the possible reports. The manual review of this
filtered set was completed in 4 months by three ab-
stractors rather than 12 months without the NLP
filtering.
Limitations
Our study has a notable limitation. We only address
reports from the VA EHR; therefore, pyConTexts
performance may or may not generalize to reports from
other institutions. However, if the reports contain
similar Sections, structures, and expressions, we would
expect similar results. We will evaluate pyConTexts
generalizability on University of Utah Healthcare System
reports for both genotype-phenotype association and stroke
risk assessment studies in the near future.
Future work
Although for this study, we developed a sensitive NLP algo-
rithm to identify high risk patients for stroke to support a
comparative effectiveness review study, we plan to extend
our algorithm to extract additional stroke risk factors for
precise stroke subtype phenotyping e.g., ischemic and
hemorrhagic stroke subtypes and endotypes e.g., ischemic
stroke endotypes of cardiac embolism, large artery athero-
sclerosis, and lacunar infarction, other uncommon causes
for genotype-phenotype association studies. We are actively
generating a pipeline with our knowledge base authoring
system, Knowledge Author, to leverage existing vocabular-
ies such as the Unified Medical Language System (UMLS)
[27] and Radiology Lexicon (RadLex) as well as ontologies
such as our Modifier Ontology to encode these stroke risk
factors in a more streamlined manner [28, 29].
Conclusions
We conclude that an information content analysis can
provide important insights for algorithm development and
evaluation including understanding information redun-
dancy and challenges when processing clinical texts to
Mowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 10 of 12
identify stroke risk factors. Our study demonstrates that,
in spite of these challenges, a simple NLP algorithm, can
be leveraged to reduce chart review efforts by filtering
reports with no/insignificant carotid stenosis findings and
flagging reports with significant carotid stenosis findings
from Veteran Health Administration clinical reports
to support a comparative effectiveness study of stroke
prevention strategies.
Availability of the supporting data
The supporting annotated dataset contains protected
health information and is stored in the Veteran Affairs
Informatics and Computing Infrastructure (VINCI). It is
not available to researchers outside of the Department of
Veteran Affairs. However, pyConText is available through
https://github.com/chapmanbe/pyConTextNLP. Additional
study information and collaborative development for
pyConText can be found at http://toolfinder.chpc.utah.edu/
content/pycontext.
Abbreviations
CPT: current procedural terminology; RAD: radiology; TIU: text integration
utility; EHR: electronic health records; GWAS: genome-wide association
studies; PheWAS: phenotype-wide association studies; ML: machine learning;
NLP: natural language processing; eMERGE: electronic medical records and
genomics; SHARPn: Strategic Health IT Research Program; PAD: peripheral
artery disease; IRB: Institute Review Board; VA: veteran affairs;
CHIR: consortium for healthcare informatics research; PPV: positive predictive
value; NPV: negative predictive value; UMLS: unified medical language
system; RadLex: radiology lexicon; VINCI: veteran affairs informatics and
computing infrastructure; PMRP: personalized medicine research project;
UIMA: unstructured information management architecture; QDM: quality data
model; NIH: National Institute of Health.
Competing interests
We have no competing interests.
Authors contributions
DM and BS designed the evaluation. EM and SK defined the stroke risk factor
definition, queried the dataset from the Veteran Affairs electronic medical
record, and facilitated the annotation of the dataset. BEC and WWC designed
the original pyConText algorithm and knowledge base. DM extended these
knowledge bases and adapted the pyConText algorithm. DM and MC wrote
the original manuscript and provided this manuscript for editing and feedback
to all coauthors. All authors read and approved the final manuscript.
Acknowledgments
This work was partially funded by VA HSR&D Stroke QUERI RRP 12-185, NHLBI
1R01HL114563-01A1, NLM R01 LM010964, & NIGMS R01GM090187. We thank
Alexandra Woodbridge, Ann Abraham, Rosa Ahn, and Susan Saba for their
excellent annotation efforts. We would also like to thank the Phenotype Day
workshop organizing and program committee for the opportunity to extend
our original submission [18] for this special issue and our anonymous
reviewers for their insightful feedback.
Author details
1Department of Biomedical Informatics, University of Utah, Salt Lake City, UT,
USA. 2IDEAS Center, Veteran Affair Health Care System, Salt Lake City, UT,
USA. 3San Francisco Veteran Affair Health Care System, San Francisco, CA,
USA.
Received: 21 October 2015 Accepted: 19 April 2016
Bolleman et al. Journal of Biomedical Semantics  (2016) 7:39 
DOI 10.1186/s13326-016-0067-z
RESEARCH Open Access
FALDO: a semantic standard for
describing the location of nucleotide and
protein feature annotation
Jerven T. Bolleman1*, Christopher J. Mungall2, Francesco Strozzi3, Joachim Baran4, Michel Dumontier5,
Raoul J. P. Bonnal6, Robert Buels7, Robert Hoehndorf8, Takatomo Fujisawa9, Toshiaki Katayama10
and Peter J. A. Cock11
Abstract
Background: Nucleotide and protein sequence feature annotations are essential to understand biology on the
genomic, transcriptomic, and proteomic level. Using Semantic Web technologies to query biological annotations,
there was no standard that described this potentially complex location information as subject-predicate-object triples.
Description: We have developed an ontology, the Feature Annotation Location Description Ontology (FALDO), to
describe the positions of annotated features on linear and circular sequences. FALDO can be used to describe
nucleotide features in sequence records, protein annotations, and glycan binding sites, among other features in
coordinate systems of the aforementioned omics areas. Using the same data format to represent sequence positions
that are independent of file formats allows us to integrate sequence data from multiple sources and data types. The
genome browser JBrowse is used to demonstrate accessing multiple SPARQL endpoints to display genomic feature
annotations, as well as protein annotations from UniProt mapped to genomic locations.
Conclusions: Our ontology allows users to uniformly describe  and potentially merge  sequence annotations from
multiple sources. Data sources using FALDO can prospectively be retrieved using federalised SPARQL queries against
public SPARQL endpoints and/or local private triple stores.
Keywords: SPARQL, RDF, Semantic Web, Standardisation, Sequence ontology, Annotation, Data integration,
Sequence feature
Background
Describing regions of biological sequences is a vital part
of genome and protein sequence annotation, and in areas
beyond this such as describing modifications related
to DNA methylation or glycosylation of proteins. Such
regions range from one amino acid (e.g. phosphorylation
sites in singalling cascades) to multi megabase contigs
mapped to a complete genome. Such annotation has been
discussed in biological literature since at least 1949 [1] and
recorded in biological databases since the first issue of the
Atlas of Protein Sequence and Structure [2] in 1965.
*Correspondence: jerven.bolleman@sib.swiss
1Swiss-Prot group, SIB Swiss Institute of Bioinformatics, Centre Medical
Universitaire, 1 rue Michel, Servet, 1211 Geneva 4, Switzerland
Full list of author information is available at the end of the article
There are many different conventions for storing
genomic data and its annotations in plain text flat file for-
mats such as Generic Feature Format version 3 (GFF3),
Genome Variation Format (GVF) [3], Gene Transfer For-
mat (GTF) and Variant Call Format (VCF), and more
structured domain specific formats such as those from
INSDC (International Nucleotide Sequence Database
Collaboration) or UniProt, but none are flexible enough
to discuss all aspects of genetics or proteomics. Fur-
thermore, the fundamental designs of these formats are
inconsistent, for example both zero-based and one-based
counting standards exist, a regular source of off-by-one
programming errors, which experienced bioinformati-
cians learn to look out for.
© 2016 Bolleman et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Bolleman et al. Journal of Biomedical Semantics  (2016) 7:39 Page 2 of 12
Although non-trivial, file format interconversion is a
common background task in current script-centric bioin-
formatics pipelines, often essential for combining tools
supporting different formats or format variants. As a
result of this common need, file format parsing is a partic-
ular strength of community developed open source bioin-
formatics libraries like BioPerl [4], Biopython [5], BioRuby
[6] and BioJava [7]. While using such shared libraries can
reduce the programmer time spent dealing with differ-
ent file formats, adopting Semantic Web technologies has
even greater potential to simplify data integration tasks.
As part of the Integrated Database Project (http://
lifesciencedb.mext.go.jp/en/) and the Core Technol-
ogy Development Program (http://biosciencedbc.jp/en/
33-en/programs/236-programs) to integrate life science
databases in Japan, the National Bioscience Database Cen-
ter (NBDC) and the Database Center for Life Science
(DBCLS) have hosted an annual BioHackathon series
of meetings bringing together biological database teams,
open source programmers, and domain experts in Seman-
tic Web and Linked Data [811]. At these meetings it
was recognised that failure to standardise how to describe
positions and regions on biological sequences would be
an obstacle to the adoption of federalised SPARQL Pro-
tocol and RDF Query Language (SPARQL) queries which
have the potential to enable cross-database queries and
analyses. Discussion and prototyping with representa-
tives from major sequence databases such as UniProt
[12], DDBJ (DNA Data Bank of Japan) [13] (part of the
INSDC partnership with the National Center for Biotech-
nology Information (NCBI)-GenBank [14] and European
Molecular Biology Laboratory (EMBL)-Bank [15]), and a
number of glycomics databases (BCSDB [16], GlycomeDB
[17], GLYCOSCIENCES.de [18], JCGGDB, RINGS [19]
and UniCarbKB [20]) and assorted open source devel-
opers during these meetings led to the development of
the Feature Annotation Location Description Ontology
(FALDO).
FALDO has been designed to be general enough to
describe the position of annotations on nucleotide and
protein sequences using the various levels of location
complexity used in major databases such as INSDC
(DDBJ, NCBI-GenBank and EMBL-Bank) and UniProt,
their associated file formats, and other generic annotation
file formats such as Browser Extensible Data (BED), GTF
and GFF3. It includes compound locations, which are the
combination of several regions (such as the join location
string in INSDC), as well as ambiguous positions. It allows
us to accurately describe ambiguous positions today in
such a way that future more precise knowledge does not
introduce logical conflicts, which potentially could only be
resolved by intervention of an expert in the field.
FALDO is suited to accurately describe the position of
a feature on multiple sequences. This is expected to be
most useful when lifting annotation from one draft assem-
bly version to another. For example, a gene can start at a
position for a given species genome assembly, while the
conceptually same gene can start at another position in
previous/following genome assemblies for the species in
question.
FALDO has a deliberately narrow scope which does not
address general annotation issues about the meaning of
or evidence for a location, rather FALDO is intended be
used in combination with other relevant ontologies such
as the Sequence Ontology (SO) [21] or database-specific
ontologies. That is, it is used only to describe the loci
of features, not to describe the features themselves. A
FALDO position relative to a sequence record is compa-
rable to a coordinate position on a map: it makes no claim
about how that sequence record or map is related to the
real world.
Implementation
FALDO is a small web ontology language version 2
(OWL2) ontology with 16 classes, 11 of these deal with
the concept of a position on a sequence (Fig. 1). The
instances of the faldo : ExactPosition represent
positions that are accurately determined in respect to a
reference sequence. There are two convenience subclasses
of faldo : ExactPosition to represent positions on
the N and C-terminal of a amino acid sequence. Three
of those classes are used to describe accurately what we
know of a position that is not precisely determined. Four
classes are used to describe the concept of a position on a
strand of DNA, e.g. positive, negative and on both strands.
All ten of these classes are sub classes of the generic
faldo : Position super-class. The eleventh class is the
concept of a region i.e. something with a end and start
position. The remaining 3 classes are used to group
regions which are biologically related but for which no
exact semantics are available e.g. some legacy data sources
cannot be mapped cleanly without expert intervention.
In contrast to other representations, FALDO has no
explicit way to say that it is not known on which strand
a position is, because this explicit statement unknown
strand position can introduce contradictions when merg-
ing different data sets. For example, some positions could
end up being contradictorily typed both as forward-
stranded as well as being located on an unknown strand
position.
Thereare3moreclasses (faldo: CollectionOfRegions
and its subclasses) that are only there for backwards
compatibility with INSDC join features with uncer-
tain semantics. i.e. those join regions where a con-
version program can only state that there are some
regions and that the order that they are declared in
the INSDC record might have biological significance.
However, here the INSDC record needs intelligent
Bolleman et al. Journal of Biomedical Semantics  (2016) 7:39 Page 3 of 12
Fig. 1 The classes and object properties used in FALDO
inspection before the data can be cleanly converted to a
data model with rich semantics.
FALDO defines a single datatype property,
faldo : position, that is used to provide a one-
based integer offset from the start of a reference
sequence. This property, when used together with
the faldo : reference property, links the con-
cept of a faldo : Position to an instance of a
biological sequence. Note that these terms are case-
sensitive: faldo : position is a property, and
faldo : Position is a concept.
For compatibility with a wide range of data, FALDO
makes very few assumptions about the representa-
tion of the reference sequence, and can be used to
describe positions on both single- and double-stranded
sequences. When both strands of a double-stranded
sequence are represented by a single entity (recommended
over each strand being represented separately), integer
faldo : position properties are counted from the 5
end of whichever strand is considered the forward
strand.
A key part of the FALDOmodel is the separation of fea-
ture and where a feature is found in a sequence record. For
this we use the faldo : location object property. This
property is used to distinguish between a conceptual gene
as an unit of inheritance and the corresponding repre-
sentation of the DNA sequence region encoding the gene
as stored in a database.
As in the INSDC data model and the associated Gen-
Bank ASN.1 notation, each location in FALDO has an
identifier for the sequence it is found on [22]. This means
that the position information is complete without further
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 
DOI 10.1186/s13326-016-0109-6
RESEARCH Open Access
tESA: a distributional measure for
calculating semantic relatedness
Maciej Rybinski and José Francisco Aldana-Montes*
Abstract
Background: Semantic relatedness is a measure that quantifies the strength of a semantic link between two
concepts. Often, it can be efficiently approximated with methods that operate on words, which represent these
concepts. Approximating semantic relatedness between texts and concepts represented by these texts is an
important part of many text and knowledge processing tasks of crucial importance in the ever growing domain of
biomedical informatics. The problem of most state-of-the-art methods for calculating semantic relatedness is their
dependence on highly specialized, structured knowledge resources, which makes these methods poorly adaptable
for many usage scenarios. On the other hand, the domain knowledge in the Life Sciences has become more and more
accessible, but mostly in its unstructured form - as texts in large document collections, which makes its use more
challenging for automated processing. In this paper we present tESA, an extension to a well known Explicit Semantic
Relatedness (ESA) method.
Results: In our extension we use two separate sets of vectors, corresponding to different sections of the articles from
the underlying corpus of documents, as opposed to the original method, which only uses a single vector space. We
present an evaluation of Life Sciences domain-focused applicability of both tESA and domain-adapted Explicit
Semantic Analysis. The methods are tested against a set of standard benchmarks established for the evaluation of
biomedical semantic relatedness quality. Our experiments show that the propsed method achieves results
comparable with or superior to the current state-of-the-art methods. Additionally, a comparative discussion of the
results obtained with tESA and ESA is presented, together with a study of the adaptability of the methods to different
corpora and their performance with different input parameters.
Conclusions: Our findings suggest that combined use of the semantics from different sections (i.e. extending the
original ESA methodology with the use of title vectors) of the documents of scientific corpora may be used to
enhance the performance of a distributional semantic relatedness measures, which can be observed in the largest
reference datasets. We also present the impact of the proposed extension on the size of distributional representations.
Keywords: Bioinformatics, Semantic relatedness, Semantic similarity, Distributional linguistics, Knowledge extraction,
Explicit semantic analysis, Biomedical semantics
Background
Introduction
A rapid growth in scientific publishing has been observed
in recent years. Thanks to online resources, the access
to this literature seems easier and quicker than ever, but
often the sheer volume of potentially relevant articles
makes it extremely difficult for the end user. However,
working with these large text collections may actually
*Correspondence: jfam@lcc.uma.es
Departamento LCC, University of Malaga, Campus Teatinos, 29010 Malaga,
Spain
result in the development of methods for automatic
semantic processing and annotation that could greatly
improve intelligent data access. This paper focuses on
the problem of calculating distributional semantic relat-
edness based on a large document corpus by leveraging
the semantics from different sections of the corpus ele-
ments (i.e. by making an explicit use of the semantics of
titles of scientific papers). Semantic relatedness is a met-
ric that can be assigned to a pair of labels in order to
represent the strength of the relationship of the concepts
described by those labels. The automated calculation of
© The Author(s). 2016 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 2 of 14
the metric is the building block for numerous semantically
enhanced data processing techniques such as: word sense
disambiguation [1] (used for matching word contexts to
the best word senses), text summarization [2] (used for
evaluating cohesion of the lexical chains) and information
retrieval [3] (incorporated in the query-document ranking
method). Similar applications of relatedness and similar-
ity (which is a narrower concept) metrics within the scope
of Life Sciences include entityentity relationship extrac-
tion [4, 5], semantic search [6] and redundancy detection
in clinical records [7]. An overview of applying semantic
similarity to the problem of comparing gene products is
discussed in [8]. In [9] the authors discuss the application
of a relatedness measure as an approximation of semantic
similarity in the biomedical domain.
The methods for calculating semantic relatedness can
be roughly divided into two main groups: those that rely
entirely on a specialized and structured knowledge-rich
resource (e.g. [1012]), and distributional measures that
rely on implicit statistical features of a large document
collection (e.g. [13, 14]). With the increased popularity of
using Wikipedia as a Knowledge Base (KB) for seman-
tic relatedness estimation this division has become much
less clear, as Wikipedia combines the features of both
worlds. It does implicate a structure, as it comprises a set
of topic-oriented and categorized entries, which are also
interconnected with hyperlinks. It can also be treated as
a large collection of documents, as it contains over 2M
articles with at least 150 words each.
In this paper we focus on corpus-based distributional
methods for calculating semantic relatedness and we
present a new measure, which can be applied in the
biomedical domain without having to rely on specialized
knowledge rich resources. In our approach, which is an
extension of a well-established state-of-the-art method,
we superimpose the semantics of different sections of
documents (i.e. wemake additional use of the titles of sci-
entific articles). We demonstrate that our method slightly
outperforms other state-of-the-art approaches while rely-
ing on the very limited structure of the documents within
the corpus (only abstracts and titles from the Medline
corpus [15] are used in the best performance setting).
Related work
There is a significant body of work devoted to biomedi-
cal ontology-independent (to a certain degree) relatedness
measures that rely on context vectors, i.e. the immediate
neighborhoods of the phrases/words throughout a docu-
ment corpus, e.g. [16]. In the method presented in [16],
context vectors are created using a sliding window tech-
nique, that is, scanning through contexts of a certain size
throughout the entire corpus of documents in order to
find words that co-occur with certain terms or phrases of
interest. In order for this technique to be employed, the
authors use a predefined set of these terms/phrases, i.e.
SNOMED CT. SNOMEDCT is the largest medical vocab-
ulary collection, with over 400K systematically organized
concepts with their lexical representations and additional
information. In the method presented in [16], the distri-
butional representations are created for each SNOMED
CT concept by adding word vectors of tokens relevant to
respective concepts. Despite the fact that the approach
uses additional resources (SNOMED, Mayo Clinic The-
saurus), the relatedness calculation depends on the corpus
co-occurence distribution, without referring explicitly to
the ontological structure of SNOMED. Both in [17], and
more recently in [9], a similar approach has been used
with a different set of resources. Themain feature that sets
the method presented in our paper apart is that it does not
need pre-existing concept descriptions (such as those of
SNOMED CT) in order to produce the relatedness score.
As mentioned briefly, there is a large group of methods
that use Wikipedia as a knowledge resource/document
collection, some examples include [1820]. Most of these
measures exploit Wikipedia-specific features such as links
or categories. Nonetheless, Wikipedia as a resource (at
least currently) is too general in nature for many Life Sci-
ences applications. Therefore, from our perspective, the
methods that treat the data more like a generic document
collection seem more appealing, the most notable exam-
ple being Explicit Semantic Analysis (ESA) [21]. In ESA,
the input texts are represented by a vector, in which each
element corresponds to aWikipedia article. Values of each
of the elements are determined by the importance of the
input text to the contents of each article, i.e. i-th element
of the vector for a word or a phrase will be determined by
the importance of the word within the i-thWikipedia arti-
cle (formal description of the method is provided further
on in this paper). The relatedness between the inputs is
calculated as the cosine similarity between those vectors.
Numerous extensions of ESA have been proposed,
many of which combine the original approach with the
Wikipedia-specific features, through concept-to-concept
feature/similarity matrices, e.g. [2224]. Some of those
extensions, e.g. NESA [25] (Non - Orthogonal ESA), also
provide variants that are generic enough to be used with
any document collection. The aim of NESA is to lever-
age inter-document similarity in the calculations. In our
measure the input is modeled in a way similar to ESA,
but we propose an extension so as to capture the fea-
ture based similarity between sets of documents. However
our method is much more resource efficient than NESA,
which facilitates handling a large corpus of documents.
In the biomedical domain there have also been sev-
eral attempts to use Wikipedia based methods, recent
examples include [26] and [27]. The former presents an
application of the ESA methodology to a KB extracted
automatically from MedLine Plus corpus in the context
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 3 of 14
of semantic relatedness. The latter uses ESA inspired
methodology with yet another KB in the context of docu-
ment classification.
As we have previously argued [28], results compara-
ble to those of state-of-the-art methods can be obtained
by approximating the context vectors with the vectors
extracted from the relatively small sample of best-fit
documents from a moderately sized PMC open subset
corpus [29]. We now expand on these conclusions in
combination with an ESA inspired approach to achieve
better results, coverage and independence from the spe-
cific parameters of the algorithm, which was one of the
drawbacks in our previous approach. The new method
takes advantage of a larger document collection (Med-
line), but performs well with only the abstracts and titles
available.
Within the NLP community, so called word embedding
methods have received much attention. In these tech-
niques words or phrases from the original corpus are
mapped to low dimensional vectors through language
modelling and/or feature learning. One of the most widely
discussed representative of this group, word2vec [30] is a
group of methods that use neural networks for unsuper-
vised training of a model that either predicts a context
given a word, or predicts the word given a context. Appli-
cation of word2vec in biomedical settings is presented in
a recent study [31].
There is also a significant body of work related to KB-
based semantic relatedness measures which use highly
specialized resources, described in a detailed overview in
[32] and [33]. KB-based methods are useful wherever an
adequate domain knowledge model can be used to com-
pute semantic relatedness. In [34] the authors showcase
the performance of a wide spectrum of ontology based
Information Content (IC) methods, which use SNOMED
CT as a knowledge resource. The IC measures use an
ontological structure (positions of concepts in the ontol-
ogy, distance between them, number of sub-concepts,
etc.) to compute a semantic score between a pair of con-
cepts. Our method, although dependent on a specific
corpus, does not rely on high level KB representations of
the domain, which makes it more flexible and easier to
adapt to non-standard use cases.
Contributions
Here we present Title vector Explicit Semantic Analysis
(tESA), a novel approach for approximating word-based
semantic relatedness, which uses a document corpus
as its only source of background knowledge. The tESA
method itself is an extension of ESA, based on using
two sets of vectors corresponding to different sections of
the documents of the corpus. Together with the exper-
iments detailing its performance, tESA is our primary
contribution.
Additionally, we present a parallel evaluation of the
original ESA methodology in the same settings (corpora
and reference standards). To the best of our knowl-
edge it is the first time that the ESA implementation
has been evaluated in such detail within the biomedical
domain.
In the Methods section we present a detailed descrip-
tion of ESA, tESA and the experimental evaluation. We
also highlight the distinguishing design features of tESA
by comparing it to other corpus-based methods. Then, in
the Results and discussion section we present the results
obtained through the evaluation, compare them to other
state-of-the-art methods and discuss some of the impli-
cations. In the final Conclusions section, apart from pre-
senting the final remarks, we also outline possible lines of
future work.
Methods
In this section, we firstly explain the basic concepts
that will help clarify the design of the tESA method.
We then provide a short description of the origi-
nal ESA method and then we introduce the tESA
method, while outlining the main differences between the
two.
Basic notions
The black-box view of a semantic relatedness approxima-
tion system is fairly simple - the system takes two input
texts (also referred to as inputs) and returns a relatedness
approximation (score). The inputs can be texts of vari-
able length, typically single words or short phrases are
considered.
The actual processing involves the inputs and a col-
lection of documents - referred to as the corpus. We
use a term document to denote a semistructured textual
resource that forms part of this collection, i.e. a document
can be formed by a number of sections; here, we focus on
a simplified case of documents consisting either of titles
and abstracts or titles and the fulltext body (depending on
their availability in various document collections included
in the evaluation).
As mentioned, our method is based on a distribu-
tional vector representation of input texts. As is com-
mon in many distributional linguistics algorithms, we
use certain variations of the tf-idf (term frequency,
inverse document frequency) weighting scheme as the
underlying vector model for text representation. So,
at the most basic level, prior to relatedness calcu-
lations, any texts (inputs, document abstracts, titles)
are modeled as tf-idf weighted vectors. Term fre-
quency is the number of times a given term appears
within the scope of a certain text (i.e. certain section
of a document), while inverse document frequency is
defined in the context of a specific document collection:
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 4 of 14
idf(t,D, f ) = log N|df ? D : t ? df | , (1)
where D denotes a certain corpus of documents, N
denotes size of the corpus, t denotes the term and d
a document, f denotes a section of documents from
the corpus and df a text of the section f of a docu-
ment d. Those elements lead us to the formula for tf-idf :
tfidf(t, df ,D) = tf(t, df ) × idf(t,D, f ) (2)
The equation presents a basic implementation of tf-idf
weighting, whereas within our approach we use slightly
different variants. For modelling abstracts the in-built
Lucene [35] scoring function is used. It uses a document
length normalization factor, a square root norm for the
tf factor and a square norm for the idf factor. For titles
we assume tf equals 1 whenever a term appears within
the title and zero otherwise. Nonetheless, the basic idea is
that within a vector for a single document higher weights
are assigned to terms that either appear more often within
the document or are less common throughout the entire
corpus. When creating the vector representation of text
using the tf-idf scheme, vectors are assembled by placing
a weight corresponding to each of the documents terms
at the position corresponding to the term, so the dimen-
sionality of the model is given by the number of unique
words present in the section of the documents throughout
the collection. Therefore the vector space is of a very high
dimension, while the actual vectors are normally sparse.
It is worth noting, that, given a corpus and a specific
section of its documents, the vector representation can
be created for any text, regardless of whether the text
belongs to the corpus or not. This representation will
obviously differ depending on the choice of the corpus
and the section. This notion is typically used in vector-
based information retrieval (IR), where most relevant
documents are found for an input query and a field or a
combination of fields of an index, where fields correspond
to sections and index to the corpus. Commonly, to decide
whether a document fits the query, one can compare the
vector representing the query with the vector represent-
ing the section of a document. We use cosine similarity as
the basic tool for pairwise vector comparison. This applies
to word-based tf-idf vectors and extends to other types
of vectors, as explained further on in this section. For a
pair of n element vectors A and B the cosine similarity is
defined as follows:
cosine(A,B) =
n?
i=1
AiBi
?
n?
i=1
A2i
?
n?
i=1
B2i
(3)
Text preprocessing
We use standard Lucene mechanisms for pre-processing
of texts prior to the tf-idf vectors computations. Texts
are transformed to lowercase and stopwords (words that
occur very commonly, but provide little or no semantic
information, e.g. the, of, at, a, etc.) are eliminated. Num-
bers are also eliminated and non-alphanumeric characters
(e.g. -) are normalized. In case of the titles, we also disre-
gard words that appear in less than 3 different documents
of the respective corpora.
ESA
These basic notions lead us to the more complex one
of a doc vector (also referred to as concept vector in the
original ESA paper [21]), which is the central building
block of ESA. In the ESA method the doc vectors are used
to provide a distributional representation of the inputs.
The relatedness is then approximated for a pair of inputs
by comparing their doc vectors. Cosine similarity is used
to obtain the numeric result of this comparison. By a
doc vector of an input q we mean a vector in which the
value of an i-th element is calculated as a cosine similar-
ity between: (a) the tf-idf vector representing the input
q w.r.t. the IDF values calculated for the abstracts of the
corpus; (b) tf-idf weighted vector representing an abstract
of an i-th document of the corpus 1. It is worth not-
ing that the dimensionality of the doc vector is given
by the size of the corpus. Ttf-idf vector qabstract repre-
sents an input q w.r.t. the statistics (i.e. IDF) derived
from the abstract section of the corpus documents. We
can define the doc vector qD as a vector of weights wi,q,
where
wi,q = cosine
(
abstracti, qabstract
)
(4)
where abstracti denotes the tf-idf vector of the abstract
for the i-th document from the N document corpus. In
the original method a corpus of Wikipedia articles is
used, along with their text contents. In this paper, apart
from the original Wikipedia-based implementation, we
also present experiments with domain-focused corpora.
In practical implementations it is enough to consider a
set of M highest scores within the vector, as the tail of
N-M values are either zeroes or have little impact on fur-
ther processing. As such, ESA methodology can also be
explained in information retrieval terms, with the input
treated as a query and the results represented with a doc
vector of non-zero values at M most significant elements.
Those values, in a most basic tf-idf weighted vector space
model representation, are given with the formula for wi,q.
This intuitive explanation of ESA might clarify the step-
by-step processing of tESA, presented further on in this
section.
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 5 of 14
tESA
It can be observed, that a corpus with documents that
have more than one section can be used to establish more
than one independent vector space, i.e. a corpus with doc-
uments that consist of titles and abstracts can be used
to create a vector space of titles and a vector space of
abstracts. Creation of a doc vector involves the vector
space of abstracts to determine the weights/elements at
positions corresponding to certain documents. Nonethe-
less, the doc vector itself is expressed in a yet another
space of dimensions (of documents, rather than words).
The main idea behind tESA is to create a similar vector
expressed in a different vector space, i.e. one with notably
fewer dimensions - a vector space of document titles. The
tESA vector is a doc vector transformed through a mul-
tiplication by the column matrix of tf-idf vectors of titles
(which means term-document matrix of title-associated
tf-idf weights). The matrix represents the vector space
model of the document titles. By tf-idf vectors of titles
we refer to word-based tf-idf representations of individual
titles of documents, while a tESA vector is a distributional
representation of an input text, much like a doc vector
in ESA. C denotes the column matrix of tf-idf vectors of
titles; Cji, which denotes the element of j-th row and i-th
column of C (which therefore corresponds to the title of
the i-th document and j-th term of the title vector space),
is given by (see Eq. 2):
Cji = tfidf (kj, dtitle(i),D), (5)
where dtitle(i) denotes the text of the title of the i-th doc-
ument and D denotes the corpus of documents and kj
denotes the j-th term of the title vector space.
Given the matrix C defined above, let qT denote a tESA
vector of input q, while qD denotes the doc vector of input
q. The tESA vector qT is defined as follows:
qT = CqD (6)
This means, that using the Eq. (4) the j-th element of qT ,
qT j, corresponding to a j-th row of the matrix C (an thus
to the j-th term of the title vector space), is given by:
tT j =
N?
i=1
cosine
(
abstracti, qabstract
)
× Cji, (7)
where abstracti denotes a tf-idf vector of the abstract of
the i-th document and qabstract denotes a tf-idf represen-
tation of the input q in the vector space of document
abstracts. A j-th element of the tESA vector is therefore
defined as a weighted sum of tf-idf weights of the j-th term
(of the titles vector space) over the corpus of the docu-
ment titles. This sum is weighted with the input-abstract
cosine similarities from the doc vector.
As mentioned, in our implementation for the title vec-
tor space, we assume that tf (kj, titlej) = 1 if term kj is
present in the j-th title, otherwise the value of the tf term is
0. Additionally, to reduce the computations, in our imple-
mentation we calculate the tESA vector from a doc vector
truncated at M of its most significant elements, as: (a) the
tail values have little impact on the final results; (b) most
commonly the doc vector will have fewer thanMnon-zero
values anyway (which is discussed in the next section of
this paper).
As displayed in Fig. 1, the processing of our method can
be divided into three main steps:
I Finding doc vectors of both inputs, truncated at M
highest-value elements
II Calculation of the tESA vectors for each of the
inputs (see Eq. 5).
III Using the tESA vectors to compute the
relatedness approximation as the cosine similarity
between the tESA vectors.
Under information retrieval terminology, we use the
input text as a query for the abstract/fulltext based vec-
tor space model. Results of this query (scores for each of
the individual documents, M values at the most) are rep-
resented by the doc vectors. In ESA we would use the doc
vectors as the final representations of the inputs, mean-
while in tESA we perform an additional calculation. In
other words, we transform the doc vectors to tESA vectors
using the title vector space of the corpus and the formula
of Eq. 6. Therefore, the resulting vector will have non-zero
weights at positions corresponding to the vocabulary of
titles of the documents in which the input terms appear
within the abstracts. Additionally, we promotemeaningful
terms from the titles (through IDF), especially in the con-
text of documents, in abstracts of which the input terms
play a prominent role (modeled with the doc vector ele-
ments, here used as a prior). We expect this additional
computational effort to provide an improvement on two
levels: (a) an improvement in the quality of the results and
(b) using smaller representation vectors to model inputs.
When it comes to improving the quality of the results,
our expectations are based on the fact, that statistically
it is likely that sets of titles of similar/related documents
will share some part of the vocabulary. Our approach
adds another level of intrinsic similarity between doc-
ument sets, i.e. the input terms are related not only if
they appear in the same abstracts, but also if the sets
of abstracts they appear in share common features (title
vocabulary). Our expectation of smaller representations
can be derived directly from two assumptions. Firstly, the
dimensionality of the vector space of titles is much smaller
when compared to the dimensionality of the vectors used
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 6 of 14
Fig. 1 Overview. Overview of the methods components
in ESA (e.g. in the case of Medline the difference is
of 300K compared to 14M). Secondly, using very short
tf-idf word vectors to represent titles (the vectors are
truncated to represent only the top-idf  vocabulary), com-
bined with the expectation that some title vocabulary will
overlap between documents, should result in representa-
tion vectors with fewer non-zero elements than the doc
vectors. Both hypotheses, (a) and (b) are evaluated in the
experiments.
Design differences: tESA vs other distributional approaches
On a conceptual level the processing in our method is
similar to ESA, except that in ESA the relatedness approx-
imation is calculated directly as the cosine similarity of the
doc vectors. The direct application of the ESA approach
will also be discussed. As mentioned, the tESA vectors
were designed to take advantage of inter-document sim-
ilarity, by expressing the doc vector in the title vector
space, in which the documents, or more importantly
groups of documents, may share common features. XESA
and NESA also benefit from the use of inter-document
similarity but in an explicit manner, through the use of
the document-to-document similarity matrix. The NESA
approach uses an N × N sized dense document similarity
matrix, which requires costly preprocessing and signif-
icant resources for runtime processing. The authors of
XESA also contemplate the use of a truncated similarity
matrix.
ESA and tESA provide a flexibility and efficiency advan-
tage over approaches such as those presented in [16]
and [17] and their extensions. Specifically, they use cor-
pus statistics instead of relying on contex window word
counts, which means that the new distributonal represen-
tations can be created without having to actually scan
through all the documents that contain the input terms,
so the cost of creating the representation vectors is much
lower.
Word embeddings (i.e. word2vec) have the advantage
of using dense representation vectors of relatively low
dimension (typically around 200), which makes those
methods computationally appealing. However, the use of
machine learning to pre-train the model hinders the flex-
ibility of those methods to a certain degree. For example,
switching from unigram to bigram inputs would require
either re-training of the entire model or using some kind
of composition strategy involving unigram vectors (addi-
tion, multiplication), while ESA and similar methods can
be adapted relatively easily or need no adapting at all,
depending on the actual implementation.
tESA can also be presented as an extension of the
method presented in [28]. The previous approach uses a
much smaller M to limit the number of relevant docu-
ments even further. Furthermore, it does not distinguish
the importances of those documents, i.e. the represen-
tation vector was created simply by adding the M most
important tf-idf truncated vectors of fulltext documents
(not their titles). The extensions that differentiate tESA
from the original method at the design level can there-
fore be summarized as follows: increased size of M, use
of a vector transformation (see Eq. (5)) and use of title
vectors instead of fulltext/abstract vectors. These changes
might seem minor, but they actually represent an impor-
tant change of focus, from an attempt to capture the
sample of most relevant vocabulary to represent an input,
to modeling the distribution an input generates over a
title vocabulary of a corpus.
Experiments
The tESA method was designed to work with the Med-
line baseline corpus, which provides us with over 14M
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 7 of 14
abstracts with corresponding titles. In addition, the meth-
ods were tested with different document collections,
which included PMC Open Access (PMC OA) and
Wikipedia articles. A summary of the corpora used in the
experiments is presented in Table 1.
The reference datasets used in the experiments were:
mayo101 [36], mayo29c, mayo29ph [16], umnsrsRelate,
umnsrsSim [37]. Each of the datasets represents a separate
experiment, in which a group of annotators rated pairs
of concepts for semantic relatedness (mayo101, mayo29c,
mayo29ph, umnsrsRelate) or similarity (umnsrsSim). The
datasets contain a list of pairs with a single consen-
sus score. The consensus score available in the reference
datasets was achieved by calculating an average score over
multiple annotators. It is important to note that mayo29c
and mayo29ph are high-agreement sets, rated by medical
coders and physicians respectively. The mayo101 dataset
consists of 101 concept pairs rated by a group of profes-
sional medical coders from Mayo Clinic. The remaining
two datasets, i.e. umnsrsRelate and umnsrsSim, contain
clinical concept pairs rated for similarity/relatedness by a
group of medical residents. The latter two also include a
standard deviation calculated for each pair of the labels,
which can be used to approximate an inter-annotator
agreement on each of the average scores. We use this
feature to demonstrate the performance of the methods
under discussion on high-agreement subsets of these two
datasets. The size and other features of the reference
datasets are summarized in Table 2.
In the experimental evaluation of an automated mea-
sure, the pairs of labels from the reference dataset are
treated as inputs. Inmost cases each input is a single word,
although there are two-word inputs as well. For a list of
pairs of inputs a list of relatedness scores is generated by
the system. This list is then compared to the list of average
scores generated by human annotators. The performance
of the methods in approximating human judgement was
measured as the Spearmans rank correlation coefficient,
as the problem can be seen as one of ordering the con-
cept pairs within each dataset by their relatedness, i.e.
both the consensus score and the approximation system
rank the pairs within each reference dataset from the
most related to the least related (by assigning scores).
The performance has been measured for our imple-
mentation of ESA and tESA and is evaluated against
other state-of-the-art methods, which, to the best of our
knowledge, represent the best results reported in the
literature.
Additionally, due to the nature of the methods, each
pairing of a dataset and corpus may be associated with a
certain recall value, which provides information on how
appropriate the corpus is for the benchmark. Recall in
our setting is defined as a ratio of the number of inputs
with a representation to the total number of distinct
items from a given dataset. It therefore gives the per-
centage of inputs that are present in each of the corpora,
which means that they can be assigned a distributional
representation.
Our experiments involved three methods: ESA, tESA,
and the method presented in [28]. Each of the methods
was evaluated with a combination of three different cor-
pora. Additionally, we also compared them to the best
results reported in the literature. NESA and XESA were
not present in the evaluation, largely due to the high com-
putational cost involved in creating an N × N similarity
matrix for a corpus as large as Medline. Furthermore, our
early experiments with a truncated similarity matrix actu-
ally caused an important performance drop compared to
the original ESA setup with the same domain-focused cor-
pus, which might indicate a high corpus sensitivity of the
method and is is briefly discussed in the following section.
As stated, the quality of the methods is measured as a
rank correlation with the reference scores produced by
human annotators. In order to compare the performance
of two methods we effectively compare the correlations
they produce w.r.t. a specific reference sample of limited
size. To provide a full perspective on our results, we eval-
uate the statistical significance of correlation comparisons
using a methodology presented in [38]. Specifically we
construct a 0,95 confidence level confidence intervals (CI)
Table 1 Presentation of the general characteristics of the corpora used in the experiments
MEDLINE PMC OA Wikipedia
Size 14073912 1024890 3807314
Type Scientific Scientific Encyclopedic
Documents Abstacts and titles Mostly fulltext +abstracts +titles Fulltext +titles
Snapshot date Autumn 2015 September 2015 December 2015
Token count [M] 2531,14; 264,84 3684,89; 15,8 2434,55; 11,13
Unique token count [M] 3,85; 1,24 35,57; 0,48 12,53; 0,98
Token counts and unique token counts are expressed in millions. These statistics are collected for raw texts (before preprocessing) and raw corpora (e.g. there might be an
uneven number of titles and abstracts in Medline). For each corpus and count type we provide two metrics - of the documents textual contents (abstract or full articles) and
titles. The statistics are included to highlight the compositional differences between the corpora
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 8 of 14
Table 2 Presentation of the general characteristics of the datasets used in the experiments; number of pairs and distinct items
describe the size of the datasets; the focus of the dataset column contains the information on the type of relationship captured in the
reference results
Dataset No of pairs Distinct items Reference Focus of the dataset Annotators Scale ICC(2,1)
umnsrsSim 566 375 [37] Similarity Residents 0 - 1600 0.47
umnsrsRelate 587 397 [37] Relatedness Residents 0 - 1600 0.5
mayo101 101 191 [36] Relatedness Medical coders 1 - 10 0.5
mayo29c 29 56 [16] Relatedness Medical coders 1 - 10 0.78
mayo29ph 29 56 [16] Relatedness Physicians 1 - 10 0.68
The ICC (2,1) presents interclass corelation coefficient, which provides an objective measure of inter-annotator agreement; the issues of inter-annotator reliability are covered
in more detail in the corresponding reference papers
for dependent overlapping correlations (as for a pair of
methods, both of them produce their correlation against
the same reference dataset). This test allows us to refute,
under the assumed confidence level, the null hypothesis
of the two correlations being equal. As our main goal is to
evaluate tESA, we test the statistical significance of tESA
correlations vs those of other methods. We used [39] as a
practical guide to implement the statistical test.
Results and discussion
Table 3 shows the scores obtained with ESA, tESA, and
the method presented in [28], with different corpora, for
each of the reference datasets. The table also features
the best reported score for each of the datasets. The
results for tESA and ESA were obtained for M=10000, so
each doc vector has non-zero values at, at most, 10000
positions (corresponding to the highest scoring docu-
ments). This value of the M parameter has been selected
as a possibly small value for optimal performance of all
setups/methods included in the evaluation - Fig. 2 shows
how the results depend on the values of M for ESA
and tESA with different corpora on the umnsrsRelate
dataset.
Figure 3 presents the correlation coefficient obtained
by the methods set up with the Medline corpus in the
function of inter-annotator agreement for the umnsrsRe-
late dataset. For each run the dataset had a standard
deviation threshold decreased in order to exclude the
low agreement portions of the datasets. The data pre-
sented in Fig. 3 indicates that both ESA and tESA provide
more accurate results for the sets that were more agreed
upon by the human annotators. Although this seems intu-
itive, the improvement of the ranking in the function of
inter-annotator agreement indicates that the method does
provide a decent approximation of human judgment par-
ticularly w.r.t. the difficulties in reaching a correct score
for the same pairs of inputs which seemed problematic
for human annotators. In the case of a similar experiment
Table 3 Overview of the results for different experimental settings - corpus and benchmark pairs; ESA and tESA runs with M=10000
and DS (the method described in [28]) runs with M=200 and cutoff at 0,02 (robust parameters, that can be expected to provide decent
results in different experimental settings)
Corpus Method umnsrsRelate umnsrsSim mayo101 mayo29ph mayo29c
ESA 0.608 0.621 0.546 0.835 0.734
Medline tESA 0.649 0.639 0.549 0.783 0.687
DS 0.46 0.438 0.511 0.483 0.493
ESA 0.588 0.597 0.543 0.855 0.75
PMC tESA 0.595 0.607 0.484 0.796 0.7
DS 0.574 0.626 0.504 0.738 0.673
ESA 0.501 0.5 0.548 0.822 0.722
Wiki tESA 0.484 0.484 0.502 0.801 0.755
DS 0.444 0.463 0.413 0.627 0.597
Best reported (citation) 0.54 [28] 0.58 [28] 0.6 [28] 0.84 [16] 0.9 [34]
The table row for best reference results has been compiled with results reported in the domain literature for the respective datasets, regardless of the type of method used to
achieve those results. Best reported results for umnsrsRelate, umnsrsSim and mayo101 were attained with specific parameter combinations in our experiments (presented in
[28]), whereas for the two smaller datasets the best results were previously obtained with knowledge-rich methods (distributional and IC-based respectively for mayo29ph
and mayo 29c). Updated best results are highlighted with bold font
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 9 of 14
Fig. 2 Performance changes for different M (cutoff limit for a maximum number of documents considered in the distributional representation). The
figure shows the correlation with human judgement of ESA and tESA with different corpora in the function of M; the values were obtained for
umnsrsRelate dataset
performed on the umnsrsSim dataset, see Fig. 4, the link
between the IAA and the quality of the results does not
seem to be evident for tESA (which begins to show a
decrease in performance at some point), while for ESA
the performance decreases initially and begins to improve
at a certain point. Considering that there is little evi-
dence (only two experiments) it is difficult to reach a
definite conclusion. There is a possibility. that the results
presented in Fig. 4 are due to the fact that the umn-
srsSim dataset is focused on semantic similarity, which is
a narrower concept than semantic relatedness.
As shown in Table 4, all corpora provide similar recall
values, with the highest values for Medline and lowest for
Wikipedia. In other words, the datasets contain informa-
tion on a similar percentage of inputs, so the differences in
performance of the methods set up with different datasets
will be related to the quality/precision of the information
coverage rather than to its range.
Table 5 shows the results of the statistical signifi-
cance testing for pairs of experimental runs. We show
which correlation differences from Table 3 are statis-
tically significant w.r.t. a 0.95 confidence interval. The
Fig. 3 Performance in the function of increased inter-annotator agreement - umnsrsRelate. The figure shows the correlation with human
judgement of ESA and tESA in the function of decreasing threshold for standard deviation, which is used to model the inter-annotator agreement,
calculated for the umnsrsRelate reference dataset
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 10 of 14
Fig. 4 Performance in the function of increased inter-annotator agreement - umnsrsSim. The figure shows the correlation with human judgement
of ESA and tESA in the function of decreasing threshold for standard deviation, which is used to model the inter-annotator agreement, calculated
for the umnsrsSim reference dataset
table lists CIs which indicate statistical significance of the
comparisons, i.e. only CIs that do not include zero are
presented.
A quick glance at Table 3 reveals that both methods,
i.e. tESA and ESA, surpass the existing methods on the
two larger datasets, with the improvement beingmore evi-
dent in the case of tESA and the umnsrsRelate dataset
(which is also evident in Table 5). This gain is less evi-
dent for the smaller datasets, nonetheless the ESAmethod
paired with the PMC OA corpus provides a result which
is better than the previously known best score. Addition-
ally, the mayo29 datasets contain a very small data sample
and mayo101 is only of moderate size, so it seems reason-
ably safe to assume that they are somewhat less reliable
or at least more prone to incidental variations (which also
shows in Table 5). Nonetheless, the scores achieved on
mayo29 benchmarks seem to be comparable with several
well established KB-based relatedness measures (refer to
the evaluation presented in [34]).
Table 4 Recall for different dataset-corpus pairs. Recall is
measured as a ratio of unique items (single input labels)
represented by non-zero vectors to the total number of unique
items in their respective datasets. As mayo29ph and mayo29c
contain the same set of item pairs, the recall is identical for both
datasets
Dataset Medline PMC Wiki
umnsrsRelate 0.985 0.977 0.95
umnsrsSim 0.989 0.981 0.963
mayo101 0.957 0.951 0.929
mayo29 0.982 0.982 0.982
Also, tESA and ESA are only outscored by the previous
method for a specific combination of runtime param-
eters for a specific dataset. They do however seem to
display more robustness, both in terms of parameter
and corpus variations, i.e. they outperform the original
method method presented in [28] on sub-optimal (con-
sensus) settings used in Table 3. Furthermore, data pre-
sented in Fig. 2 suggest that both ESA and tESA perform
consistently through a range forM values, so little corpus
specific optimization for M is necessary (for the samples
between 10K-40K, at 5K interval, range for neither of the
methods exceeded 0,005). Obviously the value of M is still
corpus dependent to some extent, i.e. it is best to avoid
cutting off the significant portions of the vectors. The
data presented in Fig. 2 suggests that setting the value of
M well above the average vector length works well, while
keeping the size of long-tailed vectors (which represent
very common tokens) under the limit. TheM value of 10K
was chosen for the main experiments, as it does not seem
to hinder the performance of any of the method-corpus
combinations.
Table 6 shows the mean number of non-zero vector
elements throughout the reference datasets for ESA and
tESA set-up with each of the corpora. Although tESA does
require more processing to obtain a vector representa-
tion of an input (the method does the same as ESA, and
then more, i.e. the computation of tESA vectors using
the C matrix), the data shows that one can reasonably
expect tESA vectors to have fewer non-zero values, which
is especially evident in the case of the optimal Medline-
based configuration. Additionally, tESA vectors are also
less dimensional, as the titles contain fewer unique tokens
(see Table 1) than the total number of documents in
each of the corpora considered in our evaluation. These
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 11 of 14
Table 5 Statistical tests (confidence intervals) for differences
between correlations reported in Table 3
tESA config Other method Dataset CI Comparison
Medline DS (Medline) mayo29ph (0.09; 0.59) +
Medline ESA (Medline) umnsrsRel (0.003; 0.09) +
Medline ESA (PMC) umnsrsRel (0.025; 0.099) +
Medline ESA (Wiki) umnsrsRel (0.097; 0.2) +
Medline DS (Medline) umnsrsRel (0.13; 0.25) +
Medline DS (PMC) umnsrsRel (0.026; 0.12) +
Medline DS (Wiki) umnsrsRel (0.15; 0.26) +
Medline tESA (PMC) umnsrsRel (0.02; 0.09) +
Medline tESA (Wiki) umnsrsRel (0.11; 0.22) +
Medline ESA (PMC) umnsrsSim (0.004; 0.08) +
Medline ESA (Wiki) umnsrsSim (0.09; 0.19) +
Medline DS (Medline) umnsrsSim (0.14; 0.26) +
Medline DS (Wiki) umnsrsSim (0.11; 0.24) +
Medline tESA (Wiki) umnsrsSim (0.1; 0.21) +
PMC DS (Medline) mayo29ph (0.1; 0.61) +
PMC ESA (Wiki) umnsrsRel (0.04; 0.15) +
PMC DS (Medline) umnsrsRel (0.07; 0.2) +
PMC DS (Wiki) umnsrsRel (0.096; 0.21) +
PMC tESA (Wiki) umnsrsRel (0.06; 0.16) +
PMC ESA (Wiki) umnsrsSim (0.056; 0.16) +
PMC DS (Medline) umnsrsSim (0.1; 0.24) +
PMC DS (Wiki) umnsrsSim (0.09; 0.2) +
PMC tESA (Wiki) umnsrsSim (0.07; 0.18) +
Wiki DS (Medline) mayo29c (0.04; 0.55) +
Wiki DS (Medline) mayo29ph (0.11; 0.62) +
Wiki DS (Wiki) mayo29ph (0.01; 0.41) +
Wiki ESA (Medline) umnsrsRel (-0.18; -0.07) -
Wiki ESA (PMC) umnsrsRel (-0.15; -0.05) -
Wiki DS (PMC) umnsrsRel (-0.16; -0.025) -
Wiki ESA (Medline) umnsrsSim (-0.19; -0.086) -
Wiki ESA (PMC) umnsrsSim (-0.16; -0.06) -
Wiki DS (PMC) umnsrsSim (-0.21; -0.07) -
The CIs were constructed for pairs of correlations involving at least one tESA setup.
The table provides all the information necessary to track the CI back to Table 3, i.e.
the corpus of the tESA method, the method (and corpus) to which the tESA results
are being compared and the reference dataset. We also provide the CI itself,
additionally indicating if the result is positive or negative
features account for an advantage of tESA over ESA, espe-
cially in scenarios where the costly part of the method
can be delegated to a one time pre-processing effort. In
other words, once the distributional representations have
been computed, tESA is faster than ESA with two out
of three corpora. Most importantly, it is more efficient
in handling the representations extracted from Medline,
Table 6 Average vector length
Medline PMC Wiki
tESA 3222,7 3547,4 535,8
ESA 4579,4 3391,9 751
The table shows an average of non-zero elements in tESA and ESA vectors,
calculated throughout reference datasets for each of the corpora
which is the largest of the corpora and also provides the
best-performance setting.
From the perspective of the corpus choice, it can
be argued that ESA-related methods rely on domain-
adequacy of the entire corpus (thus the slight drop in
performance for Wikipedia), but could also benefit from
a larger document collection (increase in performance
for Medline over PMC), all of which is consistent with
the conclusions drawn in [40]. On the other hand, the
method presented in [28] apparently depends more on the
quality of individual documents, i.e. PMCs full research
papers return better results than Wikipedia articles and
Wikipedia articles still give better results than abstracts in
the Medline collection. This can be explained by the fact
that the ESA-related methods, with high enough values of
M, rely on the distribution of words throughout the col-
lection. Whereas, the method presented in [28] relies on
the presence of a small sample of documents fromwhich a
decent representation of the input can be retrieved. Bear-
ing this in mind, one should note that the quality of each
method is closely related to a combination of its intended
use and available document collection.
The ESA methodology paired with the Wikipedia cor-
pus is essentially an implementation of the original ESA
with a cutoff, so it provides an important baseline for
other methods to be compared against. This baseline
score is surpassed by ESA combined with domain spe-
cific corpora (Medline/PMC) on all benchmarks with the
exception of mayo101, where the difference is statistically
insignificant. tESA provides significantly better results
than the original ESA baseline for the two larger datasets.
It also provides a better result for themayo101 dataset, but
the gain is statistically insignificant.
When comparing the performances of ESA and tESA,
tESA seems to provide better results (at least for the most
relevant benchmarks) when the methods use domain-
oriented collections. One possible explanation is that the
titles of scientific articles are simply more descriptive
than those of Wikipedia. At the same time, the Wikipedia
titles are usually short and contain discriminative tokens
(almost like identifiers), and those tokens are sometimes
accompanied by a broad categorical description (e.g.
Medicine) intended for human disambiguation, which in
the presented settings may increase noise. We believe that
fine tuning the extraction method for title representa-
tion could improve tESA even to the point of achieving
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 12 of 14
results more comparable with ESA with both methods
using Wikipedia as the document corpus. Nonetheless
using a document collection with more descriptive titles
seems to be a safer choice when it comes to improving
performance.
The results obtained both with tESA and ESA (espe-
cially with the Medline corpus) seem ecouraging given
the results presented recently in [31]. Both tESA and ESA
seem to achieve better results when evaluated against the
two largest benchmarks than all the methods discussed in
the study, while performing at least comparably to the best
ones on the smaller reference datasets, although a deeper
statistical analysis would be needed to provide more per-
spective. It is worth noting however, that both tESA and
ESA operate on much larger structures (vectors) than
some of the methods presented in the cited evaluation
(e.g. word2vec-trained word embedding), which means
that ESA-based approaches might be less appropriate for
large scale tasks.
The approach used in tESA is similar to that used in
the NESA methodology in the sense that it is aimed at
leveraging the inter-document similarity. In NESA this
is achieved by the explicit usage of a similarity matrix
for all the documents, while in tESA it is done through
the creation of the representation vectors as described in
the Methods section. In other words, NESA and XESA
contemplate leveraging the actual document-document
similarity, while in tESAwe assume that sets of documents
might share common vocabulary features. The advantage
of tESA is that it can be directly applied to larger corpora,
as it needs a representation vector per word or document
(depending on the actual implementation) and the tar-
get vector space is relatively small, while NESA requires
storing a dense similarity matrix of an N × N size. In
[22], the use of a truncated matrix is contemplated, how-
ever our initial experiments with the truncated cosine
similarity matrix have shown decreased performance and
increased processing and preprocessing times when com-
pared to tESA and ESA, which might point to an issue
with the adaptability of the approach to domain-specific
corpora and the specificity of the concepts within the eval-
uation datasets (especially when we compare it with the
length and coverage of biomedical journal papers). As the
task of adapting the similarity based ESA extensions is an
independent research problem (which might be or not be
feasible), it has been left to be considered in our future
work, as outlined below.
Obviously, the tESA model is limited in terms of
representing the inter-document similarity (as it does
not reflect the similarity of actual document-document
pairs), it does however seem to benefit from the intrin-
sic characteristics of the titles of the scientific papers.
Nonetheless, our impression is that relatedness methods
could be further enhanced by experimenting with the
mapping and the target representation space. The goal
of further work should therefore be to provide a bet-
ter similarity modelling within the target representation
space. We believe that this could be achieved by: (A) an
intelligent approach towards extracting more informative
representations from full texts/abstracts, (B) using NESA-
like distribution based representations obtained for titles.
With respect to (A) it has to be noted that prelimi-
nary experiments with the parameters of the approach
presented in [28] (increasing the query size, decreasing
the cutoff threshold) did not provide satisfactory results,
probably due to the amount of noise introduced in the
representations, therefore research thread (A) will center
on finding a representation extraction method that maxi-
mizes information content, while reducing noise. The line
of research related to (B) will focus on providing represen-
tations that do not lead to dimensionality problems and
can be adapted to the biomedical domain, and comparing
their performance with the NESA-like approaches.
Conclusions
In this paper we have presented a new, robust method
for computing lexical semantic relatedness for biomedi-
cal use - tESA. The approach uses a vector space of titles
of scientific articles combined with ESA principles. We
have also provided a side-by-side comparison of tESA and
ESA, the latter method having not been evaluated as thor-
oughly in similar experimental settings. Both methods
were reviewed with direct benchmarks, i.e. their abil-
ity to approximate human judgement was assessed. The
algorithms outperfomed other state-of-the-art methods
in the largest-to-date datasets used to evaluate biomedical
semantic relatedness and similarity, with the original tESA
method gaining a slight advantage.
Also, we have demonstrated that tESA uses smaller
and more dense vectors than ESA, so it might be a bet-
ter fit in cases where vector computation cost (which is
higher in tESA) is less important than the cost of online
computations.
The results obtained with both tESA and ESA seem to
be on par with the other state-of-the-art methods, a recent
study [31] being a good point of reference.
The results obtained in our evaluation seem to indicate
that the performance of the method can be optimized
by choosing a correct background corpus, i.e. a domain
oriented corpus of documents will provide a quality
improvement in assessing domain-oriented relatedness.
The baseline score of the original ESA has been sur-
passed by bothmethods on the two largest (and thus more
statistically significant) reference datasets.
We believe that the approach and detailed evaluation
that we have presentedmay be a good fit wherever seman-
tic relatedness approximation is a necessity, especially
within subdomains that lack a detailed KB domain model,
Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 13 of 14
but are well covered in the scientific literature. Guidelines
to tuning and applicability of the discussed methods have
also been presented here. Finally, two interesting lines for
future research have been outlined, both of which we hope
to pursue in the near future.
Endnote
1 The method actually uses either abstracts or full arti-
cles, depending on the features of the actual corpus, as
explained further on.
Abbreviations
ESA: Explicit semantic analysis; KB: Knowledge base; NESA: Non-orthogonal
explicit semantic analysis; PMC: PubMed Central, also refers to PubMed Central
Open Access document corpus; tESA: Title vector explicit semantic analysis; Tf
- idf: Term frequency inverse document frequency
Acknowledgements
Not applicable.
Funding
Work presented in this paper was partially supported by grants
TIN2014-58304-R (Ministerio de Ciencia e Innovación), P11-TIC-7529 and
P12-TIC-1519 (Plan Andaluz de Investigación, Desarrollo e Innovación) and EU
FP7-KBBE-289126 (the EU 7th Framework Programme, BIOLEDGE).
Publication costs for this article were funded by grants TIN2014-58304-R
(Ministerio de Ciencia e Innovación) and P11-TIC-7529 and P12-TIC-1519 (Plan
Andaluz de Investigación, Desarrollo e Innovación).
Availability of data andmaterials
The reference datasets used in this study are available at: http://rxinformatics.
umn.edu/SemanticRelatednessResources.html. The Medline corpus is
available (on request) at: http://www.nlm.nih.gov/bsd/pmresources.html. The
Wikipedia data is available at: https://meta.wikimedia.org/wiki/Data_dump_
torrents#enwiki. The PMC OA corpus is available at: http://www.ncbi.nlm.nih.
gov/pmc/tools/ftp/. The snapshots of the datasets used in this study, as well
as the data supporting our findings are available from the corresponding
author on reasonable request.
Authors contributions
Both authors contributed to the design of the method and experiments. MR
was responsible for the implementation, performing the experiments and
writing of the manuscript. Both authors have read and approved the final
manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Received: 26 February 2016 Accepted: 13 November 2016
RESEARCH Open Access
Representing vision and blindness
Patrick L. Ray1, Alexander P. Cox1, Mark Jensen1, Travis Allen1, William Duncan2 and Alexander D. Diehl2,3*
Abstract
Background: There have been relatively few attempts to represent vision or blindness ontologically. This is
unsurprising as the related phenomena of sight and blindness are difficult to represent ontologically for a variety of
reasons. Blindness has escaped ontological capture at least in part because: blindness or the employment of the
term blindness seems to vary from context to context, blindness can present in a myriad of types and degrees,
and there is no precedent for representing complex phenomena such as blindness.
Methods: We explore current attempts to represent vision or blindness, and show how these attempts fail at
representing subtypes of blindness (viz., color blindness, flash blindness, and inattentional blindness). We examine
the results found through a review of current attempts and identify where they have failed.
Results: By analyzing our test cases of different types of blindness along with the strengths and weaknesses of
previous attempts, we have identified the general features of blindness and vision. We propose an ontological
solution to represent vision and blindness, which capitalizes on resources afforded to one who utilizes the Basic
Formal Ontology as an upper-level ontology.
Conclusions: The solution we propose here involves specifying the trigger conditions of a disposition as well as
the processes that realize that disposition. Once these are specified we can characterize vision as a function that is
realized by certain (in this case) biological processes under a range of triggering conditions. When the range of
conditions under which the processes can be realized are reduced beyond a certain threshold, we are able to say
that blindness is present. We characterize vision as a function that is realized as a seeing process and blindness as a
reduction in the conditions under which the sight function is realized. This solution is desirable because it leverages
current features of a major upper-level ontology, accurately captures the phenomenon of blindness, and can be
implemented in many domain-specific ontologies.
Background
The human visual system is a complex, organized collec-
tion of specialized cells and structures that function
together to encode and represent one type of stimulus
(light). Light enters the system through the eye, a com-
plex organ that includes structural, optical, and muscular
components, which operate together to ensure an intact
focused image reaches the retina. In the retina, two types
of photoreceptor cells encode visual information in the
form of light by converting it to a neuronal signal. A
network of synapses connect the photoreceptor cells via
bipolar neurons, ganglion cells, horizontal cells, and
amacrine cells to the optic nerve, which connects to the
brain proper. Many of the fibers of the optic tract
innervate at the lateral geniculate nucleus in the poster-
ior thalamus while others project to the superior collicu-
lus. The lateral geniculate nucleus sends projection cell
axons to the visual cortex. The superior colliculus, in
addition to signals from the retina, receives indirect
signals from the visual cortex. The visual cortex is
composed of layers that are responsible for different
tasks. Most of the input from the lateral geniculate
nucleus occurs in layer IV of the primary visual cortex.
Cells in layer IV are connected in patterns that work
to integrate visual data from the lateral geniculate
nucleus into a cognitive representation of objects in
the visual field [1].
Because the human visual system involves many
anatomical structures and complex functions, it can fail
in a multitude of ways. Such failures can occur in any of
the various structures that make up the system, and
* Correspondence: addiehl@buffalo.edu
2New York State Center of Excellence in Bioinformatics and Life Sciences,
University at Buffalo, Buffalo, NY, USA
3Department of Neurology, Jacobs School of Medicine and Biomedical
Sciences, University at Buffalo, Buffalo, NY, USA
Full list of author information is available at the end of the article
© 2016 Ray et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Ray et al. Journal of Biomedical Semantics  (2016) 7:15 
DOI 10.1186/s13326-016-0058-0
often result in visual impairment or blindness. Blindness
has been studied from multiple perspectives, whether as
the result of accident or disease. Most of the associated
anatomical entities are well described, and many of the
affected processes involved in visual perception are well
understood; yet the representation of the fundamental
nature of blindness from an ontological perspective
remains incomplete.
In its most basic form, blindness is the impairment of
visual function below a certain threshold. Where this
threshold is set varies from context to context, and
standards for blindness vary across international and
institutional borders. The World Health Organization
characterizes blindness as visual acuity of less than 20/
500 or a visual field of less than 10 degrees. In the
United Kingdom, the Certificate of Visual Impairment
characterizes blindness as visual acuity of less than 20/
400. In the United States, the American Medical Associ-
ation characterizes blindness as visual acuity of less than
20/200 or a visual field of less than 20 degrees. There
have also been recent calls by the International Council
of Ophthalmology to define blindness and visual impair-
ment according to their own standards, at least part of
which involve visual substitution skills employed by
persons [2]. This expansion to include visual substitution
skills in the account of blindness is important because it
demonstrates that visual acuity only represents one
dimension of blindness. Other types of visual impairments
fall outside the scope of visual acuitysuch as the ability
or inability to differentiate colors.
The relationships between blindness, visual impair-
ment, and visual acuity can be difficult to describe with-
out resort to fiat definitions, such as those described
above, which offer little insight into the underlying
nature of the phenomena. We start with the premise
that blindness is a specific type of visual impairment and
that blindness results from a failure in the visual process.
Visual acuity is the specific ability an individual has with
respect to vision (how well or poorly an individual is
able to visually process stimulior perhaps the sharp-
ness of the representations of external stimuli by an
individual). Many of the problems associated with repre-
senting blindness result from a conflation of closely
associated concepts about the vision process and its
absence. By identifying the entities in reality, the compo-
nents of the visual system and their associated functions
and processes, we may therefore sort out what blindness
is, and what blindness results from.
Current efforts to define terms related to blindness,
while systematic, are rather problematic. The National
Health Interview Service (NHIS) offers a definition of
'an individual who suffers from vision loss' as those who
report they have trouble seeing [3]. The problem with
such a definition is that self-assessment is a reliably poor
criterion for sensory perception. People are often
mistaken about their abilities and skills regarding their
perception. An older NHIS survey, the National Health
Interview Survey on Disability (NHIS-D) implemented a
sharp cut-off for those who are legally blind, a visual
acuity of 20/200 or less. The American Community
Survey (ACS) provides a definition of those with
'difficulty seeing' as individuals who self-report either
blindness or serious difficulty seeing even when wearing
eyeglasses [3]. Along with the aforementioned problem
with self-assessment we now have an additional problem
of qualification with corrective lenses. (Notice that this
definition is too vague in that even a person with ex-
ceptional vision could have serious difficulty seeing
when wearing glasses with a strong prescription; we
surely would not want to label such a person as
having difficulty seeing).
Governmental organizations implement their own
standards for blindness. The Department of Labor's
Bureau of Labor Statistics employs a Current Population
Survey as a means for identifying those who have serious
vision loss or blindness. The standard, once again, is one
of self-assessment. People with vision loss were identi-
fied by the CPS if they reported that they or someone in
their household is blind or has serious difficulty seeing
when wearing eyeglasses [3]. One of the problems with
such wide-ranging definitions or criteria of blindness is
that there seems to be no one standard. As a result it is
very difficult to determine precisely how many people
suffer from blindness or vision loss.
From a diagnostic perspective, the National Eye Insti-
tute (NEI) defines blindness as the best-corrected
visual acuity of 6/60 or worse (=20/200) in the better
seeing eye [4]. The American Optometric Association
follows the protocol of the World Health Organization
in defining total blindness as no light perception and
near total blindness as a best-corrected visual acuity
of less than 20/1000 [5]. The standard United States
legal definition of blindness follows the criterion
endorsed by the NEI, as a best-corrected visual acuity
of 20/200 or worse.
The problems that arise for a definition of blindness
date to at least the beginning of the 20th century. N.
Bishop Harman writes of a proposed definition of blind-
ness, By no possible phrasing can we present a suffi-
ciently simple definition that will embrace the possible
variations in these several factors that make up sight,
and state that such and such a variation shall be accounted
blindness [6]. Other, more recent assessments of the feasi-
bility of defining blindness have been met with similar
reluctance but somewhat more optimism. The fact is,
however, that the briefest analysis of any dictionary
definition will reveal the word blind to be neither
straightforward nor respectable [7].
Ray et al. Journal of Biomedical Semantics  (2016) 7:15 Page 2 of 12
The aforementioned problems notwithstanding, there
are still difficulties surrounding blindness as a subject of
investigation. If we understand blindness as the impair-
ment of visual functioning beyond a certain threshold
and accept that there are many different ways in which
visual functioning can be impaired, then there are many
different ways in which one may be blind. Furthermore,
there are many different mechanisms that will cause
blindness. Moreover, if we focus on the different types of
visual impairment with respect to the features of the
world that are not effectively represented in the visual
process, blindness can be used to describe these differ-
ent types of failures of representation along the lines of
features of the world (e.g. color blindness). This compli-
cates our task because it shows that not only are there
many different ways one can become blind, but different
ways in which one may be blind. The nature of blindness
is more complicated than it appears at first glance. Not
only are there complications with respect to contextual-
ized thresholds for determining blindness, but also diffi-
culty in examining the natures of the differing types of
visual impairment that lead to blindness.
There are two obvious and immediate challenges for
representing and defining visual impairment and blind-
ness. First, different groups use different standards of
measurement. Second, different standards of classifica-
tion can be used in conjunction with a single standard
of measurement. These challenges make it difficult to
share, reuse, and compare data on blindness and vision
related disorders. In addition, there are more complex
problems that arise in representing blindness in formal
ontology. This paper explores the difficulties that arise
in representing blindness ontologically and proposes a
novel solution to representing visual impairment and
blindness in a formal manner. While we believe that
the definition of blindness (and its related terms) will
not come easily, our goal in this paper is to provide an
ontological representation of blindness through a
careful examination of the nature of blindness and
related phenomena.
Current ontological representations of vision and
blindness
The Gene Ontology defines visual perception as The
series of events required for an organism to receive a
visual stimulus, convert it to a molecular signal, and
recognize and characterize the signal. Visual stimuli are
detected in the form of photons and are processed to
form an image [8]. Thus, visual perception, or seeing, is
a relational process between an agent and the stimulus
itself. The process of seeing is representational insofar as
the agent represents the stimulus in some manner (we leave
the source of this stimulus and the nature of this represen-
tation to further examination). We may characterize the
diminishment or cessation of this relational process as
blindness or a loss of vision.
Currently there are relatively few ontologies that
contain the term blindness and fewer still that offer a
well-formed definition of blindness. There are no
realism-based ontologies that represent the phenomena
surrounding blindness in a manner that reflects its
complexity. Results of previous attempts to characterize
blindness using current ontologies are listed in Table 1.
Of the attempts to represent blindness in biomedical
ontologies, it is a popular strategy to classify blindness
as a phenotype [9].
Biomedical ontologies that seek to represent pheno-
types typically rely on Entity-Quality (EQ) methodology
[10]. The EQ methodology leverages the existing struc-
ture of ontologies to generate a schema where the sub-
ject of the phenotype (the entity) is described by the
phenotype that inheres in that entity (the quality). This
approach is advantageous in that it provides a computa-
tional resource for researchers working with phenotypic
qualities. This allows researchers to leverage reasoners
where they were not leveraged before. In addition, the
EQ method allows that [phenotypes be] recorded
using multiple ontologies in a highly expressive and
finely detailed manner while maintaining correct logic
and computability [10]. This is a very powerful and
innovative method used within the biomedical ontol-
ogy community.
The EQ methodology works well for phenotypes and
qualities that inhere in entities. For example, if we
wanted to say, some human being has red eyes, we could
accomplish this via EQ methodology by leveraging terms
from the Uber Anatomy Ontology (UBERON) and the
Phenotype and Trait Ontology (PATO). We would use the
terms red from PATO and eye from UBERON and apply
the EQ methodology to yield EQ =UBERON:eye +
PATO:red [10].
While this solution has its advantages, its shortcomings
outweigh its advantages for the subject of blindness. The
main problem with applying the EQ method to blindness
is that it is unclear whether blindness is a phenotype. The
Ontology for General Medical Science (OGMS) provides
the following definition for phenotype:
A (combination of) quality(ies) of an organism
determined by the interaction of its genetic make-up
and environment that differentiates specific instances of
a species from other instances of the same species [11].
If blindness is a phenotype and phenotypes are qualities
(or combinations thereof), then blindness is a quality (or
combination thereof). If blindness is a quality (or combin-
ation thereof), then blindness is a specifically dependent
continuant that needs no further process in order to be
Ray et al. Journal of Biomedical Semantics  (2016) 7:15 Page 3 of 12
realized. But this consequence is incorrectblindness is
the inability of the vision function to be realized. Blindness
is not a quality of a visual system or of an organism that
the visual system is part of. Blindness is a failure in the
realization of the visual function.
Another option to consider is an alternative definition
of phenotype. One standard definition for phenotype
is that a phenotype is the outcome of a given genotype
in a particular environment [12]. If it is the case that
blindness is a phenotype and that a phenotype is the
outcome of a given genotype in a particular environ-
ment, then blindness is the outcome of a given genotype
in a particular environment. Of course this poses no
problems when we are discussing instances of blindness
with a genetic basis. However, some instances of
blindness do not have a genetic basis, but rather are
the result of acute trauma or some other non-genetic
phenomenon. Since there are cases of blindness that
have non-genetic bases, it is clear that blindness is not
a phenotype in all cases according to this definition of
phenotype. Since blindness is not a phenotype in all
cases (if any), we should look for an alternative
account of blindness.
One of the general problems with the treatment of
blindness as a phenotype (quality) is that it is inconsist-
ent with the proposed definitions provided in these
accounts. If blindness is a lack of sight (or vision) and
sight (or vision) is a realizable entity, then blindness
should be the lack of a process or the lack of some
realizable entity. If blindness is a phenotype and there-
fore a type of quality, then there is a quality that is a lack
of a realizable entity. But this is impossible. There
cannot be an entity that is both a lack of a realizable
entity and a quality at the same time. The lesson here is
not that blindness does not exist but rather that
blindness is something other than a phenotype.
If blindness is not a phenotype, what other ontological
solutions are there? One of the most likely candidate
solutions involves using the Human Disease Ontology
(DO). The Human Disease Ontology currently does not
provide a definition of 'blindness', but we could propose
a plausible candidate on their behalf following their
characterization of color blindness as an inability or
decreased ability to detect light stimulus. 'Color blind-
ness' is defined in DO as: a blindness that is character-
ized by the inability or decreased ability to see color, or
perceive color differences, under normal lighting condi-
tions [13]. (Table 1) Moving from this definition of a
specific type of blindness to a general definition of blind-
ness should produce the result that blindness is the
inability or decreased ability to see or perceive, under
normal lighting conditions.
While this is a generally attractive view, it does not
stand up to careful examination. In the first place, DO
Table 1 Previous attempts to define and represent blindness
Ontology Term Definition Parent Class
Gene Ontology (GO) Visual perception The series of events required for an organism to receive
a visual stimulus, convert it to a molecular signal, and
recognize and characterize the signal. Visual stimuli are
detected in the form of photons and are processed
to form an image.
Sensory perception
of light stimulus
GO Detection of visible light The series of events in which a visible light stimulus is
received by a cell and converted into a molecular signal.
A visible light stimulus is electromagnetic radiation that
can be perceived visually by an organism; for organisms
lacking a visual system, this can be defined as light with
a wavelength within the range 380 to 780 nm.
Detection of
light stimulus
GO Detection of light stimulus
involved in visual perception
The series of events involved in visual perception in
which a light stimulus is received and converted into
a molecular signal.
Visual perception
GO Determination of
sensory modality
The determination of the type or quality of a sensation.
Sensory modalities include touch, thermal sensation,
visual sensation, auditory sensation and pain.
Sensory processing
Mammalian Phenotype (MP) Blindness Loss of the sense of sight. Abnormal vision
MP Abnormal vision Inability or decreased ability to see. Abnormal eye physiology
MP Decreased visual acuity Loss of visual acuity or ability to distinguish small details Abnormal visual acuity
Human Disease Ontology (DOID) Blindness N/A Retinal disease
DOID Color blindness A blindness that is characterized by the inability
or decreased ability to see color, or perceive color
differences, under normal lighting conditions.
Blindness
Human Phenotype (HP) Blindness Blindness is the condition of lacking visual perception
due to physiological or neurological factors.
Visual Impairment
Ray et al. Journal of Biomedical Semantics  (2016) 7:15 Page 4 of 12
categorizes blindness as a disease. Blindness is not a dis-
ease though it may result from a particular disease.
Moreover, it is not a type of retinal disease as DO
currently represents it. There are many diseases that
may result in blindness and many diseases can compli-
cate the sightedness of individuals, but blindness is not
itself a disease. Furthermore, blindness need not result
from a disease. It may instead be caused by a single
event, as is the case with acute trauma or flash blind-
ness. Blindness also need not be limited to problems in
the retina. Cortical blindness is a type of blindness that
does not involve any malfunction with the retina. As
detailed in the last section, certain types of blindness are
not limited to just one mechanism of realization, or to
realization in only one location.
These problems notwithstanding, the more pressing
concern with this solution is that there does not seem to
be any indication of what an inability or decreased ability
would be. If abilities are dispositions or functions, then
they are realizable entities. The concern is plainaccord-
ing to BFO a realizable entity cannot lack. Realizable
entities cannot present in degrees, as their existence is an
all-or-nothing affair. If blindness is an inability to detect
light, then all cases of blindness will be a complete inabil-
ity to detect light stimulus, which fails to capture the cases
of blindness that are not the complete inability to detect
light stimulus. If blindness is a decreased ability to detect
light, then it cannot be represented as a decreased func-
tion or disposition in BFO. But, if sight is a function and
blindness is the lack of sight, then an account of blindness
as an inability cannot be given either. Hence, we believe
that this type of account is confused.
Moving away from an account based on the DO repre-
sentation, another route for capturing blindness is to main-
tain that blindness is a disorder, where 'disorder' is defined
by OGMS as [a] material entity that is clinically abnormal
and part of an extended organism [11]. The problem with
this approach is that it is unclear that blindness is a material
entity. If one thinks that blindness is the absence of the
sight function, then it does not seem that blindness is a ma-
terial entity (absences of functions are not material entities).
Further, one cannot point to a material entity and identify it
as blindness because blindness is not spatially extended and
spatial extension is a hallmark of material entities. For these
reasons, blindness cannot be a disorder per OGMS.
The realist approach to blindness
There are several lessons to be learned from this concise
review of the representations of blindness in biomedical
ontologies. First, it is rather difficult to characterize an
entity via a lack or absence, which seems to be the case
with blindness (the lack of sight) [14]. Consider the
paradigm case of an ontological absence involving
material entities: a hole. There does not seem to be
anything to which one can attribute characteristics. In
the Basic Formal Ontology (BFO), holes are continuant
entities susceptible to characteristic ascription like ma-
terial entities, but are essentially non-material. This indi-
cates at least a prima facie problem with characterizing
entities via a lack. BFO is an upper-level ontology that is
realist, fallibilist, perspectivalist, and adequatist [15]. The
core of BFO lies in its distinction between
continuants and occurrents, which tracks the differ-
ence between the two different types of fundamental
entities in the world. BFO is currently used as the
upper-level ontology for many biomedical ontologies
and is the backbone of the Open Biological and
Biomedical Ontologies collaborative [16].
A second problem arises when we consider attempts
to capture the existence conditions of a hole. If a hole is
defined by a lack (where the lack is both necessary and
sufficient), then there are all sorts of things that would
qualify as a hole. For example, if one were to characterize
a hole as a lack of matter surrounded by matter, then the
interior of a room would count as a hole, as would the
inside of a bag. A strategy has been implemented in
anatomy for representing lacking a part; however, it is
contentious whether it will translate well for things that
are not material entities, such as processes or functions 1
[17]. The reason that such a strategy will not work well
for non-material entities is that the lacks_part relation
does not apply to functions because functions do not have
parts. 2 Processes, on the other hand, do have parts but
the parts are temporal parts, not material entities (pro-
cesses can include material entities as participants, but
these are not themselves parts of processes).
Blindness does not seem to yield a precise definition
or even clearly differentiated conditions under which it
is present or absent, apart from the fiat standards
discussed previously. Blindness often presents gradually
which is commonly due to degeneration of the eye or
apparatuses associated with vision. As a result, many
cases of blindness are progressive and it is exceedingly
difficult to determine at which point blindness has come
into existence. In addition to these complications, there
is controversy over the threshold for blindness. Hence, it
is common for publications regarding blindness to
specify which definition of blindness they employ [18].
Even given these complications regarding blindness,
we contend that it is useful to give a univocal account of
the phenomenon for purposes of ontological deve-
lopment. Such an account should capture all or a vast
majority of the cases of blindness and the various
classifications of blindness found in the literature.
Thus, the account should remain general and flexible
enough to capture a wide range of characterizations yet
it should also be rigid enough to remain informative
and insightful.
Ray et al. Journal of Biomedical Semantics  (2016) 7:15 Page 5 of 12
There are many types of blindness. For example, there
is color blindness and change blindness. Blindness can
also be defined relative to a context. An individual might
be legally blind but still be able to detect some light
stimulus. Similarly, one might be visually impaired to
the extent that they are prohibited from flying a jet
aircraft but not from driving. Hence, we might say that
someone is blind according to [x] where [x] is some
standard of evaluation for sightedness. In this sense,
blindness comes in degrees. The extent to which some-
one has a lack of vision will be graded. If we think of
seeing as a relational process between an agent who is
representing and the thing represented and assess the
accuracy of such representations on a scale of 1 to 0
(complete representational veracity to no representa-
tional veracity), the ability of the person to see will be
somewhere on the continuum from 0 to 1the closer to
0 ones representation of the stimulus is, the more blind
that individual is.
Lending to the confusion surrounding the status of
blindness (and vision) is the method used for assessing
visual acuity. Typically, visual acuity is expressed as a re-
lationship between two valuesthe distance a subject
stands from an optical chart and the distance at which a
normal subject would stand from the chart to discern
the same visual detail. Putting aside the problems
associated with this particular type of visual acuity
assessment, we have discussed above how this can
lead to confusion regarding what conditions are indi-
cative of blindness [18].
Given the above considerations, one might conclude
that there is not a single coherent ontological category
that corresponds to what blindness is as an entity.
Instead, blindness could be an amalgam of loosely related
entities or something that is not ontologically well-
formed. We find this conclusion unsatisfactory. It is useful
for clinicians and researchers to have a coherent theory of
blindness that encompasses the range of conditions
commonly understood to be forms of blindness. We
simultaneously realize that blindness seems to be charac-
terized as relative or context-sensitive (the term itself
might be context-sensitive or the phenomenon might be
context-sensitive or both). We favor the view that the
term blindness denotes a single phenomenon reflecting
severe visual impairment relative to a particular context of
evaluation. Thus, blindness denotes an ontologically well-
formed category.
According to BFO, a function is a type of disposition.
A disposition is a realizable entity whose realization
occurs in virtue of the bearers physical constitution and
because the bearer is in some special physical circum-
stances. A function is a disposition that exists as the
product of natural selection or intentional design on the
part of the agent. Because functions are dispositions they
are realizable entities. They are realized as processes that
are sometimes called functionings. Functions are regarded
as non-accidental in BFO meaning that they come into
being because of natural selection or intentional design
[19]. All of the functions a given entity possesses are
intimately tied to the type of entity under examin-
ation, whether the entity is biological or artifactual.
Functions are internally-grounded realizable entities.
An internally-grounded realizable entity is a realizable
entity that is a reflection of the (in-built or acquired) phys-
ical make-up of the material entity in which it inheres
[19]. Changing the physical structure of an entity that
bears a given function might alter the realization of the
function in question.
A reason to believe that sight is a disposition is that it
is realized by processes grounded in a material entity.
This is a hallmark of a disposition (of which functions
are a special type) as described above. We have another
reason to think that sight is a disposition: the fact that
if sight ceases to exist, then the bearer is physically
changed. Although the entities still have the dispos-
ition to see, they are blind because they can no longer
realize that disposition due to some change in their
physical constitution.
Furthermore, we believe that sight is a function of a
visual system (or at least of visual systems of entities
with higher-order cognitive functions). One reason is
that development of the ability to see is the result of an
evolutionary process. The physical makeup of particular
organisms have changed and adapted over time to select
for the existence of the sight function. For non-biological
entities possessing sight, if any, the sight that they possess
is not accidental, but rather a product of intentional
design on the part of the creator. For these reasons,
we contend that sight is a function.
It is important to note here that it is not the case that
for all instances of structural change in the bearer of a
function that the function ceases to be realizable. In
other words, it is not the case that every physical change
in a bearer alters the function in question. It is also the
case that some bearers of functions can undergo changes
that render them unable to realize a function while the
function persists in its bearer. For example, one of the
functions of the human heart is to pump blood. Physical
changes to the heart may cause the heart to be unable to
perform its functiona large tear in a ventricle wall
might be such a physical change. Other physical changes
might not cause the loss of the ability to perform a func-
tiona slightly enlarged section of muscle might be
such a physical change. But in all cases where a function
ceases to exist or comes into being there must be a
corresponding physical change. To use an artifactual
example, a radiator has the function to disperse heat.
Certain physical changes to a radiator will render the
Ray et al. Journal of Biomedical Semantics  (2016) 7:15 Page 6 of 12
radiator unable to realize its function, such as the
presence of corrosion on the cooling fins or a cracked
regulator valve. But in this case we would not say that
the radiator has lost its function, rather we would say
that its function cannot be realized. If there were a suffi-
ciently significant physical modification of the right kind
to an entity, only then would we contend that the
function ceases to exist. This is an important point
that will play a significant role later in our discussion
of blindness.
We can characterize the specific type of function (sight)
by identifying and describing its defining features.
Employing such a strategy, we provide an account of sight
as the function to receive photons and interpret them as
visual information. Similarly, we can characterize seeing as
the process by which photons contacting the retina are
coded into an action potential and interpreted as visual
information. Having given an account of vision as the
realization of the sight function, it is then natural to iden-
tify the processes by which the sight function is realized as
vision. There are many other functions involved in the
realization of the sight function as described above. We
are omitting detailed discussion of these functions for ease
of exposition.
If we are correct in our claim that sight is a function,
and that blindness is related to the realization of this
function, then it seems that we can develop our under-
standing of the phenomena of blindness through the
characterization of sight as a function. Because blind-
ness is a wide-ranging phenomenon (i.e., there are
many types of blindness), it would behoove us to
explore the range of blindness types in an attempt to
fully capture blindness.
Color blindness
Color perception is possible for humans because we
(typically) have three different types of cone cell photo-
receptors that detect photons within corresponding
ranges of wavelength. Photons within the range of a
particular type of cone cell photoreceptor cause an
action potential in the corresponding cone cell and the
action potential is sent along eventually to the primary
visual cortex. Color is coded by the differentiation of the
firing of these types of cone photoreceptor cells.
Color blindness is a condition wherein an individual has
an inability to distinguish between two or more colors. In
some cases photons of differing color spectrum wave-
length are represented or interpreted as the same when
they are distinct. In other cases, an individual cannot
report a difference between two or more wavelengths of
photons [20]. The inability to distinguish between two or
more types of light is not limited to just one cone type
[21]. Complicating this picture somewhat is that there are
many mechanisms identified as causes of color blindness
and that these mechanisms are not localized to one
anatomical region. Some color blindness is due to an indi-
vidual lacking cone cells or a certain type of cone cell.
Other times the cause is cortical [22]. For example, the
complete lack of the ability to distinguish colors, achroma-
topsia, can have its underlying causal mechanism
located within the retina (congenital achromatopsia)
or the visual cortex (cerebral achromatopsia). Thus
color blindness is similar to other types of blindness
in that its causes and the mechanisms associated with
it are diverse and complex.
Flash blindness
Flash blindness is a type of blindness that results from
sudden exposure to bright lightsuch as the bright flash
that results from the flash bulbs on older cameras or the
detonation of an atomic weapon. The sudden influx of
light oversaturates the photopigments of the retina and
the individual becomes unable to convert photons to a
neural signal [23]. Flash blindness is commonly tempor-
ary, where the subject regains their full ability to see
within a few minutes. There are some extreme cases,
however, where flash blindness results in permanent
vision loss [24].
Inattentional blindness
Inattentional blindness can be described by an individ-
uals lack of perception of a salient object or feature in
their visual field due to lack of attention [25]. Investiga-
tion into the mechanism for this phenomenon has
revealed both a nonconscious and conscious component.
The nonconscious mechanism includes contour inte-
gration in the early visual cortex while conscious
integration includes involvement from the lateral
occipital cortex and is mediated more strongly by
focused attention [26].
Proposed solution
Drawing on the lessons from the previous sections, we
propose a solution to the problem that blindness poses
for ontology development. Because sight is a function
and blindness is seemingly the non-realization of that
function, we set forth an account of blindness where
blindness is a reduction of the trigger conditions under
which the sight function is realized. To understand this,
we need to understand how dispositions are related to
triggering conditions. A disposition is a causal property
that is linked to a realization, i.e., to a specific behavior
which the individual that bears the disposition will show
under certain circumstances or as a response to a certain
stimulus (trigger) [27].
This solution is able to deal with the cases outlined
above. Color blindness is a reduction in the (color)
trigger conditions under which a vision function is
Ray et al. Journal of Biomedical Semantics  (2016) 7:15 Page 7 of 12
realized. Although different types of color blindness will
involve different types of reduction of conditions, they
are unified as a single phenomenon by the fact that they
all involve the reduction of the number of particular
light wavelengths that result in differentiated visual
representation. For flash blindness, we say that there is a
temporary (or possibly permanent) reduction in the
conditions under which the vision function is realized,
whatever the mechanism realizing the function of
sightedness may be. It may be that cases of temporary
flash blindness are impairments of functions in the
following manner: they are grounded in a physical
change that results in a malfunction but not a (per-
manent) loss of a function. Because the function is
realized by a rather complicated functioning in both
cases, the type of blindness can range over different
types of failure in functioning so long as the reduction
of conditions is similar.
The case of inattentional blindness is interesting and
problematic. It is unclear whether the inattentional
blindness is a type of blindness or the use of the term
blindness is metaphorical. If it is not the case that inat-
tentional blindness is a type of blindness, then we need
not worry about its formalization. If, however, intatten-
tional blindness is a type of blindness, then we propose
the following formalization: inattentional blindness is an
instance of a non-realized disposition (the vision func-
tion is non-realized). The disposition will not be realized
in one of two ways: nonconsciously by some mechanism
of contour integration in the early visual cortex or
consciously by failure of function by the lateral occipital
cortex. This is why inattentional blindness is much like
more traditional types of blindness even though its
mechanism and presentation differs from these trad-
itional types of blindness. One could also classify types
of blindness by the types of failure in sight functionings,
if one so chose.
Having provided an informal account of what consti-
tutes blindness we now provide a formalization of the
solution. Because our account relies heavily on triggers,
dispositions, and processes realizing those dispositions,
any formalization will have to invoke all of these at a
minimum. We can characterize triggers, dispositions, and
processes realizing dispositions in the following man-
ner. Dispositions exist outside of their realizationsthe
disposition of fragility inheres in a vase whether or not
it there is a process of shattering to realize that dis-
position. Some dispositions exist without any realiza-
tionsthat is to say, there are some dispositions that
will never be realized because the triggering conditions
are not met. Realizations are processual entities and
have as participants the material entities. Dispositions
and qualities inhere in material entities. For dispositions,
then, we can identify the following features.
Dispositions, triggers, and background conditions
The relationship between triggering conditions, disposi-
tions, and realizations described above is over-simplified.
Keep in mind that the dispositional account given
characterizes dispositions as non-probabilistic and their
associated phenomena as straightforwardly accessible. In
reality, many dispositions are most likely probabilistic
and the precise nature of their associated phenomena
(triggering conditions, realization processes) is currently
unknown. This should not deter one from seeking to
give an account of dispositions, triggers, and realization
processes, however. One of the more attractive accounts
proffered is from Rohl and Jansen [27]. According to
this account, there is a primitive (undefined) relation
(has_triggerR) that holds between the triggering process
and the realization process. The relationship between the
disposition and the triggering process is defined in terms
of that primitive relationship as follows:3
d has triggerDt iff : there exists some r
dhas realization r and r has triggerRtð Þ
Roughly, this says that some particular disposition has
a particular trigger just in case there is some realization
process such that the particular disposition has that
realization process and that realization process has some
trigger. We would then say that the particular dispos-
ition in question has that particular trigger. This is an
instance-level relationship. An example of such a rela-
tionship would be the fragility of a vase. The particular
disposition a vase has for shattering is realized by some
instance of a realization process (a shattering process),
which is triggered by some particular triggering event,
e.g., a dropping process. The relationship between the
particular realization process that realizes the disposition
and the process that triggers or precedes the realization
process is irreducible.
Once there is a relationship between the instances of dis-
positions and triggers, it is rather straightforward to extend
this to types of dispositions. Following Rohl and Jansen, this
relationship can be captured in the following manner:
Dhas triggerDT¼DEF? ð instance ofD ? ?y
 has triggerDy ? y instance of Tð ÞÞ
This definition is at the level of types. It specifies the rela-
tionship between disposition types and triggering process
types. In a similar manner the relationship between
realization types and triggering types can be captured:
R has triggerRT¼DEFfor ?  ð instance ofR ? ?y
y instance ofT  has triggerRyð ÞÞ
This relationship holds between realization process
types and triggering process types. Since dispositions
involve triggering processes, realization processes, and
Ray et al. Journal of Biomedical Semantics  (2016) 7:15 Page 8 of 12
dispositions themselves, it is essential to capture the
relationships between these three entities. The above
relationship specifications do just that. To see how this
works in our case of vision, consider the following
relation for the vision of an individual:
Agent xs disposition for vision has_triggerD particular
lighting conditions iff: there exists some neural processes
of agent x (agent xs disposition to see has_realization
neural processes of agent x and neural processes of
agent x has_triggerR particular lighting conditions).
The relationships would then exist at the universal level:
Vision Disposition has_triggerDLighting Conditions
(range of light wavelengths) = DEF ?x (x instance_of
Vision Disposition ? ?y (x has_triggerD particular lighting
conditions ? particular lighting conditions instance_of
Lighting Conditions (range of light wavelengths)).
Photon to Neural Signal Transduction has_triggerR-
Lighting Conditions (range of light wavelengths) = DEF for
?x (x instance_of Photon to Neural Signal Transduc-
tion ? ?y (y instance_of Lighting Conditions (range of
light wavelengths) & x has_triggerR y)).
4
The above specifications may be complex but have a
number of advantages. First, they capture the processes
that underlie dispositions and the dispositions them-
selves, which allows for more accurate modeling.
Second, this approach accounts for all of the processes
(or at least all of the relevant processes) involved in
dispositions. Approaches that do not account for all of
the relevant processes involved in dispositions will be
incomplete. Third, it leverages BFO as an upper-level
ontology and inherits all of the benefits therein. Namely,
using BFO provides a realist framework for modeling
biomedical entities of interest and provides pre-theoretical
interoperability with other biomedical ontologies that also
employ BFO as an upper-level ontology.
Possible objections
One possible objection to this account is that since
blindness is a reduction in the range of conditions under
which a disposition is realized, blindness is extrinsic to
the agent that is blind. So, according to our account,
turning off the lights is sufficient to make someone
blind. The thrust of this objection being that the reduc-
tion of triggering conditions is extrinsic to the agent, but
blindness is a phenomenon that is intrinsic to an agent.
Therefore our account is incorrect
We think that this objection is confused. The confu-
sion here is one between the presence of the conditions
and the range of conditions. To take an example, water
has a disposition to freeze at or under a certain
temperature at a given level of atmospheric pressure
(which just happens to be a range of conditions under
which the freezing process is realized). The temperature
constitutes a triggering condition for the freezing
disposition. Lowering the temperature brings the dispos-
ition closer to realization due to the change in the exter-
nal environmental conditions, but it does not mean that
water freezes at a lower temperature. Nor does this
change the disposition that water has to freeze. It is not
that the range of conditions that changed, rather that
the conditions themselves changed. The conditions
under which a disposition is realized are external to an
entity, much like the range, but these two things are
independent of one another. Conditions themselves can
change without the range of conditions changing, as is
the case with reductions in triggering conditions. In this
way we can see that turning off the lights does not
constitute a reduction in the range of conditions for
which a sight function is realized. Rather, it only con-
stitutes a reduction in the actual lighting conditions
in that situation.
There is an important point to be gleaned from this
objection, however, in regard to a concern over the
distinction between intrinsic and extrinsic properties
and whether blindness is intrinsic or extrinsic. We have
no reason to suppose that blindness is either intrinsic or
extrinsic, at least not pre-theoretically. The reduction in
the range of conditions under which the vision function
is realized is due to a change in the physical basis of the
bearer of the function. This, we believe, accounts for the
intuition that blindness is an intrinsic property. Indeed
our account of blindness is compatible with this obser-
vation, so those who believe that blindness is intrinsic
should not be hostile to our view. We leave further
discussion of intrinsic/extrinsic for a later and more
detailed paper.
A second objection runs along the following lines: the
solution offered fails to adequately distinguish a failure
of seeing (or sight) from the inability to see. In the case
of inattentional blindness one is failing to see something
but yet they have the ability to see that thing. This is an
important distinction: in fact it is the very difference
between being blind and being able to see perfectly well.
It is only when one lacks the ability to detect visual
stimuli that they are blind. The account provided con-
founds these two phenomena and is thus incorrect.
We think this objection raises a very important point
that we have alluded to earlier. We agree that there is a
distinction to be made between having the ability to see
and failing to notice a feature of the world versus not
having the ability to see. The former typically would not
be considered a case of blindness but the latter surely
would. As we have stated before, the case of inattentional
blindness is interesting and problematic. We have covered
the possibility that inattentional blindness is a case of
blindness and given an account of how that should be
formalized. If the envisioned case driving the objection is
one where the use of blindness is metaphorical, then we
Ray et al. Journal of Biomedical Semantics  (2016) 7:15 Page 9 of 12
need not and should not explain inattentional blindness as
a type of blindness. If, on the other hand, inattentional
blindness is not a metaphorical blindness (it is an instance
of a non-realized visual disposition), then it will be formal-
ized in the manner other types of blindness are formalized
according to this schema. That is, the disposition will not
be realized in one of two ways: nonconsciously by some
mechanism of contour integration in the early visual
cortex or consciously by a failure of function by the lateral
occipital cortex.
In the case where one fails to see not through an
inability to realize a disposition, but rather a simple
mistake of perception (the objection at hand), this would
firmly fall into a case of metaphorical blindness. If it is
the case that all instances of inattentional blindness are
such failures to see not because of an inability to realize
a disposition, but rather by a mistake of perception, then
inattentional blindness is a metaphorical blindness. If
not, then we have a formalization at the ready.
A third objection can be described in the following
way: One way to explain blindness is that there are many
sub-processes that participate in the more complex
process of vision. When one of these sub-processes does
not occur, then the vision process does not occur. What
is happening in the vision case is complicated, at least in
part because there are fine points regarding processes,
dispositions (functions), and their respective sub-processes
and sub-dispositions (sub-functions). It is the case that
many of the large, easily observable dispositions seem to be
not single dispositions but rather many dispositions work-
ing together. It is more accurate to say that the realization
of many dispositions is a complex of processes, some of
which may themselves constitute realizations of disposi-
tions that, in a way, seem to be sub-dispositions. Indeed
when we consider complex systems like the human visual
system it seems that this more complex picture is closer to
the truth than the simple explanation proffered by the
canonical dispositional picture. For example, the human
visual system contains many components and those com-
ponents each have distinct functionsthe function of the
retina is to convert photons to action potentials and the
function of the optic nerve is to transmit the action poten-
tial to the lateral geniculate nucleus and the function of the
primary visual cortex is to integrate the action potentials
and organize that information in such a way that it is intel-
ligible for a human being, etc. Each of these processes could
fail to be realized due to a failure of some mechanism, and
those are the causally responsible entities in the case
of blindness or impaired vision. When we think of
these smaller processes and the material entities they
are dependent upon, we see that color blindness is a
temporary or permanent loss of one of the disposi-
tions of one or more structures of the visual system by
their physical alteration. This view is simpler because
we can explain the disposition of an organism by
appealing to their parts and the realization of sub-pro-
cesses, which is more accurate than simply appealing to
larger-scale dispositions.
We find the approach described in the previous para-
graph problematic for a few reasons. First, the nature of
dispositions in BFO is such that the account this
response posits is insufficiently robust. As we have
discussed, dispositions are such that they cannot be
within the domain or range of the lacks_part relation-
ship. If there is a loss of a disposition or function, we
have to determine how to represent a loss of a dispos-
ition, which is not something we can do simply by
leveraging existing relationships.
Second, one of the problems for such an account is
the nature of functions, which are a special type of
disposition in BFO. It is the case that functions can be
lost under certain circumstances, i.e., under circum-
stances of extreme physical, structural change of the
bearer of the function. However, it is not the case that
any physical change in a bearer of a function such that
the function ceases will entail the loss of the function.
This is the case in BFO as well as cases of vision under
examination in this paper.
For example, a door has a function to open. It is essen-
tial to the proper functioning of the door that its func-
tion is only realized under certain conditions. If the door
opens too easily (under a range of conditions that is too
wide, say such that it opens under the pressure of the
wind), we would still contend that the door bears the
function to open, but that it is malfunctioning (due to a
faulty latch or some such). Likewise, a door can be diffi-
cult to open while retaining its function to open, also
due to some structural alteration. But we would still say
that the door has the function to open (and close) and
that it is malfunctioning. For each of these examples the
door retains its function but the function is realized
under a smaller (or larger) range of conditions (i.e., it
takes more or less force to open the door). The range of
conditions under which a function is realized is reduced
because of an alteration in physical structure of the
bearer of the function. The function of the entity is not
altered, however; its functioning is. In this way a difficult
door is similar to some cases of blindness, in that both
of the entities bearing the function retain that function
even though the function is realized under a broader or
narrower range of conditions.
Now, it could be appropriate to frame such an event
in terms of the various sub-processes or processes rea-
lizing the function (what are called functionings) not
occurring, and this is indeed what is happening. But
such a framing would fail to accurately capture all of a
single type of phenomenon, in our case blindness.
Further, what unites all cases of blindness is not that
Ray et al. Journal of Biomedical Semantics  (2016) 7:15 Page 10 of 12
some functioning or another is not occurring, but that
we have as a result a reduction of the range of condi-
tions under which a disposition is realized. Any such
reduction (under a certain threshold) is a case of blind-
ness, no matter what sub-functioning is unrealized. Of
course we may find further subtypes of blindness exist
and these may be demarcated according to the various
underlying physical mechanisms that reduce the range
in conditions under which the vision function is realized,
but this is consistent with our view. In fact it is our view.
In each of the cases discussed, the difference is the
mechanism underlying the reduction in the range of
triggering conditions for the disposition of vision.
Discussion
Given the problems associated with the currently exist-
ing accounts of blindness, we thus propose a new defin-
ition of blindness: blindness is a reduction of the trigger
conditions under which the sight function is realized. If
visual impairment admits of degrees, then it cannot be a
function or disposition. Blindness cannot be a lack of a
disposition or function because many things lack the
function yet should not be classified as blind. The best
option available for defining blindness is to say that sight
is the function to receive photons and interpret them as
visual information and then proceed to define blindness as
a reduction of the conditions under which the disposition
is realized.
One of the consequences of this reasoning is that
many of the terms in various phenotype ontologies
mistakenly refer to entities that are dispositions rather
than qualities. Our goal in this paper is not to demon-
strate that many terms in phenotype ontologies are
mistakenly characterized as denoting qualities rather
than dispositions, but merely that blindness is not a
phenotype. We think that, as is consistent with BFO,
dispositions are not phenotypes and phenotypes are not
dispositions. It would be a mistake to confuse pheno-
types and dispositions. This does not entail that there
are no terms in various phenotype ontologies that
denote dispositions, only that representing a disposition
as a phenotype is incorrect from the perspective of BFO.
While there may have been efforts to annotate data
using 'blindness' in phenotype ontologies in the past, we
feel these efforts are mistaken.
Our approach has certain advantages. First, it accounts
for the graded nature of blindness. The slow and some-
times gradual onset of blindness raises special problems
for ontology construction as it admits of degrees and
seemingly vague boundaries. Second, it classifies sight as
an internally-grounded realizable entity, which makes
use of the framework provided by an upper-level ontol-
ogy such as BFO. Third, it is ontologically innocent in
that there are no new entities to countenance in any
upper-level ontology. The entities referenced by our
solution are already present in BFO so there is no need
to introduce new entities.
Conclusions
The motivation of this project is to provide a simple yet
flexible ontological account of blindness. Since blind-
ness can result from a variety of diseases, the construc-
tion of ontologies that incorporate both blindness and
the diseases that cause blindness, either directly or indir-
ectly, is of importance to the biomedical community.
But this is not a purely classificatory exercisethe
employment of conditions under which a disposition (or
function) is realized is a novel application of a tool that
has been available for ontological developers for some
time. It is the opinion of these authors that this type of
usage could yield further fruitful results.
Endnotes
1Functions in BFO are a special type of disposition.
2Functions do not have parts but the entities that have
functions do have parts.
3Note that the relationship has_trigger is two-place due
to the limitations of formal ontology languages like OWL,
which is only able to handle two-place relationships.
4We leave bound variables uninstantiated for the reader.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
PLR composed, edited, and revised the manuscript. APC, MJ, WD, TA, and
ADD provided comments and revised the manuscript. ADD directed the
work. All authors read and approved the final manuscript.
Acknowledgements
The authors would like to thank Isaac Berger for insightful comments on the
manuscript as well as participants of the 5th International Conference on
Biomedical Ontologies (Houston, TX, October 2014) for discussion on a
presentation of an earlier version of this work.
Author details
1Department of Philosophy, University at Buffalo, Buffalo, NY, USA. 2New York
State Center of Excellence in Bioinformatics and Life Sciences, University at
Buffalo, Buffalo, NY, USA. 3Department of Neurology, Jacobs School of
Medicine and Biomedical Sciences, University at Buffalo, Buffalo, NY, USA.
Received: 26 March 2015 Accepted: 22 March 2016
López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 
DOI 10.1186/s13326-016-0101-1
RESEARCH Open Access
Can SNOMED CT be squeezed without
losing its shape?
Pablo López-García* and Stefan Schulz
Abstract
Background: In biomedical applications where the size and complexity of SNOMED CT become problematic, using
a smaller subset that can act as a reasonable substitute is usually preferred. In a special class of use caseslike
ontology-based quality assurance, or when performing scaling experiments for real-time performanceit is essential
that modules show a similar shape than SNOMED CT in terms of concept distribution per sub-hierarchy. Exactly how
to extract such balanced modules remains unclear, as most previous work on ontology modularization has focused
on other problems. In this study, we investigate to what extent extracting balanced modules that preserve the original
shape of SNOMED CT is possible, by presenting and evaluating an iterative algorithm.
Methods: We used a graph-traversal modularization approach based on an input signature. To conform to our
definition of a balanced module, we implemented an iterative algorithm that carefully bootstraped and dynamically
adjusted the signature at each step. We measured the error for each sub-hierarchy and defined convergence as a
residual sum of squares < 1.
Results: Using 2000 concepts as an initial signature, our algorithm converged after seven iterations and extracted a
module 4.7 % the size of SNOMED CT. Seven sub-hierarhies were either over or under-represented within a range of
18 %.
Conclusions: Our study shows that balanced modules from large terminologies can be extracted using ontology
graph-traversal modularization techniques under certain conditions: that the process is repeated a number of times,
the input signature is dynamically adjusted in each iteration, and a moderate under/over-representation of some
hierarchies is tolerated. In the case of SNOMED CT, our results conclusively show that it can be squeezed to less than
5 % of its size without any sub-hierarchy losing its shape more than 8 %, which is likely sufficient in most use cases.
Keywords: SNOMED CT, Ontology modularization, Biomedical terminology
Background
The large size and complexity of SNOMED CT [1] con-
stitute a problem in many biomedical applications and
studies have shown that using a much smaller subset of
interest is often sufficient [2]. Applications include prob-
lem lists [3], tagging medical images [4], and annotating
texts from cardiology [5], among others. A well-known
example of the benefits of using a subset of interest is
the CORE problem list subset of SNOMED CT, which
contains only 16 874 terms (roughly 1 % the terms of
SNOMED CT), while covering over 95 % of its usage [6].
*Correspondence: pablo.lopez@medunigraz.at
Institute for Medical Informatics, Statistics and Documentation, Medical
University of Graz, Auenbruggerplatz 2, Graz, Austria
The theory of how to extract such subsets is studied by
the ontology modularization area of research [7]. Ontol-
ogy modularization techniques are generally focused on
obtaining a minimal subset (also called module or seg-
ment) that maximally covers a specific domain or that
is representative for a particular application. This is the
case of the problem lists or annotation cases mentioned
above, or the study by Seidenberg and Rector [8], where
they described how they extracted a representative seg-
ment of the GALEN ontology [9] for cardiology using the
seed concept Heart as a signature.
A signature is an initial set of concepts (called seeds)
that bootstraps the modularization process, on which
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 Page 2 of 7
many ontology modularization techniques rely, includ-
ing graph-traversal [8, 1012] and logic-based techniques
[13, 14].
Often, these modules are not balanced when it comes
to representing the original distribution or shape of sub-
hierarchies shown by the original ontology or termi-
nology. For example, in the CORE problem list subset
of SNOMED CT, most concepts belong to the Clini-
cal Finding, Procedure, Situation with Explicit Context,
and Event sub-hierarchies. The opposite case is also pos-
sible: in a previous study, we found that modules can
excessively and uncontrollably grow and spread across
sub-hierarchies, especially when using graph-traversal
techniques [5].
These results are not surprising, because most prior
work on ontology modularization has not focused on pre-
serving the representativity of the sub-hierarchies of the
original ontology, so the shape of the original ontology is
inevitably lost in the modules.
There is a special class of use cases, however, where it
is essential that modules are representative of the sub-
hierarchies of the original ontology and therefore show a
similar shape, such as:
 In ontology-based quality assurance, where small but
representative samples of a huge ontology are to be
inspected [15];
 for obtaining a demonstration version that is
understandable for users or facilitates visualization
[16, 17];
 for alignment with a highly constrained upper level
ontology, such as the Basic Formal Ontology (BFO)
[18], especially the upcoming BFO 2.0 OWL version,
which includes relations, DOLCE [19] or
BioTopLite [20], where reasoning has to be tested on
small subsets and in iterative debugging steps;
 for performing scaling experiments for real-time
performance of a large OWL DL ontology;
 for the description logics community, who welcomes
scalable testbeds for developing tools like editors and
reasoners.
To the knowledge of the authors, little research on
ontology modularization has focused on extracting bal-
anced modules for such applications, where keeping the
original shape of a large ontology such as SNOMEDCT in
terms of its sub-hierarchies is a requirement.
In this paper, we study the concept distribution of
SNOMEDCTs sub-hierarchies, and we propose and eval-
uate an iterative algorithm for extracting balanced mod-
ules. Our main goal is to investigate to what extent it
is possible to obtain modules that preserve the origi-
nal shape of SNOMED CT in order to be used in our
identified class of use cases.
Methods
As input for our experiments, we used the July 2014
International Release of SNOMED CT [21]. We first gen-
erated its corresponding OWL-EL version using the Perl
script included in SNOMEDCTs official distribution. We
then removed the SNOMED CT Model Components sub-
hierarchy, which contains metadata concepts only. For the
remainder of this text, we refer to SNOMED CT and our
input version (containing 229 330 classes) termed SCT
interchangeably.
SNOMED CT concept distribution
Table 1 shows the main 18 sub-hierarchies of SNOMED
CT and their concept distribution. Four sub-hierarchies
(Clinical Finding, Procedure, Organism, and Body Struc-
ture) contain over 10 % of SNOMED CTs concepts each,
accounting for over 70 % of the concepts when considered
altogether. As a useful way of visualizing concept distribu-
tion and for comparative purposes (see Section Results),
the same information is displayed in the form of a treemap
in Fig. 1. The treemap represents SNOMED CTs hierar-
chical information as a set of colored rectangles, where
the area (and color) of each rectangle is proportional
(and darker/lighter) to the number of concepts in the
sub-hierarchy.
Table 1 Main sub-hierarchies of SNOMED CT. The metadata
concepts sub-hierarchy (SNOMED CTModel Components) was not
considered
Subhierarchy (Abbreviation) Concepts Distribution
Clinical Finding (CF) 100 893 33.57 %
Procedure (PR) 53 914 17.94 %
Organism (OR) 33 273 11.07 %
Body Structure (BS) 30 685 10.21 %
Substance (SU) 24 021 7.99 %
Pharmaceutical/Biologic Product 16 881 5.62 %
Qualifier Value (QV) 9 055 3.01 %
Observable Entity (OE) 8 307 2.76 %
Social Context (SO) 4 703 1.56 %
Physical Object (PO) 4 522 1.50 %
Situation with Explicit Context (SI) 3 695 1.23 %
Event (EV) 3 673 1.22 %
Environment or Geogr. Location (EG) 1 814 0.60 %
Specimen (SN) 1 447 0.48 %
Staging and Scales (ST) 1 309 0.44 %
Special concept (SP) 649 0.44 %
Record Artifact (RA) 227 0.22 %
Physical Force (PF) 171 0.08 %
López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 Page 3 of 7
Fig. 1 SNOMED CTs shape represented with a treemap. Sub-hierarchies containing less than 10 % of SNOMED CT concepts are shown in acronyms
(see Table 1)
Balanced SNOMED CTmodules
In a comprehensive study, dAquin et al. [22] concluded
that there is no universal way to extract ontology mod-
ules and that the chosen approach should be guided by
each domain or application. It is therefore important to
clearly define what constitutes a module. For our pur-
poses, presented in the introduction, we define a bal-
anced SNOMED CT module (M) as a minimal collec-
tion of classes from SCT that conform to the following
requirements:
(a) All classes in M are hierarchically connected to
SNOMED CTs root concept in the same way as in
SCT.
(b) All classes in M share the same axiomatical class
definition as in SCT.
(c) Sub-hierarchies in M are distributed (approximately)
in the same proportion as in SCT. In practical terms,
when visualized using a treemap, M should look
similar to the treemap of SNOMED CT shown in
Fig. 1.
(d) Our model is restricted to classes. SNOMED CT
metadata concepts are excluded and not subject to
modularization.
Module construction from seeds
To extract our module M, we followed a graph-traversal
ontology modularization approach, adapted from Seidenberg
and Rector [8]. Using their terminology, concepts (in
our case, classes) are represented as nodes in a graph,
and seed concepts are called target nodes. The strategy
takes seeds that conform an initial signature as input,
and then iteratively adds classes that appear in the right-
hand expressions of their definitions (i.e., are connected
by attribute links) and their links up the hierarchy, then
becoming new target nodes. Figure 2 shows an example
of a resulting module, where it can be seen that (a) all
classes are hierarchically connected to the root concept in
the same way as in the original ontology (Fig. 3), and (b)
all classes share the same axiomatical class definition as
the original ontology (i.e., show the same structure when
displayed as a graph).
Seed adjustment: an iterative algorithm
The strategy to build a module using seeds presented
above guarantees requirements (a) and (b) from our def-
inition of M, but does not guarantee requirement (c), i.e.,
that sub-hierarchies in M will be distributed (approxi-
mately) in the same proportion as in SCT. The reason for
not guaranteeing requirement (c) is that there is no control
over classes from other sub-hierarchies that are added and
become new target nodes when following the right-hand
expressions of the seeds.
Therefore, in order not to conflict with requirements
(a) and (b) when creating M, the only possibility is to
carefully select the initial signature that bootstraps the
modularization algorithm. For that purpose, we investi-
gated an iterative algorithm that dynamically adjusts the
distribution of classes used as seeds in the initial signa-
ture. Before presenting the algorithm, we introduce the
following notation:
 As introduced before, SCT represents the OWL EL
version of SNOMED CT used as input.
Sub-hierarchies are termed SHk .
 M represents the output module, whose
sub-hierarchy distribution should match SCTs as
closely as possible (Table 1).
López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 Page 4 of 7
Fig. 2 Ontology modularization strategy to build our moduleM, starting from the seed concept (target node) labeled as 10. Figure 3 shows the
original ontology from which it was extracted
 SIGN is the input signature, consisting of classes
from SCT, that is used to bootstrap the
modularization process described in
Subsection Module construction from seeds.
 Error(SHk) = Size(MSHk ) ? Size(SCTSHk ) indicates
the error on a per sub-hierarchy basis. Errors are
calculated in percentage terms (see distribution in
Table 1).
 RSS = 118
?18
k=1 Error(SHk)2, where RSS represents
the residual sum of squares. Convergence of the
algorithm is defined when RSS < 1.
The algorithm, at each iteration i is the following:
1. A random signature SIGNi consisting of 2000 classes
from SCT is selected, following the same class
sub-hierarchy distribution as SCT, and ensuring that
all sub-hierarchies in the signature contains at least
one class.
2. A moduleMi is computed following the principles
described in Subsection Module construction from
seeds. Its sub-hierarchy distribution is calculated.
3. Convergence is checked. If RSS >= 1, Steps 1 to 3
are repeated after adjusting the scaling factor for the
sub-hierarchy distribution of the signatures in the
next iteration i + 1:
f
(
SIGNi+1SHk
)
= f
(
SIGNiSHk
)
× f
(
SCTSHk
)
f
(
MiSHk
) with
f
(
MiSHk
)
being the relative frequency of sub-hierarchy
SH k measured in the resulting module in iteration i,
Mi.
Fig. 3 Sample ontology, with an initial signature containing the seed concept (target node) labeled as 10
López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 Page 5 of 7
Results
A moduleM with 10 834 classes was extracted from 2000
seeds, the module being in 4.7 % the size of the original
SCT (229 330 classes). Figure 4 shows how the algorithm
converged after 7 iterations, the error for sub-hierarchies
exceeding an error of 1 %, and the residual sum of squares.
As can be seen in the table below the graph, the sub-
hierarchies Clinical Finding, Procedure, and Organism
were under-represented in M, while Body Structure and
Substancewere over-represented. The same results can be
confirmed graphically in the treemaps shown in Fig. 5, at
iterations 1, 3, and 7.
These results were partly expected, due to the nature
of the modularization approach that uncontrollably adds
extra classes that appear in right-hand expressions to
preserve SNOMED CTs class definitions. The most rep-
resentative example is the sub-hierarchy Body Structure,
whose concepts appear often in definitions in Clinical
Findings, e.g. Finding site: Bone structure of femur (body
structure) for Fracture of femur (clinical finding).
Our experience indicates that there is a point (around
7 iterations) where the algorithm starts oscillating, and
the residual sum of squares can not diminish any longer.
In practice, this means that when the algorithm tries to
compensate the under-representation of Clinical Finding
by adding more Clinical Finding seed concepts to the
signature in the next iteration, the result of the new bal-
anced module inevitably includes also more Body Struc-
ture concepts. This is better understood using Fig. 3 as
an example. Assuming concept 10 is a Clinical Find-
ing seed added to compensate their under-representation,
the graph-traversal modularization algorithm would also
add Body Structure concepts 17, 16, 15, and 9 to the
balanced module, because concept 17 appears in a right-
hand expression of concept 10, and 16, 15, and 9 are its
ancestors (Fig. 2).
Discussion
Our results suggest that it is difficult for ontology mod-
ules to meet all of our modularization criteria without
relaxing the constraints of how concepts in the mod-
ules are distributed by sub-hierarchies: this is because
modularization criteria are conflicting. In our experi-
ments, all obtained modules over-represented or under-
represented some of SNOMED CTs sub-hierarchies to
varying degrees.
The error figures that we obtained after convergence,
however, never reached 8 % for any sub-hierarchy and all
our modules contained a fair representation of every sub-
hierarchy. Furthermore, convergence was reached after
only 7 iterations and the resulting module was 4.7 % the
size SNOMED CT. Such modules might be sufficient in
Fig. 4 Execution of the algorithm, showing convergence in iteration 7. Each line represents the difference in distribution for a particular
sub-hierarchy of a balanced module at a given iteration, when compared to SNOMED CT. For example, in the balanced module at iteration 1, Body
Structure is proportionally 11.26 % bigger than in SNOMED CT (it is over-represented), while Clinical Finding is 6.86 % (it is under-represented). The
dashed line represents the residual sum of squares of all sub-hierarchies, 0 meaning that the sub-hierarchies in the balanced module are distributed
in exactly the same way as in SNOMED CT (perfectly balanced module)
López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 Page 6 of 7
Fig. 5 Visual comparison of the shape between modulesM and SNOMED CT (d) in iterations 1 (a), 3 (b), and 7 (convergence, c). Clinical Finding,
Procedure, and Organism were under-represented, while Body Structure and Substance were over-represented
many of the use cases that motivated their creation, i.e.,
extracting modules that show an (approximate) concept
distribution to the one shown in SNOMED CT.
In this study, we focused on extracting balanced mod-
ules in SNOMED CT only, both for practical purposes
(useful input for related SNOMED CT research) and
because its size and complexity make SNOMED CT an
excellent case study. Our approach, however, should work
similarly with for any ontology where graph-traversal
modularization techniques based on an input signature
apply. The literature currently reports positive exper-
iments with NCI, GALEN, GO, SUMO, SWEET, and
DOLCE-Lite [8, 13].
Conclusions
Modules that preserve the concept distribution by sub-
hierarchy of the original ontology have been generally
neglected in the field of ontology modularization. How-
ever, balanced modules are extremely useful in applica-
tions such as ontology-based quality assurance, scaling
experiments for real-time performance, or when develop-
ing scalable testbeds for software tools.
In this study, we have proposed and evaluated an iter-
ative algorithm to investigate to what extent extracting
such balanced modules in SNOMED CT is possible. Our
results show that graph-traversal ontology modulariza-
tion techniques relying on an input signature can indeed
be used, if: the process is repeated a number of times;
the input signature is dynamically adjusted in each itera-
tion; and a moderate under/over-representation of some
hierarchies is tolerated.
Several questions are still open and need to be addressed
as future work: how to select a minimal signature; how
signature size influences the final size of the modules;
and how a change in the randomization process of the
signature selection (e.g., by stratifying the randomization
by node depth) influences the concept distribution of the
module. In addition, a validation of our experiments using
other ontologies and comparing the results would provide
a more comprehensive overview.
Our results, however, conclusively show that SNOMED
CT can be squeezed to less than 5 % its size without any
sub-hierarchy losing its shape more than 8 %, which is
likely to be sufficient in most use cases.
López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 Page 7 of 7
Acknowledgements
This manuscript is a revised version of the work presented in the ICBO 2015
conference. The authors acknowledge ICBO 2015 organizers and participants
for their useful feedback and suggestions, and Marcus Bloice for checking and
revising the final version of the manuscript.
Authors contributions
PLG wrote most of the manuscript, verified the experiments, summarized the
results, and presented the work in the ICBO 2015 conference. SS formulated
the original idea, implemented the algorithm, and sketched the first version of
the manuscript. Both authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Received: 5 March 2016 Accepted: 9 September 2016
RESEARCH Open Access
OBIB-a novel ontology for biobanking
Mathias Brochhausen1*, Jie Zheng2, David Birtwell3, Heather Williams3, Anna Maria Masci4, Helena Judge Ellis5
and Christian J. Stoeckert Jr.2
Abstract
Background: Biobanking necessitates extensive integration of data to allow data analysis and specimen sharing.
Ontologies have been demonstrated to be a promising approach in fostering better semantic integration of
biobank-related data. Hitherto no ontology provided the coverage needed to capture a broad spectrum of
biobank user scenarios.
Methods: Based in the principles laid out by the Open Biological and Biomedical Ontologies Foundry two
biobanking ontologies have been developed. These two ontologies were merged using a modular approach
consistent with the initial development principles. The merging was facilitated by the fact that both ontologies use the
same Upper Ontology and re-use classes from a similar set of pre-existing ontologies.
Results: Based on the two previous ontologies the Ontology for Biobanking (http://purl.obolibrary.org/obo/obib.owl)
was created. Due to the fact that there was no overlap between the two source ontologies the coverage of
the resulting ontology is significantly larger than of the two source ontologies. The ontology is successfully
used in managing biobank information of the Penn Medicine BioBank.
Conclusions: Sharing development principles and Upper Ontologies facilitates subsequent merging of ontologies to
achieve a broader coverage.
Keywords: Ontologies, Biobanking, Biorepository, Terminology
Background
The field of biobanking demands data integration. This
need arises on multiple levels: institutional, cross-
institutional, and sometimes even cross-national. Most
institutions operate multiple biobanks that were estab-
lished to fulfill diverse user requirements and therefore
use different data representations and schemata. Inte-
grating the data from these biobanks is a challenge at
best and impossible at worst.
In recent years a number of projects have set out to
answer the call for more specimen and data sharing
across multiple biobanks and multiple institutions, both
on a national and on a transnational level [1, 2]. One of
the earliest examples of a transnational sample sharing
project is the European Biobanking and BioMolecular
Research Infrastructure (BBMRI) [3]. Using large amounts
of data from multiple biobanks is an important way to en-
able statistical analysis, which can lead to uncovering
associations between phenotypes and diseases [4]. The
two key challenges are a) identifying specimens for re-
search, and b) utilizing the existent wealth of information
present in biobanks effectively by integrating data stored
in those repositories [5, 6].
Andrade et al. have pointed out that semantically rich
ontologies provide a promising approach to integrating
data from diverse biobanks [7]. In 2013 Brochhausen et al.
published Ontologized MIABIS (OMIABIS), a Web
Ontology Language (OWL)-coded ontology for biobank
administration based on use cases and competency ques-
tions derived from BBMRI (http://purl.obolibrary.org/
obo/omiabis/merged/omiabis.owl) [8]. OMIABIS is one of
the source ontologies that was used in creating the Ontol-
ogy for Biobanking (OBIB). It is linked to the BBMRI ef-
fort [3] and is based on the Minimum Data Set for sharing
biobank samples, information and data [9]. One of the key
advantages of using ontologies, and in particular re-using
pre-existing ontological representations is the possibility
to link data from biobanks to other biological and bio-
medical repositories using common identifiers, e.g. from
* Correspondence: mbrochhausen@uams.edu
1Department of Biomedical Informatics, University of Arkansas for Medical
Sciences, 4301 W. Markham St., #782, Little Rock, AR 72205-7199, USA
Full list of author information is available at the end of the article
© 2016 Brochhausen et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Brochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 
DOI 10.1186/s13326-016-0068-y
Gene Ontology [6]. This possibility will improve the utility
of the shared biobank data tremendously since it allows
easy retrieval of related data using semantic web technolo-
gies, such as RDF, SPARQL, and OWL. The capability to
use biobank data in that way will make biobanks even
more important for and accessible to translational re-
search since the data already exist in a way that allows
easy linkage to data across other disciplines from basic sci-
ence to clinical research.
In addition, the use of a formal logical model to repre-
sent aspects of biobanking procedures and protocols
prevents unnecessary complexity that can lead to cum-
bersome and error-prone data management and retrieval
processes. One example of a complexity issue is a biobank
protocol involving something as simple as specimen type.
Consider a blood collection protocol that collects two vials
of blood. Each vial contains an anti-coagulant, in this case
one is ethylenediaminetetraacetic acid (EDTA) and the
other monosodium citrate (NaCit). Each specimen goes
through centrifugation producing a number of plasma and
buffy coat specimens. The lab decides to tag their speci-
mens in the following way: "edta_plasma", "buffy_edta",
"nacit_plasma", "buffy_nacit". The "edta_plasma" and "buf-
fy_edta" specimens are the derivatives of the EDTA parent
specimen and analogously for "nacit_plasma" and "buffy_-
nacit" specimens. So the lab is encoding two pieces of in-
formation in the "specimen type", the parent specimen
and the anticoagulant. Notice also that they chose the
anticoagulant as the prefix for plasma specimens, but as
the suffix for the buffy specimens, due to their perception
that the anticoagulant is more important for the plasma
specimens and less so for the buffy specimens. Another
lab has a simpler protocol. They collect one EDTA speci-
men and produce plasma and buffy specimens. Since they
collect only one type of blood specimen, they tag their
specimens simply as "plasma" and "buffy". The information
about the anticoagulant is implied as is information about
the parent specimen. Imagine all these specimens end up
in a unified biobank. The following specimen types would
be present: edta_plasma, buffy_edta, nacit_plasma, buffy_-
nacit, plasma, and buffy. The inconsistencies are readily
apparent. An ontology can provide a formal model for
different attributes related to a specimen type, such as the
parent specimen and the anticoagulant. Based on these
attributes and others specimen types can be created as
axiomatically defined classes (Fig. 1).
In this paper we present and describe the Ontology for
Biobanking (OBIB). OBIB has been created by merging
OMIABIS with a more specimen-focused biobank ontol-
ogy developed at the University of Pennsylvania called
the Biobank Ontology (BO). We provide an overview of
the methodologies used to a) build both ontologies and
b) merge them. We also give an outline of the domain
covered by the newly created ontology and describe its
current use. Finally, we discuss the next steps in expand-
ing OBIB and linking it to ongoing efforts regarding bio-
bank terminology. While we think that shared ontologies
are one way to facilitate sharing information across
Fig. 1 Representation of edta_plasma, buffy_edta, nacit_plasma, buffy_nacit, plasma, and buffy specimens according to the OBIB strategy. Blue boxes
represent classes; red boxes represent individuals; red arrows represent rdfs:subClassOf; green arrows represent rdf:type; blue arrows represent OWL
object properties (the labels are specified). While all OWL object properties link instance to instance, in this figure there are object properties
connecting OWL classes to each other. This represents a property restriction on the source class with existential quantification (all-some restriction)
Brochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 Page 2 of 9
multiple sites, the description and discussion of how to
technically implement such a system (federated queries)
are regarded as out of scope for this paper.
Methods
OBIB is the result of merging two pre-existing ontologies,
OMIABIS and BO. In this section we describe criteria and
methodologies used by both the developers of OMIABIS
and BO. We will give a brief overview over the two source
ontologies and describe the merging process in detail.
Both ontologies, OMIABIS and BO are built based on
the principles of the Open Biological and Biomedical
Ontologies (OBO) Foundry (http://obofoundry.github.io/
principles/fp-000-summary.html) [10]. Both are exten-
sions of the Ontology of Biomedical Investigations (OBI)
(http://purl.obofoundry.org/obo/obi.owl) [11]. OBI is
based on Basic Formal Ontology (BFO), an Upper
Ontology frequently used to represent biological and
biomedical domains [12, 13]. One of the OBO Foundry
principles is re-use of pre-existing ontologies or their
classes and object properties in order to prevent multiple
representations of the same entities. Both, OMIABIS and
BO re-use numerous terms from existing ontologies. Both
ontologies were developed based on a methodology that
focuses on representation of the real-world phenomena
that are described by the data, which is intended to be
managed, instead of creating OWL representations of
existing data schemata. Smith and Ceusters have coined
the term ontological realism for this approach [14]. The ex-
ample provided of specimen type shows that relying exclu-
sively on pre-existing data representations can lead to
problems integrating data from heterogeneous resources.
Instead of providing representations base on the term used,
such as "edta_plasma", "buffy_edta", "nacit_plasma", "buf-
fy_nacit", "plasma", and "buffy", ontologies should represent
the specimens, the parent specimens, the anticoagulants
and the different processes that were necessary to create
the specimen. Thus, individual specimens can be sorted
into specimen types based on what their parent specimens
are and which processes they passed through. This meth-
odological paradigm fosters linking the biological sources
to the specimens to the data about those specimens.
OMIABIS
Ontologized MIABIS (OMIABIS) (http://purl.obolibrary.
org/obo/omiabis/merged/omiabis.owl) was created as an
OWL implementation of the BBMRI's Minimum Informa-
tion About BIobank data Sharing (MIABIS). It is based on
the BBMRI use cases, which are mostly population and co-
hort based. Due to juridical and ethical reasons searching
individual specimens was out of scope for the initial imple-
mentation of OMIABIS [8].
The competency questions for the development of
OMIABIS were:
 Which biobanks hold frozen specimens?
 Which biobanks hold blood, plasma and serum?
 Which blood plasma specimens are owned by one
specific biobank organization?
 Which departments of a specific university have
members that are serving as biobank contacts?
 What are the e-mail addresses of all biobank contact
persons at one specific biobank organization?
These competency questions demonstrate that the
focus of the OMIABIS development was less to retrieve
information about individual specimens and to order
those specimens, but more to obtain basic population-
level and repository specific information about biobank
administration and related study administration. OMIA-
BIS represents both relevant objects, such as "biobank"
or "biobank organization", and relevant processes, such
as "specimen handling" and "sampling specimens for bio-
bank". Terms were re-used from the Cell Type Ontology
(CL), Chemical Entities of Biological Interest (ChEBI),
Common Anatomy Reference Ontology (CARO), Informa-
tion Artifact Ontology (IAO), NCBITaxonomy, Ontology
of Medically Relevant Social Entities (OMRSE), Phenotypic
Quality Ontology (PATO), Proper Name Ontology (PNO),
and Reagent Ontology (REO). One hundred twenty-six en-
tities were created specifically for OMIABIS resulting in a
total of 428 classes, 15 individuals, 75 object properties,
and 990 logical axioms. The ontology and additional details
can be found at https://github.com/OMIABIS/omiabis-dev.
Biobank ontology (BO)
Another biobank ontology based on OBI, BO, was gen-
erated independently of OMIABIS to address use cases
provided by the Penn Medicine BioBank, which served
along with OMIABIS as a starting point for OBIB. The
competency questions for the development of BO were:
 How many study subjects have filled out a patient
questionnaire for which there is an associated
collection packet?
 What blood specimens are available from study
participants? What chemical additive was used
in the container?
 What is the storage state of the specimen of
interest? How has the specimen been processed?
The BO aimed to address these competency questions
by covering the processes along with inputs and outputs
associated with a specimen in a biobank repository.
These included human subject enrollment , informed
consent process , document editing (filling out case re-
port forms), specimen collection process , material pro-
cessing of the specimen, storage of the specimen, and
shipping and handling processes. Patient-related terms
Brochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 Page 3 of 9
(e.g., smoking behavior) were also included. Terms were
re-used from BFO, IAO, OBI, ChEBI, EFO, NCBITaxon,
OGMS, PATO, and UBERON. Approximately 50 terms
were added to address biobank specific needs (e.g., col-
lection packets for specimens) resulting in 227 classes,
18 individuals, 34 object properties, and 526 logical ax-
ioms. The ontology and additional details can be found
at https://github.com/biobanking/Penn-Biobank.
Methodology of merging OMIABIS and BO
The fact that both OMIABIS and BO were built based on
OBI and used BFO as top-level ontology greatly facili-
tated the integration of the two ontologies. However,
various BFO versions were used and some common
terms were included in both OMIABIS and BO. The fol-
lowing processes were performed before integration of
OMIABIS and BO:
1. Converted both ontologies using BFO version 2.0 Graz
release (http://purl.obolibrary.org/obo/bfo/2014-05-03/
classes-only.owl). The conversion was made using BFO
converter (http://bfoconvert.hegroup.org/).
2. Separated terms defined in OMIABIS or BO from
those defined in external resources (OBO Foundry
Ontologies) and saved those in different OWL files.
3. Identified terms defined in both OMIABIS and BO
and merged the overlapping terms where necessary.
This pre-processing of OMIABIS and BO before
merging resulted in the following files:
 omiabis.owl or biobank.owl: OMIABIS or BO
specific terms.
 import_OBI_subset.owl: OBI subset upon which
OMIABIS or BO was built containing terms needed
from both IAO and OBI. The OBI subset was
retrieved using Ontodog, a tool that can retrieve a
set of terms of interest and all related axioms from a
source ontology [15].
 import_OBO.owl: terms defined in external OBO
Foundry ontologies and retrieved using OntoFox, a
tool that can retrieve terms of interest from a source
ontology based on MIREOT mechanism [16].
 externalByhand.owl: terms defined in external OBO
Foundry ontologies added manually.
These files are available on the OMIABIS project
website:
OMIABIS: https://github.com/OMIABIS/omiabis-dev/
tree/master/BFO2%20omiabis and BO: https://github.
com/OMIABIS/omiabis-dev/tree/master/biobank-omiabis/
BFO2%20biobank.
Since OMIABIS and BO focused on different aspects
of biobanking, no overlap in terms was found between
OMIABIS and BO specific terms (omiabis.owl and
biobank.owl).
In preparing BO for merging it with OMIABIS, we
also compared BO to other OBO Foundry ontologies
relevant to the domain. We found a few BO terms re-
lated to informed consent that overlapped with terms
from the Informed Consent Ontology (ICO) [17]. These
BO terms were replaced by ICO terms. Since BO was
not officially registered as an OBO Foundry community,
BO specific terms were assigned OBIB term identifiers
after merging.
Protégé was used to perform the merging of the
OWL files prepared based on the two ontologies.
The process was a series of merges for pairs of
equivalent OWL files used by OMIABIS and BO to
create: import_OBI_subset.owl from subset OWL
files, import_OBO.owl from OWL files of external
terms retrieved using OntoFox [16], externalByhan-
d.owl from manually imported external terms OWL
files, and biobank-omiabis.owl from omiabis.owl and
biobank.owl. The one remaining BFO 1.1 class, Con-
nectedTemporalRegion, was dealt with by taking ad-
vantage of its definition as equivalent to the union
of its two subclasses, temporal_instant and tempora-
l_interval. These have both been mapped to BFO 2.0
classes, zero-dimensional temporal region and one-
dimensional temporal region, which were used to replace
ConnectedTemporalRegion.
These merged OWL files are available on github:
https://github.com/OMIABIS/omiabis-dev/tree/master/
biobank-omiabis.
The merged OMIABIS and BO ontology is available
on:
https://raw.githubusercontent.com/OMIABIS/omiabis-
dev/master/biobank-omiabis/biobank-omiabis_merged.owl.
Finally, the consistency of the merged ontology was
checked using Hermit 1.3.4 and no inconsistencies were
found.
Results
Ontology of biobanking (OBIB)
The first release of OBIB (http://purl.obolibrary.org/obo/
2014-09-22/obib.owl) was made based on the OWL files
described in section Methodology of merging OMIABIS
and BO. The development of OBIB followed the OBO
Foundry principles. OBIB is freely and openly available.
The latest release can be obtained from http://purl.
obolibrary.org/obo/obib.owl. The community driven
development is done using an open code repository,
https://github.com/biobanking/biobanking. Issues and
term requests can be communicated at https://github.
com/biobanking/biobanking/issues. Currently, OBIB con-
tains 516 classes including 126 OMIABIS and 46 OBIB
Brochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 Page 4 of 9
classes, 19 individuals, 83 object properties, and 1172 lo-
gical axioms.
One of the central terms of OBIB is biobank. This term
was merged from OMIABIS. OBIB defines biobank as:
"A biobank is a collections of samples of biological
substances (e.g. tissue, blood, DNA) which are linked to
data about the samples and their donors. They have a
dual nature as collections of samples and data".
The equivalent class axiom, which provides a for-
mal machine-parsable definition, can be accessed at:
http://www.ontobee.org/ontology/OBIB?iri=http://purl.
obolibrary.org/obo/OMIABIS_0000000.
From these definitions it is obvious that from the per-
spective of OBIB, a biobank consists of both the speci-
mens and the data about the specimens and the specimen
collections. OBIB differentiates between a biobank and a
biobank organization. The latter is defined as:
"An organization bearing legal personality that owns or
administrates at least one biobank."
This differentiation enables concise representation of
an organization running more than one biobank, as is
regularly the case for hospitals, research facilities and
others. So, OBIB fills gaps regarding the representation
of biobanking that have so far existed in OBO Foundry
ontologies and other ontologies.
Due to the fact that OBIB is the result of merging two
independently developed ontologies, coverage is broad.
Naturally, it covers information about specimens and
processes like specimen collection, specimen handling
and storage. It also represents donors and patients and
medical record data pertaining to those. In addition, it
provides representation for clinical studies and specimens
and data accrued during those. In the current release, the
OBIB-generated terms are related to the containment of
the specimen (e.g., OBIB_0000028: collection packet), ad-
ditives (e.g. OBIB_0000022: blood additive role), and stor-
age mechanisms (e.g., OBIB_0000030: blood spot card).
OBIB specific terms were also generated for patient forms
(e.g., OBIB_0000017: data confirm questionnaire) and as-
sociated health-related questions (e.g., OBIB_0000053:
duration time of smoking).
Most terms needed by OBIB are not specific to bio-
banks and available from other OBO ontologies and
imported. Figure 2 shows a selection of the most rele-
vant classes of OBIB and their superclasses. The figure
shows that many classes in OBIB are re-used from other
commonly used ontologies of the biomedical domain,
such as OBI, IAO, and others, or subclasses of those
classes. Notably, some of those are central classes such
as "specimen" and "specimen collection", both re-used
Fig. 2 Selection of central classes of OBIB and their superclasses. The leftmost four BFO classes are subclasses of further BFO classes which are not
shown here for readability
Brochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 Page 5 of 9
from OBI. Other OBI classes, such as "organization" and
"material maintenance" are superclasses for classes
highly relevant to the biobanking domain, such as "bio-
bank organization" and "specimen freezing".
This not only highlights the relevance of OBI for the
representations that are part of OBIB, but also provides
an important opportunity regarding the use of semantic
web technologies in translational research. Terms like
"specimen" are used by multiple ontologies. The National
Center for Biomedical Ontologies' (NCBO) BioPortal [18],
an ontology lookup service, retrieved 18 different repre-
sentations of the term "specimen" in BioPortal ontologies
(when queried Nov. 18, 2015). OBI's representation of
specimen is one of them. However, the only specimen rep-
resentations that are referred to by other ontologies are
the OBI representation (referenced by 11 other ontologies)
and the Semanticscience Integrated Ontology (SIO) [19]
representation (referenced by 1 other ontology). This
shows that the representation of specimen by OBI is by
far the one most widely used by other ontologies. Re-use
of OWL entities (such as "specimen") in multiple ontol-
ogies and in multiple applications using those ontologies
is relevant, since the representation comes with a Unique
Resource Identifier (URI) that allows linking data in RDF.
This is a key strategy of semantic web technology that
holds huge promise for the translational science commu-
nity. Data created for use in a specific domain (e.g. bio-
banking) can be linked easily with data created in another
domain (e.g. digital pathology) by using the same URIs to
refer to the same entities.
Usage of OBIB
At the Penn Medicine Biobank, OBIB is used as the
semantic framework for a search system that supports
cohort identification and deep data mining of the infor-
mation associated with biobank specimens and specimen
donors. We explored how OBIB could be used through
a specific competency question of case/control match-
ing. For cases, we wanted to identify patients who were
consented to the biobank, had a history of type 2 dia-
betes, took a particular statin medication on or before
the date of recruitment, and had a banked EDTA blood
specimen. Eligible controls needed to have type 2 dia-
betes, have no history of taking any statin medications,
and have a banked EDTA specimen. Controls needed to
be matched to cases by gender, age at the time of re-
cruitment, and Body-Mass Index (BMI). The end goal
was an integrated graph database that contained the in-
stances and semantics of our data and could be queried
to answer our competency question, one which exempli-
fied a typical request made about biobank data.
The data needed to answer the competency question
were located in several relational data sources. Diagnosis
and prescription data were stored in relational data
sources derived from the patients' electronic medical rec-
ord. Dates of birth and recruitment, gender, and BMI at
time of recruitment were gathered in case report forms
(CRF) at the time of enrollment. There were several itera-
tions of the CRFs that were stored in databases with differ-
ent table structures. A snapshot of the inventory data was
generated from the specimen inventory system.
The process of building an RDF search system to an-
swer our question was divided into 4 parts: 1) semantic
modeling, 2) data mapping and instantiation, 3) domain
knowledge linking, and 4) querying and testing (Fig. 3).
For semantic modeling, local domain experts and
OBO Foundry ontology experts generated an ontology
model using OBIB that included the portions of OBO
Foundry ontologies relevant to the data sources. In our
case this took the form of several Cmap [20] documents.
Each data source was mapped separately and encapsu-
lated the relational data, the semantics intrinsic in the
relational data and any relevant domain knowledge. An
additional model was created to show how the separate
data sources were related. This model served as a guide
to adding additional data sources and to the naming
convention of International Resource Identifiers shared
between data sources.
During data mapping and instantiation, the ontology
models were referenced to generate a concrete map of
how the relational data would be transformed into triples.
This mapping can be expressed in RDB to RDF Mapping
Language (R2RML) (http://www.w3.org/TR/r2rml), a lan-
guage developed by the World Wide Web Consortium.
There are several software conversion tools that use
R2RML to instantiate relational data as RDF triples. In
our case, we used D2RQ [21] as the conversion software
and a D2RQ specific mapping language that is a derivative
of R2RML.The ontology models were updated as neces-
sary while writing and testing the conversion files.
Domain knowledge linking involved loading the instan-
tiated RDF data and any related OBO ontologies into a
graph database. We used Stardog (http://www.stardog.
com) as our graph database into which we loaded RDF
instance data and OBIB.
Querying and testing involved verifying the instantiated
data were correct and answering the competency ques-
tion. Equivalent queries were generated against the rela-
tional and graph data to ensure the data were accurately
modeled.
Future work
Recent research has shown that in spite of aiming to foster
clear and concise class representation, definitions from on-
tologies do not always rate well with domain experts [22].
This highlights the need for closer collaboration between
ontology curators and domain terminology experts regard-
ing definitions, term descriptions and real life applicability.
Brochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 Page 6 of 9
Therefore, the OBIB developers have started a collabor-
ation with domain experts heavily engaged in the area of
biobank terminology, the Duke Biobank, a consortium
organization within the Duke Translational Research Insti-
tute. The Duke Biobank led the effort to select and imple-
ment a commercial biospecimen information management
system (BIMS) to integrate information from Dukes di-
verse biobanking entities. A priority at the outset of the
selection process was the establishment of the policy that
all biobanks participating in the BIMS must use a common
terminology. To that end, terms were identified and de-
fined through a consensus driven process with biobanking
domain experts across the Duke campus. Sources consid-
ered for terms included publications related to biobanking
terminology as well as related to biobanking pre-analytical
variables, existing data elements in use in the existing bio-
banks, a public comment period, and out-of-the-box terms
from the commercial BIMS, once it was purchased. The
18 month effort overall resulted in over 500 data elements
covering the lifecycle of the biospecimen, as defined by the
National Cancer Institute [23]. In an effort to further ex-
pand and share Dukes work, a collaboration between
Duke and OBIB began in the fall of 2014. Currently the
collaboration is focused on the comparison of terms and
classes between the Duke terminology and OBIB in an
effort to identify intersections and gaps while implement-
ing OBIB classes, with the goal of extending OBIB to cover
the use cases underlying the development at Duke. To
date, this approach has resulted in two different outcomes:
i) A Duke term mapped exactly the OBIB term but the
naming was different (e.g. "collect" in Duke
terminology correspond to "specimen collection"
in OBIB).
ii) Duke term was not present in OBIB (e.g. Duke term
"sample set" and "sample family").
This led to the creation of new terms specifying
already existing classes in OBIB and highlighted the de-
mand for additional general classes in OBIB (e.g.,
OBI_0002080: human specimen set; OBI_0002077: spec-
imens derived from shared ancestor). The aim of this
close interaction between the additional ontology users
(Duke) and OBIB is to create a resource fulfilling the re-
quirements of heterogeneous users. The value of this
collaboration is in joining a robust biobanking termin-
ology developed for a specific institutions use, with a
biobanking ontology created at two other institutions, le-
veraging domain knowledge in both biobanking and
ontology in order to establish a single, relevant ontology
Fig. 3 The process used for building the prototype RDF search system to answer the Penn Medicine Biobank case/control competency question.
1. Semantic Modeling-Ontology models are developed to model the semantics of the relational data and any OBO ontologies that are relevant
to the data sources and potential queries. 2. Data Mapping and Instantiation-The models developed in step 1 are used to write mapping files to
concretely map the relational data as RDF. Software tools to use these maps to instantiate the relational data as RDF data. 3. Domain Knowledge
Linking-The instantiated RDF data and any relevant OBO Foundry Ontologies are loaded into a graph database. 4. Querying and Testing-Queries
over the graph data can be created by referencing the OBIB model. To test, equivalent queries against the graph data and relational data are
constructed and run to ensure data correctness
Brochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 Page 7 of 9
resource with broad coverage to be applied to the field
of biobanking.
Another key aspect of biobanking is related to in-
formed consent and retrieving information about the
consent given by the donors to facilitate research. We
have already pointed out that OBIB already contained a
basic representation of informed consent from the ICO.
However, in order to retrieve the actual content of the
consent it is not sufficient to limit the representation to
the consent documents and the consent process. It will
also be necessary to represent the rights and obligations
that are created through the consenting. We have begun
working with the developers of ICO to address this issue
in the context of biobanking. While the development of
an in depth representation of informed consent for bio-
banking is still ongoing, one strategy that has been iden-
tified as key, is using a pre-existing ontology that allows
the representation of rights and obligations and the
socio-legal processes that give rise to them. The funda-
mental aspects of this have already been addressed by
the Ontology of Document Acts (d-acts) (http://purl.
obofoundry.org/obo/iao/d-acts.owl) [24, 25].
Conclusion
We have seen that for domains as central as biobanking
more than one ontology might exist. In order to foster
semantic integration of data for the largest possible
number of users and consumers, it might be necessary
to merge two or more ontologies.
In this paper we demonstrated that merging ontologies
that share a common design methodology, and that ex-
tend the same Upper Ontology and Reference Ontology
can be done fairly easily and with consistency. Both
OMIABIS and BO adhere to OBO Foundry principles
and were created based on the methodological paradigm
of ontological realism. Both ontologies are extensions of
OBI, which is based on BFO.
We have also demonstrated how the result of the mer-
ger is currently used and allows answering competency
questions based on real-world use cases in the Penn
Medicine Biobank. Finally, we illustrated how OBIB can
serve as a means to capture the semantics and share the
value of terminologies developed for institutional biobanks.
Abbreviations
BBMRI: Biobanking and BioMolecular Research Infrastructure; BFO: basic
formal ontology; BIMS: biospecimen information management system;
BMI: body-mass index; BO: biobank ontology; CARO: common anatomy
reference ontology; ChEBI: chemical entities of biological interest; CL: cell
type ontology; CRF: case report form; EDTA: ethylenediaminetetraacetic acid;
IAO: information artefact ontology; ICO: informed consent ontology;
IRI: International Resource Identifier; MIABIS: minimum information about
biobank data sharing; MIREOT: minimum information to reference an
external ontology term; NaCit: monosodium citrate; NCBO: National Center
for Biomedical Ontologies; OBI: Ontology for Biomedical Investigations;
OBIB: ontology for biobanking; OBO: open biological and biomedical
ontologies; OMIABIS: Ontologized MIABIS; OMRSE: ontology of medically
relevant social entities; OWL: web ontology language; PATO: phenotypic
quality ontology; PNO: proper name ontology; RDF: resource description
framework; REO: reagent ontology; SIO: semanticscience integrated ontology;
SPARQL: SPARQL protocol and RDF query language; URI: unique resource
identifier.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MB participated in the development of OBIB and led the effort of writing the
manuscript. CJS participated in the development of OBIB, coordinated efforts
on its application, and helped write the manuscript. JZ participated the
development of OBIB, took responsibility of OBIB release, contributed to
OBIB applications and manuscript preparation. AMM contributed to the
mapping between Duke terminology and OBIB, the creation of new OBIB
terms, and helped write the manuscript. DB contributed to the mapping
between PMBB terminology and OBIB, the creation of new OBIB terms, OBIB
applications, and helped write the manuscript. HW contributed to the
mapping between PMBB terminology and OBIB, the creation of new OBIB
terms, OBIB application development, and helped write the manuscript. HJE
led the development of the Duke common terminology, contributed to the
mapping between Duke terminology and OBIB, the creation of new OBIB
terms, and helped write the manuscript. All authors approved of the manuscript.
All authors read and approved the final manuscript.
Acknowledgements
The research presented here was partially funded by the UAMS' Translational
Research Institute (TRI), grant UL1TR000039 through the NIH National Center
for Research Resources and National Center for Advancing Translational
Sciences. by the Duke University Center for AIDS Research (CFAR), an NIH
funded program (5P30 AI064518), Dukes CTSA grant UL 1RR024128. The
content is solely the responsibility of the authors and does not necessarily
represent the official views of the NIH.
Author details
1Department of Biomedical Informatics, University of Arkansas for Medical
Sciences, 4301 W. Markham St., #782, Little Rock, AR 72205-7199, USA.
2Department of Genetics, Institute for Translational Medicine and
Therapeutics, Institute for Biomedical Informatics, Perelman School of
Medicine, University of Pennsylvania, Philadelphia, USA. 3Penn Medicine
BioBank, Institute for Translational Medicine and Therapeutics, Perelman
School of Medicine, University of Pennsylvania, Philadelphia, USA.
4Department of Biostatistics and Bioinformatics, Duke Medical Center, Duke
University, Durnham, USA. 5Duke Biobank, Duke Translational Research
Institute, Duke University, Durnham, USA.
Received: 1 December 2015 Accepted: 21 April 2016
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 
DOI 10.1186/s13326-016-0103-z
RESEARCH Open Access
The early bird catches the term:
combining twitter and news data for event
detection and situational awareness
Nicholas Thapen* , Donal Simmie and Chris Hankin
Abstract
Background: Twitter updates now represent an enormous stream of information originating from a wide variety of
formal and informal sources, much of which is relevant to real-world events. They can therefore be highly useful for
event detection and situational awareness applications.
Results: In this paper we apply customised filtering techniques to existing bio-surveillance algorithms to detect
localised spikes in Twitter activity, showing that these correspond to real events with a high level of confidence. We
then develop a methodology to automatically summarise these events, both by providing the tweets which best
describe the event and by linking to highly relevant news articles. This news linkage is accomplished by identifying
terms occurring more frequently in the event tweets than in a baseline of activity for the area concerned, and using
these to search for news. We apply our methods to outbreaks of illness and events strongly affecting sentiment and
are able to detect events verifiable by third party sources and produce high quality summaries.
Conclusions: This study demonstrates linking event detection from Twitter with relevant online news to provide
situational awareness. This builds on the existing studies that focus on Twitter alone, showing that integrating
information from multiple online sources can produce useful analysis.
Keywords: Twitter, Situational awareness, Event detection
Introduction
Updates posted on social media platforms such as Twitter
contain a great deal of information about events in the
physical world, with the majority of topics discussed on
Twitter being news related [1]. Twitter can therefore be
used as an information source in order to detect real
world events. The content and metadata contained in the
tweets can then be leveraged to describe the events and
provide context and situational awareness. Applications
of event detection and summarisation on Twitter have
included the detection of disease outbreaks [2], natural
disasters such as earthquakes [3] and reaction to sporting
events [4].
Using the Twitter stream for event detection yields a
variety of advantages. Normally in order to automatically
detect real-world events a variety of official and media
*Correspondence: nicholas.thapen@imperial.ac.uk
Institute for Security Science and Technology, Imperial College London,
Exhibition Road, London, UK
sources would have to be tracked. These are usually pub-
lished with some lag time, and any system monitoring
them programmatically would require customisation for
each source since they are not formatted in any standard
way. Twitter provides a real-time stream of information
that can be accessed via a single API. In addition a rich
variety of sources publish information to Twitter, since it
is a forum both for the traditional media and for a newer
brand of citizen journalists [5]. Tweets also contain meta-
data that can be mined for information, including location
data, user-supplied hashtags and user profile information
such as follower-friend relationships. The primary draw-
back of using Twitter is that it is an unstructured source
that contains a great deal of noise along with its signal.
Tweets can be inaccurate as a result of rumour, gossip or
active manipulation via spamming.
In this paper we apply existing bio-surveillance algo-
rithms, which are those used to detect outbreaks of illness,
to detect candidate events from the Twitter stream. We
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 2 of 14
employ customised filtering techniques to remove spu-
rious events. We then extract the terms from the event
tweets which best characterise the event and aremost effi-
cacious in retrieving related news. These terms, in the
form of unigrams and bigrams, are used to filter and rank
the most informative tweets for presentation to the user
along with the most relevant news articles. Where the
news articles cover the exact event being discussed on
Twitter they act as direct confirmation and explanation
for the event. Where a Twitter event has not yet been cov-
ered in the news media related background articles can
still provide additional context.
Our techniques are evaluated using two case studies,
both using a dataset of geo-located tweets from England
andWales collected in 2014. The primary case study is the
detection of illness outbreak events. We then generalise
our techniques to events strongly affecting Twitter sen-
timent, such as celebrity deaths and big sports matches.
We evaluate our event detection using ground truth data
in the form of health practitioner and news reports. The
situational awareness techniques are evaluated by com-
parisons to existing term extraction methods and human-
coded event explanations.
Background
Much of the work on event detection using social media
has focused on using topic detection methods to iden-
tify breaking news stories. Streaming document similarity
measures [6], [7] and online incremental clustering [8]
have been shown to be effective for this purpose. These
methods have no concept of location, and focus purely on
picking up distinct events being discussed in the general
stream of Twitter data.
Other approaches have aimed to pick up more localised
events. These have included searching for spatial clus-
ters in tweets [9], leveraging the social network structure
[10], analysing the patterns of communication activity
[11] and identifying significant keywords by their spatial
signature [12].
In the field of disease outbreak detection efforts have
mostly focused on tracking levels of influenza by com-
paring them to the level of self-reported influenza on
Twitter, in studies such as [13] and [14]. Existing dis-
ease outbreak detection algorithms have also been applied
to Twitter data, for example in a case study [15] of
a non-seasonal disease outbreak of Enterohemorrhagic
Escherichia coli (EHEC) in Germany. They searched for
tweets from Germany matching the keyword EHEC, and
used the daily tweet counts as input to their epidemic
detection algorithms. Using this methodology an alert for
the EHEC outbreak was triggered before standard alert-
ing procedures would have detected it. Our study uses a
modified and generalised version of this event detection
approach.
Diaz-Aviles et al. also attempted to summarize out-
break events by selecting the most relevant tweets, using a
customized ranking algorithm. Other studies which have
summarised events on Twitter by selecting the most rele-
vant tweets include [4] and [16]. Analysis of using Twitter
for situational awareness has been carried out in [17]
and [18].
There have been fewer related works on linking or sub-
stantiating events detected from Twitter with traditional
news media. One study [19] analysed various methods
of contextualizing Twitter activities by linking them to
news articles. The methods they examined included find-
ing tweets with explicit URL links to news articles, using
the content of tweets, hashtags and entity recognition.
The best non-URL based strategy that they found was
the comparison of named entities extracted from news
articles using OpenCalais with the content of the tweets.
Methods
Problem definition
Our definition of a real-world event within the context of
Twitter is taken from [8], with the exception that we have
added a concept of event location. We are interested in
only those events that attract discussion on Twitter, since
all others would be invisible to our methods.
Definition 1. (Event) An event is a real-world occurrence
e with (1) an associated time period Te and (2) a time-
ordered stream of Twitter messages Me, of substantial
volume, discussing the occurrence and published during
time Te. The event has a location Le where it took place,
which may be specific or cover a large area, and the mes-
sages have a set of locations LM1,. . . ,LMn which they were
sent from.
From the above definition, when given a time-ordered
stream of Twitter messages M, the event detection prob-
lem is therefore one of identifying the events e1,. . . ,en
that are present in this stream and their associated time
periods Te and messages Me. It is also valuable to iden-
tify the primary location or locations LMi that messages
have originated from, and if possible the event location
Le. The situational awareness problem is one of taking
the time period Te and messages Me and producing an
understandable summary of the event and its context.
Overview
Our approach to the event detection problem incorpo-
rates location by detecting deviations from baseline levels
of tweet activity in specific geographical areas. This allows
us to track the location of messages relating to events, and
in some cases determine the event location itself.
In this paper we focus on two distinct types of event:
 Outbreaks of symptoms of illness, such as coughing
or itching.
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 3 of 14
 Events causing strong emotional reactions, such as
happiness or sadness.
Initially the system was designed with disease outbreak
detection as the primary use case; this led to a system
design focused around keywords and aliases for their
keywords, since a limited range of illness symptoms char-
acterises most common diseases and the vocabulary used
to describe these symptoms is also relatively limited. After
several iterations of this approach we noted that it could
be viable as a general event detection and situational
awareness method, so we added another type of event
to determine the feasibility of the general approach. We
chose events causing strong emotions as a contrasting and
less specific event type, with the intention of picking up
a variety of localisable events such as important football
matches and rock concerts.
For each type of event we define a list of keywords,
each describing a particular sub-type of that event. For
example when looking at illness each keyword relates to
a particular symptom, such as coughing or vomiting.
When looking at events that cause emotional reactions
each keyword relates to a particular emotion. Each of
these sub-type keywords is then expanded with a list of
aliases, synonyms and related terms to form a keyword
group. For more details on how we identified the relevant
keywords and synonyms see the sections below.
We track the number of tweets mentioning each key-
word (consolidating all that lie in the same keyword
group), in each of the geographical areas and use bio-
surveillance algorithms to detect spikes in activity. Each
spike is treated as a potential event, and we use vari-
ous criteria to single out those with a high probability
of being actual events as defined above, i.e. those that
are caused by discussion of real-world occurrences on
Twitter.
Our situational awareness approach is based on iden-
tifying terms from the event tweets which characterise
the events and using them to retrieve relevant news arti-
cles and identify the most informative tweets. The news
search uses metrics based on cosine similarity to ensure
that searches return related groups of articles.
Architecture
The general approach can be described by the architecture
in Fig. 1. Every new event type requires a list of keywords
and their associated aliases. Optionally a specific data pre-
processing step can be included for the event type. For
example in the health symptom case we employ amachine
learning classifier to remove noise (those tweets not actu-
ally concerning health). These are the only two aspects
of the design that need to be altered to provide event
detection and situational awareness to a new problem
domain.
Fig. 1 Event Detection and Situational Awareness architecture: To apply to a new example a user needs to provide a keyword group list and
optionally a noise filter to remove tweets that do not strictly match the criteria of interest
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 4 of 14
Event types
We now go into a more detailed explanation of our event
types and how we formulated the keywords and associ-
ated aliases. Each keyword group consists of a primary
keyword which is used to identify the group, e.g. vomit,
and a number of aliases that expand the group, e.g. throw-
ing up, being sick, etc. see Tables 2 and 3 for the full list of
keyword groups.
Illness symptoms
To build up a list of symptoms and related keywords
we searched Freebase for /medicine/symptom. Each
of these symptoms is defined as a keyword. They are
returned with a list of aliases that are then associated with
that keyword. This returned around 2000 symptoms. In
order to filter down to a more manageable number we
next filtered these symptoms by their frequency in the
Twitter data; any symptoms not appearing frequently in
this data would not produce enough activity to gener-
ate events for analysis. All symptoms with fewer than 10
mentions in the Twitter data were removed from the can-
didate list. This excluded a large proportion of symptoms,
reducing the set to around 200.
We further limited the set by removing symptoms
not related to infectious diseases. We also added pri-
mary keywords and aliases for some common condi-
tions such as hayfever and flu. This step resulted in a
reduction to the 46 symptoms which formed our search.
The average number of aliases per primary keyword
was 3.8.
Emotion states
For a list of emotion states and associated keywords we
used the work of Shaver et al. They conducted research
[20] to determine which sets of words were linked to
emotions and how these cluster together. We took the
six basic emotions identified in the work as primary key-
words: love, joy, surprise, sadness, anger and fear. Shavers
work associated each of these with a list of terms to form
a tree. We took the terms from lower leaves on the tree for
each emotion as our alias sets (see Table 1 for examples).
The average number of aliases per primary emotion key-
word was 7.3. The only alteration we made was that after
some initial analysis we discovered that the term happy
from the joy category was a very strong signal of special
Table 1 Selected emotion keyword groups and some of their
aliases
Keyword Aliases
Surprise Amazed, astonished, surprised...
Sadness Depressed, unhappy, crying...
Joy Glad, delighted, pleased...
events such as Valentines Day, Mothers Day and Easter.
It was also very often used on a daily basis due to peo-
ple offering birthday greetings. We therefore separated
happy into its own category separate from joy.
In addition we employed SentiStrength [21], a sentiment
analysis tool, to classify our tweets into positive and nega-
tive emotional sentiments.We took those tweets classified
as being very positive and very negative as additional
categories.
Data collection
Using Twitters live streaming API we collected geo-
tagged tweets between 11th February 2014 and 11th
October 2014. Tweets were collected from within a geo-
graphical bounding box containing England and Wales.
Retweets were excluded due to our focus on tweets as pri-
mary reports or reactions to events. This resulted in a
data-set of 95,852,214 tweets from 1,230,015 users. 1.6 %
of users geo-tag their tweets [22], so our data is a lim-
ited sample of the total tweet volume from England and
Wales during this period.We chose to use only geo-tagged
tweets since they contain metadata giving an accurate
location for the user. This allows us to locate each tweet
within our geographical model. In total we found 240,928
matches for our symptom keywords in the set of tweets
classified as health-related, and 20,570,753 matches for
our emotion keywords. See Tables 2 and 3 for details.
Location assignment
Ourmethodology relies on the collection of baseline levels
of tweet activity in an area, so that alarms can be triggered
when this activity increases. We therefore amalgamated
the fine-grained location information from the geo-coded
tweets by assigning them to broader geographical areas.
We used a data driven approach to generate the geograph-
ical areas rather than using administrative areas such as
towns or counties. This technique allowed us to select
only those areas with a minimum level of tweet activity,
and also did not require any additional map data. It would
therefore be be reusable for any region or country with a
sufficient level of Twitter usage.
We began by viewing a sample of the collected tweets
as geo-spatial points. Viewed on a map these clearly
clustered in the densely populated areas of England and
Wales. We therefore decided to use a clustering algo-
rithm on these points in order to separate out areas for
study. We employed the Density-Based Spatial Cluster-
ing of Applications with Noise (DBSCAN) algorithm [23]
for clustering, as this does not require a priori knowledge
of the number of clusters in the data. The features pro-
vided to DBSCANwere the latitudes and longitudes of the
tweets.
The clusters produced by the algorithm matched the
most populated areas, corresponding to the largest cities
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 5 of 14
Table 2 Tweets matching each symptom keyword group
Symptom Number of tweets
Headache 42947
Vomit 30429
Hayfever 24175
Sore throat 21744
Pain 15142
Malaise 12354
Flu 10913
Cough 9589
Tonsillitis 7283
Common cold 6768
Infection 5955
Abdominal pain 5582
Sneeze 5131
Asthma 4457
Shortness of breath 4037
Earache 2990
Nasal congestion 2930
Tremor 2727
Itch 2410
Anxiety 2250
Fever 2198
Nosebleed 1971
Faint 1944
Skin rash 1633
Cramp 1444
Diarrhea 1365
Chest pain 1293
Swollen gland 1138
Conjunctivitis 941
Stinging sensation 891
Bleeding 854
Chickenpox 835
Runny nose 785
Swelling 692
Meningitis 641
Pneumonia 622
Seizure 413
Constipation 389
Palpitation 360
Norovirus 239
Neck pain 203
Scarlet fever 142
Dehydration 68
Dysentery 28
Tearing 16
Dry mouth 10
Table 3 Tweets matching each emotion keyword group
Emotion Number of tweets
Very negative 5716797
Love 4943706
Very positive 3823994
Joy 2129279
Happy 1613447
Anger 1228890
Sadness 562193
Fear 395770
Surprise 156677
in the UK as shown in Fig. 2. They also separated most
cities into distinct clusters (a notable exception being the
conglomeration of Liverpool and Manchester). In total 39
clusters were created for England andWales and each was
given an ID and a label. We then created a convex hull
around each cluster, providing a polygon that can be used
to check whether a point is in the cluster or outside it.
Points outside all of the clusters were assigned to a special
noise cluster, and not included in the analysis. Overall
80 % of tweets were assigned to specific clusters and the
remainder to noise, giving us good coverage of geo-tagged
tweets using our cluster areas.
Tweet processing
As tweets are received by our system they are processed
and assigned to the symptom and emotion state classes
if they contain one of the relevant keywords. They are
assigned a location by checking whether they fall into one
of our cluster areas.
For the illness symptoms we introduce a noise removal
stage at this point. It is particularly relevant for this class
of events because there are many fewer tweets relating
to illness than showing emotion states. This means that
the signal is more easily blocked out by random noise.
To remove noise we construct a machine learning classi-
fier with the aim of removing tweets containing alterna-
tive word usages or general illness discussion rather than
reporting of illness events. The classifier therefore classi-
fies tweets into those that are self-reports of illness and
those that are not. The classifier we use is a linear SVM
trained on a semi-supervised cascading training set, cre-
ated on the principles described in Sadilek et al. [24]. Our
classifier uses the LibSVM [25] library, and was initially
trained on 4600 manually classified tweets. It achieves a
classification accuracy of 96.1 % on a held out test set of
920 manually classified tweets.
The number of tweets assigned to each class in each area
are then saved on a daily basis. These counts are first nor-
malised to take account of Twitters daily effect pattern,
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 6 of 14
Fig. 2 UK population density (left) compared to a sample of geo-located tweets (centre) and the clusters found (right). Note that only clusters
located in England and Wales were used in this study. Contains Ordnance Survey data c Crown copyright and database right, CC BY-SA 3.0, https://
commons.wikimedia.org/w/index.php?curid=26070175
which shows more tweeting on weekends than weekdays.
Event detection is run daily since we are attempting to
pick up temporally coarse-grained events. Disease out-
breaks take weeks to develop, and events that shift public
sentiment or emotion will generally take hours or days to
unfold.
Detecting events
Our event detection methodology leverages considerable
existing syndromic surveillance research by using an algo-
rithm designed and developed by the Centers for Disease
Control and Prevention (CDC), the Early Aberration
Reporting System (EARS) [26].
Definition 2. (Alarm) An alarm is an alert produced by
the first stage of our event detection system. The alarm
has an associated keyword group and location. It also has
a start and end date, and associated tweet counts for each
date within this period. When certain criteria are met an
alarm is deemed to be an event.
We employ the C2 and C3 variants of EARS. These algo-
rithms operate on a time series of count data, which in our
case is a count of daily symptomatic tweet activity. The C2
algorithm uses a sliding seven day baseline, and signals an
alarm for a time t when the difference between the actual
count at t and the moving average at t exceeds 3 standard
deviations. The C3 algorithm is based on C2, and in effect
triggers when there have beenmultiple C2 alarms over the
previous 3 days.
These C2 and C3 candidate alarms are then grouped
together so that alarms for the same keyword group and
area on consecutive days are treated as a single alarm. An
alarm is therefore made up of one or more days, each with
an observed count of tweets. An alarm ends when the C2
and C3 algorithms no longer signal an outbreak occurring.
Some of our Twitter count time series data is zero-
skewed and non-normal, since the number of geo-tagged
users reporting illness can be low. The number of stan-
dard deviations from the mean used in the C2 and C3
algorithms can be an unreliable measure of central ten-
dency in those circumstances. Hence to determine how
far above general baseline activity an observed count
is we employ the median of the series to date and its
Median Absolute Deviation (MAD) to produce a new
metric of alarm severity. Here the series is defined as all
of the previous observed counts for the keyword group
and location in question. The number of Median Abso-
lute Deviations from the median, ?, gives a comparable
figure across alarms as to how sharp a rise has been over
expected levels. This figure is produced from the following
equation:
? = (observation ? median)/MAD (1)
We then find the highest metric for an alarm, ?max,
by finding the highest value of ? within the observations
making up the alarm.
?max = argmax
?
(observations in alarm) (2)
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 7 of 14
The ?max is the primary statistic which we use to deter-
mine which events are real and which have just been
generated by random noise.
Another statistic which we employ in order to filter out
noise is the tweet-user ratio. This is the ratio of tweets in
an event to that of distinct users involved in an event. A
high value of this statistic would imply that some users
have tweeted a large number of times across a short time
period, which is an indication that they may be spammers
and that the alarm is spurious.
In summary, we use the output from EARS to produce
alarms. We filter the alarms to a set of high likelihood
events by using the ?max and tweet-user ratio parameters.
From this point we refer to those alarms that are high-
likelihood as events, according to our earlier event defi-
nition. The alarms have an associated stream of Twitter
messages and a location given by the node which they
occur in. The following situational awareness results show
that the Twitter messages in these alarms discuss real-
world occurrences, therefore fulfilling all of our definition.
Situational awareness
Once an event has been identified our next objective is to
automatically provide additional context for it, which may
provide an explanation of the underlying cause. A human
interpreter could achieve this by reading all of the tweets
and synthesizing them into a textual explanation, which
might be some text such as People reacting to the death
of Robin Williams. We do this in two main ways: by pro-
viding themost representative tweets from those that trig-
gered the alarm, and by linking to relevant news articles.
The steps involved in the Terms, News and Tweets (TNT)
Event Summarisation process are detailed in Algorithm 1.
The steps and terminology are then explained in more
detail.
1© The first step is to retrieve the relevant tweets from
the processed tweet and alarm databases. Tweets are
fetched for both the alarm gist and from a historical base-
line. 3©We discard those events with fewer than 30 tweets
as we found that they did not contain sufficient data to
produce good summarisation results.
Definition 3. (Gist) The gist consists of the tweets for the
time period of the event which match the events keyword
group and area.
Definition 4. (Baseline) The baseline consists of the
tweets for the same keyword group and area as an event
from the 28 days prior to that event.
5© The next task is to find unigrams and bigrams that
are more prevalent in the gist than in the baseline. These
are likely to come from tweets discussing the event and
will thus be characteristic of the event. We first extract the
most common unigrams and bigrams from both sets of
tweets, after removal of stopwords. Our list of stopwords
Algorithm 1 Terms, News and Tweets (TNT) Event Sum-
marisation
1© Fetch gist tweets and baseline tweets
2© if ??gist tweets?? < 30 then
3© Do not attempt to summarise event
4© else
5© Extract unigrams and bigrams appearing in at
least 5% of the gist tweets
6© for all ngrams extracted do
7© Perform Fishers Exact Test to determine
whether ngram is significantly more likely to appear
in gist than baseline
8© for Top 2 most significant unigrams and bigrams
and the primary keyword do
9© Search news database using ngram for the
alarms date range and return the top 10 documents
10© Compute PCSS for documents returned
11© for ngrams with PCSS values above threshold do
12© Compute title similarity PCSS between ngram
documents and those for each other ngram
13© Good search terms ? term with title similar-
ity PCSS above threshold
14© Good articles ? documents returned from good
search terms
15© Filtered tweets? tweets containing a good search
term
16© Rank good articles by cosine similarity to average
vector of good news articles
17© Rank filtered tweets by cosine similarity to average
vector of filtered tweets
includes a standard list, plus the 200 most frequent words
from our tweet database.We select all non-stopwords that
appear in at least 5 % of the tweets.
7© We then do a Fishers Exact Test to determine which
of the common unigrams and bigrams in the gist appear
significantly more frequently (? < 0.05) here than in the
baseline set. Our candidate terms are the top two most
significant unigrams and bigrams. We select the top two
as this was found to give the best results on our test
examples. To this set we append the primary keyword that
triggered the alarm.
9© For this research Google was used as the news
database. Using the candidate terms we perform a search
on Google for documents published in the United King-
dom during the time period of the alarm. Due to Googles
Terms of Service this step was performedmanually. A fully
automated system would replace this step with a search of
a news database, which could be created by pulling down
news articles from RSS feeds of major content providers.
10© We take the first 10 documents retrieved for each
search term, remove stopwords and apply stemming using
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 8 of 14
a Lancaster stemmer. We then convert each document
into a Term Frequency/Inverse Document Frequency
(TF/IDF) vector. In order to determine whether the search
term has retrieved a coherent set of related documents we
define a metric based on cosine similarity, the Pairwise
Cosine Similarity Score (PCSS):
 The Pairwise Cosine Similarity Score of a group of
TF/IDF vectors is calculated by taking the cosine
similarity between each pair of vectors and adding
them to a set. The standard deviation of this set is
subtracted from its mean to form a score.
The PCSS rewards articles which are similar and
penalises any variance across those article similarities.
This reduces the effect of some articles being strongly
related in the document set and others being highly unre-
lated. Any term which retrieves a set of documents with a
score below a threshold value is not considered further.
It is possible for a search term to hit on a coherent
set of documents purely by chance, perhaps by find-
ing news articles related to another event in a different
part of the world. In order to guard against this we
institute another check to ensure that the set of docu-
ments returned from a search term is sufficiently closely
related to the set returned from at least one other search
term.
12© In order to perform this check we compare the titles
of the articles returned from the two different searches
using a similar process to our earlier document compar-
ison. We found it more effective to compare titles than
whole documents, since sets of documents with similar
topics can contain similar language even for fairly unre-
lated search terms. For example the terms ebola and
flu will both return health-related documents contain-
ing similar language, but we would not wish to say that
these search terms are related. To convert the titles to
TF/IDF vectors we remove stopwords but do not apply
stemming. Since the titles are so short we include all uni-
grams, bigrams and trigrams in the vector representation.
We then compute a PCSS between the two document
sets, pairing each document in the first set with each in
the second and vice versa. 13© A search term must be
related to at least one other term for it to be used going
forward.
14© Once TNT has identified good search terms we then
return the news articles fetched using those terms. 16© In
order to rank the top news articles for a search we take
the mean TF/IDF vector of the articles. and then rank the
articles by cosine similarity to this mean vector. We return
the top ranked articles from each search term.
17© To select the summary tweets for an event we firstly
determine the set of tweets to consider and then choose
the most relevant tweets within that set. The set of tweets
can either be:
 1) All tweets in the gist.
 2) The gist tweets containing one of the extracted
terms.
 3) The gist tweets containing one of the good search
terms (as determined by the TNT algorithm).
1) is always available and is labelled the Gist Top Tweets
(GTT). If the TNT algorithm has found terms that are
significantly different in frequency from the baseline then
set 2) is available for use and if terms from that set have
good newsmatches then set 3) can be used. The Summary
Top Tweets (STT) are from set 3) if it exists and fallback
to set 2) if the good news match terms are not available. If
no terms were found to be significantly different from the
baseline then only the GTT is available.
In order to choose the top tweets we rank them by their
cosine similarity to the mean TF/IDF vector of all tweets
in the set, an approach similar to that of [4]. This attempts
to finds tweets which capture and summarise the aggre-
gate information of all of those in the set. The top five
tweets ranked by this measure are returned.
Results and discussion
Candidate event selection
Over the course of the study the bio-surveillance algo-
rithms generated 820 disease-related alarms and 2021
emotion-related alarms. A brief survey of these revealed
that many were false alarms generated by random fluctu-
ations in the noisy social media data. In order to separate
out alarms that could be labelled as events with high
confidence we conducted the following analysis.
Firstly we compiled an initial set of 13 focus example
alarms. These were taken from events that the authors
knew had happened in the evaluation time period and
from those alarms in our dataset with low and high values
of ?max.
The most important threshold parameter in the context
of the event detection is the ?max figure which mea-
sures the deviation of the alarm counts from the median
level. Examining the distribution of the number of alarms
for each value of ?max revealed that it started to tail off
sharply at ?max ? 5. The distribution of alarms for each
value of ?max is shown in Fig. 3. We therefore took this as
a value to segment additional test examples, drawing ten
more at random with a ?max less than 5 and ten with a
?max greater than or equal to 5. The resulting evaluation
set of 33 candidate events is shown in Table 4. The event
ID used to refer to the events is composed of the first two
letters of the event keyword followed by a 12 letter area
code. The final part of the ID is the day and month of the
event start date.
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 9 of 14
Fig. 3 Alarms detected with differing values of ?max
Event detection evaluation
It is difficult to provide a completely automated evaluation
procedure for detecting previously unknown events. Diaz
et al. used the time to detection on a known outbreak as
their evaluation criterion [15]. In our case we do not know
a priori that these are genuine outbreaks or events. Hence
we need to make an assessment of the alarms produced
to see what they refer to and if there is a way of exter-
nally verifying that they are genuine events. For all 33 of
the selected alarms the authors read the tweets and deter-
mined whether they described a real world event. The
coders found 26 YES answers, 5 NO answers and 2 DIS-
AGREED answers, producing a 94 % agreement. Where
an event was present they wrote a short summary.
For external verification of events two different meth-
ods were used, depending on whether the event was
symptom-related or emotion-based. For symptom related
events the activity spike was checked against official
sources for the same time period. The General Prac-
titioner (GP) in hours bulletin for England and Wales
[27] was used and an event was deemed verified if the
symptom exhibited an increasing trend for that period.
Table 4 Evaluation set of events
ID Event ?max Keyword Node ID Event ?max Keyword Node
SAL-11-08 YES 20 Sadness London HFB-10-04 YES 5 Hayfever Birmingham
HFM-01-06 YES 19 Hayfever Manchester VOL-20-04 YES 5 Vomit London
SAL-07-04 YES 14 Sadness London SAC-05-05 YES 5 Sadness Cardiff
FEL-18-07 YES 13 Fear London HFL-04-07 NO 5 Hayfever London
ASL-02-04 YES 12 Asthma London FLB-23-09 NOa 5 Flu Birmingham
FLP-06-10 YES 11 Flu Portsmouth VPBR-10-05 YES 4 VeryPos Bristol
HAM-02-04 YES 9 Happy Manchester FRL-30-05 YES 4 Fever London
HAM-18-04 YES 9 Happy Manchester FLM-19-09 YES 4 Flu Manchester
SAL-08-07 YES 8 Sadness London VOL-22-02 NO 3 Vomit London
HALE-01-08 YES 8 Happy Leeds HFB-29-04 NO 3 Hayfever Birmingham
HFL-14-05 YES 7 Hayfever Leeds JONO-23-02 YES 2 Joy Norwich
SUN-29-08 YES 7 Surprise Newcastle HEM-06-03 NO 2 Headache Manchester
ITL-08-06 YES 6 Itch London SUC-23-05 NO 2 Surprise Cardiff
SAB-09-06 YES 6 Sadness Birmingham SUL-16-08 NO 1 Surprise London
HABE-01-03 YES 5 Happy Bridgend FEBR-17-04 NO 0 Fear Bristol
SAL-21-03 YES 5 Sadness London STL-26-08 NO 0 Sore Throat London
HFC-09-04 YES 5 Hayfever Cardiff
Shows whether events were externally verifiable and their ?max value
aNote: this event not confirmed by the GP in hours report of that week. However, the following week showed an increase and it is possible that social media detected
increased Influenza activity before this was confirmed by GP visits
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 10 of 14
This detail is noted in the summary document pro-
duced by Public Health England for that reporting period.
Emotion-based events were verified by checking if there
were any articles (via Web search) that could corroborate
the cause of the event (as given by the summary).
We manually investigated all examples from the initial
focus set and found initial parameters for the score func-
tions in our algorithms that worked reasonably well. These
provided possible ranges of values which were evaluated
more systematically over the entire alarm set. For event
detection we evaluated which alarms were flagged as
events by the system for each parameter value against
whether those events were externally verifiable. The final
evaluation for all algorithms contains all 33 of the alarms
in both sets, not just the twenty expanded test examples.
To determine if an alarm is an event that we should be
concerned about we consider two properties of the alarm.
The first is the tweet-user ratio. From exploratory testing
we found a value of 1.5 separated our spam and genuine
alarms very well, leaving only a small number of alarms
with large tweet sets and some spam. The spam detection
problem should be straightforward and will be addressed
more completely in future work.
The second figure which gives the strength of the activ-
ity above the usual baseline is the ?max figure. This is
the essence of the modified EARS algorithm and the
value of this figure should generally separate events from
non-events.
The criterion for selecting the best threshold for ?max
is context dependent. We have used the balanced measure
for this scenario as that is a fair representation of both pre-
cision and recall. For each threshold value of ?max tested
the classification success and error types are:
 True positive: instances at or above the threshold
that are verified events
 False positive: instances at or above the threshold
that are not verified events
 True negative: instances below the threshold that
are not verified events
 False negative: instances below the threshold that
are verified events
The precision, recall and F1 values for all the tested
values of ?max are displayed in Fig. 4. All figures were cal-
culated with reference to the set of 33 example events dis-
cussed above. The maximum F1 value, 0.9362, is observed
at ?max ? 4, so this is a well balanced threshold and
the recommended parameter. Those seeking higher con-
fidence events (willing to accept that some events may be
missed) could use a value of 6 for this parameter which
yields a precision of 1. The maximum observed recall
value is at the minimum parameter value and is not very
informative. Essentially it says that everything is an event
and hence does not produce any false negatives.
In summary the event detection mechanism based on
the EARS C2 and C3 algorithms with the addition of the
?max and tweet-user ratio was found to perform well at
detecting events that could be externally verified as gen-
uine. The recommended ?max parameter (4) produced a
good balance of precision and recall in our sample set. It
must be noted however that we cannot gain a true picture
of the overall recall of the system, since we have no way
of analysing the number of genuine events that were not
picked up.
Situational awareness evaluation
Both situational awareness components were evaluated.
Firstly the news linkage was tested to see whether rele-
vant news was retrieved for the sample events. As part of
this analysis we compared our method of extracting infor-
mative search terms (the TNT algorithm) with a compa-
rable automated technique. Secondly the tweet ranking
was validated to determine whether highly ranked tweets
effectively summarised the events.
Comparative news linkage evaluation
The news linkage component works by selecting good
search terms for articles based on the TNT algorithm.
Within this there is a term extraction step to generate
search terms, and then a filtering step using PCSS to
remove terms which retrieve unrelated sets of articles.
We iterate over different threshold values for the PCSS
score to find the optimum, using an F0.5 measure as
the evaluation criterion. F0.5 was selected because preci-
sion was judged to be more important than recall in this
setting. As a further evaluation we compare the results
of replacing our term extraction algorithm with Latent
Dirichlet Allocation (LDA). LDA is a popular topic mod-
elling technique that extracts sets of terms characterising
each topic in a group of documents. The success and error
types used to compute the F0.5 measure are:
 True positive: relevant news returned for
newsworthy event
 False positive: news returned for an event with no
genuine news
 True negative: no news returned for an event with
no genuine news
 False negative: no news returned for newsworthy
event
The evaluation is presented in Figs. 5 and 6 as well as
the different levels of article PCSS that were iterated over
to find the maximum F0.5 value in a step-wise procedure.
It is clear from these images that the TNT algorithm has a
higher F0.5 at all tested values of the article PCSS, due to
its higher recall. The outcome of the parameter selection
process was that a PCSS threshold of ?0.08 produced the
best results. Using this value the F0.5 was 0.79, showing
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 11 of 14
Fig. 4 ?max event detection parameter selection
that our system was successful in retrieving relevant news
for the sample events.
Selecting top ranked relevant news articles is one part of
our situational awareness contribution. The second is the
selection of tweets that provide a representative summary
of an event.
Top ranked tweets evaluation
We have employed two evaluations for the tweet ranking
exercise: comparison to human-coded event explanation
and comparison between GTT and STT. The human-
coded event explanations were created by two of the
authors after reading through all of the tweets linked to
each event. There were 26 alarms that had an identifiable
cause. The tweet ranking match (to human-coded event
assessment) performance is presented in Table 5. The
tweets were considered a full match if a human summary
of the 5 top ranked tweets would match the human-coded
event explanation for the whole set of tweets.
The partial matches were: FRL-30-05 (Fever: London,
May) and FLP-06-10 (Flu: Birmingham, October).
These events had more than one explanatory cause. Cur-
rently our algorithms work best in the single event case.
The three cases that did not match were: JONO-23-02
(Joy: Norwich, February), STL-26-08 (Sore throat:
London, August) and SUN-29-08 (Surprise, Newcastle,
August). The coders disagreed as to whether STL-26-08
was actually an event. The remaining two examples were
Fig. 5 News linkage accuracy from Terms, News, Tweets terms
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 12 of 14
Fig. 6 News linkage accuracy from Latent Dirichlet Allocation terms
not summarised well by the significant tweets as they both
exhibited high disparity in terms used to describe a con-
textually related event and SUN-29-08 also included a
number of spam tweets that distorted the results of TNT.
The second evaluation for the tweet ranking exercise
was a comparison between the GTT and the STT. A qual-
itative assessment of the tweets led to the conclusion that
STT tweets were better in 11 out of 33 cases and there
was no significant difference between the two for 21 cases
out of 33. In one case, FLP-06-10, the GTT included a
mention of flu jab (one of the manually selected terms)
which the STT did not include. Hence the STT provided
an improvement over ranking based off the alarm tweets
in one out of three instances.
Notable examples discussion
We now discuss four example events that highlight the
strengths and limitations of our approach. These examples
are listed in Table 6.
The first example case is JONO-23-02. From a read-
ing of the tweets there were definitely some relating to a
Table 5 STT tweet ranking evaluation
Match Count
Full 21
Partial 2
No 3
The STT tweet summary fully matched the human-coded event summarisation in
21 cases. This yields a full match fraction of 0.81
single event: Norwich City Football Club beating Totten-
hamHotspur Football Club 1?0 in a football match. Both
TNT and LDA term extraction failed to find terms repre-
sentative of this event. This was due to the disparity of the
language used; the following example tweets should help
elucidate this point:
 #canarycall absolutely delighted with the win :) good
performance, good result
 #yellows almost didnt go today glad i did
 so glad i chose to come today!#ncfc
It is difficult for a term-based solution to find any com-
mon thread here. Finding the cause of this event would
require contextual knowledge of football matches, team
names and commonly employed aliases. The news linkage
algorithm did initially find a news story for the term joy
on this date. The British Prime Minister let out a little cry
of joy over David Bowie Scottish independence comments
(Telegraph, Feb 24, 2014). The articles returned all con-
cerned this story and were found to be closely related, but
were dropped from the news linkage because they did not
match those returned from the other search terms. This
highlights the benefits of searching with multiple terms
and ensuring that the results are related.
The second example is ASL-02-04. This event was
due to increased levels of air pollution observed in Lon-
don at the beginning of April, caused by a Saharan dust
cloud. This event had a ?max of 12 indicating a signifi-
cant increase in baseline activity for the alert period. It was
well summarised by all aspects of our situational aware-
ness algorithm. The top ranked tweets provided by our
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 13 of 14
Table 6 Example cases and the terms extracted for them
ID TNT terms LDA terms
JONO-23-02 Joy, enjoy Enjoy, glad, loss
ASL-02-04 Asthma, air pollution, smog, pollution Asthma, smog, pollution, attack air
VOL-20-04 Vomit, chocolate, easter Chocolate, eaten, easter, vomit, headache
SAL-11-08 Sadness, robin williams, sad news, robin, williams Sad, robin, williams, rip, riprobinwilliams
summary method (STT) produced tweets more represen-
tative of the event than those from all tweets in the gist.
This is demonstrated by the top tweet selected by both:
 STT top tweet: i cant breathe #asthma #smog
 GTT top tweet: my asthma is literally so bad
Here selecting the top tweets from the filtered event
set captures tweets representative of the event as opposed
to the baseline illness activity. The news linkage for this
example worked well, with all five of the top selected
articles being representative of the event. The top article,
Air pollution reaches high levels in parts of England -
BBC, gives the cause of the event in the first few lines:
People with health problems have been warned to take
particular care because of the pollution - a mix of local
emissions and dust from the Sahara.
The third case is VOL-20-04. Reading the tweets
makes it clear that this one day event is caused by peo-
ple feeling sick after eating too much chocolate on Easter
Sunday. In this case the TNT summary and all tweet
ranking return similar tweets as there is little baseline
activity and that baseline activity is not strongly related.
The top tweets from both sets therefore both produce
good summaries:
 STT top tweet: seriously i feel sick having all this
chocolate
 GTT top tweet: eaten too much chocolate feel sick
While the top ranked tweets are similar the event tweet
filtering does remove baseline tweets referring to general
illness. No good news searches were found in this case.
This event may be valid in the context of social media but
it is not newsworthy.
The fourth example is SAL-11-08 which is the UK
Twitter reaction to the death of Robin Williams. These
tweets from the sadness keyword group exhibit both the
highest ?max (20) and the highest overall tweet count
for any single event (4472). The prominence of celebrity
deaths within our detected events mirrors earlier find-
ings [6]. As with all of our high ?max events the TNT
tweet ranking and news linkage work well. The top news
article returned is an article reporting the death of Mr.
Williams: Robin Williams dies aged 63 in suspected sui-
cide (Telegraph, August 12, 2014). The top five ranked
tweets by TNT tweet filtering are better than those ranked
on all tweets as they remove baseline general sadness
tweets from the ranking:
 STT top tweet: rip robin williams. sad day
 GTT top tweet: yep , very sad
Conclusions
We have presented techniques for event detection and sit-
uational awareness based on Twitter data. We have shown
that they are robust and generalisable to different event
classes. New event classes could be added to this system
simply by producing a list of keywords of interest and
an optional noise filter. Our event detection is based on
the EARS bio-surveillance algorithm with a novel filtering
mechanism. The maximum Median Absolute Deviations
from the median provides a robust statistic for determin-
ing the strength of relative spikes in count-based time
series. As it is based on the median, this measure handles
cases where data is non-normal as was the case for some of
our symptom based geo-tagged tweets. The event detec-
tion approach achieved an F1 score of 0.9362 on our event
examples.
By filtering to words that are significantly different (? <
0.05) in frequency from baseline levels we have extracted
terms to search news sources for related articles. Where
good news matches are found these revise our event
term list. We have created two novel algorithms that pro-
vide additional situational awareness about an event from
these event terms. The baseline tweet activity thus pro-
vides valuable context in allowing the character of the
detected event to be discerned.
Firstly, we rank the filtered set of news articles to pro-
duce the top five representative articles. The news linkage,
weighted towards precision, achieved an F0.5 score of 0.79
on our example set, with no false positives.
Secondly, we produce a top five ranked list of tweets
that summarise an event. These ranked tweets are cal-
culated from the tweet set, filtered by those that contain
the extracted event terms. The top ranked tweets fully
matched our human-coded event summaries in 21 out of
26 cases.
In future work we aim to improve our news linkage
algorithm with a final step checking whether the arti-
cles returned are similar to the event tweets, using cosine
Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 14 of 14
similarity or other features such as entities identified
in the news articles. Additional improvements to event
detection would lie in improving spam detection and
adding sentiment classification to our emotion example
as a classifier. Collecting data over longer time periods
would also allow us to look into using bio-surveillance
algorithms which require seasonal baseline information.
Acknowledgements
This research was carried out in cooperation with the UK Defence Science and
Technology Laboratory. It was funded by the U.S. Department of Defenses
Defense Threat Reduction Agency (DTRA), through contract
HDTRA1-12-D-0003-0010.
Authors contributions
NT and DS jointly undertook the data collection, statistical analysis and writing
the manuscript. CH conceived of the study, and participated in its design and
coordination as well as helping to draft the manuscript. All authors read and
approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Received: 2 December 2015 Accepted: 20 September 2016
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 
DOI 10.1186/s13326-016-0091-z
RESEARCH Open Access
Supporting the analysis of ontology
evolution processes through the combination
of static and dynamic scaling functions in
OQuaRE
Astrid Duque-Ramos1, Manuel Quesada-Martínez1, Miguela Iniesta-Moreno1,
Jesualdo Tomás Fernández-Breis1* and Robert Stevens2
Abstract
Background: The biomedical community has now developed a significant number of ontologies. The curation of
biomedical ontologies is a complex task and biomedical ontologies evolve rapidly, so new versions are regularly and
frequently published in ontology repositories. This has the implication of there being a high number of ontology
versions over a short time span. Given this level of activity, ontology designers need to be supported in the effective
management of the evolution of biomedical ontologies as the different changes may affect the engineering and
quality of the ontology. This is why there is a need for methods that contribute to the analysis of the effects of
changes and evolution of ontologies.
Results: In this paper we approach this issue from the ontology quality perspective. In previous work we have
developed an ontology evaluation framework based on quantitative metrics, called OQuaRE. Here, OQuaRE is used as
a core component in a method that enables the analysis of the different versions of biomedical ontologies using the
quality dimensions included in OQuaRE. Moreover, we describe and use two scales for evaluating the changes
between the versions of a given ontology. The first one is the static scale used in OQuaRE and the second one is a
new, dynamic scale, based on the observed values of the quality metrics of a corpus defined by all the versions of a
given ontology (life-cycle). In this work we explain how OQuaRE can be adapted for understanding the evolution of
ontologies. Its use has been illustrated with the ontology of bioinformatics operations, types of data, formats, and
topics (EDAM).
Conclusions: The two scales included in OQuaRE provide complementary information about the evolution of the
ontologies. The application of the static scale, which is the original OQuaRE scale, to the versions of the EDAM
ontology reveals a design based on good ontological engineering principles. The application of the dynamic scale has
enabled a more detailed analysis of the evolution of the ontology, measured through differences between versions.
The statistics of change based on the OQuaRE quality scoresmake possible to identify key versions where some
changes in the engineering of the ontology triggered a change from the OQuaRE quality perspective. In the case of
the EDAM, this study let us to identify that the fifth version of the ontology has the largest impact in the quality
metrics of the ontology, when comparative analyses between the pairs of consecutive versions are performed.
Keywords: Ontology quality, Ontology metrics, Oquare, Ontology repositories
*Correspondence: jfernand@um.es
Equal contributors
1Universidad de Murcia, IMIB-Arrixaca, Campus de Espinardo, 30071 Murcia,
Spain
Full list of author information is available at the end of the article
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 2 of 20
Background
In recent years the biomedical community has increased
its effort in the development of ontologies and this is
likely to continue [1]. Ontology developers tend to pub-
lish their ontologies on the Web and they are accessi-
ble from different sources. BioPortal [2], for instance,
contains more than 500 ontologies at the time of writ-
ing and new content is published frequently. BioPortal
enables updates by user submissions of new versions,
which are accessible via web browsers and through web
services [2].
The curation of ontologies is often a complex task
because of their high level of activity and rapid evolution
[3]. For this reason, the number of versions of an ontology
may grow rapidly. The evolution process turns the devel-
opment of an ontology into a dynamic process. Each of
the different versions of an ontology constitutes a snap-
shot of this process. The analysis of versions was intro-
duced by [4], in which ontology versioning was defined
as the ability to handle changes in ontologies by creating
and managing different variants of it and which pointed
out the importance of highlighting differences between
versions. Later, [5] claimed that a versioning system for
ontologies must compare and present structural changes
rather than changes in text representation or source
files. They described a version-comparison algorithm
that produces a structural difference between ontologies,
which was presented to users through an interface for
analysing them [6]. As mentioned in [7], there is no dis-
tinction between versioning and evolution in ontologies
since both account for the management of changes in
ontologies.
If we approach ontology changes from a logical perspec-
tive those changes are usually materialised by modifying
the axioms of a given ontology. Those modifications may
imply the addition or removal of classes, properties, indi-
viduals or constraints, as well as modifying the charac-
teristics, domains and ranges of properties. Such number
and types of changes have been the inputs for different
approaches that have tried to understand the evolution of
ontologies:
 Bubastis [3, 8] analysed the degree of activity in
biomedical ontologies by considering 5 major types
of ontology changes between two consecutive
versions: added or removed axioms to an existing
named class (NC), NCs added, NCs made obsolete
and edited annotation properties.
 Copeland et al. 2013 [9] focused on changes in
asserted and inferred axioms taking into account
reasoning capabilities in ontologies [10].
 In [11] a web application providing an interactive and
user-friendly interface to identify (un)stable regions
in large life science ontologies is proposed. A method
that computes change intensities for regions based on
changes between several succeeding versions of an
ontology within a specific time interval is used.
It makes sense to think that the changes made to
an ontology across its different versions should have an
impact on its quality. In addition, assuming that the
changes in an ontology should have a positive impact on
the quality of that ontology is also reasonable. In this con-
text, the main contribution of this work is to the study of
the evolution of ontologies from the perspective of ontol-
ogy quality, since, to the best of our knowledge, this aspect
has not been significantly researched to date. The analy-
sis of quality in ontologies has been addressed in different
ways in the ontology evaluation community, such as in the
following works:
 Gangemi et al. 2006 [12] approached it as a diagnostic
task based on ontology descriptions, using three
categories of criteria (structural, functional and
usability profiling).
 Rogers 2006 [13] proposed an approach using four
qualitative criteria (philosophical rigour, ontological
commitment, content correctness, and fitness for a
purpose).
 Yao et al. 2005, Tartir and Arpinar 2007 [14, 15]
presented metrics for evaluating structural properties
in the ontology.
 Duque-Ramos et al. 2011 [16] proposed OQuaRE,
which adapts the SQuaRE standard for software
quality evaluation for defining a qualitative and
quantitative ontology quality framework.
Our proposal is based on the OQuaRE Framework [16],
which is a qualitative and quantitative ontology quality
framework. The OQuaRE is based on the standard for
Software product Quality ISO/IEC 25000:2005 (SQuaRE)
[17]. The application of SQuaRE (1) provides a compre-
hensive specification and evaluation model for software
product quality; (2) makes quality evaluation reproducible
and objective, based on observations; and (3) allows for a
common language for specifying user requirements that
is understandable by users, developers and evaluators. All
these properties are desirable for an ontology quality eval-
uation approach. Ontologies, conceived as a special kind
of information object or computational artifact [18], have
a series of shared notions with Object Oriented Design
[19]. For example, the existence of classes, individuals
and properties can be exploited to adapt Object Ori-
ented Programming metrics to ontologies. This leads us
to believe that the principles of SQuaRE can be adapted
to ontologies. Thus, the main goal of OQuaRE is to pro-
vide an objective, standardised framework for ontology
quality evaluation, applicable to different ontology evalu-
ation scenarios, in which the ontologies are evaluated as
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 3 of 20
final products. For this, OQuaRE includes a generic scal-
ing function that transform metrics values into quality
scores.
In this work, we adapt OQuaRE for the purpose of
measuring the impact of the evolution of ontologies in
their quality. In [20], we described how OQuaRE could
be used to evaluate the quality of the different versions of
the ontology of Bioinformatics operations, types of data,
formats, and topics (EDAM) [21]. The standard quality
model and metrics defined in OQuaRE were used and the
method was able to detect changes in the measured qual-
ity of the different versions of the EDAM. The present
work is an extension of [20], presenting methodologi-
cal evolution and progress. First, we further formalise
the method to measure differences between versions of
the same ontology based on the OQuaRE performance.
Second, we take advantage of such a formalisation for
proposing a more sensitive scaling function to be able to
detect small differences between consecutive versions of
an ontology from the quality metrics perspective. This
will let OQuaRE to have two different scaling functions;
one for evaluating ontologies and final products and one
for evaluating the different versions of a given ontology.
The latter is used as feedback to adjust or define new
profiles of the static scale. Third, a statistical analysis
of the relation of changes in OQuaRE with the profile
of activity of the ontology is included. This extension
of the OQuaRE framework will allow a better under-
standing of the evolution of ontologies from a quality
perspective and will contribute to demonstrating how
ontology quality methods can be used to study ontology
evolution.
Methods
OQuaRE
OQuaRE is adapted from SQuaRE [17]. SQuaRE defines
a quality model and the process for software product
evaluation through five divisions: Quality Model, Qual-
ity Measurement, Quality Requirements, Quality Evalua-
tion and Quality Management. First, quality requirements
are identified. Second, the requirements are measured
using a quality model, which is quantified through qual-
ity metrics. These three divisions are used by the qual-
ity evaluation division, which is managed by the quality
management division. The usage of SQuaRE requires the
definition of these five divisions. OQuaRE defines all
the elements required for ontology evaluation: evalua-
tion support, evaluation process and metrics. OQuaRE
structures the evaluation of the quality of an ontology
using the four levels proposed by SQuaRE: quality require-
ments, quality characteristics, subcharacteristics andmet-
rics. OQuaRE uses the six quality characteristics proposed
by SQuaRE for measuring quality: functional adequacy,
reliability, operability, maintainability, compatibility, and
transferability. Besides, OQuaRE defines a new charac-
teristic, structural, which accounts for the quality of the
structure of the ontology (see Table 1). Each quality char-
acteristic has a set of associated quality subcharacteris-
tics, which are measured through quality metrics. The
quality metrics are the units of measurement of qual-
ity evaluation. The current version of OQuaRE has 49
subcharacteristics and 14 metrics. Some OQuaRE sub-
characteristics are reused and adapted from SQuaRE,
but some others are specific to ontology evaluation. For
example, the functional adequacy subcharacteristics are
Table 1 OQuaRE characteristics and subcharacteristics used in our method
Characteristic Description Associated subcharacteristics
Structural Formal and semantic relevant ontological properties that
account for: the correct use of formal properties, clarity of
cognitive distinctions and appropriate use of ontology
modelling primitives and principles
formalisation, formal relations support, redundancy,
consistency, tangledness, cohesion
Functional
Adequacy
Capability of theontologies to be deployed fulfilling functional
requirements, that is, the appropriateness for its intended
purpose according to state-of-the art literature [22]
reference ontology, controlled vocabulary, schema and
value reconciliation, consistent search and query, knowledge
acquisition, clustering and similarity, indexing and linking,
results representation, text analysis, guidance and decision
trees and knowledge reuse and inferencing
Reliability Capability of an ontology tomaintain its level of performance
under stated conditions for a given period of time
recoverability and availability
Operability Effort needed for the ontology use. Individual assessment of
such use, by a stated or implied set of users
learnability
Compatibility Capability of two ormore ontologies to exchange information
and/or to perform their required functions while sharing a
hardware/software environment
replaceability
Maintainability Capability of ontologies to be modified for changes in
environments, in requirements or in functional specifications
modularity, reusability, analysability, changeability,
modification stability and testability
Transferability Degree to which the ontology can be transferred from one
environment (e.g., operating system) to another
adaptability
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 4 of 20
extracted from the intended uses for ontologies identi-
fied in [22]. Following a bottom-up approach, OQuaRE
metrics are combined in order to compose the subchar-
acteristics, and the subcharacteristics are grouped by the
characteristics. Tables 2 and 3 describe respectively how
the 14 OQuaRE metrics are calculated and how some of
the metrics are associated with the subcharacteristics. We
have not included all of them for simplicity, but they are
available at [16, 23].
The evaluation of an ontology comprises a score for
those requirements measured through the quality model.
OQuaRE metrics reuse and adapt a set of well known
metrics from both ontology evaluation and software engi-
neering communities [14, 22, 24]. The quality metrics
provide quantitative values in different ranges, which are
called raw quality metrics values. OQuaRE applies a scal-
ing method recommended in SQuaRE that assigns values
in the range [1,5] (5 levels):
 1 - Not Acceptable
 2 - Not Acceptable - Improvement Required
 3 - Minimally Acceptable
 4 - Acceptable
 5 - Exceeds Requirements
Table 2 OQuaRE metrics and a brief description of how we
calculate them
OQuaRE metric Description
ANOnto Mean number of annotation properties per class
AROnto Number of restrictions of the ontology per classes
CBOnto Number of superclasses divided by the number of class
minus the subclasses of Thing
CROnto Mean number of individuals per class
DITOnto Length of the largest path from Thing to a leaf class
INROnto Mean number of subclasses per class
NACOnto Mean number of superclasses per leaf class
NOCOnto Mean number of the direct subclasses per class minus
the subclasses of Thing
NOMOnto Mean number of object and data property usages
per class
LCOMOnto Mean length of all the paths from leaf classes to Thing
RFCOnto Number of usages of object and data properties and
superclasses divided by the number of classes minus
the subclasses of Thing
RROnto Number of usages of object and data properties divided
by the number of subclassof relationships and properties
TMOnto Mean number of classes with more than 1 direct ancestor
WMCOnto Mean number of properties and relationships per class
Let us suppose that a user wants to evaluate the ontol-
ogy requirement Multiple inheritance of an ontology,
which might require to evaluate the Structural char-
acteristic. This characteristic has 9 subcharacteristics,
but only two will be used in this example (see Fig. 1)
for simplicity, namely, Tangledness and Formal rela-
tion support. The traceability from the OQuaRE quality
metrics to the quality requirements is shown in Fig. 1.
Tangledness depends on the TMOnto metric, whose
value depends on the mean number of classes with more
than 1 direct ancestor, so two primitive measurements
(number of classes and number of direct ancestors) are
used for computing the raw value of the metric, which
in this example is 1.28. Raw values are transformed
into quality scores using a scaling function. The scaling
method (see Table 4) is based on the recommendations
and best practices of the Software Engineering commu-
nity for software metrics and ontology evaluation met-
rics. For TMOnto, the scaling function transforms this
value into the quality score 5 because the raw value is
in the range [1, 2]. Given that Tangledness has only
the TMOnto metric associated, this is also its score.
In case one subcharacteristic has more than one met-
ric associated, its score would be the weighted mean of
the quality scores of the metrics. In Fig. 1 we can see
that quality score for Formal relation support is 2, so
the score of the Structural characteristic is 3.5, that is,
(5+2)/2.
Adapting OQuaRE for ontology evolution
Definitions
In this section, we define a series of concepts related to
ontology evolution from the OQuaRE perspective.
Definition 1 Versioned corpus of an ontology (vC? ): is
a list of versions {vi} of the same ontology ? , where i
represents the chronological position of vi in vC? .
The comparison of different versions of the same ontol-
ogy highlights changes and commonalities between the
versions [5]. The comparison can be done using metrics of
different nature (real-valued metrics, factor, ordered fac-
tors, etc.). In order to include all of them in a common
context, the method requires the adaptation of the met-
rics, because they need to satisfy the constraints described
in Definition 2.
Definition 2 Comparison criteria (f? ): is a discretisation
framework that, for every version vi ? vC? , provides a vec-
tor si of integers that can be used to rank those versions in
vC? .
The number of components of the vector si is r. For
example, if we use TMOnto as a unique comparison
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 5 of 20
Table 3 Summary of the associations between the characteristics, subcharacteristics and the associated metrics
OQuaRE characteristic OQuaRE subcharecteristic OQuaRE metric
Structural Formal relations support RROnto
Tangledness TMOnto
Cohesion LCOMOnto
. . . . . .
Functional adequacy Controlled vocabulary ANOnto
Inference RROnto, CROnto
Consistent search and query ANOnto, RROnto, AROnto, INROnto
Knowledge acquisition and representation ANOnto, RROnto, NOMOnto
. . . . . .
Maintainability Modularity WMCOnto, CBOOnto
Analysability WMCOnto, DITOnto, RFCOnto, NOMOnto, LCOMOnto, CBOOnto
Modification stability WMCOnto NOCOnto RFCOnto LCOMOnto CBOOnto
. . . . . .
Reliability Recoverability WMCOnto, DITOnto, NOMOnto, LCOMOnto,
Availability LCOMOnto
. . . . . .
Operability Learnability WMCOnto, LCOMOnto, RFCOnto, NOMOnto, CBOnto, NOCOnto
. . . . . .
The associations of the reminding 36 subcharacteristics with metrics can be found at http://miuras.inf.um.es/oquarewiki
criterion, f? discretises its real-value, using the quality
score, to the range [1,5]. Moreover, in this case these inte-
gers are related to the different qualitative levels defined
by OQuaRE, although different levels could be used.
Then, given two versions vi and vj, if f? produces the scores
5 and 1 respectively, that means that vj is more tangled
than vi. Similarly, the remaining 13 metrics can be added
to the comparison criteria, and this is what we propose as
a means to analyse the evolution of ontologies. Therefore,
the application of f? to vi generates a vector si of 14
components. The more components the vector si has, the
harder it is to compare and interpret the changes. For this
reason we provide the user with some definitions whose
aim is to describe different types of changes. Hence, given
two consecutive versions vi?1, vi ? vC? , with i > 1, and
given the vectors si?1 and si obtained by the application of
Fig. 1 OQuaRE example that represents the traceability from the OQuaRE quality requirement to quality metrics divisions
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 6 of 20
Table 4 OQuaRE static scale with [1-5] values, where 1 means not acceptable, 3 minimally acceptable and 5 exceeds the requirements
Metric\Score 1 2 3 4 5
LCOMOnto >8 (6-8] (4, 6] (2, 4] <=2
WMCOnto >15 (11, 15] (8, 11] (5, 8] <=5
DITOnto >8 (6, 8] (4, 6] (2, 4] [1, 2]
NACOnto >8 (6, 8] (4, 6] (2, 4] [1, 2]
NOCOnto >12 (8, 12] (6, 8] (3, 6] [1, 3]
CBOnto >8 (6, 8] (4, 6] (2, 4] [1, 2]
RFCOnto >12 (8, 12] (6, 8] (3, 6] [1, 3]
NOMOnto >8 (6, 8] (4, 6] (2, 4] <= 2
RROnto [0, 20] % (20, 40] % (40, 60] % (60, 80] % >80 %
AROnto [0, 20] % (20, 40] % (40, 60] % (60, 80] % >80 %
INROnto [0, 20] % (20, 40] % (40, 60] % (60, 80] % >80 %
CROnto [0, 20] % (20, 40] % (40, 60] % (60, 80] % >80 %
ANOnto [0, 20] % (20, 40] % (40, 60] % (60, 80] % >80 %
TMOnto >8 (6, 8] (4, 6] (2, 4] (1, 2]
Those metrics adapted from object oriented programming have been scaled based on the best practices for object oriented programming and the metrics whose result is a
relative value are scaled in percentage
the comparison criteria f? , a change in scale of version vi
from version vi?1 is described in Definition 3.
Definition 3 Change in scale: vector of change associ-
ated with different values of the components of the vector
si with respect to si?1. The vector li, which is calculated as
si ? si?1, represents the levels in size and direction of the
changes from vi?1 to vi version, with i > 1.
It should be pointed out that the change in scale applies
to all the versions of an ontology except to the first one,
which corresponds to i = 1 in vC? . Since the OQuaRE
quality scores are the comparison criteria the level ranges
from [-4, 4], so the direction can be positive or negative.
For example, let us suppose a vC? that contains six ele-
ments v1, . . ., v6. The application of f? to vC? generates a
matrix with 6 rows, like the one shown in Expression 1.
The row i represents the vector si and has 14 components,
with i = 1,. . . ,6.
1 . . . . . . . . . . . . 14(r)
s1
s2
s3
s4
s5
s6
?
???????
5 4 2 1 . . . .
5 4 2 1 . . . .
4 3 2 1 . . . .
3 4 5 1 . . . .
1 5 5 2 . . . .
5 1 4 3 . . . .
?
???????
(1)
Using as input the matrix in Expression 1 we apply the
Definition 3 and obtain a matrix with 5 rows, like the one
shown in Expression 2. The row i represents the change
in scale by the vector li, with i = 2, . . . , 6. In the context
of quality scores, a negative component in li represents
a decreasing level in the corresponding quality score of
vi from vi?1, a positive one means the opposite and 0
indicates that the metric score remains invariant.
1 . . . . . . . . . . . . 14(r)
l2
l3
l4
l5
l6
?
?????
0 0 0 0 . . . .
-1 -1 0 0 . . . .
-1 1 3 0 . . . .
-1 0 0 1 . . . .
4 -4 -1 1 . . . .
?
?????
(2)
We propose to use a summarised representation of the
change in scale of the r metrics and between vi and vi?1
by using the frequency distribution Fi associated with the
change in scale li, which is defined in the following way:
Definition 4 Frequency distribution of the chase in scale
(Fi): it is an ordered list of the frequencies fl associated with
the different change levels l in the vector li.
The change levels range between lmin and lmax. In the
context of OQuaRE quality scores, lmin and lmax are ?4
and 4 respectively. Therefore, in this case the frequency
distribution Fi has 9 components, which represent the
frequencies fl of the ranks l from -4 to 4. For example,
Expression 3 shows the frequency distributions of our
running example. The interpretation of F2 is: there are 4
out of r metrics that have not suffered any change in scale
between v1 and v2. The change is larger between v2 and v3
(F3) as there are 2 metrics that have decreased one scale
and other 2 remain unchanged.
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 7 of 20
f?4 f?3 f?2 f?1 f0 f1 f2 f3 f4
F2
F3
F4
F5
F6
?
?????
0 0 0 0 4 0 0 0 0
0 0 0 2 2 0 0 0 0
0 0 0 1 1 1 0 1 0
0 0 0 1 2 1 0 0 0
1 0 0 1 0 1 0 0 1
?
?????
(3)
Hence the frequency distribution Fi can be used for
describing different types of changes between two con-
secutive versions vi?1 and vi with respect to the set of
OQuaRE quality scores. Next, we define some associated
statistics such as weighted means.
Definition 5 Forward Mean Change: weighted mean of
the positive change levels l, calculated as:
?lmax
1 l × fl?lmax
1 fl
Definition 6 Backward Mean Change: weighted mean
of the negative change levels l, calculated as:
??1
lmin l × fl??1
lmin fl
To avoid possible undefined values of the forward or
backward means, we also use the size of the forward and
backward changes defined as the numerator of the pre-
vious definitions, but considering absolute values |l| in
backward mean changes. Now, Definition 7 provides the
definition for the global mean change.
Definition 7 Mean change: weightedmean of the change
levels l, calculated as:
?lmax
lmin l × fl?lmax
lmin fl
In our running example, the frequency distribution F3
does not provide a determined finite value for the forward
mean change, whereas the backward mean change is ?1
and the mean change is?0.5. The sizes of the forward and
backward changes are 0 and 2, respectively.
The value of the mean change can be interpreted as
follows:
 It takes a positive value when the forward mean
change is greater than the backward one and negative
when the opposite.
 It becomes zero when forward and backward mean
changes take equal and finite values.
 It becomes zero if vi and vi?1 are identical. In this
case forward and backward mean changes do not
take a determined finite value (undefined value).
The mean change provides information about changes
in quality scores. For analysing the number of metrics that
have changed regardless of the direction of the change, we
define next the conceptmagnitude of change.
Definition 8 Magnitude of change: percentage of metrics
with change in scale, which is calculated as follows:
?
l =0 fl?lmax
lmin fl
In our example, the magnitude of change of version
v2 is 50 %. The largest number of metrics with changes
happens in v6 (see F6 in Expression 3), having a magni-
tude of change of 100 %, but the mean change is 0.0. The
major increase in quality scores happens in v4 (see F4 in
Expression 3) withmean change 0.75.
A dynamic scaling function for ontology evolution
We propose to take advantage of the information available
in the vC? to derive a dynamic scaling function. For this
purpose, each ontology in such a corpus is processed with
OQuaRE, so calculating the raw values of the 14 qual-
ity metrics. These original values are used for generating
a scale in k categories determined by k-means clustering
[25], which groups similar values into the same category
by minimising the intra-class variance and emphasises
the differences among categories maximising the inter-
class variance. In this paper, the number of categories is
k = 5 because the OQuaRE scale is [1,5]. This is illus-
trated using Fig. 2. The metric RROnto measures the
richness of relations and it is calculated using the mean
number of usages of object and data properties divided
by the number of subClassOf relationships and object
properties. The standard scale for RROnto is shown in
Table 4.
The RROnto raw values obtained for all the versions
within a vC? are represented in the x-axis of Fig. 2. The
static scale is represented in the upper-part of the figure,
and the dynamic scale obtained using k-means is shown
in the bottom-part. While the raw RROnto value 0.74
is matched with the quality score 4 in the static scale,
it is matched with 5 in the dynamic scale. It should be
pointed out that the dynamic scale forces data to be cat-
egorised between 1 and 5, 1 being the lowest raw value
found in vC? and 5 the highest. If the amount of different
data is not enough to generate 5 categories the algorithm
does not include any value in the lowest categories of
the scaling function (see for example the solid line for
DITOnto metric in Fig. 3). Therefore, the application of
the dynamic scale should help users to study the evolution
of the observed quality metrics values for all the versions
within a vC? .
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 8 of 20
Fig. 2 Example of the static and dynamic scale for RROnto metric. The x-axis represent the observer raw values of the metric for a vC.
Semi-transparent rectangles shows the limits of the levels of the scale. While the static scale remains constant, the dynamic will depends of the
observer raw values of RROnto in a vC
The ontology of Bioinformatics operations, types of data,
formats, and topics (EDAM)
We are going to study the evolution of the EDAM ontol-
ogy [21, 26]. The EDAM is an ontology of well established
and familiar concepts that are prevalent within bioinfor-
matics. The EDAM includes types of data, data identifiers,
data formats, operations and topics. We have chosen this
ontology as an example because:
 It is well documented and its developers use a control
version system (CVS) [27] so that we can trace
changes.
 Its source files are accessible online. The latest
version (v1.9) is published in the official project web
page. Links to old versions can be found in BioPortal
(18 versions) and in the CVS (13 versions).
 It has received 900 mean visits per month since
Oct-2013 to Apr-2014 and 6 declared projects use the
EDAM.
 The number of versions (18) makes it an ontology of
interest for studying its evolution. Its size (2 597
classes as mean) is intermediate, which facilitates the
analysis of the results in this first application of the
method.
Results and discussion
Experimental setup
The versioned corpus comprised the 18 EDAM versions
in BioPortal as CVS content, which was processed using
a software tool developed in house that implements the
OQuaRE framework. This framework and tool are pub-
licly accessible at http://sele.inf.um.es/oquare as a web
form and a web service. The framework uses the OWL
API [28] and Neo4j [29] for the calculation of OQuaRE
metrics. We carried out the computation of the dynamic
scaling by using the function bin.var of the package
RcmdrMisc of R [30].
We applied a normalisation process to the 18 versions.
In the normalisation, we removed deprecated classes and
checked the consistency of the ontology. Before applying
the normalisation, 4 out of 18 versions were discarded by
the tool: one could not be processed by the OWL API,
Fig. 3 Graphical representation of the static and dynamic scaled metrics along the versions
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 9 of 20
and the other three were found to be inconsistent by the
reasoner Hermit [31]. Therefore, the versioned corpus
contained 14 ontologies. In the remainder of this paper,
we label each version according to its original id version. It
should be pointed out that the statistics of change of a cer-
tain version vi were calculated with respect to the previous
processed version. For example, the change in v16 was cal-
culated with respect to v12 because v13, v14 and v15 could
not be processed.
The normalisation process made consistent v13 and
v14 and, therefore, they were included in the study. We
decided to perform two types of experiment: one with
the deprecated classes (14 consistent ontologies) and one
without the deprecated classes (16 consistent ontologies)
with the goal of studying the impact of the obsolete classes
in the structure of the ontology. We applied the tool to
obtain the scores of the metrics, subcharacteristics and
characteristics for all the versions. Such measurements
were the comparison criteria, which allowed the scores
to be obtained by using both the static scaling function
and the dynamic one. After presenting those results, we
will discuss the evolution of the EDAM in terms of qual-
ity scores and analyse the advantages and disadvantages of
both scaling methods. The whole set of results is available
at http://miuras.inf.um.es/oquare/jbsm2016.
Analysis of quality characteristics with the static scale
Table 5 shows the results obtained at the quality character-
istics level. Two quality scores are shown for each quality
characteristic: original (org) and normalised (nrm). Bold
numbers highlight changes in scale. Next, we discuss the
changes observed in the quality characteristics.
We can observe in Table 5 that the mean quality score
ranges from 3.99 in the first version to 3.85 in the last
one, so its quality scores have always stayed between 3
and 4. A quality score higher than 3 reveals that good
ontological engineering principles have been applied by
the EDAM developers. However, this difference has not
produced a change in scale in global terms. Despite this
fact, investigating why the quality score decreased is rel-
evant because lower OQuaRE levels provide users with
more fine grain information. For example, those decisions
made during the construction or modification of large and
complex ontologies may have collateral effects in their
engineering, which may have different implications from
a quality perspective. For example, reducing the usage of
properties might benefit the maintainability of the ontol-
ogy but fewer queries might be asked. Therefore, a lower
value in OQuaRE metrics related to the usage of proper-
ties would contribute positively to the Maintainability
of the ontology but negatively to the Formal relations
support. Understanding how different changes influence
different quality aspects is difficult to study if we use only
the mean quality score. This is why the analysis at the level
of characteristics, subcharacteristics and even metrics is
recommended.
First, we describe which characteristics have changes in
scale. The analysis of the evolution of quality scores of
the characteristics (between the first version and last one)
shows that 6 out of the 7 quality characteristics had a
change in scale: 4 positive and 2 negative. In the remain-
ing case, there was no change in scale for Functional
Adequacy. The score of the Reliability characteristic
decreased from 3 to 2 in v2; and the Structural one
decreased from 4 to 3 in v11. The scores for Operabil-
ity, Compatibility, Maintainability and Transferabil-
ity increased from level 3 to 4 in v5. Moreover, the
ontology has maintained the score at this level since
then. This behaviour happened for all their associated
sub-characteristics. The scores for the whole set of sub-
characteristics can be found at http://miuras.inf.um.es/
oquare/jbsm2016.
Analysis of the quality metrics with the static scale
Next, we describe the changes observed at the level of
OQuaRE metrics because this enables us to focus on con-
crete structural changes, which can help us to discuss and
explain the variations obtained in higher levels. Figure 3
(dashed lines) shows the quality scores of the static scale
for the 14 OQuaRE metrics. It can be observed that 9
OQuaRE metrics did not change for any version. The 5
metrics that have changed are LCOMOnto, NOMOnto,
RFCOnto, TMOnto and RROnto. Next, we discuss the
impact of the changes in these metrics at the level of
OQuaRE characteristics and sub-characteristics.
 RROnto had 3 changes in scale. The first 2 changes
were consecutive and due to the usage of properties,
which decreased 86 % between v4 and v6. Refactoring
towards a common set of properties can often be a
sign of good ontology engineering practise, however
the usage measures the number of times that a
property is linked with an entity through an axiom.
For example, while v4 defines 16 properties with
6 734 usages, v5 and v6 define the same number of
properties but with 1 979 and 937 usages respectively.
The usage of properties also decreased 8 % between
v10 and v11. This variation is smaller than the previous
one but, together with an unusual increase in the
number of relations (18 %), it triggered the change in
scale of RROnto. This increase in the number of
relations is a consequence of a structural change in
v11: deprecated classes were grouped as descendants
of an ontology class in the first taxonomic level and
this increased the number of relations.
 RFCOnto and NOMOnto had 1 change in scale
growing from 4 to 5 in v4. This behaviour was also
related to the usage of properties. However, for these
D
uque-Ram
os
etal.JournalofBiom
edicalSem
antics
 (2016) 7:63 
Page
10
of20
Table 5 OQuaRE characteristics metric values for eighteen versions of the EDAM ontology
V. Date Status
Struct. F. Adeq. Reliab. Operab. Compat. Maint. Transf. Mean
Org. Nrm. Org. Nrm. Org. Nrm. Org. Nrm. Org. Nrm. Org. Nrm. Org. Nrm. Org. Nrm.
1 2010-05-14 beta 4.67 4.67 4.61 4.61 3.25 3.25 3.83 3.83 3.75 3.75 4.10 4.10 3.75 3.75 3.99 3.99
2 2010-05-28 beta 4.50 4.50 4.60 4.60 2.88 2.88 3,67 3.67 3.75 3.75 3.99 3.99 3.75 3.75 3.88 3.88
3 2010-08-18 beta 4,50 4.50 4.60 4.60 2.88 2.88 3.67 3.67 3.75 3.75 3.99 3.99 3.75 3.75 3.88 3.88
4 2010-10-07 beta 4,50 4.50 4.60 4.60 2.88 2.88 3.67 3.67 3.75 3.75 3.99 3.99 3.75 3.75 3.88 3.88
5 2010-12-01 beta 4.17 4.17 4.46 4.46 2.75 2.75 4.00 4.00 4.00 4.00 4.23 4.23 4.00 4.00 3.94 3.94
6 2011-01-22 beta 4.00 4.00 4.28 4.28 2.75 2.75 4.00 4.00 4.00 4.00 4.23 4.23 4.00 4.00 3.90 3.90
7 2011-06-17 beta 4.00 4.00 4.28 4.28 2.75 2.75 4.00 4.00 4.00 4.00 4.23 4.23 4.00 4.00 3.90 3.90
8 2011-12-05 beta 4.00 3.83 4.28 4.27 2.75 2.38 4.00 3.83 4.00 4.00 4.23 4.12 4.00 4.00 3.90 3.78
10 2012-12-10 beta 4.00 3.83 4.28 4.27 2.75 2.38 4.00 3.83 4.00 4.00 4.23 4.12 4.00 4.00 3.90 3.78
11 2012-12-14 release 3.83 3.83 4.11 4.27 2.75 2.38 4.00 3.83 4.00 4.00 4.23 4.12 4.00 4.00 3.85 3.78
12 2014-02-18 update 3.83 3.83 4.11 4.27 2.75 2.38 4.00 3.83 4.00 4.00 4.23 4.12 4.00 4.00 3.85 3.78
13 2014-09-26 update - 3.83 - 4.27 - 2.38 - 3.83 - 4.00 - 4.12 - 4.00 - 3.78
14 2014-11-14 update - 4.00 - 4.28 - 2.75 - 4.00 - 4.00 - 4.23 - 4.00 - 3.90
16 2014-12-08 update 3.83 4.00 4.11 4.28 2.75 2.75 4.00 4.00 4.00 4.00 4.23 4.23 4.00 4.00 3.85 3.90
17 2014-12-16 update 3.83 3.83 4.11 4.11 2.75 2.75 4.00 4.00 4.00 4.00 4.23 4.23 4.00 4.00 3.85 3.85
18 2015-02-02 update 3.83 3.83 4.11 4.11 2.75 2.75 4.00 4.00 4.00 4.00 4.23 4.23 4.00 4.00 3.85 3.85
These values are scaled from 1 to 5, where 1 is not acceptable and 5 exceeds the requirements. Bold numbers highlight changes in scale between two consecutive versions
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 11 of 20
metrics such a primitive metric influences positively
the quality score because, in the case of NOMOnto,
the lower the mean number of property usage per
class is the easier the maintainability of the ontology
is. This behaviour triggered the change in scale for
the characteristics Operability, Compatibility and
Transferability in v5.
 TMOnto measures the distribution of the parents in
the ontology. 10 % of the classes had more than 1
direct parent in v4, while this value grew up to 24 % in
v5. This metric has a negative effect across the
ontology because of the multiple inheritance,
although this might be needed to reflect some aspects
within the ontology. This fact influenced the decrease
in the Tangledness subcharacteristic, which also
contributed to the decrease of the the Structural
characteristic. However, for this metric this change
did not trigger by itself a change in scale, which was
produced in v11 with the collaboration of RROnto.
 LCOMOnto uses the number of paths in the
ontology in its calculation and it suffered one change
in scale in v2. This metric is used in the
subcharacteristics Cohesion, Knowledge reuse,
Learnability , Recoverability and Availability.
Moreover, this metric is the unique used to measure
Cohesion and Availability, so it has a deeper
impact for these two subcharacteristics than for the
others. On the one hand the lowest score for the
Structural characteristic was for Cohesion but this
did not trigger a change in scale for v2. On the other
hand, the Recoverability and Availability are
grouped in the Reliability characteristics and for it,
the behaviour of the LCOMOnto metric triggered
the change in scale in v2.
Influence of deprecated classes
The presence of deprecated classes grew from 3.51 %
(v1) to 29.58 % (v18). Deprecated classes caused incon-
sistencies in v13 and v14. Table 5 shows that there were
no significant changes at the characteristic level between
the ontologies with (Org) and without the deprecated
classes (Nrm), but some changes happened at the met-
ric level. The change in the Structural characteristic with
deprecated classes anticipated the drop of RROnto to v11,
whereas it happened in v17 in the normalised version.
Besides, LCOMOnto temporarily descended to score level
2 between v8 and v13 in the normalised version. This effect
on LCOMOnto could not be appreciated in the ontologies
with the deprecated classes. Deprecated classes remain in
the ontology, so they are influencing the OQuaRE results.
For example, RROnto uses the number of subClassOf
relations in the denominator, to which deprecated classes
(see Table 2) contribute. The removal of the deprecated
classes had an impact on this metric, which produced this
effect of anticipating or delaying changes in scale. More-
over, the scaling function cushioned smaller changes such
as the one produced by LCOMOnto.
Application of the dynamic scale
We have obtained a dynamic scale using the EDAM ontol-
ogy versions composing the experimental vC? . The values
obtained after applying the k-means clustering are shown
in Table 6. Moreover, Fig. 3 shows the evolution of the
values of the metrics for both the static (dashed lines)
and dynamic scales (solid lines). It can be seen that the
dynamic scale is able to capture more changes in those
values than the static one. This is an expected result as
the [1,5] scale limits for each metric is derived from the
raw values of the metrics for the different versions of
the ontologies. This means that both scales reflect differ-
ent aspects and, therefore, are complementary in helping
to understand the engineering and the evolution of the
ontologies. Next, we discuss how changes are detected by
both scales.
The changes in some metrics were detected by both
scales. In the case of RROnto, although the first version
starts in 4 for the static scale and in 5 for the dynamic
scale, both scales detected changes between the same
pairs of versions, except for v17. However, this did not hap-
pen for RFCOnto, TMOnto, NOMOnto or LCOMOnto.
The dynamic scale is more sensitive so it detected more
changes between pairs of versions for these 4 character-
istics. The static scale did not detect changes for nine
metrics, but the dynamic one did. For example, while the
DITOnto value remained in 1 in the static scale, in the
dynamic scale it started in 5 and ended in 4. Moreover, it
decreased to 2 in v7.
The value of DITOnto remained in 1 with the static scale
for all the versions of the EDAM ontology. DITOnto mea-
sures the depth of the ontology. The raw values obtained
for our corpus were (11, 11, 11, 11, 13, 13, 14, 13, 13,
13, 13, 12, 12, 12). All of them are greater than 8, which
is scaled to the quality score 1, according to the best
practice applied. However, in the field of ontologies an
appropriate value for DITOnto might depend on many
factors, and it is here where the dynamic scale can com-
plement the static one. According to [32], well-structured
OO systems have a forest of classes rather than one
large inheritance lattice. However, whether a high or low
value is desired from a metric for better code quality
still must exercise judgement when determining the best
approach for the task at hand. According to [32], the lower
the DITOnto the better, so the OQuaRE scaling method
matches DITOnto low values to 5 and high values to 1.
Then, the dynamic scale uses the lowest and highest val-
ues observed for the versions of the ontology to assign
the scores 5 and 1, respectively. With this scale, the high-
est quality scores were reached from v1-v4, then it went
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 12 of 20
Table 6 Coordinates of the dynamic scale obtained after applying the k-means algorithm using the versions of the EDAM within the
experimental vC
Metric\Score 1 2 3 4 5
LCOMOnto [5.646945, 5.782834] [5.505317, 5.505317] [5.158599, 5.190406] [5.072177, 5.093400] [3.874391, 4.109421]
WMCOnto [4.123580, 4.176131] [1.931285, 1.931285] [1.536827, 1.559519] [1.401986, 1.478964] [1.334862, 1.347192]
DITOnto [, ] [14, 14] [13, 13] [12, 12] [11, 11]
NACOnto [1.275837, 1.279578] [1.261488, 1.264644] [1.245146, 1.245352] [1.228666, 1.230561] [1.099615, 1.104098]
NOCOnto [1.332252, 1.342622] [1.276790, 1.286796] [1.263569, 1.263569] [1.229043, 1.230952] [1.103604, 1.108706]
CBOnto [1.602873, 1.637277] [1.559101, 1.559101] [1.404925, 1.456697] [1.230693, 1.281911] [1.143644, 1.152976]
RFCOnto [4.364891, 4.383669] [2.306886, 2.306886] [1.900142, 2.022841] [1.541327, 1.564187] [1.438217, 1.475449]
NOMOnto [3.068605, 3.115014] [0.799515, 0.799515] [0.3790453, 0.3790453] [0.2754958, 0.3078338] [0.2071335, 0.2423935]
RROnto [0.144421, 0.144694] [0.164751, 0.180672] [0.2195698, 0.2562910] [0.4139807, 0.4139807] [0.7441604, 0.7459092]
AROnto [4.14, 5.00] [7.0, 7.0] [14.0, 14.0] [16.0, 16.0] [21.0, 21.0]
INROnto [1.037050, 1.061705] [1.094152, 1.099919] [1.13177, 1.13177] [1.227018, 1.228871] [1.261331, 1.277758]
CROnto [0.0, 0.0] [0.35285e?3, 0.36778e?3] [0.40420e?3, 0.40453e?3] [0.45433e?3, 0.45433e?3] [0.47103e?3, 0.48123e?3]
ANOnto [1.097413, 1.102329] [1.114493, 1.117287] [1.131656, 1.131656] [1.144306, 1.144975] [1.150423, 1.153622]
TMOnto [0.2556087, 0.2599199] [0.2461048, 0.247064] [0.2400970, 0.2436178] [0.2171334, 0.2173766] [0.09961501, 0.10456901]
down from v4-v5 and again from v6-v7, then it remained
stable until v12, where it again increased one level. As we
have explained previously, it should be pointed out that
the dynamic scaling method for DITOnto did not span
the range [1. . .5] because there were only 4 raw values
observed.
Analysis of major changes between versions
The graphical representation of the frequency distribu-
tions Fi is shown in Fig. 4. The left-half of the Figure shows
the frequency distributions Fi obtained with the static
scale, on the right-half the ones obtained with the dynamic
scale. For each box, the y-axis represents the components
from the levels lmin to lmax; it should be pointed out that
this figure just represents in the y-axis those components
with at least one observed frequency fl distinct than 0
for any version in vC? ). Finally, the x-axis represents the
frequency of each component. For example, with the static
scale and for v8 (Fi = 8) the frequency of l0 is 13 because
the value of 13metrics did not change in scalewith respect
to the previous version; similarly, with the dynamic scale
and for v2 (F2) the frequency of l?1 is 1 because 1 metric
(LCOMOnto) decreased one level.
Now we describe how to use the magnitude and mean
change to analyse major changes between consecutive ver-
sions. This will be done by discussing the data shown in
Table 7, where rows 25 show the values of the four statis-
tics of change using the static scale, and rows 69 show
those statistics for the dynamic scale.
Analysis of magnitude of change
The magnitude of change with the static scale was dif-
ferent than 0 for v2, v5, v6 and v11 (see Table 7 row 2).
For example, the largest magnitude of change happened
Fig. 4 Frecuency distributions of the changes in scale
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 13 of 20
Table 7 Statistics for static and dynamic scales:magnitude of change,mean change forward,mean change backward, andmean change
Change v2 v3 v4 v5 v6 v7 v8 v10 v11 v12 v16 v17 v18
Sta. Magnitude 7 % 0 % 0 % 28 % 7 % 0 % 0 % 0 % 7 % 0 % 0 % 0 % 0 %
Mean. For 1 - - 1 - - - - - - - - -
Mean. Back - - - 1 1 - - - 1 - - - -
Mean -0.07 0.00 0.00 0.00 -0.07 0.00 0.00 0.00 -0.07 0.00 0.00 0.00 0.00
Dyn. Magnitude 7 % 0 % 7 % 79 % 42 % 71 % 36 % 56 % 56 % 21 % 64 % 14 % 0 %
Mean. For - - - 1.25 1.33 1.00 1.50 1.50 1.75 1.00 1.38 - -
Mean. Back 1.00 - 1.00 1.67 1.33 1.12 1.00 1.25 1.75 - 1.00 1.00 -
Mean -0.07 0.00 -0.07 -0.71 0.00 -0.50 0.36 0.07 0.00 0.21 0.71 -0.14 0.00
The symbol - in this table represents the undefined value
for v5, 28 %, and this was a consequence of the changes
in RFCOnto, NOMOnto, TMOnto and RROnto; these
changes in the OQuaRE quality metrics can be observed
in Fig. 3 (dashed lines). For v2, v6 and v11, the magnitude
of the change is 7 % because only one metric had a change
of level. There were no changes in the quality scores for
the rest of the versions. The magnitude of change with
the dynamic scale was different than 0 for 11 out of 13
versions. This is a consequence of the higher sensitivity
of the dynamic scale. This scale enabled the identifica-
tion of versions like v3 or v18 to be very similar with
respect to their previous one, because the magnitude and
mean changewere 0 % and 0.00 respectively. By similar we
mean that there were not enough changes between them
that produced a change in scale for any of the OQuaRE
metrics.
In order to analyse pairs of consecutive versions, we are
going to use the median (Me) of the absolute difference
between the values of the 14 metrics, and the Wilcoxon
test for contrasting the alternative hypothesis Me > 0.
Table 8 sorts the versions by increasing critical value and
p-value associated with the null hypothesis (Me = 0) for
each test performed. These results show that:
 We reject the null hypothesis (Me = 0) in all the
comparisons, so we can interpret that all the changes
are significant.
 We have evaluated the magnitude of change using
the quality scores (scaled metrics). The critical value
shows the magnitude from which the difference
median (Me) is significantly higher at the 0.05 level of
significance. Using this criterion for sorting the
changes between versions we obtain that the largest
change happens in v5.
 The four versions with the largest changes according
to this analysis are also the four versions with the
highest magnitude of change for the dynamic scale,
as shown in Table 7 row Magnitude. This shows the
goodness of the criteria used in the dynamic scaling
function.
Analysis of mean changes
Themean change using the static scale is negative because
the score of one metric decreased for v2, v6 and v11
(see Table 7 row 6). However, the magnitude of change
had a different evolution. The largest magnitude hap-
pened for v5, but the mean change for v5 was 0.0,
because the number of positive weighted changes was
equal to the number of negative ones. For this partic-
ular case, two metrics increased 1 level (RFCOnto and
NOMOnto) and 2 exactly the opposite (TMOnto and
RROnto) (see dashed line in Fig.3). The higher sensitiv-
ity of the dynamic scale is also observed in the mean
change values, because more changes were detected. For
example, if we focus on v5, the Mean. Back (1.67) was
higher than the Mean. For (1.25) regardless of the num-
ber of metrics that had changed. Therefore, the Mean is
-0.71, so there were more negative changes than positive
ones.
Table 8 Versions sorted from less to high critical value and
p-value associated with the null hypothesisMe after applying the
test of Wilcoxon using the difference in absolute values of the
median of 14 OQuaRE metrics and consecutive pairs of versions
Version Critical_value P_value
18 0.0001825782 1.263087e?3
2 0.0013102421 1.263087e?3
3 0.0020865871 1.263087e?3
4 0.0021207897 1.263087e?3
17 0.0044867447 1.263087e?3
8 0.0072293707 6.103516e?5
12 0.0113504041 1.263087e?3
10 0.0119625746 8.308472e?4
6 0.0259642025 8.308472e?4
11 0.0303236313 8.308472e?4
16 0.0324480617 8.308472e?4
7 0.0420278822 6.103516e?5
5 0.1587347761 8.308472e?4
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 14 of 20
As a complement, the graphical representation of the
backward and forwardmean change size is shown in Fig. 5.
The upper-half of this figure (size forward) stands for
the positive changes, whereas the lower-half (size back)
represents the negative ones. The largest positive change
happened for version v16, and the largest negative one was
for v5.
Profile of change in quality scores
Regardless of the scale used, the information provided
by the mean change can be used to calculate a profile
of quality based on the OQuaRE framework. This profile
takes into account the accumulativemean changes during
the whole life-cycle of the ontology. Figure 6 shows the
evolution of the quality scores using both scales:
 The use of the static scale shows a trend of negative
mean change. The accumulative mean change value
remained negative for all the versions and all the
pairs, which is also reflected in the decrease of the
quality scores of the characteristics as mean from
3.99 to 3.85, which was discussed previously.
 The complementary use of the dynamic scale allows a
different evolution to be observed. The mean change
for the first 7 versions was negative, whereas it was
positive for the next 9 versions. As a consequence,
the accumulative mean change growed from -1,35 to
1,63. Finally, it decreased until 1.49 for v17 and
remained constant for v18.
Finally, if we take into account the status used to define
each version in BioPortal: they are considered beta from
version 1 to 10. Using the dynamic scale, we observe that
the quality scores decreased until v7, and in particular
in v5 with the lowest mean change (see Fig. 6). Having
such changes during the beta stage makes sense. Once the
ontology is considered released, the increase of the quality
scores was over the mean.
Relation between quality scores and the level of activity in
an ontology
So far, we have analysed aspects related to variability in
the quality scores. Now, we study the possible relation
between these changes and the level of activity in an ontol-
ogy. The level of activity has beenmeasured in [3] in terms
of changes in ontology classes, namely, number of classes
that have been added, deleted or modified. These three
variables are calculated by Bubastis [8], so we call them
the Bubastis variables.
In http://miuras.inf.um.es/oquare/jbsm2016, several
Principal Component Analysis (PCA) studies can be
found. Here, we use the three statistics related to mean
change (using the dynamic scale) and the Bubastis
variables for performing a PCA, with the objective
of obtaining the relation between these two different
ontology aspects, as well as obtaining a bi-dimensional
representation of the changes between two versions.
The coordinates of the variables for the new axis are
shown in Table 9, and they are graphically represented
in Fig. 7 upper half. The variable representation of Fig. 7
suggests the presence of two normalised uncorrelated
factors:
 The Bubastis variables have the largest positive
correlations (0.88, 0.80 and 0.85, for new, changed
Fig. 5 Statistics of size for static and dynamic scales: forwardmean change size and backwardmean change size
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 15 of 20
Fig. 6 Graphical representation of the accumulativemean change using the static and dynamic scales
and deleted classes respectively) with Factor 1
(represented in the x-axis), so we interpret this factor
as a gradient representing the increasing volume of
activity associated with the Bubastis activity. We call
this factor Bubastis Activity.
Table 9 Representation in 2-dimensions of the coordinates of
the variables for the new axis
(x-axis) Factor 1 (y-axis) Factor 2
Number.New.Classes 0.8862 0.3539
Number.Changed.Classes 0.7970 0.3300
Number.Deleted.Classes 0.8458 0.4253
Dynamic.Backward.Size 0.6883 ?0.4895
Dynamic.Forward.Size 0.3823 0.6623
Dynamic.Mean.Change ?0.3557 0.9186
Factor Name Bubastis Activity OQuaRE Dynamic Quality
Three statistics related tomean change (using the dynamic scale) and the Bubastis
variables have been used for performing a PCA, with the objective of obtaining the
relation between these two different ontology aspects. The variable representation
of Fig. 7 suggests the presence of two normalised uncorrelated factors: Bubastis
Activity and OQuaRE Dynamic Quality. The representation of these coordinates can
be found in Fig. 7 above
 The Dynamic mean change has the largest positive
correlation (0.92) with Factor 2 (represented in the
y-axis), whereas dynamic backward size has a
negative correlation with this factor. Those facts
allow us to interpret this second factor as a gradient
from lower OQuaRE quality scores to higher ones.
We call this factor OQuaRE Dynamic Quality.
According to the previous comments, the versions rep-
resented in the first diagonal will be relevant in activity
and quality, the more the farther from the origin they
are.
The two previous factors explain more than 80 % of
the information contained in the six variables shown in
Table 9; and the first factor explains roughly 48 % of
such information. Apart from the two factors, in Fig. 7
we also observe the next correlations using the Pearson
test: (1) the number of classes deleted and new classes
(0.99, p-value 0.0000); and (2) the dynamic mean change
is almost independent of new (-0.01, p-value 0.9650) and
deleted classes (0.07, p-value 0.8137) and (3) the dynamic
forward size is almost independent of the number
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 16 of 20
Fig. 7 Principal Component Analysis: factors and principal components plots
of changed classes (0.01, p-value 0.9808). Those pairs
whose p-value is lower than 0.05 indicate a significant
correlation.
Figure 7 bottom represents the principal components
of the changes between consecutive versions in our vC? ,
where four changes can be highlighted:
 The Bubastis activity of v16 was below the mean
value. However, this activity produces a remarkable
increment in the OQuaRE quality scores using the
dynamic scale.
 The Bubastis activity of v10 and v11 was atypically
high with respect to the rest of the versions.
Moreover, the OQuaRE quality scores using the
dynamic scale are over the mean value.
 The Bubastis activity of v5 was over the mean,
producing a decrease in the OQuaRE quality scores
using the dynamic scale and a high level in the
number of classes changed.
 The Bubastis activity of v18 was the lowest and
around the mean value in OQuaRE quality scores
using the dynamic scale.
The most relevant changes obtained by this representa-
tion are the same as those obtained by the mean change
statistics shown in Fig. 5, where v5 and v16 had the highest
value of back and forward size respectively.
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 17 of 20
A view on the evolution of the EDAM ontology
In this section we discuss how the application of our
method enables some insights about the EDAM ontology
and its evolution in terms of quality scores as well as the
benefits of using the static or dynamic scales.
If we analyse the quality of the EDAMontology from the
OQuaRE perspective, we can identify different strengths
and flaws, driving our attention to those quality scores
obtained for the latest version analysed v18 (see Table 5).
According to the OQuaRE static scale, the mean value
3.85 reveals that good ontological engineering principles
have been applied. The analysis of the characteristics and
sub-characteristics gives us more information. Next, we
comment on the results for the highest and lowest score:
maintainability, functional adequacy and reliability (4.23,
4.11 and 2.75 respectively).
 The highest quality score is obtained for
maintainability (4.23). All its subcharacteristics
associated have quality score over 4 (see values at
http://miuras.inf.um.es/oquare/jbsm2016). This
reveals some strengths of the EDAM, such as the
reduced rate of negative side-effects due to changes
in the ontology (modification stability 4.60) and the
possibility to validate the ontology and detect flaws
on it (testability 4.00).
 The second highest quality score applies to
functional adequacy (4.11). For example, the EDAM
is good for use as a controlled vocabulary to avoid
heterogeneity of terms because all their classes have
labels expressed in natural language. However, not all
its subcharacteristics obtain high scores. For
example, one weakness of the EDAM is elucidated by
the score of the inference subcharacteristic. Its score
is 1.0 due to the low usage of properties, despite the
fact it is defined using a formal language. The absence
of instances also contributed to this score.
 The lowest score is obtained for reliability (2.75),
whose subcharacteristics are recoverability (2.50) and
availability (3.00). The recoverability score is below 3,
so it can be considered as a weakness of the EDAM
because in case of inconsistency, incompleteness or
redundancy of the content of the ontology, that
would be difficult to re-established and to recover the
ontologys performance.
There is only another subcharacteristic with a qual-
ity score under 3, formal relation support, whose score
is 1. The formal relations support measures the capabil-
ity of the ontology to represent relations supported by
formal theories different from taxonomy. This is calcu-
lated by analysing the usage of properties (RROnto). As
we have shown in previous sections, RROnto has a score
of 1 in the latest versions whereas the value of the first
version was 4, which makes it a potential weakness of the
ontology in the latest versions. The previous discussion
about RROnto comes from the comparison of different
versions, so it is done in terms of evolution. Continuing
with the analysis of the evolution of the EDAM ontology
from the OQuaRE perspective, we can draw the following
conclusions:
 v5, v2, v7 and v11 were the versions with the highest
magnitude of change, that is, number of metrics with
changes. The analysis of the characteristics using the
static scale has revealed that, as mean, there are no
changes in scale in the EDAM ontology. This is also
observed in the negative trend of the accumulative
mean change when the static scale is used (Table 6).
Interestingly, the dynamic scale has revealed the
observation that the accumulative mean change
trend is positive from v7 to v18.
 At the characteristics level, the application of the
static scale to the EDAM ontology has revealed that
the evolution of the ontology has produced higher
quality scores for four characteristics, and lower ones
for two of them, as can be observed in Table 1.
 The analysis of changes at the OQuaRE metrics level
helps us to identify that the usage of properties is the
reason that has triggered the major descend in quality
scores between v4-v6, and again between v10-v11.
Moreover, an unusual increment of the number of
relations in v11 triggered this change in scale. It
should be pointed out that the application of our
method can draw out these types of
suggestions.
Discussion about the method
In the previous sections we have described the main
results of our work, as well as provided some discussion
about the application of the method to the EDAM. Next,
we provide some discussion about different aspects of the
method.
In our previous work, the application of the standard,
static scaling function used by OQuaRE proved its useful-
ness to detect strengths and flaws of ontologies and even
to detect changes between versions of the same ontol-
ogy. However, we believed that the use of more precise
and sensitive methods for detecting changes would allow
OQuaRE to be more supportive of ontology evolution
processes. This is whywe have proposed the dynamic scal-
ing function, which should be used in conjunction with
the static one, because they provide complementary infor-
mation. Hence, this does not mean that the static scaling
function cannot be used on its own for ontology evolution.
It can be used to measure how the different versions have
changed across their history, taking into account fixed
criteria. For example, here we have evaluated the EDAM
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 18 of 20
using the static function using as reference the current
configuration that evaluates the ontology from an engi-
neering point of view. This static scaling approach enables
users to measure the quality of ontologies using a com-
mon framework, but, of course, this framework can be
extended or fit to certain contexts in case that the con-
text is clearly identified. Nevertheless, the dynamic scal-
ing function should provide more useful information for
ontologies for which new versions are frequently released
or that do not constitute major changes with respect to the
previous ones.
The development a common reference framework that
can be used for those different requirement scenarios is a
challenging task. An open question is whether the ranges
can be universally set for the static scaling method. The
dynamic scaling function tries to overcome this uncer-
tainty by performing an evaluation based on the behaviour
of the ontology during its evolution. It should be pointed
out that the goal of the dynamic scale is not to replace
or substitute the static one. In fact, the dynamic function
does not discretise the raw values of the metrics using a
continuous function, but the limits are set on the observed
values (see Fig. 2). However, the dynamic scale result could
be used to define new profiles based on re-adjusted static
scales.
As future work, we propose to use the lessons learned
in this experiment to analyse a larger set of ontolo-
gies. From our experience, reaching a community agree-
ment for certain aspects of ontologies is not always
an easy task, such as to what extent axiomatic rich-
ness is needed in biomedical ontologies [33]. On the
one hand, those biomedical ontologies used as sim-
ple plain taxonomies or controlled vocabularies do not
need a complex axiomatisation. On the other hand,
those biomedical ontologies used as domain ontolo-
gies should be as rigorous and axiomatically rich as
possible.
This debate is also related to the OQuaRE quality
model. For example, the static scaling of the metric
NOMOnto (see Table 4) could be interpreted as favouring
more plain taxonomies over heavily axiomatised ontolo-
gies, because it would not be very difficult for ontolo-
gies with low axiomatisation to obtain a high quality
score for NOMOnto. Another example, ontologies with-
out instances have lower scores for some metrics, but
sometimes the absence of instances is a design criterion
for such ontologies. In such cases, the metrics that take
into account instances should not be applied, or not con-
sidered relevant.We are currently working on enabling
OQuARE profiles, which would allow users or commu-
nities of users to customise the associations between
OQuaRE metrics, subcharacteristics and characteristics.
The future OQuaRE users will be able to include newmet-
rics or to define the scaling functions. The new metrics
will have to be associated with current sub-characteristics.
This solution is useful for users and communities with
particular needs.
We consider that we could extend the idea of the
dynamic scale and obtain a repository-based scale by
using a repository like Bioportal [2] or AberOWL [34]
as reference. The repository-based scale would be the
result of applying the dynamic scaling method proposed
in this paper but considering a vC? where ? represents
the ontologies and versions within the repository. This
repository-based scale would provide users some feed-
back to determine the ranges of the static scaling function
based on a large set of existing ontologies. However, work-
ing with large repositories that can contain hundreds or
thousands of versions for some ontologies can be chal-
lenging. We plan to use a sliding window approach,
which would include the last 10-20 versions of an ontol-
ogy, or x versions that cover the whole life-cycle of the
ontology and having them equally separated across the
time period. Such representative sample of versions would
be used for creating the dynamic scaling function. Finally,
the inclusion of new unsupervised clustering algorithms
that automatically decide the number of categories of
quality scores for each metric based on the raw data is also
in our future work.
Conclusions
We have developed a method that combines the analysis
of versions with an ontology quality evaluation frame-
work. The main objective of this paper was to study how
the OQuaRE framework can support ontology evolution
processes by informing, from the perspective of ontology
quality, about the changes observed across the different
versions of an ontology.
The two scaling functions proposed in this work should
be jointly used for a better understanding of the engi-
neering and the evolution of an ontology. The static scale
is more useful when a single version of an ontology
needs to be inspected and evaluated from an engineer-
ing point of view, or when there are significant dif-
ferences between consecutive versions. However, when
the different versions of an ontology are less distinct
and evolution-oriented studies are our goal, the dynamic
scale is able to provide more information. If we assume
that the scaling function normalises the values regard-
less of the type of scale used, the values can be grouped
and compared as done in this work with the magnitude
of the change or the mean change between versions. It
should be noted that judging the evolution of an ontol-
ogy in terms of how its content conforms to the domain
that is to be represented by the ontology are beyond
the scope of this work. That would be the main objec-
tive of complementary methods such as realism-based
ones [35, 36].
Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 19 of 20
The application of themethod to the EDAM reveals that
good ontological engineering principles were applied in its
development. The analysis of changes in the quality scores
at both subcharacteristic andmetric levels have shown the
capability of the OQuaRE framework to identify weak-
nesses and strengths of the ontology. The OQuaRE met-
rics are capable of identifying changes in the engineering
of the different versions of the ontology. The design deci-
sions of the developers of the ontology have produced 18
versions of the EDAM ontology, and we have been able to
describe the impact of such decisions from the quality per-
spective provided by OQuaRE: the scores for four charac-
teristics increased, one characteristic remained invariant,
and the scores for two characteristics decreased. Further-
more, our study has found relations between the level of
class activity and the variability of quality scores for the
EDAM ontology. Evaluating the relation between these
changes in the quality scores and the design decisions
of the ontology developers is beyond the scope of the
present work. Our method provides the developers with
data they can use for evaluating whether their decisions
have the expected impact on the quality scores of the
ontology.
In summary, we believe that the OQuaRE framework
contributes to the engineering of the analysis of the evolu-
tion of ontologies and that provides relevant information
for developers about the evolution of their ontologies.
Abbreviations
EDAM: ontology of bioinformatics operations, types of data, formats, and
topics; OQuaRE: ontology quality requirements and evaluation; OO: object
oriented; PCA: principal component analysis; SQuaRE: software product quality
requirements and evaluation
Funding
This paper is an extension of the paper presented at ICBO 2015 [20]. This
project has been possible thanks to the Spanish Ministry of Science and
Innovation and the FEDER Programme through grants TIN2014-53749-C2-2-R,
BES-2011-046192 (MQM), and by the Fundación Séneca through grants
15295/PI/10 and 19371/PI/14.
Availability of data andmaterials
The description of the OQuaRE framework is available at http://miuras.inf.um.
es/oquarewiki. The OQuaRE web platform is available at http://sele.inf.um.es/
oquare. The ontologies used in this study and the complete set of results
included in this paper are available at http://miuras.inf.um.es/oquare/
jbsm2016.
Authors contributions
Conceived and designed the approach: ADR, MQM, MIM, JTFB, RS.
Implemented the approach and performed the experiments: MQM, ADR, MIM,
JTFB, RS. Analysed the results: ADR, MQM, MIM, JTFB, RS. Contributed to the
writing of the manuscript: MQM, ADR, MIM, JTFB, RS. All the authors have
approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Ethics approval and consent to participate
Not applicable.
Author details
1Universidad de Murcia, IMIB-Arrixaca, Campus de Espinardo, 30071 Murcia,
Spain. 2School of Computer Science, University of Manchester, Oxford Road,
M13 9PL Manchester, UK.
Received: 8 March 2016 Accepted: 2 August 2016
REVIEW Open Access
Reporting phenotypes in mouse models
when considering body size as a potential
confounder
Anika Oellrich1,2, Terrence F. Meehan3, Helen Parkinson4, Sirarat Sarntivijai4,5, Jacqueline K. White6
and Natasha A. Karp1*
Abstract
Genotype-phenotype studies aim to identify causative relationships between genes and phenotypes. The International
Mouse Phenotyping Consortium is a high throughput phenotyping program whose goal is to collect phenotype data
for a knockout mouse strain of every protein coding gene. The scale of the project requires an automatic analysis
pipeline to detect abnormal phenotypes, and disseminate the resulting gene-phenotype annotation data into public
resources. A body weight phenotype is a common result of knockout studies. As body weight correlates with many
other biological traits, this challenges the interpretation of related gene-phenotype associations. Co-correlation can
lead to gene-phenotype associations that are potentially misleading. Here we use statistical modelling to account for
body weight as a potential confounder to assess the impact. We find that there is a considerable impact on previously
established gene-phenotype associations due to an increase in sensitivity as well as the confounding effect.
We investigated the existing ontologies to represent this phenotypic information and we explored ways to
ontologically represent the results of the influence of confounders on gene-phenotype associations. With the
scale of data being disseminated within the high throughput programs and the range of downstream studies
that utilise these data, it is critical to consider how we improve the quality of the disseminated data and
provide a robust ontological representation.
Introduction
In genotype-phenotype studies, one approach to iden-
tify abnormal phenotypes is a statistical comparison
of data collected from control and gene-altered ani-
mals. In this paper we use the International Mouse
Phenotyping Consortium (IMPC) statistical analysis
pipeline as a use case study [1]. The goal of the
IMPC is to produce and phenotypically characterise
20,000 knockout mouse strains in a reproducible
manner across multiple research centres. This high-
throughput phenotyping is based on a pipeline con-
cept where a mouse is characterised in a series of
phenotype screens underpinned by standard operating
procedures defined by the IMPC in the International
Mouse Phenotyping Resource of Standardised Screens
(IMPReSS) resource [2]. This pipeline approach char-
acterises seven males and seven females for each
knockout line and results in data for over 200 physio-
logical variables that cover a variety of disease-related
and biological systems. As the scale of the program
requires the statistical analysis to be automated, we
have developed the statistical package PhenStat [3] to
analyse genotype-phenotype associations. In order to
provide a consistent representation of results, area ex-
perts have reviewed the IMPReSS screens and have
associated one or more terms from the Mammalian
Phenotype Ontology (MP) [4] with each variable. For
example, the variable fasted blood glucose concentra-
tion is associated to three MP terms: abnormal-,
increased-, and decreased- -fasted circulating glu-
cose level. Using this approach, abnormal phenotypes
identified via statistical analysis are summarised as
gene-phenotype associations, easily understood by the
biological community and facilitating dissemination to
* Correspondence: nk3@sanger.ac.uk
1Mouse Informatics Group, Wellcome Trust Sanger Institute, Hinxton,
Cambridgeshire, UK
Full list of author information is available at the end of the article
© 2016 Oellrich et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Oellrich et al. Journal of Biomedical Semantics  (2016) 7:2 
DOI 10.1186/s13326-016-0050-8
the community (Fig. 1). The current analysis pipeline
only takes sex into consideration when identifying ab-
normal phenotypes. Sharing these gene-phenotype anno-
tations also enables data mining across species and studies
e.g. for disease gene candidate discovery, pharmacogenet-
ics and evolutionary studies [57].
During the statistical comparison of control and gene-
altered data, confounding variables associated with both
the genotype change and the phenotype of interest can
lead to an association that is true but potentially bio-
logically misleading. The presence of the confounding
relationship can lead to errors in the estimates of the re-
lationship between the treatment of interest (here the
genotype change) and the variable of interest (here the
phenotype). Good experimental design can manage
many potential confounders using standardisation e.g.
with the potential confounder of age, the study would
only test animals of the same age. An alternative strategy
is randomisation, in which animals of multiple ages are
tested in both control and the experimental knockout
group. Yet another strategy is grouping (blocking) ac-
cording to a confounding variable (e.g. pup or adult).
Depending on the strategy applied, the final annotation
could be specific to one particular age. To minimise the
potential impact of confounders within IMPC, the com-
munity identified critical sources of variation in screens
and used this to develop a standardised operating pro-
cedure which, where possible, minimises variation and
captures potential sources of variation as metadata with
each dataset. Metadata parameters (e.g. X-ray equip-
ment) are included in the IMPReSS protocols and sub-
mitted metadata is used to determine comparison
groups as part of the statistical analysis pipeline.
In many research studies, it is not possible to man-
age confounding variables during the design. For ex-
ample, in many gene knockout studies, the knockout
animals show an abnormal body weight change.
Therefore, any other phenotypic traits (e.g. abnormal
body fat mass MP:0012320) that correlate with body
weight will also be impacted. As the experimenter
cannot control this potential confounder through the
design, it is necessary to consider statistical methods
for non-equivalent groups [8]. These include regres-
sion methods where the confounder is treated as a
covariate, meaning the statistical test will assess the
effect of the genotype on the phenotype after adjust-
ing for the confounders relationship. This requires a
dataset to be processed twice, first without and then
with the confounder in the statistical analysis; giving
two sets of results for the test of genotype. This
granularity has a high potential value to improve our
interpretation of the relationship between a gene and
associated phenotypes. However, the vast majority of
MP terms represent absolute phenotype changes in a
variable of interest. The Mouse Genome Informatics
database (MGI) [9] developed MP to manually curate
the scientific literature. However, only in rare, clear
cause and effect cases are confounding variables rep-
resented as part of the ontology. For example, the
term progressive muscle weakness (MP:0000748) is
defined as a muscle weakness that increases with
time. Time or age are clearly contributing to the se-
verity of the phenotype and thus represent knowledge
that should be represented in the ontology [10]. How-
ever, in many studies a confounding variable is noted
by authors to contribute to a phenotype, but a clear
cause and effect relationship is not established. The
current mechanism employed by MGI is to manage
confounders at the level of annotation by utilising
free text qualifiers. For example, the curator will note
if an author states body weight was a confounder
when associating a phenotype to a genotype. With
the scale of IMPC data and the automated aspect of
statistical analysis and subsequent annotation, we have
the potential to manage these issues in a consistent
way and through standardisation better support
downstream informatic analysis. The interest in in-
cluding body weight as a covariate, in both high
throughput phenotyping studies and small scale stud-
ies, is growing [8, 1113]. This manuscript aims to
raise awareness of the issues and demonstrate the po-
tential value of addressing the problems. We then
identify adaptations to the existing mechanisms
Statistical
analysis
Experiment
Reporting
MP
terms
Disease gene prediction
Application
PhenoDigmAnnotations
RawdataandAnnotations
Control
Gene-altered
Fig. 1 The phenotyping pipeline. The high throughput phenotyping
pipeline integrates a series of screens to assess the impact of the
genotype amendment on a variety of disease-related and biological
systems. Statistical analysis comparing data from the gene altered
and control animals allows the identification of abnormal phenotypes,
assignment of ontology annotation and dissemination of data to
public database for data mining across species and studies. IMPC
represents the International Mouse Phenotyping Consortium web
portal [26] where the data is collected, analysed and annotations
disseminated. Annotations are assigned using the Mammalian
phenotype ontology (MP)
Oellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 2 of 9
utilised by the community that could address this
new aspect where we wish to disseminate the out-
come of an analysis that considers body weight as a
confounder.
Data and scripts used to investigate and demonstrate
issues presented within this manuscript are available at
Zenodo [14].
Body weight as a confounder
Body weight is a highly heritable trait and is estimated to
be a potential latent variable in a third of experiments
studying knockout mice [11]. It has been shown that body
weight correlates with many variables, ranging from body
composition to clinical chemistry [15]. Including body
weight in the computational analysis allows the phenotype
to be assessed after adjusting for weight differences (see
Additional file 1: Supplementary Methods).
Dual analysis can lead to annotations that differ de-
pending on the analysis pipeline (Table 1) as one can
then assess whether the phenotype has changed in a
relative and absolute sense. For example, when the ab-
normality is due solely to correlation with a body weight
phenotype, then the inclusion of body weight as a covar-
iate adjusts for this confounding relationship and the
phenotype (as a relative term) would no longer be called
significant (Table 1 row 1). Alternatively, a line may only
have a significant abnormal annotation in the analysis
pipeline when body weight is included. The inclusion of
body weight accounts for more variation in the data, in-
creasing the sensitivity to detect other phenotypes
(Table 1, row 3). Lines can also be significant in both
analysis pipelines (Table 1, row 4), and this can arise
from two scenarios which differ in whether there is a
body weight difference or not. As the difference arises
from presence or absence of a body weight difference, it
could be argued that the interpretation could be driven
by the assessment of whether a body weight phenotype
was also annotated. However, a body weight phenotype
might be the reason statistically, but the abnormal body
weight annotation might not have been made due to low
statistical sensitivity (ability to detect a difference).
For example, consider the Dlg4 knockout mouse line
that has a reduced body weight phenotype (MP:0001262)
where we are also interested in assessing the impact of the
genotype change on body composition. As body compos-
ition variables such as lean mass (MP:00039590) are
dependent on the body weight, we would expect these to
be decreased as an absolute phenotype change (Fig. 2a
and b). When we include body weight in the analysis, we
find that the change in lean mass is as expected for the
change in body weight and determine that the phenotype
relative to body weight is not statistically significant
(Fig. 2c) (Equivalent to row 1 of Table 1). The knockout
gene Akt2 similarly has a body weight phenotype (Fig. 3a).
However, the inclusion of body weight in the analysis finds
that the relative lean mass is still statistically significant
(Fig. 3b-d) (Equivalent to row 4 of Table 1). By adding a
statistical step where we study the phenotype after adjust-
ing for body weight, we gain a more detailed understand-
ing of the impact of the genotype on the phenotype.
Even in cases where it is clear that body weight is truly
acting as a confounding variable and is not just explain-
ing data variance (Table 1, row 1), causality is not deter-
mined. For example, we cannot assess whether the lean
mass is lower in the Dlg4 line because the body weight
is fundamentally lower or because there is less lean mass
leading to a lower body weight. The refinement is there-
fore to consider the data and assess for both relative and
absolute changes and disseminate this richness.
Magnitude of impact and complexity
The Wellcome Trust Sanger Institutes (WTSI) Mouse
Genetics Project (MGP) is part of the IMPC community
effort to phenotype knockouts for all mouse protein cod-
ing genes [16]. To support the argument that we need to
consider body weight, we provide the results of a sup-
porting analysis of the WTSI MGP data (see Additional
file 1: Supplementary Methods for details). Firstly, we
demonstrate that for the majority of the dataset, weight
Table 1 Possible outcomes of a dual analysis process
Row A1 A2: + weight Conclusion Insight
1 + - Absolute phenotype ? No longer significantconfounded by BW
2 - - No abnormality
3 - + Relative phenotype ? Adding weight increases sensitivity to detect ?
4 + + Absolute phenotype ? and relative phenotype ? Two scenarios
1. BW difference: still there is a significant ? as ? larger/smaller than
expect for BW difference
2. BW same: a significant ?. Weight explains variation but does not
lead to phenotype difference.
Possible outcomes when assessing for a genotype effect for a variable of interest when the analysis excludes (A1) or includes body weight as a covariate (A2). In
this table, + indicates a statistically significant genotype effect;?indicates a non-significant genotype effect; ? indicates change; BW indicates body weight
Oellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 3 of 9
is often a significant source of variation (Fig. 4). This is
seen across biological processes and not only includes
screens that assess body composition but also screens
such as plasma chemistry. Secondly, this data allows us
to compare the impact of the dual analysis process
using the standard pipeline (A1) which does not ac-
count for weight, compared to the additional analysis
pipeline (A2) including body weight as a covariate. This
analysis demonstrates that including body weight has a
significant impact on the final abnormality annotations
(Fig. 5). We find that 70 % of the abnormal annotations
from the standard pipeline were also annotated when
we included body weight in the analysis. Furthermore,
we find that 30 % of annotations in the standard pipe-
line (A1) were no longer significant in A2 as they arose
from the confounding impact of body weight (equiva-
lent to row 1 of Table 1). 21 % of the annotations in A2
only occurred when body weight was included and
BA
Analysis Genotype role
p value
Genotype effect p value
Without weight (A1) 5.25e-6
Female*knockout =
-1.26±0.61
0.04017
Male*knockout =
-2.86±0.58
1.35e-6
With weight (A2) 0.29345 -0.44±0.39 0.257
C
Fig. 2 Example line Dlg4, where body weight confounds the phenotype. Body composition data were collected with a dual-energy X-ray
absorptiometry at 14 weeks of age for the Dlg4tm1e (EUCOMM) Wtsi/Dlg4tm1e (EUCOMM) Wtsi knockout line on the C57BL6/N genetic background.
The comparison was based on 249 female and 227 male wildtype mice and 7 female and 7 male knockout mice. a A scatterplot of the
lean mass readings for the control and knockout animals for the males. b A scatterplot of the lean mass readings for the control and
knockout animals for the females. c The genotype estimate with associated standard error and statistical significance when estimated using standard
methodology (A1: Analysis Pipeline 1) and then after inclusion of body weight as a covariate (A2: Analysis Pipeline 2). As there was evidence of sexual
dimorphism in the phenotype in A1, the genotype effect was estimated for male and female knockout mice separately. The scatter plots and analysis
highlight how a body weight phenotype is observed in both sexes of the knockout animals and as the lean mass is associated with body
weight, a statistically significant difference is seen in the lean mass until assessed as a relative abnormality
Oellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 4 of 9
arose from the increase in sensitivity from including
body weight (equivalent to row 3 of Table 1).
Challenges applying existing solutions
As demonstrated with the provided analysis, taking con-
founding variables such as body weight into account
may lead to a more comprehensive dataset and should
be further investigated (see Table 1). The dissemination
of the resulting annotation data is achieved through a
collaboration between different communities. IMPC cur-
rently uses MP to annotate genes with phenotypes. MP
is a pre-composed phenotype ontology in which every
concept semantically describes one particular phenotype,
e.g. decreased lean body mass (MP:0003961). While this
paper generalises to gene-phenotype annotations, MGI
distinguishes further the additional data such as the gen-
etic background or the sex if there is a difference be-
tween male and female mice. While the majority of the
annotations contained in MGI do not take confounders
into consideration, sex in the presence of sexual di-
morphism could be regarded as such and is captured at
times in MGI. For example, the gene Dmxl2 [17]
BA
Analysis Genotype 
role p value
Genotype 
effect
Without weight (A1) 0.00e-7 -5.41±0.36
With weight (A2) 4.02e-3 -1.01±0.34
DC
WTAkt2-/-
Fig. 3 Example line Akt2, where body weight confuses the phenotype interpretation. Body composition data were collected with dual-energy
X-ray absorptiometry at 14 weeks of age for the Akt2tm1e (KOMP) Wtsi/Akt2tm1e (KOMP) Wtsi knockout line on the 129S5/SvEvBrd/Wtsi;129S7/SvEvBrd/Wtsi
genetic background. The comparison was based on 71 female and 84 male wildtype mice and 12 female and 14 male knockout mice. a A scatterplot
of the lean mass readings for the wildtype and knockout animals for the males. b A scatterplot of the lean mass readings for the wildtype and
knockout animals for the females. c Representative photograph demonstrating body weight phenotype. d The genotype estimate with associated
standard error and statistical significance when estimated using the standard methodology (A1: Analysis Pipeline 1) and then after inclusion of
body weight as a covariate (A2: Analysis Pipeline 2). The scatterplots of the lean mass against body weight highlight that there is a clear body
weight phenotype and the difference between the knockouts and wildtype mice cannot be fully explained by the association between lean mass and
body weight
Oellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 5 of 9
exhibits sexual dimorphism such that the phenotype was
only found to be significant in the females of heterozy-
gous mice and this is recorded as a curator note.
Body weight is not the only variable that could be used
to adjust for the size of the animal; alternatives include
body length or width. Adjustment for body size as a con-
founder has unique challenges (see section Body weight
as a confounder) and particular issues with determin-
ing causality. Thus, we investigated solutions for the
standardised reporting of phenotypes after considering
body weight as a confounder as a relative phenotype
change within existing semantic frameworks and report
our findings here. Potential solutions were limited to
those we believed could be implemented as they had the
lowest modification requirements on the existing dis-
semination pipelines, such as those maintained by MGI.
We note that the discussed solutions only focus on fu-
ture dissemination but do not include strategies on how
to deal with legacy data.
Use of pre-composed ontologies
As mentioned before, the vast majority of phenotypes
represented in the current version of MP constitute
absolute changes that cannot readily be applied to
confounder-adjusted phenotypes. In order to represent
the results of a confounder-sensitive analysis, additional
MP concepts would be needed that would allow a user
to report relative phenotype changes (see column 2,
Table 1, rows 3 and 4). For example, to represent the
changes in the absolute and relative changes in mouse
line Dlg4, we would need the additional concept relative
increase in lean body mass after body weight adjust-
ment. However, pre-composing concepts for relative
phenotype changes would mean that for each phenotype
that is influenced by one or multiple confounders (e.g.
body size or length), multiple concepts for each unique
phenotype-confounder relationship would need to be
added (abnormal/increased/decreased). This would lead
to a vast increase in the number of terms (i.e. term ex-
plosion) that need to be added and maintained within
MP, which would be untenable. This may also be confus-
ing for the user community of curators and annotators
as the number and complexity of terms exposed for
search and/or annotation grows.
Tagging pre-composed terms
An adaption to the pre-composed term is to associate an
attribute to the annotation by addition of free text tags.
This is equivalent to the current implementation used in
literature curation at MGI. For example, a gene could
possess an annotation increased lean body mass, with
an annotation or tag on this annotation detailing if any/
Modelling that included weight (%)
N
um
be
r 
of
 v
ar
ia
bl
es
0 20 40 60 80 100
0
5
10
15
20
25
Fig. 4 The inclusion of weight as a source of variation. The
distribution of weight inclusion in the PhenStat analysis of 85086
control-knockout datasets which covers 154 variables (average
number datasets = 552) from the high throughput phenotyping
data collected at the WTSI MGP. The PhenStat analysis was completed
using the Mixed Model framework with a starting model that included
weight. The model optimisation process means that the final
model will only include weight if it is statistically significant in
explaining variation in the data (p < 0.05)
- Weight + Weight
Tested: 85086
1703751 521
Fig. 5 The impact of including body weight as a covariate on
abnormal phenotype annotations. The relationship between the
abnormal phenotype annotations made when assessing for a
genotype effect by processing through A1 (standard statistical
analysis pipeline) and A2 (statistical analysis including body
weight as a covariate). The analysis used a mixed model method
implemented within PhenStat [9] on data collected by the WTSI
MGP (for more details see Additional file 1: Supplementary Methods).
Shown in red are those annotations, where the phenotype difference
was due to the confounding effect of body weight (row 1 of Table 1).
Shown in green are those annotations where adding weight to the
analysis has increased sensitivity (row 3 of Table 1). Shown in yellow
are annotations made in common by both pipelines (row 4 of Table 1).
Data available from Zenodo [14]
Oellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 6 of 9
which confounder has been used for adjustment, e.g.
after adjusting for body weight. However, as the tags
are not standardised this may result in non-comparable
annotations of genes and an increase in curatorial work-
load. Furthermore, informatics tools are not capable of
interpreting tags of gene annotations and may lead to er-
roneous presumptions (in the case of a relative change
after confounder adjustment that would not be reported
with absolute changes only; row 3, Table 1).
In order to disseminate relative phenotype changes to
the broader community using tagged pre-composed
phenotype ontology annotations, existing gene-annotation
databases need to be able to store this additional data and
expose this for query. This may require not only changes
to the database itself, but also to web interfaces as well as
services for data download, in addition to strategies for
handling legacy data.
Standardised qualifiers of pre-composed term
A refinement to the preceding method, is to add stan-
dardised qualifiers to the genotype-phenotype annota-
tion. One ontology that can be used to represent these
standardised qualifiers is the Phenotype And Trait
Ontology (PATO) [18, 19]. The difference between this
solution and the previous is that the free text tag is re-
placed with an ontology term. This suggestion is similar
to how sexually dimorphic associations are currently
treated. For example, Kcne2 knockout mice have a num-
ber of abnormalities that are specific to the male mice
and this is captured as a MP term with associated sex
classification tag [20]. The advantage of this solution is
that the variability that may occur with free-text tags is
reduced to a defined set of ontology concepts. However,
following this solution would need an agreed set of on-
tologies used for the annotation of relative changes and
possibly extension to these to account for all possible
confounders.
Similar to the latter approach, third parties such as
MGI can then choose to add these additional annota-
tions to their data storage to hold the information for
relative phenotype changes. This may mean that data-
base schemes as well as provision and distribution
methods need to be adapted to handle the additional
data and be able to distinguish between absolute and
relative phenotype changes. If these changes were to be
integrated in existing databases, ways of handling legacy
data need to be taken into consideration.
Post-composed phenotypes
An alternative to pre-composed phenotype annotations
is the use of post-composed phenotypes. One method to
post-composed phenotypes are entity-quality statements
[18, 19], where the phenotype is broken down into an af-
fected entity and a quality describing the entity further,
e.g. increased body weight (MP:0001260) would be
broken down into the entity multicellular organism
(UBERON:0000468, UBERON is a species-agnostic anat-
omy ontology) [21] and the quality increased weight
(PATO:0000582). The following example illustrates how
a post-composed ontology-representation could be used
to represent a relative phenotype change:
Entity 1: lean body mass
Quality: relative to
Entity 2: body weight
Qualifier: increased
Applying a post-composed representation to confounder-
adjusted phenotypes may lead to multiple sets of annota-
tions to the same set of data as it still needs to be created
for each confounder. Where required (e.g. Table 1, row 4),
the absolute phenotype change could then be added as it
has been done so far with MP annotations or if desired,
uniformly with post-composed phenotype annotations.
Representation of confounder association with RDF triple
representation
The Standardised qualifiers of pre-composed terms
approach could be formally represented with the Re-
source Description Framework (RDF) triple model [22].
In an RDF triple, the annotation conforms to the format
of < subject, predicate, object>. In our scenarios this
would be an MP term as the subject which would be re-
lated to the confounder body weight (the object) via the
relationship specified as the relative to (the predicate).
The triple representation is only needed in the annota-
tion arising from including the potential confounders as
covariates in the analysis and is a natural extension of
the preceding approach Post composed phenotypes.
There are multiple advantages of using RDF models.
The first advantage arises from the graphical nature of
ontologies in which the inter-relationships of multiple
tiers are captured with a graph schema. In an ontology,
a class can have multiple parents leading to the inherit-
ance of qualities from different parents, which can be
well and efficiently defined within RDF models. The
alternative of storing this information is to use a Rela-
tional Database Management Systems (RDBMS). In
RDBMS, a table scheme is used which faces the compu-
tational challenges of multiple joins when querying
across many tables and is therefore less scalable. The
second advantage is that RDF is a well-established com-
munity standard recommended by the World Wide
Web Consortium (W3C) [22] and is readily extendible.
For example, the same MP term can be associated to
other confounders (e.g. body length) using the same
predicate. This common structure will lead to a robust
data model which will improve efficiency when searching
Oellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 7 of 9
for information. The Ontology for Biomedical AssociatioN
(OBAN) is an example of an RDF implementation and
has been successfully exploited to represent disease-
phenotype associations [23] (Extended version will be
published within this special issue) [24].
RDF triples can be stored within relational as well as
graphical databases and data queries are performed with
the SPARQL query language [25]. In consequence, one
would need to understand the technology and the query
language to work with the data effectively, through
provision of a (non-SPARQL) Application Program
Interface (API) would address this for accessing the data.
Conclusions and future perspectives
In gene-phenotype studies, we have identified challenges
with reporting phenotypes after adjusting for body
weight using currently available semantic data represen-
tation frameworks. Weight is a complex confounder, as
it cannot be controlled within the experiment and caus-
ality cannot be determined. However, analysing the data
with and without body weight returns a richer under-
standing of the phenotypic abnormality. With interest
growing in the impact of body weight on phenotypes
and the scale of projects being conducted by high
throughput phenotyping consortiums, being able to dis-
seminate annotated phenotype data has become an im-
portant issue. We have demonstrated that the impact of
including weight as a confounder in the analysis has sig-
nificant impact on the annotations returned. While this
example focuses on the description of mouse pheno-
types, we perceive that this is a general problem with
accessing phenotypes in all mammals including humans.
The current solution implemented with mouse data has
arisen from adapting the mechanisms developed for cur-
ating literature to a high throughput scenario and use of
the ontology for analyses.
We coordinated our efforts with Medical Research
Council (MRC) Harwell and MGI in discussions on re-
fining annotation in high throughput phenotyping stud-
ies, where MRC Harwell focused on aging studies and
how to manage time course studies [10]. The issues were
determined to be distinct, as the interpretation is more
complex when considering body weight as a confounder.
The complexity arises as we cannot determine causality,
rather we are annotating the outcome of the statistical
analyses.
In the process of this study, we were able to identify
several possible solutions (see Challenges applying
existing solutions) that could help with applying
confounder-relevant information to gene-phenotype as-
sociations. These options have been limited to what we
believe have the lowest modification requirements on
existing dissemination pipelines, such as those main-
tained by MGI. However, each of these outlined options
have to be assessed now in the broader community to
arrive at a conclusion what is the best to pursue.
In future work, we aim to not only communicate with
the broader community to find the most suitable solu-
tion, but also to assess the impact for other potential
confounders not just body weight. These additional con-
founders will then be verified with what has been deter-
mined as the best solution to see that it can scale with
the demands of the different confounders.
While we have assessed in this study the impact of con-
founders of gene-phenotype associations in mouse, this is
highly likely to be equally relevant in other mammalian
model organisms (e.g. rat). However, we identified
practical solutions based on the mouse annotation-
dissemination pathways and these might not be the
optimal for other model organisms. The discussions
within this manuscript are a good starting point for
managing confounder in their community.
Additional file
Additional file 1: Supplementary Methods. (DOCX 21 kb)
Abbreviations
A1: analysis pipeline 1; A2: analysis pipeline 2; API: application program
interface ; BW: body weight; IMPC: international mouse phenotyping
consortium; IMPReSS: international mouse phenotyping resource of
standardised screens; MGI: mouse genome informatics database;
MGP: mouse genetics project; MP: mammalian phenotype ontology;
MRC: medical research council; OBAN: ontology for biomedical association;
PATO: phenotype and trait ontology; RDBMS: relational database
management systems; RDF: resource description framework ; WTSI: wellcome
trust sanger institute.
Competing interests
The authors have declared that no competing interests exist.
Authors contributions
NAK conceived the question and executed the statistical analysis. AO, NAK,
SS, TFM and HP analysed several ontological aspects that form the
discussions in the Challenges applying existing solutions section of this
paper. JKW managed the WTSI MGP phenotyping pipelines generating the
data used in the analysis. All authors have contributed to the writing of this
paper. All authors read and approved the final manuscript.
Acknowledgements
We thank staff from the Sanger Institutes Research Support Facility, Mouse
Genetics Project and Mouse Informatics Group for their excellent support.
NAK, AO, TFM were funded by the National Institutes of Health Common
Fund grant: (NIH) [1 U54 HG006370-01], HP is funded by EMBL-EBI Core
funds. SS is funded by the Centre for Therapeutic Target Validation. In
addition, the individuals affiliated to the WTSI institute were also funded by
the Wellcome Trust grant: WT098051.
We thank Cynthia Smith, Susan Bello and Janan Eppig from the Jackson
Laboratory, James Malone of FactBio Ltd, and Ann-Marie Mallon and Michelle
Simon from MRC Harwell for helpful discussion on these issues.
The funders had no role in study design, data collection and analysis,
decision to publish or preparation of the manuscript
Author details
1Mouse Informatics Group, Wellcome Trust Sanger Institute, Hinxton,
Cambridgeshire, UK. 2Social Genetic & Developmental Psychiatry, Kings
College London, London, UK. 3Samples, Phenotypes and Ontologies,
European Molecular Biology LaboratoryEuropean Bioinformatics Institute,
Oellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 8 of 9
Hinxton, Cambridge, UK. 4Samples, Phenotypes and Ontologies, European
Bioinformatics Institute (EMBL-EBI), European Molecular Biology Laboratory,
Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK. 5The
Centre for Therapeutic Target Validation, Wellcome Trust Genome Campus,
Hinxton, Cambridge CB10 1SD, UK. 6Mouse Genetics Project, Wellcome Trust
Sanger Institute, Hinxton, Cambridgeshire, UK.
Received: 21 October 2015 Accepted: 2 February 2016
SOFTWARE Open Access
Knowledge Author: facilitating user-driven,
domain content development to support
clinical information extraction
William Scuba1, Melissa Tharp1, Danielle Mowery1, Eugene Tseytlin2, Yang Liu3, Frank A. Drews4
and Wendy W. Chapman1*
Abstract
Background: Clinical Natural Language Processing (NLP) systems require a semantic schema comprised of
domain-specific concepts, their lexical variants, and associated modifiers to accurately extract information from
clinical texts. An NLP system leverages this schema to structure concepts and extract meaning from the free texts.
In the clinical domain, creating a semantic schema typically requires input from both a domain expert, such as a
clinician, and an NLP expert who will represent clinical concepts created from the clinicians domain expertise
into a computable format usable by an NLP system. The goal of this work is to develop a web-based tool,
Knowledge Author, that bridges the gap between the clinical domain expert and the NLP system development
by facilitating the development of domain content represented in a semantic schema for extracting information
from clinical free-text.
Results: Knowledge Author is a web-based, recommendation system that supports users in developing domain
content necessary for clinical NLP applications. Knowledge Authors schematic model leverages a set of semantic
types derived from the Secondary Use Clinical Element Models and the Common Type System to allow the user
to quickly create and modify domain-related concepts. Features such as collaborative development and providing
domain content suggestions through the mapping of concepts to the Unified Medical Language System
Metathesaurus database further supports the domain content creation process.
Two proof of concept studies were performed to evaluate the systems performance. The first study evaluated
Knowledge Authors flexibility to create a broad range of concepts. A dataset of 115 concepts was created of
which 87 (76 %) were able to be created using Knowledge Author. The second study evaluated the effectiveness
of Knowledge Authors output in an NLP system by extracting concepts and associated modifiers representing a
clinical element, carotid stenosis, from 34 clinical free-text radiology reports using Knowledge Author and an NLP
system, pyConText. Knowledge Authors domain content produced high recall for concepts (targeted findings:
86 %) and varied recall for modifiers (certainty: 91 % sidedness: 80 %, neurovascular anatomy: 46 %).
Conclusion: Knowledge Author can support clinical domain content development for information extraction by
supporting semantic schema creation by domain experts.
Keywords: Natural Language Processing, Information extraction, Semantics, Knowledge representation, Unified
Medical Language System
* Correspondence: wendy.chapman@utah.edu
1Department of Biomedical Informatics, University of Utah, Salt Lake City, UT
84108, USA
Full list of author information is available at the end of the article
© 2016 Scuba et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Scuba et al. Journal of Biomedical Semantics  (2016) 7:42 
DOI 10.1186/s13326-016-0086-9
Background
Natural language processing
Natural Language Processing (NLP) provides a set of
computational methods and techniques for automatically
extracting and structuring information from free-text
documents. NLP research has been successfully applied
to free texts for several applications ranging from se-
mantic search to information extraction to text analytics
[13]. The development and availability of biomedical
knowledge resources such as the Unified Medical Language
System [4], has enabled biomedical NLP to move beyond
retrieval and classification to modeling of semantic predi-
cates represented in the literature [5]. Within the clinical
domain, NLP systems have been implemented to support
pharmaco-vigilance, patient screening, patient narrative
summarization, and quality improvement [613]. The de-
velopment of text processing pipelines and components
specific to clinical text such as the Medical Language
Extraction and Encoding System (MedLEE) [14], clinical
Text Analysis and Knowledge Extraction System (cTAKES)
[15], and Health Information Text Extraction [16] have per-
mitted the analysis of clinical free texts e.g., emergency
department notes, radiology reports, etc. using lexical, syn-
tactic, and semantic information [17].
Ontologies
NLP tools designed to support information extraction
routinely use the Web Ontology Language (OWL) to
provide a structured way to represent domain content
[1821]. In order for an NLP tool to use a given domain
ontology, the tool must contain code to parse and inter-
pret the data model represented in the ontology. This
creates a close coupling between the ontology and the
NLP tool. It is generally not possible to directly share
the domain ontology used in one NLP tool with another
NLP tool and semantic schematic changes are not easily
propagated between tools. To help resolve this issue of
incompatibility, a common type system [22] was devel-
oped which provides a common framework to create on-
tologies across a range of clinical domains. Our lab has
converted the common type system described by Wu et
al. into OWL format and extended its content using the
Secondary Use Clinical Element Models (Secondary Use
CEMs) [23]. We use this new OWL-base common type
system, which we call the Schema Ontology, as the
framework to create domain specific ontologies.
The Schema Ontology can be loaded into Protégé [24]
or other OWL editors and used as the template for do-
main ontology creation. Domain ontology creation in this
manner, however, requires deep understanding of OWL
and an understanding of the structure of the Schema
Ontology data model. This creates a potentially burden-
some learning curve for those users who simply want to
create Schema-Ontology-based domain ontologies and
have little training in ontology development. To improve
ease of use and support wide-spread adoption of the
Schema Ontology, a system which minimizes complexity
and allows simple interaction for users is needed. Know-
ledge Author provides a simple user interface to guide
users in development of complex domain ontologies.
Furthermore, a domain ontology development tool that
supports collaborative editing and has built-in access to
the UMLS would speed up the domain ontology cre-
ation process. Many OWL editors, such as Protégé or
NeOn [25] allow user-created plugins to extend their
functionality; however, there are no editors that are suf-
ficiently modifiable to support all of this desired func-
tionality. In this paper we introduce Knowledge Author
which provides a web-based interface that is simple to
use, facilitates domain content development with direct
UMLS terminology lookup, and supports collaborative
domain content creation.
Implementation
Terminology
The terminology used in the domains of clinical NLP
and ontology creation can often vary; however, for the
purposes of this paper the following terms are defined
as such:
 Semantic Schema  the target extraction template
for an NLP tool.
 Atomic Concept  a concept found in a standardized
terminology such as the UMLS. For example 
PNEUMONIA, TEMPERATURE, COUGH, or
IBUPROFEN.
 Lexical Variant  a lexical variant is another way of
phrasing an atomic concept or modifier in the clinical
text. This can include synonyms, misspellings and
abbreviations. For example  two lexical variants for
TEMPERATURE are temperature and temp.
 Modifier  additional information that narrows
down, or modifies an atomic concept. Knowledge
Author separates the modifiers into two distinct
types  shared and semantic. Shared modifiers are
applicable to all concepts (with the exception of
Person concepts which has its own unique set of
modifiers such as age, race and gender). Semantic
modifiers vary depending on the semantic type
associated with the concept. For example, a concept
with a semantic type of Medication will have a
different set of available modifiers than a concept
with semantic type Vital Sign.
 Concept  the combination of the atomic concept
with its associated modifiers and their lexical variants.
For example  80 mg Ibuprofen is comprised of the
atomic concept IBUPROFEN and semantic modifiers
of Dosage: 80 mg. Lexical variants for IBUPROFEN
Scuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 2 of 11
can include Advil, Midol, Motrin, Ibu, and
Ibuprofen, etc. Lexical variants for Dosage can
include 80 mg, 80 mg, and 0.08 g, etc.
Knowledge Author overview
The overall goal of Knowledge Author is to aid a user in
quickly creating a semantic schema, which is the target
extraction template for a clinical NLP tool. The semantic
schema represents salient concepts of interest to be ex-
tracted from the clinical text. It contains a list of atomic
concepts, associated modifiers and the lexical variants for
those concepts and modifiers. It is the job of the NLP sys-
tem to extract words and phrases associated with these
concepts and modifiers from the clinical text then map
this information to the concepts in the semantic schema.
Knowledge Author provides a web-based graphical user
interface that guides the user in developing a semantic
schema, which is output as an OWL ontology. Knowledge
Author standardizes the semantic schema creation process
by constraining concept creation to a set of standard se-
mantic types (e.g., Procedure, Medication) and by only
allowing the user to assign a pre-defined set of modifiers
to the concept. The semantic types and modifiers are
based on the Secondary Use Clinical Element Models and
the Common Type System (CTS). The Secondary Use
CEMS are semantic types and modifier sets used for com-
puterized provider entry and secondary use of clinical
data, and the CTS are semantic type sets used for infor-
mation extraction from clinical text. By adhering to a stan-
dardized data model it becomes possible to use the output
of Knowledge Author in any NLP system which imple-
ments that model.
Knowledge Author also supports the semantic schema
creation process by:
 Providing domain content suggestions through
mapping of user-created concepts to concepts in the
UMLS Metathesaurus database. This allows the auto-
matic import of synonyms, concept definition, and se-
mantic type into the Knowledge Author interface.
 Supporting modifier creation through the use of
dropdown menus and the filtering of possible
modifiers to only those relevant to a given concept
type. Dropdown menus are possible because the
Knowledge Author data model has a fixed set of
allowable modifiers.
 Allowing the user to store and share their semantic
schemas in an organized way.
 Supporting collaborative development of domain
content.
Using Knowledge Author
To illustrate the use of Knowledge Author, the creation
of an example semantic schema for carotid stenosis will
be walked through. Figure 1 illustrates the Knowledge
Author workflow to be described below.
Defining a concept
The first step in Knowledge Author is to create a con-
cept. Knowledge Author supports creation of two types
of concepts: Person and Event. A Person concept can be
defined with modifiers such as birth date, death date,
race, age, and gender to facilitate creation of complex
Fig. 1 Illustrates the common set of steps to create a semantic
schema using Knowledge Author. It is not required to map a concept
to UMLS terminology as the synonyms, definition and semantic type
can be entered in manually through the Knowledge Author interface
Scuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 3 of 11
concepts such as African American females above
65 years of age.
The carotid stenosis use case only has Event concepts.
To create the first concept  aneurysm  the new con-
cept button + (Fig. 2a) was clicked and the concept
name, aneurysm, was entered. Upon saving the new
concept, the Terminology Lookup button (Fig. 2b) be-
comes available. Clicking that button allows the user to
search the UMLS Metathesaurus for the concept name
and displays a list of potential matches (Fig. 3). For this
concept there is a UMLS atomic concept ANEURYSM
which we choose. Knowledge Author will now download
the definition, synonyms, semantic type and Concept
Unique Identifier (CUI) for that atomic concept. All
imported information can be changed, deleted, or sup-
plemented as necessary. For the carotid stenosis ex-
ample, twenty-six of the twenty-eight concepts were able
to be mapped to UMLS concepts.
Choosing a semantic type
The next step is to assign a semantic type to the con-
cept. If the concept is mapped to a UMLS atomic con-
cept, the semantic type for the atomic concept will have
already been downloaded and assigned to the concept
(Fig. 2d). If not, the user can manually assign a semantic
type. In the context of Knowledge Author, there are two
types of modifiers  shared and semantic. The semantic
type determines which type of semantic modifiers can
be assigned to the concept.
Selecting semantic modifiers
Semantic modifiers are a type of modifier that is associ-
ated with specific semantic types. Each semantic type
contains a number of possible semantic modifiers based
on the Secondary Use CEMs and CTS. Each of the se-
mantic modifiers has, in turn, a number of possible
values associated with it. For example, the semantic type
Medication allows the user to choose from semantic
modifiers such as dosage or delivery route. The delivery
route modifier has a number of possible values such as
oral or intravenous. Table 1 lists the 12 semantic types,
the modifier classes associated with each semantic type
and the number of semantic modifiers associated with
each modifier class.
Semantic modifier values can either be chosen from a
dropdown list, or for the case of numeric values, entered
directly into an editable text box. Some modifiers, such
as medication dosage, consist of two numeric value
boxes and a dropdown list. The numeric value boxes
allow the user to specify a value range, and the drop-
down list is for units (Fig. 4). For example the user could
create a concept for 80 to 100 mg Ibuprofen (Fig. 4). By
leaving one or the other numeric value box empty con-
cepts such as >80 mg Ibuprofen, or <80 mg Ibuprofen
can be created. To create a single numeric value such as
80 mg Ibuprofen, enter the same number into both
boxes. For the aneurysm concept created earlier, only
the mild form is of interest so the sematic modifier of se-
verity is enabled, and the value of mild is chosen from
the dropdown list.
Fig. 2 Knowledge Author concept creation interface. The large red letters with arrows point out a) concept creation button; b) terminology
lookup button; c) shared modifiers; d) semantic type; e) concept list
Scuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 4 of 11
Selecting shared modifiers
A user can also narrow the definition of a concept
through the use of shared modifiers. For all Event con-
cepts, Knowledge Author allows the user to specify the
temporality (whether the concept occurs in the past,
present, or future), certainty (whether the concept is
asserted, negated, or hedged), and experiencer (whether
the patient or someone else experiences the concept)
(Fig. 2c). Several other shared modifiers are also avail-
able (Table 2). For the carotid stenosis example, a con-
cept for no occlusion is needed, so a new concept is
created and linked to the atomic concept, OCCLUSION,
which is then assigned the lexical variant for shared
modifier for certainty: no from the certainty dropdown
list (Fig. 5). The user could also use shared modifiers to
create concepts such as family history of breast cancer
or probable chest pain.
Building a semantic schema
Once a concept is created and saved, the + button is
clicked to create a new concept and the process de-
scribed above is repeated. A concept, once created, is
added to the concept list on the left hand side of the
Knowledge Author GUI (Fig. 2e). The concept list can
be arranged by the order in which the concepts were
created, or by the semantic type they belong to. Once all
of the concepts are created, the user can export the se-
mantic schema for use in an NLP system.
Exporting data
As the user works, Knowledge Author saves the users
work to an internal database that is available upon login.
Once all of the domain content is entered into Know-
ledge Author, the user can choose to export the data for
use in an NLP system. The Export button at the top of
the application will prompt the user to save the output
file to their computer.
The file exported from Knowledge Author is OWL
based and imports and uses the classes defined in the
Schema Ontology file. This file contains the semantic
categories and modifiers used by the interface as classes.
The Schema Ontology is the base ontology file that or-
ganizes these classes into appropriate hierarchies. This
Schema Ontology file is then imported into every new
domain ontology created by Knowledge Author. During
the export process, each of the concepts is exported as a
subclass of the appropriate semantic category class (i.e.,
mild aneurysm is a subclass of the "Problem" class found
in the Schema Ontology). All of the concept metadata
(i.e., synonyms, misspellings, preferred term, CUI from
UMLS, etc.) is added as annotation properties to that
class. The modifiers are added as restrictions on the
concept class (i.e., mild aneurysm has the restriction
"hasSemAttribute some Mild_Severity"). Therefore, all of
the data gathered by the Knowledge Author user inter-
face is transformed into an ontological representation
that can be parsed by a compatible NLP system.
Fig. 3 UMLS terminology lookup interface
Scuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 5 of 11
Table 1 Semantic types, modifier classes, and modifiers available to the user
Semantic Type Modifier Class # of Modifiers Sample of Modifiers
Allergy Intolerance Allergy/Intolerance Type 2 allergy, intolerance
Allergen unlimited any drug or food concept
Severity 7 mild, moderate, severe
Anatomical Site Body Side 3 right, left, bilateral
Body Laterality 33 dorsal, medial, superior
Disease Disorder Course 37 increased, worsened, maintained
Severity 7 mild, moderate, severe
Encounter From Location unlimited home, ER, SICU, nursing home
To Location unlimited home, ER, SICU, nursing home
Lab/Test/Measurement Abnormal Interpretation 3 abnormal, not abnormal, very abnormal
Delta Flag 8 changed, unchanged, increased
Lab/Test/Measurement Value unlimited 500 cc, 100 kg, 12000 WBCs
Ordinal Interpretation 35 excessive, high, low, positive
Medication Medication Form 27 capsule, cream, liquid, tablet, pill
Medication Route 21 inhalation, intradermal, oral
Medication Strength unlimited 500 mg
Status Change 8 changed, unchanged, increased
Dosage unlimited 250 mg, 16 units
Patient Demographic Birth Date unlimited
Death Date unlimited
Age unlimited
Gender 2
First Name unlimited
Last Name unlimited
Middle Name unlimited
Problem Course 37 increased, worsened, maintained
Severity 7 mild, moderate, severe
Procedure Intervention Delta Flag 8 changed, unchanged, increased
Procedure Completion 3 complete, incomplete, N/A
Procedure/Intervention Device unlimited
Procedure/Intervention Method unlimited arthroscopic surgery
Sign or Symptom Course 37 increased, worsened, maintained
Severity 7 mild, moderate, severe
Social Risk Factor Delta Flag 8 changed, unchanged, increased
Social Risk Qualifier 6 occasional, frequent, social
Social Risk Quantity unlimited 5 packs, 3 drinks
Social Risk Status 5 former risk, current risk
Vital Sign Abnormal Interpretation 3 abnormal, not abnormal, very abnormal
Delta Flag 8 changed, unchanged, increased
Ordinal Interpretation 37 excessive, high, low, positive
Vital Sign Value unlimited 19 bpm, 86 %, 101.4 F
Scuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 6 of 11
It is also of note that the Knowledge Author output
file can be viewed and modified directly by any OWL
editor such as Protégé. This could be useful for users
who want to use the Knowledge Author feature set, such
as UMLS terminology mapping, semantic schema man-
agement, and dropdown lists, but have a small number
of concepts with rare features that are not currently sup-
ported in Knowledge Author. Those concepts could be
added by hand using the OWL editor.
Collaborative development and semantic schema
management
Over time a user can develop a large number of seman-
tic schemas. Each schema a user creates is saved to the
Knowledge Author database and is accessible to the user
upon login. The five most recent schemas a user worked
on are displayed in the quick launch window. All other
schemas can be viewed in a searchable table.
A semantic schema can be designated by the creator
as either public or private. Public schemas can be
viewed and edited by anyone using Knowledge Author.
This allows multiple users to work on the same
schema. It also allows for the creation of a library of
public schemas which can be used as the starting
point for building a new schema in a similar domain.
Private schemas can only be viewed and edited by the
original creator.
Software tools and specifications
Knowledge Author is a web-based platform written in
Java 7 on top of a MySQL database. It runs on an
Apache Tomcat 7 Server. The SeaCore [26] framework
is used to facilitate the web development. The UMLS
terminology is accessed through both the use of a local
copy of the UMLS database and the Java based UMLS
Terminology Service API 2.0 [27] which queries a re-
mote UMLS Metathesaurus service. The mapping of a
users concept to a UMLS atomic concept uses the
UMLS Terminology Service API because of the com-
plexity of performing that operation. The synonyms, def-
inition, and semantic type for a concept are retrieved
from the local copy of the UMLS for speed. The OWL
API 3.4 [28] is used for converting the semantic schemas
to OWL XML.
Integration with existing NLP tools
Currently, only the pyConText [29] NLP system accepts
the output from Knowledge Author as input. Work is
also underway to integrate cTAKES and a developmen-
tal system called Moonstone [30] with the Knowledge
Author output.
Results and discussion
Knowledge Author standardizes the concept creation
process by constraining the semantic types and modi-
fiers that can be assigned to a concept to a discreet set.
This enables the use of dropdown lists for assigning
modifiers and allows for a standard output format which
makes it possible to build NLP systems that use the out-
put directly. We conducted two proof-of-concept stud-
ies, using different datasets, to assess the usability of
Knowledge Author by demonstrating that (a) the user
interface is sufficiently flexible to allow for the creation of
most concepts a user will want to create and (b) the out-
put of Knowledge Author can be utilized by an NLP sys-
tem to produce viable results.
Fig. 4 Semantic modifier interface box showing numeric range input boxes with units dropdown list
Table 2 Shared modifiers available to the user
Category Shared Modifiers
Certainty Definite Existence, Definite Negated Existence, Probable
Existence, Probable Negated Existence
Experiencer Patient, Family Member, Donor Family Member, Donor
Other Member, Other Member
Temporality Before, Before-Overlap, Overlap, After
Contextual
Aspect
Continues, Initiates, Intermittent, Novel, Reinitiates,
Terminates
Contextual
Modality
Hypothetical, Conditional
Degree Little, Most
Permanence Finite, Permanent
Scuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 7 of 11
User interface flexibility assessment
We assessed the flexibility of the Knowledge Author
user interface by assembling a dataset of 115 concepts to
be created using Knowledge Author. The Additional file
1 contains a full list of the concepts. The concepts were
drawn from three disease or procedure areas: pneumo-
nia, colonoscopy quality, and influenza. The concepts
were selected to cover a range of complexity and provide
a broad view of the types of concepts that can and can-
not be created using Knowledge Author.
In order to assess whether or not the required con-
cepts could be created using Knowledge Author, we con-
sidered three degrees of representation: complete
creation, partial creation, and no creation supported.
We observed that 76 % (87 of 115) of the concepts for
the pneumonia, colonoscopy, and influenza use cases
could be completely created using Knowledge Author.
Table 3 describes the 24 % (28 of 115) of concepts that
could be partially created in their entirety (see Additional
file 1 for a full list of 115 concepts created). Knowledge
Author supported the creation of a very high proportion
of simple concepts (69 of 73), but a lower proportion of
complex concepts (18 of 42) by the knowledge engineer.
Complex concepts include compound concepts developed
from two semantic types, such as lab test positive for in-
fluenza. Knowledge Author supports creation of the con-
cept lab test positive and influenza but does not yet
support linking the two into a single concept. Knowledge
Author, also, does not support creation of concept repre-
senting a single atomic concept with a set of modifiers
combined with a disjunction, such as new or progressive
infiltrate. The four simple concepts that were not able
to be created in Knowledge Author are a result of the re-
quired modifiers not being listed in the Knowledge Author
data model.
Even though Knowledge Author does not support the
creation of some concepts, it is possible to add the de-
sired data by hand outside of Knowledge Author. The
Knowledge Author data model allows for the use of the
Semantic Web Rule Language (SWRL) [31] rules, even
though the Knowledge Author interface itself does not.
SWRL is an OWL-based rule language. Through man-
ual editing of the Knowledge Author output file, com-
plex variables can be created by inserting SWRL rules.
For modifiers that are not in the data model, it is pos-
sible to add the appropriate modifier classes by hand
to the Knowledge Author output file. Correctly de-
signed NLP tools that use the Knowledge Author out-
put are able to handle user created classes. Having to
add information outside of the Knowledge Author
interface is time consuming and as Knowledge Author
Fig. 5 Certainty shared modifier dropdown list
Table 3 Types and number of concepts that were not able to
be created in Knowledge Author
Reason Not Created Total # of
Concepts
% of Total
(115)
Element or modifier type
not found in Schema Ontology
21 18 %
Relation between concepts missing - could
only create separate concepts without linking
7 6 %
Scuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 8 of 11
matures we expect to expand its functionality to cover
the vast majority of concepts.
Knowledge Author-powered information extraction
evaluation
We assessed the viability of the Knowledge Author out-
put for use in clinical NLP by creating a semantic
schema for carotid stenosis in Knowledge Author and
using it as the target extraction template in the pyCon-
Text [32, 33] NLP system.
pyConText is a regular-expression, rule-based infor-
mation extraction system which accepts two files  one
for target concepts and one for associated modifiers.
The target file contains regular expressions or lexical
variants describing target concepts of interest such as
those representing carotid disease. The modifier file
contains regular expressions or lexical variants describ-
ing the types of modifiers such as certainty, anatomical
location or temporality. A software script was written to
automatically marshal the data contained in the Know-
ledge Author output file into the file format and schema
supported by pyConText.
We selected 34 carotid ultrasound reports from the
MT Samples corpus [34] that were used in a previous
study [32]. The reports were de-identified and selected
at random from the MT Samples corpus. Two physi-
cians independently annotated each report and adjudi-
cated each disagreement with consensus review using an
annotation tool called eHOST [35]. Each report was an-
notated for the targeted finding concepts for carotid
stenosis along with the following associated modifiers:
certainty, sidedness, and neurovascular anatomy.
We applied pyConText using the Knowledge Author
semantic schema to the texts and converted its output
to Knowtator.xml to be read into eHOST to conduct
our error analysis. We computed recall for each type of
target and modifiers (the proportion of concept men-
tions correctly identified from the reference standard)
because we are predominately concerned with whether
we have enough lexical variants to identify these con-
cepts from free-text.
Reasonably high recall was achieved identifying tar-
geted finding concepts (86 %) and shared modifiers (cer-
tainty: 91 %) and high to low recall for the semantic
modifiers (sidedness: 80 %, neurovascular anatomy:
46 %) (Table 4).
The low recall can be partially attributed to missing
cues from the terminology lookup. In particular, many
false negatives were due to missing acronyms and abbre-
viations in the semantic modifier file e.g., ICA which
stands for neurovascular anatomy: Internal carotid ar-
tery and l which stands for sidedness: left which are
commonly used in carotid ultrasound reports. Addition-
ally, low recall can be partially attributed to the inability
for Knowledge Author to represent ranges of severity for
some semantic modifiers e.g., 70-80 % which indicates
significant stenosis. We are actively incorporating this
functionality in the system. A manual input of additional
acronyms and abbreviations using the Knowledge Au-
thor synonym interface and manual input of regular ex-
pressions for semantic modifiers using an OWL editor
could improve the results. Overall, this result suggests
that the Knowledge Author output has the potential to
be used by an NLP system to create viable results.
Future development
We are continuing to develop Knowledge Author and
add new features. Some of the features that we expect to
be added in the near future include:
 Adding constructs that will allow users to link
concepts together using relationships (i.e. ibuprofen
treats pain) and logical operators.
 Allowing the user to search a default corpus of de-
identified medical records for phrases that would po-
tentially be retrieved for the new concept. This would
allow the user to test the accuracy of synonyms and
numeric thresholds.
 Allowing the user to share and collaboratively work
on an ontology with a select group of users.
Knowledge Author is the first part of a pipeline that
will allow the user to create an NLP schema, annotate
documents, process documents using various NLP sys-
tems, and analyze the results. We envision an end-to-
end system that allows the user to rapidly build custom
clinical text queries using a variety of NLP systems. We
are actively developing a recommendation module
within the pipeline that will suggest new lexical variants
for concepts and modifiers from clinical text leveraging
active learning methods to improve recall i.e., acronyms
and abbreviations observed from development data in
real-time. Currently, only the pyConText algorithm uses
the output from Knowledge Author. Additional systems
are under development.
Conclusions
Knowledge Author is a new, web-based tool for building
a semantic schema of domain content that could be used
Table 4 pyConText performance leveraging Knowledge Author
knowledge base
Concept Types Total Correct Recall
Targets Findings 79 68 86 %
Modifiers Certainty 11 10 91 %
Sidedness 41 33 80 %
Neurovascular Anatomy 41 19 46 %
Scuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 9 of 11
in an NLP application. It leverages three existing know-
ledge resources  the Secondary Use CEMs, CTS, and
the UMLS  to provide the user with relevant informa-
tion for creation of domain-specific concepts, which al-
lows for rapid semantic schema creation. The output of
Knowledge Author can be used directly as input into
compatible NLP systems.
Availability and requirements
Knowledge Author is publically available and can be found
at http://blulab.chpc.utah.edu/KA/. The user can create an
account to access the tool by clicking on the Create Ac-
count link. The data model used by Knowledge Author
can be found at http://blulab.chpc.utah.edu/ontologies/
SchemaOntology.owl. The completed carotid stenosis se-
mantic schema can be found at http://blulab.chpc.utah.edu/
ontologies/schemas/bscuba/carotid_stenosis.owl and in the
Additional file 2.
Additional files
Additional file 1: Full list of use case concepts. (XLSX 12 kb)
Additional file 2: Carotid Stenosis OWL file. (OWL 232 kb)
Abbreviations
cTAKES, clinical Text Analysis and Knowledge Extraction System; CTS,
common type system; CUI, Concept Unique Identifier; eHOST, extensible
Human Oracle Suite of Tools; HiTex, Health Information Text Extraction;
MedLEE, Medical Language Extraction and Encoding System; NLP, Natural
Language Processing; OWL, Web Ontology Language; Secondary Use CEM,
Secondary Use Clinical Element Model; SWRL, Semantic Web Rule Language;
UMLS, United Medical Language System; XML, Extensible Markup Language
Acknowledgements
We would like to acknowledge Effective Dynamics for their excellent work
programming the system. This work was supported by National Library of
Medicine grant LM010964.
Authors' contributions
WWC, MT and WS designed the Knowledge Author interface. WS was the
software architect and project manager. WWC provided the vision for the project.
MT managed all things related to the Schema and Modifier Ontologies. ET and
MT coded the OWL file input and output. FD provided interface design support.
YL coded the initial Knowledge Author prototype. MT created the use cases for
testing. DM implemented and assessed the proof of concept study leveraging
pyConText. MT, WS, DM, and WWC drafted the manuscript. All authors read and
approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Author details
1Department of Biomedical Informatics, University of Utah, Salt Lake City, UT
84108, USA. 2Department of Biomedical Informatics, University of Pittsburgh,
Pittsburgh, PA 15206, USA. 3University of California, San Diego, CA 92093,
USA. 4Department of Psychology, University of Utah, Salt Lake City, UT 84108,
USA.
Received: 28 February 2015 Accepted: 1 June 2016
DATABASE Open Access
An ontology for major histocompatibility
restriction
Randi Vita1* , James A. Overton1, Emily Seymour1, John Sidney1, Jim Kaufman2, Rebecca L. Tallmadge3,
Shirley Ellis4, John Hammond4, Geoff W. Butcher5, Alessandro Sette1 and Bjoern Peters1
Abstract
Background: MHC molecules are a highly diverse family of proteins that play a key role in cellular immune recognition.
Over time, different techniques and terminologies have been developed to identify the specific type(s) of MHC molecule
involved in a specific immune recognition context. No consistent nomenclature exists across different vertebrate species.
Purpose: To correctly represent MHC related data in The Immune Epitope Database (IEDB), we built upon a
previously established MHC ontology and created an ontology to represent MHC molecules as they relate to
immunological experiments.
Description: This ontology models MHC protein chains from 16 species, deals with different approaches used to
identify MHC, such as direct sequencing verses serotyping, relates engineered MHC molecules to naturally occurring
ones, connects genetic loci, alleles, protein chains and multi-chain proteins, and establishes evidence codes for MHC
restriction. Where available, this work is based on existing ontologies from the OBO foundry.
Conclusions: Overall, representing MHC molecules provides a challenging and practically important test case
for ontology building, and could serve as an example of how to integrate other ontology building efforts into
web resources.
Keywords: Major histocompatibility complex, Ontology, MHC, Immune epitope
Background
Major histocompatibility complex (MHC) proteins play
a central role in the adaptive immune system. First
discovered due to their role in transplant rejection,
MHC molecules are encoded by a large family of genes
with wide variation within each species. MHC molecules
typically bind peptide fragments of proteins and display
them on the cell surface where they are scanned by T
cells of the immune system. If a peptide fragment is
displayed by MHC, it can trigger a T cell immune re-
sponse. Peptides triggering a response are referred to as
epitopes. Thus, binding of epitopes to MHC molecules
is an integral step for immune recognition. The specific
MHC molecule that presents an epitope to a T cell is
knowns as its MHC restriction, often called its MHC
restriction (or restricting) element. Accurately representing
this MHC restriction, which can be determined in different
manners, is the goal of the work presented here. Most
MHC molecules consist of two protein chains, of which at
least one gene is present within the MHC locus. In humans
this locus is known as the human leukocyte antigen (HLA)
and is depicted in Fig. 1a. There are thousands of different
allelic variants of these genes coding for different proteins
that result in diverse MHC binding specificities found in
the human population. The most precise way of specifying
MHC restriction is to identify the exact protein chains that
make up the MHC molecule. However, until recently such
exact molecular typing was not possible, and patterns of
antibody binding were utilized to group MHC molecules
together into serotypes that share a common serological
(antibody based) recognition pattern, as shown in Fig. 1b.
Tying such traditional serotype information together with
current sequence based MHC typing techniques is one of
the goals of our study. In yet other cases, such as inbred
mouse strains, MHC restriction is narrowed down based
on the haplotype of the animal, the set of alleles present on
* Correspondence: rvita@liai.org
1La Jolla Institute for Allergy and Immunology, 9420 Athena Circle La Jolla,
San Diego, California 92037, USA
Full list of author information is available at the end of the article
© 2016 Vita et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Vita et al. Journal of Biomedical Semantics  (2016) 7:1 
DOI 10.1186/s13326-016-0045-5
a single chromosome and thus expressed consistently
together in select subspecies or strains. Another way MHC
restriction is sometimes inferred is based on the T cells rec-
ognizing the epitope. MHC molecules are divided into
three classes: MHC class I, MHC class II, and non-classical
MHC. MHC class I molecules present epitopes to CD8+ T
cells and are made up of one alpha chain and one ?2
microglobulin chain, which is invariant and encoded out-
side the MHC locus. MHC class II molecules present epi-
topes to CD4+ T cells and are composed of one alpha and
one beta chain, as shown in Fig. 1c. Thus knowing if the
responding T cell expresses CD4 verses CD8 can be used to
narrow down the possible MHC restriction into classes. At
the same time, current research has identified that
some T cell populations do not follow this pattern exactly
(e.g. some T cells recognizing MHC-II restricted epitopes
express CD8). It is therefore important to capture not only
the inferred restriction information, but also the evidence
upon which it was based.
Methods
The Immune Epitope Database (www.iedb.org) presents
thousands of published experiments describing the rec-
ognition of immune epitopes by antibodies, T cells, or
MHC molecules [1]. The data contained in the IEDB is
primarily derived through manual curation of published
literature, but also includes some directly submitted
data, primarily from NIAID funded epitope discovery
contracts [2]. The goal of the current work was to repre-
sent MHC data as they are utilized by immunologists to
meet the needs of the IEDB users. We collected user
input at workshops, conferences and the IEDB help sys-
tem regarding how they wanted to retrieve data from
the IEDB regarding MHC restriction. These requests
were used to identify goals for this ontology project and
the final ontology was evaluated if it could answer these
requests. As shown in Additional file 1: Table S1, an
example of such a request was to be able to query for
epitopes restricted by MHC molecules with serotype A2
and retrieve not only serotyped results but also those
where the restriction is finer mapped e.g. to MHC mol-
ecule A*02:01 which has serotype A2. We set out to
logically represent the relationships between the genes
encoding MHC, the haplotypes linking together groups
of genes in specific species, and the individual proteins
comprising MHC complexes, in order to present immuno-
logical data in an exact way and to improve the functional-
ity of our website. Our work builds on MaHCO [3], an
ontology for MHC developed for the StemNet project,
using the well-established MHC nomenclature resources of
the international ImMunoGeneTics information sys-
tem (IMGT, http://www.imgt.org) for human data and
The Immuno Polymorphism Database (IPD, http://
www.ebi.ac.uk/ipd) for non-human species. It contains
Fig. 1 MHC presentation and restriction. a. HLA locus of human chromosome 6 encodes specific MHC protein chains. b. The MHC on APC
presenting epitopes can be bound by antibodies to establish the serotype. c. If responding effector cells are known to be CD4 cells, the MHC
presenting the epitope can be presumed to be class II restricted
Vita et al. Journal of Biomedical Semantics  (2016) 7:1 Page 2 of 7
118 terms for MHC across human, mouse, and dog.
We were encouraged by the success of MaHCO in ex-
pressing official nomenclature using logical defini-
tions. However, we needed to extend it for the
purpose of the IEDB to include data from a growing
list of 16 species, as well as data about MHC protein
complexes (not just MHC alleles), haplotypes and se-
rotypes. Thus, our current work goes beyond MaHCO,
and we have utilized this opportunity to also enhance
the integration with other ontological frameworks.
We used the template feature of the open source
ROBOT ontology tool [4] to specify the content of our
ontology in a number of tables. Most of the tables cor-
respond to a single branch of the ontology hierarchy,
in which the classes have a consistent logical structure,
e.g. gene loci, protein chains, mutant MHC molecules,
haplotypes, etc. The OWL representation of our ontol-
ogy is generated directly from the tables using ROBOT.
This method enforces the ontology design patterns we
have chosen for each branch, and makes certain editing
tasks easier than with tools such as Protégé.
Results and discussion
Our MHC Restriction Ontology (MRO) is available in a
preliminary state at https://github.com/IEDB/MRO. It is
based on existing ontology terms, including: material
entity from the Basic Formal Ontology (BFO) [5], pro-
tein complex from The Gene Ontology (GO) [6], pro-
tein from The Protein Ontology (PRO) [7], organism
from The Ontology for Biomedical Investigations (OBI)
[8], genetic locus from The Reagent Ontology (REO)
[9], has part, in taxon, and gene product of  from The
Relation Ontology (RO) [10]. The NCBI Taxonomy was
used to refer to each species [11]. Although it is not yet
complete, we strive to conform to Open Biological and
Biomedical Ontologies (OBO) [12] standards. MRO
currently contains 1750 classes and nearly 9000 axioms,
including more than 2100 logical axioms. Its DL expres-
sivity is ALEI, and the HermiT reasoner [13] completes
reasoning in less than 10 seconds on a recent laptop.
Synonyms were also included, as immunologists
often utilize synonyms that are either abbreviations or
based on previous states of the nomenclature. The
current MHC nomenclatures for various species have
been revised through several iterations. In order to
ensure accuracy and remain up to date with the latest
nomenclature, we referred to the well-established
MHC nomenclature resources of the IMGT and IPD.
For specific species where the literature was most
formidable, such as chicken, cattle, and horse, we col-
laborated with experts in these fields. These experts
reviewed the encoded hierarchy by determining whether
the inferred parentage hierarchy in their area of expertise
reflected their input.
Each MHC molecule for which the IEDB has data is
modeled as a protein complex consisting of two chains.
Each chain is a gene product of a specific MHC genetic
locus. For certain species, sub-loci are also defined, when
useful. For example, as shown in Fig. 2 HLA-DPA1*02:01/
DPB1*01:01 consists of one HLA-DPA1*02:01 chain,
encoded by the DPA sub-locus of DP, and one HLA-
DPB1*01:01 chain, encoded by the DPB1 sub-locus of DP.
Together these two chains make up one DPA1*02:01/
DPB1*01:01 MHC molecule.
When the identity of only a single chain of the com-
plex is known, a generic second chain is used to
make up the MHC complex. Thus, MHC restriction of
HLA-DPB1*04:02 is modeled as one HLA-DPB1*04:02
chain in complex with an HLA-DPA chain that is not
further specified, as shown within the context of the
hierarchy in Fig. 3.
The data in the ontology drives the Allele Finder on
the IEDB website, available at http://goo.gl/r8Tgrz, an
interactive application that allows users to browse MHC
restriction data in a hierarchical format. We evaluated
the ability of MRO to meet the needs of IEDB users, as
shown in Additional file 1: Table S1, and found it to
meet our initial goals. Currently the use of the ontology
is behind the scenes, but we have requested namespace
and permanent identifiers from The Open Biomedical
Ontologies (OBO). As soon as these identifiers are in
place, they will be utilized and displayed on the IEDB
website to allow users to link out to the ontology.
In MHC binding and elution assays, the exact MHC
molecule studied is typically known; however this is
often not the case for T cell assays. When a T cell re-
sponds to an epitope, the identity of the MHC molecule
presenting the epitope may not be known at all, it may
be narrowed down to a subset of all possible molecules
or it may be exactly identified. In the context of T cell
assays, the MHC restriction can be determined by the
genetic background of the host, conditions of the experi-
ment, or the biological process being measured; there-
fore we represent MHC molecules at a variety of levels
and specify the rationale behind the determined restric-
tion using evidence codes.
As shown in Fig. 4a, IEDB Evidence codes include
author statement for cases where authors report pre-
viously defined restriction and MHC ligand assay
used for MHC restriction established via an experiment
that demonstrated the ability of the epitope to bind
strongly to the MHC molecule or to have been eluted
from that molecule. Figure 4b shows the metadata asso-
ciated with this evidence code. MHC binding predic-
tion is used when computer algorithms are used to
predict the likelihood of an epitope to bind to a specific
MHC molecule. In cases where authors analyze the
MHC phenotype of a study population and conclude a
Vita et al. Journal of Biomedical Semantics  (2016) 7:1 Page 3 of 7
likely restriction based upon epitope recognition pat-
terns among the subjects, statistical association is
used as the evidence code. We use a set of evidence
codes to communicate restriction shown by the response of
T cells to the epitope: MHC complex. These include Single
MHC available for cases where T cells respond to the epi-
tope when only a single MHC molecule is available and re-
activity of same T cells with different MHC is used when
different APC expressing different MHC are used to nar-
row the potential restriction. The use of antibodies to block
or purify subsets of MHC molecules typically determines
restriction to an imprecise level, such as HLA-DR and is
conveyed by set of MHC available. When the Tcells being
studied are known to be CD8 or CD4 cells, the restriction
can be deduced to be class I or class II, respectively, due to
the known binding pattern of the molecules, as depicted in
Fig. 1c. This case is communicated by the evidence code of
type of effector T cell. Lastly, certain T cell responses can
indicate the effector cell phenotype of CD8 or CD4, based
upon known functions of the subsets and thus, class I or II
restriction can be inferred and is noted by the evidence
code of biological process measured. Figure 4c shows the
Fig. 2 Ontologic relationships between MRO terms
Fig. 3 Ontological model showing human MHC class II molecules
Vita et al. Journal of Biomedical Semantics  (2016) 7:1 Page 4 of 7
modeling of these evidence codes in terms of the specific
experiments, data transformations performed (using OBI
terms), and the type of conclusion drawn. This work is
being conducted in parallel with the general alignment of
the Evidence Ontology (ECO) [14], which provides succinct
codes for such types of evidence, with OBI, which can
break down how such a code translates to specific experi-
ments performed.
The IEDB MHC Allele Finder application, shown in
Fig. 5, now allows users to browse data in different
views. MHC molecules are first categorized into class
I, class II or non-classical, and then further subdivided
by species. Within each species, MHC molecules are
organized by genetic locus. For select species, such as
human, there are a large number of MHC molecules
known and studied per genetic locus, thus sub-loci are
also used in order to present the data in a more user-
friendly format. Each MHC molecule is presented
under its locus, its haplotype, and/or its serotype,
when available, all representing newly added function-
alities. The haplotype the host species expresses is
represented as immunologists often rely on the known
haplotypes of research animals to narrow the potential
MHC restriction. For example, when BALB/c (H2d)
mice demonstrate a response to an epitope and the
responding T cells are CD4+, the restricting MHC can
be assumed to be one of the two MHC class II mole-
cules of that haplotype, namely H2 IAd or IEd.
The serotype of an MHC molecule, defined by anti-
body staining patterns, is relevant in immunology as this
was the method of choice to identify MHC molecules
until quite recently. In contrast to molecular definitions
Fig. 4 Evidence codes in MRO
Vita et al. Journal of Biomedical Semantics  (2016) 7:1 Page 5 of 7
of MHC molecules based on their specific nucleotide or
amino acid sequence, serotyping classifies MHC mole-
cules based entirely on antibody binding patterns to the
MHC molecule. These patterns are linked to the panel
of antibodies used. Changing the antibody panel changes
the serotype of a molecule. This can result in serotype
splits where MHC molecules that were previously con-
sidered identical by one antibody panel, are later found
to actually be two different molecules by a different anti-
body panel. To reflect this extrinsic nature of serotyping,
we refer to serotypes as information entities rather than
physical entities. Alternatively, the concept of serotype
could also be modeled as collections of binding disposi-
tions, but we chose what we thought was the simpler
approach. MHC for all 16 species currently having MHC
data in the IEDB are modeled to give users the ability to
browse the tree in multiple ways and search IEDB data
broadly, by entire MHC class, for example, or narrowly
by a specific MHC protein chain. As new MHC mole-
cules are encountered, they can be easily incorporated
into this ontology.
Conclusions
In conclusion, we formally represented MHC data
building on established ontologies in order to represent
MHC restrictions as required by immunologists. Ac-
cordingly, we modeled MHC molecules as a protein
complex of two chains and established the relationships
between the genes encoding these proteins, the haplo-
types expressed by specific species, and the MHC clas-
ses. Traditional serotype information was also related
to specific MHC molecules. Precise MHC restriction
was conveyed, as well as inferred MHC restriction and
also the experimental evidence upon which the restric-
tion was established. We will continue to formalize this
work and will release a completed interoperable ontol-
ogy later this year. Thus, MHC data in the IEDB is now
presented to its users in a hierarchical format which
simplifies searching the data and additionally instructs
users on the inherent relationships between MHC
genes and MHC restriction.
Additional file
Additional file 1: Goals and status of the MRO project. (XLSX 16 kb)
Abbreviations
MHC: Major histocompatibility complex; IEDB: The Immune Epitope
Database; APC: Antigen presenting cell; HLA: Human leukocyte antigen;
IMGT: ImMunoGeneTics; IPD: Immuno Polymorphism Database; MRO
MHC: Restriction Ontology; BFO: Basic Formal Ontology; GO: Gene Ontology;
PRO: Protein Ontology; OBI: Ontology for Biomedical Investigations;
ECO: Evidence Ontology; OBO: The Open Biomedical Ontologies.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
RV, JAO and BP conceived of the ontology, and participated in its design and
coordination and helped to draft the manuscript. ES prepared and analyzed
MHC datasets. JS, JK, RLT, SE, JH, GWB, AS and BP provided expert guidance
with relationship to specific MHC subsets to direct the development of the
ontology. All authors read and approved the final manuscript.
Acknowledgements
We wish to thank Kirsten Fischer Lindahl and Lutz Walter for their kind
assistance with the mouse and rat MHC molecule nomenclatures, respectively.
The Immune Epitope Database and Analysis Project is funded by the
National Institutes of Health [HHSN272201200010C].
Fig. 5 IEDBs MHC Allele Finder, demonstrating chicken haplotypes
Vita et al. Journal of Biomedical Semantics  (2016) 7:1 Page 6 of 7
Author details
1La Jolla Institute for Allergy and Immunology, 9420 Athena Circle La Jolla,
San Diego, California 92037, USA. 2University of Cambridge, Trinity Ln,
Cambridge CB2 1TN, UK. 3Cornell University College of Veterinary Medicine,
Ithaca, New York 14853-6401, USA. 4The Pirbright Institute, Ash Rd, Woking
GU24 0NF, UK. 5The Babraham Institute, Cambridge CB22 3AT, UK.
Received: 29 September 2015 Accepted: 3 January 2016
Fernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 
DOI 10.1186/s13326-016-0077-x
RESEARCH Open Access
The Orthology Ontology: development
and applications
Jesualdo Tomás Fernández-Breis1*, Hirokazu Chiba2, María del Carmen Legaz-García1 and Ikuo Uchiyama2
Abstract
Background: Computational comparative analysis of multiple genomes provides valuable opportunities to
biomedical research. In particular, orthology analysis can play a central role in comparative genomics; it guides
establishing evolutionary relations among genes of organisms and allows functional inference of gene products.
However, the wide variations in current orthology databases necessitate the research toward the shareability of the
content that is generated by different tools and stored in different structures. Exchanging the content with other
research communities requires making the meaning of the content explicit.
Description: The need for a common ontology has led to the creation of the Orthology Ontology (ORTH) following
the best practices in ontology construction. Here, we describe our model and major entities of the ontology that is
implemented in the Web Ontology Language (OWL), followed by the assessment of the quality of the ontology and
the application of the ORTH to existing orthology datasets. This shareable ontology enables the possibility to develop
Linked Orthology Datasets and a meta-predictor of orthology through standardization for the representation of
orthology databases. The ORTH is freely available in OWL format to all users at http://purl.org/net/orth.
Conclusions: The Orthology Ontology can serve as a framework for the semantic standardization of orthology
content and it will contribute to a better exploitation of orthology resources in biomedical research. The results
demonstrate the feasibility of developing shareable datasets using this ontology. Further applications will maximize
the usefulness of this ontology.
Keywords: Semantic web, Knowledge representation, Ontology, Comparative genomics, Orthology
Background
Owing to rapid progress in sequencing technologies, the
number of genome sequences determined has signifi-
cantly increased; recently, the targets of genome projects
are not limited to the model organisms but include unin-
vestigated organisms of particular interest. In this new
genomic era, the role of computational analysis is becom-
ing increasingly important. There is an urgent need for
consolidating a comprehensive foundation of comparative
analysis toward effective knowledge discovery. In par-
ticular, the orthology information is a key resource; it
guides establishing evolutionary histories among genes of
multiple organisms and provides a basis for functional
inference of gene products.
*Correspondence: jfernand@um.es
Equal contributors
1Departamento de Informática y Sistemas, Universidad de Murcia,
IMIB-Arrixaca, 30071 Murcia, Spain
Full list of author information is available at the end of the article
The concepts of orthology and paralogy are defined
as specific types of homology [1]; homologs are genes
diverged from an ancestral gene, and specifically,
orthologs are those diverged by a speciation event,
whereas paralogs diverged by a duplication event. Figure 1
shows a schematic representation of evolutionary rela-
tions among genes of multiple organisms, which exem-
plifies orthology/paralogy. Orthologs are usually more
conserved in biological functions than paralogs; thus, the
orthology relation is particularly useful in transferring the
biological knowledge of model organisms to organisms
with newly sequenced genomes. Whereas the homology
relations are basically calculated in a pairwise perspec-
tive, they are often represented as a cluster of homologs.
Likewise, an ortholog cluster stands for a group of genes
derived from a speciation event, and a paralog cluster
for a group of genes derived from a duplication event.
Ortholog/paralog clusters can be structured in a form of
nested hierarchies, reflecting their evolutionary histories.
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Fernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 2 of 11
Fig. 1 A schematic representation of the evolutionary relations among genes of multiple organisms. The leaf nodes of the tree represent the genes
and the internal nodes correspond to evolutionary events. X1 has two ancestral nodes associated with speciation events; one is the last common
ancestor with Y1, and the other is common with Z1. Thus, Y1 and Z1 are orthologs of X1. On the other hand, X2 and Y2 are paralogs to X1, since
their last common ancestor has a duplication event associated. Likewise, all the pairwise orthology/paralogy relations can be defined according to
the strucutre of the given tree
A simple example of hierarchical clusters can be seen in
Fig. 1.
The Quest for Orthologs (QfO) Consortium has iden-
tified more than forty resources about orthology (http://
questfororthologs.org/orthology_databases), which re-
flect different scopes of information management in the
orthology field. Many of these databases store information
about prediction of gene evolutionary relations and there
is a diversity of objectives for these databases. There is het-
erogeneity in how data are stored and provided by these
databases. For example, InParanoid [2] stores orthology
relations between two species, whereas OrthoMCL [3]
and MBGD [4] stores ortholog groups among multiple
genomes. OMA [5] provides various types of orthology
relations including pairwise orthologs and hierarchical
orthologous groups. Traditionally, each resource has used
its own representation format based on tabular files,
but this community has developed in the last years the
OrthoXML format [6] to standardize the representation
of orthology data. OrthoXML permits the comparison
and integration of orthology data from different resources
within the orthology community. However, only a limited
number of databases have provided their content using
OrthoXML so far.
In recent years, the Semantic Web formats have been
used for representing orthology data. OGO [7] was
created with the purpose of providing an integrated
resource of information about genetic human diseases
and orthologous genes. OGO integrated information from
orthology databases such as InParanoid or OrthoMCL,
plus OMIM [8]. This resource developed an OWL
ontology for representing the domain knowledge. More
recently, RDF has been used to share the content of the
Microbial Genome Database for Comparative Analysis
(MBGD) [9]. This resource also developed an OWL ontol-
ogy for representing the domain knowledge, calledOrthO,
which had similar concepts to the OGO ones, despite
being developed independently.
The report of the 2013 QfO meeting [10] identified
a series of aspects about semantics that have been the
key drivers of our activities: (1) the orthology commu-
nity should use shared ontologies to facilitate data sharing;
(2) exploiting automated reasoning should be beneficial
for the QfO consortium. In this paper, we describe the
construction of the Orthology Ontology by reusing the
existing related ontologies and we explain how we inte-
grated the existing orthology datasets using the Semantic
Web technologies. This work provides a step forward
towards the standardization in the orthology community.
Construction and content
Construction of the Orthology Ontology
As the first principle, we followed the best practices in
ontology engineering: reusing the existing ontologies to
Fernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 3 of 11
facilitate interoperability across biomedical domains; and
designing the ontology with amodular perspective, so that
different modules of the ontology are created in differ-
ent sub-taxonomies of the ontology, and the classes from
different modules are connected through object proper-
ties. The second principle is to define local URIs for basic
terms of the domain. In case that equivalent classes are
found in the reused ontologies, such equivalency is stated
by means of axioms.
Two application-oriented domain ontologies were the
starting point for this work, namely, OGO [7] and OrthO
[9]. These ontologies provided a basis for discussion and
identification of the relevant classes and properties for this
domain. Those ontologies already reused some ontologies
such as the Relations Ontology (RO) or the NCBI Taxon-
omy (NCBIT), so these were included in the initial set of
candidate ontologies to reuse.
The objective of the Orthology Ontology (ORTH) is
to become the reference in the orthology domain and
across the biological domains, so it must be beyond
the application-oriented ontologies. In order to facilitate
interoperability, we decided to search for existing ontolo-
gies which could play such interoperability enabler role.
We searched repositories such as BioPortal [11], Onto-
bee [12] and AberOWL [13], and identified ontologies
containing classes and properties for the entities identi-
fied in our analysis. This list of ontologies is described
next:
 Comparative Data Analysis Ontology (CDAO)1 [14]:
Classes and properties relevant for evolutionary
studies.
 Relations Ontology (RO) 2 [15]: Collection of
biomedical properties to support standardization
across biomedical ontologies.
 Homology Ontology (HOM)3 [16]: Classes related to
homology.
 Sequence Ontology (SO)4 [17]: A set of classes and
properties to define sequence features used in
biological sequence annotation.
 Ontology of Genes and Genomes (OGG)5 [18]:
Classes and properties to represent relations among
genes, genomes and organisms.
 Protein Ontology (PR)6 [19]: Protein-related entities,
including evolutionary relations between proteins.
 Semanticscience Integrated Ontology (SIO)7 [20]:
Classes and properties for rich description of
biomedical objects and processes.
 NCBI Taxonomy (NCBIT)8 [21]: Curated
classification and nomenclature for all the organisms.
 Clusters of Orthologous Groups Analysis Ontology
(CAO)9 [22]: Classes to support the Clusters of
Orthologous Groups enrichment method using
Fishers exact test.
The HOM and the CAO ontologies were discarded for
different reasons. On the one hand, we found that the RO
properties were more appropriate than the HOM classes
for describing relations between biological sequences. On
the other hand, the CAO was found too specific and
presented overlaps with other ontologies that we con-
sider more relevant for our goal. OGG and PR were not
used because their classes of interest are covered by other
ontologies. Next, we enumerate the ontologies selected for
reuse:
 The RO is the main reference for the properties
included in the ORTH.
 The CDAO provides classes for representing
evolutionary events such as speciation and
duplication, which are fundamental for the orthology
domain. Besides, it defines classes and properties for
representing the tree, which is a hierarchical
structure widely used to represent evolutionary
relations. This ontology is reused specially for
evolution-oriented entities.
 The SIO provides classes and properties that describe
biomedical objects and processes, therefore it is a
more general ontology than the CDAO. This is why
we have used it as a reference for the general
biomedical entities.
 The SO provides classes related to biological
sequences, some of which are of interest for the
orthology domain: biological region, gene and protein.
 The NCBIT provides the classes for the species
associated with the biological sequences.
Besides, it must be taken into account that some SIO
properties are equivalent to RO ones. For those cases,
we have selected the SIO one. In summary, we selected
to reuse SIO and RO for a more general content, CDAO
for the evolution-oriented content, SO for the biological
sequence types, and NCBIT for the organisms. The above
described ontologies provide the biological background
knowledge for the orthology domain. Besides, the ORTH
reuses other vocabularies:
 dcterms10: It includes the metadata terms maintained
by the Dublin Core Metadata Initiative. We reused
properties such as identifier.
 VoID11: RDF Schema vocabulary for expressing
metadata about RDF datasets. ORTH needs to
represent orthology databases, so the properties and
classes representing datasets and membership to
them are reused.
The content of the Orthology Ontology
The Orthology Ontology is available at http://purl.org/
net/orth in OWL format. In our model, evolutionary
information among sequences are primarily represented
Fernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 4 of 11
as membership of the sequences to clusters of
homologs, orthologs or paralogs. Note that the pairwise
orthologs/paralogs can be obtained by traversing the tree
structure of the clusters. When we see the example shown
in Fig. 1, each gene represented by the leaf node belongs
to ancestral nodes corresponding to clusters of orthologs
or paralogs, from which pairwise orthology/paralogy can
be extracted.
Figure 2 shows the core classes and properties included
in the ORTH, where three areas can be distinguished as
follows. The left side of the figure contains the CDAO
module that defines cladogenetic changes, that is, the
types of evolutionary events relevant for the orthology
domain, such as cdao:speciation or cdao:geneDuplication.
The central part of the figure describes the main
orthology-specific classes from our modeling perspec-
tive, that is, the clusters of homologs, orthologs and
paralogs, which are represented by means of the classes
HomologsCluster, OrthologsCluster and ParalogsCluster
respectively. Given that these clusters are usually orga-
nized as trees, we have also defined the class GeneTreeN-
ode, which is a subclass of cdao:Node. Provided that the
types of cluster are related to a specific type of cladoge-
netic change, the property cdao:has links the types of clus-
ters with the corresponding cladogenetic changes. Again,
we are reusing CDAO content to provide interoperable
evolutionary content. Besides, the membership to a given
cluster is expressed through the property hasHomologous,
which is a subproperty of sio:has_part, whose inverse
property is equivalent to ro:part _of. We use this prop-
erty instead of two hasOrthologous and hasParalogous,
because the pairwise relations are obtained by analyzing
the tree.
The right side of the figure focuses on the defini-
tion of the biological sequences relevant for the orthol-
ogy domain: genes, subgenes and proteins. These classes
are subclasses of SequenceUnit, which is a subclass
of cdao:TU, which represents taxonomic units. The
class Subgene has been created because of the increas-
ing interest in creating evolutionary analyses of gene
subsequences. Hence, its relation with gene has been
made explicit through the property sio:is_part_of. These
classes of the ontology are connected with the cen-
tral module through the rdfs:subClassOf relationship
between SequenceUnit and GeneTreeNode. Genes and
proteins, which are defined equivalent to classes in the
SO, are linked to ncbit:organisms through the prop-
erty ro:in_taxon and to biological databases through
ro:contained_in.
Although the original terms orthology and paralogy are
binary relationships between genes, the ORTH does not
include these terms. Instead, the ORTH defines these
Fig. 2 The core classes and properties of the Orthology Ontology. The classes are represented as boxes and the properties as arrows. The prefixes
cdao, sio, ro, ncbit and void represent entities reused from the corresponding ontologies. The entities without prefix are defined in the ORTH. On the
whole, this figure includes three kinds of classes, each shown in the left/center/right parts, respectively: (left) classes for evolutionary changes; (center)
classes for groups of biological sequences holding particular evolutionary relations; and (right) classes for biological sequences of interest
Fernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 5 of 11
concepts through the classes OrthologsCluster and Par-
alogsCluster. This is because the relationships orthology
and paralogy are not transitive [23], and the tree (or
hierarchical clustering) representation as shown in Fig. 1
is a better representation for these relationships among
multiple genes. In fact, any pairwise orthology/paralogy
relation can be extracted from this representation using a
query as shown in the next sections. The content has been
modeled with the aim of providing an appropriate degree
of axiomatization. For instance, we have mentioned that
classes such as OrthologsCluster and ParalogsCluster are
associated with the corresponding evolutionary event
through an object property. For example, the semantically
equivalent definition for anOrthologsCluster according to
our ontology and thanks to the axiomatization would be a
cluster of homologs whose event associated is speciation.
This corresponds to the OWL axiom
orth:OrthologsCluster rdfs:subClassOf
(orth:HomologsCluster and
cdao:has only cdao:CDAO_0000121)
and in SPARQL as follows:
?cluster rdf:type/rdfs:subClassOf*
orth:HomologsCluster.
?cluster cdao:has cdao:CDAO_0000121.
Ontology metrics
The current version of the ORTH has a core that consists
of 21 classes, 14 object properties, 5 datatype properties
and 142 axioms, whereas the whole knowledge frame-
work, that is, with the imported ontologies, consists of
4613 classes, 806 object properties, 15 datatype properties
and 43140 axioms.
We have applied the OQuaRE framework [24] to
evaluate the quality of the ontology produced. With
this framework a series of metrics can be calculated,
providing scores in the range 1 (lowest) to 5 (high-
est) for the OQuaRE quality characteristics. Table 1
shows the OQuaRE scores for the ontology with the
imported, reused ontologies (complete) and the ontol-
ogy without the imports (no imports). The ontology also
passed successfully the test of the OOPS! Ontology Pitfall
Scanner [25].
Applying the ORTH to orthology datasets
In this section we illustrate through an example how
the availability of ORTH can benefit the exploitation of
orthology data. Experiences have been gained with OMA
and InParanoid [26], which have been recently extended
to TreeFam [27] in the context of the BioHackathon 2015.
Let us suppose that we are doing some research on serum
amyloid A1 (SAA1) protein which is known as an inflam-
matory marker, and that we are interested in finding out
if this human gene has orthologs in mouse, because this
could permit to carry out some related research withmice.
In this example we assume the existence of three orthol-
ogy resources: OMA, InParanoid and TreeFam. The use
of the original resources to answer this question would
require to perform three queries, one per resource and
to process and interpret the set of results knowing how
orthology relations are represented in each resource. The
ORTH ensures that each data represented has a pre-
cise meaning, so the user can focus on interpreting the
results. Besides, the use of the ORTH for representing
the datasets enables to obtain the results with one, non
resource-dependent query. The joint exploitation of the
orthology datasets requires (1) generating RDF versions
of the datasets; and (2) defining and executing the corre-
sponding queries in SPARQL. Both tasks are described in
the next subsections.
Generation of the RDF datasets
We describe next an example of how the source data
are transformed into RDF. Let us consider the infor-
mation available in InParanoid 8 about Homo sapiens -
Mus musculus orthologs12. Table 2 shows fragments of
the corresponding OrthoXML file. We use OrthoXML as
data schema because it is considered a standard in the
orthology community. The species tags are used to spec-
ify the name of the species, the database from which
the genes/proteins are retrieved, and the genes used in
this file. For each <gene> three attributes are shown: (1)
id, whose scope is the OrthoXML file; it is the ID used
for associating a gene with the corresponding clusters;
(2) protId, which is the identifier of the protein in the
database; and (3) geneId, which represents a gene sym-
bol in this example. For example, the gene with id 33162
is the human protein whose UniProt accession number
(AC) is P0DJI8 and whose gene symbol is SAA1. In this
fragment we can see that it contains genes from humans
and mice. After the declaration of species and genes, the
OrthoXML file includes the cluster with id 16021, which
contains the human genes SAA1 and SAA2 and themouse
genes Saa1 and Saa2. This implies: (1) a many-to-many
Table 1 Scores of the OQuaRE quality characteristics for the ORTH
ORTH Structural Funct. adequacy Compatibility Maintainability Operability Reliability Transferability
Complete 4.5 4.56 3.0 3.97 4.33 3.12 4.0
No imports 4.0 4.03 4.25 4.09 3.66 3.0 4.0
The first row shows the scores for the OWL file including the imported ontologies, whereas the second row shows the ones for the ontology without the imported ones
Fernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 6 of 11
Table 2 Fragments of the InParanoid OrthoXML file that stores
orthology relations between human and mouse
<species name="Homo sapiens " NCBITaxId="9606" >
<database name="UniProt"
version="UniProt_Complete_Proteomes_2013_06"
protLink="http://www.uniprot.org/uniprot/">
<genes>
<gene id="33162" protId="P0DJI8" geneId="SAA1"/>
<gene id="33163" protId="P0DJI9" geneId="SAA2"/>
</genes>
</species>
<species name="Mus musculus " NCBITaxId="10090">
<database name="UniProt"
version="UniProt_Complete_Proteomes_2013_06"
protLink="http://www.uniprot.org/uniprot/">
<genes>
<gene id="33164" protId="P05366" geneId="Saa1"/>
<gene id="33165" protId="P05367" geneId="Saa2"/>
</genes>
</species>
<orthologGroup id="16021">
<geneRef id="33162"/>
<geneRef id="33163"/>
<geneRef id="33164"/>
<geneRef id="33165"/>
</orthologGroup>
orthology relation between the human and mouse genes,
that is, <(SAA1, SAA2), (Saa1, Saa2)>; and (2) the paral-
ogy relation between the genes of the same species, that
is, (SAA1, SAA2) and (Saa1, Saa2).
The RDF representation of the XML content using the
ORTH is obtained by (1) mapping the OrthoXML for-
mat to the ORTH; and (2) applying the mappings to the
data. Briefly speaking, the mappings associate entities
and attributes of the OrthoXML schema with owl:Class,
owl:DatatypeProperty and owl:ObjectProperty defined in
the ORTH. The mapping file can be found at https://
github.com/qfo/OrthologyOntology. An example of the
mapping for the clusters of orthologs is shown in Fig. 3,
where the left part shows the OrthoXML schema and
the right side shows the ORTH schema. There, we can
see that the entity orthologGroup is mapped to the class
OrthologsCluster and that the membership of a gene to an
orthologGroup, which is represented by the link between
orthologGroup and geneRef, is mapped to the object
property hasHomologous. Consequently, for each geneRef
included in an orthologGroup, the corresponding triple is
obtained in the form of OrthologsCluster hasHomologous
Gene.
Technically speaking, the generation of the RDF
datasets is supported by the SWIT tool 13. SWIT is able
to generate RDF and OWL content by applying the map-
ping rules to the OrthoXML versions of OMA, InParanoid
and TreeFam. Besides, SWIT uses automated reasoning
to ensure that only logically consistent content is trans-
formed. This means that the data instances inconsistent
with the axioms of the ORTH are not transformed into
RDF or OWL. Table 3 shows the RDF triples generated for
describing the orthologous group 16021.
The RDF datasets generated from InParanoid 8, OMA
hierarchical orthologous groups (Sep 2014) and TreeFam
9 are available on our website, and contain 8798758 genes
from OMA, 1713180 genes for Homo sapiens orthologs
and 1367940 genes for Mus musculus orthologs from
InParanoid, and 1376021 genes from TreeFam. Overall,
the complete dataset has over 2 billion triples.
Exploitation of the RDF datasets
Here we assume the existence of an RDF repository
with the data from the three resources, which has been
generated as described in the previous section. In case
of using three distinct RDF repositories, the SPARQL
queries should be adapted by including the corresponding
SERVICE clauses. Given that we are interested in retriev-
ing the orthologs of the human gene SAA1 in mouse,
answering this query requires to extract pairwise orthol-
ogy relations betwen the human gene and mouse genes
from the repository. The ORTH associates each gene with
(hierarchical) clusters of homologs to which it belongs and
each cluster with an evolutionary event. Thus, extracting
the pairwise orthologs means searching for such genes
that are members of the same cluster as the human gene
SAA1 belongs to and the last ancestor cluster is a cluster
of orthologs (i.e., it has a speciation event associated).
Such description can be expressed as the SPARQL
query shown in Table 4. This query can extract orthol-
ogous pairs by identifying their last common ancestors;
it extracts pairs of genes (?gene1 and ?gene2), that are
the descendants of respective two distinct nodes of the
tree (?tree_node1 and ?tree_node2) whose common parent
(?common_ancestor) is a cluster of orthologs. The tree-
based cluster analysis is facilitated by the use of the single
property hasHomologous, which is transitively used when
the symbol * is attached to it. The FILTER and VALUES
clauses serve to define the gene and organisms of interest
for the query, meaning that it could be used as a template
for finding pairwise orthologs for any given gene; only the
VALUES clause would have to be modified.
The results of the query are shown in Table 5. We
can see that there is a one-to-many relation between the
human gene SAA1 and its orthologs in mouse (SAA1,
Fernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 7 of 11
Fig. 3 Excerpt of the mapping from the OrthoXML schema (left) to the ORTH (right). The dashed lines represent the concrete mappings from
OrthoXML entities to ORTH classes or properties
SAA2, SAA3) and that two resources contain the results.
The relationship with SAA1 and SAA2 is supported by the
two resources, and only OMA proposes the one for SAA3.
It should be noted that the predicted orthology relation is
generally reliable in the case of one-to-one, but conflicts
between methods often happen when the relationship is
not one-to-one. More details, including sample queries
that exploit the three resources, can be found at https://
github.com/qfo/OrthologyOntology.
Table 3 RDF triples for the cluster of orthologs 16021
orth_data:orthologsCluster_16021 rdf:type orth:OrthologsCluster.
orth_data:orthologsCluster_16021 void:inDataset orth_data:
orthologyDataset_InParanoid.
orth_data:orthologsCluster_16021 dcterms:identifier "16021".
orth_data:orthologsCluster_16021 orth:hasHomologous orth_data:
gene_33162.
orth_data:orthologsCluster_16021 orth:hasHomologous orth_data:
gene_33163.
orth_data:orthologsCluster_16021 orth:hasHomologous orth_data:
gene_33164.
orth_data:orthologsCluster_16021 orth:hasHomologous orth_data:
gene_33165.
Utility and discussion
Potential applications
The development of the Orthology Ontology enables a
series of activities that will show progress in how orthol-
ogy data are represented and exploited.
1) Linked Orthology Data to promote interoperability.
Many biological databases include information about
orthology relations, which derive from different orthol-
ogy resources created by different methods. The ORTH
vocabulary can be used to generate shareable RDF
datasets that could be queried by biomedical informat-
ics tools. An initial research on how the ORTH can drive
the transformation of orthology databases in OrthoXML
is reported in this work. The development of a Linked
Data API for ORTH datasets would permit to standard-
ize a series of methods that would return data from
different resources preserving the meaning of the enti-
ties, so promoting the standardization of the orthology
data obtained from different resources such as UniProt or
Ensembl.
2) Meta-predictor of orthology for better prediction of
biological functions. Predicting biological functions is
likely to be the most widespread application of orthology
resources. The availability of the ORTH and the exis-
tence of RDF orthology datasets based on the ORTH
will facilitate the development of methods for improv-
ing orthology prediction by exploiting the predictions
Fernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 8 of 11
Table 4 A sample query for getting the orthologs of a given gene
SELECT ?gene ?species ?database WHERE {
?common_ancestor a orth:OrthologsCluster .
?common_ancestor ort:hasHomologous ?tree_node1 .
?common_ancestor orth:hasHomologous ?tree_node2 .
?common_ancestor void:inDataset ?dataset .
?dataset orth:hasSource ?database .
?tree_node1 orth:hasHomologous* ?gene1 .
?tree_node2 orth:hasHomologous* ?gene2 .
?gene1 a orth:Gene .
?gene2 a orth:Gene .
?gene1 obo:RO_0002162 ?species1 .
?gene2 obo:RO_0002162 ?species2 .
?gene1 dcterms:identifier ?id .
?gene2 dcterms:identifier ?gene .
?species2 rdfs:label ?species .
bind( str(?id) as ?str_id )
FILTER (?tree_node1 != ?tree_node2 && ?species1 != ?species2)
VALUES (?str_id ?species1 ?species2) {(SAA1 ncbit:9606 ncbit:10090)}
}
In this example, the mouse (ncbit:10090) orthologs of the human (ncbit:9606) gene
SAA1 are retrieved. obo:RO_0002162 stands for the property in_taxon
of many of the existing orthology resources, which can
improve the function prediction by orthology relations.
The potential of this meta-approach will be reinforced by
the standardization effort of the orthology content.
3)Migration of existing resources. Datamigration of exist-
ing orthology resources described by previous ontologies
such as OrthO is also necessary and can be done with the
support of the results of the present work. We provide
information that helps the ontology users catch up the
evolution of the ontologies and work on the data migra-
tion. As an example, we have summarized the term-by-
term correspondence between previous ontologies (OGO,
OrthO) and the current ORTH (see https://github.com/
qfo/OrthologyOntology). This table will help replace the
previous ontologies with the current ontology. In fact,
we have already replaced the OrthO ontology of MBGD
database (http://mbgd.genome.ad.jp/sparql) according to
Table 5 Results of the query shown in Table 4 for a repository
that integrates InParanoid and OMA
Gene Species Database
SAA1 Musmusculus OMA
SAA3 Musmusculus OMA
SAA2 Musmusculus OMA
Saa1 Musmusculus InParanoid
Saa2 Musmusculus InParanoid
this table. The replacement was straightforward as the
current ontology covers the previously used concepts.
Toward the ontology standardization, not the distinct
researchers but a community-oriented approach is cru-
cial. The current ontology with enhanced consensus
and semantics will be more suitable than previous ones
for standardization and further application of orthology
resources.
Data integration issues
ORTH-based data integration can be carried out following
two main approaches: links or warehouse. In the link-
based approach, there would be one RDF dataset using
the ORTH vocabulary per orthology resource, and it is the
data integration strategy used in projects such as Bio2RDF
[28]. The application of the link-based strategy to OMA
and InParanoid for the SAA1 example would produce
one instance of SAA1 in each repository. Both instances
could have the same URI (e.g., http://identifiers.org/hgnc.
symbol/SAA1) or different ones, which would depend
on the decision made by the data providers, since gene
nomenclature is well maintained only for limited species.
In this latter case, owl:sameAs links should be defined to
identify that they refer to the same gene.
The warehouse approach stores the whole dataset in
the same repository, and it is the approach followed in
projects such as OGOLOD [29]. This approach requires
to be able to identify which instances from the different
datasets refer to the same gene or protein, which is easy to
find in case shared identifiers are used, but difficult other-
wise. The application of the warehouse strategy to OMA
and InParanoid for the SAA1 example would produce one
instance of SAA1, which would integrate the content from
OMA and InParanoid.
In the current work, we have followed a warehouse
approach using resource-oriented URIs, with the objec-
tive of studying and make visible the data integration
issues that would impede the orthology community to
have semantically interoperable datasets even with the
availability of the ORTH.We wanted to test to what extent
the availability of the ORTH and the definition of a com-
mon transformation process could help, and what addi-
tional work should be done. The results obtained are three
datasets that use the same knowledge framework, which
can be jointly queried, which is one of the contributions of
the present work, since those datasets could not be jointly
queried to date. This means that there is one instance of
SAA1 for the gene from each resource. Besides, the proper
integration of data has not been carried out through links.
The heterogeneity at the identifier level is an impor-
tant issue in the application of the ORTH. Table 6 shows
that different identifiers are used by the three resources:
InParanoid, OMA and TreeFam. OMA uses local identi-
fiers for the proteins; InParanoid uses the UniProt AC for
Fernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 9 of 11
Table 6 Summary of sequence sources and identifiers used in
three different orthology resources
Orthology resource Database source Protein ID Gene ID
OMA Multiple sources OMA ID Gene symbola
InParanoid UniProt UniProt AC Gene symbola
TreeFam Multiple sources Multiple sources Multiple sources
aThe gene symbol is used for model organisms, including human and mouse, but
this is not the case for other organisms in general
proteins and the gene symbol for genes; and the identi-
fiers used in TreeFam depend on the database used for
the corresponding species (e.g., Ensembl, FlyBase, Worm-
Base). We think that the generation of highly interop-
erable RDF orthology datasets following the link-based
approach would require the data providers to support
with a mapping service for generating the corresponding
links or to use common reference identifiers. One practi-
cal approach is that each database provider should provide
RESEARCH Open Access
Trivalent influenza vaccine adverse
symptoms analysis based on MedDRA
terminology using VAERS data in 2011
Jingcheng Du1, Yi Cai2, Yong Chen3 and Cui Tao1*
Abstract
Background: Trivalent Influenza Virus Vaccine (FLU3) is a traditional flu vaccine to protect people against three
different flu viruses, including influenza A H1N1 virus, an influenza A H3N2 virus and one B virus.
Methods: We searched Vaccine Adverse Event Reporting System (VAERS) for US reports after FLU3 vaccination in
the year of 2011. We conducted descriptive analyses on symptoms from serious reports (i.e., death, life-threatening
illness, hospitalization, prolonged hospitalization, or permanent disability). We then further grouped these symptoms to
the System Organ Classes (SOC) based on the MedDRA Terminology using NCBO Web Services. We fitted zero-truncated
Poisson regression models to estimate the average number of symptoms per subject and compared it across different
age groups and between genders. In addition, we compared the risk of occurrence for an SOC across different age
groups and between genders by using logistic regression models. Finally, we constructed the pairwise correlation matrix
of the SOCs by calculating Spearmans rank correlation coefficients.
Results: We identified 638 unique serious FLU3 reports from year 2011. There are 1410 unique symptoms from these
reports. Descriptive statistics shows that the most common symptom and symptom pair are Pyrexia and Guillain-Barre
syndrome  Hypoesthesia respectively. The estimated average number of symptoms per subject in the study cohort is
8.74 (95 % CI 6.76, 10.73). There are statistically significant differences in number of symptoms among four age groups
and between genders. Age category and gender are significantly associated with several individual SOCs. Pairwise
correlation matrix shows that Endocrine disorders and Neoplasms benign, malignant and unspecified (incl cysts
and polyps) are strongly correlated.
Conclusions: This paper reports a novel method that combining statistical analyses with terminology grouping
using VAERS data. The analyses revealed differences of reactions among different age groups and between genders
and correlation on both symptoms and System Organ Class level independently. The results may lead to additional
studies to uncover factors contributing to the individual differences in susceptibility to influenza infection. This method
can also be applied to other vaccine types and conduct similar analysis.
Keywords: Trivalent influenza virus vaccine, VAERS, MedDRA
* Correspondence: cui.tao@uth.tmc.edu
Equal contributors
1The University of Texas School of Biomedical Informatics, 7000 Fannin St
Suite 600, Houston, TX 77030, USA
Full list of author information is available at the end of the article
© 2016 Du et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Du et al. Journal of Biomedical Semantics  (2016) 7:13 
DOI 10.1186/s13326-016-0056-2
Background
Influenza (flu) is a contagious respiratory illness caused
by influenza viruses, which may cause mild to severe ill-
ness including hospitalization or even death. Certain
groups of people, such as the elders, young children, and
those with certain health conditions, are at high risk for
serious flu complications [1]. The primary and most
used method for the control of influenza and its compli-
cations are influenza vaccines. Over the years, hundreds
of millions of Americans have received seasonal influenza
vaccines to protect themselves against the flu viruses.
Commonly, the side effects following flu vaccinations are
mild, including symptoms such as soreness, redness or
swelling at the injection sites, headache, muscle aches and
nausea after the shot. Serious adverse reactions, however,
could happen, which may cause some life-threating illness
even death.
Clinical trials are generally not large enough to detect
rare influenza vaccine adverse events. In 1990, Vaccine
Adverse Event Reporting System (VAERS) was created
as a passive surveillance system to accept reports of
adverse events following any US licensed vaccines form
providers, health care workers, and the public [2]. VAERS
is co-administered by Center for Disease Control and
Prevention (CDC) and the Food and Drug Administration
(FDA). It is one of the largest databases containing adverse
events reported in temporal association with vaccination.
Since 1990, VAERS has received more than 400,000 vac-
cine adverse event reports, which makes it one of the most
important sources to detect rare vaccine adverse events.
Although VAERS cannot usually prove the causal relation-
ships between vaccines and adverse events, it could be
used to detect signals to be tested with more rigorous
methods [3].
For years, trivalent Influenza Virus Vaccine (FLU3) is
the traditional flu vaccine to protect people against three
different flu viruses, including an influenza A H1N1
virus, an influenza A H3N2 virus and one B virus [4].
Among all the VAERS reports, FLU3 is the most com-
mon vaccine type reported. Thus VAERS is a very im-
portant data source for studying FLU3 adverse events.
Much work has been done on the study of influenza
adverse events using VAERS data [57]. Most of them
only deal with some specific symptoms on the FLU3
vaccine adverse reports. A single vaccine and AE associ-
ation, however, should not be considered as an isolated
event. The associations of other vaccines with the same
AE and other AEs with the same vaccine should also be
taken into consideration [8].
Our research takes advantage of the Medical Dictionary
for Regulatory Activities (MedDRA) terminology system
for semantically grouping the VAERS adverse symp-
toms, which are already coded using MedDRA terms
[2]. The MedDRA Terminology is the international
medical terminology developed under the auspices of
the International Conference on Harmonization (ICH)
of Technical Requirements for Registration of Pharma-
ceuticals for Human Use. MedDRA Terminology has a
five-level structural hierarchy. They are lowest Level
Term (LLT), Preferred Term (PT), High Level Term
(HLT), High Level Group Term (HLGT), and System
Organ Class (SOC). VAERSSYMPTOMS contains symp-
toms terms that are in the level of LLT [9]. VAERS used
symptom terms from PT, which always have its own iden-
tical term as LLT. The full MedDRA has 72,637 LLT
symptoms and VAERS uses 9593 of them (13 %) [10].
System organ class (SOC) is the highest level of the hier-
archy that provides the broadest concept for data retrieval,
which comprises grouping by etiology, manifestation site
and purpose. MedDRA has 26 different types of SOCs and
each LLT is at least linked to one SOC [9].
As the number of unique symptoms is relatively large
in VAERS, we consider them in the SOCs to facilitate
further statistical analyses. We leveraged The National
Center for Biomedical Ontology (NCBO) web service to
automatically map LLTs to their corresponding SOCs.
The NCBO offers a range of Web services that would
allow users to access various biomedical terminologies
and ontologies and to identify terms from controlled ter-
minologies and ontologies that can describe and index
the contents of online data sets (data annotation) [11].
All the MedDRA terminology system is stored in JSON
format on the NCBO web services and each term is a
JSON node. We used NCBO web services to search the
hierarchical information of the symptom terms in the
VAERS reports and assigned each symptom term one pri-
mary SOC. After grouping the symptoms to SOCs, we
conducted multiple statistical analyses on the SOC level.
Materials and methods
Data source
We searched the VAERS for US reports after FLU3
vaccination in year 2011 and extracted serious reports
(i.e., death, life-threatening illness, hospitalization, pro-
longed hospitalization, or permanent disability). VAERS
raw data of each year contains three Comma-separated-
value (CSV) files: VAERSDATA.CSV, VAERSVAX.CSV
and VAERSSYMPTOMS.CSV. VAERSDATA is about the
patients demographic information, lab test, symptom
text and outcomes. VAERSVAX is about vaccine types.
VAERSSYMPTOMS is about symptoms that are equi-
valent to the PT TERM from the MedDRA codebook.
These three tables are linked by using VAERS_ID. For
each report, the VAERS also provides annotations for
post-vaccination symptoms in MedDRA terms. To fa-
cilitate further statistical analyses, we further grouped
these symptoms based on the MedDRA SOC using the
NCBO Web Services [11].
Du et al. Journal of Biomedical Semantics  (2016) 7:13 Page 2 of 7
Descriptive analysis
We calculated descriptive statistics including the num-
ber of reports, symptoms, and unique symptoms in the
selected reports. We also calculated the frequency of
each symptom and co-occurrence of symptom pairs.
We grouped the reports in five age groups based on
cut points (0.5, 17, 49, 64) suggested by CDC [12]. The
frequency of observations in age category (0 to 0.5) is
relatively small (n = 14) compared to other age categories.
In order to be consistent with our previous analysis, we
excluded those subjects in our data analysis, which lead to
1663 subjects [13].
Grouping symptoms using MedDRA terminology
SOC is the highest level of the MedDRA terms, which
comprises grouping by etiology, manifestation site or
purpose. Each LLT is linked to only one PT and each PT
is linked to at least one SOC. This indicates that each
LLT could be grouped to more than one SOC. To avoid
double counting, we will need to identify the primary
SOC for each term [9]. The rules to assign SOC to the
symptoms, however, are complicated, which needs expert
reviews that could be time consuming and expensive.
Our study proposed a simple way to group them into
SOCs by using the international agreed order of the
SOC list (see Table 1). The order of the SOCs was based
upon the relative importance of each SOC, which is
determined by the Expert Working Group [9]. First, we
retrieved all possible SOCs a VAERS symptom term be-
longs to. We applied Depth-first Search (DFS) algorithm
by using recursive tree-traversing method to find all
the SOCs that is linked by a symptom term. If one
symptom term belongs to several SOCs, we choose the
SOC that ranked highest as its primary SOC. If a re-
port has N symptoms that belong to the same specific
SOC, we count N times of that specific SOC. Table 2
shows the sample data set (partial) we prepared for
further analysis.
Statistical methods
After grouping the symptoms into 26 SOCs, we explored
the grouped data with regression models and correlation
analysis. Specifically, we estimated the average number
of symptoms per subject given stratified age groups and
gender. For individual SOC, we fitted logistic regression
to evaluate the association of occurrence of an SOC with
age and gender. A rank-based correlation matrix is also
estimated to investigate the correlation among SOCs.
Zero-truncated Poisson regression
As each subject has at least one SOC to be reported
(i.e. the number of SOCs > 0), we fitted a zero-truncated
Poisson regression model (a modified model of Poisson re-
gression by excluding the probability mass at 0) to conduct
Table 1 International Agreed Orders of SOCs
SOC Order
Infections and infestations 1
Neoplasms benign, malignant and unspecified
(incl cysts and polyps)
2
Blood and lymphatic system disorders 3
Immune system disorders 4
Endocrine disorders 5
Metabolism and nutrition disorders 6
Psychiatric disorders 7
Nervous system disorders 8
Eye disorders 9
Ear and labyrinth disorders 10
Cardiac disorders 11
Vascular disorders 12
Respiratory, thoracic and mediastinal disorders 13
Gastrointestinal disorders 14
Hepatobiliary disorders 15
Skin and subcutaneous tissue disorders 16
Musculoskeletal and connective tissue disorders 17
Renal and urinary disorders 18
Pregnancy, puerperium and perinatal conditions 19
Reproductive system and breast disorders 20
Congenital, familial and genetic disorders 21
General disorders and administration site conditions 22
Investigations 23
Injury, poisoning and procedural complications 24
Surgical and medical procedures 25
Social circumstances 26
Table 2 Sample data set we prepared for further analysis
(partial, data of year 2011)
ID  SOC7 SOC8 SOC9 SOC10 
413830 0 1 0 0
413836 0 0 0 0
413913 0 3 0 0
413937 0 11 0 2
413946 0 5 1 0
413959 0 5 0 0
413966 3 15 3 0
413993 1 5 0 0
413994 1 3 0 0
 
ID refers to the report ID number. The number in a cell refers to the number
of appearance in the corresponding SOC in that report
Du et al. Journal of Biomedical Semantics  (2016) 7:13 Page 3 of 7
data analysis [14]. Firstly, we fitted a model with intercept
only to estimate the average number of symptoms for all
subjects. After that, we included covariates of age category
and gender in the model to estimate and compare the
number of symptoms in different age and gender strata.
Logistic regression
To explore the association of the occurrence of individual
SOC with age and gender, we conducted a logistic regres-
sion with covariates of age group and gender [15]. The
original count number of SOC is dichotomized into binary
outcomes (1: SOC ? 1, 0: SOC = 0).
Spearman rank correlation coefficient
As the number of SOCs is highly right skewed and non-
normally distributed, we use a nonparametric correlation
coefficient to measure the correlation structure among
SOCs. This rank based correlation coefficient is known as
Spearmans ? [16]. We constructed a correlation matrix
of SOCs to present the pairwise correlation coefficients
between SOCs.
Results
During the study period (year 2011), VAERS received
7986 FLU3 reports; 638 were serious. Out of the 638 re-
ports, 324 were for female patients, 295 were for male
patients, and 19 were for unknown sex, see Fig. 1. Out
of these reports, there were 5447 symptom terms (not
unique) appeared in total, which were grouped into 26
SOCs. The most frequent SOCs in the 638 reports are
nervous system disorders, general disorders, and admin-
istration site condition and investigations. Please note
that some symptom terms cannot be grouped to any
MedDRA SOCs. There were 48 symptom terms (21 of
them are unique) cant be mapped into MedDRA SOCs,
including Drug exposure during pregnancy, Herpes
zoster multi-dermatomal and etc. (The complete list
can be seen in Additional file 1: Table S1A). This may
due to the MedDRA version differences between the
NCBO Web Services and VAERS. As the number of
these symptoms is relatively small (0.9 %), we did not
take these symptoms into consideration.
We then calculated the frequency of each symptoms
and symptom concurrences. The most frequent symp-
toms happened after FLU3 vaccination in year 2011
were Pyrexia (131 times), Hypoaesthesia (95 times), and
Guillain-Barre syndrome (90 times), the visualization
result can be seen in Fig. 2. (The complete results can
be seen in Additional file 1: Table S1B). The most fre-
quent symptom co-concurrences were Guillain-Barre
syndrome + Hypoaesthesia (24 times).
Analysis using zero-truncated Poisson model with
intercept only indicated that the estimated average num-
ber of symptoms per subject in the study cohort is 8.74
(95 % CI: 6.76, 10.73). The results from fitting zero-
truncated Poisson regression with covariates age groups
and gender suggest that there are statistically significant
differences in the numbers of symptoms among four age
Fig. 1 Descriptive results of 2011 VAERS FLU3 data. a the proportion of serious reports of the total FLU3 reports. b the gender distribution of
these serious reports. c the age distribution of these serious reports
Du et al. Journal of Biomedical Semantics  (2016) 7:13 Page 4 of 7
groups and between different genders. The average num-
ber of symptoms per year for a female patient with age
between 0.5 years and 17 years is estimated as 7.76
(95 % CI: 5.769.76). We use this group as the reference
age group. The youngest age group (0.517 years) has
the smallest number of symptoms per year, followed by
age group 4 (>64 years), age group 2 (1749) and finally
age group 3 (4964 years). The average number of
symptoms for subjects of 1749 years old is 1.13 times
of the average number of symptoms for subjects of 0.5
17 years old with the same gender. This is consistent
with previous reports about different immune responses
after vaccination for different age groups. For example,
there is high-dose influenza vaccine available for elders
(>65) because ageing decreases the body's ability of im-
mune response after vaccination [17]. In addition, the
males have 3.2 % lower number of symptoms. This is
also consistent with previous studies that female experi-
ence more adverse reactions to influenza vaccine [18].
We plotted the estimated residuals versus fitted values
of zero-truncated Poisson regression mode. We found
that the residuals are generally small and scattered
around zero for most of the data points with only a few
extreme values. This suggests that the zero-truncated
Poisson regression model may fit the data well.
Analysis on individual SOCs also revealed some in-
teresting results. Overall, there are 15 SOCs that show
significant association with the age groups or between
genders. The males have fewer responses for most SOCs
except SOC1 (infections and infestations, 48.1 % more).
Males are 0.369, 0.552 and 0.535 times less likely to have
SOC 9 (Eye disorders), SOC 11 (Cardiac disorders) and
SOC 12 (Vascular disorders) than females respectively.
For SOC 14 (Gastroin-testinal disorders) and SOC 1
(Infections and infestations), female has significant
higher possibilities than male. For SOC 1 (Infections
and infestations), SOC 9 (Eye disorders), SOC 12 (Vascular
disorders), and SOC 17 (Musculoskeletal and connective
tissue disorders), people who are older than 17 show sig-
nificant higher possibilities to get those adverse symptoms.
Age group 3 (4964) shows significant higher chance of
experiencing symptoms relevant to infections and infesta-
tions, immune system disorders, eye disorders, vascular
disorders, musculoskeletal and connective tissue disorders,
surgical and medical procedures, and social circumstances
compared to age group 1 (0.517) with the same gender.
To evaluate the overall goodness of fit of the logistic
regression model for each SOC, we conducted Hosmer-
Lemeshow test [15] and calculated the corresponding p-
values. Most of the p-values for all 26 tests are much
smaller than 0.05, which suggests that the logistic regres-
sion model fits the data well.
We also calculated the pairwise correlation matrix of
SOCs determined by Spearmans method [16]. Fig. 3
shows the correlation plot. The color and area of spot
represents the strength of correlation between SOCs.
The threshold to assert a correlation used by us is
whether the correlation coefficient is larger or equal
than 0.2 as suggested by Evans [19]. As illustrated in
Fig. 3, SOC 5 (Endocrine disorders) has the strongest
correlation with SOC 2 (Neoplasms benign, malignant
and unspecified (incl cysts and polyps)). Besides, we
find that SOC 23 (Investigations) has a correlation with
SOC 8 (Nervous system disorders). We can also find
that SOC 22 (General disorders and administration site
conditions) has correlations with SOC6 (Metabolism
and nutrition disorders), and SOC 17 (Musculoskeletal
and connective tissue disorders). In addition, SOC 12
Fig. 2 Symptoms frequency visualization for year 2011, the bigger the frequency the larger the font size of that symptom
Du et al. Journal of Biomedical Semantics  (2016) 7:13 Page 5 of 7
(Vascular disorders) shows correlations with SOC 3
(Blood and lymphatic system disorders), and SOC 6
(Metabolism and nutrition disorders). In addition, SOC
18 (Renal and urinary disorders) has correlations with
SOC 3 (Blood and lymphatic system disorders), SOC 6
(Metabolism and nutrition disorders) and SOC 12
(Vascular disorders).
Conclusions and future work
This paper reports a novel method that combining stat-
istical analyses with terminology grouping using VAERS
data to study Trivalent Influenza Vaccine. Our prelimin-
ary statistical analyses reveal differences of reactions
among different age groups and between genders. To
our best knowledge, there are very few studies about the
adverse events analysis on MedDRA SOC level. Most of
our findings on the relationship between adverse events
with individual difference have not been reported by
other studies. The results may lead to additional studies to
uncover factors contributing to the individual differences
in adverse reactions to influenza vaccination.
For the limitations of this paper, due to the multiple
inheritance nature of MedDRA, many symptom terms
can be mapped to more than one SOCs. The order list
we used to assign the primary SOC may be subjective.
Ontology of Adverse Events (OAE) could provide a better
hierarchy than MedDRA. The current version of OAE,
however, does not include all the symptoms in the VAERS
(coded in MedDRA) yet. We will consider using OAE to
map the symptom terms as the OAE grows.
There are a few future directions we would like to
pursue: (1) we will extend and apply the methodology
to more VAERS reports over different years; and (2)
this method can also be applied to other vaccine types
and conduct similar analysis.
Additional file
Additional file 1: Table S1A. Symptom terms that cant be mapped
into SOC level in the year 2011. Table S1B. Symptoms and their
frequency for year 2011. Top 50 frequently occurred symptoms in serious
FLU3 reports in the year of 2011. (DOCX 16 kb)
Competing interests
The authors declare that they have no competing interests.
Authors contributions
JD collected that data, wrote the initial draft and revised subsequent draft.
JD and YC developed the method, preformed the evaluation, and conducted
analysis of the results. YC guided the statistical analysis design and modeling.
CT provided institutional support, contributed to research design, and guided
the data analysis. All authors read and approved the final manuscript.
Acknowledgments
This research is partially supported by the National Library Of Medicine of
the National Institutes of Health under Award Number R01LM011829. The
authors also gratefully acknowledge the support from the UTHealth Innovation
for Cancer Prevention Research Training Program Pre-doctoral Fellowship
(Cancer Prevention and Research Institute of Texas grant # RP140103).
Author details
1The University of Texas School of Biomedical Informatics, 7000 Fannin St
Suite 600, Houston, TX 77030, USA. 2The University of Texas School of Public
Health, 1200 Pressler Street, Houston, TX 77030, USA. 3Department of
Biostatistics and Epidemiology, University of Pennsylvania, 423 Guardian
Drive, Philadelphia, PA 19104, USA.
Received: 10 November 2015 Accepted: 12 March 2016
RESEARCH Open Access
An accurate and precise representation of
drug ingredients
Josh Hanna* , Jiang Bian and William R. Hogan
Abstract
Background: In previous work, we built the Drug Ontology (DrOn) to support comparative effectiveness research
use cases. Here, we have updated our representation of ingredients to include both active ingredients (and their
strengths) and excipients. Our update had three primary lines of work: 1) analysing and extracting excipients, 2)
analysing and extracting strength information for active ingredients, and 3) representing the binding of active
ingredients to cytochrome P450 isoenzymes as substrates and inhibitors of those enzymes.
Methods: To properly differentiate between excipients and active ingredients, we conducted an ontological
analysis of the roles that various ingredients, including excipients, have in drug products. We used the value
specification model of the Ontology for Biomedical Investigations to represent strengths of active ingredients and
then analyzed RxNorm to extract excipient and strength information and modeled them according to the results of
our analysis. We also analyzed and defined dispositions of molecules used in aggregate as active ingredients to
bind cytochrome P450 isoenzymes.
Results: Our analysis of excipients led to 17 new classes representing the various roles that excipients can bear. We
then extracted excipients from RxNorm and added them to DrOn for branded drugs. We found excipients for 5,743
branded drugs, covering ~27 % of the 21,191 branded drugs in DrOn.
Our analysis of active ingredients resulted in another new class, active ingredient role. We also extracted strengths
for all types of tablets, capsules, and caplets, resulting in strengths for 5,782 drug forms, covering ~41 % of the
14,035 total drug forms and accounting for ~97 % of the 5,970 tablets, capsules, and caplets in DrOn.
We represented binding-as-substrate and binding-as-inhibitor dispositions to two cytochrome P450 (CYP)
isoenzymes (CYP2C19 and CYP2D6) and linked these dispositions to 65 compounds. It is now possible to query
DrOn automatically for all drug products that contain active ingredients whose molecular grains inhibit or are
metabolized by a particular CYP isoenzyme.
DrOn is open source and is available at http://purl.obolibrary.org/obo/dron.owl.
Background
In previous work, we built the Drug Ontology (DrOn) to
support comparative effectiveness research use cases and
reported on its theoretical basis, the methodology we
used to build it, and its ability to meet the use cases [13].
Motivated by critiques and requests from end-users of
DrOn of its representation of ingredients, we describe
how we have improved the accuracy and coverage of our
representation of ingredients.
The work involved three major components. The first
component was the inclusion of excipients. Although
active ingredients and their strengths have obvious
effects on the efficacy of a drug, excipients also influence
drug effects in significant ways [46]. Additionally, it is
not uncommon for excipients to cause allergic reactions
in patients [7, 8]. The second component was the im-
provement and extension of the representation of active
ingredients, including the addition of strength informa-
tion. The last component was representing for the first
time in an open-access, machine-readable ontology the
binding disposition of certain molecules to cytochrome
P450 (CYP) isoenzymes as substrates and / or inhibitors.
* Correspondence: joshhanna@ufl.edu
Biomedical Informatics Program, Department of Health Outcomes and Policy,
University of Florida, Gainesville, FL, USA
© 2016 Hanna et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Hanna et al. Journal of Biomedical Semantics  (2016) 7:7 
DOI 10.1186/s13326-016-0048-2
Methods
In Hogan et al. [1], we differentiated between excipients
and active ingredients but did not define or represent their
differences explicitly. To do so, we first conducted an
ontological analysis of the roles various ingredients have
in drug products. We also represented strengths of active
ingredients according to the value specification model of
the Ontology for Biomedical Investigations (OBI) [9]. We
documented and reviewed our definitions and proposed
classes and their axiomatizations on the DrOn wiki page
[10]. Once complete, we then analyzed RxNorm [11] to
extract excipient and strength information and modeled
them according to the results of our analysis.
Analysis of excipients and method of extracting them
from RxNorm
We reviewed publicly available sources of information
about the various roles of excipients and conducted an
ontological analysis of them from the realist perspective.
Excipients have numerous roles that aid in the manufac-
ture, administration, identification, and preservation of
drug products. To represent these roles, we defined the
following and included them in DrOn: excipient role, lu-
bricant excipient role, glidant excipient role, anti-
adherent excipient role, anti-friction excipient role, bind-
ing excipient role, coating excipient role, protective coat-
ing excipient role, enteric coating excipient role,
administration coating excipient role, flavor coating ex-
cipient role, lubricant coating excipient role, color excipi-
ent role, flavor excipient role, disintegrant excipient role,
preservative excipient role, sorbent excipient role, and ve-
hicle excipient role. We present the results of our onto-
logical analysis, including textual and axiomatic
definitions of these terms in the Results section.
RxNorm contains excipient information that it obtains
from Structured Product Labels (SPLs). SPLs are a digital
form of the physical product label that the Food and Drug
Administration (FDA) collects from drug manufacturers.
RxNorm includes information extracted from SPLs and
stores it with a source abbreviation (used to identify the
source of the information) of MTHSPL. RxNorm in-
cludes a has_inactive_ingredient relationship extracted
from the SPLs, which we used to identify the excipients
for drug products in DrOn. Since DrOn previously only
contained information from RxNorm under the source
abbreviation RXNORMwhich is data collected from
the other sources and then normalizedwe needed to
match the MTHSPL atoms to the appropriate RxNorm
concepts and then to the appropriate DrOn entities. It
should be noted that the MTHSPL data is denoted source
restriction level 0 in RxNorm, meaning it is licensed for
creation of derivative open source works.
We also make extensive use of Semantic Clinical Drugs
(SCDs) and Semantic Branded Drugs (SBDs) in RxNorm.
Each SCD represents a unique combination of active in-
gredients, their strengths, and dose form. An SBD repre-
sents everything that an SCD represents plus information
about a drug products trade name.1 Both SCDs and SBDs
are the result of RxNorms normalization process, and
thus are assigned concept identifiers (RxCUIs).
Using the April, 2015, release of RxNorm, we:
(1) Found all the atoms in the RXNREL table that have
a source abbreviation of MTHSPL and a
relationship type of has_inactive_ingredient.
(2) Mapped both atoms to the appropriate RxNorm
concept unique identifier (RxCUI).
(3) Mapped the RxCUIs to atoms within the
RXNCONSO table that have a source abbreviation
of RXNORM and a term type of IN (for
ingredients) or SBD (for drugs).
(4) Mapped the RxCUIs to DrOn drug product and
ingredient classes that have the same RxCUI
annotated on them.
This process gave us a mapping that connected
branded drugs in DrOn to various excipient ingredients.
Because we used unique identifiers from both DrOn and
RxNorm (RxCUIs) to create this mapping, the process
was straightforward, and required no manual resolution
of ambiguity.
We excluded excipients linked to SCDs in RxNorm
because we found that multiple generic and branded
products extracted from SPLs were linked to SCDs but
not SBDs, resulting in SCDs being linked to all the excipi-
ents of many drug products at the same time. For ex-
ample, dimethicone 10 MG/ML Topical Cream (RxCUI
200010) is associated with 39 different SPL drug products,
including many branded drugs like Proshield Glove Skin
Protectant (RxAUI 4232431) or Better Than Nature Eye
Essence (RxAUI 4660113), for which there does not also
exist in RxNorm a SBD. Future work involves represent-
ing these products distinctly in DrOn.
Analysis of active ingredients and extracting their strengths
from RxNorm
Although in Hogan et al. [1], we recognized the ac-
tive ingredient as being a scattered molecular aggre-
gate as defined and represented in the Ontology of
Biomedical Investigations, the Web Ontology Lan-
guage (OWL) representation of DrOn lagged behind
this recognition. Our first major change, then, was to
update the OWL representation of active ingredients
from, for example:: (has_proper_part some
ramipril) was updated to (has_proper_part
some (scattered molecular aggregate
and (has granular part some
ramipril))).
Hanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 2 of 9
The second update was to define active ingredient as
a role (see Results) and assert that the scattered molecu-
lar aggregate is the bearer of this role:
has_proper_part some (scattered molecu-
lar aggregate and (has granular part
some ramipril) and (is bearer of some
active ingredient role))
The third update was to begin capturing strength
information starting with the most prevalent and easi-
est case: tablets, capsules, and caplets. DrOn already
contains all of the active ingredients found within
RxNorm with a source abbreviation of RXNORM. In
RxNorm, strengths are related to Semantic Drug
Components (SCDCs), which are not represented in
DrOn. RxNorm creates one SCDC per unique com-
bination of active ingredient and strength and also re-
lates a drug to its active ingredients via SCDCs with
a consists_of relationship. We therefore carried out
the following steps to map the active ingredients of
drug products in DrOn to their appropriate strengths.
We did this using the April, 2015, version of RxNorm
as follows:
(1) Mapped the clinical drugs within DrOn to RxNorm
concepts in the RXNCONSO table with a source
abbreviation of RXNORM and a term type of
SCD using the annotated RxCUI.
(2) Mapped the SCDs from the previous step to the
appropriate concepts with a source abbreviation of
RXNORM, a relationship of consists_of , and term
type of SCDC using the RXNCONSO and
RXNREL tables.
(3) Mapped the SCDC concepts from the previous step
to the appropriate concepts with a source
abbreviation of RXNORM, a relationship of
has_ingredient, and term type of IN using the
RXNCONSO and RXNREL tables.
(4) Mapped the IN concepts to the ingredients within
DrOn using its RxCUI.
(5) Pulled out the strength of the SCDC from the
RXNSAT table using the RXN_STRENGTH
attribute name.
This process gave us a mapping between clinical drug,
ingredient, and strength that we then used to build the
OWL representation as illustrated below.
In DrOn, we place branded drug classes (corre-
sponding to SBDs) as subclasses of classes that repre-
sent preparations of specific active ingredients, their
strengths, and dose form (corresponding to SCDs).
Thus, we only needed create axioms representing
strengths at the SCD-equivalent level since these ax-
ioms are inherited by classes further down the hier-
archy and thus apply to the branded drugs.
Results
Our work has three key contributions: 1) a realist analysis
and resulting ontological representation of drug excipients
and the various roles they play, 2) a realist analysis of ac-
tive ingredients and their strengths, and 3) a realist ana-
lysis of cytochrome P450 isoenzyme binding. In the rest of
this section, we will describe them in detail.
Realist analysis of drug excipients
The excipients used in drug products have varied roles.
We define an excipient role as a role of a scattered mo-
lecular aggregate in aiding the manufacture, prolonging
the shelf life, aiding the identification, or ensuring proper
administration of a drug product.
Before creating a new term, we surveyed other OBO
Foundry resources for existing terms that met our needs.
The Chemical Entities of Biological Interest (ChEBI)
ontology [12] defines an excipient role as a generally
pharmacologically inactive substance that is formulated
with the active ingredient of a medication.
This definition would seem to be inline with our
usage, but the term seems to be used within ChEBI to
apply to individual molecules rather than aggregates,
meaning every molecule of magnesium stearate in some
drug tablet has its own role to, for instance, decrease the
adhesion between the other ingredient molecules and
the manufacturing machinery. Although it is true that
each molecule has some disposition that, in aggregate,
leads to lower adhesion, a single molecule is not suffi-
cient when added to a drug preparation by itself. Its
intended usage, and thus its role, can only be realized in
the aggregate, and thus we assign the role to the aggre-
gate of all magnesium stearate molecules used in the
manufacture of the drug product (not just those mole-
cules remaining).
Furthermore, an excipient role as defined in ChEBI is
too general. An excipient is added to a drug product
with a specific intent, unless we are to count contami-
nants. If, in the process of manufacturing a drug prod-
uct, some minor contaminant makes it into a gel
capsule, it is not an excipient. Therefore, assigning the
role of exicpient to all things formulated with the active
ingredient is too broad.
In addition to a general excipient role, we have identi-
fied sixteen specific subtypes of excipients based on spe-
cific uses. Figure 1 shows the various types of excipient
roles and the relations between them.
Lubricant excipient role: An excipient role that is real-
ized by a process of drug administration or a process of
drug manufacturing and results in either 1) decreased
adhesion between drug ingredients and manufacturing
equipment or between drug ingredients and some part of
an organism; 2) decreased friction between drug ingredi-
ents and manufacturing equipment or between drug
Hanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 3 of 9
ingredients and some part of an organism; or 3) de-
creased cohesion among particles within the drug
preparation.
Lubricant excipients are added to drug preparations to
prevent ingredients from sticking to themselves (cohe-
sion) and to other things with which they come into
Fig. 1 Excipients. The various excipient roles and their is-a relationships
Hanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 4 of 9
contact (adhesion). Common lubricants are minerals like
magnesium stearate. There are three major subtypes of lu-
bricants: glidants (glidant excipient role), anti-adherents
(anti-adherent excipient role), and anti-friction lubricants
(anti-friction excipient role). In defining the three subtypes,
we make the distinction between adhesion (which is a
steady or firm atachment) and friction (which is the force
that provides resistance to relative motion). To see the dif-
ference consider a wet piece of paper: it will adhere to a
plate of glass, but offer minimal friction to movement
along the glass.
Glidant excipient role: A lubricant excipient role that
is realized by a process of drug administration or a
process of drug manufacturing and results in decreased
cohesion or friction among particles within a drug
preparation.
A glidant is added to a drug product to reduce cohe-
sion and interparticle friction. Common glidants are talc
and magnesium carbonate.
Anti-adherent excipient role: A lubricant excipient role
that is realized by a process of drug administration or a
process of drug manufacturing and results in decreased
adhesion between drug ingredients and manufacturing
equipment or between drug ingredients and some part of
an organism.
Anti-adherents are added to drug products to decrease
the tendency of drug molecules to adhere to manufac-
turing equipment or some body part such as the throat
or esophagus during swallowing.
Anti-friction excipient role: A lubricant excipient that
is realized by a process of drug administration or a
process of drug manufacturing and results in decreased
friction between drug ingredients and manufacturing
equipment or between drug ingredients and some part of
an organism.
Anti-friction excipients are added to decrease either
internal friction (i.e., friction between ingredient parti-
cles) or friction between the drug ingredients or product
and some other object, such as manufacturing equip-
ment or some body part.
Binding excipient role: An excipient role that is real-
ized by a process of drug manufacturing and results in
increased volume or cohesion of the drug product.
Binding excipients are added to drug preparations to
1) bind active ingredients together, and 2) increase the
volume of the preparation (which is especially important
for formulations with otherwise small volumes). Com-
mon binding agents are saccharides (like sucrose) or
synthetic compounds like polyethylene glycol.
Coating excipient role: An excipient role borne by an
aggregate of molecules on the surface of a solid drug
product that is realized by a process of delaying inter-
action between entities outside the drug product and the
other ingredients in the drug product.
Coatings are extremely common excipients, added to
protect the drug preparation from destruction or con-
tamination, to ease administration by making it easier to
consume, or to improve flavor. There are five major sub-
types of coating excipient.
Protective coating excipient role: A coating excipient
role that is realized by delaying denaturation, disintegra-
tion, or some other method of destruction of a drug prep-
aration including its active ingredients.
A protective coating acts against destruction or
contamination of a drug preparation by keeping the
other drug ingredients, especially active ingredients,
away from potentially reactive substances like oxygen,
water, and various forms of electromagnetic radiation
(e.g., light).
Enteric coating excipient role: A protective coating ex-
cipient role that is realized by a process of delaying release
of one or more active ingredients from the drug product
until some targeted time or location, typically the small or
large intestine, within an organism.
An enteric coating also protects the drug preparation
from destruction or contamination, but also is designed
to disintegrate on a controlled timeline or in a particular
place. For instance, some enteric coatings are designed
to withstand the relatively high PH of the stomach, but
break down in the relatively low PH of the large intes-
tine, allowing an ingredient that would otherwise be
destroyed by or absorbed by the stomach to be absorbed
in the intestine.
Administration coating excipient role: A coating ex-
cipient role that is realized by facilitating a process of
drug administration.
An administration coating is one that somehow im-
proves administration of the drug, by for example mak-
ing insertion or consumption of the drug easier or
masking undesirable flavors.
Flavor coating excipient role: An administration coat-
ing excipient role that is realized by a drug manufactur-
ing process that results in the drug product bearing a
particular flavor quality.
Flavored coatings make it more palatable to consume
a drug product by improving its taste, often by masking
the unpleasant taste of the active ingredients.
Lubricant coating excipient role: An administration
coating excipient role that is realized by decreased fric-
tion between the drug preparation and some part of an
organism during drug administration.
A lubricant coating makes it easier to consume or in-
sert a drug product by decreasing the friction or adhe-
sion between the drug preparation and some body part
such as the throat or esophagus.
Color excipient role: An excipient role that is realized
by a process of drug manufacturing that results in a par-
ticular, desired color quality of the drug product.
Hanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 5 of 9
Colored excipients are added to a drug preparation to
make various kinds of drugs more easily identifiable by
sight to decrease the possibility of using the wrong dos-
age or wrong drug product altogether.
Flavor excipient role: An excipient role that is realized
by a process of drug manufacturing that results in the
drug product bearing a particular flavor quality.
Like a flavored coating, a flavored excipient is added
to the drug preparation to make it more palatable. This
is especially important for drug products targeted to-
wards children to make administration easier.
Disintegrant excipient role: An excipient role that is re-
alized by a process of drug administration followed by
the drug product breaking apart.
A disintegrant is added to a drug preparation to cause
it to break apart whenever it is introduced to moisture.
A disintegrant can improve administration (such as oral
medications that dissolve in the mouth) or improve up-
take of active ingredients for example, in the intestine.
Preservative excipient role: An excipient role that is re-
alized by increasing the duration of time that a drug
product is effective or by inhibiting contamination of the
drug product with a microorganism.
Preservatives are added to a drug preparation to in-
crease the lifetime of the drug preparation. Examples in-
clude antioxidants such as ascorbic acid that prevent
oxidation-reduction reactions that change active ingredi-
ents into inactive compounds and methyl paraben which
is an antimicrobial preservative.
Sorbent excipient role: An excipient role that is real-
ized by its bearer binding with water in the environment
to prevent water binding with other ingredients in the
drug product.
Sorbents are added to protect the drug preparation
from destruction or disintegration by water. A common
example is a desiccant, which is a sorbent that prevents
absorption of water into the drug product.
Vehicle excipient role: An excipient role that is realized
by a completed process of the active ingredient reaching its
intended destination during drug administration.
Generally, vehicles are the media in which the active
ingredient is dispersed to facilitate the active ingredient
reaching its intended target tissue. For example, active
ingredients that exist in solid form such as a powder
cannot be directly injected intravenously without causing
damage to veins or becoming emboli that cause damage
to the lungs. Thus they are dissolved in solution for safe
and proper administration. Other examples of vehicles
include creams, ointments, lotions, gels, and solvents for
ophthalmic, otic, and oral solutions.
Having reviewed and defined the major subtypes of ex-
cipients, we next illustrate how we represent molecular
aggregates and their excipient roles in the DrOn OWL
files. Consider a drug tablet that contains povidone and
pregelatinized starch as excipients. We axiomatize this
tablet as follows:
tablet and (has_proper_part some ('scat-
tered molecular aggregate' and
(has_granular_part some povidone)
and
(bearer_of some 'binding excipient
role'))) and
(has_proper_part some ('portion of
pregelatinized starch' and
(bearer_of some 'binding excipient
role')))
Our extraction of excipient information from RxNorm
resulted in the representation of excipients for 5,743
branded drugs, covering ~27 % of the 21,191 total num-
ber found in DrOn. There are a total of 35,455 different
drug productexcipient relationships. By comparison,
there are 22,845 relationships between drug products
and active ingredients. The main reasons there are fewer
relationships between drugs and active ingredients than
there are between drugs and excipients is that there are
fewer active ingredients and that active ingredients are
specified at a higher level in the taxonomy of drugs. Ac-
tive ingredients are defined at the level of clinical drug
form (for example, furosemide oral tablet) whereas ex-
cipients exist at the level of branded drug (more specific
than clinical drug form), because each brand of a drug
product such as furosemide 20 mg oral tablet typically
contains a different set of excipients.
Realist analysis of active ingredients
Although DrOn has always included active ingredients,
we have updated the representation to more accurately
reflect reality and to allow us to add strengths to drug
products. To do so, it was necessary to represent active
ingredient role, which we define as a role borne by an
aggregate of molecules that is a proper part of a drug
product and that is realized by (1) administration of the
drug to an organism followed by (2) some change in the
structure or functioning of some part of the organism or
its endosymbiotic organisms.
This definition meets several criteria we identified dur-
ing our analysis of active ingredients. First, it is a
realizable entity. Note that an active ingredient does
nothing until and unless the drug product is appropri-
ately administered. Second, it is a role rather than a dis-
position (or, more specifically, a function). Some
ingredients can serve as either an excipient or an active
ingredient depending on the specific drug product. For
example, calcium carbonate is an active ingredient in
certain antacid tablets, but an excipient in other prod-
ucts. Furthermore, calcium carbonate neither evolved
nor was designed to neutralize acids (a key criterion of
functions per BFO). Of course, there is some disposition
Hanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 6 of 9
at the molecular level that the realization of the active
ingredient role depends on; in the case of calcium car-
bonate, its physical makeup causes it to react with
strong acids, releasing carbon dioxide. But this dispos-
ition inheres in each individual molecule of calcium car-
bonate whereas the active ingredient role inheres in the
entire aggregate in the tablet: clinically signficant acid
neutralization occurs only with the aggregate delivered
via the tablet.
We represent the active ingredient role in OWL in a
manner similar to how we represent the excipient role.
We represent a drug tablet that has acetaminophen as
an active ingredient with a strength of 325 MG as the
following:
tablet and (has_proper_part some ('scat-
tered molecular aggregate' and
(has_granular part some acetaminphen)
and
(bearer_of some active ingredient
role') and
(bearer_of some (mass and
(has_specified_value 325)
and
(has_measurement_unit value
milligram)))))
We added strengths to 5,782 clinical drugs, cover-
ing ~41 % of the 14,035 total number, and account-
ing for ~97 % of the 5,970 tablets, capsules, and
caplets in DrOn. Representing strengths for drug
products in other dose forms (e.g., injectable solu-
tions, creams, lotions, etc.) is future work.
Realist analysis of cytochrome P450 isoenzyme binding
When a particular molecule binds to one of the isoen-
zymes in the cytochrome P450 (CYP) family, it does so
as substrate, inhibitor, or both. Induction of CYP isoen-
zymes does not involve binding to individual enzyme
molecules themselves, but rather it involves increasing
transcription of CYP isoenzyme genes so that more indi-
vidual enzymes come into existence. We could not find
in the literature any case where molecular binding of a
small molecule to a CYP isoenzyme induced or facili-
tated the activity of the isoenzyme.
We therefore represent binding of a small molecule to
a CYP isoenzyme as a disposition of the molecule. It is
not a function because the small molecule neither
evolved nor was designed to have this binding effect. It
is not a role because the tendency to bind is internal to
the physical structure of the molecule itself. Nothing ex-
ternal or socially-designated causes the binding tendency
to exist (note: we are loosly using the word tendency
here to equate to what BFO calls realizable entity).
We subdivide the binding disposition based on
whether its realization results in transformation of the
molecule into another type of molecule (that is, metab-
olism of the molecule into something else) versus
whether its realization causes inhibition of the isoen-
zyme in metabolizing other small molecules. We note
that many types of molecules used as active ingredient
drugs have both substrate and inhibitory dispositions
(for example, esomeprazine is both an inhibitor and sub-
strate of CYP2C19).
To represent enzyme binding, we identified an enzyme
binding class in the Gene Ontology (GO) and imported
it into DrOn via the Minimum Information to Reference
an External Ontology Term (MIREOT) methodology
[13]. Its definition is the following: interacting selectively
and non-covalently with any enzyme. For completeness,
we also import protein binding and binding, the parent
and grandparent of enzyme binding, respectively, into
DrOn.
For binding to the active site of an enzyme, we were
unable to identify a candidate term from any other real-
ist ontology. Thus, we created the term enzyme active
site binding disposition, which we defined as a dispos-
ition borne by some molecular entity that is realized by
binding to some enzyme and being destroyed in a process
that realizes some function of said enzyme. Similarly, we
could not find and therefore created the term function-
inhibiting enzyme-binding disposition, and defined it as a
disposition borne by some molecular entity that is real-
ized by 1) binding to some enzyme and 2) subsequent in-
ability of the active site of the enzyme to bind its
substrate(s).
We represented substrate and inhibitory binding dis-
positions for CYP2C19 and CYP2D6, because these are
the major two isoenzymes targeted by the personalized
medicine program at the University of Florida [14, 15].
We represented these in OWL by adding axioms as fol-
lows, using CYP2C19 inhibitory disposition as an ex-
ample, to the molecular entities for which they are
applicable:
subclassOf (bearer_of some function-
inhibiting CYP2C19 binding disposition)
In total, we added CYP2C19 and CYP2D6 binding dis-
positions to 65 molecules, with some of them being the
bearer of both an inhibitory and substrate definition
(Table 1). Our source of data for the types of molecules
that bear the particular types of binding dispositions was
Table 1 Number of CYP binding dispositions of various types in
DrOn. The total number of molecular entities is 65; many have
more than one disposition
Substrate Inhibitory
CYP2C19 26 16
CYP2D6 45 36
Hanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 7 of 9
the P450 drug interaction table of the Indiana University
School of Medicine [16].
Discussion
We have significantly updated and improved the repre-
sentation of ingredients in the Drug Ontology. In the
process, we have defined a number of key terms in
DrOn including active ingredient role, excipient role,
terms for numerous subtypes of excipient, and various
terms for cytochrome P450 substrate and inhibitory
binding. This representation enables automated algo-
rithms to distinguish active ingredients from excipients
in drug products, as well as determine the strength of
drug products that are capsules, tablets, and caplets.
Given that excipients have important clinical conse-
quences, including hypersensitivity reactions, their inclu-
sion could help improve research on drug products,
pharmacogenomics, and clinical decision support.
A key use of DrOn is in the improvement and
standardization of knowledge of drug-drug interactions
(DDIs) [17]. This work requires accurate representations
of active ingredients with strengths and excipients since
they impact the potential for, likelihood, and severity of
interactions. Our work on CYP isoenzymes further en-
ables query of DrOn for all drug products that contain
an ingredient whose molecular grains bind particular
CYP isoenzymes. A researcher could use these represen-
tations, for example, to identify all patients who are tak-
ing one drug that inhibits a given isoenzyme and
another drug that is metabolized by it, and thus is at risk
for adverse effects of the latter drug (that is, the inhib-
ition caused by the first drug will reduce the metabol-
lism of the second drug, leading to increased levels and
thus increased risk of toxicity).
Other work seeks to infer DDIs based on common
properties including the structure of compounds [18].
Specifically, if some but not all compounds with a given
property or structure X are asserted to have a DDI with
some compound Y, then this method identifies the
remaining compounds with X as candidates for also hav-
ing a DDI with Y. As DrOn is increasingly used for DDI
representation, it will be interesting future work to com-
pare this method applied to DrOn-based DDI represen-
tations vs. other artifacts.
For DrOns representation of strengths, we were able
to reuse the value specification of the Ontology of Bio-
medical Investigations as well as its object and datatype
properties. We used the MIREOT Protégé plugin we de-
veloped [13] to include these properties as well as the
units of measure required.
While adding excipients, we discovered that there was a
significant sparsity of branded drugs in RxNorm with ex-
cipient information. The reason is likely that RxNorm
began incorporating SPLs, the current source of excipient
information, only recently in 2012. Additionally, RxNorm
has mapped many drugs with FDA SPLs to Semantic
Clinical Drugs only. For example, dimethicone 10 MG/ML
Topical Cream (RxCUI 200010) is associated with 39 dif-
ferent SPL drug products, including many branded drugs
like Proshield Glove Skin Protectant (RxAUI 4232431) or
Better Than Nature Eye Essence (RxAUI 4660113). This
SCD has around 170 different excipients associated with
it. Another example is Dextromethorphan Hydrobromide
2 MG/ML / Guaifenesin 20 MG/ML Oral Suspension
(RxCUI 1605844), which is associated with Tussin Cough
and Chest Congestion DM Adult (RxAUI 6836489). The
excipients linked to these Semantic Clinical Drugs appear
to be a superset of all the excipients of the SPL-derived
drug products that RxNorm links to the SCD. Because
RxNorm does not represent generic drug products, the
excipients of all generic products also appear to be linked
to the SCD. Of course, these observations are likely re-
lated. Further analysis is required.
In the process of defining the active ingredient role,
we added the capability to represent pharmaceutical
strength. We began with tablets, capsules, and caplets
because they represent the total quantity of active ingre-
dient, which is simpler to represent than concentrations.
For other dose forms, RxNorm specifies the quantity of
active ingredient per unit of drug product (e.g., per milli-
liter of solution, per gram of ointment) and the total
quantity of drug product (e.g., 5 mL vial, 25 g tube of
ointment) is not always available from which the total
mass of active ingredients could be derived.
Future Work
We have three primary directions for future work. First,
we intend to increase coverage of excipients and
strengths of active ingredients. Our strength coverage
for the dose forms we used in this analysis is sufficiently
high, but we still need to work out the representation
and then extract strength information for other dose
forms, which are expressed as relative vs. total quantity
of active ingredient. Additionally, we intend to tease out
the excipients that are currently mapped to SCDs in
RxNorm, which requires further analysis.
Second, we intend to represent the induction of CYP
isoenzymes by particular active ingredients in drug prod-
ucts. The inductive effect is indirect through an increased
rate of genetic transcription that creates additional copies
of CYP isoenzymes, rather than through mere binding to
the isoenzyme. It is therefore somewhat more complex to
represent. It is also likely an aggregate effect as opposed a
property of any individual molecule (although dispositions
of the molecules are certainly involved along the way).
Third, we intend to represent therapeutic indications
of drug products. We currently posit that a therapeutic
indication is a function borne by a drug product that is
Hanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 8 of 9
realized by a process of administration to an organism,
distribution of one or more active ingredients to some
target tissue, and resulting in some physical change in
the targeted tissue. However, this work requires further
development of use cases and ontological analysis.
Conclusions
In this paper, we describe three primary lines of work: 1)
an update to our representation of active ingredients, in-
cluding adding strengths; 2) a new representation of ex-
cipients; and 3) a new represention of substrate and
inhibitory binding dispositions for CYP2C19 and
CYP2D6. We created new terms and definitions for ex-
cipient role and sixteen different subtypes, the active in-
gredient role, and various terms to represent substrate
and inhibitory binding dispositions for CYP2C19 and
CYP2D6. We also reported on how these terms were
used in the Drug Ontology, and made the updated rep-
resentations available at http://purl.obolibrary.org/obo/
dron.owl.
Endnotes
1And hence only branded drug products, and not generic
drug products, of manufacturers are assigned RxCUIs.
Competing interests
The authors declare that they have no competing interests.
Authors' contributions
Author JH contributed to the class definitions, the mining of strengths and
excipients from RxNorm, and preparation of this manuscript. Author WRH
contributed to the class definitions, the representational work for ingredients
and strengths, and preparation of this manuscript. Author JB contributed to
the preparation of this manuscript. All authors read and approved the final
manuscript.
Acknowledgements
This work was supported in part by the NIH/NCATS Clinical and Translational
Science Award to the University of Florida UL1 TR000064.
Received: 11 November 2015 Accepted: 2 February 2016
RESEARCH Open Access
Ontology-based collection, representation
and analysis of drug-associated neuropathy
adverse events
Abra Guo1, Rebecca Racz1, Junguk Hur2, Yu Lin1, Zuoshuang Xiang1, Lili Zhao1, Jordan Rinder1, Guoqian Jiang3,
Qian Zhu4 and Yongqun He1,5*
Abstract
Background: Neuropathy often occurs following drug treatment such as chemotherapy. Severe instances of
neuropathy can result in cessation of life-saving chemotherapy treatment.
Results: To support data representation and analysis of drug-associated neuropathy adverse events (AEs), we
developed the Ontology of Drug Neuropathy Adverse Events (ODNAE). ODNAE extends the Ontology of Adverse
Events (OAE). Our combinatorial approach identified 215 US FDA-licensed small molecule drugs that induce signs
and symptoms of various types of neuropathy. ODNAE imports related drugs from the Drug Ontology (DrON) with
their chemical ingredients defined in ChEBI. ODNAE includes 139 drug mechanisms of action from NDF-RT and 186
biological processes represented in the Gene Ontology (GO). In total ODNAE contains 1579 terms. Our analysis of
the ODNAE knowledge base shows neuropathy-inducing drugs classified under specific molecular entity groups,
especially carbon, pnictogen, chalcogen, and heterocyclic compounds. The carbon drug group includes 127 organic
chemical drugs. Thirty nine receptor agonist and antagonist terms were identified, including 4 pairs (31 drugs)
of agonists and antagonists that share targets (e.g., adrenergic receptor, dopamine, serotonin, and sex hormone
receptor). Many drugs regulate neurological system processes (e.g., negative regulation of dopamine or serotonin
uptake). SPARQL scripts were used to query the ODNAE ontology knowledge base.
Conclusions: ODNAE is an effective platform for building a drug-induced neuropathy knowledge base and for
analyzing the underlying mechanisms of drug-induced neuropathy. The ODNAE-based methods used in this study
can also be extended to the representation and study of other categories of adverse events.
Background
The word neuropathy is derived from two parts: neuro
referring to the nerve and pathy indicating disease.
Neuropathy refers herein to nerve damaging. The mani-
festation of neuropathy often includes chronic pain, loss
of sensation, paresthesia, dysesthesia, and motor move-
ment disorders [1]. Drug-induced neuropathies are usually
uncommon (24 % of cases in one outpatient neurology
setting), but crucial to recognize because intervention can
lead to significant improvement or symptom resolution
[2]. Typically, chemotherapy drugs cause higher inci-
dences of severe neuropathy than other drugs. For ex-
ample, bortezomib (indicated for multiple myeloma and
mantle cell lymphoma) caused treatment-related severe
peripheral neuropathy (PN) (grade 34) in ~35 % of 331
relapsed multiple myeloma patients (Drugs@FDA). Be-
sides affecting patient quality of life, an effective treatment
could be discontinued if PN is intolerable. The signs,
symptoms and severity of drug-induced neuropathy are
related to many variables such as mechanism of drug ac-
tion, drug dose, duration of treatment, and host factors.
Drug targets in the nervous system are diverse and include
cell bodies in the dorsal root ganglia, ion channels, myelin
sheath, and neuronal mitochondria. These neurotoxic tar-
gets often overlap with drug therapeutic mechanisms. For
* Correspondence: yongqunh@umich.edu
Equal contributors
1University of Michigan Medical School, Ann Arbor, MI 48109, USA
5Unit for Laboratory Animal Medicine, Department of Microbiology and
Immunology, Center for Computational Medicine and Bioinformatics, and
Comprehensive Cancer Center, University of Michigan Medical School, 1301
MSRB III, 1150 W. Medical Dr., Ann Arbor, MI 48109, USA
Full list of author information is available at the end of the article
© 2016 Guo et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Guo et al. Journal of Biomedical Semantics  (2016) 7:29 
DOI 10.1186/s13326-016-0069-x
example, taxanes, which interfere with cell division and
apoptosis by binding to ?-tubulin subunits, can disrupt
axonal transport in neurons and eventually lead to axono-
pathy. While therapeutic strategies to alleviate neuropathy
exist, a better understanding of pathophysiological mecha-
nisms of the drug-induced neurotoxicity is needed to aid
the development of novel chemotherapeutics with a lower
neurotoxic profile.
The study of drug-associated neuropathy adverse
events (AEs) relies on the use of different ontologies.
Biomedical ontologies are sets of terms and relations
that represent entities in the scientific world and how
they relate to each other. Ontologies have been used in
applications such as the establishment of knowledge
base and computer-assisted automated reasoning.
The Ontology of Adverse Events (OAE; http://
www.oae-ontology.org/) is a community-based bio-
medical ontology in the domain of adverse events
[3]. OAE provides a logically defined terminology
and term relations for various adverse events, in-
cluding different types of neuropathy adverse events.
OAE, together with related theories, also provides a
semantic framework that links clinical adverse event
phenotypes with underlying biological mechanisms
[4, 5]. Drug Ontology (DrON) is a newly generated
ontology of drugs and related drug information [6].
DrON incorporates drug information from RxNorm,
a normalized drug naming system provided by the
National Library of Medicine at NIH [7]. DrON also
links drugs to chemical names based on chemical
nomenclature as represented in Chemical Entities of
Biological Interest (ChEBI) [8]. NDF-RT is another
ontology that includes mechanisms of action for
drugs. The mechanisms of actions may be linked to
Biological Processes, a part of the Gene Ontology
(GO) [9]. All these ontologies provide the basis for
interdisciplinary study, representation, and analysis
of neuropathy adverse events.
By integrating these ontologies with known drug-
associated neuropathy AEs, it is possible to generate a
domain-specific ontology to represent and study drug-
associated neuropathy AEs. In this paper, we report our
efforts in developing a community-driven Ontology of
Drug Neuropathy Adverse Events (ODNAE). We col-
lected neuropathy-inducing drugs from a number of
datasets, ontologically represented the drugs and their
mechanisms, and generated scientific insights using
ontology-based approaches.
Methods
Identification of FDA-approved drugs with neuropathy in
their labels
Several methods were applied to identify the US Food
and Drug Administration (FDA)-approved drugs known
to cause neuropathy. First, our study included a list of
neuropathy-associated drugs identified from a previous
study using literature mining, survey of three databases
(Drugs@FDA, DailyMed, and SIDER), and manual cur-
ation [10]. This study uses neuropathy related terms
from CTCAE [11] and MedDRA [12]. Secondly, we used
an ADEpedia dataset developed at Mayo Clinic (http://
adepedia.org) [13] to obtain the information on drugs
associated with neuropathy. In the ADEpedia dataset,
drugs are represented using the RxNorm codes (i.e.,
RxCUIs) and AEs are represented using the SNOMED
CT [14] codes. Thirdly, we searched LinkedSPLs, a
Linked Data resource that published the information
of FDA-approved drug package inserts from Dai-
lyMed [15]. Lastly, we manually reviewed all the
package insert documents and selected drugs after
manual confirmation.
ODNAE editing and existing ontology term import
ODNAE was developed using the format of W3C standard
Web Ontology Language (OWL2) (http://www.w3.org/TR/
owl-guide/). For efficient editing of OAE, Protégé 4.3 or 5.0
OWL ontology editor (http://protege.stanford.edu/) was
used. Based on the annotated data, we used OntoFox
(http://ontofox.hegroup.org/) [16] to extract subsets of re-
lated terms from different ontologies. Neuropathy AEs
from OAE and drugs from DrON, were retrieved and
imported to ODNAE, respectively. The mechanisms of
most of these drugs are extracted from NDF-RT and
imported to ODNAE. Gene Ontology (GO) biological pro-
cessing terms that match the drug mechanisms were manu-
ally identified and imported to ODNAE using OntoFox.
Given that many terms from multiple ontologies (OAE,
DrON, NDF-RT, and GO) were imported into ODNAE,
the alignment of all the imported terms was a challenge
and had been solved by a carefully designed strategy to
manually assert top level terms of these imported ontology
subsets under the ODNAE ontology hierarchical structure.
Once the top level terms are aligned, the middle and bot-
tom level ontology terms will be aligned automatically. In
addition, we used Ontorat, another internally developed
web-based program (http://ontorat.hegroup.org/) [17],
to assign RxNorm and NDF-RT identifiers to corre-
sponding DrON drug terms using the annotation
property rdfs:seeAlso.
Generation of new ODNAE terms and axioms related to
drug-induced neuropathy AEs
Ontorat was used to generate specific drug-induced
neuropathy AE terms with ODNAE_ prefix, and define
new axioms to link the newly generated ODNAE terms
with corresponding drugs and neuropathy AEs. To run
the Ontorat program, all the related data were formal-
ized into a structure Excel template. Ontorat scripts
Guo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 2 of 12
were developed to identify sets of data and insert them
into ODNAE under appropriate hierarchical structures.
ODNAE access, visualization, and licensing
The ODNAE project website is located at Github:
https://github.com/odnae. ODNAE has been deposited
in the repositories of Ontobee (http://www.ontobee.org/
ontology/ODNAE) and NCBO BioPortal (http://bio-
portal.bioontology.org/ontologies/ODNAE). The
ODNAE source code is also freely available under the
Creative Commons 3.0 License (http://creativecommon-
s.org/licenses/by/3.0/). This licensing allows ODNAE
users to freely distribute and use ODNAE.
SPARQL query of ODNAE
The Ontobee [18] SPARQL query web page (http://
www.ontobee.org/sparql) was used to perform SPARQL
queries of the ODNAE ontology to answer specifically
designed questions. In total, six files of 20 SPARQL
scripts were generated for this study. These files are
stored on the Github website: https://github.com/odnae/
odnae/tree/master/docs/SPARQL. Additional file 1 con-
tains a summary of these 20 scripts.
Heatmap analysis of ODNAE data
The correlation between drug molecular entities and ad-
verse events were presented using a heatmap. The heat-
map was created using n ×m count matrix, where n is
the number of AEs and m is the number of drug mo-
lecular entities (i.e., the top level drug chemical entity
groups). Each cell in the matrix is the number of drugs
under the drug chemical group (i.e., column) that are as-
sociated with a specific AE (i.e., the row). The matrix
was generated by first using SPARQL to obtain the raw
data of each drug chemical group, drug, and drug-
associated AEs, and using an R program to process and
transfer the data to the desired format. The heatmap
was ordered using the Manhattan distance and the mo-
lecular entities were clustered using the complete link-
age method. The heatmap was plotted using R 3.1.3.
Results
The overall goal of this project is to generate and
analyze an ontology knowledge base of drug-associated
neuropathy AEs. By ontology knowledge base, we
mean that the ontology itself serves as a knowledge base
that integrates various aspects of knowledge related to a
specific domain, promoting knowledge integration and
discovery. Therefore, the ODNAE serves as a knowledge
base comprising drug components, chemical entities of
active drug ingredients, drug mechanisms, and drug-
inducing neuropathy AEs. To achieve this goal, we first
used different methods to identify drugs associated with
different types of neuropathy AEs. Related information
was then represented in the ODNAE and further
analyzed.
In what follows, single quotation marks  are used to
quote specific ontology terms.
Drugs associated with neuropathy adverse events
As described in the Methods section, three methods
(i.e., literature mining followed by manual curation [10],
ADEpedia query [13], and LinkedSPLs query [15]) were
used to obtain drugs associated with neuropathy
AEs. Each of these methods identified from 150-230
neuropathy-inducing drugs. After a second round of
manual verification, we verified 215 chemical drugs
known to induce neuropathy AEs. This list of drugs
does not include 36 drugs that were originally iden-
tified from our data sources due to either a lack of
DrON IDs, a clear label of a neuropathy AE, or ab-
sence of a subclass of neuropathy AE. It is noted
that the data from user-reported FDA adverse event
case reporting systems (FAERS) [19] were not used
since the FAERS results can be quite noisy. An Excel
file containing 215 annotated drugs and neuropathy
AEs is stored in the ODNAE GitHub repository:
https://github.com/odnae/odnae/raw/master/src/ontology/
Ontorat_inputs/odnae-data-outputupdate.xlsx.
General ODNAE design and statistics
The top level hierarchy of ODNAE is demonstrated in
Fig. 1 and explained below.
First, ODNAE extends OAE and reuses the upper level
of OAE. Like OAE, ODNAE uses the Basic Formal
Ontology (BFO) [20] as the upper level ontology. BFO
contains two branches, continuant and occurrent
[21, 22]. The continuant branch represents time-
independent entities such as material entity and qual-
ity. The occurrent branch represents time-related en-
tities such as adverse event, drug administration, drug
metabolism, and dose accumulation in human. By
aligning different terms under the two branches of
BFO, knowledge from broad biological areas related
to drug-associated neuropathy AEs were captured and
organized under a unified ontology-level structure.
Among several drug ontologies (RxNorm, NDF-RT,
and DrON), we selected DrON as the default ontology
for representing drugs, as DrON allows mapping be-
tween drugs and ChEBI chemical terms. In addition, like
ODNAE and OAE, DrON is also aligned with BFO. The
advantage of using BFO is that BFO has been adopted
by over 100 biomedical ontologies. All these ontologies
follow ontology design principles of the Open Biomed-
ical Ontologies (OBO) Foundry [22]. Therefore, we were
able to easily import and integrate related terms from
DrON, OAE, and other OBO ontologies into ODNAE.
In order to enable data integration and data reuse, we
Guo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 3 of 12
added links from the DrON terms to RxNorm and
NDF-RT IDs by annotation property rdfs:seeAlso in
ODNAE.
Figure 2 shows the basic design pattern of ODNAE
representation of drug-associated neuropathy AEs
(Fig. 2a) and one example of implementing the design
(Fig. 2b). Specifically, a drug-associated neuropathy AE
(e.g., bupropion-associated neuropathy AE) occurs after
(preceded_by) an administration of a drug (e.g., Bupro-
pion Oral Tablet or Aplezin) in a human patient. The
human has different qualities (such as age, gender, and
disease history) and genomics background which may
affect adverse event outcomes. The drug has a proper
component of a molecular entity (e.g., bupropion). The
drug also has a specific role in a biological process. The
NDF-RT mechanism of action (MoA) terms (e.g.,
dopamine uptake inhibitor) is represented as role
(BFO_0000023), which is realized in a Gene Ontology
(GO) biological process (e.g., negative regulation of
dopamine uptake GO_0051585) (Fig. 2).
The linked information illustrated in Fig. 2 is logically
defined in ODNAE. Logical constraints allow proper in-
tegration and hierarchies among terms from different
ontologies of drugs, the chemicals of active drug ingredi-
ents, GO processes, and other information cross-linked
with axioms. As a result, the ontology knowledge base of
drug neuropathy AEs can be analyzed at different levels
of classification.
As shown in Figs. 1 and 2, ODNAE imports terms
from many existing ontologies and also contains newly
generated, ODNAE-specific terms. In total, ODNAE
contains 1579 terms, including 249 terms with
ODNAE_ prefix and terms imported from other on-
tologies such as 25 OAE terms, 500 ChEBI terms,
and 331 DrON terms. The detailed statistics of
ODNAE is available at the Ontobee website: http://
www.ontobee.org/ontostat/ODNAE.
In the next sections, we will provide more details
about the ODNAE contents and scientific insights from
ODNAE data analysis.
Various types of neuropathy AEs are associated with
drugs
Our study identified 20 types of neuropathy AEs, each of
which is associated with at least one drug (Fig. 3). Repre-
sented in a hierarchical structure, these AEs are logically
defined and cross-referenced to existing AE representa-
tion systems including MedDRA [12].
Ontology representation of neuropathy-associated drugs
and drug ingredients
The active ingredient of a drug product plays a vital role
in its mechanism. The chemical structures of the drug
active ingredients are represented in ODNAE using
ChEBI terms. Additional ChEBI terms are also imported
to form the hierarchy of these active ingredients of
drugs. The relation between a drug and a ChEBI chem-
ical is presented by an object property has_proper_part
(Fig. 2).
Fig. 1 Top level ODNAE hierarchy
Guo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 4 of 12
Most drug-associating ChEBI terms are under the branch
of molecular entity (CHEBI_23367). There are 23 classes,
for example, carbon group molecular entity (CHEBI_
33582), at the third layer below ChEBI term molecular en-
tity (Fig. 4a). Among all these 23 classes, the carbon group
molecular entity class is associated with 127 drugs (the
highest number). All drugs under this group were indeed
all organic molecular entities (Fig. 4a). Among 13 sub-
classes of organic entities, heterorganic entities link to 116
neuropathy-inducing drugs (Fig. 4a). Figure 4b provides an
example of a subclass of heterorganic entities (Fig. 4b). All
the results can be counted from the ontology display in the
Protégé OWL editor. Alternatively, as detailed later, a
SPARQL script can obtain the same count results.
Ontology-based representation and analysis of drug
mechanisms
A total of 139 mechanisms of action (MoA) terms re-
lated to neuropathy-inducing drugs was identified from
NDF-RT and imported to ODNAE. We identified 13 GO
biological processes that directly realize roles, or MoAs
from NDF-RT. Many MoA terms do not have matched
GO terms. ODNAE also includes 173 GO terms that are
the ancestor (or related) terms of these 13 GO terms.
Much insight was gained by examining the NDF-RT
MoAs collected in ODNAE (Fig. 5). All NDF-RT roles
were organized as subclasses of role in cellular and mo-
lecular interactions, including the roles as enzyme in-
hibitors, immunological and biological factors, and
receptors of different biological interactions. Our results
showed that 12 neuropathy AE related drugs inhibit the
uptake of three neurotransmitters [dopamine (1), nor-
epinephrine (10), and serotonin (11)]. There are 20
drugs that interact with the G-protein receptors that
contribute to neuropathy adverse events. We identified
39 drug agonist and antagonist terms, including 16 ago-
nists and 23 antagonists. Among them, there are four
pairs of agonists and antagonists of the same targets
Fig. 2 ODNAE design pattern and example. a ODNAE design pattern of representing drug-associated neuropathy AE. b ODNAE representing
bupropion-associated neuropathy AE
Guo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 5 of 12
Fig. 3 Various drug-associated neuropathy AEs as represented in OAE and imported to ODNAE. Red numbers represent the numbers of drugs
associated with the corresponding AEs. Circled are 3 terms not asserted but inferred under peripheral neuropathy AE using a Hermit reasoner in
Protege OWL editor
Fig. 4 Example ChEBI classification of drug chemicals inducing neuropathy AEs. a 14 neuropathy-inducing drugs are classified under nucleotide.
b 21 drugs containing organohalogen compounds as active ingredients were found to induce neuropathy AEs
Guo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 6 of 12
(Table 1). Specifically, there are 3 agonist drugs and 7
antagonist drugs of the adrenergic receptor, 4 agonists
and 2 antagonists of dopamine, 4 agonists and 3 antago-
nists of serotonin, and 5 agonists and 2 antagonists of
hormone receptor (Table 1). In addition, there are 3 hor-
mone receptor modulators (i.e., Leuprolide, Leuprolide
acetate, and Taxoxifen) that are also associated with
neuropathy AE (Table 1).
The GO terms in ODNAE cover a variety of processes,
including negative regulation of neurotransmitter uptake
and synaptic transmission. GO terms are linked to genes
and proteins. We will investigate in the future how
ODNAE can represent gene/protein-based neuropathy
mechanisms with the support of GO.
Query of drug-induced neuropathy AEs
The ODNAE knowledge base can be queried through the
Ontobee SPARQL program. Different questions can be
addressed using SPARQL queries. For example, a SPARQL
script was generated to identify what drugs act as a sero-
tonin agonist (Fig. 6). The query resulted in four drugs:
eletriptan, almotriptan, sumatriptan, and zolmitriptan.
In addition to the query shown in Fig. 6, many other
SPARQL scripts were also generated to meet different
requirements for many studies introduced in this article,
we have generated many SPARQL scripts. All these
query scripts have been collected and provided in the
Additional file 1.
Heatmap analysis of the correlations between drug
molecular entities and neuropathy AEs
One question is how to correlate the drug molecular en-
tity group with specific neuropathy AEs. To address this
question, a heatmap analysis was performed (Fig. 7). The
data for the heatmap analysis was achieved using SPARQL
in the Methods section. The data obtained from SPARQL
queries include the drug molecular entities and AEs that
are associated with different drugs in ODNAE. The heat-
map explores the relation between drug molecular entities
and various neuropathy AEs (Fig. 7).
Fig. 5 Various roles in cellular and molecular interactions played by
drugs associated with neuropathy AEs. The branch circled in blue
indicates inhibitor roles related to neurotransmission. Roles circled in
red indicate either agonists or antagonists associated with
neuropathy AEs
Table 1 Four pairs of agonists and antagonists of neuropathy-inducing drugs
Target Agonists/antagonists Drugs
adrenergic receptor agonists 3: Tizanidine, Salmeterol, Salmeterol xinafoate
antagonists 7: Ziprasidone, Amiodarone, Amiodarone HCL, Propafenone, Betaxolol, Sotalol, Sotalol HCL
dopamine agonists 4: Bromocriptine, Pergolide, Pramipexole, Ropinirole
antagonists 2: Haloperidol, Ziprasidone
serotonin agonists 4: Almotriptan, Eletriptan, Sumatriptan, Zolmitriptan
antagonists 3: Alosetron, Cyproheptadine, Ziprasidone
hormone receptor agonists 5: Nevirapine, Megestrol, Dexamethasone, Fluticasone, Fluticasone propionate,
antagonists 2: Bicalutamide, Megestrol
modulators 3. Leuprolide, Leuprolide acetate, Taxoxifen
Guo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 7 of 12
Our results showed that drug-associated carbon group
molecular entities (CHEBI_33582), pnictogen (CHEBI_
33302), chalcogen compounds (CHEBI_33304), and
heterocyclic compounds (CHEBI_5686) were associ-
ated with the highest numbers of AE cases, and these
four groups of chemicals also form a cluster by them-
selves in the heatmap analysis (Fig. 7). The chemicals
in each group are also associated with different types
of neuropathy AEs. For example, in the carbon group
molecular entities, 45 drugs are directly associated
with the top level neuropathy AE, 40 drugs associated
with peripheral neuropathy AE, 6 drugs with neuro-
toxicity AE, and 5 with paresthesia AE. In addition to
the four groups of chemicals with the highest num-
bers of neuropathy-inducing drugs, other groups of
chemicals, including monocyclic, bicyclic, and poly-
cyclic compounds, are also associated with high num-
bers of neuropathy-inducing drugs. The chemical
groups that are the least associated with neuropathy
AEs include copper group entity, ring assembly, and
gold molecular entity chemicals (Fig. 7).
Discussion
The contributions of this article are multiple. First, 215
neuropathy-inducing drugs were manually collected and
curated from different reliable resources. Second,
ODNAE serves as a ontology knowledge base that repre-
sents drug-induced neuropathy AEs and links these AEs
to different sets of entities (e.g., drugs, chemical charac-
teristics, drug targets, drug mechanisms of action, and
biological processes). Third, the knowledge in the ODNAE
knowledge base was analyzed for obtaining scientific in-
sights into drug-associated neuropathy AEs from different
aspects, including related neuropathy AE classifications,
chemical structure patterns, findings from mechanisms of
actions, and the heatmap relations between chemical clas-
sifications and AE types. ODNAE also provides a semantic
platform for further knowledge addition/integration and
advanced analysis. For example, the ODNAE framework
can be extended for other drug and AE studies.
Different from many reference ontologies (e.g., OAE
and DrON) that represent terms and relations among
the terms in a specific domain (e.g., adverse events and
drugs), ODNAE serves as a ontology knowledge base
that reuses reference ontology terms and provides logical
axioms to link different pieces of information such as
neurophathy AEs, drugs, chemical elements of drug ac-
tive ingredients, and mechanisms of action. As a know-
ledge base, ODNAE captures knowledge extracted from
biomedical bench research, clinical practices, and public
health. Owing to the parsable and machine readable
nature of the AE knowledge base, ODNAE supports
neuropathy AE data exchange, data integration, and
automated reasoning and classification.
To demonstrate the advantages of ontology-supported
data integration and classification, we have mined the
ODNAE knowledge base through systematic classifica-
tion and statistical analysis and obtained many scientific
findings from this study. First, we identified the neur-
opathy AE types induced by 215 drugs. Our systematic
classification identified the major drug chemical element
groups (e.g., carbon molecular groups) and their
Fig. 6 SPARQL query of drugs acting as a serotonin agonist. The query was done in Ontobee SPARQL website (http://www.ontobee.org/sparql)
Guo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 8 of 12
subgroups that are associated with neuropathy AEs
(Figs. 4 and 7). We have also found an interesting obser-
vation that many agonists and antagonists of the same
targets (e.g., dopamine, serotonin, and sex hormone re-
ceptor) both lead to neuropathy AEs (Fig. 5 and Table 1).
Such observation suggests that these target molecules
require a balanced level in the host, and too high or too
low may lead to neuropathy AEs. We have also
generated a heatmap to further identify the relations be-
tween drug chemical entities and different types of neur-
opathy AEs (Fig. 7).
It is noted that many findings from our ontology
knowledge base analysis have been reported in the litera-
ture [2331]. For example, agonists and antagonists of
the same targets associated with neuropathy AEs have
been reported previously [23, 24]. Specific chemical
Fig. 7 Heatmap analysis of drug molecular entity-AE relations. Drug molecular entities include 20 DrON terms at the third layer under ChEBI term
molecular entity. Color scheme indicates the numbers of AEs for different groups of drugs: light grey is 0, dark grey is 1, and the rest are ordered
by yellow, orange and red
Guo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 9 of 12
structures, which are among the structures found in our
analysis, have been found to be required for the induc-
tion of neuropathy [2531]. These literature reports in-
deed confirm our analysis results from the usage of the
ontology-based neuropathy-inducing drugs as the only
input data. Given the complete list of the neuropathy-
inducing drugs in our study, our analysis also provides a
comprehensive view of features covered in the ontology.
In addition, our ontology-based strategy generates a se-
mantic framework that brings related information to-
gether in a structured and logical format and supports
knowledge integration and analysis. Such a machine-
readable ODNAE framework is novel and has not been
reported in any neuropathy adverse event studies.
ODNAE also provides a basis for educational learning,
further extension, and interaction with external domains
of knowledge to support integrative neuropathy pharma-
covigilance research.
Beyond the papers primary focuses on the data collec-
tion, ontology representation, and ONDAE data analysis
for discovering scientific insights, ODNAE can be fur-
ther used for more case studies in the future. For ex-
ample, the integrated ODNAE knowledge and data can
be used to possibly predict potential neuropathy AEs for
particular drugs based on the structures of the drugs
that have been enriched in our study. Our study found
that over half of the neuropathy-inducing drugs are
organic carbon molecules with special enrichment on
heteroorganic and organic cyclic compounds (Fig. 4). It
is known that some specific chemical structures are
required for the induction of neuropathy [2531]. For
example, 1,2-diacetylbenzene (1,2-DAB) (but not its
isomer 1,3-DAB), 1,2-Diethylbenzene (1,2-DEB), and
1,2,4-Triethylbenzene (1,2,4-TEB) are able to induce
chromogenic changes and neurotoxicity; and the 1,2-
spaced ethyl (or acetyl) moieties on a benzene ring of
these hydrocarbons have been found to be a critical mo-
lecular arrangement resulting neurotoxic properties [25
27]. It is interesting that our results show 3 benzene
drug compounds (i.e., fentanyl, sulfasalazine, and
acetylsalicylic acid) and 3 other benzoid drug com-
pounds (i.e., mitoxantrone, fluoxetine, and losartan)
are also associated with neuropathy AEs. Any spe-
cific structures in these and other drug chemical
compounds that may facilitate neuropathy processed
require further analysis. It is likely that structural
similarity analysis combined with biological studies
[2831] could be conducted among these drugs. To
validate the association between drug structures and
specific neuropathy, observational clinical trials and
laboratory experiments with valid animal models can
be considered. If a structure (e.g., 1,2-spaced ethyl
moieties on a benzene ring) is found to be more
preferentially than others in inducing neuropathy, we
can specifically avoid or modify the structure (with a
balance of efficiency) to increase drug safety.
Another future use of ODNAE is to make ODNAE a
platform to model and represent other information re-
lated to neuropathy-inducing drugs. Chemical character-
istics of drug, drug disposition in humans, and patient
factors could all play a role in the induction of adverse
drug events. For example, drug dosage, environmental
factors, individual patient age, disease, genotype (e.g.,
genetic variations compared to others), and physiological
conditions each plays a critical role in specific drug
neuropathy AEs. These parameters can be linked to
other elements presented in the ODNAE semantic
framework. Such an ontology-based semantic framework
can also be guided by related biological network theor-
ies, including the OneNet Theory of Life [5, 32]. The
ODNAE-based and theory-guided integrative analysis
would be able to identify relations between those factors
and drug-associated neuropathy. Therefore, our work
defines a very important framework for understanding
drug-induced peripheral neuropathy. Ultimately it will
allow us to advance personalized medicine, including the
development of neuroprotective strategies for cancer pa-
tients or patients suffering from neurological disorders
such as diabetic neuropathy.
Our ODNAE will be continuously expanded and com-
puterized to integrate multiple layers of information, in-
cluding chemical characteristics of drugs, biological
receptors and processes at the cellular and molecular
levels, drug disposition in patients, pharmacogenetics,
and population level variables. Drug-associated neur-
opathy AEs are most likely associated with various per-
sonal backgrounds such as age, gender, and genotype.
ODNAE can be expanded to cover these more personal-
ized factors to find trends in neuropathy and better pre-
dict events.
Conclusions
Drugs of diverse pharmacological classes may cause dif-
ferent levels of neuropathy AEs. In this study, 215 drugs
were collected and represented in the Ontology of Drug
Neuropathy Adverse Events (ODNAE). ODNAE serves
as a knowledge base that reuses existing ontologies and
includes logical axioms to represent the relations among
different entities including these 215 drugs, drug-
associated chemical elements, specific neuropathy types,
mechanisms of drug action, and biological processes.
The analyses of logically formed ODNAE information
revealed remarkable scientific insights into drug-
associated neuropathy adverse events. Particularly, our
study found different types of neuropathy AEs induced
by these 215 drugs, major neuropathy-inducing drug
chemical entity groups, the observation of agonists and
antagonists of the same targets that are associated with
Guo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 10 of 12
neuropathy AEs, and specific relations between chemical
groups and types of neuropathy AEs. These findings are
consistent with existing reports, further confirming the
validity of our ontology-based analyses that use the list
of neuropathy-inducing drugs as the only input. Overall,
ODNAE provides a useful platform for integrating and
analyzing currently known information related to drug-
induced neuropathy AEs and is extensible for future new
knowledge representation, analysis and discovery.
Additional file
Additional file 1: SPARQL scripts developed for the ODNAE analyses
used in the ODNAE manuscript. (PDF 475 kb)
Abbreviations
AE: adverse event; CTCAE: common terminology criteria for adverse events;
DL query: description logics query; FDA: Food and Drug Administration;
NCBO: The National Center for Biomedical Ontology; OAE: ontology of
adverse events; OBI: ontology for biomedical investigations; OBO: The Open
Biological and Biomedical Ontologies; ODNAE: ontology of drug neuropathy
adverse events; OWL: web ontology language; RDF: resource description
framework; SPARQL: SPARQL protocol and RDF query language.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
AG: Manually annotated drugs and their associated neuropathy AE
information from FDA and other reliable resources, helped data analysis; RR:
Manually annotated and verified information of drug-inducing neuropathy
AEs from FDA and other reliable resources, served as a pharmacology do-
main expert, helped data analysis and interpretation; JH: Literature mining of
neuropathy-inducing drugs from FDA package insert documents; YL: SPARQL
implementation, and data interpretation; ZX: Generated SPARQL queries and
analysis; LZ: Conduced the initial Heatmap analysis; JR: Standardized the
Heatmap method initiated by LZ; GJ: Queried ADEpedia for neuropathy-
inducing drugs; QZ: Queried LinkedSPLs for neuropathy-inducing drugs; YH:
Overall project design, primary ODNAE developer, design pattern generation,
adverse event domain expert, and data interpretation, and manuscript
drafting. All authors edited and approved the manuscript.
Acknowledgements
This work was supported by a NIH grant R01AI081062 and a University of
Michigan MCubed 2.0 grant to YH. We appreciate Dr. Jane Bais help in
discussion and data interpretation in the domain of neuropathy adverse
events.
Author details
1University of Michigan Medical School, Ann Arbor, MI 48109, USA. 2School
of Medicine and Health Sciences, University of North Dakota, Grand Forks,
ND 58203, USA. 3Mayo Clinic, Rochester, MN, USA. 4University of Maryland,
Baltimore County, Baltimore, MD 21250, USA. 5Unit for Laboratory Animal
Medicine, Department of Microbiology and Immunology, Center for
Computational Medicine and Bioinformatics, and Comprehensive Cancer
Center, University of Michigan Medical School, 1301 MSRB III, 1150 W.
Medical Dr., Ann Arbor, MI 48109, USA.
Received: 5 April 2016 Accepted: 28 April 2016
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 
DOI 10.1186/s13326-016-0075-z
RESEARCH Open Access
Generation of open biomedical datasets
through ontology-driven transformation and
integration processes
María del Carmen Legaz-García1, José Antonio Miñarro-Giménez2, Marcos Menárguez-Tortosa1
and Jesualdo Tomás Fernández-Breis1*
Abstract
Background: Biomedical research usually requires combining large volumes of data from multiple heterogeneous
sources, which makes difficult the integrated exploitation of such data. The Semantic Web paradigm offers a natural
technological space for data integration and exploitation by generating content readable by machines. Linked Open
Data is a Semantic Web initiative that promotes the publication and sharing of data in machine readable semantic
formats.
Methods: We present an approach for the transformation and integration of heterogeneous biomedical data with
the objective of generating open biomedical datasets in Semantic Web formats. The transformation of the data is
based on the mappings between the entities of the data schema and the ontological infrastructure that provides the
meaning to the content. Our approach permits different types of mappings and includes the possibility of defining
complex transformation patterns. Once the mappings are defined, they can be automatically applied to datasets to
generate logically consistent content and the mappings can be reused in further transformation processes.
Results: The results of our research are (1) a common transformation and integration process for heterogeneous
biomedical data; (2) the application of Linked Open Data principles to generate interoperable, open, biomedical
datasets; (3) a software tool, called SWIT, that implements the approach. In this paper we also describe how we have
applied SWIT in different biomedical scenarios and some lessons learned.
Conclusions: We have presented an approach that is able to generate open biomedical repositories in Semantic
Web formats. SWIT is able to apply the Linked Open Data principles in the generation of the datasets, so allowing for
linking their content to external repositories and creating linked open datasets. SWIT datasets may contain data from
multiple sources and schemas, thus becoming integrated datasets.
Keywords: Semantic web, Ontologies, Biomedical open data, Data transformation
Introduction
Biomedicine is a knowledge based discipline, in which the
production of knowledge from data is a daily activity. Cur-
rent biomedical research generates an increasing amount
of data, whose efficient use requires computing support.
Traditionally, biomedical data have been stored in hetero-
geneous formats in various scientific disciplines. Since the
*Correspondence: jfernand@um.es
1Departamento de Informática y Sistemas, Universidad de Murcia,
IMIB-Arrixaca, 30071 Murcia, Spain
Full list of author information is available at the end of the article
development of the Protein DataBank [1] in the seventies,
life scientists have developed many biological databases,
and there are more than 1500 biological databases accord-
ing to the 2015 Molecular Biology Database Update [2].
As a consequence, biological data are represented in dis-
parate resources [3], which makes data retrieval and man-
agement hard for life scientists because they are required
to know: (1) which resources are available and contain the
desired information; (2) the meaning of the data types and
fields used in each resource; and (3) how such resources
can be accessed and queried. There is, therefore, a clear
© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 2 of 17
need for facilitating the integrated use of such resources.
Unfortunately, there is also heterogeneity in the formats
used for storing such data, since they are not usually the
most machine-friendly ones [4].
On the medical and clinical side, the advent of elec-
tronic health records (EHRs) contributes to making more
data available for computer processing, but suffers from
similar problems. The heterogeneity of EHR systems can
be assimilated to the one of biological databases. The
semantic interoperability of EHR data not only has been
identified as a need but also considered as a reason for
inefficiencies within the healthcare system [5, 6] and
for the waste of billions of dollars in the United States
annually [7].
Translational research aims at applying basic biologi-
cal results and data into clinical activities and routine. In
recent years, supporting data-driven medicine has been
set as a challenge for translational bioinformatics [8].
For this purpose, the integration and joint analysis and
exploitation of heterogeneous data, both biological and
medical becomes critical, hence, solutions in this area are
required.
In the technical side, the Semantic Web [9] describes
a new form of Web content meaningful to computers,
in which the meaning is provided by ontologies. An
ontology represents a common, shareable and reusable
view of a particular application domain [10]. The fact
that machines know the meaning of content enables
the use of automated reasoning, which permits to infer
new information or to check the logical consistency of
the content. The Semantic Web has been proposed as
a technological space in which biomedical data can be
integrated and exploited [11]. The growing interest of
the biomedical community in the Semantic Web can
be illustrated by the fact that repositories such as Bio-
Portal [12] contain at the time of writing more than
500 biomedical ontologies, controlled vocabularies and
terminologies.
The Semantic Web community wishes to achieve the
Web of Data, which would semantically connect datasets
distributed over the Internet. The Linked Open Data
(LOD) effort1 pursues the publication and sharing of
biomedical datasets using semantic formats such as RDF2
or OWL3. The biomedical community is heavily involved
in the development of the LOD cloud [13], since integra-
tion and interoperability are fundamental for biomedical
data analysis [14]. The LOD cloud offers a promising
infrastructure for such a goal. The availability of consen-
sus ontologies generated by the biomedical community
facilitates the publication of data in the LOD cloud, since
those ontologies can be used as vocabularies for the RDF
datasets. Most efforts in this area have been solved by
in-house solutions, implementing resource-specific trans-
formation scripts. Hence, we believe that there is a need
for methods and tools that contribute to standardise
the process of getting biomedical datasets in semantic
formats.
Since the development and success of the Gene Ontol-
ogy [15], ontologies have been used to support data anno-
tation processes. The development and evolution of the
Semantic Web technologies has permitted to increase
the variety of use of such technologies in biomedical
domains. In the area of biomedical databases we can point
out two efforts of particular significance. First, the Euro-
pean Bioinformatics Institute (EBI) has developed an RDF
platform which permits the semantic exploitation of the
content of a series of EBI resources, including UniProt
[16]. Second, the Bio2RDF initiative [13] has created RDF
versions of thirty five biomedical resources (Release 3
July 2014). These efforts pursue the development of the
biomedical LOD. In the area of EHRs, the SemanticHealth
project identified that ontologies should play a fundamen-
tal role for the achievement of the semantic interoperabil-
ity of EHRs [6]. Since then, Semantic Web technologies
have been increasingly applied in the EHR domain with
different purposes: representation of clinical models and
data [1719]; interoperability of models and data [2022];
application of quality measurements and protocols to
data [23, 24].
The main objective of the present work is to pro-
pose a method that could serve to simplify the process
of generating integrated semantic repositories from het-
erogeneous sources. The approach will be able to work
with relational databases, XML documents, and EHR
data and will produce datasets described by means of
ontologies. The transformation process is independent of
the formalism used for capturing the data to be trans-
formed. This process will be driven by the semantics of
the domain to ensure the correctness and logical con-
sistency of the resulting content. This will be achieved
by defining mappings between the data schemas and the
ontologies, which will provide the semantic content. Our
approach will be able to create a repository from mul-
tiple sources, which will require to define mechanisms
for merging the data about the same entity contained
in the different resources. Besides, the resulting content
will be generated according to the principles of Linked
Open Data. We will also describe our Semantic Web
Integration Tool (SWIT), which implements the trans-
formation and integration methods, and the application
of our method in different use cases. The expected con-
tributions of our research are (1) the common trans-
formation and integration process for heterogeneous
biomedical data; (2) enabling the design of reusable map-
pings between schemas driven by domain knowledge; (3)
the application of Linked Open Data principles to gen-
erate interoperable, semantically-rich, open, biomedical
datasets.
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 3 of 17
Background
Biomedical data
The term biomedical data covers a wide range of types
of data used in biomedicine. Such data are usually
stored and represented in different, heterogeneous for-
mats, which makes their joint exploitation difficult. In
this work we are specially interested in the information
contained in biomedical databases and in the content of
electronic healthcare records because of their importance
for biomedical and clinical research.
On the one hand, biomedical databases contain large
volumes of complex, dynamic information about biomed-
ical entities. The information about a concrete biomedical
entity, like a protein, is distributed along many different
databases, which makes necessary to combine informa-
tion from different sources to get all the information.
These heterogeneous resources do not even share iden-
tifiers for the biological entities, although this particular
aspect is being addressed by initiatives like identifiers.org
[25]. XML files and relational databases are popular for-
mats used for the representation and sharing of biomedi-
cal databases. For instance, OrthoXML and SeqXML [26]
are two XML formats to standardise the representation of
orthology data. Relational databases have gained popular-
ity in the last years because they are effective in retrieving
data through complex queries. Biomedical resources such
as the Gene Ontology [15] or CHEBI [27] provide their
data in relational format.
On the other hand, the electronic health record of
a patient stores all the information digitally recorded
from the interactions of the patient with the health sys-
tem. In the last decades, many efforts have addressed
the development of EHR standards and specifications,
such as HL7 [28], openEHR [29], and ISO EN 13606
[30]. Such standards and specifications are based on the
dual model architecture, which distinguishes two mod-
elling levels. On the one hand, the information model
provides the generic building blocks to structure the
EHR information. On the other hand, clinical mod-
els are used to specify clinical recording scenarios by
constraining the information model structures. In both
openEHR and ISO EN 13606, clinical models are named
archetypes and they have been considered a promising
way of sharing clinical data in a formal and scalable way
[5]. Archetypes are used to specify clinical recording sce-
narios. An archetype may be used to record clinical data
about a laboratory test, a blood pressure measurement,
a medication order, etc. They constitute a standardised
way of capturing clinical data according to the archetype
model [31]. They are usually defined in the Archetype
Definition Language (ADL)4. EHR data extracts are
usually represented as XML documents, whose con-
tent should satisfy the constraints specified in the
archetype.
The joint semantic exploitation of data stored in XML
files or in relational databases requires methods for
the transformation of the data into semantic formats.
Both XML technologies and relational databases provide
schemas which define the structure of the datasets. In our
approach, such schemas will be used to define generic pro-
cessing methods able to transform and exploit XML and
relational data using semantic technologies. More con-
cretely, XML schemas, ADL archetypes and the schema
of relational databases will be managed in our approach.
In practical terms, ADL archetypes play the role of XML
Schemas.
Semantic representation and access to biomedical data
The World Wide Web Consortium has developed a series
of Semantic Web standards for exchanging data (e.g.,
RDF), for representing their semantics (e.g., OWL) and for
querying these data (e.g., SPARQL5). Automated reason-
ers (e.g., Hermit [32], Pellet [33]) can be used in conjunc-
tion with Semantic Web content to check for the consis-
tency of data or to infer new information. Semantic Web
technologies also offer mechanisms for storing seman-
tic data called triplestores and whose performance for
complex queries is continuously improving [34]. Linked
Open Data is a Semantic Web initiative aiming to materi-
alise the Web of Data through the publication and sharing
of datasets using semantic formats. Linked Open Data
datasets meet four requirements [35]: (1) use URIs as
names for things; (2) use HTTP URIs so that people can
look up those names; (3) when someone looks up an URI,
provide useful information, using the SemanticWeb Stan-
dards like RDF and SPARQL; and (4) include links to other
URIs, so related things can be discovered.
The data published in the Linked Open Data (LOD)
cloud are diverse in granularity, scope, scale and origin,
and the LOD cloud is constantly growing with informa-
tion from new domains. Berners-Lee6 suggested a five-
star deployment scheme for Open Data, where each level
imposes additional conditions. The use of RDF and an
appropriate use of URIs permit the achievement of four-
stars datasets. The fifth one can be achieved by linking
your dataset to external ones. It should be noted that the
community is trying to impose additional conditions to
get such stars [36]. The number of biomedical datasets
in the LOD cloud is still reduced in comparison with
the number of existing biomedical databases, but our
approach aims at facilitating biomedical communities to
join and follow the LOD principles and effort. We believe
that the development of methods that permit to get five-
star datasets would contribute to the development of the
Semantic Web.
Next, we describe the two major approaches for
data exploitation using semantic technologies: (1) the
transformation of data into semantic formats; and (2)
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 4 of 17
ontology-based data access, which works on traditional
formats.
Semantic transformation of biomedical data
Data transformation methods have been traditionally
used in projects that use the data warehouse approach and
OLAP for the semantic exploitation of data [37], and with
both XML datasets and relational databases. On the XML
side, [38] presented an approach that transforms XML ele-
ments into RDF statements, but does not transform XML
attributes. Another approach7 transforms XML instances
into RDF according to a mapping between XSD and
OWL8. These XSD2OWL mappings are canonical, since
all the XML files are transformed into RDF by applying
the same rules. Canonical XSLT-based approaches have
also been proposed [39, 40]. More recently, [41] proposed
the transformation of XML into RDF by applying XPath-
basedmappings. On the relational database side, theW3C
RDB2RDF specification9 proposes a canonical transfor-
mation/mapping for relational databases to RDF. Such a
transformation can be considered a change of format,
because the real meaning of the entities represented is not
used in such a process. This is an important limitation we
find in the state of the art transformation approaches and
tools, since they do not take into account the underlying
model of meaning.
In the last years, Bio2RDF has become the most promi-
nent initiative for the generation of biomedical RDF
datasets. Bio2RDF has developed RDF versions for 35
datasets (Release 3 July 2014), and its website contains
non-canonical transformation scripts for such resources.
To the best of our knowledge, the links between the data
and the domain knowledge are not made explicit in such
transformation scripts. For instance, there is no guaran-
tee that content about a protein from different resources
is transformed using the same meaning, and this makes
more difficult to expand the approach and to find errors.
From a process perspective, the semantic transfor-
mation of data requires the execution of Extraction-
Transformation-Load (ETL) processes. Canonical
transformation approaches apply the same ETL process
to all the data. The required information about the
semantics of the data sources is sometimes missing in
the data schema or coded in natural language [42], which
makes such canonical processes not effective enough to
obtain semantically-rich datasets. Ontology-driven ETL
processes use ontologies for giving precise meaning to
the source data, which will be made explicit in the trans-
formation phase. This also enables consistency checking
in the transformation and/or load phases, which prevents
from the creation of logically inconsistent content. Tools
like RDB2OWL [43] and Karma [44] are examples of tools
that exploit mappings between relational schemas and
ontologies to generate RDF content.
Ontology-based data access
Ontology-Based Data Access (OBDA) permits to exploit
repositories in traditional formats using semantic tech-
nologies. As stated in [45], the underlying idea is to facili-
tate access to data by separating the user from the raw data
sources. In OBDA, an ontology provides the user-oriented
view of the data and makes it accessible via queries for-
mulated solely in semantic languages such as SPARQL. In
OBDA approaches, a mapping between the ontology and
the data sources defines the view on the source data that
can be exploited using semantic technologies.
Different OBDA approaches for accessing XML and
relational data can be found in the literature. On the
XML side, XSPARQL10 was proposed as a query language
combining XQuery and SPARQL for data transforma-
tion between XML and RDF, and XS2OWL [46] creates
OWL ontologies from XML schemas for allowing query-
ing XML data using SPARQL queries. On the relational
databases side, Triplify [47], D2RQ [48], Virtuoso [49],
Quest [50], Ultrawrap [51] and Ontop [52] are likely to
be the most popular OBDA systems nowadays. Such sys-
tems differ in how they express the mappings, how they
translate the queries and in the reasoning capabilities.
Current OBDA approaches are limited in their support
for reasoning. For example, D2RQ does not support rea-
soning and OWL2 QL is the level of reasoning offered
by Ontop. OBDA tools are starting to provide support
to rule languages such as SWRL11 for enabling users to
exploit Semantic Web rules over data in traditional for-
mats. Given that our approach will rely on reasoning for
guaranteeing the consistency of the transformation and
integration of data, OBDA is not the best option for our
work.
Integration of biomedical data
A variety of XML-based approaches for the integration
of data are presented in [53]. The main conclusion of
such study is that XML has succeeded in the integration
of data, and has opened new opportunities for research,
but the variety of XML-based data formats makes very
difficult the effective integration of data sources. The solu-
tion proposed is the adoption of semantic formats, which
leads us to semantic data integration scenarios, in which
ontologies ideally provide the global schema. When this
happens, the integration process can also take advan-
tage of the benefits described for ETL processes such as
the use of precise meaning or consistency checking. This
semantic approach is also supported by the fact that the
Semantic Web is a natural space for the integration and
exploitation of data [11].
There are four major types of data integration archi-
tectures [53]: data warehouse, mediator-based, service-
oriented and peer-based. The data warehouse approach
is more related to the semantic transformation methods,
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 5 of 17
and the other three are more related to ODBA, since they
perform a virtual integration. In the literature, we can
find biomedical semantic integration approaches such as
Ontofusion [54] or TAMBIS [55], which fall in the area
of mediator-based systems or OGO [56], which follows
the data warehouse approach. Bio2RDF uses a data inte-
gration approach based on links. This is a case of virtual
integration that uses owl:sameAs statements to identify
instances referring to the same entity in other resources.
One limitation of state of the art approaches and tools
is that they are not generic enough in the sense of their
applicability to both XML and relational data. Data inte-
gration has to overcome issues such as redundancy or
inconsistency between the data sources. Most media-
tor or link-based approaches aggregate the data from
the different sources, but the availability of mechanisms
for preventing redundancy or inconsistency is not com-
mon. Those mechanisms are easier to include in data
warehouse-oriented methods, which provide more con-
trol on the data. Our approach will be mostly based on
data warehouse, since the integrated datasets (from XML
and relational resources) are assumed to be created in
a common repository. Besides, in order to preserve the
original datasets in the integrated, semantic repository,
the configuration of the integration process will enable to
merge those equivalent instances or linking them through
owl:sameAs statements.
Methods
In this section we describe the methods included in
our approach for the generation of the open biomedical
datasets. Figure 1 provides a generic description of the
method for a single input data resource. Our data trans-
formation approach is based on the definition of rules
between an input schema and an OWL ontology. Once
defined the mapping rules, the transformation approach
may also take into account identity rules defined over the
OWL ontology. Identity rules establish which properties
permit identifying an individual of a certain ontology
class. Thus, these rules permit to merge different individ-
uals of the same class. Besides, the transformationmethod
will be able to detect and, therefore, avoid the creation of
logically inconsistent content by checking the consistency
of the OWL ontology. This is done because the whole
process is supported by OWL ontologies and, therefore,
automated reasoning techniques can be applied. In gen-
eral, the approach can be applied to any input data model
providing entities, attributes and relations. In this work,
we will use XML and relational databases as input data
models. A practical requirement for our approach is that
the input schema and the ontology should have some
domain content in common. In addition to this, the output
data instances shown in Fig. 1 can be expressed in RDF or
OWL.
Data transformation rules
The transformation rules define how the content of the
input dataset is transformed into a semantic format, and
play two major roles: (1) controlling that the information
represented according to the input schema is correctly
transformed into the semantic format; and (2) prevent-
ing redundancy in the set of output dataset. For this
purpose, two major types of rules are defined in our
approach, namely, mapping rules and identity rules. Both
are described in the next subsections.
Mapping rules
The definition of the mapping rules will be illustrated with
the example described in Fig. 2. In this example, (1) the
input schema is OrthoXML [26] (Figure 2 top left), which
is a standardised format for the representation of infor-
mation about orthologous genes, (2) the ontology is the
Orthology Ontology (ORTH)12(Fig. 2 top right), which
models domain knowledge about orthology. In the exam-
ple, the entities of the input schema are represented with
boxes, the attributes with@, and the relations with arrows.
Fig. 1 Overview of the transformation approach
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 6 of 17
Fig. 2 Description of the mapping between the OrthoXML schema (left) and the orthology ontology (ORTH)(right), where dashed lines represent
the links between the content of the OrthoXML schema and the ontology and corresponding instances that fulfill the relation of congruence
In the ontology, the classes are represented with rounded
corner boxes, the datatype properties with a pin attached
to the classes, and the object properties with arrows.Map-
ping rules link entities, attributes and relations of the input
model with the ontology classes, datatype properties and
object properties. In Fig. 2, dashed lines represent the
mappings from the XML Schema to the ontology. For
simplicity, this figure does not include mappings involv-
ing relations or object properties. The ontology contains
a series of prefixes, which refer to ontologies reused in
the ORTH: ro (Relations Ontology13), ncbi (NCBI Tax-
onomy14), cdao (Comparative Data Analysis Ontology15),
and sio (Semanticscience Integrated Ontology16)
Generally speaking, the application of a mapping rule to
an instance of the input dataset generates one individual
in the ontology. The instance and the individual must ful-
fill a relation of congruence. For us, an ontology individual
t is congruent with an input instance s if t can be obtained
from s by applying a mapping rule and t is consistent with
the axioms expressed in the ontology and with s. The con-
sistency with the axioms expressed in the ontology has to
be understood in OWL DL terms. The individual must
be, in logical terms, a member of the class to which the
membership is stated.
The bottom of Fig. 2 shows an input instance (left)
and the result of transforming it into an ontology
individual (right) by applying the corresponding mapping
rules. The input instance is a gene with attributes pro-
tId=O17732, geneId=pyc-1, it is included in UniProt
and it is associated with the species Caenorhabdi-
tis elegans (NCBITaxId=6239). The ontology individ-
ual Gene_1 is a member of the class Gene, it has a
datatype property Identifier with value pyc-1. It is
linked to other individuals through the properties encodes
(Protein_O17732), ro:in_taxon (6239), and contained_in
(UniProt). These individuals are members of the classes
Protein, ncbi:organisms, and sio:database, respectively.
The ontology individual Gene_1 is consistent with the
ORTH ontology and consistent with the data defined for
the input instance. Therefore, both entities are congruent.
Our approach requires transforming entities, attributes
and relations, so themapping rules must permit to achieve
congruence at those three levels. To this end, three types
of basic mapping rules have been defined:
Entity rule. It links an entity of the input schema with
an OWL ontology class. It permits to create individ-
uals in the OWL ontology. Let S be an entity of the
input schema and T be a class of the target ontol-
ogy. Then, the entity_rule(S, T) means that for every
instance s of S, there is a congruent individual t which
is an instance of T. For example, an entity rule in Fig. 2
serves to link the element Gene of the XML Schema
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 7 of 17
and the class Gene in the ontology. An entity rule
can be complemented by a conditional statement that
transforms only those instances of S holding a cer-
tain condition on the value of a particular attribute.
Using the classes of the previous example, let A1 be
an attribute associated with S, and let c1 be a boolean
condition over A1. The entity_rule(S, T, c1) means
that for every instance s of the entity S fulfilling c1
there is a congruent individual t which is an instance
of T.
Attribute rule. It links an attribute of an entity of the
input schema with the datatype property of an OWL
ontology class. It permits to assign values to datatype
properties in the ontology. Let S be an entity of the
input schema, T an ontology class, and A1 and A2
attributes/datatype properties associated with S and
T respectively. Then, the attribute_rule((S, A1), (T,
A2)) means that for each instance of S associated with
A1 from the same schema, there is a congruent indi-
vidual of T associated with the datatype property A2
from the ontology and that A1 and A2 have the same
value. For example, an attribute rule in Fig. 2 links
the attribute id of the element gene in OrthoXML and
the datatype property Identifier of the ontology class
Gene.
Relation rule. It links a relation associated with two
entities of the input schema with an object property
associated with two classes of the OWL ontology. Let
S1 and S2 be entities of the input schema associated
through R1 and T1 and T2 be classes of the ontol-
ogy associated through the object property R2. Then,
the relation_rule((S1, R1, S2), (T1, R2, T2)) means that
given S1 and S2 linked through the relation R1, and
given the entity_rule(S1, T1) and the entity_rule(S2,
T2), then for each instance of S1 and S2 there will
be individuals of T1 and T2 respectively that will be
linked through R2. For example, a relation rule in
Fig. 2 would link the hierarchical relation between
species and gene in the XML Schema and the object
property ro:in_taxon in the ontology.
Ontology transformation patterns
The previous basic rules do not support all the types of
transformations needed in order to get semantically-rich
datasets, because sometimes we need (1) to define rules
that involvemultiple input entities and one ormany ontol-
ogy classes, or (2) to add additional information to enrich
the input data. Consequently, more complex transforma-
tions are needed. For this purpose we have adopted the
ontology pattern approach. Our ontology transformation
patterns represent a partial or complete semantic descrip-
tion of a class of the ontology. Patterns are intermedi-
ary entities among the input schema and the ontology
from the perspective of the definition of mappings. Our
patterns are templates designed by using OWL ontology
classes, datatype properties, object properties and con-
straints. Such patterns have variables which are bound to
the corresponding entities, attributes or relations.
A pattern can be defined as the tuple < S, V >, where
S stands for the set of classes, datatype properties, object
properties and individuals used in the pattern that are a
subset of those defined in the OWL ontology, and V is the
set of variables associated with the instances of classes or
the values of properties in S. A pattern is instantiated by
linking the variables with entities of the input schema, and
can be used for creating new content in the OWL ontol-
ogy. A pattern allows creating several new individuals,
giving value to datatype properties and linking individu-
als through object properties. Moreover, a pattern can be
reused several times acting as a template. A pattern also
allows for specifying fixed content that does not depend
on the input dataset or that cannot be obtained from it, so
contributing to the semantic enrichment of the content.
Figure 3 shows an example of mapping between an
XML Schema (left) that represents information about
molecules and a molecule ontology (right). The ontol-
ogy classes Molecule, Atom and Bond have a direct
mapping with elements of the XML Schema but, for
instance, the ontology does not have a class for represent-
ing chiral molecules. A chiral molecule can be defined
as a molecule with the chemical property of chirality. In
OWL, such definition can be represented as Molecule
and has_chemical_property some Chirality. In the input
schema, chirality is represented by the element property
with attribute name isChiral and whose value is repre-
sented in the element val, whose value is 0 or 1. The
pattern shown in Table 1, which is expressed in OPPL217,
defines the variable ?chiralMolecule and such rule defines
the axioms to be generated. The data instances with value
1 for the isChiral attribute (not shown in the table) are the
input for this pattern.
We could have used basic mapping rules for defining the
entity_rule(molecule, Molecule, on_condition(property
(@name = isChiral)/val, 1))), which links molecules
with value 1 for the isChiral property with the OWL
Class Molecule. The OWL instances would be incom-
plete, because they would not contain information about
chirality. That would make the input instances and the
ontology individuals not congruent. The use of the pattern
allows defining the mapping with the variable ?chiral-
Molecule, and the additional, fixed information pro-
vided by the pattern permits to satisfy the congruence
relation.
Identity rules
Identity rules define the set of datatype properties and
object properties that permit to distinguish each individ-
ual in the ontology. These rules are useful to prevent the
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 8 of 17
Fig. 3 Description of the mapping between a XML schema and an ontology in the domain of molecules
creation of redundant content and to support the inte-
gration of data from multiple sources, since identity rules
permit to identify which entities with different URI, from
the same or different datasets, represent the same entity.
Let IR be the set of datatype properties and object prop-
erties of the ontology that univocally defines the identity
for the class C. The identity_rule(C, IR) means that all
the individuals of C with the same value for the elements
in IR are considered the same. We can define an iden-
tity rule for the class Gene using the datatype property
Identifier and the object property ? Gene, ro:in_taxon,
ncbi:organisms ?. This identity rule means that two indi-
viduals of the class Gene (see Fig. 2), associated with the
same individual of the class ncbi:organisms through the
object property ro:in_taxon, and with the same value for
the datatype property Identifier are the same individual.
The execution of the transformation process
The method runs the mapping rules in the following
specific order:
 The basic entity rules are retrieved and executed. As
a result of this step, a set of new individuals of each
class of the ontology, written I, is generated.
 The group of patterns represents a special case, since
they may generate new individuals (not obtained in
the previous step) and may also add content to new
Table 1 Definition of the pattern for ChiralMolecule
?chiralMolecule: INDIVIDUAL
BEGIN
ADD ?chiralMolecule instanceOf (Molecule and
has_chemical_properties some Chirality)
END;
generated ones. Therefore, the statement of the pat-
terns that create new individuals are executed and
those new individuals are added to I. The identifica-
tion of which statements of the patterns generate new
individuals is done by checking their definition.
 For each instance of the set I the process continues as
follow:
 The rest of statements of the patterns are exe-
cuted to add any additional content to the individ-
uals.
 The basic attribute rules are retrieved and exe-
cuted, so the values of the datatype properties of
the individuals are assigned.
 The basic relations rules are retrieved and exe-
cuted, so the object properties are instantiated.
 The identity rules are checked and, in case the
instance is unique, it is added to the output
dataset. Otherwise it is merged or linked to
the equivalent one, depending on the behaviour
defined for the rule.
Data integration
The integration approach
Our approach for the integration of heterogeneous
resources is based on the application of the transforma-
tion approach described above to the different resources,
using the same OWL ontology as driver of the process.
The construction of the integrated content requires map-
ping the schemas to the OWL ontology. The OWL ontol-
ogy may have a series of ontology transformation patterns
associated, which support the integration process. The
use of patterns facilitates (1) reusing the transformation
rules with different resources, and (2) overcoming the
structural heterogeneity of input data schemas. Table 2
shows the pattern that defines a protein in the OWL
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 9 of 17
Table 2 Definition of the pattern for protein
?protein: INDIVIDUAL
?cds: INDIVIDUAL
?transcript: INDIVIDUAL
BEGIN
ADD ?protein instanceOf Polypeptide,
ADD ?protein derives_from ?cds,
ADD ?cds instanceOf CDS,
ADD ?cds part_of ?transcript,
ADD ?transcript instanceOf Transcript
END;
ontology used in one of our use cases. This pattern not
only avoids the user the need for knowing the struc-
ture of the ontology but also can be applied with minor
modifications to data resources which store the relation
protein-cds-transcript in different ways, or might even
not be defined in the input schema. Table 3 shows how
parametrizing the variable ?cds from the variable ?protein,
the pattern can be applied to data resources with a direct
protein-transcript relation without cds.
The integration process
The integration of data is carried out through the trans-
formation of each input resource. The mapping rules
enable to generate OWL content, and the identity rules are
applied during the transformation process to limit redun-
dancy and tomerge data instances. Their role is to identify
which instances from different resources correspond to
the same domain instance. Obviously, individuals with the
same URI are considered the same one.
The integration method makes the following decisions
concerning the usual problems in integration processes:
 Naming conflicts: Different input schemas may use
different terms for the same element (i.e., entity,
Table 3 Definition of the pattern for protein with minor
modification for resources without CDS content
?protein: INDIVIDUAL
?cds: INDIVIDUAL = create(?protein.RENDERING+_CDS)
?transcript: INDIVIDUAL
BEGIN
ADD ?protein instanceOf Polypeptide,
ADD ?protein derives_from ?cds,
ADD ?cds instanceOf CDS,
ADD ?cds part_of ?transcript,
ADD ?transcript instanceOf Transcript
END;
attribute, relation). The output OWL ontology pro-
vides the common vocabulary for the integrated
repository, so the mappings from the different
resources to the OWL ontology solve this problem.
 Data redundancy: More than one instance of the
input resource may describe the same domain entity,
so they are mapped to the same class in the OWL
ontology. Identity rules permit to detect such situa-
tions and to merge or link the corresponding OWL
data to minimise redundancy.
 Inconsistency due to incomplete data: The input
schema may store less attributes and relations for a
given entity than the OWL ontology. This may lead
to an inconsistent OWL knowledge base in case the
identity rules cannot be checked. When such situa-
tion is detected, the corresponding source data are
not transformed, so inconsistencies are prevented.
Patterns providing values for missing data to such
identity properties could be used to overcome this
situation.
 Differences between the resources: It may happen
that an OWL individual is created by using different
instances of the data resources, which may have dif-
ferent values for common attributes or relations. This
may be an issue for properties associated with the
identity rules. In such case, they are considered differ-
ent individuals, which are created unless they would
make the knowledge base inconsistent.
Results
In this section we describe the main results of our work.
First, we will describe the tool that implements the trans-
formation approach. Second, wewill describe how the tool
has been used in different biomedical scenarios.
The SWIT tool
The transformation approach has been implemented in
a web tool called SWIT18. SWIT provides a web inter-
face through which the user is guided to perform all the
steps of the process. SWIT is currently supportingMySQL
databases, XML schemas and ADL archetypes as input
schemas. SWIT permits to generate the output dataset
in OWL or RDF formats, which can be downloaded or
directly stored in Virtuoso [49] or in a Jena knowledge
base19.
The user can define the mappings between the input
schema and the OWL ontology. For this purpose, map-
pings created in other transformation processes can be
uploaded and reused. Once the mappings have been
defined, they can be executed, thus generating the cor-
responding RDF/OWL content. SWIT applies the map-
ping rules to the data source to generate the semantic
content, checking the identity rules to guarantee that
redundant individuals are not created. This process also
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 10 of 17
uses automated reasoning to ensure that only logically
consistent content is transformed. SWIT uses both the
OWLAPI [57] and the Jena API for processing and gener-
ating the RDF/OWL content, Hermit [32] as reasoner, and
the patterns are implemented using OPPL2.
Figure 4 shows a part of the mapping interface, which
has three main parts. The left side shows the input schema
using a hierarchical representation. The right side cor-
responds to the OWL ontology. The lower part of the
figure is a text box, which contains the mapping rules
defined. For example, the third line defines the mapping of
the attribute coorddimension of the entity molecule to the
datatype property coord_dimension of the ontology class
Molecule.
Figure 5 is a screen snapshot of the definition of the
mapping of entities of the input schema to a transfor-
mation pattern. In this case the input schema consists
on openEHR archetypes (left), which are mapped onto
an ontology transformation pattern for histopathology
reports. In the figure, we can see that the mapping would
associate a particular element of the archetypes with each
variable of the pattern. In this case, the expression corre-
sponding to the mapping rule is not shown in the figure.
Data transformation use cases
In this section we explain how we have used SWIT in
three biomedical domains. Our website contains more
information about these efforts, including the mapping
files and examples of content generated.
Orthology data
We have used SWIT to generate the integrated OGOLOD
Linked Open Dataset [58]. The first version of OGOLOD
was created with the purpose of providing an integrated
resource of information about genetic human diseases
and orthologous genes, given the increasing interest in
orthologs in research [59]. OGOLOD integrates informa-
tion from orthology databases such as Inparanoid [60]
and OrthoMCL [61], with the OMIM database [62]. The
content of OGOLOD was generated using a method to
transform the content of relational databases into an
RDF repository. The OGOLOD repository uses the OGO
ontology [56] as a scaffold to link the integrated informa-
tion about orthologs and diseases.
SWIT is currently being used to support the standardi-
sation of orthology content20 [63] promoted by the Quest
for Orthologs consortium21. For this purpose, OrthoXML
[26] is the input schema. OrthoXML defines a standard-
ised XML schema that provides the elements to describe
an orthology relationship in a uniform way, and this for-
mat is being used by a number of orthology databases.
We have defined and executed the corresponding map-
ping rules between OrthoXML format and the Orthology
Ontology (ORTH) using SWIT. So far, this has permitted
Fig. 4Mapping interface of SWIT: (left) part of an XML schema about molecules; (right) part of the classes and properties of domain ontology;
(bottom) excerpt of the mappings defined between the XML schema and the ontology
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 11 of 17
Fig. 5 Example of pattern mapping in SWIT
to generate an integrated dataset containing more than 2
billion triples.
EHR data
SWIT was the tool used for transforming EHR data of
colorectal cancer patients in the study described in [24].
The study was performed with real anonymised data from
the Colorectal Cancer Program of the Region of Murcia
and included data from more than two thousand patients.
This work required to transform data from a proprietary
format to a semantic format in order to apply colorec-
tal cancer protocols to the patient data. Such protocols
were applied using automatic reasoning over the seman-
tic content to determine the level of risk of each patient.
SWIT was the tool employed in the processing and trans-
formation of the EHR data once transformed from the
proprietary format into openEHR XML extracts. In this
case, those extracts and archetypes were the source data
and input schema for SWIT. The domain ontology used
was developed in the context of this study.
Figure 5 is an example of transformation pattern applied
in this research study, whose implementation in OPPL2 is
shown in Table 4. This pattern defines a histopathology
report according to the domain ontology, which contains
a set of findings (hasFinding), records the total number
of adenomas found (number) and the size of its biggest
adenoma (maxsize).
Chemical compounds
A third use case is currently in progress, although the
generation of the semantic dataset has been completed.
The objective of this effort is to use semantics to improve
compound selection for virtual screening. Virtual screen-
ing methods use libraries of small molecules to find
the most promising structures that could bind with
drug targets. One of such libraries is ZINC [64], a free
database of commercially-available compounds for vir-
tual screening. ZINC data can be downloaded in XML
format. In this effort, we created the XML Schema and
defined the mappings with an ontology developed by our
group.
Discussion
The availability of biomedical datasets in open, semantic
formats would facilitate the interoperability of biomedi-
cal data and would enable to carry out scientific studies
Table 4 Definition of the pattern for histopathology reports
?histopathologyReport:INDIVIDUAL,
?finding:INDIVIDUAL,
?size:CONSTANT,
?number:CONSTANT
BEGIN
ADD ?histopathologyReport instanceOf HistopathologyReport,
ADD ?histopathologyReport hasFinding ?finding,
ADD ?histopathologyReport number ?number,
ADD ?histopathologyReport maxsize ?size
END;
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 12 of 17
with larger, connected datasets. In this paper we have pre-
sented a solution based on the transformation and inte-
gration of heterogeneous data resources in which ontolo-
gies play a central role. A series of important aspects of
our work are discussed next.
The transformation and integration approach
Our transformation process follows a data warehouse
approach instead of an OBDA one because of the fol-
lowing reasons. First, we believe that the availability of
RDF/OWL resources is the most natural way of develop-
ing the LOD. In our opinion, efforts like Bio2RDF or the
EBI RDF platform are correct ways of proceeding for the
practical exploitation of biomedical data and the develop-
ment of the Semantic Web. Second, we are interested in
generating OWL knowledge bases over which OWL2 DL
reasoning can be applied, which is not ensured by cur-
rent OBDA approaches. That would be a limitation from
our exploitation point of view. We aim to obtain datasets
linked to external sources, which is also easier to achieve
with our approach. Third, to the best of our knowledge,
current OBDA approaches do not facilitate the application
of ontology patterns as we do in this work, which also per-
mits a semantically-richer representation and exploitation
of data.
Most state-of-the-art transformation approaches and
tools from XML or relational databases into RDF/OWL
are based on canonical transformations or are not based
on mappings with domain knowledge (i.e., ontologies).
Such tools mainly perform a syntactic transformation of
the traditional formats, making the semantic interoper-
ability of the obtained datasets difficult. Besides, there
are no methods that can be applied to both XML and
relational databases. Ourmethod provides a semantic rep-
resentation of the input datasets by performing a trans-
formation guided by domain knowledge using an OWL
ontology, so performing an ontology-driven ETL pro-
cess. This is similar to RDB2OWL [43] and Karma [44].
SWIT and RDB2OWLhave in common that themappings
between the input model and the ontology are manually
defined, but RDB2OWL is limited to input datasets in
relational format and does not provide any solution for the
problem of complexity on the manual definition of map-
pings when using complex ontologies or data integration.
Karma has the advantage of performing semi-automatic
mapping of input databases and ontologies. However, this
mapping process depends on the existence of a knowledge
base of previous mappings.
We follow a data warehouse-oriented integration
method, although our approach has features associated
with the integration based on links, because our map-
ping rules permit to define links to external datasets. This
architecture is similar to the one applied in Bio2RDF, with
the difference that our repositories may contain data from
multiple sources. Although that could also be possible in
the Bio2RDF effort, it is focused on transforming single
datasets. In fact, we believe that an effort such as Bio2RDF
could benefit from our approach. Currently, one transfor-
mation script has to be written to include a new dataset in
Bio2RDF. SWIT would reduce the implementation effort
for relational or XML sources in the sense that only the
definition of the mappings would be needed, since SWIT
would execute the data transformation. Besides, SWIT
mappings could be reused for new datasets. Using SWIT
would have the cost of making explicit the mappings with
an OWL ontology, but it would also provide benefits in
terms of consistency checking and homogeneity in both
the richness of the semantic description and the structure
of the data.
Next, some additional aspects concerning the different
subprocesses are discussed.
Mapping
Data transformation and integration are based on the
definition of mappings between the data schema and
the OWL ontology. The difficulty and the complexity
of mapping not only relies on finding the correspond-
ing entities in the domain ontologies but also on being
able to design the corresponding ontology content pat-
terns. Once the rules and patterns are designed, SWIT
reduces the implementation effort by executing them
and generating the corresponding semantic content. Pat-
terns are also used in Populous [65], which is focused
on creating ontologies from spreadsheets. Our experience
reveals that semi-automatic mapping techniques would
contribute to significantly reduce the mapping time, so
efforts in this area are key to support the mapping
process.
To the best of our knowledge, there is no standard lan-
guage to define mappings from different types of input
models to OWL ontologies. We are currently using a
language based on the former ontology alignment for-
mat22, which has evolved into EDOAL23. The W3C has
developed the R2RML mapping language24 for mapping
relational databases to RDF, but does not cover XML. Our
current language permits to express mappings from rela-
tional databases and XML schemas to OWL ontologies,
and it could be easily extended to cover new types of
inputmodels (i.e., OWL ontologies). Ourmappings can be
reused, especially for data transformation processes that
use the same OWL ontology, but the lack of standardisa-
tion in this area forces third parties to do some additional
work in order to include the mappings generated with
SWIT.
In this paper, we have used the mapping rules for cre-
ating OWL individuals from XML or relational data, but
the process can also be applied for the creation of ontol-
ogy classes. This might be helpful in case the content of
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 13 of 17
the OWL ontology used is not sufficient for mapping the
input schema. For this purpose, the mapping rules were
extended to produce OWL classes instead of individuals.
This class-based approach also permits to use patterns.
The only difference is that the set of variables associated
with the pattern are bound to entities instead of instances.
We have actually applied such approach for the generation
of openEHR archetypes from CEM clinical models [22].
In that study, the input schemas were OWL ontologies
corresponding to CEM clinical models and the openEHR
OWL ontology was the output schema. Basically, the cre-
ation of the openEHR clinical models was approached as
extending the openEHR OWL ontology with the specific
content of the clinical model. In OWL, being an individual
or a class can be seen as the role played by a given con-
cept [66]. The representation of knowledge may therefore
be based on individuals or classes, this decision depending
on the expected use of such knowledge. In fact, punning
was included in OWL 2 DL to enable different uses of
the same term, so an individual and a class can have the
same URI. From a formal ontology perspective, enabling
this possibility might be reason enough for a criticism to
our approach, but it is needed from a practical perspec-
tive. This situation can be exemplified in the orthology use
case. Orthology databases basically contain information
about genes, proteins and organisms. In the database they
are represented as individuals, but they semantically cor-
respond to classes, since there are many instances of each
gene, protein and organism.
Transformation
The transformation method checks all the formal aspects
that guarantee the generation of consistent content, inde-
pendently of the use case and the intended exploitation of
the data. The logical consistency of the content is guaran-
teed by the application of OWL reasoning. Consistency is
granted independently of the output format, that is, RDF
or OWL, because OWL DL semantics is applied during
the transformation process. The individuals are expressed
in RDF or OWL at the end of the process. In case the
methods find that inconsistent content is going to be
generated, such content is automatically discarded. Using
OWL for ensuring the consistency of the data generated
in ETL processes has also been done in other works, such
as [42].
Our experience in semantic data transformation in the
last years reveals that the semantic representation of the
data sometimes needs additional content that is not made
explicit in the XML schema or in the corresponding table,
so the additional meaning is not provided by the canonical
transformation methods. We believe that such additional
meaning can be included during the ETL process. In
this context, our patterns are equivalent to ontology con-
tent patterns [67], which are focused on the definition
of templates for the semantically-rich, precise description
of the meaning of the content. We believe that ontol-
ogy content patterns are a solution for those situations
in which the source data does not contain all the infor-
mation needed to generate semantically-rich RDF/OWL
content.
The computational complexity of the full method
depends on the number of individuals and the mean
number of properties and relations per individual. As a
consequence, for medium and large datasets, the transfor-
mation time may be longer than expected because of the
number of instances of axioms to be generated. Thismight
not be a problem in case of stable datasets or batched
transformation processes. However, according to our
experience with SWIT datasets, the intended exploitation
of the datasetmight permit to relax some conditions of the
transformation process. In case of not performing inte-
gration processes, identity rules are only needed if, for
instance, two entities from the input are mapped onto the
same ontology class. In case of not requiring automated
reasoning on the transformed dataset, the generation of
some types of axioms might be omitted, saving time and
space. owl:differentFrom axioms are an example of a time
and space consuming type of axiom, but they might be
skipped in some cases. The lesson learned here is that the
optimal configuration of transformation depends on the
use case, so the flexibility of the process is basic for get-
ting the desired semantic dataset. All these aspects can be
considered adjustable parameters for the execution of the
process using tools like SWIT.
Integration
The integrationmethod is useful for scenarios that require
the creation of a repository that includes portions of data
from different resources. In case of wishing a link-based
integration, the mechanism offered by SWIT to include
links in the mapping rules could be sufficient. The key
objectives of the integration method are (1) detecting
equivalent data instances to reduce redundancy and (2)
ensuring the consistency of the resulting repository. Both
tasks are supported by OWL reasoning.
Identity rules are fundamental in the integration pro-
cess, because they control the redundancy of the indi-
viduals created. They describe which properties permit
identifying an individual of a certain ontology class. For
example, we could integrate two resources about proteins
which use different identifiers for the proteins, and those
identifiers are used in the URI of the individual. Those
resources might be using the Human Genome nomencla-
ture for naming the gene associated with the protein. If the
gene name is used in the identity rule, then SWIT would
find that both individuals refer to the same protein.
In addition to this, the meaning of our identity rules
is similar to the identity criteria proposed by formal
Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 14 of 17
ontologists [68], because they determine which conditions
are sufficient for identity. The properties used in iden-
tity rules are those that would be included in OWL Key
axioms25. Key axioms associate a set of object properties
and datatype properties with a class, so each individ-
ual is identified by the values of such set of properties.
Hence, when two individuals have the same values for
such properties, they are considered the same individual.
Key axioms are only applied over those individuals with
asserted membership to a class. Such inferencing-related
limitation made us to define our identity rules.
Interoperability
Next, we discuss how and to what extent SWIT promotes
or facilitates the interoperability of the datasets. Let us
consider a resource about proteins, which uses a local URI
for each protein, but also stores the UniProt Accession
Numbers (AC). Let us suppose that we want to link every
protein to the corresponding URI in UniProt. It should
be noted that the UniProt URI for each protein differs
in the AC. For example, the URI for the protein P63284
is http://purl.uniprot.org/uniprot/P63284. SWIT provides
two different ways for creating such link:
1. Redefinition of the URI. We can use the UniProt URI
instead of the dataset URI, since SWIT permits to
define which prefix has to be used in the transforma-
tion of each entity.
2. Linkage of resources with owl:sameAs. The transfor-
mation of the protein uses the dataset URI but creates
an owl:sameAs link to the UniProt URI.
Either action can be applied to transform the data with
RESEARCH Open Access
MicrO: an ontology of phenotypic and
metabolic characters, assays, and culture
media found in prokaryotic taxonomic
descriptions
Carrine E. Blank1*, Hong Cui2, Lisa R. Moore3 and Ramona L. Walls4
Abstract
Background: MicrO is an ontology of microbiological terms, including prokaryotic qualities and processes, material
entities (such as cell components), chemical entities (such as microbiological culture media and medium
ingredients), and assays. The ontology was built to support the ongoing development of a natural language
processing algorithm, MicroPIE (or, Microbial Phenomics Information Extractor). During the MicroPIE design
process, we realized there was a need for a prokaryotic ontology which would capture the evolutionary
diversity of phenotypes and metabolic processes across the tree of life, capture the diversity of synonyms and
information contained in the taxonomic literature, and relate microbiological entities and processes to terms
in a large number of other ontologies, most particularly the Gene Ontology (GO), the Phenotypic Quality
Ontology (PATO), and the Chemical Entities of Biological Interest (ChEBI). We thus constructed MicrO to be
rich in logical axioms and synonyms gathered from the taxonomic literature.
Results: MicrO currently has ~14550 classes (~2550 of which are new, the remainder being microbiologically-relevant
classes imported from other ontologies), connected by ~24,130 logical axioms (5,446 of which are new), and
is available at (http://purl.obolibrary.org/obo/MicrO.owl) and on the project website at https://github.com/carrineblank/
MicrO. MicrO has been integrated into the OBO Foundry Library (http://www.obofoundry.org/ontology/micro.html), so
that other ontologies can borrow and re-use classes. Term requests and user feedback can be made using MicrOs
Issue Tracker in GitHub. We designed MicrO such that it can support the ongoing and future development of
algorithms that can leverage the controlled vocabulary and logical inference power provided by the ontology.
Conclusions: By connecting microbial classes with large numbers of chemical entities, material entities, biological
processes, molecular functions, and qualities using a dense array of logical axioms, we intend MicrO to be a powerful
new tool to increase the computing power of bioinformatics tools such as the automated text mining of prokaryotic
taxonomic descriptions using natural language processing. We also intend MicrO to support the development of new
bioinformatics tools that aim to develop new connections between microbial phenotypes and genotypes (i.e., the
gene content in genomes). Future ontology development will include incorporation of pathogenic phenotypes and
prokaryotic habitats.
Keywords: Prokaryotes, Microbes, Ontology, ChEBI, Gene Ontology, Metabolic characters, Natural language processing,
Prokaryotic taxonomy, Bacteria, Archaea, Microbial bioinformatics
* Correspondence: carrine.blank@umontana.edu
1Department of Geosciences, University of Montana, Missoula, MT 59812,
USA
Full list of author information is available at the end of the article
© 2016 Blank et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Blank et al. Journal of Biomedical Semantics  (2016) 7:18 
DOI 10.1186/s13326-016-0060-6
Background
Microorganisms comprise most of the evolutionary and
genetic diversity in the tree of life [13], and produce a
significant proportion of the standing crop of cellular
carbon on the Earth [4, 5]. Prokaryotic microorganisms
manifest their diversity in the form of morphological
phenotypes (such as biofilm formation, multicellularity,
and differentiation into specialized structures), ecological
phenotypes (inhabiting environments that have particu-
lar temperature, salinity, and pH values), metabolic
phenotypes (the ability to catalyze discrete chemical
reactions), and the ability to perform biological pro-
cesses (carrying out photosynthesis) [6]. Several studies
have examined the evolution of microbial phenotypic
traits in deep time [713]. Nevertheless, most of these
studies have focused on relatively small taxonomic groups,
or have used a small number of phenotypic traits. This is
because the taxon-by-character matrices (which record
the presence and absence of traits for each taxon) required
for these studies have been constructed manually and thus
require significant efforts to build. Hence, the field needs
to develop tools that can allow the accelerated, broad-
scale study of the evolution of phenotypic traits across the
prokaryotic domains of life.
Bioinformatics resources that are needed to accelerate
such evolutionary studies include tools that permit the
rapid processing of large amounts of legacy text and
databases (which contains detailed information on
phenotypes and metadata) as well as tools that facili-
tate the rapid processing of genotypic data (genomic
sequences). Such tools could lead to new profound
insights in broad-scale microbial evolution, as well as
lead to new mechanisms for genome annotation (by
associating novel phenotypes with genotypes). To ad-
dress some of these needs, our team has developed
an ontology to assist development of a new natural
language processing (NLP) algorithm, MicroPIE (or
Microbial Phenomics Information Extractor; https://
github.com/biosemantics/micropie2) [14]. MicroPIE is
designed to automatically extract text from prokary-
otic taxonomic descriptions and to export a character
matrix. The character matrix can then be used to
study the evolution of traits using phylogenetic com-
parative methods. Most prokaryotic taxonomic de-
scriptions are published in the International Journal
of Systematic and Evolutionary Microbiology (IJSEM)
and follow a semi-formalized structure. However, this
structure has changed over time, and the content
within descriptions (types of reported or assayed
phenotypic characters, as well as naming conventions
for chemical entities) has also changed. Some taxo-
nomic descriptions (such as for the Cyanobacteria)
are usually published outside the IJSEM and have his-
torically followed the botanical code [15, 16], thus
they often have different information content. Also,
during the development of MicroPIE, we observed
that different authors can have markedly different
ways of naming or describing synonymous prokaryotic
structures and processes (for example they might de-
scribe the morphology of rods as an elongated cocci,
short cylinders, or bacilli), making NLP treatment of
text from taxonomic descriptions challenging.
Presently, some of the text extraction algorithms
within MicroPIE use a list of terms that includes all the
synonyms we have found in a sampling of the prokary-
otic taxonomic literature. However, the term lists treat
all synonyms as distinct terms. Also, in prokaryotic taxo-
nomic descriptions there is variability in how common
traits are described. For example, we have observed that
authors report a positive result of the indole assay as
indole test positive, indole-positive, indole pro-
duction, indole reaction is positive, indole formed,
or tryptophanase produced. While a domain-expert
would immediately recognize that these are all syn-
onymous, a computer or non-domain expert may not.
Finally, NLP supported by term lists lacks inference
power. For instance, NLP cannot infer that an organ-
ism with an optimal growth temperature of 60 °C is a
thermophile.
Through the MicroPIE development process, it be-
came evident that the field needed a robust ontology.
While an early version of the Ontology of Microbial
Phenotypes (OMP) was available [17, 18], it was focused
on E. coli phenotypes and had a structure that did not
readily lend itself to term re-use. Thus, we created a
new ontology, MicrO, which suited our projects needs
and that could be usable by the ontology community at
large. This ontology captures much of the evolutionary
diversity of prokaryotic traits and processes and the rich
legacy of material entity, quality, and assay terms that
encompasses the vast diversity found throughout the
prokaryotic taxonomic literature. We also designed the
ontology to use as a controlled vocabulary that linked
the diversity of synonyms found in the literature to cen-
tral terms that will help support text mining algorithms
such as MicroPIE. The ontology leverages logical infer-
ence power (for example, to predict that an aerobic
microorganism that metabolizes glucose is both a che-
moorganotroph and uses oxygen as a terminal electron
acceptor) to help populate character matrices and to
infer higher-order character states that are not explicitly
stated in taxonomic descriptions. Finally, MicrO relates
microbiological entities and processes to entities and
processes in a large number of other ontologies, includ-
ing the Gene Ontology (GO), the Phenotypic Quality
Ontology (PATO), and the Chemical Entities of Biological
Interest (ChEBI) [1921]. The relationship of classes in
MicrO to classes in other ontologies is formalized in a
Blank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 2 of 10
logical, structured, computable way such that MicrO
will be able to support future advances in microbial
bioinformatics, for example in the automated extraction
of text using NLP and integrating microbial characters
from different databases or repositories. We anticipate
the ontology will provide an important new tool for fa-
cilitating the incorporation of massive amount of text
descriptions into future generations of biological ana-
lysis and computational tools.
Methods
For the development of MicrO, we took a hybrid top-
down and bottom-up approach. For the top-down ap-
proach, we used established ontology development
principles and practices, such as the use of an upper
ontology. In following a bottom-up approach, we used
the principles of literary and user warrants [22] and
attempted to make the ontology capture the vast di-
versity of phenotypic character information reported
in the prokaryotic taxonomic literature.
Top-down ontology development
The ontology was constructed using Protege OWL
(Web Ontology Language; version 4.3) [23]. It is built
upon a Basic Formal Ontology (BFO) foundation, and
followed OBO Foundry principles [24, 25]. During the
early developmental stages of MicroPIE and MicrO, we
created extensive term listsmanually generated lists of
terms and synonyms (including variations on spelling)
from a large corpus (~1,500) of diverse prokaryotic taxo-
nomic descriptions obtained from the primary scientific
literature. We focused on taxonomic descriptions from
the Archaea, Cyanobacteria, Mollicutes, Bacteroidetes,
and Firmicutes. In this way, we sampled characters from
extremophilic chemotrophs, Cyanobacteria (which often
have very different taxonomic descriptions and morpho-
logical traits), as well as a rich diversity of heterotrophic
and chemotrophic, non-pathogenic and pathogenic,
species found in the Mollicutes, Bacteroidetes, and
Firmicutes. Most non-cyanobacterial descriptions were
obtained from the IJSEM, while most cyanobacterial
descriptions were sampled from AlgaeBase (an online
database of taxonomic descriptions from cyanobac-
teria and algae) [26].
The term lists were organized hierarchically using
categories and subcategories, and this organizational
structure is currently used to support MicroPIE. Ex-
amples of categories include Colony Morphology, Cell
Shape, Metabolic Substrates, Growth Conditions, and
Antibiotic Physiology. Each category has a varied
number of subcategories, for example, the category
Colony Morphology included Colony Shape, Colony
Texture, and Colony Color as subcategories. Categor-
ies and subcategories in the term lists were matched
to higher-level ontology classes and re-organized into
candidate qualities, processes, and material entities
(including chemical entities and cellular components).
These were then used to create the higher-level ontol-
ogy classes in MicrO, which were then incorporated
into the upper level BFO hierarchy.
Bottom-up ontology development
Lower-level terms in the term list were manually
grouped into candidate classes and synonyms in Microsoft
Excel. Terms synonymous to existing classes in ontologies
in the OBO Foundry were identified using OntoBee [27].
These were imported into MicrO using OntoFox [28].
Imported classes
Imported classes (Additional file 1: Table S1) were
used to provide higher-level classes for the nesting of
MicrO-specific classes, to represent microbiological
concepts present in other ontologies, and to construct
logical axioms for classes in MicrO. Eight classes in
BFO were imported, to provide the top-level structure of
the ontology. For many ontologies, a relatively small num-
ber of lower-level classes were imported. These included
BSPO, CHMO, CL, DRON, IAO, NCBI Taxonomy, NDF-
RT, OBI, PO, PR, REO, RO, and Uberon (respectively: the
Biological Spatial Ontology, Chemical Methods Ontology,
Cell Ontology, Drug Ontology, Information Artifact
Ontology, NCBI Taxonomy, National Drug File Reference
Terminology, Ontology for Biomedical Investigations,
Plant Ontology, Protein Ontology, Reagent Ontology, Re-
lations Ontology, and Uber Anatomy Ontology) [2937].
For other ontologies (ChEBI, GO, PATO), a larger number
of higher- and lower-level classes were imported. This was
because these classes were used to construct the bulk of
the logical axioms in MicrO. Classes from CL, ENVO,
and IDO (the Cell Ontology, Environment Ontology, and
Infectious Disease Ontology) [38, 39] were imported to
help support the future construction of logical axioms as
MicrO expands to incorporate new sets of classes (such as
pathogenic phenotypes and microbial habitats). For IAO
and RO, imported terms were nearly entirely object and
datatype properties. These were used to construct logical
axioms, and also served as parent classes for new object
properties in MicrO.
Because much of microbial diversity lies in the meta-
bolic transformation of chemicals, most of the imported
classes were from ChEBI (~6,450 classes). Imported clas-
ses included various chemical substances (e.g., lecithin,
bacitracin, collagen), roles (e.g., biological pigment, bio-
marker, visual indicator, reducing agent), and large
numbers of inorganic chemicals, organic chemicals, and
mixtures. In addition, we submitted term requests for
several hundred microbial-specific compounds to ChEBI,
including minerals, antibiotics, dyes/stains, lipids, cell
Blank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 3 of 10
wall constituents, and metabolic substrates and prod-
ucts. These new chemical classes were then imported
into MicrO. Finally, a large number of synonyms were
added to existing and new chemical classes in ChEBI.
Because phenotypes (size, shape, relationships of
cells, cell parts, and colonies) are frequently present in
prokaryotic taxonomic descriptions, a large number of
imported classes (1,580) came from PATO. Imported
classes included quality classes (such as morphology,
size, shape, physical quality and their children),
process quality classes, and increased and decreased
quality classes.
Features of prokaryotic cells (e.g. vacuoles or flagella)
as well as biological processes and enzymatic activities
are common in prokaryotic descriptions. Hence, many
classes (632) were imported from GO. These included
classes involved in prokaryotic cell parts (e.g., cell hair,
pilus, periplasmic flagellum), biological processes (e.g.,
photorespiration), enzymatic activities (e.g., metalloen-
dopeptidase activity), and biological responses to various
chemicals (e.g. response to bile acid).
One hundred and fifteen classes were imported from
Uberon. These included classes associated with anatom-
ical structures and organism substances, which can serve
as disease targets for pathogenic microorganisms and as
material that is processed to generate chemical entities
(e.g., 'brain heart infusion') used in the cultivation of
microorganisms.
A handful of classes (22) were imported from OBI,
including assay, various entities involved in microbio-
logical assays such as test tube, microscope slide, micro-
scope, culture medium and associated entities such as
cultured cell population and cultured clonal cell popu-
lation. One hundred and five classes were imported
from CHMO, and included classes such as evaporation,
grinding, autoclaving, and sample heating. These were
used to construct axioms involved in microbiological
medium ingredients. Imported classes from BSPO (83)
included anatomical margin, anatomical region, and
anatomical side and their respective children, to sup-
port creation of logical axioms relating to the spatial re-
lationships of differentiated prokaryotic structures.
Classes from CL (284) included native cell, prokaryotic
cell, eukaryotic cell, and differentiated red and white
blood cells (associated with pathogenic phenotypes and
used in microbiological diagnostic assays). Seven classes
were imported from PO, these included fruit, seed,
plant embryo. These classes were used in the construc-
tion of logical axioms for microbiological medium ingre-
dients for MicrO classes such as malt extract, soya
extract, soy peptone, olive oil, and filtered tomato juice.
Over 500 classes were imported from NCBI Taxonomy
to construct logical axioms for entities and qualities that
inhere to particular prokaryotic taxa, and to logically
connect culture medium recipes used to cultivate par-
ticular prokaryotic taxa.
A large number of classes relevant to microbiological
habitats and processes (1,962) were imported from
ENVO. Although currently few of these classes are used
in logical axioms in the current version of MicrO, their
presence will support the future development of MicrO
(which will involve the incorporation of microbial habi-
tats). Similarly, microbiologically relevant classes from
IDO (81 classes) were imported to support the future in-
corporation of pathogenic phenotypes into MicrO.
MicrO-specific classes
If no relevant classes in existing ontologies in the OBO
Foundry Library could be identified, the candidate clas-
ses were converted into ontology classes, and entered
into MicrO. Some classes were derived from information
contained in commercial and non-commercial websites
outlining microbiological concepts (such as colony
morphologies, diagnostic assays, and culture medium
recipes) or from scientific publications. In such cases,
the definition source (website or publication) was cited.
Each class also has a list of synonyms found in the cor-
pus of taxonomic descriptions. Class synonyms were an-
notated in the ontology as exact synonyms, broad
synonyms, or related synonyms using naming conven-
tions developed by GO [40]. Classes under the parent
imported class OBI:assay were created and structured
using the conventions used by OBI. Compound class
naming followed the ANSI/NISO guidelines [22]. Finally,
we made use of the HermiT 1.3.8 and the FaCT++ rea-
soner in Protege to verify performance of logical axioms.
Availability
MicrO is available in OWL format as a permanent URL
[41] and from the project website [42]. MicrO has been
incorporated into the OBO Foundry Library so that
other ontologies can import classes and build upon it
[43]. The contents of the ontology are available under a
CC-BY license [44].
Results and discussion
Overview of ontology contents
MicrO (version 1.3, released on March 23, 2016) con-
sists of ~2550 classes (plus thousands of synonyms) de-
rived from text contained in the taxonomic descriptions
of diverse prokaryotic taxa that span the archaeal and
bacterial domains of life. MicrO incorporates more than
12,000 additional relevant terms from 19 other ontol-
ogies in the OBO Foundry Library and these imported
terms are connected to MicrO classes using a large
number of logical axioms (over 24,130, with 5,446 spe-
cific to MicrO). The largest categories of classes in the
ontology include assays (enzymatic, metabolic, and
Blank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 4 of 10
phenotypic assays), microbiological culture media and
media ingredients, and prokaryotic qualities (including
colony morphologies, shapes, and sizes). Other types of
classes (such as those describing prokaryotic cell and cell
parts) are scattered and nested within GO classes. Fi-
nally, a handful of classes in MicrO are scattered in
various other parts of the ontology. The large-scale
architecture of classes of material entities, processes,
and qualities in MicrO, and how they nest in other
ontologies, is shown in Additional file 1: Figures S1-S3.
Prokaryotic chemical entities
A large number of new chemical classes (>750) were
entered into ChEBI as a result of MicrO development.
New ChEBI classes include minerals (including sulfide
minerals), stains/dyes, metabolic substrates, lipids, inor-
ganic chemicals, and antibiotics. In addition, requests
were made to add synonyms (188) to existing and new
ChEBI classes. Many microbiologically specific chem-
ical mixtures, however, were retained under MicrO.
These were categorized into defined inorganic chem-
ical mixture (62 classes), undefined inorganic chemical
mixture (4 classes), defined organic chemical mixture
(29 classes), and undefined organic chemical mixture
(121 classes; Additional file 1: Figure S4). Examples of
defined inorganic chemical mixtures include trace ele-
ments solution SL-6 and modified MJ synthetic sea
water. Examples of undefined inorganic chemical mix-
tures, used as ingredients in microbiological culture
media, include filtered aged seawater and sea salt. Ex-
amples of defined organic chemical mixtures include
Balch vitamin solution, dried bovine hemoglobin, and
hemin solution. Examples of undefined organic chem-
ical mixtures include clarified rumen fluid, ox bile
salts, egg yolk oil, laked rabbit blood, and inspissated
serum. Additional classes were created for complex
mixtures that were produced from hydrous, enzymatic,
or chemical extraction of other material entities (e.g.,
yeast extract, proteose peptone, casamino acids, crude
oil extract, and casein hydrolysate).
Culture media recipes
Microbiological culture media recipes (~910 classes)
were included, under the parent class OBI:culture
medium (Fig. 1). Annotations include the recipe, the
citation or web link to the recipe, and synonyms of
the class. Logical axioms included the chemical ingre-
dients used for each medium (connecting MicrO
terms to ChEBI terms). Value Partitions were created
to categorize different types of culture media. For ex-
ample, one Value Partition is related to the pH of the
medium; whether it was strongly acidic (pH <4), mod-
erately acidic (pH 45.5), slightly acidic (pH 5.56.5), near
neutral pH (pH 6.57.5), slightly alkaline (pH 7.58.5),
moderately alkaline (pH 8.510.0), or strongly alkaline
(pH >10.0). Another Value Partition related to the salinity
of the medium using salinity values that are commonly
used in biology; whether it was freshwater (<0.05 % salts),
brackish (0.053.0 %), marine (3.05.0 %), or hypersaline
(> 5.0 %). A third Value Partition related to the redox (the
oxidation-reduction potential) of the medium; whether it
was oxidizing (oxygen or air were present and not con-
taining reducing agents), mildly reducing (containing
organosulfides or thiosulfate), or strongly reducing (con-
taining cysteine, glutathione, 2-mercaptoethanol, dithio-
threitol, sodium sulfide, hydrogen sulfide, dithionite, or
titanium citrate). Covering axioms were put in place for
each of the Value Partitions. The logical axioms that were
created were designed to facilitate future studies that rely
on the logical inference power of the ontology to gain
higher-order knowledge of microbial taxa based on the
chemical composition of their growth media, such as
studies seeking to identify correlations between phylogeny
and culture medium chemistry [45]. Finally, the logical ax-
ioms put in place can help fill out the knowledge gap of
MicroPIE. For instance, taxonomic descriptions will often
state the type of media in which an organism is capable of
growing. The logical inference power made possible by
the ontology allows MicroPIE to immediately compute
the chemical conditions under which that particular or-
ganism is capable of growing (even if given only the names
of the culture medium).
Assays
A large number of classes (~570) describe microbio-
logical diagnostic assays, under the parent class OBI:as-
say. Assays include cell staining assays, commercial
suites of diagnostic assays (e.g., API microbial ID test
kits, Biolog, RapID, and VITEK), salinity, pH and redox
assays), a large number of organic carbon metabolism
assays (including organic acid alkalinization assays, or-
ganic carbon assimilation assays, organic carbon fermen-
tation assays, and organic carbon fermentation/oxidation
assays), milk reactivity assays, motility assays, hemad-
sorption/hemagglutination/hemolysis assays, coagulase
assays, growth response assays (including growth re-
sponse to various antibiotics, inorganic chemicals, and
organic chemicals), and finally a large number of specific
enzymatic assays (e.g. beta-galactosidase assay, catalase
assay, lecithinase assay, pyruvate decarboxylase assay).
Assays, with axioms connecting substrates, products,
and enzymatic activities were important to have in the
ontology, because most prokaryotic taxonomic descrip-
tions describe the outcomes of particular assays per-
formed on the particular isolate being described and
logical axioms for this set of classes tended to be more
complex. The assays are logically connected to chemical
entities (e.g. is an assay for the metabolic product some
Blank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 5 of 10
hydrogen sulfide and is an assay using the culture
medium some sulfide indole motility agar) and pro-
cesses (e.g., is an assay for the biological process of 
some cell motility and is an assay for the enzymatic
activity of  some tryptophanase activity; Fig. 2 and
Additional file 1: Figure S5). Logical axioms also in-
clude the enzymatic substrates (some of which are
colorimetric compounds, such as 5-bromo-4-chloro-
3-indolyl beta-D-galactoside) and products, and the
culture medium used to perform the test (e.g., is an
assay using the culture medium some sulfide indole
motility agar).
Sometimes, taxonomic descriptions will report lists of
enzymatic reactions that were tested and provided a
positive or negative test result (e.g., positive for valine
arylamidase), while other times they will report lists of
Fig. 1 Screen Capture Showing Microbiological Culture Medium Recipes and Logical Axioms Employed. Logical axioms for these classes included
the chemical ingredients used to make up the medium in addition to several Value Partitions that described the pH, salinity, and redox of the
culture medium (for example: has salinity some brackish salinity)
Blank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 6 of 10
the substrates hydrolyzed or not hydrolyzed (e.g., L-
valine-2-naphthylamide hydrolyzed). The structure of
the ontology connects these two concepts and recog-
nizes that they both relate to the same enzymatic trait
(in this case, valine arylamidase activity, assayed using
the L-valine arylamidase assay). This is accomplished
by including the assay substrates (in this case L-
valine-2-naphthylamide) as a substrate in the logical
axiom for the valine arylamidase assay class.
Prokaryotic qualities
Several classes (97) were created to describe prokary-
otic qualities. These include prokaryotic cell part
qualities (such as gas vacuole quality, thylakoid qual-
ity, Gram stain quality, and prokaryotic cell wall lysis
susceptibility), prokaryotic cell qualities (such as cell
granulation, cell pigmentation, cell size quality, and
flagellar quality), and prokaryotic colony quality.
Classes also included prokaryotic metabolic qualities
(aerobic, microaerophilic, aerotolerant, obligately aerobic,
photofermentative, chemolithoautotrophic, photoorgano-
heterotrophic, etc.) and prokaryotic physiological qualities
(including barophilic, obligately barophilic, barotolerant,
and requires magnesium for growth).
Prokaryotic cell and cellular components
Many new classes (255) were placed under the parent
prokaryotic cell including flagellated cell (with subclasses
including multiply flagellated, amphilophotrichous cell,
amphitrichous cell, lophotrichous cell, and peritrichous
cell), gas vacuolated cell, granulated cell, nanocytes, and
pigmented cell. Classes under morphologically distinct
prokaryotic cell include bacilloid cell, cuboidal cell, pear-
shaped cell, and prosthecate cell. Classes under prokary-
otic differentiated cell include hormogonium, central
endospore, lateral endospore, subterminal endospore,
basal heterocyte, and terminal heterocyte. Classes under
prokaryotic metabolically differentiated cell include auto-
troph, obligate aerobe, and chemoorganoheterotroph.
Classes under prokaryotic physiologically differentiated
cell include acidophile, obligate barophile, thermophile,
and facultative halophile. Classes under differentiated
cyanobacterial filament part include conical apical cell,
tapered by apical narrowing, isopolar metameric, multi-
seriate filament, and subterminal meristematic zones.
Classes (49) were created to describe prokaryotic col-
onies. The structural organization of classes relating to
colonies with distinct morphologies, sizes, and shapes,
mirrored the class organization of morphology, size,
and shape in PATO (Additional file 1: Figure S6). This
helped to facilitate the construction of logical axioms be-
tween classes in MicrO and PATO. For example, under
the parent class prokaryotic colony were placed the
classes morphologically distinct colony, physically
distinct colony, and colony having distinct process
quality. Morphologically distinct colony is logically
defined as prokaryotic colony and has morphology
some PATO:morphology.
Fig. 2 Pattern for Assay Classes. Schematic showing part of the logical design pattern for microbiological diagnostic assays (e.g. the class sulfide
indole mobility assay). Multiple logical axioms connect various assay classes to other classes (such as microbiological culture medium) in other
parts of the ontology using object properties (shown with curved, bolded lines)
Blank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 7 of 10
MicrO classes of cell parts (~128 classes) include
pseudopeptidoglycan-based cell wall, teichoic acid-
based cell wall, sheath, and proteinaceous sheath.
Additional prokaryotic cell parts include cyanobacter-
ial filament part, filament branch, trichome, hetero-
polar trichome, tapered trichome, isopolar trichome,
trichome part, apical cell, basal heterocyte, medial
cell, necritic cell, etc. Under cyanobacterial filament,
classes include multi-trichomous filament, multiseri-
ate filament, biseriate filament, and uniseriate filament.
Our plan is to submit term requests for relevant classes of
cell parts that should belong in GO.
Prokaryotic biological processes
Finally, 41 classes were created that defined prokaryotic
biological processes (lithotrophy, mixotrophy, anaerobic
respiration using various electron acceptors and donors).
These classes are embedded into GO classes, and may
be expanded upon and incorporated into GO in the fu-
ture. Logical axioms connect these biological processes
with chemical entities (e.g. uses electron acceptor some
nitrate, uses carbon source some organic molecular en-
tity), other processes (e.g., has part some phototrophy
and has part some heterotrophy), and biological en-
tities (e.g., is prokaryotic metabolic process occurring in
some mixotroph).
Object and datatype properties
In order to connect classes in MicrO to those in external
ontologies, we imported object properties from IAO,
OBI, RO, and Uberon. We also created ~77 new object
and datatype properties to relate microbial-specific clas-
ses to one another (Additional file 1: Table S2). Many of
the new Object Properties are nested within OBI or RO
parent classes. New object properties were assigned defi-
nitions and (when possible) domains and ranges.
Application and future directions
Microbial diversity is vast. Our ontology did not focus
on pathogenic phenotypes (such as hosts, target organs,
and diseases). These are areas that will need further
ontology integration with other existing ontologies (for
example, with OMP, the Disease Ontology, Infectious
Disease Ontology, the Pathogenic Disease Ontology, and
the Human Disease Ontology) [4648]. MicrO also did
not focus on microbial habitats. Development of ENVO
is ongoing and the incorporation of microbial habitats
into ENVO is a potential fruitful new approach for inte-
grating MicrO with ENVO. Also, there are a number of
new prokaryote-focused ontologies in development fo-
cusing on microbial metagenomic metadata and micro-
bial habitats/environments (such as MEOWL; Microbial
Environments described using OWL; https://github.
com/hurwitzlab/meowl). These can be incorporated into
MicrO and formal logical axiom linkages added to fur-
ther increase axiomization of microbial terms. Finally,
our ontology did not cover traits associated with micro-
bial eukaryotes.
In the near future, we plan to incorporate MicrO into
our developing NLP program (MicroPIE), and in doing
so will greatly increase the computing power of Micro-
PIE. Currently, MicroPIE relies on term lists, which treat
each term as an individual entity. MicroPIE cannot de-
termine that the terms rod, bacillus, bacilli, elongated
cocci, and short cylinders are all synonyms for the same
concept (a bacillus shape). MicrO, with its controlled vo-
cabulary, logical axioms, and annotations including syn-
onyms, can inform NLP programs like MicroPIE that
these are indeed the same class, and hence streamline
the functionality of the algorithm. The ontology will help
MicroPIE recognize that terms such as mixotroph and
mixotrophic all point to the same concept (the ability
to carry out process of mixotrophy). The ontology will
also reduce confusion in facilitating the identification of
synonymous concepts when it comes to the varied
reporting of the results of prokaryotic diagnostic assays
(as discussed above).
Because of the logical inference power provided by the
ontology, MicrO will allow algorithms like MicroPIE to
infer new information about a microbial taxon that is
not explicitly stated in the taxonomic description. For
example, if an organism metabolizes glucose and is
photosynthetic, MicrO-enabled MicroPIE can infer that
it is a photoorganotroph. If an organism grows at 89 °C,
MicrO-enabled MicroPIE can infer that it is a hyperther-
mophile (given that the logical definition for a hyper-
thermophile in MicrO constrains an organisms optimal
growth temperature to being above 85 °C). If an organ-
ism has akinetes, MicrO-enabled MicroPIE will be able
to infer that it is in the Nostocales or Stigonematales
(two Orders in the Cyanobacteria). These inferred char-
acter states can help to populate cells of a matrix that
can be quite sparse when NLP is used to extract literal
characters from text.
Additionally, MicrO will be able to support a future
generation of bioinformatics capabilities for the micro-
biological community. For example, because MicrO con-
nects phenotypic information and diagnostic assays with
the enzymatic activities in GO, it could be used to sup-
port future work aimed at connecting microbial pheno-
types with genotypes (i.e., the gene content in genomes).
Exciting new tools and approaches for connecting phe-
notypes with genotypes are being developed for meta-
zoans [4951]. These tools could be adapted and
expanded to similarly function with microbial taxa and
microbial genomes in the future, given that the field of
microbiology now has a rich ontology. In this manner,
MicrO could be a useful tool for other researchers in the
Blank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 8 of 10
field of metagenomics and evolution of microbial pheno-
typic traits.
Conclusions
MicrO is an ontology of prokaryotic phenotypes and
metabolic characters, which also includes classes for
microbiological media recipes and diagnostic assays. The
ontology uses a controlled vocabulary, detailed annota-
tions, and an extensive set of logical axioms to connect
prokaryotic classes (including qualities, processes, assays,
and entities) to terms from 19 outside ontologies. By
connecting microbial concepts with chemical entities,
material entities, biological processes, molecular func-
tions, and qualities from existing ontologies in the OBO
Foundry using logical axioms, we intend MicrO to be a
powerful new tool which will help push forward progress
on the natural language processing of prokaryotic
taxonomic descriptions, and make possible new con-
nections between microbial phenotypes and genotypes
(i.e. gene content in genomes). Future ontology devel-
opment will include incorporation of pathogenic phe-
notypes (such as hosts, target organs, and diseases)
and prokaryotic habitats.
Additional file
Additional file 1: Supplemental Figures (Figures S1-S6) and Tables
(Tables S1 and S2). (DOCX 1785 kb)
Abbreviations
BFO: basic formal ontology; BSPO: biological spatial ontology;
ChEBI: chemical entities of biological interest; CHMO: chemical methods
ontology; CL: cell ontology; DRON: drug ontology; NDF-RT: national drug file
reference terminology; ENVO: environment ontology; GO: gene ontology;
IAO: information artifact ontology; IDO: infectious disease ontology;
OBI: ontology for biomedical investigations; OMP: ontology of microbial
phenotypes; MicroPIE: microbial phenomics information extractor; NLP: natural
language processing; OBO: open biomedical ontologies; OWL: web ontology
language; PATO: phenotype quality ontology; PO: plant ontology; PR: protein
ontology; REO: reagent ontology; RO: relations ontology; Uberon: uber anatomy
ontology.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
CEB constructed the term lists and the ontology. LRM assisted with the
collection of the corpus of taxonomic descriptions and term/synonym lists.
HC and RW provided significant input on the composition of terms and the
construction of logical axioms. RW oversaw formatting of ontology import
files and establishing purl addresses. All authors contributed to the
manuscript. All authors read and approved the manuscript.
Acknowledgements
This work was funded by grants from the National Science Foundation
(award DEB-1208534 to CEB, DEB-1208567 to HC, and DEB-1208685 to LRM)
and by a travel grant (to CEB) to attend the 2013 NESCent Ontologies
for Evolutionary Biology workshop. RW was supported by CyVerse and
the National Science Foundation under award numbers DBI-0735191 and
DBI-1265383. Many thanks to Elvis Hsin-Hui Wu (University of Arizona),
Gail Gasparich (Towson University), and Gordon Burleigh (University of
Florida) for comments and/or assistance with ontology construction and
compilation of taxonomic descriptions. We would also like to thank Chris
Mungall (LBNL), Oliver He (University of Michigan) for technical assistance with
OntoBee and OntoFox, and Gareth Owen (ChEBI project leader, head curator)
and other curators at ChEBI for assistance in the incorporation of microbial-
specific chemical terms and synonyms into ChEBI. Thanks also to the instructors
(Melissa Haendel, Matt Yoder, Jim Balhoff) and students of the 2013 NESCent
Ontologies for Evolutionary Biology workshop, and to Karen Cranston (NESCent)
and the support staff at NESCent. Thanks also to the OBI-devel team for
comments regarding the overall structure of assay terms, and associated
object properties, in MicrO.
Author details
1Department of Geosciences, University of Montana, Missoula, MT 59812,
USA. 2School of Information, University of Arizona, Tucson, AZ 85719, USA.
3Department of Biological Sciences, University of Southern Maine, Portland,
ME 04104, USA. 4CyVerse, University of Arizona, Tucson, AZ 85721, USA.
Received: 3 December 2015 Accepted: 2 April 2016
EDITORIAL Open Access
Vaccine and Drug Ontology Studies
(VDOS 2014)
Cui Tao1*, Yongqun He2 and Sivaram Arabandi3
Abstract
The Vaccine and Drug Ontology Studies (VDOS) international workshop series focuses on vaccine- and drug-related
ontology modeling and applications. Drugs and vaccines have been critical to prevent and treat human and animal
diseases. Work in both (drugs and vaccines) areas is closely related - from preclinical research and development to
manufacturing, clinical trials, government approval and regulation, and post-licensure usage surveillance and
monitoring. Over the last decade, tremendous efforts have been made in the biomedical ontology community to
ontologically represent various areas associated with vaccines and drugs  extending existing clinical terminology
systems such as SNOMED, RxNorm, NDF-RT, and MedDRA, developing new models such as the Vaccine Ontology (VO)
and Ontology of Adverse Events (OAE), vernacular medical terminologies such as the Consumer Health Vocabulary
(CHV). The VDOS workshop series provides a platform for discussing innovative solutions as well as the challenges in
the development and applications of biomedical ontologies for representing and analyzing drugs and vaccines, their
administration, host immune responses, adverse events, and other related topics. The five full-length papers included in
this 2014 thematic issue focus on two main themes: (i) General vaccine/drug-related ontology development and
exploration, and (ii) Interaction and network-related ontology studies.
Introduction and background
Drugs and vaccines have been critical to prevent and
treat human and animal diseases. Work in both (drugs
and vaccines) areas is closely related - from preclinical
research and development to manufacturing, clinical trials,
government approval and regulation, and post-licensure
usage surveillance and monitoring. Ontologies can serve
important roles in managing, normalizing, sharing, and
leveraging drug and vaccine relevant data. The 2014
Vaccine and Drug Ontology Studies workshop (VDOS
2014) was an international forum for researchers to
identify, propose, and discuss solutions for important
research problems in ontology representation and analysis
of vaccine and drug development, administration, func-
tion mechanisms, safety, and education.
VDOS 2014 was held on October 7, 2014, in Houston,
Texas. This workshop was part of the Fifth International
Conference on Biomedical Ontology (ICBO 2014). The
workshop attracted interests from many international
attendees, including paper presenters, academic and
government scientists, postdoctoral fellows, and graduate
students. After a rigorous peer review process (all submis-
sions have been reviewed by at least three independent
reviewers), five full-length papers were accepted for pro-
ceeding paper publications and oral presentations in the
workshop. After additional reviewing by the independent
reviewers, workshop co-organizers, and the journal
editors, the selected five full-length papers were extended
and accepted for publication in the thematic track of the
Journal of Biomedical Semantics (JBMS).
The VDOS-2014 workshop is the 3rd in this series.
The first workshop of the series was organized as the
Vaccine and Drug Ontology in the Study of Mechanism
and Effect workshop (VDOSME 2012) [1] on July 21,
2012, in Graz, Austria, as part of the third International
Conference on Biomedical Ontology (ICBO 2012). In 2013,
the name has been changed to Vaccine and Drug Ontology
Studies (VDOS) to reflect the expansion in the scope to
more than just mechanism and effect. The workshop
series also covers vaccine and drug-related clinical data
representation and analysis, including clinically reported
vaccine and drug adverse events. VDOS 2013 was held
on July 7, 2013, in Montreal, Qc, Canada [2].
* Correspondence: cui.tao@uth.tmc.edu
1University of Texas, Health Science Center at Houston, of Biomedical
Informatics, Houston, TX, USA
Full list of author information is available at the end of the article
JOURNAL OF
BIOMEDICAL SEMANTICS
© 2016 Tao et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Tao et al. Journal of Biomedical Semantics  (2016) 7:6 
DOI 10.1186/s13326-015-0039-8
Summary of selected papers in the thematic issue
The five papers selected for this thematic issue are ex-
tended versions of the original full-length papers presented
at the VDOS 2014. This year the workshop focuses on two
main themes: (i) General vaccine/drug-related ontology
development and exploration, and (ii) Interaction and
network-related ontology studies.
In the area of general vaccine/drug-related ontology de-
velopment and exploration, Winnenburg and Bodenreider
introduced their approach on identifying common class-
level signals for every individual drug in a class [3]. They
created a dataset of Adverse Drug Events (ADE) based on
information extracted from PubMed abstracts and aggre-
gated drugs into Anatomical Therapeutic Chemical (ATC)
classes and ADEs into Medical Subject Headings (MeSH)
terms. They then applied a visual approach to uncover
known associations and a computational approach to
systematically analyze the associations between drug classes
and ADEs. Amith and Tao introduced their Vaccine
Information Statement Ontology (VISO), which is a
comprehensive vaccine information ontology that can
support personal health information applications using
patient-consumer lexicon, and lead to outcomes that
can improve patient education [4]. The current VISO con-
tains the knowledge included in the Vaccine Information
Statement (VIS) documents collected from the USA
Centers for Disease Control and Prevention (CDC)
website. The overarching goal of the VISO is to positively
influence the development of intelligent ontology-driven
applications and mitigate the knowledge gap that often
exists in patients seeking accurate and reliable information
but encountering complex or inaccurate sources.
In the area of interaction and network-related
ontology studies, Peters et al. investigated the drug 
drug interaction (DDI) information in two publically
available drug resources, NDF-RT and DrugBank [5].
Their study indicated that the overlaps of DDIs be-
tween the two resources are limited. Further studies
need to be conducted to either determine how to best
leverage the DDI information included in these two
resources in clinical and translational applications.
Hur et al. introduced their work on the development
of the interaction Network Ontology (INO) with an
aim to classify over 800 interaction keywords for literature
mining various interactions between genes and proteins
[6]. Based on the INO-classified interaction type hierarchy
and the Vaccine Ontology (VO)-based vaccine hierarchy, a
modified Fishers exact test on PubMed literature mined
results was established to successfully analyze signifi-
cantly over- and under-represented enriched gene-gene
interaction types within the vaccine domain. This study
demonstrated that hierarchical ontological definitions
facilitate novel approaches for literature mining and
analysis of gene interaction networks. Zhang et al.
introduce a novel approach that combines ontologies
and network analysis technologies for studying sex-
associated patterns in vaccine adverse events [7]. The
authors leverage data downloaded from the Vaccine
Adverse Event Report System (VAERS) and con-
structed a condition-specific association network. The
outcome of this research can potentially provide guidance
on sex-specific dose recommendations in personalized
vaccinology.
Workshop presentations and discussions
In the 2014 VDOS workshop, the five full-length papers
described above were orally presented. In addition, the
program also included a keynote speech and a podium
abstract presentation. The keynote speech was given by
Dr. Khalid F. Almoosa, Associate Professor of Medicine at
the Department of Internal Medicine, UTHealth Medical
School, Houston, TX. Dr. Almoosa also serves as the
Vice-Chair for Healthcare Quality and the Director of
Interprofessional Collaboration at UTHealth. In his
keynote, Dr. Almoosa discussed important issues in
healthcare quality and safety and the roles of vaccine and
drug safety in healthcare quality management. For the
podium abstract presentation, Dr. Yongqun He provided
updates on the development of the Ontology of Adverse
Events (OAE) and its applications. An in-depth discussion
of the OAE is available in the recently published article in
Journal of Biomedical Semantics [8].
The discussion session of the workshop focused on
the topic of Drug and vaccine ontology informatics, we
first discussed the similarities and differences between
the adverse event ontologies in the Open Biological and
Biomedical Ontologies (OBO) domain (e.g., OAE and
AERO) and non-OBO style ontologies (e.g., MedDRA
and SNOMED). The necessity and possible approaches
on how to bridge OBO and non-OBO ontologies were
explored. Furthermore, the participants of the
workshop discussed ontology-based clinical and ex-
perimental data processing and analysis, including
ontology-supported literature mining of biomedical and
electronic health records. We all agreed to continue the
workshop in 2015 which would be held in conjunction
with the ICBO-2015 conference at Lisbon, Portugal, in the
end of July 2015.
Overall, the VDOS 2014 workshop was well received
with positive feedbacks. The VDOS workshop series
have served and will continue to serve a platform for
biomedical and clinical researchers to discuss ontology-
relevant issues and progress in drug and vaccine
research and applications.
Competing interests
The authors declare that they have no competing interests.
Tao et al. Journal of Biomedical Semantics  (2016) 7:6 Page 2 of 3
Acknowledgements
As editors of this thematic issue, we thank all the authors who submitted
papers, the Program Committee members and the reviewers for their
excellent work. We appreciate the support and help from the ICBO 2014
meeting organizers. We are grateful for editorial reviews from Dr. Dietrich
Rebholz-Schuhmann from JBMS.
Author details
1University of Texas, Health Science Center at Houston, of Biomedical
Informatics, Houston, TX, USA. 2Unit for Laboratory Animal Medicine,
Department of Microbiology and Immunology, and Center for
Computational Medicine and Bioinformatics, University of Michigan Medical
School, Ann Arbor, MI, USA. 3Ontopro LLC, Houston, TX, USA.
Received: 17 September 2015 Accepted: 12 November 2015
RESEARCH Open Access
Linking rare and common disease:
mapping clinical disease-phenotypes to
ontologies in therapeutic target validation
Sirarat Sarntivijai1,2* , Drashtti Vasant1,2, Simon Jupp1, Gary Saunders1,2, A. Patrícia Bento1,2, Daniel Gonzalez1,2,
Joanna Betts2,3, Samiul Hasan2,3, Gautier Koscielny2,3, Ian Dunham1,2, Helen Parkinson1 and James Malone1,2
Abstract
Background: The Centre for Therapeutic Target Validation (CTTV - https://www.targetvalidation.org/) was
established to generate therapeutic target evidence from genome-scale experiments and analyses. CTTV aims to
support the validity of therapeutic targets by integrating existing and newly-generated data. Data integration has
been achieved in some resources by mapping metadata such as disease and phenotypes to the Experimental
Factor Ontology (EFO). Additionally, the relationship between ontology descriptions of rare and common diseases and
their phenotypes can offer insights into shared biological mechanisms and potential drug targets. Ontologies are not
ideal for representing the sometimes associated type relationship required. This work addresses two challenges;
annotation of diverse big data, and representation of complex, sometimes associated relationships between concepts.
Methods: Semantic mapping uses a combination of custom scripting, our annotation tool Zooma, and expert
curation. Disease-phenotype associations were generated using literature mining on Europe PubMed Central abstracts,
which were manually verified by experts for validity. Representation of the disease-phenotype association was achieved
by the Ontology of Biomedical AssociatioN (OBAN), a generic association representation model. OBAN represents
associations between a subject and object i.e., disease and its associated phenotypes and the source of evidence for
that association. The indirect disease-to-disease associations are exposed through shared phenotypes. This was applied
to the use case of linking rare to common diseases at the CTTV.
Results: EFO yields an average of over 80 % of mapping coverage in all data sources. A 42 % precision is obtained
from the manual verification of the text-mined disease-phenotype associations. This results in 1452 and 2810
disease-phenotype pairs for IBD and autoimmune disease and contributes towards 11,338 rare diseases associations
(merged with existing published work [Am J Hum Genet 97:111-24, 2015]). An OBAN result file is downloadable
at http://sourceforge.net/p/efo/code/HEAD/tree/trunk/src/efoassociations/. Twenty common diseases are linked to
85 rare diseases by shared phenotypes. A generalizable OBAN model for association representation is presented in
this study.
Conclusions: Here we present solutions to large-scale annotation-ontology mapping in the CTTV knowledge
base, a process for disease-phenotype mining, and propose a generic association model, OBAN, as a means
to integrate disease using shared phenotypes.
Availability: EFO is released monthly and available for download at http://www.ebi.ac.uk/efo/.
Keywords: Rare disease, Phenotype disease associations, OBAN, CTTV, EFO
* Correspondence: siiraa@ebi.ac.uk
1European Bioinformatics Institute (EMBL-EBI), European Molecular Biology
Laboratory, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD,
UK
2Centre for Therapeutic Target Validation, Wellcome Trust Genome Campus,
Hinxton, Cambridge CB10 1SD, UK
Full list of author information is available at the end of the article
© 2016 Sarntivijai et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Sarntivijai et al. Journal of Biomedical Semantics  (2016) 7:8 
DOI 10.1186/s13326-016-0051-7
Introduction
Drug discovery research involves diverse analytical activ-
ities and integration of many sources of data about di-
verse entities from single nucleotide polymorphisms
(SNPs) to pathways, proteins to populations. The Centre
for Therapeutic Target Validation (CTTV) is a collabor-
ation between the European Bioinformatics Institute
(EMBL-EBI), GlaxoSmithKline (GSK) and the Wellcome
Trust Sanger Institute (WTSI) to develop a knowledge
base of evidence for drug targets based on genomic ex-
periments and bioinformatics analyses. A CTTV goal is
to develop a better understanding of the rare and com-
mon disease relationship via shared phenotypes, genes,
and pathways, as information from rare disease can pro-
vide mechanistic insight to common disease and vice
versa. This requires integration of data generated by
CTTV projects with existing data residing in EMBL-EBI,
WTSI and GSK resources. Data types include variants,
genes, proteins, gene expression, pathways, compounds,
literature and related experimental variables such as dis-
ease and phenotype with data generation on different ex-
perimental platforms such as Genome Wide Association
Studies and next generation sequencing.
The integration of disease and phenotypic information,
where a group of phenotypes are associated with a dis-
ease, becomes increasingly important when considering
rare diseases where research is typically fragmented
across omics types and disease. Rare disease data are not
always compatible with each other as they come from
different resources, e.g., OMIM [1] and ORPHANET
[2], represent different perspectives of the diseases, such
as diagnostics or treatment, and data are typically popu-
lation, or even individual, specific. The sparseness and
heterogeneity of this data therefore introduces a major
challenge in the integration of rare and common disease
information [3].
CTTV uses the Experimental Factor Ontology (EFO)
[4] as its application ontology to provide an integrated
and consistent ontological representation of the CTTV
platform data. EFO provides an integration framework
for ontologies and reuses components of domain-specific
ontologies such as Orphanet Rare Disease Ontology
(ORDO) [5], ChEBI [6], Gene Ontology [7] and Uberon
[8]. Typically a data or use case driven SLIM (a subset of
the referenced ontology with MIREOT import closures
[9]) of a source ontology is created, and then imported
into EFO. Figure 1 illustrates the exponential growth of
EFO where a large amount of classes are imported from
externally-sourced ontologies. This presents challenges
representing the imported knowledge in EFO without los-
ing the structural integrity of the original ontologies. We
therefore use MIREOT to import classes, or small sections
of hierarchies from external ontologies to avoid potentially
importing the whole or most of a source ontology into
EFO due to the complexity of class organization. This also
helps ensure amenability of EFO to wider data integration.
For example, rare disease terms are imported from ORDO
and phenotypes from Human Phenotype Ontology terms
as both ontologies are compatible with EFOs disease and
phenotype design pattern respectively and common dis-
ease terms are defined locally with EFO-namespace URI.
Even though other ontologies exist that aim to describe
disease, there is not one single-origin representation of
common disease in any of the available ontologies that is
compatible with the current design pattern of disease rep-
resentation used in EFO, thus creating common disease
classes in the EFO namespace is currently necessary for
CTTV. Figure 1 shows that despite considerable growth
in EFO-native classes (3992 EFO-native classes in 2015, as
opposed to 2214 classes in 2010), EFO use of imported
classes from external domain ontologies is increasing.
EFO uses common design patterns that are consistent
throughout the EFO ontology development process (e.g.,
term creation, and term importing) to integrate and
organize the ontologies imported. For example, the design
pattern for cell line representation: cell line derives_-
from a cell type, which is part_of an organism, which is
a bearer_of some disease links an EFOs cell line class
to the Cell Ontologys cell type class, an NCBI Tax-
onomy class, and EFOs or ORDOs disease class. This
cell line design pattern as shown in Fig. 2 is also shared
with the Cell Line Ontology [10]. Webulous [11] (ex-
tended publication in JBMS Bioontologies SIG The-
matic issue), a tool which implements these design
patterns in a Google Sheets add-on, is used to create
new terms (the class), and to allow users to define new
terms for EFO in spreadsheet format. These are trans-
formed to OWL and imported prior to each monthly
release. The use of design patterns also provides
consistency with other ontology consuming resources
such as the EBI RDF Platform [12]. In order to be inter-
operable with OBO foundry ontologies EFO uses BFO
1.1 [13] upper level classes. For example EFO repre-
sents disease as a child of BFO:Disposition [14]
whereas, following the same process, HP:phenotype is
modelled as a child of BFO:Quality. In EFO, a common
design pattern is such that an EFO:disease has_pheno-
type HP: links EFO disease terms and HP. EFO diseases
are organized utilizing an object property has_disease_-
location using anatomical classes imported from
UBERON.
Data resources integrated into CTTV have local stan-
dards for annotation and many aggregate data from mul-
tiple external sources, where each external resource also
has a resource specific annotation and/or curation
process. They have also historically used different ontol-
ogies and dictionaries for disease and phenotype annota-
tion; examples include Online Mendelian Inheritance in
Sarntivijai et al. Journal of Biomedical Semantics  (2016) 7:8 Page 2 of 11
Man (OMIM) [15], the Systematized Nomenclature of
Medicine  Clinical Terms (SNOMED-CT) [16], the
Human Disease Ontology (DO) [17], and the Medical
Dictionary for Regulatory Activities (MedDRA) [18] as
seen in Table 1. We note that these resources often do
not differentiate between disease and phenotype when
selecting and applying the vocabularies to their data. We
have standardized this for CTTV, differentiating pheno-
type from disease, and defaulting to HP imported terms
in EFO for the description of phenotypes where possible.
For example, the GWAS Catalog trait myopia is anno-
tated to the HPs IRI http://purl.obolibrary.org/obo/
HP_0000545 Myopia. EFO therefore contains pheno-
typic terms that are clearly distinguished from disease
terms for annotation of CTTV data.
Diseases are associated with phenotypes which mani-
fest in the disease with qualifying information about the
nature of the association. The disease-phenotype associ-
ation is established to represent disease connections via
shared phenotypes. For example, the rare disease
Aicardi-Gourtieres syndrome has several associated phe-
notypes affecting the brain, immune system, and skin,
such as microcephaly, hepatosplenomegaly, elevated
blood liver enzymes, thrombocytopenia, and abnormal
neurological response. It is often not observable at birth,
and all phenotypes are unlikely to be present in all pa-
tient presentations. Additionally phenotypes may also
vary by kindred and/or by population in their frequency
and penetrance. The same is true for common disease,
for example, phenotypes of Crohns disease may range
from inflammation of any part of the gut (but most
likely ileum or colon), diarrhea, or constipation, but not
all symptoms are necessarily present in one patient.
Fig. 1 There were 2214 EFO-native classes in January 2010, and 3992 EFO-native classes in January 2015. Although EFO has significantly grown in
its number of native classes, the number of imported classes has grown at a much higher rate. Importing more than 6000 rare disease classes
from ORDO in 2012, and axiomatizing them into EFO has resulted in a sudden increase between 2012 and 2013. This reflects the use of EFO as
an application ontology providing interoperability across domain ontologies through semantic axiomatization
Fig. 2 The cell line design pattern in EFO links an EFO class cell line
to external ontologies via import mechanism. An EFO cell line
derives_from a cell type class from Cell Ontology, which is part_of an
organism  a class imported from NCBI Taxon. EFO cell line class is
also a bearer_of a disease  a class imported from ORDO or class
native to EFO itself
Table 1 An overview of ontologies usage by each CTTV data
source. Cross-reference sources of each CTTV data resource are
normalized to EFO for CTTV data validation process
Database Cross-reference annotation sources
EVA OMIM, SNOMED-CT, MeSH
ArrayExpress GO, OMIM, EFO
UniProt OMIM, Orphanet, MeSH
Reactome OMIM, GO
ChEMBL MedDRA, ATC, GO
GWAS Catalog EFO, DO
Sarntivijai et al. Journal of Biomedical Semantics  (2016) 7:8 Page 3 of 11
Representation of the disease-phenotype association in
an OWL ontology with the statement disease has_phe-
notype some phenotype requires that all instances of a
disease have that specific phenotype and our examples
above illustrate that this representation is problematic
for many cases. We have therefore chosen to represent
disease-phenotype association in a generic association
model OBAN (the Open Biomedical AssociatioN),
which allows us to represent both the disease-phenotype
association and qualify the association with evidence,
and, in the future, to represent information such as fre-
quency of association. In order to test this model, and to
populate it with disease-phenotype associations for
Inflammatory Bowel Disease we used a text mining ap-
proach to extract these from the literature, building a
corpus using an expert nominated set of journals as our
experience described in Vasant et al. [19], indicates that
constraining the corpus improves precision on post-hoc
validation by experts. Abstracts were accessed using the
EuropePMC API [20] and the Whatizit text mining
pipeline [21] was usd to mine the corpus using a dic-
tionary comprised of phenotype terms from the Human
Phenotype Ontology [22] and the Mammalian Phenotype
ontology [23].
Methods
Mapping CTTV data sources disease and phenotype terms
to EFO
In order to perform semantic integration of multiple re-
sources for CTTV, the data from each source (listed in
Table 1) was mapped to EFO identifiers. Challenges in
performing such mapping pertain in the non-
standardized use of vocabulary sets by different re-
sources. Some of the resources used an ontology, e.g.,
Disease Ontology, a taxonomy such as MeSH [24], or
cross-referenced another resource such as OMIM. Dis-
eases and phenotypes are often mixed in the same re-
source and sometimes in the same category annotation.
For example, the European Variation Archive (EVA 
http://www.ebi.ac.uk/eva/) [25] trait names labeling uses
a mixed set of vocabularies from HP, SNOMED-CT,
OMIM, and non-standardized local identifiers used in-
ternally at source from the ClinVar records. The identi-
RESEARCH Open Access
VICO: Ontology-based representation and
integrative analysis of Vaccination Informed
Consent forms
Yu Lin1,2,3,4, Jie Zheng5 and Yongqun He1,2,3,4*
Abstract
Background: Although signing a vaccination (or immunization) informed consent form is not a federal requirement in
the US and Canada, such a practice is required by many states and pharmacies. The content and structures of these
informed consent forms vary, which makes it hard to compare and analyze without standardization. To facilitate
vaccination informed consent data standardization and integration, it is important to examine various vaccination
informed consent forms, patient answers, and consent results. In this study, we report a Vaccination Informed
Consent Ontology (VICO) that extends the Informed Consent Ontology and integrates related OBO foundry ontologies,
such as the Vaccine Ontology, with a focus on vaccination screening questionnaire in the vaccination informed
consent domain.
Results: Current VICO contains 993 terms, including 248 VICO specific terms and 709 terms imported from 17 OBO
Foundry ontologies. VICO ontologically represents and integrates 12 vaccination informed consent forms from the
Walgreens, Costco pharmacies, Rite AID, University of Maryland College Park, and the government of Manitoba,
Canada. VICO extends Informed Consent Ontology (ICO) with vaccination screening questionnaires and questions.
Our use cases and examples demonstrate five usages of VICO. First, VICO provides standard, robust and consistent
representation and organization of the knowledge in different vaccination informed consent forms, questionnaires, and
questions. Second, VICO integrates prior knowledge, e.g., the knowledge of vaccine contraindications imported from
the Vaccine Ontology (VO). Third, VICO helps manage the complexity of the domain knowledge using logically defined
ontological hierarchies and axioms. VICO glues multiple schemas that represent complex vaccination informed consent
contents defined in different organizations. Fourth, VICO supports efficient query and comparison, e.g., through the
Description Language (DL)-Query and SPARQL. Fifth, VICO helps discover new knowledge. For instance, by integrating
the prior knowledge imported from the VO with a users answer to informed consent questions (e.g., allergic reaction
question) for a specific vaccination, we can infer whether or not the patient can be vaccinated with the vaccine.
Conclusions: The Vaccination Informed Consent Ontology (VICO) represents entities related to vaccination informed
consents with a special focus on vaccination informed consent forms, and questionnaires and questions in the forms.
Our use cases and examples demonstrated how VICO could support a platform for vaccination informed consent data
standardization, data integration, and data queries.
* Correspondence: yongqunh@med.umich.edu
1Unit for Laboratory Animal Medicine, University of Michigan Medical School,
Ann Arbor, MI 48109, USA
2Department of Microbiology and Immunology, University of Michigan
Medical School, Ann Arbor, MI 48109, USA
Full list of author information is available at the end of the article
© 2016 Lin et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Lin et al. Journal of Biomedical Semantics  (2016) 7:20 
DOI 10.1186/s13326-016-0062-4
Background
Different countries and organizations have various regu-
lations in terms of the requirement of informed consent
to vaccination before vaccinating a patient. Signing a
vaccination (or immunization) informed consent form is
not a federal requirement in the US and Canada. As an
exception, the 1976 swine flu immunization program
established by the US Federal legislation, included re-
quirements that recipients of the swine flu vaccine be
fully informed of the risks and benefits of immunization
and that written consent forms be used (42 U.S.C.A.§247b
(j)(1)(F) (Supp.1977) [1]. The USA National Childhood
Vaccine Injury Act of 1986 (NCVIA - 42 U.S.C. § 300aa-
26) requires that the Vaccine Information Statements
(VISs) (http://http://www.immunize.org/vis/) must be
provided by all public and private vaccination providers to
the patient (or parent or guardian) prior to every dose of
specific vaccines. Although there is no US federal require-
ment, the documentation of consent is recommended or
required by some states, local health authorities, school
authorities, or pharmacies. The immunization protocols
from many pharmacies require the signatures of recipients
or legal representatives on specific vaccination informed
consent forms before vaccination in accordance with local
states legislation (https://www.pharmacist.com/guidelines-
pharmacy-based-immunization-advocacy). The Manitoba
Province in Canada provides vaccination informed consent
guidelines for routine immunization in accordance with
The Public Health Act (C.C.S.M. c. P210) in Canada [2].
Additionally, new laws in Texas were passed in 2013
to allow pregnant minors and minor parents to con-
sent to their own vaccination [3]. Under this circum-
stance, one can conclude that there must be a variety
of vaccination informed consent forms if provided
from different sources, such as pharmacies, schools
and hospitals. An important question is whether the
informed consent for vaccination provides enough
adequate information for the patients to understand
their risks and benefits prior to their vaccinations. To
address this question, an overview and comparison of
current informed consent forms are critical.
Due to different policies and vaccination informed
consent forms offered by authorities and pharmacies, it
is difficult to compare, evaluate, and manage vaccination
informed consent procedures and data from different
resources. Ontologies, sets of terms and relations that
represent entities in a domain and how they relate to
each other, support computer-assisted data integration,
knowledge management, and new knowledge discovery.
Developing a vaccination informed consent ontology
that can advance integrating data from different re-
sources may facilitate the research in informed consent
and vaccine policy. Therefore, more efficient patient
safety management can be achieved.
Based on the principle of ontology reuse, two established
ontologies can be re-utilized to ontologically represent
vaccination informed consents. One is the community-
based Informed Consent Ontology (ICO) [4]. ICO repre-
sents the documentations and processes involved in in-
formed consent. ICO aims to support informed consent
data integration and reasoning in the clinical research
space. Vaccine Ontology (VO) [5, 6] represents licensed
vaccines, vaccines in clinical trials, and experimentally
verified vaccines in research laboratories. In addition,
vaccine-related information including vaccine compo-
nents, vaccine licenses, vaccine manufacture, vaccin-
ation, and vaccination doses are also represented in
VO. Both Informed Consent Ontology and Vaccine
Ontology are aligned with the upper level Basic Formal
Ontology (BFO) [7] and compliant with the Open Bio-
logical and Biomedical Ontologies (OBO) Foundry ontol-
ogy development principles [8]. Built on the same upper
ontology, Informed Consent Ontology and Vaccine
Ontology can be seamlessly integrated for represent-
ing vaccination informed consent.
In this paper, we report the development of the
Vaccination Informed Consent Ontology (VICO) by
extending Informed Consent Ontology and integrating
other OBO Foundry ontologies, such as Vaccine Ontology.
The goal of VICO is to represent the vaccination informed
consent document, organize vaccination informed consent-
related entities, and to establish relations between those
entities so that the knowledge of vaccination in-
formed consent can be captured explicitly. The
current development focus is to consistently represent
different vaccination/immunization informed consent
forms, especially the immunization screening ques-
tionnaires inside these forms. We then apply VICO to
conduct systematic vaccination/immunization informed
consent form comparison and patient centered informed
consent data query and analysis. We hypothesized that
VICO would significantly enhance standard data represen-
tation, integration and query of informed consent related
to various vaccine immunization procedures. To test the
hypothesis, we lay out five major usages of VICO related
to the hypothesis, and provide use cases to justify these
usages.
Methods
Collection of vaccination informed consent forms
Twelve vaccination informed consent forms were
collected from the following resources: Costco Pharmacy
website [9] (accessed on 01/26/2016), Walgreens Phar-
macy website [10] (accessed on 0126/2016), Rite AID
website [11] (accessed on 01/26/2016), Manitoba govern-
ment Public Health division [12] (accessed on 01/26/
2016), and the University of Maryland College Park [13]
(accessed on 01/26/2016).
Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 2 of 14
VICO ontology development
VICO was formatted in the Web Ontology Language
(OWL2) [14]. The backbone of VICO composed of a
portion of Informed Consent Ontology (ICO), a portion
of Vaccine Ontology (VO), and VICO specific terms.
The workflow of developing VICO includes following
four processes:
1) Extract subset of ICO. After manually identifying
the ICO terms related with vaccination informed
consent, the OntoFox tool [15] was used to extract
portion of ICO.
2) Extract subset of VO. This process includes two
steps of extracting portion of VO using two different
tools, Ontobee SPARQL endpoint and OntoFox tool.
First, all licensed vaccines in USA and Canada were
retrieved using the Ontobee SPARQL endpoint
(http://www.ontobee.org/sparql) [16] based on
the logical axiom defined in VO: bearer_of some
USA licensed vaccine role. Next, OntoFox tool
was applied to extract a portion of VO that
contains all the related terms, logical axioms
(classes and relations), definitions, and
annotations of each licensed vaccine retrieved
from the first step.
3) Importing ICO and VO subsets to VICO. The
ICO and VO subsets form the backbone of VICO.
The importing process is done by using an
owl:import statement in the VICO owl file. This
backbone of VICO with imported ICO and VO
subsets can be displayed using the Protégé OWL
Editor tool.
4) Enrich VICO with VICO-specific terms. We first
identify a vocabulary from abovementioned
questions, build hierarchy from this vocabulary,
and then relate these terms using existing defined
relations or VICO specific relations. We establish
the logical axioms, so that the questions can be
normalized with clear semantics. For example, a
question of have you had received any vaccinations
in the past 4 weeks is related with the vaccination
term from VO, and is asserted as a subclass of
questions on past vaccination information VICO
term, which has a mother term of question textual
entity from IAO. Categorizing questions from
different forms are mainly based on ontologists
manual assessment in accordance with hierarchies
of question related entities established in existing
OBO Foundry ontologies. This way, different
forms and questions appeared as different text
strings can be related to common existing
ontology entities, thus enable automated
analyzation of vaccine informed consent form
programmatically.
The Protégé OWL Editor version 5.0 beta [17] was
used to develop VICO. The HermiT reasoner (http://
hermit-reasoner.com/) tool was employed to perform the
reasoning over VICO to detect inconsistencies or
conflicts.
The VICO Github project (https://github.com/VICO-
ontology/VICO) was created to facilitate the project
version control and tracking issues.
VICO evaluation by use cases
Use cases and examples were laid out to evaluate and
demonstrate the applications of VICO in supporting dif-
ferent usages. For application evaluation, SPARQL and/
or DL languages were often applied. The SPARQL queries
were performed using the Protégé SPARQL program or
Ontobees SPARQL query endpoint (http://www.ontobee.
org/sparql) [16]. The DL queries were performed using DL
Query plugin of Protégé 5.0 (beta 15) to answer questions
from use case 2. Query scripts generated for this project
were stored in the Github under the folder:
https://github.com/VICO-ontology/VICO/tree/master/
src/SPARQL%20query.
Ontology source access and license
The VICO is an open source project. The source code
including development version and released version are
freely available at the URL: https://github.com/vico-
ontology/VICO. VICO is released under a Creative Com-
mons 3.0 License.
Results
VICO ontology design and top level structure
VICO is a community-driven ontology that crosses both
informed consent domain and vaccine domain. In VICO,
we extended Informed Consent Ontology (ICO) and
Vaccine Ontology (VO) by adding VICO specific terms
representing vaccination informed consent forms (Fig. 1).
For example, specific VICO terms were generated to
represent the vaccination/immunization informed con-
sent forms from different vaccination providers or gov-
ernments, such as Costco vaccination informed consent
form (Fig. 1). Another example is vaccination screening
questionnaire, defined as A questionnaire that contains
different questions of a vaccination patients health
history, allergy history, and current condition, in order
to assess the contraindication and precaution for admin-
istering a vaccine in VICO. This term was generated for
representing questionnaire embedded in a vaccination
informed consent form. The questions for vaccination
patients can be answered by the patients themselves, or
their legal representatives, prior to a vaccination proced-
ure. VICO defines these questions in a structured logical
manner (Fig. 1).
Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 3 of 14
The basic VICO ontology design pattern is composed
of various entities linked by defined relations as illus-
trated in Fig. 2. Specifically, before a vaccination process,
a vaccination informed consent form is documented by
an organization/company (e.g., the company Costco). A
vaccination screening questionnaire containing a list of
questions is provided for a vaccination patient or his/her
legal representative to answer. The documented_by is a
relation defined by VICO as an object property that rep-
resents a relation between a document and an entity that
writes, maintains and releases the document. A specific
vaccination consent form is often restricted to be used
in a specific geographic location (e.g., Manitoba in
Canada or South Carolina in USA). The contents of the
questions in the form often cover different topics such
as the vaccination patients current health status, current
treatment, allergic reaction history, past vaccination
history, and so on. The vaccination patients age and bio-
logical sex (female or male) are modelled using PATO
term has_quality linking the age and biological sex to
patient. The vaccination process occurs via a specified
vaccination route (e.g., intranasal influenza vaccination
for FluMist) (Fig. 2). A question, for example, question
on serious nasal condition (asked for FluMist
vaccination only) is about vaccination procedure some
FluMist vaccination, which is a live attenuated influenza
vaccine. A vaccination related question may also be
related to a disease (e.g., cancer) or an adverse event
process, which has been expressed in VICO using the
relation is about (Fig. 2). Details about ontological
modeling of vaccine/vaccination can be found in the
previous VO paper [5, 6]. Informed consent representation
Fig. 1 Top level terms and hierarchical structure of VICO. VICO imports many top level terms from VO and ICO and includes VICO-specific terms
as exampled with two VICO terms (bold)
Fig. 2 Basic VICO design pattern
Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 4 of 14
is introduced in previous ICO paper [4]. By integrating
VO/ICO representations and including VICO-specific
contents, VICO provides a framework to link vaccination
patient, vaccinee quality, vaccine, vaccine quality,
vaccination, vaccination targets, informed consent
process, informed consent forms, questions, question-
naires, and related information.
As of January 30, 2016, VICO (version 1.0.51) contains
993 terms, including 707 classes, and 92 object proper-
ties. VICO includes 243 VICO-specific classes and prop-
erties with the VICO_ prefix, which are new ontology
terms not covered in ICO, VO, or any other OBO Foun-
dry ontologies. As shown in the Ontobee VICO statistics
page (http://www.ontobee.org/ontostat/VICO), in addition
to ICO and VO, VICO also reused terms from other OBO
Foundry ontologies such as Information Artifact Ontology
(IAO) [18] and Ontology for Biomedical Investigations
(OBI) [19], which mainly indirectly imported due to im-
ports of ICO and VO. VICO is deposited in the Ontobee
RDF triple store [16], and can be visualized and quer-
ied on the Ontobee website: http://www.ontobee.org/
ontology/VICO.
VICO Usages
In general, ontologies can be used in different aspects of
knowledge management, including: (i) provide robust and
consistent knowledge representation and organization, (ii)
integrate prior knowledge, (ii) manage the complexity of
domain knowledge, (iv) support efficient query and
comparison, and (v) help discover new knowledge. Below
we will elaborate how VICO is applied in these above
categories of usages.
Usage 1: Provide robust and consistent knowledge
representation and organization
VICO provides a robust and consistent representation of
the knowledge of vaccination informed consent in the
three major branches: vaccination informed consent
form, vaccination screening questionnaire, and ques-
tions. The basic relations between these three branches
are that a vaccination informed consent form 'has part' a
vaccination screening questionnaire that 'has compo-
nent' many questions. Each of the branches is organized
with a hierarchical structure through the is a relation.
For example, Costco vaccination informed consent form
is a vaccination informed consent form. The following
example axioms illustrate the relations of terms in the
three branches:
1. Costco vaccination informed consent form:
'has part' some 'questionnaire for Costco
vaccination consent'
2. questionnaire for Costco vaccination consent: 'has
component' some 'question whether allergic to egg'
All the questions in VICO are organized in a hierarch-
ical structure under ICO term: question textual entity
(ICO_0000141). VICO modelled 158 questions collected
from 12 vaccination informed consent forms. After
standardization with newly generated mother terms and
hierarchical terms, there are 168 VICO question terms
arranged under the IAO term question textual entity.
For robust representation, VICO sometimes breaks
down a question with mixed information to more than
one specific question. For example, the Costco form lists
12 questions (Fig. 3a). Figure 3b demonstrates how VICO
represents these questions, questionnaires and their
relations. The question Do you have allergies to medica-
tions, food or vaccines? is represented in VICO as three
VICO question terms: question whether allergy to food,
question whether allergy to medication, and question
whether allergy to vaccine.
VICO consistently represents linguistic variants of
questions with standardized VICO terms. For example,
the are you sick today and are you well today is anno-
tated as term question whether currently sick. Textual
definitions, definition sources, and comments are also
provided to ensure clarity and consistency.
The answer to an informed consent question can be
Yes, No, or unknown. VICO represents the answer
to a question using answer option text entity (ICO_0000
171), a subclass of textual entity (IAO_0000300). We
have two ICO terms to represent the Yes or No
answer: yes answer text entity (ICO_0000172) and no
answer text entity (ICO_0000173). VICO created its own
dont know answer text entity (VICO_0000006), since in
some vaccination informed consent form, dont know
was another choice of answer in addition to Yes or No.
The standard, robust, and coherent representation and
organization of the knowledge in the domain of vaccin-
ation informed consent are the foundation of the other
usages as described below.
Usage 2: Integrate prior knowledge
VICO provides a way to seamlessly integrate prior
knowledge as represented in other ontologies such as
the Vaccine Ontology (VO). As a vaccination informed
consent ontology, VICO does not focus on the attributes
of any licensed vaccines. Fortunately, such information
is available in the VO. VO provides informed consent
related information for licensed vaccines, including
vaccine ingredients, vaccine manufactures, vaccine admin-
istration routes, contraindications, etc. Such information
is important for interpreting vaccination patients answers
of informed consent questions. Instead of generating such
information from scratch, VICO imported the related
information directly from VO.
As a demonstration of importing prior knowledge from
VO, Fig. 4 shows the VOs representation of a vaccine,
Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 5 of 14
Afluria. Afluria is a human influenza viral vaccine licensed
for use in the USA. It is an inactivated vaccine against
Influenza virus A. It is manufactured by the CSL Limited,
and distributed by Merck & Co, Inc. Various characteris-
tics of Afluria were represented using logical axioms in
VO. For example, the axiom: has vaccine allergen some
chicken egg protein allergen encodes that the Afluria
vaccine contains a trace of chicken egg protein, which is
able to induce allergic reaction in certain population.
Therefore, the hypersensitivity to chicken egg becomes a
contraindication to this vaccine. Another vaccine allergen
associated with this vaccine is neomycin. This vaccine is
administered via an intramuscular route. The vaccine
information has been imported to VICO from VO as
detailed in the Methods section. Such importing mechan-
ism provides a valid strategy for VICO to integrate prior
knowledge as recorded in VO.
In addition, VICO also imports prior knowledge
related to informed consent questions by adding disease
information and organizing the questions based on
diseases or symptoms. To support the interoperability,
the VICO questionnaire questions have been mapped to
the Logical Observation Identifiers Names and Codes
(LOINC; https://loinc.org/) [20, 21] and semantically
linked to the Disease Ontology (DOID) [22], Symptom
Ontology (SYMP), Ontology of Adverse Event (OAE),
and some newly generated VICO terms for medical
procedure or health condition. For example, in VICO,
question whether currently sick with a moderate to high
fever, vomiting/diarrhea is linked to SYMP:fever and
DOID:diarrhea through the IAO relation is_about.
Another example is allergy question, which is related
to DOID term hypersensitivity reaction type I disease
using is_about relation. Likewise, question whether
allergy to food is about food allergy (DOID). This
way, the VICO bridges questions with existing terms and
prior knowledge from these other ontologies.
Usage 3: Manage the complexity of domain knowledge
The complexity of the vaccination informed consent lies
in the complex relations between vaccination patients,
patient attributes (e.g., age and gender), vaccine, vaccine
attributes, and different levels of question complexity.
Such complexity is managed in VICO by generating and
Fig. 3 Questionnaire in Costco vaccination informed consent form and its representation in VICO. a All the questions shown in the in Costco informed
consent form. b VICO representation of the questions in the Costco form questionnaire
Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 6 of 14
following a concise and valid design pattern (Fig. 2) that
semantically link all the related classes together. The
linkages among different classes are established with
solid object properties (i.e., relations).
The complexity of the vaccination informed consent is
also reflected by the fact that various vaccination infor-
med consent forms exist for different purposes and are
used in different locations. For example, in Rite Aids,
different informed consent forms are distributed in dif-
ferent states: South Carolina, North Carolina, California,
and other states. Informed consent forms for kids are
often different with those for adults, or college students.
Although many informed consent forms are generally
applied for different vaccinations, in our corpus of vaccin-
ation informed consent forms, four different informed
consent forms are all about Flu vaccination: University of
Maryland College Park injectable influenza vaccination
informed consent, University of Maryland College Park
Flumist vaccination informed consent, Rite Aid injectable
flu vaccination informed consent in South Carolina, and
Manitoba Seasonal Influenza and Pneumococcal vaccin-
ation informed consent form.
The management of these complex forms and ques-
tions in VICO relies on the consistent and robust repre-
sentation and organization (see Usage 1), and the use of
logical axioms to clearly lay out the differences. For
example, the relation between a form and its source is
clearly defined using the relation documented_by as
exemplified here: Costco vaccination informed consent
form: documented by value Costco. Another example
axiom to link a form with a purpose is here: UM College
Park flumist vaccination informed consent: is about
vaccination procedure some Flumist vaccination. The
Flumist vaccination is then linked to the influenza vac-
cine Flumist with another axiom. Compared to different
schemas representing various vaccination informed con-
sent contents from different organizations, VICO is
unique in that it glues multiple schemas together using
well-organized ontological representations.
Usage 4. Support efficient query and comparison
Based on the consistent and well-organized VICO repre-
sentation and organization of complex knowledge, we
can easily browse in an ontology editor or web ontology
browser, or perform SPARQL queries against the VICO
to more specifically compare the questions in different
forms. SPARQL is an Resource Description Framework
(RDF) query language able to retrieve ontology data
stored in the RDF format [23]. These ontology browsing
and queries can be used to compare questions from dif-
ferent forms.
For example, SPARQL queries were used to compare
Costco and Walgreen vaccination informed consent
forms. Figure 5 shows how a SPARQL query can be used
to identify the common (Fig. 5a) and different (Fig. 5b)
questions raised in Costco and Walgreen vaccination
informed consent forms. In this SPARQL, we used an
object property documented by that represents the
relation between an informed consent form and an
organization (e.g., pharmacy) (Fig. 5b). Our comparative
analysis using SPARQL queries found 39 unique ques-
tions listed in Walgreens and Costco vaccination in-
formed consent forms (Table 1). Among these questions,
12 questions are shared by both forms, six questions are
listed by Costco form only, and 21 questions are unique
to the Walgreen form. Compared to the Costco form,
four more questions are listed in the Walgreens form.
Note that Walgreens unique question question on
whether fainted or felt dizzy after immunization (ques-
tion #19) is the subclass of Costco unique question
question on reaction after immunization (question
#18) in VICO. Similarly, question whether currently
sick (question #13) used by Costco is the parent term
of question whether currently sick with a moderate to
high fever, vomiting/diarrhea (question #21) used by
Walgreens. These observations reveal that although both
Costco and Walgreens ask the adverse events after
vaccination, Walgreens asks more specific and nar-
rower questions.
Fig. 4 The ontology hierarchy and associated axioms of the Afluria
influenza vaccine. The contents are defined in VO and imported to
VICO. The figure is a screenshot from Ontobee [16]. The highlighted
egg allergen axiom is used for a use case demo detailed later in
this paper
Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 7 of 14
The questions asked are typically consistent with pa-
tient requirements before vaccinations. For example, the
question on current aspirin therapy (question #25 in
Table 1) is a question specifically for the vaccine FluMist
Quadrivalent, asked by Walgreens. To investigate more
on why Walgreens asks these vaccine-specific questions,
we examined the package insert document for FluMist
Quadrivalent. The contraindication section of the
FluMist Quadrivalent package insert document says:
Concomitant aspirin therapy in children and adoles-
cents [24]. This contraindication statement indicates
that a patient with concomitant aspirin therapy cannot
be vaccinated with the FluMist Quadrivalent vaccine.
This information provides the solid reason why the
Walgreens form asks whether a patient is taking aspirin.
Usage 5: Help discover new knowledge
The consistent representation, robust organization, and
prior knowledge integration of informed consent forms
make VICO a useful platform for new knowledge discov-
ery based on patients answers of a questionnaire.
Based on a patients answers to informed consent
questions, we will demonstrate with a use case on how
VICO and OWL-based technologies can be used to dis-
cover whether a patient can or cannot be vaccinated with
a vaccine containing a trace of egg protein. This use case
is related to vaccine contraindication (e.g., allergic reaction
to chicken egg), a rare condition in a recipient that in-
creases the risk for a serious adverse reaction. Ignoring
contraindications can lead to dangerous vaccine adverse
reactions. As shown in Fig. 3, the Afluria influenza vaccine
has the contraindication of hypersensitivity to chicken
egg since it has an egg allergen. Therefore, vaccination of
egg-allergic patients with a vaccine (e.g., Afluria) that has
an egg allergen is currently not recommended [25].
Basically, this use case contains two-steps: Step 1: Find
if a vaccine contains egg allergen, and Step 2: Find if a
patient is allergic to egg. If the answers to both questions
are positive, this patient cannot be administered with the
vaccine found in step 1. For the first step, we developed
a SPARQL query to identify vaccines that have restric-
tion on egg allergic reaction (Fig. 6). As a result, eight
licensed vaccines were identified as vaccines containing
egg protein allergen. This step is essentially to query the
prior knowledge that is imported from VO to VICO.
To implement the second step in our sandbox demon-
stration, first we ontologically represented a patients
answers to the questions in a filled informed consent
form. In this sandbox use case study, we hypothetically
created twelve patients answers to the questions in the
Costco vaccination informed consent form, the Walgreens
vaccination informed consent form, or the Manitoba in-
formed consent form respectively. All the answers were in-
stantiated as VICOs instance data (owl:NamedIndividual).
Fig. 5 SPARQL query of shared and different questions in Costco and Walgreens vaccination informed consent forms. a Query common questions.
b Query different questions. The two screenshots show the query executions and results generated using the Protégé OWL editor
Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 8 of 14
As we mentioned before, in VICO, a patients answers to a
question is represented as a component of a filled question-
naire instance (Fig. 7). For example, in a Costco vaccination
informed consent form, a patients No answer to the ques-
tion whether the patient is allergic to egg is represented as:
filled questionnaire for Costco vaccination consent
and (has component some (question whether
allergic to egg and (has component some no answer
text entity)))
Table 1 Comparison of specific questions listed in Walgreens and Costco vaccination informed consents
Number Question Walgreens/Costco
1 question on vaccination in past 4 weeks Walgreens; Costco
2 question on blood transfusion in past year Walgreens; Costco
3 question whether allergic to vaccine Walgreens; Costco
4 question on asthma or wheezing history Walgreens; Costco
5 question on leukemia Walgreens; Costco
6 question whether allergic to medication Walgreens; Costco
7 seizure disorder question Walgreens; Costco
8 question on cancer Walgreens; Costco
9 question whether allergic to egg Walgreens; Costco
10 X-ray treatment question Walgreens; Costco
11 question whether allergic to latex Walgreens; Costco
12 question on woman pregnancy Walgreens; Costco
13 question whether currently sick Costco
14 question on long-term heart disease Costco
15 cortisone treatment question Costco
16 immunocompromisation question Costco
17 question whether allergic to food Costco
18 question on reaction after immunization Costco
19 question on whether fainted or felt dizzy after immunization Walgreens
20 question on TB skin test in past 4 weeks Walgreens
21 question whether currently sick with a moderate to high fever, vomiting/diarrhea Walgreens
22 question on serious nasal condition Walgreens
23 question on high-dose steroid therapy for longer than 2 weeks Walgreens
24 question on thymus disease Walgreens
25 question on current aspirin therapy Walgreens
26 question on current antibiotics usage Walgreens
27 question on history of thrombocytopenia or thrombocytopenic purpura Walgreens
28 question on CSF leak Walgreens
29 question on asplenia Walgreens
30 question on azathioprine or 6-mercaptopurine usage Walgreens
31 question on cochlear implant Walgreens
32 question on high-dose methotrexate usage Walgreens
33 question on home infusion Walgreens
34 question on weekly injection Walgreens
35 question on weekly injection of adalmumab Walgreens
36 question on weekly injection of etanercept Walgreens
37 question on weekly injection of infliximab Walgreens
38 question whether recieving aspirin-containing therapy Walgreens
39 question on current anti-malarial medication Walgreens
Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 9 of 14
Fig. 6 SPARQL query of vaccines that have egg protein allergen. This query was performed using both the Protégé 5.0.0 SPARQL Query plug-in
and the Ontobee SPARQL query website (http://www.ontobee.org/sparql/). The SPARQL query codes are available in the Github repository. This
figure is a screenshot of the query execution and results using the Protégé SPARQL program
Fig. 7 DL query for patients who are allergic to egg. a Example of the answers of a Costco patients answers to questionnaire. b A DL query and
its results. The DL query was performed using the Protégé OWL editor
Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 10 of 14
After the patients answers to the informed consent
questions were transformed in VICOs representation,
we then performed queries to find out the patient who
may be in the danger of trigger a serious egg allergic
reaction if given a specific vaccine, e.g., Afluria. In Fig. 7,
an OWL Description Logics (DL) query written in the
Manchester OWL syntax, a user-friendly syntax for
OWL DL [26], was shown and executed using DL Query
Tab in the Protégé OWL editor [27]. Out of these twelve
patients answers, we identified four patients (one Costco
patient, one Walgreens patient, and two Manitoba
patients) who answered that they were allergic to egg pro-
teins. Therefore, these patients may not be recommended
for vaccination with those vaccines containing egg
allergen (Fig. 7).
This simplified use case demonstrated how VICO can
utilize, integrate prior knowledge from VO (Usage 2),
and discover new knowledge (Usage 5) that the patient
cannot be vaccinated with a vaccine that contains the
chicken egg allergen (e.g., Aflura). In addition, this use
case queries patients answers (Usage 4) from three
different forms in a consistent way, demonstrating the
advantages of consistent representation and organization
of complex information in the vaccination informed
consent forms (see Usages 1 and 2).
Discussion
This paper introduces the development and application
of a Vaccination Informed Consent Ontology (VICO).
VICO represents 12 vaccination informed consent forms
and corresponding screening questionnaires from
different organizations, and over 150 questions in
these forms. The top level hierarchical structure and
general VICO design pattern are represented, followed
by the description of five usages of VICO with exam-
ples and use cases.
VICO was developed by following the best practice
and recommended strategy of ontology reusing [15].
Instead of coding everything from scratch, we
imported related terms from both Vaccine Ontology
(VO) and Informed Consent Ontology (ICO) into
VICO. The VO includes the information of licensed
human vaccines in different countries and related
vaccine characteristics. ICO is an ontology represent-
ing informed consent that may be applied in broader
areas (e.g., clinical trials, clinical research) than vac-
cine immunization. By importing related terms from
these two BFO-based ontologies, VICO demonstrates
a seamless integration of existing ontologies. On top
of the imported terms, VICO can then focus on the
representation of more specific entities such as
vaccination informed consent forms from different
pharmacies and authorities. As demonstrated in the
Results section, VICO has also provided a solution to
map vaccination consent questions to the commonly used
LOINC standard and generate logical axioms to directly
link questions to the diseases shown in the Disease
Ontology (DOID). Such a development strategy has been
proven successful in our VICO development.
The VICO ontology can not only be used to
support Semantics Web applications, but they are also
applicable to support relational database systems. One
primary reason is that relational database schemas
also need consistent, structured, and computer-
understandable representation of the informed consent
forms, questions, and the relations among questions,
vaccines, and patients. VICO standardizes these terms
with logical and textual definitions and consistently
represents the relations among these terms. VICO
also efficiently includes prior knowledge. VICO
provides an integrative framework to well represent
organize the complexity of different levels of informa-
tion and knowledge. These usages are also needed for
a typical relational database system to occupy. It
would be very difficult to hard code all the
information, relations, and prior knowledge in soft-
ware code or relational database. By incorporating
VICO to a relational database system, such a system
will obtain all the ontology benefits.
VICO is also able to support the integration of
different relational databases that most likely use
different database schemas. The 12 vaccination
informed consent forms collected in our study can be
matched to multiple relational database schemas from
different organizations. Assuming we want to com-
pare and integratively query the data from these orga-
nizations, it is almost impossible to do with relational
databases since each organization very likely does not
know the database schemas from other organizations.
However, with the support of VICO, such a task can
be done efficiently as shown in our paper. If each
database schema understands the VICO contents,
VICO can indeed serve as a hub system that makes
different relational database schemas understand each
other. This is another reason why VICO can support
relational database system.
The Semantic Web formats (e.g., RDF and OWL)
provide an inherent capability of reasoning. One
example of such reasoning can be found in the refer-
ence [28]. Our ontology provides a foundation for
reasoning. New rules and equivalent classes can be
added to the ontology to support reasoning. The OWL
ontology is established based on the RDF technology.
Individual RDF triple stores may exist independently and
lack an appropriate way to communicate with each other.
An ontology provides an effective way to address such a
silo issue and semantically link different RDF triple store
data to support better data integration and queries.
Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 11 of 14
Linguistic polishing has been used in our process of
mapping questions and assigning ontology IDs. How-
ever, linguistic polishing cannot achieve many features
that we would like to gain by representation in ontology.
For example, linguistic polishing does not provide: (i) a
hierarchy of questions, (ii) relations of questions to
diseases, and (iii) classification of the questions based on
diseases and symptoms. Different questions may be asso-
ciated with the same diseases and concerns. Different
questionnaires may have different questions that are
related to different vaccines and vendors. These onto-
logical strategy is flexible and extensible to address these
questions. In addition, ontology can easily reuse existing
knowledge represented in other ontologies (e.g., VO).
Most of these use cases cannot be achieved by simple
question mapping and intersection of two sets of
questions, and their analyses require logical axioms
specifically defined in the ontology.
Our VICO usage demonstration makes it feasible to
develop electronic interoperable vaccination informed
consent forms, e.g., by building up a list of questions for
generating the questionnaire in the form. Furthermore,
it is now possible for vaccine recipients or their legal
representatives to sign electronic informed consent
forms. With advantage of sharing Electronic Health Rec-
ord (EHR), patients allergic history, treatment history,
and severe adverse event history will be integrated into
pharmacys IT system, software programs can be devel-
oped to automate the screening procedure before recipi-
ent or his/her legal representative sign the informed
consent form. Clinical decision making system can help
to prevent avoidable contraindications prior to vaccin-
ation. Compared to an electronic information manage-
ment system without ontology support, the usage of
VICO for supporting an electronic vaccination informed
consent management system has many advantages. First,
acting as a separate middle layer from the functionalities
of the management system, the VICO ontology can be
easily updated without additional and costly software
engineering work. Secondly, the ontology layer makes it
possible to easily perform decision support. Lastly,
different informed consent data can be seamlessly inte-
grated together using the same data representation
ontology, supporting data integration.
To make VICO useful in practice, there are still several
issues to solve. Given the power of our VICO-based
SPARQL or a DL queries, in reality, it may be impossible
for a clinical professional to type in a SPARQL or a DL
query. To bypass the difficulty of using DL or SPARQL,
it is possible to develop template-based and natural
language-implemented query capability to easily query
ontologies [2931]. A user-friendly query web interface
can be provided to support convenient search for those
who do not know any programming language. One
closely related work is the usage of ontologies for adap-
tive questionnaires for clinical risk assessment [32, 33].
Adaptive questionnaires are able to dynamically modify
the behavior of the structure of the questionnaire in
response to user interaction. The context-sensitive
adaptation approach can use an ontology as the basic for
robust adapted information collection and patient risk
assessment [32]. It is possible to use VICO as the onto-
logical basis for adaptive questionnaire formation in an
interactive vaccination informed consent management
system. To make VICO usable in practice, it is important
to link VICO to existing standards and ontologies such
as LOINC and DOID, which has been incorporated in
our study. Furthermore, we will also need to convince
pharmacies and governments who perform or monitor
the vaccination procedures with more complete infor-
mation in the ontology and empirical examples.
VICO-based vaccination informed consent system
may be linked to computerized immunization informa-
tion systems (IIS, or called immunization registries) that
are developed to collect and consolidate vaccination data
from multi health-care providers, generate automatic
notifications, and assess vaccination coverage. Such IIS
have been widely established in the US [34]. For example,
KSWebIZ Kansas Immunization Registry is a web-based
statewide immunization registry that provides a central-
ized birth to death database of complete and accurate
immunization records for all Kansas residents (https://
www.contactkswebiz.info/). It would be ideal to eventually
link electronic informed consent data to IIS directly, and
VICO would facilitate such an effort.
In the future, VICO can also be expanded to cover the
information related to vaccine adverse events. Typically,
such information is included in the VIS, and the patients
need to be notified of the VIS before their consent. Instead
of the plain text described in the Vaccine Information
Statements (VIS) documents, an Ontology of Vaccine
Adverse Event (OVAE) was recently developed to repre-
sent various adverse events for each licensed vaccine [35].
OVAE will be imported into VICO, which allows recipients
better understanding of possible side effects of vaccinations
prior to vaccination, therefore, enhances the informed
consent process.
In addition, the original methods identified in this
study can be applied to represent informed consent
forms in other domains of research (e.g., biobanking
[36]). Interoperability strategies applied in this study can
be incorporated into existing electronic health records
or decision support systems as some stage in the future.
It is noted that current study is still at the prototype
stage to prove the rationale and feasibility of applying
ontology to solve the issue of data and query disinte-
gration in the area of vaccination inform consents from
different vendors and agents. For the current stage of
Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 12 of 14
development, manual efforts with software supports have
been used to design the ontology, process the data, and
implement the use cases. For real usage, automatic com-
putational systems such as natural language processing
text mining programs can further be used to improve the
efficiency of informed consent content processing.
Overall, such a strategy is promising to be used in real
clinical setting.
Conclusions
VICO ontologically represents various entities related to
vaccination informed consent including vaccination
informed consent forms, various questions in question-
naires, and answers to those questions. Current VICO
represents the contents of 12 vaccination informed con-
sent forms from pharmacies, a collage health center, and
the government of Manitoba, Canada. Our SPARQL and
OWL DL queries demonstrated that VICO could be
used as a standard platform for consistently and system-
atically representing vaccination informed consent form
questions, linking question answers to vaccine attributes,
identifying potential vaccine contraindications, and
enforcing safe vaccination procedures.
Abbreviations
AE: adverse event; BFO: Basic Formal Ontology; DL query: Description Logics
query; DOID: Disease Ontology; IAO: Information Artifact Ontology;
ICO: Informed Consent Ontology; LOINC: Logical Observation Identifiers
Names and Codes; NCBO: The National Center for Biomedical Ontology;
OAE: Ontology of Adverse Events; OBI: Ontology for Biomedical
Investigations; OBO: The Open Biological and Biomedical Ontologies;
OVAE: Ontology of Vaccine Adverse Events; OWL: Web Ontology Language;
RDF: Resource Description Framework; SPARQL: SPARQL Protocol and RDF
Query Language; VAE: Vaccine Adverse Event; VICO: Vaccination Informed
Consent Ontology; VIOLIN: Vaccine Investigation and Online Information
Network; VO: Vaccine Ontology.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
YL: Overall project design, VICO developer, SPARQL and DL-query design,
implementation, and data interpretation. JZ: VICO developer, SPARQL and
DL-query design, and data interpretation. YH: Overall project design, primary
VICO developer, design pattern generation, and data interpretation. All
contributed to manuscript preparation and revision. All authors read and
approved the final manuscript.
Acknowledgements
This research was supported by a discretionary bridge fund in the Unit for
Laboratory Animal Medicine in the University of Michigan Medical School.
Author details
1Unit for Laboratory Animal Medicine, University of Michigan Medical School,
Ann Arbor, MI 48109, USA. 2Department of Microbiology and Immunology,
University of Michigan Medical School, Ann Arbor, MI 48109, USA. 3Center for
Computational Medicine and Bioinformatics, University of Michigan Medical
School, Ann Arbor, MI 48109, USA. 4Comprehensive Cancer Center, University
of Michigan Medical School, 1301 MSRB III, 1150 W. Medical Dr., Ann Arbor,
MI 48109, USA. 5Department of Genetics, University of Pennsylvania
Perelman School of Medicine, Philadelphia, PA 19104, USA.
Received: 16 November 2015 Accepted: 5 April 2016
RESEARCH Open Access
Large scale biomedical texts classification: a
kNN and an ESA-based approaches
Khadim Dramé*, Fleur Mougin and Gayo Diallo
Abstract
Background: With the large and increasing volume of textual data, automated methods for identifying significant
topics to classify textual documents have received a growing interest. While many efforts have been made in this
direction, it still remains a real challenge. Moreover, the issue is even more complex as full texts are not always
freely available. Then, using only partial information to annotate these documents is promising but remains a
very ambitious issue.
Methods: We propose two classification methods: a k-nearest neighbours (kNN)-based approach and an explicit
semantic analysis (ESA)-based approach. Although the kNN-based approach is widely used in text classification, it
needs to be improved to perform well in this specific classification problem which deals with partial information.
Compared to existing kNN-based methods, our method uses classical Machine Learning (ML) algorithms for ranking
the labels. Additional features are also investigated in order to improve the classifiers performance. In addition,
the combination of several learning algorithms with various techniques for fixing the number of relevant topics is
performed. On the other hand, ESA seems promising for this classification task as it yielded interesting results in
related issues, such as semantic relatedness computation between texts and text classification. Unlike existing
works, which use ESA for enriching the bag-of-words approach with additional knowledge-based features, our
ESA-based method builds a standalone classifier. Furthermore, we investigate if the results of this method could
be useful as a complementary feature of our kNN-based approach.
Results: Experimental evaluations performed on large standard annotated datasets, provided by the BioASQ
organizers, show that the kNN-based method with the Random Forest learning algorithm achieves good
performances compared with the current state-of-the-art methods, reaching a competitive f-measure of 0.55 %
while the ESA-based approach surprisingly yielded unsatisfactory results.
Conclusions: We have proposed simple classification methods suitable to annotate textual documents using only
partial information. They are therefore adequate for large multi-label classification and particularly in the biomedical
domain. Thus, our work contributes to the extraction of relevant information from unstructured documents in order
to facilitate their automated processing. Consequently, it could be used for various purposes, including document
indexing, information retrieval, etc.
Keywords: Biomedical text classification, Semantic indexing, Multi-label classification, k-nearest neighbours,
Explicit semantic analysis, Information extraction, Machine learning
* Correspondence: khadim.drame@u-bordeaux.fr
University of Bordeaux, ERIAS, Centre INSERM U897, F-33000 Bordeaux,
France
© 2016 Dramé et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 
DOI 10.1186/s13326-016-0073-1
Introduction
The amount of textual data is rapidly growing with an
abundant production of digital documents, particularly
in the biomedical domain (biomedical scientific articles,
medical reports, patient discharge summaries, etc.). Fur-
thermore, these data are generally expressed in an un-
structured form (i.e., in natural language), which makes
its automated processing increasingly difficult. Thus, an
efficient access to useful information is challenging. To
do so, a suitable representation of textual documents is
crucial. Controlled and structured vocabularies, such as
the Medical Subject Heading (MeSH®) thesaurus, are
widely used to index biomedical texts [1] and conse-
quently to facilitate access to useful information [2, 3].
As regards conceptual indexing, concepts defined in the-
sauri or ontologies are often used to annotate documents.
For example, the MEDLINE® citations are manually
indexed by the National Library of Medicine® (NLM)
indexers using the MeSH descriptors. Although the task
of annotators is now facilitated by a semi-automatic
method [4], the rapid growth of biomedical literature
makes manual-based indexing approaches complex, time-
consuming and error-prone [5]. Thus, fully automated
indexing approaches seem to be essential. While many
efforts have been made to this end, indexing full biomed-
ical texts according to specific segments of these texts,
such as their title and abstract, remains a real challenge
[6]. Furthermore, with the large amounts of data, using
only partial information to annotate documents is promis-
ing (reduction of computational cost).
In this paper, we propose two classification methods for
discovering and selecting relevant topics of new (unanno-
tated) documents: a) a kNN-based approach and b) an
ESA-based approach. Our main contribution is to be able
to suggest relevant topics to any new document based
solely on portion of it thanks to a classification model
learnt from a large collection containing several hundreds
of thousands of previously annotated documents.
Text classification is the process of assigning labels
(categories) to unseen documents. The principle of the
kNN-based approach is to consider the set of topics
(MeSH descriptors, in this case) assigned manually to
the k most similar documents of the target document.
Then, these topics are ordered by their relevance score
so that the most relevant ones are used to classify the
document. In a previous work [5], authors noted that
over 85 % of MeSH descriptors relevant for classifying a
given document are contained in its 20 nearest neigh-
bours. This appears to better represent the documents
rather than what can be found in their title and abstract
solely.
First, we have developed a method based on the vector
space model (VSM) [7] to determine similar documents.
The latter uses the TF.IDF (term frequency  inverse
document frequency) weighting scheme for representing
documents by vectors constituted by unigrams they con-
tain, and the cosine measure for retrieving the document
neighbours. Then, we have investigated different types of
features and several ML algorithms for selecting relevant
topics in order to classify a given document.
On the other hand, ESA [8] has yielded good results in
related issues such as semantic relatedness computation
between texts [8] and even the text classification [9]. For
this reason, we propose to explore it using different
association measures in the context where only partial in-
formation is exploited for classifying a whole document.
Unlike most works in document classification, our
approaches use only partial information (titles and ab-
stracts) of documents in order to predict relevant topics
for representing their full content. Since the content of
documents is not fully exploited, using large datasets for
building the classifiers could be useful for capturing
more information. For this reason, we used classifiers
built from large collections of previously annotated doc-
uments. This is a very challenging task, which has moti-
vated the recent launch of BioASQ: an international
challenge on large-scale biomedical semantic indexing
and question answering1 [6].
The rest of the paper is organized as follows. First,
related work concerning biomedical document indexing
and, more generally, multi-label classification is reviewed
in Section 2. Then, the two proposed methods are detailed
in Section 3. In Section 4, the experiments are shown
while the results are described in Section 5 and discussed
in Section 6. Conclusion and future work are finally
presented in Section 7.
Related work
The identification of relevant topics from documents in
order to describe their content is a very important task
widely addressed in the literature. In the biomedical do-
main, the MTI (Medical Text Insdexer) tool [4] is one of
the first attempts to index biomedical documents (MED-
LINE citations) using controlled vocabularies. To map
biomedical text to concepts from the Unified Medical
Language System® (UMLS) Metathesaurus - a system
that includes and unifies more than 160 biomedical ter-
minologies - the MTI tool uses the well-known concept
mapper MetaMap [8] and combines its results with the
PubMed Related Citations algorithm [10]. The combin-
ation of these methods results in a list of UMLS con-
cepts which is then filtered and recommended to human
experts for indexing citations. Recently, the MTI was
extended with various filtering techniques and ML algo-
rithms in order to improve its performance [11]. Ruch
has designed a data independent hybrid system using
MeSH for automatically classifying biomedical texts [12].
The first module is based on regular expressions to map
Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 2 of 12
texts to concepts while the second is based on a VSM
[7] considering the vocabulary concepts as documents
and documents as queries. Then, the rankers of the two
components are merged to produce a final ranked list of
concepts with their corresponding relevance scores. His
results showed that this method achieved good per-
formances, comparable to ML-based approaches. One
limitation of this system is that it may return MeSH
concepts which match partially the text [1].
ML-based approaches are also proposed to deal with
such a task. The idea is to learn a model from a training
set constituted of already annotated documents and then
to use this model to classify new documents. Trieschnigg
et al. [1] have presented a comparative study of six sys-
tems which aim at classifying medical documents using
the MeSH thesaurus. In their experiments, they showed
that the kNN-based method outperforms the others,
including the MTI and the approach developed by Ruch
[12]. In their work, the kNN classifier uses a language
model [13] to retrieve documents which are similar to a
given document. The relevance of MeSH descriptors is
the sum of the retrieval scores of documents annotated by
these descriptors among the neighbouring documents. A
similar kNN-based approach has been proposed in [5]. A
language model is used to retrieve the neighbours of a
given document. Then, a learning-to-rank model [14] is
used to compute relevance scores and consequently to
rank candidate labels2 collected from these document
neighbours. In this work, the number of labels to classify a
document is set to 25. Experiments on two small standard
datasets (respectively 200 and 1000 documents) showed
that it achieves better performances than the MTI tool.
On the other hand, indexing biomedical documents in
which each document of the dataset is assigned one or
several categories (also called labels) can be assimilated
as a multi-label classification task. Multi-label classifica-
tion (MLC) is increasingly studied and especially for text
classification purposes [15]. Several methods have been
developed to deal with this task [16, 17]. They can be
categorized into two main approaches [15]: the problem
transformation approach [18] and the algorithm adapta-
tion approach [17, 19]. The problem transformation ap-
proach splits up a multi-label learning problem into a set
of single-label classification problems whereas the algo-
rithm adaptation approach adjusts learning algorithms to
perform MLC.
In MLC, the kNN-based approach is widely used. This
approach has been proven efficient for MLC in terms
of simplicity, time complexity, computation cost and
performance [17]. Zhang and Zhou [19] proposed a ML-
KNN (for Multi-Label kNN) method which extends the
traditional kNN algorithm and uses the maximum a pos-
teriori principle to determine relevant labels of an unseen
instance. For an instance t, the ML-KNN identifies its
neighbours and estimates respectively the probabilities
that t has and has not a label l based on the training set,
for each label l. Then, it combines these probabilities with
the number of neighbours of t having l as a category to
compute the confidence score of l. Spyromitros et al. [17]
propose a similar method, named BR-KNN (for Binary
Relevance KNN), and two extensions of this method. The
proposed approach is an adaptation of the kNN algorithm
using a BR method which trains a binary classifier for each
label. Confidence scores for each label are computed using
the number of neighbours among the k neighbours that
include this label. In [20], an experimental comparison of
several multi-label learning methods is presented. In this
work, different approaches were investigated using various
evaluation measures and datasets from different appli-
cation domains. In their experiments, authors showed that
the best performing method is based on the Random
Forest classifier [21]. Other recent works address MLC
with large number of labels [22]. Indeed, in many applica-
tions, the number of labels used to categorize instances is
generally very large. For example, in the biomedical
domain, the MeSH thesaurus consisting of thousands
descriptors (27,149 in the 2014 version) is often used to
classify documents. This large number of descriptors can
affect the effectiveness and performance of multi-label
models. To address this issue, a label selection based on
randomized sampling is performed [22].
Methods
In this section, we present the text classification ap-
proaches developed in our work: a kNN-based approach
and an ESA-based approach.
The kNN-based approach: kNN-classifier
This approach consists of two steps. First, for a given
document, represented by a vector of unigrams, its k most
similar documents are retrieved. To do so, the TF.IDF
weighting scheme is used to determine the weights of dif-
ferent terms in the documents. Then, the cosine similarity
between documents is computed. Once the k nearest doc-
uments of a target document are retrieved, the set of
labels assigned to them are used for training the classifiers
(in the training step) or as candidates for classifying the
document (in the classification step). Labels, which are
the instances here, are first represented by a set of attri-
butes. Thereafter, ML algorithms are used to build models
which are then used to rank candidate labels for annotat-
ing a given document. For ranking labels, different learn-
ing algorithms are explored.
Nearest neighbours retrieval
Our kNN-based approach requires a collection of docu-
ments previously annotated for the neighbours retrieval.
For a given document, the aim is to retrieve its k most
Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 3 of 12
similar documents. To do so, like the PubMed Related
Citations approach [10], we consider that two docu-
ments are similar if they address the same topics. The
cosine similarity measure, which is commonly used in
text classification and information retrieval (IR) with the
VSM [7], is chosen for this purpose. The documents are
first segmented into sentences and tokens, while stop
words are removed. From these pre-processed texts, all
unigrams are extracted and normalized according to a
stemming technique [23]. Then, the cosine measure
enables to compute similarity between documents, which
are represented by vectors of unigrams. Formally, let
C = {d1, d2,, dn}, a collection of n documents, T =
{t1, t2,, tm}, the set of terms appearing in the docu-
ments of the collection and the documents di and dj
being represented respectively by the weighted vectors
di = (w1
i , w2
i ,, wm
i ) and dj = (w1
j ,w2
j ,,wm
j ), their cosine
similarity is defined by [12, 24]:
Sim di; ; dj
  ¼
Xm
k¼1w
i
kw
j
kffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiXm
k
wik
 2q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiXm
k
w jk
 2r
where wk
l , is the weight of the term tk in the document
dl. It is the TF.IDF value of the term.
In order to optimize the search, the documents in the
search space are indexed beforehand using the open
source IR API Apache Lucene.3 The k-nearest neighbours
retrieval thus becomes an IR problem where the target
document is the query to be processed.
Collection of candidate labels
For a given document, once its kNN are retrieved, all
labels assigned to these documents are gathered in order
to constitute a set of candidate labels likely to annotate
this document. Since this can be seen as a classification
problem, we use ML techniques to rank these candidate
labels. Thus, classical classifiers are used to build classifi-
cation models which are then exploited to determine the
relevant labels for annotating any unseen document. For
that purpose, candidate labels are used as training in-
stances (in the training step) or instances to be classified
(in the classification step).
Feature extraction
To determine the relevance of a candidate label, it is
represented by a vector of features (also called attributes).
In the training step, its class is set to 1 if the label is
assigned to the target document and otherwise 0 while in
the classification step, the model uses the label features to
determine its class. We defined six features based on
related works [5, 17].
For each candidate label, the number of neighbour
documents to which it is assigned is used as a feature
(Feature 1). This value represents an important clue to
determine the class of the label. Moreover, in the clas-
sical kNN-based approach, it is the only factor used to
classify a new instance. In practice, a voting technique is
used to assign the instance to the class that is the most
common among its k nearest neighbours.
For each candidate label, the similarity scores between
the document to classify and its nearest neighbours an-
notated with this candidate label are summed and this
sum is another feature (Feature 2). Since the distance
between a document and each of its neighbours is not
the same, we consider that the relevance of the labels
assigned to them for the target document is inversely
proportional to this distance. In other words, the closer
a document is to the target document, the more its asso-
ciated labels are likely to be relevant for the latter. In [1],
this is the only feature used to determine the relevance
scores of candidate labels.
Formally, like in [17], let L = {lj}, j = 1,, n, be the can-
didate labels set of a new document d, and V = {di}, i =
1,, k, its k nearest neighbours, the values of these
attributes for the label lj are respectively defined as:
f 1 lj
  ¼ 1
k
Xn
i¼1
assigned lj; di
 
f 2 lj
  ¼ 1
k
Xn
lj?di
sim d; dið Þ
where the binary function assigned(lj, di) returns 1 if the
label lj is assigned to the document di, 0 otherwise;
sim(d, di) is the similarity score between the documents
d and di and is computed using the cosine measure.
For each candidate label, we also checked if all the
constituent tokens appear in the title and abstract of
the document and consider it as the third feature
(Feature 3). This binary feature has been chosen because
it captures disjoint terms (terms constituted of disjoint
words) which are frequent in the biomedical texts.
In addition to these features, we computed two other
features using term synonyms. Indeed, for indexing
biomedical documents, the MeSH thesaurus is commonly
used. The latter is composed of a set of descriptors (also
called main headings) organized into a hierarchical struc-
ture. Each descriptor includes synonyms and related
terms, which are known as its entry terms. Thus, for each
label (called descriptor here), we check whether one of its
entries appears in the document. If this is the case, the
fourth binary feature (Feature 4) is set to 1 and the
descriptor frequency in the document is computed as a
value corresponding to the fifth feature (Feature 5), other-
wise the two features are set to 0.
Finally, another feature (Feature 6) is used to verify
whether a candidate label is contained in the documents
Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 4 of 12
title. Our assumption is that if a label appears in the
title, it is relevant for representing this document.
The relevance of each of these features is estimated
using the information gain measure (Table 1). The first
two features mainly permit to compute relevance scores
of candidate labels.
Classifier building
To build the classifiers, a labelled training set consisting
of a collection of documents with their manually associ-
ated labels is constituted. For each document in the
training set, its nearest neighbours and their manually
assigned labels are collected. Each label of this collected
set is considered as an instance for the training. Thus,
for each label, its different features (see the previous sec-
tion) are computed. Thereafter, labels obtained from
neighbours of the different documents of the training set
are gathered to form the training data. Then, classifiers
are built from this labelled training data. We have tested
the following classification algorithms: Naive Bayes (NB)
[25], Decision Trees (DT also known as C4.5 in our case
[24]), Multilayer Perceptron (MLP) and Random Forest
(RF) [26]. We chose these classifiers as they have yielded
the best performances in our tests.
For the implementation of these classifiers, we use the
WEKA4 (Waikato Environment for Knowledge Analysis)
tool, which integrates many ML algorithms [27], includ-
ing the four ones we have tested.
Document classification
Given a document to be classified, the candidate labels
collected from its neighbours are represented as the
training ones (see the previous section). The trained
model is then used to estimate the relevance score of
each candidate label. Indeed, the model computes, for
each candidate label, its probabilities to be relevant or
not. From these probability measures, the relevance
score of each label is derived. Candidate labels are then
ranked according to their corresponding scores and the
N top-scoring ones are selected to annotate the docu-
ment, where N is determined using three different
techniques.
Selection of the optimal value of N
In order to determine the optimal value of N, we explore
three strategies:
a) Initially, N is set as the number of labels having a
relevance score greater than or equal to a threshold
arbitrarily set to 0.5. This strategy based only on the
relevance score of the label regarding the document
is inspired by the original kNN algorithm.
b) We then set the value of N as the average size
(number of labels assigned) of the sets of labels
collected from the neighbours. This strategy has
been successfully used for extending the kNN-based
method proposed in [17].
c) Finally, in the third strategy, we use the method
described in [28]. The principle is to compare the
relevance scores of successive labels of a list of
candidate labels ranked in descending order for
determining the cut off condition enabling to
discard the irrelevant or insignificant ones. This
strategy is defined by the following formula:
siþ1
si
?
i
iþ 1þ ?
where si is the relevance score of a label being at pos-
ition i and ? a constant whose optimal value is deter-
mined empirically.
The ESA-based approach
ESA is an approach proposed for representing textual
documents in a semantic way [8]. In this method, the
documents are represented in a conceptual space consti-
tuted of explicit concepts automatically extracted from a
given knowledge base.5 For this, statistical techniques
are used to explicitly represent any kind of text (simple
words, fragments of text, entire document) by weighted
vectors of concepts. In the approach proposed in [8], the
titles of Wikipedia articles are defined as concepts. Thus,
each concept is represented by a vector consisting of all
terms (except stop words) that appear in the correspond-
ing Wikipedia article. The weight of each word of this
vector is the association score between the term and the
corresponding concept. Theses scores are computed using
the TF.IDF weighting scheme [29].
At the end of this step, each concept is represented by
a vector of weighted terms. Then, an inverted index,
Table 1 Importance of each feature for the prediction
according to the Information Gain measure
Feature Description Information gain
Feature 1 Number of neighbours in which the
label is assigned
0.16
Feature 2 Sum of similarity scores between the
document and all the neighbours
document where the label appears
0.17
Feature 3 Check whether all constituted tokens
of the label appear in the target
document
0.01
Feature 4 Check whether one of the label
entries appears in the target document
0.03
Feature 5 Frequency of the label if it is
contained in the document
0.03
Feature 6 Check if the label is contained in
the document title
0.02
Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 5 of 12
wherein each term is associated with a vector of its
related concepts, is created. In this inverted index, the
less significant concepts (i.e., concepts with low weight)
for a vector are removed. The index is then used to clas-
sify unseen textual documents.
The classification process consists of two steps. For a
given document, it is first represented by a vector of
terms. The concepts corresponding to these terms are
then retrieved in the inverted index and merged to con-
stitute a vector of concepts representing the document.
The retrieved concepts are finally ranked according to
their relevance score in descending order. The most
relevant ones are then selected. This process is illus-
trated by Fig. 1.
Formally, let T be a text, {ti} the terms appearing in T
and vi, their respective weights. Let kj, be the association
score between the term ti and the concept cj with cj ?
C, C the set of Wikipedia concepts. The weight of the
concept cj for the text T is defined by:
W cj
  ¼ X
wi?T
vi:kj
Our ESA-based approach explores this technique in
the specific case where only partial information is con-
sidered (i.e., the title and abstract in the case of scientific
articles). First, we assume the availability of concepts
(generally defined in semantic resources) to be used for
document classification as well as a labeled training set
in which each document is assigned a set of concepts.
Unlike the original ESA method where each article is as-
sociated with a single concept, in our approach, each
document in the training set may be assigned one or
more concepts (also called labels here).
From the training set, we use statistical techniques to es-
tablish associations between labels and terms extracted
from the texts. Thus, for each label, the unigrams that are
more strongly associated with it are used for its represen-
tation. If the concepts are seen as documents, we face with
an IR problem where the goal is to retrieve the most rele-
vant documents (concepts) for a given query (a new docu-
ment). Therefore, the classical IR models can be used to
represent documents and queries, but also to compute the
relevance of a document with respect to a given query. In
this work, the VSM is used to determine the most relevant
concepts for annotating the given document. Like in the
kNN-based approach, the documents are processed using
the following techniques: segmentation into sentences,
tokenization, removal of stop words and normalization
using the Porter's stemming algorithm [23].
For computing the association scores between a
concept c and a term t, we experimented the following
measures:
 The TF.ICF measure (the TF.IDF scheme adapted to
concepts) [7]:
TF :ICF t; cð Þ ¼ TF t; cð Þ  logN
ni
where N is the total number of concepts, ni the number
of concepts associated with t. The factor TF(t, c) is the
number of occurrences of t in the documents annotated
by the concept c and is defined by:
TF t; cð Þ ¼
X
d?Dc
freq t; dð Þ
dj j
Fig. 1 The process of the Explicit Semantic Analysis based approach. The two steps of the ESA-based approach are presented: the indexing step
and the classification step
Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 6 of 12
where freq(t, d) is the frequency of t in the document
d, |d| is the number of words of d and Dc the set of
documents annotated by the concept c.
 The Jaccard coefficient [30]:
J t; cð Þ ¼ cocc t; tð Þ
occ tð Þ þ occ cð Þ?cocc t; cð Þ
where cocc(t, c) is the number of documents in which
the concept c and the term t co-occur occ(c) is the
number of documents annotated by the concept c and
occ(t) is the number of documents in which the term t
appears.
Finally, to estimate the relevance of a concept to
annotate a document, we use the following metric. The
relevance score of a concept c for a new document d is
defined by:
Rel c; dð Þ ¼
X
w?d
TF :IDF t; dð Þ  score t; cð Þ
where score(w,c) is the association score between the
term t and the concept c and TF. IDF(t, d) is the TF.IDF
value of the term in the document d.
Evaluation
In order to assess the effectiveness of our approaches,
we performed two different experiments: one in the con-
text of the task 2a of the international BioASQ challenge
to which we participated [31] and the second experiment
conducted on a derived dataset from the BioASQ chal-
lenge, as described below.
Datasets
The BioASQ organizers, within the 2014 edition, pro-
vided a collection of over 4 million documents consti-
tuted by only titles and abstracts of articles (called also
citations), coming from specific scientific journals for
the task 2a of this challenge [6]. These documents, ex-
tracted from the MEDLINE database, are annotated by
descriptors of the MeSH thesaurus.
In addition, during the challenge, the organizers pro-
vided each week PubMed® citations not yet annotated
which were used as test sets to evaluate the systems par-
ticipating in the task 2a. Participants were asked to classify
these test sets using descriptors of the MeSH thesaurus.
The test sets have subsequently been annotated by
PubMed® human indexers for evaluating the proposals of
the participating systems.
Experiments
First experiment
For the kNN retrieval, we used a dataset consisting of all
articles of this collection published since 2000 (2,268,724
documents). The motivation for this choice is to discard
old documents which are not annotated by descriptors
recently added to the MeSH thesaurus (the MeSH the-
saurus is regularly updated). This dataset is thereafter
extended to the entire collection. For training the classi-
fiers we randomly selected 20.000 articles out of those
published since 2013; the citations of the training set are
discarded from the former dataset. We assume this
training set sufficient to capture relevant information for
building the classifiers.
Only the kNN-based approach was used for our par-
ticipation to the challenge. To assess this method, five of
the different test sets provided by the challenge orga-
nizers were used.
Second experiment
For the second experiment, we first extracted all articles
published since 2013 (133,770 documents) from the pre-
vious dataset provided by the challenge organizers. We
then selected randomly 20,000 documents to be used for
training the classifiers and one thousand for constituting
the test set. The data used to train the classifiers were
then extended to 50,000 documents, since we believed it
could improve the classification performances; using
large training dataset should enable the classifiers to
capture more information. The test collection was also
increased to 2,000 documents. Like in the training data-
set, each document in the test set was assigned a set of
labels by PubMed® annotators. These manually assigned
labels were thus used to evaluate the results of our dif-
ferent methods.
Regarding the evaluation of our ESA-based approach,
except the documents in the test set, the rest of the collec-
tion (i.e., 4,430,399 documents) was exploited to compute
the association scores between words and labels.
Evaluation measures
As previously said, indexing biomedical documents can
be assimilated to a multi-label classification (MLC) prob-
lem. Instead of one class label, each document is assigned
a list of labels. Thus, measures usually used for evaluating
indexing methods were adapted for the MLC context [15].
The example-based precision (EBP) measures how many
of the predicted labels are correct while the example-
based recall (EBR) measures how many of the manually
assigned labels are retrieved. Since EBP and EBR evaluate
partially the performance of a method, the example-based
f-measure (EBF) combines both measures for a global
evaluation. The accuracy (Acc) is also a complementary
measure [15]. These measures are computed as follows.
Let Yi be, the set of true labels (labels manually assigned
to the documents), Zi the set of predicted labels and m
the size of the test set:
Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 7 of 12
EBP ¼ 1
m
Xm
i¼1
Y i?Zij j
Zij j
EBR ¼ 1
m
Xm
i¼1
Y i?Zij j
Y ij j
EBF ¼ 1
m
Xm
i¼1
Y i?Zij j
Zij j þ Y ij j
Acc ¼ 1
m
Xm
i¼1
Y i?Zij j
Zi ?Y ij j
These measures, in addition to being common, are
representative and enable the global evaluation of the
systems performances. The results of our two approaches
are presented in the next section.
Experiment environment
In our different experiments, we used the computing
facility of the Bordeaux Mésocentre, Avakas,6 which
includes:
 the compute nodes c6100 (x264), which are the
machines on which algorithms are executed.
They have the following characteristics:
? Two processors of hexa-cores (12 cores per
node) Intel Xeon X5675 @ 3.06 GHz;
? 48 GB RAM.
 the computation nodes bigmem R910 (x4), which
have more memory and whose cores have slower
processors:
? 4 processors of 10 cores (40 cores per node)
Intel Xeon E7-4870 @ 2.4 GHz;
? 512 GB RAM.
In our case, we used two computation nodes c6100,
which provide 48 GB of RAM and 24 cores Intel Xeon
X5675.
Results
Results of the kNN-based approach
Experiment within the BioASQ challenge
First, we present the results obtained in the task 2a of
the BioASQ challenge. For that purpose, we report re-
sults of batch 3 in terms of EBP, EBR and EBF. We
chose only these measures since they are representative
and allow estimating the global performance of the MLC
methods. Table 2 shows the results of our best system
using the kNN-based approach and the ones which
obtained the highest measures within the different tests
of batch 3. In tests 2 and 5, our best system uses a Naïve
Bayes classifier and selects only labels having a confi-
dence score greater than or equal to 0.5 while in the
others, the best system sets N to the average size of the
sets of labels collected from the neighbours. In most
cases, using this value for N yields better or similar re-
sults than the other strategy. In the challenge, we do not
use the automatic cut-off method to fix the number of
labels as described in [28] but in the second experiment,
this technique is explored.
Second experiment
We evaluate our kNN-based approach with different
configurations in the test set of the second experiment
and compare the achieved performances. Thus, we test
combinations of various classifiers with different tech-
niques for determining the number of labels for annotat-
ing a given document. The evaluation of configurations
with the two best classifiers in our experiments, NB and
RF, are presented in Table 3. The parameter k is empiric-
ally set to 25 using a cross-validation technique. When
the minimal score threshold is used, the precision often
increases significantly, mainly with the RF classifier but
the recall is lower. Regarding the average size strategy,
it yields a good recall but the precision decreases
slightly. In this case, the results of both classifiers are
similar but the RF one slightly outperforms the NB clas-
sifier. The best results are achieved with the cut-off
Table 2 Results of our kNN-based system and the best systems
participating in the BioASQ challenge on the different tests of
the batch 3
Test Number of documents System EBP EBR EBF
Test 1 2,961 kNN-Classifier 0.55 0.48 0.49
Best 0.59 0.62 0.58
Test 2 5,612 kNN-Classifier 0.52 0.50 0.48
Best 0.62 0.60 0.60
Test 3 2,698 kNN-Classifier 0.55 0.49 0.49
Best 0.64 0.63 0.62
Test 4 2,982 kNN-Classifier 0.49 0.55 0.49
Best 0.63 0.62 0.62
Test 5 2,697 kNN-Classifier 0.50 0.53 0.48
Best 0.64 0.61 0.61
Table 3 Results of the kNN-Classifier according to the classifier
and strategy used for fixing N: a) 0.5 as the minimal confidence
score threshold, b) the average size of the sets of labels
collected from the neighbours and c) the cut-off method.
A training set of 20,000 documents is used
Strategy Classifier EBP EBR EBF
a) NB 0.58 0.49 0.49
RF 0.74 0.34 0.43
b) NB 0.51 0.54 0.51
RF 0.52 0.54 0.52
c) NB 0.56 0.52 0.51
RF 0.61 0.52 0.53
Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 8 of 12
method which balances both precision and recall, and
yields the best F-measure. Except for the minimum
threshold technique where the NB classifier results are
better, the best F-measure is achieved with the RF classi-
fier. The DT (C4.5 algorithm of Weka) and the Multi-
layer Perceptron (MLP) classifiers have also been tested
but their results are less interesting. The former yields
lower performances while the latter performs very slowly
and gets results comparable to the RF ones. The MLP
classifier requires more CPU and memory during the
training process.
When the training set is raised from 20,000 to 50,000,
the performances are slightly improved in two test sets
(one of 1000 documents and another of 2000). Table 4
presents the results of the different classifiers in this
larger training set. The value of ? (constant used in the
strategy based on label scores comparison - strategy c- for
optimizing N) also affects the classification performance.
The lower the value of ?, the higher the precision is but
the lower the recall is and vice versa. In these experiments,
we set ? to 1.6 which yields the best results using cross-
validation techniques. Furthermore, we note that when
the classifiers are trained on this extended dataset, they
yield similar performances but the RF classifier slightly
outperforms the others. Table 5 gives an example of labels
suggested for classifying the document having the PMID
23044786 (Table 6) with the kNN-based approach.
In terms of training time, NB, DT and RF classifiers
performed similarly with respectively 4, 6 and 9 min
once data were represented in suitable format for Weka
(e.g. ARFF format (Attribute-Relation File Format)). The
pre-processing step (retrieval of neighbours and compu-
tation of features values) however takes more time (1 h
and 43 min). Note that since we have different types
(binary and numeric) of attributes, we discretize the lat-
ter in nominal attributes. The MLP classifier is, mean-
while, very costly in terms of training time (23 h).
Results of the ESA-based approach
After processing the training set composed of a collec-
tion of 4,432,399 documents (titles and abstracts), we
obtain 1,630,405 distinct words and 26,631 descriptors
assigned to these documents among the 27,149 MeSH
descriptors (98.1 %). To simplify the computation and
optimize the results of the classification, each concept is
represented by a vector consisting of 200 terms, which
are the most strongly associated with it. Only terms
appearing in at least five documents are considered. Our
choice is motivated by the will to simplify the scores
computation by excluding the less representative terms.
Here, since we used test sets already labelled, the num-
ber of concepts which are relevant to annotate the docu-
ment is known and is used; therefore, EBP and EBR are
equivalent; thus we only report the EBF and the accuracy
measures.
After evaluating the ESA-based approach, we note, as
in previous work, that its performance varies depending
on the measure used to estimate the association scores
between words and concepts. This behaviour is illustrated
in Table 7 where the Jaccard measure yields the best
results.
Discussion
While textual classification has been widely investigated,
few approaches are currently able to efficiently handle
large collections of documents, in particular when only a
portion of the information is available. This is a challen-
ging task, particularly in the biomedical domain.
Our experiments show that our kNN-based approach
is promising for biomedical documents classification in
the context of a large collection. Our results confirm the
findings presented in [1], where among the multiple
classification systems, the kNN-based one yielded the
best results. If we compare our method with the latter, we
use more advanced features to determine the relevance of
Table 4 Results of the kNN-Classifier according to the classifier
using the cut-off method with a training set of 50,000 documents
Classifier EBP EBR EBF Acc
NB 0.59 0.54 0.54 0.39
RF 0.62 0.54 0.55 0.41
C4.5 0.63 0.52 0.54 0.39
MLP 0.64 0.46 0.51 0.36
Table 5 Labels generated by the kNN-Classifier with their
corresponding relevance scores for the document having the
23044786 PMID
Labels Relevance Manual validation
Humans 0.99 Yes
Postoperative care 0.75 Yes
Female 0.60 Yes
Male 0.60 Yes
Middle aged 0.32 Yes
General surgery 0.32 Yes
Medical errors 0.32 Yes
Patient care team 0.32 No
Postoperative complications 0.32 No
Adult 0.26 Yes
Safety management 0.26 No
Aged 0.25 Yes
Prospective studies 0.21 Yes
Length of stay 0.21 No
Patient safety 0.20 Yes
Surgical procedures, operative 0.20 No
Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 9 of 12
a candidate label. Indeed, Trieschnigg and his colleagues
determine the relevance of a label by summing the re-
trieval scores of the k neighbour documents that are
assigned to the label [1]. In our method, this sum is only
considered as one feature among others for determining
the confidence scores of labels. While the results of our
method do not outperform the extended (and improved)
MTI system [11] which is currently used by the NLM
curators, it gets promising results (0.49 against 0.56
of F-measure). A direct comparison with the method
proposed in [5] is not simple since the authors used
an older collection than the official datasets provided
in the BioASQ challenge, which are recent and anno-
tated with descriptors of the recent MeSH thesaurus
(2014 version). Similarly to their experiments, when
our method is evaluated on 1,000 randomly selected
documents, it outperforms this method (0.55 against
0.50 for the F-measure). But a comparison with their
recent results in the first challenge of BioASQ [28]
where they integrated the MTI outputs, their system
performs better than ours (F-measure of 0.56 against
0.49). Compared with the two approaches proposed in
[32], one based on the MetaMap tool [33] and another
using IR techniques, our method gets better results (0.49
against 0.42 for the F-measure). Our approach outper-
forms also the hierarchical text categorization approach
proposed in [34].
As part of our participation in the challenge, the NB
classifier is combined with the average size of labels
assigned to the neighbours to determine relevant descrip-
tors for a given document. In the second experiment, we
note however that a combination of RF with the cut-off
technique proposed in [28] yields better results [35]. A
more recent evaluation of our kNN-based approach using
a large dataset (50,000 documents) for training the classi-
fiers shows that it provides better performances, com-
parable to the best methods described in the literature
(with an f-measure of 0.55). Moreover, unlike the exten-
ded MTI system [11], we do not use any specific filtering
rules. This makes our approach generic and its reuse in
other domains straightforward. A comparison of our basic
kNN-based system (trained on 20,000 documents, and
improved later) to the performing classification systems
[3638], which also participated in the 2014 BioASQ
challenge [31] and the baseline (extended MTI) [11] is
shown in Table 8. The two best systems, Antinomyra [36]
and L2R [38], rely on the learning to rank (LTR) method.
Table 6 Example of a PubMed® (23044786) citation manually annotated by human indexers using MeSH descriptors. This is an
example of a PubMed citation, consisting of a title and an abstract, with MeSH descriptors manually selected by indexers for
annotating it
Title An observational study of the frequency, severity, and etiology of failures in postoperative care after major
elective general surgery
Abstract Objective:
To investigate the nature of process failures in postoperative care, to assess their frequency and preventability,
and to explore their relationship to adverse events.
Background:
Adverse events are common and are frequently caused by failures in the process of care. These processes are
often evaluated independently using clinical audit. There is little understanding of process failures in terms of
their overall frequency, relative risk, and cumulative effect on the surgical patient.
Methods:
Patients were observed daily from the first postoperative day until discharge by an independent surgeon.
Field notes on the circumstances surrounding any non routine or atypical event were recorded. Field notes
were assessed by 2 surgeons to identify failures in the process of care. Preventability, the degree of harm caused
to the patient, and the underlying etiology of process failures were evaluated by 2 independent surgeons.
Results:
Fifty patients undergoing major elective general surgery were observed for a total of 659 days of postoperative
care. A total of 256 process failures were identified, of which 85% were preventable and 51% directly led to
patient harm. Process failures occurred in all aspects of care, the most frequent being medication prescribing
and administration, management of lines, tubes, and drains, and pain control interventions. Process failures
accounted for 57% of all preventable adverse events. Communication failures and delays were the main
etiologies, leading to 54% of process failures.
Conclusions:
Process failures are common in postoperative care, are highly preventable, and frequently cause harm to
patients. Interventions to prevent process failures will improve the reliability of surgical postoperative care and
have the potential to reduce hospital stay.
MeSH descriptors assigned
manually to the citation
Adult, Aged, Aged, 80 and over, Digestive System Surgical Procedures*, Elective Surgical Procedures*, Female,
General Surgery, Hospitals, Teaching, Urban, Humans, Interprofessional Relations, London, Male, Medical Errors,
Medical, Errors, Middle Aged, Outcome and Process Assessment (Health Care)*, Patient Safety, Postoperative,
Care, Postoperative Care, Prospective Studies
Table 7 Results of the ESA-based approach according to the
association score
Association score EBF Acc
Jaccard coefficient 0.26 0.16
TF.ICF 0.22 0.13
Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 10 of 12
The former extends features generated from the neigh-
bour document retrieval with binary classifiers and the
results of the MTI and uses then the LTR method to rank
the candidate labels. Meanwhile, the other system, com-
bines information obtained by the neighbours retrieval,
binary classifiers and the MTI results as features and also
uses the LTR for the ranking. The Hippocrates system
presented in [37] only relies on binary SVM (Support
Vector Machine) classifiers and trains them on a large
dataset (1.5 million documents) in contrast to our
basic kNN approach trained on 20,000 documents.
Note that these three systems use binary classifiers
for building a model for each label [31]. These sys-
tems require therefore considerable resources in terms of
computation and storage compared to our kNN-based
approach.
For the kNN retrieval, we have investigated the cosine
similarity which is widely used in IR. It would be inter-
esting to combine this measure with domain knowledge
resources, such as ontologies, to overcome the limitation
of similarity computation based only on common words.
The second method based on the ESA, meanwhile, yields
very low performances comparable to basic methods using
a simple correspondence between the text and the seman-
tic resource inputs. Thus, although the ESA technique has
shown interesting results in text classification [9], it does
not seem appropriate for our targeted classification prob-
lem where only partial information is available. Indeed, to
compute the association scores between a term and a
label, this method exploits the occurrences of this term in
the documents annotated by the label. However, in this
specific classification problem, labels used to annotate a
document are not always explicitly mentioned in the later.
Documents are short and it is thereby unlikely that they
contain mentions of all relevant labels. It is worth men-
tioning that in our approach, each concept is represented
by a vector consisting of 200 terms, and only terms
appearing in at least five documents are considered.
For example, the most associated stemmed terms (with
their corresponding Jaccard scores) to the label Body Mass
Index are: index (0.1), waist (0.087), mass (0.079),
bodi (0.077), circumfer (0.068), anthropometr (0.062),
fat (0.059), adipos (0.048), smoke (0.039), weight (0.038),
nutrit (0.037).
Note that we do not use the large Wikipedias know-
ledge base, like the work presented in [8], for the con-
ceptual representation of documents since most of the
MeSH descriptors cannot be directly mapped to this
resource. Furthermore, contrary to existing works [9],
which use ESA for enriching the bag-of-words approach
with additional knowledge-based features, our ESA-based
method builds a standalone classifier. However, this ap-
proach will be explored in the future in order to enrich
the features and consequently improve the performance
of our k-NN approach.
Conclusion
In this paper, we have described two approaches for
improving the classification of large collections of bio-
medical documents. The first one is based on the kNN
algorithm while the second approach relies on the ESA
technique. The former uses the cosine measure with the
TF.IDF weighting method to compute similarity between
documents and therefore to find the nearest neighbours
for a given document. Simple classification methods de-
termine the most relevant labels from a set of candidates
of each document. We have investigated an important
feature of the classification problem: the decision bound-
ary which permits to determine the relevant label(s) for
a target document. Thus, instead of using voting tech-
niques like in the classical kNN algorithm, ML methods
were used to classify documents. The latter is based on
the ESA technique which exploits associations between
words and labels.
Thanks to an evaluation on standard benchmarks, we
noted that the kNN based method using the RF classifier
with the cut-off method yielded the best results. We also
noted that this approach achieved promising performances
compared with the best existing methods. In contrast, our
findings suggest that the ESA is not suitable for classifying
a large collection of documents when only partial informa-
tion is available.
For indexing purpose, the representation of docu-
ments as bags of words is limited since similarity be-
tween the latter is only based on the words they
share. Therefore, to improve the performance of our
kNN-based approach, we plan to use a wide biomed-
ical resource, such as the UMLS Metathesaurus, for
computing the similarity between documents (exploit-
ation of synonyms and relations) and thus overcome
this limitation. Other features and similarity measures
will be studied to improve the performances of our
method.
Table 8 Comparison of our kNN-Classifier used for participating
in the challenge with the best systems and the MTI baseline on
the test set of the week 2 of batch 3 consisting of 3009 documents.
The used measures are: example-based precision (EBP),
example-based recall (EBR), example-based f-measure (EBF)
and micro f-measure (MiF) (Source BioASQ 2014)
Systems EBP EBR EBF MiF
Antinomyra [36] 0.59 0.62 0.59 0.60
L2R [38] 0.59 0.60 0.58 0.59
Hippocrates [37] 0.59 0.60 0.57 0.59
MTI 0.59 0.58 0.56 0.57
kNN-Classifier 0.55 0.49 0.49 0.51
Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 11 of 12
Endnotes
1http://bioasq.lip6.fr
2Labels are categories used to classify documents
3http://lucene.apache.org/core/
4http://www.cs.waikato.ac.nz/ml/weka/
5Wikipedia in most cases
6http://www.mcia.univ-bordeaux.fr/index.php?id=45
Acknowledgements
The work presented in this paper is supported by the French Fondation Plan
Alzheimer. The authors would like to thank the BioASQ 2014 challenge
organizers who provided the datasets used in this study for evaluating the
classification methods. They would also like to thank the anonym reviewers
of the previous version of our paper in the (Symposium on Semantic Mining
in Biomedicine) 2014.
Authors' contributions
KD, FM and GD all participated in designing the methods and contributed to
the results analysis. KD performed the experiments, discussed the results and
drafted the manuscript. GD and FM participated in the correction of the
manuscript. All authors read and approved the final version of the
manuscript.
Competing interests
The authors declare that they have no competing interest.
Received: 1 March 2015 Accepted: 8 May 2016
