JOURNAL OF
BIOMEDICAL SEMANTICS
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51
http://www.jbiomedsem.com/content/5/1/51RESEARCH Open AccessComparative analysis of knowledge representation
and reasoning requirements across a range of life
sciences textbooks
Vinay K Chaudhri1*, Daniel Elenius1, Andrew Goldenkranz2, Allison Gong3, Maryann E Martone4, William Webb5
and Neil Yorke-Smith6,7Abstract
Background: Using knowledge representation for biomedical projects is now commonplace. In previous work, we
represented the knowledge found in a college-level biology textbook in a fashion useful for answering questions.
We showed that embedding the knowledge representation and question-answering abilities in an electronic
textbook helped to engage student interest and improve learning. A natural question that arises from this
success, and this papers primary focus, is whether a similar approach is applicable across a range of life science
textbooks. To answer that question, we considered four different textbooks, ranging from a below-introductory college
biology text to an advanced, graduate-level neuroscience textbook. For these textbooks, we investigated the following
questions: (1) To what extent is knowledge shared between the different textbooks? (2) To what extent can the same
upper ontology be used to represent the knowledge found in different textbooks? (3) To what extent can
the questions of interest for a range of textbooks be answered by using the same reasoning mechanisms?
Results: Our existing modeling and reasoning methods apply especially well both to a textbook that is
comparable in level to the text studied in our previous work (i.e., an introductory-level text) and to a textbook
at a lower level, suggesting potential for a high degree of portability. Even for the overlapping knowledge
found across the textbooks, the level of detail covered in each textbook was different, which requires that
the representations must be customized for each textbook. We also found that for advanced textbooks, representing
models and scientific reasoning processes was particularly important.
Conclusions: With some additional work, our representation methodology would be applicable to a range of
textbooks. The requirements for knowledge representation are common across textbooks, suggesting that a
shared semantic infrastructure for the life sciences is feasible. Because our representation overlaps heavily with
those already being used for biomedical ontologies, this work suggests a natural pathway to include such
representations as part of the life sciences curriculum at different grade levels.
Keywords: Ontology, Textbook knowledge, Knowledge representation, Reasoning, Question answering,
Semantic infrastructure* Correspondence: Vinay.Chaudhri@sri.com
1SRI International, Menlo Park, CA 94025, USA
Full list of author information is available at the end of the article
© 2014 Chaudhri et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain
Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article,
unless otherwise stated.
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 2 of 19
http://www.jbiomedsem.com/content/5/1/51Background
Using knowledge representation is now commonplace
across a range of biomedical projects [1-3]. This usage
is evidenced by the success of the National Center of
Biomedical Ontologies, which, as of 2014, publishes and
disseminates more than 350 ontologies [4]. Despite this
widespread application of knowledge representation in
biomedical projects, further significant value could be
reaped: The Journal of Nucleic Acids Research catalogues
thousands of databases that could substantially benefit if
they were accompanied by an explicit ontology [5]. We
anticipate that knowledge representation will play a cru-
cial role in future biomedical research, especially for
exploiting, leveraging, and understanding big data.
During an artificial intelligence (AI) project called Project
Halo, we developed an intelligent textbook technology that
leverages an explicit ontology and a question-answering
system, and that helps students learn better [6]. Obvious
overlaps exist between the technologies used in our
project and the methods that are commonplace for bio-
medical ontologies [7,8]. This convergence presents an
unprecedented pathway for synergy between work on on-
tologies and life sciences education. If textbook knowledge
could be represented and encoded in an educational con-
text, as we propose here, then it could eventually be more
widely incorporated into biomedical projects, thus com-
plementing the existing knowledge resources.
Our work on the intelligent textbook [6] focused on
an introductory college-level biology textbook called
Campbell Biology [9]. We encoded substantial portions
of Campbell and then used this knowledge representa-
tion as a basis for an intelligent textbook called Inquire,
which enables students to explore topics across multiple
levels of organization and to pose their own questions,
which are then answered by machine reasoning. The
intelligent textbook is a powerful learning tool that both
gives students information such as definitions and de-
scriptions of terms, and enables them to explore structure,
function, and concepts across different levels of biological
organization.
The current papers focus is on investigating the ques-
tion: To what extent can a generic methodology for cap-
turing textbook knowledge be developed that is applicable
across a range of life sciences textbooks? We have broken
this high-level question into three sub-questions: (1) To
what extent is knowledge shared between the different
textbooks? (2) To what extent can the same upper ontol-
ogy be used to represent the knowledge found in different
textbooks? (3) To what extent can the questions of inter-
est for a range of textbooks be answered by using the
same reasoning mechanisms? A desired outcome is quan-
tifying the extent to which the already developed methods
apply to different textbooks and quantifying any differ-
ences or novel requirements across textbooks. Thesequestions are important, because if we could apply the
same methodology to textbooks at both lower and higher
grade levels, then this generalizability would enable mak-
ing semantics integral to science textbooks. The answers
to these questions will also be informative to others as
they seek insights both into generic techniques for ontol-
ogy design and into the requirements that differ across
domains.
For the remainder of this section, we give an overview
of our project, review the prior work on knowledge repre-
sentation, describe the ontology, and provide the rationale
for the textbooks that were selected for comparison. We
follow that by a description of our methods and results.
Context of Project Halo
Project Halo was an AI project funded by Vulcan, Inc.,
with the goal of creating a system called Digital Aristotle
that could answer questions on a wide variety of science
topics. SRI International participated in this project from
20032013 [6,10,11]. During this period, we advanced
the state of the art in knowledge base (KB) systems by
enabling domain experts with little background in know-
ledge representation to author knowledge that could be
used for answering questions. This works results are
embodied in a knowledge-authoring system called AURA
[11]. To demonstrate the scalability of the approach, we
used AURA to encode substantial fractions of Campbell
Biology [9], which resulted in the knowledge base KB
Bio 101 [12]. A team of biologists trained in AURA but
having no background in knowledge representation per-
formed the encoding work. We designed a knowledge-
factory process that the biologists used to systematically
convert the textbook content into KB Bio 101 [12]. Al-
though accurately assessing the total effort invested in
the encoding is difficult, we estimate that the effort was
at least twelve person years. KB Bio 101 represents a
substantial fraction of Campbell Biology and contains
more than 100,000 axioms [13].
We incorporated KB Bio 101 into an electronic textbook
application called Inquire, which helps students with read-
ing and homework problem solving [6]. An evaluation of
Inquire with students showed the practical utility of in-
corporating a KB into an electronic textbook, as the
Inquire students exhibited higher scores than did the con-
trol group and received no grades D or F, while these lower
grades were seen in the control group. A video based
on Inquire won the best video award at the annual confer-
ence of the Association for Advancement of Artificial
Intelligence (AAAI) in 2012a.
Knowledge representation in AURA
The AURA knowledge-authoring system uses Knowledge
Machine (KM) as its knowledge representation and rea-
soning engine [14]. KM supports standard representational
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 3 of 19
http://www.jbiomedsem.com/content/5/1/51features such as classes; individuals; class-subclass hier-
archy; disjointness; slots; slot hierarchy; necessary and suf-
ficient properties; and deductive rules. The representation
in KM can be formally understood as first-order logic with
equality. Uniquely, KMs representation supports graph-
structured class descriptions. We illustrate KM in the
following example.
Suppose we wish to represent the statement: Every cell
is an entity that has a ribosome and a chromosome as its
parts. We can express this statement in first-order logic
as follows. (We implicitly assume that the statements hold
over all times).
Axiom A1:
? x : Cell xð Þ?? y1; y2 : Entity xð Þ
?has?part x; y1ð Þ ? has?part x; y2ð Þ
? Ribosome y1ð Þ ? Chromosome y2ð Þ
Next, suppose we wish to represent: Every eukaryotic
cell has as parts a ribosome, a nucleus, and a eukaryotic
chromosome such that the chromosome is inside the
nucleusb. We can capture this statement in first-order
logic as follows:
Axiom A2:
? x : Eukaryotic?Cell xð Þ?? y1; y2; y3 : Cell xð Þ
? has?part x; y1ð Þ ? has?part x; y2ð Þ
? has?part x; y3ð Þ ? is?inside y2; y3ð Þ
? Ribosome y1ð Þ ? Eukaryotic?Chromosome y2ð Þ
? Nucleus y3ð Þ
In the class definition of a Eukaryotic-Cell, specifying
the is-inside relationship between the Chromosome and
the Nucleus violates the tree model property [15]. In
models satisfying tree model property, each node has (at
most) a unique direct predecessor, and in general, it is a
good indicator of decidability. To see how the valid
models for A2 violate the tree model property, we create
a directed graph as follows: each variable in the axiom is
represented by a node, and a directed edge exists be-
tween the nodes representing a variable x and a variable
y if they both participate in the same predicate such that
x appears in the first position and y appears in the sec-
ond position. (Because DLs are limited to binary predi-
cates, we limit our discussion to only binary predicates.)
For a graph for axiom A2, the node y3 has two incoming
edges from x and y2, and thus, violates the tree model
property. DL systems achieve decidable reasoning by
limiting the representation to only allow tree models,
and this limitation is well known [16]. Active research is
in progress to address this limitation [17-20].
Next, suppose we wish to explicitly state the inherit-
ance relationships in our representation by asserting that
a Eukaryotic-Cell inherits a Chromosome and Ribosome
from a Cell, and further, by specifying the inheritedChromosome as a Eukaryotic-Chromosome. We can cap-
ture such relationships if we rewrite A1 and A2 by using
Skolemization, a well-known technique to approximate
existential variables in the antecedent of an axiom [21].
With Skolemization, in an axiom of the form ? Y1 Yn ?
X ?, the existential variable X can be removed and re-
placed everywhere in ? with the function term f(Y1
Yn), where f is a new function symbol that does not
occur anywhere else in the axiom. The rationale for such
a substitution is that, for any query, the original axiom is
unsatisfiable if and only if the transformed axiom is
unsatisfiable [21]. This implies that a query with an ori-
ginal axiom in the KB can be answered if and only if it
can be answered when posed against the KB with the
Skolemized version of the same axiom. However, from
the point of view of logical entailment, the Skolemized
KB is stronger than the original one, which is why we
say that Skolemization only approximates existential
quantification and is not equivalent to it. Skolemization
of A1 and A2 enables referring to the Skolem functions
introduced in them outside the scope of the existential
quantifier. In the Skolemized versions of axioms A1 and
A2 shown below, we can see that A4 refers to the Skolem
functions introduced in A3.
Axiom A3:
? x : Cell xð Þ?Entity xð Þ?
has?part x; f cell1 xð Þð Þ ? has?part x; f cell2 xð Þð Þ
? Ribosome f cell1 xð Þð Þ ? Chromosome f cell2 xð Þð Þ
Axiom A4:
? x : Eukaryotic?Cell xð Þ? Cell xð Þ ?
has?part x; f ecell1 xð Þð Þ ? has?part x; f ecell2 xð Þð Þ
? has?part x; f ecell3 xð Þð Þ
? Eukaryotic?Chromosome f ecell3 xð Þð Þ
? Nucleus f ecell1 xð Þð Þ ? Ribosome f ecell2 xð Þð Þ
? is?inside f ecell3 xð Þ; f ecell1 xð Þð Þ
? f ecell3 xð Þ ¼ f cell2 xð Þ ? f ecell2 xð Þ ¼ f cell1 xð Þ
The equality statement used in A4 proves to be a power-
ful tool that explicitly shows the inheritance relationship.
In some cases, equality statements can be inferred. For ex-
ample, if a cardinality constraint asserts that a Cell has
exactly one Chromosome, then one can deductively con-
clude that the Eukaryotic-Chromosome must be the same
as the inherited Chromosome. However, associating such
constraints is incorrect in many situations, as is the case
for a Eukaryotic-Cell.
More details about our approach to knowledge repre-
sentation [22] and reasoning are available in previously
published papers [23-25]. We have translated KB Bio 101
into multiple different formats including Web Ontology
Language Version 2 (OWL2) functionalc, answer set pro-
gramming, and the Thousands of Problems about Theorem
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 4 of 19
http://www.jbiomedsem.com/content/5/1/51Proving syntaxd. The translation into OWL2 is lossy, as it
cannot fully capture the graph structures represented in
the KB; the other translations are non-lossy. These trans-
lations are available through our websitee, and an OWL
version is available through BioPortalf.
Upper ontology in AURA
AURA uses an upper ontology called Component Library
or CLIB [26]. CLIB is a linguistically motivated ontology
designed to support representation of knowledge for au-
tomated reasoning. CLIB uses four simple, upper-level
distinctions: (1) Entity (things that are); (2) Event (things
that happen); (3) Relation (associations between things);
and (4) Role (ways in which entities participate in events).
A unique feature of CLIB is that it provides a vocabu-
lary of actions for modeling biological processes. An Ac-
tion is a subclass of Event. In CLIB, the class Action has
42 direct subclasses, with 147 subclasses in all. Examples
of direct subclasses include Attach, Impair, and Move.
Other subclasses include Move-Through (which is a sub-
class of Move) and Break (which is a subclass of Damage,
which is a subclass of Impair). To ensure generality, these
subclasses were developed by consulting lexical resources,
such as WordNet [27]; the Longman Dictionary of Con-
temporary English [28]; and Rogets Thesaurus [29].
CLIB provides semantic relationships to define the
participants of an action. These relations are based on a
comprehensive study of case roles in linguistics [30] and
include agent, object, instrument, raw-material, result,
source, destination, and site. (The syntactic and se-
mantic definitions that we developed for these relations
are available elsewhere [31].) As an example, we considerFigure 1 A simplified view of the structure of Biomembrane represen
quantified, and every other node (shown in gray) is existentially quantified.
Biomembrane, there exists an instance of Phospholipid-Bilayer and an instan
the instance of Glycoprotein is-inside the instance of Phospholipid-Bilayer. T
instance-instance relationships [33].the definition of raw-material. The semantic definition of
raw-material is any entity that is consumed as an input to
a process. The syntactic definition of raw-material is ei-
ther it is the grammatical object of verbs such as to use
or to consume, or the word using precedes it.
CLIB also provides the vocabulary needed to define
the relationships that exist between entities, and between
entities and events, and to associate properties with both
entities and events. For example, the most frequent rela-
tionships help define the structural relationships that
exist between entities [32]. We use such relationships for
representing structure: has-part, has-region, material,
element, and possesses. We have developed detailed defi-
nitions and guidelines for their usage. For example, we say
that X has-region Y if Y is a region of space or a Spatial-
Entity defined only in relation to X. The complete defini-
tions of the CLIB concepts and relationships are available
onlineg.
As an illustration of the use of CLIB, in Figure 1, we
show a simplified representation of the structure of a
Biomembrane. From the representational point of view,
the graph in Figure 1 represents an existential rule of the
sort seen in axioms 1 and 2. In this figure, the node shown
in white is universally quantified, and every other node,
shown in gray, is existentially quantified. Therefore, we can
read a portion of Figure 1 as follows: for every instance of
Biomembrane, there exists an instance of Phospholipid-
Bilayer and an instance of Glycoprotein that are in has-part
relationship to it, and further the instance of Glycoprotein
is-inside the instance of Phospholipid-Bilayer. In the
context of the relationships used in biomedical ontol-
ogies, our usage of has-part and other relationshipsted in AURA. The Biomembrane node (shown in white) is universally
We can read a portion of this figure as follows: for every instance of
ce of Glycoprotein that are in has-part relationship to it, and further
he usage of has-part and other relationships corresponds to
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 5 of 19
http://www.jbiomedsem.com/content/5/1/51corresponds to instance-instance relationships [33]. The
arrows go from the first argument of a predicate to the sec-
ond argument. For example, an arrow from Biomembrane
to a Phospholipid-Bilayer labeled as has-part corresponds
to the predicate has-part(b,p), where b is an instance of a
Biomembrane, and p is an instance of a Glycoprotein.
The numbers on some of the edges indicate cardinality
constraints. For example, the instance of Phospholipid-
Bilayer in Figure 1 has exactly two phospholipid layers
that are in a has-region relationship to it. In Figure 2,
we show the functions of a Biomembrane. A portion of
this figure can be read analogously to Figure 1 as fol-
lows: for every instance of a Biomembrane, there exists a
function Block in which the agent is a Hydrophobic-
Core, the object is a Hydrophilic-Compound, and an
instrument is a Fatty-Acid-Tail. More details about our
representation of functions are available elsewhere [32].
Reasoning in AURA
The KM system [14] provided the core reasoning ser-
vices for AURA. KMs reasoning combines description-
logic-style classification [34] with backward chaining
on rules. We extended KMs basic reasoning with sev-
eral higher-level reasoning methods to answer ques-
tions [24,25]. AURA also contained a natural language
processing interface that processed an input English ques-
tion and converted it to a formal representation for evalu-
ation by the reasoner [11]. We list below several abstract
question templates, each followed by an example of its
instantiation. A detailed formalization of different rea-
soning processes in AURA has been published else-
where [24,25]. To make this paper self-contained, we
follow each question either by giving a high-level de-
scription of how that question was formalized or by
specifying a logical query that could be evaluated by
a general-purpose reasoner.Figure 2 Functions of Biomembrane. The top half of this figure can be rea
of chemical entities that it is permeable to, and that this movement is througQ1. What are the R of X? (e.g., What are the parts of a
cell?)
Q1 is a very common and basic form of query with
numerous variations. Because the relevant knowledge to
answer Q1 is in the form of axioms such as A1, the
formalization of Q1 contains a premise that extends the
KB to KB by creating a sample instance of Cell. For ex-
ample, for the class Cell, and corresponding to the axiom
A1, KB will contain the individual c1. By the application
of A1, KB is further extended by adding r1 and ch1 such
that they are instances of Ribosome and a Chromosome,
respectively, and by adding the assertions (has-part c1 r1)
and (has-part c1 ch1), which are conclusions derived by
using A1. To answer Q1, we query for all literals matching
(has-part c1 ?x), returning c1 and ch1 as answers. In more
complex examples, query evaluation can involve inheriting
information from super-classes and applying multiple
rules.
Some instantiations of Q1 leverage the relation hier-
archy in the KB. For example, What is the structure of
a cell? Here, the word structure maps to the has-struc-
ture relationship in our ontology, which has four sub-
relations: has-part, has-region, material, and possesses.
For the values returned for each of these relationships,
the system further retrieves spatial relationships to
complete the structural description.
In more complex forms of Q1, further constraints on the
values returned can exist. For example, consider: What does
X do during Y? Assuming that we are interested in those
steps such that X is a raw-material, those steps must also
satisfy an additional constraint that they must be sub-steps
of Y. Here, steps correspond to the phases of a process.
Q2. What are the subclasses of X? (e.g., What are the
subclasses of a eukaryotic cell?)d as follows: every Biomembrane has a function to allow Move-Through
h its Hydrophobic-Core, which is a region of its Phospholipid-Bilayer.
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 6 of 19
http://www.jbiomedsem.com/content/5/1/51Q2 is an example of a taxonomic query that queries
for all subclass relationships for a class. In AURA, this
query is answered by traversing the class-subclass hier-
archy. Other queries similar to Q2 are: What are the
super-classes of X? Is X a subclass of Y?
Q3. How many X does a Y have for a relation R? (e.g.,
How many chromosomes does a human cell have
as its part?)
Q3 queries for the cardinality constraints on the has-part
relationship for a human cell. AURA answers this query by
a straightforward lookup of cardinality constraints.
Q4. Describe X? (e.g., Describe a Cell?)
To answer Q4, AURA computes all the facts known
about a class. The facts about a class include taxonomic re-
lationships (i.e., its super-classes and subclasses as com-
puted in Q2); its relation values (as computed in Q1); and
its cardinality constraints (as computed in Q3). AURA
evaluates Q4 by issuing Q1, Q2, and Q3 as sub-queries,
and then organizes the results in a concept description
page.
Q5. What is the difference/similarity between X and Y?
(e.g., What is the difference/similarity between an
integral protein and a peripheral protein?)
AURA computes the answer to Q5 in three steps: (1)
computing descriptions of X and Y as explained in Q4,
(2) computing the similarities and differences between
the two descriptions, and (3) then summarizing the re-
sults. We have described the details of the computations
in a previous paper [25].
AURA supports more specific forms of Q5. For
example: What are the structural differences between X
and Y?; What is the difference between the size of X
and size of Y?; etc.
Q6. What is the relationship between X and Y? (e.g.,What
is the relationship between DNA and a gene?)
Here, we are interested in computing how the individual
instances of X and Y are related to each other. For ex-
ample, how is an individual instance of a DNA-Molecule
related to an individual instance of a Gene. One possible
answer to this question is that a DNA-Molecule has as its
part a DNA-Strand, which in turn, has as its part a Gene.
To answer Q6, AURA first creates an individual instance
of X and recursively computes its relation values (as in
Q1) until it encounters an instance of Y. In general, mul-
tiple such relationships exist in the KB that should be
ranked in the order of interest. AURA uses a variety ofheuristics to limit the search process (for example, first
searching the taxonomic relationships, preferring struc-
tural relationships, etc.).
AURA supports several questions that leverage the
computation supported in Q6. Examples include: What
are structural relationships between X and Y?; X is to Y
as A is to what?; and Why is it important that X has
property Y? To answer the question X is to Y as A is to
what?, AURA first computes a path between X and Y,
and then starting from A, traverses the same path to de-
termine the answer [32]. An example formulation of the
question Why is it important that X has property Y? is
How does the selective permeability of membranes facili-
tates its function? To answer this question, AURA com-
putes a path that begins from the permeability of a
membrane and ends at the function of the Membrane,
and that involves the relation facilitates [32].
We have implemented these reasoning methods in
AURA and have extensively tested them. In the first stage
of testing, we conducted a trial with students studying
from Inquire. This initial test was done for the chapter on
membranes. The results showed that the question tem-
plates were useful to the students, as the students using
the facility achieved higher scores than the students study-
ing from traditional methods, validating the choice of
question templates [6]. Once the question templates were
validated, we instantiated them for the first eleven chap-
ters. The test suite for each chapter was spread across the
content of the chapter and consisted of approximately 150
questions each. We executed the questions against AURA,
and the domain experts rated the answers for correctness.
From the 1,836 questions that we tested, the system cor-
rectly answered 1,540 questions, giving an overall correct-
ness score of approximately 85%. These results showed a
very high degree of system competence for answering
questions. (For example, IBMs Watson system that won
the television game show Jeopardy! had a passing rate in
mid-seventies [35].)
Textbooks used for comparison
We chose to compare four textbooks spanning a range of
breadth and depth of coverage (i.e., scope) based on the fol-
lowing rationale: choose one textbook comparable to
Campbell, one textbook at a grade level lower, one textbook
at a grade level higher, and one textbook at the advanced
graduate level. Specifically, we used (1) Raven, which rep-
resents a textbook with a similar scope to Campbell [36];
(2) Levine, which offers both less breadth and depth than
Campbell, and is used in a lower-division undergraduate,
non-major course [37]; (3) Alberts, which has a narrower
breadth, but a greater depth than Campbell, and is used in
an upper-division undergraduate class in cell biology, and
is considered a reference text for cellular and molecular
biologists [38]; and (4) Kandel, which targets the specific
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 7 of 19
http://www.jbiomedsem.com/content/5/1/51field of neuroscience, and is therefore, narrower in breadth
but has greater depth than Campbell, and also contains
additional topics such as cognitive scienceh [39]. Kandel is
a textbook written for advanced undergraduates, graduate
students, and medical students studying neuroscience, a
specialized field that is largely biological, but also concerns
itself with psychology and cognitive science. Kandel differs
from the other textbooks in that different authors who are
experts in their respective fields contributed most of the
individual chapters. This approach may lead to a less-
uniform treatment across the book than the other text-
books, which are each written by a small team of authors.
Data comparing the relative lengths of these four text-
books is summarized in Table 1 below.
Goals of research
We divided the high-level goal of investigating to what
extent do our current process and methodology for cap-
turing the semantics of textbook knowledge generalize to
a range of life sciences textbooks into the following three
more-specific questions: (1) To what extent is knowledge
shared between the different textbooks? (2) To what
extent can the same ontology be used to represent the
knowledge found in different textbooks? (Based on our
work with Campbell Biology, we were aware of many of
CLIBs limitations, especially because, from an AI perspec-
tive, fully capturing natural language text is an extremely
difficult problem. Our goal here was to quantify the extent
to which we could represent knowledge by using the exist-
ing CLIB vs. extending it to address any new require-
ments as we model different textbooks.) (3) To what
extent can the questions of interest for a new textbook be
answered by using the reasoning mechanisms already
available in AURA? Because the foundational set of ques-
tions is expected to be similar in all domains, we expected
good generality, but we wished to quantify it against each
textbook.
Methods
We now consider our methods for answering each of
the three specific questions introduced in the previous
section.Table 1 Data on page length and chapters in selected
textbooks
Textbook Pages Chapters Pages/chapter
Campbell 1263 56 23
Raven 1298 57 23
Levine 1034 45 30
Alberts 1728 45 69
Kandel 1316 67 20Domain analysis
The goal of domain analysis is to answer the question:
To what extent is knowledge shared between the differ-
ent textbooks? More specifically, we were interested in
understanding whether KB creation for each new book
should start from scratch or some knowledge from one
book could be shared from another. Answering this
question for the topics that appear in one textbook but
not in another is straightforward. Therefore, we selected
the topics of action potential and membrane structure,
which appeared in each of the four textbooks. The team
undertook a coarse analysis of the selected material and
selected a few paragraphs for detailed analysis. The team
compiled information such as the length of coverage, the
actual biological content covered, figures, and the type
of language used for describing the material. Such com-
parison gave us insight into the commonality of know-
ledge across different textbooks, and that information
guided us as to what extent we could share the domain-
specific content across the KBs for different textbooks.
Knowledge representation analysis
The goal of the knowledge representation analysis was to
answer the question: To what extent can the same upper
ontology be used to represent the knowledge found in dif-
ferent textbooks? Next, we give an overview of the AURA
knowledge-engineering process that was the basis of the
representation analysis, we provide an approach for deal-
ing with subject matter consensus, and we introduce
categories of representation requirements.
AURA knowledge-engineering process
We used an already established knowledge-engineering
process to represent the content of a textbook [31] as the
basis of this analysis. This process has two distinct phases:
(1) representation design and (2) knowledge encoding. For
the representation-requirements analysis, we performed
only the representation design phase, which includes the
following three steps: (1) determining relevance: analyze
each sentence in the textbook for its relevance for answer-
ing questions; (2) writing universal truths (UTs): for each
relevant sentence, paraphrase it as a universally true state-
ment about a specific entity or an event; and (3) develop-
ing action items for encoding: for each universally true
statement, identify the concepts and relations that will be
used for representing it.
We illustrate the above process by considering an ex-
ample sentence: Many cells, including most prokaryotes,
also produce a strong supporting layer around the mem-
brane known as a cell wall. Multiple UTs can be derived
from this sentence. One UT is: Many cells produce a cell
wall. The use of word many is also indicative of the fact
that there are some exceptions to this UT. To handle such
exceptions, our knowledge-engineering process dictates
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 8 of 19
http://www.jbiomedsem.com/content/5/1/51that this statement should be further specialized for cells
(for example, plant cells always produce a cell wall). Thus,
the UT will be reformulated as All plant cells produce a
cell wall.
Our general strategy to deal with exceptions is finding a
class for which that statement is applicable as a universal
truth. We ignore any exceptions that cannot be dealt with
by using such a strategy. With the CLIB ontology, the UT
under consideration will be represented by asserting that
every Plant-Cell is an agent of a process called Synthesis-
of-Cell-Wall, which has a result of Cell-Wall which is-
part-of the Plant-Cell. Here, agent, result, and is-part-of
are relations from the CLIB ontology. As a second ex-
ample, consider the UT: Every plant cell has a cell wall
that is a strong supporting layer. This UT will be repre-
sented by asserting that every Plant-Cell has-part a Cell-
Wall that has-function a Support that has an object the
Plant-Cell itself, and has an intensity value of strong.
Here, has-function, object, and intensity are relations in
the CLIB ontology. As a final example, consider the fol-
lowing sentence: A protoplast is a plant cell without a cell
wall. The UT for this sentence will be: Every protoplast
is a cell without a cell wall. Clearly, the sentence frag-
ment every protoplast is a plant cell cannot be univer-
sally true in our representation, because in that case,
Protoplast will inherit all the properties of a Plant-Cell in-
cluding a Cell-Wall. We will define Protoplast as a sub-
class of Cell in our class hierarchy. The relationship
between a Plant-Cell and a Protoplast will be captured by
other means.
Another central feature of AURAs knowledge-engineering
process is the division of labor between knowledge engi-
neers and domain experts: the knowledge engineers have
access to the full power of the representation language
which, as was explained earlier, is comparable to first-
order logic with equalitybut the domain experts create
only new classes, declare classes to be disjoint, specify
cardinality constraints, and, most importantly, author
existential rules of the sort visualized in Figures 1 and 2.
Achieving consensus among domain experts
Our approach to achieving consensus among the differ-
ent domain experts working on the project is driven by
the following observations: (1) Even for biological know-
ledge at the level of an introductory college course, no
two textbooks are exactly the same. (2) A textbook such
as Campbell has a large number of reviewers who are
able to approve the content of the textbook. (3) Despite
the differences in the textbooks, the students can be
evaluated using a common test, and their answers can be
rated. The key lessons that we drew from the textbook-
authoring process is to aim for a process in which the
project experts could review a representation and have
an objective test for evaluating the knowledge in thesystem. We developed an extensive set of knowledge-
engineering guidelines that prescribe how the domain
experts should go about capturing textbook knowledge
[31,40]. Just as a textbook undergoes a review process, the
representations undergo a review process that ensures an
adequate application of the guidelines. This review does
not mean that a representation meets an experts personal
view on how the knowledge should be modeled, but ra-
ther ensures that the established encoding guidelines are
adequately applied. Question and answer pairs stated in
English provide a natural objective test to check the ad-
equacy of the representation in the same way as students
can be objectively tested on an exam.
Inventory of representation requirements
The representation requirements can be put into two
categories: (1) requirements that are already supported in
CLIB and (2) requirement that are not currently sup-
ported. When we cannot model a universal truth in a
straightforward manner by using the constructs available
in the CLIB, we note this as a new KR requirement. The
new KR requirements are strongly dependent on the state
of CLIB at the time of the analysis. For answering the
question of whether the same upper ontology could be
used across multiple textbooks, however, the primary issue
is the applicability of the representations supported in
CLIB and the commonality of each new requirement
across different textbooks.
KR requirements can arise due to the following rea-
sons: (1) The knowledge can be represented by using the
current features of the representation language and
CLIB, but no established knowledge-engineering guide-
lines exist to handle it. We refer to the challenges arising
due to this reason as process issues. (2) Representing the
knowledge requires intervention from a knowledge engin-
eer to extend the upper ontology. We refer to the issues
arising due to this reason as requiring knowledge-engineer
support. (3) Representing the knowledge is a topic of
current and future research, and the current research has
not yet been incorporated into the project. We refer to
such issues as requiring research and application. We now
give an inventory of the KR requirements that were en-
countered during the process, and we indicate into which
of the above three categories each requirement fell.
Negative information
We say that a UT has a negative information KR issue if
it cannot be modeled by using any of the four existing
methods for handling negative information: (1) disjoint-
ness between classes, (2) cardinality constraints, (3) rela-
tions with negative meaning, and (4) negative values. As
an illustration, consider the following sentence from
Raven: Because these chains are nonpolar, they do not
form hydrogen bonds with water, and triglycerides are
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 9 of 19
http://www.jbiomedsem.com/content/5/1/51not water-soluble. Here, we can state that a polar mol-
ecule is disjoint from a nonpolar molecule (to capture
the nonpolarity), and we can assign a value of insoluble
to the property solubility-in-water. In principle, one
could introduce a slot with negative meaning (for example,
does-not-form, or use a qualified number constraint on
all Create processes in which Nonpolar-Chains participate
that asserts that the result contains exactly zero Hydrogen-
Bonds). However, no established methodology exists re-
garding which approach to use. Therefore, dealing with the
example of negative information considered here is a
process issue.
Missing relationships
We say that a UT cannot be expressed because of a missing
relationship if the necessary relationship is missing from
the vocabulary. An issue already known based on our work
with Campbell is the lack of certain spatial relationships.
As an illustration of this issue, consider the following sen-
tence from Raven: Although the distribution of membrane
lipids is symmetrical in the ER where they are synthesized,
this distribution is asymmetrical in the plasma membrane,
Golgi apparatus, and endosomes. Here, we need a new
relation to capture asymmetrical distribution. Missing rela-
tionships require knowledge-engineer support.
Inability to state graded quantifiers
Recall that whenever the textbook uses words such as
many, most, typically, etc., our KE strategy is to
find a more-specific subclass for which the statement is
universally true. This strategy breaks down when the
textbook does not contain information about such a spe-
cific subclass. For example, consider the following sentence
from Levine: Most prokaryotes and many eukaryotes have
cell walls. The main difference between this sentence and
the sentence: Many cells, including most prokaryotes, also
produce a strong supporting layer around the membrane
known as a cell wall, which we considered earlier, is the
that Levine does not offer any specific examples of cells
that do contain cell walls, so we cannot apply our KE strat-
egy that worked for the earlier sentence. Whenever we
encounter such a situation, we label it as an inability to
state graded quantifiers, and it is a research and applica-
tion issue.
Modeling biological models and reified statements
The textbooks frequently describe models and theories
about natural phenomena. The statements about models
are not universally true statements, but instead are
contextual statements that hold true only in the context of
that model. As an illustration, consider the following state-
ment from Alberts: These regions cannot be identified in
hydropathy plots and are only revealed by x-ray crystallog-
raphy, electron diffraction (a technique similar to x-raydiffraction but performed on two-dimensional arrays of
proteins), or NMR studies of the protein's three-dimensional
structure. Here, the presence of the regions is contextual
to a particular set of techniques. Such knowledge can be
captured in AURA, but the relevant guidelines have not
been developed yet, and therefore, it is a process issue.
Property value comparison
A need frequently exists to compare property values. The
CLIB ontology contains several comparison operators for
properties, but we saw some examples where none of the
existing operators were directly applicable to some sen-
tences in the new textbooks. For example, consider the fol-
lowing sentence from Raven: However, at the end of each
action potential, the cytoplasm contains a little more so-
dium and a little less K than it did at rest. Here, we
need qualitative operators to capture relationships such
as little more and little less. This issue requires
knowledge-engineer support.
Causation
The notion of causality associated in the context of pro-
cesses where causal relationships of events are of primary
interest is already supported in CLIB. The textbook very
often explains things by using the words such as be-
cause, causes, etc. We use the category label of caus-
ation to capture such issues as the current CLIB does not
provide support to model such information. For example,
consider the following sentence from Alberts: The shape
and amphiphilic nature of the phospholipid molecules
cause them to form bilayers spontaneously in aqueous
environments. This KR requirement requires both research
and application.
Disjunction
A need arises to capture two or more alternatives in a
UT that cannot be modeled by another means. For
example, consider the following sentence from Alberts:
Hydrophilic molecules dissolve readily in water because
they contain charged groups or uncharged polar groups
that can form either favorable electrostatic interactions or
hydrogen bonds with water molecules. This KR require-
ment requires both research and application.
Conditionality
Capturing a conditional statement in a UT that cannot be
modeled by another means is sometimes necessary. Our
general approach for capturing conditional statements has
been using the class hierarchy. We create a new class, and
the if part of the condition becomes a sufficient property
for that class, while the else part of the condition
becomes the necessary properties of that class. Such an
approach works for most situations; but in some cases, it
leads to unnatural classes, and thus is undesirable. For
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 10 of 19
http://www.jbiomedsem.com/content/5/1/51example, consider the following sentence from Alberts:
This change of state is called a phase transition, and the
temperature at which it occurs is lower (that is, the mem-
brane becomes more difficult to freeze) if the hydrocarbon
chains are short or have double bonds. Here the condi-
tionality is between the temperature and the properties of
hydrocarbon chains. If we model this knowledge by using
sufficient properties, then creating unnatural classes, such
as phase transition for short hydrocarbon chains, would
be necessary. Handling this requirement is a process issue.
Possibility
Many sentences make statements of the form A can B,
without necessarily stating that A always does B. We
refer to the representation needs of such sentences as
possibility. For example, consider the following sen-
tence from Alberts: The free hydroxyl group contrib-
utes to the polar properties of the adjacent head
group, as it can form hydrogen bonds with the head
group of a neighboring lipid, with a water molecule,
or with a membrane protein. Dealing with this KR
requirement is a research and application. Initial steps
in this direction could be undertaken by using research
result on representing dispositions [41].
Data interpretation
In the advanced textbooks, figures are shown that
contain representative data. The text then describes the
form of the data and what conclusions either were or
could be derived from this data. Thus, the figures are
not just meant to illustrate a model but also to teach
students how the actual data led to a set of conclusions.
As an illustration, consider the following sentence from
Alberts: In a normal unclamped axon, an inrush of
Na + through the opened Na channels produces the
spike of the action potential; inactivation of Na chan-
nels and opening of K channels bring the membrane
rapidly back down to the resting potential. Dealing
with this requirement is a research and application
issue.
Science as a process
Particularly in Kandel and also in Alberts, many of the
biological concepts are presented in the context of the
process of science, i.e., scientists go through a process of
testing, interpreting data, and developing hypotheses
that are then tested again. For example, consider the
following sentence from Kandel: A simple interpretation
of these results is that the depolarizing voltage step se-
quentially turns on active conductance channels for two
separate ions: one type of channel for inward current and
another for outward current. Dealing with this require-
ment is a research and application issue.Qualitative number constraint
Our current representation approach enables quantita-
tive number constraints. We saw several examples in the
textbooks where the constraint values are qualitative, and
no other encoding approach sufficed. For example, con-
sider the following example from Raven: Mammalian
membranes, for example, contain hundreds of chemically
distinct species of lipids. Dealing with this requirement
requires knowledge-engineer support.
Mathematical reasoning
CLIB provides two different representations to facilitate
mathematical reasoning: (1) simple qualitative relationships
such as direct proportionality and (2) reasoning with math-
ematical equations. However, Kandel presents more com-
plicated equations beyond CLIBs current representational
and reasoning capabilities. Kandel also includes derivations
of mathematical formulas that cannot be represented by
using current capabilities. For example, consider the follow-
ing sentences from Kandel: When tetraethylammonium is
applied to the axon to block the K+ channels, the total
membrane current lm, consists of lc, lv and lNa. This outward
current reaches a plateau that is maintained for the dur-
ation of the pulse (Figure nine-3B). Dealing with this re-
quirement requires knowledge-engineer support.
Vagueness/ambiguity
Advanced textbooks cover frontiers of our knowledge,
and hence, this vagueness or ambiguity is not due to
pedagogical presentation. However, it can lead to a uni-
versally true statement that is relevant but too vague to
properly encode. These sentences are found across all
textbooks, and seem to be more common in Alberts.
(For example: Membrane attachment through a single
lipid anchor is not very strong, however, and a second
lipid group is often added to anchor proteins more firmly
to a membrane.) Dealing with this requirement requires
research and application.
Other issues
We use the KR category of other issues for representation
problems that do not clearly fit into any of the previous cat-
egory. For example, consider the following sentence from
Raven: From this simple molecular framework, a large var-
iety of lipids can be constructed by varying the polar organic
group attached to the phosphate and the fatty acid chains
attached to the glycerol. Here the author is trying to con-
vey the salient variance between different phospholipids.
Certain aspects of this knowledge are easily captured as
sufficient properties, but that approach may not always be
enough, especially to answer a question of the form How
can you get different instances of a phospholipid? For the
purposes of answering similarity and difference questions,
Table 2 Data on the length of description of action
potential and membrane potential
Action potential Membrane structure
Textbook Pages Images Sentences Pages Images Sentences
Campbell 7 7 91 6 12 160
Raven 10 9 58 6 5 91
Levine 2 2 37 2 1 17
Alberts 14 14 20 12 18 270
Kandel 20 16 280 4 1 75
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 11 of 19
http://www.jbiomedsem.com/content/5/1/51and relationship questions, a representation based on
sufficient properties is adequate.
Reasoning requirements analysis
The goal of the reasoning requirements analysis was
answering the question: to what extent can the questions
of interest for a new textbook be answered by using the
reasoning mechanisms already available in AURA? We
wanted to confirm that as we move across textbooks, we
would not have to develop new sets of reasoning methods
for answering questions. To perform the analysis, the
domain-expert team developed sample questions about
membrane structure and action potential for each of the
four textbooks. The overall guidance was to focus on the
kinds of questions that a student studying from the book
might have. The biologists had access to the examples of
educationally useful questions that we had previously
developed for Campbell. Some variability in the style and
difficulty of questions potentially exists, because we did
not have a mutual validation of question sets authored by
different biologists. The possibility also exists that we
biased their question-authoring effort by showing them
the questions from the prior effort on Campbell. However,
because the questions from the previous effort received
extensive feedback from multiple teachers and students,
we believe that they were a good guideline for this exercise.
The domain-expert team and the knowledge-engineering
team jointly analyzed the questions.
The questions stated in English needed to be translated
into the question templates supported by the system. Such
translation is done by AURAs question-understanding
module [42]. In many cases, the English statement of a
question is not very helpful for determining the computa-
tion that must be performed in answering that question.
For example, consider the question: How does the pos-
ition of the gates in gated proteins cause the blocking of the
movement of ions across the membrane? We can re-
formulate this question as: What is the causal relation-
ship between the position of the gates in gated proteins and
the blocking of the movement of ions across the mem-
brane? Another formulation of the same question is:
How are the position of the gates in gated proteins and
the blocking of the movement of ions across the membrane
causally related? In AURA, both of these formulations
will be handled by using Q6, in which we search for
the causal relationships between the two entities in
the question, and we expect the answer to be contained
in the retrieved path. To develop such reformulations, the
knowledge engineers must extensively rely on their know-
ledge of AURA to determine whether a given question in
the corpus could be translated into one of the existing
templates. This approach introduces some imprecision
into the analysis, but this is unavoidable without undertak-
ing the actual implementation.Results and discussion
We now consider the results of our analysis of domain
knowledge, and representation and reasoning require-
ments, for the four textbooks.
Results and discussion on domain knowledge
analysis
We first analyze the two topics that we chose for compari-
son: action potential and membrane structure, and then
offer conclusions based on the analysis.
In Table 2, we summarize data about the length of de-
scription of the different topics across the five textbooks.
To the extent that different textbooks emphasize differ-
ent levels of detail, the corresponding KBs need to match
that level of detail. To make this observation concrete, we
consider below specific example comparisons of content
across the three textbooks.
Campbell covers membrane structure in greater depth
than Levine, Raven, or Kandel, but is limited in its
description of the molecular structure of phospholipids.
Raven and Alberts devote more detail to the molecular
structure of phospholipids. Levine introduces lipids but
has no mention of their more specific forms, such as
glycolipids, which are mentioned in the other textbooks.
In Kandel, membrane structure is not a major topic (it
is more a topic in general biology than in neuroscience).
Campbell describes equilibrium potential by providing a
definition and presenting an equation for the mathemat-
ical model known as the Nernst equation, along with two
examples using this equation. Raven provides a similar
amount of information to Campbell, but omits any exam-
ples using the Nernst equation. Alberts provides a defin-
ition, derives the Nernst equation, and shows several
examples. Kandel provides the greatest breadth and depth
for membrane potential, and devotes an entire chapter
(Chapter 8) to the passive electrical properties of the
neuron that are important for understanding the influence
of neuronal structure and other properties on short and
long-range signaling. Kandel also covers the contribution
of different types of membrane channels to the signaling
properties of different parts of the neuron.
Next, we consider the biological themes that occur
inconsistently across our sample of textbooks: evolution,
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 12 of 19
http://www.jbiomedsem.com/content/5/1/51disorders/disease, scientific uncertainty, and animal models.
Campbell, Raven, and Levine do not mention evolution in
the context of action potential, but Alberts and Kandel
discuss evolution of action potential function and structure
of membrane proteins, respectively. Although Campbell
and Raven omit discussion of disease, Levine, Alberts, and
Kandel provide examples of diseases that affect normal
functioning of action potentials. The presentation of
scientific uncertainty also varies considerably across text-
books. Raven omits any mention of scientific uncertainty
in the context of action potential, while Campbell and
Levine simply report its existence. Alberts suggests that
scientists will resolve uncertainty without exception, but
Kandel presents scientific inquiry with respect to action
potential as an iterative process with some degree of un-
certainty. Animal models for the study of action potential
are not described in Levine, and a single experimental
model is described in both Raven and Campbell. Although
Kandel describes a single experimental model, the giant
squid axon, this text also emphasizes experimental tech-
niques and their specific role in elucidating aspects of the
action potential. Alberts describes multiple experimental
models for the study of action potential.
The examples above suggest a great deal of common-
ality as well as differences in how different topics are de-
scribed across the textbooks. For example, on the topic
of membrane structure, the KB for Levine will contain
far fewer terms than the other KBs (e.g., terms such as
Glycolipid would need to be omitted.) Similarly, the KB
for Alberts and Raven will provide a much more detailed
account of phospholipid structure than the KB forTable 3 Observed knowledge representation issues
Category of KR issue Occurs in textbooks
Levine Raven
Negative information x
Spatial relation x x
Missing slot (other than spatial relation) x
Inability to state graded quantifiers x x
Biological models and reified statements x x
Property-value comparison x
Causation
Disjunction x
Conditionality
Possibility
Data interpretation
Science as a process
Qualitative number constraint x
Mathematical reasoning
Vagueness/ambiguity x
Other xCampbell. Similarly, while the Nernst equation will
exist in all the KBs, the example associated with its use
(as in Alberts), and a description of electrical properties (as
in Kandel), will be specific to the KBs for those textbooks
only. Differences in how to handle evolution, uncertainty,
diseases, and animal models can have major repercussions
in KB design.
Our analysis above suggests that a great deal of com-
monality across textbooks can be leveraged in creating a
KB for each of them. At the minimum, the experience and
representation approaches developed for one textbook can
contribute toward a faster design of representations for a
different textbook. Our analysis does not provide sufficient
information about whether the domain-specific axiom
writing for the textbook for a new KB should begin from
scratch or should reuse the axioms from the previous
ones. Clearly, some reuse should be possible, but the ex-
tent of reuse and its cost effectiveness is an open question.
Further, our analysis provides concrete examples of where
the textbooks have substantial differences requiring repre-
sentation design that is specific to that textbook.
Results and discussion on knowledge representation
requirements
In Table 3 below, we summarize all the KR issues along
with the textbooks for which the issue was encountered.
The column labeled as New issue indicates an issue
that we have not encountered or so far addressed in our
work with Campbell Biology.
In Table 4 below, we show the results that indicate the
number of UTs for each of the textbooks that could notOccurs in
Campbell?
New
issue?Alberts Kandel
x x x No
x x x No
x x x No
x x No
x x x No
x No
x x No
x x No
x x No
x x No
x x Yes
x x x No
x No
x x No
x x x No
x No
Table 4 KR requirements by category, for the topic action potential
Category of KR/KE issue Number of UTs affected (%)
Levine Raven Alberts Kandel
Negative information 3 (8%) 6 (6%) 3 (2%)
Spatial relation 3 (2%)
Missing slot (other than spatial relation) 4 (8%) 3 (3%) 3 (2%) 6 (7%)
Inability to state graded quantifiers 1 (1%) 2 (2%)
Modeling biological models and reified statements 1 (1%) 3 (3%)
Property-value comparison 3 (3%)
Causation
Disjunction 1 (3%)
Conditionality
Possibility 11 (8%)
Data interpretation 11 (8%) 4 (4%)
Science as process 8 (9%)
Qualitative number constraint
Mathematical reasoning 3 (3%)
Vagueness/ambiguity 1 (1%) 2 (1%)
Other
Total 8 (21%) 13 (13%) 35 (26%) 26 (30%)
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 13 of 19
http://www.jbiomedsem.com/content/5/1/51be adequately represented for the topic of action poten-
tial. For each UT that could not be represented, we iden-
tify a knowledge representation category to indicate the
nature of requirement. We next explain these results for
each of the textbooks.
For Levine, approximately 20% of UTs for action po-
tential had new KR requirements. In addition, negative
information that could not be adequately encoded oc-
curred for action potential, and we encountered one
instance of disjunction that could not be adequately
encoded. The issues of lack of specificity and models did
not arise for action potential for this text. For the Raven
textbook, 13% of the UTs were problematic. For Alberts,
approximately 25% of UTs presented new KR requirements
for action potential. In addition, the new KR requirement
of data interpretation arose. In Kandel, approximately 30%
of UTs presented new KR requirements. Thus, like Levine
but unlike Raven and Alberts, Kandel presented propor-
tionally more issues for action potential. For example, data
interpretation issues and science as process issues arose
frequently. Further, for action potential, Kandel contained
sentences outside AURAs current mathematical represen-
tation and reasoning capabilities.
In Table 5 below, we show our results of how well we
could represent the topic of membrane structure for
each of the four textbooks. Detailed explanations follow.
For Levine, we encoded approximately 85% of UTs with-
out any facing any new requirements. The most common
new KR requirements were missing relations (namely,
spatial relations), and the inability to identify a sufficientlyspecific concept, as illustrated in the earlier example.
Raven exhibits a greater percentage and breadth of new
KR requirements than Levine. Nearly 35% of UTs did have
new KR requirements. The most common KR require-
ments were, again, specificity of concepts and missing
slots. A common requirement for Raven was representing
biological models. Raven (and the other textbooks) had
several examples of negative information of a form that
cannot be represented with AURAs current capabilities.
Alberts has a similar percentage of new KR requirements
to Raven and a greater breadth. Again, more than 30% of
UTs posed some new KR requirement. Further require-
ments come from conditionality, causation, and possibility.
Because Alberts is a research-oriented textbook, it de-
scribes topics at the limit of current biological knowledge.
This leads to the greater number of UTs with the KR issues
of vagueness compared to other textbooks. For Kandel,
more than 80% of UTs were encoded without facing any
new KR requirement, and no new requirements arose that
did not arise for another textbook, except the need to
represent knowledge about science as a process. Hence, in
terms of number of issues, Kandel proved amenable to our
KE process despite its more advanced nature.
Let us now consider how these results address the ques-
tion: to what extent can the same upper ontology be used
to model knowledge across a range of life science text-
books? The results in Table 3 suggest that all the require-
ments that were identified for the new textbooks, with the
exception of data interpretation, were also requirements
for Campbell. This finding is strong evidence in support
Table 5 KR issues by category, for the topic membrane structure
Category of KR/KE issue Number of UTs affected (%)
Levine Raven Alberts Kandel
Negative information 3 (1%) 17 (7%) 2 (2%)
Spatial relation 3 (4.5%) 17 (7%) 15 (6%) 2 (2%)
Missing slot (other than spatial relation) 6 (2.5%) 8 (3%) 4 (4%)
Inability to state graded quantifiers 6 (9%) 28 (12%) 10 (4%)
Modeling biological models and reified statements 1 (1.5%) 19 (8%) 1 6 (7%)
Property-value comparison
Causation 2 (1%)
Disjunction 1
Conditionality 5 (2%)
Possibility 3 (1%)
Data interpretation
Science as process 2 (2%)
Qualitative number constraint 1
Mathematical reasoning
Vagueness/ambiguity 2 15 (6%)
Other 3 (1%)
Total 10 (15%) 79 (33%) 77 (31%) 16 (18%)
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 14 of 19
http://www.jbiomedsem.com/content/5/1/51of the claim that if these requirements were supported in
an upper ontology, such ontology would be applicable
across multiple textbooks. From Table 3, we also see that
spatial relationships and biological models are the require-
ments that occur most uniformly across the textbooks,
followed by negative information, graded quantifiers, and
science as a process. These constitute high-priority areas
for extending the CLIB ontology.
From Tables 4 and 5, we see that the existing upper
ontology enabled us to capture at least 67% of all the UTs
across all topics and across all the textbooks. In some
cases, the coverage was as high as 87%. Based on these re-
sults, we can conclude that CLIB already provides a good
foundation for representing knowledge across the range of
life science textbooks considered here.
Results and discussion on reasoning requirements
Recall that our high-level question regarding reasoning
requirements was: To what extent can the questions of
interest for a new textbook be answered by using the
reasoning mechanisms already available in AURA? We
gave an overview of the current questions supported in
an earlier section.
To answer the above question, we assembled a suite of
new questions for each of the four textbooks and put
them into two different categories: (1) answerable with
existing system capabilities, or minor extensions of them,
supposing that the requisite concepts are encoded; and
(2) require new reasoning capabilities, or major extensionsof existing capabilities, or beyond anticipated feasible
reasoning, or contingent on significant new research.
We will now present the results of our analysis and
will illustrate the questions that fall into each of these
categories.
In Table 6 below, we summarize the overall analysis of
questions about action potential and membrane structure.
Across the four textbooks on average, we observe that
for action potential, approximately 85% of the questions
are category 1 (existing capability), and 15% are category
2 (representational extension or significant reasoning re-
quirements). For membrane structure, nearly 90% of the
questions are category 1 (existing capability), and 10%
are category 2 (representational extensions or significant
reasoning requirements).
From each of the four textbooks, we now give example
question forms that could be answered by using the exist-
ing capability. For each question form, we give an example
question, its model answer if provided, and a reformula-
tion of the question. Because each of these question forms
can be answered by using the existing capability (or a
minor extension of it) through the given reformulations,
new question templates are not required.
? Question template in English: What is the role of X
(in context Y)?
? Example instantiation from the sample question set:
In the equivalent electrical circuit model, what
cellular element serves as the resistor? [Kandel]
Table 6 Reasoning requirements analysis for action potential and membrane structure
Action potential Membrane structure
Textbook Questions Existing Research Questions Existing Research
Raven 16 12 4 21 17 4
Levine 53 43 10 32 25 7
Alberts 19 16 3 48 47 1
Kandel 50 43 7 29 26 3
Total 138 114 22 130 115 15
The column labeled as Questions indicates the total number of questions considered in the analysis. The column labeled as Existing indicates the number of
questions that could be handled by using existing capabilities in AURA, and the column labeled as Research indicates the number of questions that cannot be
handled by the current capabilities in AURA and that require further research.
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 15 of 19
http://www.jbiomedsem.com/content/5/1/51? Question template in English: Why is it important
that X has property Y?
? Example instantiation from the sample question set:
Why is it important that membranes are selectively
permeable? [Levine]
? Reformulate as: How does the selective
permeability of membranes facilitate its function?
? Question template in English: What kinds of X are
common in Y?
? Example instantiation from the sample question set:
What kinds of lipids are common in cell
membranes? [Levine]
? Reformulate as: What are the lipid parts of a cell
membrane?
? Question template in English:What does X do during Y?
? Example instantiation from the sample question set:
What is the sodium potassium pump doing during
an action potential? [Levine]
? Reformulate as: What does sodium potassium do
during an action potential?
? Question template in English: What features of X
affects its role in Y?
? Example instantiation from the sample question set:
What features of the voltage-gated sodium channel
affect its role in an action potential? [Raven]
? Reformulate as: What is the relationship
between a voltage-grated sodium channel and
action potential? (This reformulation is
approximate as it does not specifically ask for
the relationship to role in the action potential.)
We now consider example questions that require new
question templates. For each, we give an example ques-
tion template and its instantiation.
? Question template in English: What is the importance
of X?
? Example instantiation from the sample question set:
What is the importance of plasma membrane
fluidity? [Alberts]? Question template in English: What aspects of X can
be seen by Y? (where Y is a inspection technique,
instrument, or process)
? Example instantiation from the sample question set:
What aspects of the plasma membrane can be seen
by transmission electron microscopy (TEM)? [Raven]
? Question template in English: What properties of X
contribute to the property Z of Y?
? Example instantiation from the sample question set:
What characteristic of phospholipids contributes
most to the membrane-forming properties of these
molecules? [Alberts]
? Question template in English: Given that X does Y,
why does Z also not do Y?
? Example instantiation from the sample question set:
Given that the sodium-potassium pump results in a
net transport of positive ions from the inside of the cell
to the outside, why don't negative ions also leave the
cell to balance out the charge difference? [Raven]
? Question template in English: Which strategy does X
use to achieve Y?
? Example instantiation from the sample question set:
Vertebrate systems generally rely on what adaptive
strategy for increasing the rate of axonal
conduction? [Kandel]
The quantitative results in Table 6 support the conclu-
sion that a large fraction of the questions in the test
suite assembled by the domain experts (greater than
85%) for a new textbook could be answered by using the
reasoning mechanisms already available in AURA. This
finding is an extremely positive result that attests to the
generality of the already-implemented reasoning mecha-
nisms. However, we would like to emphasize that given
the bias introduced by exposing the domain experts to
the existing capabilities, we should not take these results
to conclude that the existing capabilities could answer
greater than 85% of all possible questions posed against
these textbooks. These results are applicable to only to a
specific style of educationally useful questions that have
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 16 of 19
http://www.jbiomedsem.com/content/5/1/51been found helpful in our work on the intelligent text-
book. These results show that such questions have a high
degree of generality and applicability across the range of
textbooks considered in this analysis.
Comparison to related work and broader impacts
In this section, we relate the work presented here to re-
lated efforts in modeling knowledge by using OWL and
other biomedical ontology development efforts. We also
comment on how our work can be exploited by others.
Most of the representation features used in AURA are
also found in OWL (for example, classes; class-subclass
relationships; disjoint statements between classes; domain;
range; qualified number constraints; etc.). Our work to
capture graph-structured knowledge of the sort illustrated
in axioms A1A4 is closely related to recent efforts to
extend OWL to capture graph-structured descriptions
[17]. Others have recognized the need to support graph-
structured descriptions to capture chemical structures
[16], and active research is underway to address it [17-20].
KB Bio 101 already contains several hundred examples of
complex concepts that utilize such graph-structured repre-
sentation [13], such as the ones shown in Figures 1 and 2.
One possible technique to achieve decidable reasoning in a
KB with graph-structured descriptions is to avoid certain
kinds of cyclical dependence among concepts [17], but
no empirical evaluation exists of such a technique on a
realistic, large-scale dataset. KB Bio 101 is an excellent
candidate data set for undertaking such evaluation. More
generally, KB Bio 101 can be used as a dataset for testing
techniques for ontology modularization, ontology map-
ping, ontology evaluation, development of ontology design
patterns, etc.
In several prior publications, we related the represen-
tations supported in CLIB with the ones adopted for bio-
medical ontologies (for example, in [32], we describe our
representation for structure and function; in [43], we de-
scribe representation of roles; and in [44], we describe the
representation of genetic entities). Gene Ontology or GO
[45] is a closely related community-wide effort that sup-
ports molecular-level and cellular-level representations for
gene function. Because life science textbooks cover know-
ledge at organismal, species, and population levels, the
scope of knowledge represented in KB Bio 101 is much
broader than the knowledge represented in GO.
A unique feature of our ontology that none of the other
biomedical ontologies supports is a vocabulary of process
classes (e.g., Move, Attach, Release, etc.) and their detailed
definitions using semantic relationships (e.g., agent,
object, source, destination, etc.). Due to lack of such
vocabulary, ontologies such as GO define functions using
only textual strings and functions are not compositionally
defined to capture their complete meaning. The CLIB
approach to modeling processes and their participantscan be readily exploited by biomedical ontologies to
achieve a much greater depth of knowledge capture for
biological functions.
A driving use case for GO, and a major contributor to
its success, has been its use in annotation projects. The
question templates Q1Q6 introduced in our work can
provide another compelling use case for exploiting GO
and other biomedical ontologies. Although Q1Q6 were
driven by the needs of education applications, similar
reasoning can be useful for biological discovery applica-
tions such as [46].
Many educational innovations begin at the graduate
level, and slowly find their way to undergraduate and
precollege-level education. Therefore, perhaps, the most
impactful way to exploit this work is using it as an ex-
ample to start incorporating biomedical ontologies into
undergraduate and high-school-level curricula for life sci-
ence education. Future life sciences graduates will need to
routinely use ontology resources, and some of these gradu-
ates will need to help create new ones. However, ontologies
are not yet a standard part of the life sciences curriculum.
Students are not normally exposed to ontologies unless
they enter a graduate program in bioinformatics. We
believe that now is the time to begin making training in
formal languages and their ontological commitments an
integral part of the life sciences curriculum. Wider use of
ontologies in the life sciences will lead to better under-
standing and communication of knowledge by teachers
and students. Such explicit usage of ontologies is different
from the methods used by search tools such as Google,
which are excellent for retrieval but do little to improve
our understanding of the subject matter.
Conclusions
We present our conclusions for each of the three major
analyses presented here: (1) domain knowledge require-
ments, (2) knowledge representation requirements, and
(3) reasoning requirements. We acknowledge at the out-
set that our conclusions are based on the data gathered
for the topics of action potential and membrane struc-
ture. Our generalized conclusions are based on the hy-
pothesis that these data could be generalized to other
biological topics in the textbook.
The results of our domain requirements analysis show
that, as expected, the Levine textbook, which is aimed at a
lower instructional level than Campbell, presents material
from a more general perspective, omitting details that
Campbell and Raven include. Likewise, the textbooks
aimed at a higher instructional level than Campbell present
details that Campbell does not. The breadth and depth of
coverage for action potential and membrane structure
appear most similar between Campbell and Raven. We also
found that the textbooks for instruction levels higher than
Campbell and for a specific field of biological sciences do
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 17 of 19
http://www.jbiomedsem.com/content/5/1/51not cover the broad range of knowledge in Campbell but
instead rely on Campbells prerequisite biology knowledge,
and build on a fraction of this foundation. For example,
Kandel provides considerably less breadth on the topic of
membrane structure compared to Campbell. The details of
membrane structure are likely omitted from Kandel be-
cause the authors deem such information as prerequisite or
not germane to the sub-discipline of neuroscience. Our re-
sults suggest that the modeling effort invested in represent-
ing any of these books will reduce the cost of doing
additional books. Because the considered textbooks vary in
detail, and in their choice of the aspects of biology know-
ledge to emphasize, the KB for each of these textbooks also
must be customized and made specific to that particular
textbook.
The results of our knowledge representation require-
ment analysis showed that the knowledge-engineering
process used for Campbell appears to be effective across
the range of considered textbooks. We encountered no
major surprises regarding modeling issues: most of
the issues that we saw in these textbooks also exist in
some form for Campbell. We confirmed that the stud-
ied textbooks that were written for the same grade level
(i.e., Campbell and Raven) were comparable in their
knowledge content and representation requirements. We
found an increase in presentations of theories, models,
and history in the higher-level textbooks, which is ex-
pected as the textbooks for the higher grade levels are
closer to the frontiers of knowledge. For example, Kandel
describes the experiments that are used to test a model or
hypothesis, and the reasoning process that was used to
support or refute that model. Our overall conclusion was
that our existing representation tools are applicable for
modeling knowledge across the range of considered text-
books, and that the new requirements identified here will
have broad applicability to multiple textbooks.
Based on the reasoning requirements analysis, we
can conclude that a majority of the biologist-authored,
educationally useful questions for each of the textbooks
can be adequately addressed by using extensions to
AURAs current capabilities. This assertion is true because
all the textbooks had the same foundational set of ques-
tions and were all based on the same foundational biology.
The reasoning patterns of relationship questions and com-
parison questions seem to be directly applicable across
multiple textbooks. We also found that the answers for
one textbook may contain vocabulary or detail that is
unexpected at a different grade level. For example, Levine
does not use the term phospholipid bilayer. In Kandel
and Alberts, most answers are with respect to models and
cannot be considered as universally true. We definitely
cannot conclude that the existing question templates are
adequate for the space of all questions that the readers of
each of the textbooks might want to ask. Our previouswork with Campbell also showed us that the existing
templates are inadequate for capturing all the reasoning
patterns.
A possible way forward is aligning the presented repre-
sentations and approach with the methods that are
already commonplace in biomedical research, and then
start incorporating those representations in life sciences
textbooks. As an example, consider the representation
for Kandel. Especially for a field as broad as neurosci-
ence, different groups will need to be engaged for differ-
ent parts of a text like Kandel. In fact, in this textbook,
different experts author different chapters to ensure that
the content aligns with current thinking in the field.
Efforts are underway through projects like the International
Neuroinformatics Coordinating Facilityi, the Blue Brain
Projectj, and the Neuroscience Information Frameworkk
to create a semantically unified body of broad neurosci-
ence knowledge. When textbook knowledge is comple-
mented with resources like these, the enhanced version is
not only useful for biomedical research but can also serve
as a valuable education tool.
Undertaking textbook knowledge representation as pro-
posed here will profoundly shift the way we think of life
science education. The semantic representations would
serve as a conceptual mathematics that computers could
rigorously reason over. Exposure to such representations
as part of a life science education will likely instill grad-
uates with an increased level of rigor in learning and
working with biological concepts. The time is now
ripe to introduce these techniques at all levels of biol-
ogy education, so that students are well prepared for the
computational thinking [47] that is both so vital to
practitioners in todays knowledge economy and indis-
pensable for researchers pursuing advanced biomed-
ical discoveries.
Endnotes
ahttp://www.aaaivideos.org/2012/inquire_intelligent_
textbook/
bAlmost every universally true statement in biology
has an exception. For example, there are eukaryotic cells
that do not have a nucleus.
chttp://www.w3.org/TR/owl2-syntax/
dhttp://www.tptp.org
ehttp://www.ai.sri.com/~halo/public/exported-kb/biokb.
html
fhttps://bioportal.bioontology.org/ontologies/AURA
ghttp://www.ai.sri.com/~halo/public/clib/20130328/clib-
tree.html
hThe 5th edition of Kandel appeared when our study
was underway.
ihttp://incf.org
jhttp://bluebrain.epfl.ch
khttp://neuroinfo.org
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 18 of 19
http://www.jbiomedsem.com/content/5/1/51Abbreviations
AURA: Automated User Centered Reasoning and Acquisition System;
CLIB: Component Library; GO: Gene Ontology; KB: Knowledge Base;
KE: Knowledge Engineering; KM: Knowledge Machine; KR: Knowledge
Representation; OWL: Web Ontology Language; UT: Universal Truth.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
VKC was the technical leader for the project and was responsible for
formulating, directing, and managing the project, and for writing the final
report. DE served as a knowledge engineer and conducted the knowledge
representation and reasoning requirements analysis. A. Goldenkranz served
as a domain expert and performed the domain analysis tasks for Levin. A.
Gong served as a domain expert and performed the analysis tasks for Raven.
MM served as a domain expert and performed the analysis tasks for Kandel.
WW served as a domain expert and performed the analysis tasks for Alberts.
NYS served as a knowledge engineer, conducted the knowledge
representation and reasoning requirements analysis, and helped in writing
the report. All authors read and approved the final manuscript.
Authors information
Dr. Vinay K. Chaudhri is a program director in the Artificial Intelligence (AI)
Center at SRI International. His research focuses on the science and
engineering of large knowledge base systems and spans knowledge
representation and reasoning, question answering, knowledge
acquisition, and innovative applications. His most recent work has been on
creating an intelligent textbook in biology that answers a students questions
and leads to significant learning gains. He has co-edited a volume on the
Theory and Application of conceptual modeling, and two special issues of
AI Magazine  one on Question Answering Systems, and another on
Application of AI to Contemporary and Emerging Education Challenges.
He has taught a course on Knowledge Representation and Reasoning at
Stanford University. He holds a Ph.D. in Computer Science from University
of Toronto where he was a Connaught Scholar. He also holds a Masters in
Industrial and Management Engineering from Indian Institute of Technology
Kanpur, and a Bachelors degree in Mechanical Engineering from
National Institute of Technology, Kurukshetra. He is a senior member
of AAAI.
Daniel Elenius is a Computer Scientist at the Computer Science Lab at SRI
International. He holds an MS in computer science and engineering from
Linköping University, Sweden. He has developed several reasoning systems,
including a policy reasoner for the DARPA neXt Generation (XG) program, a
probabilistic fault propagation analysis tool for the DARPA META program,
and a system that reasons about hierarchical tasks and resource assignments
for the DoD ONISTT and ANSC projects. His research interests include
automated reasoning, knowledge representation, and the semantic web.
Andrew Goldenkranz is a biology teacher at Monta Vista High School in
Cupertino, California. He has helped position the Inquire application (an iPad
app for AURA) so that it is useful for teaching students studying from
Campbell Biology.
Allison Gong studies marine biology, particularly invertebrate zoology, and
teaches biology at the community college and university levels in California.
She teaches marine biology, zoology, and evolution to science majors and
science-phobes alike. Her interests in marine biology focus on marine
invertebrate life histories, larval biology, and ecology of the rocky intertidal. She
holds a PhD in biology from the University of California.
Maryann E. Martone received her BA from Wellesley College in biological
psychology and her PhD in neuroscience in 1990 from the University of
California, San Diego, where she is currently a professor in the Department
of Neuroscience. She is the principal investigator of the Neuroinformatics
Framework project, a national project to establish a uniform resource
description framework for neuroscience. Her recent work has focused on
building ontologies for neuroscience for data integration. She has completed
her tenure as the US scientific representative to the International
Neuroinformatics Coordinating Facility (INCF), where she still heads the
program on ontologies. MM recently joined FORCE11, an organization
dedicated to advancing scholarly communication and e-scholarship, as
Executive Director.William Webb is an expert in wildlife biology and has taught biology courses
to community college students for five years across multiple campuses.
Dr. Webb has community college teaching experience in diverse topics
within biology, including general education courses such as general
biology and health science, in addition to majors courses such as human
anatomy and physiology and animal biology. He holds a PhD in wildlife science
from the University of Washington.
Neil Yorke-Smith is an assistant professor at the American University of Beirut,
Lebanon and a visiting scholar at St Edmunds College, Cambridge. His
research interests include intelligent agents, planning and scheduling,
constraint-based modeling, intelligent user interfaces, and their real-world
application to managerial decision-making. He holds a PhD in optimization
from Imperial College London, UK.
Acknowledgements
Vulcan Inc. and SRI International funded this work. We thank the AURA
development team for providing the context for this effort. We thank Prof.
Craig Heller for his comments on an early version of this manuscript. Finally,
we thank Ilinca Tudose for her work on the representation of Biomembrane
that is used in this paper.
Author details
1SRI International, Menlo Park, CA 94025, USA. 2Monta Vista High School,
Cupertino, CA, USA. 3Cabrillo College, Aptos, CA, USA. 4University of
California, San Diego, CA, USA. 5Foothill Community College, Los Altos Hills,
CA, USA. 6American University of Beirut, Beirut, Lebanon. 7University of
Cambridge, Cambridge, UK.
Received: 23 May 2014 Accepted: 26 November 2014
Published: 18 December 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Diallo Journal of Biomedical Semantics 2014, 5:44
http://www.jbiomedsem.com/content/5/1/44RESEARCH Open AccessAn effective method of large scale ontology
matching
Gayo DialloAbstract
Background: We are currently facing a proliferation of heterogeneous biomedical data sources accessible through
various knowledge-based applications. These data are annotated by increasingly extensive and widely disseminated
knowledge organisation systems ranging from simple terminologies and structured vocabularies to formal ontologies.
In order to solve the interoperability issue, which arises due to the heterogeneity of these ontologies, an alignment task
is usually performed. However, while significant effort has been made to provide tools that automatically align small
ontologies containing hundreds or thousands of entities, little attention has been paid to the matching of large sized
ontologies in the life sciences domain.
Results: We have designed and implemented ServOMap, an effective method for large scale ontology matching. It
is a fast and efficient high precision system able to perform matching of input ontologies containing hundreds of
thousands of entities. The system, which was included in the 2012 and 2013 editions of the Ontology Alignment
Evaluation Initiative campaign, performed very well. It was ranked among the top systems for the large ontologies
matching.
Conclusions: We proposed an approach for large scale ontology matching relying on Information Retrieval (IR)
techniques and the combination of lexical and machine learning contextual similarity computing for the generation of
candidate mappings. It is particularly adapted to the life sciences domain as many of the ontologies in this domain
benefit from synonym terms taken from the Unified Medical Language System and that can be used by our IR strategy.
The ServOMap system we implemented is able to deal with hundreds of thousands entities with an efficient
computation time.
Keywords: Ontology matching, Life sciences ontologies, Entity similarity, Information retrieval, Machine learning,
Semantic interoperabilityIntroduction
With the wide adoption of Semantic Web technologies,
the increasing availability of knowledge-based applications
in the life sciences domain raises the issue of finding
possible mappings between the underlying knowledge
organisation systems (KOS). Indeed, various terminolo-
gies, structured vocabularies and ontologies are used to
annotate data and the Linked Open Data Initiative is
increasing this activity. The life sciences domain is very
prolific in developing KOS ([1-4] are examples of such
resources) and intensively using them for different pur-
poses including documents classification [5] and coding
systems to Electronic Health Records [6].Correspondence: gayo.diallo@u-bordeaux.fr
University Bordeaux, ISPED, Centre INSERM U897, F-33000 Bordeaux, France
© 2014 Diallo; licensee BioMed Central Ltd. Th
Commons Attribution License (http://creativec
reproduction in any medium, provided the orOne of the key roles played by these KOS is providing
support for data exchanges based on a common syntax
and shared semantics. This particular issue makes them
a central component within the Semantic Web, the
emerging e-science and e-health infrastructure.
These KOS, which are independently developed at the
discretion of various project members, are heteroge-
neous in nature, arising from the terminology used, the
knowledge representation language, the level of semantics
or the granularity of the encoded knowledge. Moreover,
they are becoming more complex, large and multilingual.
For instance, the Systematized Nomenclature of Medicine
Clinical Terms (SNOMED-CT) [7], a multiaxial, hier-
archical classification system that is used by physicians
and other healthcare providers to encode clinical health
information, contains more than 300,000 regularly evolvingis is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly credited.
Diallo Journal of Biomedical Semantics 2014, 5:44 Page 2 of 19
http://www.jbiomedsem.com/content/5/1/44concepts. Each concept is designated by synonymous
terms, sometimes by several. Another example is the
International Classification of Diseases (ICD), the World
Health Organizations standard diagnostic tool for epi-
demiology, health management and clinical purposes used
to monitor the incidence and prevalence of diseases and
other health issues. The current ICD-10 version con-
tains more than 12,000 concepts designated with terms
in 43 different languages including English, Spanish and
French.
There is a clear need to establish mappings between
these different KOS in order to make inter-operable
systems that use them. For instance, the EU-ADR pro-
ject [8] developed a computerised system that exploits
data from eight European healthcare databases and
electronic health records for the early detection of
adverse drug reactions (ADR). As these databases use
different medical terminologies (ICD-9, ICD-10, Read
Codes, International Classification of Primary Care) to
encode their data, mappings are needed to translate a
query posed to the global system into queries under-
standable for the different data sources. Performing
manual mappings between all the mentioned resources
is not feasible within a reasonable time. Generally
speaking, the data integration domain [9], the semantic
browsing of information domains [10] and web ser-
vices composition [11] are areas where the matching of
knowledge resources is usually performed.
There is therefore a crucial need for tools which are
able to perform fast and automated mapping computa-
tion between entities of different KOS and which can
scale to large ontologies and mapping sets. Significant
effort has been expended in the ontology alignment/
matching domain. A matching system is defined by the
Ontology Alignment Evaluation Initiative (OAEI) [12]
as a software program capable of finding mappings
between the vocabularies of a given set of input ontol-
ogies [13]. Formally, given two ontologies, a mapping is
a 4-tuple [14]:
< id; e1; e2; r >
such that:
 id is an identifier for the given mapping;
 e1 and e2 are entities, i.e. classes and properties of
the first and second ontology, respectively;
 r is a relation, e.g. equivalence (=), subsumption (?),
disjointness (?) between e1 and e2.
Some metadata, including a confidence value, w (usually
? [0, 1]), are often associated with the mapping.
In the following section we will briefly give an over-
view of different approaches and systems in line with theapproach we propose in this paper. In particular, we will
review approaches which use a space reduction strategy
for large scale ontology matching and machine learning-
(ML) based matching and briefly present systems evalu-
ated recently for the largest task in the context of the
international OAEI campaign. We will further discuss,
in Discussion, systems for matching ontologies in the
biomedical domain.
Related work
Ontology matching is an active research area. Existing
ontology matching systems use terminological, structural
and semantic features for the computation of candidate
mappings (please see [14-16] for a complete survey).
Despite the advances achieved in matching relatively
small size ontologies, the large scale matching problem
still presents real challenges to tackle, due to the com-
plexity of such a task. These challenges include effi-
ciency issues in term of space and time consumption,
the use of background knowledge, user involvement
and the automated evaluation of the matching system
[14,17]. Therefore, approaches for ontology matching
have been proposed in the literature including cluster-
ing and blocking strategies (reduction of search space),
ML- based matching (in particular for reusing existing
alignments or combing results for parallel matches),
interactive alignment (taking into account the user) and
the use of specialised background knowledge (in particular
for the life sciences domain).
A structure-based clustering approach for the matching
of large ontologies is introduced in [18]. The idea is to
partition each input schema graph into a set of dis-
jointed clusters before identifying similar clusters in the
two schema graphs to be matched. The COMA++ system
[19] is finally used to solve individual matching tasks and
combine their results. Hamdi et al. provide TaxoMap [20],
a tool which is based on the implementation of the
partition-based matching algorithm proposed in [21] to
find oriented alignment from two input ontologies.
TaxoMap provides one-to-many mappings between single
concepts and establishes three types of relationships:
equivalence, subclass and semantically related relation-
ships. The semantically related relationships denote an
untyped link indicating the closeness of two concepts.
Hu et al. [21] address the issue of aligning large ontol-
ogies by proposing a partition-based block approach for
the matching of large class hierarchies. Their matching
process is based on predefined anchors and uses struc-
tural affinities and linguistic similarities to partition
small block input class hierarchies. In contrast to these
divide-and-conquer methods, Wang et al. [22] use two
kinds of reduction anchors to match large ontologies
and reduce time complexity. In order to predict ignor-
able similarity calculations, positive reduction anchors
Diallo Journal of Biomedical Semantics 2014, 5:44 Page 3 of 19
http://www.jbiomedsem.com/content/5/1/44use the concept hierarchy while negative reduction
anchors use locality of matching. A partial reference
alignment strategy is used in [23] in order to partition
ontologies to be aligned, computing similarities be-
tween terms and filter mapping suggestions. To test
the approach, alignments provided by OAEI and from
previous evaluation of the SAMBO system [24] are
used.
On the other hand, Nezhadi et al. use an ML approach
to combine similarity measures of different categories in
order to align two given ontologies [25]. Their evaluation
of different learning classifiers  K Nearest Neighbor,
Support Vector Machine (SVM), Decision Tree (DT)
and AdaBoost  on real life (small) ontologies for bib-
PROCEEDINGS Open Access
SEE: structured representation of scientific
evidence in the biomedical domain using
Semantic Web techniques
Christian Bölling1*, Michael Weidlich2, Hermann-Georg Holzhütter1
From Bio-Ontologies Special Interest Group 2013
Berlin, Germany. 20 July 2013
* Correspondence: christian.a.
boelling@gmail.com
1Institute of Biochemistry, Charité
Universitätsmedizin Berlin, Berlin,
Germany
Abstract
Background: Accounts of evidence are vital to evaluate and reproduce scientific
findings and integrate data on an informed basis. Currently, such accounts are often
inadequate, unstandardized and inaccessible for computational knowledge
engineering even though computational technologies, among them those of the
semantic web, are ever more employed to represent, disseminate and integrate
biomedical data and knowledge.
Results: We present SEE (Semantic EvidencE), an RDF/OWL based approach for
detailed representation of evidence in terms of the argumentative structure of the
supporting background for claims even in complex settings. We derive design
principles and identify minimal components for the representation of evidence. We
specify the Reasoning and Discourse Ontology (RDO), an OWL representation of the
model of scientific claims, their subjects, their provenance and their argumentative
relations underlying the SEE approach. We demonstrate the application of SEE and
illustrate its design patterns in a case study by providing an expressive account of
the evidence for certain claims regarding the isolation of the enzyme glutamine
synthetase.
Conclusions: SEE is suited to provide coherent and computationally accessible
representations of evidence-related information such as the materials, methods,
assumptions, reasoning and information sources used to establish a scientific finding
by adopting a consistently claim-based perspective on scientific results and their
evidence. SEE allows for extensible evidence representations, in which the level of
detail can be adjusted and which can be extended as needed. It supports
representation of arbitrary many consecutive layers of interpretation and attribution
and different evaluations of the same data. SEE and its underlying model could be a
valuable component in a variety of use cases that require careful representation or
examination of evidence for data presented on the semantic web or in other
formats.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2014 Bölling et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in
any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver (http://
creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Background
Scientific evidence, as a concept, can be defined as information that is relevant to assess
the likelihood that a particular scientific idea is correct. Representation of the corre-
sponding evidence is therefore key to evaluating hypotheses and assessing claims con-
tained in scientific articles, databases or any other repository of scientific information.
Biomedical knowledge is often highly context-dependent and based on evidence obtained
from the skilful combination and evaluation of individual results, involving, among other
aspects, a range of model organisms, diverse experimental and computational techniques,
different forms of interpretation, and various inference schemes. Consequently, all those
aspects - the materials, methods and information sources used, the observations made,
the reasoning employed and the context-specific assumptions made - are important for
comprehensive evidence accounts. Likewise, when data, often from disparate sources, is
integrated to study complex biological systems an account of the evidence that was used
to infer a models properties and those of and among its components is critical for cor-
rect and transparent understanding of that model.
Scientific findings are now routinely published as resources on the World Wide
Web. Besides electronic versions of natural language texts more and more information
from both new and legacy sources becomes available through databases [1] and web
services [2] which provide through structured formats and interfaces consolidated
views of and programmatic access to biomedical data. Semantic web technologies and
standards in particular offer by virtue of their well-defined semantics and broad applic-
ability potent means for the computational integration and analysis of biomedical data
from heterogeneous and distributed sources on a large scale [3-5]. Accordingly, the
Resource Description Framework (RDF, [6]) is increasingly employed to represent and
disseminate new and legacy biomedical data [7,8] and biomedical ontologies specified
in the Web Ontology Language (OWL, [9]) are being developed to encode domain-
specific knowledge and annotate data from biomedical investigations [10-12]. As with
any other means for communicating scientific results, findings encoded in semantic
web formats need to be accompanied by an account of how they have been established
to evaluate their relevance. Towards this end different models, tools and methods have
been proposed: for representing and evaluating research hypotheses [13,14], contextua-
lization [15], models of discourse [16], of argument [17], extended means for annota-
tion [18,19], or specific container formats [20]. There is, however, currently no
dedicated model supporting a coherent, extensible and semantic-web compatible repre-
sentation of all those aspects routinely considered by a researcher inspecting the
evidence for a given scientific finding, i.e. a representation of (i) the experimental and
computational methods and settings that were used to establish the observational
results and process the data, (ii) the reasoning including additional findings and
assumptions used to infer the result in question, and (iii) information sources and
agents through which the corresponding views were communicated and propagated.
Here we introduce SEE (Semantic EvidencE), an RDF/OWL based approach for pro-
viding detailed, extensible and computationally accessible accounts of evidence even in
complex settings. SEE is designed to enable the fabric of observations, methods,
assumptions, and inferences examined by researchers to evaluate the evidence for a
claim to be formally represented along with their sources using semantic web techni-
ques. Evidence is captured in terms of the argumentative structure of the supporting
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 2 of 22
background for a claim i.e., by a coherent representation of claims, of the entities the
claims are about, of the argumentative relations between the claims and of claim pro-
venance. SEE accommodates nested layers of interpretation and attribution and different
evaluations based on the same data. We demonstrate its application in a case study that is
typical for the task of collecting, representing and evaluating evidence for systems biology
approaches such as genome-scale metabolic network reconstruction by providing an
expressive account of evidence for the location of the enzyme glutamine synthetase.
Results
Overview of the SEE approach
The SEE approach for representing evidence consists of providing (i) a formal representa-
tion of scientific claims, their provenance and the argumentative structure used to justify
them by other claims, (ii) a formal representation of claim content and (iii) a coherent
integration of the two. SEE relies on an abstract model for the representation of claims,
provenance and argumentative structure specified in the Reasoning and Discourse
Ontology (RDO), a lightweight OWL vocabulary developed for this purpose. Claim con-
tent e.g., what is claimed regarding the properties of biological entities or the results and
methods of an investigation is represented in RDF graphs by using appropriately defined
semantic web resources and design patterns which as a best practice should, if possible, be
re-used from existing domain ontologies. The connection between claims as representa-
tional primitives and their content relies on named RDF graphs [21] which enable pointing
to collections of RDF-triples or OWL-axioms serialized as such.
After outlining general requirements and design principles for representation of evi-
dence we describe the RDO. We then demonstrate the application and design patterns of
the SEE approach in a case study generating an expressive representation of evidence
reported in the literature for the location of the enzyme glutamine synthetase.
Deriving design principles and requirements for representation of evidence
We posit two design principles for the representation of evidence and explain their
rationale in the following:
DP1: Representation of evidence amounts to representation of claims and argumentative
structure.
DP2: Evidence relations in the sense of A is evidence for B obtain between the
things being claimed.
Accounts of evidence are directed towards the justification of scientific claims. The SEE
approach is based on the notion that scientific claims put forward possible, more or less
likely scenarios and outcomes - states of affairs [22] - as being accurate descriptions of a
subject of scientific inquiry. Something is evidence for a certain state of affairs, if and only
if it gives reason to believe that this state of affairs in fact obtains [23]. A pairing of evi-
dence and what it is claimed to be evidence for therefore corresponds to the set of pre-
mises and the conclusion of an argument in which the truth of the premises alleges to
give reason to believe the conclusion is true. Therefore the evidence used by authors or
agents to justify a claim, possibly using further unstated background assumptions, can be
mirrored by an argumentative structure having the claim as its conclusion. Typically, what
is used to justify the authors conclusions within this argumentative structure are claims in
themselves accepted as true on the basis of observations or inferences of the same or of
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 3 of 22
other investigators. SEE, therefore, models evidence relations in the sense of A is evidence
for B specifically as relations between claims.
We derive two additional requirements:
DP3: A researchers assessment of the evidence for a finding usually includes evaluation
of which materials and methods were used, what kind of data was obtained and which
properties were observed, inferred or assumed to establish the finding. Consequently, a
representation of the materials, methods, data items and other elements forming the
subject of a claim should be part of a computationally accessible evidence representation.
In RDF and OWL the subject of a claim, a state of affairs, must be expressed, using appro-
priately defined resources, as (one or more) triples and axioms, respectively. It follows
then, in accordance with DP2 that in an RDF/OWL-based representation of evidence that
includes claim subjects the representation of evidential relationships should operate
between claim subject representations, i.e. between sets of RDF-triples and/or
OWL-axioms.
DP4: Representation of claims and hence representation of evidence must take into
account claim provenance, in particular through which source and by which agents the
claims were made. Knowing which agent made the claim is crucial for evaluating inde-
pendence and reproducibility. Tracking the original source of a claim provides a natural
reference point for all subsequent representations of the claim and its supporting back-
ground and for re-evaluation of the claim within the original context in which it was
communicated.
We therefore identify as minimal components for modelling evidence elements repre-
senting (i) scientific claims and the argumentative structure used to justify them by other
claims, (ii) the subjects of the claims i.e. that what is claimed with regard to a subject of
inquiry, (iii) the agents making the claims and arguments, (iv) the sources in which claims
were originally made e.g., the original scientific articles or database records.
Reasoning and Discourse Ontology (RDO)
Based on the foregoing we developed an abstract model for representation of evidence in
terms of claims, their argumentative structure and their provenance. It is specified here as
the Reasoning and Discourse Ontology (RDO) using the Web Ontology Language (OWL).
This section outlines the core classes and properties of RDO. Full, formal specification of
all RDO constructs is provided in the ontology file provided as additional file 1.
The typical scenario that underlies the constructs defined in RDO is the following:
Agents (e.g., individual scientists) make claims on particular occasions (e.g., as authors
of a published scientific article) about a subject of inquiry. The subject of the claim - i.
e. what is claimed - is communicated in some linguistic form, often as part of a more
comprehensive report (e.g., a scientific article) authored by the agents. Claims are
usually justified by other claims the subject of which has been accepted as true, usually
on the basis of yet other claims. RDO (Figure 1) rests on the distinction of a claim, its
subject and the linguistic form in which this subject is communicated and is centered
around the concept of an assertion [24]: instances of the class assertion (courier
typeface denotes OWL classes, courier in italics denotes OWL properties)
represent particular claims made by particular agents on a particular occasion that a
particular proposition, the subject of the claim, is true. Propositions, in our model, are
represented by the class proposition and taken to represent the semantic content
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 4 of 22
of contextualized lexical entities formulated in some natural or artificial language [25].
The lexical entities by which the subject of a claim and propositions and reports in
general are formulated are represented using the class text. Further core classes are
report representing accounts intended to accurately describe an event or situation.
Thus, scientific journal articles or database records as typical sources of assertions are
examples of a rdo:report. Agent is used to represent individual persons, corporate
bodies or information processing devices as roleplayers in the creation of reports or
assertions. RDO specifies various properties to represent the relations between
instances of these classes (Figure 1). In particular, argumentative structure is captured
by the property is inferred from which relates an instance of assertion to
another if and only if the former is, directly or indirectly, inferred from the latter (and
possibly other premises).
Application: representation and evaluation of evidence for a source of glutamine
synthetase
Introducing the case study
We applied SEE to generate a computationally accessible, expressive and extensible
account of evidence gathered in the literature regarding a claimed source of the enzyme
glutamine synthetase (GS). We have chosen this particular test case because obtaining reli-
able information on location of enzyme activities is a subject area of particular importance
for systems biology approaches such as the reconstruction of cell-type specific [26] or
organism-level [27] metabolic networks. Furthermore, it embodies the typical task of
acquiring knowledge on a subject of inquiry by extracting and combining evidence from
different sources.
Figure 1 Core classes and properties in RDO. Boxes denote labelled classes, arrows denote labelled
properties, direction of arrows denotes property domains and ranges. Asterisk: property has domain and
range-specific sub-properties. The color code used here is also used in subsequent figures.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 5 of 22
Starting point is our evaluation of a scientific journal article [28] (referred to as Meister
1985 in the following) authored by Alton Meister which asserts in the second paragraph
of the text, among other things, that the enzyme glutamine synthetase (GS) was isolated
from rat liver. This assertion is based, by way of citation, on the contents of another article
by Tate, Leu and Meister [29] (referred to as Tate 1972 in the following). In Tate 1972
the isolation of GS from rat liver is reported. The finding is reported to be based on an
investigation which involved, among other things, extraction of rat livers, protein purifica-
tion and g-glutamyl hydroxamate synthesis (g-GHS) assays. In the following we show how
this context is formalized using the SEE approach to yield a detailed formal account of the
evidence presented through these articles for rat liver as source of GS. In doing so, we
illustrate various design patterns used in SEE for representing the relevant items. For
clarity assertion instances will be indexed as A1, A2, and so forth.
Representing the evidence
Figure 2 shows how the assertion from Meister 1985 that GS was isolated from rat
liver is represented using RDO, exemplifying the design pattern used to represent the
relations between a particular assertion and its subject and provenance: The article
Figure 2 Representation of assertions with subject and provenance. Assertion instances are
related to proposition instances representing the subject of the assertion by asserts, to the agents
making the assertion by is_assertion_made_by and to the reports in which they are made by
is_assertion_made_in. See text for additional relations among assertions, agents, reports and textual
representations. Assertion and proposition labels reflect the graph representation of assertion subjects (see
text). Color code of assertion and proposition labels indicates the structured representation of assertion
subjects (yellow: class, blue italics: property, red: Manchester syntax restriction keyword). Circle shows the
index by which the assertion is referred to in the text.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 6 of 22
itself, Meister 1985, is classified as instance of report annotated with a uniform
resource locator (URL) providing its digital representation. The second paragraph of
Meister 1985 constitutes a report_part. It is expressed as the English language text
as which it is written and which is represented as an instance of text. The original
text is linked to it via the data property has_lexical_structure. Meisters claim
that glutamine synthetase was isolated from rat liver contained in this paragraph is
represented by an instance of assertion (A1) labelled as ! some GS-enzyme isolated
from some rat liver ! AM to indicate the assertion subject in a concise, human read-
able manner (formalization of assertion subjects is described below). A1 is related to a
corresponding instance of proposition identifying the subject of the claim, to an
instance of agent representing Alton Meister, and to said report part by the prop-
erties asserts, is_assertion_made_by and is_assertion_made_in,
respectively.
Claims which reiterate previous findings are represented as assertions on the same sub-
ject made by the respective agents. Formally, the reiterating claim is represented as an
assertion instance which is linked to the source assertions by is_directly_in-
ferred_from and linked to the same proposition instance as the source assertions
by asserts. Each assertion can be linked to its corresponding agents and reports. Appli-
cation of this design pattern to our case study is shown in Figure 3: The fact that Meisters
assertion (A1) reiterates what Tate & co-workers have asserted on the isolation of GS
from rat liver (A2), is represented by a relation of the former to the latter via is_direc-
tly_inferred_from and by sharing the same proposition instance via asserts.
The argumentative structure within and across the publications is represented as a
series of assertion instances and is_directly_inferred_from relations with
additional links to represent assertion subjects and provenance (Figure 4). The
assertion instances linked to A2 reflect the results and the reasoning of the authors
at various steps of their investigation based on a careful analysis of the internal argu-
mentative structure of Tate 1972. Specifically, Tate et al.s main conclusion that GS-
enzyme was isolated from rat liver (A2) is essentially based on asserting that (A3)
there is a biological sample (labelled sample-1) which has GS-activity, that (A4) any
GS-activity is borne by some GS-enzyme and that (A5) sample-1 was isolated from
some rat liver (precise definitions for GS-enzyme, GS-activity in the context of the
case study are detailed in additional file 2). The joint use of A3, A4 and A5 to infer A2
Figure 3 Representation of claims which reiterate previous findings. The fact that Meisters assertion
(A1) reiterates what Tate & co-workers have asserted on the isolation of GS from rat liver (A2), is
represented by a relation of the former to the latter via is_directly_inferred_from and by
sharing the same proposition instance via asserts. Each assertion instance is linked to its
corresponding agents and reports. Color code as in figure 1.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 7 of 22
is made explicit by using the has_conjunctive_part property to link them to the
same composite assertion instance which in turn is related to A2 using the
is_directly_inferred_from property. This pattern is used whenever an asser-
tion is inferred from more than one premise. A3, the assertion that sample-1 has GS-
activity is justified in turn by asserting that (A6) it was input to a particular assay
(labelled assay-1), that (A7) this assay produced a particular result, data item 1, and
that (A8) this data item is a measurement of some GS-activity. A8, in turn, is justified
by asserting that (A9) the data item is output of assay-1, that (A10) this assay was a g-
GHS assay, and that (A11 & A12) this type of assay is suited to measure GS-activity.
Some assertions are not further justified, either because they reflect factual descriptions
in Tate 1972 (A9, A10), represent general assumptions of the authors (A11) or are
expressions of terminological domain knowledge (A12, A4). A5 exhibits a similar justi-
fication trail, as shown in Figure 4. Full, formal representation of the argumentative
structure for the test case is provided in additional file 2.
The prevalent pattern in SEE for recording individual and logically relevant steps of an
investigation is for any such step to link its outcomes (data or material), the techniques
used to produce these outcomes, and their objectives as exemplified in the composite
assertions comprising assertions A9-A12 and A15-A20 (Figure 4). In A9-A12, for example,
the experimental process type (g-GHS assay) is linked to the objective of its application
(GS-activity measurement) and in turn to the quality that is intended to be determined
(GS-activity). Generally, the relations between these ontologically different entities are not
Figure 4 Representation of argumentative structure within and across publications. The
argumentative structure used to justify Meisters claim on the isolation of GS from rat liver (A1) is
represented as a series of assertion instances linked by is_directly inferred_from relations.
Dashed line separates assertions made in Meister 1985 and Tate 1972. Dashed-dot boxes indicate composite
assertions with their component assertions placed inside signifying the has_conjunctive_part
relations. Author initials tags of the assertions made by Tate et al. are omitted, as are links to propositions,
reports, authors and texts. Color code as in figure 1.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 8 of 22
trivial and not one-to-one (one objective can consist of the determination of several quali-
ties recognized in a scientific domain, a certain quality can be the subject of inquiry in sev-
eral objectives). However, in this particular case the objective and quality are narrowly
defined and directly correlated.
Representation of assertion subjects
The representation of argumentative structure and claim provenance as an interrelated set
of assertion instances described so far is complemented by a structured representation
of what is asserted in each assertion, the assertion subject. To this end each assertion
instance is linked to a corresponding proposition instance the IRI (Internationalized
Resource Identifier) of which identifies a named RDF graph. This graph provides a struc-
tured representation of the assertion subject using appropriately defined resources (Figure
5). This setup enables querying the elements forming the assertion subject. In assertion
A10, shown in Figure 5 as an example, it is asserted by Tate and co-workers that the parti-
cular assay they performed was a g-GHS assay. The representation of this statement as a
graph identified by the IRI of the proposition instance linked to the assertion
instance representing A10 enables to access the entities A10 is about: the particular assay,
its asserted type, and the typing relation itself. Full specification of all propositions as
named graphs in the context of the case study is provided in additional file 3.
Figure 5 Representation of assertion subjects as named graphs. In SEE structured, queryable
representations of assertion subjects are provided as named graphs. A) The structured representation of the
subject of an assertion can be accessed as the RDF graph identified by the IRI of the proposition instance
related by the asserts property to the assertion instance representing the assertion. B) Structured
representation of the subject of assertion A10 asserting that the particular assay performed by Tate et al.
(:assay-1) was a g-GHS assay (:gamma_ghs_assay). C) TriG representation of the graph shown in B.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 9 of 22
To generate the graph representations of the assertion subjects, the natural language
expressions of the assertions identified in the Meister 1985 and Tate 1972 reports were for-
malized in RDF using appropriately defined resources (see additional files 2, 3 and 4). Most
assertion subjects could be formalized in a straightforward manner applying OWL 2 RDF-
based semantics [30]. The principal claim that glutamine synthetase was isolated from rat
liver which is the common subject of assertions A1 and A2 was formalized in RDF by
instantiating the class gs_enzyme and is_isolated_from some rat_liver (shown
as :proposition-1 in additional file 3). This exemplifies instantiation of the OWL-class
(A and related_to some B) as a design pattern for formalization of statements which
can, in natural language, be represented in the form some A related to some B (A and B
denoting OWL-classes used to represent the types A and B, respectively and related_to
denoting an OWL-property used to represent the relation among some of their instances).
Labels of assertion and corresponding proposition instances are directly
derived from the graph representation of the assertion subject (see methods section).
In particular, the label some A related_to some B is used for proposition
instances that represent statements of the form some A related to some B by applying
the design pattern described above.
Representing consecutive layers of interpretation and own conclusions
We use the test case to specify additional design patterns to represent activity of a
curator or generally of a third party evaluating a scientific report. Our representation
of the evidence in the Meister 1985 and Tate 1972 reports is the result of the interpre-
tation by another agent (Christian Bölling - CB). This can be explicitly represented in
SEE using its familiar design pattern for propositions and assertions. For example, the
claim that Tate et al. indeed assert that the assay they performed was a g-GHS assay in
their 1972 publication can be represented as an assertion instance in its own right,
made by another agent, CB (Figure 6). This pattern allows for representing arbitrary
many consecutive layers of interpretation or attribution.
So far the presented account consists of assertions attributed to the authors of the
Meister 1985 and Tate 1972 reports, i.e. a representation of what these authors assert.
SEE also provides the resources to append own conclusions. For example, an agent, CB,
could upon evaluation of the claims made by Tate et al. conclude for himself that GS
was indeed isolated from rat liver. This is represented as an assertion instance in its
own right (A30, labelled ! some GS-enzyme isolated from some rat liver ! CB). It is
linked to the corresponding proposition via asserts and the assertions made by Tate
et al. via is_directly_inferred_from. We describe two semantically different pat-
terns to make this connection. In pattern 1 assertion A30 is linked to assertion A2
(Figure 7). In pattern 2 (Figure 8) A30 is linked to a new composite assertion that
involves two more curator assertions (A31, A32) and A4 as a representation of termino-
logical domain knowledge. A31 and A32 are linked by is_directly_inferred_-
from to composite assertions reflecting factual descriptions of data and procedures
given in Tate 1972. There is a subtle, yet important difference in meaning between these
two representations. In pattern 1 CBs conclusion is based on Tate et al.s assertion on
the same subject, i.e., it is based on the author statement itself and does not necessarily
imply an affirmation of how Tate et al. reached their conclusion. In pattern 2 the curator
inference is based on factual descriptions in Tate 1972, i.e., it affirms the conclusions of
Tate et al. as own conclusions on the basis of the reported experimental results.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 10 of 22
Evaluation of a given set of data might also lead to conclusions different from those
of the authors. Such alternative interpretations can be represented using SEE. For
example, one might dispute that g-GHS assays are suited to measure GS-activity (EC
6.3.1.2). The g-GHS assay works by measuring the formation of L-g-glutamyl hydro-
xamate rather than glutamine [31]. Tate et al. assert as the objective of its application
GS-activity measurement, accepting the formation of the hydroxamate under the
conditions of the assay as a proxy for the formation of glutamine and the actual reac-
tion mechanism. Assertion A11 using the property achieves_objective reflects
this acceptance by Tate et al.. Alternatively, a third party could assert that g-GHS
assays merely achieve the less specific objective of measuring g-glutamyl transferase
(GGT) activity (EC 2.3.2.2) (Figure 9, assertion A45). In this case the data reported
by Tate et al. can still be used to infer that rat liver is a source of GGT-enzyme
(Figure 9, assertion A40).
Figure 6 Representation of consecutive layers of interpretation. Consecutive layers of interpretation
can be represented as assertions the subject of which is about other assertions. A) CBs assertion that Tate
and co-workers assert that assay-1 was a gamma-GHS assay in their 1972 report is represented as an
assertion instance linked to a proposition instance whose named graph representation relates the
assertion instance A10 to the Tate 1972 report via the is_assertion_made_in property. B) TriG
representation of the graph shown in A. In combination with the RDF dataset shown in Figure 5C this is
an example of a named graph referencing a named graph via the corresponding assertion and
proposition instances. Color code as in figure 1.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 11 of 22
Evaluating the test case evidence representation
The test case evidence representation that was created using the RDO constructs and
design patterns was evaluated in terms of its potential to answer, within the confines of
the case study, a list of competency questions reflecting different aspects of the evidence
a researcher investigating glutamine synthetase knowledge would be interested in:
Q1: Which locations of GS have been asserted?
Rat liver.
Q2: Where has rat liver GS been reported?
The Meister 1985 and Tate 1972 reports.
Q3: Do the assertions made in these reports pertain to independent observations?
No. Meisters assertion is based on Tate et al.s assertion. Moreover, some of the
authors of the two reports are identical.
Q4: Is there experimental evidence and where is it described?
Yes. In the Tate 1972 report.
Figure 7 Representation of curator activity: inference from author statement . The pattern to
represent inference from author statement is illustrated here by linking curator assertion A30 (shown in
bold) to assertion A2 of the Tate et al. argumentation signifying inference of A30 on the basis of an author
statement of Tate et al. on the same subject. Color code as in Figure 1.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 12 of 22
Q5: Which observations and techniques were used for establishing rat liver as GS source?
1. extraction of a protein sample from rat liver (technique: TLM purification)
2. that sample has GS-activity (technique: g-GHS assay)
Q6: Did Tate et al. really make these observations and conclusions? Who created this
account of their findings?
Christian Bölling.
Based on the SEE design patterns, these questions could be formulated as SPARQL
[32] queries and successfully answered (see additional file 5). In each of Q1-Q6 the
structured representation of assertion subjects as named graphs, besides the other SEE
design patterns, is used to identify assertions which are relevant to answer the query.
For answering Q1 assertions are identified whose subjects graph representation
includes a graph pattern indicative for the isolation of GS from some location (Figure
10A). For answering Q3, pairs of assertions are identified whose subjects share the
same graph representation and where one is inferred from the other (Figure 10B).
Figure 8 Representation of curator activity: inference from experimental evidence. The pattern to
represent inference from evaluation of the reported experimental evidence, in contrast to inference based
on author statement (Figure 7), is illustrated here by linking curator assertion A30 to a new composite
assertion involving curator assertions A31 and A32. These are, in turn, linked to the composite assertions
A9-A12 and A15-A20, respectively. These composite assertions reflect data and procedures reported in Tate
1972. Taken together this graph therefore represents a curator conclusion (A30) based on the affirmative
outcome (A31, A32) of the evaluation of the data and procedures reported in Tate 1972. Note that A2, the
principal conclusion of Tate et al. is unrelated to the new composite assertion. Curator assertions and their
links to the Tate et al. argumentation are shown in bold. Color code as in Figure 1.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 13 of 22
The following evidence-related information can be queried exploiting property chains
and other axioms defined for the RDO constructs:
- all assertions which are directly or indirectly used to infer a given assertion
- all assertions made in a given report
- all assertions made by a given agent
- all assertions on the same subject
- all agents making assertions on a given subject
For the corresponding queries see additional file 5. As an example, in Figure 11 the
object property assertions inferred for assertion A1, Meisters assertion that GS was
isolated from rat liver, are shown. These inferences, simply derived in Protégé 4 with
HermiT 1.3.8 as a reasoner include all assertions which A1 is directly or indirectly
inferred from and all reports and texts A1 is based on.
Discussion
SEE design
SEE offers a tangible interpretation of the concept of evidence in terms of the argu-
mentative structure of the supporting background for a claim. It rests on the
Figure 9 Representation of curator activity: alternative interpretations of reported data. Alternative
interpretations of reported data can be represented as assertions that are made by a third party and linked
to assertions reflecting factual descriptions of the reported data. Based on CBs assertion that g-GHS assays
measure GGT activity (A45) - rather than GS-activity - it is inferred that GGT-enzyme has been isolated from
rat liver (A40). The corresponding inference chain relies on a number of new curator assertions (A41-A46)
and their combination into composite assertions but re-uses assertions on the quantitative data obtained
and the procedures conducted by Tate et al. Curator assertions and new inference links are shown in bold.
Color code as in Figure 1.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 14 of 22
Figure 10 Competency questions SPARQL queries. A) SPARQL query to identify all asserted locations of
GS (Q1). This query identifies patterns in which an assertion (_:q11) has a subject (_:q12) which includes a
graph pattern indicative for the isolation of GS from some location. B) SPARQL query to identify
dependency of assertions on the same subject. The query identifies assertions (_:q31, _:q33) which share
the same subject (?proposition) and are inferred from one another.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 15 of 22
disctinction between claims as such (assertion), their subjects (proposition) and the
linguistic form in which these subjects are communicated (text). As a consequence of
this design evidential relations (as in A is evidence for B) can be represented consis-
tently as relations between assertions. This means that statements of the form this
dataset / experiment / publication / method is evidence for B are regarded as figura-
tive expressions. Instead, the relation between a dataset, an experiment or a publication
and the state of affairs it is claimed to be evidence for is represented indirectly through
relations between assertions the subjects of which relate to the entities in question.
The advantage of this design is that it enables a coherent representation not only of
extensive argumentative networks but also of arbitrary many layers of consecutive
Figure 11 Inferred object property assertions. Inferred object property assertions for Meisters assertion
that GS was isolated from rat liver (A1).
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 16 of 22
interpretations and alternative evaluations of the same observations or information
sources. RDO offers clear, formally defined types and relations for representing claims,
their subjects, their linguistic representations, related information sources and agents
on the basis of well established concepts from epistemology and the philosophy of lan-
guage [22,24,25,33,34]. The case study examples suggest that the SEE design principles
and their implementation in RDO are capable of correctly representing, in a computa-
tionally accessible and coherent form, the entire evidence trail for a claim needed to
evaluate its relevance including observational data, research techniques, assumptions
and information sources.
SEE represents argumentative structure at its foundational level of premises being
used to infer a conclusion using the is_directly_inferred_from property and
its transitive superproperty is_inferred_from. This allows for a coherent repre-
sentation of different argument forms and larger rhetorical structures which can be
mapped onto their underlying assertions.
SEE aims to capture arguments as they are presented in their sources rather than to
evaluate their quality or to categorize them. How conclusive an argument is will typi-
cally depend on agent background knowledge or application-dependent requirements.
The SEE design enables users to evaluate evidence according to their own, possibly
domain- and application-specific criteria.
SEE-based accounts could also be used alongside specified rules, or argument forms
considered as acceptable by individual researchers or within specific domains of
inquiry which could then be leveraged to automatically infer new assertions on the
basis of the already asserted information.
With regard to the extraction of assertion subjects and a specific argumentative
structure from a natural language text SEE relies in its current form on a heuristic
approach leveraging expert domain knowledge to identify assertions and formalize
them in OWL. As OWL is a subset of first order logic there may be statements from
natural or artificial languages which cannot directly be translated into OWL, constrain-
ing the formalization of assertion subjects in SEE. It is, however, not clear which actual
limitations arise from this theoretical constraint for the representation of evidence in
specific use cases. The test case presented here suggests that within a specified domain
of discourse, using appropriate constructs and design patterns, the relevant contents of
the statements made originally in a context-rich narrative format such as a scientific
journal article can be adequately formalized.
Formalization of natural language statements is an important prerequisite for compu-
tational approaches to data evaluation. For applications that can forego this need the
statements can be represented in their original form as texts or referenced by links to
the original information sources. Both are by default designed to be provided in SEE as
reference points for evaluation.
The presented design patterns make SEE-based accounts of evidence extensible. This
design is in line with the open world assumption on which RDF and OWL as knowledge
representation languages operate. The particular argumentative structure and level of
detail presented in the case study are based on heuristics reflecting domain-specific
requirements to understand how an enzyme was characterised. This representation can be
extended or shortened as required. For example, details on the protein purification process
performed by Tate et al. or indeed any other detail that becomes relevant for the
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 17 of 22
evaluation of the presented evidence could be appended to the existing assertions. Like-
wise, as we have demonstrated, alternative views and conclusions can be accommodated.
On the other hand, for applications which only require information on claim provenance,
only the source publications of the main claims could be represented.
Evidence types
Evidence type schemes provide a useful shorthand categorization of research techni-
ques used to establish a claim. SEE could be aligned with any categorization of
research techniques and hence evidence type scheme to characterise the evidence for
an assertion. Essentially, SEE provides a platform to define custom, extensible evidence
types and apply them as needed. For example, the evidence for rat liver as a source of
GS in the test case could be characterised as experimental evidence as based on a
direct assay as based on a g-GHS-assay or as based on a g-GHS assay, protein puri-
fication involving Sephadex chromatography, and samples from Sprague-Dawley rats
depending on the level of accuracy desired.
The flexibility and extensibility of the SEE approach may also be useful to character-
ise evidence where several techniques have been combined to establish a scientific
result or evidence is characterised in combination with claim provenance. We illustrate
this with a comparison to the Gene Ontology (GO) evidence codes which are meant to
reflect the type of work or analysis described in the cited reference which supports the
GO term to gene product association [35]. GO evidence codes consist of a collection
of terms arranged in a hierarchical format. In this taxonomy the terms representing
justifications based on author statements (TAS, NAS) are unrelated to those representing
experimental techniques (EXP and child terms). Consequently, GO associations marked
as being made on the basis of an author statement are usually not qualified with respect
to how this author statement came about. In contrast, as demonstrated in the case
study, using SEE any author statement can be extensively qualified in terms of the
experimental evidence or other author statements it is directly or indirectly based on.
Use cases
Representations which use SEE or its underlying model could be productive in a variety
of use cases requiring careful examination or recording of evidence, e.g.,
- providing supporting background information for biomedical knowledge bases,
- creating digital abstracts of research publications,
- adding a claim-level perspective on research publications which could be used by
publishers, in bibliographic databases and in personal bibliography managers,
- providing open linked data which can be integrated on an informed basis using
varying, application specific evidence criteria.
Related and future work
The SWAN biomedical discourse ontology [16] developed in the context of the
Semantic Web Applications in Neuromedicine (SWAN) project offers a formal model
of scientific discourse based on two different classes of statements; swan:hypothesis
and swan:claim. Claim subjects are to be represented in natural language and the reso-
lution of their supporting background is confined to the document level. The
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 18 of 22
Annotation Ontology (AO) [18] has been implied as a means to provide formalized
accounts of claims and their supporting background conceptualized as annotations and
document parts, respectively. While it is possible in this way to relate individual ontol-
ogy terms to parts of documents, the AO semantics and use cases suggest that its
main application area is representation and support of annotations of documents
rather than representation and evaluation of extensive, possibly nested, networks of
claims. Nanopublications have been proposed as a container format to encode and
publish individual assertions using Semantic Web and Linked Data principles [36].
PROCEEDINGS Open Access
Semantic Web repositories for genomics data
using the eXframe platform
Emily Merrill1*, Stéphane Corlosquet1*, Paolo Ciccarese1,2, Tim Clark1,2,3, Sudeshna Das1,2*
From Bio-Ontologies Special Interest Group 2013
Berlin, Germany. 20 July 2013
* Correspondence:
mmerrill@partners.org;
scorlosquet@gmail.com;
sdas5@partners.org
1Massachusetts General Hospital,
Partners Research Building, 65
Landsdowne St, Cambridge, MA,
02139, USA
Abstract
Background: With the advent of inexpensive assay technologies, there has been an
unprecedented growth in genomics data as well as the number of databases in
which it is stored. In these databases, sample annotation using ontologies and
controlled vocabularies is becoming more common. However, the annotation is
rarely available as Linked Data, in a machine-readable format, or for standardized
queries using SPARQL. This makes large-scale reuse, or integration with other
knowledge bases very difficult.
Methods: To address this challenge, we have developed the second generation of our
eXframe platform, a reusable framework for creating online repositories of genomics
experiments. This second generation model now publishes Semantic Web data. To
accomplish this, we created an experiment model that covers provenance, citations,
external links, assays, biomaterials used in the experiment, and the data collected during
the process. The elements of our model are mapped to classes and properties from
various established biomedical ontologies. Resource Description Framework (RDF) data
is automatically produced using these mappings and indexed in an RDF store with a
built-in Sparql Protocol and RDF Query Language (SPARQL) endpoint.
Conclusions: Using the open-source eXframe software, institutions and laboratories
can create Semantic Web repositories of their experiments, integrate it with
heterogeneous resources and make it interoperable with the vast Semantic Web of
biomedical knowledge.
Background
There has been a rapid cost reduction per megabase of genomic information obtained,
beating Moores law [1] many-fold [2,3], resulting in an exponential growth of geno-
mics data, especially next generation sequencing data [4]. Standards to unambiguously
describe the experimental details are required to facilitate the understanding, quality
checking, reusing, reproducing and integrating the data. The bioinformatics community
has responded to the challenge and several standards have been developed over the
years. The first standard to be published provided requirements for the Minimum
Information About a Microarray Experiment (MIAME) [5]. Several other standards
were published as new technologies evolved and then the Minimum Information for
Biological and Biomedical Investigations guideline was proposed for reporting all types
Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3
http://www.jbiomedsem.com/content/5/S1/S3 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2014 Merrill et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in
any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver (http://
creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
of biomedical experiments [6]. The major public repositories of genomics experiments,
Gene Expression Omnibus (GEO) [7] and ArrayExpress [8], are compliant with these
standards.
While standards addressed the need for uniform experiment representation, controlled
vocabularies, terminologies and ontologies were developed to describe the samples, assays
and other experimental details in an unambiguous manner. For example, the Ontology for
Biomedical Investigations (OBI) [9] provides a model for biomedical experiments with
classes that describe elements of the experimental investigation process. The Experimental
Factor Ontology (EFO) [10] was developed as an application ontology to describe the
genomics data in ArrayExpress [8]. In addition several ontologies and vocabularies have
also been developed to describe biological specimens such as the organism, tissue, cell
type, disease state. These include the Cell Ontology (CL) [11], the Foundation Model of
Anatomy (FMA) [12], Disease Ontology (DO) [13] among numerous others.
Several repositories of genomics data have adopted the MIAME or MIBBI standards and
are leveraging these biomedical ontologies to provide consistent annotation of experi-
ments. A few examples from diverse domains include the Gemma repository - a resource
for sharing, reuse and meta-analysis of microarray data [14], Chemical Effects in Biological
Systems (CEBS) database that contains data of interest to environmental health scientists
[15] and Oncomine an integrated database and mining platform for oncology data mine
[16]. Although these resources make use of ontologies to represent experimental data in a
standardized manner, the annotations are not machine-readable by other software and
thus integration with other knowledge resources remain a challenge.
Meanwhile, Semantic Web [17] technologies such as Linked Data, Resource Description
Framework (RDF) and SPARQL are increasingly being used in the bioinformatics commu-
nity to respond to the knowledge integration needs [18]. Semantic Web allows one to
query across disparate resources using a single flexible interface. For example, the Bio2RDF
project successfully applies Semantic Web technologies to create a mashup of key publicly
available databases using a common ontology and normalized Uniform Resource Identifiers
(URI) [19,20]. Cheung et al. demonstrate the use of Semantic Web technologies for a
federated query in the neuroscience domain [21]. There are several other examples across
various biomedical domains that demonstrate the power of Semantic Web technologies.
However, surprisingly there has been no wide spread adoption of Semantic Web
technologies for experiment repositories, where queries using domain ontologies can
help bridge different disciplines, for important applications such as translational
medicine. Recently the European Bioinformatics Institute (EBI), recognizing this urgent
need, has released an RDF platform that includes a SPARQL endpoint for the Gene
Expression Atlas [22], a database that summarizes gene expression from ArrayExpress
experiments[23]. However, it doesnt provide reusable software that can be used by
other institutions to house and query their genomics data.
To address this gap, we developed eXframe as a reusable software platform to build
genomics repositories that automatically produce Linked Data and a SPARQL
endpoint. Our platform is based on an open source content management system and
uses existing biomedical ontologies to produce Semantic Web data enabling intero-
perability with the other resources. The code is freely available and application is
demonstrated with a repository of stem cell data.
Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3
http://www.jbiomedsem.com/content/5/S1/S3
Page 2 of 12
Implementation
In this section we describe the implementation of eXframe and how it automatically
generates Linked Data.
Framework
The eXframe software framework [24] enables creation of web-based genomics experi-
ment repositories. It is based on an open source content management system, Drupal
[25], with modifications to support genomic experiment data. In this paper, we report a
re-factored second generation of eXframe, which produces Linked Data and a SPARQL
endpoint for querying it. The revised version also includes an updated experiment
model that has been generalized to support various types of biomedical experiments as
well as an upgrade to Drupal 7.
We have defined content types (e.g. experiments, assays, biomaterials and bibliographic
citations) as well as their relationships as first class objects in Drupal. These predefined
content types are packaged as Drupal features and available for use within eXframe. All
content types and their fields are mapped to appropriate ontologies and vocabularies as
described in the following section. Using these mappings, the Drupal RDF modules [26]
are used to produce RDF as well as a SPARQL endpoint. Data can also be exported in
other standard formats such as ISA-Tab [27]. A simple schematic of the architecture is
shown in Figure 1. The software also includes a basic theme (colors, fonts and style) for
the website. Any group or institution that uses eXframe can customize the content types,
theme or ontology mappings.
Data model
The main content type within eXframe is an experiment. It describes the experiment
and its meta-data including title, description, contributors, design, citations, and links
Figure 1 eXframe architecture . Overall schematic of eXframe architecture displaying the major
components.
Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3
http://www.jbiomedsem.com/content/5/S1/S3
Page 3 of 12
to external resources such as GEO [28] and ArrayExpress [8]. The experiment content type
is mapped to the OBI investigation class obo:investigation. The experiments publication
meta-data is represented using the Dublin Core ontology [29]. However, we are currently
evaluating the PAV ontology [30] as it provides more detailed and precise provenance
information. For example, the Dublin Core ontology specifies the relation dc:date; but does
not provide precise information as to whether the date is the submitted date, published
date or last updated date. The researchers that conducted the experiment are repre-
sented as Drupal users with a profile and mapped to foaf:Person in the FOAF ontology
[31]. While we do not specify the principal investigator (for the sake of simplicity), one
could use VIVO [32] to do so. Bibliographic citations are represented using the Drupal
biblio module and mapped to the bibliographic ontology, BIBO [33]. These classes and
mappings are illustrated in Figure 2.
The experiment class also describes the overall protocol; measurement type and
includes the experimental-factors, which can be exploited by bioinformaticians for data
analysis. Experiments are composed of assays represented by the bioassay content type.
Figure 2 Data Model. Data model outlining the relationship between the experiment, its assays and
biomaterials. The Drupal content types are indicated as green circles with the mapping listed underneath.
Arrows indicate the relationships.
Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3
http://www.jbiomedsem.com/content/5/S1/S3
Page 4 of 12
The bioassay content type is mapped to obo:bioassay and specifies the technology plat-
form used and other assay details. Bioassays are typically performed on several repli-
cates specified by the replicate content type and mapped to efo:replicate (OBI only
models replicate design and analysis). Each replicate is associated with the biological
material on which the assay is conducted and is specified by the biomaterial content
type. Thus technical replicates reference the same biomaterial, whereas biological repli-
cates reference the unique materials used for the assay. The assays have raw data as
their output. Data transformations and analyses conducted on the raw data are cur-
rently not represented, but are included in future plans for the system.
Biomaterial is deeply annotated using Drupal Taxonomies and mapped to various
controlled vocabularies and ontologies. In the eXframe default package, the organism,
tissue type, cell type, disease state and chemical treatment taxonomies are mapped to
NCBI Taxonomy (NCBITaxon) [34], FMA [12], CL [11], Disease Ontology (DO) [13]
and Chemical Entities of Biological Interest Ontology (ChEBI) [35] terms, respectively.
EFO [10], NCI Thesaurus [36] or Breda Tissue Ontology (BTO) [37] is also used to
increase coverage when required. Biomaterial properties and their mappings are config-
urable and can be easily customized to a particular domain as required. The mappings
of the main content types (experiments, bioassay, citation, biomaterials etc.) to ontolo-
gies are configured in PHP code, in a single file (an excerpt of which is shown in
Figure 3). Attributes of the experiment, bioassays, and biomaterials that can be defined
via structured vocabularies are stored as Drupal taxonomies. For example, Cell Type,
an attribute of the biomaterial, is represented as taxonomy. Each term in the taxonomy
is mapped to a class or classes in external ontologies. Thus, Fibroblast a term in the
Cell Type taxonomy, is easily added, edited and mapped to ontologies through the
web interface.
Linked data & SPARQL endpoint
We use the Drupal RDF modules to produce RDF using the mappings discussed above.
RDF generated using the Drupal modules [26] is indexed into an RDF store powered
by the ARC2 PHP library [38]. A SPARQL endpoint is also published by this RDF
store. The RDF indexer in Drupal is designed to be backend-agnostic and allow for
any RDF store to be plugged in. Were using ARC2, which is sufficient for our needs,
but other stores can be used depending on the size of the dataset, or particular
SPARQL features that might be needed.
Some of the data in the repository is kept private until the researchers publish their
work. To maintain privacy, we utilize two stores: one of which solely contains the
Figure 3 Ontology mapping code. Excerpt from exframe.entity_rdf.inc showing how Drupal classes are
mapped to external ontologies.
Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3
http://www.jbiomedsem.com/content/5/S1/S3
Page 5 of 12
public data, and whose SPARQL endpoint is publicly available; the other which con-
tains the entire data and is kept secure using an API key. The secure, administrative
endpoint is used by R scripts (described in the next section) to access data for query
and analysis by members who have access authorization. The other benefit of having
decoupled stores is that we have the flexibility of optimizing the performance and scal-
ability of each store independently from the other.
R Integration
We wanted to provide programmatic access to the repository data to retrieve experi-
mental information in a manner that is independent of the Drupal database schema.
The R statistical programming language [39] and platform is a popular tool for analyz-
ing genomics data. Thus, we decided to provide support for accessing RDF data and
the SPARQL endpoint using R. The publicly available R packages to access RDF data
are not yet fully featured; for example the SPARQL package doesnt support
DESCRIBE queries. Hence the RDF package that does support DESCRIBE statements
was used to provide information about the resources. Using the package, first the
experiment RDF is used to obtain information about the assays, and then the assays
provide information about the biomaterial (See relationships in Figure 2). The RDF
package also had problems; it is hindered by UTF8 encoding issues. The resulting R
scripts included in the eXframe package produce data structures compatible for analy-
sis with R packages such as BioConductor [40,41].
Results
Case study: Stem Cell Commons
Stem Cell Commons (SCC) is a project of the Harvard Stem Cell Institute (HSCI) to
freely share biomedical data, tools and resources within the research community [42].
Our platform, eXframe, was first implemented independently for the Blood genomics
program at HSCI, and then later extended to support all researchers at the Institute, as
the repository of Stem Cell Commons. Data from both the previously developed Blood
Genomics store and the Stem Cell Discovery Engine (SCDE) [43] was merged into the
eXframe-based SCC database.
Genomics datasets are actively curated into the database; currently the repository con-
tains over 200 datasets from 20 laboratories representing 4 organisms and 119 different cell
types and 39 tissue types. Results based on approximately half of the datasets (86) have
been published in scientific journals, and these datasets are therefore available to the public.
All bioassays and samples have been deeply annotated with ontologies. First we used
the OBI ontology [9] for the main entities (experiment, biomaterial and assays) as
described in the data model section. Dublin Core [29] and FOAF [31] were used for the
metadata and researcher respectively. The ontologies used to annotate the biomaterials
are listed in Table 1. All the Stem Cell Commons public data is available as Linked Data
as well as a SPARQL endpoint as described in the next sections.
RDF generation
RDF for the experiment, bioassay and biomaterials are automatically generated using
the Drupal RDF modules as described previously. A screenshot of actual RDF output
for an experiment curated in the Stem Cell Commons is depicted in Figure 4. It is a
Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3
http://www.jbiomedsem.com/content/5/S1/S3
Page 6 of 12
next-generation sequencing experiment performed by a HSCI researcher and measures
DNA methylation (using bisulphite sequencing) in the leukemia cell line K562, repro-
grammed leukemia cell lines (LiPS) and the human embryonic stem cell line H1. From
Figure 4, we see how the Dublin Core ontology provides the provenance information
JOURNAL OF
BIOMEDICAL SEMANTICS
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30
http://www.jbiomedsem.com/content/5/1/30RESEARCH Open AccessA framework for assessing the consistency of
drug classes across sources
Rainer Winnenburg and Olivier Bodenreider*Abstract
Background: The objective of this study is to develop a framework for assessing the consistency of drug classes
across sources, such as MeSH and ATC. Our framework integrates and contrasts lexical and instance-based ontology
alignment techniques. Moreover, we propose metrics for assessing not only equivalence relations, but also inclusion
relations among drug classes.
Results: We identified 226 equivalence relations between MeSH and ATC classes through the lexical alignment, and
223 through the instance-based alignment, with limited overlap between the two (36). We also identified 6,257
inclusion relations. Discrepancies between lexical and instance-based alignments are illustrated and discussed.
Conclusions: Our work is the first attempt to align drug classes with sophisticated instance-based techniques, while
also distinguishing between equivalence and inclusion relations. Additionally, it is the first application of aligning
drug classes in ATC and MeSH. By providing a detailed account of similarities and differences between drug classes
across sources, our framework has the prospect of effectively supporting the creation of a mapping of drug classes
between ATC and MeSH by domain experts.
Keywords: Drug classes, MeSH, ATC, Instance-based mapping, Lexical mappingBackground
Motivation and objectives
Drug classes provide a convenient mechanism for organizing
drugs in terms of chemical structure (e.g., Sulfonamidesa
group of compounds that contain the structure SO2NH2),
function (e.g., Anti-Bacterial Agentsoften referred to as an-
tibiotics), mechanism of action (e.g., Hydroxymethylglutaryl-
CoA Reductase Inhibitorsa group of drugs, also called
statins, which block an enzyme involved in the production
of cholesterol in the liver), metabolism (e.g., inhibitors of
CYP2C9drugs that block an enzyme from the Cytochrome
P450 protein family, which is involved in the metabolism
of drugs, such as ibuprofen and fluoxetine, and whose ac-
tivity is influenced by other drugs, such as rifampicin and
fluconazole), and adverse events (e.g., drugs that induce
QT prolongationthe antimalarial drug halofantrine slows
down ventricular repolarization, which predisposes to
certain types of arrhythmias). The interested reader is
referred to [1] for more details about drug classes.* Correspondence: obodenreider@mail.nih.gov
Lister Hill National Center for Biomedical Communications, National Library
of Medicine, Bethesda, MD, USA
© 2014 Winnenburg and Bodenreider; license
of the Creative Commons Attribution License
distribution, and reproduction in any medium
Domain Dedication waiver (http://creativecom
article, unless otherwise stated.Several drug classifications have been developed for
different purposes. For example, the Anatomical Therapeutic
Chemical (ATC) classification of drugs supports pharmacoe-
pidemiology, while the Medical Subject Headings (MeSH)
is oriented towards the indexing and retrieval of the
biomedical literature [2,3]. Moreover, sources tend to
provide different lists of drug classes, and such lists
tend to be organized in different ways according to the
purpose of a given source. For example, the ATC uses
a complex classificatory principle, in which the first
subdivision is primarily anatomical (i.e., distinction
based on the target organs or anatomical systemse.g.,
cardiovascular system drugs vs. dermatologicals), followed
by a therapeutic subdivision (i.e., therapeutic intent of the
drugs in each anatomical groupe.g., antibacterial drugs
vs. antiviral drugs), followed by a chemical subdivision
(i.e., distinction between the structural and functional
characteristics of drugs within a therapeutic subgroupe.
g., macrolides, such as erythromycin, vs. fluoroquinolones,
such as ciprofloxacin, among the antibacterial drugs). On
the other hand, MeSH maintains two parallel classifica-
tions, one based on chemical structure (e.g., ciprofloxacin
is represented under fluoroquinolones), and one based one BioMed Central Ltd. This is an Open Access article distributed under the terms
(http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,
, provided the original work is properly credited. The Creative Commons Public
mons.org/publicdomain/zero/1.0/) applies to the data made available in this
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 2 of 14
http://www.jbiomedsem.com/content/5/1/30functional characteristics, including mechanism of action,
physiologic effect and therapeutic use. (e.g., ciprofloxacin
is linked to the mechanism of action Topoisomerase II In-
hibitors and to the therapeutic use Anti-Bacterial Agents).
In contrast to ATC, MeSH does not make distinctions
based on the target anatomical location of the drug
(e.g., there are two Fluoroquinolones classes for oph-
thalmological use vs. for systemic use in ATC, but only
one Fluoroquinolones class in MeSH).
Ideally, drug classes with similar names should have
similar members and drug classes with similar members
should have similar names. In practice, however, the same
name can be used to refer to different classes. For ex-
ample, in ATC, Fluoroquinolones refers to both a set of
ophthalmological drugs (8 members) and a set of systemic
drugs (20 members), while, in MeSH, it refers to over 50
chemical compounds with similar structural properties. In
the absence of an authoritative reference for drug classes,
the task of determining when two classes are equivalent
across sources remains extremely challenging. At the same
time, the use of multiple classifications is often required in
applications. This is increasingly the case as the use of
ATC for pharmacovigilance is on the rise (e.g., [4]).
The objective of this study is to develop a framework
for assessing the consistency of drug classes across sources,
leveraging multiple ontology alignment techniques. This
framework is meant to assist experts in the curation of a
mapping between drug classes across sources. We present
two applications of this framework, one to the alignment of
drug classes between MeSH and ATC, and the other to the
integration of MeSH and ATC drug class hierarchies. To
our knowledge, this work represents the first effort to align
drug classes between MeSH and ATC using a sophisticated
instance-based alignment technique. Moreover, we propose
metrics for assessing not only equivalence relations be-
tween classes, but also inclusion relations.
Application of ontology alignment techniques to drug classes
The broad context of this study is that of ontology alignment
(or ontology matching). Various techniques have been pro-
posed for aligning concepts across ontologies, including lex-
ical techniques (based on the similarity of concept names),
structural techniques (based on the similarity of hierarchical
relations), semantic techniques (based on semantic similarity
between concepts), and instance-based techniques (based on
the similarity of the set of instances of two concepts). An
overview of ontology alignment is provided in [5]. The main
contribution of this paper is not to propose a novel tech-
nique, but rather to apply existing techniques to a novel
objective, namely aligning drug classes between MeSH
and ATC. To this end, we use lexical and instance-based
techniques, because the names of drug classes and the list
of drugs that are members of these classes are the main
two features available in these resources.Lexical techniques
Lexical techniques compare concept names across on-
tologies and are a component of most ontology align-
ment systems [5]. When synonyms are available, they
can be used to identify additional matches. Matching
techniques beyond exact match utilize edit distance or
normalization to account for minor differences between
concept names.
As part of the Unified Medical Language System (UMLS),
linguistically-motivated normalization techniques have been
developed specifically for biomedical terms [6]. UMLS
normalization abstracts away from inessential differences,
such as inflection, case and hyphen variation, as well as
word order variation. The UMLS normalization techniques
form the basis for integrating terms into the UMLS
Metathesaurus, but can be applied to terms that are not
in the UMLS. For example, the ATC class Thiouracils
(H03BA) and the MeSH class Thiouracil (D013889) match
after normalization (ignoring singular/plural differences).
Lexical techniques typically compare the names of con-
cepts across two ontologies as provided by these ontologies.
However, additional synonyms can be used, for example,
synonyms from the UMLS Metathesaurus. In other words,
we leverage cosynonymy similarity for matching drug clas-
ses. In this case, although the ATC class Anticholinesterases
(N06DA) and the MeSH class Cholinesterase Inhibitors
(D002800) do not match lexically, both names are cosyno-
nyms, because they are found among the synonyms of the
UMLS Metathesaurus concept C0008425.
While there have been attempts to map individual drugs
from ATC to concepts in the UMLS and MeSH through
lexical techniques, [7] note that these techniques are not
appropriate for the mapping of drug classes.
Instance-based techniques
Also called extensional techniques, instance-based tech-
niques compare classes based on the sets of individuals
(i.e., instances) of each class. While instance-based tech-
niques are also available in many ontology alignment sys-
tems, the applicability of this technique is limited, because
most biomedical ontologies consist of class hierarchies, but
do not contain information about instances. Here, however,
individual drugs (e.g., atorvastatin) are the membersnot
subclassesof drug classes (e.g., statins). In other words,
drug classes have individual drugs as instances, not sub-
classes and are therefore amenable to instance-based
alignment techniques.
Several methods have been proposed to implement
instance-based matching. [8] decompose these methods into
three basic elements: (1) A measure is used for evaluating
the association between two classes based on the proportion
of shared instances. Typical measures include information-
based measures (e.g., Jaccard similarity coefficient) and stat-
istical measures (e.g., log likelihood ratio). (2) A threshold is
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 3 of 14
http://www.jbiomedsem.com/content/5/1/30applied to the measures and pairs of classes for which
the measure is above the threshold are deemed closely
associated and mapping candidates. (3) Hierarchical re-
lations in the two ontologies to be aligned can also be
leveraged by deriving instance-class relations between
instances of a given class and the ancestors of this class.
In other words, in addition to asserted classes (i.e., the
classes of which individual drugs are direct members),
we also consider inferred classes (i.e., the classes of
which asserted classes are subclasses). For example,
the class asserted in MeSH for the drug atorvastatin is
Hydroxymethylglutaryl-CoA Reductase Inhibitors (i.e.,
statins), whose parent concepts include Anticholesteremic
Agents. Therefore, the class Anticholesteremic Agents is
an inferred drug class for atorvastatin.
To our knowledge, our work is the first attempt to align
drug classes with instance-based techniques (i.e., beyond
name matching), and the first application of aligning drug
classes in ATC and MeSH. Moreover, while most ontology
alignment systems mainly consider matches between
equivalent classes, we are also interested in identifying
those cases where one class is included in another class.
Related work on drug classes, MeSH and ATC
In previous work, we compared drug classes between
the National Drug File-Reference terminology (NDF-RT)
and SNOMED CT from the perspective of semantic
mining [9]. We also used an instance-based alignment
technique, but only considered overlap between classes,
not inclusion. Lexical alignment of the classes was not
performed. Overall, we found that the overlap between
NDF-RT and SNOMED CT classes was very limited. In
[10], we mapped selected drug classes between NDF-RT
and ATC through lexical techniques, observed the limi-
tations of lexical techniques for the alignment of drug
classes (also noted by [7]), and argued that the alignment
could be improved by identifying mappings between the
drugs in these classes.
As part of the EU-ADR project, [11] extracted adverse
drug reactions from the biomedical literature and mapped
MeSH drugs to ATC through the UMLS. However, their
mapping was limited to individual drugs and did not
include drug classes. The alignment of drug classes is
one element of the broader integration of drug information
sources in systems, such as the one developed by [12].
However, the preliminary version of their system integrates
ATC, NDF-RT, RxNorm and the Structured Product labels,
but not MeSH or the biomedical literature.
Resources
Our framework leverages several knowledge sources.
In addition to ATC and MeSH, the two sources of drug
classes we propose to align and from which we extract
information about drug-class membership, we also takeadvantage of RxNorm for aligning and normalizing indi-
vidual drugs, and of the UMLS Metathesaurus as a source
of synonymy for the lexical mapping of drug class names.
Anatomical Therapeutic Chemical Drug Classification
System (ATC)
The ATC is a clinical drug classification system developed
and maintained by the World Health Organization (WHO)
as a resource for drug utilization research to improve qual-
ity of drug use [2]. The system is organized as a hierarchy
that classifies clinical drug entities at five different levels: 1st
level anatomical (e.g., C: Cardiovascular system), 2nd level
therapeutic (e.g., C10: Lipid modifying agents), 3rd level
pharmacological (e.g., C10A: Lipid modifying agents, plain),
4th level chemical (e.g., C10AA: HMG CoA reductase
inhibitors), and 5th level chemical substance or ingre-
dient (e.g., C10AA05: atorvastatin). The 2013 version
of ATC integrates 4,516 5th-level drugs and 1,255 drug
groups (levels 1-4). We refer to these drug groups as
ATC classes.
Medical Subject Headings (MeSH)
The Medical Subject Headings (MeSH) is a controlled
vocabulary produced and maintained by the NLM [3]. It
is used for indexing, cataloging, and searching the bio-
medical literature in the MEDLINE/PubMed database,
and other documents. The MeSH thesaurus includes
26,853 descriptors (or main headings) organized in 16
hierarchies (e.g., Chemical and Drugs). Additionally,
MeSH provides about 210,000 supplementary concept
records (SCRs), of which many represent chemicals and
drugs (e.g., atorvastatin). Each SCR is linked to at least
one descriptor through a heading mapped to relation
(e.g., atorvastatin is associated with Heptanoic Acids and
Pyrroles). These descriptors mapped to generally denote
the chemical structure of the drug. While most chemical
descriptors provide a structural perspective on drugs,
some descriptors play a special role as they can be
used to annotate the functional characteristics of drug
descriptors and SCRs through a pharmacologic action
relation (e.g., atorvastatin is linked to the mechanism of
action Hydroxymethylglutaryl-CoA Reductase Inhibitors
and to the therapeutic use Anticholesteremic Agents).
MeSH 2013 is used in this study.
RxNorm
RxNorm is a standardized nomenclature for medications
produced and maintained by the U.S. National Library
of Medicine (NLM) [13]. RxNorm concepts are linked
by NLM to multiple drug identifiers for commercially
available drug databases and standard terminologies,
including MeSH and ATC. (While RxNorm integrates
drugs and classes from ATC and drugs from MeSH, it
does not integrate classes from MeSH.) RxNorm serves
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 4 of 14
http://www.jbiomedsem.com/content/5/1/30as a reference terminology for drugs in the U.S. The
August 2013 version of RxNorm used in this study inte-
grates 10,108 substances, including ingredients (IN) and
precise ingredients (PIN). Ingredients generally represent
base forms (e.g., atorvastatin), while precise ingredients tend
to represent esters and salts (e.g., atorvastatin calcium).
RxNorm also represents clinical drugs, i.e., the drugs
relevant to clinical medicine (e.g., atorvastatin 10 MG
Oral Tablet). The relations among the various drug entities
are represented explicitly in RxNorm (e.g., between
ingredients and precise ingredients, and between in-
gredients and clinical drugs). NLM also provides an
application programming interface (API) for accessing
RxNorm data programmatically [14].
Unified Medical Language System (UMLS)
The UMLS is a terminology integration system created and
maintained by the National Library of Medicine (NLM)
[15]. The UMLS Metathesaurus integrates over 150 ter-
minologies, including MeSH, but not ATC. Synonymous
terms across terminologies are grouped into concepts
and assigned the same concept unique identifier. The
Metathesaurus provides a comprehensive set of synonyms
for biomedical concepts, including drug classes, and is
often used for integrating terminologies beyond its own
(e.g., [16]). Therefore, the UMLS is a useful resource for
mapping class names from ATC to drug class concepts
present in the source vocabularies of the Metathesaurus.
NLM provides an application programming interface
(API) for accessing UMLS data programmatically. Version
2013AA of the UMLS is used in this studya.
Methods
Our framework for assessing the consistency of drug classes
across sources (here MeSH and ATC) uses techniques for
aligning drug classes based on their names and drug
instances as depicted in Figure 1. It can be summarized
as follows. Having established a reference list of drugs
and drug classes, we compare the drug classes between
MeSH and ATC based on their names (lexical align-
ment, Figure 1, right) and on the individual drugs these
classes contain as members (instance-based alignment,
Figure 1, left). Toward this end, we leverage similarityRxNorm
INs/PINs
MeSH
drugs
ATC
drugs ATC (classes)
MeSH (classes)
instance-based 
alignment
lexical
alignment
Figure 1 Alignment of ATC and MeSH classes.measures to compare the set of drugs in a class to the
set of drugs in another class from the dual perspective
of equivalence and inclusion. Finally, we compare the
alignments obtained by the two approaches.
Establishing a common reference for drugs, drug classes
and drug-class members
Drugs
As of August 2013, both ATC and MeSH are integrated
in RxNorm. We consider all MeSH drugs present in
RxNorm, regardless as to whether they correspond to
descriptors (also called main headings) or Supplementary
Concept Records (SCR) in MeSH. Our starting set of ATC
drugs consists of 5th-level ATC entities, from which we
exclude combination drugs, often underspecified and
unlikely to be represented in MeSH.
As a result of the integration of MeSH and ATC into
RxNorm, the same RxNorm identifier is assigned to an
ATC drug and to the equivalent drug in MeSH. Individ-
ual drugs in MeSH and ATC correspond to ingredients
(IN) and precise Ingredients (PIN) in RxNorm. In order
to facilitate the comparison of individual drugs between
MeSH and ATC, we normalize the drugs by mapping
each precise ingredient to its corresponding ingredient.
We restrict our set of drugs to drugs of clinical relevance
by filtering out those ingredients that are not associated
with any clinical drugs in RxNorm. The set of individual
drugs described here constitutes the set of eligible drugs
for this study.
Drug classes
In order to minimize the number of pairwise compari-
sons between MeSH and ATC drug classes, we exclude
broad, top-level classes from MeSH and ATC, for
which the alignment would not be meaningful anyway.
In practice, we exclude the 14 ATC classes of level 1
(anatomical classification). Similarly, we exclude the
top-level descriptors of the Chemicals and Drugs hier-
archy (i.e., D01-D27) in MeSH, as well as the top-level of
the pharmacological action descriptors (Pharmacologic
Actions, Molecular Mechanisms of Pharmacological Action,
Physiological Effects of Drugs, and Therapeutic Uses).
Additionally, we exclude 167 of the 1,241 ATC classes
(2nd4th level) corresponding to drug combinations,
because combination drugs are often underspecified in
ATC. We define drug combination classes in ATC as
classes that contain combination (case-insensitive) in their
labels or have ancestor classes with combination in their
labels (e.g., G03EA: Androgens and estrogens are excluded
along with their ancestor class G03E: ANDROGENS
AND FEMALE SEX HORMONES IN COMBINATION).
Finally, we further exclude from MeSH and ATC any
classes that are not connected to any eligible individual
drug (as defined above), directly or through a subclass
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 5 of 14
http://www.jbiomedsem.com/content/5/1/30(e.g., A03AC: Synthetic antispasmodics, amides with
tertiary amines contains three drugs (dimethylamino-
propionylphenothiazine, nicofetamide, tiropramide), of
which none are eligibleb). The set of drug classes de-
scribed here constitutes the set of eligible drug classes
for this study.
Drug-class membership
As mentioned earlier, the relation between a class and
its drug members can be either direct (i.e., asserted) or
indirect (i.e., inferred). In ATC, we consider as direct re-
lations the relations asserted between 5th-level drugs
and their 4th-level chemical classes. We infer drug-class
relations between 5th-level drugs and the corresponding
ATC classes at the 3rd and 2nd level. For example, as il-
lustrated in Figure 2, the drug temafloxacin (J01MA05)
is a member of the chemical class Fluoroquinolones
(J01MA - asserted), the pharmacological class QUINOLONE
ANTIBACTERIALS (J01Minferred, 3rd level), and the
therapeutic class ANTIBACTERIALS FOR SYSTEMIC USE
(J01inferred, 2nd level). Level-1 classes are ignored.Figure 2 Individual drugs and drug classes in RxNorm, MeSH and ATCExtracting drug-class membership relations from MeSH
is a more complex process, because drugs can be repre-
sented at different levels (descriptor or supplementary
concept record), structural classes and functional classes
are represented by different types of descriptors, and drugs
are related to classes through various kinds of relationships.
Relations between drugs (descriptors or SCRs) and func-
tional classes (i.e., descriptor from the pharmacological
actions hierarchy) are asserted through a pharmacologic
action relationship. Relations between an SCR drug and
its heading mapped toc constitute the asserted relations
to structural classes, as do relations between a descriptor
drug and its direct parent. We infer drug-class relations
between any drug and all the ancestors (direct or indirect)
of the descriptors corresponding to their structural and
functional (asserted) classes.
For example, as illustrated in Figure 2, the SCR
temafloxacin has Anti-Bacterial Agents as pharmacological
action and Fluoroquinolones as heading mapped to. From
these asserted classes, we infer membership to Anti-Infective
Agents (from Anti-Bacterial Agents) and to Quinolones,.
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 6 of 14
http://www.jbiomedsem.com/content/5/1/30Quinolines, and Heterocyclic Compounds, 2-Ring (from
Fluoroquinolones). Top-level classes are ignored.
Aligning drug classes
Lexical alignment
We leverage the UMLS (synonyms and lexical match-
ing features) for aligning drug classes by their names.
In practice, we consider equivalent classes those MeSH
and ATC classes, whose names map to the same UMLS
concept. If both MeSH and ATC were integrated in the
UMLS, we would only have to extract all UMLS con-
cepts to which both a MeSH class and an ATC class
are mapped. Since MeSH is integrated in the version of
the UMLS used in this study, but ATC is not, we map
ATC classes to the UMLS in order to link them to the
equivalent classes in MeSH. More precisely, we use the
ExactString and NormalizedString search function of
the UTS API 2.0 to establish mappings between the
names of the ATC classes and UMLS concepts. We use
normalization only when the exact technique does not
result in a match. We then associate the ATC class to a
MeSH class through the UMLS concept to which they
both map (e.g., H03BA: Thiouracils to D013889: Thiouracil
through UMLS concept C0039957).
Instance-based alignment
We assess the similarity between two classes based on
the individual drug members (instances) they share. In
practice, we perform a pairwise comparison between all
ATC classes and all MeSH classes, asserted and inferred.
We define two scores for identifying equivalence and in-
clusion relations between ATC and MeSH classes.
Equivalence Score (ES) The Jaccard coefficient (JC) is a
measure of the similarity between two sets, for example
between the set of drugs in a given ATC class (A) and in
a given MeSH class (M). However, many drug classes
only contain a small number of drugs, and, in this case,
a small number of shared drugs between classes can
yield relatively high Jaccard values. In order to reduce
the similarity of pairs of classes with small numbers of
shared drugs, we use a modified version of the Jaccard
coefficient, JCmod, as suggested in [8],
JC A;Mð Þ ¼ am
aþmþ am
ES A;Mð Þ ¼ JCmod A;Mð Þ ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
am am?0:8ð Þp
aþmþ am
where am represents the number of drugs common to
A and M, and a +m + am the total number of unique
drugs in both classes.
Inclusion Score (IS) The Jaccard coefficient measures
the similarity between the two classes, but does not reflectwhether one class is included in the other. Because of the
difference in organization and granularity between classes
in ATC and MeSH, a given ATC class may not have an
equivalent class in MeSH, but can be included in another
MeSH class (e.g., C07AA: Beta blocking agents, non-selective
included in D000319: Adrenergic beta-Antagonists). Such
inclusion relations are crucial for a comprehensive align-
ment of the drug classes. We introduce a metric for finding
fine-grained (child) classes that are included in coarse
(parent) classes. This metric combines two elements. The
first one measures the intensity of the one-sidedness, i.e.,
the extent to which the instances outside the intersection
are not distributed between both sides, but rather belong to
only one of the two classes. The second element measures
the coverage of the finer-grained (child) class by the
coarser (parent) class.
IS is calculated as follows:
IS A;Mð Þ ¼ 0; for Cp A;Mð Þ ¼ 0 and Cc A;Mð Þ ¼ 0
IS A;Mð Þ ¼ a?m
aþm
am
min amþ a; amþmð Þ ; otherwise
where am represents the number of drugs common to A
and M, and a and m the number of drugs specific to A
and M, respectively.
For example, if A contains 10 drugs and M contains
20 drugs and if the two classes share 9 drugs, IS(A,M)= 0.75,
providing a strong indication that A is included in M.
More generally, a value of IS close to 0 indicates that
the drugs that are not shared by the two classes are
evenly distributed between the ATC and MeSH class, i.
e., there is no inclusion relation between the classes. In
contrast, a value of IS close to 1 (in absolute value) indi-
cates that the parent class contains most of the drugs
that are not shared by the two classes and that the child
class has a small proportion of specific drugs. The IS(A,M)
score varies between ?1 and 1, and a score of 1 corresponds
to the inclusion of A in M, while a score of ?1 corresponds
to the inclusion of M in A.
Selecting classes with the best equivalence and inclusion
relations A given class in ATC or MeSH may have both
equivalence and inclusion relations to classes from the
other terminology. Moreover, it may have more than
one equivalence relation and often has multiple inclu-
sion relations. We propose an approach for selecting
the best equivalence and inclusion relations for a given
class. We heuristically determined 0.5 to be a reasonable
threshold for both ES and IS. Therefore, none of the
pairs of classes with ES or IS values lower than 0.5 will
be considered for equivalence or inclusion, respectively.
For a given class Cc, the class Ce selected as the best
equivalent class is the one with the highest ES. In con-
trast, the class Cp selected as the best inclusion class is
not necessarily the one with the highest IS, because the
Table 1 Selection of the ATC and MeSH classes suitable
for the instance-based alignment
ATC MeSH
Candidate drugs in terminology 2,730 4,153
Corresponding drug entities in RxNorm (IN, PIN) 2,239 5,274
Drug entities after normalization of PINs to INs 2,215 4,112
Restriction to clinically-significant ingredients 1,706 2,339
Restriction to clinically-significant ingredients
present in both terminologies
1,685 1,685
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 7 of 14
http://www.jbiomedsem.com/content/5/1/30class with the highest IS is most likely a very broad class.
IS favors large parent classes, while the best parent class
is the smallest parent class that covers a large propor-
tion of the child class. Therefore, we select as the best
inclusion relation the first pair among the best candi-
date equivalence pairs for which IS is above the thresh-
old of 0.5. Although it might seem counterintuitive to
select inclusion pairs among the candidate equivalence
pairs, the high ES is consistent with the requirement for
coverage of the child class by the parent class.
Usually the best equivalence and inclusion pairs are
different, but not always. For instance, the mapping be-
tween two very similar classes, where one class contains
a few specific drugs, might have both IS and ES above
the threshold. Different use cases may call for different
strategies for determining the best equivalent and inclu-
sion pairs. For instance, while our strategy considers
both scores, ES and IS, when they are above the thresh-
old, an alternative strategy could be to choose one score
over the other based on max(ES, IS).
Assessing the consistency between lexical and
instance-based alignments
We hypothesize that classes with similar drugs should
have similar names and classes with similar names should
contain similar drugs. We compare the results of the
lexical and instance-based alignment methods and assess
their consistency. We expect the lexical alignment to iden-
tify equivalence classes, not class inclusion. Therefore, pairs
of classes identified through the lexical alignment (LEX+)
and identified as equivalent through the instance-based
alignment (EQ+) are considered consistent, as are the pairs
of classes neither identified through the lexical alignment
(LEX-) nor identified as equivalent through the instance-
based alignment (EQ-). Conversely, pairs of classes identified
through the lexical alignment (LEX+) but not identified as
equivalent through the instance-based alignment (EQ-) are
considered inconsistent, as are the pairs of classes not identi-
fied through the lexical alignment (LEX-) but identified as
equivalent through the instance-based alignment (EQ+).
Results
Establishing a common reference for drugs, drug classes
and drug-class members
Drugs
As shown in Table 1, we retrieved from RxNorm 2,239
Ingredients (IN) and Precise Ingredients (PIN) that are
mapped to 2,730 unique drugs in ATC, and 5,274 that are
mapped to 4,153 drugs in MeSH. After normalization to
INs, we selected 2,215 INs for ATC and 4,112 for MeSH.
Finally, after restricting the RxNorm INs to those that are
clinically relevant, we selected 1,706 INs for ATC and
2,339 for MeSH. Of these, 1,685 drugs are present in both
ATC and MeSH.Drug classes
From the 1,255 ATC classes (1st4th level) we excluded
14 ATC classes at the 1st level (anatomical classification)
and 167 classes corresponding to drug combinations,
leaving 1,074 classes eligible for the lexical alignment.
We further excluded 81 empty classes without any
drug (ATC contains empty classes by design), and 159
classes containing only drugs that cannot be mapped to
RxNorm. The final set of ATC classes eligible for the
instance-based alignment, A*, contains 834 drug classes,
of which 558 are considered asserted (4th level) and 276
inferred (2nd3rd level).
In MeSH, we identified 1,516 descriptors as drug classes
for the eligible drugs, including 1,223 asserted classes and
293 inferred classes. These classes constitute the set of
MeSH classes eligible for both the lexical and the instance-
based alignment, M*. We classify 403 of the drug classes in
M* as functional classes, i.e., their descriptors are located in
the Chemical Actions and Uses [D27] sub-tree in MeSH,
and 1,113 as structural classes.
Drug-class membership
For the 1,685 eligible drugs in MeSH, we established
15,122 drug-class pairs, of which 4,759 are asserted
and 10,363 inferred. For the eligible drugs in ATC, we
established 6,368 drug-class pairs, of which 2,140 are
asserted and 4,228 inferred.
Aligning drug classes
Lexical alignment
For the 1,074 eligible ATC classes, we were able to retrieve
226 mappings to descriptors from the Chemicals and Drugs
([D]) tree in MeSH. We found 18 mappings for therapeutic
classes (2nd level), 43 for pharmacological classes (3rd level),
and 165 for chemical classes (4th level). Of the 226 map-
pings, 99 are to pharmacological actions (functional classes)
in MeSH, whereas 127 are to other descriptors at various
levels of the MeSH hierarchy (structural classes).
Instance-based alignment
Equivalence and inclusion scores Of the 834 ATC
classes eligible for instance-based alignment (|A*| = 834),
828 (99%) could be associated with at least one MeSH class.
Table 3 Characterization of the associations between
ATC and MeSH classes based on scores for equivalence
and inclusion
ATC to MeSH Best equivalence
>.5 <.5 Total
Best inclusion >.5 148 (17%) 580 (70%) 728 (87%)
<.5 1 (1%) 99 (12%) 100 (13%)
Total 149 (18%) 679 (82%) 828 (100%)
MeSH to ATC Best equivalence
>.5 <.5 Total
Best inclusion >.5 120 (9%) 390 (30%) 510 (39%)
<.5 45 (3%) 762 (58%) 807 (61%)
Total 165 (12%) 1,152 (88%) 1,317 (100%)
Table 4 Consistency between lexical and instance-based
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 8 of 14
http://www.jbiomedsem.com/content/5/1/30Of the 1,516 eligible drug classes in MeSH (|M*| = 1,516),
1,317 (87%) could be associated with at least one ATC class.
We conducted a pairwise comparison of all ATC classes
with all MeSH classes (|A*| x |M*| = 1,264,344). For the
26,842 pairs that had at least one drug in common, we cal-
culated the equivalence (ES) and inclusion (IS) scores. As
shown in Table 2, 223 pairs (<1%) had an ES ? .5 and were
considered equivalent (EQ+), and 6,257 pairs (23%) had an
IS ? .5 and were considered in inclusion relation (IN+). Of
note, there were 108 pairs with both strong equivalence and
inclusion relations (EQ+ and IN+). The remaining 20,470
pairs were considered unrelated, absent any strong equiva-
lence or inclusion relations (EQ- and IN-).
Classes with strong equivalence and inclusion relations
A given class in ATC or MeSH may have more than one
strong relation to a drug class from the other terminology.
We determined the best equivalence and inclusion map-
pings (not mutually exclusive) for each of the 828 ATC and
1,317 MeSH classes with shared drugs, respectively.
As shown in Table 3 (top), 828 ATC classes had some re-
lation (equivalence or inclusion, but not necessarily strong)
to a MeSH class. Of these, we identified 149 ATC classes
(18%) with at a strong equivalence relation to MeSH, all
but one of which also showed a strong inclusion to some
MeSH class (albeit not necessarily the same as the equiva-
lent class). A strong inclusion relation to MeSH was found
for 728 (87%) of these ATC classes. On the other hand,
1,317 MeSH classes had some relation to an ATC class.
Of these, we identified 165 MeSH classes (12%) with a
strong equivalence relation to ATC, most of which also
showed a strong inclusion relation to some ATC class. A
strong inclusion relation to ATC was found for 510 (39%)
of these MeSH classes (Table 3, bottom). The 1,317 MeSH
classes linked to ATC include 374 functional classes (28%)
and 943 structural classes (72%). Overall, a strong relation
(equivalence or inclusion) was found between 729 ATC
classes in ATC and the 555 MeSH classes.
Assessing the consistency between lexical and
instance-based alignments
The results of the comparison between the lexical and
instance-based alignments are shown in Table 4. We
performed the comparison on the cross-product of the 834Table 2 Analysis of the instance-based alignment between
ATC and MeSH classesequivalence vs. inclusion relations
Inclusion relation
Yes (IN+) No (IN-) Total
Equivalence relation Yes (EQ+) 108 115 223
No (EQ-) 6,149 20,470 26,619
Total 6,257 20,585 26,842eligible ATC and 1,516 MeSH classes (1,264,344 pairs). Of
the 226 pairs of equivalent classes between ATC and MeSH
identified through the lexical alignment, 36 (16%) were con-
firmed through the instance-based approach (LEX+/EQ+),
of which 14 were also categorized as inclusion relations.
Not surprisingly, no equivalence relation was identified by
either approach for the bulk of the pairs from the cross-
product between ATC and MeSH classes. A total of 313 in-
consistencies between the two alignment approaches were
identified, including 126 pairs identified exclusively by the
lexical alignment (LEX+/EQ-), and 187 pairs specific to the
instance-based alignment (LEX-/EQ+). This finding dis-
proves our initial hypothesis that classes with similar names
have similar drugs and vice versa. Of note, 64 pairs of
equivalent classes identified through the lexical alignment
were not amenable for processing by the instance-based
alignment, because at least one class of the pair did not
contain any eligible drug.
Discussion
Analysis of similarities and discrepancies between lexical
and instance-based alignments
As illustrated through a few examples throughout this
section, our framework facilitates the comparison of drug
classes across sources and reveals inconsistencies in the
classes, as well as deficiencies in the alignment techniques.alignments of drug classes (italics values denote
inconsistencies)
Lexical alignment
Yes (LEX+) No (LEX-) Total
Instance-based
alignment
Yes (EQ+) 36 187 223
No (EQ-) 126 1,263,995 1,264,121
Total 162 1,264,182 1,264,344
No data 64
Total LEX+ 226
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 9 of 14
http://www.jbiomedsem.com/content/5/1/30Valid mappings
We identified an equivalence relation between the 4th-level
ATC class Tetracyclines (J01AA) and the MeSH descriptor
Tetracyclines (D013754). The two classes share nine drugs.
The MeSH class has one extra drug (meclocycline), which is
in a different class in ATC (Antiinfectives for treatment of
acne), because, although structurally similar, it is not used
systemically but topically. Jaccard similarity is high (0.86).
This (equivalence) mapping is also identified by the
lexical technique (exact match). Of note, the inclusion
score (1 in absolute value) is also high, because there is only
one drug that is not in common, which is - automatically -
located on only one side of the intersection.Erroneous lexical mappings
We identified an inclusion mapping between the 4th-
level ATC class Fluoroquinolones (S01AE) and the MeSH
descriptor Fluoroquinolones (D024841). Although the two
class names are identical, which would suggest an equiva-
lence relation, our mapping is identified as an inclusion,
with seven drugs in common, one drug specific to the
ATC class and eleven drugs specific to the MeSH class. In
fact, the ATC class is the specific class of fluoroquinolones
for ophthalmic use (S01AE), in contrast to the class of
fluoroquinolones for systemic use (J01MA)d. The fluor-
oquinolones used for eye disorders are (almost) a subset
of all fluoroquinolones and the ATC class S01AE is ap-
propriately characterized as being included in the MeSH
class for fluoroquinolones. This example also constitutes
an erroneous lexical mapping, since lexical mappings are
expected to reflect equivalence relations.Missing instance-based mappings
Many ATC and MeSH classes share only one or very
few drugs, making it difficult to assess equivalence or inclu-
sion with confidence. For example, the 4th-level ATC class
Silver compounds (D08AL) and the MeSH descriptor Silver
Compounds (D018030) share only one drug (silver nitrate),
where Silver Compounds (D018030) contains another drug
(silver acetate), which is in RxNorm but not in ATC. The
modified version of the Jaccard coefficient has a score of
0.22 in this case, which is below our threshold of 0.5 for
equivalence. However, we classified the ATC class D08AL
as being included in the MeSH class Silver Compounds.
During this failure analysis, we discovered that some
MeSH drugs did not have a pharmacological action
assigned to them as we expected. For example, while pyr-
antel is listed as Antinematodal Agents, oxantel is note.
The MeSH editorial rules require that a certain number of
articles assert a given pharmacologic action for it to be
recorded in MeSH. Because of these missing pharmaco-
logic actions, the 3rd-level ATC class ANTINEMATODAL
AGENTS (P02C) fails to be mapped to the MeSHpharmacological action Antinematodal Agents (D000969),
the Jaccard similarity being below the threshold (0.37).
As mentioned earlier, some ATC classes only contain
drugs that cannot be mapped to MeSH through RxNorm,
which we used to bridge between the two. Such classes
may be amenable to lexical alignment, but cannot be
aligned through their instances. Similarly, some drug
entities and biologicals (e.g., vaccines) are less well stan-
dardized than most common drugs. For this reason, the
instance-based alignment may not be able to align these
classes, when simple lexical techniques can. For example,
the instance-based method fails to align the two classes
Epoxides (L01AG) and Epoxy Compounds (D004852) be-
cause the ATC class does not contain any eligible drug
(the only instance, etoglucid (L01AG01), is not listed as
a clinical drug in RxNorm).
Missing lexical mappings
Despite the use of UMLS synonymy and normalization,
the lexical alignment fails to identify a mapping between
the 3rd-level ATC class POTASSIUM-SPARING AGENTS
(C03D) and the MeSH pharmacological action Diuretics,
Potassium Sparing (D062865). In contrast, the instance-
based alignment identifies an equivalence mapping with
high Jaccard similarity (0.72). This finding is consistent with
the conclusions of [7].
Further characterization of equivalence and
inclusion relations
Even when considering only strong relations and the
best inclusion relations between ATC and MeSH classes,
it is difficult to give a detailed account of the direction-
ality of the relations, and the distribution between struc-
tural and functional classes. Some salient findings are
summarized in Table 5. For example, we found 223
(strong) equivalence relations between 149 unique ATC
classes and 165 unique MeSH classes, distributed al-
most evenly between structural and functional classes in
MeSH. When restricting the analysis to the best inclu-
sion relations, more ATC classes (728) are found to be
included in some MeSH class, than MeSH classes (510)
are in some ATC classes. And fewer functional classes
(146) than structural classes (364) in MeSH are included
in some ATC class.
For almost all drug classes in ATC that have an
equivalence mapping to a drug class in MeSH, there is
also at least one inclusion mapping to a broader class
in MeSH. There is only one exception. The class Drugs
used in diabetics (A10) is equivalent to Hypoglycemic
Agents (D007004), which is already at the highest level we
consider in MeSH (we ignore its parent class Physiological
Effects of Drugs because it is too general). In contrast,
there are 45 classes in MeSH that are equivalent to ATC
classes but are not included in another class in ATC. For
Table 5 Detailed analysis of the mapping between ATC
and MeSH classesStructural vs. functional classes
Type of
relation
Direction # strong
relations
# unique
ATC classes
# unique
ATC classes
Equivalence
(all)
ATC-MeSH (all) 223 149 165
ATC-MeSH (St) 115 77 84
ATC-MeSH (Fn) 108 86 81
Inclusion
(all)
ATC to
MeSH (all)
4914 728 650
MeSH (all)
to ATC
1343 358 510
Inclusion
(best)
ATC to
MeSH (all)
1267 728 483
ATC to
MeSH (St)
597 559 275
ATC to
MeSH (Fn)
670 657 208
MeSH (all)
to ATC
568 264 510
MeSH (St)
to ATC
406 211 364
MeSH (Fn)
to ATC
162 102 146
Details of the instance-based alignment between functional (Fn) and structural
(St) classes in ATC and MeSH.
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 10 of 14
http://www.jbiomedsem.com/content/5/1/30example, Antiparkinson Agents (D00978) maps to the 2nd
level class Anti-Parkinson Drugs (N04) in ATC. Because
we exclude 1st level classes in ATC, there is no parent
class in ATC which would include the drug of the MeSH
class Antiparkinson Agents. Conversely, the ATC class
Anti-Parkinson Drugs (N04) is included in the higher level
class Central Nervous System Agents (D002491) in MeSH,
which is a parent of Antiparkinson Agents.
The alignment between ATC classes and MeSH classes
can be further characterized, especially in order to ac-
count for concomitant occurrences of a strong inclusion
relation to a structural class and to a functional class. As
shown in Table 6, of the 505 strong equivalence and bestTable 6 Detailed analysis of the mapping between ATC
and MeSH classesequivalence vs. inclusion relations
ATC to MeSH To a
structural
class only
To a
functional
class only
To both a
structural and a
functional class
Total
Equivalence
relation only
0 1 0 1
Both equivalence
and best
inclusion relations
1 8 58 67
Best inclusion
relations only
50 75 312 437
Total 51 84 370 505
Analysis of concomitant equivalence and best inclusion relations between ATC
and MeSH classes, when structural and functional classes in MeSH are
considered separately.inclusion relations to structural and functional clas-
ses in MeSH, the most frequent situation is the con-
comitant occurrence of inclusion to both a structural
and a functional class. Of note, there is only one case
where an equivalence relation occurs without a con-
comitant inclusion relation.
Application of the framework to the alignment of
important drug classes
One typical use case for the alignment of drug classes
is to find equivalent classes in reference sources for a
given class (e.g., to find which class best represents
macrolides in MeSH and ATC). In order to illustrate
how our approach supports the alignment of drug
classes between MeSH and ATC, we applied our
framework to a set of clinically relevant drug clas-
ses. We used the set of high-severity, clinically sig-
nificant drugdrug interactions created by [17], in
which most drugs are categorized in reference to
drug classes.
We extracted all 13 drug classes from the list of veri-
fied critical drugdrug interactions discussed in their
paper (Table 7). We first performed a lexical mapping
to identify these 13 classes in MeSH and ATC (using
normalized string matches against the UMLS). Only
in six cases did the lexical mapping approach retrieve
classes in both classifications. In another six cases, we
were able to retrieve the class in either ATC or MeSH.
The class QT prolonging agents was not found in either
source.
For each drug class that we retrieved through lexical
mapping, we used our instance-based approach to deter-
mine the best corresponding class in the other termin-
ology. Table 8 shows the strength of the mappings in
terms of equivalence and inclusion. There is only one case
(HMG CoA reductase inhibitors) where the two lexical
matches also correspond to the best equivalent classes
based on the drug instances. For five other classes we
found equivalent class pairs starting from one lexical
match. For four classes we could not find equivalent
mappings across the two classifications, but inclusion map-
pings instead. Finally, three classes were left unmapped.
(Two of these classes were underspecified as evidenced by
the mention [and] derivatives in their name. The last one,
QT prolonging agents, was not represented in either source,
which is often the case for drug classes defined in reference
to adverse effects [18]).
This application illustrates the effectiveness of our
framework to support a clinical expert in the curation of
an alignment of drug classes between MeSH and ATC.
It helps identify lexically similar classes in these two
sources, but, more importantly, it helps identify which
class of the other source is most closely related to a
given class. This feature enables experts to verify if the
Table 7 Lexical mapping to ATC and MeSH for 13 clinically relevant drug classes
DDI class ATC class lexical match MeSH class lexical match Best corresponding
class in ATC
Best corresponding
class in MeSH
Triptans - Tryptamines (D014363) Selective serotonin (5HT1)
agonists (N02CC)
-
Proton pump inhibitors Proton pump inhibitors
(A02BC)
Proton pump inhibitors
(D054328)
- 2-Pyridinylmethylsulfinyl-
benzimidazoles (D053799)
HMG CoA reductase inhibitors HMG CoA reductase
inhibitors (C10AA)
Hydroxymethylglutaryl-CoA
Reductase Inhibitors (D019161)
- -
Tricyclic antidepressants - Antidepressive agents,
Tricyclic (D000929)
Non-selective monoamine
reuptake inhibitors (N06AA)
-
Protease inhibitors Protease inhibitors (J05AE) Protease inhibitors (D011480) - HIV Protease inhibitors
(D017320)
Narcotic analgesics - Narcotics (D009294) OPIOIDS (N02A) -
Selective serotonin
reuptake inhibitors (SSRIs)
Selective serotonin
reuptake inhibitors
(N06AB)
Serotonin uptake inhibitors
(D017367)
Selective serotonin reuptake
inhibitors (N06AB)
Serotonin uptake inhibitors
(D017367)
MAO inhibitors MAO inhibitors (C02KC) Monoamine oxidase
inhibitors (D008996)
Monoamine oxidase inhibitors,
non-selective (N06AF)
Benzylamines (D001596)
Macrolides Macrolides (J01FA) Macrolides (D018942) Macrolides (J01FA) Macrolides (D018942)
Azoles - Azoles (D001393) Imidazole and triazole
derivatives (D01AC)
-
Amphetamine derivatives - Amphetamines (D000662) - -
Ergot alkaloids and derivatives Ergot alkaloids
(C04AE, G02AB, N02CA)
Ergot Alkaloids (D004876) - Ergotamines (D004879)
QT prolonging agents - - - -
Lexical mapping to ATC and MeSH (columns 2-3) for 13 clinically relevant drug classes, along with their corresponding class in the other source obtained through
instance-based mapping (columns 4-5). Italicized classes denote best corresponding pairs of classes.
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 11 of 14
http://www.jbiomedsem.com/content/5/1/30equivalence suggested through lexical mapping is also
supported by a large proportion of shared drugs be-
tween these two classes. For example, the original class
Proton pump inhibitors is mapped lexically to Proton
pump inhibitors in ATC and to Proton Pump Inhibitors
in MeSH. The best corresponding class in MeSH for the
ATC class Proton pump inhibitors, however, is not Proton
Pump Inhibitors, but rather 2-Pyridinylmethylsulfinyl-
benzimidazolesf. Moreover, in many cases, the original
class can only be mapped lexically to either MeSH or
ATC. In these cases, the instance-based mapping offers
a solution for finding which class of the other source
has the best correspondence. For example, the original
class Tricyclic antidepressants can only be mapped
lexically to the class Antidepressive Agents, Tricyclic in
MeSH. However, the instance-based mapping identifies
the ATC class Non-selective monoamine reuptake inhibitors
as a potential equivalence.
While exploring mappings for these 13 clinically signifi-
cant drug classes, we actually found no cases where the best
corresponding classes in MeSH and ATC had exactly the
same members. Here are some reasons why.
 As mentioned earlier, the classificatory principles
used by ATC and MeSH are different. For example,Azoles represents a broad structural class in MeSH,
whereas ATC splits azole drugs into several classes
based on their therapeutic use (e.g., antibacterials
and antimycotics).
 Some drugs appear to be missing from ATC,
because of differences in the scopes of MeSH and
ATC. Such drugs include dietary supplements
(e.g., red yeast rice), veterinary drugs (e.g., many
macrolides exclusively marketed for veterinary use),
drugs of abuse (e.g., heroin) and drugs that only
exist in combinations (e.g., lopinavir and ritonavir,
but not lopinavir alone).
 Even though they are present in MeSH, some drugs
appear to be missing from MeSH classes, because of
missing relations to a drug class. For example, the
class assigned to tipranavir is Anti-HIV Agents,
while most of the drugs from the same ATC class
are (more appropriately) in the MeSH class HIV
Protease Inhibitors.
 In many cases, the name of an ATC class is
underspecified, i.e., derives part of its meaning from
its position in the hierarchy. As a consequence, the
lexical mapping of such class names is likely to point
to a broader class in MeSH. For example, the ATC
class Protease inhibitors is under the class Antivirals
Table 8 Best corresponding classes in ATC and MeSH for 13 clinically relevant drug classes
DDI class ATC class MeSH class Drugs
common
Drugs only
in ATC
Drugs only
in MeSH
ES. IS Rel.
Triptans Selective serotonin
(5HT1) agonists
Tryptamines 7 0 1 0.82 ?1 Eq
Proton pump inhibitors Proton pump inhibitors 2-Pyridinylmethylsulfinyl-
benzimidazoles
5 1 0 0.76 1 Eq
HMG CoA reductase inhibitors HMG CoA reductase
inhibitors
Hydroxymethylglutaryl-
CoA Reductase Inhibitors
8 0 2 0.76 ?1 Eq
Tricyclic antidepressants Non-selective monoamine
reuptake inhibitors
Antidepressive agents,
Tricyclic
10 2 2 0.69 0 Eq
Protease inhibitors Protease inhibitors HIV Protease inhibitors 8 3 1 0.63 0.44 Eq
Narcotic analgesics OPIOIDS Narcotics 15 3 11 0.50 ?0.48 Eq
Selective serotonin
reuptake inhibitors (SSRIs)
Selective serotonin
reuptake inhibitors
Serotonin uptake
inhibitors
6 0 8 0.40 ?1 In
MAO inhibitors Monoamine oxidase
inhibitors, non-selective
Monoamine oxidase
inhibitors
3 0 5 0.32 ?1 In
Macrolides Macrolides Macrolides 8 0 21 0.26 ?1 In
Azoles Imidazole and
triazole derivatives
Azoles 11 1 147 0.07 ?0.90 In
Amphetamine derivatives - - -
Ergot alkaloids and derivatives - - -
QT prolonging agents - - -
Best corresponding classes in ATC and MeSH for 13 clinically relevant drug classes, with the equivalence (ES) and inclusion (IS) scores from our frameworks
metrics, and the relation, equivalence or inclusion, between the two classes (Rel).
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 12 of 14
http://www.jbiomedsem.com/content/5/1/30for systemic use, which means that it represents not
all protease inhibitors, but only those that are used
to treat viral infections (which, in practice, means
HIV infections.)g In contrast, the MeSH class
Protease Inhibitors truly represent all drugs, whose
mechanism of action is to block some protease
enzyme. Therefore, despite the similarity of their
names, the ATC class Protease inhibitors is actually
included in the MeSH class with the same name,
and the best equivalence in MeSH for the ATC class
Protease inhibitors is actually the class HIV Protease
Inhibitors.
 Differences in granularity between MeSH and ATC
classes are also responsible for some of the
discrepancies observed in the mapping between the
two sources. For example, the MeSH class
Monoamine Oxidase Inhibitors is not found in ATC,
which provides three more specific classes instead
(Monoamine oxidase inhibitors, non-selective,
Monoamine oxidase A inhibitors, Monoamine
oxidase B inhibitors).
Application of the framework to the integration of the
MeSH and ATC classifications
The equivalence and inclusion relations obtained through
our framework can be combined in order to integrate the
hierarchical structures of two drug classifications, such asMeSH and ATC. These additional relations create bridges
across the original classifications, yielding an emerging
hierarchy that combines both of them. As an illustration,
we integrated the classes related to alkylating agents in
MeSH and ATC. As depicted in Figure 3, all 4th-level
classes under Alkylating Agents (L01A) in ATC have in-
clusion mappings to Antineoplastic Agents, Alkylating
and Alkylating Agents in MeSH. The 3rd-level ATC class
Alkylating Agents (L01A) itself is found to be equivalent to
these two classes in MeSH and is included in their parent
classes, Antineoplastic Agents and Toxic Actions, respect-
ively. The 2nd-level ATC class Antineoplastic Agents (L01)
can be regarded as equivalent to one of these parents,
namely Antineoplastic Agents, although the equivalence
score ES is slightly under the threshold of 0.5. Such a
representation helps users make sense of the similarities
and differences in the organizational structure of the
classifications.
Limitations and future work
The purpose of this framework is to provide a set of
methods for assessing the consistency of drug classes
across sources. While we believe our framework will
facilitate the curation of an alignment of drug classes
between two sources, it is beyond the scope of this
work to provide such a reference alignment. Moreover,
different reference alignments will most likely be
Alkylang Agents
Anneoplasc Agents, Alkylang
Noxae
Other alkylang agents L01AX
Nitrogen mustard analogues L01AA
Alkyl sulfonates L01AB
Nitrosoureas L01AD
Ethylene imines L01AC
ALKYLATING AGENTS L01A
ANTINEOPLASTIC AGENTS L01 Anneoplasc Agents Toxic Acons
Nitrogen Mustard Compounds D009588
Butylene Glycols D002072 / Mesylates D008698
Aziridines D001388 / Triethylenephosphoramide D013721
Nitrosourea Compounds D009607
Funconal
(PA)
Structural
(MH)
has_PA
*
Inclusion
Equivalence
Hierarchical  relaon
asserted by terminology
* Below the threshold
Figure 3 Integration of MeSH and ATC through the equivalence and inclusion relations obtained through our framework.
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 13 of 14
http://www.jbiomedsem.com/content/5/1/30required for different use cases, as different applica-
tions require different degrees of confidence.
As part of this framework, we have developed equiva-
lence and inclusion scores, for which we have deter-
mined thresholds heuristically. We have not, however,
fully investigated the impact of increasing or lowering
these thresholds on the quality of the alignment. We plan
to do so in future work.
Another limitation is that we have only applied our
framework to one pair of drug classifications, MeSH and
ATC. However, our framework is amenable to aligning
any pairs of classifications for which instance-level
data are available. We plan to revisit our earlier work
on NDF-RT and SNOMED CT classes to demonstrate
the generalizability of our approach.
As mentioned earlier, the instance-based alignment can
be applied only to those classes for which both MeSH and
ATC have drug members. This has been shown to be a
limitation. On the other hand, the lexical alignment
can still be used on these classes.
The UMLS Methesaurus relies for a large part on lexical
similarity for determining synonymy among terms. With
the recent inclusion of ATC in the UMLS Metathesaurus
(in version 2013AB of the UMLS), it would no longer be
necessary for us to perform the lexical alignment of ATCclasses to MeSH classes, since we could simply derive it
from the UMLS, where synonymous terms from various
sources are given the same UMLS concept unique identi-
fier. However, as discussed earlier, the lexical similarity of
class names does not always reflect equivalence and our
instance-based mapping remains an important alternative
method for comparing classes.Conclusions
To our knowledge, our work is the first attempt to align
drug classes with sophisticated instance-based techniques,
while also distinguishing between equivalence and inclusion
relations. Additionally, it is the first application of aligning
drug classes in ATC and MeSH. Moreover, this is the first
systematic investigation of the consistency between lexical
and instance-based alignment techniques for these two
drug resources. We believe that the proposed framework
will effectively support the curation of a mapping between
ATC and MeSH drug classes by providing a detailed ac-
count of the interrelations between the two resources.Endnotes
aATC was integrated for the first time in version 2013AB
of the UMLS released after this study was completed.
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 14 of 14
http://www.jbiomedsem.com/content/5/1/30bNone of these drugs are currently available on the
U.S. market.
cIf the SCR is mapped to a drug, rather than a structural
class descriptor, we associate it with the structural class of
this drug descriptor instead.
dWhen ATC was integrated into the UMLS Metathe-
saurus, new terms were created for ambiguous classes
such as Fluoroquinolones, which appears at several loca-
tions in the ATC hierarchy with slightly different mean-
ings (e.g., Fluoroquinolone antiinfectives, ophthalmologic
for S01AE and Fluoroquinolone antibacterials, systemic
for J01MA).
eThe pharmacological action Antinematodal Agents for
oxantel was not present in MeSH 2013, but was added
to MeSH in the 2014 edition.
fUpon investigation, it appears that some proton pump
inhibitor drugs, such as esomeprazole, were missing a
link to the class Proton Pump Inhibitors in the 2013 ver-
sion of MeSH. This was corrected in the 2014 version.
gWhen ATC was integrated into the UMLS Metathe-
saurus, the new term Protease inhibitors, direct acting
antivirals was created for the underspecified class Protease
inhibitors (J05AE).
Competing interests
The authors declare that they have no competing interests.
Authors contributions
RW and OB conceived the project and contributed equally to performing
the acquisition, analysis, and interpretation of data and to the writing of the
manuscript. Both authors read and approved the final manuscript.
Acknowledgements
This work was supported by the Intramural Research Program of the NIH,
National Library of Medicine (NLM). This work was also supported by the
Office of Translational Sciences, Center for Drug Evaluation and Research at
the Food and Drug Administration (FDA) through an interagency agreement
with NLM (XLM12011 001). The authors want to thank Fred Sorbello, Ana
Szarfman, Rave Harpaz and Anna Ripple for useful discussions.
Received: 3 December 2013 Accepted: 4 February 2014
Published: 9 July 2014
RESEARCH Open Access
The pathway ontology  updates and
applications
Victoria Petri1*, Pushkala Jayaraman1, Marek Tutaj1, G Thomas Hayman1, Jennifer R Smith1, Jeff De Pons1,
Stanley JF Laulederkind1, Timothy F Lowry1, Rajni Nigam1, Shur-Jen Wang1, Mary Shimoyama1,4,
Melinda R Dwinell1,2, Diane H Munzenmaier1,2, Elizabeth A Worthey1,3 and Howard J Jacob1,2,3
Abstract
Background: The Pathway Ontology (PW) developed at the Rat Genome Database (RGD), covers all types of
biological pathways, including altered and disease pathways and captures the relationships between them within
the hierarchical structure of a directed acyclic graph. The ontology allows for the standardized annotation of rat,
and of human and mouse genes to pathway terms. It also constitutes a vehicle for easy navigation between gene
and ontology report pages, between reports and interactive pathway diagrams, between pathways directly
connected within a diagram and between those that are globally related in pathway suites and suite networks.
Surveys of the literature and the development of the Pathway and Disease Portals are important sources for the
ongoing development of the ontology. User requests and mapping of pathways in other databases to terms in the
ontology further contribute to increasing its content. Recently built automated pipelines use the mapped terms to
make available the annotations generated by other groups.
Results: The two released pipelines  the Pathway Interaction Database (PID) Annotation Import Pipeline and the
Kyoto Encyclopedia of Genes and Genomes (KEGG) Annotation Import Pipeline, make available over 7,400 and
31,000 pathway gene annotations, respectively. Building the PID pipeline lead to the addition of new terms within
the signaling node, also augmented by the release of the RGD Immune and Inflammatory Disease Portal at that
time. Building the KEGG pipeline lead to a substantial increase in the number of disease pathway terms, such as
those within the infectious disease pathway parent term category. The drug pathway node has also seen
increases in the number of terms as well as a restructuring of the node. Literature surveys, disease portal
deployments and user requests have contributed and continue to contribute additional new terms across the
ontology. Since first presented, the content of PW has increased by over 75%.
Conclusions: Ongoing development of the Pathway Ontology and the implementation of pipelines promote an
enriched provision of pathway data. The ontology is freely available for download and use from the RGD ftp site at
ftp://rgd.mcw.edu/pub/ontology/pathway/ or from the National Center for Biomedical Ontology (NCBO) BioPortal
website at http://bioportal.bioontology.org/ontologies/PW.
Keywords: Biological pathway, Ontology, Pipeline, Pathway annotations, Pathway diagrams
* Correspondence: vpetri@mcw.edu
1Human and Molecular Genetics Center, Medical College of Wisconsin,
Milwaukee, WI, USA
Full list of author information is available at the end of the article
JOURNAL OF
BIOMEDICAL SEMANTICS
© 2014 Petri et al.; licensee BioMed Central Ltd. This is an open access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Petri et al. Journal of Biomedical Semantics 2014, 5:7
http://www.jbiomedsem.com/content/5/1/7
Background
Introduction
The Pathway Ontology (PW) originated and is being de-
veloped at the Rat Genome Database (RGD) [1]. Its goal
is to cover any type of biological pathway, including al-
tered and disease pathways, and to capture the relation-
ships between them within the hierarchical structure of
a controlled vocabulary or ontology. The building of bio-
logical ontologies as directed acyclic graphs (DAG) and
the use of structured or controlled vocabularies was first
advanced and implemented by the Gene Ontology (GO)
project [2,3]. Many bio-ontologies have been developed
since [4], as witnessed by the ever-growing number sub-
mitted to and made available at the National Center for
Biomedical Ontology (NCBO) BioPortal [5,6]. Several
ontologies, including the Pathway Ontology, are being
developed at RGD ([7], in the Biomedical Ontologies
thematic series of the Journal of Biomedical Semantics).
Within the structure of a DAG, terms have defined rela-
tionships to one another and a particular term can have
more than one parent. This means that there can be
more than one path in the ontology tree from a broader,
more general parent term to a more specialized child
term. Within the tree structure, terms are nodes whose
names designate the class(es) they represent and which
are connected by edges that represent the relationship(s)
between them. In PW, a node is the network/pathway
class it stands for, and its features and aspects are cap-
tured in the definition. A pathway is a set of inter-
connected reactions and interactions whose delineation
and scope are used as a model for exploring and study-
ing, describing and understanding the working of and
relationships between biomolecules within a context.
The categories or types of pathways are conceptualized
and referenced in the scientific literature and repre-
sented in pathway databases such as the Kyoto
Encyclopedia of Genes and Genomes (KEGG), the
Pharmacogenomics Knowledge Base (PharmGKB), the
Small Molecule Pathway Database (SMPDB) and Wiki-
Pathways, among others [8-11].
The pathway ontology structure
The first of the main five nodes of the ontology, the meta-
bolic node, contains networks/pathways that stand for/rep-
resent the set of reactions underlying the transformation of
compounds. The set of reactions/interactions underlying
the coordinated responses that maintain the cellular/tissue
and/or organ/organismal status quo and homeostasis are
placed under the regulatory node. The set of reactions/in-
teractions initiated or triggered by a binding/molecular
interaction/conformational change event are found under
the signaling node. The set or sets of interactions where
one or more are deviant and represent the systems perturb-
ation(s) fall under the disease node. Finally, the set or sets
of reactions/interactions representing the systems response
to and handling of treatment(s) geared towards dealing with
those perturbation(s) are housed in the drug node. Thus,
the main nodes of the Pathway Ontology are: metabolic,
regulatory, signaling, disease and drug pathway (Figure 1A).
Two types of relationships are being used in the ontology:
is_a and part_of. For instance, insulin and glucagon are
peptide hormones whose signaling - insulin signaling path-
way and glucagon signaling pathway, are children terms in
an is-a relationship to the parent term peptide and protein
hormone signaling pathway. The two signaling pathways
which are initiated in response to high levels of circulating
glucose  insulin signaling pathway, or low  glucagon sig-
naling pathway, and whose engagement of intracellular cas-
cades aims at restoring the normal physiological levels of
glucose, are also in a part-of relationship to the glucose
homeostasis pathway term, along with other pertinent
terms. Insulin also plays important roles in energy homeo-
stasis. In the brain, insulin (and leptin) act to increase the
expression of appetite-decreasing Pomc while decreasing
the expression of appetite-stimulating Agrp genes. The
peptide and protein hormone signaling pathway term is in
turn a child of the more general term hormone signaling
pathway, as other classes of compounds with very different
physico-chemical properties can also act as hormones. For
instance, the steroid hormones and the eicosanoids which,
as the names suggest, are hormones, are lipid molecules.
The signaling pathways they initiate are children of the
lipid hormone signaling pathway term which in turn, is a
sibling of peptide and protein hormone signaling pathway
and child of hormone signaling pathway terms (Figure 1B).
The nodes are not disjoint and a given pathway class can
be the child of terms residing in different nodes, as the ex-
amples of insulin and glucagon signaling above show. The
peptide and protein hormone signaling pathway and the
glucose homeostasis pathway are both parents of the sig-
naling pathways of insulin and glucagon, albeit with differ-
ent relationships to their children; the two parent terms are
within the signaling and regulatory nodes, respectively. The
energy homeostasis pathway term is also a parent of insu-
lin signaling and like glucose homeostasis, it is within the
regulatory node (Figure 1C).
The pathway and the process concepts, although at
times interchangeably used, are distinct. A pathway conveys
the idea of a set of interacting molecules, of the reactions
and interactions underlying its functioning. A process on
the other hand, conveys the idea of the end result, the con-
clusion of a plan of action, whether the consequence of the
combined work that the set of reactions and interactions
produces, in the case of a simpler one, or in the case of a
more complex one, the combined work of pathways that
contribute to or in some fashion modulate the end result.
At the same time, a given pathway can participate in and/
or regulate several processes [12]. In the Biological Process
Petri et al. Journal of Biomedical Semantics 2014, 5:7 Page 2 of 12
http://www.jbiomedsem.com/content/5/1/7
(BP) ontology of GO there are metabolic and other process
terms that map to KEGG pathways and to terms in PW.
For instance, the formation of a fatty acid molecule is the
fatty acid biosynthetic process term in GO; it is the fatty
acid biosynthetic pathway term and the fatty acid biosyn-
thesis entry in PW and at KEGG, respectively. While the
phrasing is similar in GO, PW and KEGG, the term repre-
sents a process in GO, a pathway in PW and the KEGG
database. KEGG is a primary source for metabolic pathways
and projects such as databases and ontologies that in some
fashion represent metabolism are going to exhibit a sharing,
or an overlapping of terms/entries naming, but not an over-
lapping of concepts and/or contexts. Likewise, there are sig-
naling pathway terms in BP that relate to similar terms in
the signaling pathway node of PW and map to entries in
pathway databases such as KEGG and others. However, the
positions of and relationships between such terms are dif-
ferent, as are the perspectives of the two ontologies.
Disease and altered pathways
The provision of terms for the altered versions of path-
ways and the representation of disease pathways and dia-
grams as collections of altered pathways are unique to
PW and its use at RGD. An altered pathway is one
where defects in one or several components of the path-
way affect its normal functioning with potential implica-
tions for a diseased phenotype. The severity of an
altered pathway or the convergence of several altered
pathways can overcome the ability of the system to ad-
just and is manifested in the diseased state. Viewing
A
C
B
Figure 1 The pathway ontology main nodes and positions of selected terms. A. The five nodes of the Pathway Ontology. B. The term lipid
hormone signaling pathway in the ontology showing the parent, siblings and children terms. C. The term insulin signaling pathway in the
ontology showing the position of the term within the tree. Insulin signaling pathway is in a part_of relationship to the glucose and energy
homeostasis pathway terms within the regulatory node and in an is_a relationship to peptide and protein hormone signaling pathway term
within the signaling node.
Petri et al. Journal of Biomedical Semantics 2014, 5:7 Page 3 of 12
http://www.jbiomedsem.com/content/5/1/7
diseases from a network- rather than a gene-centric per-
spective, from the systems level of pathway cross-talk
and alterations within, is an approach increasingly being
considered [13-15].
As an example, a large-scale study carried out on a
number of pancreatic tumors identified several sets of
genes that were altered in the majority of tumors. Of
these, many were associated with core signaling path-
ways and altered in 67% to 100% of tumors [16]. Perhaps
not surprisingly, these are pathways important for
growth and proliferation and in some cases, also known
to be oncogenic (Figure 2). What may be intriguing is
the relatively large number of altered pathways and one
is tempted to wonder/speculate whether it is this num-
ber and the combinations that result from it, that over-
come the ability of the system to adjust and/or recover
and render the condition intractable. The pancreatic
cancer pathway diagram presents the main pathways al-
tered in the condition with the culprit genes shown
color coded. Additional links to a list of miRNAs
(microRNAs) aberrantly expressed in pancreatic tumors
and to the Cancer Portal at RGD are provided (see
Figure 2).
Pathway annotations, interactive pathway diagrams,
pathway suites and suite networks
The use of the ontology allows for the standardized an-
notation of rat, human and mouse genes to pathway
terms. Generally, annotations are made for the term ra-
ther than on a gene-by-gene basis; thus, what is being
targeted for annotation is the pathway itself  like the
ontology the overall pathway curation process is
network-centered [12,17]. Importantly, the ontology pro-
vides the navigational means to access pathway annota-
tions, interactive pathway diagrams, pathway suites and
suite networks as well as a variety of tools, from many
entry points. A pathway suite is a collection of pathways
that revolves around a common concept or is globally
Figure 2 Pancreatic cancer pathway diagram. The interactive pathway diagram page for the pancreatic cancer pathway. The altered
pathways associated with the condition are shown as gray rectangles that link to the ontology report(s) for the those terms. Culprit genes within
the pathways are shown color-coded (default is red). The icon for the microRNAs (miRNA) with potential roles in pancreatic cancer links to a page
where several down- and up-regulated miRNAs are shown with some targets listed and with links to their report pages in RGD and the microRNA
database (MiRBase). The icon for the condition links to the Cancer Disease Portal in RGD.
Petri et al. Journal of Biomedical Semantics 2014, 5:7 Page 4 of 12
http://www.jbiomedsem.com/content/5/1/7
related. If two (or more) pathway suites relate in some
fashion, they constitute a suite network. For instance,
the Glucose Homeostasis Pathway Suite Network
brings together the suite dedicated to the various meta-
bolic pathways involving glucose and the one dedicated
to the contributing signaling and regulatory pathways.
Together, the pathway ontology, the pathway annota-
tions and the graphical representations of pathways,
constitute the elements of the Pathway Portal [12,17,18],
an important project at the Rat Genome Database
[19,20]. Pathway, along with disease, phenotype and bio-
logical process, are the major concepts around which
the Disease Portals are built and are entry points to
access the data they contain. The Disease and Pathway
Portals can be accessed from the main homepage of
RGD (Figure 3A). The Pathways entry point leads to
the Molecular Pathways link which houses the collection
of interactive pathway diagrams and suites that RGD
publishes. This entry point also provides access to path-
way related publications by members of RGD as well as
other information and data links (Figure 3B).
An ontology search, accessed through the Function
entry point (see Figure 3A), brings up all the ontologies
that have terms which contain the keyword(s) used. Se-
lection of an ontology will show the terms containing
the keyword(s) with the option to search the tree or view
A
B
Figure 3 Pathway portal data access. A. Rat Genome Database homepage with the main entry points to its content; the Pathways and
Function entry points described in the text, are circled. B. Accessing the Pathways entry point and entries within.
Petri et al. Journal of Biomedical Semantics 2014, 5:7 Page 5 of 12
http://www.jbiomedsem.com/content/5/1/7
the annotations. Selecting the branch icon to the left of
a term brings up a browser result showing the parent,
siblings and children of the term. The browser has been
developed at RGD and recently updated to indicate
whether interactive pathway diagrams are available or
not for terms and/or their children in the form of a
boxed D of darker or paler green color, respectively
(see Figure 1A-B). Any dark green D box links to that
interactive diagram page. In addition, if the searched
term has a diagram, a small icon will be shown in the
term entry, to the right of the term description; it will
also link to the diagram page. [The boxed A in
Figure 1A-B denotes the presence of annotations].
Selecting a term brings up an ontology report page with
the GViewer tool  a genome-wide view of rat chromo-
somes with genes annotated to the term, a tabular list of
genes annotated to the term by species with links to re-
spective gene report pages and a diagram showing the
paths to the root term in the ontology tree. If there is an
interactive pathway diagram for the chosen term, an
icon is present at the top of the page to the right of the
diagram and it links to the pathway diagram page.
Every diagram page consists of several sections. The
first provides an in-depth, expandable description of the
pathway and the diagram itself whose objects link to
their report pages in RGD (genes, chemicals, pathways)
or other websites. Beneath that is a tabular list of anno-
tated genes by species with each entry linking to its re-
port page and other links. As applicable, the altered
version of the pathway and additional elements in the
diagram can also be found in this section. The next sec-
tion contains tabular lists of genes in the pathway that
have been annotated to disease, other pathway and
phenotype terms with links to corresponding report
pages. The user has the option of toggling between
terms and genes and can follow links to ontology report
pages for terms and to gene report pages for genes.
PROCEEDINGS Open Access
Evolving BioAssay Ontology (BAO):
modularization, integration and applications
Saminda Abeyruwan1, Uma D Vempati2, Hande Küçük-McGinty1, Ubbo Visser1, Amar Koleti2, Ahsan Mir2,
Kunie Sakurai3, Caty Chung2, Joshua A Bittker5, Paul A Clemons5, Steve Brudz5, Anosha Siripala6, Arturo J Morales6,
Martin Romacker6, David Twomey6, Svetlana Bureeva7, Vance Lemmon2,3, Stephan C Schürer2,4*
From Bio-Ontologies Special Interest Group 2013
Berlin, Germany. 20 July 2013
* Correspondence: sschurer@med.
miami.edu
2Center for Computational Science,
University of Miami, 1320 S. Dixie
Highway, Gables One Tower, 33146
Coral Gables, FL, USA
Abstract
The lack of established standards to describe and annotate biological assays and
screening outcomes in the domain of drug and chemical probe discovery is a severe
limitation to utilize public and proprietary drug screening data to their maximum
potential. We have created the BioAssay Ontology (BAO) project (http://
bioassayontology.org) to develop common reference metadata terms and definitions
required for describing relevant information of low-and high-throughput drug and
probe screening assays and results. The main objectives of BAO are to enable
effective integration, aggregation, retrieval, and analyses of drug screening data.
Since we first released BAO on the BioPortal in 2010 we have considerably expanded
and enhanced BAO and we have applied the ontology in several internal and
external collaborative projects, for example the BioAssay Research Database (BARD).
We describe the evolution of BAO with a design that enables modeling complex
assays including profile and panel assays such as those in the Library of Integrated
Network-based Cellular Signatures (LINCS). One of the critical questions in evolving
BAO is the following: how can we provide a way to efficiently reuse and share
among various research projects specific parts of our ontologies without violating
the integrity of the ontology and without creating redundancies. This paper provides
a comprehensive answer to this question with a description of a methodology for
ontology modularization using a layered architecture. Our modularization approach
defines several distinct BAO components and separates internal from external
modules and domain-level from structural components. This approach facilitates the
generation/extraction of derived ontologies (or perspectives) that can suit particular
use cases or software applications. We describe the evolution of BAO related to its
formal structures, engineering approaches, and content to enable modeling of
complex assays and integration with other ontologies and datasets.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2014 Abeyruwan et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Background
Introduction to BAO and the domain
The development of novel small molecule therapeutics (drugs) typically begins with the
identification of suitable compounds with desirable biological activity in simple model
systems such as a purified protein that is a validated disease target or a cell related to
a disease or disease state. Target-based and cell-based phenotypic high-throughput
screening (HTS) are among the most important approaches to identify new hits and
leads from large compound libraries [1,2]. Innovations in assay design and technologi-
cal advances in detection and throughput have dramatically increased the size and
diversity of HTS datasets generated in pharmaceutical companies and in public
research projects. Examples of NIH-funded large-scale screening programs in which
we have been participating include the Molecular Libraries Program (MLP) [3] and the
Library of Integrated Network-based Cellular Signatures (LINCS) program [4]. In the
MLP, a large library (up to 430, 000 compounds) has been screened in over 600 probe
projects to develop novel tool and drug compounds. This data is deposited in Pub-
Chem [5] and is also being curated and made available for structured analysis in the
BioAssay Research Database (BARD) [6]. The LINCS project, in contrast to traditional
screening, generates extensive signatures of cellular responses consisting of thousands
of results for any perturbation (such as small molecule drugs) to enable the develop-
ment of better system-level disease models. Examples of LINCS screening results and
assays include Landmark gene expression signatures (L1000), Kinome-wide binding
affinities (KINOMEscan), phenotypic profiling across 1,000 cell lines, and many others,
covering omics and HTS data. LINCS results are currently available via participating
centers and can be queried and explored via the LINCS Information FramEwork
(LIFE) developed by our group [7]. Several other publicly accessible resources of
screening data exist, for example ChEMBL, a database that contains structure-activity
relationship (SAR) data curated from the medicinal chemistry literature [8], the Psy-
choactive Drug Screening Program (PDSP), which generates data from screening novel
psychoactive compounds for pharmacological activity [9], or Collaborative Drug Dis-
covery (CDD), a private company enabling drug discovery research collaborations [10].
Despite being publicly available, current data repositories suffer from structural, syntac-
tic, and semantic inconsistencies, complicating data integration, interpretation and analy-
sis. As one of the largest and first repositories of public drug screening data, PubChem,
has been essential to illustrate the need for clear metadata standards to describe drug and
chemical probe discovery assays and screening results [11]. To address these prevailing
issues; we have previously developed the first version of the BioAssay Ontology (BAO)
[12]. This first version was developed iteratively based on domain expertise and available
assay data, primarily from the MLP, which we annotated using evolving versions of BAO.
Since the first release of BAO, we have engaged with several more groups in public
research projects and in pharmaceutical companies and the biomedical ontology commu-
nity. We aligned the organization of BAO with existing efforts as much as possible, most
importantly at the Novartis Institutes of BioMedical Research, and we have significantly
extended the terminology and axioms in BAO to cover a broader range of assays and
related concepts. One of our objectives in redesigning BAO was to introduce an upper-
level ontology to facilitate alignment and integration with other biomedical domain ontol-
ogies and to provide a more formal ontology development framework. However, a critical
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 2 of 22
requirement was to maintain a native organization of BAO that is meaningful to end
users and which enables straight-forward incorporation into software systems, such as our
previously developed BAOSearch application [13]. This led us to a formal, structural, and
functional modularization of BAO, which we describe here. We also provide a general
solution to defining profile and panel-type assays in which many results are generated in
parallel, such as those in the LINCS project. Meanwhile, BAO has been applied in several
new projects, most importantly BARD, which also contributed to extending and improv-
ing BAO further.
Semantic Web technologies have become increasingly popular to integrate biome-
dical research information; a prominent example is the Bio2RDF project [14]. In
addition to open-world integration of diverse omics and high-throughput drug
screening data, Semantic Web technologies provide capabilities for inference reason-
ing with many potential benefits over traditional systems [15]. Only very recently
however, have large public drug screening datasets been made available as Resource
Description Framework (RDF) format. One such resource is ChEMBL, whose RDF
model leverages BAO to describe the results [16]. A large initiative to develop an
integrative solution to diverse drug discovery data is the Open Pharmacological Con-
cepts Triple Store (Open PHACTS) consortium [15]. Because of increasing adapta-
tion of Semantic Web technologies in drug discovery data management, it was
critical to develop BAO as a formal Description Logic (DL) ontology implemented in
Web Ontology Language (OWL). We show modeling examples illustrating BAO
semantic inference capabilities to identify mechanistically related assays in absence of
such explicit annotation.
Description logic
Description logic (DL) contains a set of decidable constructs from the first-order predi-
cate logic, and it is the corner stone for the development of OWL DL ontologies in
knowledge representation [17]. The computational complexity of a given DL depends
on the constructs that are being used, and they are traditionally represented with dif-
ferent complexity classes. Attribute Language with Complement (ALC) provides the
preliminary DL constructs with classes, roles, and individuals. The formal syntax of
ALC is defined as follows (as a convention, we indicate conceptualization by capital
letters (e.g., C, D) or sans serif letters (e.g., Thing, bioassay), sets by bold face letters
(e.g., C, I), and functions by lower case letters (e.g., fC,fI)). Let A be a named atomic
class, and, without loss of generality, let R be an abstract role. The class expressions
(concepts or concept expressions) C, D are recursively constructed by: C, D ¬ A | ? |
? | ?C | C ? D | C ? D | ?R.C | ?R.C, where, ? is the top concept, ? is the bottom
concept, the symbols for conjunction, disjunction, and negation are given by ?, ?, and
? respectively, and ? and ? represent the universal and existential quantifier. ALC
DL knowledge bases consist of two groups: (1) TBox provides statements about the
terminological knowledge; and (2) ABox provides the statements about the assertional
knowledge about individuals. These statements are also known as axioms in descrip-
tion logic. For class expressions C and D, the TBox statements are of the form C ? D
or C ? D, where ? denotes the equivalences among classes and ? constructs the sub-
sumption or general class inclusion (GCI) axioms. On the other hand ABox consists of
axioms of the form C(a) and R(a, b), where R is a role, and, a, b are individuals.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 3 of 22
ALC DL has been extended toSROIQ(D) DL with the following syntactic con-
structs: {a} | ?R.Self | ? nR.C | ? nS.C, where, {a} represents nominals, ?R.Self relates
an individual to itself, and n ? ?+ with ? nR.C and ? nS.C provide the qualified cardin-
ality restrictions. SROIQ(D) DL introduces an RBox with general role inclusion
axioms of the form R1 . . . R2 ? R, which provides the meaning that concatenation of
R1, . . . , R2 is a subrole of R. In addition, there exists constructs to represent transitive,
symmetric, asymmetric, reflexive, irreflexive, functional, inverse functional, and disjoint
roles and concepts. It is to be noted that roles can either be abstract or concrete.
The interpretation of DL is given by the direct model-theoretic semantics. The
classes, roles, and individuals are given symbols from mutually disjoint sets of C, R,
and I respectively. There exists another set called the domain of interpretation, ?,
which contains entities for resources, individuals, or single objects. Using the domain
of interpretation, the individuals, classes, and roles are interpreted by functions fI : I
??, fC : C ? 2
?, and fR : ? 2
?×? respectively. The complex classes and role expressions
are interpreted by an extended interpretation function, .I , such that the interpretation
faithfully capturers the structure of the knowledge base. If a model exists, then the
knowledge base is satisfiable, and the implicit knowledge (logical consequence) is
entailed though an inference procedure. DL logic uses efficient tableau algorithms to
infer subsumption, class equivalence, class disjointness, global consistency, class consis-
tency, instance checking, and instance retrieval.
DL provides an appropriate trade-off between expressivity and scalability in practice.
The complexity of DL is dominated by the data complexity, which is NP-hard for
SROIQ(D) DL ABox and N2ExpTime-complete for the combined TBox, RBox, and
ABox. Modern SROIQ(D) DL reasoners such as the (1) tableau-based FaCT++ [18]
and Pellet [19] reasoners; and the (2) hyper-tableau HermiT [20] reasoner, use intelli-
gent heuristics and optimization methods to perform inferencing as efficiently as possi-
ble. The reader is referred to [21,22] for a comprehensive discussion on SROIQ(D)
DL syntax, semantics, deduction procedures, and model construction.
Results and discussion
BAO 2.0 native organization and main components
The new BAO 2.0 formally describes perturbation bioassays in the domain of drug and
probe discovery, such as small molecule HTS assays and screening results for the purpose
of categorizing the assays and outcomes by concepts that relate to the screening model
system (format), assay method, the biology interrogated in the assay (such as a protein tar-
get or biological process), the detection method (how does the assay work), and types of
results (endpoints). BAO 2.0 is organized into several major sections, which include multi-
ple levels of subcategories of subsumption class hierarchies. A number of specific object
property relationships were created to connect the classes and develop a knowledge
representation.
The main categories in BAO 2.0, titled components, include bioassay, assay biology,
assay method, assay format, assay endpoint, assay screened entity (Figure 1). Each of
these component classes includes the subsumption trees of terms corresponding to the
category and additional trees of related terms to describe each of the main components
properly and formally. In BAO 2.0, we incorporated a slightly different pattern from
BAO 1.6, since we were interested in making BAO 2.0 compatible with the existing
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 4 of 22
upper-level and other domain-level ontologies. The BAO 2.0 categories also lend BAO
to its native structures that is most useful to users, for example to annotate assays or
to implement a user interface in a software application. We describe briefly the main
class hierarchies of BAO 2.0 corresponding to the above components (Figure 1):
 BAO assay bioassay component includes the bioassay subsumption tree, and sev-
eral other classes to describe assays, including assay kit, bioassay type, and bioassay
specification, which contains terminology trees to describe various details about a
bioassay and its context. The class hierarchy bioassay includes the list of the bioas-
says and their formal description, e.g., cell cycle assay, enzyme activity assay. Bioas-
says are organized roughly by their application (what the assay is used for). The
class hierarchy assay kit includes the reagents and their cocktails that are commer-
cially available to perform the different chemical reactions that encompass an assay
(i.e., out of the box, ready to run assays). The information in bioassay specification
is similar to BAO 1.6.
 BAO assay format component includes the assay format subsumption tree to
describe the biological model system; a conceptualization of assays based on the
biological and/or chemical features of the experimental system.
 BAO assay method component includes terminologies to describe how the assay
is performed, most importantly assay method and physical detection method. It
also includes computational method, instrument, and relevant other material entity
assay ingredients. The class hierarchy assay method includes assay design method
Figure 1 BAO 2.0 main classes with some relationships between them. Six main components (shaded
classes) are used to formally describe bioassays by terms related to bioassay, biology, screened entity, assay
method, format, and endpoint. The most important classes and their relations as shown including bioassay,
measure group, biological macromolecule, screened entity, assay method (specifically assay design method
and physical detection method), assay format, and endpoint. There exists complex interactions among
these entities. OWL DL 2 (SROIQ(D)), the decidable subset of the first-order-predicate logic provides the
interpretations, models, and logical consequences.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 5 of 22
and assay supporting method; assay design method describes how a biological per-
turbation of the model system is translated into a detectable signal. The class hier-
archy computational method contains various methods that are based on the
application of information technology to chemistry and biology. The physical detec-
tion method hierarchy includes the method (technology) used to detect the signal
that corresponds to the perturbagen in the assay environment and enabled by the
assay design method. Class instrument consists of instruments used for detection/
readout from an assay and their components, e.g., FLIPR, ViewLux plate reader,
PHERAstar, etc; software lists the types of software that are used in the various
instruments, e.g., image analysis software, which is a component of the high con-
tent screening (HCS) platforms.
 BAO assay biology component includes various class hierarchies to describe the
biology of the assay including biological process, biological macromolecule, cell line
cell, cellular component, cell phenotype, anatomical entity, disease, function, organ-
ism. Many of these are mapped to external sources (vide infra). To describe the
biology of a simple binding assay for example, a biological macromolecule protein
would have the biological role target. Many other role classes exist (vide infra). The
class function includes the physiological function of biological macromolecules, e.g.,
protein binding, kinase activity. This module was imported from the Gene Ontol-
ogy (GO). The class cellular phenotype encompasses both the molecular character-
istics of a cell and the (morphological) shape and structure of a cell and its parts.
 BAO assay screened entity component includes screened entity, which is the che-
mical or biological entity that is tested/screened in the assay. The screened entity
typically modulates the function of the (known or unknown) biological macromole-
cule with the role of a target. The most important screened entity for BAO is the
class small molecule, that contains compounds that are tested in the process of
developing chemical probes and drugs, which is the primary domain of BAO.
 BAO assay endpoint component includes subsumption trees to describe the assay
result or endpoint and other required information to quantitatively or qualitatively
express the biological perturbation measured in a bioassay, such as units of measure-
ment (imported from UO), and other details to interpret the results in the context of
the assay methodology and the biology, such as as the mode of action of the pertur-
bagen that the endpoint characterizes, or the signal direction and endpoint action
correlation of the assay. More details about the class endpoint are described below.
 Additional classes that were not assigned to any one of the main BAO compo-
nents are organization, people, role, and quality: Organization includes, for example
manufactures of assay kits, instruments, etc., or screening center where assays are
performed. People include the individuals who are involved in performing scientific
research, such as assay development, compound screening, chemical synthesis, etc.
Role describes the action that an entity performs in a given context; an entity can
have more than one role, e.g., target, perturbagen. BAO 2.0 has imported roles
from the Chemical Entities of Biological Interest (ChEBI) ontology and we have
added some missing classes. Quality lists the characteristics that inhere in an entity
of biological origin, namely, organism, cell, and molecule or a physical entity, e.g.,
intensity, optical quality. Most of the terms in this class were imported from the
Phenotypic Quality Ontology (PATO); missing ones were added to BAO.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 6 of 22
 BAO properties include both the object and data types that are required to create
relationships among the different concepts in BAO 2.0. These properties were either
imported from the Relationship Ontology (RO), where available or created in BAO 2.0.
Upper level ontology structure and aligning external ontologies
Since, there are several advantages of using upper level ontologies (ULOs), BAO 2.0
makes use of the Basic Formal Ontology (BFO) and OBO Relations Ontology (OBO-
RO) as its upper level ontologies. We have used the current release of BFO ontology
(http://purl.obolibrary.org/obo/bfo.owl), which is also tightly coupled with OBO-RO
ontology (http://purl.obolibrary.org/obo/ro.owl). Figure 2 shows the main categories of
BFO and examples of corresponding BAO 2.0 classes. BFO conceptualization abstractly
represents objects, entities, and relations in our domain of discourse, and it is substan-
tially used in biomedical ontologies compared to other OWL version of ULOs such as
SUMO (http://www.ontologyportal.org/SUMO.owl) or DOLCE (http://www.loa.istc.cnr.
it/ontologies/DLP 397.owl). The advantage of using an ULO is that it allows integra-
tion of existing domain ontologies, by grounding them on a formally rigid ontological
framework [23,24]. We make available a development instance of the BAO BFO ver-
sion. Figure 2 also illustrates external ontologies, components of which we currently
use in BAO (see Methods). Their alignment with BAO is facilitated in part by the BFO
structure [25,26]. One important mid level ontology is the Ontology for Biomedical
Investigations (OBI) [27]. We have previously outlined the different focus of BAO vs.
OBI [12]. However, this is not to say that they are incompatible; alignment is one of
the future tasks required to evolve BAO further. We have created a version of BAO
Figure 2 BAO 2.0 makes use of BFO as the upper-level ontology and incorporates several external
ontology modules. BAO classes were mapped under appropriate BFO concepts. The BFO framework also
facilitates alignment to external ontology modules. Blue boxes are examples of BAO classes categorized by
BFO (black rectangles). External ontologies used in BAO are shown as red boxes and labels.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 7 of 22
2.0 that contains BFO and OBO-RO as ULOs (bao_complete_bfo_dev, which is a
development version) and another one without them (bao_complete, released).
Bao_complete_bfo_dev simplifies alignments to external ontologies and is targeted
to the ontology development community while bao_complete is targeted to the drug
and probe screening community and developers of software applications (such as our
BAOSearch application). We emphases the fact that the BAO-to-BFO alignment is based
on our knowledge and understanding of BFO and OBO-RO structures, and BAO
mechanisms. The alignment is an ongoing process, and we have a community wide bug
reporting system to uses to provide feedback to provide semantically better alignments.
bao_complete_bfo_dev and bao_complete are targeted towards different users
groups, the latter is more amenable to perform on large-scale analysis of the chemical
biology data without the additional constraints imposed by BFO and OBO-RO.
BAO 2.0 modular architecture and implementation
The modularization implementation is described in detail in Methods. Our modulari-
zation approach is illustrated in Figure 3. The modularization framework uses a layered
architecture and uses the modeling primitives, vocabularies, modules and axioms.
Vocabularies only contain terms (classes with subsumption only). Module layers enable
combining vocabularies in flexible ways to create desired ontology structures or sub-
sets. Axioms are separate files that do not contain any classes or properties. Classes
and relationships are imported (directly or indirectly) from module and/or vocabulary
Figure 3 BAO 2.0 ontology modularization framework. The framework uses a layered architecture to
abstract complexities from different sources. It provides modeling primitives of vocabularies, modules,
axioms, and perspectives to develop heterogeneous ontologies.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 8 of 22
files. The above mentioned classes in BAO 2.0 were created as separate vocabulary
files. They were then imported into the bao_core file. BAO core only contains
axioms incorporating BAO classes and BAO properties. In our modularization
approach we separate external and internal sources. External modules (compare Figure
2) are generated as described in Methods. Overlap among external and internal classes
and properties (i.e., those required in BAO core) are resolved using combinator mod-
ules, that is, external classes and properties are mapped (equivalence or subsumption)
to corresponding BAO classes and properties. This approach assures that BAO core
remains stable and independent from external sources that may change. The complete
BAO includes external axioms and imports BAO core (indirectly importing all voca-
bularies and properties) and external modules (bao_complete file). Using this
approach we also generated the BFO version of BAO. All internal and external vocabu-
lary, module and axiom files are available via the BAO website (http://bioassayontol-
ogy.org). Figure 4 shows the current implementation of the modularization illustrating
vocabularies, intermediate modules, ontology axioms, BAO internal and external
sources and their mappings.
Modeling assays and results using BAO 2.0
In addition to the BAO modularized design and systematic construction, we also tried to
make the definitions of concepts in BAO consistent. We especially defined bioassays with
their essential components such as assay design method, endpoints, measure groups, and
molecular participants. Figure 1 illustrates how assays are modeled by specifying informa-
tion related to the biology (such as target and/or biological process), assay format, assay
method (including assays design method and physical detection method, screened entity
and endpoint (result) as described above. The BAO 2.0 architecture allows a more flexible
definition of bioassays, for example the same biomolecule can participate in assays in
different roles and functions. Important classes include:
 target: The target concept is defined by using the relationships has participant
and has role. That is because targets are biological entities (i.e., participants) of
assays that are playing the role target. Assays may have single or multiple targets
depending on the assay type.
 biological process: A large number of assays are designed to measure outcomes of
biological processes. Thus, based on the assay in study, we have written axioms for
these information in the assay definitions.
 screened entity: This concept refers to a molecular entity with the role screened
entity role.
Figure 4 BAO 2.0 ontology modularization framework implementation. BAO 2.0 modularization
framework provides effective software engineering methods to build complex ontologies. Shown are the
current vocabularies, modules, and axiom files also indicating internal vs. external sources.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 9 of 22
 participants: Every assay has at least one participant, usually more. While axiomizing
the assays, we try to define the particular roles that these participants play in the differ-
ent assays. However, when we are not certain about the roles, we choose not to put
axioms in order to avoid false reasoning cases.
 assay design method: Every assay has an assay design as the underlying method to
generate a detectable signal and could correlate with the strength of the perturba-
tion of the biological model system by the screened entity.
 physical detection method: An assay design method, generating a type of signal is
linked to a corresponding detection method (the physical principle of detecting the
signal), which is typically performed by a detection instrument.
The concepts listed above along with various other classes are used while modeling
the concepts bioassay, measure group, and endpoint.
We had previously introduced the concept measure group to link multiple endpoints
to the same bioassay [28]. We have now generalized this model so that measure group
can be derived from one or more measure groups. This allows the formal and iterative
construction of more complex assays and endpoints that are derived from multiple
measurements (Figure 5). The axiomatization was done in a way that infers measure
group as a subclass of bioassay (compare Figure 1). The axiomatization was motivated
Figure 5 Graphical illustration of BAO 2.0 measure group class definition. The class measure group is
used to group and link one or more sets of experimental results to one bioassay. By definition one assay can
have multiple measure groups. The measure group contains overlapping axioms with the bioassay, which
allows the reasoner to infer that the measure group is acting like an equivalent class of the bioassay; i.
e., measure group is inferred as subclass of bioassay. Shown is an example of kinase concentration-response
profiling panel assay, in which compounds are tested at m concentrations against n kinase targets.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 10 of 22
by pragmatic considerations for the workflows and perspectives for organizing and
analyzing the assay results, which is the core focus of BAO. It may be argued, that
operationally it is not formally an assay; however that is not in conflict with the BAO
perspective. It should be noted that BAO measure groups and results remain asso-
ciated with their corresponding subclasses of bioassay, whose instances are procedu-
rally, methodologically, and materially real. To understand better the relations between
the concepts measure group and endpoint we explore them in more depth:
 The class measure group is a concept to group and link one or more (different)
sets of experimental results to one bioassay. A bioassay can have multiple measure
groups. A measure group contains overlapping axioms with the bioassay, which
allows the reasoner to infer that the measure group is acting like an equivalent
class of bioassay. This equivalence cannot simply be asserted. The measure group,
in addition to holding the assay component metadata for each reported endpoint,
also provides flexibility to generate different derived endpoints, e.g., IC50 (gener-
ated from several response values at different concentrations, i.e., concentration-
response), or profile endpoints (e.g., a kinase panel assay). This can be formally
done via derived measure groups, in cases where we have multiple measure group
that vary in one parameter (such as concentration or kinase target).
 The class endpoint, alternatively called result, is a quantitive or qualitative represen-
tation of a perturbation (change from a defined reference state of the model system)
that is measured by the bioassay. An endpoint consists of a series of data points, one
for each perturbing agent employed by the assay. Every endpoint is obtained by using
at least one measure group. For each endpoint, there exists a unit and a value, which
is a number (e.g., float, which makes this concept a data property, and the concept is
axiomized using a data property as opposed to an object property). For example, for a
concentration endpoint (e.g., IC50), there exists a concentration unit and a concentra-
tion value, which is a float number (data property, not functional). Assays could have
single or multiple endpoints depending on the assay type.
Endpoints are not used to handle the different measurements in the same assay. That
is axiomized through the measure group concept. They may vary due to parameters
such as time, concentration, target, and so on, or combinations. The formal definitions
allow us to create individuals for different endpoints that might be using the same
measure groups, i.e., results are measured once and different methods are applied on
these measurements to find different derived endpoints. We can group different mea-
sure groups to define intermediate results. We can create profile endpoints and we
can define profiles of intermediate aggregated measure groups (Figure 5). An endpoint
individual is associated with a specific measure group and a specific compound combi-
nation and has a specific value and unit.
In BAO 2.0, endpoints are classified into several categories; the most important ones
are concentration endpoint (which includes concentration response endpoint),
response endpoint, protein substrate and ligand constant, and physical property end-
point. The class mode of action defines the functional effect and physical binding char-
acteristics of the screened entity on the target using the subclasses ligand function
mode of action (inhibition, activation, etc.) and ligand binding mode of action
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 11 of 22
(reversible, irreversible, competitive, etc). Each endpoint is associated with a mode of
action, e.g., IC50 and percent activation have inhibition and activation as the functional
mode of action, respectively. The class signal direction defines how the functional
effect of the perturbation corresponds to the intensity of the detected signal, i.e.,
increase or decrease with activation or inhibition. This is important to identify suitable
counter screens; for example if the detected perturbation results in signal decrease in a
cell-based assay, cytotoxic compounds may be detected as actives. The class endpoint
action correlation defines if the endpoint value corresponds to increased or decreased
functional effect (inhibition, activation). Both signal direction and endpoint action cor-
relation are required to formally interpret the results, because the same perturbation
(e.g., inhibition of substrate-protein binding by a competing ligand) may be measured
via a different molecular entity with the role measured entity (e.g., substrate-bound
protein or ligand-bound protein) and the effect can be expressed in different ways
(e.g., normalized as remaining percent activity or percent inhibition). Further, depend-
ing on the assay design method, the same perturbation in the same model system may
result in increased or decreased signal.
Application to model LINCS profiling and panel assays and results
The concepts bioassay, measure group, and endpoint as described above enable the
formal definition of panel and profiling assays such as those routinely run in the
LINCS program. An effective modeling solution is relevant, because of the emphasis of
LINCS to operate on result profiles and signatures, in contrast to individual endpoints.
We define a panel assay as the parallel, spatially separate implementation of several
identical assays, but that vary in one parameter (other than the screened entity),
typically the target. A popular example is a kinase panel, for example the DisoveRx
KINOMEscan assay that is also run at LINCS and in which compounds are screened
against over 450 kinases in parallel. Similar to a panel assay, a profiling assay can gen-
erate a large number of readouts for any given tested compound, but all results are
obtained from the same physical experiment, i.e., the same well. Such assays are also
called multiplexed assays and rely on sophisticated assay methods and/or detection
technologies that enable the detection of many signals in parallel, such as flow cytome-
try, mass spectrometry or imaging. One example also run at LINCS is the L1000 tran-
scriptional profiling assay (vide supra). As illustrated in Figure 5, our approach would
also allow to define concentration response (e.g., IC50) kinase profiling assays via itera-
tive aggregation of sets of measure groups corresponding to two parameters, namely m
screening concentration (values) and n kinase targets. The first aggregation by screen-
ing concentration (e.g., via curve fitting) defines the IC50 endpoint for each kinase and
the second aggregation defines an IC50 kinase profile endpoint. An actual example of
such a assay is the ActivX Biosciences KiNative assay, which is also run in the LINCS
program. We have modeled several LINCS assays including KINOMEScan assay, tran-
scriptional response profiling assay, cell cycle state assay. The specific instances of
these assays including hundreds of kinase targets, transcribed genes, cell lines, etc was
implemented in an application ontology and these assays and screening results are
available in our LIFE software system [7].
An example of a phenotypic cell-based LINCS assay is the cell cycle state assay. It is
also described in BAO 2.0. In the LINCS project, several small molecules that are
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 12 of 22
known to function as kinase inhibitors were tested on cancer cell lines for their ability
to arrest the mitotic cell cycle. This assay was modeled in BAO as follows: the assay
design method is S phase assessment or M phase assessment method. The presence of
the markers, namely, EdU and anti-MPM-2 antibody, indicates that cells have entered/
completed S phase and M phase, respectively. Hoechst 33342 was used to stain nuclei
from all cells to obtain the total cell count in the assay. The detection method is fluor-
escence microscopy and the measured entity is DNA. The assay readout parameters
are intensity parameter and counting parameter. The intensity of EdU and MPM2
were measured in the nucleus and cytoplasm, respectively. The counts of Hoechst
33342, EdU and MPM2 positive cells were reported after the threshold to signal inten-
sity of each marker was applied. The endpoint was derived from the assay readout
parameters after normalizing with the assay controls. The endpoint for this assay is
percent apoptotic cells, percent mitotic cells, percent interphase cells, percent DNA
replicated cells, percent G2 arrested cells, and/or percent mitotic arrested cells. The
cellular phenotype or its disposition is obtained by quantifying cells which are positive
for each of these markers.
Categorizing mechanistically related assays by inference
BAO 2.0 contains detailed description of a range of common HTS assay, including the
categories: binding assay, cell cycle assay, cell viability assay, cytotoxicity assay, enzyme
activity assay, gene expression assay, redistribution assay, and signal transduction assay.
The essential information that was described for each assay type includes format,
method (including assay design), detection, endpoint, and molecular and cellular enti-
ties and their roles, qualities and functions describing the biology of the system or
which are key components involved in the assay design or detection methods. We
have previously shown how promiscuous frequent hitter compounds (undesired assay
artifacts) can be deconvoluted and categorized mechanistically based on detailed
knowledge about the assays and their related design and detection methods [29]. How-
ever, using the previous version of BAO (1.6) these assays were not yet defined in a
way that formalizes all necessary knowledge about their commonalities. This means
that previously, in order to perform mechanism-based cross assay analysis, some
human expert knowledge was required to identify and categorize related assays beyond
their asserted annotations.
BAO 2.0 provides a framework that enables automated classification of assays into
meaningful categories of interest, for example to aid in identifying common assay arti-
facts and their likely mechanism of action. We illustrate this using several related
assays: luciferase reporter gene assay, cell viability ATP quantitation assay, cytochrome
P450 enzyme activity assay, kinase activity assay, and luciferase enzyme activity assay.
Of these, the reporter gene and cell viability assays are cell-based, while the others are
biochemical assays. The modeling of these assays is illustrated in Figure 6. All assays
use a different assay design method. Therefore they cannot be identified as mechanisti-
cally related based on that annotation alone. The physical detection method chemilu-
minescence is the same for all assays, but it is too generic to classify the assays by
mechanisms that underlie artifacts, because luminescence can be generated by many
methods. However, among these examples, all assays perform (in different ways) the
luciferase-catalyzed chemical reaction of luciferin and ATP forming oxyluciferin and
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 13 of 22
light (luminescence) and thus luciferase and ATP participate in all these assays,
although in different roles. For example in the reporter gene assay the amount of
expressed luciferase is quantified by the intensity of light (luminescence) produced in
the presence of substrates, ATP, and luciferin. In the viability assays the proportion of
living cells is quantified by measuring ATP content, again by the same reaction (with
ATP as the limiting reagent in the role measured entity). Similarity ATP-coupled
assays measure the residual amount of ATP (e.g., after a kinase reaction) by a coupled
luciferase reaction. The P450 luciferin-coupled assay mentioned above measures the
amount of luciferin generated after detoxification by cytochrome P450 enzyme activity.
Luciferase enzyme activity assays quantify the biochemical luciferase enzyme activity
by the intensity of light, again using the same chemical reaction. In BAO2.0 we mod-
eled these assays with the necessary formalism to enable the reasoning engine to cate-
gorize the assays as mechanistically related. As an example, Figure 7 shows the
asserted TBox of the assay design method ATP quantitation using luciferase and ATP
coupled enzyme activity measurement method and the inferred TBox in which the lat-
ter is classified as a subclass of the former. For illustrative purposes we defined a class
of all assays with an assay design method in which luciferase participates (in any role).
The axioms and the asserted and inferred hierarchies are shown in Figure 8. All assays
mentioned above are inferred as assays that use luciferase, thus illustrating how BAO
formal assay definitions enable a classification based on the mechanistic principle of
the assay (assay design method). This in turn classifies the assay based on likely com-
mon artifacts (e.g. compounds that stabilize or inhibit luciferase) [29]. Figure 8 also
shows the justification for classifying the assays mentioned above under this category.
Collaborative development and application of BAO to annotate assays
We had previously annotated (using BAO 1.6) a large set of assays from PubChem [28]
and made these annotations searchable in BAOSearch [13], which is a Semantic Web
application. These annotations were now mapped to BAO 2.0 and expanded to include
additional information such as bioassay type, cell culture conditions, DNA construct
Figure 6 Conceptual modeling of different luciferase assays. Shown are bioassay, assay design
method, physical detection method and participants (molecular entities with a specified role in the assay).
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 14 of 22
Figure 7 Examples of luciferase assay design methods . Shown are the asserted TBox of ATP
quantitation using luciferase and ATP coupled enzyme activity measurement method and the inferred
TBox in which the latter is classified as a subclass of the former.
Figure 8 An example that infers all bioassays in the ontology that use luciferase. This example
provides asserted and inferred hierarchies for bioassays that use luciferase as a participant. It also provides
justification for luciferase reporter gene assay being a subclass of bioassay uses luciferase.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 15 of 22
details, roles, functions and qualities of molecular entities participating the the assays.
297 luciferase assays (vide supra) containing 328 measure groups are among the anno-
tated assays, including the ones mentioned above and others. As explained above, the
formalization of assays in BAO allows us to retrieve these assays based on a participant
such as luciferase or ATP, even though these molecular entities were not explicitly
annotated. A large project in which BAO has been applied and which in-turn significantly
influenced the evolution of BAO is BARD [6]. In BARD, all MLP data, consisting of over
600 probe discoveries, are curated and annotated using controlled terms and organized
into probe projects. BARD makes these data searchable in various ways and enables inte-
grative analysis. During the development of BARD, data curation and annotation, the
development of new terminology, and evolution of BAO has occurred in parallel. The
development of terminologies and ontologies to annotate assays at Novartis also influ-
enced our work in the BARD and BAO projects and highlights its relevance. We also
applied BAO to define LINCS assays (vide supra); we make LINCS data searchable and
explorable via the LIFE [7], which leverages Semantic Web technologies to integrate and
search diverse data types. Our RegenBase project [30] also leverages BAO. BAO is also
explored in the ChEMBL and PubChem projects, where BAO endpoints are used in RDF
schema [16], and at PubChem. As another example from the pharmaceutical industry, a
research group at Astra Zeneca is using BAO to annotate assays in the context of the
Open PHACTS project (personal communications).
Conclusions
We have developed BAO 2.0 as a reference for standard metadata terms and definitions
required to describe relevant information of low and high-throughput drug and probe
screening assays and results to enable effective data integration, aggregation, retrieval,
and analyses. BAO 2.0 has been developed collaboratively to provide wider scope in
describing and modeling diverse and complex assays. BAO is extended significantly with
regard to the previous version using domain knowledge and data annotated in BARD
and by other collaborators. We have described a flexible layered architecture to develop
and integrate plethora of modules from established biomedical ontologies and upper
level ontologies. Our modularization approach defines several integral distinct BAO 2.0
modules and separates internal from external modules and domain-level from structural
components. This approach facilitates the generation/extraction of derived ontologies (or
perspectives) that suit a particular use case or software application. We have generalized
BAO to enable modeling of result profiles (signatures) generated in panel and profiling
assays, for example those in the LINCS project. BAO leverages OWL DL
(SROIQ(D)) to capture and formalize knowledge about assays and screening results
and to enable computational systems to utilize knowledge. This enables the classification
of assays and screening results into categories that relate to the assay model system, the
biology (e.g., protein target or process), how a signal is generated and how it is detected,
and screening results. We demonstrated inference reasoning capabilities of BAO to clas-
sify assays into categories that relate to how the assay works. This offers the potential to
identify common promiscuous frequent hitters and their possible mechanism of action.
We have leveraged BAO in software tools, such as the Semantic Web software applica-
tions BAOSearch, LIFE, and the BARD system. We continue to develop and expand
BAO further with the goal to establish a standard to report chemical biology assays and
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 16 of 22
their results. For example, to better describe the pharmacology of GPCRs, we recently
developed a GPCR ontology framework [31]. We are also expanding BAO further to
describe high-content phenotypic assays. BAO is currently used in several public and
private screening projects and evaluated by a number of organizations and projects. The
participation of groups from industry and academia to develop and use BAO illustrates
the utility of the product as well as increasing public-private collaboration in pre-
competitive areas, such as the development of standards and ontologies. BAO 2.0 is freely
available from the BAO project website (http://bioassayontology.org) and the NCBO
BioPortal. Additional file 1 contains BAO 2.0 ontologies as well as the examples illustrated
in this manuscript.
Methods
BAO 2.0 development approach
BAO 2.0 was developed from BAO 1.6. It was performed in the following steps: First,
upper level classes were created to include the various entities that participate in a
bioassay, and their roles and qualities. Second, vocabulary files were created by moving
the individual upper classes to the respective files. Third, all the vocabulary files were
imported into a single file, called bao core (see Modularization below). Fourth, the
upper level ontology, BFO was imported into bao complete and the various vocabulary
files (either intact or separated as required) were moved to the respective upper level
ontology classes. The process of importing external ontology modules, including object
properties are described in detail below.
Generating and processing external ontology modules
BAO is currently using excerpts from eleven external ontologies (including BFO):
(1) Gene Ontology (GO); (2) Cell Line Ontology (CLO); (3) Unit Ontology (UO);
(4) NCBI Taxonomy (NCBITaxon); (5) Human Disease Ontology (DOID); (6) Chemical
Entities of Biological Interest (ChEBI); (7) UBERON (a comparative anatomy ontology);
(8) Phenotypic Quality Ontology (PATO); (9) Information Artifact Ontology (IAO); (10)
Relationship Ontology (RO); and (11) Basic Formal Ontology (BFO). The workflow for
extracting external ontologies is as follows: Domain experts provide the list of concepts
of interest and their ontology IDs. Based on these lists and the expression level of the
external ontologies, we either use Java programs with OWL API to extract modules
from the external ontologies of interest or we use the online tool OntoFox [32] to
extract the concepts of interest. Several of the ontologies listed above are taxonomies,
where we use OntoFox to avoid overlapping efforts and/or redundant code. Currently
we use OntoFox for the ontologies listed below: (1) GO; (2) CLO; (3) NCBITaxon;
(4) DOID; (5) ChEBI; (6) PATO; and (7) UBERON.
BAO modularization implementation
The Web Ontology Language (OWL) [33] Description Logic (DL) - provides a rich set of
constructors to model a domain of discourse. The DL expressivity comes with a substan-
tial computational cost, as the state-of-the-art DL reasoners costs 2NExpcomplete (e.
g., [22]). When the size of the ontology increases (number of axioms), the computational
cost increases exponentially. In order to manage the size complexity, we provide a modu-
larization methodology that preserves the required expressivity, yet being able to scale
with the size of the ontology. Figure 3 shows the basic structure of the methodology.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 17 of 22
The proposed modularization framework uses concepts from Directed Acyclic
Graphs (DAG)s [34]. First, we determine the abstract horizon between TBox and
ABox. TBox contains modules, which define the conceptualization without dependen-
cies. These modules are self contained and well defined with respect to the domain of
discourse. In these modules we provide concepts, relations, and individuals. The indivi-
duals are restricted to nominals, therefore they only act to close the class expressions.
Figure 3 shows the main components of the framework: the top left boxes are physical
files, i.e., each of them is an .owl file. They contain parts of our ontology, e.g., the top
left file may contain everything of the domain of discourse that we think is necessary
and important. We can have n of these modules.
Second, once the n modules are defined and if those modules have interdependent
axioms, they are provided with another ontology (or module), which imports the
necessary modules. At this level one could create any number of gluing modules,
which import other modules without dependencies or with dependencies. At this level,
the modules depends only on the modules of discourse. So, they all are combined in
another physical .owl file, which we may call bao core (c.f., 3). The purpose of this
core file is that it not only combines all of the submodules together (by referring to
concepts from other physical files), it also is self-contained. This means that there is
no outside term or relationship in this file.
Third, at this level we can design modules that import modules from our domain of
discourse, and also from third party ontologies. Third party ontologies could be large,
therefore a suitable module extraction method (e.g., OWL API) can be used to extract
only part of those ontologies (vide supra). An example would be using a BFO term or
a RO relationships. We would model this in the bao axiom level. We can have one bao
axiom file or multiple files, each may be modeled for a different purpose, e.g., tailored
for various research groups. Thus, bao Axiom 1 . . . n as seen in Figure 3. Once these
ontologies are imported, the alignment takes place. The alignments are defined for
concepts and relations using equivalence or subsumption DL constructs. The align-
ment depends on the domain experts best guesses.
Forth, release the TBox based on the modules created from the third phase. Depending
on the end-users, the modules are combined without loss of generality. With this
methodology we make sure that we only send out physical files that contain our (and the
absolute necessary) knowledge.
Fifth, at this level, the necessary modules ABoxes (again 1 . . . n ABoxes) are created.
ABoxes can be loaded to a triple store or to a distributed file system (Hadoop DFS
[35]) in a way that one could achieve pseudo-parallel reasoning.
Finally, using modules, we define views on the knowledge base. These are files that con-
tain imports (both direct and indirect) from various TBoxes and ABoxes modules for the
end-user. It can be seen as a view, using database terminology. In essence, we will be able
to tailor these views based on the modules that we need. We expect that this methodology
will speed-up the loading process, since only the necessary modules are loaded rather than
every file that imports thousands of unnecessary and possibly redundant terms (e.g., due
to potential loops in the imports). Therefore, BAO modules: (1) modify, expand, and
maintain BAO independently; (2) use BAO in related efforts, such as knowledge reporting,
more efficiently; (3) expand and synchronize BAO concepts in related efforts (e.g., BARD,
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 18 of 22
LIFE, RegenBase, etc.); (4) reuse parts of BAO for different projects; (5) use other ontolo-
gies easily and without effecting BAO in order to support community efforts; and (6) pro-
vide transparent mapping of BAO to upper ontologies.
In summary, our methodology is as follows: (1) different files for different modules
should be created and each module should contain all the concepts as a taxonomy file;
(2) after the n modules are created as taxonomies, the core owl file should combine
these modules; (3) the axioms related to the core ontology terms should be added to
the core owl file; (4) once the core owl file is created that has nothing but the ontol-
ogys native concepts and axioms created by the native concepts, the third level file
that has external ontologies should be created. The external ontologies can be added
by using different combinations and related axioms can be added to the ontology at
this level; (5) after creation of the one or more owl files that link different external
ontologies and contain related axioms, individuals related with the ontology are added/
loaded in the next level; (6) the view file that contains imports (both direct and indir-
ect) from various TBoxes and ABoxes is created from user specifications. Based on this
framework and methodology we have modeled the BAO 2.0. Figure 4 provides a complete
description of the current vocabularies, modules, and axioms files and their connections
developed for the ontology.
Our modularization framework differs significantly from existing methodologies: In
decision making, a state represents a situation in which decisions should be made. An
ontology provides a basic framework to represent situations in which decisions are
made. Depending on the layer in which the decisions needs to be made, a state can
represent from low-level signals to high-level mental abstractions. Therefore, state
abstraction provides the basis in which layer-wise decisions are made. OWL ontologies
provides mechanisms such as owl:import to represent state abstractions. But this has
not been explicitly studied in large scale ontologies. Our modularization framework
assesses the capabilities of OWL ontologies to represent state abstractions in different
complexities.
Ontology interpretation provides mechanism to represent vocabularies for classes,
roles, and individuals. Without any other assumption, the interpretations of these entities
provides the state of the system. These entities are analogues to low-level sensations
from perceptions. Our modularization framework captures these representations in the
vocabulary layer. One can use these representations for tasks such as to populate drop-
down menus in a web-application etc. These representations are at its basic levels and
the system does not assume any constraints. Having provided constraints leads to OWL
ontologies to represent state abstractions.
In order to provide additional information related to basic entities, the next step is to
enrich the state with constraints. In first-order-predicate logic, constraints are provided
by axioms. Therefore, in OWL ontologies, we use axioms as the method to provide the
constraints, hence, the state abstractions. The modules in the modularization frame-
work provides different constraints. The modules are connected though the owl:
imports mechanism, and the constraints are provided by OWL constructs available in
SROIQ(D) description logic. Therefore, at each layer, do-main experts provide
axioms for the best of their knowledge. The modularization framework provides hard
boundaries in which, a domain user can extract constraints. This partially addresses
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 19 of 22
one of the problems in ontologies: axioms extraction, which has NP-hard complexity.
The hard boundaries provides decision points where abstract state of another system
should have been extracted, for example, one can extract whole BAO abstract state
without reference to a upper-level ontology such as BFO.
The proposed modularization framework provides basic steps to implement our
knowledge base reporting (KBR) application. It needs to infer knowledge from massive
ABoxes with parallel reasoning using frameworks such as Map-Reduce. KBR needs deci-
sion layers in which to knowledge to be reported, and our modularization framework
provides those decision points.
In artificial intelligence, state representations and state abstractions are an open pro-
blem. OWL ontologies, with respect to first-order-predicate logic, provides methods to
represent knowledge, but, to our knowledge the state abstraction is not discussed
widely. Our modularization framework addresses these problems and possibilities in
which state abstraction can be generalized.
Assay annotations: terminology alignment, reformatting and processing
As described in our earlier publication [12], assays from PubChem were annotated
using BAO 1.6 terminology. These existing annotations were mapped to correspond-
ing BAO 2.0 classes and annotations were expanded including cell culture condi-
tions, DNA construct, quality, role and function of molecular entities. In addition,
the object properties and data properties were refined; many were imported from
the RO. Cell line, gene and protein names were standardized by importing the
nomenclature from CLO or specific repository, NCBI or HUGO, and UniProt,
respectively. In total, 1,000 assays in the PubChem database were annotated using
BAO 2.0. These are leveraged in BARD; however, BARD includes all assays and
results generated by the MLP screening centers (>6, 000) and organizes them by
probe projects (>600). In the process of annotating assays, new terms were collected
and subsequently mapped or added to BAO manually (after expert review). In addi-
tion, we incorporated terms from some of the Novartis ontology modules and terms
requested by other collaborating group (e.g., Astra Zeneca). Assay annotations were
captured in a spread sheet with column headers that correspond to BAO classes or
relations. For the luciferase assays, we translated the columns headers for the most
important annotations and their contents into triples (by mapping column headers
to corresponding relations) and loaded them into a RDF triple store as previously
described. Figure 8 shows an example where we have used BAO 2.0 to infer all
bioassays that use a method in which Luciferin 4-monooxygenase is a participant.
We have defined the equivalent class bioassay uses luciferase as bioassay ? ?has
assay method (assay design method n ?has participant Luciferin 4-monooxygen-
ase). OWL DL reasoners infer that cell viability ATP quantitation assay, cytochrome
P450 enzyme activity assay, kinase activity assay, luciferase enzyme activity assay,
and luciferase reporter gene assay are indeed luciferase assays. Figure 8 provides jus-
tification for luciferase reporter gene assay being a subclass of bioassay uses lucifer-
ase. This allows us to identify assays that are annotated with any of these assay
design methods as assays that use luciferase, which is relevant to identify assay arti-
facts across various different bioassays.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 20 of 22
Additional material
Additional file 1: BAO 2.x build 2866. BAO version 2.x (build 2866) is a collection of OWL files that describe our
domain of discourse. These files are serialized in OWL XML file format. In order to view and perform reasoning of
BAO, we recommend of using a standard OWL editor that support SROIQ(D) DL constructs, such as, Protégé-4.3
open source ontology editor and knowledgebase framework (http://protege.stanford.edu). In addition, up-to-date
BAO releases are freely available from the BioAssay Ontology (BAO) project website (http://bioassayontology.org/
wp/bao) and the NCBO BioPortal.
Competing interests
The authors declare that they have no competing interests. The views presented in this paper do not necessarily
represent or reflect those of the funding organizations.
Authors contributions
SA and UDV contributed equally to the presented work. UDV, SA, AM, KS, PAC, JAB, AS, AJM, MR, SB, VL, and SCS
developed the ontology (terminology, definitions, properties). UDV mapped BAO concepts to BFO. SA, HK, UV, and
SCS developed the modularization method and OWL DL implementation. SA and HK developed the ontology
supporting tools. HK, UDV, AJM, PAC, SB, SA, and SCS provided the ontology alignment with external ontologies and
BARD. SA, UDV, HK, AK, DT, and SB developed ontology applications (assay annotation, software implementation). CC,
VL, and SCS conducted ontology releases, QC, support, and CC maintains the BAO website. SCS envisioned and
designed the BAO project. SA and SCS wrote the paper with contributions from UDV, HK, UV, and VL.
Acknowledgements
This work was funded by the National Institutes of Health (NIH) National Human Genome Research Institute (RC2-
HG005668 and RC2-HG005668-02S1, awarded to SCS and VL), the NIH LINCS program via the National Heart, Lung,
and Blood Institute (U01-HL111561 and U01-HL111561-02S1, awarded to SCS), the National Institute of Neurological
Disorders and Stroke (R01-NS080145, awarded to VL, Prof John Bixby and SCS ). JAB, PAC, and SB were supported as
part of the NIH RoadMap Molecular Libraries Initiative (U54-HG005032, awarded to Prof Stuart L. Schreiber). We also
acknowledge resources from the Center of Computational Science of the University of Miami.
Declarations
The publication costs for this article were funded by the Center of Computational Science of the University of Miami.
This article has been published as part of Journal of Biomedical Semantics Volume 5 Supplement 1, 2014: Proceedings
of the Bio-Ontologies Special Interest Group 2013. The full contents of the supplement are available online at http://
www.jbiomedsem.com/supplements/5/S1.
Authors details
1Department of Computer Science, University of Miami, 1365 Memorial Drive, 33146 Coral Gables, FL, USA. 2Center for
Computational Science, University of Miami, 1320 S. Dixie Highway, Gables One Tower, 33146 Coral Gables, FL, USA.
3The Miami Project to Cure Paralysis, 1095 NW 14th Terrace, 33136 Miami, FL, USA. 4Department of Molecular and
Cellular Pharmacology, University of Miami School of Medicine, 1120 NW 14th Street, CRB 650 (M-857), 33136 Miami,
FL, USA. 57 Cambridge Center, Cambridge, MA 02142, MA, USA. 6Novartis Institutes for BioMedical Research, 250
Massachusetts Avenue, 02139 Cambridge, MA, USA. 7Thomson Reuters, 5901 Priestly Drive, Suite 200, 92008 Carlsbad,
CA, USA.
Published: 3 June 2014
PROCEEDINGS Open Access
Preserving sequence annotations across reference
sequences
Zuotian Tatum1,4*, Marco Roos1,2, Andrew P Gibson1, Peter EM Taschner1, Mark Thompson1, Erik A Schultes1,
Jeroen FJ Laros1,3
From Bio-Ontologies Special Interest Group 2013
Berlin, Germany. 20 July 2013
* Correspondence: z.tatum@lumc.nl
1Department of Human Genetics,
Center for Human and Clinical
Genetics, Leiden University Medical
Center, Einthovenweg 20, 2333 ZC
Leiden, the Netherlands
Abstract
Background: Matching and comparing sequence annotations of different reference
sequences is vital to genomics research, yet many annotation formats do not specify
the reference sequence types or versions used. This makes the integration of
annotations from different sources difficult and error prone.
Results: As part of our effort to create linked data for interoperable sequence
annotations, we present an RDF data model for sequence annotation using the
ontological framework established by the OBO Foundry ontologies and the Basic
Formal Ontology (BFO). We defined reference sequences as the common domain of
integration for sequence annotations, and identified three semantic relationships
between sequence annotations. In doing so, we created the Reference Sequence
Annotation to compensate for gaps in the SO and in its mapping to BFO, particularly
for annotations that refer to versions of consensus reference sequences. Moreover,
we present three integration models for sequence annotations using different
reference assemblies.
Conclusions: We demonstrated a working example of a sequence annotation
instance, and how this instance can be linked to other annotations on different
reference sequences. Sequence annotations in this format are semantically rich and
can be integrated easily with different assemblies. We also identify other challenges
of modeling reference sequences with the BFO.
Background
Sequence annotations and their relationship with reference sequences
Sequence annotations are information artifacts that add biologically meaningful informa-
tion to specific locations on genomic, gene, transcript or protein sequences. For example:
1) Gene OR4F5 is located on human chromosome 1 (build hg19), from position
69090 to 70008.
2) Substitution of C by T at location 178 of transcript reference sequence
NM_004006.2 results in nonsense variant Gln60* in protein reference sequence
NP_003997.1.
Tatum et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S6
http://www.jbiomedsem.com/content/5/S1/S6 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2014 Tatum et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in
any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver (http://
creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Sequence annotations are only meaningful if the reference sequence is known. However,
specifying a stable reference is not necessarily straightforward. Before the Human Genome
Project, Locus Specific Databases (LSDB) were recommended for storing and sharing gene
centric variant annotations [1]. To date, the most popular platform for storing these tran-
script variants is the Leiden Open-source Variation Database v.2 (LOVD2) [2]. In each
LOVD2 instance, a stable transcript sequence is chosen as the reference sequence of
each gene. Variants are annotated with descriptions of sequence variations and positions
according to the chosen transcript sequence. There are many advantages of using gene/
transcript centric annotation approach. First, the length of a gene is much shorter than a
locus/chromosome, therefore maintaining the sequence content is much easier. Secondly,
it limits annotations mainly to the protein coding regions of the genome, therefore focus-
ing more on easy to predict phenotypic effects. However, LSDBs typically limit descrip-
tions of DNA variants to a single transcript, even when multiple transcripts may be
affected. Depending on which transcript is used, the variant description may look very
different. To calculate the location of a variant based on a different reference sequence, an
external conversion tool has to be used for the position conversion [3]. Disambiguation of
the variant description is an essential step in the context of data integration and
preservation.
However, not all biological questions are locus specific. As sequencing technologies
advanced in the past 15 years, more and more studies are omics focused, requiring a
stable and complete reference genome [4]. The Human Genome project was com-
pleted in April 2003, followed by the release of human genome assembly NCBI35/hg17 in
May 2004. Sequence gaps and assembly errors were removed and newly discovered genes,
(non-coding) transcripts and proteins were annotated with every new release up to
GRCh37/hg19 (February, 2009) [5]. As reference sequences are revised, it becomes
increasingly difficult to track and compare annotations. Researchers today share their
results of genome-wide genomic and epigenetic studies in publications and databases, but
they often fail to mention the exact version of the reference genome sequence. Moreover,
many popular annotation file formats do not explicitly ask for reference sequence version
information. It is up to the user to embed this information in the file description through
natural language. Consequently, when using these formats to exchange data for computa-
tional analysis and data integration, essential metadata is too easily lost. For example, the
ENCODE Project Consortium [6] has effectively shared their data by publishing them as
annotation tracks in the UCSC genome browser [7]. However, these annotation tracks use
Browser Extensible Data (BED) format, which does not explicitly state the reference
assembly version within the file. To propagate current annotations to the forthcoming
GRCh38/hg20 and alternative genome assemblies, it is crucial to preserve annotations
with their respective reference sequence versions.
A Semantic Web approach to data integration
A possible approach to exposing sequence variation annotations in a computer accessible
format is provided by Sematic Web languages and tools [8]. It effectively removes
the boundaries between annotating data, linking data, and making data machine readable
[9-11]. By representing data and metadata in Resource Description Framework (RDF) and
using shared ontologies in RDF and Web Ontology Language (OWL), mismatches
between database schemas and the identity of its content can be addressed [12,13].
Tatum et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S6
http://www.jbiomedsem.com/content/5/S1/S6
Page 2 of 13
A zfirst attempt for mutation data was presented by Zappa and coworkers, who produced
a mutation database for TP53 as Linked Open Data [14]. They followed the principles of
Linked Data [15] and applied various existing ontologies to achieve optimal interoperabil-
ity. However, they did not address the problem of integrating mutation data that were
annotated using different reference sequences. They did not model genomic locations of
annotations in detail, which makes querying this dataset difficult.
Ontological framework for data integration across resources
Formal ontologies play an important role in semantic data integration between informa-
tion systems [16,17], bringing conceptual coherence, stability, and scalability to the applied
domain, which can greatly increase data interoperability [17,18]. The Open Biological and
Biomedical Ontologies (OBO) Foundry provides a suite of orthogonal interoperable ontol-
ogies to aid knowledge integration in the biomedical domain [19]. To take advantage of
the OBO Foundry ontologies, we have chosen Basic Formal Ontology (BFO) [20] as our
upper ontological framework for data modeling [20]. Other ontologies in OBO that are
relevant to this paper include the Information Artifact Ontology (IAO) [21], the Sequence
Ontology (SO) [22], the Ontology for Genetic Interval (OGI) [23], and the Relation
Ontology (RO) [24].
Previous efforts on modeling biological sequences and sequence annotations in the OBO
community have taken primarily a biological viewpoint. Thus, sequences refer to biologi-
cal molecules, and sequence annotations refer to features defined with respect to biologi-
cal process [22,25]. The SO focuses on creating a set of consistent vocabularies that
describe the biological functions of these sequences and defining the biological relation-
ships between these sequences [22]. OGI models the biological physical sequence by
adopting the realism approach from BFO, and further contributes to this model by adding
spatial topological relationships between sequences [23]. However, Hoehndorf et al.
pointed out a gap between this biological model and information systems that are used to
store sequence annotations [26]. To bridge this gap, they have proposed three views of
biological sequences: molecular, syntactic, and abstract. Molecular sequences are DNA and
RNA molecules as well as proteins. Syntactic sequences are strings like ACAC and repre-
sent the arrangement of the molecules in the molecular sequences. Abstract sequences
represent an equivalence class of sequence tokens or representations. They point out that
without such a clear distinction data integration is hampered. Indeed, the SO community
acknowledged the lack of distinction that is made by biologists between abstract, syntactic,
and molecular sequences. Bada and Eilbeck proposed a strategy of separating SO into two
parallel ontologies: one for molecular sequences, the other with abstract sequences
(abstract in a broader sense than meant by Hoehndorf). The former would be an extension
of the Molecular Sequence Ontology while the SO would focus more on the abstract
sequences referring to sequences, and parts of sequences [27]. However, this new
alignment strategy is still under discussion.
Beyond the OBO Foundry there are additional relevant ontologies applicable to
sequence annotation. The Feature Annotation Location Description Ontology (FALDO)
is the latest effort to address the void of describing sequence annotations from the
information systems perspective [28]. It is designed to be general enough to describe
annotations with various level of location complexity, but not addresses issues such as
the meaning of or the evidence of the location.
Tatum et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S6
http://www.jbiomedsem.com/content/5/S1/S6
Page 3 of 13
Aim of this paper
Our aim is to create an RDF data model for describing sequence annotation instances
within an established ontological framework that fits our practice of working with refer-
ence sequences and different versions of genome assemblies. We provide a mechanism
for linking annotation instances to different reference sequences. We also present some
of the challenges in aligning our approach with current OBO Foundry ontologies.
Results and discussion
Describing sequence annotation instances
Our starting point for modeling sequence annotations was the BED format, a widely used
table-based format for sequence annotations that is easy to use and efficient to store (see
Figure 1). It typically consists of rows with a reference (e.g. a chromosome identifier), start
and end position on that reference, and a value for the annotation. Most UCSC genome
browser annotations can be downloaded as BED tracks. We started by deriving our RDF
model from the BED format: (i) we identified the desired upper ontological framework for
the domain of interest; (ii) we converted data in the BED track to RDF triples; (iii)
we further transformed the resulting triples by adding class definitions and ontology
mappings to the final model. We describe these steps below:
Upper ontological framework
We chose to use the BFO (version 1.1) as our top-level ontological framework. We
augmented BFO with a minimal Reference Sequence Annotation (RSA) ontology to
capture classes and predicates, and defined alignment strategies for RSA with OBO.
Data transformation to triples
As a preparative step, we first created annotation instances that closely matched our ori-
ginal data format. We created a naive model for sequence annotation to directly trans-
late the information in the BED file with the addition of the reference assembly name
(Figure 2). Predicates linking the resource and its property values were derived from the
BED format description. At this stage, we used rdfs:Literal to capture concepts without
further ontological grounding (i.e., rdf:type relations). This data-centric approach to
semantic modeling is similar to the syntactic conversion that is often used for integra-
tion of non-RDF resources, where table values are converted to literals, and table names
and headers to classes and properties without any further semantic modelling [29].
Figure 1 BED file examples. RefSeq transcript annotation in BED format on genome builds hg19 (a) and
hg18 (b). The second line contains the start and end positions of the NM_001005484 transcript encoded
by the OR4F5 gene that differ per assembly. Note that the BED file header line does not explicitly state the
reference sequence information. The submitter can only embed this information in the track description
through natural language.
Tatum et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S6
http://www.jbiomedsem.com/content/5/S1/S6
Page 4 of 13
These naive models usually have limited semantic depth, such that finding common
elements for integration with other data sources can be difficult. Therefore, the model is
often linked to a more sophisticated, or personal model. In our case, we used the naive
model as a starting point in the modeling process, replacing it step by step by a more
precise model (Figure 3). Content of rdfs:Literals from the naive model were thus
converted to owl:instances, and class definitions were added. Below, we discuss our deri-
vation of the new model step-by-step, while explaining the placement of new RSA
classes and predicates, the reuse of existing ontologies, and potential problems with
OBO alignment. An RDF representation of the final model is shown as follows:
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix rsa: <http://rdf.biosemantics.org/ontologies/rsa#> .
@prefix hg19: <http://rdf.biosemantics.org/data/genomeassem-
blies/hg19#> .
@base <http://rdf.biosemantics.org/examples/sequence_annota-
tion#> .
:transcript a rsa:SequenceAnnotation ;
rsa:refseqID NM_001005484";
rsa:isAnnotatedAt :location .
:location a rsa:AnnotationLocation ;
rsa:start 69090"^^xsd:int ;
rsa:end 70008"^^xsd:int ;
rsa:mapsTo hg19:chr1 ;
Figure 2 Naive model. Naïve transformation of a BED sequence annotation. Predicates used in this model
are placeholders and replaced in a later stage.
Figure 3 Semantic model. A sequence annotation instance after semantic transformation.
Tatum et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S6
http://www.jbiomedsem.com/content/5/S1/S6
Page 5 of 13
rsa:hasOrientation rsa:forward
JOURNAL OF
BIOMEDICAL SEMANTICS
Hastings et al. Journal of Biomedical Semantics 2014, 5:38
http://www.jbiomedsem.com/content/5/1/38
RESEARCH Open Access
Evaluating the Emotion Ontology through use
in the self-reporting of emotional responses at
an academic conference
Janna Hastings1,2*, Andy Brass3,4, Colin Caine3, Caroline Jay3 and Robert Stevens3
Abstract
Background: We evaluate the application of the Emotion Ontology (EM) to the task of self-reporting of emotional
experience in the context of audience response to academic presentations at the International Conference on
Biomedical Ontology (ICBO). Ontology evaluation is regarded as a difficult task. Types of ontology evaluation range
from gauging adherence to some philosophical principles, following some engineering method, to assessing fitness
for purpose. The Emotion Ontology (EM) represents emotions and all related affective phenomena, and should enable
self-reporting or articulation of emotional states and responses; how do we know if this is the case? Here we use the
EM in the wild in order to evaluate the EMs ability to capture peoples self-reported emotional responses to a
situation through use of the vocabulary provided by the EM.
Results: To achieve this evaluation we developed a tool, EmOntoTag, in which audience members were able to
capture their self-reported emotional responses to scientific presentations using the vocabulary offered by the EM. We
furthermore asked participants using the tool to rate the appropriateness of an EM vocabulary term for capturing their
self-assessed emotional response. Participants were also able to suggest improvements to the EM using a free-text
feedback facility. Here, we present the data captured and analyse the EMs fitness for purpose in reporting emotional
responses to conference talks.
Conclusions: Based on our analysis of this data set, our primary finding is that the audience are able to articulate
their emotional response to a talk via the EM, and reporting via the EM ontology is able to draw distinctions between
the audiences response to a speaker and between the speakers (or talks) themselves. Thus we can conclude that the
vocabulary provided at the leaves of the EM are fit for purpose in this setting. We additionally obtained interesting
observations from the experiment as a whole, such as that the majority of emotions captured had positive valence,
and the free-form feedback supplied new terms for the EM.
Availability: EmOntoTag can be seen at http://www.bioontology.ch/emontotag; source code can be downloaded
from http://emotion-ontology.googlecode.com/svn/trunk/apps/emontotag/ and the ontology is available at
http://purl.obolibrary.org/obo/MFOEM.owl.
*Correspondence: hastings@ebi.ac.uk
1Cheminformatics and Metabolism, EMBL  European Bioinformatics Institute,
Wellcome Trust Genome Campus, Hinxton CB10 1SD, UK
2Swiss Center for Affective Sciences, University of Geneva, Geneva, Switzerland
Full list of author information is available at the end of the article
© 2014 Hastings et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication
waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise
stated.
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 2 of 17
http://www.jbiomedsem.com/content/5/1/38
Background
Ontology evaluation is the assessment of how good an
ontology is for one or multiple purposes [1]. Biomedi-
cal ontologies are being developed to address multiple
requirements in biology andmedicine including standard-
isation, data annotation and statistical analysis [2]. Ontol-
ogy evaluation is recognised to be a difficult problem [1],
with modes of evaluation ranging from conformance to
some philosophical principle [3,4], adherence to a speci-
fied method [5], conformance to a corpus of text or data
[6], to fitness for purpose for a given task [7].
Formal evaluations of biomedical ontologies are rare
and this paper presents an evaluation of an ontology
vocabularys ability to make the distinctions necessary in
a field of interest. To this end, we report on an evalua-
tion of the suitability of the Emotion Ontology (EM, [8,9])
in use for the self-reporting of emotional experiences
at an academic conference. As the ontology has previ-
ously been described in [9] we do not here repeat that
material. Rather, we focus on describing our experiment
in which in order to assess the emotional vocabulary
of the EMs fitness for purpose for the self report-
ing of emotional experience, we used the ontologys
vocabulary to capture an audiences emotional responses
to academic presentations at the International Confer-
ence on Biomedical Ontology (ICBO) [10] that was
held in Graz, Austria in July 2012. We conducted this
evaluation through the development of a tool, EmOn-
toTag, by means of which audience members were
able to capture their emotional responses to the sci-
entific presentations using the vocabulary offered by
the EM.
An ontology makes distinctions between entities in a
field of interest. In our case, the EM makes distinctions
between types of emotions, such as being bored or inter-
ested. In an academic conference we can assume that
neither talks nor the audience are homogeneous in emo-
tional response provoked or elicited. In the biomedical
ontology community in particular, there are well known
contentious approaches to ontology engineering [3,11].
Thus we can expect that different talks will provoke differ-
ent emotional responses and that audience members wil
have a range of differing emotional responses to talks at
ICBO 2012. From this, our null hypothesis (H0) is:
The EM will not enable audience members to
articulate their emotional response to a talk
appropriately such that we can cluster the audience by
their response to a talk. We will find that people and
talks are not able to be distinguished by the
descriptions of emotional responses.
If the null hypothesis is rejected, we may expect confer-
ence participants to be able to use the EM to articulate
an emotional response to a talk and that talks and the
audience can be partitioned by emotional response.
To test this hypothesis, we allowed audience members
to give their emotional responses to talks using the vocab-
ulary drawn from the ontology, and also asked them to
rate how appropriate an EM vocabulary term was for
articulating an emotional response.
Examples of the phrases that were used to capture emo-
tions during the conference include I feel interested,
I feel bored and I think that this is being caused super-
naturally. The rating given by users as to how easy it was
to use the EM to capture their emotions ranged from 1
(it was difficult to capture the emotion being experienced)
to 5 (easy to capture the emotion being experienced). We
also asked participants to suggest improvements to the
ontologys content using a free-text feedback facility; the
aim here was to capture emotions that participants felt
they could not articulate using the EM.
We were also able to collect the following information:
 Which vocabulary terms were used and with what
frequency;
 The numbers of terms used per talk;
 The time at which a term was used and by which
(anonymous) audience member;
 The number of people participating in the study;
 The strength of emotional response to talks.
While the evaluation of ontologies in use in applications
have been conducted before (as discussed in [1,12]; for a
recent example evaluating the Gene Ontology in use see
[13]), we believe that the approach we have followed of
combining the use of an ontology in an application with
the simultaneous rating of the ease of use of the ontologys
vocabulary for that application is a novel technique that
could have applicability outside the scope of the present
investigation.
The Emotion Ontology
Capture of emotional experience is a component of a vari-
ety of different research and application scenarios. For
example, self-reported emotional experiences are often
captured to monitor mood fluctuations between clinical
visits in the clinical treatment of depression and bipo-
lar disorder [14,15]. Self-reported emotional experiences
may also be useful in the assessment of response to soft-
ware tools, new products, or audience response to aca-
demic presentations. Various tools have been developed
that allow capture of emotional experience in the con-
text of specific application needs (e.g. [16,17]). However,
there has been no agreement on shared identifiers for the
underlying structure of the emotional domain such that
annotations could be compared between different tools
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 3 of 17
http://www.jbiomedsem.com/content/5/1/38
and across different projects that employ different levels
of specificity.
Ontologies provide a flexible hierarchically organised
structure for defining entities and vocabulary within a
domain [18], and have been highly successful in enabling
standardisation in biomedical contexts [2]. Reflected in
generic ontology languages such as the Web Ontology
Language (OWL, [19]), ontologies are computable and
supported by many open source libraries across multi-
ple languages, thus are suitable for implementation into
a wide range of different tools. Reuse of a shared ontol-
ogy across multiple tools enables subsequent aggrega-
tion and comparison between annotations arising from
heterogeneous projects [20], as has been amply illus-
trated by successful applications of the Gene Ontology
project [21,22].
The Emotion Ontology (EM) is an ontology being
developed for the domain of the emotions and all
related affective phenomena [8,9]. The ontology aims to
address diverse requirements arising from the full range
of disciplines involved in research into affective phe-
nomena, including psychology, psychiatry, neuroscience,
biomedicine and the life sciences. Such applications
include standardised data annotation for aggregation
across databases, meta-analyses of primary research
results, mapping across disciplines for translation of pri-
mary research into candidate therapeuticals, semantic
searching and querying of literature and databases such
as implemented by the Neuroscience Information Frame-
work [23], and automated text analysis for addressing the
semi-automatic curation of the vast quantities of scien-
tific literature [24]. We have previously used the ontology
in the automatic detection of emotions in the text of sui-
cide notes, with potential application to the analysis of the
diary writings of suicide-risk patients to assist in suicide
prevention measures [25].
The EM currently consists of distinct branches for emo-
tions and related phenomena. As is documented in the
metadata of the ontology, the vocabulary included in
the EM ontology has largely been drawn from [26] and
the vocabularies used in the GRID cross-cultural project
[27]. The upper-level structure of the ontology, as
reported in [9], distinguishes emotions proper as complex
processes, for example anger or fear, from other physio-
logical and mental processes that may form a part of an
emotion process, including cognitive appraisal processes
and subjective feelings. From this complex structure, we
identified three branches of the ontology that we believed
to be of relevance for the self-reporting of emotional
experiences: appraisals (cognitive judgements that may
trigger emotions), subjective feelings (inner awareness of
affective feelings), and emotions proper.
For example, anger is an emotion defined in the EM as
Anger is a negative emotion, characterised by feelings of
unpleasantness and high arousal, in the form of antago-
nistic feelings and action tendencies, and fear is defined
as An activated, aversive emotion that motivates attempts
to cope with events that provide threats to the survival
or well-being of organisms. Characterised by feelings of
threat and impending doom, and by an urge to get out
of the situation. Feeling restless is a subjective feeling
defined in the EM as The subjective emotional feeling of
restlessness, a state of not being calm, of an agitation to do
something.
The other branches of the ontology, including
behavioural responses to emotions such as facial expres-
sions and physiological responses to emotions such as an
increased heart rate, were excluded from this experiment
by virtue of these not being appropriate to the use case of
self-reporting of emotional experience.
Methods
We used self-reporting of emotional response to the
talks at ICBO 2012 and our approach had the following
components:
1. Design and implementation of a Web application
(EmOntoTag) that enables users to anonymously
login, tag their emotional response to an ICBO
2012 presentation using terms from the EM, and
record how appropriate they felt an EM term was
at articulating an emotional response.
2. Obtaining permission from presenters for their
presentation at ICBO to be included in this study.
3. Running the experiment during the ICBO conference
on July 2325 2012, and analysis of the data obtained.
Design and development of EmOntoTag
We conceived a tool that would be light-weight and able
to run in any Web browser to enable the broadest range
of conference attenders to participate in the experiment.
The primary requirement that we identified for this tool
was that it would allow the user to self-report their cur-
rent emotional experience with a minimum of overhead,
such as technical terminology or excessive clicking. Fur-
thermore, in order to address the hypothesis we forced
the user to capture how well the EMs vocabulary was able
to capture the respondants emotional response as a rat-
ing attached to every record they made of their emotional
experience with this tool.
AWeb tool was implemented in the Python language on
top of a MySQL database. This was subsequently wrapped
with the Jython JavaPython bridge to enable deployment
in a Tomcat Web application server. All source code for
the implementation of EmOntoTag is available from the
repository hosted at [8]. EmOntoTag can be accessed via
http://www.bioontology.ch/emontotag with login guest,
for which responses will not be recorded.
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 4 of 17
http://www.jbiomedsem.com/content/5/1/38
In order to anonymise the users of the tool, while still
controlling access in order to ensure that we could distin-
guish different users responses, we prepared anonymous
random access codes printed on sheets of paper which
were handed out to conference participants by the con-
ference organisers. Only the anonymous random codes
were stored against the tags in the underlying database. An
example of the sheet handed to conference participants is
available as Additional file 1.
After obtaining an access code and logging into the tool,
users were presented with the list of available presenta-
tions linked to information about the conference schedule.
The tool had a register of the conference schedule and was
able to direct users to the currently ongoing presentation
(at least according to the conference schedule). Presenta-
tions were indexed by author names and by presentation
title. Only those presentations for which the presenter
agreed in advance to participate in the experiment were
enabled in the tool; for those presentations for which the
presenter did not agree, the tool showed a message that
the presentation was not available for tagging.
For each presentation, the users were offered a response
capture interface that allowed them to articulate their
emotional response using the vocabulary from the under-
lying EM ontology. As described in [9], EM distinguishes
emotions proper, i.e. full complex emotional experi-
ences associated with an object, from subjective feelings,
which are simpler feelings and which dont necessarily
have an object, and appraisals, which are the cognitive
(thought) component of emotions which are viewed in
some theories to be the triggers of emotions [28]. As a
design choice to enable natural emotional expression,
options were provided to the user in the context of sen-
tence completion, where the allowed sentences beganwith
I feel and I think. I feel was used as the precursor
to the selection options from the emotion and subjec-
tive feeling branches of the ontology, while I think was
used as the precursor to the appraisal (cognitive) branch
of the ontology. To accommodate the fact that the labels
for emotion terminology in the ontology were in the noun
form, e.g. fear, additional synonyms were added to the
ontology that would fit better in the context of a sentence,
e.g. afraid.
Examples of the sentences that were available for expres-
sion of emotions include:
 I feel interested ,
 I feel despairing,
 I feel calm and
 I think that this is familiar.
The full list of options that were provided in the drop-
down selections in the tool interface are provided in
Table 1.
Options for sentence completion were presented in a
random sequence, not sorted alphabetically, in order to
avoid bias towards certain terms. This almost certainly
reduced usability of EmOntoTag, but our desire to avoid
too much bias over-rode this usability issue. Figure 1
shows a screenshot of the EmOntoTag user interface for
selection of an EM sentence during the experiment.
Having specified a sentence describing the emotional
experience at that moment, the user was required to
rate how well the vocabulary provided by the EM cap-
tured their emotional response. They were also able
to offer a strength of response for the EM sentence
used. All the users previously captured sentences for
that talk were displayed in a table lower on the screen,
indexed by the time of capture, and it was possible to
delete previous sentences, to allow for the correction of
errors. Deleted sentences were not used in the subsequent
analysis.
It was also possible to use a separate free text input field
to record requests for content for the EM or problems
with the EmOntoTag user interface. We used this as the
means to gather information about possible extensions to
the EM.
Obtaining permission to run the experiment
We obtained permission from the presenters before
including their presentations in the experiment. Presen-
ters were contacted individually by email in order to
request permission to include their presentation in the
experiment, and their permission was sent by reply email.
Only the scientific presentations were included in the
experiment, together with the two invited keynote talks.
The response was overwhelmingly positive; from the 26
papers and 2 keynote talks: all but one paper presen-
ter gave their permission for inclusion in the experiment.
Audience participation was anonymous, with no realistic
way of tracing alphanumeric login codes to any individ-
ual. Actual participation in recording emotional response
was voluntary. All data were stored securely. We will not
report here on which responses were made for which par-
ticular talks, with the exception of a selected talk for which
we obtained specific permission.
Experiment execution and data analysis
We enabled the EmOntoTag software on the weekend
before the conference was due to start, and announced
and explained the experiment during the opening ses-
sion of the conference on the morning of July 23rd 2012.
While the tool is still available online to enable inter-
ested parties to examine the interface, the cutoff date for
responsses which we included in the experiment was set at
27th July 2012, i.e. 2 days after the conference closed. This
excluded two extremely late tags, but allowed for slow
responses.
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 5 of 17
http://www.jbiomedsem.com/content/5/1/38
Table 1 Emotion Ontology vocabulary used in the
experiment
Emotion Ontology vocabulary
Emotion Subjective feeling Appraisal
Prefix: I feel Prefix: I feel Prefix: I think
Surprised Out of control This is not expected
Happy Good I am being treated justly
Mastery At ease This is not predictable
pleasure
Passionately In control This is not being deliberately
loving caused
Sensory Exhausted This is dangerous
pleasure
Disgusted Energetic A response is needed urgently
Grieving Tired This is being deliberately caused
Furious Restless This is not dangerous
Amused Weak This is expected
Despairing Bad I am not at the centre of attention
Jealous Strong There are consequences and they
are unavoidable
Embarrassed Nervous I am being treated unjustly
Serene Calm This is not familiar
Terrified Alert This is not important for my goals
Irritated This is familiar
Proud This is being caused by chance
Interested This is pleasant
Sad I have irrevocably lost something
important
Elated This is against my ideals
Loving This is predictable
Stressed This is being caused by me
Sexual
pleasure
This has undesirable consequences
Aesthetic
pleasure
This is in line with my ideals
Compassionate This is unpleasant
Euphoric There are consequences but they
are avoidable
Social
pleasure
This is being caused supernaturally
Anxious This is important for my goals
Enraged This has desirable consequences
Bored This is not sudden
Contemptuous This is being caused by someone
else
Pleasure A response is needed but not
urgently
Ashamed This is sudden
Panicked I am at the center of attention
Table 1 Emotion Ontology vocabulary used in the
experiment (Continued)
Hateful
Angry
Contented
Disappointed
Guilty
Joyful
Afraid
Compassionately loving
The table gives a listing of the vocabulary drawn from the Emotion Ontology
that was provided to users of the EmOntoTag tool during the conference.
The dataset was analysed using the R statistical analy-
sis package and Matlab. We aggregated all the data into a
data table indexed by anonymous user ID, ontology term
ID, time of response, talk ID, strength of response, and
appropriateness of the EMs content. Furthermore, the
ontology terms were grouped by their valence into three
categories  positive, negative and neutral.
To test the hypothesis that emotional response can be
partitioned into those for talks and those for the audience,
we took these raw data and created two tables:
1. One capturing users by the EM terms they had used;
2. One capturing talks by how they were described.
These tables were then normalised (so that the sum of
entries was one) to allow for the variation in number of
EM terms used to describe talks and in the numbers of
talks to which different users had responded.
These two tables were then analysed using a princi-
pal components strategy to determine which linear com-
binations of terms described the greatest variation in
responses of people and EM terms. This gave us a set
of eigenvalues and eigenvectors which could be used to
describe the data.
Results
Raw data are not provided in order to protect confidentiality.
Number of respondants and EM terms used
The total number of EM terms captured in the experiment
was 553, spread across the 27 presentations that agreed
to participate in the study (25 paper presentations and 2
keynotes). Of these, all 27 had at least 4 EM terms cap-
tured in the experiment, and the largest number of terms
captured against one talk was 67. There were 35 distinct
users from the 80 registered conference attenders (44%)
who captured EM terms during the experiment. Of these,
the range of numbers of responses was large, with the
most active user providing 78 terms and the least active
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 6 of 17
http://www.jbiomedsem.com/content/5/1/38
Figure 1 Screenshot of EmOntoTag facility for capturing emotional experience.
user providing one term. The number of responses per
user is shown in Figure 2.
The full set of counts for users and usages per term type
is given in Table 2.
Ease of articulating emotional response
The rating of how well the EM vocabulary allowed the
user to articulate their emotional response had a mean of
3.42, with standard deviation 1.12. This significantly dif-
fered from the median of 3; i.e. the users reported, on
average, that the vocabulary did allow them to capture
their emotions well
(
t = 8.7324, df = 552, p < 2.2?16).
Our result can further be decomposed by grouping
the responses per ontology term type. There were three
different types of ontology term used in this experi-
ment: appraisal (thoughts), subjective feeling and emo-
tion. Of these, in fact, the emotions have the highest
mean and the highest significance, while the thoughts
category, with a mean of 3.14, was not significantly dif-
ferent from the median of 3 (Thoughts: mean = 3.14,
t = 1.6859, df = 187, p = 0.09348; Feelings: mean= 3.37,
t = 2.7126, df = 66, p = 0.008505; Emotions: mean =
3.60, t = 9.7564, df = 297, p < 2.2?16).
Strength of response
The mean of the strength of response is 3.07, standard
deviation 0.92. In contrast to the appropriateness of an
EM term, the mean strength of response does not dif-
fer significantly from the median of 3 (two-tailed t =
1.8007, df = 552, p = 0.0723).
Valence of responses
The ontology terms were separated into three categories
according to their valences: positive, negative, and neu-
tral. Neutral was used for emotions such as surprisewhich
are known to have either positive or negative valence, and
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 7 of 17
http://www.jbiomedsem.com/content/5/1/38
Figure 2 The counts of responses per participating user.
for many of the appraisal categories in which the same
applied.
Based on this division, we found that the majority of
responses were positive (300 positive, 139 negative, 114
neutral). Figure 3 shows the counts of terms used per
ontology type and per valence.
Positive responses were also found to have been rated
as stronger, i.e., having a greater strength of response
rating. Indeed, while no significant effect was detected
for the overall rating of strength of response and also
not for the neutral or negative response groups, the
positive responses were significantly stronger than the
mean (mean of strength of response (neutral): 2.991228;
mean (positive): 3.186667 (t = 3.6273, df = 299, p =
0.0003366); mean (negative): 2.884892).
The positive responses also obtained a slightly bet-
ter appropriateness score than the negative or neutral
responses (mean of appropriateness (neutral): 3.210526;
mean (positive): 3.386667; mean (negative): 2.654676).
The negative mean is significantly different to the posi-
tive mean: t = 2.3365, df = 253.678, p = 0.02024, while
the positive mean is not significantly different to that for
neutral responses (p = 0.1734). The negative mean is
also significantly different to that for neutral responses:
t = 2.9837, df = 235.493, p = 0.003147.
Usage of ontology terms
Of the total of 89 ontology terms that were included in the
study, 67 were actually used. The most commonly used
term was interested, with 86 occurrences. The distribu-
tion of counts per ontology term is shown in Figure 4.
The number of EM terms used per talk varies, with
the highest being 67 and the lowest 4. There was a spike
in responses during the first talk (58) and thereafter
fewer in general for subsequent talks, with spikes at the
two keynotes and a resurgence in the last day. The two
keynotes gave sharp increases in the number of EM terms
captured relative to the remainder of the talks (67 and 30),
which also makes sense given that the time the speakers
were talking was much longer. The second day had the
lowest number of responses, with a bit of a revival on the
last day for the last three talks.
The mean number of EM terms used per talk was 20.48,
median 17 and standard deviation 15.48.
Can talks and audience members be distinguished by EM
terms?
An analysis of the eigenvalues (scree plots) showed that
the data could not be readily embedded in a low dimen-
sional space. For both data sets, the first 5 eigenvectors
combined only captured 75% of the variation in the data.
This is unsurprising in a domain with such inherent com-
plexity as an academic audiences emotional response
to a series of research presentations; in contrast, an
embedding into a low-dimensional space would have been
surprising.
However, looking at the EM terms that dominated the
first five eigenvectors in these two sets shows that there
were differences between talks and the audience. The
ordering of key emotional terms needed to account for the
varience in the talks and the audience are shown in Table 3
in descending order of strength (including the counts of
usage).
Free text comments
The free text comments yielded several suggestions of
terms that were missing from the ontology at the time of
the experiment. These were:
1. Curious (requested twice)
2. Concerned
3. Dubious
4. Worried
5. Confused (requested four times)
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 8 of 17
http://www.jbiomedsem.com/content/5/1/38
Table 2 Number of users and tags
Term id Term label Number of tags Number of users Valence Type
33 Interested 86 28 Positive Emotion
42 Happy 30 18 Positive Emotion
166 Bored 26 13 Negative Emotion
68 This is expected 25 12 Neutral Thought
44 Amused 22 12 Positive Emotion
64 This is familiar 20 13 Neutral Thought
73 This is important for my goals 19 12 Positive Thought
12 Annoyed 19 8 Negative Emotion
70 This is pleasant 17 13 Positive Thought
95 This is in line with my ideals 17 11 Positive Thought
86 This has desirable consequences 16 13 Positive Thought
11 Irritated 16 7 Negative Emotion
80 Tired 14 8 Negative Feeling
111 Restless 13 8 Negative Feeling
37 Mastery pleasure 11 3 Positive Emotion
47 Contented 10 6 Positive Emotion
114 Calm 9 6 Neutral Feeling
41 Proud 9 4 Positive Emotion
66 This is predictable 8 7 Neutral Thought
51 Disappointed 8 6 Negative Emotion
84 This is not being deliberately caused 8 4 Neutral Thought
74 This is not important for my goals 7 7 Neutral Thought
65 This is not familiar 7 6 Neutral Thought
32 Surprised 7 5 Neutral Emotion
43 Serene 7 4 Positive Emotion
124 Nervous 6 6 Negative Feeling
79 Good 6 6 Positive Feeling
71 This is unpleasant 6 6 Negative Thought
35 Pleasure 6 5 Positive Emotion
101 I am being treated justly 6 4 Positive Thought
34 Joyful 6 4 Positive Emotion
121 Alert 6 3 Positive Feeling
30 Despairing 5 5 Negative Emotion
109 Energetic 5 5 Positive Feeling
67 This is not predictable 5 4 Neutral Thought
92 There are consequences but they are avoidable 4 4 Neutral Thought
96 This is against my ideals 4 4 Negative Thought
107 At ease 4 4 Positive Feeling
52 Compassionate 4 4 Positive Emotion
56 Sad 4 3 Negative Emotion
46 Euphoric 4 3 Positive Emotion
83 This is being deliberately caused 4 2 Neutral Thought
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 9 of 17
http://www.jbiomedsem.com/content/5/1/38
Table 2 Number of users and tags (Continued)
39 Aesthetic pleasure 3 3 Positive Emotion
87 This has undesirable consequences 3 2 Negative Thought
69 This is not expected 3 2 Neutral Thought
36 Sensory pleasure 3 1 Positive Emotion
40 Sexual pleasure 2 2 Positive Emotion
28 Anxious 2 2 Negative Emotion
54 Embarrassed 2 2 Negative Emotion
62 This is sudden 2 2 Neutral Thought
19 Disgusted 2 2 Negative Emotion
76 This is being caused by me 1 1 Neutral Thought
81 This is being caused supernaturally 1 1 Neutral Thought
93 There are consequences and they are unavoidable 1 1 Neutral Thought
102 I am being treated unjustly 1 1 Negative Thought
110 In control 1 1 Positive Feeling
116 Out of control 1 1 Negative Feeling
13 Furious 1 1 Negative Emotion
50 Passionately loving 1 1 Positive Emotion
55 Ashamed 1 1 Negative Emotion
90 A response is needed but not urgently 1 1 Neutral Thought
112 Exhausted 1 1 Negative Feeling
26 Afraid 1 1 Negative Emotion
78 This is being caused by someone else 1 1 Neutral Thought
89 A response is needed urgently 1 1 Neutral Thought
106 I have irrevocably lost something important 1 1 Negative Thought
119 Weak 1 1 Negative Feeling
9 Angry 1 1 Negative Emotion
Results table with count of usages per term, with distinct users, valence and type. The table gives the numbers of users and tags for each of the tag types that was
used by participants in the experiment.
6. Distracted or unfocused
7. Indifferent, emotionally neutral or feeling nothing
(requested three times)
8. Expectant or anticipative
9. Hopeful
10. Inspired (requested twice)
11. Intrigued
12. Schadenfreude.
As a concrete outcome of this experiment, almost all
of these missing emotional terms have been added to the
ontology. The exceptions are distracted or unfocused,
which was deemed not to be an emotion term per se but
rather having relevance to attention, which will be cov-
ered in the context of the broader Mental Functioning
ontology project [24], and emotionally neutral or feel-
ing nothing, which again was not considered to be an
emotion but rather the absence of an emotion. The latter
case, emotionally neutral, should however be added as an
option provided by the user interface in subsequent ver-
sions of the EmOntoTag tool. Additionally, amused was
requested, despite this term actually being available in the
list of options.
Some suggestions were received via the free text com-
ment facility for alternative phrasing for certain of the
listed emotions and feelings, for example why cant I just
say pleased? and feeling of mastery better than mastery
pleasure. These suggestions have been incorporated into
the ontology by updating the tag display synonyms for
the relevant terms.
A small number of comments related to the usabil-
ity of the EmOntoTag tool employed in the experiment,
specifically: Sometimes I get the red warning, This field
is required, sometimes not, for what is apparently the
same behaviour, and Why did you not list the emotions
alphabetically?. The choice of unsorted presentation of
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 10 of 17
http://www.jbiomedsem.com/content/5/1/38
Figure 3 Counts of response by valence and by ontology term.
selection options was done to avoid bias, though it does
have an obvious usability penalty. By verbal communica-
tion, another comment that we received as feedback on
the usability of the tool was that it was not optimised for
smart phone and other smaller screens. These enhance-
ments will be incorporated into subsequent versions of the
tool.
Finally, several comments requested that the appraisal
or thought list was not specific enough because it did not
allow the specification of the actual cause of the emotion
in question. The appraisal list included generic appraisal
components such as I think that there will be consequences
or I think that this is being supernaturally caused. Partici-
pants, on the other hand, used the free text to request the
ability to express the specific cause of their emotion, for
example I feel bored because I have heard this all before, I
was surprised that what I thought was an important aspect
of the topic was missing from the talk, and I was afraid
he would run out of time. Those causes that are reflected
in the vocabulary of the EM are derived from those that
have been found to be fairly generic (i.e. applicable across
multiple scenarios) in the cross-cultural GRID project that
investigated the meaning of emotion terms [27]. Clearly,
the EM cannot include a vocabulary for all the possi-
ble scenarios and objects that can cause an emotion. The
accurate description of the objects of the emotion and
the way that these objects are intricately linked to the
type of the emotion will be the subject of future Emotion
Ontology development.
Exploring the emotional response to one talk
We were given permission by one of the presenters to
reveal the results of the audiences emotional response
to his/her talk. A bar chart summarising the responses
Figure 4 Counts of tags for each ontology term used (non-zero occurrence).
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 11 of 17
http://www.jbiomedsem.com/content/5/1/38
Table 3 Terms that most describe users compared with terms that most describe talks
Talks partition Audience partition
Term Type Valence Id Count Term Type Valence Id Count
Interested Emotion Positive 33 86 This is pleasant Thought Positive 70 17
Restless Feeling Negative 111 13 Happy Emotion Positive 42 29
Bored Emotion Negative 166 26 This is familiar Thought Neutral 64 20
Annoyed Emotion Negative 12 19 Interested Emotion Positive 33 86
Happy Emotion Positive 42 29 This is expected Thought Neutral 68 24
Amused Emotion Positive 44 22
Eigenvectors of terms that most describe the audience compared with terms that most describe talks; ordered in descending strength. The table gives the terms that
best describe users and the terms that best describe talks.
to the talk can be seen in Figure 5. Fifteen participants
responded; the highest number of responses was 19 (par-
ticipant 125) and the lowest was 1 response (partici-
pant 79, who displayed mastery pleasurethe feeling of
mastery of the subject). Figure 6 shows the spread of
emotional responses during the talk and just after the talk.
Discussion
Whilst we may have the emotional terms used by partici-
pants, we do not know the motivation for the articulation
of that emotion. So, the discussion that follows is some-
what speculative. Also, tying reporting events to events in
the talks themselves risks identification of the talks, so this
discussion is limited to generalities.
The main part of our null hypothesis that the EM will
not enable audience members to articulate their emo-
tional response. . . can be rejected. This is given sup-
port by the scores for the rating of how well the given
vocabulary sentences captured the emotion the audience
member wanted to express; these rating scores were sig-
nificantly higher than the median value. This alone indi-
cates that the EM is sufficient to allow emotions to be
articulated.
The following terms from the EM vocabulary
interested, happy, amused, this is familiar, this is
expected, bored, (all with a count greater than or equal
to 20) are terms most responders have used for most
talks. These are emotional responses one would expect
Figure 5 The EM terms used to articulate the emotional response to one of the ICBO 2012 talks; y-axis are the terms used and x-axis is the
number of times each term was used.
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 12 of 17
http://www.jbiomedsem.com/content/5/1/38
Figure 6 A time-line of emotional responses to the sample talk; the EM terms are put into bins and displayed as tags, the size of which is
proportional to the number of times the tag was used. The general area of the EM is indicated by colour: appraisals in blue, emotions in red and
feelings in green. Valence is indicated by shading  darker for negative and lighter for positive. Exact times and durations are obscured to avoid the
talk being identified.
to dominate in an academic conference, with an audi-
ence interested and sometimes bored, with much that is
familiar or expected, with a good deal of happiness and
amusement thrown into the mix.
The principle components strategy was used in the sec-
ond part of our hypothesis, that the responses using the
EM would be sufficient to cluster emotional responses
about talks and the audience. The PCA determined which
linear combinations of tags described the greatest varia-
tion in responses about the audience and talks, and gave us
a set of eigenvalues and eigenvectors which could be used
to describe the data. Table 3 (above) shows six EM terms
that partition to talks and five EM terms that partition to
the audience. The key emotional terms (identifiers shown
in parentheses) needed to describe the talks were: inter-
ested (33), restless (111), bored (166), annoyed (12),
happy (42) and amused (44).
Whereas those that best described the audience were:
this is pleasant (70), happy (42), this is familiar (64),
interested (33) and this is expected (68).
As the order matters, terms this is pleasant (70) and
happy (42) are the EM terms that strongly describe the
audience, while the terms interested (33) and restless
(111) were important for distinguishing the talks.
The six talk terms are five emotions and one feel-
ing; with three positive valence emotions (interested,
amused and happy) and two negative emotions (bored
and annoyed) and one negative valence feeling (restless).
The five EM terms for the audience (two emotions
and three thoughts) were all either positive or neu-
tral; two positive emotions (interested and happy) and
one positive feeling (this is pleasant), with two of the
thoughts being neutral (this is familiar and this is
expected).
Two of the EM emotions (interested and happy) par-
tition to both talks and audience (with interested being
strongly in the talks partition) suggesting that overall
the talks and audience provoke or cause happiness and
interest. The audiences emotional responses are either
positive or neutral and the EMs feelings are associated
with the people articulating the emotions. Much of the
ICBO content may well be either expected or familiar
in some form to the audience; this is to be expected at
most conferences  this is the kind of thing that would
be expected from X. The general positive response of the
audience wil be discussed further below.
The talks partition is, perhaps, more interesting; here
some negative terms appear as well as interested and
happy  we have annoyed, bored and restless. That
any negative terms appearing are associated with the talks,
rather than the audience, is reassuring; it is the talks that
cause annoyance, boredom and restlessness  while it is
the audience that feel that the talk is familiar, is expected
and is pleasant.
That terms partition sensibly between talks and audi-
ence, with each having high counts, together with the
partition being readily explicable leads us to believe that
the EM has the ability to enable the articulation of an
emotional response (at least in this situation).
Our PCA analysis shows that the EM is sufficient to
allow discrimination between audience members emo-
tional responses to a conference talk. We can see emo-
tional tags that are associated more strongly with the
audience and emotional tags that are associatedmore with
the talk itself. Overall, as the EM terms partitioningmakes
sense in the context of an academic conference, it indi-
cates that the EM is competent to support conference
attenders to articulate their emotional response to a talk
and thus further supports our hypothesis.
The commonest tags used were interested, used 86
times by all users at some point in the conference, and
happy, used 29 times by 17 participants in 18 talks,
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 13 of 17
http://www.jbiomedsem.com/content/5/1/38
presumably reflecting a general level of contentment with
the conferences material (contented was used 10 times
by 6 participants in 5 talks; other tags of this kind can be
seen in Table 2). Interested is the dominant emotional
response from the audience, which may well be expected
in a conference about ontologies, which an audience of
ontologists has chosen to attend.
Bored was used 26 times by 13 users in 14 talks; a
related set of tags, angry, was used once; annoyed was
used 19 times by 8 participants in 13 talks; furious was
used once; irritated was used 16 times by 7 participants
in 10 talks. The tags are related in the context of the com-
munity itself: irritation, fury, annoyance, and anger may
be felt by those with entrenched opinion in opposition to
those of the speaker  or about bad science, though the
two may not be unrelated in the minds of the participants.
Members of the biomedical community will be familiar
with the divisions that exist within the community on fun-
damentals of ontologies [3,11] and these divisions might
have been reflected in the participants responses. In addi-
tion, a conference in which all talks are of an equally high
quality and are equally highly appreciated will be rare or
non-existent.
This is expected was used 24 times by 11 participants
in 13 talks. The straight-forward interpretation is that the
participants in question were hearing what they expected
from the speaker in question, either positively or nega-
tively. On the positive interpretation, this tag could be
grouped with the feeling of pride (if this is expressed as,
for instance, a result of ones work or oneself being men-
tioned in a positive light). Amusement was a response
articulated by 12 participants, 22 times in 11 talks.
The most straight-forward interpretation of this is that
talks contained an element of humour and the audience
responded to this humour. Participants may have articu-
lated amusement as schadenfreude (an emotion that was
requested as an addition to the EM). However, taking the
most straight-forward interpretation of this tag, we can
observe that ICBO had a reasonable amount of humour in
its talks.
Some of the tags not used were compassionately loving,
contemptuous, guilty, terrified, I am at the centre of
attention. It is posssible to conceive of ways in which
these unused tags could have been used  a person sin-
gled out in a talk may feel to be the centre of attention,
or being guilty of an ontological crime highlighted by
a speaker. Others, such as being contemptuous are per-
haps not required in this context when being angry is
available, despite the obvious differences. The tag sex-
ual pleasure was used once each by two participants. If
a true reflection of response to either speaker or topic
it may be disturbing, but it may also have been used in
jestdespite this, the EM still enabled the emotion to be
articulated.
There is a strong tendency towards EM terms with a
positive valence being used. There are at least four posible
factors involved:
1. The anonymity of reporting should allow negative
as well as positive emotional responses to be
reported;
2. Factions within the biomedical ontology community
and variability in the quality of the presented work
should mean there are negative emotional responses
to talks;
3. Basic ideas of reduction in cognitive dissonance [29]
may incline reported emotions to be positive; that is,
audience members need to justify to themselves their
presence at the conference  an individual giving a
broadly negative emotional response would suggest
he or she had attended the wrong conference.
Similarly, acquiescence bias [30] leads to individuals
tending to respond yes or positively to questions or
situations.
4. The ICBO audience is self-selecting and will be
pre-disposed to liking talks about biomedical
ontologies; so, in spite of factionalism in the
community, most people will be emotionally
positive most of the time.
Points three and four seem to have out-weighed points
one and two. In addition, point one may not have been
strong enough to overcome the need to reduce cognitive
dissonance (point three).We can speculate that there were
more negative emotional responses than were reported
and the reduction in cognitive dissonance works partic-
ularly well at the level of reporting. However, from the
reported evidence, the emotional response to ICBO talks
is overwhelmingly positive.
We described the emotional response to one talk in
detail. Linking responses to times or events in the fea-
tured talk may break confidentiality, so the description
below is only at the most general level. Participant 125
gave many responses (in time order): joyful, this is in
line with my ideals, contented, this is in line with my
ideals, pleasure, good, this is important for my goals,
this is pleasant, contented, this is expected, amused,
contented, tired, proud, I am being treated justly, I
am being treated justly, euphoric, this is in line with
my ideals and contented. From this we may infer that
he or she found the talk in line with their thinking and
we could speculate that the participant was mentioned or
his/her work was mentioned. Participants 23, 40 and 123
also expressed a similar emotional profile. For example,
participant 40 was interested, amused, energetic, and
thought this is pleasant.
Amused, I feel contented, good and interested are
among the most frequent emotional responses to this talk,
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 14 of 17
http://www.jbiomedsem.com/content/5/1/38
which, assuming these 15 responses are indicative of the
audience as a whole, means this talk was well received
emotionally (though, as discussed, one may suspect that
negative responses are less likely to be expressed). There
were eight amused responses from eight participants and
seven of these were spread throughout the talk, suggest-
ing an even level of amusement; one of the participants
being amused two hours after the talk, suggesting either
sustained amusement or a tardy response.
In contrast, Participant 6 had a different profile and
was bored, bored, tired, restless and angry during the
course of this talk. The response to this talk was gener-
ally positive emotionally, but the ability to discriminate
between participants (as shown in the earlier analysis) is
exhibited.
Related work
Emotion self-reporting
Mood or emotion monitoring via questionnaires and self-
reporting has been used in mental healthcare contexts,
and more recently mobile phones have been adopted to
serve that purpose [15]. Morris et al. [31] describe a
mobile phone application developed to allow the self-
monitoring of emotional state. The application prompted
users to self-report their emotions several times a day,
giving them a scale on which they could set either a sin-
gle dimensional rating with different emotion types or
a multidimensional Mood Map rating which allowed
users to select a point on a valence vs. arousal graph.
The single dimensional scales were offered for the emo-
tion types happiness, sadness, anxiety, and anger. These
emotion types are all present in the EM ontology. Com-
pared to our tool, their tool offered a reduced number
of distinct emotion types, with easier usability (using a
touch interface). They coupled this experience sampling
application with a mobile therapy utility that offered cog-
nitive behavioural therapy via questions and suggested
thought exercises designed to improve the well-being of
the user through altering their reaction to common stres-
sors. Reid et al. [32] developed a mobile phone appli-
cation for self-reporting that offers a questionnaire to
users on their mood, experiences and level of stress at
times throughout the day. The application prompts users
to collect data on mood and stress levels at four ran-
dom times per day, and allows notes to be captured
about locations and activities, correlated with usage of
substances such as alcohol and cannabis. Mood capture
used Likert scales in which adjectives indicating increas-
ing degrees of the relevant mood were displayed on the
phone screen rather than numbers. The focus was on neg-
ative moods, offering scales for angry, sad, tired, stressed,
and anxious moods. Mood or emotion tagging smart-
phone applications that are commercially available and
may be recommended for self-monitoring in cases of
bipolar disorder include MoodTrak [33] and the T2 mood
tracker [34]. MoodTrak allows free-text description of the
present emotion being experienced coupled with a star
rating (15) that ranks the mood from positive to neg-
ative. Tracking of moods is done online and a graphing
facility shows a history. However, no private option is
available, raising difficulties for confidential or sensitive
usage scenarios. The use of free text to capture the name
of the emotion being experienced hinders subsequent har-
monization for research purposes of heterogeneous data
arising from different users. The T2 mood tracker offers
variable scales along which a rating can be selected. Pre-
loaded scales include anxiety, stress, depression, brain
injury, and general well-being. However, the scales are
customizable.
It is our belief that an ontology such as the EM could
benefit such applications as these by providing agreed-on
standard categories for emotion self-reporting, localiz-
ing the vocabulary management function (which includes
translation management) in one central community-
agreed facility. However, many of the applications cur-
rently used for emotion capture allow only a very
restricted vocabulary of emotion types, thus not captur-
ing the broad range of different types of emotion that can
be experienced and reported on, but also not requiring
much by way of vocabulary management. Others use free
text to enable the widest range of emotion types to be
reported, but this approach sacrifices the facility for later
data aggregation across different users and even differ-
ent tools and may hinder subsequent interpretation and
analysis.
Ontology evaluation
Tartir et al., 2010 [12] distinguish several broad tech-
nical approaches to evaluation of ontologies, including
logic-based approaches that use the knowledge encoded
in axioms in the ontology to check for unsatisfiable
classes, and feature-based approaches that rely on metrics
about the content of the ontology, such as the percent-
age of classes that lack textual definitions. According to
Brank et al. [1], ontology evaluation approaches can be
divided into 1) those that compare the ontology to a gold
standard, 2) those that compare the ontology to a source
of data about the domain being modeled in the ontol-
ogy, 3) those which involve human assessment according
to a predefined set of criteria, and 4) those which involve
the use of the ontology in a given application together
with an evaluation of the results. Our approach follows
the fourth strategy in that grouping, namely we have
implemented an application that makes use of the ontol-
ogy and built into the application the infrastructure to
evaluate the ontology in use for that application. Simi-
larly, [13] use a task-based approach to evaluate the Gene
Ontology in use for the enrichment analysis of gene sets
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 15 of 17
http://www.jbiomedsem.com/content/5/1/38
resulting from microarray experiments. As highlighted
in [35],
Since users of ontologies will benefit from something
that ontologies can do, research in applied ontology
has to be measured based on how well ontologies do
their tasks.
Our evaluation of the EM is notable as a rare example
of a designed study on an ontologys fitness for purpose.
Most evaluations of biomedical ontologies tend to fall into
the first and third groups above, if they are evaluated at
all, and thus the work presented here is a contribution into
the field of evaluation of biomedical ontologies.
Limitations
In [1], several limitations of the ontology-evaluation-in-
application-use paradigm are raised. Firstly, they point out
that the evaluation of an ontology in a particular appli-
cation can yield a result that only has scope for similar
tasks. That is, in the context of the EM, our evaluation in
use for self-reporting of emotional experiences in a con-
ference can only inform the applicability of the ontology
for self-reporting tasks (at academic conferences) and not
for other types of use to which the ontology may be put.
This is a fair point, and one that we are happy to concede
as a limitation of the present study.
Secondly, they raise the concern that the ontology con-
tribution to the overall application might be minimal
compared to the remainder of the implementation, and
that it can be difficult to separate the contribution of
the ontology alone from the other aspects of the appli-
cation. While our application design tried to minimise
the effect of all but the most direct aspects of the appli-
cation aside from the available vocabularies, we agree
that there nevertheless might have been some impact on
our results due to non-ontology-related aspects of the
application. For example, the order in which words were
presented in the selection boxes might have had some
influence on the selected results and on the experienced
ease of use. To control for these effects, however, we
explicitly asked users to rate how easy it was for them
to express their feelings using the vocabulary provided,
rather than how easy the application as a whole was
to use.
They further report that this paradigm cannot easily
be used to compare different ontologies unless a single
application can be reused with different pluggable ontolo-
gies. It was not our objective in the present study to
compare different ontologies in the emotional domain,
but we do believe that the application we have designed
would be able to accommodate different sources of vocab-
ulary should such an experiment be conducted in the
future.
One clear limitation of our study is that only the vocab-
ulary offered in the ontology has been evaluated, rather
than the logical or hierarchical structure of the EM. Our
findings thus only relate to the vocabulary component of
the ontology, and a separate evaluation would be needed
for the other aspects of the ontology. However, the study
does show that the EM largely contained the vocabulary
necessary for the participants to articulate their emotions.
Another limitation is that the study only evaluates the
users self-assessed reports of their emotions, that is, the
study makes no attempt to calibrate the reporting of
emotions that the users provided with any objective psy-
chometric evaluation of the emotions they were actually
undergoing at that time. Our findings are thus only rele-
vant to the self-reporting of self-assessed emotions, and
not to the objective measuring of emotions as might
be required in clinical settings. As the evaluation was
intended to reveal whether the EM was sufficient to allow
participants to self-report their emotional response, as
opposed to revealing the true state of emotions at the
ICBO 2012 conference, we do not see this as a significant
issue in this evaluation.
A further limitation of the environment in which our
study was conducted was that internet difficulties and
power failures might have prevented some participants
from recording their emotion effectively and the associ-
ated rating at the time that they would have liked. We
did allow for post-hoc capture of tags to get around
this problem, and we did see a spike of captures late at
night (around 11pm) that was probably explained by this
phenomenon. We did not optimise the appplication for
use on mobile devices; this could have eased use and
increased the number of users. Also, we were asking a
lot of the conference attenders  many responses for
many talks.
Finally, the emotional words from the EM that were
used in the experiment themselves had implicit strengths
which were not exposed in the analysis or correlated with
the stated strength of response. This information was not
available as an annotation in the EM ontology; however, it
may be added in a future release.
Conclusion
We find that the vocabulary provided by the leaves of the
Emotion Ontology is suitable for use for the self-assessed
self-reporting of emotional experiences in a conference
setting. We evaluated the EM in the wild in this set-
ting and found that the EM can be used to discriminate
between and articulate emotional experiences of audi-
ence members. We have released the EmOntoTag tool
as open source for community adoption, adaptation and
reuse in similar scenarios in future projects. We under-
stand this experiment as a contribution to the ontology
evaluation domain from a perspective of use-case driven
Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 16 of 17
http://www.jbiomedsem.com/content/5/1/38
evaluation, and we have been able to enhance the ontol-
ogy based on the feedback and comments we received
during the course of this experiment. Ontology evaluation
is recognised to be a hard task that is not often per-
formed and this work is a contribution of a formal exper-
iment designed to evaluate the ability of an ontologys
vocabulary to make the distinctions necessary in a field
of interest.
Future work will involve further adapting the ontology
to allow more comprehensive descriptions of the con-
text and causes for a particular emotional experience, and
evaluating the ontology for use in the self-reporting of
emotions in more clinical contexts, e.g. to facilitate emo-
tional monitoring in the treatment of patients with mood
disorders. There are a broad range of application sce-
narios in which the self-reporting of emotions might be
relevant  almost any situation that has human involve-
ment  and the EM is a candidate vocabulary for such
applications.
Additional file
Additional file 1: Sample user handout for ICBO conference
participants. Conference participants were given a handout together with
their conference pack that detailed the experiment that we were
conducting and assigned each user a different, unique, random access
code for the system. An example of these handouts is included as a
supplementary file.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
RS conceived and supervised the project, and negotiated permission for the
experiment to be conducted during the ICBO conference. CC developed the
tool that was used in the experiment. AB, RS and JH analysed the results. CJ
helped with the interpretation of results. JH provided guidance on the UI
development of the tool used, and drafted the initial version of the
manuscript. All authors contributed to, and have read and approved, the final
version of the paper.
Acknowledgements
We thank the ICBO scientific organisers, in particular Stefan Schulz and Ronald
Cornet, and all participating ICBO presenters for agreeing to allow the
experiment to be conducted during the conference. We are grateful to all
participants who recorded their emotions and rated the ease of using the
ontology for this task during the conference. We thank Aitor Apaolaza Llorente
for assistance in creating one of the Figures in the manuscript. Furthermore,
we acknowledge support from EPSRC grant EP/C536444/1 and EPSRC grant
EP/J014176/1.
Author details
1Cheminformatics and Metabolism, EMBL  European Bioinformatics Institute,
Wellcome Trust Genome Campus, Hinxton CB10 1SD, UK. 2Swiss Center for
Affective Sciences, University of Geneva, Geneva, Switzerland. 3School of
Computer Science, University of Manchester, Oxford Road, Manchester M13
9PL, UK. 4Faculty of Life Sciences, University of Manchester, Oxford Road,
Manchester M13 9PL, UK.
Received: 10 December 2013 Accepted: 14 August 2014
Published: 3 September 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Torii et al. Journal of Biomedical Semantics 2014, 5:3
http://www.jbiomedsem.com/content/5/1/3RESEARCH Open AccessDetecting concept mentions in biomedical text
using hidden Markov model: multiple concept
types at once or one at a time?
Manabu Torii1*, Kavishwar Wagholikar2 and Hongfang Liu2Abstract
Background: Identifying phrases that refer to particular concept types is a critical step in extracting information
from documents. Provided with annotated documents as training data, supervised machine learning can automate
this process. When building a machine learning model for this task, the model may be built to detect all types
simultaneously (all-types-at-once) or it may be built for one or a few selected types at a time (one-type- or
a-few-types-at-a-time). It is of interest to investigate which strategy yields better detection performance.
Results: Hidden Markov models using the different strategies were evaluated on a clinical corpus annotated with
three concept types (i2b2/VA corpus) and a biology literature corpus annotated with five concept types (JNLPBA
corpus). Ten-fold cross-validation tests were conducted and the experimental results showed that models trained
for multiple concept types consistently yielded better performance than those trained for a single concept type.
F-scores observed for the former strategies were higher than those observed for the latter by 0.9 to 2.6% on the
i2b2/VA corpus and 1.4 to 10.1% on the JNLPBA corpus, depending on the target concept types. Improved
boundary detection and reduced type confusion were observed for the all-types-at-once strategy.
Conclusions: The current results suggest that detection of concept phrases could be improved by simultaneously
tackling multiple concept types. This also suggests that we should annotate multiple concept types in developing a
new corpus for machine learning models. Further investigation is expected to gain insights in the underlying
mechanism to achieve good performance when multiple concept types are considered.
Keywords: Natural language processing, Information storage and retrieval, Data mining, Electronic health recordsBackground
Concept mention detection is the task of identifying
phrases in documents that refer to particular concept
types. Provided with documents annotated with concept
phrases as training data, supervised machine learning
can be used to automate concept mention detection. In
the biological domain, sets of annotated documents have
been developed and made publicly available over the
years [1,2]. Similarly in the clinical domain, annotated
clinical notes have been recently released to the research
community through pioneering efforts [3,4]. These an-
notated data sets have promoted application of machine* Correspondence: torii@udel.edu
1Department of Radiology, Georgetown University Medical Center,
Washington, DC, USA
Full list of author information is available at the end of the article
© 2014 Torii et al.; licensee BioMed Central Ltd
Commons Attribution License (http://creativec
reproduction in any medium, provided the orlearning methods to concept mention detection in the
clinical domain [5-8].
When the detection task involves two or more target
concept types, there is an option to build one machine
learning model for all types (all-types-at-once strategy)
or to build multiple models each tackling one type (one-
type-at-a-time strategy). The former strategy may have
an advantage in exploiting dependency among concept
types. In this work, we posed a question if these strat-
egies have impacts on detection performance. We found
this question important in two ways. First, it is useful to
know if one strategy is better than the other in terms of
the detection performance. Second, when a new corpus
is developed, the results of the current study may en-
courage us to annotate additional concept types in order
to potentially enhance detection of the target concept
type. With current ongoing efforts on corpus development. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Table 1 Descriptive statistics of the corpora
i2b2/VA corpus JNLPBA corpus
Documents 349 2,000
Tokens 260,570 492,301
Concept phrases Problem 11,968 Protein 30,269
Test 7,369 DNA 9,530
Treatment 8,500 Cell Type 6,710
Cell Line 3,830
RNA 951
Torii et al. Journal of Biomedical Semantics 2014, 5:3 Page 2 of 6
http://www.jbiomedsem.com/content/5/1/3in the clinical domain, we believe this would be a timely
question to pose.
In this study, we used two kinds of annotated corpora.
The one is a clinical corpus released in the 2010 i2b2/
VA natural language processing (NLP) shared-task
challenge [4] and the other is a biological literature
corpus released in the Joint Workshop on Natural
Language Processing in Biomedicine and its Applica-
tions (JNLPBA) [9]. The two corpora are different in
terms of writing styles as well as concepts presented and
annotated, while they share challenges in identifying bio-
medical concepts, such as difficulty in detecting proper
names that may not have initial capital letters and in
processing ambiguous acronyms and abbreviations. The
best performing system in the i2b2/VA challenge and
that in the JNLPBA workshop achieved, respectively,
F-scores of 0.852 and 0.726 on the evaluation corpora.
These and the other top-ranked systems in the work-
shops used various machine learning methods, includ-
ing Hidden Markov Model (HMM), Support Vector
Machine (SVM), and Conditional Random Field (CRF),
along with various techniques and resources. Our inter-
est in this work is to compare all-type-at-once and one-
type- (or a-few-types-) at-a-time strategies, and not to
aim for the best performance on these corpora by ex-
ploring rich domain features. To focus on this goal, we
employed HMM that uses features internal to input text.
Methods
Experimental design
One strategy we considered in building a concept detec-
tion system was to train one machine learning model
that covered all concept types. An alternative strategy
tested was to build separate models for different concept
types. An HMM program implemented in the LingPipe
suite [10] was used to train these models. Detection per-
formance was measured with F-score, the harmonic
mean of precision (the number of correctly extracted
phrases divided by the number of all extracted phrases)
and recall (the number of correctly extracted phrases di-
vided by the number of all phrases to be extracted). We
conducted 10-fold cross-validation tests and calculated
the average F-score.
Data
Descriptive statistics of the two data sets used in our ex-
periments are shown in Table 1. The first data set used
was a training corpus in the 2010 i2b2/VA NLP shared-
task challenge [4]. This data set was made available
through our participation in the shared-task challenge
and, hence, no additional ethical approval was required
for the current study. This corpus consists of 349 clinical
documents, including 268 discharged summaries from
three institutions and 81 progress notes from oneinstitution. The documents were manually annotated
with three concept types: Problem, Test, and Treatment.
These annotations (spans of concept phrases) do not
overlap each other in text, except for eight annotations
that we excluded in the current study.
The second data set used was a training corpus of the
Bio-Entity Recognition Task in the JNLPBA workshop,
which was publicly available online. The corpus consists
of 2,000 abstracts of biology research articles retrieved
from the MEDLINE database using the search terms
(Medical Subject Headings) of human, blood cells and
transcription factors [9]. It is the same document set as
the GENIA version 3.02 corpus, but the thirty six con-
cept types originally annotated in the corpus were sim-
plified to five types for the shared-task workshop:
Protein, DNA, Cell Type, Cell Line, and RNA. There is
no overlap among annotated concept phrases in this
corpus.
Detection strategies
One or a few concept types at a time
In this strategy, independent detection tasks were as-
sumed for subsets of the target concept types. For each
subtask, the BIO notation was used [11]. Each token in
the corpus was assigned one of the labels, B_ConceptType,
I_ConceptType, and O, representing a token being the Be-
ginning of a concept phrase, Inside of a concept phrase, or
Outside of a concept phrase. For example, in order to in-
dicate Problem phrases in the i2b2/VA corpus, the three
labels, B_Problem, I_Problem, and O, were used.
All concept types at once
In this strategy, a single detection task was assumed for
all the target concept types. For example, given the three
concept types in the i2b2/VA corpus, one HMM model
was built using the seven labels, B_{Problem, Treatment,
Test}, I_{Problem, Test, Treatment}, and O.
Machine learning method
Concept mention detection was often tackled as a se-
quence labeling problem [4,9]. Input text is viewed as a
sequence of tokens and the task is defined as assignment
Torii et al. Journal of Biomedical Semantics 2014, 5:3 Page 3 of 6
http://www.jbiomedsem.com/content/5/1/3of each token with an appropriate label to demarcate
spans of tokens referring to target concept types. We
used a sequence labeling program, named CharLmRes-
coringChunker, from the LingPipe suite [10,12]. This
program was chosen because it exploits features internal
to text and the performance is not affected by additional
external resources and parameters associated with them.
Also, this program runs fast and it was desirable in con-
ducting cross-validation tests. A model trained with this
program first extracts candidate concept phrases using a
first-order Hidden Markov Model (HMM). In HMM,
the likelihood of a sequence of labels is calculated based
on the two types of probabilities, the transition probabil-
ities and the emission probabilities, learned from the
training data set. In the implementation of the LingPipe
suite, the emission probabilities that capture the relation
between observed words and corresponding labels are
calculated using character language models. Transition
probabilities that capture the ordering of labels assigned
to words are calculated using a bigram model. As for la-
bels to demarcate phrases, instead of using BIO labels
given as inputs to the program, enriched BMEWO+ rep-
resentation is used internally [13]. Namely, B of BIO is
divided into W (a token of a single-word concept) and B
(beginning of a multi-word concept), I into M and E
(Middle or End of a multi-word concept), and similarly
O into {B, M, E, W}_O, where {B, E, W}_O is further di-
vided based on the type of the neighboring concept.
Candidate concept phrases extracted by an HMM model
are rescored using another level of character language
models to identify the best candidates. We varied theTable 2 Comparison of detection performance
TP
i2b2/VA Problem All-at-once 964
One-at-a-time 932
Test All-at-once 582
One-at-a-time 551
Treatment All-at-once 653
One-at-a-time 625
JNLPBA Protein All-at-once 2,373
One-at-a-time 2,251
DNA All-at-once 581
One-at-a-time 527
Cell Type All-at-once 496
One-at-a-time 455
Cell Line All-at-once 233
One-at-a-time 212
RNA All-at-once 36
One-at-a-time 33character n-gram size in our experiments, but the ex-
perimental results exhibited the same trends across the
different choices of the size n and they did not affect our
conclusion. Therefore, we chose to report the results for
n = 50 that generally yielded good performance. In train-
ing the two kinds of models involved, the model for can-
didate phrase detection and that for their rescoring,
eighty and twenty percent of sentences in the training
data were used, respectively.
Results and discussion
Table 2 shows the performance of HMM models trained
using the all-types-at-once and the one-type-at-a-time
strategies. As stated in the Methods section, we con-
ducted ten-fold cross-validation tests on the two corpora
and the detection performance was measured with the
average F-score. Figure 1 shows how the detection per-
formance varies when a-few-types-at-a-time was em-
ployed for all the three concept types annotated in the
i2b2/VA corpus. As for the JNLPBA corpus that is anno-
tated with five concept types, there are many combina-
tions for a few types to be selected for the strategy and
hence we report on selected combinations for a single tar-
get type, Protein, in Figure 2. As seen in the figures as well
as in the table, for every concept type annotated in the
two corpora, the F-score was the highest when all concept
types were considered simultaneously, and the lowest
when each type was tackled individually. The differences
in the F-scores were statistically significant at the 0.01
alpha level using the two-tailed paired t-test. We inspected
errors in one-type-at-a-time that were correctly handledFP FN Prec. Rec. F-score
267 231 0.783 0.806 0.794
244 264 0.792 0.779 0.785
114 153 0.835 0.791 0.813
112 185 0.831 0.748 0.787
139 196 0.823 0.769 0.795
138 223 0.818 0.737 0.775
840 653 0.739 0.784 0.761
752 775 0.749 0.744 0.747
270 371 0.683 0.610 0.644
339 425 0.609 0.553 0.580
167 174 0.748 0.740 0.744
168 215 0.730 0.678 0.703
102 149 0.695 0.610 0.649
180 170 0.543 0.554 0.548
24 59 0.594 0.383 0.462
18 62 0.640 0.345 0.447
Figure 1 Detection performance for the 2010 i2b2/VA
challenge corpus. The horizontal axis shows incremental sets of
types, including the selected target type (e.g., Problem in the top
figure), and the rightmost set corresponds to the all-at-once setting.
The reported F-scores are for the selected target type.
Figure 2 Detection performance for the JNLPBA corpus. The
horizontal axis shows incremental sets of types, including the
selected target type, and the rightmost set corresponds to the all-
at-once setting. The reported F-scores are for the selected
target type.
Torii et al. Journal of Biomedical Semantics 2014, 5:3 Page 4 of 6
http://www.jbiomedsem.com/content/5/1/3in all-types-at-once, anticipating that the latter would take
advantage of multiple concept types to identify target
phrases. We noticed three major error patterns, and one
of them, type confusion, explicitly involves multiple con-
cept types. In the following description of the error pat-
terns, we use examples of the Problem type, but similar
instances were observed for the other concept types con-
sidered in the experiments.Type confusion
In one-type-at-a-time, phrases not of the target type may be
falsely detected as target type phrases, e.g., <Treatment
(durg)> for <Treatment (procedure)> where the latter
Treatment phrase was falsely detected as Problem, when
Problem alone was tackled.
Boundary errors
We observed that boundary detection was degraded
in one-type-at-a-time. Such cases included simple errors,
e.g., His melanomaProblem where the word His was
missed when Problem type was tackled alone, and also
errors involving more complex syntactic patterns, e.g.,
his <Problem> and <Problem> where the first Problem
phrase (and the word his) was missed. Over extension
of boundaries was also observed for one-type-at-a-
time, but majority of its boundary errors were under
extension.
No detection
Concept phrases correctly identified in all-types-at-once
were sometimes totally missed in one-type-at-a-time, e.g.,
The patient had no further complaintsProblem where the
Problem phrase was not detected at all when Problem
type was tackled alone.
In our review, type confusion was observed less than
what we anticipated. For example, when Problem type
was tackled alone, across ten folds, there were 42
phrases falsely detected as Problem (false negatives) that
were correctly identified as Test (8 phrases) and Treat-
ment (34 phrases) when all the types were tackled simul-
taneously. Meanwhile, there were 439 Problem phrases
that were correctly identified when all the types were
tackled but were not identified either partially (199 cases
Table 4 Time to train and apply HMM models on the
i2b2/VA and JNLPBA corpora1
A set of types considered Training
(sec)
Application
(sec)
i2b2 Problem, Test, Treatment 619 42
Problem, Treatment 763 41
Problem, Test 879 42
Problem 1,117 43
JNLPBA Protein, DNA, Cell Type, Cell line, RNA 3,010 88
Protein, DNA, Cell Type, Cell line 3,812 92
Protein, DNA, Cell Type 4,292 98
Protein, RNA 4,694 100
Protein 4,763 98
1The experiments were conducted on a server with six-core AMD Opteron
2.8 GHz processors running CentOS 2.6. The reported times are the average of
ten runs in ten-fold cross-validation.
Torii et al. Journal of Biomedical Semantics 2014, 5:3 Page 5 of 6
http://www.jbiomedsem.com/content/5/1/3of boundary errors) or fully (240 cases of no detection)
when Problem type was tackled alone. Note, however,
counting and interpretation of such error types involves
subtlety when more closely relevant concept types are
densely annotated as in the JNLPBA corpus because
boundary errors and type confusion errors coincide fre-
quently. We summarize the numbers of error instances
on the i2b2/VA corpus in Table 3. We initially expected
that different outputs would be observed among cases
involving different concept types, e.g., <Test> demon-
strated <Problem>, where we might imagine that the
recognition of the Test phrase affects that of the Prob-
lem phrase or vice versa. We, however, encountered
such instances rarely, e.g., <Test> revealed <Problem>
and <Test> showed <Problem>, in which the Problem
phrases were not detected when Problem alone was
tackled. The detection mechanism in the all-concept-
types-at-once strategy needs to be examined to under-
stand the advantage it has.
In selecting these detection strategies, another im-
portant consideration is the time to train and apply de-
tection models. As shown in Table 4, it took more time
to train a model using the one-type-at-a-time strategy.
Training of an HMM model does not require
optimization unlike other popular machine learning
methods, such as SVM and CRF, and the increase in the
number of target types may not incur extra training
time. However, reduction in the training time for all-
types-at-once was not expected. That may be attributed
to smaller per-type data structures used in all-types-at-
once, compared to larger per-type data structures in
one-type-at-a-time. The size of the model file was
smaller for all-concept-types-at-once, compared to that
for one-type-at-a-time, e.g., 159 MB for all-types-at-
once and 255 MB for Problem in one run of ten-fold
cross-validation.
Review of individual errors and analysis of run time
made us pay attention to the implementation of the
HMM program and the impacts of model parameters in-
volved, such as pruning of n-grams in the model and
smoothing of probabilities. We explored a wide range of
n-gram sizes to test if the choice of the tagging strategy,
but it was difficult to explore all the parameters simul-
taneously, e.g., the n-gram size, the smoothing param-
eter, and the pruning parameter. Further investigation is
required to gain insight in the combination of differentTable 3 Additional errors introduced in one-type-at-a-time
on the i2b2/VA corpus
Type confusion Boundary error No detection
Problem 42 199 244
Test 50 92 299
Treatment 47 266 113parameters, as well as the use of different machine learn-
ing paradigms other than HMM.Conclusions
In this study, we compared all-types-at-once and one-
type-at-a-time strategies in applying HMM taggers on a
clinical corpus released in the 2010 i2b2/VA NLP chal-
lenge workshop and a biological literature corpus released
in the JNLPBA workshop. We also tested a-few-types-at-
a-time in building a model. The experimental result shows
that tackling multiple concept types at once could im-
prove concept mention detection performance. When
building a new corpus, which has become an imminent
agenda particularly in the clinical domain, we should con-
sider annotating multiple concept types. The current re-
sults are limited to one machine learning method, but
notably the best performing systems in the i2b2/VA chal-
lenge and the NLPBA workshop employed all-types-at-
once for Semi-Markov CRF [14] and HMM with SVM
[15]. Further investigation is expected to test various
machine learning methods for these different detection
strategies.Availability of supporting data
The clinical corpus used in this research was a training
data set in the Fourth i2b2/VA Shared-Task and Work-
shop Challenges in Natural Language Processing for
Clinical Data. Information of this data set is found at
https://www.i2b2.org/NLP/Relations/.
The biology literature corpus used in this research was
a training data set for the Bio-Entity Recognition Task in
the Joint Workshop on Natural Language Processing in
Biomedicine and its Applications. The data set is avail-
able at http://www.nactem.ac.uk/tsujii/GENIA/ERtask/
report.html.
Torii et al. Journal of Biomedical Semantics 2014, 5:3 Page 6 of 6
http://www.jbiomedsem.com/content/5/1/3Abbreviations
i2b2: Informatics for integrating biology and the bedside; CRF: Conditional
random field; FN: False negative; FP: False positive; HMM: Hidden Markov
Model; JNLPBA: Joint Workshop on Natural Language Processing in
Biomedicine and its Applications; NLP: Natural Language Processing;
SVM: Support Vector Machine; TP: True positive.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
HL conceived the original idea of the study and coordinated the resources
for the experiments. MT designed the experiments and carried them out
with KW. All authors participated in the analysis of the experimental results.
MT drafted the manuscript, and KW and HL significantly revised it. All
authors read and approved the final manuscript.
Acknowledgements
This article has been published as part of thematic series Semantic Mining
of Languages in Biology and Medicine of Journal of Biomedical Semantics.
An early version of this paper was presented at the Fourth International
Symposium on Languages in Biology and Medicine (LBM 2011), held in
Singapore in 2011. De-identified clinical records used in this research were
provided by the i2b2 National Center for Biomedical Computing funded by
U54LM008748 and were originally prepared for the Shared Tasks for
Challenges in NLP for Clinical Data organized by Dr. Ozlem Uzuner, i2b2 and
SUNY. The authors also thank all the other researchers and developers who
made their software resources and annotated corpora available to the
research community. The authors acknowledge the funding from National
Science Foundation (ABI: 0845523) and the National Institute of Health
(R01LM009959A1).
Author details
1Department of Radiology, Georgetown University Medical Center,
Washington, DC, USA. 2Department of Health Sciences Research, Mayo Clinic
College of Medicine, Rochester, MN, USA.
Received: 5 September 2012 Accepted: 26 November 2013
Published: 17 January 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Wu et al. Journal of Biomedical Semantics 2014, 5:32
http://www.jbiomedsem.com/content/5/1/32
RESEARCH Open Access
BioBenchmark Toyama 2012: an evaluation of
the performance of triple stores on biological
data
Hongyan Wu1, Toyofumi Fujiwara2, Yasunori Yamamoto1, Jerven Bolleman3 and Atsuko Yamaguchi1*
Abstract
Background: Biological databases vary enormously in size and data complexity, from small databases that contain a
few million Resource Description Framework (RDF) triples to large databases that contain billions of triples. In this
paper, we evaluate whether RDF native stores can be used to meet the needs of a biological database provider. Prior
evaluations have used synthetic data with a limited database size. For example, the largest BSBM benchmark uses 1
billion synthetic e-commerce knowledge RDF triples on a single node. However, real world biological data differs from
the simple synthetic data much. It is difficult to determine whether the synthetic e-commerce data is efficient enough
to represent biological databases. Therefore, for this evaluation, we used five real data sets from biological databases.
Results: We evaluated five triple stores, 4store, Bigdata, Mulgara, Virtuoso, and OWLIM-SE, with five biological data
sets, Cell Cycle Ontology, Allie, PDBj, UniProt, and DDBJ, ranging in size from approximately 10million to 8 billion triples.
For each database, we loaded all the data into our single node and prepared the database for use in a classical data
warehouse scenario. Then, we ran a series of SPARQL queries against each endpoint and recorded the execution time
and the accuracy of the query response.
Conclusions: Our paper shows that with appropriate configuration Virtuoso and OWLIM-SE can satisfy the basic
requirements to load and query biological data less than 8 billion or so on a single node, for the simultaneous access
of 64 clients.
OWLIM-SE performs best for databases with approximately 11 million triples; For data sets that contain 94 million and
590 million triples, OWLIM-SE and Virtuoso perform best. They do not show overwhelming advantage over each
other; For data over 4 billion Virtuoso works best.
4store performs well on small data sets with limited features when the number of triples is less than 100 million, and
our test shows its scalability is poor; Bigdata demonstrates average performance and is a good open source triple
store for middle-sized (500 million or so) data set; Mulgara shows a little of fragility.
Background
Semantic Web encodes information from theWorldWide
Web in a machine-readable syntax to make web infor-
mation automatically recognizable and processable by
computers [1]. Semantic Web, which is about common
formats for integration and combination of data drawn
from diverse sources [2], facilitates the integration of het-
erogeneous data on the World Wide Web by applying
*Correspondence: atsuko@dbcls.rois.ac.jp
1Database Center for Life Science, Research Organization of Information and
Systems, 178-4-4 Wakashiba, Kashiwa, Chiba 277-0871, Japan
Full list of author information is available at the end of the article
formal ontologies to specify the semantics of the data
explicitly [3]. Semantic Web has unleashed a revolution of
data publication and interconnection [4].
Semantic Web has gained significance in the life sci-
ences. Due to the success of the Human Genome Project
(HGP) [5] and high-throughput sequencing, a large quan-
tity of biological data is available to the scientific commu-
nity via the Internet [4]. One challenge posed by biological
databases is the diversity of data types, which include
sequence (e.g., NCBIs GenBank [6]), microarray gene
expression (e.g., SMD [7] and GEO [8]), pathway (e.g.,
BIND [9]), and proteomic data (e.g., PeptideAtlas [10]).
These diverse data types are highly heterogeneous both
© 2014 Wu et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited.
Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 2 of 11
http://www.jbiomedsem.com/content/5/1/32
in structure and semantics [11]. However, the complex-
ity of a disease cannot be explained without referring to
multiple biological databases. For example, to understand
Parkinsons disease requires both neuroscience informa-
tion as well as mapping of gene expression across the
whole brain [12,13]. Semantic Web provides a way to
integrate heterogeneous data source.
Life and health science communities [14] have made
remarkable progress as early adopters of Semantic Web
technologies [15]. For example, the UniProt knowledge-
base [16] is one of the core public databases in the life
sciences. UniProt connects more than 150 molecular biol-
ogy and chemoinformatics databases and integrates, inter-
prets, and standardizes data from numerous resources
to achieve the most comprehensive catalogue of pro-
tein sequences and functional annotations. As another
example, the Protein Data Bank Japan(PDBj) [17] accepts
and processes PDB entries that are deposited mainly
from Asian and Oceanic researchers and maintains a
centralized archive of macromolecular structures in col-
laboration with other wwPDB [18] members, including
the RCSB-PDB [19], the BMRB [20] in the US, and the
PDBe [21] in Europe.
The popularity of Semantic Web has accelerated the
rapid development of one of its core techniques, the triple
store. A triple store [22] is designed to store and retrieve
triples, which is a statement relating one object to another.
This paper evaluates the performance of five native triple
stores on biological data.
Our evaluation was motivated by a project that is sup-
ported by the Japan Science and Technology Agency to
integrate data in the life sciences. Our aim is to evaluate
whether RDF native stores can meet the needs of a biolog-
ical database provider. Existing benchmarks, such as the
Lehigh University Benchmark (LUBM [23]) and the Berlin
SPARQL Benchmark (BSBM [24]), use a data generator to
produce synthetic e-commerce knowledge data, and the
largest database on a single machine generated by such
a data generator includes 1 billion triples. However, real
world biological data differs from the simple synthetic
data much. The UniProt data has 164 owl classes and
uses more than 180 properties, while SP2Bench [25] uses
only 23 properties, and BSBM [24] uses a similar number
of properties and only 8 classes. Due to one RDF triple
including only one property, 180 properties may theoreti-
cally need the times of join over 180. This means that both
the graph and queries in UniProt are significantly different
in form to the generated data in either SP2Bench or BSBM.
In addition each instance in RDF may differ much from
each other even in the same class, which makes RDF flexible
to express heterogeneous data, and therefore to pick up a
set of instances covering all 180 properties and 164 classes
will take a lot of effort. It is quite difficult to guarantee the
conclusions drawn from synthetic benchmarks or other
fields are applicable to biological data. The biological data
benchmark, Cell Cycle Ontology [26] uses real biologi-
cal data. However, it includes only 10 million triples. We
used five groups of real biological data set ranging from
10 million to 8 billion to make sure that the data was scal-
able and variable enough. Due to hardware requirement
of running a datastore of the UniProt and DDBJ size there
are few if any dependable public benchmark results i.e.
fully describing the disk system and software used. There
are no reports for single node installations with dataset
sizes of more than 1 billion nodes. Our target is to verify
applicability of a triple store for biological databases.
For this evaluation, we used biological databases, Cell
Cycle Ontology, Allie, PDBj, UniProt, and DDBJ con-
taining as many as 8 billion triples. Biological databases
are also characterized by diverse and sparse data, which
may impact performance. We evaluated the load and
query costs of five popular triple stores: 4store, Bigdata,
1
10
100
1000
10000
Cell Cycle
Ontology
Allie PDBj UniProt DDBJ
M
ill
io
n
log
Figure 1 The size of the data sets. The five biological data sets that were used in our evaluation, with sizes ranging from 11 million to
approximately 8 billion triples.
Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 3 of 11
http://www.jbiomedsem.com/content/5/1/32
Table 1 The loading cost for each triple store
Triple store Cell Cycle Allie PDBj UniProt DDBJ
Ontology
OWLIM-SE (min) 3 22 140 3770 7750
Virtuoso (min) 4 47 92 3508 4759
4store (min) 2 12 4834 X X
Bigdata (min) 3 272 1158 X X
Mulgara (min) 10 86 X X X
Mulgara, Virtuoso, and OWLIM-SE. To the best of our
knowledge, we evaluated the largest scale of real biological
data possible on a single node.
Methods
Triple store
We selected five native triple stores. Three of them were
recommended by the Bioinformaticians in the interna-
tional symposium Biohackathon 2011, who had used
or tested these triple stores for their biological data.
4store was used in the Cell Cycle Ontology [26]. Mul-
gara was used as an internal triple store in DDBj.
OWLIM-SE has been applied as UniProt triple store.
Virtuoso showed good performance in BSBM and DBpe-
dia SPARQL Benchmark. Bigdata, a complete free open
source triple, performed averagely well in BSBM and sup-
ported most of inference functions and could run in
both single node and cluster mode. It could be a poten-
tially good candidate to customize ones own triple store.
Neither Jena TDB nor Jena SDB showed attractive per-
formance in [26], in which both of them worked worse
than 4store and Virtuoso. Sesame showed bad load perfor-
mance in BSBM Version 1. We evaluated the triple stores
using their newest versions as of June 30, 2012.
4store
4store [27,28] is a RDF/SPARQL store that is written in
C and designed to run on UNIX-based systems. 4store
can be run on a single machine or networked clusters. We
evaluated 4store version 1.1.4.
Bigdata
Bigdata [29] is designed as a distributed database archi-
tecture that runs on clusters of hundreds to thousands of
commodity machines. However, Bigdata can also run in
high-performance single-server mode. Bigdata supports
RDFS and limited OWL inference. Bigdata is open-source
software that is written in Java. We evaluated version
RWSTORE_1_1_0.
OWLIM-SE
OWLIM-SE [30,31] is a member of the OWLIM fam-
ily (OWLIM-Lite, OWLIM-SE, OWLIM-Enterprise, and
OWLIM on Amazon AWS), which provides native RDF
engines that are implemented in Java and deliver full per-
formance through both Sesame and Jena. Beginning with
version 4.3, OWLIM-SE supports SPARQL 1.1 Federation.
OWLIM-SE also supports the semantics of RDFS, OWL
2 RL, and OWL 2 QL. OWLIM-SE is available by com-
mercial license only. We evaluated OWLIM-SE version
5.1.5269.
Mulgara
Mulgara [32] is an open-source triple store that is writ-
ten in Java. Mulgara provides a SQL-like language shell,
iTQL (Interactive Tucana Query Language), to query and
update Mulgara databases. Mulgara supports RDFS and
OWL inference. In addition, Mulgara also provides a
SPARQL query parser and query engine. We evaluated
Mulgara version V2.1.13.
Virtuoso
Virtuoso [33,34] provides a triple storage solution for RDF
on RDBMS platforms. Virtuoso is a multi-purpose data
server that supports RDBMS, RDF, and XML. Virtuoso
offers stored procedures to load RDFXML, ntriples, and
1
10
100
1000
10000
Cell Cycle 
Ontology
Allie PDBj UniProt DDBJ
m
in
s
OWLIM-SE
Virtuoso
4store
Bigdata
Mulgara
log
Figure 2 The loading cost of each triple store. The loading cost for each triple store for each data set. A missing value indicates that we failed to
load the data set.
Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 4 of 11
http://www.jbiomedsem.com/content/5/1/32
Table 2 The space cost to load the data for each triple store
Triple store Cell Cycle Allie PDBj UniProt DDBJ
Ontology
OWLIM-SE 3.7G 8.2G 27G 213G 513G
Virtuoso 0.84G 6.4G 30G 308G 538G
4store 2.2G 14.7G 66G X X
Bigdata 0.78G 6.2G 34G X X
Mulgara 2.4G 15.8G X X X
compressed triples. Virtuoso also supports SPARQL as
well as limited RDFS and OWL inference. Virtuoso can be
run on both standalone and clusteredmachines. The stan-
dalone triple store server is available through both open
source and commercial licensing. We evaluated Virtuoso
version 6.4 commercial because we found some bugs in
the open source version.
Data set
We chose five typical biological data sets to evaluate. The
number of triples in these data sets ranging from 10 mil-
lion to 8 billion. The data were available as either a set
of large files, such as uniprot.rdf.gz, uniparc.rdf.gz, and
uniref.rdf.gz in the UniProt data set, or a set of small files,
e.g., 77,878 files in the PDBj data set. Figure 1 shows the
data size for each data set. Their formats and download
addresses are as follows:
Cell Cycle Ontology [26]: .rdf format, 11,315,866 trip-
les, from http://www.semantic-systems-biology.org/. We
downloaded the data on December 21, 2011.
Allie [35,36]: .n3 format, 94,420,989 triples, from
ftp://ftp.dbcls.jp/allie/. We used the data published on
December 12, 2011.
PDBj [37]: .rdf.gz format, 589,987,335 triples, 77,878
files, from ftp://ftp.pdbj.org/XML/rdf/. We downloaded
the data on December 19, 2011.
UniProt [38]: .rdf.gz format, 4,025,881,829 triples,
including 3 larger files, uniprot.rdf.gz, uniparc.rdf.gz, and
uniref.rdf.gz, and 7 smaller files, including citations.rdf.gz,
enzyme.rdf.gz, journals.rdf.gz, etc. from ftp://ftp.uniprot.
org/pub/databases/uniprot/. We used the version that
was released in November 2011.
DDBJ [39,40]: .rdf.gz format, 7,902,743,055 triples, 330
files, from ftp://ftp.ddbj.nig.ac.jp/ddbj_database/ddbj/.
We downloaded the data on December 20th, 2011.
SPARQL Query
The query use cases we used in this study were designed
based on the daily usage of the data set. These use
cases reflected the main search functions in the website
of each data set, http://www.semantic-systems-biology.
org/biogateway/querying for Cell Cycle Ontology, http://
allie.dbcls.jp/ for Allie, http://beta.sparql.uniprot.org/ for
UniProt, http://legacy.pdbj.org/index.html for PDBj, and
http://www.ddbj.nig.ac.jp/searches-e.html for DDBJ.
The SPARQL queries in our benchmark included
queries aimed at retrieving one record as well as larger
result sets. Our queries included as many as 11 joins. Dif-
ferent types of queries have a large impact on the query
store performance. The same query written in two dif-
ferent ways can produce radically different query times.
In addition, the designed queries considered the perfor-
mance of many functions, including join, orderby, filter,
distinct, union, optional, count, limit, and offset. Section
Additional file 1SPARQL Query shows the detailed
queries that we tested.
Benchmark
Load time
We searched for the best performance for each triple
store. We imported the data with default parameters
as well as several empirically improved settings and
identified the best configuration (please see the section
Table 3 The queries for Cell Cycle Ontology
Endpoint case1 case2 case3 case4 case5 case6 case7 case8 case9 case10
OWLIM-SE (ms) 121 9 2740 5 149 1722 3 39 25 1
Virtuoso (ms) 24 2 23280 3 42500 13073 5 7562 41 2
4store (ms) 56 18 1236 13 33 64 22 67 2035 7
Bigdata (ms) 282 35 3247 13 52 3320 11 93 47 10
Mulgara (ms) 1294 20 2207 9 343 2325 32 58 33 4
Endpoint case11 case12 case13 case14 case15 case16 case17 case18 case19
OWLIM-SE (ms) 6 47 2 1 52779 7 4 24 17
Virtuoso (ms) 120 19 5 1 56058 46 15 16 16721
4store (ms) 6 1563 8 7 X X X X 15
Bigdata (ms) 20 27 5 6 18126 X X X 30
Mulgara (ms) 14 X 9 6 X X X X 38
Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 5 of 11
http://www.jbiomedsem.com/content/5/1/32
Figure 3 The results of the Cell Cycle Ontology query evaluation. The detailed query results for all 19 queries that were submitted to the Cell
Cycle Ontology database for each triple store. For queries 16, 17, and 18, only the performance data for OWLIM-SE and Virtuoso are reported
because the other triple stores failed to execute these queries.
Additional file 2Configuration for our optimal con-
figuration). We tested each triple store with the best
configuration twice and reported the load time as the aver-
age cost over the two tests. Every time we cleared the file
system memory cache, deleted the previous database and
then loaded the data on an empty store.
Disk space requirement
The disk space requirement is the total disk storage that
is used to load the data set for each triple store. We
report the disk space requirement as the size of the whole
directory that was used by the data repository.
Query response time
We executed the whole query sequence for every triple
store and recorded the query response time. We did this
five times. Considering some unsteady factor (such as the
system cache situation) may incur a higher query response
time cost, we removed the highest one and reported the
query response time as the average cost of the remaining
four queries. In this paper, we present only the average
cost; details about the five time costs for each triple store
can be found at our website [41].
To evaluate simultaneous executions with multi-clients,
we sequentially picked up the queries successfully exe-
cuted by all the tested triple stores (e.g., we used the 14
queries without case 12, 15, 16, 17, 18 in Cell Cycle Ontol-
ogy data set) to form five query mixes, and then execute
each query mix five times with 1, 4, 8, 64 clients, respec-
tively, for each data set and triple store.Wemeasured their
time cost.
Query soundness
We checked whether the triple store was able to return
query results with the default query setting. For a query
that neither gave a result nor provided an error message
in one hour, we would report it failed the query. If a
query failed, we reported the unsupported clause or error
message. In addition, for a query with limit predicate,
we checked whether the demanding or maximum size was
returned. For queries asking for returning all the results,
we examined the result size of each triple store. If the
result size that some triple store returned was smaller, we
tuned its configuration and performed the query again to
try to return more results until its maximum results were
returned.
Environment
Our evaluation focused on the data store on a single
machine and a single end-user query. We used an Intel(R)
Xeon(R) CPU E5649@ 2.53GHz with a 12 core hyper-
threaded system (24 virtual cores), 64G of RAM, three 2T
SCSI disk storages, an ext3 file system, CentOS release 5.7,
and JDK 1.6.0_26.
Results
Load time
We conducted performance tuning to determine the best
performance for every triple store. We found that Virtu-
oso having one stream per core to load the data was a
good performance point which would keep all parts of
the system busy. The results showed that Virtuoso had
better performance with parallel loading on a multi-core
machine when a set of small files with multiple threads
was uploaded. Therefore, to test UniProt loading, we used
Table 4 The queries for Allie
Endpoint case1 case2 case3 case4 case5
OWLIM-SE(ms) 136 1530 1091 31 78942
Virtuoso (ms) 23 1413 152 95 27299
4store (ms) X 217 X X 65128
Bigdata(ms) 365 690 1779 98 38523
Mulgara(ms) 373 121 X X X
Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 6 of 11
http://www.jbiomedsem.com/content/5/1/32
Figure 4 The results of the Allie query evaluation. The detailed query results for all five queries that were submitted to the Allie database for
each triple store. A missing value indicates that the query failed for that triple store.
the Virtuoso procedure language to split all of the files
into a set of smaller files that were each composed of
200,000 triples, and we used 12 loading threads in our
test. This splitting cost an additional 17 hours of runtime
(The last load time for Virtuoso in Table 1 includes this 17
hours). For some triple stores, such as OWLIM-SE, per-
formance was improved by adjusting the JVM parameters
(e.g., -Xmx, -Xms, etc.). However, other triple stores, such
as Mulgara, were not influenced by adjusting the JVM
parameters. For details about our loading approaches,
please see our website [41].
Table 1 shows that OWLIM-SE and Virtuoso are able
to finish all of the loading tasks. The X mark in the
table indicates a failure to load the data set. Using 4store,
the time cost to load approximately 100 million triples
in the Allie data set to 500 million triples in the PDBj
data set increased 400-fold. Therefore, we did not eval-
uate the performance of 4store on UniProt or DDBJ
because of its poor scalability. Mulgara failed to load
PDBj with the error message Unable to load file: Ille-
gal character ABSA_(A^2); Unexpected XAException
error occurred when loading UniProt and DDBJ. Bigdata
had difficulty in loading all of the UniProt data, such that
the loading process almost stopped when the loaded triple
number exceeded 3.5 billion. However, Bigdata was able to
load the data easily when the triple number was less than
3 billion. Figure 2 illustrates the loading cost for each data
set.
In addition, the format (such as the data set is composed
of a big file or a set of small files) of the data set affected
the performance of the triple store. The time to load PDBj,
UniProt, and DDBJ was less for Virtuoso compared with
OWLIM-SE, while the time to load Cell Cycle Ontology
and Allie was greater for Virtuoso than OWLIM-SE. This
difference may be partly due to the format of the data;
the three former data sets were composed of many small
files. The triple number of DDBJ was nearly two times
that of UniProt (7.9 billion compared with 4.0 billion,
respectively). However, using Virtuoso, the loading cost
for DDBJ was much less than two times the loading cost
for UniProt. Virtuoso demonstrated good performance
when loading multiple small files with multiple threads.
Our experiment on OWLIM-SE 4.3 [42] demonstrated
that OWLIM-SE 4.3 took less time to load DDBJ com-
pared with UniProt, which also suggests that the format
of the data set (e.g., multiple small files) affects the per-
formance of a triple store. The difference can partly come
from the data sets themselves since we use five different
real data sets.
Disk space requirement
Table 2 shows the space that was consumed when load-
ing the data set for every triple store. When loading each
data set we cleared the database and then loaded the data
on an empty store. Therefore the presented space is just
what the data set occupied. The experiment shows that the
space used by OWLIM-SE increased slowest as the data
size increased. 4store, Bigdata andMulgara were relatively
poor.
Table 5 Query for PDBj
Endpoint case1 case2 case3 case4
OWLIM-SE(ms) 72 2 162 7
Virtuoso (ms) 147 2 2 138
4store (ms) 1025 1274 131 1524
Bigdata(ms) 190 14 35 54
Mulgara(ms) X X X X
Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 7 of 11
http://www.jbiomedsem.com/content/5/1/32
1
10
100
1000
10000
case1 case2 case3 case4
m
s
OWLIM SE
Virtuoso
4store
Bigdata
Mulgara
log
Figure 5 The results of the PDBj query evaluation. The detailed query results for all 4 queries that were submitted to the PDBj database for each
triple store. The performance of Mulgara is not reported because Mulgara failed to load PDBj.
Query response time and query soundness
Cell cycle ontology
Table 3 shows the query performance for the Cell Cycle
Ontology data set. The X mark indicates a query that
failed, and boldface shows the fastest response for each
query. It is the same to the following tables. Both Virtuoso
and OWLIM-SE demonstrated sound query ability. Both
Virtuoso and OWLIM-SE completed all of the queries.
The query soundness of 4store depended on the set-
ting of the parameter Softlimit. In the first query, with
Softlimit equal to 5000, 4store was able to return all 53
results. However, when Softlimit was equal to 1000, 4store
returned only 17 results. In addition, 4store, Bigdata and
Mulgara could not support the count() function in queries
16, 17, and 18. Mulgara returned a zero result for query 15
(the result size should be 7354) and an Unknown Con-
straintExpression exception for query 12. 4store gave no
response to query 15 in one hour.
Figure 3 shows the corresponding bar chart for the
Cell Cycle Ontology results. For this smallest data set,
Virtuoso responded faster than other triple stores for
some queries but was slowest for other queries, such as
query 5 and query 19. We say that Virtuoso has worst
cases. OWLIM-SE performed best on this data set and
had no worst cases. Bigdata had average performance on
this data set. Although 4store had poor query sound-
ness, the performance of 4store was distinctly better for
some cases, such as query 5 and query 6. Mulgara per-
formed the worst of all of the triple stores on this data
set.
Allie
Table 4 shows the query performance for the Allie data set.
Virtuoso, Bigdata, and OWLIM-SE demonstrated sound
query ability on this data set. 4store did not support the
lang() function in queries 1, 3, and 4. Mulgara was unable
to support the arbitrarily complexORDER BY clause.
The triple number of this data set nearly 10-fold higher
than that of the Cell Cycle Ontology data set. Figure 4
shows that Virtuoso and OWLIM-SE performed better
than the other triple stores. For this data set, Virtuoso had
no worst cases. Bigdata had average performance on this
data set. 4store was limited but performed well on query 2.
PDBj
Virtuoso andOWLIM-SE performed better than the other
triple stores on the PDBj data set, as shown in Table 5
and Figure 5. However, neither of them had a signifi-
cant advantage. 4store demonstrated sound query abil-
ity, but the query performance on this data set was the
worst of the five triple stores. Bigdata again displayed
average performance. Mulgara failed to load the PDBj
data set, and therefore we could not present its query
performance.
Table 6 the queries for UniProt
Endpoint case1 case2 case3 case4 case5 case6 case7 case8 case9 case10
OWLIM-SE (ms) 931 1920 2627 142 61 89586 86380 674 994 1053
Virtuoso(ms) 51 95 114 2 7 2206 34916 413 605 652
Endpoint case11 case12 case13 case14 case15 case16 case17 case18
OWLIM-SE (ms) 50 10 9 7 15037 32055 2818 8548
Virtuoso (ms) 53 4 289 269 10631 9052 2 76
Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 8 of 11
http://www.jbiomedsem.com/content/5/1/32
Table 7 The queries for DDBJ
Endpoint case1 case2 case3 case4 case5 case6 case7 case8 case9 case10
OWLIM-SE (ms) 4783 4528 4867 12 25 4 470 1078 22 1
Virtuoso(ms) 226 218 418 56 7 98 5 4 7 1
UniProt and DDBJ
For the two largest data sets, UniProt and DDBJ, Virtuoso
performed the best.
We were able to completely load these two data sets only
with OWLIM-SE and Virtuoso. Table 6 and Table 7 report
that both Virtuoso and OWLIM-SE performed well on the
UniProt and DDBJ data sets, respectively. However, Vir-
tuoso performed better as the triple number increased.
Figure 6 and Figure 7 are the corresponding bar charts for
the results of the UniProt andDDBJ data sets, respectively.
Simultaneous execution
Table 8 shows simultaneous executions withmulti-clients,
1, 4, 8, and 64 clients respectively. Mulgara reported the
error Interrupted while waiting to acquire lock when
doing queries with over 2 clients.We only evaluated 4store
with Cell Cycle Ontology and Allie because it showed
unsteady performance with multi-clients when data is
larger. Virtuoso, OWLIM-SE and Bigdata finished the
simultaneous executions with good scalability.
Conclusions
Our paper shows that with appropriate configuration Vir-
tuoso and OWLIM-SE can satisfy the basic requirements
to load and query biological data less than 8 billion or so
on a single node, for the simultaneous access of 64 clients.
OWLIM-SE performs best for databases with approxi-
mately 11 million triples, with no worst query cases; For
data sets that contain 94 million and 590 million triples,
OWLIM-SE and Virtuoso perform best in the five eval-
uated triple stores, and they do not show overwhelming
advantage over each other; For data over 4 billions Virtu-
oso works best.
As for other triple stores, (1) 4store performs well on
small data sets (e.g. Cell Cycle Ontology) with limited
features, and our test shows its scalability is poor; (2) Big-
data demonstrates average performance on both loading
and querying and may be a good open source triple store
for middle-sized (500 million or so) data set; (3) Mulgara
shows a little of fragility.
Discussion and future work
Our evaluation shows that both Virtuoso and OWLIM-SE
are able to efficiently load and query data sets with up to
approximately 8 billion triples on a single machine. The
scalability of both Virtuoso and OWLIM-SE is good. Vir-
tuoso has the best performance with parallel loading on
a multi-core machine for sets of small files with multiple
threads. Although Virtuoso has some worst cases when
the data set is very small, its performance improves as the
number of triples increases. 4store performs the best on
small data sets with limited features. The performance of
4store worsens from Cell Cycle Ontology to PDBj as the
size of the data set increases, indicating that the scalabil-
ity of 4store is poor. Bigdata had average performance on
all data sets with acceptable loading and query costs. Big-
data may therefore be a good open source triple store for
smaller data sets. BecauseMulgara failed to load several of
the data sets that were tested, its query performance could
not be demonstrated.
Our results indicate that 4store can perform well on
both loading and querying data with limited features when
Figure 6 The results of the UniProt query evaluation. Only OWLIM-SE and Virtuoso were able to load the UniProt database.
Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 9 of 11
http://www.jbiomedsem.com/content/5/1/32
Figure 7 The results of the DDBJ query evaluation. Only OWLIM-SE and Virtuoso were able to load the DDBJ database.
the number of triples is less than 100 million. For data
sets of moderate size (100million to 500million), Virtuoso
andOWLIM-SE perform similarly. Of the five tested triple
stores, Virtuoso performs best on data sets with several
billion triples.
The conclusions in our benchmark are basically con-
sistent to BSBM when data size is less than 1 billion,
however, not to all other benchmarks. Biological data
benchmark [26] shows that OWLIM responded in rela-
tively short time, 4store in moderate time and Virtuoso
was slowest. Our benchmark shows some difference. Vir-
tuoso performed best in many cases while it was slowest
in some others. Our benchmark proves that Virtuoso had
good scalability, while it could perform not well for small
data. Another real-world data triple store benchmark [43]
shows that Virtuoso was slowest to load the data. Our
benchmark shows that Virtuoso worked faster in load-
ing and querying as increasing the data size. In addition
our evaluation shows that both Virtuoso and OWLIM-SE
scaled well up to 8 billion in both loading and querying on
a single node.
Our detailed evaluation of the configurations of each
triple store (please see the detailed configurations in
Additional file 2  Configuration for each triple store, or
refer to our website) demonstrated that the cost associ-
ated with loading the data depends on multiple factors,
including the server configuration (e.g., CPU, memory,
hard disk, etc.), the system property (e.g., vm.swappiness,
JVM, etc.), the application configuration (e.g., cachemem-
ory in OWLIM-SE, etc.), and the data format and the size
Table 8 Simultaneous execution
Triple store Number of clients Cell Cycle Ontology Allie PDBj UniProt DDBJ
OWLIM-SE(ms)
1 6,402 6,704 861 1,651,466 83,179
4 8,474 13,967 1,041 1,911,144 89,626
8 14,190 20,891 1,033 2,216,634 109,195
64 120,126 159,211 2,286 6,058,957 442,181
Virtuoso(ms)
1 14,742 1,421 789 31,876 49,624
4 22,459 7,189 1,168 50,953 5,246
8 27,297 9,870 1,655 58,498 10,426
64 194,850 55,366 8,496 905,697 35,879
4store(ms)
1 4,706 682 x x x
4 15,825 1,413 x x x
8 27,604 2,191 x x x
64 237,246 15,288 x x x
Bigdata(ms)
1 10,757 100,683 2,028 x x
4 15,617 129,136 2,138 x x
8 82,579 850,852 2,051 x x
64 108,755 4,467,378 2,930 x x
Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 10 of 11
http://www.jbiomedsem.com/content/5/1/32
of the data set (e.g., DDBJ is nearly two times the triple
size of UniProt, but the loading cost when using Virtuoso
is two times less for DDBJ than UniProt, which indicates
that the scaling is not proportional, etc.).
For each database, several results were obtained by
adjusting parameters that may significantly influence the
performance of each triple store. These parameters may
also perform differently with different hardware and soft-
ware platforms as well as with different data sets. A
test of all possible parameter combinations is difficult
because some data sets, such as UniProt and DDBJ, may
take several days to load. Therefore, one limitation of
our evaluation is that we cannot guarantee that we have
demonstrated the best absolute performance of each triple
store.
In the future, we will evaluate federated queries as well
as the inference ability of each triple store. The use cases
we used in this study were designed based on their daily
usage, including do join operations over 10 times, dif-
ferent types of filter operations, and almost all of the
clauses that are frequently used in the SPARQL queries.
Some other special use cases can be designed to test the
detailed performance of each triple store, such as tests
of PSO (in predicate-subject-object order) and POS (in
predicate-object-subject order) indices. In addition, the
triple stores themselves are also improving as newer ver-
sions are released. For example, disk space requirements
and loading costs have been improved in OWLIM by
introducing compression and fixing bugs in the engine.
Although Virtuoso 7 seems a mere major update to Vir-
tuoso 6, the underlying technologies are very different.
Virtuoso 6 is a row store database, but Virtuoso 7 adopts
column store technology, which makes them a totally dif-
ferent performance. For Allie data set, Virtuoso 7 took 7
minutes to import. As for five query use cases, it took 61,
1107, 391, 71 and 5633 milliseconds, respectively. Com-
pared with Virtuoso 6, response for use case 2, 4 and 5
were faster, 1 and 3 were slower. However, we found that
there are still some problems to use Virtuoso 7, such as
system crashed when uploading our DDBJ data with error
log GPF:Dkpool.c:munmap failed. We will keep evaluat-
ing new triple stores or versions and their clusters, and
updating the results in our website http://kiban.dbcls.jp/
togordf/wiki.
Additional files
Additional file 1: SPARQL Query. This file includes the details of the
SPARQL queries that we used in our evaluations [26].
Additional file 2: Configuration. This file presents the modified
parameters for each database.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
The work presented in this paper was conducted in collaboration between all
authors. HW conducted the experiments, analysed the data, and drafted the
paper. TF, YY, and JB worked on data collection, use-case design, and software
tuning as well as other related tasks. AY coordinated and managed the entire
experimental process. All authors have contributed to revisions to the
manuscript and have approved the final version of the manuscript.
Acknowledgements
This work has been supported by the National Bioscience Database Center
(NBDC) of the Japan Science and Technology Agency (JST). Jerven Bolleman
was supported by the Swiss Federal Government through the State Secretariat
for Education, Research and Innovation SERI and by the National Institutes of
Health (NIH) grant 4U41HG006104-04.
Author details
1Database Center for Life Science, Research Organization of Information and
Systems, 178-4-4 Wakashiba, Kashiwa, Chiba 277-0871, Japan. 2INTEC Inc, 1-3-3
Shinsuna, Koto-ku, Tokyo 136-8637, Japan. 3Swiss-Prot group, SIB Swiss
Institute of Bioinformatics, CMU, 1 Michel Servet, 1211 Geneva 4, Switzerland.
Received: 13 May 2013 Accepted: 27 April 2014
Published: 10 July 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Palombi et al. Journal of Biomedical Semantics 2014, 5:20
http://www.jbiomedsem.com/content/5/1/20
RESEARCH Open Access
My Corporis Fabrica: an ontology-based tool
for reasoning and querying on complex
anatomical models
Olivier Palombi1,2*, Federico Ulliana3, Valentin Favier1, Jean-Claude Léon2 and Marie-Christine Rousset3
Abstract
Background: Multiple models of anatomy have been developed independently and for different purposes. In
particular, 3D graphical models are specially useful for visualizing the different organs composing the human body,
while ontologies such as FMA (Foundational Model of Anatomy) are symbolic models that provide a unified formal
description of anatomy. Despite its comprehensive content concerning the anatomical structures, the lack of formal
descriptions of anatomical functions in FMA limits its usage in many applications. In addition, the absence of
connection between 3D models and anatomical ontologies makes it difficult and time-consuming to set up and
access to the anatomical content of complex 3D objects.
Results: First, we provide a new ontology of anatomy called My Corporis Fabrica (MyCF), which conforms to FMA but
extends it by making explicit how anatomical structures are composed, how they contribute to functions, and also
how they can be related to 3D complex objects. Second, we have equipped MyCF with automatic reasoning
capabilities that enable model checking and complex queries answering. We illustrate the added-value of such a
declarative approach for interactive simulation and visualization as well as for teaching applications.
Conclusions: The novel vision of ontologies that we have developed in this paper enables a declarative assembly of
different models to obtain composed models guaranteed to be anatomically valid while capturing the complexity of
human anatomy. The main interest of this approach is its declarativity that makes possible for domain experts to
enrich the knowledge base at any moment through simple editors without having to change the algorithmic
machinery. This provides MyCF software environment a flexibility to process and add semantics on purpose for
various applications that incorporate not only symbolic information but also 3D geometric models representing
anatomical entities as well as other symbolic information like the anatomical functions.
Background
Computer modeling and simulation of the human body is
becoming a critical and central tool inmedicine but also in
many other disciplines, including engineering, education,
entertainment. Multiple models have been developed, for
applications ranging from medical simulation to video
games, through biomechanics, ergonomics, robotics and
CAD, to name only a few. However, currently available
anatomical models are either limited to very specific areas
or too simplistic for most of the applications.
*Correspondence: OPalombi@chu-grenoble.fr
1Department of Anatomy, LADAF, Université Joseph Fourier, Grenoble, France
2LJK (CNRS-UJF-INPG-UPMF), INRIA, Université de Grenoble, Grenoble, France
Full list of author information is available at the end of the article
The most generic models used to describe the anatomy
are ontologies. Ontologies provide a unified view of a
domain of interest resulting of a joint effort of a whole
community to standardize a common vocabulary with
a clear semantics that can then be shared by users to
annotate, index and retrieve data and tools.
A lot of more or less specialized medical ontologies
have flourished recently. Most of them are grouped into
the Open Biological and Biomedical Ontologies foundry
(OBO) [1]. For human anatomy, the reference domain
ontology is the Foundational Model of Anatomy (FMA)
[2] which is a comprehensive description of the struc-
tural organization of the body. Its main component is a
taxononomy with more then 83000 classes of anatomical
© 2014 Palombi et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication
waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise
stated.
Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 2 of 13
http://www.jbiomedsem.com/content/5/1/20
structures from the macromolecular to the macroscopic
levels. The FMA symbolically represents the structural
organization of the human body.
The complexity of human anatomy can make it difficult
for users to comprehend and interact with the anatomical
knowledge embedded. This complexity may explain the
gap between available anatomical ontologies and poten-
tial users [3]. In practice, anatomical concepts are usually
used through the scope of other ontologies. For instance
in SNOMED CT [4], anatomical concepts are linked to
specific diseases or symptoms. In fact, four of the 8 Open
Biological and Biomedical Ontologies (OBO) Foundry
ontologies and 39 of the sites other listed ontologies, cover
aspects of the representation of anatomical knowledge.
Whilst the OBO Foundry has a stated goal of creating a
suite of orthogonal interoperable reference ontologies in
the biomedical domain [5], most of these ontologies have
been developed to address a species-specific need artic-
ulated by a community working with a particular model
organism.
Relations between anatomical structures and their func-
tions appear to be a relevant knowledge. These structural
and functional relationships have been explicitly defined
at the level of cells in [6] or at the level a whole organ-
ism as the drosophila [7]. Also, Uberon [8] takes into
account functions in order to query multiple ontologies of
species.
Yet, human body modeling relies on morphological
components on the one hand and functional and pro-
cess descriptions on the other hand. This fundamental
interaction between structures and functions has been
already highlighted by Smith et al. [9,10]. This philosoph-
ical approach relies on the idea that the link between
anatomy and physiology must be formalized in an new
reference ontology. We try to turn this concept into
action.
The ICF is an International Classification of Func-
tioning, Disability and Health (ICF) [11] endorsed by
the World Health Organization since 2001. However, its
analysis reveals that the current version of ICF exhibits
non-conformances to many formal ontological principles
[12,13]. The need for a formal description of anatomical
functions has been outlined in [14], with some guidelines
for getting a separate ontology of anatomical functions
based on an ontological analysis of functions in general
formal ontologies such as GFO [15] or Dolce [16].
Another limitation is that, despite the proliferation of
specialized or general ontologies, they are far from being
fully exploited, mainly because they are only seen as a
standard common structured vocabulary used for anno-
tating and navigating among resources. Yet, they also
come with a formal logical semantics that makes them
processable by machines through inference algorithms.
However, until now, only few existing works in biomedical
ontologies (e.g., [17-20]) take advantage of available auto-
matic reasoners. Most of these works rely on ontologies
expressed in OWL and use OWL reasoners that are based
on Description Logics [21]. OWL is one the standards
recommended by the W3C for the Semantic Web, which
enables expressing sophisticated ontological constraints
(using Description Logics constructors) but with a high
computational complexity in the worst case. RDFS is
another W3C standard that is broadly used in particular
in Linked Data. Whereas OWL is often seen as an exten-
sion of RDF and RDFS, this is not exactly the case, mainly
because RDF(S) offers interesting non-first-order features
which are not present in the Description Logics at the
basis of OWLprofiles, like the possibility of treating values
both as constants and as classes or properties. In the same
spirit, the RDF query language SPARQLmakes possible to
query at the same time the data and the schema and allows
that variables stand for classes and properties. This goes
beyond the first-order conjonctive queries typically con-
sidered inDL-based settings. Recently, RDF-based seman-
tic environments such as Jena (http://jena.apache.org/) or
Cwm (http://www.w3.org/2000/10/swap/doc/cwm) have
included logical rules to perform inferences on top of RDF
datasets. Logical rules and Description Logics are two
orthogonal decidable fragments of first-order logics that
have been extensively studied in knowledge representa-
tion and in deductive databases. The interest of logical
rules (a la Datalog) is that they are easy to read and
write for practitioners and they have a polynomial data
complexity while allowing expressing complex interaction
between properties and recursivity.
When application software provides capabilities to dis-
play/select 3D graphic entities, its interactive behavior
becomes a mandatory feature to manage the graphic enti-
ties attached to the digital objects taking part to this
application. In such software, selection functions have
been under focus to provide users with efficient means
to reach the 3D content they are looking for. The most
common approaches for multiple object selection include
serial selection techniques that require the user to select
objects one at a time, e.g. the ubiquitous ctrl + click (or
shift + click) approach, and parallel selection techniques
such as brushes, lassos, and selection shapes. However, as
Lucas et al. [22] point out, each has certain limitations,
especially in 3D. For instance, multiple objects may be dif-
ficult to distinguish, isolate, or even see due to occlusion,
rendering size, environment clutter, and other display fac-
tors. Requiring the user to adjust the view can be tedious,
cumbersome, and even burdensome, especially when the
number of objects to select is high, and may still fail to
make certain objects accessible. This is especially true for
display scenes with anatomical entities.
Systems commonly address this issue with an indirect
selection technique, that is, by allowing the user to specify
Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 3 of 13
http://www.jbiomedsem.com/content/5/1/20
the desired selection using an alternate representation
such as a model tree or component list. Some systems
allow selection by common attribute or provide a more
general selection query or search [23]. Such indirect selec-
tion techniques are useful, but are generally abstract and
less intuitive than direct manipulation techniques. They
can become cumbersome if the user has to browse a very
large amount of entities, which is the case of anatomical
entities of the human body.
In the case of man-made objects, recent advances
have been made in this field using additional functional
information attached to the components of large digi-
tal assemblies [24] using an ontology-based approach.
This additional information provides the user with effi-
cient means to select/process groups of components that
would be otherwise tedious and error prone to identify
[25]. Other interactive approaches, like Oh et al. [26]
propose group selection with a dynamically computed
hierarchy based on the notion of gravitational proxim-
ity. Such approaches are less appropriate for rigid and
exact specification of selections, particularly when objects
or components are frequently or always in contact with
or intersecting each other, e.g. when managing sets of
anatomical entities. If man-made objects can take advan-
tage of their modeling process to rely on concepts like
geometric constraints [27] and simple spatial structures
like repetitive placement of objects along lines or circles
[28], such structures do not exist for anatomical enti-
ties. It is therefore difficult to rely on spatial structures to
display/select 3D anatomical entities.
Our approach for supporting efficient navigation and
selection of objects in 3D scenes of human body anatomy
is to make explicit the anatomic and functional semantics
of 3D objects composing a complex 3D scene through a
symbolic and formal representation that can be queried
on demand.
In this context, our contribution is twofold:
 First, we address the lack of a formal description of
human body functions and we provide a new
ontology, called My Corporis Fabrica (MyCF),
containing the following items:
 a taxonomy of anatomical functions conform
to the ICF terminology;
 a taxonomy of anatomical structures based on
FMA;
 and relations between them and with 3D
models, that make explicit how anatomical
structures are composed, how they contribute
to functions, and also how they can be related
to 3D complex objects describing
patient-specific body parts, declared as
instances of appropriate mesh 3D models
used for simulation or 3D rendering;
 Second, we equip MyCF with automatic reasoning
capabilities that enable model checking and complex
queries answering, and we show the added-value of
such a declarative approach for interactive simulation
and visualization as well as for teaching applications.
In particular, we provide new visualization/selection
capabilities to manage and browse 3D anatomical
entities based on the querying capabilities
incorporated in MyCF.
Results and discussion
MyCF is an ontology-based tool for automatic reasoning
and querying on complex anatomical models. The core of
MyCF is a comprehensive anatomical ontology, the nov-
elty of which is to make explicit the links between anatom-
ical entities, human body functions, and 3D graphic mod-
els of patient-specific body parts. It is equipped with
inference-based query answering capabilities that are par-
ticularly interesting for different purposes such as:
 automatic verification of the anatomical validity of
3D models. Indeed, it is important to select the
correct set of anatomical entities that contributes to a
simulation, e.g. a simulation of movements where the
correct bones, muscles, ligaments, . . . , are required
to set up all the 3D and mechanical simulation
parameters. These requirements are very close to the
selection requirements described in the Background
section. They can be regarded as equivalent to a
selection operator;
 automatic selection and display of anatomical entities
within a 3D scene. Anatomical entities can vary
largely in size, can be very close to each other or even
hidden by other anatomical entities. The use of
geometric means to select useful sets of entities is not
suited whereas inference-based queries using human
body functions can provide much more suited means.
Such selection capabilities are particular relevant for
diagnosis for instance;
 training students on anatomical entities participating
to a certain body function. Here again, this purpose is
close to that of selection functions where the
connection between function and anatomical entities
provides new means to browse and highlight features
of anatomical structures accessible in 3D.
The first version of MyCF has been published in 2009
[29]. This version was limited to anatomical entities. The
current version of MyCF has been widely improved by
adding (i) new anatomical entities with more details than
FMA for some body parts, (ii) almost 4000 human body
functions, and (iii) the set of classes related to 3D mod-
els. The current version of the ontology contains almost
74000 classes and relations as well as 11 rules stored
Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 4 of 13
http://www.jbiomedsem.com/content/5/1/20
in a deductive RDF triple-store using a Sesame server,
and that can be queried with a remote-access facility via
a web server [30]. The ontology can be easily updated,
just by entering or deleting triples and/or by modifying
the set of rules, without having to change the reasoning
algorithmic machinery used for answering queries. It is
the strength of a declarative approach that allows a fine-
grained domain-specific modeling and the exploitation of
the result by a generic (domain-independent) reasoning
algorithm.
MyCF features three distinct taxonomies linked by rela-
tions and rules:
 Anatomical entities, such as knee, shoulder, and
hand, denote parts of the human body, and give a
formal description of canonical anatomy;
 Functional entities, such as gait, breath, and stability,
denote the functions of the human body, and are the
fundamental knowledge to explain the role of each
anatomical entity;
 Finally, 3D scenes with entities such as 3D-object,
3D-scene define the content required to get 3D views
of patient-specific anatomical entities described by
3D graphical models related to anatomical entities.
Figure 1 shows the top classes of the three taxonomies
as they are displayed by the Protégé editor. We now in
turn describe each of the taxonomies ( of anatomical enti-
ties, anatomical functions, and 3D objects respectively),
the relations existing within and between them and the
inference rules on which reasoning is performed.
The taxonomy of anatomical entities
The taxonomy of anatomical entities of MyCF contains
69000 classes at the moment. It is inherited from the
Foundational Model of Anatomy (FMA) ontology, in the
sense that we have extracted from FMA a lot of terms
that we have incorporated into MyCF. The correspon-
dences between terms of the two ontologies are defined
by means of the owl:sameAs relation. For example, to say
that mcf:Femur corresponds to fma:Femur, we use the
triple ?mcf:Femur, owl:sameAs, fma:Femur?. However, we
have skipped the top levels of the FMA taxonomy, that
we have judged too general for our needs. For example, we
skipped the FMA classes Physical anatomical entity and
Non-physical anatomical entity declared in FMA as sub-
classes of the top FMA class Anatomical entity. We also
skipped the two FMA classes Material anatomical entity
and Immaterial anatomical entity appearing in FMA as
subclasses of Physical anatomical entity, and the two FMA
classes Postnatal anatomical structure and Developmen-
tal anatomical structure appearing as subclasses of the
FMA subclass Anatomical structure ofMaterial anatomi-
cal entity.
On the other hand, we have added to MyCF anatomi-
cal classes that are not present in FMA but relevant for
biomechanical simulations and for 3D visualization such
as tendons and other anatomical entities, especially for
free limbs. Figure 2 illustrates the level of detail with which
a musculature is described in MyCF.
We have also introduced the possibility of making
explicit the left and right specializations of anatomical
entities. Many anatomical entities indeed can be special-
ized into two symmetric anatomical entities, representing
its left and right version. For instance, the entity knee is
specialized into the left knee and the right knee. Distin-
guishing the right and left versions of a given anatomical
structure may be very important, for instance when link-
ing anatomy with 3D models for simulation purposes.
This provides new and convenient means of interacting
Figure 1 Protégé display of the top classes of the three MyCF taxonomies. The three top classes of MyCF are 3D entity, Anatomical entity
(same as FMA) and Functional entity.
Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 5 of 13
http://www.jbiomedsem.com/content/5/1/20
Figure 2 Description of musculature in MyCF anatomical taxonomy (an extract about tendons of sartorius).Muscles in MyCF are subdivided
into different parts. A particular focused has been made on tendons that play important roles in biomedical simulations.
with a 3D scene containing graphical entities that can be
easily accessed, e.g. displaying the left knee. Our solution
to this specialization in a semi-automatic and systematic
way is the following one:
 we add two new relations mcf:leftSubClassOf and
mcf:rightSubClassOf, that we declare as
specializations of the rdfs:subClassOf relation
between anatomical entities using rules that will be
detailed at the section The taxonomy of anatomical
entities. These relations are not topological, in the
sense that they do not aim at describing the absolute
or relative displacement of an anatomical entity with
respect to another, like the ones proposed in [8].
They aim at capturing the specialization of an
anatomical concept, like the knee, in its left and right
declensions;
 for every anatomical entity for which we want to
declare its left and right specialization, e.g.
knee_joint, we introduce two new names of classes,
e.g. Left_knee_joint and Right_knee_joint, that we
declare respectively as mcf:leftSubClassOf and
mcf:rightSubClassOf of the anatomical entity. This is
done by adding two RDF triples. For instance, in the
case of the knee joint we have:
? Left_knee_joint mcf:leftSubClassOf knee_joint ?,
? Right_knee_joint mcf:rightSubClassOf knee_joint ?;
 we iteratively replicate all links, expressed as RDF
triples, between an anatomical entity and one of its
subclasses through the left and right specializations.
For instance, in Figure 3, we report the result of these
operations for the subtree of the anatomical
taxonomy rooted in knee_joint.
Classifying entities with these new properties brings
the following advantages: by querying on SubClassOf
property, one can navigate the anatomical entities regard-
less of left/right parts, thus obtaining a more succinct
representation of the data; by querying on leftSubClas-
sOf and rightSubClassOf properties, one can navigate the
complete taxonomy of the left and right specializations of
an anatomical entity, if it is needed. Finally, we can eas-
ily relate a 3D object to the corresponding left or right
specializations of anatomical entities, which is efficient to
provide new means of selecting anatomical entities using
the left and right concepts that are relevant in many 3D
applications and biomechanical simulations.
In addition to the generic rdfs:subClassOf relation
and its subproperties mcf:rightSubClassOf and mcf:
rightSubClassOf that are the basis of the tree structure
of the anatomical taxonomy, we have introduced the
mcf:PartOf and mcf:InsertOn domain-specific relations.
Then, we have declared in the form of RDF triples and
rules that will be explained as knowledge on how anatom-
ical entities are related by these two properties:
 The property mcf:PartOf is used to make explicit the
subparts of anatomical entities, which is an important
anatomical knowledge. For example, a joint is a part
of the articular system (but joint is not a subclass of
an articular system), is declared by adding to the
ontology data base the RDF triple:
? mcf:Joint mcf:PartOf mcf:Articular_System ?.
Note that, like for FME (the explorer of FMA), the
mcf:PartOf relation can be chosen for defining the
tree structure through which the user wants to
visualize the anatomical entities in 3D, as an
alternative to the tree structure defined by the
rdfs:subClassOf relation.
 The property mcf:InsertOn is used to specify attach
points of anatomical entities. This knowledge is
important in anatomy and also for biomechanical
simulation purposes. For instance, the distal tendon
of right sartorius is inserted on the Medial part of
proximal epiphysis of right tibia, is expressed by
adding the RDF triple:
? mcf:Distal_Tendon_Of_Right_Sartorius mcf:InsertOn
mcf:Medial_part_of_proximal_epiphysis_of_right_tibia ?.
In the current version of MyCF, there are 4000 RDF
triples involving the property mcf:PartOf, and 850 RDF
triples involving the propertymcf:InsertOn.
Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 6 of 13
http://www.jbiomedsem.com/content/5/1/20
mcf:Knee_joints
mcf:Left_knee_joint mcf:Tibiofemoral_joint mcf:Patellofemoral_joint mcf:Right_knee_joint
mcf:Left_tibiofemoral_
joint
mcf:Right_Tibiofemoral_
joint
mcf:Left_patellofemoral_
joint
mcf:Right_Patellofemoral_
joint
mcf:Knee_joints
mcf:Tibiofemoral_joint mcf:Patellofemoral_joint mcf:Right_knee_joint
mcf:Right_Tibiofemoral_
joint
mcf:Left_knee_joint
mcf:Left_tibiofemoral_
joint
mcf:Left_patellofemoral_
joint
mcf:Right_Patellofemoral_
joint
(a)
(b)
Figure 3 Example of left and right structures of the knee joint. Left-right structure of the knee-joint subclasses. (a) FMA taxonomy and (b)MyCF
taxonomy. The novel propertiesmcf:LeftSubClassOf andmcf:RightSubClassOf are drawn in red and blue, respectively.
The taxonomy of anatomical functions
The taxonomy of anatomical functions of MyCF is the
true added-value of MyCF that distinguishes it from the
state-of-the-art anatomical ontologies. It contains 4000
classes at the moment, most of the terms used to denote
them come from the ICF terminology. The anatomical
functions are structured using two relations:
 the generic rdfs:subClassOf relation between
functions. For instance, the extension of the knee is a
subclass of the simple movement function, is
expressed by the RDF triple:
? mcf:Extension_Of_The_Knee rdfs:subClassOf
mcf:Simple_Movement ?
 and the domain-specific relation mcf:IsInvolvedIn
which plays a role analogous to that of the partOf
relation between anatomical entities. For example,
the eversion of the foot is involved in the mobility of
ankle joints, is expressed by adding the RDF triple:
? mcf:Eversion_Of_The_Foot mcf:IsInvolvedIn
mcf:Mobility_Of_Ankle_Joints ?
Notice that the eversion of the foot is not a subclass
of the mobility of ankle joints.
In the current version of MyCF, there are 4000 RDF
triples specifying rdfs:subClassOf relations between func-
tions, and 1300 RDF specifying mcf:IsInvolvedIn relations
between functions.
The real added-value ofMyCF is to link the taxonomy of
functions with the anatomical taxonomy to make explicit
the functional roles of anatomical entities. Exploiting the
relationships between anatomical and functional entities
is decisive to retrieve the entities participating to some
functions, and vice-versa. In particular, this is crucial for
medical diagnosis and it is also of key importance to be
able to display/select interactively 3D geometric entities.
To address this issue, we have introduced two domain-
specific relations, mcf:hasFunction and mcf:contributesTo
to describe how an anatomical entity contributes to a
given function. The former is used to denote that an
anatomical entity, as a whole, realizes a given function.
The latter is used to denote that an anatomical entity
simply contributes to the realization of a given function,
but taken alone it may not be sufficient to execute this
function.
 The relation mcf:hasFunction relates an anatomical
entity with the function(s) that it realizes. For
instance, we can declare that the function knee
movement is performed by the knee by the following
RDF triple:
? mcf:Knee mcf:hasFunction mcf:Knee_Movement ?
Similarly, we can make explicit the functions of
ensuring sliding motion of articular surface and
ensuring transmission and amortization of charges
Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 7 of 13
http://www.jbiomedsem.com/content/5/1/20
of joint cartilages by the two following RDF
triples:
? mcf:Joint_Cartilage mcf:hasFunction
mcf:Ensure_Sliding_Motion_Of_Articular_Surface ?
? mcf:Joint_Cartilage mcf:hasFunction mcf:Ensure_
Transmission_And_Amortization_Of_Charges ?
 The relation mcf:contributesTo is a weaker relation
than the relation mcf:hasFunction, which allows to
specify that a given anatomical entity contributes to
the realization of a given (set of) function(s). For instance,
despite the fact that the toe does not have as function
the body stability, the toe contributes to the body
stability. This can be expressed by the RDF triple:
? mcf:Toe mcf:contributesTo mcf:Body_Stability ?
Note that, as it will be explained in the next section,
this triple can be declared or inferred by rules. We
will also show how we express by a rule that the
relation mcf:hasFunction is stronger than the relation
mcf:contributesTo, which enables to infer that any
anatomical enity that is declared as having as
function a given anatomical function, contributes to
this function a fortiori.
In the current version of MyCF, there are 700 RDF
triples specifying mcf:hasFunction between anatomical
entities and anatomical functions, and 500 RDF specifying
a mcf:contributesTo relation between anatomical entities
and anatomical functions.
The taxonomy of 3D objects
The taxonomy of 3D objects of MyCF is simple but
mandatory to connect the anatomical entities and their
functions to graphic entities used to interact with these
3D objects. It also illustrates well the declarative way
to connect additional knowledge for different purposes
to a given ontology. Here, we want to connect to the
anatomical ontology (patient-specific) to 3D geometric
models displaying a body part, so that the different 3D
objects contained in the scene are related to the anatomi-
cal entities they describe, thus providing the user with new
means to select/display these entities using the knowl-
edge embedded in the taxonomies of anatomical entities
and anatomical functions as well as their relationships.
This is a new scheme to avoid the selection/display of
these entities using purely geometry-based approaches.
The proposed taxonomy aims at defining the smallest
content enabling elementary tasks to display/select 3D
objects though this can be enriched to refer to geometric
criteria for these tasks that would add other entities in this
taxonomy. Our approach for doing so consists in:
 Designing a taxonomy of geometric objects (shown in
Figure 4) made of two classes respectively called
mcf:3D-scene and mcf:3D-object, a relation called
mcf:Contains having the class mcf:3D-model as
domain and the class mcf:3D-object as range, and
four relations respectively called mcf:Position,
mcf:hasMesh, mcf:hasTexture and mcf:hasColour
respectively, in order to possibly relate each specific
3D-object to a position matrix, a mesh file, a texture
file, and a color,
 linking this 3D geometry taxonomy to the anatomical
taxonomy through two relations called respectively
mcf:Describes relating instances of the class mcf:3D-
object to instances of the class fma:Anatomical
Entity, and mcf:Displays relating instances of the
class mcf:3D-scene to instances of fma:Anatomical
Entity or of mcf:Anatomical Function.
 declaring each new patient-specific 3D model that we
acquire as an instance of the class 3D-scene, by a
mcf:3D_entity
rdfs:subClassOf
mcf:3D_scene mcf:3D_object
mcf:object_2
rdf:type rdf:type
mcf:contains
mcf:contains
rdfs:subClassOf
"texture.jpg"
"mesh.obj"
(255,255,0)
rdf:type
mcf:texture
mcf:mesh
mcf:color
mcf:scene_1 mcf:object_3
Figure 4 3D taxonomy in MyCF. 3D taxonomy of MyCF is basic with only three classes. The individual, for instance, called object_3 is an
mcf:3D_object that has a geometry (obj file) and a texture (jpg file) allowing a 3D visualization and interaction.
Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 8 of 13
http://www.jbiomedsem.com/content/5/1/20
RDF triple: ? mcf:id rdf:type 3D-scene ? where mcf:id
denotes an identifier (e.g., an URI) where the file model-
ing the 3D-scene is stored, and stating which body part
or function it displays by an RDF triple, for instance:
? mcf:id mcf:Displays mcf:Knee ?
 identifying all the 3D-objects segmented within the
3D-scene and corresponding to anatomical entities as
instances of the class 3D-object, for which a number
of RDF triples are declared to specify that they
identify 3D-objects contained in the 3D-scene from
which they have been extracted, and that they
describe the corresponding anatomical entity.
For instance, the 3D-scene displayed in Figure 5, stored
in a file identified by mcf:id, in which the different
coloured 3D-objects corresponding to muscles and bones
have been extracted by segmentation, would be described
in myCF by the following RDF triples:
? mcf:id rdf:type mcf:3D-scene ? ? mcf:id mcf:Displays mcf:Leg ?
? mcf:id mcf:Contains mcf:id1 ? ? mcf:id1 rdf:type mcf:3D-object ?
? mcf:id mcf:Contains mcf:id2 ? ? mcf:id2 rdf:type mcf:3D-object ?
? mcf:id mcf:Contains mcf:id3 ? ? mcf:id3 rdf:type mcf:3D-object ?
....
? mcf:id1 mcf:Describes mcf:Left_sartorius ?
? mcf:id1 mcf:hasMesh ¨ ..\geometries\l_sartorius.obj¨ ?
? mcf:id2 mcf:Describes mcf:Left_bicepsfemoris ?
? mcf:id2 mcf:hasMesh ¨ ..\geometries\l_bicepsfemoris.obj¨ ?
? mcf:id3 mcf:Describes mcf:Left_semimembranosus ?
?mcf:id2 mcf:hasMesh¨ ..\geometries\l_semimembranosus.obj¨ ?
.....
Figure 5 Example of a 3D scene containing complex 3D
anatomical models. 3D-model of the proximal part of the left lower
limb. Only the left sartorius is pointed out.
Figure 6 summarizes the structure of MyCF ontology
made of its three taxonomies interrelated by relations.
The inference rules
The inference rules of MyCF express complex connec-
tions between relations. They allow the ontology designer
to declare part of his/her domain knowledge in the form
of abstract rules. These rules capture in a very compact
way implicit facts that can be made explicit on demand
or at query time by an inference mechanism. This mecha-
nism is automatic and consists in applying the rules on the
explicit facts declared and stored as RDF triples, in all the
possible manners satisfying the conditions of these rules.
For each possible instantiation of the variables (denoted
by a name starting by ?) appearing in the condition part
of a given rule such that all its conditions are satisfied by
explicit facts, the new facts corresponding to the (appro-
priately instantiated) conclusion of the rule are added.
This saturation process is iterated as long as new facts can
be produced. The termination is guaranteed by the form
of the rules that are considered. They correspond to safe
rules, also called Datalog rules: all the variables appearing
in the conclusion of a rule also appears in the condition
part. This contrasts with description logics axioms or with
Datalog+? rules [31] in which we can infer that there exists
(unknown) individuals verifying a given property.
The rules that are considered in the current version of
MyCF are the following ones. It is important to note how-
ever that adding, removing or modifying a rule is very
simple and does not impact the inference mechanism that
remains unchanged as long as the rules added that are safe
ones.
The three following rules express the transitivity of the
generic relation rdfs:subClassOf, as well as of the domain-
specific relations mcf:PartOf between anatomical enti-
ties and mcf:IsInvolvedIn between anatomical functions,
respectively.
IF? ?a rdfs:subClassOf ?c ?AND? ?c rdfs:subClassOf ?b ?
THEN? ?a rdfs:subClassOf ?b ? (R1)
IF? ?a mcf:PartOf ?c ?AND? ?c mcf:PartOf ?b ?
THEN? ?a mcf:PartOf ?b ? (R2)
IF? ?a mcf:IsInvolvedIn ?c ?AND? ?c mcf:IsInvolvedIn ?b ?
THEN? ?a mcf:IsInvolvedIn ?b ? (R3)
The three following rules express specializations of rela-
tions: mcf:LeftSubClassOf and mcf:RightSubClassOf are
both two specializations of rdfs:subClassOf ; and the rela-
tion mcf:hasFunction (between an anatomical entity and
an anatomical function) is more specific (i.e., more pre-
cise) than the relation mcf:contributesTo (between an
anatomical entity and an anatomical function).
Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 9 of 13
http://www.jbiomedsem.com/content/5/1/20
mcf:3D_entity
rdfs:subClassOf
mcf:3D_scene mcf:3D_object
mcf:object_1 mcf:object_2mcf:scene_1
rdf:type rdf:type rdf:type
mcf:contains
mcf:contains
mcf:Anatomical_entity
rdfs:subClassOf
mcf:Muscle
mcf:Sartorius
rdfs:subClassOf
rdfs:subClassOf
mcf:describes
mcf:Functional_entity
rdfs:subClassOf
mcf:Simple_mouvement_
of_knee_joints
mcf:Flexion_of_knee_joint
rdfs:subClassOf
mcf:Gait
rdfs:subClassOf mcf:isInvolvedIn
mcf:participatesTo
mcf:displays
Figure 6 The general structure of MyCF ontology (extract). The three taxonomies of MyCF are interconnected allowing a high level of
knowledge expression.
IF? ?a mcf:LeftSubClassOf ?b ?THEN? ?a rdfs:subClassOf ?b ?
(R4)
IF? ?a mcf:RightSubClassOf ?b ?THEN? ?a rdfs:subClassOf ?b ?
(R5)
IF? ?a mcf:hasFunction ?b ?THEN? ?a mcf:contributesTo ?b ?
(R6)
Finally, the following rules express connections that
hold in the domain of anatomy between the relations
rdfs:subClassOf and mcf:InsertOn, rdfs:subClassOf and
mcf:IsInvolvedIn, rdfs:subClassOf andmcf:contributesTo,
mcf:contributesTo and mcf:IsInvolvedIn, mcf:PartOf and
mcf:InsertOn respectively.
For example, the first rule says that if a given class
representing an anatomical entity ?a (e.g., Sartorius) is a
subclass of an anatomical entity ?c (e.g., Muscle) that is
known to be inserted on an anatomical entity ?b (e.g.,
Bone), then ?a is inserted on ?b (Sartorius inserts on a
Bone).
IF? ?a rdfs:subClassOf ?c ?AND? ?c mcf:InsertOn ?b ?
THEN? ?a mcf:InsertOn ?b ? (R7)
IF? ?a mcf:IsInvolvedIn ?c ?AND? ?c rdfs:subClassOf ?b ?
THEN? ?a mcf:IsInvolvedIn ?b ? (R8)
IF? ?a mcf:contributesTo ?c ?AND? ?c rdfs:subClassOf ?b ?
THEN? ?a mcf:contributesTo ?b ? (R9)
IF? ?a mcf:contributesTo ?c ?AND? ?c mcf:IsInvolvedIn ?b ?
THEN? ?a mcf:contributesTo ?b ? (R10)
IF? ?a mcf:InsertOn ?c ?AND? ?c mcf:PartOf ?b ?
THEN? ?a mcf:InsertOn ?b ? (R11)
The point is that we can easily add rules crossing the
anatomy domain and the 3D domain, to express, for
instance, conventional colors associated with the visual-
ization of some organs (such as bones, muscles, and so
on). The following rule expresses that the conventional
color for visualizing bones in anatomy is yellow:
IF? ?x rdf:type mcf:3D-object ?AND? ?x mcf:Describes ?y ?
AND? ?y rdfs:subClassOf mcf:Bone ?
THEN? ?x mcf:hasColour yellow ? (R12)
Querying: illustration by example
In the Figure 7, we illustrate a complete example from
query to 3D visualization. Data are presented as a graph
with corresponding RDF triples on the bottom. The query
is explained in English and translated in SPARQL. The
answers are used to select and highlight corresponding 3D
models in the 3D scene.
Conclusions
We have described MyCF with a particular emphasis
on its ontology structure, showing how the FMA ontol-
ogy can be used as basis of the anatomical description
of human bodies and empowered with a taxonomy of
anatomical functions conforming to the ICF terminology.
We have introduced new concepts that are particularly
useful for checking the anatomical validity of 3D models
containing multiple anatomical entities, also for select-
ing sets of anatomical entities on the basis of functions
rather than being bound to geometric approaches that are
not efficient enough to process complex 3D geometric
configurations.
These high level functionalities can be achieved thanks
to the combination of different types of knowledge related
to anatomy. The reasoning capabilities brought by the
inference rules increase the power of realizing complex
tasks by reducing them to querying a knowledge base
implemented as a deductive database.
The main interest of this approach is its declarativ-
ity that makes possible for domain experts to enrich the
knowledge base at any moment through simple editors
without having to change the algorithmic machinery.
Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 10 of 13
http://www.jbiomedsem.com/content/5/1/20
Figure 7 Example of querying about anatomy of the sartorius. The graph on the left is a visual representation of data. The query about left
sartorius, translated in SPARQL, gives the bones on which the tendons of sartorius are inserted. In the final 3D scene the sartorius is showed alone
and the corresponding bones are highlighted in yellow.
This provides MyCF software environment a flexibil-
ity to process and add semantics on purpose for various
applications that incorporate not only symbolic informa-
tion but also 3D geometric models representing anatomi-
cal entities as well as other symbolic information like the
anatomical functions.
The MyCF ontology is at the heart of the the MyCF
Browser: a tool for exploring anatomical 3D models [32].
Further work will address the use of this environment
to feed a bio-simulation engine with the appropri-
ate anatomical entities so that mechanical simulations
can be easily set up and extract the required geo-
metric information from the 3D models of anatomical
entities.
Methods
Through the presentation of MyCF, we develop a novel
and promising vision of ontologies equipped with infer-
ence algorithms, that enables a declarative assembly of
different models to obtain composed models guaranteed
to be anatomically valid while capturing the complexity of
human anatomy.
Methodology overview of the design of MyCF ontology
We have designed a unifying representation framework
to combine several types of structured knowledge about
anatomy.
For the types of anatomical knowledge for which ontolo-
gies or terminologies exist, our approach is to enrich
them while remaining conform to them. The descriptions
in MyCFs ontology of the anatomical concepts and the
human body functions are thus conform respectively to
the Foundational Model of Anatomy (FMA) [2] and to
the International Classification of Functioning, Disabil-
ity and Health (ICF) [11]. In fact, MyCFs ontology both
enriches and links together two standard taxonomies that
have been developed separately and independently.
For incorporating 3D models in MyCF ontology, in
order to follow a unifying approach, we have chosen to
define a taxonomy of 3D scenes and 3D objects, and
to relate it to the taxonomies of anatomical entities and
functions through relations.
One particularity of MyCF is to use (generic and spe-
cific) relations both to structure each taxonomy but also to
establish bridges between them. Inference rules are used
to express how relations interact.
Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 11 of 13
http://www.jbiomedsem.com/content/5/1/20
We give now some details on our methodological
choices both for incorporating 3D models and inference
rules.
3Dmodels
We want to be able to incorporate different types of 3D
models. Some 3D models can describe patient-specific
body parts acquired by CT (Computerized Tomography)
or MRI (Magnetic Resonance imaging) scans. In this
case, the 3D models are obtained by reconstruction using
classic surface modeling techniques. The 3D models used
to illustrate this article are based on the Zygote human
anatomy collections [33]. The resulting 3D models are
mesh-based files associated with position matrices and
texture files for 3D view rendering. The storage and the
processing of the files describing the 3D objects are spe-
cially time and memory consuming. Our approach is to
disconnect the identification of these files from their stor-
age and processing, and to connect them to the ontology
through their identifiers: each files identifier is declared
as an instance of a 3D scene capturing an anatomical
structure, a body part, or a human body function in the
ontology. By segmentation, the 3D scene is decomposed
into components that are in turn declared as instances of
3D objects describing the anatomical entities declared in
the ontology as parts of the given anatomical structure.
Inference rules
We have chosen the formalism of rules to express
properties of relations (such as transitivity) but also
properties or constraints between domain-specific rela-
tions. For instance, the following rule involving two
domain-specific relations (ContributesTo and IsIn-
volvedIn) expresses that any anatomical entity ?C
participating to a function ?F that is involved in a function
?F ? contributes to this function ?F ? too:
IF? ?C mcf:ContributesTo ?F ?AND? ?F mcf:IsInvolvedIn ?F ?
THEN? ?C mcf:ContributesTo ?F ?
Such a rule is a compact formula that enables to infer
as many instantiated facts as there exist pairs of facts sat-
isfying its conditions. For example, using this rule, we
can infer that the muscle sartorius (but also the biceps
femorus muscle) contributes to the function of move-
ment of knee from the facts that sartorius (but also the
biceps femorus) contributes to the function knee flex-
ion and that the function knee flexion is involved in the
functionmovement of knee. Similarly, by using the same
rule, we can infer that the different muscles (such as Ten-
sor fascia lata, Rectus femoris, Vastus lateralis, Vastus
medialis, and Vastus intermedius) contributing to the
function knee extension contribute too to themovement
of knee since knee extension is involved in the move-
ment of knee. This is a simple but powerful piece of
knowledge that can guide diagnosis by iteratively identify-
ing the anatomical entities to check in case of dysfunction
of the movement of a knee. It can also help setting up an
appropriate 3D scene or a biomechanical simulation. In
Figure 8 Architecture of the MyCF environment. Overview of the architecture of MyCF.
Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 12 of 13
http://www.jbiomedsem.com/content/5/1/20
both cases, the interest is to select the anatomical enti-
ties relevant to display them and meet the users needs
or to select the anatomical entities to simulate a knee
movement. Rules can be very useful too for guiding image
segmentation or image registration in medical imaging.
For instance, a rule stating that every sinovial joint has
an articular capsule can guide automatic segmentation of
patient-specific images.
Semantic technologies used for building and exploiting
MyCF
In order to make it easy to connect MyCF to the Linked
Data cloud [34], we have followed the recommendations
of W3C and we have chosen the RDF(S) language for
expressing MyCF ontology.
RDF [35] is a standard notation recommended by the
W3C for the semantic Web composed of Web data and
(simple) ontologies. RDF (Resource Description Frame-
work) provides a simple language for describing annota-
tions about Web resources identified by URIs. An RDF
fact consists of a triple made of a subject, a predicate and
an object. It expresses a relationship denoted by the pred-
icate between the subject and the object. In a triple, the
subject, but also the predicate, are URIs pointing to Web
resources, whereas the object may be either a URI or a lit-
eral representing a value. RDFS is the schema language for
RDF. It allows specifying a number of useful constraints
on the individuals and relationships used in RDF triples.
In particular, it allows declaring objects and subjects as
instances of certain classes. In addition, inclusion state-
ments between classes and properties make it possible to
express semantic relations between classes and between
properties. Finally, it is also possible to semantically relate
the domain and the range of a property to some classes.
The point is that these constraints can be written in triple
notation, i.e., RDFS statements can be written using RDF
as a notation. Therefore, a RDF data store can contain in
the same format triples expressing that a given acquisi-
tion file (identified by a given URL u) is an instance of an
anatomical structure (for instance the patella), and triples
describing knowledge known in the domain of anatomy
about this structure (for instance that the patella is a
circular-triangular bone, and that it is part of the knee):
? u rdf:type mcf:Patella ?
? mcf:Patella rdfs:subClassOf mcf:CircularTriangularBone ?
? mcf:Patella mcf:PartOf mcf:Knee ?
As ontology editors, we have used Protégé [36] and Top-
Braid Composer [37]. Protégé is supported by a strong
community of developers and academic, government
and corporate users. The Protégé open source platform
supports modeling ontologies in a variety of formats via a
web client or a desktop client. TopBraid is a commercial
tool specifically designed for RDF, which is also available
as free version.
Finally, we have chosen to store and process the result-
ing ontology as a deductive RDF triple-store using a
Sesame server. Sesame [38] is a de-facto standard frame-
work for processing RDF data. This includes parsing,
storing, inferencing and querying of/over such data. It
offers an easy-to-use API that can be connected to all
leading RDF storage solutions. Sesame fully supports the
SPARQL [39] query language for expressive querying and
offers transparent access to remote RDF repositories using
the exact same API as for local access. However Sesame
currently has no built-in support for custom inference
rules. Therefore, we had to implement a rule engine on
top of it in order to enable sound and complete deduc-
tive capabilities. This architecture is of course modular
and adjustable. For instance, it is possible to change the
triple-store server, or to use an external reasoner support-
ing Datalog rules for saturating the data. Figure 8 sketches
the general architecture of the MyCF environment.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
OP conceived the original idea, contributed to the models (3D models and
ontologies), participated to the software development of a preliminary version
of My Corporis Fabrica, and supervised this project. FU contributed to the
ontological modeling and to the design of the overall architecture,
implemented the rule engine, and is the main software developer. VF is the
main developer of the taxonomy of anatomical functions. JCL contributed to
the models (3D models and ontologies). MCR contributed to the ontological
modeling and to the design of the overall architecture, and co-supervised this
project. All authors read and approved the final manuscript.
Acknowledgements
This work has been partially supported by the LabEx PERSYVAL-Lab
(ANR-11-LABX-0025-01) and by the project PAGODA (ANR-12-JS02-007-01).
Author details
1Department of Anatomy, LADAF, Université Joseph Fourier, Grenoble, France.
2LJK (CNRS-UJF-INPG-UPMF), INRIA, Université de Grenoble, Grenoble, France.
3LIG (CNRS-UJF-INPG-UPMF), Université de Grenoble, Grenoble, France.
Received: 2 January 2014 Accepted: 23 April 2014
Published: 6 May 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Dahdul et al. Journal of Biomedical Semantics 2014, 5:34
http://www.jbiomedsem.com/content/5/1/34RESEARCH Open AccessNose to tail, roots to shoots: spatial descriptors
for phenotypic diversity in the Biological Spatial
Ontology
Wasila M Dahdul1,2*, Hong Cui3, Paula M Mabee1, Christopher J Mungall4, David Osumi-Sutherland5,
Ramona L Walls6 and Melissa A Haendel7Abstract
Background: Spatial terminology is used in anatomy to indicate precise, relative positions of structures in an
organism. While these terms are often standardized within specific fields of biology, they can differ dramatically
across taxa. Such differences in usage can impair our ability to unambiguously refer to anatomical position when
comparing anatomy or phenotypes across species. We developed the Biological Spatial Ontology (BSPO) to
standardize the description of spatial and topological relationships across taxa to enable the discovery of
comparable phenotypes.
Results: BSPO currently contains 146 classes and 58 relations representing anatomical axes, gradients, regions,
planes, sides, and surfaces. These concepts can be used at multiple biological scales and in a diversity of taxa,
including plants, animals and fungi. The BSPO is used to provide a source of anatomical location descriptors for
logically defining anatomical entity classes in anatomy ontologies. Spatial reasoning is further enhanced in anatomy
ontologies by integrating spatial relations such as dorsal_to into class descriptions (e.g., dorsolateral placode
dorsal_to some epibranchial placode).
Conclusions: The BSPO is currently used by projects that require standardized anatomical descriptors for
phenotype annotation and ontology integration across a diversity of taxa. Anatomical location classes are also
useful for describing phenotypic differences, such as morphological variation in position of structures resulting from
evolution within and across species.
Keywords: Anatomy, Spatial relationships, Position, Axes, Reasoning, BSPO, Ontology, PhenotypeBackground
Variation among anatomical phenotypes, whether across
species or between mutant and wildtype model organisms,
frequently involves changes in position and orientation of
structures. Among fish species, for example, the position
of the mouth may be ventral, dorsal, or terminal; bony
vertebral processes may be oriented laterally or medially;
pelvic fins may be located posteriorly or anteriorly relative
to the abdomen. Computation across phenotypes thus
requires a vocabulary of positional terms to understand
the patterns of variation in the positioning of structures* Correspondence: wasila.dahdul@usd.edu
1Department of Biology, University of South Dakota, Vermillion, SD, USA
2National Evolutionary Synthesis Center, Durham, NC, USA
Full list of author information is available at the end of the article
© 2014 Dahdul et al.; licensee BioMed Central
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.relative to others within and between organisms, and to
understand the possible relationships to gene expression
and regulation. Positional terms have long been used in
anatomy to describe the spatial aspects of the impressive
diversity of organismal forms of both plants and animals.
For example, positions in animals are often described in
relation to those of a bilaterally symmetrical animal
(Figure 1). Accordingly, the primary or main axis is con-
sidered the anterior-posterior (AP) axis, which extends
longitudinally from head to tail. The dorsal-ventral (DV)
axis is recognized in that ventral typically faces toward,
and dorsal away, from a substrate (meaning towards the
ground for land-dwelling organisms or towards the ocean
or river/lake bottom for marine or aquatic organisms),
whereas the left-right (LR) axis is defined in relation to aLtd. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
oral
aboral
dorsalventral
right
anterior
posterior
left
rostral
caudal
anterior posterior
dorsal
ventral
right
left
anterior posterior
A B
C
Figure 1 Comparison of primary organismal axes designated in a diversity of species and their representation in BSPO. In fishes
(A) and in humans (B), anterior-posterior axis (narrow synonym rostral-caudal axis in humans) is shown in red, dorsal-ventral axis (narrow
synonym anterior-posterior axis in humans) shown in blue, and left-right axis shown in yellow. A cnidarian (sea anemone) (C) is bilaterally
symmetrical and has an oral-aboral axis, shown in orange.
Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 2 of 13
http://www.jbiomedsem.com/content/5/1/34plane running along the anterior-posterior midline. We
created the Biological Spatial Ontology (BSPO) to develop,
define, and standardize terms that can be used to describe
spatial and topological relationships, at multiple biological
scales from cells to whole organisms, and across diverse
taxa.
In the past two decades the developmental and genetic
underpinnings of positional axes have been investigated
for model species, and highly conserved key patterning
molecules have been identified across widely divergent
taxa. Overlapping patterns of Hox gene expression, for
example, are required for organization along the AP body
axis in bilaterian animals [1]. Wnt/?-catenin expression
has also been shown to determine primary body axis
orientation in both bilaterian and non-bilaterian animals
[2]. The DV axis is patterned by the chordinbone
morphogenetic protein (BMP) network and is conserved
across organisms as diverse as flies and humans (reviewed
in [3]). Nodal signaling has been shown to control LR
symmetry, which also appears to have an ancient prebila-
terian origin [4]. Within plants, homeobox genes, such as
knotted-like homeobox (knox), also play a central role in
spatial developmental patterns [5]. Although specification
of organismal axes may appear straightforward with respectto their application within model organisms (e.g., Arabi-
dopsis, Caenorhabdites elegans, Drosophila, Danio rerio,
Xenopus, mouse, etc.), there are taxon-specific differ-
ences in the application of spatial terms, such as to
human anatomy, that render the development of a
universal terminology complicated. Moreover, there are
fundamental differences across the more than 35 animal
body plans (e.g., tapeworms, sea urchins) and various
plant growth forms (e.g., tree, shrub, herb, and thallus)
that present some very difficult axes to interpret. Despite
these challenges, which we describe further below, the
development of a set of spatial classes is necessary for
query and description of phenotypes across species.
Here we describe the development of the BSPO, which
contains 146 classes and 58 relations representing
anatomical axes, gradients, regions, sections, sides, and
surfaces that apply to whole organisms and their parts.
The BSPO is integrated with other ontologies and is
currently used by projects that require standardized
spatial descriptors for anatomy ontologies, ontology
integration, and phenotype annotation. For example,
the free-text description anterodorsal margin of oper-
cle can be represented formally as BSPO:anterodorsal
margin part_of some opercle (the latter class from an
Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 3 of 13
http://www.jbiomedsem.com/content/5/1/34anatomy ontology). Because it is driven by research
needs, the spatial terminology currently represented in
BSPO is particularly developed for animals and, to a lesser
extent, plants. However, BSPO is organized in a frame-
work that is flexible enough to incorporate spatial termin-
ology for other taxa (e.g., fungi).
Results and discussion
Ontology organization and content
Classesa in BSPO represent various aspects of spatial
organization and are partitioned into categories for ana-
tomical axes (14 classes), anatomical surfaces (12 classes),
anatomical regions (81 classes, including margins and
sides), anatomical gradients (6 classes), and anatomical
planes (7 classes) (Figure 2). BSPO classes for anatomical
compartment and anatomical compartment boundary
(11 classes) refer to anatomical structures defined by
lineage restriction [6] rather than by axial position, and
thus these classes will be moved to the Common Anatomy
Reference Ontology (CARO) [7] in the future. Individual
compartments and their boundaries are typically named
with respect to some axis. For example, most of the imagi-
nal discs and embryonic segments of insects are bisected
by a boundary running medial to lateral that cells do not
cross during development [8]. The regions of the disc
or segment anterior and posterior to the boundary are
referred to as anterior and posterior compartments,
respectively, while the boundary is referred to as the
anterior-posterior compartment boundary. BSPO pro-
vides relationships that allow these compartments andanterior-posterior axis
an
is_a
anterior side
starts_axis
posterior side
finishes_axis
anterior_to
posterior_to
opposite_to
is_a
anatomical side
is_a
anterior margin
overlaps
anatomical margin
is_a
anatomical region
is_a
anterior region
overlaps
is_a
surface_
anatomical 
passes
anatomical gradient
dorsal-vent
approximately_
perpendicular_to
anterior-posterior gradient
has_axis
is_ais_a
anatomical structure
is_a
material anatomical entity
is_a
is_a
is_a
Figure 2 Organization of high-level spatial classes in BSPO and some
(pink fill), anatomical axis (blue fill), anatomical plane (purple fill), and ana
white fill. Subclass (is_a) relations are shown in black and spatial relations inboundaries to be defined with respect to anatomical
axes, in individual cases, but general classes such as
anterior compartment seem of dubious usefulness and
so will not be maintained in either BSPO or CARO.
BSPO classes are linked by a rich set of 58 relationship
types. In addition to their logical relationships, all BSPO
classes have text definitions that are written as broadly
as possible to encompass taxonomic variability in body
form. Synonyms are included where applicable and include
commonly used abbreviations for terms such as LR axis
for left-right axis.
BSPO is open to all users and freely available in OBO
and OWL formats at http://purl.obolibrary.org/obo/
bspo.{ obo,owl }. BSPO can also be browsed online at
http://www.ontobee.org/browser/index.php?o=BSPO.
Anatomical axes
Primary organismal axes
Axes form the basis of the BSPO, with other concepts,
such as relations and planes, defined in terms of these
axes. In animals, three whole body axes are generally
applicable (described below). Unlike the case in animals,
there is generally no single primary organismal axis for a
whole plant. Instead, axes are described for one or more
modular organs that compose a plant, such as shoots
(stems and branches), roots, and phyllomes (leaves,
petals, etc.) (see Axes of organism parts, below).
In animals, the AP, DV, and LR axes (Figures 1A, B)
are applicable to Bilateria and most of their descendants.
The Bilateria include all metazoans except the spongesatomical axis
anterior surface
of
anatomical surface
is_a
entity
_through
immaterial anatomical entity
is_a
anatomical plane
sagittal plane
is_a
midsagittal plane
is_a
ral axis
is_a
orthogonal_to
left-right axis
is_a
is_a is_a
is_a
anatomical boundary
is_ais_a
anatomical line
apical-basal axis 
relative to substrate
is_a
apical-basal axis 
relative to direction of growth
is_a
of their children. Anatomical region (green fill), anatomical gradient
tomical surface (yellow fill). Parent classes from CARO are shown with
orange.
mouth anus
Figure 3 An individual zooid of the colonial ectoproct Bugula.
This species possesses a U-shaped gut and the location of the anus
is adjacent to the mouth. Image based on illustration from the
BIODIDAC image library.
Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 4 of 13
http://www.jbiomedsem.com/content/5/1/34(Porifera), placozoans, cnidarians, and ctenophores. Most
sponges are asymmetrical as adults, although an AP axis
has been identified in their larvae [9]. Cnidarians are
primitively bilaterally symmetrical [10], with radial or
biradially symmetrical axes developing in more derived
members of the clade (Figure 1C). Genes regulating
bilaterian head development are expressed in the sea
anemone at the larval aboral pole, indicating that the
anterior, head-forming, region of bilaterians and the
aboral region of cnidarians may have been derived
from the same domain of their last common ancestor
[11]. Bilateral symmetry of the body, including an anterior
head with an oral opening and a posteriorly extended
trunk/tail with an anal opening, is thought to characterize
the common ancestor of Bilateria. Many textbook defini-
tions of the three fundamental axes of bilaterians (AP, DV,
LR) reference structures such as head, oral opening/
mouth, anus, tail or gut that are not present in all
larval or adult bilaterians. Our definitions for these axes
also reference anatomical structures, but aim to use only
the minimum that are those hypothesized to be present
based on phylogenetic reconstruction of the ancestral
bilaterian [12].
In defining anterior-posterior axis in BSPO, we designate
anterior as the end of the animal with a head. Interest-
ingly, a head, however defined (e.g., based on concentration
of neurons [11], sensory structures, oral opening), has
been lost multiple times in development and evolution
(e.g., adult tunicates, echinoderms, bivalve molluscs,
ectoprocts, endoprocts), and as such the AP axis is hard
or impossible to identify in these taxa. Although the oral
opening/mouth is used as a proxy for an anterior end,
it, as well, has been lost multiple times in various taxa
(acanthocephalans, pogonophorans) or moved posteriorly
in others (flatworms such as planarians) [13]. On the other
end, criteria for recognizing posterior are conventionally
related to an anal opening at or near to the terminus of
the body. However, given multiple independent losses
of an anal opening (e.g., gnathostomulids, some echi-
noderms) and the many taxa with a U-shaped gut in
which the anus is adjacent to the mouth (e.g., sipunculids,
ectoprocts, entoprocts, some gastropods) (Figure 3), using
a digestive tract as a proxy for the longitudinal axis (AP)
of the body is problematic. Interestingly, the U-shaped gut
in some taxa is AP regionalized using highly conserved
transcription factors [14]. As Minelli [15] points out, many
taxa, such as those with a U-shaped gut, demonstrate
dissociation between an apparently evident elongate AP
somatic body axis and a very different visceral axis. In
fact, dramatic metamorphic development of many inverte-
brates renders body axes very difficult to interpret (e.g.,
[16]). Even in taxa with distinct head and tail ends, modifi-
cations in body form can result in unconventional applica-
tion of AP axis terminology. For example, seahorses, withtheir distinct upright posture, orient their AP axis
perpendicular relative to the substrate rather than parallel
[17]. Differential application of axes to the body and its
parts is necessary in cases where they have been dissoci-
ated in development or evolution.
Developmental and evolutionary changes to the DV axis
likewise pose challenges for simple application of termin-
ology. In BSPO, dorsal-ventral axis is defined as An axis
that is approximately perpendicular to the anterior-
posterior axis and that extends through the horizontal
plane of the body. An inversion of the DV axis occurred
during evolution resulting in correspondence between the
ventral side of arthropods and the dorsal side of verte-
brates, as evidenced by phenotype (position of the neural
cord/tube) and inversion of the Chordin/BMP/Tolloid
pathway markers [18].
Another challenge in the application of the terminology
of the fundamental bilaterian axes (AP, DV) is that these
axes are uniquely conflated in humans and other anthro-
poid apes that are bipedal. In humans, superior is applied
Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 5 of 13
http://www.jbiomedsem.com/content/5/1/34to the head end (anterior) and inferior towards the feet.
Anterior and posterior are applied to the human front
(ventral) and back (dorsal), respectively (see also Axes of
organism parts, below). We have added human-specific
terminology as synonyms to BSPO (Figure 1B) to assist in
unambiguous reference to anatomical position when
comparing anatomy or phenotypes across species.
In BSPO, the left-right axis is defined as An axis that
extends through an organism from left to right sides of
body, through a sagittal plane, and it is orthogonal_to
sagittal plane. The LR axis of many organisms is also
modified in development and evolution. For example,
flatfishes (order Pleuronectiformes) undergo a dramatic
developmental change in the LR body axis. In ontogeny,
the left or right side of the body comes in contact with
the substrate, and one eye migrates to the other half of
the head. Thus one side (left or right depending on the
species) has two eyes and the side in contact with the
substrate is referred to as "eyeless" or "blind". Description
of this modified anatomy requires specialized terms to
refer to the blind side and eyed side of the fish. Other
structures typically located along the midsagittal plane,
such as the dorsal fin, are displaced horizontally. Reason-
ing across flatfish and unmodified vertebrate eye morph-
ologies may thus require specifying the spatial location
(left or right side) of the blind or eyed side of the
organism.
Despite the modifications to the primary axes we
describe above, larvae and adults of many animal taxa
do in fact retain the ancestral bilaterian AP, DV, and LR
axes, and many conserved molecular and genetic deter-
minants of these axes have been described in model or-
ganisms. Few of the non-model taxa with the interesting
deviations from symmetry described above have been
investigated from a developmental or genetic standpoint,
and thus much remains to be discovered and understood
about axis specification.
Several other primary organism axes are represented
in BSPO. The medial-external axis extends from an
internal point towards the outside of the body or body
part. This class is a superclass of medial-lateral axis
(ML) and medial-radial axis (MR). In animals, the ML
axis applies to the left or right sides of a bilaterally
symmetrical animal. The oral-aboral axis (Figure 1C)
is defined as the axis that extends from the oral open-
ing to the furthest point in an organism that is directly
opposite. It is the major axis in cnidarians, cteno-
phores, and echinoderms. During development, an
animal-vegetal axis (AV) is defined for most animal
eggs, where the yolky (less rapidly dividing) end is
vegetal and the less yolky (more rapidly dividing) end
is animal. These terms are also often applied to the
poles (e.g., the animal pole) and the hemispheres (e.g.,
the animal hemisphere).Axes of organism parts
In plants, as mentioned above, the axes primarily relate
to organismal parts and are generally defined relative to
the direction of growth. The main axis of growth is typic-
ally the apical-basal (AB) axis, which is determined by the
growth of an apical meristem or apical cell. In BSPO, we
refer to this axis as apical-basal axis relative to direction
of growth to distinguish it from the apical-basal axis rela-
tive to substrate (described below), which is applied to
animal bodies. In Figure 4A, which shows a seedling of a
vascular plant, an AB axis suggests a single straight line
running through the center of the plant, from the tip of
the root to the tip of the shoot apical meristem. However,
even in this very simple plant, there are two AB axes, one
for the shoot system and one for the root. Most plants
have more complex, branching growth forms with
multiple AB axes. While one might describe the
abstract, overall shape of a plant (e.g., an ellipsoid, cube,
or pyramid) and define axes for that shape, those axes
would not necessarily relate to the actual axes along which
the plant develops. The AB axis for a whole plant can be
used only with the simplest of growth forms, such as a
non-branching liverwort or fern thallus. In plants with
secondary growth (that is, growth that thickens axial or-
gans such as stems), the medial-radial (MR) axis extends
from the center of the organ to the outside. The DV and
ML axes for a whole plant are also used with thalloid
growth forms (whether branching or not), because they
grow roughly in a plane along the surface of the substrate
(Figure 4B).
In animals, the apical-basal axis relative to substrate
is often applied to substrate-bound organisms such as
Porifera, where the basal direction is towards the sub-
strate. For bilaterian animals, this axis often refers to cell
or tissue-level axes where one portion of the cell or tissue
is adjacent to a substrate, such as a basal lamina or lamina
propria, and the apical portion faces a lumen, for example
an intestinal epithelial cell with its microvilli facing the
lumen of the intestine.
In animals and plants, a proximal-distal axis (PD) is
used to describe the position of parts in relation to
attachment to another part, such that parts closer to the
plane of attachment (e.g., the point where a leaf attaches
to a branch) are proximal and those further away are
distal (Figure 4A). In animals, the terms proximal and
distal are often applied to outgrowths of the body, such
as limbs and other appendages such as antennae, para-
podia, and feathers. In animals the regulatory gene
distal-less has a role in specifying the PD axis, and it is
expressed in the distal portion of many appendages
[19]. Proximal-distal terminology can also be applied
across different levels of anatomical organization to
organs, tissues, and cells; for example, the proximal/distal
epiphysis of femur, the proximal/distal collecting tubule
basal
apical
apical
proximal
distal
adaxial
abaxial
dorsal
ventral
superior
inferior
anterior
posterior
posterior
rostral
caudal
caudal
A
D
E
dorsal
anterior posterior
anterior
anterior
posterior
posterior
C
ventral
dorsal
ventral
basal
apical
lateral
lateral
B
medial
basal
apical
proximal
distal
Figure 4 Axes applied to organism parts. In vascular (A) and non-vascular plants (B), the apical-basal axis relative to direction of growth
(purple) runs in the direction of apical growth, in both shoots and roots. For lateral organs such as branches or leaves (A), the primary axis is the
proximal-distal axis (green) and the adaxial-abaxial axis (pink). In plants or organisms with a thalloid growth form (B), the apical-basal axis
relative to direction of growth often runs parallel to the substrate, resulting in a dorsal-ventral axis that runs perpendicular to the substrate and
a medial-lateral axis that is perpendicular to the apical-basal axis. C) Hippocampal pyramidal neuron, showing the application of the BSPO
classes apical-basal axis relative to substrate and proximal-distal axis to the whole cell or portions thereof. D) AP axes for the head, neck and
trunk of the giraffe. Note that these axis definitions delineate a bent version of the primary AP axis. E) AP axis of the human brain (double-headed
red arrow) relative to the AP axis of the body (single red arrow). Note the use of superior and inferior to refer to structures relative to the substrate.
Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 6 of 13
http://www.jbiomedsem.com/content/5/1/34of the kidney, or the distal/proximal apical dendrite of a
neuron (Figure 4C).
In plants, proximal and distal should be applied to
organs or organ parts that do not develop from an apical
meristem (and therefore have no AB axis) such as vascu-
lar leaves, leaflets, petals, or sepals. The PD axis can also
be used for organs with an apical meristem that branch
from another organ, such as branches or lateral roots,
but in these examples it is redundant with the AB axis.
The adaxial-abaxial axis (AA) is also important for
leaves and other types of phyllomes, with adaxial being
adjacent to the shoot axis (usually the top of the leaf )
and abaxial being away from the shoot axis (usually the
bottom of the leaf ). If a leaf or other organ is held hori-
zontally, the adaxial-abaxial axis may be described asdorsal-ventral. The distribution of tissues varies along
the AA axis in leaves, including characteristics of each
surface, reflecting the different microclimates on the
adaxial versus the abaxial sides of the leaf, such as sun
exposure and humidity.
Medial-external axes are also applied to parts of an
organism. The medial-radial axis in plants is used to
describe organs or organ parts that are roughly circular
in cross-section, such as stems, roots, and petioles,
while medial-lateral axis is used to describe laminar
(flattened) plant parts such as many leaves and petals
or some shoot axes (e.g., cactus paddles) that expand
through growth of marginal meristems [20]. Variation
in the development of meristems along the AA, PD, or
ML axes results in much of the variation found in leaf
Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 7 of 13
http://www.jbiomedsem.com/content/5/1/34shapes. For example, some leaves that have a rounded
cross-section, such as some species of Sanseveria, have
an early developmental pattern in which growth of
either the abaxial or adaxial leaf meristem is suppressed,
to the effect that the opposite meristem (adaxial or abax-
ial, respectively) grows around to cover the entire surface
of the leaf.
In cases where the application of axis terminology is
difficult, molecular determinants may be used as evidence
for spatial reference of body parts. For example, the region
of the fin or limb bud in vertebrates with a high concen-
tration of sonic hedgehog (Shh) is posterior because
Shh posteriorizes the phenotype [21]. Note that this
can apply to either portions of a given body axis, or to
structures that are not themselves part of a main body
axis (see also Anatomical gradients below).
Axis terminology applied to substructures of an animal
requires reference to a main axis of the body, such as
anterior or posterior, and sometimes the ancestral
condition of the body. For example, in the giraffe
(Figure 4D), the AP axis is applied to several body
segments (head, neck, trunk) and the DV axis is desig-
nated as perpendicular to the AP axis for each of these
segments. As a result, the DV axis of the neck is nearly
parallel to the AP axis of the trunk. Similarly, for humans
and other bipedal anthropoids, the application of an
organismal head or brain axis is uniquely conflated with
the primary axis of the organism. In this case, the AP axis
(often called rostral-caudal) of the human brain is at
almost a right angle to the AP axis of the rest of the body
(Figure 4E).
The traditional use of superior and inferior refers
to parts that are the furthest or nearest to the substrate
respectively. In BSPO, we define inferior side and
superior side classes to support reference to the sub-
strate. However, confusion can arise when these terms are
applied to homologous structures across species where
they may not retain the same relationship to the substrate.
For example, the human superior vena cava is further
from the substrate than the inferior vena cava, but in the
mouse, these terms no longer reference differential distance
from the substrate. For this reason, we do not recommend
their use in defining axes or relations to axes, for structures
that are likely to be compared across taxa.
The BSPO does not yet have a complete terminology
for describing the spatial dimensions of fungal anatomy,
which could be integrated with existing anatomy ontol-
ogies for fungi (Fungal Subcellular Ontology [22] and
Fungal Anatomy Ontology (FAO; http://purl.obolibrary.
org/obo/fao.owl)). Nonetheless, BSPO can easily accom-
modate the spatial terminology used to describe fungi,
and some existing BSPO terms are applicable to fungal
anatomy. For example, lateral is used in fungi, as in
animals, to refer to the side of the organism [23], andthe medial-radial axis can be used to describe cylin-
drical structures in fungi such as the stem or stalk of a
mushroom. The dorsal-ventral axis and medial-lateral
axis used to describe thalloid plant structures (Figure 4B)
could easily be applied to thalloid lichens. The terms
adaxial and abaxial are used in fungi, similar to their
application in plants, to describe the side of an anatomical
structure that is adjacent to or away from the long axis of
another structure. While the abaxial-adaxial axis can
be used fairly generally to describe multiple types of
organs in plants, within fungi, adaxial and abaxial
are restricted to describing the sides of basidiospores
in relation to the basidium, a specialized cell or organ
in the basidiomycetes [23].
Relations along anatomical axes
Fifty-eight relations have been specified for use with
BPSO terms. For each axis in BSPO we define a pair of
relations specifying relative position along the axis. For
example, for the DV axis we have the relations dorsal_to
and its inverse ventral_to. An entity x is dorsal_to an
entity y if x is further along the DV axis than y towards
the dorsum. Each of these relations is also declared to
be transitive (i.e., if x is dorsal_to y, and y is dorsal_to z,
then x is dorsal_to z). We also define non-transitive
versions of these relations, e.g., immediately_dorsal_to
and immediately_ventral_to as subproperties of the tran-
sitive forms. These are useful for specifying the order of
serially arranged, contiguous structures such as the tag-
mata and segments of an arthropod body, the segments of
an arthropod leg, or internodes of a plant stem.
Additional challenges in the application of anatomical axes
Although the designation of the primary organism axes
may appear straightforward, pronounced developmental
and evolutionary changes in organ presence, morphology,
and symmetry in many taxonomic groups have made
these axes biologically difficult to interpret and thus made
it correspondingly difficult to apply a standardized termin-
ology. The evolutionary shift to pentaradial symmetry in
the adults of extant echinoderms, starfish, brittlestars, sea
urchins, sand dollars, and crinoids is one of the most
spectacular examples. All echinoderm larvae are bilaterally
symmetrical, but upon metamorphosis, little or no trace
of the larval AP axis remains in the pentaradial adult [24].
Whether there are five AP axes, one central AP axis, or
none at all is still under molecular and genetic investi-
gation. Similarly, the bilaterally symmetrical swimming
larvae of tunicates, with their characteristic chordate
features including pharyngeal arches and a post-anal
tail, metamorphose into sedentary sac-like adults with
no apparent remnant of an AP axis. The tapeworm lacks a
clear AP axis: adults lack a digestive tract (no mouth or
anus) and neither end contains a concentration of neurons
transverse plane
radial
plane
tangential
plane
transverse plane
 midsagittal plane
horizontal
plane
A
B
Figure 5 Anatomical planes in BSPO. A) The three anatomical
planes used to describe bilaterally symmetrical organisms are midsagittal
plane (blue), horizontal plane (red), and transverse plane (purple).
B) Anatomical planes used to describe wood (secondary xylem)
anatomy. A transverse plane (purple), or cross-section, is perpendicular
to the apical-basal axis relative to direction of growth in an axial organ
or to a proximal-distal axis in a lateral organ. A radial plane (green)
follows the two dimensions specified by an apical-basal axis relative to
direction of growth and a medial-lateral axis. A tangential plane
(orange) is perpendicular to a radial plane.
Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 8 of 13
http://www.jbiomedsem.com/content/5/1/34that might be considered cephalic. Although the end with
the holdfast organ (scolex) is commonly considered
anterior, evidence including the manner of development
of new segments and the positioning of testes relative to
ovaries within segments points to the opposite conclusion
[15]. Biologically meaningful application of anatomical
position terms requires further molecular and genetic
understanding of the development of these taxa.
Within plants, axis specification across species is fairly
straightforward because of the association between axes
and developmental patterns (i.e., the apical-basal axis
relative to direction of growth is associated with apical
growth and the medial-radial axis is associated with
radial growth). Nonetheless, unusual developmental
patterns, such as the adaxialization of cylindrical leaves
(described above under Axes of organism parts) can
obscure the normal axes used to describe plant structures.
Thus, it is the precise specification of spatial terminology
that allows for logical comparisons among forms that
deviate from the norm.
Anatomical planes and sections
Anatomical investigation is frequently based on histological
sections (i.e., anatomical planes) to support a better under-
standing of three-dimensional structure. For example, long
before the days of computerized image reconstruction,
anatomists leveraged coronal, horizontal, sagittal,
and parasagittal tissue sections to support inferred
three-dimensional representation of anatomical entities
within an animal. These are evident in numerous landmark
atlases such as The Rat Brain in Stereotaxic Coordinates
by Paxinos [25] and Kaufmans The Atlas of Mouse
Development [26]. Even in more modern digital ap-
proaches, reconstruction can happen only if the two-
dimensional axes are accurately specified and registered
(for examples, see [27]).
Anatomical planes (Figure 5) are defined as perpen-
dicular or parallel to an axis in BSPO. For example,
sagittal plane is defined as Anatomical plane that
divides a bilateral body into left and right parts, not
necessarily of even size and has relationships orthogo-
nal_to left-right axis (Figure 2) and parallel_to anterior-
posterior axis and dorsal-ventral axis. The use of BSPO
can aid integration and error-checking of section-based
views through coordinates related to BSPO axes, based on
the logic within the ontology. For example, if a histological
feature is annotated to a particular structure that has in
turn been declared to be located on the left side of the
organism, a right parasagittal section should not include
such a structure.
Traditional plant anatomy refers to three planes:
transverse plane (or cross-section), radial plane, and
tangential plane (Figure 5B). The transverse plane is
used for plant parts that are both round or flattened incross-section, such as stems or leaves, whereas radial
plane and tangential plane are generally used only with
structures that are roughly round in cross-section. All
three planes are essential for the characterization and
identification of wood (secondary xylem found generally
in plant axes such as stems and roots), as woody tissues
appear different in each plane [28].
The Foundational Model of Anatomy (FMA) ontology
[29] for humans has an extensive classification of planes.
These include horizontal anatomical plane, Frankfurt
plane, and thoraco-abdominal plane. The FMA uses
these planes to demarcate the boundaries of organism
subdivisions such as the thorax. We cross-reference
FMA classes where they exist in the representation of
planes in BSPO but focus on planes that are widely
applicable across organisms.
In addition to the relations along anatomical axes de-
scribed above, we specify a number of relations relative
to anatomical planes. For example, relative to the sagittal
plane, ipsilateral_to holds between two structures on the
same side of an organism; contralateral_to holds between
Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 9 of 13
http://www.jbiomedsem.com/content/5/1/34two structures on the opposite sides of an organism. In
contrast to the standard axial relations, these are not
transitive, but they do hold the characteristic of being
symmetric. If x is on the same side as y, then it must be
the case that y is on the same side as x.
Anatomical topology: regions, sides, and margins
Unlike axes and planes that are immaterial, the ana-
tomical topology classes in BSPO refer to the material
regions, sides, and margins of anatomical structures.
These are labeled and defined in BSPO relative to the
axis classes. For example, subtypes of anatomical region
(Figure 2) include anterior region, dorsal margin, and
posterior side. These classes can be used to spatially
define anatomical structures relative to an axis of the
whole organism. Variation in the topology of homologous
structures across species is informative for phylogenetic
inference. Examples include differences in surface fea-
tures, such as the textured or smooth surface of cranial
bones in catfishes [30], differences in the margins of skel-
etal elements, such as the dorsal margin of the ilium in
amniotes [31], and differences in the adaxial and abaxial
regions of a leaf (Figure 4A).
Anatomical sides are defined with the non-transitive
subproperties of part_of that specify which side of a
bisecting plane a structure is part of. Where these refer-
ence the axes of the whole organism, they can apply to a
side of either the whole organism or its substructures.
For example, in_left_side_of can apply to the position of
the heart relative to the whole organism, or apply to part
of the heart, such as its left side. Where the referenced
side only applies to some part of an organism, so do the
relations. For example, in the long bones of limbs that
have proximal and distal sides, the proximal epiphysis
of the femur can be defined as an epiphysis that is
in_proximal_side_of the femur. We also define property
chains to propagate information about sides down the
partonomy, so that, for example, if X part_of Y and Y
in_left_side_of heart then a reasoner can infer that X
in_left_side_of the heart.
Some structures are not completely on one side or the
other of a bisecting organismal plane but instead cross
it. For example, the heart may asymmetrically span the
midsagittal plane of an animal. For such cases, we define
the relation: intersects_midsagittal_plane_of. This relation
applies to midline structures such as the single unpaired
nostril of the hagfish, which is positioned along the
midline of its head (median external naris EquivalentTo
external naris and intersects_midsaggital_plane_of some
head). This relation does not imply that the structure is
unpaired, although this may often be the case. Structures
to which the intersects_midsaggital_plane_of does not
apply stand in a in_lateral_side_of relation to the whole.
For example, in most vertebrates, the naris (nostril) isbilaterally paired, and it is thus declared in UBERON (the
cross-species metazoan Uber Anatomy Ontology) [32,33]
as being in_lateral_side_of a head. This relation does not
imply, however, that the structure is paired. To indicate
whether a structure is paired or unpaired, classes such as
bilateral from the Phenotype and Trait Ontology (PATO)
[34] can be used, although further work needs to be done
to connect these PATO classes to BSPO.
Anatomical gradients
Anatomical gradients are defined in BSPO as Material
anatomical entity defined by change in the value of some
quantity per unit of distance across some spatial axis.
Note that these classes are defined as structures whereby
the differentiating characteristic is the distribution of
some factor across a gradient. For example, Sonic hedge-
hog (Shh) is expressed in a posterior to anterior gradient
in the developing limb buds of vertebrates. The concen-
tration of Shh is interpreted by the cells and influences
the phenotypic outcome of digit morphology according
to the gradient [21]. An anatomical gradient can also be
applicable to the whole organism, such as in the case of
early anterior specification by bicoid, a maternal effect
RNA that is translated in the fertilized egg and was
discovered in Drosophila melanogaster in the 1980s
(see [35] for review). Anatomical gradient subclasses
for some of the primary organismal axes are included
in BSPO, for example, anterior-posterior gradient,
which could be used to indicate the presence of the
bicoid morphogen in the example above.
BSPO and interoperability with other ontologies
The classes and relations in BSPO uniquely represent
the spatial aspects of anatomical entities and can be used
to create class expressions to enable spatial reasoning.
UBERON simplifies the specification of spatial patterns
in taxon-specific anatomy ontologies by doing this, e.g.,
UBERON:forelimb BSPO:anterior_to some UBERON:
hindlimb. Thus use of BSPO in UBERON can be lever-
aged to infer spatial relations by new or existing anatomy
ontologies without those relationships. For example, fore-
limb and hindlimb in the Xenopus Anatomy Ontology
(XAO) [36] reference the UBERON classes for forelimb
and hindlimb, and therefore it can be inferred that a
XAO:forelimb is anterior_to some XAO:hindlimb.
Pre-composition using BSPO classes can enhance the
definitions of some classes in anatomy ontologies that
refer to the spatial aspects of structures. For example,
the Plant Ontology [37,38], a unified vocabulary for all
green plants contains a class for phyllome base that is
defined as The basal part of a phyllome, where it attaches
to a shoot axis. Currently, only the relationship part_of
phyllome is specified in the ontology, but a more precise
logical definition could be created by specifying that a
Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 10 of 13
http://www.jbiomedsem.com/content/5/1/34phyllome base is a BSPO proximal region that is part_of
a phyllome. The Gene Ontology (GO) also contains
classes that could be defined in terms of BSPO classes
and relations. For example, GO:AP axis specification is
defined as The establishment, maintenance and elabor-
ation of the anterior-posterior axis. The anterior-posterior
axis is defined by a line that runs from the head or
mouth. This class could also be formally related to BSPO:
anterior-posterior axis.
Post-composition allows one to create classes that are
more granular than those available in an anatomy ontol-
ogy while avoiding the complexity and potential unwieldi-
ness to the ontology that may result from pre-composing
very specific classes [39]. Thus post-composing classes for
the regions, margins, and surfaces of structures needed
for annotation avoids creating a great number of pre-
composed classes. BSPO is used by the Phenoscape pro-
ject (Phenoscape.org; [40,41]) to create post-compositions
for the annotation of morphological variation within and
among vertebrate species resulting from evolution. For
example, the posterior location of a bony projection on
the cleithrumb (a shoulder girdle bone) is represented
by combining the following anatomical and spatial
classes: anatomical projection part_of some BSPO:-
posterior region and part_of some cleithrum. Spatial
classes are also used to specify the region of a structure
that varies in some quality; for example, BSPO:anterior
margin part_of some scapula is annotated as concave
or straight using quality classes from PATO. BSPO is
also used in post-composition for the annotation of
gene expression in ZFIN (zfin.org; [42]) and used to
formally represent taxonomic species descriptions for
wasps [43].
PATO is an ontology of biological qualities that con-
tains a number of relational qualities representing spatial
concepts. Formally there is a difference between these
spatial qualities and BSPO relations: PATO relational
qualities are classes (e.g., dorsal to) rather than relations
as in BSPO (e.g., dorsal_to). This difference manifests
itself in concrete ways when modeling the world using
languages such as the Web Ontology Language
(OWL). For example, A is dorsal to B (where A and B
are instances) is asserted as a simple triple < A dor-
sal_to B>. However, to refer to this dorsality relation-
ship (e.g., to say that A is more dorsal to B than it is to
C; or that this dorsality is caused by some genetic alter-
ation), the relationship must be turned into an individ-
ual, i.e., a relational quality (reified relation). These
spatial quality classes in PATO could be pre-composed
with the relevant BSPO class. For example, dorsalized
is defined as a bearer's gross morphology containing
only what are normally dorsal structures. This class
could be formally defined by relating it to the BSPO
class dorsal region.Use of BSPO for text mining
BSPO is useful at different levels for the natural
language processing of morphological descriptions. For
text mining software such as CharaParser [44], which
is being developed to assist biocurators in annotating
anatomical phenotypes, BSPO can be used at the lexical
level as a dictionary for identifying spatial classes in free
text descriptions. This most basic usage of the ontology
makes more complex uses possible.
After spatial classes are identified at the syntactic level,
BSPO is used to post-compose anatomical entities when
pre-composed classes from an anatomy ontology, such
as UBERON, are not available. For example, for the phrase
anterior margin of maxilla, CharaParser would propose
the expression BSPO:anterior margin and part_of some
UBERON:maxilla after it failed to find term variations
such as anterior margin of maxilla, maxilla anterior
margin, or maxillary anterior margin in UBERON.
Phrases such as anterior process of the maxilla are
handled similarly in that post-composition is consid-
ered only when pre-composed classes/components are
not found in ontologies. In this case, CharaParser would
propose the post-composition: UBERON: anatomical pro-
jection (synonym: process) and part_of (BSPO: anterior
region and part_of UBERON: maxilla), along with other
possible proposals.
Sometimes additional domain knowledge is needed to
annotate a phenotype that is not obviously spatially related.
For example, the semantics of the phenotype clavicle
blades articulate is built on the knowledge that clavicle
blades are bilaterally paired structures. The BSPO in_left_-
side_of and in_right_side_of relations (children of the
BSPO relation in_lateral_side_of) can be used to explicitly
define this type of structure. This makes it possible for
CharaParser to use the ELK reasoner [45] to find all struc-
tures that are bilaterally paired in UBERON by obtaining
the union of (BSPO:in_lateral_side_of some Thing) and
(part_of some BSPO:in_lateral_side_of some Thing).
When CharaParser processes qualities that are in the
relation_slim of PATO, such as articulated with, it will
understand that two entities are expected and then look
into a list of bilaterally paired structures for possible
matches for the two entities (i.e., clavicle blade and
(in_left_side_of some multi-cellular organism) and clavicle
blade and (in_right_side_of some multi-cellular organ-
ism)). Note that non-bilaterally paired structures can also
use PATO relational qualities (e.g., frontal PATO: articu-
lated with parietal).
Towards formalization of BSPO relations
The BSPO is represented in OWL, which provides a lim-
ited number of constructs for characterizing relationship
types. We make use of characteristics such as transitivity,
superproperties, and domain/range constraints to allow
Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 11 of 13
http://www.jbiomedsem.com/content/5/1/34for limited forms of reasoning. For example, if A is an-
terior to B, and B is anterior to C, then the transitivity
characteristic of anterior_to entails that A is anterior to C.
Similarly, we can also trivially infer that C is posterior_to
A, using inverse axioms. Other more sophisticated forms
of reasoning are not possible at this time. For example,
the orthogonal_to relation has domain and range con-
straints (it holds between an axis and a plane), but it has
no definitional axioms that capture the textual definition
of crossing a plane at a right angle. Spatial extensions
to OWL would be required to rigorously capture this
meaning, but it is not clear what the use case for these
advanced types of reasoning would be. One possibility
would be the integration of classic description logic
queries with geometric 3D model or anatomical atlas
data. For example, asking for all genes expressed in
epithelial cells dorsal to a plane formed by bisecting a
particular organ. One possibility is to extend OWL
using custom datatypes  this is possible using a system
such as OWL-Eu [46]. For many practical scenarios, it
may be sufficient to encode the logic of the relation
directly into the query engine. This is an area that would
require further exploration.
Conclusions
The BSPO supports unambiguous usage of positional
terminology in the context of anatomical data and in the
building of anatomy ontologies. BSPO also serves as a
source of classes and relations for post-composition of
anatomical entities, a requirement for the representation
of morphological variation within and among species.
To aid in its use, we include textual information indicating
the taxon-appropriateness of different classes and rela-
tionships in BSPO. In the future, we will also include
taxon constraints [47] and add additional constraints
encoded as OWL axioms.
The BSPO provides an ontological representation of
anatomical position classes that can be used for spatial
reasoning. For example, queries can be enabled to find
structures that are proximal to one another, or to compare
levels of phenotypic variation in dorsal vs. ventral regions.
Particularly in light of the high level of conservation in
gene pathways underlying these axes across species (e.g.,
BMP gradients in dorsal-ventral patterning), the BSPO is
critical to enable interesting queries across phenotypes at
different anatomical positions.
Methods
BSPO contains classes and relations (object properties in
OWL) for the representation of anatomical axes, gradients,
regions, planes, sides and surfaces (Figure 2). Spatial classes
are classified along a single subclass hierarchy with upper
level classes (e.g., material anatomical entity, immaterial
anatomical entity) imported from CARO. Coordination ofclasses with a new CARO release is ongoing, and we antici-
pate making a coincident new release of both ontologies
soon. Some relations (e.g., part_of) used in the BSPO are
defined in the Relations Ontology [48] and more specific
relations (e.g., posterior_to) are exclusively defined in
BSPO. The specific relations in the BSPO currently lack
higher-level parents in the Relations Ontology. Some rela-
tions in BSPO are used to relate anatomical region classes
to those of anatomical axis, such as anterior side which
has a starts_axis relationship to anterior-posterior axis
(Figure 2). Other commonly used relations in BSPO
include overlaps (e.g., anterior region overlaps anterior
side), and surface_of (e.g., anterior surface is a surface_of
anterior side). Note that we define relations textually but
we are unaware of a way to create a complete formal
definition using OWL, which has limited capabilities for
reasoning with relations.
The original version of BSPO was derived from the
FlyBase annotation qualifier section of the FlyBase
JOURNAL OF
BIOMEDICAL SEMANTICS
Yamagata et al. Journal of Biomedical Semantics 2014, 5:23
http://www.jbiomedsem.com/content/5/1/23RESEARCH Open AccessAn ontological modeling approach for abnormal
states and its application in the medical domain
Yuki Yamagata1*, Kouji Kozaki1, Takeshi Imai2, Kazuhiko Ohe2 and Riichiro Mizoguchi3Abstract
Background: Recently, exchanging data and information has become a significant challenge in medicine. Such
data include abnormal states. Establishing a unified representation framework of abnormal states can be a difficult
task because of the diverse and heterogeneous nature of these states. Furthermore, in the definition of diseases
found in several textbooks or dictionaries, abnormal states are not directly associated with the corresponding
quantitative values of clinical test data, making the processing of such data by computers difficult.
Results: We focused on abnormal states in the definition of diseases and proposed a unified form to describe an
abnormal state as a property, which can be decomposed into an attribute and a value in a qualitative representation.
We have developed a three-layer ontological model of abnormal states from the generic to disease-specific level. By
developing an is-a hierarchy and combining causal chains of diseases, 21,000 abnormal states from 6000 diseases have been
captured as generic causal relations and commonalities have been found among diseases across 13 medical departments.
Conclusions: Our results showed that our representation framework promotes interoperability and flexibility of the
quantitative raw data, qualitative information, and generic/conceptual knowledge of abnormal states. In addition, the
results showed that our ontological model have found commonalities in abnormal states among diseases across 13
medical departments.
Keywords: Ontology, Abnormal state, Disease, Property, Attribute, InteroperabilityBackground
With the development of newer technologies, data and
information exchange have been required for several
applications such as electronic health records (EHR) in
medicine. Such data and information include abnormal
states. However, abnormal states are difficult to share
because of their heterogeneity, caused by the variety of
grain sizes, from the level of cells, tissue, and organs to
that of the entire human body. This results in diverse
representations with little uniformity. BFO [1,2] and
DOLCE [3] have contributed to the formalization of the
quality description of entities. BFO provides E (Entity),
P (Property) (e.g., <Eye (E), red (P)>) and DOLCE provides
E (Entity), A (Attribute), V (Value) triple (e.g., <esophagus
(E), length (A), short (V)>). However, we found that there
are more complicated forms of quality representations
in medicine. For example, hypertension is a compound* Correspondence: yamagata@ei.sanken.osaka-u.ac.jp
Equal contributors
1ISIR, Osaka University, 8-1 Mihogaoka, Ibaraki, Osaka, Japan
Full list of author information is available at the end of the article
© 2014 Yamagata et al.; licensee BioMed Cent
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.concept, which has three elements: blood, pressure, and
high joined to form one concept/word. Another example
is hyperglycemia, composed of four concepts: blood,
glucose, concentration, and high. Furthermore, in the case
of intestinal polyposis, it is unclear whether intestine
or polyp should be considered as the entity.
This motivated us to establish a common framework
for the representation of abnormal states supported by
sound theories. In this study, we investigate the repre-
sentation of abnormal states from the content-oriented
view, which focuses on how to capture the content to be
represented, on the basis YAMATO [4].
YAMATO has been built to target both high utility
and philosophical soundness while maintaining compati-
bility with BFO and DOLCE. In brief, YAMATO has the
following characteristics:
a) Quality-related concepts (dependent continuant
entities) are divided into Property, Generic
quality, and Quality value. Quality in BFO is
identical to Property in YAMATO.ral Ltd. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 2 of 14
http://www.jbiomedsem.com/content/5/1/23b) Quality value is classified in a manner identical to
the classification in scales of measurement.
c) The context dependency of Ordinal value is
represented by using the theory of Role.
d) Multiple kinds of informational entities are
symbolically represented.
For the quality description, representations with
both EAV and EP formalisms are defined. Furthermore,
PATO2YAMATO aims to integrate phenotype >descriptions
that exist in different structured comparison contexts [5].
It allows (1) the classification of quality values, in which
scales of measurements are properly represented; (2) strict
modeling of the context dependency of ordinal values;
and (3) clear distinction between true values and mea-
sured data. It provides the mapping of ontology terms
of PATO [6] to YAMATOs framework and enables
the interoperability of the quality framework between
different top-level ontologies such as BFO and DOLCE. For
example, in the YAMATO framework, PATO:0000582
(increased weight) is defined as a Property that is a com-
bination of Generic quality (Attribute), weight, and a
context-dependent Quality value (Attribute Value), heavy.
The context-independent value is defined as a class
Weight quality value.
Another issue is that in several medical textbooks or
medical dictionaries, abnormal states in the definitions
of diseases have not been directly associated with the
corresponding quantitative values of clinical test data (e.g.,
ischemia in ischemic heart disease or muscular weak-
ness in muscular dystrophy), which makes their processing
by a computer difficult.
Furthermore, clinicians often deal with abnormal states
specific to each disease only in a particular medical division,
which makes it difficult to spread awareness regarding
the common nature of abnormal states. To address these
issues, we have been developing abnormality ontology for
the systematization of knowledge regarding abnormal
states, using ontological engineering, which represents a
unified framework [7]. We focus on abnormal states in
the definitions of diseases, which should be referred to
in several applications. In addition, we discuss the rep-
resentation of the various abnormal states on the basis
of ontological theories in a consistent manner.
Our claim in this study is not isolated to adopting one
of the representational forms used in the existing re-
sources. The aim of our work is to formalize and organize
different representations used in clinical medicine on the
basis of ontological theories, and to realize the interoper-
ability between them. It's not a simple matter of the use of
existing resources such as PATO, LOINC [8], and others.
Unified theoretical considerations make the various repre-
sentation forms interoperable, which enables the establish-
ment of a consistent and computer understandable modelfor abnormal states that are used in the definitions of
diseases and medical data.
In this study, we first define abnormal states and
explain our representation model. Then, we introduce our
ontology of abnormal states and demonstrate an applica-
tion of our work. We have constructed a disease ontology
and captured a disease as one or more causal chains of
the abnormal states in the human body [9]. Till date, clini-
cians have described the causal chains of approximately
21,000 abnormal states for approximately 6,000 diseases
across 13 medical departments. Thus, we believe that the
use of our ontology will contribute to various clinical
applications.
Results
Definition of abnormal states
In the human body, abnormal states are highly diverse
and involve various grain sizes, from the level of cells,
tissue, and organs to that of the whole organism. There-
fore, to systematize the knowledge about abnormal states,
it is important to clarify the essential characteristics of the
abnormal states, and to conceptualize them in a consistent
manner.
In this section, we focus on the abnormal states that
appear in the definition of diseases rather than in reality.
A state is modeled as a time-indexed propertya that is
associated with an entity, and has the value of an attri-
bute that changes with time [10]. For example, imagine
the state of hunger. It is represented by being hungry
or not at some time point in time. We define Property
as a characteristic that is inherent in an entity, having an
attribute along with its value, such as being red: <color,
red>. Properties are distinct from attribute values; for ex-
ample, the Property hypertension is differentiated from
an Attribute Value such as high as in blood pressure is
high. An Attribute Value has three subclasses: categorical
value (e.g., viviparous/oviparous), quantitative value (e.g.,
160 mmHg), and qualitative value (e.g., high/low, large/
small, much/few). On one hand, the Attribute Value high
can be used for several attributes such as temperature,
density, and velocity. On the other hand, the Property
hypertension cannot be used for the values of the above-
mentioned attributes and it has a set of attributes and
values like < pressure, high >.
In several textbooks and dictionaries, diseases have
been defined in terms of abnormal states. For example,
the definition of diabetes is Diabetes mellitus is charac-
terized by chronic hyperglycemia with disturbances of
[11]. Another disease myocardial ischemia is presented
as The term acute myocardial infarction should be used
when there is evidence of myocardial necrosis with acute
myocardial ischemia [12]. Therefore, we can say that a
disease can be defined in terms of an assertion about the
patient being in an abnormal state or not.
Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 3 of 14
http://www.jbiomedsem.com/content/5/1/23In the medical domain, various types of representa-
tions for abnormal states are used, and we conceptualize
these representations into three categories:
(1) Quantitative representation (e.g., blood pressure is
180 mmHg, blood glucose concentration is
135 mg/dL).
(2) Qualitative representation (e.g., blood pressure is
high, blood glucose concentration is high).
(3) Property representation (e.g., hypertension,
hyperglycemia).
Because the upper ontology YAMATO [4] has been care-
fully designed to cover the property, quality, and quantity
ontologies, it supports our work on abnormal states.
A quantitative representation is important for diagnosis
because a concrete value should be identified by clinical
examination for each patient. However, in the definition
of a disease, a property such as being hypertensive or
being hyperglycemic is essential instead of quantitative
data. Thus, as our basic policy, we first capture the
abnormal states as properties, represented by a tuple
like < Property (P), Property Value (Vp)>. The Property
Value takes a binary value, i.e., <true/false>. For example,
if the state stenosis exists, it is described as < stenosis,
true>. In addition, when necessary, a Degree Value (Vd)
can be used for describing the degree of the Property
Value, such as < stenosis, severe >.
Some readers may think that a property represented in
the above manner is extremely conceptual to be of prac-
tical use because of the lack of a representation, which
would give a more concrete meaning to data. Therefore,
we specify a property by decomposing it into a tuple: <At-
tribute (A), Attribute Value (V)>. The Attribute Value
can be either a Qualitative Value (Vql) or a Quantita-
tive Value (Vqt). For example, in a case of a qualitative
representation, stenosis (P) is decomposed into < cross
sectional area (A), small (Vql)>, and in another case of
a quantitative representation, stenosis (P) is described
as a concrete value, e.g., <cross sectional area (A),
5 mm2 (Vql)>. This approach contributes to promoting
consistency in representation, as well as the interoper-
ability between the quantitative raw data and the
generic/conceptual knowledge regarding abnormal states
(see after the section Interoperability between properties
and attributes).
In clinical medicine, decomposition of some prop-
erties cannot be achieved, because the precise mech-
anisms in the human body have not yet been completely
uncovered. For example, in the case of nausea, property
representation could be nondecomposable. Whether such
abnormal states represented in terms of properties defined
above can be decomposed into a known attribute and its
value will depend on advances in medicine.Representation of abnormal states
Basic representation
In this section, we introduce our representation model
for clinical abnormal states and show that we can appro-
priately represent them in a consistent manner.
Because an attribute cannot exist by itself but always
exists in association with an independent object, we need
to identify the object (hereinafter referred to as target
object). For example, in the case of gastrectasia, the
target object of its attribute volume is the stomach.
Accordingly, we introduce the Object to represent the
target object of the attribute and decompose the prop-
erty into a triple: <Object (O), Attribute (A), Attribute
Value (V)>. This is our basic representation model for
abnormalities. For example, gastrectasia is decomposed
into < stomach, volume, large > b (Table 1(a), row 1).
Extended representation
We recognize that some properties may be difficult to
decompose into the basic triple representation, such as a
ratio and what we call a meta-attribute, discussed below.
Accordingly, we introduce a Sub-object (SO) to repre-
sent a focused object (see next paragraph) as an extended
representation, so that a property can be decomposed into
a quadruple: <Object (O), Sub-Object (SO), Attribute (A),
Attribute Value (V) >.
In the case of a ratio, in addition to identifying the
target object with the ratio, it should represent for what
will be focused on (focused object). Therefore, we
introduce a Sub-Object (SO) to represent a focused ob-
ject. For example, the representation of hyperglycemia is
a quadruple, <blood (O), glucose (SO), concentration (A),
high (V)>, where the Object is blood and the Sub-Object
is glucose [Table 1(a), row 3].
There appear to be different kinds of ratios depending
on what is focused on. As a result, the Object and Sub-
Object vary according to the type of ratio. Our representa-
tion model can represent all of them, as shown in Table 1
(b). A detailed discussion can be found in a report by
Yamagata Y et al. [13].
Furthermore, we show the representation of a meta-
attribute. In the case of the property gastric polyposis,
although color and size are attributes of polyps, many
polyps is not an attribute of polyps because it is not
inherent in each polyp. Following the meta-attribute
approach in YAMATO, where, in the case of the road
is curvy, number of curves is identified as a meta-
attribute of curves, and the road, which has many
curves, can be represented in terms of it (the number
curves). Accordingly, we regard the number of polyps
as a meta-attribute of polyps and the stomach can be
described in the same manner as a road. By introducing
Sub-object, the property gastric polyposis can be
decomposed into a quadruple < stomach (O), polyps (SO),
Table 1 Representations of abnormal states
(a) Representation Abnormal statesProperty (P)
Property
Value (Vp) Attribute (A)
Attribute
Value (V) Object (O) Sub-Object (SO)
Basic representation
Gasrtectasia (gastric dilation) True Volume Large Stomach
Nausea True Patient
Extended representation
Hyperglycemia True Concentration High Blood Glucose
Gastric polyposis True Number Many Stomach Polyp
(b) Variant of Ratio Abnormal statesProperty (P)
Property
Value (Vp) Attribute (A)
Attribute
Value (V) Object (O) Sub-Object (SO) Ratio
m/n (no unit) High m ratio True Ratio High The whole Focused m/n
Example Hyperglycemia True Concentration High blood glucose Glucose/Blood
m/n (focused on m of
same object)
High m ratio True Ratio High Object m m/n
Example High Albumin ratio True Concentration High Urine Albumin Albumin/Creatinine
m/n (focused on the ratio
of same object)
High m/n ratio True Ratio High Object m/n
Example Increased A/G ratio True Ratio High Blood A/G Albumin/Globulin
(a): Basic and extended representations (b): Representations of ratios.
Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 4 of 14
http://www.jbiomedsem.com/content/5/1/23number (A), many (V)>, where the stomach is identified
as the Object, and the polyps as the Sub-object [Table 1
(a), row 4].
Interoperability between properties and attributes
Our claim in this clause is the interoperability between
the abnormal states and data. Considering the interoper-
ability between the abnormal states and clinical test data,
the OP form itself may not be compatible with the
observational data.
A large amount of clinical test data is stored in hospitals.
To ensure the cross-compatibility between those data and
the abnormal states (described in EHR), the unified form
should be required for computer processing, so that an
exchange mechanism between the OP and OAV is
indispensable.
The OAV form can deal with both the quantitative and
qualitative representation of values. Most clinical test data
are quantitative, e.g., the arterial cross sectional area of
24 mm2 and can be represented by OAVqt as < artery (O),
cross sectional area (A), 24 mm2 (Vqt) > in our model.
Notably, the quantitative value can be converted to a
qualitative value such as small (Vql), with the threshold
given by each hospital.
In the case of an abnormal state arterial stenosis, we
can guarantee the interoperability between the quantita-
tive data and abnormal state by decomposing stenosis
into cross sectional area is small, which is represented
as < artery (O), cross sectional area (A), small (Vql) >.
Another example is the quantitative data blood glucose
concentration level is 260 mg/dL. It is represented by <
blood (O), glucose (SO), concentration (A), 260 mg/dL
(Vqt)>, and the interoperability between the quantitative
test data and the abnormal states hyperglycemia used inthe definition of diabetes is realized via qualitative repre-
sentation such as < blood (O), glucose (SO), concentration
(A), high (Vql) > in the extended OSoAV form.
Furthermore, another issue is the requirement of the
degree value of the property. Clinicians usually need to
transform test data into abnormal states in the case
report. Imagine a case where a highly elevated value is
observed in the clinical test. A simple OP <O, P, true >
does not satisfactorily capture such data, and thus the
degree of abnormal states may be needed. Therefore, we
introduce the Degree Value (Vd) like severe. Therefore,
we can describe such data in terms of the degree value
in the OPVd form < blood (O), hyperglycemia, severe
(Vd) > in a triplet like in the OAV form.
We recommend that the degree value should only
have minimum variations such as mild/moderate/severe
for representation because numerous degree values would
lead to dispersion and destruction of the unified represen-
tation. Taking the interoperability into account, it would
be considered preferable to decide how to set a threshold
for determining the degree value of severe. However, be-
cause such concrete threshold values tend to change with
time, such threshold values are left undetermined in this
study.
Here we introduce Property Value (Vp). A Property has
a meaning of < hyperglycemia, existence > or < hypergly-
cemia, true>. The value as existence/nonexistence or
true/false should be independent of the degree value.
However, because adding another form to the degree
value would make the system of representation forms
more complex, we deal with the degree value (e.g.,
mild/moderate/severe) as the specialization of the state
of hyperglycemia; we treat these values of existence/
nonexistence or true/ false in the same manner as
Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 5 of 14
http://www.jbiomedsem.com/content/5/1/23the degree value. Consequently, we use Vp to represent
both Vd and Vp.
The use of the value is e.g., a representation of the
condition of not being hyperglycemia, < hyperglycemia,
false>, in latent diabetes. For this reason, we consider
that the true/false value is needed for computer processing.
In conclusion, our model provided the following inter-
operable representation forms:
(1) The < OAV > form as clinical test data
(2) The < OPV > form as abnormal states and
(3) The extended form <OSoAV > as clinical test data
and < OSoPV > as abnormal states
Consequently, the OPV form is the same as the OAV
form. Therefore, the OPV can be compliant with the
unified representation (OAV triplet), which realizes the
interoperability between the data and abnormal states.Is-a Hierarchy of abnormality ontology
Ontologically, the structural abnormality, dysfunction,
pathological conditions, pathological processes, etc. are
kinds of abnormal states. We propose abnormality
ontology, which is a comprehensive ontology that covers
all of the abovementioned concepts, and abnormal states
are defined as the top level.
Clinicians work with strongly domain-specific know-
ledge, which causes difficulties in finding common and
generic knowledge across domains. A clear distinction be-
tween the basic/generic and specific concepts is required
to be made. To this end, we propose the following three
levels of abnormal states (Figure 1):
 Level 1: Generic abnormal states
 Level 2: Object-dependent abnormal states
 Level 3: Specific context-dependent abnormal statesLevel 1: generic abnormal states
Level 1 defines very basic (or generic) concepts, which
do not depend on any structural entity, i.e., object-
independent states. Examples include deformation, add-
itional/missing anatomical structures, translocation, and
dysfunction, which are commonly found in several objects,
and can be usable in several domains besides medicine,
such as machinery, materials, and aviation.
The top-level category of the generic abnormal states has
three subclasses: structural abnormality, functional ab-
normality, and other abnormality (Figure 2). A structural
abnormality is defined as an abnormal state associated with
structure. It has subcategories of material abnormality (e.g.,
degeneration), shape abnormality (e.g., deformation), size
abnormality, and conformational abnormality, such as
topological abnormality (e.g., translocation), or structuraldefects (e.g., additional/missing structures) etc., while still
retaining the identity of the structural body in question.
A functional abnormality is defined as an abnormal
state that is related to an impaired function and is classi-
fied into hyperfunction and malfunction. Malfunction is
subcategorized into dysfunction, function arrest, and
hypofunction.
Other abnormal states include parametric abnormal-
ities, which are classified into increased or decreased
parameters, depending on whether or not the attribute
has a higher or lower value than a threshold level.
Examples included increased/decreased pressure or
increased/decreased weight.
Our model has a recursive structure, in which the gen-
eric abnormal states at Level 1 are referred to by Level 2
object-dependent abnormal states.
Level 2: Object-dependent abnormal states
Level 2 defines object-dependent abnormal states. The
top level concepts at Level 2 are dependent on generic
structures, such as wall-type structure, tubular struc-
ture, and bursiform structure, which are common and
are used in several domains. Level 2 has been developed
by identifying the target object and specializing generic
abnormal states at Level 1 with consistency. For example,
by specializing small in area at Level 1, narrowing tube,
where the cross-sectional area has become narrow, is de-
fined at Level 2 this is further specialized in the definitions
oil pipe narrowing or tracheal stenosis.
In the lower layer of Level 2, abnormal states that are
dependent on medical domain-specific objects, such as
human anatomical structures, are defined and designed
to represent concepts at all required granularities in the
medical domain. Here in general, one problem arises in
how fine the level of granularity needs to be supported
in our ontology. In the case of stenosis, the term, cor-
onary artery stenosis in a specific organ (the coronary
artery) may be redundant. However, it is noteworthy that
the abnormal states in one anatomical object can influ-
ence the adjacent objects, which causes other abnormal
states. For example, although both are types of stenosis,
coronary artery stenosis is different from rhinostenosis
because the former causes myocardial ischemia and ische-
mic heart disease, whereas the latter causes sleep apnea.
Therefore, there is a need for distinct abnormal states at
specific organ levels.
From an ontological engineering point of view, our
framework for modeling abnormal states is intended to
capture the abnormal states from generic to specific
levels, so as to provide abnormal states at the required
granularity of specific organ/tissue/cell layers in the
medical domain.
Here, the abnormal states of a specific object defined
at Level 2 should be distinct from the disease-dependent
Figure 2 Top-level categories related to abnormal states. The
top level categories of abnormal states are classified into three
subclasses: structural abnormality, functional abnormality, and
other abnormality such as parametric/nonparametric change and
so on.
Figure 1 Three-level ontological model of abnormal states. This figure shows an example of the three levels of structural abnormality of our
abnormality ontology. Level 1 defines generic concepts, which are object-independent states, e.g., small in area. Level 2 defines object-dependent
abnormal states. States at the upper levels of Level 2 are dependent on generic structures, such as the narrowing tube and narrowing valve, which
are common and are used in several domains. Note that concepts at the lower level of the tree are specialized into medicine-specific concepts such
as vascular stenosis, arterial stenosis, and coronary artery stenosis. Level 3 defines disease-dependent concepts. For example, coronary artery stenosis
in angina pectoris is defined as a constituent of the disease angina pectoris at Layer 3.
Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 6 of 14
http://www.jbiomedsem.com/content/5/1/23concepts at Level 3. For example, hyperglycemia is defined
in a context-independent manner at Level 2, and this is
referred to in Level 3 concepts in various diseases, such as
diabetes, metabolic syndrome, and lipodystrophy.
Currently, we are developing and enriching Level 2
concepts to link each Level 3 concept to the upper
common level concept.
Level 3: specific context-dependent abnormal states
Level 3 consists of context-dependent abnormal states,
which refer to the Level 2 abnormal states, and are spe-
cialized into specific disease-dependent ones. For example,
rectal stenosis, which is dependent on the rectum at
Level 2, is defined as a constituent of Crohns disease at
Level 3; this is also defined as a cause or an effect of other
diseases, such as rectal cancer, Hirschsprung disease, or
intestinal tuberculosis.
Application work
Causal chains of disease
We have been developing a disease ontology, in which a
disease is defined as a causal chain of abnormal states
[9]. We divided the diseases into two major kinds: (1)
ones where the etiological and pathological processes
are well understood and (2) otherwise. Case (2) includes
the so-called syndromes, typically represented in terms
of the criteria for diagnosis. Diseases of type (1) is identi-
fied by its inherent etiological/pathological process(es).
Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 7 of 14
http://www.jbiomedsem.com/content/5/1/23In this paper, we deal only with type (1) diseases. After
careful examination of several diseases, we believe that
every disease of type (1) should have a cue for identifica-
tion. This means that we should be able to find the so-
called main pathological/etiological condition(s), which
theoretically characterize the disease to identify it. We
know that diseases of type (2) necessarily employ criteria
for diagnosis to identify the disease because of the lack of
knowledge regarding etiological/pathological processes.
In addition, we believe that we need a formulation for
organizing diseases in an is-a hierarchy in a disease model.
According to the definition of a disease, this would consist
of a causal chain(s), which consisted of nodes and links; a
disease would be represented as a directed acyclic graph
(DAG). We can introduce the is-a relation between dis-
eases using the inclusion relationship between the causal
chains as noted below.
Is-a relation between diseases Disease A is a super
class of disease B if all the causal chains at the class level
of disease A are included in those of disease B. The
inclusion of nodes (disorders) is judged by considering
the is-a relation between the nodes into account as well
as the sameness of the nodes.
Core causal chain of a disease Causal chain(s) of a
disease included in the chains of all its subclass diseases
is called the core causal chain of a disease.
Derived causal chains of a disease Causal chains of a
disease defined as possible causal chains of abnormal
states are called derived causal chains.
For example, <myocardial stenosis?myocardial ische-
mia > is the core causal chain for ischemic heart disease
and <myocardial stenosis?myocardial ischemia?myo-
cardial necrosis > is the core causal chain for myocardial
infarction. Here the core causal chain of Prinzmetal an-
gina is defined as < coronary spasm?>, and if there are
some possible causes of spasm, e.g., smoking, it would be
added to the upstream of causal chains as < smoking
(nicotine absorption through the respiratory tract)? cor-
onary spasm? > as a derived causal chain (Figure 3).
Till date, clinicians have described the causal chains of
diseases and abnormal states. We have been using these
abnormal states to develop an is-a hierarchy of abnor-
malities. Abnormal states used in disease definitions in
the ontology are defined as abnormal states at Level 3,
where clinicians defined diseases in the respective med-
ical departments. We collected all causal relationships
from all disease concepts defined in the 13 medical
departments and combined the causal chains, including
the same abnormal states. As a result, the generic causal
chains that contain all causal relationships, including
approximately 21,000 abnormal states from 13 medicaldepartments have been generated [14]. For example, we
assume that a cardiovascular specialist in the division of
cardiovascular medicine describes coronary artery sten-
osis and its causal chain as < coronary artery stenosis?
myocardial ischemia?myocardial hypoxia > in ischemic
cardiac disease. This can be linked with coronary artery
stenosis in other diseases (e.g., hyperlipidemia) in other
departments (metabolic medicine). As a result, a generic
causal chain < accumulation of cholesterol? coronary
artery stenosis?myocardial ischemia?myocardial hyp-
oxia > of hyperlipidemia can be obtained as a possible
causal relationship of abnormal states in the disease.
Discussion and related work
Discussion
We have introduced a unified form that represented an
abnormal state as a time-indexed Property, and decom-
posed it into its Attribute and Value. Furthermore, we
introduced the Sub-Object, which increases the flexibil-
ity with consistency. A property representation has several
advantages. First, it easily captures the essentials of each
disease because of its abstract nature. Second, it is rela-
tively insusceptible to a small parameter modification.
Third, it allows for the distinction between a definition of
a disease and a diagnostic task that requires a quantitative
representation.
Here, it should be noted that an abnormality can be
explained as some bodily feature that is not part of the
human life plan (unlike pregnancy) [15]; however, making
a decision about whether or not a particular state is ab-
normal is not the job of ontologists but medical experts,
who are required to make decisions on the basis of their
medical knowledge. For example, answering a question
about whether or not high HDL cholesterol level is an
abnormal state would not be a task for ontologists but
for medical experts; therefore, we do not discuss this issue
in the present study.
We demonstrated that our model is interoperable
between the quantitative and qualitative data found in
several medical records, and the conceptual knowledge
of the abnormal states in the definition of diseases.
In this study, we do not deal with the concrete value
that is to be set for the threshold because thresholds
may tend to change with time; for example, the cutoff
value of the fasting plasma glucose (FPG) level was revised
to 140 mg/dL in 1980 and to 126 in 1999 [16]. Therefore,
we can freely change the threshold, and to do so is
intrinsic. Nevertheless, even if the threshold changes,
hyperglycemia will remain as < blood, glucose concen-
tration, high >.
Diversity and heterogeneous representation problems
of abnormal states are solved by a unified and consistent
framework. However, in clinical DB, compound concepts
are often found in clinical terms. For example, blood
Figure 3 Types of ischemic heart disease constituted of causal chains. This figure shows a couple of causal chain-constituted ischemic heart
disease. Each node shows the abnormal states, and each link indicates the causal relation between the abnormal states. A core causal chain of
each disease is colored differently: ischemic heart disease is orange, and the subclasses of the ischemic heart disease, myocardial infarction are
yellow. Prinzmetal angina is also a subclass of the ischemic heart disease consists of a pink core causal chain, and by an upstream extension
smoking is added in the derived causal chain. Organic angina pectoris is green and the accumulation of cholesterol is added to the derived
causal chain, which is a possible cause of arterial sclerosis.
Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 8 of 14
http://www.jbiomedsem.com/content/5/1/23pressure is considered to be a compound concept, which
consists of two elements blood (Object) and pressure
(Attribute) joined for one meaning denoting an attribute.
Other examples are WBC count and blood glucose
level. Precisely speaking, these concepts should not be
dealt with as an ontological matter but as a variation of
data representation. Because medicine also requires an
exchange of the real world data such as clinical test data
between hospitals or institutions, in the next step, we will
deal with them as a variation of data representation.
We have developed an ontology of abnormal states
from generic to specific levels.
We have confirmed that abnormal states in the definition
of 10 major diseases from three medical departments can
be described with our description framework and suc-
ceeded in developing the is-a hierarchy from Level 1 to
Level 3 in our preliminary work.
Till date, ontological engineers have defined Level 1
concepts together with a major portion of the higher
levels of Level 2; clinicians have defined Level 3 concepts,
including 21,000 abnormal states in 6,000 diseases in
ontology. We plan to reformulate all the abnormal states
at Level 3 in terms of our framework and complete the
development of the middle concepts at Level 2 to link
both the upper Level 1 and Level 3 abnormal states.
Some readers might think Level 3 is unnecessary and
should be treated as the diagnostic instance level. How-
ever, by introducing Level 3 concepts, it will providecontextual information in the specific disease and contrib-
ute to the understanding of the background knowledge
related to the underlying mechanisms of pathological
process in the disease. Furthermore, Level 3 concepts are
important for finding commonalities between the various
diseases in terms of abnormal states. Therefore, we need
to develop disease context-dependent levels as Level 3.
Our ontology is able to distinguish the common con-
cepts from specific ones. Such an ontological approach
contributes to finding commonalities not only across
diseases in one division but also across departments. For
example, in cardiovascular medicine, coronary artery
stenosis in ischemic heart disease has a commonality
with pulmonary artery stenosis in the tetralogy of Fallot
in that they have the same upper abnormal state arterial
stenosis. In addition, it has a commonality with cerebro-
vascular stenosis in brain infarction in cerebral surgery in
that they both have the same upper abnormal state vas-
cular stenosis. A further commonality can be found with
intestinal stenosis in the ileus in gastroenterological
medicine in that they have the same generic structure-
dependent abnormal state narrowing tube. Therefore,
finding commonalities across medical departments could
offer a multidisciplinary perspective, allowing our method
to be applied to a wide range of research.
In our application work, we have captured all 21,000
abnormal states across the 13 medical departments with
both the is-a hierarchical structure of the abnormality
Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 9 of 14
http://www.jbiomedsem.com/content/5/1/23ontology, and a causal chain as a relationship between
different classes of abnormal states that are influenced
by each other. This allows us to integrate fragmented
knowledge of abnormal states, which might support the
application of various kinds of medical knowledge, as
follows.
(1) Conceptualization with little ambiguity
In the medical domain, there are quite a few ambigu-
ous clinical terms with the same name but different
meanings. One reason behind this is that clinicians use
each term in the context of specific diseases in their own
departments. For instance, the medical term cardiac
hypertrophy is used in both a division of cardiovascular
medicine and metabolic medicine. The definition of
hypertensive heart disease in cardiovascular medicine in-
dicates an increase in the thickness of the heart muscle,
which results from a pressure overload caused by hyper-
tension in the context of the heart. On the other hand, in
glycogenosis II (Pompe diseases) in metabolic medicine, it
implies a glycogen accumulation in the heart muscle,
which is caused by metabolic dysfunction (Figure 4). Be-
cause our model can provide the appropriate upper levels
of concepts and can give contextual information, it is
possible to clarify their difference.
Thus, our model can reveal the context of the mean-
ings that is usually hidden in the implicit backgroundFigure 4 Examples of hypertrophy constituted of causal chains. This f
hypertrophy is red. One usage is a constituent of a causal chain of the hyp
and the other is a constituent of a glycogenesis type II disease (Pompe dis
chain of each disease is yellow).knowledge of clinicians, and will contribute to making a
clear distinction between different types of concepts.
(2) Management of attributes by unified representation
If we allow clinicians to freely express the various attri-
butes/abnormalities, it would lead to a lack of consistency
and interoperability. Our model solves this problem by
providing a unified representation model of attributes/
abnormal states, as discussed in section Representation of
abnormal states, in which the attributes and properties are
differentiated; the properties are decomposed into < attri-
bute, attribute value>, as well as the advanced representa-
tion for ratios and meta-attributes.
(3) Quantitative assessment of commonality
Traditionally, abnormal states have been dealt with in
a manner specific to each disease in a particular medical
division. Here, our model enables the capture of abnormal
states common to several diseases, i.e., those that are at
the first two levels and those that are disease-independent,
which allows clinicians to overlook all abnormal states
across medical departments.
As a result, we can quantify and assess the degree of
commonality of abnormal states between different medical
departments. In addition, it is possible to verify the com-
monality of generic concepts by abstracting, or to findigure shows two different uses of cardiac hypertrophy. Each cardiac
ertensive heart disease in the cardiovascular department (upper figure),
ease) in the metabolic disease department shown below (A core causal
Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 10 of 14
http://www.jbiomedsem.com/content/5/1/23disease-specific abnormal states with no commonality to
any disease in other departments. For example, esopha-
gostenosis, which is a subclass of narrowing tube, may
demonstrate that it is specific to esophageal disease by
showing no commonality with other diseases, whereas
vascular stenosis can be confirmed as being more common
by showing a higher rate of commonality across multiple
diseases. Furthermore, our model may find commonalities
of abnormal states that have always been treated as quite
different abnormal states in different departments.
The clinicians treatment of the abnormal states in a
manner specific to a disease and/or particular clinical
division may have caused fragmentation of the same
concept into different ones that are treated as differ-
ently. Because our approach finds commonalities in the
organ-independent abnormal states, we can clean up
and deal with abnormal states more simply.
Thus, our ontology will provide a clue to revealing the
context embedded as background knowledge, which will
allow us to compare abnormal states and evaluate their
commonalities across medical departments.
Related work
Upper ontologies such as BFO [1,2], DOLCE [3], and
Galen [17] also deal with qualities and have contributed to
dealing with the semantics of data. BFO formalizes < Entity,
Property > (e.g., <rose, red>), whereas DOLCE uses < Entity,
Attribute, Value > formalization (e.g., <rose, color, red>),
and Galen adopts < Entity, Property, Value > formalization,
(e.g., <rose, redness, high>). Phenotypic Quality (PATO) [6]
is an ontology of phenotypic qualities, where the de-
scription was changed from < Entity, Attribute, Value > to
< Entity, Property (Quality) > (e.g., <eye, red>) when they
employed BFO. As explained in the Background section,
the YAMATO ontology is an upper ontology in Japan [4],
and offers interoperability among all of these descriptions,
allowing us to handle all three kinds of descriptions in our
representation model. Furthermore, PATO2YAMATO
provides the mapping of ontology terms of PATO to
YAMATOs framework [5], and enables the interoper-
ability of the quality framework between the different
top-level ontologies such as BFO and DOLCE. In a prelim-
inary study, the application has succeeded in making the
connection between rat or mouse phenotype data, and re-
lated human abnormal states in the definition of diseases in
this study [18]. Because our extended model enables the
capture of commonalities of abnormal states across bio-
logical species, it may contribute to translational research
linking mouse experimental data and clinical research.
Furthermore, on the basis of the ontological approach,
if we make explicit the commonality and specificity of
abnormal states among multi-species, it should support
a comprehensive understanding of the basic common
mechanism or principles underlying organisms, and wouldlead to scientific discoveries by acquiring biomedical know-
ledge through an interdisciplinary approach across species.
In the medical domain, medical ontologies and stand-
ard vocabularies, such as ICD-10 [19], SNOMED-CT [20],
have been developed and extensively used in practice.
However, they are largely based on legacy system termin-
ologies, and thus have some ontological problems [21].
SNOMED-CT is a comprehensive terminology, which
contains more than 311,000 clinical terms. However, it
is not compliant with any formal upper level ontology.
SNOMED-CT allows for multiple inheritance that causes
a messy situation in the classification of entities, despite
the fact that partitioning implies sibling classes are
mutually disjoint, siblings at lower levels overlap each
other, which results in complex taxonomic graphs and
maintenance of the ontology difficult [20]. Furthermore,
SNOMED-CT does not distinguish disorders from dis-
eases. Not all disorders are diseases.
Our ontological proposal will help avoid such problems.
On the basis of YAMATO, we systematically define
abnormal states from the generic level (Level 1) to the
specific anatomical structure-dependent level (Level
2). Furthermore, by specializing Level 2 concepts into
a disease-context (disease-specific) level (Level 3), we
can distinguish abnormal states from diseases.
In our future plan, our ontology will be translated into
English and be mapped with SNOMED-CT clinical terms.
The mapping will evaluate the standard terminologies in
line with fundamental ontology engineering and provide
useful information about causal relationships of abnormal
states in the definition of each disease.
LOINC [8] provides the universal code names and
clinical terms by decomposing them. However, because
it focuses on the clinical observations, several of the
abnormal states appearing in the definition of diseases
are out of the scope for LOINC.
Although LOINC has < O (SO) A > like our model, it
does not have Value (V). For example, a test for glucose
tolerance about after 2 hours serum glucose for 100 g oral
is represented by GLUCOSE^2H POST 100 G GLU-
COSE PO:MCNC:PT:SER/PLAS:QN. The aim of LOINC
is to standardize the vocabulary for the representation of
clinical test data and is useful for interoperability among
various data. However, our claim is not isolated to adopt-
ing the OAV form. In order to realize the interoperability
between the clinical test data and abnormal states, a
Quantitative Value (Vqt) is needed in the representation
form. Our model can deal with quantitative data in the
OAV form; therefore, we can transform it into the OP
form of abnormal states. As a result, our model has an
ability to maintain the interoperability between the clinical
test data to abnormal states in diseases. Our model is not
merely a theoretical contribution. Only reutilizing the
existing resources cannot realize the interoperability
Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 11 of 14
http://www.jbiomedsem.com/content/5/1/23between the various representation forms. We need
more sophisticated organization of related representations
including quantitative and qualitative data to exploit all of
them in a consistent manner. To the best of our know-
ledge, our model is the first to make such an exploitation
possible, which will contribute to medical practices.
Our model contributes to the systematization of ab-
normal states on the basis of ontological theory, and is
able to distinguish between generic abnormal states,
object-dependent ones, and disease-specific ones with
unified representation. Moreover, the generic abnor-
mal states are referred to the lower level of abnormal
states by specializing them into the required granularity.
In the future, we plan to examine mappings to other data
sets of representations of clinical observations such as in
LOINC or MEDIS [22] that have been opened to the
public by The Medical Information System Development
Center in Japan (MEDIS-DC) for interoperability. These
mappings would provide a more comprehensive analysis
of interoperability between the clinical observation dataFigure 5 Computational representation of abnormal states from gene
states from small in area to ischemic heart disease specific coronary arterand the conceptual knowledge of abnormal states in the
definition of diseases.
OGMS, which uses BFO as an upper-level ontology
[23], and DO [24] are both medical ontologies. However,
they do not have causal relationships between the abnor-
mal states in one disease. Our strategy will contribute to
providing a good resource for several medical researchers
to analyze the causes of diseases from the viewpoint of the
causal relationships of the abnormal states. Collaborative
efforts in OBO Foundry have tried to coordinate various
ontologies to support biomedical data integration [25]. In
the next step, we plan to convert our abnormal state
ontology into OBO format, and provide useful informa-
tion about the causal relationships in diseases.
In our practical application work, we published some
parts of the causal chain in disease ontology as Linked
Open Data (Disease Chain LOD) on the basis of our
RDF model [26]. It includes definitions of 2,103 diseases
and 13,910 abnormal states in six major medical depart-
ments extracted from the disease ontology on May 11,ric to specific level. This figure shows the specialization of abnormal
y stenosis using HOZO.
Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 12 of 14
http://www.jbiomedsem.com/content/5/1/232013. In addition, we have developed the visualization
system for disease chains called the Disease Chain LOD
Viewer, which is available at http://lodc.med-ontology.jp/.
Furthermore, a browsing system of disease chains with
related information, which are obtained from other linked
data or web service from external datasets (e.g., ICD 10,
MeSH from DBPedia), is currently under development.
Conclusions
We proposed a representation model of abnormal states
designed in a unified manner. Our medical ontology
project was started seven years ago. Since then, it has been
refined and revised several times through discussion with
both ontologists and clinicians. Till date, we have applied
this model to approximately 21,000 abnormal states from
approximately 6000 diseases.
We have demonstrated that our model has interoper-
ability between quantitative and qualitative data and the
conceptual knowledge of abnormal states in the definition
of diseases. With this model, we have been developing an
ontology of abnormal states from generic to specific levels.Figure 6 A visual editing tool for causal chains to define disease conc
definition of disease concepts. It visualizes the causal chains defined in a seIn the application we considered, we built disease chains
consisting of causal relationships of abnormal states. By
combining the disease chains and the ontology, we have
captured all causal relations of the 21,000 abnormal states
in the 6,000 diseases across 13 medical departments.
Although abnormal states have traditionally been con-
sidered to be specific to each disease in a particular med-
ical department, our approach has found commonalities
among abnormal states across medical departments.
Methods
Data sources for representation and ontology
development of abnormal states
Medical doctors of The University of Tokyo Hospital
described the disease ontology, and definitions of dis-
eases were determined. Medical dictionaries [10,27] and
JOURNAL OF
BIOMEDICAL SEMANTICS
Vihinen Journal of Biomedical Semantics 2014, 5:9
http://www.jbiomedsem.com/content/5/1/9SOFTWARE Open AccessVariation ontology: annotator guide
Mauno VihinenAbstract
Background: Systematic representation of information related to genetic and non-genetic variations is required to
allow large scale studies, data mining and data integration, and to make it possible to reveal novel relationships
between genotype and phenotype. Although lots of variation data is available it is often difficult to use due to lack
of systematics.
Results: A novel ontology, Variation Ontology (VariO http://variationontology.org), was developed for annotation of
effects, consequences and mechanisms of variations. In this article instructions are provided on how VariO
annotations are made. The major levels for description are the three molecules, namely DNA, RNA and protein.
They are further divided to four major sublevels: variation type, function, structure, and property, and further up to
eight sublevels. VariO annotation summarizes existing knowledge about a variation and its effects and formalizes it
so that computational analyses are efficient. The annotations should be made on as many levels as possible. VariO
annotations are made in reference to normal states, which vary for each data item including e.g. reference
sequences, wild type properties, and activities.
Conclusions: Detailed instructions together with examples are provided to indicate how VariO can be used for
annotation of variations and their effects. A dedicated tool has been developed for annotation and will be further
developed to cover also evidence for the annotations. VariO is suitable for annotation of data in many types of
databases. As several different kinds of databases are in a process of adapting VariO annotations it is important to
have guidelines to guarantee consistent annotation.
Keywords: Variation ontology, Annotation instructions, Systematics, Variation effects, Mutation, OntologyBackground
Variations have different effects and mechanisms. To
capture and describe the character of the variations at
DNA, RNA and protein level, the Variation Ontology
(VariO) was developed [1]. VariO annotations allow sys-
tematic descriptions which can be used e.g. for searches
of complex queries also simultaneously from several da-
tabases. Systematic descriptions have several benefits es-
pecially for computational searches and analyses and for
software development.
Variation information has been collected to various da-
tabases. Locus specific databases (LSDBs) are for individ-
ual genes or diseases and usually curated by experts in
the genes and diseases. There are currently thousands of
LSDBs available, mainly on the LOVD database manage-
ment system [2]. Central databases like SwissProt [3]
and ClinVar [4] contain information on large numbersCorrespondence: mauno.vihinen@med.lu.se
Department of Experimental Medical Science, Lund University, BMC D10,
SE-22184 Lund, Sweden
© 2014 Vihinen; licensee BioMed Central Ltd.
Commons Attribution License (http://creativec
reproduction in any medium, provided the orof genes and/or proteins and variations in them. Additional
types of variation databases include national or ethnic da-
tabases such as ETHNOS [5], chromosomal variation data-
bases such as The Database of Genomic Variants archive
(DGVa) [6], variation frequency databases including FIND-
base [7], and databases dedicated on certain types of varia-
tions or for an effect or mechanism, such as ProTherm,
a database for protein stability affecting variations [8].
Benchmark databases, such as VariBench [9], are dedi-
cated for providing gold standard datasets for method de-
velopers and assessors. All these databases would benefit
from systematic descriptions. Human Genome Variation
Society (HGVS) nomenclature is widely used for naming
variations [10], however additional systematics would be
needed. HGVS and Human Variome Project (HVP) have
released a number of recommendations [11], also for in-
creased systematics, including recommendation to use
VariO annotations [12]. The recent recommendations for
LSDB establishment and curation emphasize the import-
ance of systematics [13,14]. The goal of the GEN2PHENThis is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly credited.
Figure 1 General structure of VariO. The ontology is designed for
annotation of effects, consequences and mechanisms of variations
at the three molecular levels, DNA, RNA and protein. Each of these
has further terms on four major sublevels: variation type, function,
structure and property. Attributes are used to modify terms at
structure or property levels to further define the terms.
Vihinen Journal of Biomedical Semantics 2014, 5:9 Page 2 of 8
http://www.jbiomedsem.com/content/5/1/9project was to develop tools, data models and solutions
for this domain [15].
The HGVS variation nomenclature is a systematic
naming convention that is in use in some journals and
numerous databases. It provides guidelines and naming for
almost all variation types based on reference sequences.
The Mutalyzer tool can generate the names automatically
and performs a number of consistency checks [16].
VariO is intended for the description of what is chan-
ged in the variant in comparison to the normal or wild
type. Thus, it does not describe the properties of the
wild type. The annotations are made in comparison to a
reference, which varies depending on the annotated
property including e.g. reference sequences, reference
states such as wild type enzyme activity and normal kin-
etic properties. The application area of VariO is in
describing effects, consequences and mechanisms in di-
verse data sources. These include all the different types
of variation databases mentioned above. In addition,
it can benefit variation naming services [16], LSDB man-
agement systems [2], data integrators, journals etc. As
VariO has been described previously [1], the goal of this
annotator guide is to explain how annotations are made
and used.
VariO design principles
VariO terms have three biological molecules, DNA,
RNA and protein, as the starting point. All these have
four major sublevels: variation type, function, structure
and properties with more detailed sublevels. There are
altogether 8 levels of terms. The terms have a clear hier-
archy and the organization of terms for DNA, RNA and
protein has a similar and coherent layout whenever ap-
propriate. The terms for the three molecular levels are
consistent and related terms are used for related features
at different levels. Suggestions for additional terms and
updates can be sent to the ontology developers.
For visualization of terms, their definitions, relation-
ships and paths, the AmiVariO browser is available at
the VariO website. Figure 1 indicates how the levels are
organized. When possible, variations are explained at
DNA, RNA and protein levels. Each of these has sub-
levels, out of which structure and property levels can be
further modified with attribute terms. The hierarchy of
the terms has been designed to allow for a versatile and
flexible annotation.
VariO is intended for the description of all kinds of
variations and situations. In Figure 2 the distinction be-
tween terms of genetic and non-genetic origin is shown.
Terms with genetic origin describe changes either in
DNA or inherited from it to RNA and protein levels.
The non-genetic terms, called variations emerging at the
RNA or protein level, are for either biological or artificial
modifications that originate at the RNA or protein level.For example, RNA editing modifies a synthetized RNA
chain and the variations are not coded in DNA.
To keep the ontology compact, modifier attributes
form the fourth major level. These terms are used to
modify the terms at the other levels. For example, quan-
tity terms are used to modify other terms when the ef-
fect is increased, decreased or is missing the quantity of
the parameter, or when it is not changed. Instead of hav-
ing separate terms for describing increase or decrease of
a feature, existing terms can be modified with attribute
terms. This way the number of terms could be reduced
considerably.
VariO terms should be combined with other systemat-
ics and ontologies. Evidence Ontology (ECO) terms [17]
are used to describe the methods with which the annota-
tions were obtained.
VariO aims at describing any effect, consequence and
mechanism, at any organism. The variations can be of
genetic or non-genetic origin. The size of the variation
does not matter, anything ranging from nucleotide or
amino acid changes to chromosome or genome duplica-
tions can be annotated. However, the annotations are
position based, even if the position means e.g. an entire
chromosome.
Example of VariO annotation
This example highlights a number of aspects of annota-
tions with VariO. The annotations are used to explain
Figure 2 Variation types for terms with genetic or non-genetic origin. The variation type annotations are made based on whether the
variation is of genetic or non-genetic origin. Genetic terms are used for alterations originating from the genetic material of the organism (DNA or
RNA, depending on the organism), while terms with non-genetic origin are either artificial or originated from the processing of RNA or protein
molecules without change in the corresponding DNA.
Vihinen Journal of Biomedical Semantics 2014, 5:9 Page 3 of 8
http://www.jbiomedsem.com/content/5/1/9additional features of the instances in a database in a
systematic way. It may be tiring for a human reader to
see the same concept every time it is mentioned, how-
ever, for computational analyses it is a blessing and facili-
tates fast searches. For the annotation we are developing a
tool called VariOtator [18].
Here is a real life example of variation in the AIRE
gene leading to the autoimmune polyendocrine syn-
drome type 1 (APS-1) also called for APECED disease
(autoimmune polyendocytopathy-candidiasis-ectodermal
dystrophy), an autoimmune polyendocrine syndrome.
AIRE, autoimmune regulator, is a transcriptional regulator
of tissue-specific antigens. Variations affect the regulation,
leading to the production of self-reactive antigens. The
T >C variation leading to a L to P substitution in the homo-
geneous staining region (HSR) domain is disease causing
[19] (AIREbase [20,21] entry A0087). The functional and
other aspects of APECED-causing variations were further
studied in [22].
The genomic variant g.4789 T > C in the IDbase refer-
ence sequence D0003 (http://structure. bmc.lu.se/cgi-
bin/fetch_idrefseq.cgi?ac = D0003&format = embl, cross-
reference to EMBL:AB006682) is annotated as follows.
VariO:0128 variation affecting DNA
VariO:0129 DNA variation type
VariO:0322 DNA variation classification
VariO:0135 DNA chain variation
VariO:0136 DNA substitution
VariO:0313 transition
VariO:0314 pyrimidine transitionThe variation is of genetic origin
VariO:0128 variation affecting DNA
VariO:0129 DNA variation type
VariO:0127 DNA variation origin
VariO:0130 DNA variation of genetic origin
There is a pyrimidine transition of genetic origin.
The effect to RNA sequence (IDbase reference sequence
C:0003 http://structure.bmc.lu.se/cgi-bin/fetch_idrefseq.
cgi?ac = C0003& cross referenced to EMBL; AB006682)
is, similar at DNA level, a pyrimidine transition, which
causes a missense variation.
VariO:0297 variation affecting RNA
VariO:0306 RNA variation type
VariO:0328 RNA variation classification
VariO:0312 RNA substitution
VariO:0313 transition
VariO:0314 pyrimidine transition
VariO:0308 missense variation
On the protein level the reference sequence is UniProt
entry O43918. A variation has different annotations at
different levels. In this example, the amino acid substitu-
tion at protein level is annotated as DNA substitution
on DNA level, and on RNA level as RNA nucleotide
substitution of type missense variation.
The annotations are richer on protein level as the ef-
fects of the variation affect protein function, structure
and properties.
Vihinen Journal of Biomedical Semantics 2014, 5:9 Page 4 of 8
http://www.jbiomedsem.com/content/5/1/9VariO:0002 variation affecting protein
VariO:0012 protein variation type
VariO:0325 protein variation classification
VariO:0021 amino acid substitution
The protein variation is due to change at the DNA level.
VariO:0002 variation affecting protein
VariO:0323 protein variation origin
VariO:0013 protein variation of genetic origin
This variant was shown to prevent transactivation, a
protein information transfer function.
VariO:0002 variation affecting protein
VariO:0003 variation affecting protein function
VariO:0011 effect on protein information transfer
The annotations for the structure include predicted ef-
fects on the protein secondary and tertiary structure.
Introduction of a proline in the middle of ?-helix affects
the protein fold leading to conformational change.
VariO:0002 variation affecting protein
VariO:0060 variation affecting protein structure
VariO:0064 effect on protein 3D structure
VariO:0070 effect on protein tertiary structure
VariO:0079 effect on protein secondary structural
element
VariO:0080 effect on protein helix
VariO:0082 effect on right handed protein helix
VariO:0085 effect on alpha helix
VariO:0002 variation affecting protein
VariO:0060 variation affecting protein structure
VariO:0064 effect on protein 3D structure
VariO:0070 effect on protein tertiary structure
VariO:0073 effect on protein fold
VariO:0074 protein conformational change
Properties are used for annotating various characteris-
tics. As many properties should be annotated as data is
available. The variation is disease-causing, which is indi-
cated by the pathogenicity association attribute. In the
case of attributes, only the relevant attribute, not path to
it is given. Attributes can be used to modify structure
and property terms.
VariO:0002 variation affecting protein
VariO:0032 variation affecting protein property
VariO:0047 association of protein variation to
pathogenicity; VariO:0294 disease causingThe variant affects transactivation inactivating the pro-
tein function. The property term (effect on protein activ-
ity) is again modified by the attribute (missing).
VariO:0002 variation affecting protein
VariO:0032 variation affecting protein property
VariO:0053 effect on protein activity; VariO:292 missing
Effects to protein interaction can be described in detail.
Interaction attribute terms were derived from the Protein
Interaction ontology [23] but modified for the purpose of
VariO.
VariO:0002 variation affecting protein
VariO:0032 variation affecting protein property
VariO:0058 effect on protein interaction; VariO:0292
missing
The variant prevents AIRE homomultimerization, which
is required for the transactivation activity, and is essential
for the interaction with nuclear dots and cytoplasmic fila-
ments. This can be further described by the attributes as
follows for the homomultimerization, i.e. interaction with
another protein molecule.
VariO:0232 variation attribute
VariO:0236 interaction
VariO:0262 interactor
VariO:0273 biopolymer
VariO:0277 protein
and the interaction with nuclear dots and cytoplasmic
filaments are annotated as effects on protein complexes.
VariO:0232 variation attribute
VariO:0236 interaction
VariO:0262 interactor
VariO:0267 complex
VariO:0269 protein complex
Even more detailed descriptions would be possible with
VariO, however, in this case as details are missing, the an-
notations remain somewhat shallow. For instance, inter-
action physical forces cannot therefore be annotated.
The variant alters the subcellular localization of the pro-
tein. Terms for the actual change to the compartment tar-
geting are not provided as they have not been systematized.
VariO:0002 variation affecting protein
VariO:0032 variation affecting protein property
VariO:0033 effect on protein subcellular localization
This example highlights how the experimental and pre-
dicted results can be explained at multiple levels. Note that
Vihinen Journal of Biomedical Semantics 2014, 5:9 Page 5 of 8
http://www.jbiomedsem.com/content/5/1/9e.g. details of protein function are further explained by
the property terms. Above, attributes are used as simple
modifiers of quantity for protein activity and for disease
causality, but these modifiers allow also more elabor-
ate annotations as for interactions. The VariO annota-
tion is modular and therefore any number of terms can
be used, whatever is needed to capture the type and
effects of a variant. More examples are available at http://
variationontology.org/examples.shtml.
How to get started
When annotating a variant one should combine all the
existing information about the variant and its effects.
The steps from the variation identification to a func-
tional annotation are depicted in Figure 3. In case of
contradictory results try to obtain consensus. Databases
should not reflect personal opinions, therefore the anno-
tations have to be according to the approved standards
in the field. Once the data is available all relevant aspects
should be described. This may require annotations at
several sublevels, for example variation types can be fur-
ther defined at structural level.
Once the data is available the annotation process is
relatively straightforward. The annotation tool, VariOtator,Figure 3 Flowchart for annotation with VariO. All existing
information for the variation should be available when starting
annotation. Reference sequence is needed to indicate the position
of the variation. With this information can be generated the
variation type annotation. The effects on function, structure and
property are annotated based on the obtained information. The
methods used for obtaining the results are indicated by Evidence
JOURNAL OF
BIOMEDICAL SEMANTICS
Kalankesh et al. Journal of Biomedical Semantics 2014, 5:2
http://www.jbiomedsem.com/content/5/1/2RESEARCH Open AccessThe languages of health in general practice
electronic patient records: a Zipfs law analysis
Leila R Kalankesh1,2, John P New3,4, Patricia G Baker5 and Andy Brass1*Abstract
Background: Natural human languages show a power law behaviour in which word frequency (in any large
enough corpus) is inversely proportional to word rank - Zipfs law. We have therefore asked whether similar power
law behaviours could be seen in data from electronic patient records.
Results: In order to examine this question, anonymised data were obtained from all general practices in Salford
covering a seven year period and captured in the form of Read codes. It was found that data for patient diagnoses
and procedures followed Zipfs law. However, the medication data behaved very differently, looking much more
like a referential index. We also observed differences in the statistical behaviour of the language used to describe
patient diagnosis as a function of an anonymised GP practice identifier.
Conclusions: This works demonstrate that data from electronic patient records does follow Zipfs law. We also
found significant differences in Zipfs law behaviour in data from different GP practices. This suggests that
computational linguistic techniques could become a useful additional tool to help understand and monitor the
data quality of health records.Background
A recent survey has shown that 90% of patient contact
with the National Health Service (NHS) in the UK is
through General Practices and General Practitioners
(GPs) [1]. Over 98% of the UK population is registered
with a general practitioner and almost all GPs use com-
puterised patient record systems, providing a unique and
valuable resource of data [2]. About 259 million GP con-
sultations are undertaken every year in the UK. However,
capturing structured clinical data is not straightforward
[3]. Clinical terminologies are required by electronic pa-
tient record systems to capture, process, use, transfer and
share data in a standard form [4] by providing a mechan-
ism to encode patient data in a structured and common
language [5]. This standard language helps improve
sharing and communication of information through-
out the health system and beyond [6,7]. Codes assigned to
patient encounters with the health system can be used for
many purposes such as automated medical decision sup-
port, disease surveillance, payment and reimbursement of* Correspondence: abrass@manchester.ac.uk
1School Of Computer Science, University of Manchester, Oxford Road,
Manchester M13 9PL, UK
Full list of author information is available at the end of the article
© 2014 Kalankesh et al.; licensee BioMed Cent
Commons Attribution License (http://creativec
reproduction in any medium, provided the orservices rendered to the patients [8]. In this work we are
focusing our attention specifically on the coding system
used predominantly by UK GPs, the Read codes.
Read codes provide a comprehensive controlled vo-
cabulary that has been structured hierarchically to pro-
vide a mechanism for recording data in computerised
patient records for UK GPs [9]. They combine the char-
acteristics of both classification and coding systems [10].
Most data required for an effective electronic patient
record (demographic data, lifestyle, symptoms, history,
symptoms, signs, process of care, diagnostic procedures,
administrative procedures, therapeutic procedures, diag-
nosis data, and medication prescribed for patient) can be
coded in terms of Read codes [11]. Each Read Code is
represented as 5-digit alphanumeric characters and each
character represents one level in hierarchical structure
of Read codes tree [12]. These codes are organised into
chapters and sections. For example Read codes begin-
ning with 09 are processes of care, those beginning
with A  Z (uppercase) are diagnosis, and those begin-
ning a-z (lowercase) represent drugs (described further
in the Methods section). Of some concern, however, is
the quality of the data captured in this way.ral Ltd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Table 1 An example of the 5-byte Read code that shows
how the specificity of a term increases as a function
of depth
Depth Read code Term
1 G Circulatory system diseases
2 G3 Ischaemic heart disease
3 G30 Acute myocardial infarction
4 G301 Other specified anterior myocardial infarction
5 G3011 Acute anteroseptal infarction
It is straightforward to examine the datasets to determine the range of term
depths that have been used in the coding process.
Kalankesh et al. Journal of Biomedical Semantics 2014, 5:2 Page 2 of 8
http://www.jbiomedsem.com/content/5/1/2At its heart, medical coding is a process of communi-
cation, with clinical terminologies bridging the gap be-
tween language, medicine and software [13]. Read codes
can be thought of as a vocabulary for primary care medi-
cine, providing words (terms) used to describe encoun-
ters between GPs and patients. The GPs (annotators) are
attempting to encode information regarding the consult-
ation; information that the wider community then needs
to decode. The bag of codes associated with a consult-
ation can therefore be thought of a sentence made up of
words from Read, a sentence written by a GP to convey
information to a range of different listeners.
One of the best known and universal statistical behav-
iours of language is Zipf s law. This law states that for
any sufficiently large corpus, word frequency is approxi-
mately inversely proportional to word rank. In fact,
Zipf s law is considered as a universal characteristic of
human language [14] and as a wider property of many
different complex systems [15] as well as human lan-
guages [16]. Zipf suggested that this universal regularity
in languages emerges as a consequence of the competing
requirements of the person or system coding the informa-
tion (speaker) compared with the person or system trying
to decode the information (listener). From the perspective
of the speaker, it would be most straightforward for them
to code the signal using high level, non-specific terms as
these are easy to retrieve. It is more difficult to code the
signal using very specific terms as this requires hunting
through long lists and navigating deep into the termin-
ology. The problem is very different for the listener. For
them the problem is one of resolving ambiguity. If the data
is coded using very specific terms then ambiguity is min-
imal and interpreting the message is straightforward. If
only high level general terms are used, then it is much
harder to discern the meaning of the message. In any com-
munication system there is therefore a tension between the
work being done by the speaker and the listener. Indeed,
some controversial recent papers have attempted to show
that Zipf s law emerges automatically in systems that sim-
ultaneously attempt to minimise the combined cost of cod-
ing and decoding information [16-18].
Similar issues clearly arise in medical coding in which
there needs to be a balance between the efforts required
from the coder with those of the person interpreting and
using the data. Reaching a proper balance between com-
prehensiveness and usability of clinical vocabularies is
regarded as one of the challenges in the medical inform-
atics domain [19].
The hypothesis we are therefore exploring in this paper
is whether a Zipfian analysis of medical coding data can
provide useful insights into the nature and quality of
data. For example, we can ask where this balance lies
across different aspects of the data medically-coded
captured in GP records, information about diagnosis,information about the medical procedures applied and
medication prescribed, and whether this balance is differ-
ent across different general practices. We have therefore
performed a computational linguistics analysis of a large
corpus of anonymised Read code data from GPs in
Salford to see whether such analyses might have value in
understanding and characterising coding behaviour and
data quality in electronic patient records. Salford is a city
in the North West of England with an estimated popula-
tion of 221,300. The health of people in Salford is gen-
erally worse than the English average, including the
estimated percentage of binge drinking adults, the rate of
hospital stays for alcohol-related harm, and the rate of
people claiming incapacity benefit for mental illness. How-
ever, the percentage of physically active adults is similar
to the English average and the rate of road injuries and
deaths is lower.
Methods
The data set
For this study we took GP data from Salford. Data from
2003 to 2009 was collected from 52 General Practice
groups from Salford. This data consisted of anonymised
patient identifiers, anonymised GP practice identifiers
and the set of Read codes collected. In total, the data set
contains over 136 million Read codes derived from
34200 distinct codes. Ethical permission for this study
was granted through North West e-Health. Table 1 shows
an example of a set of Read codes and demonstrates the
way in which specificity increases with code depth.
Zipfs law analysis
Mathematically, Zipf s law can be expressed as:
f rð Þ ¼ r??
where f(r) refers to the frequency of the word with rank
r and a is the Zipf s law exponent. There are a number
of different ways in which this behaviour can be repre-
sented mathematically - power law behaviour, Zipf s law,
Paretos law - that can be demonstrated to be equivalent
Kalankesh et al. Journal of Biomedical Semantics 2014, 5:2 Page 3 of 8
http://www.jbiomedsem.com/content/5/1/2[20]. For example, if P () is the proportion of words
in a text with frequency  then Zipf s law can also be
expressed as:
P ð Þ
e
 ?
It is straightforward to show that ? and ? are related by:
? ¼ 1þ 1
?
Figures in this paper have been presented in the form
of the Pareto distribution (named after a nineteenth cen-
tury Italian economist) as they provide the most con-
venient form for calculating an accurate exponent. The
Pareto distribution is expressed in terms of the cumula-
tive distribution function (CDF):
P X ? xð Þ
e
x?ka)
c)
Diagnoses
Figure 1 The Pareto plots for the Salford data showing the cumulativ
for the subset of the Read codes used in the Salford corpus. a) diagno
diagnosis and procedure codes could be effectively modelled, at least in p
b). However, there was no range on which the medication data could be mwhere the distribution shape parameter, k, can be con-
verted to the Zipf s law exponent (a) via:
? ¼ 1
k
and to the power law exponent (?) as below:
? ¼ 1þ k
Pareto plots and parameter estimations were calcu-
lated using the Matlab packages plfit, plplot and, plpva
developed by Clauset and Shalizi [21]. These packages
attempt to fit a power law model to the empirical data
and then determine the extent to which the data really
can be effectively modeled using a power law. These
tools provide two statistics describing the data. The first
is a p-value that is used to determine the extent to which
the power law model is appropriate. If the p-value isb)
Procedures
Medication
e distribution function Pr(x) plotted as a function of frequency (x)
sis codes; b) procedure codes; c) medication codes. The data for
art of their range, by a power law (shown as the dotted lines in a and
odelled by a power law, c).
Kalankesh et al. Journal of Biomedical Semantics 2014, 5:2 Page 4 of 8
http://www.jbiomedsem.com/content/5/1/2greater than 0.1 we can regard the power law to be a
plausible model of our data. The second statistic pro-
duced is ?, the exponent of power law.
A number of Zipfian analyses were then performed
on different subsets of the Read code data within the
Salford corpus. In particular we looked at the subsets of
Read codes for codes to do with diagnosis, procedure
and medication separately (Read codes used for diagno-
sis start with an upper case character (A-Z), Read codes
for procedures begin with a number (09), and those
medication with a lower case character (a-z) [22]). We
were able to further subdivide the data into chapters
based on the first letter of the Read code for more de-
tailed analysis.
We also performed a number of other simple analyses to
characterise the Salford corpus. We first measured the
type-token ratio (TTR). The TTR is calculated by dividing
the types (the total number of different Read codes) by to-
kens (total number of Read codes used), expressed as aFigure 2 Percentage of Read codes at each level of granularity as a fupercentage. In essence, this measure is equal to the number
of distinct terms (Types) in the corpus divided by the total
number of terms (Tokens) used [23]. A low TTR is a signal
that there is a lot of repetition in the terms used, a high
TTR ratio is a signal that the vocabulary (distinct terms)
used is rich. A second analysis examined the typical depth
of the terms used from the Read codes in each of the sub-
sets of data. In a final analysis we characterised the Read
code terminology itself, to how many terms at each level
there were available to GPs in each chapter. We then re-
peated this analysis in the Salford data looking at the set of
codes that were actually used from this full set. From this
we were able to determine the extent to which GPs did, or
did not, take advantage of the structure inherent in the
terminology.
Results
In the first analysis, the data was split by the three Read
code sections (diagnosis, procedure and medication) andnction of the Read code chapter.
Kalankesh et al. Journal of Biomedical Semantics 2014, 5:2 Page 5 of 8
http://www.jbiomedsem.com/content/5/1/2the Pareto distributions and power law exponents were
determined. The Pareto plots for these data are shown
below in Figures 1a to c. For these data sets, the values
of the power law exponent for diagnosis, procedures,
and medication were 1.66, and 1.68, and 1.94, with asso-
ciated Type-Token Ratios (TTRs) of 2.7%, 0.32%, 0.35%
respectively. However, the data in Figure 1c was not ef-
fectively modelled by a power law (as determined by a
p-value < 0.1) as there is no region of this curve that
could be modelled by a straight line. A similar analysis
was performed on data from specific sub trees from the
diagnosis chapters. In all cases we found clear Zipfian
behaviour (data not shown) for chapters in the diagnosis
and procedure sections.
It is evident from Figure 1c) that the medication codes
do not show Zipfian behaviour. We therefore explored
the difference between the medication codes and other
codes from two perspectives: the depth of the codes pro-
vided by the coding system itself for different categories
of data (Figure 2), and the depth of codes used for0% 20% 40%
0
1
2
3
4
5
6
7
8
9
A
B
C
D
E
F
G
H
J
K
L
M
N
P
Q
R
S
T
U
Z
a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
s
u
y
M
ed
ic
at
io
n
s
D
ia
g
n
o
se
s
P
ro
ce
d
u
re
s
Figure 3 Percentage of Read codes at each level of granularity as a fu
data set.describing different categories of data by doctors in prac-
tice (Figure 3). In some chapters of Read codes, the hier-
archies are deeper than in others. For example, the highest
depth of hierarchy for medication codes in the coding sys-
tem is 4, whereas the highest depth of hierarchy for diag-
nosis and procedure codes in the coding system is 5. It is
interesting to note that in the medication data all the
codes used had depth 4 and that there were no codes with
depths less than this. This contrasts sharply to the codes
used in procedure and diagnosis which use a range of
depths comparable to those provided in the Read code
hierarchy. This is an indication that the medication data
have been encoded in such a way that information transfer
can be maximised toward satisfying decoder needs (the
speaker has navigated to the roots of the hierarchy to en-
code the information). It can be also interpreted that the
medication Read Code r has been referred to the drug d
only if r can be understood as referring to d by someone
other than the speaker (encoder) as a result of the com-
munication act, an indexical reference system [24].60% 80% 100%
Level 1
Level 2
Level 3
Level 4
Level 5
nction of the Read code chapter as used by GPs in the Salford
Kalankesh et al. Journal of Biomedical Semantics 2014, 5:2 Page 6 of 8
http://www.jbiomedsem.com/content/5/1/2The data were then analysed as a function of the anon-
ymised GP practice identifier. The typical values of ? in
the data ranged from 1.56 to 2.08. Percentage of type
token ratio for aforementioned GP practices ranged from
2.47% to 10.63%. This strongly suggests that the range of
coding vocabulary used by different GP practices varies
considerably in its richness and degree of repetition. In
most of the graphs, two different regions could be recog-
nised, a linear region on the left hand side (the more
uncommon terms) that fits the power law behaviour and
a second region of higher frequency terms; the transi-
tion between these region being the point at which the
graph deviates from the fitted line (Figure 4). A similar
pattern has been observed in a Zipfian analysis of the
British National Corpus (BNC) [25]. In the BNC cor-
pus, the region of more commonly deployed codes was
defined as a core vocabulary  the words commonly
used - and the region of less commonly used codes as
a peripheral vocabulary  words more rarely used. A simi-
lar interpretation can be made of the data from the medical
records. Despite difference in the value of exponents, allFigure 4 The Pareto plots for diagnosis Read codes used from six sep
figure we also show the measured values of ?, the measured Zipfs law expplots have one feature in common: average depth of codes
in the region of core vocabulary is smaller (range 3.3-3.7)
than that found in the regions of peripheral vocabulary
(range 3.6-4.3). The analogy with language would be that
the codes near the top of the Read code hierarchy consti-
tute a core, commonly used, vocabulary, whereas the more
specialist terms found deeper in the hierarchy relate to a
more peripheral and rarely used vocabulary.
Discussion and conclusions
Within the Salford corpus, the usage of Read codes for
diagnosis and process show a power law behaviour with
exponents typical of those seen in natural languages.
This supports the hypothesis being made in this paper
that there are overlaps between the processes involved
in describing medical data (terms chosen from a the-
saurus to describe an encounter between a patient and
a GP) and human communication (words chosen to de-
scribe a concept to a listener). This was not only true of
the complete data sets; it was also seen to be true of the
data from the specific chapters.arate GP practices from 20032006 (denoted as a to f). On each
onent, and the TTR, the type-token ratio.
Kalankesh et al. Journal of Biomedical Semantics 2014, 5:2 Page 7 of 8
http://www.jbiomedsem.com/content/5/1/2However, the story is not completely straightforward.
There was one section of data captured by Read codes
that showed a very different behaviour, namely the medi-
cation data. These data showed no evidence of Zipf s law
behaviour and it would appear that the principle of reach-
ing a balance between the encoding and decoding costs
has broken down. The pattern of code use from the hier-
archy of Read codes is very different for the medication
data compared with process or diagnosis code. All Read
codes used by GPs for encoding the drug information is
from the highest level provided by the hierarchy of Read
Code System. This would suggest that, in the case of medi-
cation information, doctors attribute very high value to
creating minimal ambiguity in the message to the max-
imum extent the coding system allows them. This is per-
haps unsurprising as the prescription data are an input for
another health care professional in the continuum of care
(pharmacist) and any ambiguity in the case of this sensitive
data could be harmful or fatal to a patient. The exact
match between expression and meaning by someone other
than encoder is critical. From this perspective, medication
data seem to behave as an indexical reference in which an
indexical expression e refers to an object o only if e
can be understood as referring to o by someone other
than the speaker as a result of the communicative act.
It is also the case that not all GPs use language in the
same way. It is known that capture of diagnosis informa-
tion is very variable between different GP practices [26].
At this stage, it is difficult to provide detailed explan-
ation reasons for this. It could be that this reflects a dif-
ference in the populations being served by each GP;
however we do not have the information available to us
in this study to allow us to address this. However, it is
suggestive that this form of computational linguistic ana-
lysis could provide useful information on the quality of
data being captured from different GP surgeries. There
is a significant body of work in language processing
looking at power law exponents and how they change
with different qualities of language, an analysis that
could well have useful analogies for these data. At this
stage we do not have the information to determine the
extent to which the signal mirrors the quality of the data
capture by the GPs, but this is clearly something that
would warrant further study.
Therefore, there are aspects of GP records that behave
very like a language and for which it would be appropri-
ate to apply the methodologies of computational linguis-
tics. Our hope is that the development of such methods
could provide important new tools to help assess and
improve the quality of data in the health service.Abbreviations
BNC: British National Corpus; CDF: Cumulative Distribution Function;
GP: General Practitioners; NHS: National Health Service; TTR: Type-Token Ratios.Competing interest
The authors declare that they have no competing interests.
Authors contributions
LRK performed the analyses and helped draft the paper. JPN and JP provided
the data sets for analysis and helped in the data interpretation. AB conceived
of the study, participated in its design and coordination and helped to draft
the manuscript. All authors read and approved the final manuscript.
Acknowledgements
This article has been published as part of thematic series Semantic Mining
of Languages in Biology and Medicine of Journal of Biomedical Semantics.
An early version of this paper was presented at the Fourth International
Symposium on Languages in Biology and Medicine (LBM 2011), held in
Singapore in 2011.
Author details
1School Of Computer Science, University of Manchester, Oxford Road,
Manchester M13 9PL, UK. 2Tabriz University of Medical Sciences, Tabriz, Iran.
3Salford Royal NHS Foundation Trust, Stott Lane, Salford M6 8HD, UK.
4School of Medicine, University of Manchester, Oxford Road, Manchester M13
9PL, UK. 5Northwest Institute for Bio-Health Informatics, University of Manchester,
Oxford Road, Manchester M13 9PL, UK.
Received: 3 September 2012 Accepted: 26 November 2013
Published: 10 January 2014
PROCEEDINGS Open Access
The influence of disease categories on gene
candidate predictions from model organism
phenotypes
Anika Oellrich1*, Sebastian Koehler2, Nicole Washington3, Sanger Mouse Genetic Project3, Chris Mungall1,
Suzanna Lewis3, Melissa Haendel4, Peter N Robinson2, Damian Smedley1
From Bio-Ontologies Special Interest Group 2013
Berlin, Germany. 20 July 2013
* Correspondence: ao5@sanger.ac.
uk
1Wellcome Trust Sanger Institute,
Wellcome Trust Genome Campus,
CB10 1SA Hinxton, UK
Abstract
Background: The molecular etiology is still to be identified for about half of the
currently described Mendelian diseases in humans, thereby hindering efforts to find
treatments or preventive measures. Advances, such as new sequencing technologies,
have led to increasing amounts of data becoming available with which to address
the problem of identifying disease genes. Therefore, automated methods are needed
that reliably predict disease gene candidates based on available data. We have
recently developed Exomiser as a tool for identifying causative variants from exome
analysis results by filtering and prioritising using a number of criteria including the
phenotype similarity between the disease and mouse mutants involving the gene
candidates. Initial investigations revealed a variation in performance for different
medical categories of disease, due in part to a varying contribution of the phenotype
scoring component.
Results: In this study, we further analyse the performance of our cross-species
phenotype matching algorithm, and examine in more detail the reasons why disease
gene filtering based on phenotype data works better for certain disease categories
than others. We found that in addition to misleading phenotype alignments
between species, some disease categories are still more amenable to automated
predictions than others, and that this often ties in with community perceptions on
how well the organism works as model.
Conclusions: In conclusion, our automated disease gene candidate predictions are
highly dependent on the organism used for the predictions and the disease
category being studied. Future work on computational disease gene prediction using
phenotype data would benefit from methods that take into account the disease
category and the source of model organism data.
Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4
http://www.jbiomedsem.com/content/5/S1/S4 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2014 Oellrich et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in
any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver (http://
creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Background
Despite many success stories in the identification of genetic causes for human heritable
diseases, half of the currently described disorders with a presumed genetic etiology are
still without an identified molecular basis [1]. Although the identification of a novel
disease gene rarely leads to immediate, novel treatment options, clearly an understand-
ing of the cellular pathways and networks affected by a genetic mutation is the basis
for developing improved treatment strategies and optimal genetic counseling. To sup-
port the identification of genetic causes, and with that treatment of human heritable
disorders, biological as well as computational methods have been developed [2-7].
However, none of the existing solutions is capable of providing reliable answers for all
diseases and improvements are still needed.
Technology advances have led to solutions enabling rapid and cheap identification of
variants in human genomes and exomes. However, these methods yield long lists of var-
iants reflecting the fact that each individual harbours more than 30,000 variants identifi-
able by exome sequencing, with typically 5% or more of variants not being listed in
databases of variants such as dbSNP. Typical bioinformatic filtering procedures remove
common variants and those deemed to be nonpathogenic, but are not able to narrow the
search down to only a short list of candidates based only on the sequence variants.
In a recent study, we presented the PHenotypic Interpretation of Variants in Exomes
(PHIVE) algorithm that in addition to traditional variant filtering and evaluation also
includes the phenotype manifestations in individuals as well as the signs and symptoms
of diseases [8]. It was shown that including phenotype information into the prioritisa-
tion of candidate genes leads to an up to 54.1 fold improvement over methods purely
based on variant information. To assess the phenotypic suitability of a gene variant,
PhenoDigms phenotype comparison algorithm was used [4]. The study also showed
that the performance of the PHIVE algorithm is influenced by the mode of inheritance
(autosomal dominant vs. autosomal recessive) and by the class of mutation (nonsense
and missense mutations). However, our investigations did not include an evaluation of
the characteristics of the diseases in question.
Ongoing debates highlight that model organisms do not necessarily constitute ideal
fits for certain diseases [9], due to e.g. changes in gene expression [10], but can still
provide valuable insights into a disease even though only part of the phenotypes may
be reproduced in a model organism [11,12]. To facilitate the linkage of model organ-
isms and diseases, ongoing efforts such as the International Mouse Phenotyping
Consortium (IMPC) and the Zebrafish Mutation Project (ZMP) record a range of pre-
defined parameters that are not just restricted to phenotypes related to the disease
area a researcher is studying [13].
The number of automated disease gene candidate prediction tools using cross-species
information is also increasing [14,4,6,5]. The aforementioned tools rely on the availability
of logical definitions for phenotypes that allow their comparison across species [15].
Typically, precision and recall measures for known gene-disease association are reported
to give an indication about the potential of the method and its suitability to the task of
disease gene candidate identification. While Börnigen et al. worked on unbiased evalua-
tion of the tools [16], to our knowledge, no further evaluation for performance of differ-
ent disease categories has been undertaken. Tools that use model organism data for the
prediction are limited not only to the availability of sufficient and unbiased experimental
Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4
http://www.jbiomedsem.com/content/5/S1/S4
Page 2 of 14
data, but are also restricted to disease areas where model organisms recapitulate the
disease and where the phenotype associated genes are orthologous.
In this study, we analysed Exomisers performance with respect to disease categories
provided by Orphanet [17]. As the performance of Exomiser is influenced by the Phe-
noDigm phenotype comparison algorithm, we based our experiments on the evaluation
of PhenoDigm and its applicability to disease categories. Using known gene-disease
associations in the Orphanet and Online Mendelian Inheritance in Man (OMIM) data-
bases [1], we identified areas for further improvements that will consequently influence
Exomisers performance. Although we only currently use PhenoDigms mouse-based
predictions in Exomiser, we plan to take advantage of zebrafish phenotypes amongst
other model organism data in the future as part of our participation in the Monarch
Initiative. Hence, we performed our assessment across both mouse and zebrafish data.
One factor in the poorly performing disease categories was the sub-optimal imple-
mentation of our approach for certain phenotype annotations, leading to missing
phenotype alignments. These will be addressed in future releases. Other clinical pheno-
types were not matched because they can not be accurately observed in the model
organism in question. Interestingly, some perceptions of how well or how easily different
model organisms can be fitted to particular disease areas are mirrored in the evaluation
results. We conclude that automated prediction methods could potentially benefit from
taking into consideration the categories of disease in which semantic model organism
phenotype matching works best.
Results and discussion
Exomiser provides functionality to filter and prioritize gene variant lists using our PHIVE
algorithm which combines phenotype comparisons from PhenoDigm in addition to allele
frequency and pathogenicity scores [8]. Our benchmarking of Exomiser was based on
28,516 known disease-causing mutations from the Human Gene Mutation Database [18].
Using Orphanets disease categorisation [19], we further divided Exomisers evaluation
exome data sets by disease category. Figure 1 shows that Exomisers ability to identify the
disease causing genes using the PHIVE algorithm varies for the different disease cate-
gories. All results fall into the range of 35 to 78%, with best performance in the gastroen-
terological diseases category. Figure 1 also shows the performance of Exomiser if only the
phenotype prioritization for genes is used but not allele frequency and pathogenicity. It is
apparent that the phenotype score works better for some disease categories than for
others. For example, in the case of gastroenterological diseases the phenotype comparison
seems to contribute a lot to the identification of disease gene candidates while in the case
of surgical maxillo facial diseases, the contribution seems to be comparatively small. Note
that not all of Orphanets disease categories are represented due to the limited coverage in
our evaluation set of 28,516 known disease-causing mutations.
As the PHIVE algorithm combines PhenoDigm, allele frequency and pathogenicity
prioritisation as well as some pre-filtering steps, evaluation of just the performance of the
phenotype comparison is problematic. Therefore, we decided to further investigate the
effect of disease category on phenotype comparisons by just looking at the performance of
PhenoDigm. Due to the inclusion of these other steps in Exomiser, we do not expect
observations based on PhenoDigm performance for different disease categories to translate
directly to Exomiser but should indicate potential categories where we may see enhanced
Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4
http://www.jbiomedsem.com/content/5/S1/S4
Page 3 of 14
or reduced performance. In addition, PhenoDigm covers all of OMIM and Orphanet as
well as including zebrafish so we were able to perform a more extensive evaluation of the
influence of disease categories on gene candidate predictions from model organism
phenotypes.
Analogous to the evaluation of Exomiser, we divided the diseases covered in Pheno-
Digm into the disease categories provided by Orphanet and known gene-disease asso-
ciations contained in Orphanet and OMIM. We then assessed precision and recall
over the different ranks of diseases genes and determined the Area Under Curve
(AUC) of the corresponding Receiver Operating Characteristic (ROC) curve. The AUC
measures obtained for the individual disease categories and both the species (mouse
and zebrafish) are presented in Table 1. AUC measures in mouse vary in the range of
[0.774, 0.901] and in zebrafish in the range of [0.540, 0.835]. These results show that
AUC measures calculated over all diseases may mask disease categories that are per-
forming well, e.g. zebrafish performs better than mouse for cardiac malformations
although the overall performance is much worse.
Comparing the two model organisms, the most striking observations are first of all,
that the performance for zebrafish for nearly all disease categories is reduced and sec-
ondly, that performance is much more dependent on the disease category than it is for
the mouse. Given the species-divide between human and zebrafish compared to
mouse, some of this reduced performance and increased variability maybe expected.
Figure 1 Performance of PHIVE score in Exomiser by Orphanet disease category, together with just
the phenotype-based scores. The number of diseases tested for each category are shown in parentheses.
Note that many diseases belong to multiple disease categories. The overall PHIVE performance and the
contribution of the phenotype score used in Exomiser is seen to vary with respect to the disease category.
The highest contribution of the phenotype score is in the category of gastroenterological diseases, the
smallest in the category of abdominal surgical diseases.
Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4
http://www.jbiomedsem.com/content/5/S1/S4
Page 4 of 14
Although many of the organ systems and biological processes are similar in the zebrafish,
some differences obviously exist that will affect certain disease categories more than
others. However, much of the difference could also be due to focus of research using these
different model organisms as well as varying technical difficulties with applying our
semantic comparison approach to the different phenotype ontologies used for human,
mouse and fish.
An additional factor, may be the extra difficulty of assigning orthology between
human and zebrafish genes due to the greater evolutionary distance. In addition, many
Table 1 PhenoDigm performs best for urogenital diseases for mouse and cardiac
malformation for fish out of 31 disease categories.
disease category* diseases
(mouse)
AUC
(mouse)
diseases
(fish)
AUC(fish)
abdominal surgical 104 0.856 (0.336) 67 0.716
(0.033)
Allergic 5 - 0 -
Bone 368 0.870 (0.002) 185 0.650 (0.110)
cardiac 128 0.857 (0.138) 58 0.675
(0.049)
cardiac malformations 34 0.822 (0.221) 23 0.835 (1E-4)
circulatory system 63 0.825 (0.239) 31 0.658 (0.417)
developmental anomalies in
embryogenesis
943 0.852 (0.177) 475 0.673 (1E-4)
endocrine 307 0.874 (0.029) 128 0.629 (0.382)
eye 582 0.864 (0.034) 269 0.646 (0.147)
gastroenterological 74 0.842 (0.391) 36 0.739
(0.031)
haematological 151 0.816 (0.151) 53 0.603 (0.215)
hepatic 41 0.774 (0.011) 8 -
immunological 134 0.843 (0.391) 36 0.540
(0.014)
inborn errors of metabolism 384 0.789 (5E-9) 91 0.646 (0.103)
infectious 3 - 2 -
infertility 41 0.817 (0.154) 18 0.635 (0.496)
neurological 777 0.787 (2E-11) 328 0.630 (0.486)
odontological 44 0.899 (0.078) 18 0.693 (0.161)
otorhinolaryngological 150 0.890 (0.043) 74 0.731
(0.015)
renal 277 0.846 (0.479) 130 0.676
(0.048)
respiratory 65 0.808 (0.126) 35 0.594 (0.135)
skin 418 0.852 (0.161) 154 0.636 (0.442)
surgical maxillo facial 89 0.836 (0.367) 56 0.723 (5E-4)
surgical thoracic 33 0.816 (0.176) 12 0.641 (0.364)
systematic and rheumatological 68 0.832 (0.297) 14 0.592 (0.311)
teratologic 1 - 1 -
tumors 239 0.835 (0.388) 130 0.677
(0.044)
urogenital 62 0.901 (0.050) 31 0.608 (0.207)
all diseases 3728 0.845 1558 0.630
* disease categories according to Orphanet [19];  number of diseases falling into this category with phenotype data for
the orthologue(s) of the associated gene;  AUC, measured on disease-gene associations from OMIMs MorbidMap and
Orphanet curation. Value in brackets shows the p-value of obtained this result compared to those obtained from
randomly selecting the same number of diseases. Significant results (p value <0.05) are shown in bold. Note that one
disease may fall into different categories due to multiple systems affected by disease.
Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4
http://www.jbiomedsem.com/content/5/S1/S4
Page 5 of 14
of the genes are part of a genome duplication event in zebrafish so that one human
disease gene may correspond to two zebrafish genes and it may take disruption of
both to recapitulate the clinical phenotypes.
To investigate some of these issues we analysed the annotations and the corresponding
PhenoDigm matches in more detail for the best, intermediate and worst performing dis-
ease categories. In addition to the calculation of AUC measures, we further investigated
six of the disease categories (three for each species) to obtain a better understanding of
the shortcomings of either the method or the data. For each disease category, we investi-
gated the 10 most common clinical phenotypes and their best matches in the model
organism phenotypes.
Investigation of mouse model prediction results
To identify reasons for the differences in performances with respect to the applied disease
categories, we further investigated the following three categories of diseases: urogenital,
hepatic and neurological diseases. We studied their annotations and the corresponding
best phenotype matches in mouse produced by PhenoDigm, together with potential biolo-
gical reasons for differences in performance. The results for each of the three further
investigated disease categories are shown in Additional File 1 and discussed in more detail
in the following sections.
Urogenital diseases
The ten most frequently occurring clinical Human Phenotype Ontology (HPO) pheno-
types in this category include expected urogenital phenotypes such as Cryptorchidism
as well as others such as Short stature, Microcephaly and Cognitive impairment. The
latter are due to the Orphanet classification allowing diseases to be assigned to multi-
ple categories e.g. many diseases may be classified as both urogenital and neurological
leading to a preponderance of both urogenital and neurological phenotypes when looking
at each individual category.
The common urogenital and other types of clinical phenotypes all matched the expected
mouse phenotypes in Mammalian Phenotype Ontology (MP) (and their more specific
child terms when present) with the exception of Cognitive impairment. Cognitive impair-
ment was the 10th most commonly observed clinical phenotype in this disease category
and is obviously a more difficult phenotype to measure in a mouse model than physical
abnormalities such as Cryptorchidism and there is no directly corresponding MP equiva-
lent term. Hence, PhenoDigm ends up matching numerous general abnormalities of
higher mental function such as increased anxietyrelated response (MP:0001363) which will
lead to Cognitive impairment not being an informative phenotype for selecting specific
mouse model matches.
The specific recall for most of the associated clinical phenotypes increases the likelihood
of mouse models being predicted that are relevant to the urogenital diseases. Assuming
also, that mouse models disrupting the known urogenital disease genes produce a pheno-
copy of the disease, then performance of PhenoDigm would be expected to be good for
this category as was observed.
Hepatic diseases
Useful animal models of liver disease have only very recently been identified [20], so it is
perhaps not surprising that we found that this disease category to be the worst performing
for mouse. Despite the fact that mice do not necessarily constitute a good model
Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4
http://www.jbiomedsem.com/content/5/S1/S4
Page 6 of 14
organism to study hepatic diseases, we still investigated the phenotype matches together
with the predicted disease models for this disease category.
In contast, to some of the other disease categories, the ten most frequently occurring
phenotypes were all consistent for hepatic disease rather than some being a consequence
of Orphanets classification of certain diseases to multiple categories. Eight of these most
frequent occurring clinical phenotypes recalled the expected mouse phenotype as the best
match in MP (and their child terms if they existed). For example, Figure 2a shows Pheno-
Digm results in Hepatomegaly matching enlarged liver, liver hyperplasia and increased
liver weight as the best mouse phenotypes.
The other two concepts mapped sub-optimally or produced completely misleading
results. For example, Figure 2b shows how the HPO concept Pruritus is matched to
abnormal skin physiology as the best hit in mouse as well as all its child terms due
to the lack of logical definitions for Pruritus (HP:0000989) and increased pruritus
(MP:0010072). Although the 28 child terms include the ideal match, the additional
matches will lead to non-specific mouse models being recovered. Finally, the best
matches for the HPO concept Elevated hepatic transaminases (HP:0002910) were
increased liver copper level and increased liver iron level based purely on increased
concentrations of any object in the liver. Even though a corresponding MP concept
exists, increased circulating aspartate transaminase level (MP:0005343), the correct
logical definitions do not yet exist for PhenoDigm to have identified this
relationship.
Figure 2 Relationships between common HPO clinical phenotypes for the hepatic disease class and
MP terms. (a) The HPO term for Hepatomegaly is identified as being equivalent to the MP term for
enlarged liver via their logical definitions. This MP term and its children are identified as the best scoring
matches. (b) Lack of a logical definition for Pruritis leads to the best scoring MP matches being to the
higher level term of abnormal skin physiology and all of its multiple child terms, many of which having no
relationship to pruritis.
Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4
http://www.jbiomedsem.com/content/5/S1/S4
Page 7 of 14
Although the performance could be improved if accurate logical definitions were created
for the two poorly-mapped phenotypes, the fact that the others matched the expected
mouse phenotypes as the best hit suggests that the poor overall performance of Pheno-
Digm in this disease category may be due to mouse mutants of hepatic disease genes not
recapitulating the same phenotypes.
Neurological diseases
In previous studies it has been found that there are sufficient commonalities between
humans and mice to determine disease gene candidates for some of the diseases belong-
ing to the category of neurological diseases, e.g. diseases related to addiction [21].
However, there are still differences between mice and humans related to gene structure
and spatiotemporal expression patterns that may prevent mice being in general applicable
to neurological diseases [12]. Despite a mouse model not faithfully recapitulating a human
disease, the mouse model may still provide insights into the origin of the disease [11].
The ten most commonly occurring HPO annotations in this category included two that
do not have a neurological basis but are due to Orphanets co-classification of diseases:
Short stature and Scoliosis. However, both match the expected terms as the best hit in MP
and their inclusion would therefore not be expected to account for the relatively poor
performance of PhenoDigm for this disease category.
Looking at just the eight neurological phenotypes, four match the expected terms in MP
and their child terms where present: Seizures, Muscular hypotonia, Microcephaly and
Nystagmus. The other four only match high level terms in MP and all their child terms as
the best scoring hits: Cognitive impairment, Intellectual disability, Global developmental
delay and Hyperreflexia. These multiple matches lead to an imprecision when mouse
models are ranked according to their phenotype similarity with the disease. This poten-
tially leads to noisy results as multiple models are associated that are not necessarily rele-
vant for the disease but due to the misaligned phenotypes. These issues in semantically
mapping behavioural phenotypes may account for a large proportion of the poor perfor-
mance in this disease category as opposed to underlying problems with using mice to
model the biology of neurological diseases.
Investigation of zebrafish model prediction results
To identify reasons for the differences in performances with respect to the applied dis-
ease categories, we further investigated the following three categories of diseases: cardiac
malformations, immunological and bone diseases. We studied their annotations and the
corresponding matches from PhenoDigm and the results are summarised in Additional
File 1.
Cardiac malformations
The main reason for zebrafishs adoption as a model organism is the translucency of the
organism in the embryonic stage, allowing in vivo, non-intrusive visualisation of organs as
well as biological processes. Therefore, zebrafish are ideal model systems for studying
developmental diseases and this may go some way to explaining why zebrafish outper-
formed mouse as a model of the congenital cardiac malformations in our analysis.
We observe that the most common clinical phenotype annotations in this disease
category are matched efficiently by PhenoDigm. For example, Abnormality of the aorta
(HP:0001679) matches various more specific aortic abnormalities in the Zebrafish
Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4
http://www.jbiomedsem.com/content/5/S1/S4
Page 8 of 14
Phenotype Ontology (ZP) annotations, whilst the clinical phenotypes Defect in the
atrial septum (HP:0001631) and Ventricular septal defect (HP:0001629) are best
aligned with the zebrafish phenotype abnormally closed atrioventricular node. Tetral-
ogy of Fallot (HP:0001636) in itself comprises four separate phenotypes and here only
matches abnormally aplastic ventricular endocardium epithelium as the best hit which
is only a close association at best. In contrast, Patent ductus arteriosus (HP:0001643)
does not match any sensible zebrafish phenotype but closure of the ductus arteriosis
on birth, allowing the lungs to get their own supply of blood, is known to be a specific
aspect of air-breathing vertebrates [22].
Many of the non-cardiac associated annotations seen in this disease category match
less well to the fish phenotypes but are also less commonly seen and the cardiac
matches alone appear to have been enough to efficiently recall the correct zebrafish
models for most cardiac malformations. For example, no match to Cognitive impair-
ment (HP:0100543) is retrieved and Microcephaly (HP:0000252) is aligned with abnor-
mally decreased thickness cranial nerve VIII as the best match. Future investigation of
why zebrafish phenotypes such as abnormally hypoplastic head were not the best hit
for Microcephaly and some of the learning/memory fish phenotypes were not picked
up for Cognitive impairment may further improve recall.
Immunological diseases
Although the zebrafish immune system closely approximates that of mammals, the
main use of zebrafish in immunology comes from the fact that the embryonic stage
already has a fully competent innate immune system allowing contrasting studies with
the adaptive system [23]. One explanation for the poor performance in this disease
category is that human adult immune phenotypes from a mixture of innate and adap-
tive responses are being compared to zebrafish embryonic innate phenotypes. The
most common clinical phenotypes seen in this category are Splenomegaly
(HP:0001744) and Hepatomegaly (HP:0002240) and these match the expected ZP
terms of abnormally increased size spleen and abnormally increased size liver as the
best scoring hits. However, other common clinical annotations such as Recurrent bac-
terial or respiratory infections (HP:0002718, HP:0002205) are not matched to anything
in the zebrafish annotations beyond generalized immune system abnormalities. Recur-
rent infections suggest a long-lasting loss of protective immunity due to a perturbance
in the adaptive immune system and as described above, this would not be observed in
the embryonic zebrafish stages.
Other common immunological annotations in this category include Lymphadenopa-
thy (HP:0002716), Anemia (HP:0001903), Neutropenia (HP:0001875) and Thrombocyto-
penia (HP:0001873) but none of these match the expected phenotypes in the zebrafish
as the best scoring hit using our approach. The fact that zebrafish lack lymph nodes
explains the first one but there are fish annotated with abnormally present in fewer
numbers in organism nucleate erythrocyte , abnormally present in fewer numbers in
organism neutrophil and abnormally present in fewer numbers in organism thrombo-
cyte, so it would be expected that the clinical phenotypes should have recalled these
fish phenotypes. Investigation and restructuring of the underlying ontologies and/or
logical definitions to pick up these matches would presumably lead to an improvement
in performance for this disease category.
Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4
http://www.jbiomedsem.com/content/5/S1/S4
Page 9 of 14
Bone diseases
Despite having a different skeletal organisation, zebrafish has recently emerged as a use-
ful complementary model for bone research due to the ability to study in vivo, processes
such as osteogenesis and mineralization thanks to the existence of osteoblast-specific
reporter lines [24].
Using PhenoDigm, the performance was mid-range for this disease category relative
to the others. Looking at the most common skeletal clinical phenotypes we find that
some match to the equivalent concepts in zebrafish whilst others warrant further
attention. For example the most common clinical phenotype, Short stature
(HP:0004322), is completely mis-matched to abnormally decreased height enterocyte as
the best match. Fixing our approach such that abnormally decreased length whole
organism is the best match would probably lead to a dramatic increase in performance.
Other matches such as Scoliosis (HP:0002650) with abnormally curved lateral vertebral
column, Micrognathia (HP:0000347) with abnormally aplastic dentary and Brachydac-
tyly syndrome (HP:0001156) with abnormally aplastic pectoral fin skeleton are reason-
able considering the evolutionary distance.
Conclusions
Exomiser is a tool to narrow down gene candidate lists that have been identified in
exome analyses using cross-species phenotype comparisons amongst other sources of
evidence. Here we investigated the underlying PhenoDigm algorithm for different disease
categories to understand where the approach is currently working well and to identify
areas for further improvement. We demonstrated that the phenotype comparisons work
better for some disease categories than for others. Furthermore, the prediction results
depend on the organism and when automatically predicting disease gene candidates
careful consideration is required as to which organism to apply for the predictions.
However, it is somewhat difficult to disentangle whether performance differences exist
due to differences in biology, the annotation methods used for each species or the focus
of annotations for mouse and fish.
In addition to the identified biological restrictions that partially mirror community per-
ceptions of how well the model organism can be fitted to human diseases, we showed that
the underlying methodology still needs improvements. Even though a lot of work has
been done in this direction, more logical definitions are needed in addition to improving
the quality of the existing definitions to improve semantic mapping between the species-
specific phenotype ontologies. Future work, will focus on improving these definitions and
will undoubtably lead to improvements in the performance of PhenoDigm and Exomiser.
Even with a perfectly aligned set of phenotype ontologies, our results highlight that it
will be dangerous to discount a model just because it does not perfectly match all the
clinical phenotypes of the disease. For example, matches to clinical phenotypes such as
lymphadenopathy were not seen in our assessment of the zebrafish results due to the
lack of lymph nodes in fish rather than our alignment approach. In addition, different
areas of interest of the researchers who phenotype the models need to be taken into
account when using model organism to understand the genetic basis of disease i.e. par-
ticular phenotypes may not have been assessed. In conclusion, smarter tools are
required that take into account the differences between species and accumulate
Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4
http://www.jbiomedsem.com/content/5/S1/S4
Page 10 of 14
predictions not only over multiple species but apply a sorting with respect to the
applicability of the species in the particular area of disease.
Methods
Benchmark data: MorbidMap and Orphanet
Assessing the performance of a gene prediction or prioritisation algorithm requires
benchmark data containing established gene-disease associations. One database con-
taining manually confirmed associations between human diseases and genes is OMIM
[1]. The human-centric gene-disease associations from OMIM are available via a
download file called MorbidMap [25]. In addition we used the disease-gene associa-
tions curated by Orphanet. Both OMIM and Orphanet have HPO annotations that can
be used by PhenoDigm and both were downloaded on 20 July 2013 and Mouse
Genome Database (MGD)s orthology file (see [26]) was used to convert the genes into
mouse-specific gene identifiers that can be used for evaluation purposes. The final
dataset contained a total of 3,429 diseases associated with 2,662 unique genes, which
mapped to 2,772 orthologous genes in mouse and 1862 in fish.
Generating prediction results with PhenoDigm
PhenoDigm [4] uses phenotype descriptions of human heritable disease and individual
animal models to predict potentially gene candidates that may be causative for a dis-
eases. The PhenoDigm algorithm uses a pairwise semantic similarity based on pheno-
type ontology annotations, such as HPO or MP, and prioritises genes according to this
similarity measure. Applying the PhenoDigm method, a database was generated con-
taining all the results displayed in the online web interface [27]. Instead of regenerating
the data, we used the data built from 20 July 2013 so that the results presented here
correspond with the current publicly available data.
Dividing diseases into sets according to Orphanet categorisation
To divide the disease into sets that are biologically meaningful, we downloaded the Orpha-
net categorisation files from the Orphanet data download page [19] on 18 July 2013. We
downloaded and processed 31 data files, one for each of the high level disease categories
in the Orphanet categorisation. Each of the files contains a number of diseases that may
or may not be referenced to OMIM. Furthermore, a disease may not only be assigned to
one category and instead be mentioned in multiple files. For example, X-linked myotubu-
lar myopathy (OMIM:#310400) is categorised as a rare eye and neurological disorder
because the most prominent symptoms include weakness, hypotonia and respiratory
failure, as well as external ophthalmoplegia.
We note here that the Orphanet web interface also provides a category of sucking/
swallowing disorders. This disease category was not included here as no categorisation
file was provided on the Orphanet download page [19].
Assessing PhenoDigms performance according to disease categories
To determine PhenoDigms performance, we applied ROC curves based on the gene-
disease associations (see Benchmark data: MorbidMap and Orphanet ). We divided
the diseases into sets according to Orphanets categorisation (see Dividing diseases into
sets according to Orphanet categorisation) and consequently generated 31 evaluation
Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4
http://www.jbiomedsem.com/content/5/S1/S4
Page 11 of 14
sets. PhenoDigms ranking was then compared using the 31 evaluation sets corre-
sponding to each of the disease categories by determining true and false positive
counts individually for each rank. As true positive counts a gene that is associated to a
disease in MorbidMap or Orphanet. Conversely, a gene that is not mentioned in
MorbidMap or Orphanet for a particular disease counts as false positive. We note
here, that gene-disease associations may be counted as falsely identified connections,
even though there is a relationship but it is not yet confirmed. However, we assume
that this number is relatively small compared to the large number of possible combina-
tions of genes and diseases and assume that our evaluation procedure is still appropri-
ate. As a consequence, the true predictive rates provided here may be lower than they
are in reality. To test the significance of each ROC analysis we performed 50 simulations
per disease category where a set of diseases of the same size as the evaluation set was
randomly chosen. These simulations provided a mean and standard deviation for the
random distribution of scores for each evaluation set and these were used to calculate a
p-value for the obtained result.
Manual assessment of six disease categories
Further investigations into individual disease categories were necessary to identify
potential shortcomings in either method or data. We chose six categories of diseases
based on the worst, best, and one intermediate AUC score for each species (mouse
and zebrafish). Two curators assessed the phenotype matches to either mouse or fish
(see Additional File 1) for the ten most frequently occuring phenotypes accumulated
over all the diseases falling into this category. The matches were assessed with respect
to their biological correctness and whether they were sufficiently suitable to identify
models from the respective organism.
Additional material
Additional file 1: Annotations and their matches for urogenetial, hepatic and neurological diseases with
respect to mouse and fish models. Excel sheet that contains the HPO annotations for the six further
investigated disease categories: urogenital, neurological, hepatic, cardiac malformations, bone and immunological.
In addition to the ten most frequent HPO annotations, we included the best scoring semantic matches to the
respective model organism (either MP or ZP) as well as the frequency of this annotation.
List of abbreviations
Mouse Genetics Project (MGP), Mouse Genome Database (MGD), Online Mendelian Inheritance in Man (OMIM),
Mammalian Phenotype Ontology (MP), Human Phenotype Ontology (HPO), Receiver Operating Characteristic (ROC),
Area Under Curve (AUC), Standard Operating Procedure (SOP),
Competing interests
The authors declare that they have no competing interests.
Authors contributions
AOE and DS designed the study, as well as implemented all required scripts. CM developed the PhenoDigm software
and NW, MH contributed annotation datasets. PR and SK contributed to the analysis of the results. All authors
contributed to the final manuscript.
Acknowledgements
This work was supported by core infrastructure funding from the Wellcome Trust and National Institutes of Health
(NIH) grant [1 U54 HG006370-01].
Declarations
Publication in this supplement was support by National Institutes of Health (NIH) grant [1 U54 HG006370-01]. This
article has been published as part of the Journal of Biomedical Semantics, Volume 5 Supplement 1, 2013: Proceedings
Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4
http://www.jbiomedsem.com/content/5/S1/S4
Page 12 of 14
of the Bio-Ontologies Special Interest Group Meeting 2013. The full contents of the supplement are available online at
http://www.jbiomedsem.com/supplements/5/S1.
This article has been published as part of Journal of Biomedical Semantics Volume 5 Supplement 1, 2014: Proceedings
of the Bio-Ontologies Special Interest Group 2013. The full contents of the supplement are available online at http://
www.jbiomedsem.com/supplements/5/S1.
Authors details
1Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus, CB10 1SA Hinxton, UK. 2Institute for Medical
Genetics and Human Genetics, Universitaetsklinikum Charite, Augustenburger Platz 1, 13353 Berlin, Germany. 3Berkeley
Bioinformatics Open-Source Projects, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, CA 94720 Berkeley,
USA. 4Ontology Development Group, OHSU Library, Oregon Health & Science University, 3181 S.W. Sam Jackson Park
Rd, OR 97239 Portland, USA.
Published: 3 June 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Zhang et al. Journal of Biomedical Semantics 2014, 5:33
http://www.jbiomedsem.com/content/5/1/33RESEARCH Open AccessNetwork-based analysis reveals distinct association
patterns in a semantic MEDLINE-based
drug-disease-gene network
Yuji Zhang1*, Cui Tao2, Guoqian Jiang3, Asha A Nair3, Jian Su4, Christopher G Chute3 and Hongfang Liu3Abstract
Background: A huge amount of associations among different biological entities (e.g., disease, drug, and gene) are
scattered in millions of biomedical articles. Systematic analysis of such heterogeneous data can infer novel
associations among different biological entities in the context of personalized medicine and translational research.
Recently, network-based computational approaches have gained popularity in investigating such heterogeneous
data, proposing novel therapeutic targets and deciphering disease mechanisms. However, little effort has been
devoted to investigating associations among drugs, diseases, and genes in an integrative manner.
Results: We propose a novel network-based computational framework to identify statistically over-expressed
subnetwork patterns, called network motifs, in an integrated disease-drug-gene network extracted from Semantic
MEDLINE. The framework consists of two steps. The first step is to construct an association network by extracting
pair-wise associations between diseases, drugs and genes in Semantic MEDLINE using a domain pattern driven
strategy. A Resource Description Framework (RDF)-linked data approach is used to re-organize the data to increase
the flexibility of data integration, the interoperability within domain ontologies, and the efficiency of data storage.
Unique associations among drugs, diseases, and genes are extracted for downstream network-based analysis. The
second step is to apply a network-based approach to mine the local network structure of this heterogeneous
network. Significant network motifs are then identified as the backbone of the network. A simplified network based
on those significant motifs is then constructed to facilitate discovery. We implemented our computational
framework and identified five network motifs, each of which corresponds to specific biological meanings. Three
case studies demonstrate that novel associations are derived from the network topology analysis of reconstructed
networks of significant network motifs, further validated by expert knowledge and functional enrichment analyses.
Conclusions: We have developed a novel network-based computational approach to investigate the heterogeneous
drug-gene-disease network extracted from Semantic MEDLINE. We demonstrate the power of this approach by
prioritizing candidate disease genes, inferring potential disease relationships, and proposing novel drug targets,
within the context of the entire knowledge. The results indicate that such approach will facilitate the formulization
of novel research hypotheses, which is critical for translational medicine research and personalized medicine.* Correspondence: yuzhang@som.umaryland.edu
1Division of Biostatistics and Bioinformatics, University of Maryland
Greenebaum Cancer Center and Department of Epidemiology and Public
Health, University of Maryland School of Medicine, Baltimore, MD, USA
Full list of author information is available at the end of the article
© 2014 Zhang et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited.
Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 2 of 13
http://www.jbiomedsem.com/content/5/1/33Background
A large amount of associations among biomedical en-
tities are scattered in biomedical literature. Systematic
analysis of such heterogeneous data provides biomedical
scientists with unprecedented opportunities to infer novel
associations among different biological entities in the
context of personalized medicine and translational re-
search studies. MEDLINE (http://www.nlm.nih.gov/bsd/
pmresources.html), for instance, currently contains more
than 22 million citations of biomedical literature. Semantic
MEDLINE is a knowledge base consisting of associations
automatically extracted from MEDLINE by integrating
document retrieval, advanced natural language processing
(NLP), and automatic summarization and visualization
[1]. However, it is computationally challenging to perform
queries directly from Semantic MEDLINE where associa-
tions among different biomedical entities are very complex
yet sparse. It is also very difficult to investigate those asso-
ciations at a large scale. Advance informatics approaches
have the potential to fill gaps between knowledge needs of
translational researchers and existing knowledge discovery
services.
In Semantic MEDLINE, biomedical entities and associa-
tions are semantically annotated using concepts in the
Unified Medical Language System (UMLS) [2]. The se-
mantic information defined in the UMLS can be further
leveraged to extract associations among concepts in spe-
cific domains and identify domain patterns for specific
studies through advanced computational methods such as
network-based analysis.
In the last decade, network-based computational ap-
proaches have gained popularity and become a new
paradigm to investigate associations among drugs, dis-
eases, and genes. Applications of these approaches in-
clude disease gene prioritization [3-5], identification of
disease relationships [6,7] and drug repositioning [8,9].
However, majority of these approaches focus on rela-
tionships between only two kinds of entities (e.g., asso-
ciation between gene and disease). For instance, Hu and
Agarwar [10] created a human disease-drug network
based on genomic expression profiles collected from
the Gene Expression Omnibus (GEO) (http://www.ncbi.
nlm.nih.gov/geo/). In total, 170,027 interactions between
diseases and drugs were considered significant, including
645 disease-disease, 5,008 disease-drug, and 164,374 drug-
drug associations. These expression-based associations
among diseases and drugs could serve as a backend
knowledge base to facilitate discovery. Bauer-Mehren
et al. [11] developed a comprehensive disease-gene as-
sociation network by integrating associations from sev-
eral sources that cover different biomedical aspects of
diseases. The results indicate a highly shared genetic
origin of human diseases. Functional modules were also
detected in several Mendelian disorders as well as incommon diseases. To systematically analyze drug-disease-
gene relationships, Daminelli et al. [12] proposed a
network-based approach to predict novel drug-gene
and drug-disease associations by completing incom-
plete bi-cliques in the network. This approach holds
great potential for drug repositioning and discovery of
novel associations. However, the analysis was limited
to only certain associations among drugs, genes, and
diseases (e.g., drug-disease and drug-gene associations). A
network-based investigation of all pair-wise associations
among these entities is necessary to understand the com-
plexity of existing associations and to infer novel associa-
tions within the context of the whole knowledgebase.
Network-based computational approaches enable us
to analyze heterogeneous networks such as drug-disease-
gene networks by decomposing them into small sub-
networks, called network motifs (NMs) [13]. NMs are
statistically significant recurring structural patterns found
more often in real networks than would be expected in
random networks with the same network topologies. They
are the smallest basic functional and evolutionarily con-
served units in biological networks. Our hypothesis is that
NMs of a network are the significant sub-patterns that
represent the backbone of the network, which serves as
the focused portion out of thousands of nodes (e.g., drugs,
diseases, and genes,) [14,15]. These NMs could also form
large aggregated modules that perform specific functions
by forming associations in overlapping NMs.
In this paper, we propose a network-based compu-
tational framework to analyze the complex network
formed by a large amount of associations. We focus on
a heterogeneous drug-disease-gene network derived
from Semantic MEDLINE and investigated underlying
associations using network-based systems biology ap-
proaches. Three case studies demonstrate that our ap-
proach has potential to facilitate formulization of novel
research hypotheses, which is critical for translational
medicine research. In the following, we first present
Materials and methods. We then describe the results
and case studies in detail.
Materials and methods
To comprehensively investigate the integrated drug-
disease-gene network formed by associations available in
Semantic MEDLINE, we propose the following two-step
computational framework: (1) extraction and optimization
of drug-disease-gene network in Semantic MEDLINE; (2)
network topology analysis of this heterogeneous network
at two levels: statistics and degree distribution of high-
confidence association networks, and distinct pattern de-
tection at NM level. In this section, we first describe the
steps to extract association network data from MEDLINE
database, followed by a description of the proposed
network-based approach to investigate this heterogeneous
Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 3 of 13
http://www.jbiomedsem.com/content/5/1/33drug-disease-gene association network. Figure 1 illustrates
the steps of the proposed approach.
Data sources and preprocessing
Extraction of association data from Semantic MEDLINE
Semantic MEDLINE currently contains more than 56
million associations extracted from MEDLINE citations
and consists of eight tables, including concepts, concept
semantic types, concept translations, predication, predi-
cation arguments, and sentences. Data from different
tables need to be joined in order to obtain information
for a particular association between two entities. The
database contains an all-embracing joined table that pro-
vides information about associations (source concept,
predicate, and object concept), and their source PubMed
IDs (PMIDs).
We optimize and reorganize the relevant data in
Semantic MEDLINE into the Resource Description
Framework (RDF) format. Based on the UMLS semantic
types and groups [16], we extract unique associations
among drugs, diseases, and genes, and represent them in
six views in relational database tables. We then use the
Web RDF transformation tool D2R server to convert
the six views into RDF triples through a D2RQ mappingovarian cancer
5000 ci ons
Retrievel
Sema
Network
Summariza on
Significant N
Evalua on
 Biological interpret on 
of NMs
 Core network 
inves on
Degree distri on
Hub nodes analysis
0 100 200 300 400 500 600 700 800
0
20
40
60
80
100
120
140
Drug
Degree
No
de
s
0 200 400 600 800 1000 1200
0
500
1000
1500
2000
2500
Disease
Degree
No
de
s
Figure 1 Overview of the network-based computational framework fofile (http://d2rq.org/d2r-server). This mapping file spe-
cifies the mappings between those six relational data-
base table schemas and the output RDF graphs [17]. A
detailed description of this approach is described in our
previous work [18]. These six tables are used as prelim-
inary association data resources including all unique
associations from Semantic MEDLINE.
Data preprocessing using FDA-approved drugs in DrugBank
Since the extraction accuracy of associations in Semantic
MEDLINE is about 77% (precision is 76% to 96%, and
recall is 55-70%) [19], a filtering strategy is applied to
extract high-confidence association data using the FDA-
approved drug list from DrugBank, a database contain-
ing drug information and the corresponding drug target
and treatment indication information [20]. As of July 31
2012, the database contains 1,578 FDA-approved drug
entries, including 131 FDA-approved biotech drugs, and
1,447 FDA-approved small molecule drugs. We extract as-
sociations involving these FDA-approved drugs from each
drug-related association table. After manually removing
generic and nonsensical terms in the association tables
(e.g., gene, homologous gene, and protein), we limit the
drug-drug, drug-gene, and drug-disease associations ton c Medline
 of rela ons
Op miza on
Associ on Table
NM Detec on
Ms Integrated 
Drug-gene-disease Network
Confidence filtering
r an integrated drug-disease-gene network.
Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 4 of 13
http://www.jbiomedsem.com/content/5/1/33those involved in the 1,578 FDA-approved drugs. Based
on the filtered drug-gene and drug-disease associations,
we generate related gene and disease lists and then ob-
tained gene-gene, disease-disease, and gene-disease associa-
tions using these genes and diseases. This filtering strategy
enables us to focus on associations related to FDA-
approved drugs only in this study. These associations are
then analyzed by the proposed network-based approach.
Network motif analysis
Network motifs are topologically distinct subnetwork
patterns that are present more frequently in true net-
works than in random networks [21]. They are usually
well conserved and possess specific processing tasks in
same types of networks. For example, in gene regulatory
networks, the same set of network motifs have been
repeatly identified in diverse organisms from bacteria to
human [22]. The hypothesis is that network motifs were
independently selected by evolutionary processes in a
converging manner and have characteristic dynamical
functions [23]. This suggests that network motifs serve
as building blocks of in gene regulatory networks that
are beneficial to the organism.
In this study, we extend network motif analysis to the
disease-drug-gene network. Six different types of associ-
ations among drugs, diseases, and genes are integrated
into a heterogeneous disease-drug-gene network. In this
network, nodes represent biomedical entities stored in
the RDF triples (i.e., diseases, drugs, or genes in subject
and object), and edges represent associations between
two biomedical entities (i.e., relationships in predicate).
For simplicity, we consider all associations as undirec-
tional association relationships in this study, discarding
the directionality and types in the original RDF graph. In
other words, as long as there is an association between
two nodes, we consider there is an edge between these
two nodes. We hypothesize that even within such sim-
plified disease-drug-gene association network, network
motifs in the network can (1) represent basic inter-
relationships among diseases, drugs, and genes; (2) re-
flect a framework in which particular functions are
achieved efficiently. Specifically, we focus on three-node
network motifs in this disease-drug-gene network since
they are the building blocks for larger size network motifs
(number of nodes > 3) [24]. All connected subnetworks
containing three nodes in the association network are col-
lated into isomorphic patterns [25], and the frequency of
the patterns are counted. We also generated 1000 random
networks from the original network by switching edges
between vertices and preserving the number of edges be-
tween types of nodes (i.e., disease, drug and gene). By the
default of FANOMD algorithm, if the number of occur-
rences for each pattern is at least five in the real network,
which is significantly higher than randomized networks,the pattern is considered to be a network motif. Statistical
significance test is performed by computing the fraction of
randomized networks in which the pattern appears at least
as often as in the interaction network [24]. The z score is
calculated using the following equation:
Z ¼ Nreal? Nrandh i
?rand
ð1Þ
where Nreal is the number of times one three-node sub-
network is detected in the real network, Nrand is the
mean number of times this subnetwork is detected in
1000 randomized networks, and ?rand is the standard devi-
ation of the number of times this subnetwork is detected
in randomized networks. The p value of a motif is the num-
ber of random networks in which it occurs more often than
in the original networks, divided by the total number of
random networks. A pattern with p ? 0.05 is considered
statistically significant. This network motif discovery pro-
cedure is performed using the FANMOD tool [26].
Construction of the core drug-disease-gene network
It has been shown that in gene regulatory networks, for
each network motif, the majority of matches overlap and
aggregate into homologous motif clusters [27]. Many of
these motif clusters largely overlap with modules of
known biological processes [28]. The clusters of overlap-
ping matches of these motifs aggregate into a superstruc-
ture that presents the backbone of the network and is
assumed to play a central role in defining the global topo-
logical organization. Accordingly, we aggregate matches of
significant network motifs into a core drug-disease-gene
network. In this core network, we investigate the distribu-
tion of the connectivity degree of different types of nodes.
Nodes with significantly larger number of links in the
network are called hub nodes, which is critical in the
information flow exchange throughout the entire network.
Results
An integrated drug-disease-gene network reconstructed
from Semantic MEDLINE
We constructed a drug-disease-gene network with the
following two steps:
First, we extracted unique association data from
Semantic MEDLINE. Using a use-case driven database
optimization approach developed in our previous work
[18], we extracted six different types of associations
from Semantic MEDLINE database. Table 1 shows basic
statistics of these six groups of associations. As illustrated
in Table 1, the number of unique associations (the Unique
Association column) for each type of associations is
significantly less than the number of total associations
(the Record column). Since the prediction accuracy of
Semantic MEDLINE is approximately 77% [29], we used a
Table 1 Statistics of the six extracted association types
Association type Record in Semantic MEDLINE Unique associations Associations involving
FDA-approved drugs
Unique entity number
Disease-Disease 2,516,049 843,221 1684 2,248
Disease-Gene 206,155 111,117 21,444 5,954
Disease-Drug 3,021,256 1,277,879 54,996 3,414
Drug-Gene 398,572 248,491 3758 1,451
Drug-Drug 4,780,394 1,900,576 266 382
Gene-Gene 108,035 49,593 2169 2,792
Total 11,030,461 4,430,877 84,317 7,2431
1This is the unique number of entities by summarizing all the associations.
Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 5 of 13
http://www.jbiomedsem.com/content/5/1/33filtering strategy to focus on associations involving FDA-
approved drugs for downstream network-based analysis.
Second, we constructed association related data involving
FDA-approved drugs. We applied the filtering strategy
discribed in the Materials and methods section to extract
association data involving FDA-approved drugs from the
unique association data set. As shown in the Associations
Involving FDA-approved Drugs column in Table 1, the
association number of each table was further reduced.
We used this focused association data to construct an
integrated disease-drug-gene network for downstream
network-based analysis.
Network topology analysis of the core drug-disease-gene
network
The network motif analysis was performed on the inte-
grated disease-drug-gene network obtained in SectionFigure 2 Degree distribution of three biomedical entities: drug, geneAn integrated drug-disease-gene network reconstructed
from Semantic MEDLINE. Since the network contains
thousands of associations among 865 drugs, 2791 genes,
and 3578 diseases (Table 1), it is too complex for a direct
visualization. We overcame this problem by identifying
enriched network motifs and interpreting them through
an enhanced visualization. Out of this heterogeneous
network consisting of 84,317 associations among 7,234 en-
tities (including drugs, diseases, and genes), five significant
network motifs were identified. Figure 2 presents de-
tailed statistics on these network motifs. The matches of
these network motifs were extracted and number of
matches for each network motif was counted (Num-
ber of Matches column in Figure 2).
Based on the network motifs identified in the analysis,
we constructed a core disease-drug-gene network aggre-
gated from significant network motif instances. We then, and disease.
Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 6 of 13
http://www.jbiomedsem.com/content/5/1/33investigated the degree distribution of different types of
entities in the integrated network. Figure 3 represents
the degree distribution of disease, drug, and gene nodes
in the core drug-disease-gene network. All three distri-
butions follow the power-law distribution, indicating
that networks related to different types of nodes are
scale-free. The majority of the nodes in the network
have only a few (less than 10) links but few other
nodes have a large number of links. Such distributions
have been observed in many studies of biological(A
(B)
Figure 3 Subnetworks extracted from NM 1. (A) Overview of the subne
associated with Malignant neoplasm of prostate and tumor growth. (C)
infection and multicentric Castleman's disease.networks [30]. Our analysis demonstrates for the first
time that in an integrated network consisting of het-
erogeneous associations, the scale-free network struc-
ture still holds. The hub nodes (i..e, the nodes have a large
number of links) can provide scientists future research
directions.
Local network structure: from network to network motif
The five significant network motif patterns in Figure 2
have strong biological meanings and could suggest)
(C)
twork, consisting of 126 diseases and 79 genes. (B) Subnetwork
Subnetwork associated with communicable diseases, West Nile viral
Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 7 of 13
http://www.jbiomedsem.com/content/5/1/33scientists future directions in their research field. We
provided three case studies in the following sections to
illustrate results based on three significant network
motifs.
Case study 1 - prioritization of disease genes
We first investigated whether the network motif analysis
could help prioritize disease genes based on the associa-
tions between diseases and their surrounding genes. One
example is Network Motif 1 (NM 1) in Figure 2, in
which two diseases that are associated with each other
are also associated with one common disease gene. This
indicates that diseases identified to be associated in lite-
rature are more likely to share same associated disease
genes. To further investigate the relationships highlighted
by NM 1, We extracted all associations relationships
among 126 diseases and 79 genes in NM 1. In total, there
are 71 disease-disease, 853 disease-gene, and 3 gene-gene
associations (Figure 4(A)) in this subnetwork, suggesting
that diseases that are associated with each other are more
likely to associate with a group of common disease genes.
For instance in Figure 4(B), Malignant neoplasm of pros-
tate shares all 35 associated genes with tumor growth.
Similar findings have also been discovered in other studies
demonstrating same functional modules/pathways being
affected in similar diseases [6,31,32]. There are 10 genes
only associated to tumor growth in literature. Such in-
formation will help scientists generate testable hypotheses
of possible roles of these genes in prostate cancer research.
Another example is shown in Figure 4(C), where commu-
nicable diseases was identified to have common associ-
ated genes with both West Nile viral infection and
multicentric Castlemans disease. Thirteen genes associ-
ated only with communicable diseases can be considered
as candidate disease genes for the other two diseases and
help scientists design future exploratory experiments. The
detailed network information is presented in Additional
file 1: File S1.
Case study 2 - inference of disease relationships
Very interestingly, we also identified another similar
disease-gene network motif in our analysis (NM 4). The
only difference between NM 1 and NM 4 is that NM 4
doesnt have the associations between two diseases
themselves. We extracted all associations among 2,664
diseases and 1,122 genes in NM 4. In total, there are 860
disease-disease, 17,242 disease-gene, and 310 gene-gene
associations in this subnetwork (Figure 5(A)). Based on
the guilt by association rule  diseases similar to each
other are more likely to be affected by the same genes/
pathways, two diseases involved in the same NM 4 are
more likely to be similar/associated than other diseases
[6]. For instance in Figure 5(B), Kidney Failure and
skin disorder are associated with a group of fivecommon associated genes. A wide variety of different
skin disorders have been observed in patients with kid-
ney diseases [33]. One example is the psoriasis disease.
During the treatment of psoriasis with fumaric acid
derivatives, patients could develop acute kidney failure
[34]. In the subnetwork that consists of first neighbors
of these two diseases, psoriasis is also included and has
common associated genes with both kidney failure and
skin disorder. Some genes in the network are associ-
ated with one of these diseases only but not both. To
investigate enriched biological functions/processes, we
performed functional enrichment analysis on neighbor
genes of three diseases with Ingenuity Pathway Analysis
(IPA) Suite (http://www.ingenuity.com/). These genes are
enriched in kidney-related disease categories (Table 2).
Although a major portion of neighbor genes are related
to skin disorder or psoriasis only, they have been an-
notated with kidney related dysfunctions in the IPA
database. Given the fact that associations among thou-
sands of diseases are complex yet incomplete, the in-
ferred association relationships based on our network
motif-based analysis can mine the significant network
topology properties of association networks and guide
scientists to investigate significant association relation-
ships in future experiments. The detailed network infor-
mation is presented in Additional file 2: File S2.
Case study 3  Drug repositioning
Network Motif 2 (NM 2) suggests another association
pattern between diseases and drugs, in which two dis-
eases associated with each other are targets for the same
drug. It has been shown by Suthram et al. [7] that diseases
with significant correlations based on mRNA gene expres-
sion data also share common drugs. This NM supports
the hypothesis that similar diseases can be treated by same
drugs, allowing us to make hypotheses for drugs reposi-
tioning purpose. We extracted all associations among 468
disease and 162 drugs in NM 2. In total, there are 279
disease-disease, 8,730 disease-drug, and 14 drug-drug
associations in this subnetwork (Figure 6(A)). We further
investigated whether any drugs or diseases were hub
nodes in this subnetwork. In Figure 6(B), Alzheimers
Disease and nervous systems disorder are hub diseases
surrounded by 51 FDA-approved drugs. Both diseases are
associated with 20 common drugs, while nervous systems
disorder has associations with additional 31 drugs. These
drugs can be considered repositioned for treatment of
Alzheimers Disease since it is a central nervous system
disorder characterized by the presence of neurofibrillary
tangles, neuritic plaques and dystrophic neurites in the
brain [35]. In Figure 6(C), we observed two hub drugs
surrounding by 129 diseases, 16 of which have associa-
tions with both drugs. Dobutamine is a sympathomimetic
drug used in the treatment of heart failure and cardiogenic
(A)
(B)
Figure 4 Subnetworks extracted from NM 4. (A) Overview of the subnetwork, consisting of 2,664 diseases and 1,122 genes. (B) Subnetwork
associated with Kidney Failure and skin disorder.
Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 8 of 13
http://www.jbiomedsem.com/content/5/1/33
(A)
(B) (C)
Figure 5 Subnetworks extracted from NM 2. (A) Overview of the subnetwork, consisting of 468 disease and 162 drugs. (B) Subnetwork
associated with Alzheimers Disease and nervous systems disorder. (C) Subnetwork associated with Dobutamine and Doxorubicin.
Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 9 of 13
http://www.jbiomedsem.com/content/5/1/33shock. Doxorubicin is a drug used in cancer chemother-
apy. Chemotherapy side effects may increase the risk of
heart disease in cancer patients [36]. This series of under-
lying connections can provide clinicians potential side
effects related to certain drug treatment. This could take
years to study in the clinic to identify such side effects.
The results derived from our approach can serve as
in silico exploratory analysis to guide such studies. Thedetialed network information is presented in Additional
file 3: File S3.
Three-gene network motif (NM 3) was also identi-
fied in this heterogeneous network. This NM is a very
common motif pattern in the protein-protein inter-
action network or gene regulatory network [37,38], in-
dicating that NM detection analysis of heterogeneous
networks can identify significant NMs even enriched
Table 2 Enriched disease and disorder categories in IPA analysis
Category p-value Molecules
Renal Inflammation 6.62E-09 VEGFA,COL4A5,CD40LG,APCS,IL1RN,CLU,MYH9,COL4A4,VDR,ACTN4,NFKB1,TNF,FAS
Renal Nephritis 6.62E-09 VEGFA,COL4A5,CD40LG,APCS,IL1RN,CLU,MYH9,COL4A4,VDR,ACTN4,NFKB1,TNF,FAS
Congenital Heart Anomaly 3.41E-06 VEGFA,HSPG2,TRIM21,EDNRA,ECE1
Liver Cirrhosis 4.13E-06 ADAM17,CD40LG,C5AR1,EDNRB,BSG,PTAFR,TNF,CCR7
Glomerular Injury 5.22E-06 VEGFA,CLU,MYH9,ACTN4
Cardiac Infarction 6.38E-06 PON1,BCL2L1,CD40LG,IL1RN,HSPA1A/HSPA1B,CLU,TNNI3,TNF,LRP1
Renal Atrophy 7.66E-06 CD40LG,EDNRB,FGF23,EDNRA,VDR,AQP2
Liver Damage 9.94E-06 BCL2L1,NLRP3,BSG,IL1RN,NFKB1,TNF,FAS
Liver Proliferation 1.75E-05 VEGFA,SOCS3,EDNRB,IL1RN,EDNRA,NFKB1,TNF,FAS
Pulmonary Hypertension 3.13E-05 EDNRB,IL1RN,KIT,EDNRA
Liver Hepatitis 4.73E-05 BCL2L1,IL23A,TNF,CCR7,FAS
Liver Necrosis/Cell Death 6.57E-05 SOCS3,BCL2L1,CD40LG,IL1RN,HSPD1,NFKB1,TNF,FAS
Cardiac Inflammation 6.64E-05 IL33,CLU,TNNI3,IL23A,NFKB1,TNF
Heart Failure 6.76E-05 BCL2L1,CA2,TNNI3,VDR,NFKB1,TNF,AQP2,PRKCA
Hepatocellular Carcinoma 6.87E-05 VEGFA,CA2,BCL2L1,SOCS3,ADAM17,BSG,KEAP1,CLU,IGFBP3,S100A4,KIT,MKI67,TNF
Liver Hyperplasia/Hyperproliferation 6.87E-05 VEGFA,CA2,BCL2L1,SOCS3,ADAM17,BSG,KEAP1,CLU,IGFBP3,S100A4,KIT,MKI67,TNF
Renal Dysfunction 2.46E-04 BSG,FGF23,TNF
Cardiac Necrosis/Cell Death 3.17E-04 VEGFA,SOCS3,BCL2L1,S100B,HSPD1,TNF,LRP1,NAD+
Cardiac Hypertrophy 5.67E-04 IL33,ADAM17,S100A6,HSPA1A/HSPA1B,FGF23,EDNRA,DMD,VDR,NFKB1,TNF,PRKCA
Renal Necrosis/Cell Death 5.83E-04 BCL2L1,HSPA1A/HSPA1B,IGFBP3,CLU,PAX2,NFKB1,TNF,FAS,PRKCA
Liver Inflammation 8.62E-04 IL1RN,FOXP3,NFKB1,TNF,FAS
Kidney Failure 1.37E-03 VEGFA,SLC9A3,PKD2,MYH9,VDR,TNF,AQP2
Cardiac Proliferation 1.66E-03 ADAM17,KIT,TNF,PRKCA
Renal Dilation 1.67E-03 EDNRB,EDNRA,AQP2
Nephrosis 2.35E-03 CLU,ACTN4
Liver Fibrosis 2.48E-03 VEGFA,SOCS3,EDNRB,PKD2,EDNRA,NFKB1,TNF,CCR7
Renal Proliferation 2.48E-03 SOCS3,HSPG2,TJP1,HSPD1,TNF,CCR7
Increased Levels of AST 3.13E-03 TNF,FAS
Cardiac Fibrosis 4.69E-03 TNNI3,DMD,VDR,NFKB1,TNF,DIO3
Increased Levels of Albumin 5.63E-03 VEGFA
Liver Regeneration 5.85E-03 SOCS3,IL1RN,TNF
Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 10 of 13
http://www.jbiomedsem.com/content/5/1/33in a single type of associations in a heterogeneous
association network.
Comparisons of network motifs from different networks
Since all five network motifs identified involve only two
out of three node types, we further investigated whether
the networks involving only two node types can generate
the same NMs. To accomplish that, we performed NM
analysis on disease-gene, disease-drug and gene networks
respectively. Not all NMs detected in the complete net-
work can be detected in disease-gene, disease-drug and
gene networks respectively (Additional file 4: File S4). The
results indicate that although the NMs dont contain all
three different node types due to small NM size, theadditional associations still introduce additional informa-
tion in the NM detection analysis.
Discussion
Literature mining approaches have been successful to
extract associations among biological entities in the last
decade. However, such information is usually large, com-
plex and multidimentional, making it impossible for bio-
medical researchers to directly investigate such data. To
leverage the gap between knowledge needs of translational
researchers and existing knowledge discovery services, we
have proposed a network-based informatics approach to
investigate the underlying relationships among different
biological entities based on associations automatically
ID Network Motif
Frequency
[Original] 
Mean-Freq
[Random] 
Standard-
Dev[Random] 
Z-Score p-Value 
Number of 
Matches
Number of 
Entities
1 0.0096% 0.0035% 9.2982e-006 6.5 < 0.001 131
126 diseases 
and 79 genes
2 0.038% 0.025% 2.3404e-005 5.4 <0.001 522
468 disease and 
162 drugs
3 0.0075% 0.0055% 7.5124e-006 2.7 0.008 103
286 genes
4 5.9% 5.1% 0.004215 2.2 0.026 81105
2664 diseases 
and 1122 genes
5 0.032% 0.024% 4.1072e-005 2.0 0.032 437
432 disease and 
148 drugs
Figure 6 Statistics of significant network motifs. Node color: black  drug, green  disease, red  gene. Edge color denotes the associations
between different biomedical entities: black  association between disease and disease, yellow - association between disease and gene, green -
association between disease and drug, red - association between gene and gene.
Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 11 of 13
http://www.jbiomedsem.com/content/5/1/33extracted from literature. The proposed approach has
advantages in several aspects.
Our approach is one of the first attempts to investigate
the disease-drug-gene associations in an integrative man-
ner. To demonstrate the superiority of NM analysis on the
heterogeneous network, we performed NM analysis on
disease-gene, disease-drug and gene networks respectively
and compared results with the ones derived from the
complete disease-drug-gene network. Not all network mo-
tifs detected in the complete network can be detected in
disease-gene, disease-drug and gene networks respectively.
The results indicates that although NMs doesnt contain all
three different node types due to their small size in this
study, the additional associations still introduce additional
information in the analysis. In addition, NM analysis of
such heterogeneous networks can extract and highlight the
hotspots in the network, leading experts in different fields
to generate testable hypotheses in their future research.
We are aware that there are many other network ana-
lysis approaches for both social networks and biological
networks. These approaches are designed for differentpurposes. For instance, biological networks can be inter-
rogated by their overall properties (e.g., average cluster-
ing coefficient and overall distributions of node degrees),
significant NMs, or clustered subnetworks/modules. In
this work, we focus on identifying statistically significant
three-node NM patterns that can help infer novel disease-
drug-gene relationships. The NM analysis can decompose
the whole heterogeneous network into smallest network
patterns that recurrently discovered in the network, con-
sidered as the backbone associations of diseases, drugs,
and genes. For instance, in NM 1 instances in Figure 2,
most of these NMs contain the first two same diseases,
while the third gene is different. By extracting all the asso-
ciations involving these two diseases from the original as-
sociation network, we found that while these two diseases
share a significant number of associated genes, they also
have some unique associations with other genes respect-
ively. Based on the assumption that similar diseases are
more likely to associate with same group (s) of genes or
involve same biological processes, the genes associated
only with one disease can be prioritized as candidate
Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 12 of 13
http://www.jbiomedsem.com/content/5/1/33disease genes of the second disease. Such inference could
only be possible through NM level analysis by considering
significant network patterns (i.e., NMs) as well as their
neighborhood in the whole network. In addition, since
these NMs are statistically significant subnetworks, they
represent the real signal from the network which usually
contains considerable amount of false positive associa-
tions, especially those from literature mining techniques.
Due to the limitation of computational resource, we didnt
include the NMs with more than three nodes. We plan to
extend our work to NMs with more nodes (i.e., >3) when
the computational resource become available. We believe
that the proposed network-based approach can comple-
ment other existing network analysis methods and provide
researchers a unique way to look at these huge heteroge-
neous networks.
From our preliminary study [18], we found that Semantic
MEDLINE lacks of gene-gene associations since such
information usually are illustrated in the main text of
literature. Semantic MEDLINE contains gene-gene inter-
action data from PubMed literature abstracts (Figure 2).
We included all the associations in Figure 2 in our analysis.
However, the number of gene-gene association in Semantic
MEDLINE (2,169 high-confidence pairs) is relevantly small
comparing to other public databases (e.g., HPRD [4]). For
instance, we compared the gene-gene associations in
Semantic MEDLINE with those in HPRD, a manually
curated gene-gene association database in human [4].
The overlap between these two databases is very small
(about 10% associations of Semantic MEDLINE can be
found in HPRD). HPRD contains many more associa-
tions than Semantic MEDLINE (41,327 versus 2,169).
Therefore, we believe that combining Semantic MED-
LINE with other public resources (such as HPRD [39]
and STRING [40]) will increase the coverage of asso-
ciations and build a more comprehensive association
database. Using linked data approach, it will be relatively
easier to link our data graph with such databases.
Conclusions and future work
In this paper, we proposed a network-based computa-
tional framework to investigate integrated heterogeneous
network extracted from MEDLINE literature, including
associations among three major entity categories: drug,
gene, and disease. Five significant NMs were identified
and considered as the backbone of the entire network.
The potential biological meanings of each network motif
were further investigated. The results demonstrated that
the proposed approach holds the potential to 1) prioritize
candidate disease genes, 2) identify potential disease rela-
tionships, and 3) propose novel drug targets, within the
context of the entire knowledge. We believe that such
analyses can facilitate the process of inferring novel rela-
tionships between drugs, genes, and diseases. One futuredirection is to develop module-based approaches to
understand associations between different biomedical
entities. Modules are condensed subnetworks in a net-
work. Modules identified in heterogeneous networks
are a group of related diseases, drugs and genes, which
gives researchers a focused network view of the associ-
ation relationships among these entities. Topology ana-
lysis of heterogeneous networks using graphic theory
can also be applied in future studies, which can lead to
the identification of diseases/drugs/genes in the context
of association networks. Pathway level information
could also be integrated in future analyses to extend
current association network.
Additional files
Additional file 1: Detailed network information derived from NM 1
(Figure 4).
Additional file 2: Detailed network information derived from NM 4
(Figure 5).
Additional file 3: Detailed network information derived from NM 2
(Figure 6).
Additional file 4: NM analysis of drug-gene, disease-drug, and
gene-drug networks.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
YZ and CT led the study design and analysis, and drafted the manuscript. GJ
contributed to the manuscript preparation and use case discussions. JS and
AAN contributed to the association data extraction and reorganization, CGC
and HL participated the design, provided support and manuscript editing. All
authors read and approved the final manuscript.
Acknowledgements
This project was supported by the National Institute Health grant P30 CA
134274-04 to the University of Maryland Baltimore, the National Science
Foundation award 0937060 and the National Center for Biomedical
Ontologies (NCBO) to C.T., and the National Institute of Health grant
R01LM009959 and National Science Foundation award 0845523 to H.L.
Author details
1Division of Biostatistics and Bioinformatics, University of Maryland
Greenebaum Cancer Center and Department of Epidemiology and Public
Health, University of Maryland School of Medicine, Baltimore, MD, USA.
2School of Biomedical Informatics, University of Texas Health Science Center
at Houston, Houston TX, USA. 3Division of Biomedical Statistics and
Informatics, Department of Health Sciences Research, Mayo Clinic, Rochester
MN, USA. 4Radiology Informatics Laboratory, Department of Radiology, Mayo
Clinic, Rochester, MN, USA.
Received: 2 February 2013 Accepted: 2 July 2014
Published: 6 August 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Magka et al. Journal of Biomedical Semantics 2014, 5:17
http://www.jbiomedsem.com/content/5/1/17
RESEARCH Open Access
A rule-based ontological framework for the
classification of molecules
Despoina Magka1*, Markus Krötzsch2 and Ian Horrocks1
Abstract
Background: A variety of key activities within life sciences research involves integrating and intelligently managing
large amounts of biochemical information. Semantic technologies provide an intuitive way to organise and sift
through these rapidly growing datasets via the design and maintenance of ontology-supported knowledge bases. To
this end, OWLa W3C standard declarative language has been extensively used in the deployment of biochemical
ontologies that can be conveniently organised using the classification facilities of OWL-based tools. One of the most
established ontologies for the chemical domain is ChEBI, an open-access dictionary of molecular entities that supplies
high quality annotation and taxonomical information for biologically relevant compounds. However, ChEBI is being
manually expanded which hinders its potential to grow due to the limited availability of human resources.
Results: In this work, we describe a prototype that performs automatic classification of chemical compounds. The
software we present implements a sound and complete reasoning procedure of a formalism that extends datalog
and builds upon an off-the-shelf deductive database system. We capture a wide range of chemical classes that are not
expressible with OWL-based formalisms such as cyclic molecules, saturated molecules and alkanes. Furthermore, we
describe a surface less-logician-like syntax that allows application experts to create ontological descriptions of
complex biochemical objects without prior knowledge of logic. In terms of performance, a noticeable improvement is
observed in comparison with previous approaches. Our evaluation has discovered subsumptions that are missing
from the manually curated ChEBI ontology as well as discrepancies with respect to existing subclass relations. We
illustrate thus the potential of an ontology language suitable for the life sciences domain that exhibits a favourable
balance between expressive power and practical feasibility.
Conclusions: Our proposed methodology can form the basis of an ontology-mediated application to assist
biocurators in the production of complete and error-free taxonomies. Moreover, such a tool could contribute to amore
rapid development of the ChEBI ontology and to the efforts of the ChEBI team to make annotated chemical datasets
available to the public. From a modelling point of view, our approach could stimulate the adoption of a different and
expressive reasoning paradigm based on rules for which state-of-the-art and highly optimised reasoners are available;
it could thus pave the way for the representation of a broader spectrum of life sciences and biomedical knowledge.
Keywords: Semantic technologies, Knowledge representation and reasoning, Logic programming and answer set
programming, Datalog extensions, Cheminformatics
Background
Life sciences data generated by research laboratories
worldwide is increasing at an astonishing rate turning
the need to adequately catalogue, represent and index
the rapidly accumulating bioinformatics resources into a
pressing challenge. Semantic technologies have achieved
*Correspondence: magkades@gmail.com
1Department of Computer Science, University of Oxford, Oxford, UK
Full list of author information is available at the end of the article
significant progress towards the federation of biochemical
information via the definition and use of domain vocab-
ularies with formal semantics, also known as ontologies
[1-3]. OWL [4], a family of logic-based knowledge repre-
sentation (KR) formalisms standardised by the W3C, has
played a pivotal role in the advent of Semantic technolo-
gies. This is to a great extent thanks to the availability
of robust OWL-based tools that are capable of deriving
knowledge that is not explicitly stated by means of logical
inference. In particular, OWL bio- and chemo-ontologies
© 2014 Magka et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 2 of 15
http://www.jbiomedsem.com/content/5/1/17
with their intuitive hierarchical structure and their formal
semantics are widely used for the building of life sciences
terminologies [5,6].
Taxonomies provide a compelling way of aggregat-
ing information, as hierarchically organised knowledge
is more accessible to humans. This is evidenced, e.g. by
the pervasive use of the periodic table in chemistry, one
of the longest-standing and most widely adopted classi-
fication schemes in natural sciences. Organising a large
number of different objects into meaningful groups facil-
itates the discovery of significant properties pertaining to
that group; these discoveries can then be used to predict
features of subsequently detected members of the group.
For instance, esters with low molecular weight tend to be
more volatile and, so, a newly found ester with low weight
is expected to be highly volatile, too. As a consequence,
classifying objects on the basis of shared characteristics
is a central task in areas such as biology and chem-
istry with a long tradition of taxonomy use. Due to the
availability of performant OWL reasoners, life scientists
can employ OWL to represent expert human knowledge
and thus drive fast, automatic and repeatable classifica-
tion processes that produce high quality hierarchies [7,8].
Nevertheless, a prerequisite is that OWL is expressive
enough to model the entities that need to be classified as
well as the properties of the superclasses that lie higher up
in the hierarchy.
Two main restrictions have been identified in the
expressive power of OWL as hindering factors for the rep-
resentation of biological knowledge [9,10]. First, due to
the tree-model property of OWL [11] (which otherwise
accounts for the robust computational properties of the
language) one is not able to describe cyclic structures with
adequate precision. Second, because of the open-world
assumption adopted in OWL (according to which missing
information is treated as not known rather than false) it is
difficult to define classes based on the absence of certain
characteristics. These limitations manifest themselves
among othersvia the inability to define a broad range of
classes in the chemical domain. For instance, one cannot
effectively encode in OWL the class of compounds that
contain a benzene ring or the class of molecules that do
not contain carbon atoms, i.e. inorganic molecules.
These inadequacies obstruct the full automation of
the classification process for chemical ontologies, such
as the ChEBI (Chemical Entities of Biological Interest)
ontology, an open-access dictionary of molecular entities
that provides high quality annotation and taxonomical
information for chemical compounds [6]. ChEBI fosters
interoperability between researchers by acting as the pri-
mary chemical annotation resource for various biological
databases such as BioModels [12], Reactome [13] and the
Gene Ontology [5]. Moreover, ChEBI supports numer-
ous tasks of biochemical knowledge discovery such as
the study of metabolic networks, identification of dis-
ease pathways and pharmaceutical design [14,15]. ChEBI
is manually curated by human experts who annotate
and check the validity of existing and new molecular
entries. Currently, ChEBI describes 36,660 fully annotated
entities (release 110) and grows at a rate of approxi-
mately 4,500 entities per year (estimate based on previous
releases [16]). Given the size of other publicly available
chemical databases, such as PubChem [17] that contains
records for 19 million molecules, there is clearly a strong
potential for ChEBI to expand by speeding up curat-
ing tasks. ChEBI curating tasks span a wide range of
activities such as adding natural language definitions and
structure information or classifying chemical entities by
determining their position in the ChEBI taxonomy. Thus
automating chemical classification could free up human
resources and accelerate the addition of new entries to
ChEBI.
As the classification of compounds is a key task of the
drug development process [18], the construction of chem-
ical hierarchies has been the topic of various investigations
capitalising on logic-based KR [19-23], statistical machine
learning (ML) [24-26] and algorithmic [27-29] techniques.
In KR approaches, molecule and class descriptions are
represented with logical axioms crafted by experts and
subsumptions are identified with the help of automated
reasoning algorithms; in ML approaches a set of anno-
tated data is used to train a system and the system is
then employed to classify new entries. So, KR approaches
are based on the explicit axiomatisation of knowledge,
whereas ML algorithms specify for new entries super-
classes that are highly probable to be correct. As a con-
sequence, the taxonomies produced using logic-based
techniques are provably correct (as long as the modelling
of the domain knowledge is faithful), but the statistically
produced hierarchies (although much faster) need to be
evaluated against a curated gold standard. Algorithmic
techniques involve the definition of imperative pro-
cedures for determining classes of molecules. These
approaches are usually much quicker than logic-based
techniques but have the disadvantage of requiring a
programmer for defining new classes or for modifying
the existing ones, as opposed to ontological knowledge
bases that can be manipulated and extended by non-
programmers. Here, we focus on logic-based chemical
classification, which in certain cases can complement sta-
tistical and algorithmic approaches [8,15].
In previous work, we laid the theoretical foundation
of nonmonotonic existential rules which is an expressive
ontology language that is sound and complete and that
is suitable for the representation of graph-shaped objects;
additionally, we demonstrated how nonmonotonic exis-
tential rules can be applied to the classification of
molecules [9]. The aforementioned formalism addressed
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 3 of 15
http://www.jbiomedsem.com/content/5/1/17
the expressivity limitations outlined above; however, the
performance of the implementationalthough faster than
previous approacheswas not satisfactory (more than 7
minutes were needed to classify 70 molecules under 5
chemical classes on a standard desktop computer) failing
thus to confirm practicability of the formalism.
In the current work, we describe an improved prac-
tical framework that relies on the same formalism but
with enhanced performance. Our contributions can be
summarised as follows:
1. We present a prototype that performs logic-based
chemical classification based on a sound, complete
and terminating reasoning algorithm; we model
more than 50 chemical classes and we show that the
superclasses of 500 molecules are computed in 33
seconds.
2. We harness the expressive power of nonmonotonic
existential rules to axiomatise a variety of chemical
classes such as classes based on the containment of
functional groups (e.g. esters) and on the exact
cardinality of parts (e.g. dicarboxylic acids), classes
depending on the overall atomic constitution (e.g.
hydrocarbons) and cyclicity-related classes (e.g.
compounds containing a cycle of arbitrary length or
alkanes).
3. We present a surface syntax that enables application
experts to create ontological description of chemical
entities without prior knowledge of logic. The syntax
we propose is closer to natural language than to
first-order logic notation and is uniquely translatable
to logical axioms.
4. We exhibit a significant speedup in comparison with
previous ontology-based chemical classification
implementations.
5. We identify examples of missing and contradictory
subsumptions from the expert curated ChEBI
ontology that are present and absent, respectively,
from the hierarchy computed by our prototype.
Concerning future benefits, our prototype could form
the basis of an ontology-mediated application to assist
biocurators of ChEBI towards the sanitisation and the
enrichment of the existing chemical taxonomy. Automat-
ing the maintenance and expansion of ChEBI taxonomy
could contribute to a more rapid development of the
ChEBI ontology and to the efforts of the ChEBI team to
make annotated chemical datasets available to the pub-
lic. From a modelling point of view, our approach could
stimulate the adoption of a different and expressive rea-
soning paradigm based on rules for which state-of-the-art
and highly optimised reasoners are available; it could thus
pave the way for the representation of a broader spectrum
of life sciences knowledge.
Methods
Knowledge base design
The reasoning task carried out using our methodology is
the identification of chemical classes for molecules, e.g.
assigning water to the class of inorganic molecules or ben-
zene to cyclic molecules. In this section we provide a high-
level description of the knowledge base (KB) we built for
the purposes of our chemical classification experiments.
We use the word classification to refer to the detection
of subsumptions between molecules and chemical classes
rather than to the computation of the partial order for
the set comprising the chemical classes and molecules
w.r.t. the subclass relation. The KB consists of nonmono-
tonic existential rules that formally describe molecular
structures and chemical classes; this representation can
subsequently be used to determine the chemical class sub-
sumers of eachmolecule. For a formal definition of syntax
and semantics of nonmonotonic existential rules as well
as decidability proofs, we refer the interested reader to the
relevant articles [9,30,31].
For each chemical entity that we model using rules, we
also provide its axiomatisation in the surface syntax
a less-logician-like syntax which we designed and which
enables the ontological description of structured objects
without the use of logic. Our surface syntax is in the same
style of theManchester OWL syntax [32] and draws inspi-
ration from a syntax suggested for OWL 2 rules [33].
The main motivation for designing this syntax is to pro-
vide a means for creating ontological descriptions in a
more succinct way and without the use of special sym-
bols. We have formally defined the surface syntax and
its translation into nonmonotonic existential rules, but
we have not implemented an ontology editor that would
allow to write axioms in the new syntax. Similarly, we
have not conducted experiments evaluating the use of
surface syntax by application experts, but given that the
Manchester OWL syntax has been well received by non-
logicians [32] and there is active development of tools
for supporting more human readable ontology query lan-
guages [34], we believe that the suggested syntax has the
potential to facilitate curating tasks. Since our main focus
is to illustrate the transformation of molecular graphs and
chemical class definitions into rules, we omit the technical
details and describe our methodology by means of run-
ning examples. For a complete specification of the surface
syntax including a BNF grammar and mappings to non-
monotonic existential rules we provide an online technical
report [35].
Molecular structures
Next, we describe how a molfile can be converted into a
surface syntax axiom and subsequently a rule that encodes
its structure. We use as an example the molecule of
ascorbic acid, a naturally occurring organic compound
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 4 of 15
http://www.jbiomedsem.com/content/5/1/17
hasAtom
single
double
molecule
ascorbicAcid:
o
o o
c
o
cc
o
c
c
o
c
h
Figure 1 Ascorbic acid representations.Molfile (left), molecular graph (top right) and description graph (bottom right) encoding the molecular
structure of ascorbic acid.
commonly known as vitamin C. The molecular graph of
ascorbic acid is depicted in the upper right corner of
Figure 1.
Conceptually, the structure of ascorbic acid can be
abstracted with the help of a directed labeled graph such
as the one that appears in the lower right corner of
Figure 1 and which in our framework is called descrip-
tion graph (DG) [9]. The description graph of a molecule
is a labeled graph whose nodes correspond to the atoms
of the molecule (nodes 113 for ascorbic acid) plus an
extra node for the molecule itself (node 0) and whose
edges correspond to the bonds of the molecule (e.g. (1,7))
plus some additional edges that connect the molecule
node with each one of the atom nodes (e.g. (0,1)); addi-
tionally, the atom nodes are labeled with the respective
chemical elements (e.g. o for node 1) and the bond edges
with the corresponding bond order (e.g. single for (1,7));
finally, the molecule node is labeled with molecule and
the edges that connect the molecule node with each of the
atom nodes are labeled with hasAtom. In order to sim-
plify the depiction of the ascorbic acid DG in Figure 1
a legend is used for the edge labels; all arrowless edges
are assumed to be bidirectional. In our setting, we fol-
low the implicit hydrogen assumption according to which
hydrogen atoms are usually suppressed (excluding cases
where stereochemical information is provided for the
formed bond and hydrogens are explicitly stated as in
node 13). Finally, we point out that both the nodes and
the edges can have multiple labels, allowing us to also
encode molecular properties, such as charge values for
atoms. The description graph of ascorbic acid can be
converted into the following surface syntax definition. In
the rest of the text we use alphanumeric strings start-
ing with a lower-case letter to denote predicates, that
is names of classes (e.g. ascorbicAcid) and properties
(e.g. hasAtom).
ascorbicAcidSubClassOf
molecule AND (hasAtom SOME Graph(Nodes(1 o, 2 o, 3 o, 4 o, 5 o, 6 o, 7 c, 8 c, 9 c,
10 c, 11 c, 12 c, 13 h)
Edges(1 2 single, 1 10 single, 2 7 double, 3 8 single
4 9 single, 5 12 single, 6 11 single, 7 1 single
8 7 single, 9 8 double, 10 9 single, 11 10 single
12 11 single, 13 10 single)))
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 5 of 15
http://www.jbiomedsem.com/content/5/1/17
The surface syntax axiom above can next be trans-
lated into the rule below. In fact we need a separate
rule for each conjunct in the head but we use just one
rule here to simplify the presentation; for the sake of
brevity only one direction of the bonds appear and we
shorten an expression of the form ?C1 . . . ? Cn with
?ni=1Ci:
ascorbicAcid(x) ? molecule(x) ?13i=1 hasAtom(x, fi(x))
?6i=1 o(fi(x)) ?12i=7 c(fi(x)) ? h(f13(x))
? single(f8(x), f3(x)) ? single(f9(x),
f4(x)) ?i=1,9,11,13 single(f10(x), fi(x))
?i=5,11 single(f12(x), fi(x))?i=1,8
single(f7(x), fi(x)) ? single(f11(x),
f6(x)) ? double(f2(x), f7(x))?
double(f8(x), f9(x))
The rule above is a typical first-order implication with
a single atomic formula in the body and a conjunction of
atomic formulae in the head. Informally, the rule ensures
that every time that the ascorbic acid molecule instanti-
ated, its structure is unfolded according to its specified
DG. Thus, triggering of the rule implies that (i) new terms
that correspond to the DGs nodes are generated (exclud-
ing node 0), e.g. f1(x) represents atom node 1 (ii) each
new term is typed according to the label of the rele-
vant node with the help of a unary atomic formula (e.g.
o(f1(x))) and (iii) each pair of terms with correspond-
ing nodes connected in the DG is assigned the respec-
tive label with the help of a binary atomic formula (e.g.
single(f1(x), f7(x))). In order to ensure disjointness of the
several molecular structures on the interpretation level,
distinct function symbols are used in the rule of each
molecule.
General chemical knowledge and chemical classes
Before presenting the modelling of various chemical
classes, we demonstrate how we can encode background
chemical knowledge with surface syntax axioms that can
subsequently be mapped to rules. Three such axioms
appear next.
bond SuperPropertyOf
single OR double OR triple
charged SuperClassOf
positive ORnegative
horc SuperClassOf
h OR c
Examples of such knowledge include the fact that
single and double bonds are kinds of bonds or that
atoms with positive or negative charge are charged; we
can also denote a particular class of atoms, e.g. atoms
that are hydrogens or carbons. The translation of the
above mentioned surface syntax axioms into rules appears
below.
single(x, y) ? bond(x, y) negative(x) ? charged(x) h(x) ? horc(x)
double(x, y) ? bond(x, y) positive(x) ? charged(x) c(x) ? horc(x)
triple(x, y) ? bond(x, y)
For our experiments, we represented 51 chemical
classes using rules; we based our chemical modelling on
the textual definitions found in the ChEBI ontology [16].
We covered a diverse range of classes that can be cate-
gorised into four groups. For each class that we discuss, we
provide the surface syntax definition and its correspond-
ing translation into one or more rules. Certain classes
with an intricate definition (such as the class of cyclic
molecules that appears later) are not expressible in sur-
face syntax; these can be directly added as rules. Here
we show in full detail only a sample of the rules; the
complete set of rules is available in Additional files 1, 2
and 3 [36].
Existence of subcomponents The great majority of the
modelled chemical classes is defined via containment of
atoms, functional groups or other atom arrangements.
Examples of this type include carbon molecular enti-
ties, halogens, molecules that contain a benzene ring,
carboxylic acids, carboxylic esters, polyatomic entities,
amines, aldehydes and ketones. Next we show the surface
syntax axioms that define the classes of carbon molecular
entities, polyatomic entities, carboxylic acids and esters.
In the following axioms we use the keyword GraphNL
in contrast to the previously used Graph as our surface
syntax grammar requires the use of the former when spec-
ifying nodes that are either labeled with negative literals
or are specified to be disjoint.
carbonEntity SuperClassOf
hasAtom SOME c
polyatomicEntity SuperClassOf
molecule AND (hasAtom SOME GraphNL(DisjointNodes(1, 2)
Edges()))
heteroOrganicEntity SuperClassOf
hasAtom SOMEGraphNL(Nodes (1c, 2NOT c NOT h)
Edges (1 2 bond))
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 6 of 15
http://www.jbiomedsem.com/content/5/1/17
middleOxygenSuperClassOf
o AND(bondSOME GraphNL(DisjointNodes (1, 2)
Edges()))
carboxylicAcidSuperClassOf
molecule AND (hasAtom SOME GraphNL (Nodes (1 c, 2 o, 3 o NOT middleOxygen NOT charged,
4 horc)
Edges (1 2 double, 1 3 single, 1 4 single)))
carboxylicEsterSuperClassOf
molecule AND (hasAtom SOME Graph (Nodes (1 c, 2 o, 3 o, 4 c, 5 horc)
Edges (1 2 double, 1 3 single, 1 5 single, 3 4 single)))
One can find below the corresponding translations
into rules. We define as carbon molecular entities the
molecules that contain carbon; polyatomic entities are
the entities that contain at least two different atoms.
Heteroorganic entities are the ones containing carbon
atoms bonded to non-carbon atoms. Carboxylic acids
are defined as molecules containing at least one car-
boxy group (a functional group with formula C(=O)OH)
attached to a carbon or hydrogen; due to the implicit
hydrogens assumption we are not able to distinguish
between an oxygen and a hydroxy group and, so, we need
to specify that the oxygen of the hydroxy group is not
charged (NOT charged) and participates to only one bond
( NOT middleOxygen). Similarly, carboxylic esters con-
tain a carbonyl group connected to an oxygen ((C=O)O)
which is further attached to two atoms that are carbon or
hydrogen.
Exact cardinality of parts Here we describe chemical
classes of molecules with an exact number of atoms or of
functional groups. Examples include molecules that con-
tain exactly two carbons, molecules that contain only one
atom and dicarboxylic acids, that is molecules with exactly
two carboxy groups. The surface syntax axiom for the
definition of molecules with exactly two carbons appears
next.
exactly2CarbonsSuperClassOf
molecule AND hasAtom EXACTLY 2 c
The translation into rules follows. One can read-
ily verify that the surface syntax formulation is more
direct and intuitive than its equivalent translation into
rules.
molecule(x) ? hasAtom(x, y) ? c(y) ? carbonEntity(x)
molecule(x) ? hasAtom(x, y1) ? hasAtom(x, y2) ? y1 = y2 ? polyatomicEntity(x)
?2i=1hasAtom(x, zi) ? c(z1) ? notc(z2) ? noth(z2) ? bond(z1, z2) ? heteroOrganicEntity(x)
?3i=1hasAtom(x, yi) ? o(y1) ?3i=2 bond(y1, yi) ? y2 = y3 ? middleOxygen(y1)
molecule(x) ?4i=1 hasAtom(x, yi) ? c(y1) ? o(y2) ? o(y3) ?
horc(y4) ? double(y1, y2) ? single(y1, y3) ? single(y1 , y4) ?
notmiddleOxygen(y3) ? notcharged(y3) ? carboxylicAcid(x)
molecule(x) ?5i=1 hasAtom(x, yi) ?i=1,4 c(yi) ?i=2,3 o(yi) ?
horc(y5) ? double(y1, y2) ?i=3,5 single(y1 , yi) ? single(y3 , y4) ? carboxylicEster(x)
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 7 of 15
http://www.jbiomedsem.com/content/5/1/17
molecule(x) ?2i=1 hasAtom(x, yi) ? c(yi) ? y1 = y2 ? atLeast2Carbons(x)
molecule(x) ?3i=1 hasAtom(x, yi) ? c(yi) ?3i=2 y1 = yi ? y2 = y3 ? atLeast3Carbons(x)
atLeast2Carbons(x) ? not atLeast3Carbons(x) ? exactly2Carbons(x)
Exclusive composition We next present classes of
molecules such that each atom (or bond) they contain sat-
isfies a particular property. These features are usually very
naturally modelled with the help of nonmonotonic nega-
tion. Examples include inorganic molecules that consist
exclusively of non-carbon atoms. In spite of the fact that
there are many compounds with carbons considered inor-
ganic, in this work we align our encoding with the ChEBI
definition of inorganic molecular entities (CHEBI:24835),
according to which no carbons occur in these entities;
however, if the modeller wishes it, it is straightforward to
declare exceptions within our formalism using nonmono-
tonic negation. Another example is the class of hydro-
carbons which only contain hydrogens and carbons; also
saturated compounds are defined as the compounds
whose carbon to carbon bonds are all single. The corre-
sponding surface syntax axioms appear next.
inorganicSuperClassOf
molecule AND hasAtom ONLY ( NOT c)
hydroCarbon SuperClassOf
carbonEntity AND hasAtom ONLY (h OR c)
unsaturatedSuperClassOf
molecule AND hasAtom SOME Graph ( Nodes (1 c, 2 c)
Edges (1 2 double))
unsaturatedSuperClassOf
molecule AND hasAtom SOME Graph (Nodes(1 c, 2 c)
Edges (1 2 triple))
saturatedSuperClassOf
molecule AND NOT unsaturated
Please note that one can use more than one surface syn-
tax axioms (and thus rules) to define classes that emerge as
a result of different structural configurations, which is the
case for saturated molecules. Below we list the respective
translation into rules.
molecule(x) ? notcarbonEntity(x) ? inorganic(x)
hasAtom(x, z) ? notcarbon(z) ? nothydrogen(z) ? notHydroCarbon(x)
carbonEntity(x) ? notnotHydroCarbon(x) ? hydroCarbon(x)
molecule(x) ? hasAtom(x, z1) ? carbon(z1)
hasAtom(x, z2) ? carbon(z2) ? double(z1, z2) ? unsaturated(x)
molecule(x) ? hasAtom(x, z1) ? carbon(z1)
hasAtom(x, z2) ? carbon(z2) ? triple(z1 , z2) ? unsaturated(x)
molecule(x) ? not unsaturated(x) ? saturated(x)
Cyclicity-related classes These chemical classes include
the category of molecules containing a ring of any length
as well as other definitions that depend on the cyclicity
of molecules, such as alkanes which are defined as satu-
rated non-cyclic hydrocarbons. Assuming the (somewhat
more technical) definition of cyclic molecules, the surface
syntax axiom for alkanes appears next.
alkaneSuperClassOf
saturated AND hydroCarbon AND NOT cyclic
The corresponding rule translation follows.
saturated(x) ? hydroCarbon(x) ? notcyclic(x) ? alkane(x)
Determining subclass relations
Finally, we demonstrate how meaningful subsumptions
can be derived using a KB containing the rules outlined
in the previous two sections. In order to determine the
superclasses of a certain molecule, we extend the KB with
a suitable fact (i.e., a variable-free atomic formula) and
we examine the model that satisfies the KB under the
stable model semantics (the addition of the fact and the
examination of the model is done automatically by our
implementation). A formal definition of the stable model
semantics is provided by Gelfond and Lifschitz [37]. Intu-
itively, the stable model of a KB is the minimal set of facts
that are derived by exhaustively applying the existing rules
under a particular rule order; a rule is applied if its posi-
tive body can be matched to the so far derived facts and
no atom of the negative body is in the already produced
set of facts for the said matching.
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 8 of 15
http://www.jbiomedsem.com/content/5/1/17
Figure 2 Architecture of LoPStER. Stages of the classification process using LoPStER.
The initially added fact is the molecule name predicate
instantiated with a fresh constant so that the rule that
encodes the structure of thatmolecule is triggered. For the
case of ascorbic acid, if we append the fact ascorbicAcid(a)
to the previously described KB, we obtain the stable model
that appears below.
From the stable model atoms we can infer the super-
classes of ascorbic acid, that is we deduce that ascor-
bic acid isamong othersan unsaturated, polyatomic,
heteroorganic, cyclic molecular entity that contains car-
bon and a carboxylic ester. If there is no relevant atom
for a chemical class in the stable model, then we con-
clude that the said class is not a valid subsumer, e.g.
since carboxylicAcid(a) is not found in the stable model,
carboxylic acid is not a superclass of ascorbic acid.
Decidability check
TheKB discussed above contains rules with function sym-
bols in the head, such as the rule used to encode the
molecular structure of ascorbic acid. These rules may
incur non-termination during the computation of the sta-
ble model due to the creation of infinitely many terms.
In order to ensure termination of our reasoning pro-
cess and thus decidability of the employed formalism,
we perform a decidability check on the constructed KB.
In a nutshell, the decidability check (also known and as
model-summarising acyclicity [38]) involves transforming
the rules of the KB and inspecting the stable models of the
transformed KB for the existence of a special symbol. If
the KB passes the decidability check, then termination is
guaranteed; this is the case for the types of KBs that were
Stable model for ascorbic acid
Input fact:ascorbicAcid(a)
Stable model: ascorbicAcid(a), molecule(a), hasAtom
(
a, afi
)
for 1 ? i ? 13, o
(
afi
)
for 1 ? i ? 6,
c
(
afi
)
for 7 ? i ? 12, h
(
af13
)
, single
(
af8, af3
)
, single
(
af9, af4
)
, single
(
af12, afi
)
for i ? {5, 11},
single
(
af10, afi
)
for i ? {1, 9, 11, 13}, single
(
af7, afi
)
for i ? {1, 8}, single
(
af11, af6
)
, double
(
af2, af7
)
,
double
(
af8, af9
)
, bond
(
af8, af3
)
, bond
(
af9, af4
)
, bond
(
af12, afi
)
for i ? {5, 11}, bond
(
af11, af6
)
,
bond
(
af10, afi
)
for i ? {1, 9, 11, 13}, bond
(
af7, afi
)
for i ? {1, 8}, bond
(
af2, af7
)
, bond
(
af8, af9
)
,
horc
(
afi
)
for 7 ? i ? 13, carbonEntity(a), polyatomicEntity(a), heteroOrganicEntity(a),
middleOxygen
(
af1
)
, carboxylicEster(a), atLeast2Carbons(a), atLeast3Carbons(a),
notHydroCarbon(a), unsaturated(a), cyclic(a)
Stable model of the KB with the input fact ascorbicAcid(a) and the rules described in Methods; fi(a) is
abbreviated with afi for 1 ? i ? 13.
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 9 of 15
http://www.jbiomedsem.com/content/5/1/17
previously described. Technical details of the aforemen-
tioned condition are out of the scope of this text and can
be found in the relevant sources [38].
Prototype implementation
The current section provides an overview of LoPStER
(Logic Programming for Structured Entities Reasoner)
the prototype we developed for structure-based chemical
classification. The implementation is wrapped around the
DLV system, a powerful and efficient deductive database
and logic programming engine [39]. DLV constitutes the
automated reasoning component used by LoPStER for
stable model computation of a rule set. Figure 2 depicts
the basic processing steps as well as the different files
that are parsed and produced by LoPStER. LoPStER is
implemented in Java and is available online [36]; both
LoPStER and the rules modelling chemical classes are
open-source and released under GNU Lesser GPL. Next,
we describe in more detail the several stages of execution.
1. CDK-aided parsing. LoPStER parses the molfiles
[40] of the molecules to be classified using the
Chemistry Development Kit Java library [41]. The
molfile is a widely used chemical file format that
describes molecular structures with a connection
table; e.g. the molfile of ascorbic acid appears on the
left of Figure 1. For each molecule, a description
graph (e.g. Figure 1 bottom right) representation is
generated from its molfile according to a
transformation as the one described for ascorbic acid.
2. Compilation of the KB. For each molecule the
description graph representation is used to produce a
set of rules that encode the structure of the molecule,
following the translation that was discussed in the
previous section. These rules along with the
classification rules and the facts necessary to
determine subclass relations are combined to
produce DLV programs (i.e. sets of rules) that are
stored as plain text files on disk. In particular two
kinds of DLV programs are created for each
molecule, the program needed to perform the
decidability check as described before and the
program needed to compute subclass relations
between the molecules and the chemical classes.
3. Invoke DLV for decidability check. During this
step, the model of the program, which was produced
in the previous step for acyclicity testing, is
computed. If the check is successful, then execution
proceeds to the next stage; otherwise, the program is
exited with a suitable output message.
4. Invoke DLV for model computation. This is the
stage where DLV is invoked to compute the stable
model of the KB. Due to the check of the previous
step, the computation is guaranteed to terminate.
5. Stable model storage. At this point, the stable
model computed by DLV is stored in a file on disk to
enable subsequent discovery of the subclass relations.
6. Subsumptions extraction. This is the final phase
where the stable model file is parsed in order to
detect the superclasses of each molecule. All the
subsumee-subsumer pairs are stored in a separate
spreadsheet file on disk.
Results
Empirical evaluation
In order to assess the applicability of our implementa-
tion, we measured the time required by LoPStER to
perform classification of molecules. To obtain test data
we extracted molfile descriptions of 500 molecules from
the ChEBI ontology. The represented compounds were of
diverse size, varying from 1 to 59 atoms. Next, we inves-
tigated the scalability of our prototype by altering two
different parameters of the knowledge base, namely the
number of represented molecules and the type of mod-
elled chemical classes. Initially, we constructed ten DLV
programs each of which contained rules encoding 50 · i
different compounds, where 1 ? i ? 10, and rules
defining the chemical classes (a sample of which was pre-
viously described) excluding the cyclicity-related classes
(48 classes in total). Next, we repeated the same construc-
tion but this time including the rules for the cyclicity-
related classes (51 classes in total). In the rest of the
section, we refer to the first setting as no cyclic and to the
second as with cyclic.
Additionally and in order to optimise the performance,
we explored how classification times fluctuate depending
on the size of DLV programs. In particular, we parti-
tioned the DLV programs into modules, we measured
classification times for each module separately and we
summed up the times. Each module contains the facts
and the rules describing a subset of the molecules rep-
resented in the initial DLV program; the rules defining
chemical classes are included in each one of the modules.
Thus, the size of each module depends on the number of
encoded molecules. We tested modules of various sizes as
well as DLV programs without any partitioning for both
no cyclic mode and with cyclic mode. Modifying the
size of the module had a clear impact on the measured
times and performing classification with the modularised
knowledge base was always quicker than with the unpar-
titioned one; we observed the shortest execution times for
module size 50 when testing in no cyclic mode and for
module size 20 when testing in with cyclic mode; the tim-
ings we provide next refer to the aforementioned module
sizes.
Table 1 summarises the classification times for the pre-
viously described KBs. All the DLV programs that were
tested passed the decidability check. The experiments
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 10 of 15
http://www.jbiomedsem.com/content/5/1/17
Table 1 Timemeasurements for classification
Nomolecules No of rules Time no cyclic Time with cyclic
(sec) (sec)
50 3614 4.81 7.85
100 6832 3.41 8.69
150 18072 4.25 9.97
200 23746 4.55 11.88
250 28502 6.60 18.71
300 31892 8.27 20.63
350 35046 8.14 22.58
400 38095 9.30 24.23
450 41536 9.94 29.68
500 43629 10.40 32.79
The first column is the number of molecules, the second is the number of rules
in the corresponding rule set and the third and fourth are measurements in
seconds for no cyclic and with cyclic mode, respectively.
were performed on a desktop computer (2GHz quadcore
CPU, 4GB RAM) running Linux. The first column dis-
plays the number of molecules, the second column the
number of rules contained in the corresponding DLV pro-
gram and the third (fourth) column the time needed to
perform classification in no cyclic (with cyclic) mode.
We only display the number of rules for the no cyclic
mode because there are only six more rules in the DLV
programs with cyclicity-related definitions. The classifica-
tion experiments for each knowledge base were repeated
three times and the results were averaged over the three
runs; also, the durations of Table 1 are inclusive, that is
they count the time spent from before the molfiles parsing
until after the subsumptions extraction. Figure 3 depicts
the plots of the time intervals appearing in Table 1 both
with regard to the number of molecules and the number
of rules contained in the respective DLV program.
The performance results of Table 1 are encouraging for
the practical feasibility of our approach: the classification
of 500 molecules was completed in less than 33 secons
for the suite of 51 modelled chemical classes. The drop
in classification times between the 50 and 100 molecules
case is potentially due to JVM startup overhead. One
can also observe that the rules encoding cyclicity-related
classes introduce a significant overhead for the classifica-
tion times. In fact, it is the class that recognises molecules
with cycles of arbitrary length that incurs the performance
penalty. The rules that encode the class of cyclicmolecules
need to identify patterns that are extremely frequent in
molecular graphs; as a consequence, the amount of com-
putational resources needed to detect ring-containing
molecules is much higher. However, since our class defini-
tion for cyclic molecules detects compounds with cycles
of variable length, which is a significant property for the
construction of chemical hierarchies, we consider this
overhead acceptable.
Discussion and related work
Concerning expressive power, the current approach
allows for the representation of strictly more chemical
classes in comparison with other logic-based applica-
tions for chemical classification. Villanueva-Rosales and
Dumontier [19] describe an OWL ontology of functional
groups for the classification of chemical compounds; in
their work, they point out the inherent inability of OWL to
represent cyclic functional groups and how this impedes
the use of OWL in logic-based chemical classification. As
a remedy, Hastings et al. [21] employ an extension of OWL
[42] for the representation of non-tree-like structures
and, thus, for the classification of molecular structures.
However, the used formalism only allows for the iden-
tification of cycles of fixed length and with alternating
single and double bonds. In the current approach we are
Figure 3 Classification times. Curves of classification times with respect to number of molecules (left) and number of rules (right). The lower line
is for with cyclic mode and the upper for no cyclic mode.
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 11 of 15
http://www.jbiomedsem.com/content/5/1/17
able to recognise molecules containing cycles of both arbi-
trary and fixed length and without requiring a particular
configuration of bonds.
Moreover, in both approaches outlined above the
adopted open world assumption of OWL prevents one
from defining structures based on the absence of cer-
tain characteristics. In our approach we operate under the
closed world assumption which permits the definition of
a broad range of chemical classes that were not express-
ible before such as the class of inorganic, hydrocarbon
or saturated compounds. Finally and in comparison with
previous work [9], we take full advantage of the suggested
formalism by specifying a much wider range of chemi-
cal classes and we do not require from the modeller a
precedence relation between the represented structures.
In terms of performance, the classification results
appear more promising than previous and related work.
Hastings et al. [21] report that a total of 4 hours was
required to determine the superclasses of 140 molecules,
whereas LoPStER identifies the chemical classes of 500
molecules in less than 33 seconds. LoPStER is quicker in
comparison with previous work too [9] where 450 seconds
were needed to classify 70 molecules (two orders of mag-
nitude faster). Please note that both cases discussed above
considered a subset of the chemical classes used here.
Regarding the significant change in speed, we identify the
following two factors that could explain it. First, DLV is a
more suitable reasoner for our setting due to its bottom-
up computation strategy as well as its active maintenance
team and frequent releases. Second, we employ a more
efficient condition (model-summarising acyclicity [38]
instead of semantic acyclicity [9]) in order to obtain termi-
nation guarantees which allows for a more prompt decid-
ability check. Finally, the classification times reported here
are slightly improved in comparison with a preliminary
version of this paper due to some modelling optimisations
and the use of a recent new version of DLV.
While conducting the experiments we discovered a
number of missing and inconsistent subsumptions from
the manually curated ChEBI ontology; here we only
mention a few of them. As one can infer from the
molecular graph of ascorbic acid appearing in the top
right of Figure 1, ascorbic acid is a carboxylic ester as
well as a polyatomic cyclic entity. In spite of the fact
that these superclasses were exposed by our classifica-
tion methodology, we were not able to identify them
in the ChEBI hierarchy. Figure 4 shows the ancestry
of ascorbic acid (CHEBI:29073) in the OWL version of
the ChEBI ontology; none of the concepts cyclic entity
(CHEBI:33595), polyatomic entity (CHEBI:36357) or car-
boxylic ester (CHEBI:33308) is encountered among the
superclasses of ascorbic acid. Moreover, ascorbic acid is
asserted as a carboxylic acid (CHEBI:33575) which is not
the case as it can be deduced by the lack of a carboxy group
in the molecular graph of ascorbic acid (the most com-
mon tautomer of which appears in the top right corner of
Figure 1). We interpret the revealing of these modelling
errors as an indication of the practical relevance of our
contribution.
The chemical classification methodology that we
present here is similar to other classification efforts
based on semantic technologies, such as classification
Figure 4 Ascorbic acid superclasses. Superclasses of ascorbic acid for the ChEBI OWL ontology release 102 as illustrated by the ChEBI
graph-based visualisation interface.
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 12 of 15
http://www.jbiomedsem.com/content/5/1/17
of proteins [7] or lipids [8]. Wolstencroft et al. use a
bioinformatics tool to extract composition information
from protein descriptions and subsequently translate this
information intoOWL axioms; these axioms are next used
to classify the proteins using a DL reasoner. Chepelev
et al. use a cheminformatics tool to process lipid descrip-
tions and produce annotated lipid specifications that are
then classified using an OWL ontology. The motivation
of these two investigations is similar to ours, i.e. alle-
viation of biocurating tasks; what distinguishes the two
approaches from ours is the use of a different ontology
language and the role that this language plays during clas-
sification. In particular, in our work we use nonmonotonic
existential rules instead of OWL which, unlike OWL, are
able to capture cyclic structures. Also, in the sequence
of steps followed by our classification process we do not
rely on a cheminformatics functionality to algorithmi-
cally annotate the molecular descriptions, but instead the
identification of structural features forms integral part of
reasoning. The framework we suggested can be suitable
for the domains of lipids and proteins, as long as they are
restricted to structures of finite size; however empirical
evaluation would be needed to assess the suitability of the
framework in practice. Regarding the application of our
prototype to ChEBI classification, it could be used to clas-
sify ChEBI molecules under the chemical classes defined
here, but more curating effort would be needed to model
the thousands of chemical classes that appear in ChEBI.
In this work, we represent and reason about chemi-
cal knowledge using an ontology language. However, the
majority of axioms constituting the ontology, that is the
molecule descriptions, are sourced through molfiles that
are parsed using cheminformatics libraries. The informa-
tion provided by these files includes connectivity between
atoms, types of atoms and bonds and charges of atoms.
This information is converted into logical axioms that
are subsequently processed by an automated reasoning
algorithm to identify the chemical classes of the
molecules. This approach has the advantage of allow-
ing the knowledge modeller to define new classes in a
declarative way, that is without the need of writing code
for detecting their subsumees. However, a feature that
could be detected using cheminformatics algorithms and
become part of the ontology axioms is the existence of
ring atoms. The benefits of such a modification could be
twofold: it could considerably speed up the computation
of all cyclicity-related classes (e.g. determining whether an
atom is a ring atom can be done very quickly using the
CDK library) and at the same time could allow for the def-
inition of strictly more cyclicity-related classes, such as
carbocyclic compounds.
An alternative approach could be to build rules from
chemical identifiers other thanmolfiles, such as InChi [43]
or preferred IUPAC names [44]. In particular, InChi with
its abilitiy to encode isotopical and stereochemical infor-
mation (which can be critical for biological applications)
could lead to richer chemical modelling. Also, widely used
chemical databases, such as ChemSpider [45], could be
used as a resource for adding to rules information about
molecular properties.
A category of molecules that our framework does not
cover is tautomers. A tautomer is each of two or more
isomers that exist together in equilibrium, and are readily
interchanged by migration of an atom (usually hydrogen)
or group within the molecule. InChi handles tautomerism
by allowing a compound to contain mobile hydrogen
atoms, that is some hydrogens are marked as being able
to occur in different positions. This is an approach that
could be adopted by our methodology too, if we extended
our formalism with the ability to represent disjunctive
hasParticipant
locatedIn
reactant
product
Figure 5 Transport reaction description graph.
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 13 of 15
http://www.jbiomedsem.com/content/5/1/17
hasPart
linked
Figure 6 Jasmonic acid description graph.Molecular graph of jasmonic acid (left) and description graph of jasmonic acid based on the
functional groups partonomy (right).
information. However, enriching nonmonotonic existen-
tial rules with disjunction would require to alter the design
and implementation of the reasoning algorithm, so treat-
ing tautomers could be part of a future extension of our
framework.
Conclusion
We presented an implementation that performs logic-
based classification of chemicals and builds upon a sound
and complete reasoning procedure for nonmonotonic
existential rules; our prototype relies on the DLV system
and is considerably quicker than previous approaches. For
our evaluation, we represented a wide variety of chem-
ical classes that are not expressible with OWL-based
formalisms and described a surface syntax that could
enable cheminformaticians to define ontological descrip-
tions of chemical entities intuitively and without the need
to use first-order logic notation; additionally, our software
revealed subclass relations that are missing from theman-
ually curated ChEBI ontology as well as some erroneous
ones. We demonstrated thus the capabilities of a datalog-
based ontology language that displays a favourable trade-
off between expressive power and performance for the
purpose of structure-based classification.
Future research
For the future it would be interesting to further apply
our framework towards supporting classification of other
complex biological objects. For instance, one can exploit
the expressive power of rules to represent biochemical
processes and infer useful relations about them. Figure 5
depicts a description graph abstraction of a chemical reac-
tion example discussed by Bölling et al. [46]. The process
consists of parts that are arbitrarily interconnected and
can thus be naturally modelled using our formalism. In the
same vein, our methodology could provide rigorous defi-
nitions for the representation of lipid molecules that can
be systematically classified according to their structural
features. Low et al. [47,48] introduced the OWL DL
Lipid Ontology which contains semantically explicit lipid
descriptions. One could achieve more accurate modelling
by casting lipids in terms of rules that capture frequent
cyclic patterns in a concise way; for example, Figure 6
illustrates a description graph for jasmonic acidone
of the lipids encountered in the abovementioned OWL
ontology.
Further work could involve the building of an ontology
editor for the creation of surface syntax expressions and
their automatic conversion into nonmonotonic existential
rules. We will also seek to extend our prototype to accom-
modate subsumption between chemical classes so as to
generate a complete multi-level chemical hierarchy using
ideas from our recent work [49,50]. We could extend our
formalism with numerical value restrictions [51] in order
to express e.g. classes depending on molecular weight.
Moreover, it could be of interest exploring the integration
of our prototypewith Protégé [52], Life Sciences platforms
[53] and chemical structure visualisation tools [54,55] as
well as defining a mapping of the introduced formalism to
RDF [56].
Additional files
Additional file 1: Timemeasurements and produced hierarchy of the
classification experiments. Description of data: Full list of computed
subsumptions and time measurements for each of the five experiments
discussed in Empirical evaluation.
Additional file 2: Logic programwithout cyclicity-related rules.
Description of data: Set of rules modelling the chemical classes excluding
the cyclicity-related classes.
Additional file 3: Complete logic program. Description of data: Set of
rules modelling all the chemical classes.
Abbreviations
OWL:Web ontology language; ChEBI:Chemical entities of biological interest;
W3C:World wide web consortium, KR:Knowledge representation; ML: Machine
learning; KB:Knowledge base; DG:Description graph; LoPStER:Logic
programming for structure entities reasoner; RDF:Resource description
framework.
Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 14 of 15
http://www.jbiomedsem.com/content/5/1/17
Competing interests
The authors declare that they have no competing interests.
Authors contributions
All authors conducted research on the underlying decidability conditions for
datalog-based rules and jointly discussed the present paper and its main
contributions (surface syntax, chemical modelling, experimental setup). DM
has specified the surface syntax grammar, assembled the knowledge base,
carried out the experiments and led the writing of the manuscript. MK and IH
contributed to the discussions and participated in the writing of the
manuscript. All authors read and approved the final manuscript.
Acknowledgements
We would like to thank Dr Chris Batchelor-McAuley for answering our
chemistry questions and the anonymous reviewers of this article for providing
JOURNAL OF
BIOMEDICAL SEMANTICS
Paul et al. Journal of Biomedical Semantics 2014, 5:8
http://www.jbiomedsem.com/content/5/1/8
RESEARCH Open Access
Semantic interestingness measures for
discovering association rules in the skeletal
dysplasia domain
Razan Paul1, Tudor Groza1*, Jane Hunter1 and Andreas Zankl2,3
Abstract
Background: Lately, ontologies have become a fundamental building block in the process of formalising and storing
complex biomedical information. With the currently existing wealth of formalised knowledge, the ability to discover
implicit relationships between different ontological concepts becomes particularly important. One of the most widely
used methods to achieve this is association rule mining. However, while previous research exists on applying
traditional association rule mining on ontologies, no approach has, to date, exploited the advantages brought by
using the structure of these ontologies in computing rule interestingness measures.
Results: We introduce a method that combines concept similarity metrics, formulated using the intrinsic structure of
a given ontology, with traditional interestingness measures to compute semantic interestingness measures in the
process of association rule mining. We apply the method in our domain of interest  bone dysplasias  using the core
ontologies characterising it and an annotated dataset of patient clinical summaries, with the goal of discovering
implicit relationships between clinical features and disorders. Experimental results show that, using the above
mentioned dataset and a voting strategy classification evaluation, the best scoring traditional interestingness measure
achieves an accuracy of 57.33%, while the best scoring semantic interestingness measure achieves an accuracy of
64.38%, both at the recall cut-off point 5.
Conclusions: Semantic interestingness measures outperform the traditional ones, and hence show that they are
able to exploit the semantic similarities inherently present between ontological concepts. Nevertheless, this is
dependent on the domain, and implicitly, on the semantic similarity metric chosen to model it.
Introduction
Over the course of the last decade, ontologies have
become a fundamental building block in the knowledge
acquisition and capturing processes in the biomedical
domain. Repositories such as BioPortal [1] or the OBO
Foundry [2] currently offer a varied range of ontologies,
in addition to tool support to visualise, query and inte-
grate concepts hosted by these ontologies. Subsequently,
this enables the construction of decision support meth-
ods that use ontological background knowledge in order
to produce more accurate and more refined outcomes.
*Correspondence: tudor.groza@uq.edu.au
1School of ITEE, The University of Queensland, St. Lucia, Queensland 4072,
Australia
Full list of author information is available at the end of the article
Ontologies provide structured and controlled vocabu-
laries and classifications for domain specific terminolo-
gies. Their adoption for annotation purposes provides a
means for comparing medical concepts on aspects that
would otherwise be incomparable. For example, the anno-
tation of a set of disorders (directly or via patient cases)
using a particular ontology enables us to compare these
disorders, by looking at the underpinning annotation con-
cepts. The actual comparison can be done in an exact
or inexact manner. More concretely, one may take into
account only those identical concepts that appear in all
or some disorders, or may use a semantic similarity mea-
sure that relaxes the constraint on identical concepts.
Such a semantic similarity measure represents a function
that takes two or more ontology concepts and returns
a numerical value that reflects the degree of similarity
© 2014 Paul et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 2 of 13
http://www.jbiomedsem.com/content/5/1/8
between these concepts in a given ontology. This com-
parison process represents a key aspect of typical data
mining algorithms that form the core of any decision
support method. For example, two ontological concepts,
such as HP:0004481 (Progressive macrocephaly) and
HP:0004482 (Relative macrocephaly) from the Human
Phenotype Ontology (HPO) [3], would be treated differ-
ently by any classical data mining algorithm because of
their symbolic (i.e., lexical grounding) difference. How-
ever, these two concepts, like any other two entities in an
ontology, are to a certain extent semantically similar  a
similarity that can be encoded via an existing or custom-
made metric. Replacing exact matching with semantic
similarity measures provides novel and exciting oppor-
tunities in knowledge discovery and decision support on
annotated datasets [4-6].
Association rules [7] are valuable patterns that can dis-
covered from annotated datasets. An association rule
denotes an implication relationship (or a directed co-
occurrence) between two sets of items within a transac-
tion. A widely used algorithm to discover such association
rules is Apriori [7]. However, regardless of the particu-
lar algorithm used, the discovery process has two major
challenges: (i) too many rules may be generated (the rule
quantity problem); (ii) not all rules are necessarily inter-
esting (rule quality problem). The solution to the rule
quality problem relies on specifying an interestingness
measure [8-10] to encode the utility or significance of a
pattern. These measures are intended for selecting and
ranking patterns according to their potential interest and
enables highly ranked rules to be immediately presented
or used for particular purposes.
Existing work on interestingness measures takes into
account only exact matching [10]. Semantic similarities,
however, enable novel ways of interpreting data items, and
hence may lead to the identification of association rules
that are otherwise not discoverable via exact matching. In
this manuscript, we advance the state of the art by explor-
ing the application of semantic similarities in widely used
interestingnessmeasures in the context of association rule
mining. In other terms, we aim to use existing taxonomic
relations to calculate so-called semantic interestingness
measures.
The context of our research is provided by the SKELE-
TOME project [11], which aims to create a community-
driven knowledge curation platform for the skeletal
dysplasia domain. Skeletal dysplasias are a heterogeneous
group of genetic disorders affecting skeletal development.
Currently, there are over 450 recognised bone dysplasias,
structured in 40 groups. Patients with skeletal dysplasias
have complex medical issues including short stature,
bowed legs, a larger than average head and neurological
complications. Since most skeletal dysplasias are very rare
(< 1:10,000 births), data on clinical presentation, natural
history and best management practices is sparse. To date,
we have developed an ontology, the Bone Dysplasia Ontol-
ogy (BDO) [12], and a series of decision support methods
[6,13]. BDO has been built using the latest nosology of
bone dysplasias [14] that groups disorders according to
their overlapping clinical and genetic features. For exam-
ple, Achondroplasia and Diastrophic dysplasia are similar,
and are both part of the FGFR3 Group, because they share
a range of clinical features (i.e., short stature with very
short arms and legs).
Within this manuscript, we investigate both tradi-
tional, as well as semantic interestingness measures in the
context of association rule mining, to discover implicit
relationships between clinical features and disorders in
skeletal dysplasia domain. The main contributions of this
work are the following: (i) firstly, we analyse which of
the existing traditional interestingness measures enables a
more accurate discovery of association rules in the skele-
tal dysplasia domain; (ii) secondly, we propose a series
of interestingness measures based on semantic similarity
metrics using existing ontologies as background knowl-
edge; and (iii) finally, we perform an extensive empirical
evaluation to measure the quality of the resulting rules,
using an annotated dataset built on real patient data.
At the same time, we show that, given an appropriate
semantic similarity metric, the semantic interestingness
measures outperform the traditional ones.
As alreadymentioned, our work focuses only on skeletal
dysplasias, and hence it investigates the efficiency of the
above-described methods only in this domain. However,
the generic definition of a semantic interestingness mea-
sure proposed in this manuscript is directly applicable in
any other domain, while the rest of the research can be
used as a guideline for choosing an appropriate domain-
specific semantic similarity metric to be applied as part of
the overall measure.
Background
This section provides an overview of the foundational
blocks of the experiments performed in the context of
our research. We start by introducing the Human Pheno-
type Ontology and the Bone Dysplasia Ontology  i.e., the
ontologies used as background knowledge for the seman-
tic similarity metrics. Then, we describe some of the basic
notions of semantic similarities, and finally, we discuss
some of the traditional interestingness measures.
Human Phenotype Ontology
The Human Phenotype Ontology (HPO) [3] has lately
become the de facto controlled vocabulary to capture and
represent clinical and radiographic findings. The ontol-
ogy consists of around 9,000 concepts describingmodes of
inheritance, onset and clinical disease courses and pheno-
typic abnormalities. This last category represents around
Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 3 of 13
http://www.jbiomedsem.com/content/5/1/8
95% of the ontology and it is the main subject of our
study. HPO structures phenotypic abnormalities in a hier-
archical manner (via class-subclass relationships) from
generic (e.g., HP:0000929 (Abnormality of the skull) to
specific concepts (e.g., HP 0000256  Macrocephaly).
For instance, HP:0001629 (Ventricular septal defect) is a
subclass of the concept HP:0010438 (Abnormality of the
ventricular septum) in the sense that a ventricular septal
defect is a kind of abnormality of the ventricular septum
and hence. every person with a ventricular septal defect
can also be said to have an abnormality of the ventricular
septum. This goes along the line of theTrue path rule [15],
which states that an annotation with a particular concept
implies the path from that concept to the root to be true,
or more concretely, a valid annotation with all ancestors
of that concept.
One obvious advantage of capturing phenotypic infor-
mation using ontologies is that it enables the design of
association mining algorithms that can exploit the seman-
tic relationships between concepts. For instance, an algo-
rithm can be designed to support not only the patterns
associated with a concept like HP:0001671 (Abnormal-
ity of the cardiac septa), but also those associated with
its children, HP:0010438 (Abnormality of the ventricu-
lar septum) and HP:0011994 (Abnormality of the atrial
septum).
Bone Dysplasia Ontology
The International Skeletal Dysplasia Society (ISDS 
http://www.isds.ch/) Nosology lists all recognised skele-
tal dysplasias and groups them by common clinical-
radiographic characteristics and/or molecular disease
mechanisms. The Nosology is revised every 4 years by
an expert committee and the updated version is usually
published in a medical journal. This is widely accepted as
the official nomenclature for skeletal dysplasias within
the biomedical community, with the latest version being
published in 2010 [14].
The Bone Dysplasia Ontology [12] aims to comple-
ment the spectrum of existing ontologies and address
the specific knowledge representation shortcomings of
the ISDS Nosology. Its main role is to provide the scaf-
folding required for a comprehensive, accurate and for-
mal representation of the genotypes and phenotypes
involved in skeletal dysplasias, together with their spe-
cific and disease-oriented constraints. As opposed to the
ISDS Nosology, the ontology enables a shared conceptual
model, formalised in a machine-understandable descrip-
tion, in addition to a continuous evolution and a founda-
tional building block for facilitating knowledge extraction
and reasoning. Currently, the structure of the ontology
follows closely the grouping of the disorders imposed by
the expert committee via the Nosology by using class-
subclass relationships between the 40 groups and their
associated bone dysplasia members. These groups are
then linked via the root concept Bone_Dysplasia.
Semantic similarity
Annotations using Bio-ontologies allow us to compare
concepts on various aspects by using their intrinsic
semantic similarity. Semantic similarity represents the
quantification of the degree of similarity between two
or more ontological concepts. For example, the annota-
tion of two bone dysplasias with concepts emerging from
the same ontology, e.g., HPO, enables their comparison
by looking at the semantic similarity between the con-
cepts used for annotation. In addition to this implicit role,
semantic similarity measures can also be used to discover
association rules in annotated datasets.
In principle, there are two types of approaches for
computing semantic similarity measures: node-based and
edge-based. The former uses the nodes and their proper-
ties as the data source whereas the latter uses the edges
between nodes and their associated types as data source.
The node-based approaches usually rely on the notion of
Information Content (IC) to quantify informativeness of a
concept. An IC value of a node is calculated by comput-
ing the negative likelihood of its frequency in a large text
corpora (IC(c) = ?log(p(c))), with the intuition that the
more probable is the appearance of a concept in a corpus,
the less information it conveys.
A large number of node-based measures have been pro-
posed using Information Content as a central element,
some of the most widely used being listed below, i.e.,
Resnik [16], Lin [17] and Jiang and Conrath [18]. As a note,
in the equations below,MICA denotes the Most Informa-
tive Common Ancestor, i.e., the common ancestor of the
nodes with the highest Information Content.
Resnik : SIMRes(c1, c2) = IC(cMICA) (1)
Lin : SIMLin(c1, c2) = 2 ? IC(cMICA)IC(c1) + IC(c2) (2)
Jiang and Conrath : SIMJC(c1, c2)
= 1 ? IC(c1) + IC(c2) ? IC(cMICA) (3)
In the other category, i.e., edge-based approaches, Wu
& Palmer [19] proposed a measure based on the length
of the shortest path between the Least Common Ances-
tor (LCA) and the root and on the length of shortest path
between each of the concepts and that common ancestor.
DisW&P(c1, c2) = 2 ?N3N1+ N2+ 2 ?N3 (4)
where, N3 is the length of path from LCA to the root; N1
is the length of path from c1 to LCA; N2 is the length of
path from c2 to LCA.
Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 4 of 13
http://www.jbiomedsem.com/content/5/1/8
Association rule mining
Association rules [7] provide knowledge in the form of
probabilistic if-then statements, e.g., I ? Q. The head
of the association rule (i.e., the if part  I) is called
antecedent, while the body (i.e., the then part Q) is called
consequent. The antecedent and consequent of an asso-
ciation rule are disjoint  they do not have any items in
common. To express uncertainty in association rules, i.e.,
I ? Q with a certain degree of certainty, several metrics
can be used, two of themost widely adopted being Support
and Confidence (discussed below). A set of association
rules aimed for classification is called predictive associ-
ation rule set. A class association rule set is a subset of
association rules with the specified classes as their conse-
quences. Predictive association rules form a small subset
of class association rules. Generally, mining predictive
association rules undergoes the following two steps: (i)
Find all class association rules from a database, followed
by (ii) Prune and organise the found class association rules
to return a sequence of predictive association rules.
Traditional interestingness measures. As mentioned
earlier, the rule discovery process is usually associated
with two challenges, one of them being the rule quality
problem, i.e., quantifying which of the discovered rules are
more interesting. Interestingnessmeasures play an impor-
tant role in data mining, regardless of the kind of patterns
being mined. They are intended for selecting and rank-
ing patterns according to their potential interest to the
user. Below, we present a number of existing association
rules interestingness measures [10], which we have also
applied in our experiments. This set of measures rely on
the foundational Support and Confidencemetrics.
Let T = {t1, t2, . . . , tn} be a database of n transactions
with a set of attributes (or items) I = {i1, i2, . . . , im}. For
an itemset IX ? I and a transaction t ? T , we say that t
supports IX if t has values for all the attributes in IX . By
TIX we denote the transactions that contain all attributes
in IX .
The Support of IX is computed as
Support(IX) = TIXn (5)
or the fraction of transactions that include all attributes in
IX .
The Confidence of an association rule IX ? Q, where Q
is also an itemset (Q ? I) and Q ? IX = ?, is defined by:
Confidence(IX ? Q) = Support(IX ,Q)Support(IX) (6)
or the ratio between the number of transactions that
include all items in the consequent (Q), as well as in the
antecedent (IX)  namely, the Support of the union of IX
and Q  and the number of transactions that include all
items in the antecedent (i.e., the Support of IX).
Confidence alone may not be enough to assess the
descriptive interest of a rule, as rules with high confi-
dence may occur by chance. Such spurious rules can be
detected by determining whether the antecedent and the
consequent are statistically independent. This inspired a
number of measures, including Lift, Conviction, Leverage,
Jaccard, Cosine and Correlation Coefficient [8-10]. We
provide their mathematical definitions in the following
sections.
Materials andmethods
Annotation dataset
The rare nature of bone dysplasias makes the data collec-
tion particularly challenging. In 2002, the European Skele-
tal Dysplasia Network (ESDN, http://www.esdn.org/) was
created to alleviate, at least partly, the data sparseness
issue. At the same time it aimed to provide a collabo-
rative environment to help with the diagnosis of skele-
tal dysplasias and to improve the information exchange
between researchers. To date, ESDN has gathered over
1,200 patient cases, which have been discussed by its panel
of experts. The ESDN case workflow consists of three
major steps: (i) a patient case is uploaded and an initial
diagnosis is set by the original clinician that referred the
case; (ii) the panel of experts discusses the case until an
agreement is reached; (iii) the panel of experts recom-
mends a final diagnosis. Among the total number of cases,
744 have a final bone dysplasia diagnosis (the remaining
cases were not thought to be true bone dysplasias by the
experts), with a total of 114 different skeletal dysplasias
covered.
Patient clinical summaries in ESDN are represented in
a free text format. The language used within the ESDN
clinical summaries suffers from several issues, such as
synonymy (several terms having the same meaning) or
hyponymy (one term beingmore specific than another). In
order to be able to use this data, we extracted patient phe-
notypes by annotating the text with corresponding terms
from the Human Phenotype Ontology (HPO). The actual
annotation process was performed using the National
Centre for Biomedical Ontology (NCBO) Annotator [20],
an ontology-based web service for annotation of tex-
tual sources with biomedical concepts. A bone dysplasia
expert (one of the co-authors) has manually validated the
resulting HPO annotations to ensure their correctness
and to eliminate, in particular, false positives.As a remark,
the false negatives resulted from the annotation process
may be under-estimated, and could not be validated since
we were not able to perform a full-fledged annotation of
the clinical summaries. The diagnosis associated with the
patient cases has also been annotated with concepts from
the Bone Dysplasia Ontology (BDO). More concretely,
the final diagnosis set by the panel of experts has been
converted to the corresponding BDO concept.
Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 5 of 13
http://www.jbiomedsem.com/content/5/1/8
In order to achieve realistic results using association
rule mining, from the 114 types of dysplasias present in
the ESDN dataset, we chose only those that were repre-
sented by more than 10 patient cases. This has reduced
our dataset to 394 annotated patient cases (i.e., around
33% of the total number) diagnosed with 15 different bone
dysplasias. The set features a total of 441 distinct pheno-
types, with an average of 63.67 distinct phenotypes per
disorder and an average of 4.49 distinct phenotypes per
case. The experiments described in this manuscript use
this dataset for training and testing purposes.
Proposed approach
Our goal is to discover association rules from anno-
tated and diagnosed patient cases in order to observe
co-occurrence relationships between clinical features and
disorders. In other words, we aim to find association rules
of the form {ICF } ? {IBD}, where ICF represents the set
of clinical features of a patient and IBD is a bone dys-
plasia diagnosis. From a conceptual perspective, ICF will
comprise annotations assigned to patient cases, or more
concretely, HPO concepts. We have adapted the Apriori
algorithm by adding two constraints, required to match
our aim: (i) every desired itemset must have one set of
clinical features and a single dysplasia, and (ii) both candi-
date itemsets and frequent itemsets can have at most one
dysplasia item.
Following the discovery of the desired itemsets, these
are partitioned into two components: a component con-
taining the skeletal dysplasia and one containing the phe-
notypes. A Boolean function that determines the type of
a component is used to perform this classification. Sub-
sequently, we calculate the different traditional or seman-
tic interestingness measures between the bone dysplasia
component and the phenotype set of the rule.
Modelling traditional support in the context of semantic
annotations
If an itemset consists of the items I = {i1, i2, i3, . . . , im}
for the reference concept RC and there are n transactions
in the knowledge base KB, Support is defined as the pro-
portion of instances of the reference concept RC in the
knowledge base which contain the itemset I.
Support(I,RC,KB)
= Number of instances of concept RC that contain the itemset IThe total number of instances of the concept RC
(7)
In our case, the reference concept (RC) is represented
by the patient (P) and KB is annotated dataset. Below we
present an example of traditional Support calculation.
Let us consider the following set of clinical features rep-
resented by HPO concepts (cf ? ICF ), in addition to a
bone dysplasia:
 cf1  HP:0008921 (Neonatal short-limb short
stature)
 cf2  HP:0008905 (Rhizomelic short stature)
 cf3  HP:0000772 (Abnormality of the ribs)
 cf4  HP:0000774 (Narrow chest)
 bd1  BDO:Achondroplasia
Let us also consider three reference concepts (i.e.,
patients) p1, p2 and p3 and assume that the KB contains
the following itemsets:
 I(p1) = {Icf1(p1), Icf3(p1), bd1} I(p2) = {Icf1(p2), Icf4(p2), bd1} I(p3) = {Icf2(p3), Icf3(p3), bd1}
where Icfx(px)={cfx|exhibits(px, cfx)}. Our goal is to com-
pute the support of the itemset I(p)={Icf1 (p), Icf3(p), bd1}.
We can quickly observe that there is one patient instance
that contains this pattern  i.e., p1. Since the total
number of patient instances is 3, traditional support is
then:
Support(I, P,KB) = 13 = 0.33 (8)
However, a close look at cf1 and cf2 in HPO reveals that
these concepts are fairly similar (they have a direct com-
mon ancestor in HP:0008873  Disproportionate short-
limb short stature), but not exactly the same. cf3 and cf4
are in a similar situation, with the parent of HP:0000774
(i.e., HP:0005257  Thoracic hypoplasia) being a sibling
of cf3. Unfortunately, traditional Support cannot leverage
this semantic similarity information as it relies on exact
matching. To overcome this issue, we propose an alter-
native set of semantic interestingness measures (Semantic
Support, Semantic Confidence, etc.).
Semantic similarity of items
Our intuition is that by using semantic similarity mea-
sures on patient findings (i.e., HPO concepts) we are able
to leverage and use the semantic relationships between
phenotypes that cannot, otherwise, be acquired by typical
data mining processes (due to their term-based match-
ing process). As an example, if the background knowledge
base lists HP:0000256 (Macrocephaly) as a phenotype of
Achondroplasia and a new patient exhibits HP:0004439
(Craniofacial dysostosis), we want to use the semantic
similarity value between the two concepts to associate
the later to Achondroplasia with a certain probability.
The semantic similarity between the concepts could be
inferred, for example, via their most common ancestor 
HP:0000929 (Abnormality of the skull). Such an associa-
tion is not possible when employing a typical data mining
process since each term would be considered individu-
ally and only in the context provided by the background
knowledge base.
Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 6 of 13
http://www.jbiomedsem.com/content/5/1/8
In principle, a good semantic similarity measure needs
to take into account the specific aspects of the target
domain. There are, nevertheless, a series of requirements
 emerging also from the bone dysplasia domain and the
structure of HPO  that are generally applicable:
 Given two HPO concepts, we consider them to be
more similar if they are closer to each other (i.e., the
path between them is shorter). E.g., HP:0004481
(Macrocephaly progressive) will be considered more
similar to HP:0000256 (Macrocephaly) than
HP:0004488 (Macrocephaly at Birth ), because the
distance between HP:0004481 and HP:0000256
is 1 whereas the distance between HP:0004481 and
HP:0004488 is 2.
 Several strategies have been used in choosing the
semantic similarity function. Li et al. [21], in their
work on modelling and capturing semantic similarity
in WordNet, have employed an exponent function to
transfer the path length between concepts into a
similarity value and have showed that the exponential
measure significantly outperforms traditional
similarity measures. Given that the design philosophy
of HPO andWordNet are similar, we derive the
similarity between two phenotypes as an exponent
function of the path length between their
corresponding HPO concepts. The same rationale is
valid also for BDO.
 In order to be able to calculate the semantic
interestingness measures, semantic similarity needs
to take values between 0 to 1. At the same time, an
exact match should be signalled by a semantic
similarity value of 1.
 The semantic similarity value of two concepts should
be dependent on the specificity of their LCA (i.e., its
location in the overall hierarchy). More concretely,
we consider the more specific LCA to be more
informative. E.g., HP:0004439 (Craniofacial
dysostosis) (as an LCA) should be considered more
informative than HP:0000929 (Abnormality of the
skull ), which is in this case, is its direct parent.
In the following we describe a set of domain-oriented
semantic similarity functions that satisfy the above-listed
requirements.
Domain-specific semantic similarity measures. If i1
and i2 are two items, we define the semantic similarity
between them as:
SemSim(i1, i2) = Dist(LCA(i1, i2),Root)Dist(i1, i2) + Dist(LCA(i1, i2),Root)
(9)
where Dist(LCA(i1, i2),Root) is the length of path from
LCA(i1, i2) to the root and Dist(i1, i2) is a distance
measure between i1 and i2 that depends on the underlying
types of the items.
If the items under scrutiny are phenotypes, we define
Dist(i1, i2) as shown in Eq. 10.
Dist(i1, i2) =
??
?
2lx , if i1 = i2
0, if i1 = i2 = root
1, if i1 = i2 = root
(10)
where lx is the shortest path between i1 and i2. This
formula determines the semantic similarity of two HPO
terms based on both the distance between these terms
and the location of their LCA in the HPO structure. It can
also be observed that the larger the distance between the
terms, the less similar they will be. Finally, if two concepts
are the same but do not denote the root, the value of the
function is 0, while if they do denote the root, the value of
the function is 1, to avoid the division by 0 case.
In Eq. 10 the shortest path length is scaled by an expo-
nential function to providemore weight to distance rather
than depth. Furthermore, the base and the exponent of
this power function aim to overemphasise the similarity
between phenotypes when taking into account the HPO
structure. Generally, this similarity decreases faster than
the distance. For instance, the distance between Macro-
cephaly and Macrocephaly progressive is 1 and they are
very similar, while the distance between Abnormality of
Skull and Macrocephaly progressive is 3, with the former
being much more generic and different to Macrocephaly
progressive than any of the other macrocephalies.
Similar to the phenotype distance described above, if we
consider two disorders using the Bone Dysplasia Ontol-
ogy, we define the same Dist(i1, i2) as shown in Eq. 11 
the semantic similarity equation remains unchanged (i.e.,
as per Eq. 9).
Dist(i1, i2) =
??
?
10lx?2, if i1 = i2
0, if i1 = i2 = root
1, if i1 = i2 = root
(11)
where lx is again the shortest path between i1 and i2.
The rationale behind Eq. 11 is the same as for Eq. 10 (see
above), with the remark that the overall similarity between
disorders decays at an even higher rate (with the distance
in BDO) because of their coarse grained nature, which has
led to a fairly flat structure of the ontology. The structure
of the ontology, and more concretely its maximum depth
(i.e., 2), has influenced the constant (2) in the exponent of
the formula (lx ? 2). The intuition is that concepts that
belong to the same group, i.e., they are at the second level
in the hierarchy and the distance between them is 2 (via
the LCA), should receive the highest similarity, after the
exact match.
Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 7 of 13
http://www.jbiomedsem.com/content/5/1/8
Semantic support
Given a knowledge base and an itemset, our goal is to
automatically derive a score that indicates the proportion
of transactions in the knowledge base that contain the
itemset at a semantic level, thus going beyond the exact
matching methods traditionally used for this task. This
needs to take into account the relations between items.
We attempt to model the semantic support of an item-
set as a function of the semantic similarity of the terms
present in the knowledge base and the itemset.
If we consider a database T with n transactions
{t1, t2, . . . , tn} and m items {i1, i2, . . . , im}, Semantic Sup-
port of {i1, i2, . . . , ip} (p ? m) is calculated as follows:
SemSupport(i1, i2, . . . , ip) = 1n ?
n?
q=1
p?
j=1
argmax
v=1to|tq|
||SemSim(ij , iv)||
(12)
The value of the Semantic Similarity (SemSim) ranges
from 0 to 1 and so does the value of the Semantic Support.
Semantic interestingnessmeasures
Semantic interestingness measures take into account how
data items are semantically related. To do so, it makes use
of the underlying structure of the ontology that hosts the
corresponding items (e.g. generalisation, specialisation,
etc). Hence, if we replace the traditional Support element
in the confidence calculation with Semantic Support we
get Semantic Confidence. The same process can be applied
for the other well-known interestingness measures, such
as lift, conviction, etc. Below we list the corresponding
semantic calculation for thesemeasures for an association
rule IX ? Q.
SemConfidence(IX ? Q) = SemSupport(IX ,Q)SemSupport(IX ) (13)
SemLift(IX ? Q) = SemConfidence(IX ,Q)SemSupport(Q) (14)
SemConviction(IX ? Q) = 1? SemSupport(Q)1 ? SemConfidence(IX ? Q)
(15)
SemLeverage(IX ? Q) = SemSupport(IX ,Q)
? SemSupport(IX ) ? SemSupport(Q)
(16)
SemJaccard(IX ? Q)
= SemSupport(IX ,Q)SemSupport(IX ) + SemSupport(Q) ? SemSupport(IX ,Q)
(17)
SemCosine(IX ? Q) = SemSupport(IX ,Q)?SemSupport(IX) ? SemSupport(Q))
(18)
SemCorrelationCoeff (IX ? Q)
= SemLeverage(IX ? Q)?
S Supp(IX)?S Supp(Q)?(1? S Supp(IX) ? (1? S Supp(Q))
(19)
S Supp in Eq. 19 denotes Semantic Support.
Experimental design
We have carried out a series of experiments with the
following goals:
 Firstly, we aim to analyse the accuracy of the
resulting association rules when using existing
traditional interestingness measures;
 Secondly, we are interested in finding out the same
accuracy, but when using the proposed semantic
interestingness measures;
 Finally, we aim to observe the difference between the
accuracies produced via the two methods.
The quality of discovered rules depends on their ability to
determine the correct diagnosis. Tomeasure accuracy, we
have employed a voting strategy, which is described below.
The purpose of evaluating the discovered rules is to
understand the utility of the interestingness measures.
Voting allows all firing association rules to contribute to
the final prediction. This strategy combines the associ-
ations KF( px) that fire upon a new patient case px. A
simple voting strategy considers all the rules in KF( px),
groups the rules by antecedent, and for each antecedent
IX obtains the class corresponding to the rule with high-
est confidence. We will denote the class voted by an
antecedent Ii with a binary function vote(Ii, bd) that takes
the value 1 when Ii votes for disorder bd, and 0 for the any
other class  {bdn1, bd2, . . . , bdn} ? BD represent a set of
bone dysplasias. The disorder that receives the maximum
vote is the most probable diagnosis for patient case x.
TotalVote(bdi) =
?
Ii?antecedents(KF(px))
Vote(Ii, bdi) (20)
Weighted voting is similar to simple voting, however,
each vote is multiplied by a factor that quantifies the qual-
ity of the vote. In the case of association rules, this can be
done using one of the above defined measures.
TotalVote(bdi) =
?
Ii?antecedents(KF(px))
Vote(Ii , bdi) ?QVote(Ii , bdi)
(21)
In our case, QVote(Ii, bdi) is the quality of vote, or more
concretely themaximum interestingness of that particular
antecedent group.
Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 8 of 13
http://www.jbiomedsem.com/content/5/1/8
We have performed individual experiments for each of
the interestingness measures previously described, using
the voting strategy. To assess their efficiency, we have
calculated the overall accuracy of the discovered associa-
tion rules. In all experiments, we compute the prediction
accuracy as the overall percentage of correctly predicted
disorders at a given recall cut-off point (i.e., by taking into
account only the top K predictions for different values of
K, where K is the recall cut-off point). Hence, a success
represents a correctly predicted disorder (the exact same,
and not a sub or super class of it), while a miss represents
an incorrectly predicted disorder. If N is the total number
of test cases and CP is the number of correctly predicted
disorders, then Accuracy = CP/N . This is expressed in
percentages in Tables 1, 2 and 3 in the Results section.
As mentioned earlier in the manuscript our annotated
dataset consisted of over 300 patience cases, with the clin-
ical features annotated using HPO and the disorders using
BDO. In order to provide an accurate view over the pre-
diction of the discovered rules, each experiment has been
performed as a 5-fold cross validation with an 80-20 split
(80% knowledge base, 20% test data). Tables 1, 2 and 3 lists
the resulted average accuracy at five different recall cut-off
points.
Within each experiment, we have used a relatively low
minimum Support of 5/N , where N is the total number
of cases, because we are interested in extracting both fre-
quent and occasional associations. Every rule was able to
contribute to the voting. Controlling the number of rules
using any minimum interestingness threshold can bias
the voting and hence, the overall result. Consequently, we
have not used this parameter to control the number of
rules. Finally, we have used a maximum itemset size of 10
as the computational cost increases exponentially with the
itemset size in the association rule mining process.
Results
In this section we present and discuss the experimental
results achieved using traditional and semantic interest-
ingness measures. We start with the semantic similarity
proposed in the previous sections and then compare its
results against a series of classic semantic similarity mea-
sures.
Proposed semantic similarity metric
In order to observe the quality improvements brought
by semantic interestingness measures over the traditional
ones, we have evaluated the discovered rules against real
world patient data. As already mentioned, we performed
two sets of experiments. Firstly, we have compared and
evaluated different traditional interestingness measures.
Then, we performed the same experiment but by using
semantic interestingness measures. This has enabled us to
perform an overall comparison between the two types of
measures.
Table 1 lists the experimental results for the traditional
measures. A first observation is that Confidence has the
overall best behaviour. At any recall cut-off point greater
than 2 (K > 1) Confidence outperforms or scores simi-
larly to the other measures. For example, it achieves an
accuracy of 46.58% for K = 2 and 53.42% for K = 3, both
with 1.37% higher than the second scoring measure, Jac-
card. The only exception appears for K = 1, where Jaccard
outperforms Confidence by 2.74%. A second, interesting,
observation is that with the increase in the recall cut-off
point, the measures reach a common ground, and hence,
achieve the same performance  for K = 5, six of the seven
measures score the same accuracy (57.53%).
Each of the measures we have considered in our exper-
iments studies certain properties of the data. Conse-
quently, the above-listed results enable us to reach a
better understanding of the underlying nature of the rela-
tionships manifested by the data in our bone dysplasia
annotated dataset. For example, Confidence measures the
level of causality (implication), while Jaccard measures the
degree of overlap among the given sets, or in our cases
patient phenotypes. This leads to the conclusion that the
bone dysplasia data seems to be governed more by causal-
ity and overlap, rather than, for example, co-occurrence,
which is described by Lift.
Table 1 Experimental results on finding the quality of association rules, discovered using traditional interestingness
measures
Traditional Accuracy Accuracy Accuracy Accuracy Accuracy
interestingness measures K = 1 K = 2 K = 3 K = 4 K = 5
Confidence 28.77 46.58 53.42 54.79 57.33
Lift 26.03 36.99 42.47 49.32 57.53
Conviction 28.77 43.84 46.58 49.32 57.53
Correlation coefficient 27.40 36.99 45.21 52.05 57.53
Cosine 28.76 43.84 49.31 54.79 58.90
Jaccard 31.51 45.21 52.05 54.79 57.53
Leverage 24.66 35.62 46.58 54.79 57.53
The voting strategy has been used as classification method and the association rules have been used as background knowledge.
Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 9 of 13
http://www.jbiomedsem.com/content/5/1/8
Table 2 Experimental results on finding the quality of association rules, discovered using semantic interestingness
measures
Semantic Accuracy Accuracy Accuracy Accuracy Accuracy
Interestingness measures K = 1 K = 2 K = 3 K = 4 K = 5
Semantic confidence 31.51 49.32 57.53 61.64 64.38
Semantic lift 27.40 38.36 47.95 57.53 61.64
Semantic conviction 32.88 43.84 53.42 56.16 58.90
Semantic correlation coefficient 23.29 38.36 45.21 57.53 64.38
Semantic cosine 31.51 47.95 52.05 57.53 61.64
Semantic jaccard 34.25 46.58 56.16 61.64 64.38
Semantic leverage 26.02 36.99 53.42 58.90 63.01
The voting strategy has been used as classification method and the association rules have been used as background knowledge.
Table 2 lists the experimental results for the semantic
interestingness measures. We can easily observe that the
results follow the same trend as in the previous exper-
iment. Semantic Confidence has, again, an overall best
behaviour for K > 1, outperforming Semantic Jaccard with
1.37% for K = 2 (49.32%) and K = 3 (57.53%) and achieving
the same accuracy for K = 4 (61.64%) and K = 5 (64.38%).
Semantic Jaccard achieves a better accuracy for K = 1,
i.e., 34.25%, with 2.74% higher than Semantic Confidence.
Finally, as in the previous experiment, we observe that the
increase in the recall cut-off point leads to a more uniform
accuracy across all measures, although slightly less aligned
as they do not achieve the exact same accuracy.
A comparative overview of the two types of measures is
presented in Table 3, where we can observe that semantic
measures achieve better results than the traditional ones.
Furthermore, the increase in the recall cut-off point leads
to a bigger difference in accuracy, from 2.74% for K = 1 to
6.85% for K = 5.
The main reason behind the increase in accuracy is the
use of similarity matching between terms. For instance,
an ESDN patient diagnosed with Achondroplasia had the
following phenotypes: Rhizomelic short stature,Muscular
hypotonia, Hypoplasia involving bones of the extremities
andMalar flattening. The classifier using traditional con-
fidence measures was not able to classify correctly this
case, while the classifier using semantic confidence did.
The semantic similarity employed by the latter found
an association between Rhizomelic short stature and
Achondroplasia based on the more generic Short stature
phenotype, which is common in Achondroplasia. This
Table 3 Comparative overview of the experimental results
achieved by the traditional and semantic interestingness
measures
Interestingness Accuracy Accuracy Accuracy Accuracy Accuracy
measures K = 1 K = 2 K = 3 K = 4 K = 5
Traditional 28.77 46.58 53.42 54.79 57.53
Semantic 31.51 49.32 57.53 61.64 64.38
represents a clear example where the exact matching used
by traditional classifiers fails. Another similar instance
was in the case of a MED patient that exhibited the
following phenotypes: Pes planus (i.e., flat feet), Rhi-
zomelic shortening and Frontal bossing. As in the previous
example, the classifier using traditional confidence failed
to classify this instance correctly, while the one using
semantic confidence did, based on the semantic similar-
ity between Pes planus and the diverse feet abnormalities
that characterise MED.
In order to have an accurate view over the classifica-
tion results, we have checked the statistical significance
of the increase in accuracy at recall cut-off point 5. The
purpose of this statistical significance testing was to assess
the performance of the classification using semantic rules
against the performance of the classification using tradi-
tional rules, both on the ESDN dataset. Such a test would
validate the observed increase in accuracy of 6.85% and
would show that it has not been obtained by chance.
Since the comparison is between two different
approaches on a single domain (skeletal dysplasias), we
have used the McNemars Chi-squared test with conti-
nuity correction [22]. The null hypothesis was that the
number of patient cases correctly classified by the classi-
fier using semantic confidence but not by the one using
traditional confidence is equal to the number of patient
cases correctly classified by the classifier using traditional
confidence but not by the one using semantic confidence.
Table 4 shows the distribution of the 394 patient cases
used in our experimental classification setting: (i) 205
patient cases were correctly classified by both classifiers;
(ii) 118 patient cases were misclassified by both classifiers;
(iii) 51 patient cases were correctly classified using seman-
tic confidence; and (iv) 20 patient cases were correctly
classified using traditional confidence. From this data, the
McNemar test statistic with continuity correction is:
?2McNemar =
(|51? 20| ? 1)2
51+ 20 = 12.67 (22)
Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 10 of 13
http://www.jbiomedsem.com/content/5/1/8
Table 4 Distribution of classification results in the
McNemars statistical significance test
Semantic confidence based
classifier
Positive Negative Total
Traditional confidence
based classifier
Positive 205 20 225
Negative 51 118 169
Total 256 138
A McNemar test value of 12.67 corresponds to a p-
value of 0.00037157, which provides strong evidence to
reject the null hypothesis. We can, hence, conclude that
the semantic interestingness measures we have proposed
are able, with the help of the underlying domain ontolo-
gies, to take advantage of the similarity matching between
the terms in the skeletal dysplasia domain.
Classic semantic similarity metrics
In order to understand the role carried by the semantic
similarity metric in the classification based on semantic
interestingness we have experimented with three classic
semantic similarities, defined earlier in the paper: Resnik,
Lin and Wu & Palmer. The results achieved by each of
these metrics are discussed below.
Table 5 lists the experimental results achieved by the
semantic interestingness measures employing Resnik as
semantic similarity. A first observation is that all measures
have performed uniformly, while from a comparative per-
spective, they performed worse than exact matching and
our proposed semantic similarity method. As in the previ-
ous experiments, we observe that the increase in the recall
cut-off point leads to a more uniform accuracy across
all measures. The Resnik semantic similarity method is
primarily dependent on the frequency of the most infor-
mative common ancestors. If any of the ancestors does not
exist in the corpus, the similarity value becomes infinity,
i.e., the concepts under scrutiny are completely dissimilar.
In the case of our dataset, this is the main issue behind
the failure of the Resnik semantic similarity  being a
real-world dataset, most patient cases will feature con-
crete (very specific) phenotypes, while common ancestors
represent more generic/abstract concepts rarely found in
clinical summaries. For example, the semantic similarity
of Dolichocephaly and Full cheeks is ?, due to the fact
that the frequency of all their ancestors (Abnormality of
the head, Abnormality of head and neck and Phenotype
abnormality) in the patient cases is 0.
The experimental results for the semantic interesting-
ness measures using the second semantic similarity 
Lin  have led 0% accuracy on all measures and all five
recall cut-off points  consequently we have have included
them in a table. As in the case of Resnik, Lin is also heavily
dependent on the IC of the common ancestors, and hence
suffers from the same issue discussed above. Another
problematic aspect of the Lin measure is that, in the con-
text of the ESDNdata, it assigns higher similarity values to
partial matches than to exact matches. A similarity value
of 1 is achieved when the concepts being measured are
the exact same  e.g., Short long bones. However, when
the concepts are different and any of their ancestors is
present in the underlying corpus, the similarity value will,
usually, be greater than 1. This is because the frequency
of the ancestors (more abstract concepts) will be less than
the frequency of the actual concepts and IC is inversely
proportional to frequency.
For instance, the semantic similarity value between
Macrocephaly and Hypoplasia involving bones of the
extremities is 2.19 because the frequency of their most
informative common ancestor  Abnormality of the skele-
tal system is less than that of both concepts. The latter
occurs only 5 times in the corpus whereas Macrocephaly
andHypoplasia involving bones of the extremities occur 41
and 70 times, respectively. The Resnik measure is able to
avoid this issue by treating exact and partial matches in the
samemanner  i.e., directly and only via the IC of themost
informative common ancestor and not by further diving it
by the IC of the actual concepts. In an ideal scenario, exact
matches should assign higher similarity values that partial
matches.
Table 5 Experimental results on finding the quality of association rules discovered using semantic Interestingness
measures that employed Resnik as semantic similaritymethod
Semantic interestingness measures Accuracy Accuracy Accuracy Accuracy Accuracy
(Employing Resnik) K = 1 K = 2 K = 3 K = 4 K = 5
Semantic confidence 5.48 6.85 9.59 10.96 10.96
Semantic lift 5.48 8.22 9.59 9.59 10.96
Semantic conviction 2.74 6.85 9.59 9.59 10.96
Semantic correlation coefficient 5.48 8.22 9.59 9.59 10.96
Semantic cosine 5.48 8.22 9.59 10.96 10.96
Semantic jaccard 5.48 8.22 9.59 9.59 10.96
Semantic leverage 5.48 8.22 9.59 9.59 10.96
Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 11 of 13
http://www.jbiomedsem.com/content/5/1/8
Finally, Table 6 lists the experimental results for the
semantic interestingness measures using the last seman-
tic similarity  Wu & Palmer. We can observe that
the results follow fairly closely the trend present in our
experiments with the traditional interestingness mea-
sures and the semantic interestingness measures employ-
ing our proposed metric. Similarly to those results,
there is an increase in accuracy with the increase in
the recall cut-off point, which also leads to a more
uniform accuracy across all measures. Semantic Con-
fidence has an overall best behaviour for K > 1,
while Semantic Leverage achieves a better accuracy for
K = 1, i.e., 23.29%, with 2.74% higher than Semantic
Confidence.
TheWu & Palmer similarity score ranges between 0 and
1, with 1 denoting an exact match and the rest of the val-
ues being assigned based on the depth in the hierarchy and
distance between the concepts. This is the main reason
behind its good performance  i.e., it uses only struc-
tural distances instead of information content. It is, how-
ever, biased more towards depth than the actual distance
between concepts, or more concretely it is influenced by
the depth of the common ancestor of the concepts. In
the case of out dataset, and using HPO as background
knowledge, this represents an issue because most com-
mon ancestors are located at fairly uniform depths (due
to the inherent specificity of the terms) and, as such,
do not provide enough variety for the final similarity
score.
In conclusion, none of the classic semantic similari-
ties perform better than the approach we have proposed:
node-based similarities are heavily influenced by the pres-
ence, or more precisely absence, of the common ancestor
in the dataset (which leads to complete dissimilarity),
while the edge-based similarity we have experimented
with focuses more on the depth of the common ancestor,
as opposed to the distance between the concepts, which
is more appropriate given our dataset and background
knowledge.
Discussion and conclusions
Main findings
In conclusion, based on the annotated bone dysplasia
dataset, Confidence appears to be the best interesting-
ness measure regardless of way in which is computed, i.e.,
traditional or semantic. The use of semantics provides a
marginal, but consistent, improvement in accuracy over
traditional measures. Since the semantic similarity relies
on the structure of the underlying ontology, this improve-
ment is heavily dependent on the reflection provided by
the domain ontology over the real domain knowledge.
Limitations and generalisation
Every domain is governed by a set of rules. A good seman-
tic similarity measure needs to take into account the
rules of the target domain. In our case, we have pro-
posed and used two particular similarity measures, one
tailored on the knowledge externalised by HPO and one
on the structure of bone dysplasias, provided by BDO.
These semantic similarity measures are not necessarily
directly applicable to other domains. Consequently, while
the definition of semantic support is generic, in order to
apply our approach in a different domain, an investigation
is required to determine the most appropriate semantic
similarity for that domain.
Relatedwork
The literature contains a number of studies on using
association rule mining to identify relationships among
medical attributes using biomedical ontologies [23-26].
Kumar et al. [23] used association rules to indicate
dependence relationships between Gene Ontology terms
using an annotation dataset and background knowl-
edge. Myhre et al. [24], on the other hand, have focused
entirely on proposing an additional gene ontology layer
via discovering cross-ontology association rules from GO
annotations. However, none of these approaches use the
biomedical ontologies and, in particular, their hierarchical
structure to compute interestingness measures. Another
Table 6 Experimental results on finding the quality of association rules discovered using semantic Interestingness
measures that employedWu & Palmer as semantic similaritymethod
Semantic interestingness measures Accuracy Accuracy Accuracy Accuracy Accuracy
(EmployingWu and Palmer) K = 1 K = 2 K = 3 K = 4 K = 5
Semantic confidence 20.55 35.62 36.99 42.47 54.79
Semantic lift 13.70 26.03 28.77 39.73 52.05
Semantic conviction 16.44 24.66 26.03 34.25 52.05
Semantic correlation coefficient 20.55 28.77 32.88 39.73 43.84
Semantic cosine 21.92 32.88 34.25 42.47 54.79
Semantic jaccard 20.55 35.62 38.36 41.10 54.79
Semantic leverage 23.29 30.14 32.88 38.36 45.21
Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 12 of 13
http://www.jbiomedsem.com/content/5/1/8
set of existing research on applying association rule min-
ing to biomedical ontologies includes studies on mining
single level, multi-level and cross-ontology association
rules [27-29]. Carmona-Saez et al. [27], for example, mine
single level associations between GO annotations and
expressed genes from microarray data integrated with
GO annotation information. However, as in the previous
case, the inherent information provided by the ontology
structure is not considered when computing the interest-
ingness measures, and hence limit, to some extent, the
knowledge discovered.
Interestingnessmeasures play an essential role by reduc-
ing the number of discovered rules and retaining only
those with the best utility, in a post-processing step. Dif-
ferent rule interestingness measures have different qual-
ities or flaws. There is no optimal measure and one way
to solve this challenge is to try to find a good compro-
mise. Research has been performed on finding optimal
measures for different datasets [8,9], but by taking into
account only traditional interestingness measures.
In summary, prior efforts in association rule mining
applied to datasets annotated with biomedical ontology
concepts focus on mining normal, cross-ontology and
multi-level association rules, but leave out the use of the
semantic relationships between the target concepts from
the computation of the interestingness measures.
Conclusion
Concepts defined and described by biomedical ontologies,
e.g., the Human Phenotype Ontology, enable us to com-
pare medical terms at a semantic level  a comparison
that is otherwise not possible. Our research has focused
on the use of semantic relationships between patient phe-
notypes, annotated by HPO entities, in the process of
mining association rules. In this manuscript, we have pro-
posed a method that integrates concept similarity metrics
into the computation of traditional interestingness mea-
sures, with application to finding association rules in the
bone dysplasia domain. This method has been applied on
an annotated patient dataset and used domain-specific
semantic similarities.
Experimental results have led to the conclusion that, for
our domain, Confidence is the most accurate measure,
independently on the underlying computation method,
i.e., traditional or semantic. On the other hand, Semantic
Confidence was able to take advantage of structure of the
domain ontologies and of the custom semantic similarity
to achieve better results (up to 6.85% better accuracy
over the traditional Confidence). In conclusion, these
results suggest that, given an appropriate domain-specific
ontology, semantic similarities are able to improve the
efficiency of traditional interestingness measures in the
association rule discovery process, hence enabling a
valuable semantic interestingness measures framework.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
JH and AZ formulated the basic idea behind SKELETOME. JH coordinates the
project. TG leads the development of the project. RP and TG designed the
experiments. RP run the experiments. RP and TG analysed the experimental
results. AZ provided the domain expertise. RP and TG wrote the manuscript.
JH and AZ edited the manuscript. All authors read and approved the final
manuscript.
Acknowledgements
We gratefully acknowledge the editor and anonymous reviewers whose
comments and advices have helped us improve our manuscript. This research
is funded by the Australian Research Council (ARC) under the Linkage grant
SKELETOME  LP100100156 and the Discovery Early Career Researcher Award
(DECRA)  DE120100508.
Author details
1School of ITEE, The University of Queensland, St. Lucia, Queensland 4072,
Australia. 2Bone Dysplasia Research Group, UQ Centre for Clinical Research
(UQCCR), The University of Queensland, Herston, Queensland 4006, Australia.
3Genetic Health Queensland, Royal Brisbane and Womens Hospital, Herston,
Queensland 4006, Australia.
Received: 22 February 2013 Accepted: 21 January 2014
Published: 5 February 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Hettne et al. Journal of Biomedical Semantics 2014, 5:41
http://www.jbiomedsem.com/content/5/1/41RESEARCH Open AccessStructuring research methods and data with the
research object model: genomics workflows as a
case study
Kristina M Hettne1, Harish Dharuri1, Jun Zhao3, Katherine Wolstencroft2,6, Khalid Belhajjame2, Stian Soiland-Reyes2,
Eleni Mina1, Mark Thompson1, Don Cruickshank3, Lourdes Verdes-Montenegro5, Julian Garrido5, David de Roure3,
Oscar Corcho4, Graham Klyne3, Reinout van Schouwen1, Peter A C t Hoen1, Sean Bechhofer2, Carole Goble2
and Marco Roos1*Abstract
Background: One of the main challenges for biomedical research lies in the computer-assisted integrative study of
large and increasingly complex combinations of data in order to understand molecular mechanisms. The preservation
of the materials and methods of such computational experiments with clear annotations is essential for understanding
an experiment, and this is increasingly recognized in the bioinformatics community. Our assumption is that offering
means of digital, structured aggregation and annotation of the objects of an experiment will provide necessary
meta-data for a scientist to understand and recreate the results of an experiment. To support this we explored
a model for the semantic description of a workflow-centric Research Object (RO), where an RO is defined as a
resource that aggregates other resources, e.g., datasets, software, spreadsheets, text, etc. We applied this model
to a case study where we analysed human metabolite variation by workflows.
Results: We present the application of the workflow-centric RO model for our bioinformatics case study.
Three workflows were produced following recently defined Best Practices for workflow design. By modelling the
experiment as an RO, we were able to automatically query the experiment and answer questions such as which
particular data was input to a particular workflow to test a particular hypothesis?, and which particular conclusions
were drawn from a particular workflow?.
Conclusions: Applying a workflow-centric RO model to aggregate and annotate the resources used in a bioinformatics
experiment, allowed us to retrieve the conclusions of the experiment in the context of the driving hypothesis, the
executed workflows and their input data. The RO model is an extendable reference model that can be used by other
systems as well.
Availability: The Research Object is available at http://www.myexperiment.org/packs/428
The Wf4Ever Research Object Model is available at http://wf4ever.github.io/ro
Keywords: Semantic web models, Scientific workflows, Digital libraries, Genome wide association study* Correspondence: m.roos@lumc.nl
1Department of Human Genetics, Leiden University Medical Center, Leiden,
The Netherlands
Full list of author information is available at the end of the article
© 2014 Hettne et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited.
Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 2 of 16
http://www.jbiomedsem.com/content/5/1/41Background
One of the main challenges for biomedical research lies in
the integrative study of large and increasingly complex
combinations of data in order to understand molecular
mechanisms, for instance to explain the onset and pro-
gression of human diseases. Computer-assisted method-
ology is needed to perform these studies, posing new
challenges for upholding scientific quality standards for
the reproducibility of science. The aim of this paper is to
describe how the research data, methods and metadata
related to a workflow-centric computational experiment
can be aggregated and annotated using standard Semantic
Web technologies, with the purpose of helping scientists
performing such experiments in meeting requirements for
understanding, sharing, reuse and repurposing.
The workflow paradigm is gaining ground in bioinfor-
matics as the technology of choice for recording the steps
of computational experiments [1-4]. It allows scientists to
delineate the steps of a complex analysis and expose
this to peers using workflow design and execution tools
such as Taverna [5], and Galaxy [6], and workflow sharing
platforms such as myExperiment [7] and crowdLabs [8].
In a typical workflow, data outputs are generated from
data inputs via a set of (potentially distributed) computa-
tional tasks that are coordinated following a workflow
definition. However, workflows do not provide a complete
solution for aggregating all data and all meta-data that are
necessary for understanding the full context of an experi-
ment. Consequently, scientists often find it difficult (or
impossible) to reuse or repurpose existing workflows
for their own analyses [9]. In fact, insufficient meta-data
has been listed as one of the main causes of workflow
decay in a recent study of Taverna workflows on myEx-
periment [9]. Workflow decay is the term used when
the ability to re-execute a workflow after its inception
has been compromised.
We will be able to better understand scientific work-
flows if we are able to capture more relevant data and
meta-data about them; including the purpose and context
of the experiment, sample input and output datasets, and
the provenance of workflow executions. Moreover, if we
wish to publish and exchange these resources as a unit,
we need a mechanism for aggregation and annotation that
would work in a broad scientific community. Semantic
Web technology seems a logic choice of technology, given
its focus on capturing the meaning of data in a machine
readable format that is extendable and supports intero-
perability. It allows defining a Web-accessible reference
model for the annotation of the aggregation and the
aggregated resources that is independent of how data
are stored in repositories. Examples of other efforts where
Semantic Web technology has been used for the biomed-
ical data integration includes the Semantic Enrichment of
the Scientific Literature (SESL) [10] and Open PHACTS[11] projects. We applied the recently developed Research
Object (RO) family of tools and ontologies [12,13] to pre-
serve the scientific assets and their annotation related
to a computational experiment. The concept of the RO
was first proposed as an abstraction for sharing research
investigation results [14]. Later, the potential role for ROs
in facilitating not only the sharing but also the reuse of
results, in order to increase the reproducibility of these
results, was envisioned [15]. Narrowing down to workflow-
centric ROs, preservation aspects were explored in [16],
and their properties as first class citizen structures that
aggregate resources in a principled manner in [13]. We also
showed the principle of describing a (text mining) workflow
experiment and its results by Web Ontology Language
(OWL) ontologies [17]. The OWL ontologies were custom-
built, which we argue is now an unnecessary bottleneck for
exchange and interoperability. These studies all contributed
to the understanding and implementation of the concept
of an RO, but the data used were preliminary, and the
studies were focused on describing workflows with related
datasets and provenance information, rather than from
the viewpoint of describing a scientific experiment of which
workflows are a component.
A workflow-centric RO is defined as a resource that
aggregates other resources, such as workflow(s), proven-
ance, other objects and annotations. Consequently, an RO
represents the method of analysis and all its associated
materials and meta-data [13,15], distinguishing it from
other work mainly focusing on provenance of research
data [18,19]. Existing Semantic Web frameworks are used,
such as (i) the Object Exchange and Reuse (ORE) model
[20]; (ii) the Annotation Ontology (AO) [21]; and (iii)
the W3C-recommended provenance exchange models
[22]. ORE defines the standards for the description and
exchange of aggregations of Web resources and provides
the basis for the RO ontologies. AO is a general model
for annotating resources and is used to describe the RO
and its constituent resources as well as the relationships
between them. The W3C provenance exchange models
enable the interchange of provenance information on the
Web, and the Provenance Ontology (PROV-O) forms the
basis for recording the provenance of scientific workflow
executions and their results.
In addition, we used the minimal information model
Minim, also in Semantic Web format, to specify which
elements in an RO we consider must haves, should
haves and could haves according to user-defined re-
quirements [23]. A checklist service subsequently queries
the Minim annotations as an aid to make sufficiently
complete ROs [24]. The idea of using a checklist to per-
form quality assessment is inspired by related checklist-
based approaches in bioinformatics, such as the Minimum
Information for Biological and Biomedical Information
(MIBBI)-style models [25].
Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 3 of 16
http://www.jbiomedsem.com/content/5/1/41Case study: genome wide association studies
As real-world example we aggregate and describe the
research data, methods and metadata of a computational
experiment in the context of studies of genetic variation
in human metabolism. Given the potential of genetic
variation data in extending our understanding of genetic
diseases, drug development and treatment, it is crucial
that the steps leading to new biological insights can be
properly recorded and understood. Moreover, bioinformat-
ics approaches typically involve aggregation of disparate
online resources into complex data parsing pipelines.
This makes this a fitting test case for an instantiated
RO. The biological goal of the experiment is to aid in
the interpretation of the results of a Genome-Wide
Association Study (GWAS) by relating metabolic traits
to the Single Nucleotide Polymorphisms (SNPs) that were
identified by the GWAS. GWA studies have successfully
identified genomic regions that dispose individuals to
diseases (see for example [26], for a review see [27]).
However, the underlying biological mechanisms often
remain elusive, which led the research community to
evince interest in genetic association studies of metabolites
levels in blood (see for example [28-30]). The motivation is
that the biochemical characteristics of the metabolite and
the functional nature of affected genes can be combined to
unravel biological mechanisms and gain functional insight
into the aetiology of a disease. Our specific experiment in-
volves mining curated pathway databases and a specific text
mining method called concept profile matching [31,32].
In this paper we describe the current state of RO ontol-
ogies and tools for the aggregation and annotation of a
computational experiment that we developed to elucidate
the genetic basis for human metabolic variation.Methods
We performed our experiment using workflows developed
in the open source Taverna Workflow Management System
version 2.4 [5]. To improve the understanding of the ex-
periment, we have added the following additional resources
to the RO, using the RO-enabled myExperiment [33]: 1)
the hypothesis or research question (what the experiment
was designed to test); 2) a workflow-like sketch of the over-
all experiment (the overall data flow and workflow aims); 3)
one or more workflows encapsulating the computational
method; 4) input data (a record of the data that were used
to reach the conclusions of an experiment); 5) provenance
of workflow runs (the data lineage paths built from the
workflow outputs to the originating inputs); 6) the results
(a compilation of output data from workflow runs); 7) the
conclusions (interpretation of the results from the work-
flows against the original hypothesis). Such an RO was then
stored in the RO Digital Library [34]. RO completeness
evaluation is checked from myExperiment with a toolimplementing the Minim model [24]. Detailed description
of the method follows.
Workflow development
We developed three workflows for interpreting SNP-
metabolite associations from a previously published
genome-wide association study, using pathways from
the KEGG metabolic pathway database [35] and Gene
Ontology (GO) [36] biological process associations from
text mining of PubMed. To understand an association of a
SNP with a metabolite, researchers would like to know
the gene in the vicinity of the SNP that is affected by the
polymorphism. Then, researchers examine the functional
nature of the gene and evaluate if it makes sense given the
biochemical characteristics of the metabolite with which
it is associated. This typically involves interrogation of
biochemical pathway databases and mining existing litera-
ture. We would like to evaluate the utility of background
knowledge present in the databases and literature in
facilitating a biological interpretation of the statistically
significant SNP-metabolite pairs. We do this by first
determining the genes closest to the SNPs, and then
reporting the pathways that these genes participate in.
We implemented two main workflows for our experi-
ment. The first one mines the manually curated KEGG
database of metabolic pathway and gene associations
that are available via the KEGG REST Services [37].
The second workflow mines the text-mining based data-
base of associations between GO biological processes
and genes behind the Anni 2.1 tool [31] that are available
via the concept profile mining Web services [38]. We also
created a workflow to list all possible concept sets in
the concept profile database, to encourage reuse of the
concept profile-based workflow for matching against
other concept sets than GO biological processes. The
workflows were developed following the 10 Best Prac-
tices for workflow design [39]. The Best Practices were
developed to encourage re-use and prevent workflow
decay, and briefly consists of the following steps:
1) Make a sketch workflow to help design the
overall data flow and workflow aims, and to
identify the tools and data resources required at
each stage. The sketch could be created using for
example flowchart symbols, or empty beanshells
in Taverna.
2) Use modules, i.e. implement all executable
components as separate, runnable workflows to
make it easier for other scientists to reuse parts of a
workflow at a later date.
3) Think about the output. A workflow has the
potential to produce masses of data that need to be
visualized and managed properly. Also, workflows
can be used to integrate and visualise data as well as
Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 4 of 16
http://www.jbiomedsem.com/content/5/1/41for analysing it, so one should consider how the
results will be presented easily to the user.
4) Provide input and output examples to show the
format of input required for the workflow and the
type of output that should be produced. This is
crucial for the understanding, validation, and
maintenance of the workflow.
5) Annotate, i.e. choose meaningful names for the
workflow title, inputs, outputs, and for the processes
that constitute the workflow as well as for the
interconnections between the components, so that
annotations are not only a collection of static tags
but capture the dynamics of the workflow.
Accurately describing what individual services do,
what data they consume and produce, and the aims
of the workflow are all essential for use and reuse.
6) Make it executable from outside the local
environment by for example using remote Web
services, or platform independent code/plugins.
Workflows are more reusable if they can be
executed from anywhere. If there is need to use local
services, library or tools, then the workflow should
be annotated in order to define its dependencies.
7) Choose services carefully. Some services are more
reliable or more stable than others, and examining
which are the most popular can assist with this
process.
8) Reuse existing workflows by for example searching
collaborative platforms such as myExperiment for
workflows using the same Web service. If a workflow
has been tried, tested and published, then reusing it
can save a significant amount of time and resource.
9) Test and validate by defining test cases and
implementing validation mechanisms in order to
understand the limitations of workflows, and to
monitor changes to underlying services.
10)Advertise and maintain by publishing the workflow
on for example myExperiment, and performing
frequent testing of the workflow and monitoring of
the services used. Others can only reuse it if it is
accessible and if it is updated when required, due to
changes in underlying services.
The RO core model
The RO model [12,13] aims at capturing the elements
that are relevant for interpreting and preserving the
results of scientific investigations, including the hypoth-
esis investigated by the scientists, the data artefacts used
and generated, as well as the methods and experiments
employed during the investigation. As well as these ele-
ments, to allow third parties to understand the content
of the RO, the RO model caters for annotations that
describe the elements encapsulated by the ROs, as well
as the RO as a whole. Therefore, two main constructsare at the heart of the RO model, namely aggregation
and annotation. The work reported on in this article
uses version 0.1 of the RO model, which is documented
online [12].
Following myExperiment packs [7], ROs use the ORE
model [20] to represent aggregation. Using ORE, an RO
is defined as a resource that aggregates other resources,
e.g., datasets, software, spreadsheets, text, etc. Specifically,
the RO extends ORE to define three new concepts: i) ro:
ResearchObject is a sub-class of ore:Aggregation which
represents an aggregation of resources. ii) ro:Resource
is a sub-class of ore:AggregatedResource representing a
resource that is aggregated within an RO. iii) ro:Manifest
is a sub-class of ore:ResourceMap, representing a resource
that is used to describe the RO.
To support the annotation of ROs, their constituent
resources, as well as their relationship, we use the Anno-
tation Ontology [21]. Several types of annotations are
supported by the Annotation Ontology, e.g., comments,
textual annotations (classic tags) and semantic annotations,
which relate elements of the ROs to concepts from under-
lying domain ontologies. We make use of the following
Annotation Ontology terms: i) ao:Annotation, which acts
as a handle for the annotation. ii) ao:annotatesResource,
which represents the resource(s)/RO(s) subjects to anno-
tation. iii) ao:body, which describes the target of the anno-
tation. The body of the annotation takes the form of a set
of Resource Description Framework (RDF) statements.
Note that it is planned for later revisions of the RO model
to use the successor of AO, the W3C Community Open
Annotation Data Model (OA) [40]. For our purposes, OA
annotations follows a very similar structure using oa:
Annotation, oa:hasTarget and oa:hasBody.
Support for workflow-centric ROs
A special kind of ROs that are supported by the model is
what we call workflow-centric ROs, which, as indicated by
the name, refer to those ROs that contain resources that
are workflow specifications. The structure of the workflow
in ROs is detailed using the wfdesc vocabulary [41], and is
defined as a graph in which the nodes refers to steps in
the workflow, which we call wfdesc:Process, and the edges
representing data flow dependencies, wfdesc:DataLink,
which is a link between the output and input parameters
(wfdesc:Parameter) of the processes that compose the
workflow. As well as the description of the workflow,
workflow centric ROs support the specification of the
workflow runs, wfprov:WorkflowRun, that are obtained
as a result of enacting workflows. A workflow run is
specified using the wfprov ontology [42], which cap-
tures information about the input used to feed the
workflow execution, the output results of the workflow
run, as well as the constituent process runs, wfprov:
ProcessRun, of the workflow run, which are obtained
Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 5 of 16
http://www.jbiomedsem.com/content/5/1/41by invoking the workflow processes, and the input and
outputs of those process runs.
Support for domain-specific information
A key aspect of the RO model design is the freedom to
use any vocabulary. This allows for inclusion of very
domain-specific information about the RO if that serves
the desired purpose of the user. We defined new terms
under the name space roterms [43]. These new terms
serve two main purposes. They are used to specify anno-
tations that are, to our knowledge, not catered for by
existing ontologies, e.g., the classes roterms:Hypothesis
and roterms:Conclusion to annotate the hypothesis and
conclusions part of an RO, and the property roterms:
exampleValue to annotate an example value for a given
input or output parameter given as an roterms:Work-
flowValue instance. The roterms are also used to specify
shortcuts that make the ontology easy to use and more
accessible. For example, roterms:inputSelected associates
a wfdesc:WorkflowDefinition to an ro:Resource to state
that a file is meant to be used with a given workflow
definition, without specifying at which input port or in
which workflow run.
Minim model for checklist evaluation
When building an RO in myExperiment users are pro-
vided with a mechanism of quality insurance by our
so-called checklist evaluation tool, which is built upon
the Minim checklist ontology [23,44] and defined
using Web Ontology Language. Its basic function is to
assess that all required information and descriptions
about the aggregated resources are present and complete.
Additionally, according to explicit requirements defined in
a checklist, the tool can also assess the accessibility of
those resources aggregated in an RO, in order to increase
the trust on the understanding of the RO. The MinimFigure 1 An overview of the Minim model. An overview of the four commodel has four key components, as illustrated by Figure 1:
1) a Constraint, which associates a model (checklist) to
use with an RO, for a specific assessment purpose, e.g.
reviewing an RO containing sufficient information before
being shared; 2) a Model, which enumerates of the set of
requirements to be considered, which may be declared at
levels of MUST, SHOULD or MAY be satisfied for the
model as a whole; 3) a Requirement, which is the key part
for expressing the concrete quality requirements to an
RO, for example, the presence of certain information
about an experiment, or liveness (accessibility) of a data
server; 4) a Rule, which can be a SoftwareRequiremen-
tRule, to specify the software to be present in the operat-
ing environment, a ContentMatchRequirementRule, to
specify the presence of certain pattern in the assessed
data, or a DataRequirementRule, for specifying data re-
source to be aggregated in an RO.RO digital library
While myExperiment acts mainly as front-end to users,
the RO Digital Library [34] acts as a back-end, with two
complementary storage components: a digital repository
to keep the content, as a triple store to manage the
meta-data content. The ROs in the repository can be
accessed via a Restful API [45] or via a public SPARQL
endpoint [46]. All the ROs created in the myExperiment.
org are also submitted to the RO Digital Library.Workflow-centric RO creation process
Below we describe the steps that we conducted when
creating the RO for our case study in an RO-enabled
version of myExperiment [33]. The populated RO is
intended to contain all the information required to re-
run the experiment, or understand the results presented,
or both.ponents: a constraint, a model, a requirement, and a rule.
Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 6 of 16
http://www.jbiomedsem.com/content/5/1/41Creating an RO
The action of creating an RO consists of generating the
container for the items that will be aggregated, and get-
ting a resolvable identifier for it. In myExperiment the
action of creating an RO is similar to creating a pack.
We filled in a title and description of the RO at the point
of creation and got a confirmation that the RO had been
created and had been assigned a resolvable identifier in
the RO Digital Library (Figure 2).
Adding the experiment sketch
Using a popular office presentation tool, we made an
experiment sketch and saved it as a PNG image. We
then uploaded the image to the pack, selecting the type
Sketch. As a result, the image gets stored in the DigitalFigure 2 Screenshots from myExperiment illustrating the process of c
button the user can enter a title and description (A), while pressing the cr
identifier (B).Library and aggregated in the RO. In addition, an anno-
tation was added to the RO to specify that the image is
of type Sketch. A miniature version of the sketch is
shown within the myExperiment pack (Figure 3).
Adding the hypothesis
To specify the hypothesis, we created a text file that
describes the hypothesis, and then upload it to the pack
as type Hypothesis. The file gets stored in the Digital
Library and aggregated in the RO, this time annotated to
be of type Hypothesis.
Adding workflows
We saved the workflow definitions to files and uploaded
them to the pack as type Workflow. MyExperiment thenreating a Research Object placeholder. Before pressing the create
eate button will result in a placeholder Research Object with an
Figure 3 Workflow sketch. A workflow sketch showing that our experiment follows two paths to interpret genome wide association study
results: matching with concept profiles and matching with KEGG pathways.
Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 7 of 16
http://www.jbiomedsem.com/content/5/1/41automatically performed a workflow-to-RDF transform-
ation in order to extract the workflow structure according
to the RO model, which includes user descriptions and
metadata created within the Taverna workbench. The
descriptions and the extracted structure gets stored in
the RO Digital Library and associated with the workflow
files as annotations.
Adding the workflow input file
The data values were stored in files that were then
uploaded into the pack as Example inputs. Such files
gets stored in the RO Digital Library and aggregated in
the RO, and as Example inputs.
Adding the workflow provenance
Using the Taverna-Prov [47] extension to Taverna, we
exported the workflow run provenance to a file that we
uploaded to the pack as type Workflow run. Similar to
other resources, the provenance file gets stored in the
digital library with the type Workflow run, however as
the file is in the form of RDF according to the wfprov [42]
and W3C PROV-O [22] ontologies, it is also integrated
into the RDF store of the digital library and available for
later querying.
Adding the results
We made a compilation of the different workflow outputs
to a result file in table format, uploaded to the pack as
type Results. The file gets stored in the digital library
and aggregated in the RO, annotated to be of the type
Results.
Adding the conclusions
To specify the hypothesis, we created a text file that
describes the hypothesis, and then uploaded it to thepack as type Hypothesis. The file gets stored in the
digital library and aggregated in the RO, annotated to be
of type Conclusions.
Intermediate step: checklist evaluation
At this point we checked how far we were from satisfy-
ing the Minim model, and were informed by the tool
that the RO now fully satisfies the checklist (Figure 4).
Annotating and linking the resources
We linked the example input file to the workflows that
used the file by the property Input_selected (Figure 5).
In this particular case, both workflows have the same
inputs but they need to be configured in different ways.
This is described in the workflow description field in
Taverna.
Results
The RO for our experiment is the container for the
items that we wished to aggregate. In terms of RDF, we
first instantiated an ro:ResearchObject in an RO-enabled
version of myExperiment [33]. We thereby obtained a
unique and resolvable Uniform Resource Identifier (URI)
from the RO store that underlies this version of myEx-
periment. In our experimental setup this was http://
sandbox.wf4ever-project.org/rodl/ROs/Pack405/. It is
accessible from myExperiment [48]. Each of the subse-
quent items in the RO was aggregated as an ro:Resource,
indicating that the item is considered a constituent
member of the RO from the point of view of the scientist
(the creator of the RO).
Aggregated resources
We aggregated the following items: 1) the hypothesis
(roterms:Hypothesis): we hypothesized that SNPs can be
Figure 4 Screenshot of the results from the second check with the checklist evaluation service. The results from checklist evaluation
service show that the Research Object satisfies the defined checklist for a Research Object.
Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 8 of 16
http://www.jbiomedsem.com/content/5/1/41functionally annotated using metabolic pathway infor-
mation complemented by text mining, and that this will
lead to formulating new hypotheses regarding the role of
genomic variation in biological processes; 2) the sketch
(roterms:Sketch) shows that our experiment follows two
paths to interpret SNP data: matching with concept
profiles and matching with Kyoto Encyclopedia of Genes
and Genomes (KEGG) pathways (Figure 3); 3) the work-
flows (wfdesc:Workflow): Figure 6 shows the workflow
diagram for the KEGG workflow and Figure 7 shows
the workflow diagram for the concept profile matching
workflow. In Taverna, we aimed to provide sufficient
annotation of the inputs, outputs and the functions ofFigure 5 Screenshot of the relationships in the RO in myExperiment.
Research Object have been defined in myExperiment.each part of the workflow to ensure a clear interpretation
and to ensure that scientists know how to replay the
workflows using the same input data, or re-run them
with their own data. We provided textual descriptions
in Taverna of each step of the workflow, in particular to
indicate their purpose within the workflow (Figure 8);
4) the input data (roterms:exampleValue) that we aggre-
gated in our RO was a list of example SNPs derived from
the chosen GWAS [28]; 5) the workflow run provenance
(roterms:WorkflowRunBundle): a ZIP archive that contains
the intermediate values of the workflow run, together with
its provenance trace expressed using wfprov:WorkflowRun
and subsequent terms from the wfprov ontology. We thusThe relationships between example inputs and workflows in the
Figure 6 Taverna workflow diagram for the KEGG workflow. Blue boxes are workflow inputs, brown boxes are scripts, grey boxes are
constant values, green boxes are Web services, purple boxes are Taverna internal services, and pink boxes are nested workflows.
Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 9 of 16
http://www.jbiomedsem.com/content/5/1/41stored process information from the input of the workflow
execution to its output results, including the information
for each constituent process run in the workflow run,
modelled as wfprov:ProcessRun. The run data is: 3 zip
files containing 2090 intermediate values as separate
files totalling 9.7 MiB, in addition to 5 MiB of provenance
traces; 6) the results (roterms:Result) were compiled from
the different workflow outputs to one results file (see
result document in the RO [49] Additional file 1). For
15 SNPs it lists the associated gene name, the biological
annotation from the GWAS publication, the associated
KEGG pathway, and the most strongly associatedFigure 7 Taverna workflow diagram for the concept profile mining w
internal services, and pink boxes are nested workflows.biological process according to concept profile match-
ing. Our workflows were able to compute a biological
annotation from KEGG for 10 out of 15 SNPs and 15
from mining PubMed. All KEGG annotations and most
text mining annotations corresponded to the annotations
by Illig et al [28]. An important result of the text mining
workflow was the SNP-annotation rs7156144- stimu-
lation of tumor necrosis factor production, which rep-
resents a hypothetical relation that to our knowledge was
not reported before; 7) the conclusions (roterms: Conclu-
sion): we concluded that our KEGG and text mining work-
flows were successful in retrieving biological annotationsorkflow. Blue boxes are workflow inputs, purple boxes are Taverna
Figure 8 Taverna workflow annotation example. An example of an annotation of the purpose of a nested workflow in Taverna.
Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 10 of 16
http://www.jbiomedsem.com/content/5/1/41for significant SNPs from a GWAS experiment, and pre-
dicting novel annotations.
As an example of our instantiated RO, Figure 9 provides
a simplified view of the RDF graph that aggregates and
annotates the KEGG mining workflow. It shows the result
of uploading our Taverna workflow to myExperiment, as
it initiated an automatic transformation from a Taverna 2Figure 9 Simplified diagram showing part of the Research Object for
aggregated by the Research Object-enabled version of myExperiment. Sh
KEGG pathway mining workflow.t2flow file to a Taverna 3 workflow bundle, while extract-
ing the workflow structure and user descriptions in terms
of the wfdesc model [41]. The resulting RDF document
was aggregated in the RO and used as the annotation body
of a ao:Annotation on the workflow, thus creating a link
between the aggregated workflow file and its description
in RDF. The Annotation Ontology uses named graphs forour experiment. The Research Object contains the items that were
own is the part of the RDF graph that aggregates and annotates the
Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 11 of 16
http://www.jbiomedsem.com/content/5/1/41semantic annotation bodies. In the downloadable ZIP
archive of an RO each named graph is available as a separ-
ate RDF document, which can be useful in current RDF
triple stores that do not yet fully support named graphs.
The other workflows were aggregated and annotated in
the same way. The RO model further uses common
Dublin Core vocabulary terms [50] for basic metadata
such as creator, title, and description.
In some cases we manually inserted specified relations
between the RO resources via the myExperiment user
interface. An example is the link between input data and
the appropriate workflow for cases when an RO has
multiple workflows and multiple example inputs. In our
case, both workflows have the same inputs, but they need
to be configured in different ways. This was described in
the workflow description field in Taverna which becomes
available from an annotation body in the workflow upload
process.
Checking for completeness of an RO: application of the
Minim model
We also applied Semantic Web technology for checking
the completeness of our RO. We implemented a checklist
for the items that we consider essential or desirable for
understanding a workflow-based experiment by annotat-
ing the corresponding parts of the RO model with the
appropriate term from the Minim vocabulary (Table 1).
Thus, some parts were annotated as MUST have with
the property minim:hasMustRequirement (e.g. at least one
workflow definition), and others as SHOULD have with
the property minim:hasShouldRequirement (e.g. the over-
all sketch of the experiment). The complete checklist
document can be found online in RDF format [51] and in
a format based on the spreadsheet description of the
workflow [52]. We subsequently used a checklist service
that evaluates if an RO is complete by executing SPARQL
queries on the Minim mappings. The overall result is a
summary of the requirement levels associated with theTable 1 RO items checklist
Research object item Requirement RO ontology term
Hypothesis or Research
question
Should roterms: Hypothesis/roterms:
Research Question
Workflow sketch Should roterms:Sketch
One or more workflows Must wfdesc:Workflow
Web services of
the workflow
Must wfdesc:Process
Example input data Must roterms:exampleValue
Provenance of
workflow runs
Must wfprov:WorkflowRun
Example results Must roterms:Result
Conclusions Must roterms:Conclusion
RO items for a workflow-based experiment annotated with the appropriate
term from the Minim vocabulary.individual items; e.g. a missing MUST requirement is a
more serious omission than a missing SHOULD (or
COULD) requirement. We justified the less strict require-
ments for some items to accommodate cases when an RO
is used to publish a method as such. We found that treat-
ing the requirement levels as mutually exclusive (hence
not sub properties) simplifies the implementation of
checklist evaluation, and in particular the generation of
results when a checklist item is not satisfied.
Discussion
In this paper we explored the application of the Semantic
Web encoded RO model to provide a container data
model for preserving sufficient information for researchers
to understand a computational experiment. We found
that the model indeed allowed us to aggregate the neces-
sary material together with sufficient annotation (both for
machines and humans). Moreover, mapping of selected
RO model artefacts to the Minim vocabulary allowed
us to check if the RO was complete according to our
own predefined criteria. The checklist service can be
configured to accommodate different criteria. Research
groups may have different views on what is essential,
but also libraries or publishers may define their own
standards, enabling partial automation of the process of
checking a submission against specific instructions to
authors. Furthermore, the service can be run routinely
to check for workflow decay, in particular decay related
JOURNAL OF
BIOMEDICAL SEMANTICS
Dietze et al. Journal of Biomedical Semantics 2014, 5:48
http://www.jbiomedsem.com/content/5/1/48
SOFTWARE Open Access
TermGenie  a web-application for
pattern-based ontology class generation
Heiko Dietze1*, Tanya Z Berardini2, Rebecca E Foulger3, David P Hill4, Jane Lomax3,
David Osumi-Sutherland3, Paola Roncaglia2 and Christopher J Mungall1
Abstract
Background: Biological ontologies are continually growing and improving from requests for new classes (terms) by
biocurators. These ontology requests can frequently create bottlenecks in the biocuration process, as ontology
developers struggle to keep up, while manually processing these requests and create classes.
Results: TermGenie allows biocurators to generate new classes based on formally specified design patterns or
templates. The system is web-based and can be accessed by any authorized curator through a web browser.
Automated rules and reasoning engines are used to ensure validity, uniqueness and relationship to pre-existing classes.
In the last 4 years the Gene Ontology TermGenie generated 4715 new classes, about 51.4% of all new classes created.
The immediate generation of permanent identifiers proved not to be an issue with only 70 (1.4%) obsoleted classes.
Conclusion: TermGenie is a web-based class-generation system that complements traditional ontology
development tools. All classes added through pre-defined templates are guaranteed to have OWL equivalence
axioms that are used for automatic classification and in some cases inter-ontology linkage. At the same time, the
system is simple and intuitive and can be used by most biocurators without extensive training.
Keywords: Ontology, Class generation
Background
Biological ontologies such as the GeneOntology (GO) and
the Human Phenotype Ontology (HP) provide a rich set of
constructs for describing biological entities such as genes,
alleles and diseases. Like most data resources, ontologies
are rarely complete, and healthy ontologies are continually
growing and improving, as the state of knowledge pro-
gresses. One process by which ontologies grow is from
requests for new classes (terms) by biocurators. These
ontology requests can frequently create bottlenecks in the
biocuration process, as ontology developers struggle to
keep up with a deluge of requests.
Historically the process used in projects such as the GO
Consortium would be for ontology developers to work
through a set of requests collected in an issue tracking
system, and to manually add them to the ontology, using
a specialized Ontology Development Tool (ODT) such
*Correspondence: hdietze@lbl.gov
1Genomics Division, Lawrence Berkeley National Laboratory, 1 Cyclotron
Road, 94720 Berkeley, CA, USA
Full list of author information is available at the end of the article
as OBO-Edit [1]  see Figure 1. Sometimes the ontology
developers apply documented design patterns to guide
this process, particularly where collections of classes fol-
low a common structure. For example, most classes in the
developmental process portion of the GO follow a consis-
tent lexical form and relational structure as dictated in the
GO developers documentation [2]. However, even with
this documentation in place, this has still largely been a
time-consuming and error-prone manual process, espe-
cially where ontology developers need to rearrange to the
ontology structure.
Use of the Web Ontology Language (OWL), and in
particular providing computable definitions in the form
of equivalence axioms can greatly assist in ontology
development and maintenance through the use of OWL
reasoners. However, reasoners do not in themselves
help with the task of class generation. Furthermore,
for many biological ontologies, the axioms necessary
for reasoning have been added post-hoc [3,4] rather
than prospectively at the time of class creation. This
kind of retrospective axiomatization is inefficient but
© 2014 Dietze et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction
in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 2 of 13
http://www.jbiomedsem.com/content/5/1/48
Figure 1 Conventional ontology class request workflow. General workflow for ontology class requests using a traditional issue tracker. A simple
class request may take several days, for complex cases even longer.
has in part been dictated by limitations of OBO-Edit.
This can be partly circumvented by using an ODT
that fully supports OWL such as Protégé, but this tool
can be difficult for biocurators to use, and even in the
hands of experts it can be time consuming to generate
new classes complete with axioms referencing external
ontologies.
Here we describe an application called TermGenie
that allows biocurators to generate new classes based on
formally specified design patterns or templates. The sys-
tem is web-based and can be accessed by any authorized
curator through a web browser. Automated rules and
reasoning engines are used to ensure validity, unique-
ness and relationship to pre-existing classes. The system
makes extensive use of OWL axioms, but can be easily
used without understanding these axioms. TermGe-
nie is used extensively in the GO and is currently also
in use for the Cell Type Ontology and for phenotype
ontologies.
Implementation
To minimize the entrance barrier for biocurators and
non-experts, we provide TermGenie as a web applica-
tion. The only requirement is a JavaScript enabled web
browser. There are separate interfaces for separate tasks in
TermGenie, one for class requests, and another for
request review by ontology developers.
Architectural components
The TermGenie application is based on client-server
architecture. The web client uses two JavaScript libraries
(jQuery [5] and jQuery UI [6]) to implement the user
interface in the web browser. The server is written in Java
and accepts JSON messages in AJAX RPC calls from the
client via a Java servlet listener. Figure 2 illustrates the
required TermGenie components and the general work-
flow for ontology class generation.
The TermGenie server uses a set of modules and com-
ponents to provide the required services for TermGenie.
Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 3 of 13
http://www.jbiomedsem.com/content/5/1/48
Figure 2 Overview of TermGenie Components andWorkflow. (1) Retrieve existing templates for user selection; (2) Term generation processing
and validation; (2a) Generate textual data and OWL axioms; (2b) Use reasoning to check for existing classes and new or changed relations; (3)
Review of generated classes by the user in the web interface; (4) After review, assign permanent identifiers to the new classes; (5) Add the new
classes into the queue for review; (6) Senior ontology developers review the classes: accept, modify, obsolete; (7) Commit the changes to the
ontology; (8) Send confirmation e-mail to the user.
This set includes modules for basic functionalities such
as loading ontologies, a persistence layer, reasoning, iden-
tifier generation, access to version control systems, and
sending e-mails. Some modules are used for more com-
plex components, such as the term generation, request
submission, and review interface.
Ontology loading and synchronization
For ontology loading and the in-memory model,
TermGenie relies on the OWL-API [7]. This Java library
provides an axiom-based ontology model with parsers
and writers for many OWL serialization formats. In
addition, we use a Java implementation for handling
OBO format [8], which also executes the conversion to
the OWL in-memory model from OBO format. Future
versions of the OWL-API will integrate the OBO format
library, removing the need for this extra step. An impor-
tant TermGenie feature is the support of ontology file
handling in a version control systems (VCS). Currently
TermGenie supports Subversion [9] and future support
for Git [10] is planned. In addition, for a more efficient
load of imports and files not in a VCS, TermGenie uses
a local file cache for ontology files. The caching duration
is a configuration parameter of each TermGenie instal-
lation. TermGenie loads the required ontologies during
the server start. To keep the ontologies up-to-date and in
sync with the source file, TermGenie periodically updates
the VCS files and reloads the ontologies.
Request queue and services
As shown in the workflow, TermGenie saves the requested
classes for review in a queue. This request queue is
separate from the ontology file and requires a separate
persistent storage module. The persistence module is
implemented via the Java Persistence API using Open-
JPA [11] as object-relational mapper and HSQLDB [12]
as a simple embedded database for storage on disk. This
lightweight default implementation makes TermGenie
independent from more complex database setups and
configuration issues. Because TermGenie does not push
the requests to the ontology until they are reviewed,
TermGenie provides additional services to access infor-
mation about the pending request. Option one, there
is a separate TermGenie page, which list the currently
Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 4 of 13
http://www.jbiomedsem.com/content/5/1/48
pending and recently approved requests. This table is
intended for users to quickly check their recent requests.
Option two, there is a web service to query the status of
requested class and whether it is an approved, pending,
or unknown class identifier. This service is intended for
the integration of TermGenie in curation tools. Currently
Protein2GO [13] uses the service to verify the class iden-
tifiers and prevent curators from entering invalid identi-
fiers, while still allowing the immediate usage of newly
generated classes.
Sessions and user authentication
TermGenie uses Java servlets mainly as abstraction layer,
but we make use of the built-in session handling mech-
anism. The session is used to store the relevant tokens
for the authentication of users. For the authentica-
tion, TermGenie currently relies on Persona [14] as a
lightweight service. Persona is a 3rd-party (non-profit
and open source) protocol and service, which uses an
e-mail address as primary identifier. It provides a conve-
nient JavaScript client library and easy server-side calls for
token verification. Once a TermGenie session has been
authenticated, the authorization module uses the e-mail
address as primary identifier to check whether the user
has the appropriate permissions for the requested opera-
tion. TermGenie has different sets of permissions depend-
ing on the tasks. For example, the submission of classes
requires a different set of permissions than the TermGenie
management console for administrators.
Logic-based autocompletion
An important convenience feature for TermGenie users
is autocompletion. TermGenie uses a Lucene in-memory
index to provide appropriate suggestions. To optimize the
suggested classes and restrict the classes for a template,
TermGenie can be configured to use only a subset of
all available classes. For example, to create a subset for
the molecular functions in GO, the configured set just
contains the root class GO:0003674 (molecular_function).
Using a reasoner, this set is then extended to include all
direct and indirect subclasses. The same configuration
mechanism can also be used to allow the input of classes
from multiple ontologies in an input field. For example,
it is possible to use cell-type classes form the Cell Type
Ontology and plant cell classes from the Plant Ontology.
Configuration
All the different TermGenie components and modules
are configured and combined via Google Guice [15], a
lightweight dependency injection framework. TermGenie
uses a combination of Java-based and compiler-
checked defaults, configuration property files, and
optional command-line overrides to configure a specific
TermGenie installation. For example, the Guice modules
are part of the Java configuration, creating a generic web
application. The machine-specific details and secrets (e.g.
passwords and private keys) are provided as a property
file to override the default parameters. The location of the
property file is declared via a command-line parameter.
This helps to avoid the problem of accidental release of
sensitive information into a public version control system.
Templating system
The core of TermGenie is template-based class gener-
ation. The template-based approach allows the separa-
tion of ontology design tasks, a fairly involved process,
and standard class generation, a relatively straightforward
task. For the former, the ontology developers extract or
create, and test appropriate patterns for the generation of
new classes based on the design principles of the ontology.
A template consists of the OWL equivalent class axiom
for the formal definition and reasoning, label and textual
definition building blocks and, if applicable, details for
synonym generation. These templates can then be used
by biocurators to generate desired standard classes with-
out need for knowledge of the internal workings of the
ontology.
In TermGenie each template is specified as a separate
JavaScript function and file. During the generation the
JavaScript code is executed by a Java-embedded JavaScript
engine. The embedded approach allows the use of native
Java objects and functions, such as the ontology model
and reasoner checks, in Javascript calls without the need
for conversions. The Java layer also provides a set of func-
tions intended to be used in the JavaScript code. These are
shortcut functions for common tasks, such as the retrieval
of a label for a given class. With these helpers it is possi-
ble to create fairly compact JavaScript code for a template.
This approach does not preclude the application to more
complex operations and checks. Most of the validation,
such as the search for existing classes and the inference
of relations, is done in Java using standard OWL-API
reasoners.
Every template has an associated XML-based configu-
ration file. Amongst others, this configuration specifies
the required and optional input fields, including details
on the relevant ontology subsets for the appropriate auto-
complete suggestions. For an example of a template with
its XML configuration, JavaScript code and resulting input
fields in TermGenie, see Figure 3. It should also be noted
that this particular example template is configured to
require exactly one ontology class as input. Other tem-
plates can use up to three different input classes in the
equivalent class axioms for a generated class.
Reasoning
TermGenie uses reasoning for two tasks: validation and
relation inference. Both tasks rely on the equivalent class
Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 5 of 13
http://www.jbiomedsem.com/content/5/1/48
Figure 3 Example template and configuration for TermGenie. (top-left) XML-based example template configuration for the Gene Ontology
template chemical_export. Includes declarations for required and optional input fields and corresponding JavaScript file; (top-right) Javascript
snippet from the JavaScript file. for generating a class and OWL axioms; (bottom) Screenshot of the generated TermGenie input fields. Also shows
autocompletion on ChEBI classes.
axioms specified in the templates. For the validation,
TermGenie asks the reasoner for equivalent named classes
for the given hypothetical new class. Similarly, for the
inference and update of relations, we query the reasoner
for the direct super- and subclasses of the hypothetical
class. This is done by declaring a new class using a new
temporary identifier and adding the corresponding equiv-
alent class axioms. Next, TermGenie creates an up-to-date
reasoner instance for the changed ontology. To prevent
unpredictable inferences, the ontology is checked for
inconsistency and unsatisfiable classes. Once these checks
are completed, the actual new-class-related queries are
done. After querying, the axiom changes are reverted and
the reasoner is discarded. The inferred direct subclasses
are used to assert the most specific superclasses. In addi-
tion the direct subclasses of the hypothetical new class
are checked and their relations updated. This strategy
allows the creation of not only the new leaf classes in the
ontology graph, but also new intermediate classes with
an automatic update of relations for existing classes. An
example of an inference using equivalent class axioms and
a reference ontology is available in Figure 4.
Using this workflow the reasoner creation and querying
are the most time-consuming steps of a TermGenie tem-
plate request. In theory, TermGenie can use any OWL-
API compliant reasoner, but the requirements for an
interactive web-application introduce a processing time
limit for the reasoner. We have experimented with multi-
ple reasoners and chose ELK [16] as the most convenient
compromise for TermGenie.
User workflow
In a typical workflow, the user begins by loading the rel-
evant TermGenie web page, selecting and filling in the
relevant template. After the class generation and valida-
tion, the class is submitted for user review and approval
and assignment of a permanent identifier, see also Figure 5
for a workflow diagram.
On the user side, a TermGenie template consists of a
set of required and optional input fields. TermGenie pro-
vides autocompletion for ontology classes for appropriate
input fields. Before a user starts the new class generation
and validation, a number of quick checks are executed.
The checks include one for missing entries in required
input fields, such as a missing literature reference. After
passing these checks, the request is sent to the server.
After the generation and reasoning step on the server,
the users have the chance to review the proposed classes.
They can also make modifications to textual parts if nec-
essary (e.g. definition) or add additional synonyms. The
next step is the submission of the generated classes for
review. As part of this process, a new permanent identifier
is generated using a customizable identifier pattern and
range. To complete the submission step, the user must be
Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 6 of 13
http://www.jbiomedsem.com/content/5/1/48
Figure 4 Inferences for a class using a standard OWL reasoner. Reasoning example for a genus + differentia pattern for camptothecin
catabolism in the GeneOntology. The class is defined by its genus catabolic process (GO:0009045) and differentia has_input camptothecin
(CHEBI:27656). Following that definition, the class is a subclass of catabolic process. Using the additional axioms from ChEBI and the GeneOntology,
a standard OWL reasoner can infer the more specific superclass alkaloid catabolic process (GO:0009822).
logged in (authenticated), as the server will check for the
appropriate permissions and will use the user metadata
for provenance information of the generated classes and
requested e-mail notifications.
Ideally, after generating the identifier, a biocurator can
immediately use the generated identifiers for annotation.
To facilitate this even while the identifier is not yet com-
mitted to the ontology, we provide a web service to check
the validity of class identifiers.
Review process
After a user has submitted their generated class requests
and generated the permanent identifiers, the requests are
put into a queue for review by an ontology developer.
During the review the ontology developer has the follow-
ing three choices: approve, modify, or obsolete. There is
no reject or delete option at that stage because a per-
manent identifier has already been generated. In most
cases the classes can be approved without (or with min-
imal) modifications since they rely on tested templates.
Should a developer need more details, he/she may con-
tact the original requester without making a decision, and
keep the request pending. The ontology developers can
use the e-mail information available from the provenance
information of each request.
Once the ontology developer has determined which
classes to commit to the ontology, he/she selects the
corresponding checkboxes and initiates the commit. On
the server, the classes from the requests are first quickly
checked again. For the commit, the server uses the version
control to create a clean checkout. From there TermGenie
loads the ontology as a separate instance and applies
the relevant changes. Depending on the original ontol-
ogy file format this can either be OWL axioms or OBO
term frames. After writing the changed ontology as a file,
TermGenie tries to commit the updated file into the VCS.
When there is more than one request selected for commit
to the ontology, TermGenie processes each one separately
with individual commits. This allows for a more fine-
grained and aspect-oriented tracking of changes in the
underlying VCS. See also Figure 6 for the workflow during
the review process.
Results and discussion
Usage in the Gene Ontology The pattern-based Ter-
mGenie approach has been used for the Gene Ontology
since July 2010, with the current Java implementation
in place since November 2012. During the period from
July 2010 until the end of June 2014, the Gene Ontology
instance of TermGenie has been used to generate 4715
classes, which represents 51.4% of all new classes created
in GO during that time. For a quarterly report of new
classes in GO see Table 1. The number of available tem-
plates has been growing over time and currently stands at
38 templates, see also Table 2 for a list of the templates.
Many of these templates utilize an external ontology.
As described before, TermGenie relies heavily on rea-
soning for automatic classification and validation. This
requires that the ontology underlying a TermGenie
instance be sufficiently axiomatized with equivalent class
axioms. In the case of the Gene Ontology, with its consid-
erable size and development history, a significant amount
of time and effort was needed to introduce equivalent
class axioms into the ontology. The formalization of GO
Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 7 of 13
http://www.jbiomedsem.com/content/5/1/48
Figure 5 TermGenie user workflow. To create a class in TermGenie, Biocurators go to the TermGenie website and select the relevant template for
their request. The template consists of a set of required and optional input fields. TermGenie provides autocompletion for appropriate input fields.
After passing some quick checks, the request is sent to the server, where generation and reasoning are executed. The results are send back and the
users have the chance to review the proposed classes. The next step is the submission of the generated classes for review. As part of this process, a
new permanent identifier is generated using a customizable identifier pattern and range. Furthermore, the request is added to the review queue for
final approval by the ontology developers.
started in the early 2000s [17] and is still an ongoing task.
It not only includes intra-ontology definitions [18], but
also makes use of existing other domain-specific ontolo-
gies, such as the Chemical Entities of Biological Interest
(ChEBI) ontology [19]. The most frequently referenced
external ontology is ChEBI, but we also use the Plant
Ontology (PO), Cell Type Ontology, Phenotypic Quality
Ontology (PATO), and Uberon [4] to define class pat-
terns in GO. One could argue that ontology formalization
is critical in creating a scalable and affordable long-
term maintenance strategy because it supports automatic
inferences and reasoning. The template-based formaliza-
tion process helps to make implicit design patterns and
assumptions explicit.
Streamlining ontology development The template-
based approach allows the separation of concerns and
roles between ontology engineering and everyday ontol-
ogy class requests. Most of the ontology work for creating
a template can be done by the ontology developers and
OWL experts during the design and test phase for each
of the templates. Once a pattern has been created and is
available in a TermGenie web application, adding a new
class in that same pattern is vastly streamlined. The biocu-
rators can quickly and safely create classes and permanent
identifiers on the website within minutes and return to
their annotation task. The effort for the final review by an
ontology developer for each class in TermGenie is min-
imal as it relies on a pre-existing and tested solution.
TermGenie also provides the convenient feature of e-mail
notifications.
Bounds on complexity of composed classes Even
though templates are usually tested and approved by
the ontology developers, one interesting issue for Gene
Ontology requests has come up. Some templates generate
classes of the same category as the input class (e.g. pro-
cess involved_in process, or regulation of processes). This
Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 8 of 13
http://www.jbiomedsem.com/content/5/1/48
Figure 6 TermGenie workflow during a submitted class review by an ontology developer. After a user has submitted their generated class
requests, the requests are put into a queue for review by an ontology developer. During the review the ontology developer has the following three
choices: approve, modify, or obsolete. For the commit, the server uses the version control adapter to create a clean checkout. From there TermGenie
loads the ontology as a separate instance and applies the relevant changes. After writing the changed ontology as a file, TermGenie tries to commit
the updated file into the version control. After a successful commit the queue is updated and a confirmation e-mail is sent to the requester.
means that it is possible to recursively compose classes
with definitions that unfold to a deeply nested hierarchy,
with complex textual definitions and labels that impose
a cognitive burden on users. Most of these classes are
requested for the annotation of complex biological pro-
cesses and functions with a pre-composition strategy or
legacy systems with a simplistic annotation model (e.g.
single unrelated annotations). From the formal point of
view these classes have a clear axiomatized definition and
can be unfolded into simpler annotations [20]. This kind
of class request, although not very common in TermGe-
nie, take longer to review as they often require further
discussion andmodifications by ontology developers. One
proposal has been to design a strategy to prevent the cre-
ation of these multiply compounded classes and to instead
redirect users to the issue tracker instead of proceeding
with the request. The detection and redirection feature
has not yet been implemented.
The most time-saving feature for biocurators is the
immediate creation of permanent identifiers. Therefore,
during the review by an ontology developer, this leaves
only obsoletion as a way to reject a request. In theory
this could lead to higher number of unnecessary obsoleted
classes. However, this proved not to be an issue for the
Gene Ontology TermGenie instance. Only 70 requested
classes have been obsoleted since inception, about 1.6% of
the TermGenie requests.
Non-templated class generation Biology and other
complex subjects cannot always be axiomatized in a tem-
platable way. Therefore, not all class requests can or
should be done using a template. To address this issue and
at the request of the ontology developers, we added a free-
form option to TermGenie. This allows very experienced
users to quickly specify all the relevant details of a class,
validate, and generate the new class using TermGenie. The
Table 1 TermGenie generated class counts in GO over time
Quarter 2010-III 2010-IV 2011-I 2011-II 2011-III 2011-IV 2012-I 2012-II
TermGenie 139 154 236 254 307 175 255 806
Manual 575 413 332 295 313 364 462 324
Fraction 19.47% 27.16% 41.55% 46.27% 49.52% 32.47% 35.56% 71.33%
Quarter 2012-III 2012-IV 2013-I 2013-II 2013-III 2013-IV 2014-I 2014-II Total
TermGenie 303 352 357 285 218 231 301 342 4715
Manual 371 283 62 92 170 164 109 110 4439
Fraction 44.96% 55.43% 85.20% 75.60% 56.19% 58.48% 73.41% 75.66% 51.51%
Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 9 of 13
http://www.jbiomedsem.com/content/5/1/48
Table 2 Available templates for the geneontology termgenie instance
Template Input fields Equivalent class statement
regulation: biological process
regulation X:BP GO:0065007 and regulates some ?X
negative_regulation X:BP GO:0065007 and negatively regulates some ?X
positive_regulation X:BP GO:0065007 and positively regulates some ?X
regulation: molecular function
regulation X:MF GO:0065007 and regulates some ?X
negative_regulation X:MF GO:0065007 and negatively regulates some ?X
positive_regulation X:MF GO:0065007 and positively regulates some ?X
involved_in P:BP, W:BP ?P and part_of some ?W
involved_in_mf_bp P:MF, W:BP ?P and part_of some ?W
occurs_in P:BP, C:CC ?P and occurs in some ?C
regulation_by R:GO:0050789, P:BP ?R and results_in some ?P
part_of_cell_component P:CC, W: CC ?P and part_of some ?W
chemical_transport X:chebi GO:0006810 and transports or maintains localization of some ?X
chemical_transporter_activity X:chebi GO:0005215 and transports or maintains localization of some ?X
chemical_binding X:chebi GO:0005488 and has input some ?X
metabolism_catabolism_biosynthesis
metabolism X:chebi GO:0008152 and has participant some ?X
catabolism X:chebi GO:0009056 and has input some ?X
biosynthesis X:chebi GO:0009058 and has output some ?X
chemical_transmembrane_transport X:chebi GO:0055085 and transports or maintains localization of some ?X
chemical_transmembrane_transporter_activity
transmembrane transporter activity X:chebi GO:0022857 and transports or maintains localization of some ?X
secondary active transmembrane transporter activity X:chebi GO:0015291 and transports or maintains localization of some ?X
uptake transmembrane transporter activity X:chebi GO:0015563 and transports or maintains localization of some ?X
transmembrane-transporting ATPase activity X:chebi GO:0042626 and transports or maintains localization of some ?X
chemical_response_to
response to X:chebi GO:0050896 and has input some ?X
cellular response to X:chebi GO:0070887 and has input some ?X
chemical_homeostasis
chemical homeostasis X:chebi GO:0048878 and regulates level of some ?X
cellular chemical homeostasis X:chebi GO:0055082 and regulates level of some ?X
chemical_import X:chebi GO:0006810 and imports some ?X
chemical_export X:chebi GO:0006810 and exports some ?X
chemical_import_into S:chebi, T:CC GO:0006810 and has target end location some ?T and imports some ?S
cc_transport_from_to
transport F:CC, T:CC GO:0006810 and has target start location some ?F and has target end
location some ?T
vesicle-mediated transport F:CC, T:CC GO:0016192 and has target start location some ?F and has target end
location some ?T
cc_transport
transport C:CC GO:0006810 and transports or maintains localization of some ?C
vesicle-mediated transport C:CC GO:0016192 and transports or maintains localization of some ?C
Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 10 of 13
http://www.jbiomedsem.com/content/5/1/48
Table 2 Available templates for the geneontology termgenie instance (Continued)
chemical_transport_from_to
transport X:chebi, [F:CC], [T:CC] GO:0006810 and transports ormaintains localization of some ?X [and has
target start location some ?F] [and has target end location some ?T]
vesicle-mediated transport X:chebi, [F:CC], [T:CC] GO:0016192 and transports ormaintains localization of some ?X [and has
target start location some ?F] [and has target end location some ?T]
cc_assembly_disassembly
assembly C:CC GO:0022607 and results_in_assembly_of some ?C
disassembly C:CC GO:0022411 and results_in_disassembly_of some ?C
plant_development P:plant anatomical structure development and results in development of some
?P
plant_formation X:plant anatomical structure formation involved inmorphogenesis and results in
formation of some ?X
plant_maturation X:plant developmental maturation and results in developmental progression of
some ?X
plant_morphogenesis X:plant anatomical structure morphogenesis and results in morphogenesis of
some ?X
plant_structural_organization X:plant anatomical structure arrangement and results in structural organization
of some ?X
cell_apoptotic_process C:cell cell-type specific apoptotic process and occurs in some ?C
cell_differentiation C:cell GO:0030154 and results in acquisition of features of some ?C
cell_migration C:cell cell migration and alters location of some ?C
protein_localization_to
protein localization C:CC GO:0008104 and has target end location some ?C
establishment of protein localization C:CC GO:0045184 and has target end location some ?C
protein_complex_by_activity A:MF GO:0043234 and capable_of some ?A
single_multi_organism_process
single-organism P:BP ?P and bearer of some PATO:0002487
multi-organism P:BP ?P and bearer of some PATO:0002486
biosynthesis_from T:chebi, F:chebi GO:0009058 and has output some ?T and has input some ?F
biosynthesis_via T:chebi, V:chebi GO:0009058 and has output some ?T and has intermediate some ?V
catabolism_to S:chebi, R:chebi GO:0009056 and has input some ?S and has output some ?T
catabolism_via X:chebi, V:chebi GO:0009056 and has input some ?X and has intermediate some ?V
metazoan_development X:Uberon anatomical structure development and results in development of some
?X
The first column contains the template names and available templates variations. The second column lists the expected ontology inputs for the equivalent class
statement in the third column, with BP = GO:biological_process, MF = GO:molecular_function, CC = GO:cellular_component, chebi= chemical entity (CHEBI:24431),
plant = plant anatomical entity (PO:0025131), cell = native cell (CL:0000003), Uberon = anatomical entity (UBERON:0001062).
free-from workflow extends to the existing validation pro-
cedures with additional checks. It searches for and warns
about existing similar class names and synonyms for a
given class request. For example, a request for omega-
some via free-form, produces a warning that a similar
class megasome already exists. In this case the warning
could be dismissed as the two classes refer to completely
different cell components. This additional check helps the
ontology developers to avoid the creation of redundant
classes.
Due to the different use-case, this free-form template is
implemented as a separate tool in the TermGenie webapp,
but shares many services (e.g., autocomplete, e-mail noti-
fications) and adds requests to the common review queue.
Furthermore, we use a different set of permissions to
restrict the access of users to this template. Due to the
more experimental nature of the requests via the free-
form template, the obsoletion rate is slightly higher, with
16 of 387 (4.1%) obsoleted requests.
Evaluation of OWL reasoners for use in TermGenie
Because reasoning is a core task in TermGenie, we
experimented with multiple OWL-API compliant reason-
ers. Currently, we haven chosen ELK [16] as the best
Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 11 of 13
http://www.jbiomedsem.com/content/5/1/48
compromise for TermGenie. ELK is an OWL 2 EL pro-
file [21] reasoner and provides a good trade off between
response time and supported inference. Other tested
reasoners include HermiT [22], JFact [23], Pellet [24],
MORe [25] as full OWL compliant reasoners and jcel [26]
as another OWL 2 EL compliant reasoner. In general all
full OWL2 reasoners proved to be too slow for usage in
TermGenie. The other EL reasoner, jcel, is a viable alterna-
tive, but ELK using multithreading out-performed jcel in
the initial classification step. A typical reasoning task for
the Gene Ontology and the required external ontologies
includes about 415,000 logical axioms. Using ELK, we can
respond to a single request within a few seconds.
TermGenie for other ontologies The TermGenie sys-
tem was designed from the outset to be ontology-neutral.
In addition to the Gene Ontology instance [27], we have
worked with the developers of other OBO Library ontolo-
gies to create custom TermGenie instances.
The OBO Cell Type Ontology (CL) [28] represents cell
types found in animals. One of the main uses of the CL
is to rigorously describe samples collected as part of large
next-generation sequencing projects such as Functional
Annotation of Mammalian Genomes 5 (FANTOM5) and
the Encyclopaedia of DNA Elements (ENCODE), allow-
ing analyses that yield insight into properties of different
cell types [29]. The ENCODE curators have found the
CL instance of TermGenie useful as it provides a simple
web-based way to generate new classes used to describe
samples.
The Ontology of Biological Attributes (OBA) [30] was
created as a unified representation of traits (for exam-
ple eye color) encompassing ontologies for describing
animals, plants and single-celled organisms. Many traits
follow a trivial compositional pattern, encompassing a
simple entity-attribute pattern, with the attribute being
taken from the attribute subset of PATO, and the entity
taken from ontologies such as Uberon or PO. This ontol-
ogy was originally created to be able to structure the
regulation of biological quality branch of the GO, but it
has found uses in other areas. Curators in the Monarch
Initiative project have used it to describe mouse strain
phenotypes, andmost recently it has incorporated into the
Encyclopaedia of Life (EOL) TraitBank [31] project.
Ontologies of abnormal or variant phenotypes also
benefit from a templated approach, as their classes can
often be described using an Entity-Quality combinatorial
approach, akin to that used inOBA. So far we have created
instances for the Mammalian Phenotype Ontology [32]
and the Human Phenotype Ontology (HP) [33], with plans
to create instances for other species-specific phenotype
ontologies. The HP instance was created in part to serve
the needs of the NIH Undiagnosed Diseases Program
(UDP), which is systematically describing the phenotypes
of patients with undiagnosed diseases, so that phenotype
comparison algorithms can be used to assist the hunt for
the genomic underpinnings of these diseases. In this case,
it is important that a diversity of ontology contributors can
efficiently and effectively contribute to the HP. We believe
that TermGenie will greatly facilitate contributions from
the rare disease community.
Note that in order for these instances to work effec-
tively, it was first necessary for the respective developers
to make their ontologies reasoner-ready by providing
OWL equivalent class axioms, a process that has been
underway for several years [3,34]. In contrast, once the
necessary OWL refactoring is complete, the configura-
tion of the ontology-specific TermGenie instances takes
about a week, with most of the time spent on testing the
templates.
Comparison with other approaches
Creating new classes in ontologies is a common task, one
that is typically done by a developer using an Ontology
Development Tool (ODT) such as OBO-Edit [1] or Pro-
tégé [35]. These are both comprehensive, general purpose
environments, and are not intended for use by annotators
and biocurators without requisite training. In addition,
both are desktop applications, requiring an installation on
the users machine. The limitations of desktop ontology
development software, especially for collaborative work,
led to the creation of WebProtégé, a web-based ontology
development tool [36]. All three applications are pow-
erful tools with steep learning curves and are usually
intended for knowledge/ontology engineers and ontology
developers. They do not offer the separation of design
and quick everyday use for non-experts. TermGenie is not
intended to replace comprehensive ODTs; the pattern-
based approach and ODTs complement each other in the
ontology development workflow. In fact the comprehen-
sive ODTs are required during the template design and
testing.
Other related work exists in the form of the Term
Generation plugin DOG4DAG [37]. It is available as an
OBO-Edit and Protégé plugin. The tool allows proposal of
new classes based on phrases extracted from a given text
corpus. The most common use case is to create or add
domain-specific vocabulary to an ontology. The best use is
in early stages of ontology projects as it generates mostly
list of candidate classes. For amoremature and formalized
ontology, a more axiomatized result is required.
The Network-Extracted Ontology (NeXO) [38] is an
example of an orthogonal, data-driven approach to ontol-
ogy generation. This approach takes as input a suffi-
ciently large and dense network (i.e. gene and protein
interactions), and applies a clustering algorithm to gen-
erate classes and relationships between these classes. So
far, NeXO has been used to generate a yeast cellular
Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 12 of 13
http://www.jbiomedsem.com/content/5/1/48
component ontology. It remains to be seen how well the
approach works for other portions of ontologies such as
the GO.
Another template-driven class generation approach is
Quick Term Templates [39]. There are multiple imple-
mentations available for this approach: a MappingMaster
plugin for Protégé, the OntoRat web application [40],
or custom Perl code combined with spreadsheets. Each
implementation still requires quite a bit of detailed knowl-
edge of the ontology. One huge issue is the informa-
tion flow back to the ontology developers, which also
includes the assignment of valid/permanent identifier
and access control. In the TermGenie application these
details are controlled by the server and the built-in review
mechanism.
The Cellular Phenotype Ontology (CPO) is an ontology
that was entirely generated programmatically [41]. A cus-
tom program was written using the java OWL-API to
generate a class from the cross-product of the cellular
process branch of GO and a subset of PATO. This kind of
en-mass class generation is in contrast to the TermGenie
approach, in which biocurators flesh out a subset of the
space of all possible classes on an as-needed basis. The
resulting ontology is more compact and is arguably more
usable than one in which the entire space of terms is
fleshed out in advance.
Most recently, the Tawny-OWL framework provides an
elegant and powerful way to generate an entire ontology
programmatically using a high-level declarative domain-
specific language [42]. At this time, Tawny is difficult to
integrate into a conventional ontology development work-
flow as it requires the source for the ontology to be stored
as a Clojure program rather than in a non-programmatic
format such as OBO or OWL. However, we are working
with the Tawny developers to explore ways to integrate
our approaches.
Conclusion
TermGenie is a web-based class-generation system that
complements traditional ontology development tools. All
classes added through pre-defined templates are guaran-
teed to have OWL equivalence axioms that are used for
automatic classification and in some cases inter-ontology
linkage. At the same time, the system is simple and intu-
itive and can be used by most biocurators without exten-
sive training. Its use in the Gene Ontology has removed a
significant curation bottleneck, and has freed up ontology
developers from performing time-consuming repetitive
tasks allowing them to work on high-level design issues.
In the last 4 years the Gene Ontology TermGenie instance
was used to generated 4715 new classes, about 51.4% of
all new classes created. The immediate generation of per-
manent identifiers proved not to be an issue with only 70
(1.4%) obsoleted classes. TermGenie is now in use in other
projects as well, including the Mammalian Phenotype
Ontology, the Human Phenotype Ontology, the Cell Type
Ontology and the Ontology of Biological Attributes.
Availability and requirements
 Project name: TermGenie
 Project home page: http://termgenie.org
 Operating system(s): Platform independent
 Programming language: Java, JavaScript
 Other requirements: Java 6 or higher, Jetty 6 or
higher, Maven 3.0.x
 License: New BSD (BSD 3 Clause)
 Any restrictions to use by non-academics: none
Abbreviations
GO: GeneOntology; HP: Human Phenotype; ODT: Ontology Development Tool;
OWL: Web Ontology Language; VCS: Version Control System; ChEBI: Chemical
Entities of Biological Interest; PO: Plant Ontology; CL: Cell Type Ontology;
PATO: Phenotypic Quality Ontology; OBA: Ontology of Biological Attributes.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
CJM conceptualized and implemented the initial prototype. HD designed,
implemented, and maintains the current TermGenie system. CJM, TZB, REF,
DPH, JL, DOS, PR provided the ontology expertise, templates, and test
examples, and performed necessary ontology axiomatization and refactoring.
All authors provided feedback, drove requirements and participated in testing
and improving the system. All authors read and approved the final manuscript.
Acknowledgements
We thank all the GO curators who have provided feedback on the GO
TermGenie instance. Thanks to Jen Hammock from the Encyclopaedia of Life
for testing the OBA TermGenie instance.
All authors are supported by the National Human Genome Research Institute
(NHGRI) P41 grant 5P41HG002273-09 to the Gene Ontology Consortium. In
addition, JL is funded by the European Molecular Biology Laboratory (EMBL),
European Bioinformatics Institute Outstation (EMBL-EBI) core funds. In
addition, HD and CJMs contribution was also supported by the Director,
Office of Science, Office of Basic Energy Sciences, of the U.S. Department of
Energy under Contract No. DE-AC02-05CH11231.
Author details
1Genomics Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Road,
94720 Berkeley, CA, USA. 2The Arabidopsis Information Resource, Phoenix
Bioinformatics, 94063 Redwood City, CA, USA. 3European Molecular Biology
Laboratory, European Bioinformatics Institute (EMBL-EBI), CB10 1SD Hinxton,
Cambridge, UK. 4Mouse Genome Informatics, The Jackson Laboratory, 04609
Bar Harbor, ME, USA.
Received: 28 August 2014 Accepted: 29 October 2014
Published: 11 December 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Saleem et al. Journal of Biomedical Semantics 2014, 5:47
http://www.jbiomedsem.com/content/5/1/47
RESEARCH Open Access
TopFed: TCGA tailored federated query
processing and linking to LOD
Muhammad Saleem1*, Shanmukha S Padmanabhuni2, Axel-Cyrille Ngonga Ngomo1, Aftab Iqbal2,
Jonas S Almeida3, Stefan Decker2 and Helena F Deus4
Abstract
Backgroud: The Cancer Genome Atlas (TCGA) is a multidisciplinary, multi-institutional effort to catalogue genetic
mutations responsible for cancer using genome analysis techniques. One of the aims of this project is to create a
comprehensive and open repository of cancer related molecular analysis, to be exploited by bioinformaticians
towards advancing cancer knowledge. However, devising bioinformatics applications to analyse such large
dataset is still challenging, as it often requires downloading large archives and parsing the relevant text files. Therefore,
it is making it difficult to enable virtual data integration in order to collect the critical co-variates necessary for
analysis.
Methods: We address these issues by transforming the TCGA data into the Semantic Web standard Resource
Description Format (RDF), link it to relevant datasets in the Linked Open Data (LOD) cloud and further propose an
efficient data distribution strategy to host the resulting 20.4 billion triples data via several SPARQL endpoints. Having
the TCGA data distributed across multiple SPARQL endpoints, we enable biomedical scientists to query and retrieve
information from these SPARQL endpoints by proposing a TCGA tailored federated SPARQL query processing engine
named TopFed.
Results: We compare TopFed with a well established federation engine FedX in terms of source selection and query
execution time by using 10 different federated SPARQL queries with varying requirements. Our evaluation results
show that TopFed selects on average less than half of the sources (with 100% recall) with query execution time equal
to one third to that of FedX.
Conclusion: With TopFed, we aim to offer biomedical scientists a single-point-of-access through which distributed
TCGA data can be accessed in unison. We believe the proposed system can greatly help researchers in the biomedical
domain to carry out their research effectively with TCGA as the amount and diversity of data exceeds the ability of
local resources to handle its retrieval and parsing.
Keywords: Federated queries, SPARQL, TCGA, RDF
Background
The Cancer Genome Atlas [1] (TCGA) is an effort led
by the National Cancer Institute to characterize and
sequence more than 30 cancer types from 9000 patients
at the molecular level. The goal is to analyse DNA for
every participant to discover abnormalities present in a
tumour sample that are peculiar to the oncogenic pro-
cess and whether it affect progression and regression of
*Correspondence: saleem@informatik.uni-leipzig.de
1Universität Leipzig, IFI/AKSW,PO 100920, D-04009Leipzig,Germany
Full list of author information is available at the end of the article
the tumours. Each cancer type published by TCGA has
three levels. Level 1 is raw data, level 2 is normalized
data, and level 3 is processed data. The analytics are per-
formed on the level 3 data, which is also of our interest
for the work presented in this paper. TCGA is a valu-
able resource for hypothesis-driven translational research
as all of its data results from direct experimental evi-
dence. Analysis of such evidence within cancer research
has led in recent years to clinically relevant findings in the
genetic mark-ups of different cancer types and is at the
© 2014 Saleem et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction
in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 2 of 18
http://www.jbiomedsem.com/content/5/1/47
forefront of a coordinated worldwide effort towards mak-
ing more molecular results from cancer analyses publicly
available [2].
Big data research initiatives such as the International
Cancer Genomics Consortia [3], the 1000genomes [4] and
the One Million Genomes project [5], the $10 Million
Genome Prize [6], and the remarkable drop in the cost of
genome sequencing [7] will soon mean that the current
bioinformatics paradigm in which researchers download
all the data, extract the interesting pieces and remove
the rest, will no longer be feasible [8,9]. The rapid devel-
opment of advanced statistical methods for analysing
cancer genomics [10-12] further emphasizes the need to
enable smooth online data collection and aggregation.
As pointed out in [13], Large-scale genome character-
ization efforts involve the generation and interpretation
of data at an unprecedented scale that has brought into
sharp focus the need for improved information technol-
ogy infrastructure and new computational tools to ren-
der the data suitable for meaningful analysis. A scalable
and robust solution is therefore a critical requirement,
whereby researchers can obtain a subset of big data they
are interested in by executing a query using a particular
service.
In addition to the large semi-structured experimen-
tal results available through TCGA and related projects,
there is a significant number of unstructured and struc-
tured biomedical datasets available on the Web. Most
of these datasets are critical towards annotating and
integrating the experimental results. Remote query pro-
cessing and virtual data integration, i.e., transparent on-
the-fly-view creation for the end user, can provide a
scalable solution to both challenges. Due to the major-
ity of TCGA data being available in text files (in tabular
format), it is difficult to query the contents of a par-
ticular file or to enable virtual data integration. In this
paper, we have addressed above problems by applying
Semantic Web technologies and federated query process-
ing. Semi-structured level 3 TCGA data were converted
into Semantic Web standard format RDF such that it
could be queried and publicly accessed via SPARQL end-
points. This choice of technology complies with the W3C
recommendation of integrating distributed and hetero-
geneous data sources. There are currently a large num-
ber of applications supporting SPARQL and RDF, both
academic and commercial, and both SwissProt [14] and
EBI [15] have made their databases available as SPARQL
endpoints.
In order to address the scalability issue while dealing
with big data, we propose an efficient data distribution
strategy and a TCGA tailored federated query engine
(named TopFed) that leverages the data distribution along
with the structure of triple pattern joins in a query for
smart source selection. The logistics of the proposed
solution will be assessed by comparison with a well estab-
lished federation engine FedX [16].
Motivation
Before TCGA,most cancer genomics studies have focused
on only one type of data or one cancer histology. The
Cancer Genome Atlas project changes that paradigm by
making available to oncologists and biomedical scien-
tists a comprehensive compilation of raw and processed
data files on over 30 different cancer histologies and at
several levels of Genomics (e.g. SNP, protein expres-
sion, exon expression, sequences, methylation, etc.). Since
2006, when the Cancer Genome Atlas first became avail-
able, multiple studies were devised to exploit its data.
Nevertheless, a means to easily exploit this cancer atlas
like one would exploit an atlas of planet Earth, does
not yet exist. Part of the challenge is caused by a need
to represent, organize and structure the 28.3 TB of
data [17] available to the public in a way that can be
easily queried by computational/statistics tools. Further
complicating this task has been the growth of TCGA
data. Some institutions have access to the computational
resources necessary to provide a TCGA-synchronized and
query-able interface suitable to address the most complex
questions such as comparing methylation across cancer
histologies or correlating exon expression results with
methylation patterns regardless of cancer histology. One
institution providing a tool and query language to exploit
this data is Memorial Sloan Kettering through its cBio
portal [18]. However, the data must first be constrained
to the type of cancer before it can be exploited from a
biological/molecular stand point. A second challenge is
caused by the applications of the data - not all data are
useful for all cancer researchers. Some researchers focus
on a particular type of data, or a particular cancer histol-
ogy, and therefore have little or no interest in hosting the
entire Cancer Genome Atlas in a structured, query-able
form.
The aim behind the work presented in this paper was to
develop the computational concepts - and devise a proto-
type - that enable the exposure of TCGA as a distributed,
semantically aware API (application programming inter-
face). Although the data can be freely downloaded and
analyzed by anyone with a sufficiently powerful computer,
the computational tools available nowadays do not enable
exploring this atlas without significant effort involved
in selecting and downloading the data, mapping it to
genomic coordinates and easily navigating to the sections
of the genome that are relevant for understanding can-
cer. For example, zooming into genomic regions known as
Cancer Hotspots or into the genomic coordinates where
oncogenes and tumour suppressors are encoded, requires
a combination of efforts including: 1) downloading the
data; 2) parsing the text files for relevant results; and 3)
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 3 of 18
http://www.jbiomedsem.com/content/5/1/47
mapping each file to the same set of genomic coordinates.
On the other hand, fast, easy to use and integrated access
to the big data such as TCGA requires: 1) Representing
data in a format (e.g. RDF) amenable to integrated search;
2) logically connect all data; 3) distributing data across
multiple locations (load balancing); and 4) supporting
linking and federated querying (collecting data frommore
than one location using a single query) with external data
sources.
TopFed is devised to address these requirements.
Whereas requirements 1 and 2 are addressed using RDF
and class level connectivity (see section TCGADataWork
flow), addressing requirements 3 and 4 relies on tech-
niques that make the best use of the architecture of
the Web to enable both redundancy when resources are
down and sharing the load of hosting this data across
multiple locations. As a proof of concept, TopFed links
different portions of the Cancer Genome Atlas across
two institutions, one at Insight Centre for Data Ana-
lytics at NUIG in Ireland and other at the University
of Alabama at Birmingham in United States. TopFed is
devised as a federation query engine that enables selec-
tion of the appropriate endpoints necessary to address
an incoming query as well as optimization of the ser-
vices discovery based on metadata about each endpoint.
TopFed accepts queries in SPARQL, the same universal,
standardized query language as each of the endpoints con-
nected to it, making its functionality straightforward. For
example, if someone is looking to query only one can-
cer histology, they can direct their queries at the endpoint
hosting that data. However, if someone wants to exploit
and compare multiple cancer histologies, the query can
be pointed at TopFed, which automates and optimizes the
task of discovering endpoints that contain the data nec-
essary to address the question. To illustrate a typical use
case, we exemplify a genomic region query enabled by
TopFed.
Biological query example
This example makes use of the KRAS gene, a gene that
is commonly mutated and constitutively active in many
cancer types, leading the cell to replicate DNA and make
copies of itself at a very fast pace. Genes with this type
of behaviour in the cell are commonly called oncogenes.
When mutated, these genes become constitutive active,
thus having the potential to cause normal cells to become
cancerous. Imagine that for five different cancer histolo-
gies, we used TopFed to search for the methylation status
of the KRAS gene (chr12:25386768-25403863), and cre-
ated a box plot comparing the values, shown in Figure 1.
The query (given in Listing 1) executed on each of the five
SPARQL endpointsa, resulting in five different samples.
Figure 1 Biological query results.We used TopFed to search for the methylation status of the KRAS gene (chr12:25386768-25403863) across five
cancer histologies (hosted by five SPARQL endpoints) and created a box plot comparing the methylation values. The corresponding SPARQL query
to retrieve the required methylation values is given in Listing 1.
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 4 of 18
http://www.jbiomedsem.com/content/5/1/47
Listing 1 Query to retrieve averagemethylation values for
the KRAS gene and for all patient of a particular cancer
type
PREFIX t c g a : <h t tp : / / t c g a . d e r i . i e / schema/>
PREFIX r d f : < h t tp : / /www. w3 . org /1999/02/22? rd f?
syntax?ns#>
PREFIX xsd : <h t tp : / /www. w3 . org /2001/XMLSchema#>
SELECT DISTINCT ? p a t i e n t avg ( xsd : dec imal ( ?
methylationKRAS ) ) as ? avgMethKRAS
WHERE
{
? pa t i en tURI t cga : bcr_ { p } a t i e n t _ { b } arcode ? p a t i e n t .
? pa t i en tURI t cga : r e s u l t ? recordNo .
? recordNo tcga : chromosome "12" .
? recordNo tcga : p o s i t i o n ? p o s i t i o n .
? recordNo tcga : be ta_ { v } a l u e ? methylationKRAS .
FILTER ( xsd : dec imal ( ? p o s i t i o n ) >= 25386768 && xsd :
dec imal ( ? p o s i t i o n ) < 25403863)
}
This query returns the average methylation results for
the KRAS gene of all patients in a particular cancer
histology. The results show a clear distinction between
solid tumours and hematopoetic cancers. This differen-
tial in the methylation values is not necessarily surprising
results, given that blood cancers are known to be sig-
nificantly different genetically from solid tumours. What
is interesting and worth further exploring in these cases
is the shape of the distribution: why Acute Myeloid
Leukemia (AML) samples, a cancer of the myeloid of
blood cells, appear to have high methylation, effectively
creating a bi-modal distribution? Exploring the prove-
nance of this data may provide a clue for that - one
hypothesis is that these samples were incorrectly diag-
nosed as AML or it may be that these AML sam-
ples are indeed genetically different - and therefore
should not be treated with the same therapies as the
others. Since TopFed integrates both the clinical and
genomic parameters, exploring these different hypothe-
sis is as easy as returning to the query and retrieving
the potentially relevant clinical parameters that could
explain the difference. Exploring the same gene (KRAS)
in another type of data (e.g. exon expression) could
also help explain why these samples are different. Since
TopFed is aware of which SPARQL endpoints store
each data property, it will appropriately select the correct
source for the data, thereby adding the extra parame-
ters to the query sufficient to generate sufficiently robust
hypothesis.
Further exploring these examples is beyond the scope of
this manuscript - however, we encourage our readers to
experiment themselves with their own hypothesis or with
a different set of genes/genomic locations by changing the
values for tcga:chromosome and tcga:position.We include
an example of a query that could be used to retrieve the
clinical parameters for the outlier patients (and compare
with non-outlier patients) in Listing 2.
Listing 2 Query to retrieve averagemethylation values for
the KRAS gene, along with clinical data, for all AML outlier
patients. This query can be run at
http://vmlion14.deri.ie/node45/8082/sparql
PREFIX t c g a : <h t tp : / / t c g a . d e r i . i e / schema/>
PREFIX r d f : < h t tp : / /www. w3 . org /1999/02/22? rd f?
syntax?ns#>
PREFIX xsd : <h t tp : / /www. w3 . org /2001/XMLSchema#>
SELECT ? o u t l i e r s P a t i e n t ? avgMethKRAS ? p i o rd i a gn ?
v i t a l s t a t ? a g e a t d i a g ? gender ? p r e t r e a tH i s t o r y ?
e t h n i c i t y
? r ace
{
{ SELECT DISTINCT ? o u t l i e r s P a t i e n t ( avg ( xsd :
dec imal ( ? methylationKRAS ) ) as ? avgMethKRAS ) ?
p i o rd i a gn ? v i t a l s t a t ? a g e a t d i a g ? gender ?
p r e t r e a tH i s t o r y ? e t h n i c i t y ? r ace
WHERE
{
? pa t i en tURI <ht tp : / / t c g a . d e r i . i e / schema / bcr_ { p }
a t i e n t _ { b } arcode > ? o u t l i e r s P a t i e n t .
? pa t i en tURI <ht tp : / / t c g a . d e r i . i e / schema / r e s u l t > ?
recordNo .
? recordNo tcga : chromosome "12" .
? recordNo tcga : p o s i t i o n ? p o s i t i o n .
? recordNo tcga : be ta_ { v } a l u e ? methylationKRAS .
FILTER ( xsd : dec imal ( ? p o s i t i o n ) >= 25386768 && xsd :
dec imal ( ? p o s i t i o n ) < 25403863)
SERVICE <ht tp : / / vmlion14 . d e r i . i e / node42 /8081/
spa rq l >
{
? pa t i en tURI t cga : bcr_ { p } a t i e n t _ { b } arcode ?
o u t l i e r s P a t i e n t .
? pa t i en tURI t c g a : p r i o r _ { d } i a g n o s i s ? p i o rd i a gn .
? pa t i en tURI t cga : v i t a l _ { s } t a t u s ? v i t a l s t a t .
? pa t i en tURI t cga : age_ { a } t _ { i } n i t i a l _ { p } a t h o l o g i c _ {
d } i a g n o s i s ? a g e a t d i a g .
? pa t i en tURI t cga : gender ? gender .
? pa t i en tURI t cga : p re t r ea tment_ { h } i s t o r y ?
p r e t r e a tH i s t o r y .
? pa t i en tURI t cga : e t h n i c i t y ? e t h n i c i t y .
? pa t i en tURI t cga : r a ce ? r ace .
}
}
}
FILTER ( ? avgMethKRAS > 0 . 0 5 )
}
Related work
The TCGA data has been widely used in the literature,
but mostly in its raw form. Verhaakl et al. [19] use the
gene expression results to describe a robust molecular
classification of TCGA Glioblastoma Multiforme (GBM)
into Proneural, Neural, Classical, and Mesenchymal sub-
types and integrate multidimensional genomic data to
establish patterns of somatic mutations and DNA copy
number. Other notable contributions [20-24], including
our own early analysis of DNA copy number variation in
GBM [25] make use of the TCGA data for various impor-
tant findings without, however, using more than one or
possibly two types of molecular data. To facilitate inte-
grated analysis over all cancer types, Deus et al. developed
an infrastructure using Simple Sloppy Semantic Database
(S3DB) management model to expose clinical, demo-
graphic and molecular data elements generated by TCGA
as a SPARQL endpoint [26]. Robbins et al. [27] developed
an engine to continuously index and annotate the TCGA
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 5 of 18
http://www.jbiomedsem.com/content/5/1/47
data files using JavaScript in conjunction with RDF, and
the SPARQL query language. However, both [26] and [27]
provide only file level provenance annotations without
providing structured access to actual contents contained
in the files. Recently, Saleem et al. [28] presented a Linked
Data version of the Cancer Genome Atlas Database for
effective cancer treatment. This work demonstrates three
use cases namely target cancer treatment, mechanism-
based cancer treatment, and survival outcome, where the
Linked Data approach of integrating TCGA data was
used. More recently, a visualization of the integration
of the Linked TCGA cancer data with PubMed publica-
tions is presented in [29,30]. The main aim behind this
work is to foster serendipity through big data RDFization,
continuous integration, and visualization. GenomeSnip,
a visual analytics platform, which facilitates the intuitive
exploration of the human genome and displays the rela-
tionships between different genomic features, is presented
in [31].
Advances in federated query processing methods over
the Web of Data have enabled the application of feder-
ated solutions for datasets, such as those from genomics.
Quilitz and Leser [32] propose DARQ, which makes
use of service descriptions for relevant data source
selection.
Langegger et al. in [33] propose a solution similar
to DARQ using a mediator approach, which continu-
ously monitors the SPARQL endpoints for any dataset
changes and updates the service descriptions automati-
cally. Umbrich et al. [34,35] propose a Qtree-based index
structure that summarizes the content of data source for
query execution over theWeb of Data. Schwarte et al. [16]
propose FedX, an index-free query federation for theWeb
of Data.
SPLENDID [36] makes use of VOID descriptions along
with SPARQL ASK queries to select the list of relevant
sources for each triple pattern. Both FedX and SPLENDID
are able to handle more expressive queries as compared to
previous contributions.
Other optimization techniques have also been attemp-
ted. Li and Heflin [37] built a tree structure that supported
federated query processing over heterogeneous sources.
Kaoudi et al. [38] propose a federated query technique
on top of distributed hash tables (DHT). Ludwig and
Tran [39] developed a mixed query engine that assumes
some incomplete knowledge about the sources to select
and discover new sources at run time. Acosta et al. [40]
present ANAPSID, an adaptive query engine that adapts
query execution schedulers to SPARQL endpoints data
availability and run-time conditions.
Avalanche [41] gathers endpoint dataset statistics and
bandwidth availability on-the-fly before the query federa-
tion. Saleem et al. [42] presented DAW, a novel duplicate-
aware federated query approach over the Web of Data.
DAW makes use of the min-wise independent permuta-
tions [43] and compact data summaries to extend existing
SPARQL query federation engines in order to achieve the
same query recall values while querying less SPARQL end-
points. Finally, HiBISCuS [44] is an efficient hypergraph
based source selection approach for SPARQL query fed-
eration over multiple SPARQL endpoints. A fine-grained
evaluation of SPARQL endpoint federation systems is
performed in [45].
All of the above SPARQL query federation approaches
are more generic and usually over-estimate (explained in
the Source Selection sub-section below) the set of sources
capable for answering a query. This over-estimation can
be expensive when data is large. In our case, the data
in hand is also large and we need a TCGA optimized
federation engine that selects close to optimal set of capa-
ble sources. To this end, we propose TopFed, a TCGA
tailed federated engine that make use of the intelligent
data distribution and join-aware source selection to min-
imise the source over-estimation and provide fast query
results.
The main contributions of this paper are the following:
1. We have proposed a Linked Data version of TCGA
that supports efficient data distribution and
federated SPARQL queries to integrate data from
multiple SPARQL endpoints efficiently by only
sending remote queries.
2. We have published, to the best of our knowledge, the
largest RDF dataset (20.4 billion triples) and linked it
to various datasets in the LOD cloud to enable
annotation and enhancement with public knowledge
bases as well as virtual data integration.
3. We devised the basic architecture and logic rules
governing TopFed, a smart federated query engine
for virtual integration of the TCGA data from
multiple SPARQL endpoints that comply with the
TCGA organizational schema. Further, we provide
easy to use utilities [46] in order to refine and
transform TCGA raw text files into RDF.
4. We evaluate our approach against FedX using 10
different SPARQL queries and show that our source
selection algorithm, on average, selects less than half
sources compared to FedX (with 100% recall). Also,
our average query processing time is one third in
comparison to FedX.
The remaining part of this paper is organized as follows:
we present our methodology to refine, RDFize and link
the TCGA data to LOD datasets in detail. Subsequently,
we present a thorough evaluation of our approach against
state of the art approaches. We finally conclude the paper
with a discussion of our findings and an overview of future
work.
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 6 of 18
http://www.jbiomedsem.com/content/5/1/47
Methods
Transforming TCGA data to RDF
The process of transforming TCGA data into RDFb is
shown in Figure 2. Given a TCGA text file, the first pro-
cessing step is carried out by the Data Refiner, which
selects the specific fields [47] necessary for traditional
molecular analysis algorithms. This step is necessary to
minimize the size of the resulting RDF according to what
we expect will be the most useful results. It is impor-
tant to note that the above required fields for different
types of results may not be directly accessible through
raw text files. To this end, our Data Refiner makes
use of the annotations files [48] for the required fields
lookup. For example, methlylation annotation files are
used to obtain chromosome and position values using
Probe_Name lookup. Finally, the refined text file is sent to
the RDFizer, which generates the resulting RDF dump in
N3 format [49]. Our choice of N3 was due of its efficient
space consumption. The generated RDF dumpsc are then
uploaded to various SPARQL endpoints according to the
distribution rules shown in Figure 3.
An example of the above RDFication process is shown
in Figure 4, where part of rawmethylation result of patient
TCGA-A2-A0CX is provided as input to the Data Refiner.
The Data Refiner selects chrome, position, and beta_value
out of the five available columns. The selected columns
are commonly used for traditional molecular analysis
algorithms targeting methylation data. It is important to
Text File
Data Refiner
RDFizer
SPARQL endpoint
RDF
RDF
File
Figure 2 TCGA text to RDF conversion process. Given a text file,
first it is refined by the Data Refiner. The refine file is then converted
into RDF (N3 notation) by the RDFizer. Finally, the RDF file is uploaded
into a SPARQL endpoint.
note that Data Refiner also skipped the yellow highlighted
line because beta_value is not available for that specific
methylation result. The refined text file is then passed to
RDFizer that generates the RDF dump (N3 format). The
values d1...d8 show DNA methylation results from 1 to
8. The use of this information is further explained in the
Source Selection sub-section.
The accuracy of the text to RDF conversion is 100%
(to the best of our understanding) since our Data Refiner
selects a predefined set of fields for different types of
results. Further, it skips specific field values (such as NA,
Null, Unknown, Not Reported etc.) during RDFication pro-
cess as shown in the above example. Currently, we have
RDFized 27 cancer tumours and the statistics are shown
in Table 1. We will RDFize new TCGA data once it is
available through the TCGA data portal.
Linking TCGA to the LOD cloud
One of the design principles of Linked Data [50] is the
provision of links to other data sources. Adding links
from TCGA to other knowledge bases is particularly cru-
cial to ensure that the information already contained in
other data sources can be easily (1) merged with the new
TCGA data as well as (2) queried in combination with
the TCGA data by means of federated SPARQL queriesd.
Moreover, links can facilitate other tasks such as cross-
ontology question answering, data integration and data
analytics. Yet, the sheer size of bio-medical knowledge
base available on the LOD cloud and of the TCGA knowl-
edge base itself makes it impossible to use manual linking
to provide such cross knowledge-base links from TCGA
to other data sources. We thus made use of the LIMES
framework [51] for discovering links between TCGA and
other knowledge bases. LIMES is a framework for link
discovery that provides time-efficient implementations of
several string and numeric similarity and distance mea-
sures. The framework provides both means to define link
specifications explicitly and machine-learning algorithms
for finding link specifications in an unsupervised and
supervised fashion. Given that genes and chromosomes
have dedicated IDs that are used across several biomedi-
cal knowledge bases, we used LIMES exactMatchmeasure
for linking.We focused on linking patient data and lookup
data with knowledge bases that describe genes and chro-
mosomes. In particular, we linked TCGA to HGNC [52],
OMIM [53] and Homologene [54]. Tables 2 and 3 provide
an excerpt of the links generated for the TCGA dataset,
while Listing 3 provides an excerpt of the specifications
used for linking. The linking tasks were carried out on one
kernel of a 2.3GHz i7 processor with 4GB RAM. Given
that we used exact matches, we ensured that our link dis-
covery achieves a precision of 100%. The recall of the
linking process is tedious to assess as it would require
assessing millions of links manually.
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 7 of 18
http://www.jbiomedsem.com/content/5/1/47
b1 b2 p1 p2 g1 g2 g3 p3 p4 g4 g5 g6 p5 p6 g7 g8 g9 
C = {CNV, SNP, E-Gene, E-Protein, miRNA, Clinical} 
F = {Expression-Exon} M = {beta_value,  
(CNV, SNP, E-Gene, 
miRNA,  
   E-Protein, Clinical)  
 Exon-Expression  
Meth  
D = {seg_mean, rpmmm, scaled_est, p_exp_val} 
C-2 = {{p  {E A G} {p = rdf:type  o F}} {{S-Join(p, E  F)  P-Join(p, E  F)} {!S-Join(p, M  B D  C) 
               !P-Join(p, M  B D  C) }}} 
C-3 = {{p  {M A}  {p = rdf:type  o B}} {{S-Join(p, M  B)   P-Join(p, M  B) } {!S-Join(p, E  F D  C) 
               !P-Join(p, E  F D  C) }}} 
C-1 = {{p  {D A G}  {p = rdf:type  o C}} {{S-Join(p, D  C)  P-Join(p, D  C) } {!S-Join(p, M  B E F) 
               !P-Join(p, M  B E F) }}} 
C-1  Category 
Colour = blue
IF tumour lookup is successful 
   forward to corresponding 
leaf 
Else  
 broadcast to every one 
For each query triple t(s, p, o)  T 
A = {chromosome, result, code}  G = {start, stop} 
B = {DNA-Methy  
E = {RPKM} 
Tumours 
 SPARQL 
endpoints 
C-2  Category 
Colour = pink
C-3  Category 
Colour = green
1-16  17-33           1-5      6-11  12-16  17-22  23-27  28-33 1-4      5-8     9-12  13-16 17-20  21-24 25-27 28-30  31-33  
Figure 3 TCGA data distribution/load balancing and source selection. The proposed data distribution and source selection diagram for
hosting the complete Linked TCGA data.
chromosome n beta_value
16 28890100 0.439271303584937
3 57743543 0.245147665381461
7 15725862 0.0440161061196347
2 177029073 0.741342927038953
11 93862594 0.0290713821114479
14 93813777 0.985555436681019
18 11980953 0.0109832005732912
14 89290921 0.0104525957219692
composite 
element REF gene_symbol chromosome n beta_value
cg00000292 ATP2A1 16 28890100 0.439271303584937
cg00002426 SLMAP 3 57743543 0.245147665381461
cg00003994 MEOX2 7 15725862 0.0440161061196347
cg00005847 HOXD3 2 177029073 0.741342927038953
cg00006414 ZNF425 7 148822837 NA
cg00007981 PANX1 11 93862594 0.0290713821114479
cg00008493 COX8C 14 93813777 0.985555436681019
cg00008713 IMPA2 18 11980953 0.0109832005732912
cg00009407 TTC8 14 89290921 0.0104525957219692
@prefix  
@prefix .
@prefix 
@prefix -rdf-syntax-ns#type>.
@prefix  
@prefix 
@prefix  
@prefix 
b:TCGA-A2-A0CX d: "TCGA-A2-A0CX". 
b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d1 . 
b:TCGA-A2-A0CX-d1 c: w: ; m: "16"; v: "28890100"; u: "0.439271303584937". 
b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d2 . 
b:TCGA-A2-A0CX-d2 c: w: ; m: "3"; v: "57743543"; u: "0.245147665381461". 
b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d3 . 
b:TCGA-A2-A0CX-d3 c: w: ; m: "7"; v: "15725862"; u: "0.0440161061196347". 
b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d4 . 
b:TCGA-A2-A0CX-d4 c: w: ; m: "2"; v: "177029073"; u: "0.741342927038953". 
b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d5 . 
b:TCGA-A2-A0CX-d5 c: w: ; m: "11"; v: "93862594"; u: "0.0290713821114479". 
b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d6 . 
b:TCGA-A2-A0CX-d6 c: w: ; m: "14"; v: "93813777"; u: "0.985555436681019". 
b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d7 . 
b:TCGA-A2-A0CX-d7 c: w: ; m: "18"; v: "11980953"; u: "0.0109832005732912". 
b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d8 . 
b:TCGA-A2-A0CX-d8 c: w: ; m: "14"; v: "89290921"; u: "0.0104525957219692". 
Data Refiner
RDFizer
Refined
RDFizedRaw
Figure 4 Text to RDF conversion process example. An example showing the refinement and RDFication of the TCGA file.
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 8 of 18
http://www.jbiomedsem.com/content/5/1/47
Listing 3 Excerpt of the LIMES link specification for linking
TCGA and Homologene
<SOURCE>
<ID>TCGA</ ID>
<ENDPOINT>dna_methylat ion450_Lookup . nt </ENDPOINT>
<VAR>?x </VAR>
<PAGESIZE>?1</PAGESIZE>
<RESTRICTION>?x rd f : t ype tcga?schema :
dna_methy la t ion450_ lookup </RESTRICTION>
<PROPERTY> tcga?schema : Gene_Symbol AS lowerca s e
</PROPERTY>
<TYPE>N?TRIPLE </TYPE>
</SOURCE>
<TARGET>
<ID>homologene </ ID>
<ENDPOINT>ht tp : / / homologene . b i o 2 rd f . org / s p a r q l
</ENDPOINT>
<VAR>?y </VAR>
<PAGESIZE>10000 </PAGESIZE>
<RESTRICTION>? y a homologene : HomoloGene_Group
</RESTRICTION>
<PROPERTY>homologene : has_gene_symbol AS lowerca s e
</PROPERTY>
</TARGET>
<METRIC>exactmatch ( x . tcga?schema : Gene_Symbol ,
y . homologene : has_gene_symbol ) </METRIC>
<ACCEPTANCE>
<THRESHOLD>1</THRESHOLD>
<FILE>dna_450_homologene_accepted . nt </ FILE>
<RELATION>tcga?schema : Homologene </RELATION>
</ACCEPTANCE>
TCGA data workflow and schema
To devise a fast, big data driven query federation engine,
we started by exploiting how the various files and types
of data in TCGA are interconnected. To date, 23054 raw
data files from 28 cancer tumours have been collected,
summing up to a total of 28.3 TB of data [55]. For each
level 3 data, we have identified three different types, i.e.,
we RDFized level 3 data for each cancer type and further
define 3 data types for each of the level 3 tumours data
of data. The resulting data are organized as a three layer
architecture where layer 1 contains patient data, layer 2
consists of clinical information and layer 3 contain results
for different samples of a patient. Each type of data is
assigned to a different class in the RDFized version as
depicted in Figure 5. For each patient, tumour and blood-
/normal tissue samples are collected and divided into
different portions upon which different protocols such as
DNA, RNA and so on, are applied to extract the ana-
lytes for the analysis of the sample. The extracted analytes
are distributed across plates. All these plates contain-
ing patients tumour and normal samples are shipped to
Genome Characterization Centres (GCCs) and Genome
Table 1 Statistics for 27 tumours sorted by number of triples
Tumour type Raw(GB) Refined(GB) RDF(GB) Triples(Million)
Lymphoid Neoplasm Diffuse Large 0.37 0.20 0.83 35
B-cell Lymphoma (DLBC)
Cutaneous melanoma (UCS) 1.2 0.64 2.6 113
Glioblastoma multiforme (GBM) 2.3 0.77 2.8 132
Esophageal carcinoma (ESCA) 1.5 0.88 3.4 149
Adrenocortical carcinoma (ACC) 1.6 0.90 3.6 158
Pancreatic adenocarcinoma (PAAD) 2.6 1.1 4.5 200
Kidney Chromophobe (KICH) 3.7 1.4 5.3 242
Sarcoma (SARC) 3.8 1.5 5.9 267
Cervical (CESC) 8.75 2.44 8.86 400.19
Ovarian serous cystadenocarcinoma (OV) 8.2 2.4 8.7 410
Rectal adenocarcinoma (READ) 8.07 2.25 9.04 413.31
Papillary Kidney (KIRP) 10.40 2.90 10.4 469.65
Stomach adenocarcinoma (STAD) 5.5 2.9 12 529
Liver hepatocellular carcinoma (LIHC) 8.2 3.1 12 550
Bladder cancer (BLCA) 12.16 3.39 12.3 556.38
Acute Myeloid Leukemia (LAML) 14.85 4.14 15.1 684.05
Lower Grade Glioma (LGG) 17.08 4.76 17.1 778.82
Prostate adenocarcinoma (PRAD) 18.05 5.03 18.1 821.01
Lung squamous carcinoma (LUSC) 20.63 5.75 20.5 927.08
Cutaneous melanoma (SKCM) 23.22 6.47 23.2 1050.94
Uterine Corpus Endometrial Carcinoma (UCEC) 13 5.98 24.2 1070
Colon adenocarcinoma (COAD) 18 6.64 26 1175
v Head and neck squamous cell(HNSC) 27.6 7.69 27.5 1245.37
Lung adenocarcinoma (LUAD) 23 9.1 36 1611
Kidney renal clear cell carcinoma (KIRC) 24 9.4 37 1658
Thyroid carcinoma (THCA) 26 10.1 40 1796
Breast invasive carcinoma (BRCA) 45 17 65 2959
A total of 20.4 Billion triples.
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 9 of 18
http://www.jbiomedsem.com/content/5/1/47
Table 2 Excerpt of the links for the lookup files of TCGA
Source Target Class # links Runtime (ms)
DNA27 HGNC Genes 23,181 154
DNA27 Homologene Genes 27,654 193
DNA27 OMIM Genes 15,171 158
DNA450 Homologene Genes 489,643 5,710
DNA450 OMIM Genes 212,284 429
DNA27 HGNC Chromosomes 108,662 96
DNA27 OMIM Chromosomes 16,039,535 8,055
The source column shows the name of the look-up file that was linked to the
target dataset named in the second column. The class column shows the type of
resources that were linked. The fourth column shows the number of links that
were generated while the runtime column shows the time required by LIMES to
carry out the linking process in ms.
Sequencing Centres (GSCs) that produce different data
type results which are shown in layer 3 (cf. Figure 5).
The schema of the corresponding Linked TCGA is shown
in Figure 6. We have included only important proper-
ties from clinical data (e.g., drug, follow-up, radiation
etc.) as the complete list of properties is around 300.
This diagram is useful to understand the connectivity
between the Linked TCGAdata and to formulate SPARQL
queries.
Data distribution and load balancing
A key property of the federation method described here
is the efficient distribution of the data among SPARQL
endpoints to enable access to around 20 billion resulting
triples in a virtual integrated manner, i.e., the required
data are transparently collected from different SPARQL
endpoints. Proper load balancing among SPARQL end-
points is also ensured to reduce the query execution time.
To this end, we have divided each tumour data into three
categories, each of which is assigned a different colour 
blue, pink and green  as shown in Figures 3 and 7. The
green category contains only methylation results, pink
contains expression exon results and all other data are
Table 3 Excerpt of the links for themethylation results of a
single patient
Source Target Class # links Runtime (ms)
Methylation HGNC Chromosomes 97,530 205
Methylation OMIM Chromosomes 14,407,269 6,095
Gene expression HGNC Chromosomes 86,052 80
Gene expression OMIM Chromosomes 12,535,829 4,679
The source column shows the name of the patient file that was linked to the
target dataset named in the second column. The class column shows the type of
resources that were linked. The fourth column shows the number of links that
were generated while the runtime column shows the time required by LIMES to
carry out the linking process in ms.
grouped in the blue category. The ratio of the sizes is 1:3:4
for blue, pink, and green respectively.
In order to achieve proper load balancing, if we allo-
cate one SPARQL endpoint to the blue category data
(smallest) then we must assign three SPARQL endpoints
to pink and four SPARQL endpoints to the green cat-
egory data. We propose 17 SPARQL endpoints to be
assigned for the complete TCGA level 3 data (around
33 tumours expected) distribution as shown in Figure 3.
We assigned two SPARQL endpoints for blue, six end-
points for pink and nine endpoints for green category
data.
Data are also balanced across each of the coloured
category SPARQL endpoints according to cancer type
(tumour). For example, in blue category, tumours
1-16 are stored in the first blue SPARQL endpoint and
the remaining tumours (17-33) are stored in the second
blue SPARQL endpoint. It is important to note that we
have RDFized 27 tumours while in our data distribu-
tion diagram we show 33 tumours. This is because we
are expecting around 33 cancer tumours [56] data to be
made available by the TCGA data portal in the future.
To achieve a similar size-oriented division, each of the
SPARQL endpoints in the pink category contains either
five or six tumours data as shown in Figure 3 and each
of the first six SPARQL endpoints in the green category
contain data for four tumours and each of the remain-
ing three SPARQL endpoints contain three tumours data.
Each of the three categories is used to create a con-
ditional statement (labelled C-1, C-2, and C-3 given in
Listing 4), used by the federated engine for source selec-
tion. For source selection, the predicates sets shown in
Figure 3 (D, C, B, M, F, E, A and G) are also relevant.
We further explain the decision model in Source Selection
sub-section.
Listing 4 Conditions for colour category selection
C?1 = { { p ? {D ? A ? G} ? { p = rd f : t ype ? o ? C} }
? { { S?J o i n ( p , D ? C) ? P?J o i n ( p , D ? C) }
? { ! S?J o i n ( p , M ? B ? E ? F ) ? ! P?J o i n ( p , M
? B ? E ? F ) } } }
C?2 = { { p ? { E ? A ? G} ? { p = rd f : t ype ? o ? F } }
? { { S?J o i n ( p , E ? F ) ? P?J o i n ( p , E ? F ) }
? { ! S?J o i n ( p , M ? B ? D ? C) ? ! P?J o i n ( p , M
? B ? D ? C) } } }
C?3 = { { p ? {M ? A} ? { p = rd f : t ype ? o ? B } }
? { { S?J o i n ( p , M ? B) ? P?J o i n ( p , M ? B) }
? { ! S?J o i n ( p , E ? F ? D ? C) ? ! P?J o i n ( p , E
? F ? D ? C) } } }
TopFed federated query processing approach
Before going into the details of our federated query pro-
cessing model shown in Figure 7, we first briefly explain
TopFeds index which comprise of an N3 specification
file and a Tissue Source Site to Tumour (TSS-to-Tumour)
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 10 of 18
http://www.jbiomedsem.com/content/5/1/47
Layer 1
Layer 3: Results
Layer 3: Results Layer 2: Clinical
pa
shipment
aliquotsample
slide
protocol
snp_result
expression_protein_
result
miRNA_result
dna_meth _
result
copy_number_
result
analyte
follow _Up
radi on
drug
expression_gene_
result
expression_exon_
result
expression_protein_
lookup
miRNA_lookup
dna_methyla n_
lookup
expression_exon_
lookup
expression_gene_
lookup
Figure 5 TCGA class diagram of RDFized results. Each level 3 data is further divided into three layers where: layer 1 contains patient data, layer 2
consists of clinical information and layer 3 contain results for different samples of a patient.
hash table. The N3 specification file, shown in Listing 5,
is devised based on the data distribution described in
previous section. It contains metadata relevant for data
distribution across SPARQL endpoints. For each SPARQL
endpoint, its colour category, endpoint url, and the list of
tumours data stored therein are specified. Moreover, the
specification file also contains the various sets of predi-
cates. In addition, we also create a Tissue Source Site to
Tumour (TSS-to-Tumour [57]) hash table that contains
key value pairs for TSS to tumour name. The TSS is the
Figure 6 Linked TCGA schema diagram. The schema diagram of the Linked TCGA, useful for formulating SPARQL queries.
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 11 of 18
http://www.jbiomedsem.com/content/5/1/47
Query Engine
Parser
Federator Opmizer
Integrator
Blue SPARQL 
endpoints
b1
Pink SPARQL endpoints
Specificaon
File
TSS-to-Tumour
Hash Table
Results
ResultsSPARQL Query
Sub-query Sub-query
Green SPARQL endpoints
b2 p1 p1 p6 g1 g1 g9
Figure 7 TopFed federated query processing model. TCGA
tailored federated query processing diagram, showing system
components.
location identifier from where the results of the different
tissues are obtained. This hash table was formed using
File_Sample_Map files (containing file to patient bar-
code entries) provided as meta data, with every TCGA
archive download via its Data Matrix portale. This meta
file provides a list of patient barcodes belonging to a
particular cancer tumour. We extract the TSS part of
patient barcodef and use this along with tumour name
as a hash entry. Both N3 specification file and TSS-
to-Tumour hash table are used by our federated query
processor for efficient relevant data source (SPARQL
endpoints) selection, which is explained in the next
sub-section.
Listing 5 Part of the N3 specification file
@pref ix t c ga : < h t tp : / / t c g a . d e r i . i e / schema / > .
<h t tp : / / t c g a . d e r i . i e / s e t / setA > t cga : setName "A " ;
t c ga : s e tE l ement s " r e s u l t " , " chromosome " , " bcr_ { p }
a t i e n t _ { b } arcode " .
<h t tp : / / t c g a . d e r i . i e / s e t / setE > t cga : setName "E " ;
t c ga : s e tE l ement s "RPKM" .
<h t tp : / / t c g a . d e r i . i e / s e t / setG > t cga : setName "G " ;
t c ga : s e tE l ement s " s t a r t " , " s top " .
<h t tp : / / t c g a . d e r i . i e / s e t / setM> tcga : setName "M" ;
t cga : s e tE l ement s " p o s i t i o n " , " be ta_ { v } a l u e " .
< h t tp : / / t c g a . d e r i . i e / endpoint / blue1 > t cga : c a t e go r y
" b lue " ;
t c g a : endpo intUr l " h t tp : / / 1 0 . 1 9 6 . 2 . 2 1 4 : 8 8 9 0 / s p a r q l
" ;
t c g a : containTumours "BLCA" , "CESC " , "HNSC" , " KIRP
" , "LAML" .
<h t tp : / / t c g a . d e r i . i e / endpoint / blue2 > t cga : c a t e go r y
" b lue " ;
t c g a : endpo intUr l " h t tp : / / 1 0 . 1 9 6 . 2 . 1 2 3 : 8 8 9 0 / s p a r q l
" ;
t c g a : containTumours "LGG" , "LUSC" , "PRAD" , "READ
" , "SKCM" .
<h t tp : / / t c g a . d e r i . i e / endpoint / pink1 > t cga : c a t e go r y
" pink " ;
t c g a : endpo intUr l " h t tp : / / 1 0 . 1 9 6 . 2 . 1 3 0 : 8 8 9 0 / s p a r q l
" ;
t c g a : containTumours "BLCA" , "CESC " , "HNSC " .
Given a SPARQL query, it is first parsed and then
sent to the federator that makes use of the N3
specification file along with the TSS-to-Tumour hash
table, in order to find the relevant sources for each
of the triple pattern using Algorithm 1. The opti-
mizer makes use of the source selection to generate
an optimized sub-query execution plan. The opti-
mized sub-queries are then forwarded to the relevant
SPARQL endpoints. The results of each sub-query exe-
cution are integrated and the final query result set is
generated.
Source selection
The goal of the source selection is to find the optimal
list of relevant sources (i.e., SPARQL endpoints) against
individual query triple pattern. According to the distri-
bution of Figure 3, if we can infer the category colour
and tumour number for a triple pattern then we only
need to query a single endpoint for that triple pattern.
For example, starting from the root node of Figure 3,
we can go to the second level of the tree by knowing
the category colour (blue, pink, and green). Further, at
second level, if we know the tumour number then we
can reach to a single SPARQL endpoint to query. For
each query triple pattern, our source selection algorithm
tries to get such information using the specification file
and type (star, path) of the join between the query triple
patterns.
A star join between two triple patterns is formed if
both of the triple patterns share the same subject. Con-
sider the query given in Listing 6: the first two triple
patterns form a star join and the last four triple patterns
form a second star join. A path join between two triple
patterns is formed if object of the first triple pattern is
used as subject of the second triple pattern. For example,
the second triple pattern form a path join with the third
triple pattern in the query shown in Listing 6. Moreover,
every TCGA patient is uniquely identified by its barcode
of the format <TCGA-TSS-PatientNo>. For example,
the patient barcode used in the first triple pattern of
the Listing 6 query has a TSS identifier 18 and patient
number 3406. This means we can infer tumour name/
number from patient barcode using the TSS to tumour
hash table.
Listing 6 TCGA query with bound predicate
{
? s t c g a : bcr_ { p } a t i e n t _ { b } arcode "TCGA-18-3406" .
? s t c g a : r e s u l t ? recordNo .
? recordNo tcga : chromosome ? chromosom .
? recordNo tcga : s t a r t ? s t a r t .
? recordNo tcga : s top ? s top .
? recordNo tcga : seq_ {m} ean ?mean .
}
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 12 of 18
http://www.jbiomedsem.com/content/5/1/47
Algorithm 1 triple pattern source selection
Require: Dblue = {b1, b2};Dpink ={p1, p2, ... p6};Dgreen ={g1,
g2,... g9}; T = {t1, t2, ...tn}; tumourNo //data sources,
query triple patterns, tumour number (can be null)
1: for each bgp ? T do //each BGP in query
2: for each ti ? bgp do //each triple pattern in BGP
3: sources = null; c1Sources = null; c2Sources =
null; c3Sources = null; type = null; s = subj(ti); p =
pred(ti); o = obj(ti)
4: if bound(s) then //if subject is bound
5: catColour = s.getCategorycolour() //get cate-
gory colour from subject
6: tNo = s.getTumour() //get tumour from sub-
ject
7: if catColour = blue then
8: sources = Dblue
9: else if catColour = pink then
10: sources = Dpink
11: else if catColour = green then
12: sources = Dgreen
13: end if
14: Si = sources.filter(tNo) //this will return a sin-
gle capable source
15: else if bound(p) then //if predicate is bound
16: if C-1 then
17: c1Sources = Dblue
18: end if
19: if C-2 then
20: c2Sources = Dpink
21: end if
22: if C-3 then
23: c3Sources = Dgreen
24: end if
25: sources = c1Sources ? c2Sources ? c3Sources
26: if sources = null then
27: sources = Dblue //only check for clinical
properties
28: end if
29: if tumourNo = null then
30: Si = sources.filter(tumourNo)
31: else
32: Si = sources
33: end if
34: else if !bound(p) ? !bound(s) then //if only
object is bound
35: // prune selected sources with ASK queries
36: for each si ? {Dblue ? Dpink ? Dgreen} do
37: if ASK(si, ti) = true then
38: Si = Si ? {si}
39: end if
40: end for
41: end if
42: return Si //reutrn the set of relevant sources for
triple pattern ti
43: end for
44: end for
As discussed in the Data distribution section, we have
categorized all SPARQL endpoints into three different cat-
egory colours named blue, pink, and green. Our source
selection algorithm (cf. Algorithm 1) requires the set of
SPARQL endpoints in each of the colour category and
stores three different sets named Dblue,Dpink , and Dgreen.
Moreover, it requires the tumour number tumourNo,
which can be null and is obtained from the query as follow:
if a triple pattern with predicate tcga:bcr_patient_barcode
and bound object containing the patient barcode form a
star join with a triple pattern having predicate tcga:result,
then by using the patient barcode value specified in the
former triple pattern can be used to get the required
tumour number using TSS-to-Tumour hash table. Our
source selection algorithm runs for each basic graph pat-
tern (BGP [58]) and for each individual triple pattern of
BGP as follow.
If subject of the triple pattern is bound then we can get
both the category colour and tumour name from the sub-
ject URI. The format of the TCGA URI is <http://tcga.
deri.ie/Patient_barcode-ResultType>. The tumour name
can be obtained from Patient_Barcode and the category
colour can be inferred from ResultType. For example, if
the first character is e (shortcut for exon-expression), then
it belongs to the pink category. However, if the first char-
acter is d (shortcut for dna-methylation), then it belongs
to the green category and all other characters belong to
the blue category. Consider the query given in Listing 7:
the tumour name can be obtained using hash table lookup
for TSS 18 and the colour category is pink.
Listing 7 TCGA query with bound subject
{
<h t tp : / / t c g a . d e r i . i e /TCGA?18?3406?e266 > t cga : s t a r t
? s t a r t .
< h t tp : / / t c g a . d e r i . i e /TCGA?18?3406?e266 > t cga : s top ?
s top .
<h t tp : / / t c g a . d e r i . i e /TCGA?18?3406?e266 > t cga :RPKM ?
rpkm .
}
Source selection for a triple pattern with only bound
predicate is more challenging. We have divided various
predicates and classes of the TCGA data into different sets
that are shown in Listing 8. Set D contains all the pred-
icates that uniquely identify the blue category and set C
contains a list of classes specific to it. The sets B and M
uniquely identify the methylation, i.e., the green category
while sets F and E are for the pink category. Sets A and
G contain predicates that can be found in more than one
colour category. Starting from the root of the source selec-
tion tree, if the condition C-1 given in Listing 4 holds then
all of the sources in blue category are relevant for that
triple pattern. This means that if predicate p of the triple
pattern is set member of {D ? A ? G} or it is equal to
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 13 of 18
http://www.jbiomedsem.com/content/5/1/47
rdf:type and the object o belongs to set C and either the
star or path join between p and {D ? C} is true or the star
and path join of p with {M ? B ? E ? F} is false, then all of
the sources in the blue category are relevant.
Listing 8 Predicate and class sets
D = { seq_ {m} ean , r eads_ { p } er_ {m} i l l i o n _ {m} i r n a _ {m}
apped , s c a l e d _ { e } s t imate , p r o t e i n _ { e } xp r e s s i on_
{ v } a l u e }
C = { copy_ { n } umber_ { r } e s u l t , snp_ { r } e s u l t ,
e xp r e s s i on_ { g } ene_ { r } e s u l t , e xp r e s s i on_ { p }
r o t e i n _ { r } e s u l t ,
mirna_ { r } e s u l t , C l i n i c a l }
B = { dna_ {m} e t h y l a t i o n _ { r } e s u l t }
M = { be ta_ { v } a lue , p o s i t i o n }
F = { exp r e s s i on_ { e } xon_ { r } e s u l t }
E = {RPKM}
A = { chromosome , r e s u l t , bcr_ { p } a t i e n t _ { b } arcode }
G = { s t a r t , s top }
Consider the third triple pattern of the query given in
Listing 6: the predicate chromosome is set member of
A, which means this predicate can be found in all of the
endpoints. However, chromosome has a star join with
seq_mean, which is unique for the blue category sources.
Therefore, instead of selecting all of the sources (overes-
timated as in FedX, SPLENDID etc.), TopFed will only
selectDblue as relevant sources that can be further filtered,
provided that the tumourNo given as input to Algorithm 1
is not null. Similarly, C-2 holds for Dpink and C-3 holds
for Dgreen relevant source selection. It is important to note
that more than one condition (C-1, C-2, C-3) can be true
for a triple pattern, therefore we check each of the three
conditions individually and make a union of the sources
as given in line 24 of Algorithm 1. Further, if none of the
condition is true then we need to query the blue category
sources because we did not list many of the blue category
predicates as they are numerous.
For a triple pattern with bound object, we send SPARQL
ASK queries including the triple pattern to all of the
sources and select sources that pass the test. This is sim-
ilar to the source selection technique used in FedX for all
the triple patterns. Along with Algorithm 1, Figure 3 also
provides a visual demonstration of our triple pattern-wise
source selection.
As an example, consider the query of Listing 6 and
the data distribution given in Figure 3. TopFed selects
one source for the first triple pattern because we can
obtain tumour number from the given patient barcode
and this triple pattern only passes C-1. FedX selects three
sources since every patient data can be found in each of
the three colour categories exactly at one SPARQL end-
point. For the second triple pattern, TopFed again selects
only one source because C-1 only holds. However, FedX
selects all of the 17 sources as predicate tcga:result can
be found in all of the endpoints. For each of the remain-
ing triple patterns (3 to 6), TopFex selects only one source
as tcga:seq_mean is unique for the blue category end-
points and the others triple patterns (3 to 5) has star
join with it. We have only two endpoints in blue cate-
gory, which is filtered to one using the tumour number
given in triple pattern 1. FedX selects all of the 17 sources
for tcga:chromosome, eight sources each for tcga:start,
tcga:stop, and two sources for last triple pattern. In total,
TopFed selects only six sources while FedX selects 52 to
answer this query. Additionally, FedX also needs to send
102 (6*17) SPARQL ASK queries. We want to emphasize
that we have replaced only source selection algorithm of
FedX. The join order optimization and the join implemen-
tation remains the same.
Results and discussion
Evaluation
The goal of this evaluation is to support the claim that
TopFed selects a significantly smaller number of sources
for the same recall as FedX, thus achieving a good query
execution performance for large datasets. We compare
TopFed with the state-of-the-art approach for query fed-
eration (FedX) both in terms of the total number of
sources selected and the execution time to achieve a
100% recall, using 10 TCGA benchmark SPARQL queriesg
of different shapes (i.e. star, path, and hybrid). A tex-
tual description of all the benchmark queries is given in
Table 4. FedX has been shown previously [36,45] to be the
Table 4 Benchmark queries descriptions
Query Description
Q1 Get the chromosome, start, stop and mean copy number
values of the patient TCGA-18-4721 for genome locations
554268 to 5994290
Q2 Get the chromosome, start, stop and mean exon-expression
values of all the TCGA patients
Q3 Get the chromosome, position and mean methylation values
of all the TCGA patients
Q4 Get the chromosome, start and stop values of the TCGA patient
TCGA-C4-A0F6
Q5 Get the chromosome, start, stop values of all the TCGA patients
Q6 Get the chromosome, start, stop and miRNA values of the 20th
record of TCGA patient TCGA-AB-2821
Q7 Get the chromosome, start and stop values of the TCGA patient
TCGA-AB-2823 for mean sequence value of 0.0839
Q8 Get the chromosome, start, stop, mean protein expression and
mean exon-expression values of the TCGA patient
TCGA-18-3410
Q9 Get the chromosome, mean gene expression and mean
methylation values of the TCGA patient TCGA-C5-A1BF
Q10 Get the chromosome, mean gene expression, mean exon
expression and mean methylation values of all the
TCGA patients
The corresponding SPARQL queries can be downloaded from http://goo.gl/
UxUEXk.
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 14 of 18
http://www.jbiomedsem.com/content/5/1/47
fastest and more precise SPARQL federated query engine
(to the best of our knowledge). Therefore, we evaluate
TopFeds query performance by comparing it with FedX.
TCGA benchmark setup
TCGA benchmark data consists of genomic results from
25 patients randomly selected from ten different tumour
types and distributed across ten local SPARQL endpoints
with the specifications given in Table 5. Furthermore, the
benchmark N3 specificationh file (used in the current
experiments) assigns two, three, five SPARQL endpoints
to the blue, pink, and green categories respectively.
We have selected ten SPARQL queries based on expert
opinion reflecting typical requests on TCGA data. Fur-
ther, we have categorized our benchmark queries into four
different quadrants as shown in Table 6. A single colour
query collects results from SPARQL endpoints listed in
one of the three colour categories. A cross-colour query
targets more than one colour category results. A hybrid
query contains both star and path joins between various
triple patterns. Moreover, we can also obtain the tumour
number (to be used as input to Algorithm 1) from all of
the hybrid queries. All of the benchmark data, including
benchmark queries, can be found at the project website.
Experimental results
In order to show the effects of source selection on perfor-
mance (runtime + recall of sources selected), the number
of sources selected for each triple pattern of the query are
added (equation 1). Let mi equal the number of sources
capable of answering a triple pattern ti and S is the total
number of available sources (10 in our benchmark). Then,
for a query q with triple patterns {t1, t2, . . . , tn}, the total
number of sources selected (triple pattern-wise sources
selected) is given in equation 1.
total number of sources selected =
n?
t=1
mt : 0 ? mt ? S
(1)
Table 5 Benchmark SPARQL endpoints specifications
SPARQL endpoint CPU RAM Hard disk
virtuoso-blue1 2.2 GHz, i3 4 GB 300 GB
virtuoso-blue2 2.6 GHz, i5 4 GB 150 GB
virtuoso-pink1 2.53 GHz, i5 4 GB 300 GB
virtuoso-pink2 2.3 GHz, i5 4 GB 500 GB
virtuoso-pink3 2.53 GHz, i5 4 GB 300 GB
virtuoso-green1 2.9 GHz, i7 16 GB 256 GB SSD
virtuoso-green2 2.9 GHz, i7 8 GB 450 GB
virtuoso-green3 2.6 GHz, i5 8 GB 400 GB
virtuoso-green4 2.6 GHz, i5 8 GB 400 GB
virtuoso-green5 2.9 GHz, i7 16 GB 500 GB
Table 6 Benchmark queries distribution
Single Colour Cross-Colour
Star 2 2
Hybrid (star + path) 2 4
The source selection results are shown in Figure 8.
Overall, our source selection algorithm selects on aver-
age less than half of the sources selected by FedX. This is
due to the possible overestimation of the sources by FedX
while using SPARQL ASK queries for relevant source
selection [16]. For example, any data source will likely
match a triple pattern (?s, rdf:type, ?o). However, the same
sources might not lead to any results at all once the actual
mappings for ?s and ?o are included in a join evalua-
tion. On the contrary, our source selection algorithm was
designed to resolve the join types between query triple
patterns specifically to avoid such overestimation (which
can later greatly increase the query processing time as
reflected in Table 7). Only in queries 5 and 10, TopFed
selected sources are equal to FedX. The explanation for
this can be found in the amount of useful information
available in each query - both query 5 and query 10 are
generic queries from which a tumour or a performance-
improving colour category cannot be derived, because
all logic conditions are exactly satisfied. Overall, TopFed
selects the optimal (the actual required sources) num-
ber of sources with 100% recall for all of the benchmark
queries.
We have performed a two-tailed heteroscedastic t-test
based on a sample of 10 (each query was run 10 times) to
compare the source selection execution time. The source
selection execution time and the standard error (S.E)
obtained are presented in Table 8. On average, our source
selection algorithm only requires 17 msec per query. This
is because our N3 specification file is much smaller (only
43 lines) and we have created an in-memory Sesame
repository to load and access this file. For the first run, the
FedX source selection execution time is much higher. This
delay is caused by the query engine sending a SPARQL
ASK query for each of the query triple patterns, and for
each of the sources. As explained above, FedX needs to
issue 102 SPARQL ASK queries to perform source selec-
tion for the query in Listing 6 and the data distribution
in Figure 3. In order to minimise the number of SPARQL
ASK queries, FedX makes use of the cache to store the
result of the recent SPARQL ASK request. Every time a
query is issued, the engine first looks for a cache hit before
issuing the actual SPARQL ASK query. To show the effect
of the cache, we have rerun the same query 10 times after
the first run and we have noticed a reasonable improve-
ment. For a complete cached entries (100% cache hit),
our source selection execution time is still comparable
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 15 of 18
http://www.jbiomedsem.com/content/5/1/47
0
5
10
15
20
25
30
35
40
1 2 3 4 5 6 7 8 9 10 Avg
FedX total number of sources selected
TopFed  total number of sources selected
Figure 8 Efficient source selection. Comparison of the TopFed and FedX source selection in terms of the total number of triple pattern-wise
sources selected. Y-axis shows the total triple pattern-wise sources selected for each of the benchmark query given in X-axis.
with FedX. It is important to note that all queries that
are not specific to a patient (i.e, queries 2, 3, 5, 10), the
TopFed source selection time is small (less than 10 msec).
The reason is that the tumour number cannot be inferred
from these queries and as a result less computation (index
lookups) is required in the source selection Algorithm 1.
In Table 7, we compare the execution time of TopFed
and FedEx for all of the benchmark queries using a two-
tailed heteroscedastic t-test based on a sample of 10. It
is important to mention that the query execution time
was measured when the first result was retrieved, i.e.,
we did not iterate over all results. As an overall perfor-
mance evaluation, the query execution time of TopFed
is about one third to that of FedX. Specifically, TopFed
significantly outperforms FedX in benchmark queries 2
and 3 related to exon expression and methylation, respec-
tively. These queries select the complete set of results for
all of the 25 patients. TopFed is able to infer from the
query that the category colour should be pink and green,
respectively, and issue the complete query to only the
endpoints in the corresponding colour categories. In con-
trast, FedX is not able to perform such pre-processing,
hence issuing the query to all endpoints. As a result, it
has to collect results from all of the endpoints in the blue,
pink, and green categories when only one of the cate-
gories can produce results for each query. As an example
of the FedX approach addressing query 2, the triple pat-
tern (?recordNo, tcga:chromosome, ?chromosom) relies
Table 7 Comparison of average execution time for each query (based on a sample of 10)
FedX(first run) FedX(cached) TopFed
Query no Execution time(msec) Execution time(msec) S.E Execution time(msec) S.E
1 913 401.2 5.22 341.5* 5.60
2 81619 81170.7 655.93 866.5* 22.08
3 82271 81817.8 653.22 666* 27.12
4 1199 367.6 6.88 262.7* 7.35
5 80423 78723.5 459.43 78691.5 458.70
6 837 416.9 8.38 246.1* 3.56
7 921 399.6 4.41 248.1* 7.20
8 900 89 2.45 72.7* 1.52
9 950.3 76.8 2.16 63.3* 1.89
10 912 63.6 1.99 49.6* 1.02
Average 25094.53 24352.67 180.01 8150.8 53.60
*Significant improvement.
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 16 of 18
http://www.jbiomedsem.com/content/5/1/47
Table 8 Comparison of source selection average execution time (based on a sampling of 10)
FedX(first run) FedX(cached) TopFed
Query no Execution time(msec) Execution time(msec) S.E Execution time(msec) S.E
1 530 11.7 0.35 28.1 0.98
2 487 11.4 0.67 5.2 0.57
3 470 11.9 0.78 5 0.42
4 510 12 0.52 23.6 1.57
5 473 9.8 0.65 4.8 0.29
6 371 9.9 0.38 21.7 0.68
7 521 10 0.39 24.4 0.76
8 483 9.5 0.45 29.5 0.86
9 496 9.8 0.39 20.1 0.99
10 456 10.6 0.40 7.4 0.58
Average 479.7 10.66 0.50 16.98 0.77
on retrieving the results from all of the endpoints in both
the blue and green categories, only to return an empty
set of results, after making a star join with the triple pat-
tern (?recordNo, tcga:RPKM, ?RPKM). We expect that
our approach will generally lead to much faster resolu-
tion for queries of this nature, where a large number of
triples is retrieved for a specific colour category. This
reflects the improvement that TopFeds engine is able to
determine those queries that will return empty sets prior
to requesting the data. Although the benchmark query 5
results in a very large set of triples, the execution time for
both systems is almost the same. As pointed out above,
the reason for this is that the query is too generic and
it is impossible to infer the category colour or tumour
number.
Conclusion
In this work, we have published a Linked Data version
of TCGA data level 3 (to the best of our knowledge the
largest Linked Data dump anywhere) and further linked
it to the LOD cloud. This big data resource is designed
to be used as infrastructure for biomedical and bioinfor-
matics applications that analyse and query both the file
annotations but also the internal content of the patient-
derived files of this key reference for molecular biology
and epidemiology of cancer.
Anticipating usages that traverse to other related big
data resources, we have also generated links to other
LOD data dumps such as HGNC, OMIM and Homolo-
gene. We believe that this RDFication can greatly help
researchers in the biomedical domain as the amount and
diversity of data exceeds the ability of local resources
to handle its retrieval and parsing. The RDFized data
resource can be easily traversed from a modest machine
to investigate a variety of measures at each position of
the genome, across all types of molecular information,
and across all cancer types, without the need to down-
load the files and extract the pieces of information that
satisfy the query. In fact, we would argue that this type of
analysis will eventually be all but impossible for big data
resources like TCGA without RDFication and improved
federation schemes such as those described in this
paper.
The TCGA data dump (and what we expect will be the
genomics datasets in the future) is already too large to be
effectively handled by a single server. If the relationships
between TCGA and other related resources are taken
into account, a smart data distribution framework that
distributes the data among multiple SPARQL endpoints,
such as the one reported here is, an absolute necessity.
This framework, TopFed, is specifically designed as a fed-
erated query processing engine that handles a collection
of physically distributed RDF data sources. The resulting
virtually integrated data resource was observed to enable
significantly faster querying and retrieval (one third) than
current solutions, such as FedX. The TopFed source selec-
tion algorithm achieves this result by considering the
metadata about the data distribution with the type of
the joins among query triples patterns. The substantial
improvements in efficient processing achieved, also in the
use of network traffic, suggests that the development of
systems designed to process an individual patient clinical
data to identify the drugs leading to better outcomes in
related cohorts in TCGA-like resources (e.g., ICGC [59])
is now at hand.
One of our future aims is to develop an intelligent sys-
tem, in which a cancer patients genomic data are used as
input to suggest effective drugs for treatment while com-
paring against results from TCGA patients with the same
or similar cancer sub-types. In 2009, we contributed to
CNViewer [60], a browser based tool that could be used,
via oncologists uploading their own patients copy number
Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 17 of 18
http://www.jbiomedsem.com/content/5/1/47
result, to calculate the Euclidean (or other) distance to all
other patients with the same tumour type. With TopFed,
not only we can calculate these distances using copy num-
ber results, but in future work we expect to use aggrega-
tion/correlation of molecular results to match and better
understand both the biology driving cancer and the most
effective treatment for a patient given a set of genetic
alterations.
Availability of supporting data
The TCGA data is available under the original TCGA
Data Use Certification Agreement [61] and TopFed source
code along with utilities are available under GNU GPL v3
licence at the project home page https://code.google.com/
p/topfed/.
Endnotes
aURLs of SPARQL endpoints hosting five cancer
histologies that are shown in Figure 1 can be found at
http://tcga.deri.ie/.
bA step-by-step user manual is also available at: http://
goo.gl/0oTAKV.
cAvailable to download from: http://tcga.deri.ie/
dumps/.
dSee http://www.w3.org/TR/sparql11-query/ for more
information on federated queries based on SPARQL 1.1.
eTCGA Data Matrix: https://tcga-data.nci.nih.gov/
tcga/dataAccessMatrix.htm.
fPatient barcode format: https://wiki.nci.nih.gov/
display/TCGA/TCGA+Barcode.
gBenchmark queries: http://goo.gl/UxUEXk.
hTopFed index: https://topfed.googlecode.com/files/
loadDistribution.n3.
Abbreviations
TCGA: The cancer genome atlas; LOD: Linked open data; S3DB: Simple sloppy
semantic database; GBM: Glioblastoma multiforme; RDF: Resource description
framework; DHT: Distributed hash tables; GCCs: Genome characterization
centres; GSCs: Genome sequencing centres; TSS-to-Tumour: Tissue source site
to tumour; S.E: Standard error; ICGC: International cancer genome consortium.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MS devised the system, tested it and wrote the manuscript, SSP devised the
queries and helped test the system, ACNN created links to LOD, JSA, SD and
HFD participated in discussions and provided suggestions to improve the
system. HFD wrote the biological motivation and supervised the work. AI used
the system to generate RDF dumps of cancer tumours, hosted few tumours
RDF data to various SPARQL endpoints and contributed to the major revision
of manuscript. All authors read and approved the final manuscript.
Acknowledgements
The authors acknowledge support from the German Research Foundation
(DFG) and Universität Leipzig within the program of Open Access Publishing.
We would like to thank to Bade Iriabho from University of Alabama at
Birmingham for uploading and maintaing all the data in the UAB servers.
Further, we are thankful to Alex Parker from Foundation Medicine for providing
valuable feedback on the results of biological SPARQL query show in Figure 1.
Author details
1Universität Leipzig, IFI/AKSW,PO 100920, D-04009Leipzig,Germany. 2Insight
Centre for Data Analytics, National University of Ireland (NUIG), Galway, Ireland.
3Division Informatics, Department of Pathology, University of Alabama,
Birmingham, USA. 4Foundation Medicine, Inc, Cambridge, MA 02141, USA.
Received: 9 March 2014 Accepted: 3 November 2014
Published: 3 December 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35
http://www.jbiomedsem.com/content/5/1/35
RESEARCH Open Access
Automated reasoning using abduction for
interpretation of medical signals
Marjan Alirezaie* and Amy Loutfi
Abstract
This paper proposes an approach to leverage upon existing ontologies in order to automate the annotation of time
series medical data. The annotation is achieved by an abductive reasoner using parsimonious covering theorem in
order to determine the best explanation or annotation for specific user defined events in the data. The novelty of this
approach resides in part by the systems flexibility in how events are defined by users and later detected by the
system. This is achieved via the use of different ontologies which find relations between medical, lexical and
numerical concepts. A second contribution resides in the application of an abductive reasoner which uses the online
and existing ontologies to provide annotations. The proposed method is evaluated on datasets collected from ICU
patients and the generated annotations are compared against those given by medical experts.
Keywords: Knowledge acquisition, Abductive reasoning, Sensor
Introduction
Medical monitoring of patients is becoming increasingly
device supported and thus large volumes of high fre-
quency data are generated from sensors that monitor
physiological parameters. While the use of such technolo-
gies enables a continuous monitoring, the complexity and
amount of data creates a challenge for the medical staff
to provide interpretations. Furthermore, such interpreta-
tions may be complex as sensor data is inherently uncer-
tain, there may exist interdependencies between physical
parameters, and the data is voluminous and multivariate
[1,2].
Automated analysis and mining techniques have the
potential to support the medical staff in the interpreta-
tion of the data. For time series data analysis this implies
a need for proper annotation of the signals with domain
dependent knowledge in order to facilitate decision mak-
ing and eventual diagnosis. The output generated by the
algorithms should ideally provide information that is com-
patible with the knowledge and the terms used by health
practitioners. In data-driven approaches [3] the labelling
of data is limited to those pre-defined by the engineers
*Correspondence: marjan.alirezaie@oru.se
Center for Applied Autonomous Sensor Systems (AASS), Dept. of Science and
Technology, Örebro University, SE-701 82, Örebro, Sweden
implementing the algorithms. On the other hand, knowl-
edge driven approaches offer the possibility to more
explicitly model the relations between higher level con-
cepts and data. However, these techniques e.g. rule based
methods, also require significant manual effort to encode
domain knowledge.
At the same time, the amount of structured knowledge
in the medical domain is rapidly increasing due in part
by the Linked Data model. This model which is based on
the RDF model [4] allows bodies of knowledge that are
independently structured to be directly interlinked with-
out any further customization efforts. For example,NCBO
BioPortal [5] as a repository of biomedical ontologies con-
tains more than 300 ontologies holding about 5 millions
classes that cover medical concepts including the causes
and symptoms of diseases. The rise of large and shared
machine processable knowledge repositories provides an
opportunity to automate the utilisation of information.
In this paper, we propose a system which is able to
receive as input time series signals and generate as out-
put an annotation of these signals. Domain knowledge is
inputted into the system in a flexible manner allowing the
practitioners to express freely the terms and thresholds
that are relevant for a particular physiological parame-
ter i.e. an event. To enable flexibility, these expressions
are connected to a number of ontologies containing rela-
tions between concepts expressed by the practitioner
© 2014 Alirezaie and Loutfi; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the
Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use,
distribution, and reproduction in any medium, provided the original work is properly credited. The Creative Commons Public
Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this
article, unless otherwise stated.
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 2 of 16
http://www.jbiomedsem.com/content/5/1/35
and observations measured by the various sensors. The
ontologies used are the Symptom ontology as one of
the ontologies in BioPortal [5,6], WordNet [7] and the
Semantic Sensor Network (SSN) ontologya. The symp-
tom ontology provides the medical terms and definitions
defined as concepts in a hierarchy of subsumption rela-
tions which are used in the annotations of the sensor
data. The WordNet ontology which consists of a lexical
database of the English language enables finding relations
between the concepts in BioPortal and those defined by
the practitioner. The SSN ontology is used to link the spe-
cific sensors to physiological parameters, and provide a
standardized representation of sensors, observations and
related concepts.
The reasoning process used in this paper which finds the
relations contained in the different ontologies, is abduc-
tive. Abductive reasoning is chosen as it is non-monotonic
and thus differs from deductive reasoning in that a logi-
cally certain conclusion is not guaranteed. Rather, abduc-
tive reasoning infers the best possible explanation given
a set of observations. Techniques such as Parsimonious
Covering Theory (PCT) or diagnostic reasoners which are
abductive are often used in the medical domain [8] as
they promote explicitation, and can contend with uncer-
tainty by assessing the likelihood that a specific hypothesis
entails a given conclusion [9].
This paper whose main focus is more on the reason-
ing method and its scalability and less on the auxiliary
techniques such as Natural Language Processing (NLP)
used, evaluates the use of existing ontologies and abduc-
tive reasoning to annotate sensor data from ICU patients.
One benchmark dataset provided for use in 1994 AI in
Medicine symposium submissions [10] and one dataset
collected at a local hospital (Section DataSets) are used
in the experimental analysis. The annotations generated
by the proposed approach are compared against the anno-
tations made by experts. Also, the complexity of the
reasoning method is evaluated.
The paper begins with a description of related works
in Section Related work. The Linked Data model and
Abductive Reasoning are then shortly introduced in
Section Background. We explain the details of the frame-
work in Section Method and then discuss the results
of the reasoner and evaluate the frameworks output in
Section Results and discussion. The paper ends with the
conclusion and discussion in Section Conclusion.
Related work
In the literature, research whose goal is to use knowl-
edge driven methods to annotate time series data is found
in various fields in artificial intelligence that include sen-
sor data enrichment [11,12], data stream annotation [13],
symbol grounding [14,15], and semantic perception [16].
Such works share the common feature where symbolic
knowledge is integrated to the numeric data processing.
Often high level symbolic knowledge is manually encoded
based on the requirements of the problem rather than
(re)using existing knowledge already modelled in e.g.,
ontologies (RDF graph model). For example, [16] and
[17] have proposed reasoning techniques based on abduc-
tive reasoning for data stream annotation using manually
encoded knowledge. These works including [18] imple-
mented in OWL use PCT for inferring the best possible
explanation. However, the reasoner is restricted to gener-
ate explanations with only one cause. The work presented
in [19] implements an automated reasoning which is sim-
ilar to our work in the sense that the knowledge base
consists of a RDF/OWL ontology. However, in our work,
we propose an automated reasoning over external ontolo-
gies modelled by different experts. Furthermore, the PCT
based reasoner in our work overcomes the constraint of
providing an explanation containing more than one cause
for the observations. This approach builds upon previous
work [20] and has formalized the reasoning process and
extended the experimental evaluation.
Background
In this section we introduce preliminary features of the
Linked Data model and abductive reasoning.
Linked data
Exploiting human knowledge for commonsense and auto-
mated reasoning has always been a challenge. The fast-
growing Webb which has traditionally been populated
with HTML documents is known as the biggest reposi-
tory of human knowledge in different domains. However,
despite the fact that contents of this repository are acces-
sible in the form of pages, due to the lack of semantic
interconnection among them, it is impossible for an arti-
ficial agent to retrieve a specific concept. Therefore, the
first step towards automatically using the content of Web
pages is structuring these contents so that they become
interlinked and can be queried in different levels of
abstraction.
Linked data which refers to a set of structured data,
namely global data space, has become a paradigm pro-
viding the transition from document oriented Web into a
web of interlinked data [21]. According to this paradigm,
unstructured information represented in web pages is
mapped into the RDF graph which is understood as a
set of subject-predicate-object triples, T = (S ,P ,O) [4].
Given U as a set of dereferenceable URIsc and L as a set
of literals such as numbers or strings, the aforementioned
RDF triple is defined as T ? U × U × (U ? L). In other
words, all subjects and predicates are URIs and objects
are either a URI or a literal value. Similarly, stating the set
Q = (V ? U) × (V ? U) × (V ? U ? L), where V as a set
of variables is ranging over (U ? L), we can redefine the
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 3 of 16
http://www.jbiomedsem.com/content/5/1/35
triple T as an element of the query set Q. More specifi-
cally, instead of feeding search engines with search terms,
it is possible to fetch the desired set of triples by writing
a query which is equivalent to the finite set of triples Q.
Eventually, an answer for this query is simply achieved by
binding variables of the query triples into (U ? L).
Different languages such as RDFS and OWL comply-
ing with the Linked Data model, provide different levels of
expressivity. Regardless of the implementation language,
however, it is the uniformity and the integrability features
of the Linked Data model that make the integration of
different linked datasets straightforward.
However, despite its unified structure, there are number
of issues with linked data that pose a challenge for auto-
mated reasoning [22]. For instance, in order to query large
size linked datad, the query process needs to deal with the
problem of localizing relevant parts in linked data.
In this paper, a biomedical repository called BioPor-
tal [5] is used. Using a similar data model as the Linked
Data model, BioPortal contains more than 300 ontologies
ranging in subjects from anatomy, phenotype descrip-
tion, to health [6]. Further details about dealing with
the aforementioned issue of size are discussed in Section
Hypothesis extraction.
Abductive reasoning with PCT
Reasoning processes are categorized into two main
groups, monotonic and non-monotonic reasoning.
Monotonic reasoning including deductive reasoning
implies that inferring a new piece of knowledge does
not change the set of already known information. Non-
monotonic reasoning, on the other hand, states that
adding more knowledge can invalidate current conclu-
sions. In diagnostic medical procedure where symptoms
of a disease gradually emerge, monotonic reasoning due
to the permanence of its results, are less favourable. Since
all the symptoms of a disease do not occur at a same time,
the reasoner needs to be able to deal with incomplete data
throughout the reasoning process. Incompleteness may
also extend to the high level models e.g. ontologies which
may also be dynamically changing. A non-monotonic
reasoning process whose set of answers can later be
updated is therefore useful in domains such as medicine
and industrial diagnosis process [23].
There are different models of non-monotonic reason-
ing such as default reasoning, autoepistemic logic, belief
revision and abductive reasoning [23]. In this work, we
selected abductive reasoning with the ability of deriv-
ing the best (most likely) explanations out of known
facts. Abduction as the backbone of commonsense rea-
soning and has increasingly been applied in diagnosis
systems (medical domains) [24]. Diagnostic reasoning in
particular, is based on abductive logic and represents
the knowledge within a network of causal associations.
The hypothesis-and-test approach of diagnosis reason-
ing shows its non-monotonic behaviour where the set of
plausible causes of the observed behaviour can change
whenever the observation set extends. Parsimonious Cov-
ering Theory (PCT) is a model of diagnostic reasoning [8]
used in this work.
PCT formalization is based on set theory and is defined
within a quadruple T = (O,M,H, E), where O is the set
of all observations which are either qualitative or quantita-
tive objects;M states the set of all manifestations (events)
detected over observations; subsequently, H contains all
hypotheses defined as possible causes that are in rela-
tions with expected events. Finally, E is the solution set
indicating inferred explanations for items of M. More
specifically, the inference process is about drawing E ? H
as an explanation for elements of M ? O. However, the
formalization is not complete in that it does not formal-
ize the best explanation. For this, PCT suggests various
criteria to select the final result set E . Two widely used
criteria are:
 Set covering criterion is defined as a property of a
function f which is assumed to be a mapping from a
subset ofH (set of all hypotheses) to a subset ofM
(set of manifestations) so thatX is a possible cause for
f (X ). An accepted conclusion w.r.t the set-covering
criterion is set X ? H such that f (X ) = M.
 Minimum cardinality criterion is concerned about
the cardinality of the solution set. According to this
criterion,R as a subset ofH is chosen as the solution
set if for all other covering subsets ofH, namely S ,
|R| ? |S|.
As previously mentioned, PCT is based on set the-
ory. The eventual explanation is a subset of the of the
Hypothesis set for which aforementioned criteria hold.
Selecting a subset poses an issue of the time complex-
ity. Consequently, there are a number of techniques that
address computational factors for making abductive rea-
soning NP-Hard [25]. For instance, applying constraints
that reduce the composite hypothesis set size as well as
ruling out criteria-violating candidates (and their super
classes) from the power set, can reduce the time complex-
ity. Techniques used in this work are further discussed in
Section Reasoner.
Method
The reasoner depicted in Figure 1 receives two primary
inputs, Hypothesis (H), andManifestation (M) which are
separately provided by the HypothesisExtractor and the
ManifestationExtractor processes, respectively. The out-
put of the reasoner is called Explanation (E). Each com-
ponent feeding the reasoner contains several modules that
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 4 of 16
http://www.jbiomedsem.com/content/5/1/35
Figure 1 Sensor data annotation framework based on abductive reasoning.
collaborate with ontologies including the SSN ontology
and theWordNet ontology.
Considering the PCT quadruple T = (O,M,H, E)
explained in Section Abductive reasoning with PCT,
we then follow the reasoning process of the framework
by mapping the main elements of PCT into outputs of
different components.
Configuration
The framework illustrated in Figure 1 is based on a config-
uration file which is filled by the expert of the domain. The
configuration file contains details of (possible) behaviours
of signals in which the expert is interested to monitor. To
illustrate the method in the paper, we will use a running
example of configuration files shown in Figure 2, 3 and
4. For instance, Figure 2 is about a situation where the
expert is interested to observe the heart rate, amount
of oxygen saturation and blood pressure. There is also
a section in the configuration file in which the expert,
by setting a range of values, can specify a significant
behaviour for physiological terms.
The SSN ontology is populated only with the contents
of the configuration file. There is an equivalent class or
property in SSN, for each item (key/value pair) mentioned
in the configuration file. The value of a key in the file
is used as a name of a class in SSN. Given FeatureOfIn-
terest and Property as concepts defined in SSN and the
function valueOf(key) which returns the value of a key in
the configuration file, the SSN ontology is populated as
follows:
? F , ? P , ? B (F = valueOf (feature_of _interest),
P = valueOf (property),
B = valueOf (Behaviour),
min = minValueOf (Behaviour),
max = maxValueOf (Behaviour)
?
B_P  P  ssn:Property
F  ssn:FeatureOfInterest  (?ssn:hasProperty. B_P)
F_P_Sensor  ssn:Sensor  (?ssn:observes. P))
B_P_SensorOutput  ssn:SensorOutput
(?ssn:isProducedBy. F_P_Sensor)
(?ssn:hasValue. B_P_Value)
B_P_Observation  ssn:Observation
(?ssn:observationResult. B_P_SensorOutput)
(?ssn:FeatureOfInterest.F)
B_P_Value  ssn:ObservationValue
B_P_MinValue ? B_P_Value
B_P_MaxValue ? B_P_Value
(B_P_MinValue,min) ? hasQuantityValue
(B_P_MaxValue,max) ? hasQuantityValue
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 5 of 16
http://www.jbiomedsem.com/content/5/1/35
Figure 2 Configuration file sample I (related to an infant patient).
For example, the SSN ontology populated with the con-
tent of Figure 2 will contain the following axioms:
Slow_Rate  Rate  ssn:Property
Heart  ssn:FeatureOfInterest  (?ssn:hasProperty. Slow_Rate)
Heart_Rate_Sensor  ssn:Sensor  (?ssn:observes. Rate))
Slow_Rate_SensorOutput  ssn:SensorOutput
(?ssn:isProducedBy. Heart_Rate_Sensor)
(?ssn:hasValue. Slow_Rate_Value)
Slow_Rate_Observation  ssn:Observation
(?ssn:observationResult. Slow_Rate_SensorOutput
(?ssn:FeatureOfInterest.Heart)
Slow_Rate_Value  ssn:ObservationValue
Slow_Rate_MinValue ? Slow_Rate_Value
Slow_Rate_MaxValue ? Slow_Rate_Value
(Slow_Rate_MinValue, 0) ? hasQuantityValue
(Slow_Rate_MaxValue, 157) ? hasQuantityValue
The configuration file allows expert to enter values
which denote either a normal or an abnormal behaviour
in signals. For example, in Figure 2 and 4 we can find
the definition of abnormal and normal behaviours, respec-
tively. In the experimental validation in Section Results,
we show that the eventual explanations are not literally
dependent on the content of the file. More specifically, the
signal explanation process results in same interpretation
for variations of terms used by the expert.
Hypothesis extraction
According to PCT, the Hypothesis set is defined as a set
of facts that represent relations between expected events
and their causes. The SemanticAnalysermodule (Figure 1)
initializes the process resulting in the Hypothesis set. This
module collaborating with public ontologies is responsible
for retrieving a hierarchy of related concepts formatted in
RDF/OWL.
Before going to the details of the Hypothesis Extraction,
we first explain how we deal with localizing the relevant
parts in Bioportal. The goal of the system is annotating
medical signals that contain abnormal behaviours, (i.e.,
symptoms of diseases). SemanticAnalyser queries for the
term symptom in the NCBO BioPortal. The results of
this query is 21 records out of which 15 items belong to the
Symptom ontology, as a sub ontology in BioPortal. There-
fore, due to its high rank, the Symptom ontology is chosen
as a reference ontology.
The symptom ontology illustrated in Figure 5, has been
modelled to capture signs and symptoms of diseases and
provides well-categorized medical symptoms in terms of
body part names. Due to its structure, the symptom ontol-
ogy is only used for retrieving the hierarchy of symptom
concepts modelled based on subsumption relations. Run-
ning the following SPARQL querye, the SemanticAnalyser
module retrieves a hierarchy of symptoms in terms of
subclasses of the symptom class:
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
SELECT DISTINCT ?sub ?labSub
FROM <http://bioportal.bioontology.org/ontologies/SYMP>
WHERE {
?super a owl:Class .
?super rdfs:label ?label .
?sub rdfs:subClassOf ?super.
?sub rdfs:label ?labSub.
FILTER regex(?label, "symptom")
}
Figure 3 Configuration file sample II (related to the same infant in Figure 2).
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 6 of 16
http://www.jbiomedsem.com/content/5/1/35
Figure 4 Configuration file sample III (related to an adult patient).
SemanticAnalyser, then searches through the set of
symptom classes in order to select relevant symptoms.
The relevant symptoms are those ones that are related
to parts of the body (feature_of_interest) observed by
sensors e.g., heart and blood in case of the config-
uration file in Figure 2, or heart, blood and res-
piratory system in case of the configuration file in
Figure 4. In order to find the relevant symptoms, each
symptom type passes the two phases of tokenizingf and
stemmingg. As shown in Figure 1, the SemanticAnal-
yser module uses the WordNet ontology that contains
synonym/pertainymh set of words and acquires the syn-
onym/pertainym set of each token of a symptom type.
Consequently, each symptom type (split into its tokens)
is assigned with multiple synonym/pertainym lists corre-
sponding to its tokens. The number of times that each
physiological parameter (the feature_of_interest value)
appears in the synonym set of each token is counted.
Finally, a symptom type whose tokens have the highest
total count is chosen as the top candidate which has
the highest similarity to the feature_of_interest. Table 1
shows all categorized symptom types along with the body
parts name for different configuration files. For exam-
ple, the cardiovascular system symptoms is chosen due
to its highest relevance to the heart as a feature_of_
interest.
The final Causes set shown in Figure 1 is the union of
all subclasses of the candidate symptom types returned
per each feature_of_interest. In Table 2, cause items as
the output of the SemanticAnalysermodule are listed. The
first 62 items and the total 89 items are considered as
causes related to configurations in Figure 2 and Figure 4,
respectively. As we see in Table 2, each cause can be a sin-
gle term (e.g., hypoxemia) or a combination of terms (e.g.,
atrial fibrillation). The definition of each single cause
term is retrieved from either the Symptom ontology or the
WordNet ontology (in case the former returns nothing) to
be replaced with the cause item.
Figure 5 An excerpt of the symptom ontology: cardiovascular, hemic and respiratory symptoms.
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 7 of 16
http://www.jbiomedsem.com/content/5/1/35
Algorithm 1 Similarity Matrix
Require: Causesn×1, BehaviourListm×1, Sn×m = 0
for i ? 1 to n do
tree ? getGrammaticalTree(Cause[i])
phrases ? getAllPhrases(tree)
if size(phrases) > 0 then
for k ? 1 to size(phrases) do
JJ ? phrases[ k] .getAdjective();
N1 ? phrases[ k] .getNoun1();
N2 ? phrases[ k] .getNoun2();
for j ? 1 to m do
if BehaviourList[j].getBehaviour() ? SynSet(JJ)
and BehaviourList[j].getProperty() ? SynSet(N1)
and BehaviourList[j].getFeatureOfInterest() ?
SynSet(N2) then
S[i, j] ? 1
end if
end for
end for
end if
end for
return S {//Similarity Matrix}
The Hypothesis set is generated by the SignalMap-
per module. The SignalMapper takes as input the set
of causes. It selects a subset of these causes based on
parameters mentioned in the configuration file. Specif-
ically it looks at the terms used to define behaviours.
For example, possible behaviours for a specific sig-
nal are defined as fast, slow and irregular. The
SignalMapper concatenates the values of behaviour, fea-
ture of interest and property to generate a list of phrases
such as irregular heart rate, low oxygen saturation (see
Table 3). For those configurations where the expert states
the normal behaviour, e.g., Figure 4, the term not is
added in front of the concatenated phrase, e.g. not nor-
mal respiratory rate. For phrases preceded by not an
antonym set is retrieved fromWordNet.
As the next step towards generating the Hypothesis
set, the SignalMapper process builds an n × m similar-
ity matrix S, where n and m are the number of cause
items and the number of possible behaviours, respec-
tively. The similarity matrix S which is initialized to zero,
will hold the similarity values between elements of these
two lists (Algorithm 1). For calculating the similarity val-
ues, the cause items need to be grammatically analysedi.
For instance, for each cause item, grammatical roles of
its terms such as noun (NN) or adjective (JJ) are
identified. All causes will therefore have their own gram-
matical tree by running the grammatical analysing process
over rows of the matrix. In order to set the value of ele-
ment si,j of matrix S, the process first needs to generate
the grammatical structure tree of the ith cause and then
to check whether this cause is related to an behaviour j.
For this, all adjectives (JJ) with their own substantives
(NN) of the ith cause item are retrieved. Each substantive
(called noun1) is also checked to see if it is related (e.g.,
via a preposition or a connective) to another noun (called
noun2). If such a combination is found in a cause item, at
the next step, the synonym/pertainym sets of the adjec-
tive, noun1 and noun2 are also retrieved to be checked
against the column side items. The value of si,j increases if
the following three conditions are met: (SynSet(K) refers
to the synonym/pertainym set of term K, cj refers to the
jth column and ri refers to the ith row)
Behaviour(cj) ? SynSet(Adjective(ri))
Property(cj) ? SynSet(Noun1(ri))
FeatureOfInterest(cj) ? SynSet(Noun2(ri))
Table 1 List of symptoms retrieved from the symptom ontology
Related to Figure 2 Related to Figure 4
Symptom category Heart Blood Heart Blood Respiratory
Abdominal symptoms 0 0 0 0 0
Head & neck symptoms 0 0 0 0 0
Musculoskeletal system symptoms 0 0 0 0 0
Neurological & physiological symptoms 0 0 0 0 0
Reproductive system symptoms 0 0 0 0 0
Skin & integumentary tissue symptoms 0 0 0 0 0
Digestive system symptoms 0 0 0 0 0
Cardiovascular system symptoms 1 0 1 0 0
Hemic system symptoms 0 1 0 1 0
Nervous system symptoms 0 0 0 0 0
Nutrition, metabolism symptoms 0 0 0 0 0
Respiratory system & chest symptoms 0 0 0 0 1
Urinary system symptoms 0 0 0 0 0
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 8 of 16
http://www.jbiomedsem.com/content/5/1/35
Table 2 List of causes for three different symptom types
# Cause Symptom group Body part
1 Arrhythmia Cardiovascular System
Heart
2 Atrial fibrillation Cardiovascular System
. . . . . . . . .
30 Postphlebitic ulcer Cardiovascular System
31 Hypoxemia Hemic System
Blood. . . . . . . . .
62 Cyanosis Hemic System
63 Tachypnea Respiratory System
Respiratory. . . . . . . . .
89 Dyspnea Respiratory System
Figures 6 and 7 illustrate two samples of a grammatical
structure tree for two causes and their relations with two
behaviours. The matrix element referring to arrhythmia
and irregular heart rate will be set to 1 due to the match-
ing terms found between them (Figure 6). Likewise, after
running the process of Algorithm 1, the matrix element
referring to tachypnea and not normal respiratory rate
is set to 1 (Figure 7).
After calculating the elements value of the matrix S, the
SignalMapper chooses non zero elements showing a rela-
tion between causes and behaviours. The Hypothesis set
(H) as the first input of the reasoner (Figure 1) is cre-
ated by pairs of row-column items of non-zero elements in
the matrix S. Table 4(a) and Table 4(b) partially show two
retrieved Hypothesis sets based on the two configurations
in Figure 2 and Figure 4, respectively.
Manifestation extraction
TheManifestationExtractor component is responsible for
the signal analysis process. This component contains a
module called SignalAnalyser (Figure 1) which performs
the event detection process. Using the SSN ontology
which is only populated with the configuration informa-
tion, the Signal Analyser detects those parts of signals that
Table 3 Possible abnormal behaviours
(a) Based on Figure 2 (b) Based on Figure 4
# Abnormal behaviour # Abnormal behaviour
1 Slow heart rate 1 Slow cardiac system pulse
2 Fast heart rate 2 Rapid cardiac system pulse
3 Irregular heart rate 3 Abnormal cardiac system pulse
4 High blood oxygen 4 Not normal respiratory rate
5 Low blood oxygen 5 Elevated blood pressure
6 High blood pressure 6 Low blood pressure
7 Low blood pressure
contain an abnormal behaviour mentioned in the config-
uration. An event (or an abnormal behaviour detected in
a signal) is defined based on threshold values set by the
expert of the domain according to sampling rate of signals
and the patient profile (age, gender, etc.). For example, in
Figure 2, the Behaviour section related to heart shows
the range of heart rate values as < 157 AND > 175
which is set by the expert to monitor the situation of an
infant to detect an irregular heart behaviour. The expert
would enter different values in case of an adult patient. For
instance, the upper bound of the slow heart rate for an
infant is set to 157 (Figure 2) while the same behaviour for
an adult patient is set to 60 (Figure 4).
The applied data analysis method divides signals into
several segments. A segment is created based on the num-
ber of events (set as threshold values defining a numeric
range) detected in each signal. The division process is
done within an iterative process which looks for events
in each signal and determines a set of temporal intervals
in which a number of events are included. The iterative
process starts by creating a temporal segment in the first
signal whose length is set based on the minimum required
number of events in the signal. More precisely, the starting
time point of the initial segment is the same as that of the
signal, and its ending point is when theminimum required
number of events in this signal has been met. Detecting a
new event affects the size and the number of created inter-
vals. The iteration ends whenever the size of intervals do
not change. At the end, these intervals are considered as
segments. The reasoner will then be applied on each seg-
ment separately. Therefore, the threshold values set by the
expert enables him/her to have some segments in which,
for example, one signal has no event while the others do.
Although the data analysis method can affect the even-
tual interpretation results, it is the representation tech-
nique which, in this work, is at focus. In Section Results,
examples of the threshold values set for a configuration is
given.
The output of the ManifestationExtractor component
is a set of Manifestations (M) at each segment, which is
a list of time points at which events are detected. The
Manifestation set is the second input of the reasoner
(Figure 1).
Reasoner
The reasoner module is based on Parsimonious Covering
Theory (PCT) as an abductive reasoning method whose
basis is on the set theory. The main feature of this rea-
soner is finding the best possible Explanations (E) for
the set of Manifestations (M) detected at each segment
of signals. More precisely, given the Hypothesis set (H)
which is the set of the cause/abnormal_behaviour pairs,
the reasoner calculates the power setk of the causes set.
Final Explanations are those members of the power set
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 9 of 16
http://www.jbiomedsem.com/content/5/1/35
Figure 6 Grammatical parsing tree and relation between arrhythmia as a cause item and abnormal heart rate as a behaviour.
(or subsets of the causes set) which do not violate the
reasoners principles.
The principles of the reasoner are defined within two
criteria: Covering and Minimality. According to the first
criterion shown in (1), the reasoner nominates those sub-
sets of the causes set (C) that are related to all Manifes-
tations. In other words, the covering set indicates a set
of subsets of causes with the aforementioned specifica-
tion. Moreover, the concern of the minimality criterion
(2) is the size of the selected subset. Complying with
aforementioned criteria, the reasoner finds the best pos-
sible explanations which are those covering subsets of the
causes (as part of the Hypothesis set) that are minimal in
terms of the cardinality. Algorithm 2 shows the details of
the reasoner.
Covering = {K ? C | ?m ? M, ? c ? K : (c,m) ? H}
(1)
Minimality = {c ? Covering |  ? d : (d ? c ? d ? Covering)}
(2)
Algorithm 2 Abductive Reasoning
Require: Causes, Observations, Relations
{//Removing non-participant causes}
relevantCauses? getRelevantCauses(Causes,
Relations)
explanations ?null
powerSet ? getPowerSet(relevantCauses)
for all ps in the powerSet do
if isCovering(ps,Observations) then
if isIrredundant(ps,Observations) then
addExplanation(ps, explanations)
else
removeSuperSet(ps, powerSet) {//Removing the
supersets of ps}
end if
end if
end for
return explanations
The reasoning complexity, due to the power set calcu-
lation, grows exponentially w.r.t the number of causes. In
Figure 7 Grammatical parsing tree and relation between tachypnea as a cause and not normal respiratory rate as a behaviour.
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 10 of 16
http://www.jbiomedsem.com/content/5/1/35
Table 4 List of hypotheses
(a) Hypothesis related to the configurations in Figure 2
# Cause Abnormal behaviour
1 Arrhythmia Irregular heart rate
2 Bradycardia Slow heart rate
3 Tachycardia Fast heart rate
. . . . . .
6 Hypertension High blood pressure
7 Hypotension Low blood pressure
. . . . . .
18 Hypoxemia Low blood oxygen
(b) Hypothesis related to the configurations in Figure 4
# Cause Abnormal behaviour
1 Arrhythmia Abnormal cardiac system pulse
2 Bradycardia Slow cardiac system pulse
3 Tachycardia Rapid cardiac system pulse
. . . . . .
6 Hypertension Elevated blood pressure
7 Hypotension Low blood pressure
. . . . . .
20 Tachypnea Not normal respiratory rate
21 Bradypnea Not normal respiratory rate
order to reduce the size of the power set, two steps indi-
cated in Algorithm 2 are applied. The first step filters the
set of causes by removing those causes that are not listed
in pairs of the Hypothesis set. At the second step, super
classes are removed for elements of the power set where
the minimality criterion is violated.
The output of the reasoner is the set of Explanations for
observations.
Results and discussion
DataSets
In order to evaluate the framework, we use two different
sets of multivariate medical data. The first dataset con-
tains 12-hours of time-series data from a set of medical
sensors measuring heart rate, arterial pressure, and arte-
rial oxygen saturation of an infant in an Intensive Caring
Unit (ICU). This patient is suffering from several diseases,
namely multiple liver abscesses, portal hypertension
and E. Coli sepsis, used as the ground truth for the eval-
uation of the final explanations suggested by the reasoner.
This package of data is the ICU data package provided
for use in 1994 AI in Medicine symposium submissions
[10]. The second dataset also contains multivariate data
from three sensors measuring heart rate, respiratory rate
and blood pressure of an adult patient in a Critical Caring
Unit (CCU) who is suffering from congestive heart fail-
ure (CHF). This package is provided by the caring unit
section of a hospitall.
Results
In this section, we discuss about the experiments which
are based on two different configurations and two dif-
ferent datasets. The first experiment is related to the
configurations in Figure 2 and the infant patient data
introduced above. The second experiment is based on the
configurations set in Figure 4 and the adult patient data.
Finally, the scalability of the reasoner is also evaluated base
on different configuration parameters such as: number
of feature_of_interests (F ), size of the Causes set (|C|),
number of abnormal behaviours (B) and distinct number
of causes in the Hypothesis set (|Hc|).
Experiment I
Figure 2 shows the configurations used in this experi-
ment for monitoring the heart and blood situation of a
patient. The properties of interest are rate (rate of heart),
pressure (pressure of blood) and oxygen (amount of
oxygen in blood). As mentioned in Section Hypothe-
sis extraction, in order to find the relevant symptoms,
each symptom type listed in Table 1, is assigned with
the synonym/pertainym list of its tokens. For example,
the set of tokens of the first symptom types (abdominal
symptoms) is [abdomen, symptom ]whose elements are
assigned to their synonym/pertainym list:
abdomen ?? { venter, stomach, belly}
symptom ?? { indication, evidence, gesture, mark, point,
...}
Since there is no match between items of the above
lists and the two physiological parameters (heart and
blood), the value of the abdominal symptoms item is
set to zero. However, the 8th item, cardiovascular system
symptoms, is tokenized as [cardiovascular, system, symp-
tom]. Focusing on the first token, the synonym/pertainym
list is:
cardiovascular ?? {cardiac, heart}
The score of the item cardiovascular system symptoms,
related to the heart, hence, increases to 1. The hemic
system symptoms item, in a same way, gets 1 score since
the pertainym of hemic is the term blood. Therefore,
the selected symptoms indicated in Table 1 are those ones
that are related to the cardiovascular system and hemic
system symptoms due to their highest similarity values to
the feature_of_interests set in the configuration file.
The list of 62 cause items (|C| = 62) which are subclasses
of the selected symptom types (cardiovascular system
symptoms and hemic system symptoms) are only par-
tially shown in Table 2. Furthermore, the list of all possible
behaviours mentioned in the configuration file (Figure 2),
that created by the SignalMapper module, is depicted in
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 11 of 16
http://www.jbiomedsem.com/content/5/1/35
Table 3(a). As we can see, the process of concatenating
behaviour, feature_of_interest and property values
results in 7 phrases indicating different behaviours (B =
7). For example, the first item, slow heart rate is gener-
ated by concatenating the term slow as behaviour, the
term heart as feature_of_interest and the term rate as
property.
In order to achieve the Hypothesis set (H) whose ele-
ments are the pairs of cause / abnormal_behaviour, the
SignalMapper process, at its next step, creates a 62 × 7
similarity matrix S initialized to zero. The updated value
of the element si,j will indicate the relation between the
ith cause and the jth behaviour. As mentioned before,
for each cause item whose definitions has been retrieved
from the symptom or the WordNet ontology, a grammat-
ical structure tree holding the grammatical role of each
term in the sentence, is generated. Finding the similarity
between causes and abnormal behaviours implies a need
for checking if a similar phrase to an abnormal behaviour
is detected within a cause item (Algorithm 1). For exam-
ple, the first cause item (Table 2), arrhythmia, is defined
as an abnormal rate of muscle contractions in the heart.
As we see in the grammatical tree of this cause illustrated
in Figure 6, there is an adjective (abnormal) whose sub-
stantive (rate) is also related to a noun, heart (via a
preposition, in). This cause item is found similar to the
third behaviour (irregular heart rate) since:
Behaviour(c1) = abnormal ? SynSet(Adjective(r3) = irregular)
Property(c1) = rate ? SynSet(Noun1(r3) = rate)
FeatureOfInterest(c1) = heart ? SynSet(Noun2(r3) = heart)
Therefore, the element s1,3 is set to 1. Follow-
ing Algorithm 1, the similarity matrix S will finally
contain 18 non-zero values referring to 18 pairs
cause/abnormal_behaviour that creates theHypothesis set
(|H| = 18) (Table 4(a)). Counting the number of causes,
we find 11 distinct items out of 18 in this list (|Hc| = 11).
Therefore, during the reasoning process, where the power
set of the causes set is generated, the reasoner needs to
deal with the power set with the size of 211.
The SignalAnalyser detects abnormal behaviours of
data and represents them as items of the Manifesta-
tion set (M) for each segment of data. The applied data
analysis method divides signals into several segments
which as explained in Section Manifestation extraction,
are defined based on the desired number of events at
each signal as well as the sampling rate of the signal.
Figure 8 shows three signals related to the configurations
in Figure 2. The threshold value for the Arterial Pressure
Figure 8 Segmentation result over 12-hours data (the first dataset).
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 12 of 16
http://www.jbiomedsem.com/content/5/1/35
Signal has been set as 25 ? n ? 60 meaning that a
segment needs to have at least 25 and at most 60 arte-
rial pressure events. Similarly, the threshold values for the
Arterial O2 Saturation and the Heart rate are 2 ? n ?
15 and 5 ? n ? 20, respectively. According to these
threshold values, signals in Figure 8 are divided into 3
segments.
Given the two sets Hypothesis (H) and Manifestation
(M), the reasoner separately provides inferred Explana-
tions for each segment shown in Table 5. For the patient
of the first dataset, 6 distinct diseases (explanations) have
been found (Table 6(a)). By calculating the probability of
occurrence for each disease, the soundness of the rea-
soner outputs is evaluated. The Occurrence probability
is defined as the ratio of the number of times a disease
has been seen to the number of different explanations
observed for a segmentm. According to Table 6(a), the
first (hypertension) and the forth (Septic Shock) items
are matched with the diseases mentioned in the patient
profile (portal hypertension and E. Coli sepsis) with
the probability of 100% and 33%, respectively. In addition,
other items which are discovered by the reasoner but are
not mentioned in the patient profile such as tachycar-
dia and hypertension are in the literature considered as
a sign of Sepsis [26]. Therefore, if we also count these
combinations as sepsis, as shown in Table 6(b), the true
positive diseases are the two first ones in the ordered list.
The false negative case which exists in the patient pro-
file but has not been inferred by the reasoner is liver
abscesses. This liver dependent disease to be diagnosed,
most likely requires other types of sensors information in
order to be detected.
Experiment II
In this section, we continue the experiments with the
second dataset and present results of the reasoner for sit-
uations where the expert uses the negation concept in the
configuration file. As mentioned in Figure 4, the expert
decided to monitor the heart rate, the blood pressure and
the respiratory rate of the patient. Before going to the
details, we examine the results of the HypothesisExtrac-
tion component for this case.
Table 5 Manifestations shown in Figure 8
Seg# Manifestations Explanations
1 Fast heart rate (Hypertension,hypoxemia,palpitation)
Low blood oxygen (Hypertension, palpitation,hyperemia)
High blood pressure (Hypertension,hypoxemia,septicShock)
(Hypertension,hyperemia,septicShock)
(Hypertension,hypoxemia,tachycardia)
(Hypertension,hyperemia,tachycardia)
2 Same as segment 1 Same as segment 1
3 Same as segment 1 Same as segment 1
Table 6 Occurrence probability
(a) (b)
# Disease Probability # Disease Probability
1 Hypertension 100% 1 Hypertension 66%
2 Hypoxemia 50% 2 Septicshock 66%
3 Hyperemia 50% 3 Hypoxemia 50%
4 Septicshock 33% 4 Hyperemia 50%
5 Palpitation 33% 5 Palpitation 33%
6 Tachycardia 33%
Candidate symptoms for the second dataset in Table 1
are cardiovascular system, hemic system and respi-
ratory system symptoms. The entire subclasses of these
three concepts in the Synonym ontology contain 89 causes
(|C| = 89) shown in Table 2. Moreover, for configura-
tions in Figure 4, there are 6 possible abnormal behaviours
(B = 6) (see Table 3(b)). One of these items, not normal
respiratory rate, is the phrase with negation for which
the antonym set rather than the synonym set is retrieved
from the WordNet ontology. SignalMapper, then, creates
a 89 × 6 similarity matrix in order to prepare the Hypoth-
esis set. Table 4(b) shows 21 relations (|H| = 21) out of
which 17 cause items (|Hc| = 17) are distinct. Therefore,
the reasoner has only to deal with 217 elements of the
power set.
Due to the threshold values set for the segmentation
process, the signals which are the results of 4 days of
observation with the sampling rate of once per hour, is
divided into 1 segment. Shown in Figure 9, the thresh-
old values for the heart rate, respiratory rate and blood
pressure are set as, 1 ? n ? 25, 50 ? n ? 70 and
30 ? n ? 55, respectively. The inferred Explanations
are shown in Table 7.
It is worth mentioning that for the first dataset, since
the cardinality of all inferred Explanations at each seg-
ment were the same (3 items for each explanation), we
did not consider the minimality criterion. However, for
the second dataset, since the reasoner results in explana-
tions with different sizes, the evaluation will be different.
As shown in Table 7, the first two explanations holds
the minimality criteria of the reasoner, heart failure and
dyspnea. The first one is matched with CHF, the disease
the patient is suffering from. Furthermore, the second one,
dyspnea, is considered as amain sign of heart failure in the
literature [27].
Experiment III
The purpose of the following experiment is to examine
the performance of the reasoner given various inputs.
For example, given a larger hypothesis set, the reasoner
spends more time on the processing of the power set cal-
culation. In the following, we momentarily disregard the
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 13 of 16
http://www.jbiomedsem.com/content/5/1/35
Figure 9 Segmentation result over 4-days data (the second dataset).
time for segmentation of the signals (as this is indepen-
dent of the configurations) and we represent the reasoning
time for different configurations in order to study the scal-
ability of the reasoner and the impact of the parameters in
a configuration file on the reasoning performance.
Recall that the final explanation is retrieved from the
Hypothesis set (H) which is as such extracted from the
Cause set (C). As said in Section Hypothesis extrac-
tion, the cause items are the union of the subclasses of
the candidate symptom types. The candidate symptom
types are also chosen based on the feature_of_interest
parameters mentioned in a configuration. For instance, in
the first experiment (Section Experiment I), due to the
2 mentioned feature_of_interests in the configuration
file (F = 2), there were finally 2 symptom types cho-
sen. Since, the number of subclasses for each symptom
type is not really specified, we consider it as a constant
value for all types of symptoms. Therefore, the number
Table 7 Manifestations shown in Figure 9
Seg# Manifestations Explanations
1 Rapid cardiac system pulse (Heart failure)
Not normal respiratory rate (Dyspnea)
Low blood pressure (Anemia, apnea)
(Anemia, tachycardia)
(Apnea, hypotension)
(Hypotension, tachycardia,
tachypnea)
of symptom types which is equivalent to the number of
feature_of_interests (F ) indicated in the configuration,
is considered as a significant parameter which affects the
cardinality of the Cause set (|C|). The greater the parame-
ter F , the larger the value of |C|.
In experiment I: F = 2, |C| = 62,
In experimentII: F = 3, |C| = 89,
Since the input of the reasoning process is the Hypothe-
sis set which is extracted from the Cause set, we focus on
parameters affecting the distinct number of causes in the
Hypothesis set (|Hc|). The first parameter, is the size of the
Cause set (|C|) which is also dependent on the F parame-
ter. Another parameter influencing |Hc|, is the number of
behaviours (B).
In Table 8, we listed the measured reasoning time (in
milliseconds)n for different configurations. The informa-
tion of each row in Table 8 belongs to a configuration
which is accumulated with a new configuration for its next
row. In the following the summary of four configurations
which are accumulated in order are given:
I : II :
feature_of_interest = Heart feature_of_interest = Blood
property = Rate property = Oxygen
Behaviours : Slow, Fast, Irregular Behaviours : High,Low
III : VI :
feature_of_interest = Blood feature_of_interest = Respiratory
property = Pressure property = Rate
Behaviours : High,Low Behaviours : Slow, Fast
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 14 of 16
http://www.jbiomedsem.com/content/5/1/35
Table 8 Reasoning time complexity (the unit of time is in milliseconds)
|F | |C| |B| SimilarityMatrix_time |Hc| Reasoning_time Final_reasoning_time
1 30 3 18 4 1 19
2 62 5 23 7 31 54
2 62 7 26 11 2146 2172
3 89 9 29 19 10301 10330
The first row and first configuration uses only one
feature_of_interest (F = 1) and the number of causes
retrieved from the symptom ontology is |C| = 30 (Table 2).
The distinct number of causes in the Hypotheses is |Hc| =
4 and is based on the 3 possible behaviours (B = 3). The
reasoning time for calculating the power set of causes in
the Hypothesis set is 1 ms. However, since the generation
and the filtering process of the similarity matrix is neces-
sary to reach to the final set, we consider the last column
of the table as the final reasoning time (19ms) which is the
summation of both the similarity matrix calculation time
and the reasoning time and by increasing the parameter
F in the second row of Table 2 (F = 2) the growth of the
number of behaviours (B = 5), we see the total reasoning
time also increased to 54 ms. In order to see the effect of
the parameter B, we keep the same feature_of_interests
in the third row (F = 2 and therefore |C| = 62). By adding
the third configuration, the only parameter changes is the
number of behaviours (B = 7), which results in a much
longer reasoning time (2172 ms). Although the param-
eter F influences the reasoning time, the effect of the
parameter B on the reasoning process is stronger.
The reasoning process due to the techniques explained
in Algorithm 2 (such as filtering the cause items based on
their relations with events), is much more efficient than a
pure calculation of the power set of the Cause set. Never-
theless, it still needs to deal with the power set calculation
for a smaller size of causes in the Hypothesis set, explain-
ing an exponential trend in computation time. Therefore,
the system configurations for higher scales matters. For
instance, behaviours allow the system to reduce the num-
ber of causes which are not relevant and results in a
smaller size of H. At the same time, however, the higher
number of behaviours enables the system to accept more
cause items during the similarity matrix filtering process,
which results in a bigger size of H and consequently a
higher reasoning time. The number of behaviours given
in the configuration file is therefore the most influential
parameter in the reasoning time. In summary, according
to the computational time represented in Table 8, the user
in order to have a reasonable computational time, is rec-
ommended not to define more than 3 behaviours for each
property of a feature_of_interest in a configuration file.
Although the intensive care units (ICUs) depending on
the patient situation or medical specialty are divided into
several parts such as medical intensive care unit (MICU),
surgical intensive care unit (SICU), etc., there are common
equipments in terms of monitoring critical physiological
parameters [28]. For instance, instant monitoring of pulse
oximetry, arterial blood pressure, oxygenation saturation,
temperature along with using ventilators assisting the res-
piratory systems are done by common wired sensors used
in any care units of emergency cases. Considering the
typical monitoring sensors in hospitals care units, the
computational time of our approach applied on other real
world scenarios with in average 4 sensors and 3 gen-
eral behaviours would be the same as what we discussed
above.
Conclusion
In this paper, we have presented a framework which is
able to annotate medical sensor data with labels con-
taining probable causes pertaining to sensor events. This
framework reduces the probability of losing the relevant
causes by retrieving a wide possibility of causes which are
related to sensor data. At the same time, by pruning the
retrieved concepts (removing irrelevant causes w.r.t the
probable events), the complexity of the reasoning process
is reduced.
The primary motivation to the presented work is having
the data annotation process that is as automated as possi-
ble. The process uses manually created configuration file
which is filled by the expert of the domain and is based
on events which are likely to occur. Although the process
of generating explanations of the data is dependent upon
the content of the configuration file, the expert is free to
populate this file using his/her own words. In other words,
the eventual explanations, due to synonyms of terms con-
sidered throughout the interpretation process, are literally
(but not conceptually) independent of terms used by the
expert. Certain limitations in the system include the level
of complexity of the user defined configurations. In addi-
tion, we chose to populate the SSN ontology with classes
so as to provide the opportunity of a better classifica-
tions of relevant classes for future purposes. For example,
by creating the two classes Heart and Cardiac system as
the subclasses of the feature_of_interest class, the sys-
tem will be able to, for some purposes in future, create a
owl:sameas properties between them to introduce them
as equivalent classes.
Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 15 of 16
http://www.jbiomedsem.com/content/5/1/35
Furthermore, as discussed in Section Experiment III,
the user of the system needs to consider the limitation
in number of abnormal behaviour defined in the con-
figuration to avoid the time complexity of the reasoner
to increase. In addition, the filtering process in similar-
ity matrix, where the relevant causes are chosen based
on their grammatical structure, can be further extended
towards considering complicated situations that may be
found in English definition of a cause.
Although the use of the symptom ontology is limited
to the retrieval of subclasses, still, the existence of this
ontology with its well-categorized structure was a positive
feature of the medical repository which provided read-
able categories of symptoms in terms of different parts of
the body. In order to extend the framework to be appli-
cable to other domains (e.g., Meteorology or Geography),
such a general ontology related to the domain is necessary.
For this reason, the medical domain is the more promis-
ing application domain for this approach. Considering the
requirements of this framework in terms of the structure
of knowledge, along with the reasoning issues over linked
data such as data inconsistency or redundancymay help to
efficiently develop and populate linked data for different
domains.
Endnotes
aThis ontology developed by the W3C Semantic Sensor
Networks Incubator Group (SSN-XG) describes sensors,
observations, and related concepts [29].
bThe size of the Web is 3.32 billion sites [30].
cURIs return contents of a resource that they identify.
dhttp://datahub.io/group/lodcloud (over 31 billion
triples),
eThe last access date of the Bioportals SPARQL
endpoint (http://sparql.bioontology.org) is on 27th July
2014
fThe process of splitting a sequence of strings into its
elements (tokens or words).
gThe process of reducing inflected words to their stem,
base or root form.
hIn WordNet 2.1 OWL, pertain is a property between
twoWordSense concepts that indicates the relevant term
for a word [7]
iIn this work we used StanfordParser [31] to analyse
phrases or sentences.
jIt is useful to recall that the jth column refers to
abnormal (or not + normal) behaviour that is composed
of a behaviour (as an adjective), a feature_of_interest
(as a noun) and a property (as a noun).
kThe power set of a set is the set of all its subsets.
lDue to the ethical concerns about the patients privacy
we received this dataset as an anonymous patient profile.
mSince all three segments are the same, the occurrence
probability can be calculated for one segment and its
values can be generalized.
nThe computational time has been done on a computer
which has an Intel(R) Core(TM) i7-2620M CPU
(2.70GHz), 64 bit, 4 cores,4 MB for the cache memory),12
GB memory, and Linux kernel 3.8.0-44-generic.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MA: modelling and developing different parts of the framework. AL:
supervising in modelling the framework and revising the manuscript. Both
authors read and approved the final manuscript.
Acknowledgements
This work has been supported by the Swedish National Research Council,
Vetenskapsrådet, project nr 2010-4769, on cognitive olfaction.
The evaluation of the framework presented in this article has also been
assisted by using the ICU data package provided for use in 1994 AI in Medicine
symposium submissions [10].
Received: 9 April 2014 Accepted: 4 August 2014
Published: 12 August 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Dumontier et al. Journal of Biomedical Semantics 2014, 5:14
http://www.jbiomedsem.com/content/5/1/14DATABASE Open AccessThe Semanticscience Integrated Ontology (SIO)
for biomedical research and knowledge discovery
Michel Dumontier1,4*, Christopher JO Baker2, Joachim Baran3, Alison Callahan4, Leonid Chepelev4, José Cruz-Toledo4,
Nicholas R Del Rio5, Geraint Duck6, Laura I Furlong7, Nichealla Keath4, Dana Klassen8, James P McCusker9,
Núria Queralt-Rosinach7, Matthias Samwald10, Natalia Villanueva-Rosales5, Mark D Wilkinson11 and Robert Hoehndorf12Abstract
The Semanticscience Integrated Ontology (SIO) is an ontology to facilitate biomedical knowledge discovery.
SIO features a simple upper level comprised of essential types and relations for the rich description of arbitrary
(real, hypothesized, virtual, fictional) objects, processes and their attributes. SIO specifies simple design patterns
to describe and associate qualities, capabilities, functions, quantities, and informational entities including textual,
geometrical, and mathematical entities, and provides specific extensions in the domains of chemistry, biology,
biochemistry, and bioinformatics. SIO provides an ontological foundation for the Bio2RDF linked data for the life
sciences project and is used for semantic integration and discovery for SADI-based semantic web services. SIO is
freely available to all users under a creative commons by attribution license. See website for further information:
http://sio.semanticscience.org.Background
Biomedical research is poised to enter an era of unprece-
dented large scale data analysis powered by hundreds of
public biological databases and hundreds of millions of
patient records. There is a real and urgent need to explore
effective methods for biomedical data integration and
knowledge management [1,2]. Semantic-based techno-
logies, such as ontologies, offer a proven method to
exploit expert-based knowledge in the analysis of
large datasets through terminological reasoning such as
correspondence, classification, query answering and
consistency checking [3-5].
The Semantic Web effort, as pursued under the auspices
of the World Wide Web Consortium (W3C), provides a
set of standards to facilitate the representation, publica-
tion, linking, querying and discovery of heterogeneous
knowledge using web infrastructure [6]. In particular,
the Resource Description Framework (RDF) [7] enables
triple-based assertions about resources using web-friendly
identifiers, RDF Schema (RDFS) [8] offers vocabulary to
create terminological hierarchies, and the Web Ontology* Correspondence: michel.dumontier@stanford.edu
1Center for Biomedical Informatics Research, Stanford University, Stanford,
California, USA
4Department of Biology, Carleton University, Ottawa, Ontario, Canada
Full list of author information is available at the end of the article
© 2014 Dumontier et al.; licensee BioMed Cen
Creative Commons Attribution License (http:/
distribution, and reproduction in any mediumLanguage (OWL) [9] assists in the construction and inter-
pretation of ontologies as sophisticated logic-based ex-
pressions to more precisely capture the meaning of types
and relations between entities. With dozens of high value
datasets now available in RDF and hundreds of biological
ontologies expressed using OWL, there is a tantalizing
opportunity to use these resources in knowledge discov-
ery. Biomedical researchers have made use of Semantic
Web technologies to uncover curation errors in systems
biology models [10], find putative disease-causing genes
[11], identify aberrant pathways [12], and uncover alterna-
tive drug therapies based on mechanism of action [13],
among others [14]. These knowledge-based applications
use automated reasoning over a coherent knowledge base
often crafted from multiple and different underlying rep-
resentations. Ontology-design patterns offer a simple way
to guide users towards a uniform representation of know-
ledge [15-17].
With the goal of facilitating knowledge discovery
through simple, but effective ontology-based data inte-
gration, we developed the Semanticscience Integrated
Ontology (SIO). SIO offers classes and relations to describe
and relate objects, processes and their attributes with spe-
cific extensions in the biomedical domain. Its relations
cover aspects of spatial and temporal qualitative reasoningtral Ltd. This is an Open Access article distributed under the terms of the
/creativecommons.org/licenses/by/2.0), which permits unrestricted use,
, provided the original work is properly credited.
Dumontier et al. Journal of Biomedical Semantics 2014, 5:14 Page 2 of 11
http://www.jbiomedsem.com/content/5/1/14including location, containment, overlap, parthood and
topology; participation and agency, linguistic and symbolic
representation, as well as comparative and other informa-
tion-oriented relations. Using straightforward mappings,
we report on the substantial benefits afforded by SIO in
the retrieval of RDF-based linked data and automatic
composition of OWL-described semantic web services.
Although SIO development is driven by needs in the
biomedical domain, we show that SIO can be applied to a
broader set of domains.
This paper is organized as follows: we first describe
the current state of the SIO OWL implementation, and
then we describe ontological foundations and essential
relations in mereotopology, participation and reference.
We then present three uses of SIO in knowledge repre-
sentation and outline its use in the integration of data
and web services. We finish with a brief comparison
with related work. As a matter of convention, we use
single quotes to indicate labels, boldface to indicate
classes, and italics to indicate relations.
The semanticscience integrated ontology
As of November 2013, SIO (v1.0) is implemented as an
OWL-DL ontology (SRIQ(D) expressivity) that comprises
of 1396 classes, 203 object properties, 1 datatype property,Figure 1 Selected portions of (A) class and (B) object property hierarc8 annotation properties, 7272 axioms, 1747 subClassOf
axioms, 43 equivalentClass axioms, and 209 subProper-
tyOf axioms. English labels are provided using the rdfs:
label annotation property while human readable, English
language definitions are provided using the Dublin Core
(dc:) Metadata term dc:description. The ontology has max-
imum depth of 41 subclasses while the average number of
children is 2. Figure 1 shows a slice of the class and
object-property hierarchies where entity is the top level
class and is related to is the top level object property.
Ontological foundation
SIO adheres to a three-dimensional worldview that is
familiar to most scientists  one that distinguishes be-
tween processes and the objects that participate in them.
An object is an entity that occupies space and is fully
identifiable by its characteristics at any moment in time
in which it exists. A process is an entity that unfolds
in time and has temporal parts. While an entity exists
at and is located in some space and time (Figure 2B),
these need not be real space or real time, but may instead
occur in a hypothetical (propositional), virtual (electronic),
or fictional (creative work) setting. A quality (intrinsic
attribute), capability (action specification) or role (be-
havior, right and obligation) may exist at some time in thehies in SIO.
Figure 2 Key objects and relations in SIO. (A) Key SIO entities are objects, processes and their attributes (qualities, capabilities, roles,
measurement values). Processes have objects as participants and may realize specific roles and capabilities. (B) Spatial and temporal qualification
of SIO entities is captured through a set of relations (is located in, exists at) and sub-relations (e.g. is contained in, is part of, measured at), while
(C) information in the form of literals (string, numbers, dates) are captured as instances of information content entities which are associated with
their specific objects or processes.
Dumontier et al. Journal of Biomedical Semantics 2014, 5:14 Page 3 of 11
http://www.jbiomedsem.com/content/5/1/14entity that bears it, but it is realized in a process in
which it plays a critical role (Figure 2A). The value of
an informational entity such as a measurement value
(quantity or position) is represented as a literal - string,
number (integer, float, double), boolean or date - using
the has value data property (Figure 2C).
Mereotopology
SIO offers a number of mereotopological relations that
can be used to describe one or more entities in terms of
their spatial organization (Figure 3A). The parent relation
'is located of ' is a transitive relation that holds true if the
spatial or temporal region of one entity fully overlaps with
the spatial or temporal region of another entity. 'has part'
is a relation that is reflexive in the sense that the whole is
a part of itself, and is also transitive in that a component
of a part is also a component of the whole. Therefore, a
query on the has part relation will return the whole as an
answer. 'has proper part' is an irreflexive and asymmetric
relation that ensures that the whole is different from
and not one of its proper parts. 'has direct part' enables
users to quantify the number of parts (via a cardinalityFigure 3 Relation hierarchies for (A) mereotopological relations, (B) prestriction) at a desired type granularity, which is not
otherwise possible in OWL over the transitive has part
relation. 'has component part' may be used to indicate that
the part is intrinsic to the whole, and that the removal of
the part changes the identity of the whole, with the caveat
that there is no logic in OWL to directly infer this. 'con-
tains' is a transitive relation in which the 3D spatial region
occupied by entity A fully overlaps with the spatial region
occupied by entity B, but it is not the case that A has B as
a part. 'surrounds' is a relation that can be used to indicate
that A 'contains' B and either A 'is adjacent to' B or A 'is
directly connected to' B.
The next set of mereotopological relations allows one to
specify how the parts are positioned to one another. 'is con-
nected to' is a symmetric, transitive relation that specifies
that components either directly share a boundary (they are
directly connected to each other) or that they are indirectly
connected by a path of unbroken direct connections. 'is
directly connected to' is a symmetric relation that indicates
that two components share a boundary. Since this relation
is non-transitive, we can use it in statements to quantify
the number of connections from one part to other kinds ofarticipatory relations, and (C) referential relations.
Dumontier et al. Journal of Biomedical Semantics 2014, 5:14 Page 4 of 11
http://www.jbiomedsem.com/content/5/1/14parts. 'is directly before' is a relation between entities placed
on a dimensional axis in which the projection of the pos-
ition of the first entity is numerically less than the projec-
tion of the position of the second entity, and the entities
are adjacent to one another. This is useful for indicating
the spatial positioning of residues in linear biopolymers
such as proteins or nucleic acids. A domain specific rela-
tion such as is covalently connected to then enables one to
describe the atomic connectivity within a molecule such as
methane (Figure 4).
Processes and participation
SIO provides a set of relations to describe processes in
terms of their participants and their actions (Figure 3B).
'has participant' indicates which entities participate in a
process. 'has agent' specifies entities that directly or ac-
tively participate in the process. 'has input' specifies en-
tities at the start of the process. has parameter specifies
those variables (and their values) used in the process.
'has target' specifies entities that are modified during the
process, but retain their identity. 'has substrate' specifies
entities that are consumed (or are sufficiently changed
that they lose their canonical identity). 'has product' speci-
fies new entities formed as a result of a process. Relations
such as has substrate, has target, has product are examples
of role-specialized relations. In SIO, more explicit role-
based assertions can be formulated by stating that the role
of an entity is realized in the process. For instance, Figure 5
shows a description of phosphorylation of an enzyme by
ATP in which substrate and product roles are realized.Figure 4 Exact description of a molecule of methane using mereotopSIO includes an OWL2 property chain [realizes o is
role of - > has participant] which enables an OWL2 DL
reasoner to infer that entities having the realized role are
also participants of the process.Referential relations
Referential relations in SIO are used to indicate what an
object refers to or the nature of the mention of one entity
by another (Figure 3C). At the top level, refers to enables
JOURNAL OF
BIOMEDICAL SEMANTICS
Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26
http://www.jbiomedsem.com/content/5/1/26
RESEARCH Open Access
Generalising semantic category
disambiguation with large lexical resources for
fun and profit
Pontus Stenetorp1*, Sampo Pyysalo2,3, Sophia Ananiadou2,3 and Junichi Tsujii2,3,4
Abstract
Background: Semantic Category Disambiguation (SCD) is the task of assigning the appropriate semantic category to
given spans of text from a fixed set of candidate categories, for example PROTEIN to Fibrin. SCD is relevant to Natural
Language Processing tasks such as Named Entity Recognition, coreference resolution and coordination resolution. In
this work, we study machine learning-based SCD methods using large lexical resources and approximate string
matching, aiming to generalise these methods with regard to domains, lexical resources and the composition of data
sets. We specifically consider the applicability of SCD for the purposes of supporting human annotators and acting as
a pipeline component for other Natural Language Processing systems.
Results: While previous research has mostly cast SCD purely as a classification task, we consider a task setting that
allows for multiple semantic categories to be suggested, aiming to minimise the number of suggestions while
maintaining high recall. We argue that this setting reflects aspects which are essential for both a pipeline component
and when supporting human annotators. We introduce an SCD method based on a recently introduced machine
learning-based system and evaluate it on 15 corpora covering biomedical, clinical and newswire texts and ranging in
the number of semantic categories from 2 to 91.
With appropriate settings, our system maintains an average recall of 99% while reducing the number of candidate
semantic categories on average by 65% over all data sets.
Conclusions: Machine learning-based SCD using large lexical resources and approximate string matching is sensitive
to the selection and granularity of lexical resources, but generalises well to a wide range of text domains and data sets
given appropriate resources and parameter settings. By substantially reducing the number of candidate categories
while only very rarely excluding the correct one, our method is shown to be applicable to manual annotation support
tasks and use as a high-recall component in text processing pipelines. The introduced system and all related resources
are freely available for research purposes at: https://github.com/ninjin/simsem.
Keywords: Semantic category disambiguation, Approximate string matching, Lexical resources, Named entity
recognition, Domain adaptation, Freebase
Background
Semantic Category Disambiguation (SCD) is a key sub-
task of several core problems in Natural Language Pro-
cessing (NLP). SCD is of particular importance for Named
Entity Recognition (NER), which conceptually involves
two sub-tasks that must be solved: detecting entity men-
tions and determining to which semantic category a given
*Correspondence: pontus@is.s.u-tokyo.ac.jp
1Department of Computer Science, University of Tokyo, Tokyo, Japan
Full list of author information is available at the end of the article
mention belongs. SCD is concerned with the latter, the
selection of the appropriate semantic category to assign
for a given textual span from a set of candidate cate-
gories (Figure 1). Other tasks that SCD is relevant to
include coreference and coordination resolution. In coref-
erence resolution [1], coreferring mentions must share the
same semantic category, and a method can thus exclude
candidate mentions by having access to accurate seman-
tic classifications. Also, by adding semantic information
about the members of a coordinate clause, it is possible
© 2014 Stenetorp et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited.
Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 2 of 11
http://www.jbiomedsem.com/content/5/1/26
(a)
(b)
Figure 1 Example of the prerequisite for our task setting, demarked continuous spans as seen in (a) and the output, semantic categories
assigned to the input spans as seen in (b). 2-comp-sys, Pro and +Regulation are used as short-hands for Two-component system, Protein
and Positive regulation respectively. Note the potential for partial overlap of different semantic categories as can be seen for the Protein and
Two-component system annotations.
to resolve that the most likely meaning for a phrase such
as Tea or coffee and a sandwich is [[Tea or coffee]
and a sandwich] rather than [[Tea] or [coffee and a
sandwich]] [2].
We recently demonstrated that high-performance SCD
systems can be constructed using large-scale lexical
resources and approximate string matching for several
well-established data sets [3]. However, a number of ques-
tions regarding the applicability of these methods remain
unanswered. First, this approach to SCD has only been
extensively evaluated for biomedical texts, which raises
the question whether the advances made for the biomed-
ical domain can readily be carried over to other domains
such as clinical and newswire texts. Second, state-of-
the-art NER and SCD systems typically rely on lexical
resources selected to suit the task being addressed [4,5]
and one can thus expect performance to degrade if the
system is moved to a new domain or language [6], but the
magnitude of this effect for SCD has not been established.
Third, while NER data sets are commonly annotated for
short, non-embedded text spans such as person names or
protein mentions, in a number of corpora annotations can
cover long spans and be nested in complex structures [7].
We would expect such annotations to pose issues for lex-
ical resource matching strategies that rely on a mapping
between the resource and the span being classified.
There are several practical applications that involve
SCD, such as the assignment of labels such as those of
ICD-10 [8] to documents and the production of annota-
tions to train information extraction systems [9]. For any
manual assignment task, there are cognitive limitations on
the number of distinct categories a human annotator can
process before falling victim to degrading performance
[10]. Automated systems could thus assist annotators by
limiting the number of categories presented to the user,
excluding those that are clearly irrelevant; Figure 2 shows
an illustration for a specific use-case. However, any anno-
tation support system will be subject to close scrutiny,
and an SCD system must thus have very high recall to
avoid errors and rejection by users, while at the same time
(a) (b)
Figure 2 Examples of entity type annotations from [25], illustrating how the amount of visual and user-interface complexity (a) can be
reduced using an SCD system (b). The relevant text span being annotated in both figures is heart which should be assigned the ORGAN
semantic category.
Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 3 of 11
http://www.jbiomedsem.com/content/5/1/26
limiting the number of categories presented to the highest
degree possible, even when the amount of training data is
limited.
In this work we extend our initial study [11] of the appli-
cability of SCD for annotation support and as a pipeline
system component, investigating whether SCD can be
generalised across domains and languages and the impact
of lexical resource selection and differences in annotation
criteria.
Methods
This section discusses baseline methods, evaluation met-
rics, feature sets, models, corpora and lexical resources
used for the experiments.
Previous work and baseline methods
Although SCD is central to NER and several other NLP
tasks, there have been relatively few in-domain studies
investigating SCD as a stand-alone task. However, recently
a few publications have investigated this task in isolation.
Cohen et al. [12] presented a fast and reliable approach
for associating a given textual span to one or several
ontologies. The method was based on a set of manu-
ally crafted rules and achieved an accuracy ranging from
77.1% to 95.5% for determining the semantic category
of a given annotation in a setting where each category
was defined by reference to a domain ontology. In recent
work, [3] we introduced a machine learning-based SCD
method that employed approximate string matching [13]
of continuous textual spans to several large-scale lexi-
cal resources. While the use of lexical resources such as
dictionaries covering specific semantic categories is com-
monplace in state-of-the-art NER systems [4,5], approxi-
mate string matching was a novel aspect of the work. We
evaluated the method on several data sets and achieved
results ranging from 85.9% to 95.3% in accuracy. How-
ever, although the overall best-performing model in this
study incorporated approximate string matching features,
we failed to establish a clear systematic benefit of approx-
imate, as opposed to strict, string matching for all data
sets.
Since our goal here is to evaluate the performance
of SCD for assisting other tasks such as manual text
annotation, the approach of Cohen et al. has two limita-
tions. First, it assumes that the semantic categories are
defined by ontological resources and therefore it cannot
be directly applied to annotation targets that do not match
available ontological resources. Second, unlike our previ-
ously proposed approach, their approach does not provide
ranking or classification confidence. Since this makes the
method less suitable in a setting where it is necessary to
dynamically adjust the number of suggested categories, as
is the case for annotation support, for the present study
we choose to extend our previous system.
Task setting
We define an SCD task as follows: for a fixed set of can-
didate semantic categories, given a text and a continuous
textual span in its context, assign the correct category to
the span. Figure 1 illustrates the style of annotation and
the possibility of overlapping and nested spans with dif-
ferent semantic categories. The SCD task set-up is related
to bothWord-sense Disambiguation [14] (WSD) and term
grounding (or normalisation) [15], but there are several
noteworthy differences. The spans considered in WSD
are inherently internally ambiguous (for example exploit
carrying the meaning of achievement, advantageous use,
etc.), thus requiring the word sense to be mainly inferred
by context. Further, SCD assumes a fixed set of cate-
gories, while in WSD the senses are normally different for
each ambiguous word. In term grounding, entitymentions
are to be mapped to unique identifiers, typically defined
with reference to large resources such as Entrez Gene
or Wikipedia, and each identifier represents only a small
number of entities or just a single one. The key differ-
ence in this case is that as SCD is concerned with broader
semantic categories, each covering a large number of enti-
ties, SCD methods can thus assume that the training data
will contain numerous instances of each possible category.
In our previous work [3] we cast SCD as a (single-label)
classification task, and Cohen et al. [12] considered it
as a multi-label classification task. In this work we con-
sider both the single-label classification setting as well
as a setting where we allow the method to suggest any
number of categories for a given span, in part analo-
gously to beam search with a dynamic width beam [16].
Although in our data exactly one candidate category is
correct for each span, this setting allows us to explore
high-recall approaches while aiming to keep the number
of suggestions to a minimum.
This setting matches our aim of reducing the cognitive
burden on a human annotator who has to determine the
correct answer among multiple suggestions and allows us
to study how well an SCD system can estimate its own
confidence when passing on suggestions to either a human
annotator or another system.
Metrics
For single-label classification, we report performance
using accuracy, the fraction of cases where the system pre-
dicts the correct label. When allowing the system to sug-
gest multiple categories, we measure recall and ambiguity
reduction. Recall is the fraction of cases where the correct
category is present among the suggested categories, and
ambiguity is defined as the number of semantic categories
suggested by the system. While both recall and (average)
ambiguity give insight into the performance of the system,
they are in a trade-off relation, similarly to how precision
is to recall, and should ideally be combined into a single
Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 4 of 11
http://www.jbiomedsem.com/content/5/1/26
metric. To normalise the ambiguity metric with regard to
the number of categories, we define (relative) ambiguity
reduction as follows:
AmbiguityReduction = |Categories| ? Ambiguity|Categories| ? 1 (1)
Here, we subtract one from the number of categories
in the denominator to give the metric the same range as
recall ([0.0, . . . , 1.0]). We then straightforwardly combine
average ambiguity reduction and recall into a harmonic
mean.
We train our model and produce learning curves with
data points using samples of [5%, 10%, . . . , 100%] of the
training data. At each data point we take several random
samples of the current data size and use the mean (?) of
the performance over the samples to compensate for pos-
sible sample variance. Results for eachmetric are provided
as the mean of the data points of the learning curve, as is
commonly done to approximate the Area Under the Curve
(AUC).
Feature sets andmodels
One of the primary differentiating factors between the
machine learning models considered in our experiments
are the feature sets applied in training each model.
Our three baseline models are defined by the follow-
ing feature sets: INTERNAL (INT.), a comprehensive set
of NER-inspired features derived solely from the text
span to be classified, GAZETTEER (GAZ.) features derived
from strict string matching look-ups of the span in each
of the applied lexical resources, and SIMSTRING (SIM.)
features, representing an approximate matching variant
of GAZETTEER calculated using the SimString approxi-
mate string matching library [13]. These feature sets are
described in detail in our previous studies [3,17]. The
three baseline methods are defined by the feature set
combinations INTERNAL, INTERNAL+GAZETTEER, and
INTERNAL+SIMSTRING, abbreviated as INT., INT.GAZ.
and INT.SIM., respectively.
We extended our previous system described in [3] to
allow it to determine the number of categories to propose
to optimise recall and ambiguity reduction as follows.
The machine learning method applied in the system [18]
provides probabilistic outputs, which can be used as indi-
cators of the confidence the method has for each category.
The system considers the categories ordered highest-
confidence first, and returns the smallest set of categories
so that the sum of the confidences for the set is equal to
or greater than a threshold value. This threshold becomes
an additional parameter for the system, controlling the
trade-off between ambiguity and recall. This will result
in a number of suggestions ranging from 1 to the total
number of categories in the data set. For example, for the
categories and confidences [PROTEIN 90%, CHEMICAL
6%, ORGANISM 4%] and the confidence threshold 95%,
the system would suggest PROTEIN and CHEMICAL, but
not ORGANISM. In our previous work, [11] we selected a
threshold of 99.5% as this performed well for the evalua-
tion on the development data sets, and we continued to
use this threshold for our initial experiments here.
Corpora
For evaluation, we initially included the six data sets used
in [3], listed above the mid-line in Table 1. While our pre-
vious study found promising results for these data sets,
they are all drawn from the biomedical domain, which
left the generality of our method largely unsubstantiated.
To argue that our method is applicable to other domains,
we need to show this experimentally. To demonstrate the
applicability of the method, it is also necessary to evalu-
ate against corpora containing more semantic categories
than the 17 covered by the EPI data set, the largest num-
ber in the previously considered resources. To widen our
selection of annotated resources, we thus collected a total
of nine additional corpora, listed below the mid-line in
Table 1 and presented in the following.
To extend the coverage of domains, we included the
I2B2 corpus [19] and the CoNLL-2002 data sets for
Table 1 Corpora used for evaluation
Name Semantic categories
Epigenetics and Post-Translational 17
Modifications corpus [35] (EPI)
Infectious Diseases corpus [22] (ID) 16
Genia Event corpus [36] (GE) 11
Collaborative Annotation of a Large 4
Biomedical Corpus [37] (SSC)
BioNLP/NLPBA 2004 Shared Task 5
corpus [38] (NLPBA)
Gene Regulation Event Corpus [39] (GREC) 64 (6)
Multi-Level Event Extraction corpus [21] (MLEE) 52
GeneReg corpus [40] (GREG) 10
Gene Expression Text Miner corpus [41] (GETM) 3
BioInfer [7] (BI) 119 (97)
BioText [42] (BT) 2
CoNLL-2002 Shared Task corpus, 4
Spanish subset [20] (CES)
CoNLL-2002 Shared Task corpus, Dutch 4
subset [20] (CNL)
i2b2 Medication Challenge corpus [19] (I2B2) 6
OSIRIS corpus [43] (OSIRIS) 2
Parenthesised values signify the actual number of categories after performing
pre-processing steps so as to not suffer from data sparseness (GREC conversion
into SGREC [3]) or to compensate for ontological design decisions (BI). The
mid-line indicates a cut-off between the above corpora used in previous work [3]
and the corpora added to evaluate our approach for a variety of domains and
covering a large set of semantic categories.
Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 5 of 11
http://www.jbiomedsem.com/content/5/1/26
Spanish and Dutch NER [20]. I2B2 stems from the
clinical domain which, while related to the biomedical
domain, involves a different set of semantic categories (e.g.
DOSAGE and MEDICATION). The CoNLL-2002 data sets
are both from the newswire domain, largely unrelated to
the previously considered domains, and additionally for
languages other than English. They are thus expected to
pose new challenges, in particular in regards to the lexi-
cal resources utilised. As mentioned above, the question
is still open as to whether our approach scales to a set of
categories larger than the 17 of the EPI corpus. To address
this issue, we acquired the MLEE [21] and BI [22] corpora
which contain 52 and 119 semantic categories each, rep-
resenting increases of ?3× and ?7× respectively in the
number of categories. Finally, we added four biomedical
corpora not considered in previous work to increase the
diversity of resources in this domain.
Following initial corpus selection, we performed some
pre-processing for a number of the resources, as follows.
After inspecting the annotation guidelines for the BI cor-
pus, we found that a core assumption of our task setting
was violated: mentions of entities of the three semantic
categories GENE, PROTEIN and RNA would be marked
using a single compound category (GENE, PROTEIN OR
RNA) if they were not a participant of an event annota-
tion. This is problematic for our experimental set-up since
we do not seek to model whether targeted entity mentions
participate in events. Thus, we collapsed all entries for
GENE, PROTEIN and RNA into the single GENE, PROTEIN
OR RNA category as a pre-processing step. Furthermore,
BI allows for discontinuous span annotations, which also
conflicts with the assumptions of our task setting. We
thus merged all discontinuous spans into single spans,
removing any duplicate spans that were created in the
process. Finally, to compensate for an ontological deci-
sion to differentiate between state changes and processes
(e.g. Phosphorylate compared to Phosphorylation) we
merged all paired types into single categories. After these
pre-processing steps had been carried out, we were left
with 97 distinct semantic categories, a ?6× increase
compared to the largest number of categories considered
in our previous study. We also performed some neces-
sary, but less involved, pre-processing steps for some other
corpora. In the case of BT, we removed the relational indi-
cators for each span and used the two categories DISEASE
and TREATMENT. For I2B2, we used the gold data anno-
tated and released by the organisers of the shared task,
leaving out the parts of the provided data submitted by
shared task participants.
All the data sets were randomly separated into train-
ing, development and test sets consisting of 1/2, 1/4 and
1/4 of the annotations respectively. The test set was kept
hidden during development and was only used to gener-
ate the final results. When reviewing annotation samples
and guidelines for the nine additional corpora, we found
some cases that we anticipated would be problematic
for methods using our previously proposed feature sets.
In particular, for compound noun-phrases (NPs) con-
taining mentions of entities of several different semantic
categories, the classifier could potentially be confused
by matches to resources containing semantic categories
unrelated to the entity referred to by the NP as a whole.
As a concrete example, consider Complex of fibrin and
plasminogen: the full span should be assigned the seman-
tic category COMPLEX, while the semantic categories of
fibrin and plasminogen are PROTEIN. To address such
cases, we drew on the observation that the head word of
a noun-phrase commonly determines the semantic cat-
egory of a span. Specifically, we constructed a set of
features employing a simple heuristic-based noun-phrase
head-finding algorithm, extracting two span components
of particular interest: the NP-head detected by the algo-
rithm, and the Base-NP, approximated as the combina-
tion of the NP-head and all preceding text in the span
(Figure 3). These subspans were used in feature generation
to define an extended NP feature set: for the INTERNAL
feature set, we added binary features representing the text
of the NP-head and Base-NP, and for the GAZETTEER and
SIMSTRING feature sets, we performed look-ups against
all lexical resources using strict and approximate string
matching respectively, in addition to the binary features
for the text of the NP-head and Base-NP. We will discuss
the impact of these features for the various data sets in the
Results section.
Lexical resources
As a starting point, we adopt the collection of 170 lex-
ical resources first gathered in [3]. These are partic-
ularly suited for biomedical data as they were manu-
ally selected with this single domain in mind. Since it
would be advantageous to use a general purpose col-
lection of lexical resources rather than those selected
for a specific domain, we also evaluate the data pro-
vided by the Freebase project as a source of general-
purpose lexical resources. The Freebase knowledge base
covers a wide range of domains, is multi-lingual in
nature, and has recently been utilised for several NLP
Figure 3 Example of sub-string components used to generate
the NP-based features.
Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 6 of 11
http://www.jbiomedsem.com/content/5/1/26
tasks [23,24]. Freebase is collaboratively curated by vol-
unteers and contains millions of statements. However,
not all of these are relevant to our experiments, as the
knowledge base not only covers statements regarding
semantic categories but also information such as user
data. The project defines a set of 72 Commons cat-
egories that have passed several community standards
and cover a wide array of topics such as ASTRON-
OMY, GEOGRAPHY, GAMES, etc. We created 72 lexical
resources from the 15,561,040 unique entry names listed
for these Freebase categories, referred to in the following
as FB.
Even though Freebase is a general-purpose resource,
we anticipated some issues with the granularity of the
Commons categories. In particular, the MEDICINE
and BIOLOGY categories do not make any distinction
between, for example, DRUG and INFECTIOUS DISEASE,
and ORGANISM and GENE, respectively. In order to allow
for a fair comparison to the manually selected biomedical
domain lexical resources, we constructed an additional set
of resources where these two categories anticipated to be
problematic were split into their sub-categories, giving a
total of 175 lexical resources. This set is referred to as FBX
in the following.
The GAZETTEER and SIMSTRING features are depen-
dent on the choice of lexical resources, and we can thus
create variants of these feature sets by using any of the
above-mentioned sets of lexical resources. For our exper-
iments, we also defined in addition to the basic variant
using the 170 biomedical domain resources four models
based on the GAZETTEER and SIMSTRING in combination
with the FB and FBX sets.
Results and discussion
This section introduces and discusses the experimental
outcomes. The experimental results are summarised in
Figure 4, Table 2 and Additional file 1: Table S1. We first
investigate how our baseline models perform in regards to
ambiguity reduction and recall on the subset of corpora
used in our previous work. Next, we proceed to eval-
uate how the same models perform for additional data
sets, focusing on performance for resources with large
numbers of semantic categories and those from domains
which are either different but related (clinical) or largely
unrelated (newswire) to the biomedical domain. We then
evaluate the impact of utilising different lexical resources
and evaluate the effectiveness of our proposed NP feature
set. Lastly, we consider the effects of tuning the threshold
parameter that controls the trade-off between ambiguity
and recall.
Initial evaluation on biomedical corpora
For our initial investigations, we use the six corpora
applied in our previous study [3]. Figures 4a and 4b show
the lower end of the learning curves for ambiguity and
recall, and the results for the different evaluation metrics
are given in the boxed upper left corners in Additional
file 1: Table S1.
We observe that the SIMSTRING model outperforms
other baseline models in almost all cases where there
are non-trivial differences between the different models.
We thus focus primarily on the SIMSTRING model in
the remainder of the evaluation. Our results are promis-
ing for both the ambiguity and recall metrics. Ambiguity
quickly drops to a manageable level of 23 remaining cat-
egories for all corpora (Figure 4a), and the reduction in
the number of semantic categories is on average 60% over
the data sets (Additional file 1: Table S1c). The reduc-
tion is most prominent for EPI, where the number of
categories is reduced by ?95% even for the smallest train-
ing set size considered. The positive results for ambigu-
ity reduction are achieved without compromising recall,
which stays consistently around or above ?99% for all
data sets (Figure 4b and Additional file 1: Table S1d). This
level is expected to be acceptable even for comparatively
demanding users of the system. In summary, we find that
Figure 4 Learning curves for ambiguity (a) and recall (b) for our initial ambiguity experiments.
Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 7 of 11
http://www.jbiomedsem.com/content/5/1/26
Table 2 Results for the BT, GETM, I2B2 andOSIRIS data sets
using the Int.NP.Sim. model with a confidence threshold of
95% for mean ambiguity reduction (?Amb.Red.), mean
recall (? Recall), and the harmonic mean of mean
ambiguity reduction and recall (H(?Amb.Red.,?Recall))
Data set ?Amb.Red. ?Recall H(?Amb.Red.,?Recall)
BT 78.00/+34.00 99.54/-00.31 87.46/+26.38
GETM 88.50/+32.50 99.99/-00.01 93.89/+22.10
I2B2 77.60/+42.60 98.14/-01.50 86.67/+34.87
OSIRIS 78.00/+42.00 99.79/-00.21 87.56/+34.62
The relative values are compared to the same model using a confidence
threshold of 99.5%.
for a number of biomedical domain data sets the proposed
approach is capable of notably reducing the number of
proposed semantic categories while maintaining a very
high level of recall and that our SIMSTRING model outper-
forms other baseline models.
Impact of data set domain and number of categories
We next extend our evaluation to the additional nine cor-
pora incorporated in this study. As this gives 15 corpora
in total, instead of considering performance metrics and
learning curves in detail for each, we will below focus
primarily on the summary results in Additional file 1:
Tables S1a and S1b, giving accuracy and the harmonic
mean of ambiguity reduction and recall. Among the nine
additional data sets, CES, CNS and I2B2 are of particular
interest regarding the ability of the approach to gener-
alise to new domains; the former two are for languages
different from English and from the newswire domain, a
common focus of NLP studies, and the latter from the
clinical domain. Likewise, the MLEE and BI data sets,
containing 52 and 97 semantic categories respectively,
are suited for evaluating the ability of the approach to
generalise to tasks involving a large amount of semantic
categories.
We first note that the SIMSTRING model performs well
for all metrics for the biomedical domain MLEE, GREG
and BI data sets. However, we observe several instances
of reduced performance with respect to the results of the
initial experiments for corpora of various domains. For
the newswire domain CES and CNL data sets, we find
somewhat reduced accuracy and a low harmonic mean.
The biomedical domain GETM, BT and OSIRIS corpora
and the clinical domain I2B2 corpus show high accuracy,
but share the low harmonic mean performance of the
CES and CNL data sets. In all cases the poor results in
terms of the harmonic mean of ambiguity reduction and
recall is due to low ambiguity reduction; recall remains
high in all instances, reaching a full 100% in numerous
cases (Additional file 1: Table S1d). This suggests that the
method may have problems with its optimisation target
when the number of categories is small, a property shared
by all the above resources, overemphasising recall over
ambiguity. Additionally, for the out-of-domain data sets
it is probable that our selection of lexical resources is a
poor fit, a possibility evaluated specifically in the next
section.
In regards to data sets containing large sets of seman-
tic categories, rather surprisingly both the MLEE and BI
data sets appear to pose little challenge to our approach,
even though they both contain more than three times
the number of categories considered previously. These
results suggest that, somewhat counter to expectation,
the method appears to generalize well to large numbers
of semantic categories, but poorly to small numbers of
semantic categories.
Lexical resource dependence
The poor performance for the Spanish and Dutch
newswire corpora CES and CNL could potentially be
explaned by a mismatch between the data sets and the
applied lexical resources: the lexical resources originally
used in [3] were collected specifically for the biomedical
domain, and using only English resources. This hypothesis
is supported by the observation that the models rely-
ing on lexical resources, SIMSTRING and GAZETTEER,
performed poorly for these data sets, barely outper-
forming or performing slightly worse than the strong
baseline of the INTERNAL model that does not utilise
any lexical resources. To test the hypothesis, we created
new SIMSTRING and GAZETTEER model variants using
the Freebase-based lexical resources FB and FBX. These
are denoted in Additional file 1: Table S1 by a trailing
parenthesis following the model name that contains the
resource name (e.g. INT.SIM. (FB)).
If we at first only consider the results of the FB-
based models, we observe a considerable increase in
performance for the CES and CNL data sets by approxi-
mately 45% points in mean accuracy and approximately
1220% points in harmonic mean for the SIMSTRING
model (Additional file 1: Table S1a and Additional file 1:
Table S1b). This effect is most likely due to named
entities annotated in these corpora, such as company
names, person names, and locations, now being listed in
the lexical resources and serving as strong features. An
interesting observation is that although both the SIM-
STRING and GAZETTEER models employ the same lexical
resources, the performance increase for the SIMSTRING
model greatly surpasses that of the GAZETTEER model.
This result is largely analogous to what we have previously
demonstrated for the biomedical domain, and suggests
that the benefits of approximate string matching gener-
alise also to the newswire domain and across languages.
Although the effect of using the FB version of the Free-
base data is positive for the CES and CNL data sets, there
Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 8 of 11
http://www.jbiomedsem.com/content/5/1/26
is a notable drop in performance across the board for
nearly all other data sets. At this point we should remem-
ber that we have anticipated that the Freebase Commons
categories may be of limited value for specific domains
due to their coarse granularity. We thus now also consider
the results of the FBX-based models that give a finer gran-
ularity for the MEDICINE and BIOLOGY categories. For
SIMSTRING, using FBX as opposed to FB raises the aver-
age accuracy over the data sets from 86.55% to 87.72% and
the average harmonic mean score from 60.40% to 64.79%.
Further, SIMSTRING is shown to benefit more than the
strict string matching model GAZETTEER, which fails to
realise a clear benefit from FBX as compared to FB. How-
ever, for the biomedical domain corpora, performance
remains considerably lower than when using in-domain
resources even for FBX.
These results confirm the expectation that the per-
formance of the approach is strongly dependent on the
choice of lexical resources, and suggest that while the
large, general-purpose resource Freebase can be used
to derive lexical resources applicable across domains, it
cannot match the benefits derived from using targeted
resources curated by specialists in the domain relevant to
the corpus.
Impact of noun-phrase head features
As noted in the introduction of the additional corpora, we
were concerned that annotated spans of text that cover
mentions of entities of multiple semantic categories may
cause difficulties for our approach. This is in part due to
our feature sets being inspired by features employed by
NER systems, which frequently target short spans of text
involving only single mentions of entities, such as proper
names. To address this issue, we introduced the NP exten-
sions of the feature sets of each model. In this section, we
present results on the effectiveness of these features.
We find that GAZETTEER and SIMSTRING benefit from
the introduction of the NP features, while INTERNAL
shows mixed results depending on the metric. Interest-
ingly, while GAZETTEER gains an average 0.60% points
for accuracy and 6.39% points for the harmonic mean,
the respective gains are lower for SIMSTRING, at 0.46%
points and 4.51% points. Following from what we have
observed previously, we would expect that if approximate
string matching is more beneficial than strict matching
on the level of the whole string, it would also be so on
subsets of the same string. A possible explanation is that
while the GAZETTEER model previously had no access
to any substring matches in the lexical resources, the
approximate string matching model could make some use
of this information even before the introduction of the
NP features. Thus, it is possible that in allowing matches
against smaller regions of a given span, the use of approx-
imate string matching to some extent relieves the need
to perform detailed language-specific processing such as
head-finding.
This evaluation demonstrated that the NP features are
effective for the GAZETTEER and SIMSTRING models,
with their addition to the SIMSTRING baseline feature set
producing a model that outperforms all models in our
previous work for a majority of the data sets for both
the accuracy and harmonic mean metrics. The resulting
model, INT.NP.SIM., is our best model as-of-yet for the
SCD task.
Impact of confidence threshold parameter
Until now we have not addressed the low performance
in terms of ambiguity reduction for the GETM, BT, I2B2
and OSIRIS data sets. These are from the biomedical and
clinical (I2B2) domains, but share the property of involv-
ing only a small number of semantic categories: three in
GETM and two in the others. One parameter we kept
fixed throughout experiments was the confidence thresh-
old that controls the number of suggestions proposed
by our system and the trade-off between ambiguity and
recall. To investigate whether the setting of this param-
eter could account for the low performance for these
resources, we lower the threshold from the value 99.5%,
chosen based on experiments on the corpora used in our
previous work [11], and instead use a threshold of 95.0%.
This choice is motivated by a set of preliminary experi-
ments on the development portions of all data sets. We
then performed additional evaluation on the four above-
mentioned corpora that had shown poor performance.
We can observe that, as expected, performance in terms
of ambiguity improves greatly (Table 2), roughly doubling
in absolute terms. Further, this improvement is achieved
while recall is preserved at a level of 98% or higher for
all four data sets. In hindsight, this behaviour could be
expected on the basis of our observation of close to per-
fect recall for the primary experiments for these four data
sets.
This experiment shows that while a high threshold can
cause the system to err on the side of recall and fail to
produce a notable reduction in ambiguity for corpora with
a low number of semantic categories, with an appropriate
setting of the threshold parameter it is possible to achieve
both high recall and a clear reduction in ambiguity also for
such data sets.
Conclusions and future work
We studied machine learning-based Semantic Category
Disambiguation (SCD) methods using large lexical
resources and approximate string matching, focusing on
the ability of these SCD approaches to generalise to
new corpora, domains, and languages, their dependence
on factors such as the choice of lexical resources, and
their applicability for annotation support tasks and as
Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 9 of 11
http://www.jbiomedsem.com/content/5/1/26
components in pipeline systems. Adapting an existing
SCD method to a task setting allowing the system to sug-
gest multiple candidates, we observed that performance
is dependent on the choice and granularity of lexical
resources and that resources with a low number of seman-
tic categories and annotations involving mentions of mul-
tiple entities posed specific challenges for the method.
We demonstrated how these issues could be addressed
and were able to show that a 65% average reduction in
the number of candidate categories can be achieved while
maintaining average recall at 99% over a set of 15 corpora
covering biomedical, clinical and newswire texts. We find
these numbers very promising for the applicability of our
system and will seek to integrate it as a component for
other systems to further verify these results.
In future work, we hope to address a number of remain-
ing questions. First, it should be verified experimentally
that our primary metric, the harmonic mean of ambiguity
and recall, represents a reasonable optimisation target for
SCD applications such as annotation support. By varying
the trade-off between ambiguity reduction and recall and
measuring the impact on actual human annotation time
[25], we could empirically study the relationship between
ambiguity and recall for a given task. Furthermore, as we
could observe in our lexical resource experiments, the
optimal composition of lexical resources is dependent on
the data set. While we could have manually constructed a
new collection of lexical resources to cover all the domains
in our experiments, this ad-hoc processes would poten-
tially have to be repeated for each new data set we apply
our method to. Instead, we propose to aim to automat-
ically select the set of lexical resources optimal for each
data set, which we believe to be more likely to result in
long-term benefits and to allow our method to be ben-
eficially applied to novel tasks. By integrating automatic
lexical resource construction and confidence parameter
selection, we hope to be able to create a general-purporse
SCDmethod applicable across tasks and domains without
the need for user intervention.
The system used in this study as well as other resources
are freely available for research purposes at https://github.
com/ninjin/simsem.
Availability of code, corpora and lexical resources
This section covers the availability and sources for the
code, corpora and lexical resources used in this work.
In addition to assuring that those who have provided
resources essential to this study are properly acknowl-
edged, it aims to assist in the replication of the experi-
ments presented in this paper.
The code used for the experiments is available under
a permissive license from https://github.com/ninjin/
simsem. The lexical resources used were Freebase, pro-
vided by Google and retrieved from https://developers.
google.com/freebase/data on February the 9th of 2012,
along with the 10 resources used to create dictionaries in
[3], namely the Gene Ontology [26], the Protein Informa-
tion Resource [27], the Unified Medical Language System
(UMLS) [28], Entrez Gene [29], an Automatically gener-
ated dictionary [30], Jochem [31], the Turku Event Corpus
[32], Arizona Disease Corpus [33], LINNAEUSDictionary
[34] and Websters Second International Dictionary from
1934 (included in /usr/share/dict/web2 in the FreeBSD
8.1-RELEASE). All of the above resources apart from
UMLS are freely available for research purposes without
restrictions. In UMLS, which to the best of our knowledge
is the largest collection of biomedical lexical resources to-
date, some of the component resources are restricted even
for research usage. Please see the UMLS license for further
details.
For our experiments we used the corpora originally used
in [3]. These were: the Epigenetics and Post-Translational
Modifications corpus [35], the Infectious Diseases cor-
pus [22], the Genia Event corpus [36], the Collabora-
tive Annotation of a Large Biomedical Corpus [37], the
BioNLP/NLPBA 2004 Shared Task corpus [38] and the
Gene Regulation Event Corpus [39]. For this work we
also used the following corpora: the Multi-Level Event
Extraction corpus [21], the GeneReg corpus [40], the Gene
Expression Text Miner corpus [41], BioInfer [7], BioText
[42], the Spanish and Dutch subsets of the CoNLL-2002
Shared Task corpus [20], the i2b2 Medication Challenge
corpus (I2B2) [19] and the OSIRIS corpus [43]. The above
corpora are readily available for research purposes with
the exception of the I2B2 corpus, which due to its clinical
nature does not allow for redistribution and/or exposure
beyond researchers that have been explicitly authorised to
utilise the data.
Additional file
Additional file 1: Table S1 Result tables for all data sets andmodels.
The boxed results in the upper left corner signifies the results from [11],
while the unboxed results are additions for the extension of the original
paper. The best score(s) for each data set are underlined and scores which
are not statistically significantly different from the best result(s) with a
P-value of 5% when using Fishers exact test are italicised [3].
Abbreviations
The followings abbreviations were used and introduced in this article.
NER: Named entity recognition; NLP: Natural language processing; SCD:
Semantic category disambiguation; WSD: Word sense disambiguation.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
PS and SP conceived of the methods and PS carried out the experiments and
wrote the manuscript. SA and JT provided coordination and supervision of the
overall research. All authors read and approved the final version of the
manuscript.
Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 10 of 11
http://www.jbiomedsem.com/content/5/1/26
Acknowledgements
We would like to thank Mariana Neves of the Wissensmanagement in der
Bioinformatik (WBI) group at the Humboldt-Universität zu Berlin for providing
us with format conversions for four of the corpora used in this work. We would
also like to thank Hubert Soyer for providing comments and feedback on the
final version of the manuscript. Lastly we would like to thank the reviewers for
their helpful comments, some of which helped define the direction of this
work.
This work was supported by Grant-in-Aid for Specially Promoted Research
(MEXT, Japan) and the Royal Swedish Academy of Sciences.
This article has been published as part of thematic series Semantic Mining of
Languages in Biology and Medicine of Journal of Biomedical Semantics. An
early version of this paper was presented at the Fourth International
Symposium on Languages in Biology and Medicine (LBM 2011), held in
Singapore in 2011.
Author details
1Department of Computer Science, University of Tokyo, Tokyo, Japan. 2School
of Computer Science, University of Manchester, Manchester, UK. 3National
Centre for Text Mining, University of Manchester, Manchester, UK. 4Microsoft
Research Asia, Beijing, Peoples Republic of China.
Received: 19 October 2012 Accepted: 3 April 2014
Published: 2 June 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Lin and He Journal of Biomedical Semantics 2014, 5:19
http://www.jbiomedsem.com/content/5/1/19RESEARCH Open AccessThe ontology of genetic susceptibility factors
(OGSF) and its application in modeling genetic
susceptibility to vaccine adverse events
Yu Lin1,2,3* and Yongqun He1,2,3*Abstract
Background: Due to human variations in genetic susceptibility, vaccination often triggers adverse events in a small
population of vaccinees. Based on our previous work on ontological modeling of genetic susceptibility to disease,
we developed an Ontology of Genetic Susceptibility Factors (OGSF), a biomedical ontology in the domain of
genetic susceptibility and genetic susceptibility factors. The OGSF framework was then applied in the area of
vaccine adverse events (VAEs).
Results: OGSF aligns with the Basic Formal Ontology (BFO). OGSF defines genetic susceptibility as a subclass of
BFO:disposition and has a material basis genetic susceptibility factor. The genetic susceptibility to pathological
bodily process is a subclasses of genetic susceptibility. A VAE is a type of pathological bodily process. OGSF
represents different types of genetic susceptibility factors including various susceptibility alleles (e.g., SNP and gene).
A general OGSF design pattern was developed to represent genetic susceptibility to VAE and associated genetic
susceptibility factors using experimental results in genetic association studies. To test and validate the design
pattern, two case studies were populated in OGSF. In the first case study, human gene allele DBR*15:01 is
susceptible to influenza vaccine Pandemrix-induced Multiple Sclerosis. The second case study reports genetic
susceptibility polymorphisms associated with systemic smallpox VAEs. After the data of the Case Study 2 were
represented using OGSF-based axioms, SPARQL was successfully developed to retrieve the susceptibility factors
stored in the populated OGSF. A network of data from the Case Study 2 was constructed by using ontology terms
and individuals as nodes and ontology relations as edges. Different social network analysis (SNA) methods were
then applied to verify core OGSF terms. Interestingly, a SNA hub analysis verified all susceptibility alleles of SNPs
and a SNA closeness analysis verified the susceptibility genes in Case Study 2. These results validated the proper
OGSF structure identified different ontology aspects with SNA methods.
Conclusions: OGSF provides a verified and robust framework for representing various genetic susceptibility types
and genetic susceptibility factors annotated from experimental VAE genetic association studies. The RDF/OWL
formulated ontology data can be queried using SPARQL and analyzed using centrality-based network analysis
methods.* Correspondence: yuln@med.umich.edu; yongqunh@med.umich.edu
1Unit for Laboratory Animal Medicine, University of Michigan Medical School,
Ann Arbor, MI 48109, USA
2Department of Microbiology and Immunology, University of Michigan
Medical School, Ann Arbor, MI 48109, USA
Full list of author information is available at the end of the article
© 2014 Lin and He; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited.
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 2 of 15
http://www.jbiomedsem.com/content/5/1/19Background
Genetic susceptibility, also called genetic predisposition, is
an increased likelihood or chance of developing a particular
disease (e.g., diabetes) or pathological bodily process (e.g.,
infection) due to the presence of one or more gene muta-
tions with or without a family history of an increased risk
[1]. Genetic susceptibility is associated with all kinds of dis-
ease and traits across the whole medical domain, such as
infectious diseases [2], alcoholism [3], cancer [4], and auto-
immunity [5]. As a more specific example, human vaccin-
ation may induce undesired adverse events, so called
vaccine adverse event (VAE), which may be manifested in
various forms of signs, symptoms and diseases [6]. The
VAE may appear in a small population but not in the
majority of vaccinee population, indicating the genetic sus-
ceptibility in the small population [7,8]. For example, com-
pared to white children, the native American Indian
Apache children have significant impairment of their anti-
body response to H. influenzae type b polysaccharide, thus
they may be prone to develop adverse events if adminis-
tered a H. influenzae vaccine with H. influenzae type b
polysaccharide as its component [9]. Better understanding
of genetic susceptibility factors to specific diseases will
allow us design preventative and therapeutic measures to
prevent and control the diseases in susceptible populations.
Various kinds of genetic variations bear susceptibilities,
e.g., linkage disequilibrium (LD, non-random association
between two or more loci) haplotype, a linkage region,
genetic polymorphisms, alleles and so on. These various
genetic variant entities are named genetic susceptibility
factors by the authors. The allele that confers increased
susceptibility may be inherited; whereas the disease itself
will not. The single locus genotype is usually insufficient
to cause a disease. A disease often appears when impaired
expressions of alleles at other gene loci and/or environ-
mental factors co-exist [10]. Genetic susceptibility factors
might not have obvious mutations. A genetically inherited
disorder is more likely the consequence of a polygenic
combination of variants at several genes that might be
common in healthy humans. Moreover, the main deter-
minants of susceptibility may be different in different
populations [11]. Furthermore, many environmental fac-
tors may interact with genetic factors, and they contribute
to a diseased outcome simultaneously [7,12]. Many appar-
ently contradictory findings in disease-gene association
studies associated with different study designs increase the
complexity of the problem [13]. The sophisticated nature
of genetic susceptibility makes it challenging to identify
true genetic factors associated with human susceptibility
to a specific disease or a pathological bodily process.
The general methodology to identify the genetic suscep-
tibility to complex disease is a combination of linkage and
association studies in biological experimental science. At
first, the family-based studies identify a linkage regioncontains several mega bases of DNA. To narrow down
such a region to a susceptible gene (or genes), population-
based casecontrol studies identify variants in linkage
disequilibrium with the susceptibility locus, which lead
to define the genomic region responsible for the original
linkage signal [14]. Although the original linkage signal
may not be detectable in some studies, combination of dif-
ferent levels of evidence from multiple studies may de-
cipher true genetic susceptibility. At the post-genomics
era, it is possible to use high throughput Omics methods
to identify possible genetic variations that contribute to the
genetic susceptibility. The strategy of applying Omics and
other methods to study host genetic variations and their ef-
fects in vaccine-induced host immune responses (e.g.,
VAEs) has been termed vaccinomics [12]. The notion of
genetic susceptibility can be traced back to 1926 [15]. Since
then, numerous literature reports of genetic susceptibility
have been published. As of December 23, 2013, a PubMed
search for genetic susceptibility has generated over
119,900 hits. However, a database of general genetic
susceptibility factors is not yet available. As a first step
towards systematically collecting and studying genetic sus-
ceptibility factors, there is a need to generate a consensus-
based robust ontological framework for systematically
representing and studying such genetic susceptibility and
the genetic factors contributing to the susceptibility.
A formal ontology is a set of computer- and human-
interpretable terms and relations that represent entities
in a specific domain and how these entities relate to
each other. Ontological terms are expressed in formal
logic to support automated reasoning. Lin et al. have pre-
viously developed an Ontology of Genetic Susceptibility
Factors to Diabetes Mellitus (OGSF-DM) intended to pro-
vide a framework for genetic susceptibility to diseases [14].
By using the TCF7L2 gene and its susceptibility to Type 2
Diabetes (T2D) as an example, OGSF-DM formalizes
the basic definitions of genetic susceptibility and genetic
susceptibility factor. The ontology OGSF-DM is a virtual
ontology composed of three ontologies: the Ontology of
Genetic Disease Investigation (OGDI), which imports other
two ontologies: the Ontology of Glucose Metabolism
Disorders (OGMD) and the Ontology of Geographical
Regions (OGR). The previous study found out that es-
sential variables impacting genetic susceptibility to diseases
include: genetic polymorphism, the population and geo-
graphical location, the disease entities, and related statis-
tical values (e.g., odds ratio and p-value) [14].
The Open Biological and Biomedical Ontologies (OBO)
Foundry community [16] has recently developed many on-
tologies that overlap the scope of OGSF-DM. For example,
within the OBO Foundry, the Ontology for Biomedical
Investigations (OBI) that represents biological and clinical
investigations [17] overlaps with the scope of OGDI; the
ontology Gazetteer (GAZ) that describes environmental
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 3 of 15
http://www.jbiomedsem.com/content/5/1/19places [18] overlaps with OGR. However, the ontological
modeling of genetic susceptibility remains untouched.
The original OGSF-DM was loosely aligned with BFO
1.0 by denoting some classes as subclasses of continuant
or occurrent. The structure of the three OGSF-DM ontol-
ogies did not follow the OBO Foundrys principles [16],
which makes it difficult to be integrated with other OBO
Foundry ontologies. To leverage the reusability and inter-
operability of the community developed ontologies, we
have found that the OGSF-DM would be better if being
refined and focused more on the area of genetic suscep-
tibility. We have thus proposed to develop a single
ontology: the Ontology of Genetic Susceptibility Factors
(OGSF), to represent various types of genetic suscepti-
bility and genetic susceptibility factors supported by
textual conclusions given by genetic association studies.
While the OGSF-DM modeled the genetic susceptibility
to a disease (i.e., diabetes mellitus) [14], genetic susceptibil-
ity is not always associated with only disease. In BFO, a dis-
ease is a subclass of disposition, which is positioned in the
branch of BFO:continuant. The genetic susceptibility is
often associated with the risk of a pathological bodily
process including a vaccine adverse event [19-21]. The
pathological bodily process as defined by the Ontology of
General Medical Science (OGMS) as a process positioned
under the branch of BFO:occurrent [22]. Therefore, the dis-
ease (a dependent continuant) and the pathological bodily
process (a BFO:occurrent) are located in two different
major branches of BFO. To more comprehensively repre-
sent entities related to genetic susceptibility, it is required
for OGSF to represent pathological bodily processes such
as vaccine adverse events.
In this paper, we introduce our development of a new
version of genetic susceptibility-focused ontology: the
Ontology of Genetic Susceptibility Factors (OGSF) by
using BFO 2.0 as its upper ontology. To illustrate theFigure 1 The OGSF hierarchy and key OGSF terms introduced in the pontology and verify our ontology design patterns, two
vaccine adverse event-related genetic susceptibility case
studies were specifically analyzed. Our studies demon-
strate that the OGSF successfully provides an ontological
framework for systematically representing genetic suscep-
tibility, genetic susceptibility factors, associated entities
and relations.
Results
In what follows, single quotes are used to refer to a spe-
cific term within OGSF where appropriate. The numerical
ID following the prefix of ontology is given after the term
is mentioned, which gives the indication of the terms re-
source. Italics are used to indicate the axioms or proper-
ties defined in the ontology.
The new OGSF is aligned with BFO
The development of OGSF follows the OBO Foundry
principles, including openness, collaboration, and use
of a common shared syntax [16]. To align OGSF with
BFO 2.0 version, we started with previously identified
key terms and render them using BFO's terms as par-
ent terms (Figure 1). To enable the reusability of other
ontologies, we have imported many related terms and re-
lations from existing OBO foundry ontologies. For ex-
ample, the terms vaccine (VO_0000001) and vaccination
(VO_0000002) are adopted from the Vaccine Ontology
(VO) [23,24]; the terms adverse event (OAE_0000001)
and vaccine adverse event (OAE_0000004) are imported
from OAE. The relations between these vaccine terms and
VAE terms are defined in the newly generated OVAE [8].
The vaccine related investigation is within the scope of the
OBI, so that some OBI terms, such as investigation and
textual conclusion were imported into OGSF.
In addition to the reuse of existing ontology terms, over
60 OGSF-specific class and property terms exist. The twoaper.
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 4 of 15
http://www.jbiomedsem.com/content/5/1/19OGSF core terms are: genetic susceptibility and genetic
susceptibility factor. The OGSF term genetic suscepti-
bility (OGSF_0000000) is a subclass of BFO:disposition
(BFO_0000016). The alternative term for genetic suscepti-
bility is genetic predisposition. In BFO 2.0, the genetic
and other risk factors for specific diseases are considered
as predispositions, i.e., they are dispositions to acquire
other dispositions. The realization of such a predisposition
consists in processes which change the physical makeup
of its bearer in such a way that parts of this bearer serve
as the material basis for a disease [25]. Since the term pre-
disposition is not included in current version of BFO 2.0,
we assert OGSF genetic susceptibility as an immediate
child of BFO term disposition. The child terms of genetic
susceptibility include: genetic predisposition to disease of
type X (OGMS_0000033) and genetic susceptibility to
pathological bodily process (OGSF_0000001). The term
that reflects our use cases is genetic susceptibility to vac-
cine adverse event (OGSF_0000010), which is a child term
of genetic susceptibility to pathological bodily process.
Another OGSF core term genetic susceptibility factor
(OGSF_0000004) is a subclass of material entity (BFO_
0000040). Any allele, gene, genotype, or haplotype may be
a genetic susceptibility factor if a genetic association study
supports the association between any of those entities and
a phenotype. The relation material basis of at some time
(BFO_0000127) is formalized in BFO 2.0 to represent the
relation between a material entity and a disposition [25].
BFO 2.0 refers disposition to the potentials or powers of
things in the world. Whenever a disposition exists, it is a
disposition of something, namely its material bearer [25].
This relation is adopted to represent the relation between
genetic susceptibility factor and genetic susceptibility in
OGSF. At the instance level, the same genetic susceptibil-
ity factor bearing genetic susceptibility in a person has its
entire existence all the time. But this statement may not
be true at the class level. The same genetic susceptibility
factor may contribute differently to the manifestation of a
disease at different time periods and conditions. Such a
meaning is reflected in the words at some time of the re-
lation material basis of at some time.
OGSF represents different types of genetic susceptibil-
ity factors, including haplotypes, genes, single nucleotide
polymorphisms (SNPs), and alleles. A haplotype is a com-
bination of DNA sequences at adjacent locations (loci) on a
chromosome that can be inherited together from a single
parent. A haplotype can describe a pair of genes on one
chromosome or all genes on a chromosome from a parent.
A haplotype can also refer to an inherited cluster of SNPs
that are variations at single positions in the DNA sequence
among individuals. An allele is an alternative form of the
same gene or other genetic material that occupies a specific
location on a chromosome. The Ontology for Genetic
Interval (OGI) [26] defines different subclasses of alleleincluding allele of gene, allele of SNP and allele of haplo-
type. Since every individual has two parents who each con-
tribute one allele, genetic susceptibility factors can usually
be represented by the notion of allele. Sometimes two or
more SNPs work together and contribute to genetic suscep-
tibility. Two situations existed for this condition: the collab-
orative SNPs from one haplotype, and the synergistic
combinations of SNPs from different haplotypes. Such
cases are represented as aggregate SNPs in OGSF. OGSF
fully imports OGI, thus it inherits the OGIs allele classes
and definitions. OGSF inherits the OGI classification of
haplotype, genes and SNPs as material entities containing
sequence information [27]. Different from OGI, the DNA
sequences in the Sequence Ontology (SO) represents se-
quence information itself [28]. The SO also does not dif-
ferentiate different allele types. These are the reason why
we use OGI instead of SO in OGSF. A new relation is_al-
lele_of_gene has been created to link allele of gene and
gene. This relation is required for logical definition and
correct reasoning in susceptibility allele of gene analysis as
shown in our Case Study 2 described later in the paper.
In total, OGSF contains over 600 class and property
ontology terms as shown on http://www.ontobee.org/
ontostat.php?ontology=OGSF. In our VAE susceptibility
use case studies, we have also generated many OGSF
instances as introduced later in this paper.
Modeling genetic susceptibility to vaccine adverse event
As defined in the Vaccine Adverse Event Reporting System
(VAERS) and Ontology for Adverse Event (OAE), a
vaccine adverse event is an adverse event following
vaccination and does not necessarily assume a causal
association [8,20,21]. However, a causal association be-
tween administration of a specific vaccine and an ad-
verse event in a particular population can be identified
through systematic and statistical studies [7,12,29,30].
Although a large number of studies have provided sup-
porting evidences for asserting susceptibility factors (e.g.,
susceptibility alleles) to vaccine adverse event outcomes,
the results of these studies cannot be automatically proc-
essed by computers. Our OGSF presentation aims to cre-
ate a machine-interpretable ontological representation of
these studies in order to analyze the results across studies
and search for possible causal associations.
Figure 2 illustrates the design pattern of how OGSF is
used to represent the association between a genetic sus-
ceptibility factor and a vaccine adverse event (VAE)
based on experimental studies reported in the literature.
As shown in the figure, the genetic susceptibility factor
is the material basis of genetic susceptibility. The gen-
etic susceptibility to vaccine adverse event is realized in
the process of vaccine adverse event (OAE_0000004).
In the vaccine case, the genetic susceptibility factor is a
part of a human vaccinee carrying susceptibility allele
Figure 2 General design of OGSF representing genetic susceptibility to vaccine adverse event. Square boxes denote classes, and italicized
terms along the arrows denote relations.
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 5 of 15
http://www.jbiomedsem.com/content/5/1/19for adverse event (OGSF_0000029), which actively par-
ticipates in the vaccine adverse event. As a participant
of a genetic association investigation (OGSF_0000016),
a case group (OGSF_0000022) has a member of human
vaccinee carrying susceptibility allele for adverse event.
A human vaccinee is vaccinated with a vaccine. The
vaccination occurs before (or is preceded by) a vaccine
adverse event. As a specified output of the genetic asso-
ciation investigation, the textual conclusion of genetic
susceptibility concludes the association between a genetic
susceptibility factor and a vaccine adverse event. Below
we provide more specific details to introduce this OGSF
design pattern.
The direct linkage from susceptibility-related terms to
VAE terms is often required in our OGSF modeling. For
example, in OGSF, we need to link human vaccinee carry-
ing susceptibility allele for adverse event (OGSF_0000029)
to a vaccine. An object property term (ontological relation)
reflecting such linkage is not available in existing ontol-
ogies. However, VO defines a shortcut relation vaccine
immunization for host, which relates a vaccine with a vac-
cinee [23]. The strategy of designing and using shortcut re-
lations has been promoted by Mungall et al. to simplify the
complex axioms involving nested class expressions to make
it triple-friendly for complex OWL ontologies [31]. In our
design, the inverse usage of this VO shortcut relation vac-
cine immunization for host connects from human vaccinee
carrying susceptibility allele for adverse event to vaccine.
The term genetic association investigation (OGSF_
0000016) is defined as: an investigation that aims to test
whether single-locus alleles or genotype frequencies (or
more generally, multi-locus haplotype frequencies) dif-
fer between two groups of individuals (usually diseasedsubjects and healthy controls). Different types of those
studies exist. For example, a case control genetic associ-
ation study (OGSF_0000017) is a genetic association study
that contains two types of human study subject groups:
case group and control group. The control group provides
a background control in order to properly assess the results
identified from the case group study. In contrast, a case-
only genetic association study (OGSF_0000036) includes a
case group and does not have a control group to compare.
The results obtained from a case-only genetic association
study provide sufficient evidence to detect an association
[32]. However, they are often biased by pre-condition of
non-independence between the genetic and environmen-
tal factors in the population [33]. Another type of gen-
etic association study is family-based genetic study
(OGSF_0000041) that investigates family members who
may show different phenotypes. By analyzing entire ge-
nomes of people with a disease (cases) and similar
people without the disease (controls), a Genome-Wide
Association Study (GWAS or GWA study) examines
many common genetic variants in different individuals
to see the association between variant and a trait [7,12].
Such a genome wide association study is a type of case
control genetic association study.
A reported genetic susceptibility study typically includes
a conclusion of the association between a genetic factor
and a disease (or pathologic bodily process) under specific
conditions. Such conclusion is required to be represented
ontologically. To represent the results from individual
genetic association studies as reported in different papers,
we have added an OGSF term textual conclusion of
genetic susceptibility to represent the textual conclu-
sion of a genetic susceptibility study. Ontologically, a
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 6 of 15
http://www.jbiomedsem.com/content/5/1/19textual conclusion of genetic susceptibility is asserted as a
specified output of  a genetic association investigation.
There are three types of textual conclusion of genetic sus-
ceptibility: positive textual conclusion of genetic suscepti-
bility (OGSF_0000031), negative textual conclusion of
genetic susceptibility (OGSF_0000032) and neutral text-
ual conclusion of genetic susceptibility (OGSF_0000033).
Using the vaccine adverse event example, a positive text-
ual conclusion of genetic susceptibility means that a posi-
tive conclusion is drawn based on a significant statistical
association of a genetic factor and a vaccine adverse event
as identified in a published paper. A negative textual con-
clusion of genetic susceptibility denies such a possible as-
sociation between a genetic factor and an adverse event as
declared in a published paper. Sometimes, depending on
the data, an investigator might not be able to draw a de-
finitive positive or negative conclusion on a genetic sus-
ceptibility association. This situation is captured using
neutral textual conclusion of genetic susceptibility. In
addition, OGSF also provides several datatype properties,
such as hasOddsRatio and hasPvalue, to allow the repre-
sentation of digital data for statistical evaluation of the
textual conclusion of genetic susceptibility (Figure 2).Use case studies
Case studies are used for two purposes: 1) to validate the
modeling, 2) to test possible applications of the ontology.
Below we represent two case studies reported from peer-
reviewed journal articles using the OGSF framework.Figure 3 OGSF modeling of vaccine-associated multiple sclerosis. SquCase study 1: HLA allele DBR1*15:01 is genetic susceptibility
to Pandemrix related multiple sclerosis in a case report study
Pandemrix is an influenza pandemics vaccine that is de-
veloped by the company GlaxoSmithKline. The vaccine
Pandemrix is represented in the Vaccine Ontology (VO)
with the VO ID: VO_0000410. Vrethem et al. reported the
occurrence of severe Multiple Sclerosis (MS) in a previously
healthy young male in association with the vaccination of
Pandemrix [34]. In this study, a human DBR1*15:01 allele is
responsible for association with the Pandemrix-related MS
adverse event. DBR1*15:01 is an allele of human leukocyte
antigen (HLA) complex that encodes a MHC class II cell
surface receptor. The association of this allele with MS
appears to be consistent with many previous reports on
situations other than vaccine adverse event [35,36].
This genetic susceptibility case was represented in
Figure 3 by following the general OGSF design pattern
(Figure 2). For ontological modeling, it is critical to gener-
ate description logic constraints and axioms to accurately
represent human- and computer-interpretable knowledge.
As an example, the basic information about DRB1*15:01
can be ontologically represented as:
 DRB1*15:01 is subclass of allele of gene.
 DRB1*15:01 is subclass of (is_allele_of_gene some
HLA DBR1 gene).
In addition to the above basic logical definitions, genetic
susceptibility related to DBR1*15:01 can be identified
based on different studies. Case Study 1 is such a study,are boxes denote classes, and curved box denote instances.
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 7 of 15
http://www.jbiomedsem.com/content/5/1/19which is represented as genetic association study_1
(Figure 3). This study generated a specific output posi-
tive conclusion of genetic susceptibility_1. This specific
conclusion is about the class DBR1*15:01 and the
multiple sclerosis AE. The instance of DBR1*15:01 is
a part of the specific patient in the case study. Based on
this and many other case reports [34-36], we have gen-
erated the OGSF representation at the class level:
 DRB1*15:01 is subclass of (part of continuant at all
times that whole exists some (human vaccinee and
(inverse (vaccine immunization for host) some
Pandemrix)))
 DRB1*15:01 is subclass of (material basis of at
some time some genetic susceptibility to vaccine
adverse event)
 DRB1*15:01 is subclass of susceptibility allele
This case study indicates that OGSF provides necessary
elements to represent genetic susceptibility and genetic sus-
ceptibility factors associated with vaccine adverse events.
Case study 2: genetic polymorphisms associated with
adverse events after smallpox vaccination in multiple
clinical trials
Reif et al. reported that genetic polymorphisms in several
genes encoding important immune factors, including en-
zyme methylenetetrahydrofolate reductase (MTHFR), anTable 1 Statistical summary of genetic susceptibility
factors with systemic adverse event following smallpox
vaccination
GSF& Allele Gene Odds ratio
(confidential
interval)
P-value Study
1 or 2
rs1801133 SNP T MTHFR 2.3 (1.15.2) 0.04 1
rs1801133 SNP T MTHFR 4.1 (1.411.4) 0.01 2
rs9282763 SNP G IRF1 3.2 (1.19.8) 0.03 1
rs9282763 SNP G IRF1 3.0 (1.18.3) 0.03 2
rs839 SNP A IRF1 3.2 (1.19.8) 0.03 1
rs839 SNP A IRF1 3.0 (1.18.3) 0.03 2
Haplotype 1* G,A IRF1 3.2 (1.010.2) 0.03 1
Haplotype 1* G,A IRF1 3.0 (1.09.0) 0.03 2
Haplotype 2# T,C,A IL4 2.4 (1.05.7) 0.05 1
Haplotype 2# T,C,A IL4 3.8 (1.014.4) 0.06$ 2
Notes:
&GSF stands for Genetic Susceptibility Factor.
*Haplotype 1 contains G allele of rs9282763, and A allele of rs839 in
IRF1 gene.
#Haplotype 2 contains T allele of rs2070874, C allele of rs2243268, and A allele
of rs2243290 in IL4 gene.
$In the second study, Haplotype 2 didnt achieve the significant level of
p value.
The data of this table is summarized and curated from Reif. et al.s work on the
genetic analysis of adverse event after smallpox vaccination [37].immunological transcription factor (IRF1), and interleukin-
4 (IL-4), were associated with adverse events after smallpox
vaccination [37]. In this report, two independent clinical tri-
als were conducted as initial and replicating genetic associ-
ation studies. Different from the Case Study 1 where an
allele of gene is a susceptibility factor, susceptibility alleles
of Single Nucleotide polymorphisms (SNPs) are the mater-
ial basis of genetic susceptibility in this Case Study 2. Table 1
lists all the SNPs (e.g., the A allele of rs839 SNP in the gene
irf1), their associated genes, and the Odds Ratio and p-
value from two clinical trials [37].
The OGSF design pattern was applied to represent the
information from these clinical trial studies (Figure 4).
This figure does not include many linkages and axioms
similar to those illustrated in Figure 3. Instead, Figure 4
focuses on representation of statistics providing evidence
indicating the type of genetic associations to vaccine
adverse events. In OGSF, the datatype property has-
Size allows the recording of the size of a human study
subject group such as case group. The datatype prop-
erties hasOddsRatio, hasPvalue and hasCI (confidence
interval) link the corresponding data to specific textual
conclusion of genetic susceptibility. The Odds Ratio,
P-value, and confidential interval are used to measure
the association between genotypes and vaccine adverse
event [37]. The Odds Ratio represents the ratio that an
outcome will occur given an exposure, compared to
the odds of the outcome occurring in the absence of
the same exposure [38]. Using these datatype proper-
ties, the values of these measurements were captured
and represented within the ontology. For example, the
conclusion of clinical trial 1 regarding the T allele of
rs1801133 SNP was supported by the statistical data:
having an Odds Ratio of 2.3, a P-value 0.03, and a con-
fidence interval of [> = 1.4, <=11.4]. These statistical
results support a positive genetic association between
the allele of SNP and systemic adverse events of smallpox
vaccination [37].
Since OGSF provides a framework to ontologically
represent the complex data structure (including differ-
ent variables and relations among these variables), the
representation of the knowledge and data using OGSF
supports computer-assisted data integration and rea-
soning. Such data sets can be queried efficiently using
SPARQL as described below.SPARQL query
The SPARQL Protocol and RDF Query Language
(SPARQL) is the query language and protocol for the
Resource Description Framework (RDF) data. RDF de-
composes any knowledge into triples. Each RDF triple
contains three components: subject, predicate, and ob-
ject [39]. OGSF is developed using the Web Ontology
Figure 4 OGSF modeling of case study 2. Square boxes denote classes, and curved boxes denote instances.
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 8 of 15
http://www.jbiomedsem.com/content/5/1/19Language (OWL) [40]. Both RDF and OWL are means to
express increasingly complex information or knowledge,
and both can be serialized in the RDF/XML syntax. RDF by
itself has a limited capability for formal knowledge represen-
tation. OWL adds ontological capability to RDF by defining
the components of RDF triples with formal computable first
order description logic. So OWL provides more semantic
richness. In addition, the OGSF OWL document can be
converted to RDF format and queried by SPARQL.
From the OGSF supported knowledge system, our
questions are focused on: 1) the list of susceptibility fac-
tors to a certain disease or pathological bodily process;
2) the evidences, either supportive or negative, support-
ing those susceptibilities. Using Case Study 2 as an
example, we designed a SPARQL query to identify the
genetic susceptibility factors to systemic adverse event
of smallpox vaccination and related statistical evidences.
The SPARQL script developed to query against the
OGSF ontology is provided as follows:This query was executed in the SPARQL plugin embed-
ded with Protégé 4.3, build 304, and it could also be per-
formed using the SPARQL endpoint (http://www.ontobee.
org/sparql/index.php) in Ontobee [41], a linked data web-
server where OGSF was deployed. The SPARQL execution
retrieved five susceptibility factors to systemic smallpox
vaccine adverse event as shown in Additional file 1 and
listed below:
1. T allele of rs1801133 SNP supported by 1 positive
evidence.
2. G allele of rs9282763 SNP supported by 2 positive
evidence.
3. A allele of rs839 SNP supported by 2 positive
evidence.
4. haplotype 1 in IRF1 gene supported by 2 positive
evidence.
5. haplotype 2 in IL4 gene supported by 1 positive
evidence, and 1 negative evidence.
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 9 of 15
http://www.jbiomedsem.com/content/5/1/19The SPARQL query output is consistent with the results
obtained from the paper (Table 1). Therefore, our evalu-
ation confirms the value of OGSF ontology representation
of genetic susceptibility knowledge and instance data set.
Social network analysis and visualization
After an ontology is generated, it is often valuable but
challenging to determine which ontology terms are more
central and carry more information than other terms in
the ontology. As an ontology defines terms and relations
(object properties) between terms, an ontology can be
viewed a social network. Specifically, the terms and rela-
tions of an ontology can be viewed as a directed hyper-
linked graph G = (V, E) with nodes v?V and edges e?E,
where the nodes correspond to the terms or entities in
an ontology, and a directed edge (p, q) ? E indicates the
relation that links from p (i.e., the relations domain) to
q (i.e., the relations range). Therefore, the methods used
for social network analyses may be potentially used for
identifying key ontology terms as hubs or clusters of
ontology terms [42]. In this study, we aimed to apply
known social network analysis methods to evaluate the
structure of the OGSF ontology and examine whether
OGSF was constructed effectively to represent key entities
for study of genetic susceptibility and genetic susceptibility
factors as we designed.
Social Network Analysis (SNA) is the sum of the tools
and methodologies of graph theory to analyze and thus
describe structures of social networks [43]. Many SNA
methods also overlap with network analysis methods
from other domains such as literature mining-derived
gene network analyses [44]. Two questions have been
pre-designed for such social network analyses: Firstly,
can the use case data support such identified central
terms in the network? Secondly, can different network
analysis methods generate different results and insights?
To address these questions, the data from Case Study 2
were extracted using OntoGraf [45], and then visualized
and analyzed using social network visualization tool
Gephi [46]. The software was used to conduct the ana-
lyses of the degree centrality, closeness centrality, and
hubs and authority scores to measure the relative im-
portance of a node within the network. The statistical
measurement data of these analyses are included in
Additional file 2.
The first method of our network analysis was based on
the calculation of the degree centrality (Figure 5A). The
degree centrality is simply the number of direct edges
that an entity has in a network [43,44]. The network has
24 nodes and 38 edges with an average degree of 1.538.
Our analysis found that the two terms with the highest
degree centrality scores are systemic adverse event of
smallpox vaccination, and haplotype 2 in IL4 gene.
These two terms have the highest numbers of links toother terms. These findings are consistent with the
knowledge stored in the ontology. However, the term
haplotype 2 in IL4 gene is not our intended core terms.
This gives us insights that the degree measurement only
cannot verify the core terms of the current network.
Secondly, we used the closeness centrality for network
exploration (Figure 5B). The closeness centrality mea-
sures the average shortest path from a node to all other
nodes. Specifically, the closeness centrality calculates
the inverse of the farness that is the sum of a nodes
distances to all other nodes [47]. The more closeness
centrality a node is, the easier it can be reached by
other nodes or reach out other nodes. The five ontology
terms that have the best closeness centrality scores and
have no out-reaching nodes are genetic susceptibility
to vaccine adverse event, systemic adverse event fol-
lowing smallpox vaccination, IL4 gene, IRF1 gene,
and MTHFR gene. The result is consistent with the
design and construction of the ontology: the evidence
link to genetic susceptibility and vaccine adverse event,
the variants link to genes. It is interesting that all the three
genes were identified together in this study.
The third network analysis was based on the calcula-
tion of the authority and hub scores [47,48] (Figure 6).
The terms (nodes) that many other terms point to are
called authorities. In contrast, the terms pointing to a
relatively high number of authorities are called hubs.
The authorities and hubs are a natural generalization of
the eigenvector centrality that measures the influence of
a node in a network. The authority analysis has been
used for ranking web pages, and the data and ontologies
from the Semantic Web search [49]. Figure 6A shows
that top three authority centralized nodes: systemic ad-
verse event of smallpox vaccination, genetic suscepti-
bility of vaccine adverse event, and IL4 gene. There
results indicate: 1) the main focus of this piece of linked
data is about systemic adverse event of smallpox vaccin-
ation and genetic susceptibility; 2) IL4 gene carries more
information flow than others, for it is connected with two
kinds (positive and negative) of evidence and a haplotype
of three SNPs in the network. Figure 6B shows nodes with
highest hub scores. Interestingly, these identified hubs are
all the SNPs related to the adverse event concluded in
Case Study 2.
In summary, different network characteristics cal-
culations reflect different dimensions of the ontology
knowledge. The closeness and authority centrality ana-
lyses verified the core terms of the OGSF dataset in
case study 2 are systemic adverse event of smallpox
vaccination and genetic susceptibility of vaccine ad-
verse event. Interestingly, the hub analysis identified
all the alleles of SNPs, and the closeness analysis de-
tected all three hidden genes that are related to those
alleles of SNPs. It is noted that the genes instead of the
Figure 5 Degree and closeness network analyses using Case Study 2 data modeled in OGSF. (A) Degree centrality. The size of a node
indicates the degree of the node indicating the number of connections from the node. (B) Closeness centrality. The closeness centrality analysis
identified all three genes in the case study dataset. The visible nodes in the figure all have closeness centrality value equal to 0. The nodes in the
figure represent classes and instances contained in the case study. Those nodes displayed in the same color are clustered in the same group by
the modularization method of the software Gephi [46].
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 10 of 15
http://www.jbiomedsem.com/content/5/1/19alleles of SNPs are usually found by direct literature
searching. Based on these observations, our network
analyses accurately identified ontology terms essential
for representing genetic susceptibility and genetic sus-
ceptibility factors.
Discussion
In this paper, we have introduced the development of
the new version of the Ontology of Genetic Susceptibil-
ity Factors (OGSF) and its usage for ontologically repre-
senting genetic susceptibility to vaccine adverse events.
The new OGSF is aligned with the BFO 2.0. OGSF im-
ports many terms from existing ontologies and also in-
cludes many new ontology terms. For the first time, we
have ontologically represented the genetic susceptibility
to a pathological bodily process (i.e., vaccine adverseevent). Two vaccine adverse event use cases were repre-
sented and evaluated. The SPARQL and social network
analyses were implemented to evaluate and analyze the
OGSF contents and structure. Different social network
analysis methods identified ontology terms with differ-
ent types of importance in the ontology.
OGSF emphasizes the classification of different genetic
factors and polymorphisms associated with susceptibility
to diseases or pathological bodily processes. Some suscep-
tibility factors may be genotype or mutation, which can be
expressed using different allele classes. Moreover, OGSF
has several classes, such as susceptibility SNP interval, sus-
ceptibility gene, and susceptibility haplotype to host those
entities that is not allele per se. For example, in the con-
structed network of our case study 2, the IL4 gene is the
third authoritative node but the first gene identified from
Figure 6 Authority and hub network analyses using Case Study 2 data modeled in OGSF. (A) Authority analysis. The top 3 node with the
highest authority score are systemic adverse event of smallpox vaccination, genetic susceptibility to vaccine adverse event, and IL4 gene.
(B) Hub analysis. Hub nodes in this network are all the SNPs. All the visible nodes have the highest hub score of 0.08.
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 11 of 15
http://www.jbiomedsem.com/content/5/1/19the authority analysis (Figure 6A). From the SPARQL
query result, only haplotype 2 of IL4 gene is linked to two
different evidences: the positive conclusion from trial 1
and the negative conclusion from trial 2. Moreover, the
haplotype 2 of IL4 gene is consisted of three SNPs that is
more than other haplotype in the network (Table 1). This
structure increases the ranking of IL4 gene in the author-
ity analysis comparing to other genes. More interestingly,
in another genetic susceptibility to smallpox vaccine ad-
verse event study, a haplotype in IL4 gene is related with a
decrease of the susceptibility to fever after vaccination
[50]. This haplotype contains a SNP rs2243250 located in
the promoter region of IL 4 gene, where a C?T substitu-
tion is associated with increased production of IL-4 [50].
Searching the HaploReg database [51], this SNP is pre-
dicted to be located in the same haplotype of IL4 gene in-
troduced in Case Study 2. This example shows the
complicated role that IL4 gene polymorphisms play in thesystemic adverse event triggered by smallpox vaccination.
It also shows the importance of representing the increase
or decrease (resistance) of genetic susceptibility.
In addition to the genetic susceptibility factors, many
other variables may also contribute to the manifestation of
a disease or a pathological bodily process outcome (e.g.,
vaccine adverse event) [30]. For example, the human in-
dividuals characteristics, such as race/ethnic identity,
geographical region, and disease history, may also play
an important role in the manifestation of an adverse
outcome. Different genetic study design, such as family
study or population-based study, may lead to different
conclusions. To identify possible causality between a
genetic susceptibility factor and a VAE, a statistical ana-
lysis is often required. The sample size of human subjects
involved will also affect the statistical power of genetic
association studies. Our integrative OGSF framework has
incorporated many statistical terms in order to measure
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 12 of 15
http://www.jbiomedsem.com/content/5/1/19the robustness of the genetic association with a specific
disease or pathological outcome. The statistical measure-
ment then gives foundations to support the true genetic
association between genetic susceptibility factors and re-
lated disease or pathological bodily process. Well-designed
experiments may be applied to verify the association.
Different methods can be used for ontology evalua-
tions [52]. A use case analysis is critical to evaluate the
correctness, completeness, and utility of an ontology.
Two use cases have been chosen and presented in the
paper to illustrate how OGSF is logically constructed
and useful in representing genetic susceptibility to vac-
cine adverse events. To further evaluate the ontology
utility in addressing specific questions, we designed and
implemented SPARQL queries to identify known gen-
etic susceptibility factors to smallpox vaccine-induced
systemic adverse events as shown in the second use case.
Furthermore, different social network analyses were ap-
plied to identify and verify the key ontology terms essential
in the topic.
Although social network analysis (SNA) has been widely
used in the fields of web search and social studies, its ap-
plication in ontology field is rare. SNA uses graph theories.
Since ontologies can be considered as (labeled, directed)
graphs, graph analysis techniques are promising tools for
evaluating ontologies in many dimensions. Hoser et al.
have applied SNA to analyze the structures of Suggested
Upper Merged Ontology (SUMO) and SWRC ontology
[43]. Harth et al. and Hogan et al. have been developing
search strategies using network-based approaches to mine
linked data in semantic web respectively [49,53]. Their
studies show that the SNA of a given ontology provides
deep insights into the structure of ontologies and know-
ledge base. These ontology-related SNA studies treated all
ontology classes and relations as network nodes. Different
from this approach, our SNA analyses only consider ontol-
ogy classes and their instances as nodes and make ontol-
ogy relations (i.e., object properties) as edges. Our distinct
treatment of ontology relations as edges makes senses
since these relations are designed to link different classes
and their instances. Our SNA study found that the
visualization and social network analysis results using
the Case Study 2 data provide better understanding of
ontology designing and evaluation. Interestingly, our
SNA hub and closeness analyses generated two distinct
sets of results. The hub analysis identified all five sus-
ceptibility alleles of SNPs as top key terms while the
closeness analysis detected all three susceptibility genes
collected in the Case Study 2. The SNA hubs are terms
directed to the high authority terms. Our identification
of all the SNPs as hubs is consistent with the notion that
these SNPs are essential for the authority terms such as
systemic adverse event of smallpox vaccination and
genetic susceptibility of vaccine adverse event. Thecloseness centrality measures how a node can be easily
reached by other nodes. As the genes have different sus-
ceptibility variants (i.e., SNPs of genes), it makes sense that
the genes have better closeness centrality scores than their
variants. Since these genes are not directly defined as gen-
etic susceptibility factors, the genes appear to be hidden
factors that can be mined from the OGSF data. When we
consider the gene functions, the direct gene name extrac-
tion gives more biological meaningful information than
the variants themselves. These distinct observations sug-
gest that different SNA analysis methods may identify
ontology terms essential from different aspects.
Other than OGSF, many other research projects also
focus on establishing and cataloging the relation between
genotypes and phenotypes. For example, the Database
of Genotypes and Phenotypes (dbGaP) is a repository
for archiving, curating, and distributing the information
obtained from studies investigating the interactions of
genotypes and phenotypes [54]. SNPedia is focused on
the medical, phenotypic and genealogical associations of
SNPs [55]. The Leiden Open (source) Variation Database
(LOVD) provides open data of genetic variants curated
from published paper, and the disease association in-
formation is included [56]. GWAS central (previously
called HGBASE, HGVbase and HGVbaseG2P) pro-
vides a centralized compilation of summarized findings
from genetic association studies [57]. These resources
provide structured raw or curated information related
to genotypes and phenotypes. However, unlike OGSF,
these resources do not ontologically represent different
genetic susceptibility types and genetic susceptibility
factors with all necessary information and evidence as-
sertions. OGSF is able to serve as an intermediate and
an integrative layer between various evidence-based
medicine applications and above existing structure data
resources and other unstructured data resources.
Our study clearly shows that OGSF provides a robust
platform to support logical representation and analysis
of genetic susceptibility and genetic susceptibility factors.
Such platform will allow us to logically organize the know-
ledge and data related to genetic susceptibility and genetic
susceptibility factors. With the well-organized informa-
tion, it is then possible to generate automatic reasoning
programs to analyze the data, predict new knowledge on
genetic susceptibility, and support personalized medicine
research. However, while the use case studies out of the
literature curation were meant for evaluating and validat-
ing the OGSF framework, it would be a huge effort to
manually curate all the possible data available in the litera-
ture. To improve the study of genetic susceptibility factors,
it might help to devote more programing effort to select-
ively integrate related data sources from openly accessible
resources such as the SNPedia [55] as introduced above.
Advanced text mining programs may also be developed to
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 13 of 15
http://www.jbiomedsem.com/content/5/1/19retrieved related information from unstructured literature
data. Following these programming efforts, a large amount
of manual curation may also be requested for expanding
the ontology and making it more useful. To achieve a
long-term goal of solving susceptibility issues, some spe-
cific domains may initially be focused. We are looking for
collaborations for further applying OGSF for practical
usage for scientific domains.Conclusions
Originated from previous OGSF-DM research [14], the
new Ontology of Genetic Susceptibility Factors (OGSF)
is aligned with the framework of BFO 2.0 and developed
to ontologically represents various genetic susceptibility
types, genetic susceptibility factors, and related entities
and relations. OGSF has been used to represent genetic
susceptibility and susceptibility factors associated with
vaccine adverse events as annotated from experimental
studies. Our SPARQL and network evaluations have
shown that OGSF is able to provide a robust framework
for the representation and analysis of genetic susceptibil-
ity knowledge and datasets. The social network analysis
results also demonstrated that key ontology terms crit-
ical in different aspects can be detected with different
centrality-based network analysis methods.Methods
Ontology editing
The format of OGSF ontology is W3C standard Web
Ontology Language (OWL2) (http://www.w3.org/TR/owl-
guide/). For this study, many new terms and logical def-
inition were added into original OGSF [14] using the
Protégé 4.3.0 build 304 OWL ontology editor (http://
protege.stanford.edu/).Ontology term reuse and new term generation
OGSF imports the whole set of the Basic Formal Ontology
(BFO) [58]. To support ontology interoperability, terms
from OBO Foundry ontologies, such as OBI, OAE, IAO
and etc., are reused. For this purpose, OntoFox [59] was
applied for extracting individual terms from external on-
tologies. For those genetic susceptibility-specific terms, we
generated new OGSF IDs with the prefix of OGSF_
followed by seven-digit auto-incremental digital numbers.
New OGSF terms created according to the intensive mod-
eling from the use cases.Evaluation of OGSF by SPARQL
Use case studies were designed based on literature survey.
SPARQL was performed using the SPARQL query plug-in
embedded with Protégé 4.3.0 build 304.Evaluation of OGSF by social network analysis
Graphed data used for visualization was first extracted
from OGSF using the OntoGraf plug-in [44]. After man-
ual editing, the file (Additional file 3) was used as input
for the network visualization software Gephi 0.8.2 beta
(http://gephi.org) [45]. Gephi was also used to conduct
social network data analysis and visualization based on
the extracted data. The embedded algorithms in Gephi
were used to calculate the scores of degree, closeness
[59], and hub and authority [46].
Availability and access
The website for OGSF project is available at http://code.
google.com/p/ogsf/. As an OBO Foundry library ontol-
ogy, OGSF has been deposited by default in the Ontobee
linked data server [41]. All OGSF terms can be browsed
and searched via the Ontobee at http://www.ontobee.
org/browser/index.php?o=OGSF. The source of the ontol-
ogy is also deposited in the NCBO Bioportal: http://bioportal.
bioontology.org/ontologies/3214.
Additional files
Additional file 1: Screen shots of SPARQL queries. (A) SPARQL query
used in Ontobee SPARQL query endpoint. The file includes the SPARQL
query script used in the Ontobee SPARQL query endpoint (http://www.
ontobee.org/sparql/index.php) and its results as returned by the Ontobee
SPARQL query server. (B) Additional File 4.png. Screen shot of Protégé
SPARQL query tab showing the SPARQL query result.
Additional file 2: Network characteristic measurements of each
node in the use case 2 graph. The file includes in-degree, out-degree,
degree, authority, hub, modularity, clustering, strength, local clustering
coefficient, eigenvector centrality, PageRank, eccentricity, closeness
centrality, betweenness centrality scores of the 24 nodes in the graph.
The calculation was conducted by using Gephi software.
Additional file 3: The input file used for network visualization
analysis using Gephi software. The file includes the data of individuals
and related classes of case study 2 extracted from OGSF ontology.
Abbreviations
BFO: Basic formal ontology; FOAF: Friend of a friend project; HLA: Human
leukocyte antigen; GAZ: Gazetteer; IAO: Information artifact ontology;
LD: Linkage disequilibrium; OAE: Ontology of adverse event; OBI: Ontology
for biomedical investigations; OBO: Open biological and biomedical
ontologies; OGDI: Ontology of genetic disease investigation; OGI: Ontology
for genetic interval; OGMD: Ontology of glucose metabolism disorders;
OGMS: of General medical science; OGR: Ontology of geographical regions;
OGSF: Ontology of genetic susceptibility factors; OGSF-DM: Ontology of
genetic susceptibility factors to diabetes mellitus; OVAE: Ontology of vaccine
adverse event; OWL: Web ontology language; REO: Reagent ontology;
SKOS: Simple knowledge organization system; SNA: Social network analysis;
SNP: Single polymorphism nucleotide; SPARQL: SPARQL protocol and RDF
query language; SUMO: Suggested upper merged ontology; URI: Uniform
resource identifier; VO: Vaccine ontology.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
YL: Primary OGSF developer, use case testing, social network analysis and
visualizing, drafting of manuscript. YH: OGSF developer, vaccine adverse
Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 14 of 15
http://www.jbiomedsem.com/content/5/1/19event domain expert, use case testing, and drafting of manuscript. Both
authors read and approved the final manuscript.
Acknowledgements
This work has been supported by grant R01AI081062 from the National
Institute of Allergy and Infectious Diseases. We also appreciate the editorial
review of this manuscript by Dr. John Myles Axton. The publication fee was
paid by a discretionary fund from Dr. Robert Dysko, the director of the Unit
for Laboratory Animal Medicine (ULAM) in the University of Michigan.
Author details
1Unit for Laboratory Animal Medicine, University of Michigan Medical School,
Ann Arbor, MI 48109, USA. 2Department of Microbiology and Immunology,
University of Michigan Medical School, Ann Arbor, MI 48109, USA. 3Center for
Computational Medicine and Bioinformatics, University of Michigan Medical
School, Ann Arbor, MI 48109, USA.
Received: 17 August 2013 Accepted: 20 February 2014
Published: 30 April 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
He et al. Journal of Biomedical Semantics 2014, 5:29
http://www.jbiomedsem.com/content/5/1/29RESEARCH Open AccessOAE: The Ontology of Adverse Events
Yongqun He1*, Sirarat Sarntivijai1,2, Yu Lin1, Zuoshuang Xiang1, Abra Guo1, Shelley Zhang1, Desikan Jagannathan1,
Luca Toldo3, Cui Tao4 and Barry Smith5Abstract
Background: A medical intervention is a medical procedure or application intended to relieve or prevent illness or
injury. Examples of medical interventions include vaccination and drug administration. After a medical intervention,
adverse events (AEs) may occur which lie outside the intended consequences of the intervention. The representation
and analysis of AEs are critical to the improvement of public health.
Description: The Ontology of Adverse Events (OAE), previously named Adverse Event Ontology (AEO), is a
community-driven ontology developed to standardize and integrate data relating to AEs arising subsequent to
medical interventions, as well as to support computer-assisted reasoning. OAE has over 3,000 terms with unique
identifiers, including terms imported from existing ontologies and more than 1,800 OAE-specific terms. In OAE, the
term adverse event denotes a pathological bodily process in a patient that occurs after a medical intervention.
Causal adverse events are defined by OAE as those events that are causal consequences of a medical intervention. OAE
represents various adverse events based on patient anatomic regions and clinical outcomes, including symptoms, signs,
and abnormal processes. OAE has been used in the analysis of several different sorts of vaccine and drug adverse event
data. For example, using the data extracted from the Vaccine Adverse Event Reporting System (VAERS), OAE was used
to analyse vaccine adverse events associated with the administrations of different types of influenza vaccines. OAE has
also been used to represent and classify the vaccine adverse events cited in package inserts of FDA-licensed human
vaccines in the USA.
Conclusion: OAE is a biomedical ontology that logically defines and classifies various adverse events occurring after
medical interventions. OAE has successfully been applied in several adverse event studies. The OAE ontological
framework provides a platform for systematic representation and analysis of adverse events and of the factors
(e.g., vaccinee age) important for determining their clinical outcomes.
Keywords: Ontology of Adverse Events, OAE, Adverse event, Ontology, Vaccine, Drug, Vaccine adverse event, VAERS,
Drug adverse event, Design patternBackground
A medical intervention is a medical procedure or appli-
cation intended to relieve or prevent illness or injury.
The medical intervention can be an administration of a
drug, a vaccine, a special nutritional product (for example,
a medical food supplement), or it can be the use of a
medical device. In the wake of a medical intervention,
adverse events (AEs) may occur which lie outside the
intended consequences of the intervention. These AEs
are pathological bodily processes [1]. Severe AEs include
triggering of Guillain-Barre or Stevens-Johnson Syndrome
paralysis and, in extreme cases, death. Such AEs may* Correspondence: yongqunh@med.umich.edu
1University of Michigan, Ann Arbor, MI, USA
Full list of author information is available at the end of the article
© 2014 He et al.; licensee BioMed Central Ltd.
Commons Attribution License (http://creativec
reproduction in any medium, provided the orresult in hospitalization of the patient and requiring spe-
cial care. Although typically having low incidence rates,
they may impact the usage or regulation of vaccine, drug,
or medical devices in the market. To monitor and investi-
gate adverse events of various types, reporting systems
have been established to collect the relevant information.
For example, the USA national vaccine safety surveillance
programs include the Vaccine Adverse Events Reporting
System (VAERS) [2] and the Food and Drug Administra-
tion (FDA) Adverse Events Reporting System (FAERS) [3],
established, respectively, for the spontaneous reporting of
vaccine and of drug-associated AEs.
To improve representation and organization of adverse
event information, efforts have been undertaken over the
years to develop different vocabulary resources, includingThis is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly credited.
He et al. Journal of Biomedical Semantics 2014, 5:29 Page 2 of 13
http://www.jbiomedsem.com/content/5/1/29the Medical Dictionary for Regulatory Activities (MedDRA)
[4], the Common Terminology Criteria for Adverse Events
(CTCAE) [5], and the World Health Organization (WHO)s
Adverse Reaction Terminology (WHO-ART) [6]. MedDRA
is an adverse event coding vocabulary preferred by the FDA
and utilized by VAERS and FAERS, as well as many clinical
trials. CTCAE, a product of the USA National Cancer Insti-
tute (NCI), is a standardised vocabulary used in assessing
AEs associated with drugs for cancer therapy. WHO-ART
is a dictionary maintained by the WHO to serve as a basis
for rational coding of adverse reaction terms.
While these resources have played a central role in
standardizing and improving AE vocabulary use world-
wide, their lack of text definitions and logical classification
hierarchies poses problems for automatic search and re-
trieval and for computational analysis and aggregation [7].
The Ontology for Adverse Events (OAE) is designed to
address these issues by providing logically well-formed
definitions and an associated structured classification.
These definitions and classification function as the
abstraction of the information from highly specific
particular (or instance) adverse events to more general
universals (or classes) that show commonalities often
not obvious from individual data. As first illustrated in [8]
and discussed also below, the application of OAE appears
to support reasonable classification and analysis of the
vaccine adverse events (VAE) reported in the clinical VAE
case report system. MedDRA and other classical systems
focus on the representation of the symptoms or diseases
that are the adverse event outcomes of clinical findings.
They, thus, do not take into account other elements (e.g.,
patient age) of the process that leads from initial medical
intervention to subsequent outcomes. The OAE is de-
signed to serve as a complementary resource that will
fill this gap of treatment-clinical observation association
by providing a means of linking the content coded by
these systems to other relevant biological and clinical
information.
Biomedical ontologies are consensus-based controlled
vocabularies of entities and relations modelling a part of
the biomedical world, which are represented in both
computer and human interpretable forms. They thus go
further in providing support for computational analysis of
data than the existing vocabulary resources. The Adverse
Event Ontology (AEO) was initially developed by transfer-
ring those ontology terms representing vaccine adverse
events from the Vaccine Ontology (VO) [9,10]. The top
level AEO adverse event representation was also partially
based on the work conducted in the European ReMINE
project [11]. In our previous AEO paper [12], we defined
the term adverse event as: a pathological bodily process
that is induced by a medical intervention. This definition
in the previous version of the ontology of AEs assumed a
causal association between an adverse event and a medicalintervention. A problem with this definition is that it does
not align with the common usage of the term adverse
event in medical, pharmacological and public health
contexts, where it is generally impractical to distinguish
the causal adverse consequences from all the bodily pro-
cesses that unfold in a patient temporally subsequent to
a given medical intervention. The FAERS and VAERS
systems thus state explicitly that they make no assump-
tion of a causal relation between an adverse event and a
medical intervention. The assumption of causality in our
preceding ontology would imply too large a gap between
the ontology and actual practice. Above all, this assump-
tion would make it difficult to use the term adverse event
to represent individual cases, since the existence of a
causal relation is in many cases hard to verify. Further-
more, due to a name conflict with the Anatomical Entity
Ontology that has the same abbreviation AEO, our
Adverse Event Ontology (AEO) was renamed the Ontology
of Adverse Events (OAE) in the Fall of 2011. In OAE,
adverse event (OAE_0000001) is defined as to assume
no causal association, while those adverse events for
which there is a causal association with an intervention
are defined as a subclass of adverse event and named as
causal adverse event (OAE_0000003). The latter term is to
be used only when there is definitive evidence (including
biological and statistical evidences) to assert such a causal
association under specified conditions. We contend that
with this change the OAE becomes more robust as a repre-
sentation of the domain of adverse event reporting.
In addition, other updates have been made to the OAE
as compared to the original AEO. A large number of new
OAE terms derived from a number of use cases have been
added. Different ways of representing and analyzing the
causal association between AEs and medical interventions
have been classified and represented in OAE, and the
ontology has also been used in several studies, which will
be introduced in this paper.
Breadth and Scope
The OAE ontology is a community-based biomedical
ontology in the domain of adverse events. OAE clearly
differentiates adverse event and causal adverse event,
with the latter a subtype of the former. A major effort in
OAE is to represent ontologically various AEs on anatomic
locations and adverse outcomes (including symptoms,
signs, and processes). OAE includes many logic definitions
formulated by using terms from existing ontologies (for
example the UBERON anatomy ontology). This strategy
links OAE with established ontologies and supports
computer-assisted integration and reasoning. Since OAE
defines an adverse event as a process subsequent to a
medical intervention, the ontology provides a logical first
step in the representation of this whole process. Such onto-
logical definition allows the development and application of
He et al. Journal of Biomedical Semantics 2014, 5:29 Page 3 of 13
http://www.jbiomedsem.com/content/5/1/29new analysis methods to better understanding the mecha-
nisms of adverse events associated with or induced by
different medical interventions. OAE also provides a
framework for recording and analyzing the associations
recorded on product labels for example between vaccine
or drug administration and medically relevant events.
The scope of OAE is very specific and should not be
confused with other relevant ontologies. OAE does not
target adverse event reporting by following the pattern
of the existing Adverse Event Reporting Ontology
(AERO) which focuses on the ontological representation
of the vaccine AE data or information using the Brighton
vaccine AE definitions [13]. Unlike OAE, AERO does not
define adverse event as a pathological bodily process. By
following the principle of ontological realism we argue
that the representation of data should be built as far as
possible on the real-world entities to which such data re-
late. Finally, OAE is not an ontology of symptoms or signs
as the indications of illness or diseases. The appearance
of various symptoms or signs (e.g., fever) is rather an
outcome of an adverse event, thus denoted by the suffix
AE (e.g., a fever adverse event or fever AE).
Authority and provenance of OAE
OAE targets two communities: the adverse event com-
munity and the OBO Foundry ontology community. As
concerns the former, we have focused our ontology de-
velopment on two important research communities,
targeting vaccine adverse events and drug adverse events,
respectively, and our team includes experts in both of
these areas. For example, Dr. Yongqun He (co-author) is a
domain expert in vaccinology, vaccine adverse events, and
ontology development [8,14]. Dr. Luca Toldo (co-author)
is an expert in drug adverse events [15-17]. Expanding the
OAE ontological analysis of VAERS data [8], Dr. Sirarat
Sarntivijai is now expanding and applying OAE and ontol-
ogy knowledge mapping to represent and analyze drug-
associated AEs in her systems pharmacology research.
Her OAE research has obtained strong support and
collaboration from clinical experts at the FDA Office
of Clinical Pharmacology. As an ontology in the OBO
Foundry ontology library, the development of OAE fol-
lows the OBO Foundry principles [18]. Dr. Barry Smith
(co-author) is the founder of the Basic Formal Ontol-
ogy (BFO) and also the one of the founders of the OBO
Foundry. Our core development team has also in-
cluded experts in semantics web (Dr. Cui Tao), medical
informatics (Yu Lin, MD, PhD), software developer
(Zuoshuang Xiang), and many students. Our developmen-
tal effort has received technical supports from both the
adverse event community and the OBO Foundry ontology
community, as demonstrated by positive feedbacks we
received from three recent international adverse event
related workshops [19-21].As described above, the OAE was originally derived
from the vaccine adverse event branch of the Vaccine
Ontology (VO) [9,10] and from the European ReMINE
project [11]. New OAE adverse event terms have been
generated on the basis of clinical adverse event reports
in the VAERS [22] and the FDA [3]. We have referenced
MedDRA in our OAE development by cross-referencing
related MedDRA identifiers. The data models of adverse
event analysis provided by the Clinical Data Interchange
Consortium (CDISC) [23] were also referenced. Peer-
reviewed journal articles have been used wherever possible
JOURNAL OF
BIOMEDICAL SEMANTICS
Kuhn et al. Journal of Biomedical Semantics 2014, 5:10
http://www.jbiomedsem.com/content/5/1/10
RESEARCH Open Access
Mining images in biomedical publications:
Detection and analysis of gel diagrams
Tobias Kuhn1*, Mate Levente Nagy3, ThaiBinh Luong2 and Michael Krauthammer2,3
Abstract
Authors of biomedical publications use gel images to report experimental results such as protein-protein interactions
or protein expressions under different conditions. Gel images offer a concise way to communicate such findings, not
all of which need to be explicitly discussed in the article text. This fact together with the abundance of gel images and
their shared common patterns makes them prime candidates for automated image mining and parsing. We introduce
an approach for the detection of gel images, and present a workflow to analyze them. We are able to detect gel
segments and panels at high accuracy, and present preliminary results for the identification of gene names in these
images. While we cannot provide a complete solution at this point, we present evidence that this kind of image
mining is feasible.
Introduction
A recent trend in the area of literature mining is the
inclusion of images in the form of figures from biomed-
ical publications [1-3]. This development benefits from
the fact that an increasing number of scientific articles
are published as open access publications. This means
that not just the abstracts but the complete texts includ-
ing images are available for data analysis. Among other
things, this enabled the development of query engines
for biomedical images like the Yale Image Finder [4] and
the BioText Search Engine [5]. Below, we present our
approach to detect and access gel diagrams. This is an
extended version of a previous workshop paper [6].
As a preparatory evaluation to decide which image type
to focus on, we built a corpus of 3 000 figures that allows
us to reliably estimate the numbers and types of images in
biomedical articles. These figures were drawn randomly
from the open access subset of PubMed Central and then
manually annotated. They were split into subfigures when
the figure consisted of several components. Figure 1 shows
the resulting categories and subcategories. This classifi-
cation scheme is based on five basic image categories:
Experimental/Microscopy, Graph, Diagram, Clinical and
Picture, each divided into multiple subcategories. It shows
*Correspondence: kuhntobias@gmail.com
1Department of Humanities, Social and Political Sciences, ETH Zurich, Zürich,
Switzerland
Full list of author information is available at the end of the article
that bar graphs (12.4%), black-on-white gels (12.0%), fluo-
rescencemicroscopy images (9.4%), and line graphs (8.1%)
are the most frequent subfigure types (all percentages are
relative to the entire set of images).
We targeted different kinds of graphs (i.e., diagrams
with axes) in previous work [7], and we decided to focus
this work on the second most common type of images:
gel diagrams. They are the result of gel electrophore-
sis, which is a common method to analyze DNA, RNA
and proteins. Southern, Western and Northern blotting
[8-10] are among the most common applications of gel
electrophoresis. The resulting experimental artifacts are
often shown in biomedical publications in the form of
gel images as evidence for the discussed findings such as
protein-protein interactions or protein expressions under
different conditions. Often, not all details of the results
shown in these images are explicitly stated in the caption
or the article text. For these reasons, it would be of high
value to be able to reliably mine the relations encoded in
these images.
A closer look at gel images reveals that they follow reg-
ular patterns to encode their semantic relations. Figure 2
shows two typical examples of gel images together with
a table representation of the involved relations. The ulti-
mate objective of our approach (for which we can only
present a partial solution here) is to automatically extract
at least some of these relations from the respective images,
© 2014 Kuhn et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited.
Kuhn et al. Journal of Biomedical Semantics 2014, 5:10 Page 2 of 9
http://www.jbiomedsem.com/content/5/1/10
Figure 1 Categorization of images from open access articles of PubMed Central.
possibly in conjunction with classical text mining tech-
niques. The first example shows aWestern blot for detect-
ing two proteins (14-3-3? and ?-actin as a control) in four
different cell lines (MDA-MB-231, NHEM, C8161.9, and
LOX, the first of which is used as a control). There are
two rectangular gel segments arranged in a way to form a
2× 4 grid for the individual eight measurements combin-
ing each protein with each cell line. A gel diagram can be
considered a kind of matrix with pictures of experimen-
tal artifacts as content. The tables to the right illustrate
the semantic relations encoded in the gel diagrams. Each
relation instance consists of a condition, a measurement
and a result. The proteins are the entities being measured
under the conditions of the different cell lines. The result
is a certain degree of expression indicated by the darkness
of the spots (or brightness in the case of white-on-black
gels). The second example is a slightly more complex one.
Several proteins are tested against each other in a way that
involves more than two dimensions. In this case, the use
of + and  labels is a frequent technique to denote the
different possible combinations of a number of conditions.
Apart from that, the principles are the same. In this case,
however, the number of relations is much larger. Only the
first eight of a total of 32 relation instances are shown in
the table to the right. In such cases, the text rarely men-
tions all these relations in an explicit way, and the image is
therefore the only accessible source.
Background
In principle, image mining involves the same processes
as classical literature mining [11]: document categoriza-
tion, named entity tagging, fact extraction, and collection-
wide analysis. However, there are some subtle differences.
Document categorization corresponds to image catego-
rization, which is different in the sense that it has to
deal with features based on the two-dimensional space
of pixels, but otherwise the same principles of automatic
categorization apply. Named entity tagging is different in
two ways: pinpointing the mention of an entity is more
difficult with images (a large number of pixels versus a
couple of characters), and OCR errors have to be consid-
ered. Fact extraction in classical literature mining involves
Kuhn et al. Journal of Biomedical Semantics 2014, 5:10 Page 3 of 9
http://www.jbiomedsem.com/content/5/1/10
Figure 2 Two examples of gel images from biomedical publications (PMID 19473536 and 15125785) with tables showing the relations
that could be extracted from them.
the analysis of the syntactic structure of the sentences.
In images, in contrast, there are rarely complete sen-
tences, but the semantics is rather encoded by graphical
means. Thus, instead of parsing sentences, one has to ana-
lyze graphical elements and their relation to each other.
The last process, collection-wide analysis, is a higher-level
problem, and therefore no fundamental differences can be
expected. Thus, image mining builds upon the same gen-
eral stages as classical text mining, but with some subtle
yet important differences.
Image mining on biomedical publications is not a new
idea. It has been applied for the extraction of subcellu-
lar location information [12], the detection of panels of
fluorescence microscopy images [13], the extraction of
pathway information from diagrams [14], and the detec-
tion of axis diagrams [7]. Also, there is a large amount
of existing work on how to process gel images [15-19]
and databases have been proposed to store the results
of gel analyses [20]. These techniques, however, take as
input plain gel images, which are not readily accessible
from biomedical papers, because they make up just parts
of the figures. Furthermore, these tools are designed for
researchers who want to analyze their gel images and not
to read gel diagrams that have already been analyzed and
annotated by a researcher. Therefore, these approaches do
not tackle the problem of recognizing and analyzing the
labels of gel images. Some attempts to classify biomedical
images include gel figures [21], which is, however, just the
first step in locating them and analyzing their labels and
their structure. To our knowledge, nobody has yet tried to
perform image mining on gel diagrams.
Approach andmethods
Figure 3 shows the procedure of our approach to image
mining from gel diagrams. It consists of seven steps: figure
extraction, segmentation, text recognition, gel detection,
gel panel detection, named entity recognition and relation
extraction.a
Using structured article representations, the first step is
trivial. For steps two and three, we rely on existing work.
Themain focus of this paper lies on steps four and five: the
detection of gels and gel panels. In the discussion section,
we present some preliminary results on step six of recog-
nizing named entities, and sketch how step seven could
be implemented, for which we cannot provide a concrete
solution at this point.
To practically evaluate our approach, we ran our
pipeline on the entire open access subset of PubMed Cen-
tral (though not all figures made it through the whole
pipeline due to technical difficulties).
Figure extraction
A large portion of the articles of the open access subset
of the PubMed Central database are available as struc-
tured XML files with additional image files for the figures.
We only use these articles so far, which makes the figure
extraction task very easy. It would be more difficult,
though definitely feasible, to extract the figures from PDF
files or even bitmaps of scanned articles (see [22] and
http://pdfjailbreak.com for approaches on extracting the
structure of articles in PDF format).
Segmentation and text recognition
For the next two steps  segment detection and sub-
sequent text recognition , we rely on our previous
work [23,24]. This method includes the detection of lay-
out elements, edge detection, and text recognition with
a novel pivoting approach. For optical character recogni-
tion (OCR), the Microsoft Document Imaging package is
used, which is available as part of Microsoft Office 2003.
Kuhn et al. Journal of Biomedical Semantics 2014, 5:10 Page 4 of 9
http://www.jbiomedsem.com/content/5/1/10
Figure 3 The procedure of our approach: (1) figure extraction, (2) segmentation, (3) text recognition, (4) gel detection, (5) gel panel
detection, (6) named entity recognition, and (7) relation extraction.
Overall, this approach has been shown to perform bet-
ter than other existing approaches for the images found in
biomedical publications [23].We do not go into the details
here, as this paper focuses on the subsequent steps.
Due to some limitations of the segmentation algorithm
when it comes to rectangles with low internal contrast
(like gels), we applied a complementary very simple rect-
angle detection algorithm.
Gel segment detection
Based on the results of the above-mentioned steps, we try
to identify gel segments. Such gel segments typically have
rectangular shapes with darker spots on a light gray back-
ground, or  less commonly  white spots on a dark
background. We decided to use machine learning tech-
niques to generate classifiers to detect such gel segments.
To do so, we defined 39 numerical features for image
segments: the coordinates of the relative position (within
the image), the relative and absolute width and height,
16 grayscale histogram features, three color features (for
red, green and blue), 13 texture features (coarseness, pres-
ence of ripples, etc.) based on [25], and the number of
recognized characters.
To train the classifiers, we took a random sample of
500 figures, for which we manually annotated the gel
segments. In the same way, we obtained a second sam-
ple of another 500 figures for testing the classifiers.b We
used the Weka toolkit and opted for random forest classi-
fiers based on 75 random trees. Using different thresholds
to adjust the trade-off between precision and recall, we
generated a classifier with good precision and another
one with good recall. Both of them are used in the next
step. We tried other types of classifiers including naive
Bayes, Bayesian networks [26], PART decision lists [27],
and convolutional networks [28], but we achieved the best
results with random forests.
Gel panel detection
A gel panel typically consists of several gel segments and
comes with labels describing the involved genes, proteins,
and conditions. For our goal, it is not sufficient to just
detect the figures that contain gel panels, but we also have
to extract their positions within the figures and to access
their labels. This is not a simple classification task, and
therefore machine learning techniques do not apply that
easily. For that reason, we used a detection procedure
based on hand-coded rules.
In a first step, we group gel segments to find con-
tiguous gel regions that form the center part of gel
panels. To do so, we start with looking for segments
that our high-precision classifier detects as gel segments.
Then, we repeatedly look for adjacent gel segments, this
time applying the high-recall classifier, and merge them.
Two segments are considered neighbors if they are at
most 50 pixels apartc and do not have any text segment
between them. Thus, segments which could be gel seg-
ments according to the high-recall classifier make it into
a gel panel only if there is at least one high-precision seg-
ment in their group. The goal is to detect panels with high
precision, but also to detect the complete panels and not
just parts of them.We focus here on precision because low
recall can be leveraged by the large number of available gel
images. Furthermore, as the open access part of PubMed
Central only makes up a small subset of all biomedical
publications, recall in a more general sense is anyway
limited by the proportion of open access publications.
As a next step, we collect the labels in the form of text
segments located around the detected gel regions. For a
Kuhn et al. Journal of Biomedical Semantics 2014, 5:10 Page 5 of 9
http://www.jbiomedsem.com/content/5/1/10
text segment to be attributed to a certain gel panel, its
nearest edge must be at most 30 pixels away from the bor-
der of the gel region and its farthest edge must not be
more than 150 pixels away. We end up with a representa-
tion of a gel panel consisting of two parts: a center region
containing a number of gel segments and a set of labels
in the form of text segments located around the center
region.
To evaluate this algorithm, we collected yet another
sample of 500 figures, in which 106 gel panels in 61 differ-
ent figures were revealed by manual annotation.d Based
on this sample, we manually checked whether our algo-
rithm is able to detect the presence and the (approximate)
position of the gel panels.
Results
The top part of Table 1 shows the result of the gel
detection classifier. We generated three different classi-
fiers from the training data, one for each of the threshold
values 0.15, 0.3 and 0.6. Lower threshold values lead to
higher recall at the cost of precision, and vice versa. In
the balanced case, we achieved an F-score of 75%. To get
classifiers with precision or recall over 90%, F-score goes
down significantly, but stays in a sensible range. These
two classifiers (thresholds 0.15 and 0.6) are used in the
next step. To interpret these values, one has to consider
that gel segments are greatly outnumbered by non-gel
segments. Concretely, only about 3% are gel segments.
More sophisticated accuracy measures for classifier per-
formance, such as the area under the ROC curve [29], take
this into account. For the presented classifiers, the area
under the ROC curve is 98.0% (on a scale from 50% for a
trivial, worthless classifier to 100% for a perfect one).
The results of the gel panel detection algorithm are
shown in the bottom part of Table 1. The precision is 95%
at a recall of 37%, leading to an F-score of 53%. The com-
paratively low recall is mainly due to the general problem
of pipeline-based approaches that the various errors from
the earlier steps accumulate and are hard to correct at a
later stage in the pipeline.
Table 2 shows the results of running the pipeline on
PubMed Central. We started with about 410 000 articles,
the entire open access subset of PubMed Central at the
time we downloaded them (February 2012). We success-
fully parsed the XML files of 94% of these articles (for
the remaining articles, the XML file was missing or not
well-formed, or other unexpected errors occurred). The
successful articles contained around 1 100 000 figures, for
some of which our segment detection step encountered
image formatting errors or other internal errors, or was
just not able to detect any segments. We ended up with
more than 880 000 figures, in which we detected about
86 000 gel panels, i.e. roughly ten out of 100 figures. For
each of them, we found on average 3.6 labels with recog-
nized text. After tokenization, we identified about 76 000
gene names in these gel labels, which corresponds to 6.8%
of the tokens. Considering all text segments (including but
not restricted to gel labels), only 3.3% of the tokens are
detected as gene names.e
Discussion
The presented results show that we are able to detect gel
segments with high accuracy, which allows us to subse-
quently detect whole gel panels at a high precision. The
recall of the panel detection step is relatively low, but with
about 37% still in a reasonable range. Asmentioned above,
we can leverage the high number of available figures,
which makes precision more important than recall. Run-
ning our pipeline on the whole set of open access articles
from PubMed Central, we were able to retrieve 85 942
potential gel panels (around 95% of which we can expect
to be correctly detected).
The next step would be to recognize the named entities
mentioned in the gel labels. To this aim, we did a prelimi-
nary study to investigate whether we are able to extract the
names of genes and proteins from gel diagrams. To do so,
we tokenized the label texts and looked for entries in the
Entrez Gene database to match the tokens. This look-up
was done in a case-sensitive way, because many names in
gel labels are acronyms, where the specific capitalization
Table 1 The results of the gel segment detection classifiers (top) and the gel panel detection algorithm (bottom)
Method Threshold Precision Recall F-score ROC area
Segments
Random forests
0.15 0.439 0.909 0.592 ??
? 0.9800.30 0.765 0.739 0.752
0.60 0.926 0.301 0.455
Naive Bayes 0.172 0.739 0.279 0.883
Bayesian network 0.394 0.531 0.452 0.914
PART decision list 0.631 0.496 0.555 0.777
Convolutional networks 0.142 0.949 0.248
Panels Hand-coded rules 0.951 0.368 0.530
Kuhn et al. Journal of Biomedical Semantics 2014, 5:10 Page 6 of 9
http://www.jbiomedsem.com/content/5/1/10
Table 2 The results of running the pipeline on the open
access subset of PubMed Central
Total articles 410 950
Processed articles 386 428
Total figures from processed articles 1 110 643
Processed figures 884 152
Detected gel panels 85 942
Detected gel panels per figure 0.097
Detected gel labels 309 340
Detected gel labels per panel 3.599
Detected gene tokens 1 854 609
Detected gene tokens in gel labels 75 610
Gene token ratio 0.033
Gene token ratio in gel labels 0.068
pattern can be critical to identify the respective entity.
We excluded tokens that have less than three characters,
are numbers (Arabic or Latin), or correspond to com-
mon short words (retrieved from a list of the 100 most
frequent words in biomedical articles). In addition, we
extended this exclusion list with 22 general words that
are frequently used in the context of gel diagrams, some
of which coincide with gene names according to Entrez.f
Since gel electrophoresis is a method to analyze genes and
proteins, we would expect to find more such mentions in
gel labels than in other text segments of a figure. By mea-
suring this, we get an idea of whether the approach works
out or not. In addition, we manually checked the gene and
protein names extracted from gel labels after running our
pipeline on 2 000 random figures. In 124 of these figures,
at least one gel panel was detected. Table 3 shows the
results of this preliminary evaluation. Almost two-thirds
of the detected gene/protein tokens (65.3%) were correctly
identified. 9% thereof were correct but could be more spe-
cific, e.g. when only actin was recognized for ?-actin
(which is not incorrect but of course much harder to map
to a meaningful identifier). The incorrect cases (34.6%)
can be split into two classes of roughly the same size: some
recognized tokens were actually not mentioned in the
figure but emerged from OCR errors; other tokens were
correctly recognized but incorrectly classified as gene or
PROCEEDINGS Open Access
Statistical algorithms for ontology-based
annotation of scientific literature
Chayan Chakrabarti1*, Thomas B Jones1, George F Luger1, Jiawei F Xu1, Matthew D Turner1,2, Angela R Laird3,
Jessica A Turner2,4
From Bio-Ontologies Special Interest Group 2013
Berlin, Germany. 20 July 2013
* Correspondence: cc@cs.unm.edu
1Department of Computer Science,
University of New Mexico,
Albuquerque, New Mexico, USA
Abstract
Background: Ontologies encode relationships within a domain in robust data
structures that can be used to annotate data objects, including scientific papers, in
ways that ease tasks such as search and meta-analysis. However, the annotation
process requires significant time and effort when performed by humans. Text mining
algorithms can facilitate this process, but they render an analysis mainly based upon
keyword, synonym and semantic matching. They do not leverage information
embedded in an ontologys structure.
Methods: We present a probabilistic framework that facilitates the automatic
annotation of literature by indirectly modeling the restrictions among the different
classes in the ontology. Our research focuses on annotating human functional
neuroimaging literature within the Cognitive Paradigm Ontology (CogPO). We use an
approach that combines the stochastic simplicity of naïve Bayes with the formal
transparency of decision trees. Our data structure is easily modifiable to reflect
changing domain knowledge.
Results: We compare our results across naïve Bayes, Bayesian Decision Trees, and
Constrained Decision Tree classifiers that keep a human expert in the loop, in terms
of the quality measure of the F1-mirco score.
Conclusions: Unlike traditional text mining algorithms, our framework can model
the knowledge encoded by the dependencies in an ontology, albeit indirectly. We
successfully exploit the fact that CogPO has explicitly stated restrictions, and implicit
dependencies in the form of patterns in the expert curated annotations.
Background
Advances in neuroimaging and brain mapping have generated a vast amount of scientific
knowledge. This data, gleaned from a large number of experiments and studies, pertains
to the functions of the human brain. Given large bodies of properly annotated research
papers, it is possible for researchers to use meta-analysis tools to identify and understand
consistent patterns in the literature. Since researchers often use jargon which is specific
to a small sub-field to describe their experiments, it is helpful to tag papers with standar-
dized descriptions of the experimental conditions of each papers accompanying study.
Several repositories have been created with this effort in mind.
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2014 Chakrabarti et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
BrainMap (http://www.brainmap.org) is one of the largest and most widely used reposi-
tories of neuroimaging results. The BrainMap software suite provides computational tool-
sets, scientific data sets, and other informatics resources needed to explore the different
cognitive constructs underlying brain function in various disorders, such as the constella-
tion of schizophrenia, bipolar disorder, depression, and autism [1]. Large-scale quantitative
meta-analyses demand the ability to easily identify studies using the same (or similar
enough) experimental methods and subjects. The BrainMap method for describing experi-
ments has evolved into a taxonomy composed chiefly of structured keywords that categor-
ize the experimental question addressed, the imaging methods used, the behavioral
conditions during which imaging was acquired, and the statistical contrasts performed.
The Cognitive Paradigm Ontology (CogPO), compliant with the Basic Formal Ontology
(BFO) [2], builds upon the BrainMap repository on the understanding that while the
experimental psychology and cognitive neuroscience literature may refer to certain beha-
vioral tasks by name (e.g., the Stroop task or the Sternberg paradigm) or by function (a
working memory task, a visual attention task), the presentation of these paradigms in the
literature can vary tremendously and are most precisely characterized by the unique com-
bination of the stimuli that are presented to the subject, the response expected from the
subject, and the instructions given to the subject. The prevalent use of different terminolo-
gies for the same paradigm across different sub-specialities can hinder assimilation of
coherent scientific knowledge. Discovering equivalence among these terminologies in a
structured coherent fashion will facilitate richer information retrieval operations. The
BrainMap repository structure forms the backbone of the Cognitive Paradigm Ontology.
It includes the keywords from BrainMap, as well as others, and explicitly represents the
implicit definitions and relationships among them [2]. This allows published experiments
implementing similar behavioral task characteristics to be linked, despite the use of alter-
nate vocabularies.
Each piece of literature from the BrainMap repository is annotated according to the
CogPO definitions. The process of annotation is traditionally undertaken by a human
subject matter expert, who decides the suitable annotation terms from the CogPO
schema after reading the paper, while extracting descriptions of first PET and then
fMRI experiments, and storing each papers results in a standardized system for ease of
retrieval [2,3]. Unfortunately, this task is both time and effort intensive. It presents a
major bottleneck and cost to the whole process. As a result, even though the value of
the BrainMap project has been proven, the number of publications in the literature far
outweighs the number of publications that have been included in the database [3]. In
this study, we propose solutions for replacing this human only annotation step with
automated suggestions for the experimental paradigm terms.
Text mining
Text mining methods have found application in identifying patterns and trends in rich
textual data [4-6]. Text mining algorithms have also been extended to the problem of
multi-objective multi-label classification where a variety of predictive functions can be
constructed depended on the required objective function including optimizing an F1-
score [7] or minimizing the hamming loss [8]. F1 score is the geometric mean of the
recall, a measure of the classifiers tendency to return all of the correct labels, and
accuracy, a measure of the tendency of labels returned by the classifier to be correct.
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2
Page 2 of 15
Hamming loss, on the other hand, gives a count of the number of false positives and
false negatives a classifier identifies. Both of these distinct measures give an indication
of the classifiers ability to return high quality classifications.
The performance of multi-objective multi-label classification can be further optimized
using regret analysis [9]. The binary relevance method has been used to extend the solu-
tion of multi-objective multi-label classification methods to larger datasets [10]. The main
algorithms for multi-objective multi-label classification are generally classified under the
umbrellas of problem transformation, algorithm adaptation, lazy learning, support vector
machine derived, ensemble methods, and label dependence exploitation [11]. Support
Vector Machines and Self Organizing Feature Maps have been used to reduce the inher-
ently high dimensionality of text mining problems [12] and have shown promising results
[13]. Other, perception based techniques, like artificial neural networks and radial basis
functions are useful in estimating classification functions for classes of problems with
non-linear and irregular decision boundaries [14].
Latent Semantic Analysis works on the assumption that words that are close in meaning
occur close to each other in a document [15,16]. Using Singular Value Decomposition, the
matrix representing word counts by paragraph from large document clusters are reduced
to only preserve the similarity metric among documents. Documents can then be com-
pared using projections and other distance metrics. K-means clustering partitions a corpus
of documents in to clusters, where each cluster refers to similar documents [17]. There are
many variations on this theme. In fuzzy co-means clustering, each document may belong
to more than one cluster defined by a fuzzy function [18,19]. Similarly, a variant of the
classic Expectation-Maximization algorithm assigns probabilistic distribution function
among the clusters to each document [20].
The NCBO Annotator takes free text and uses efficient concept-recognition techni-
ques to suggest annotations from the Bio-Portal repository of ontologies [21]. The
Neuroscience Information Framework [22] uses ontological annotations of a broad
variety of neuroscience resources to retrieve information for user queries.
However, most text-mining techniques do not leverage the hierarchical structures
encoded implicitly in an ontology. They consider the ontology terms as anchors for
clustering or topic modeling techniques, but have no way to use the information that
the terms may have exploitable relations to each other, either causal or hierarchical.
These terms could just be a set of high entropy keywords for the algorithms to be
equally effective. We present a framework that makes use of some of the hierarchical
information that is available from the ontology itself for the annotation task.
Ontology-based annotation of documents has been an important application area for
text mining research [23]. Since the interdisciplinary nature of this text mining applied
to ontologies leads to overlap of terminology for both fields, we clarify the terms we
use here. We use categories to denote specific superclasses in CogPO (e.g., Stimulus
Type), and labels to denote the leaf terms in each class, which are actually applied to
the abstracts (e.g., Flashing Checkerboard, which is a subclass of Stimulus Type).
Dependencies refer to the explicit interaction between the ontology and the specific cor-
pora, as captured by the expert-assigned annotations. This is an implicit function of
the interrelationships between classes (categories of labels), leaf terms, the inherent
(but not explicitly stated) logical restrictions in CogPO, and the manner in which those
relationships are reified in a specific corpus by human annotators.
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2
Page 3 of 15
In previous work using a similar dataset, we evaluated a version of k-nearest-neigh-
bor (kNN) for performing automated annotations [24,25]. We found that the perfor-
mance was comparable with results on other textual annotation datasets, but fairly
poor for the multi-label aspects of the problem. Text mining algorithms have also been
applied to the problem of multi-label annotation; the general case in which there are
more than two labels to choose from, and each paper can be best described by more
than one label [8,26].
Methods
We demonstrate techniques for automatic annotation of the neuroimaging literature
driven by the Cognitive Paradigm Ontology.
Corpus
Our corpus consists of 247 human subject matter expert annotated abstracts that are part
of the BrainMap database. We consider annotations in 5 distinct categories for each
abstract - Stimulus Modality (SM), Stimulus Type (ST), Response Modality (RM), Response
Type (RT) and Instructions (I). Each of these categories is comprised of several labels as
described in CogPO (Turner & Laird 2012) as shown in Figure 1. These human subject
matter expert annotated abstracts serve as the gold standard against which we test our
stochastic approaches. Table 1 shows a component of the schema from CogPO that we
consider along with a subset of the labels. We only work on the abstracts, and not the full
paper, because we want to interface our tool directly with the eUtils toolkit of PubMed
that can retrieve the text of abstracts in batch [27].
Each abstract is annotated by at least one label from each of the SM, ST, RM, RT, or
I categories, and possibly multiple labels from each. The average number of labels per
category per abstract ranged from 1.15 to 1.85 depending on the category. The human
Figure 1 CogPO annotations. We consider annotations from 5 distinct categories: Stimulus Modality,
Stimulus Type, Response Modality, Response Type, and Instructions. A subset of the labels for each
category is shown here.
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2
Page 4 of 15
curators annotations model implicit dependencies as a result of the CogPO-corpus
interaction. These dependencies will be specific for each different corpus of abstracts.
The CogPO ontology explicitly includes restrictions on the labels, e.g., a Tone as a
Stimulus Type label entails that the Stimulus Modality must include Auditory, or the
Instruction label Smile entails Facial as the label for Response Modality. A flat text
mining approach would be unable to make these distinctions, i.e., it would not be able
to tell that label a can change the probability of label b, in some other category. Our
approach indirectly models this by learning patterns from the expert curated corpus.
Naïve Bayes
Naïve Bayes is a probabilistic learning method, based on Bayes rule, which works surpris-
ingly well on problems where a strong independence hypothesis assumption is not met.
In fact, naïve Bayes also works well for supervised learning when the number of instances
in the training set is relatively small, which is our situation [25]. It has been extended to
the multi-label scenario using various transformation techniques [9]; we have also found in
a comparison of text mining methods applied to this corpus that a naïve Bayes approach
works better than several others [25]. Therefore, we start with a naïve Bayes approach.
The naïve Bayes technique across all categories and possible labels does not leverage
the dependencies between labels in different categories, which are implicitly encoded
in the domain ontology. Traditional text-mining techniques consider the labels to be
anchors for clustering or topic modeling techniques, but have no way to use the fact
that the anchors may have implicit dependencies to each other and to object features.
The features used to derive terms in traditional text mining are often a set of high
entropy keywords [5]. Our framework does not explicitly model the interrelationships
and restrictions in CogPO, but we exploit the fact that these relations and restrictions
do exist and implicitly model the information that is encoded in the ontology. This is
an important distinguishing characteristic of our stochastic approach.
Table 1 Overview of key terms from the CogPO Ontology (adapted from [1]).
Concepts Parent Class Definition
Stimulus
Role
BFO: role The role of a stimulus in a behavioral experiment is attributed to the
object(s) that are presented to the subject in a controlled manner in
the context of the experiment.
Response
Role
BFO: role The role of response is attributed to the overt or covert behavior
that is elicited from the subject in an experimental condition.
Stimulus BFO: ObjectAggregate The object or set of objects, internal or external to the subject,
which is intended to generate either an overt or covert response in
the subject as part of an experimental condition.
Response BFO: ProcessAggregate The overt or covert behavior that is elicited from the subject in an
experimental condition.
Instructions IAO:action specification,
BFO: generically_
independent_continuant
Instructions are the information-bearing entity that sets up the rules for
desired behavior from the subjects. An explicit direction that guides the
behavior of the subject during the experimental conditions. Instructions
serve the function that they lay out what the response behaviors
should be for any set of stimuli in the experiment.
Stimulus
Modality
BFO: Quality The quality of the sensory perception of an explicit stimulus.
Response
Modality
BFO: FiatObjectPart Class of body parts used to perform the actions which can play the
role of an overt response
We consider only a subset of the Cognitive Paradigm Ontology as defined in [1]. We consider 5 classes, Stimulus
Modality, Stimulus Type, Response Modality, Response Type, and Instructions.
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2
Page 5 of 15
In many ontologies, there are often different classes from which a label may be
drawn [1]. While naïve Bayes is able to assign certain features in a training sample to
labels in a single category, it is unable to learn about dependencies between labels and
their associated attributes in different categories. Further, it is not possible for naïve
Bayes alone to increase or decrease its confidence in one label after it has been
informed that some other label is a correct or incorrect annotation for the same sam-
ple. Our method expands on naive Bayes by restricting training sets at each node in
the tree to only those training objects pertinent to that node. This allows us to take
advantage of any underlying dependencies in the training set between labels of differ-
ent categories, which would otherwise be hidden by building a separate classifier for
each category.
Formal framework of naïve Bayes
The framework which Naive Bayes requires to operate includes a set of items to be
classified whose classifications have already been obtained through some other process
(usually a human annotator). Each item in this study, abstracts, which have been
tagged with labels from the CogPo ontology, is then recast as a feature vector. In our
work, this feature vector is a Boolean vector with one bit for every non-stop word in
the corpus. Each bit in an abstracts associated feature vector is set to true if the word
occurs in the abstract and false otherwise. Figure 2. shows an overview of the naïve
Bayes method.
More formally, we define the set of abstracts, the feature vector, and the set of fea-
ture vectors (representing words from the corpus that are not stop words) as follows.
Definition 1. The set of abstracts in the corpus is defined as
D =
{
d |d is an abstract in the corpus}
Definition 2. A feature is defined as
F =< f |f is a feature representing a non ? stop word >
Figure 2 Naïve Bayes. Naïve Bayes determines most probable labels in a category.
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2
Page 6 of 15
Definition 3. A feature vector is defined as
V =
{
vc|vc =< bc1 . . . bcn >, bcj =
{
TRUE, fi ? dc
FALSE, otherwise
}
By the previous definitions, the length or size of
|vc| =
??f ?? and |V| = |D| = number of abstracts
Definition 4. CogPO itself, as used in this study can be defined as the set of cate-
gories Stimulus Modality, Stiumulus Type, Response Modality, Response Type, and
Instruction.
C = {SM, ST,RM,RT, I}
Definition 5. Each category can be defined as a set of labels li. So for example,
SM = {l1, l2, . . .}
with li = Visual, l2 = Auditory, etc
The other 4 categories, ST, RM, RT, and I, can be similarly defined.
Now we can explain the mechanism by which naive Bayes classifies each abstract.
First, the classifier estimates
P(M(dc, lj)|bci = TRUE)
or the probability that abstract c has label j given bit i in its feature vector is TRUE,
by examining the gold standard corpus, extracting only those abstracts which have bit
i set to TRUE, and counting the frequency with which label j occurs in this set. This is
done for each label and for each of the feature bits. The classifier also estimates
P(M(dc, lj)|bci = FALSE))
for each label and feature by a similar process.
Next the classifier estimates
P(bci = TRUE|M(dc, lj))
the probability that bit i in the feature vector of abstract x is true given that abstract
x is labeled with label j, by flipping the above process around and examining only
those abstracts which have label lj and counting the frequency with which bcj is set to
TRUE in the annotated corpus. Similarly, the classifier then does this for the cases
when bci is set to FALSE.
Additionally, the classifier estimates
P(bci = TRUE)
by simply looking at the frequency with which the ith bit of each abstracts feature
vectors is true in the gold standard corpus. Similarly the classifier finds
P(bci = FALSE) = 1 ? P(bci = TRUE)
Lastly,
P(M(dc, lj))
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2
Page 7 of 15
the probability that abstract c has label j, is estimated by counting the frequency of
the occurrence of label j in the gold standard corpus.
Given these four sets of values
P
(
M
(
dc, lj
) | bci) ,P (bci| M (dc, lj)) ,P (bci)
and
P
(
M
(
dc, lj
))
for each label and each feature bit we can estimate
P
(
M
(
dc| lj
) | Vc)
or the probability that an abstract c is labeled with label j given its feature vector.
Since, for any random variable A and B,
P (A|B) = P (A ? B) /P (B)
we know that
P(M(d|lj)|V) = P(M(dc| lj) and Vc)/P(Vc).
The naive in naive Bayes comes from assuming that the probability of each bit being
true in the feature vector is independent of the state of every other bit in the feature
vector. Therefore:
P
(
M
(
d|lj
) |V) = P(M (dc, lj) ? Vc)/P (Vc)
? P (M (dc, lj)) ? I = 1 to |F| P(bci|M (dc, lj) /P (bci)
Similarly, we calculate the probability for all the other labels in SM as well as ST,
RM, RT, and I. We used binary relevance in a single category to solve the multi label
classification problem. Our method takes the raw probability calculated by the Baye-
sian classifier using the above equations for each label and accepts all labels that
receive a probability greater than an arbitrary pre-defined cutoff a.
Bayesian decision trees
Decision trees are discrete models that can predict the output labels of samples in a
data set, based on several input variables arranged in a tree-like structure with nodes
and branches. Nodes in the tree represent a decision variable and the branches corre-
spond to the next decision variable to be queried based on the outcome of the pre-
vious decision variable. We use the Bayesian classifiers to make decisions about which
labels to include at each node while traversing down the tree.
Definition 6. BC,S is a Bayesian classifier trained on set S ?D over category C.
Definition 7. If S is a training set and s ? S then label(s) is the set of correct labels
attached to item s.
Definition 8. If t is a node in a tree T such that each node in T contains a label or
an empty label, then Lt* is a set that contains the label of node t and all of the labels
of each ancestor of t, with no addition made if the label of a node is empty. In prac-
tice, the root is the only node that will have an empty label, since on the root node,
the naiveBayes algorithm will consider the entire training set.
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2
Page 8 of 15
Definition 9. T is a Bayesian Decision Tree if each node t of T consists of a category
Ct which is not the same category as any of the ancestors of t, and which is shared
among the siblings and cousins of t ; a label lt which comes from the category of the
parent of t and which is not shared with any of the siblings of t; and a multi-label
Bayesian classifier BCt,St using definition 1. The training set St has the following restric-
tion: ?s ? St , Lt* ? label(s). Finally, we require that the label of the root node be
empty.
Definition 10. If Bt is the Bayesian classifier associated with node t and I is an object
which maybe categorized by Bt, then Bt(I) is the list of all labels which Bt returns upon
classifying I.
Definition 11. If l is a label and t is a node in a tree then Child(l, t) is the child of t,
which contains label l.
Building the Bayesian decision tree
Using these definitions, we construct a framework for annotating the neuroimaging
abstracts with labels from the CogPO ontology categories of SM, ST, RM, RT, and I.
We limit the training set on the naïve Bayes classifiers in the tree in order to leverage
the dependencies that exist between labels in different categories. Thus we change the
underlying probabilities of the Bayesian classifier to better fit any dependencies
between labels in different categories. This less is more approach helps the Bayesian
classifier to focus on attributes that are more important to the current node, as seen in
Figure 3.
Our approach uses conditional learning to boost accuracy and recall in automatic
learning systems. By conditional learning we mean that when the system classifies an
abstract, it uses stochastic models (naive Bayes classifiers in this case) that were built
with training data that is limited to only those training items that have labels that were
already determined to be pertinent to the abstract currently being labeled on a higher
level of the decision tree Table 2.
For example, consider an abstract that is being evaluated by this system and that has
already been tagged by the system as having a Stimulus Modality of Auditory. When
the system reaches the Stimulus Type level of the decision tree, it will reach for a naive
bayes classifier that has not been trained on the entire gold-standard data set. Instead it
will reach for a classifier which has been trained only on abstracts that were known to
Figure 3 Less is More. The Bayesian Decision Tree limits the number of labels at each node. The pruning
is done on the basis of the F1 micro score from the gold standard annotations. Thus the naïve Bayes
process can be applied to a more concentrated set of abstract-label combinations resulting in more
accurate annotations.
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2
Page 9 of 15
have Auditory as a label. This means that the underlying probabilities of various labels
for Stimulus Type will change, making a label like Chord Sequences, a inherent Stimu-
lus Type of Auditory more likely, and making a Stimulus Type of False Fonts, from Sti-
mulus Modality Visual, less likely.
It is important to note that this is not because the algorithm has been programmed
to explicitly avoid the Stimulus Type label False Fonts when it encounters an abstract
already labeled Auditory. Instead this is due to the fact that it is implicitly the case in
the literature and given CogPO that the False Fonts label is mostly not compatible
with the Auditory label, and human annotators, with their natural understanding of
both the meaning of the literature and the ontology, capture this fact in their annota-
tions. Our process merely retrieves this underlying implicit understanding from the
annotations in the literature and then leverages that structure to aid in the annotation
process.
We asses the performance of our approach using the F1-micro score, based on preci-
sion and recall [28]. In all our calculations, we set ? = 1
F? =
(
1 + ?2
) precision ? recall
?2 ? precision + recall
We first construct 5 separate naïve Bayes classifiers for each of the 5 categories as
formalized in section 2.2. Each classifier is then trained and tested on the entire corpus
of abstracts using 10-fold cross-validation, and their F1-micro scores are calculated.
Abstracts in the testing set are annotated with a label if the label had a probability
score greater than Fb = 0.1.
Next we construct the Bayesian Decision Trees as formalized in the previous section.
Given that we have 5 categories, we build all 120 possible BDTs. We annotate the cor-
pus of abstracts using the BDTs with the criterion that if the probability of a label is
greater than 0.1 for some abstract, then that abstract is tagged with that label. Next we
aggregate the labels across each of the 5 categories and calculate a mean F-score for
each category to determine the quality of the annotations for each instance of the
category across all trees as seen in Figure 3.
Table 2 High level description of the algorithm.
Input
 Un-Labeled Item I
 Bayesian Decision Tree T
Output
 Label Vector in Multiple Categories L
Algorithm
t = Root(T)
SearchList = NULL
while t ~= NULL do
L = L : Bt(I)
for l  Bt(I) do
SearchList = SearchList : Child(l, t)
end for
t = SearchList[0]
x : SearchList = SearchList
end while
return L
This recursive program uses the Bayesian Decision Tree defined in Definition 9, along with Bayesian Classifier of
Definition 10 and the child function of Definition 11 to label an unlabeled item. Unlike a normal naive Bayes classifier
that is trained on the whole training set, this algorithm steps through a decision tree whose every node contains a
classifier that is trained on a narrow subset of the original training set. This subset is limited to only those items which
are annotated with the labels of the ancestors of the current node.
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2
Page 10 of 15
Our approach can also be extended to the case in which the human subject matter
expert is in the classification loop and has an input to the automated annotation pro-
cess, that is, the human subject matter expert is using our algorithm to more efficiently
annotate the set of abstracts. A human subject matter expert can usually determine the
label for at least one of the categories with a quick glance at an abstract. For exmaple,
if the abstract explicitly states that the experiment used a picture of faces as the stimu-
lus, or that subjects pushed a button with their foot to respond. To model this, we
trained our BDTs with the condition that the root node has already been decided. We
call this the Constrained Decision Tree (CDT). As a result we have trees rooted at SM,
ST, RM, RT, and I, corresponding to the cases where the human expert assigns the
label for that category. The rest of the tree is constructed exactly as before except that,
when the mean f-score is calculated for each category across all possible CDTs, we
remove the instances corresponding to the annotations assigned by the human subject
matter expert since we do not want them to influence the results returned by our
algorithm.
Results and discussions
Figure 4. shows an overview of the entire process. The first task of the annotation pro-
cess is handled by the naïve Bayes algorithm. The output of the naïve Bayes algorithm
is then used by the Bayesian decision tree algorithm to calculate the annotation tags.
Our results are shown in Figure 5. The error bars presented are twice the standard
deviation with respect to the mean of the F1-micro score for each category. F1-micro
scores for Stimulus Type (ST) and Instructions (I) are lower than in the other cate-
gories because of the large number of labels they incorporate, leading to lower relative
sample size for each label. Stimulus Modality (SM), Response Modality (RM), and
Response Type (RT) have fewer labels and thus produce better performance.
For Response Modality (RM), Response Type (RT), and Instructions (I), the Decision
Tree F1-micro score is slightly lower than that of the naïve Bayes because our sample
size constriction for the training sets at each level of the decision tree decreases preci-
sion and recall for labels lower down in the tree, and any increases due to underlying
correlations are not sufficient to make up for this decrease. The Constrained Decision
Tree always has a higher F1-micro score than the other methods because the guaran-
tee of correct labels in the first category of each tree is leveraged through the cascading
correlations among labels in different categories further down the tree and the labels
discovered in the root nodes category.
The combination of the stochastic representational power of the naïve Bayes with the
expressive simplicity of the Bayesian Decision Trees allows our automated classifier to
achieve a significant improvement in the annotation of literature as compared to exist-
ing string-matching tools like the NCBO Annotator. Not only are we able to annotate
across multiple categories, but our method also captures the implicit structural depen-
dencies induced in the set of labels found in the gold standard labelled corpus. Of
course, this capture process will vary with the corpus to which it is applied, and a dif-
ferent corpus for the same ontology being modeled by the same gold standard will
produce a different reification of the dependencies captured in the form of annotations
across categories. Thus, instead of explicitly modeling the relationships between super-
classes and classes directly from the ontology, we have developed a stochastic model
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2
Page 11 of 15
that can capture the effect of those superclass-class relationship indirectly from the
specific combination of human annotations and the corpus. Thus the same stochastic
meta-algorithm can be applied to solve similar automated annotation problems with
different ontologies, as well as a different gold standard for that ontology applied to
several different corpora.
Figure 4 Decision Trees. In this figure we can see an abstract going through a few steps of the annotation
process for both a regular naive Bayes classifier trained on the gold standard corpus and a Bayesian decision
tree. The abstract classified by the naive Bayes classifier is classified without regard to decisions already made
by the classifier. Therefore, it is classified with the label False Font as its stimulus modality even though its
stimulus type was Auditory. By contrast, the when the Bayesian decision tree needs to identify a Stimulus Type
it uses a classifier trained on a set of abstracts which are all annotated with the label Auditory and thus picks
Chord Sequence as the abstracts Stimulus Type.
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2
Page 12 of 15
The constrained human-in-the-loop decision tree architecture further improves upon
the naïve Bayes results. When we fix the first node of the decision tree, there is a sig-
nificant improvement in the annotation accuracy. This is a useful tool for aiding a
human expert in annotation because the expert can usually select one annotation from
several categories with a quick skim of an abstract. Our technique can then annotate
the remaining categories with high accuracy. Although this approach does not elimi-
nate the human expert from the loop, it complements their decision-making and has
the potential to reduce the time and effort for the full annotation task.
Conclusions and future work
We have demonstrated a stochastic framework for annotating BrainMap literature using
the Cognitive Paradigm Ontology. Unlike text mining algorithms, our framework can
model the knowledge encoded by the dependencies in the ontology, albeit indirectly. We
successfully exploit the fact that CogPO has explicitly stated restrictions, and implicit
dependencies in the form of patterns in the expert curated annotations. The advantage of
our pragmatic approach is that it is robust to explicit future modifications and additions
that could be made to the relationships and restrictions in CogPO. Since we do not expli-
citly model the relations and restrictions, but capture them implicitly from training
patterns, we do not have to make corresponding updates to our algorithm each time
CogPO is updated by humans. We merely need to have a correctly annotated body of work.
The constrained decision tree architecture further improves upon the naïve Bayes
results. When we fix the first node of the decision tree, there is a significant improvement
in the annotation accuracy. This is a useful tool for aiding a human expert in the annota-
tion task.
We next plan to apply our techniques to different ontologies with more complex
structures. We believe the modular nature of our framework will scale well to these
Figure 5 Comparison of Methods. F1 micro scores for the annotation returned for the Stimulus Modality,
Stimulus Type, Response Modality, Response Type, and Instructions. The error bars are twice the standard
deviation.
Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2
http://www.jbiomedsem.com/content/5/S1/S2
Page 13 of 15
new ontologies. There is additional progress to be made in algorithmically learning
gaps (missing labels) in the ontology. We speculate that our technique can find missing
restrictions and relations not explicitly defined in CogPO.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
CC and TBJ designed and implemented the experiments, the algorithms, and the formal framework. GFL and JAT are
the PIs of the project and secured the funding. GFL, as the computer science lead, coordinated the technical aspects
of the research. GFL and JAT supervised the development tasks of the project. MDT performed statistical testing and
analysis. ARL created the gold standard corpus. JFW implemented helper functions and other utilities.
Acknowledgements
This project is made possible by a collaboration agreement allowing comprehensive access to the BrainMap database,
a copyrighted electronic compilation owned by the University of Texas. The authors thank Peter T. Fox for helping
with this access. This research is supported by NIMH / NIH awards R56-MH097870, R01-MH084812, and R01-MH074457
and by the Department of Computer Science of the University of New Mexico.
Declarations
Ths article is published as part of a supplement Bio-Ontologies 2013. This supplement is supported by NIMH / NIH
awards R56-MH097870, R01-MH084812, and R01-MH074457 and by the Department of Computer Science of the
University of New Mexico.
This article has been published as part of Journal of Biomedical Semantics Volume 5 Supplement 1, 2014: Proceedings
of the Bio-Ontologies Special Interest Group 2013. The full contents of the supplement are available online at http://
www.jbiomedsem.com/supplements/5/S1.
Authors details
1Department of Computer Science, University of New Mexico, Albuquerque, New Mexico, USA. 2Mind Research
Network, Albuquerque, New Mexico, USA. 3Department of Physics, Florida International University, Miami, Florida, USA.
4Department of Psychology and the Neuroscience Institute, Georgia State University, Atlanta, Georgia, USA.
Published: 3 June 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Katayama et al. Journal of Biomedical Semantics 2014, 5:5
http://www.jbiomedsem.com/content/5/1/5REVIEW Open AccessBioHackathon series in 2011 and 2012:
penetration of ontology and linked data in life
science domains
Toshiaki Katayama1*, Mark D Wilkinson2, Kiyoko F Aoki-Kinoshita3, Shuichi Kawashima1, Yasunori Yamamoto1,
Atsuko Yamaguchi1, Shinobu Okamoto1, Shin Kawano1, Jin-Dong Kim1, Yue Wang1, Hongyan Wu1,
Yoshinobu Kano4, Hiromasa Ono1, Hidemasa Bono1, Simon Kocbek1, Jan Aerts5,6, Yukie Akune3, Erick Antezana7,
Kazuharu Arakawa8, Bruno Aranda9, Joachim Baran10, Jerven Bolleman11, Raoul JP Bonnal12, Pier Luigi Buttigieg13,
Matthew P Campbell14, Yi-an Chen15, Hirokazu Chiba16, Peter JA Cock17, K Bretonnel Cohen18,
Alexandru Constantin19, Geraint Duck19, Michel Dumontier20, Takatomo Fujisawa21, Toyofumi Fujiwara22,
Naohisa Goto23, Robert Hoehndorf24, Yoshinobu Igarashi15, Hidetoshi Itaya8, Maori Ito15, Wataru Iwasaki25,
Matú Kala26, Takeo Katoda3, Taehong Kim27, Anna Kokubu3, Yusuke Komiyama28, Masaaki Kotera29,
Camille Laibe30, Hilmar Lapp31, Thomas Lütteke32, M Scott Marshall33, Takaaki Mori3, Hiroshi Mori34, Mizuki Morita35,
Katsuhiko Murakami36, Mitsuteru Nakao37, Hisashi Narimatsu38, Hiroyo Nishide16, Yosuke Nishimura29,
Johan Nystrom-Persson15, Soichi Ogishima39, Yasunobu Okamura40, Shujiro Okuda41, Kazuki Oshita8,
Nicki H Packer42, Pjotr Prins43, Rene Ranzinger44, Philippe Rocca-Serra45, Susanna Sansone45, Hiromichi Sawaki38,
Sung-Ho Shin27, Andrea Splendiani46,47, Francesco Strozzi48, Shu Tadaka40, Philip Toukach49, Ikuo Uchiyama16,
Masahito Umezaki50, Rutger Vos51, Patricia L Whetzel52, Issaku Yamada53, Chisato Yamasaki15,36, Riu Yamashita54,
William S York44, Christian M Zmasek55, Shoko Kawamoto1 and Toshihisa Takagi56Abstract
The application of semantic technologies to the integration of biological data and the interoperability of
bioinformatics analysis and visualization tools has been the common theme of a series of annual BioHackathons
hosted in Japan for the past five years. Here we provide a review of the activities and outcomes from the
BioHackathons held in 2011 in Kyoto and 2012 in Toyama. In order to efficiently implement semantic technologies
in the life sciences, participants formed various sub-groups and worked on the following topics: Resource Description
Framework (RDF) models for specific domains, text mining of the literature, ontology development, essential metadata
for biological databases, platforms to enable efficient Semantic Web technology development and interoperability, and
the development of applications for Semantic Web data. In this review, we briefly introduce the themes covered by
these sub-groups. The observations made, conclusions drawn, and software development projects that emerged from
these activities are discussed.
Keywords: BioHackathon, Bioinformatics, Semantic Web, Web services, Ontology, Visualization, Knowledge
representation, Databases, Semantic interoperability, Data models, Data sharing, Data integration* Correspondence: ktym@dbcls.jp
1Database Center for Life Science, Research Organization of Information and
Systems, 2-11-16, Yayoi, Bunkyo-ku, Tokyo 113-0032, Japan
Full list of author information is available at the end of the article
© 2014 Katayama et al.; licensee BioMed Central Ltd. This is an open access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Katayama et al. Journal of Biomedical Semantics 2014, 5:5 Page 2 of 13
http://www.jbiomedsem.com/content/5/1/5Introduction
In life sciences, the Semantic Web is an enabling technol-
ogy which could significantly improve the quality and ef-
fectiveness of the integration of heterogeneous biomedical
resources. The first wave of life science Semantic Web
publishing focused on availability - exposing data as RDF
without significant consideration for the quality of the
data or the adequacy or accuracy of the RDF model used.
This allowed a proliferation of proof-of-concept projects
that highlighted the potential of Semantic technologies.
However, now that we are entering a phase of adoption of
Semantic Web technologies in research, quality of data
publication must become a serious consideration. This is
a prerequisite for the development of translational re-
search and for achieving ambitious goals such as personal-
ized medicine.
While Semantic technologies, in and of themselves, do
not fully solve the interoperability and integration problem,
they provide a framework within which interoperability is
dramatically facilitated by requiring fewer pre-coordinated
agreements between participants and enabling unantici-
pated post hoc integration of their resources. Nevertheless,
certain choices must be made, in a harmonized manner, to
maximize interoperability. The yearly BioHackathon series
[1-3] of events attempts to provide the environment within
which these choices can be explored, evaluated, and then
implemented on a collaborative and community-guided
basis. These BioHackathons were hosted by the National
Bioscience Database Center (NBDC) [4] and the Database
Center for Life Science (DBCLS) [5] as a part of the Inte-
grated Database Project to integrate life science databases
in Japan. In order to take advantage of the latest technolo-
gies for the integration of heterogeneous life science data,
researchers and developers from around the world were
invited to these hackathons.Figure 1 Overview of categories and topics raised during the BioHack
semantic relationships between categories.This paper contains an overview of the activities and
outcomes of two highly interrelated BioHackathon events
which took place in 2011 [6] and 2012 [7]. The themes of
these two events focused on representation, publication,
and exploration of bioinformatics data and tools using
standards and guidelines set out by the Linked Data and
Semantic Web initiatives.
Review
Semantic Web technologies are formalized as World
Wide Web consortium (W3C) standards aimed at creat-
ing general-purpose, long-lived data representation, ex-
change, and integration formats that replace current ad
hoc solutions. However, because they are general-purpose
standards, many issues need to be addressed and agreed-
upon by the community in order to apply them success-
fully to the integration and interoperability problems of
the life science domain. Therefore, participants of the
BioHackathons fall into sub-groups of interest within
the life sciences, representing the specific needs and
strengths of their individual communities within the
broader context of life science informatics. Though
there were multiple specific activity groups under each
of the following headings, and there was overlap and
cross-talk between the activities of each group, we will
organize this review under the five general categories of:
RDF data, Ontology, Metadata, Platforms and Applica-
tions (Figure 1). Results and issues raised by each group
are briefly summarized in the Table 1. We also note that
many groups have or will publish their respective out-
comes in individual publications.
RDF data
In terms of RDF data generation, data were generated
for genomic and glycomic databases (domain-specificathons of 2011 and 2012. Lines between the boxes represent
Table 1 Summary of investigated issues and results covered during BioHackathons 2011 and 2012
RDF data
Domain specific models
Genome and proteome data
Issue: No standard RDF data model and tools existed for major genomic data
Result: Created FALDO, INSDC, GFF, GVF ontologies and developed converters
Software: Converters are now packaged in the BioInterchange tool; improved PSICQUIC service
Glycome data
Issue: Glycome and proteome databases are not effectively linked
Result: Developed a standard RDF representation for carbohydrate structures by BCSDB, GlycomeDB, GLYCOSCIENCES.de,
JCGGDB, MonosaccharideDB, RINGS, UniCarbKB and UniProt developers
Software: RDFized data from these databases, stored them in Virtuoso and tested SPARQL queries among the different data
resources
Text processing
Text extraction from PDF and metadata retrieval
Issue: Text for mining is often buried in the PDF formatted literature and requires preprocessing
Result: Incorporated a tool for text extraction combined with a metadata retrieval service for DOIs or PMIDs
Software: Used PDFX for text extraction; retrieved metadata by the TogoDoc service
Named entity recognition and RDF generation
Issue: No standard existed for combining the results of various NER tools
Result: Developed a system for combining, viewing, and editing the extracted gene names to provide RDF data
Software: Extended SIO ontology for NER and newly developed the BioInterchange tool for RDF generation
Natural language query conversion to SPARQL
Issue: Automatic conversion of natural language queries to SPARQL queries is necessary to develop a human friendly interface
Result: Incorporated the SNOMED-CT dataset to answer biomedical questions and improved linguistic analysis
Software: Improved the in-house LODQA system; used ontologies from BioPortal
Ontology
IRI mapping and normalization
Issue: IRIs for entities automatically generated by BioPortal do not always match with submitted RDF-based ontologies
Result: Normalized IRIs in the BioPortal SPARQL endpoint as either the provider IRI, the Identifiers.org IRI, or the Bio2RDF IRI
Software: Used services of BioPortal, the MIRIAM registry, Identifires.org and Bio2RDF
Environmental ontologies for metagenomics
Issue: Semantically controlled description of a samples original environment is needed in the domain of metagenomics
Result: Developed the Metagenome Environment Ontology (MEO) for the MicrobeDB project
JOURNAL OF
BIOMEDICAL SEMANTICS
Van Slyke et al. Journal of Biomedical Semantics 2014, 5:12
http://www.jbiomedsem.com/content/5/1/12RESEARCH Open AccessThe zebrafish anatomy and stage ontologies:
representing the anatomy and development of
Danio rerio
Ceri E Van Slyke1*, Yvonne M Bradford1*, Monte Westerfield1,2 and Melissa A Haendel3Abstract
Background: The Zebrafish Anatomy Ontology (ZFA) is an OBO Foundry ontology that is used in conjunction with
the Zebrafish Stage Ontology (ZFS) to describe the gross and cellular anatomy and development of the zebrafish,
Danio rerio, from single cell zygote to adult. The zebrafish model organism database (ZFIN) uses the ZFA and ZFS to
annotate phenotype and gene expression data from the primary literature and from contributed data sets.
Results: The ZFA models anatomy and development with a subclass hierarchy, a partonomy, and a developmental
hierarchy and with relationships to the ZFS that define the stages during which each anatomical entity exists. The
ZFA and ZFS are developed utilizing OBO Foundry principles to ensure orthogonality, accessibility, and
interoperability. The ZFA has 2860 classes representing a diversity of anatomical structures from different anatomical
systems and from different stages of development.
Conclusions: The ZFA describes zebrafish anatomy and development semantically for the purposes of annotating
gene expression and anatomical phenotypes. The ontology and the data have been used by other resources to
perform cross-species queries of gene expression and phenotype data, providing insights into genetic relationships,
morphological evolution, and models of human disease.Background
Zebrafish (Danio rerio) share many anatomical and physio-
logical characteristics with other vertebrates, including
humans, and have emerged as a premiere organism to
study vertebrate development and genetics [1]. Zebrafish
are amenable to genetic manipulation, and several tech-
niques allow recovery of zebrafish mutations affecting
developmental patterning, organogenesis, physiology, be-
haviour, and numerous other biological processes [2-4]. In
addition to genetic screens, zebrafish are amenable to tar-
geted gene knock-down utilizing morpholino antisense oli-
gonucleotides (MOs) [5], TALENs [6], and CRISPRs [7].
Use of transgenic constructs in zebrafish has further expe-
dited the study of gene function [8,9]. These various
methods for altering gene expression and regulation have
generated a plethora of data that enable modelling of dis-
ease states and that provide a greater understanding of gene* Correspondence: van_slyke@zfin.org; ybradford@zfin.org
Equal contributors
1ZFIN, 5291 University of Oregon, Eugene, OR 97403-5291, USA
Full list of author information is available at the end of the article
© 2014 Van Slyke et al.; licensee BioMed Cent
Commons Attribution License (http://creativec
reproduction in any medium, provided the orfunction, development, and evolution. ZFIN, the zebrafish
model organism database [10] manually curates these dis-
parate data obtained from the literature or by direct data
submission.
ZFIN serves as the central repository for zebrafish
genetic, genomic, phenotypic, expression, and develop-
mental data and provides a community web based re-
source to enable access to this highly integrated data
[11,12]. To support annotation of gene expression pat-
terns and phenotype information in wild types and fish
with altered gene function, ZFIN has developed the
Zebrafish Anatomy Ontology (ZFA) [13] and the Zebra-
fish Stage Ontology (ZFS) [14]. By using the ZFA and
ZFS to annotate gene expression and phenotypic data,
ZFIN is able to provide efficient querying and analysis
across ZFIN data as well as cross-species inference [15].
ZFIN is actively involved in the zebrafish and ontology
research communities to improve the ZFA through
addition of classes, definitions, relations, and common
design patterns and efforts towards interoperability with
other ontologies. We report here on the design of theral Ltd. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly credited.
Van Slyke et al. Journal of Biomedical Semantics 2014, 5:12 Page 2 of 11
http://www.jbiomedsem.com/content/5/1/12ZFA and ZFS, the current state of the ontologies, and
ongoing efforts to maintain these ontologies for repre-
senting the knowledge of zebrafish and more broadly,
vertebrate anatomy.
Results
Design considerations ZFA
The ZFA ontology includes a representation of the anat-
omy of Danio rerio at all stages of life - from a single-
cell zygote to an adult. The main features of the ZFA, in
addition to its largely structure-based subclass hierarchy,
are its partonomy (using the part_of relation) and de-
velopmental hierarchy (using the develops_from rela-
tion). Each anatomical class in ZFA is defined using these
relationships to other classes in ZFA as well as to stage
classes in ZFS. The relations used in the ZFA and ZFS on-
tologies are listed in Table 1, and include start_stage
and end_stage. The start_stage utilized is equiva-
lent to Relation Ontology (RO) [16] starts_during and
end_stage is equivalent to RO ends_during. In this
way, each anatomical entity can be defined in terms of what
it is a type of, what it is a part of, what it develops from, and
during which stages it exists. Figure 1 shows an example of
how the ZFA describes the development of the heart and il-
lustrates the relationships used to describe the partonomy,
developmental series, and relationships between anatomical
entities and stages.
The ZFA was developed based on the original zebrafish
anatomical dictionary, which was a loosely structured part-
onomy. The anatomical dictionary was developed to: 1)
computationally disseminate gene expression and pheno-
typic data; 2) define the anatomical structures of the zebra-
fish to establish an ontological framework that could be
used by all zebrafish researchers to describe and contribute
data; 3) provide an interoperable anatomical description of
zebrafish to effectively map relationships between analo-
gous structures across species [17]. The biologists involved
in conceptualizing the ZFA used the anatomical dictionary
as a framework and structured the ZFA according to the
original version of the Common Anatomy Reference Ontol-
ogy (CARO) [18] at its upper levels of organization, makingTable 1 Examples of relationships used in the ZFA and ZFS
Property Explanation
is_a (subclass in OWL) Subtypes a class by its intrinsic nature.
part_of (BFO:0000050) Describes what structure or system the cla
a part of.
develops_from (RO:0002202) Describes a class by its progenitors.
start_stage (RO:0002091) Describes a class that is observed to begin
particular stage.
end_stage (RO:0002093) Describes a class that is observed to end
particular stage.
immediately_preceded_by
(RO:0002087)
Describes the order in which process clas
occur in time.the ZFA interoperable with other ontologies built using
CARO as a framework. This is in contrast to alternative
approaches taken by the Mouse Gross Anatomy and
Development Ontology (EMAP) [19,20], or the Drosophila
gross anatomy (FBbt) [21], where a partonomy is repre-
sented for each developmental, or life, stage. In addition to
the standard CARO classes, the ZFA includes an additional
high level term embryonic structure (ZFA:0001105)a, to
organize embryonic tissues described by fate mapping or
gene expression in the early embryo. This class is especially
useful for organizing presumptive anatomical structures or
areas described as anlagen, primordia, or undifferentiated
buds. These developmental classes are difficult to classify
structurally, without use of more complex class expressions,
thus it makes more sense for the user to organize these
classes based on ontogeny. Structural representation of
such entities was originally described by the CARO devel-
opers [18], but as was noted, requires enhancement based
on ontogeny.
ZFA classes have human-readable text definitions that
usually are structured in the genus-differentia format as
recommended in Smith et al., 2007 [22] and codified in
the 2008 OBO Foundry principles [23]. Class definitions
also include further biological description to aid in the
identification and understanding of zebrafish anatomy
structures by the user or annotator. The ZFA does not
have logical (computable) definitions at this time, though
these are targeted for future development. In the mean-
time, many computable definitions for ZFA classes can
be found in the uberon-zfa file [24].
In an effort to include cell terms in the ZFA needed to
support partonomy-based queries, we incorporated the
appropriate leaf nodes of the Cell Ontology (CL) [25].
Reusing CL classes instead of making new zebrafish cell
classes allows the ZFA to be more interoperable with the
other OBO foundry ontologies [26]. To accommodate
proper reasoning using these species-independent classes,
the file header includes the line treat-xrefs-as-genus-differ-
entia: CL part_of NCBITaxon:7955 that informs users and
JOURNAL OF
BIOMEDICAL SEMANTICS
Thacker et al. Journal of Biomedical Semantics 2014, 5:39
http://www.jbiomedsem.com/content/5/1/39RESEARCH Open AccessThe Porifera Ontology (PORO): enhancing sponge
systematics with an anatomy ontology
Robert W Thacker1, Maria Cristina Díaz2, Adeline Kerner3, Régine Vignes-Lebbe3, Erik Segerdell4,
Melissa A Haendel4 and Christopher J Mungall5*Abstract
Background: Porifera (sponges) are ancient basal metazoans that lack organs. They provide insight into key
evolutionary transitions, such as the emergence of multicellularity and the nervous system. In addition, their ability
to synthesize unusual compounds offers potential biotechnical applications. However, much of the knowledge of
these organisms has not previously been codified in a machine-readable way using modern web standards.
Results: The Porifera Ontology is intended as a standardized coding system for sponge anatomical features
currently used in systematics. The ontology is available from http://purl.obolibrary.org/obo/poro.owl, or from the
project homepage http://porifera-ontology.googlecode.com/. The version referred to in this manuscript is
permanently available from http://purl.obolibrary.org/obo/poro/releases/2014-03-06/.
Conclusions: By standardizing character representations, we hope to facilitate more rapid description and
identification of sponge taxa, to allow integration with other evolutionary database systems, and to perform character
mapping across the major clades of sponges to better understand the evolution of morphological features. Future
applications of the ontology will focus on creating (1) ontology-based species descriptions; (2) taxonomic keys that use
the nested terms of the ontology to more quickly facilitate species identifications; and (3) methods to map anatomical
characters onto molecular phylogenies of sponges. In addition to modern taxa, the ontology is being extended to
include features of fossil taxa.
Keywords: Morphology, Taxonomic identification, Phylogenetics, EvolutionBackground
Porifera (sponges) are sessile, aquatic, multicellular ani-
mals that lack true organs and a nervous system. In-
stead, sponges contain loosely aggregated cells that can
differentiate into a variety of cell types and produce di-
verse skeletal structures. These skeletal elements can be
comprised of proteinaceous spongin, chitin, collagen,
calcium carbonate and/or silica, depending on the spe-
cies. Traditional sponge systematics defines sponge taxa
by recognizing particular sets of morphological features
described in sources such as Systema Porifera [1]. Al-
though these features have been well characterized in
the Thesaurus of Sponge Morphology [2,3], and used in
pioneering Artificial Intelligence (AI) classification sys-
tems [4-6], the terms that are used to describe sponge* Correspondence: cjmungall@lbl.gov
5Genomics Division, Lawrence Berkeley National Laboratory, Berkeley, CA,
USA
Full list of author information is available at the end of the article
© 2014 Thacker et al.; licensee BioMed Centra
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.morphology have not previously been organized into the
framework of a modern ontology.
Sponges are conspicuous components of most benthic
marine ecosystems such as shallow coral reefs, man-
groves, mesophotic reefs, and deep water environments
[7]. Sponges play critical roles in these ecosystems, con-
tributing to global cycling of carbon and nitrogen, stabil-
izing (but also eroding) coral reef frameworks, and
hosting incredibly diverse communities of macroscopic
and microscopic symbionts [8,9]. Furthermore, sponges
have therapeutic potential and other human applications
due to their ability (or that of their symbionts) to
synthesize various unusual compounds [10] and there-
fore present a wealth of biotechnological application
opportunities.
Sponge life depends on the flow of water through an
aquiferous system (Figure 1), with water flowing into the
body through incurrent openings (ostia), through a net-
work of canals that are lined by internal epithelium-likel Ltd. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0) which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Figure 1 Marine sponges like Agelas conifera (a, left) contain an aquiferous system that pumps water through the sponge body
(b, right; from [2]).
Thacker et al. Journal of Biomedical Semantics 2014, 5:39 Page 2 of 8
http://www.jbiomedsem.com/content/5/1/39cells (pinacocytes), into chambers lined by collared, flag-
ellated cells (choanocytes), and out of the body through
excurrent openings (oscules). Choanocytes closely re-
semble choanoflagellates, a group of unicellular eukary-
otes that are among the closest relatives to multicellular
animals [11]. Sponges are of interest to evolutionary bi-
ologists studying the origins of multicellularity in ani-
mals and the origins of the nervous system [12]. Despite
having no neurons or synapses, some sponges have a
nearly complete set of post-synaptic protein homologs
[13]. Likewise, sponges possess the elements of the cad-
herin and ?-catenin complex that are critical for cellular
adhesion in bilaterian tissues [11]. Therefore, a more for-
mal representation of poriferan anatomy would enable
more complex queries across a diversity of taxa in search
of protein, network, and biological processes that have
regulated the evolution of multicellularity and the ner-
vous system.
Ontologies for evolution
Biological and biomedical ontologies are structured vo-
cabularies that provide consistent names and textual def-
initions for anatomical structures, biochemical entities,
processes and functions associated with gene products,
and many other kinds of biological features. With the
success of the Gene Ontology (GO) [14], ontologies have
become common in biology [15-17] and more recently
the systematics and evolutionary phenotype communi-
ties have begun to use them for character description
[18-20]. Through the use of description logic formalisms
underpinning the Web Ontology Language (OWL) [21]
(a World Wide Web Consortium standard) they facili-
tate semantic reasoning within and across domains that
can be performed by computers. Many freely available,
open-source ontologies have been developed that pro-
vide terms suitable for annotating, describing, and inte-
grating a wide array of biological data [22].Ontologies are not the same as databases or taxo-
nomic keys  however, ontologies can be used as an en-
hancement to database systems and keys, both as a
standard terminology, allowing different database sys-
tems to interoperate, and as a logical extension, allowing
domain knowledge to be encoded in a way that enhances
query capabilities or data integrity. For example, the GO
provides a stable identifier denoting the biological
process of apoptosis  different databases can use this
same identifier for describing genes involved in apoptosis,
allowing integration of data from multiple databases cov-
ering genes in a variety of species. Furthermore, the know-
ledge that apoptosis is a kind of cell death is encoded in
the ontology, which means queries for genes involved in
cell death will return genes described as being involved
in apoptosis.
One important type of biological ontology is the anat-
omy ontology. Anatomy ontologies typically include re-
lationships between structures, such as the relationship
of parthood between a hand and a limb. In sum, these
relationships form a graph structure that can be used to
enhance database queries or bioinformatic analyses. For
example, a database query for genes expressed in the
limbs can return genes expressed in different parts of
the limb (such as the hands) or deeper in the part-
hierarchy (e.g., in the distal part of the finger). Anatom-
ical ontologies are also used to standardize character-
state descriptions in evolutionary databases, such as, for
example with the Phenoscape knowledge base [18].
The fundamental unit of these anatomical graph struc-
tures are classes (also known as terms). Each class repre-
sents a distinct anatomical feature and is typically
assigned a unique identifier that provides a key with
which it may be cross-referenced to other ontologies
or databases. The open nature of commonly used
bio-ontologies allows terminology and definitions to
be re-used from other ontologies with which they
Thacker et al. Journal of Biomedical Semantics 2014, 5:39 Page 3 of 8
http://www.jbiomedsem.com/content/5/1/39overlap, reducing duplication of effort and promoting
orthogonality.
Some anatomy ontologies cover a specific taxon  for
example, the Drosophila Anatomy Ontology (DAO) [23]
Others are applicable to a wide range of taxa  for ex-
ample, the Plant Ontology (PO) [24] or the Uberon anat-
omy ontology [25], which covers metazoans. Until now,
the major focus of anatomy ontologies has been plants
and bilaterians, with no representation of the unique
biology of sponges  whilst Uberon includes structures
applicable across animals, the focus of the ontology is
chordates, with the intention of federating with other
metazoan ontologies.
For a period in the 1990s, sponges were amongst the
domains modeled in pioneering Expert System research
[4,6]. In particular, SPONGIA was a rule-based system
for classifying the species of a sponge given as input a
set of character descriptors and measurements [26]. Ex-
pert systems have some similarities with ontologies 
both are concerned with knowledge representation and
classification of concepts and data. In fact, expert sys-
tems research has largely fragmented into different data
science domains, including Bayesian networks (for rep-
resenting and reasoning with probabilistic knowledge)
and description-logic based ontologies (for representing
and reasoning with boolean knowledge). One conse-
quence of these advances in information science is that
first-generation expert systems do not interoperate with
modern information systems. Ontologies provide a
means of encoding domain knowledge in an application-
independent way.
The present study initiates an ontological approach to
the morphology of Porifera by interpreting and organiz-
ing the major anatomical characters developed by
sponge taxonomists as summarized by the Thesaurus of
Sponge Morphology [2].
Results and discussion
Ontology contents
We constructed the Porifera ontology (PORO) as a Web
Ontology Language (OWL) ontology using the The-
saurus of Sponge Morphology as a primary source. The
ontology primarily focuses on anatomical structures, but
includes other kinds of entities of interest to Poriferan
biologists  for example, traits and chemical entities.
Each anatomical entity is represented using an OWL
class which is uniquely identified by a URI (uniform re-
source identifier) in the OBO Library PORO identifier
space. In this paper, we provide examples of classes
using short forms of these URIs - for example, PORO_
0000017 identifies the class spicule. The full URI of this
class is http://purl.obolibrary.org/obo/PORO_0000017,
which resolves to an OWL document rendered as a
human-readable web page using the OntoBee system. Inthe current release, the ontology contains 625 classes
unique to PORO (i.e., not imported from other ontologies),
with 27 classes imported from other ontologies. Of the 625
unique classes, 519 have definitions that have been sourced
from the Thesaurus of Sponge Morphology [2].
Upper level classification
The ontology follows the Common Anatomy Reference
Ontology (CARO) [27] upper level, making use of stand-
ard upper-level terms such as organism substance and
material anatomical entity to structure the ontology.
Due to the fundamental biology of sponges, many
CARO classes such as organ were not used. In contrast
to other anatomy ontologies, many (50%) of the anatom-
ical classes in the ontology are subtypes of acellular ana-
tomical entity (for example, spicules and fibers). At this
time, only a minimal subset of CARO is being used (9
classes). CARO is currently being refactored and ex-
tended, and the development of PORO will serve as a
use case for this work. For example, CARO may include
a generic class for representing anatomical chambers,
which may serve as the parent class for choanocyte
chamber in PORO.
Body plan
A sponge body consists of three distinct functional
layers around an aquiferous system that can consist of a
combination of pores, incurrent and excurrent canals,
choanocyte chambers, and exhalent atria (Figure 1). The
most interior layer (the choanoderm) contains choano-
cytes, which are the collared, flagellated cells that form
the choanocyte chambers (PORO_0000025). The most
exterior layer (the pinacoderm) contains the epithelial-
like pinacocytes (PORO_0000023), which are tightly
connected to each other and line the internal canals and
external surfaces. Sandwiched between these two layers
is the mesohyl (PORO_0000002), an extracellular matrix
composed primarily of galectin, collagen, fibronectin-like
molecules, dermatopontin, and other polypeptides; the
mesohyl contains cells (microbial and eukaryotic) and
skeletal elements (collagen, spongin, chitin, and/or min-
erals) [2,28]. The choanoderm, pinacoderm, and mesohyl
are represented as separate non-overlapping partitions
of the sponge body through the use of OWL General
Class Inclusion axioms (GCIs). The sponge aquiferous
system can be very simple, as in the small, sac-shaped,
asconoid (PORO_0000149) bodies of some Calcarea, or
extremely complex, as in the leuconoid (PORO_
0000028) structures found in most other sponges [29].
Acellular structures comprising the architecture of sponges
Characteristic features of many sponges are spicules
(PORO_0000017), which form the skeleton of the organ-
ism in most cases. Spicules can be composed of calcium
Thacker et al. Journal of Biomedical Semantics 2014, 5:39 Page 4 of 8
http://www.jbiomedsem.com/content/5/1/39carbonate, silica, or spongin. They may also be classified
by size (megascleres or microscleres). The primary
means of classifying them is by their morphological, and
in particular, symmetric structure. For example, a triax-
one (PORO_0000602) is a spicule with 3 axes and 6 rays.
This can be modeled precisely in OWL using a construct
called a cardinality constraint. In the Manchester Syntax
variant of OWL, this is written as:
triaxone EquivalentTo
spicule and
(has_component exactly 3 ray axis) and
(has_component exactly 6 ray)
Figure 2 shows a subset of the spicule hierarchy,
focused on acanthostyle.
Fibers are another architecturally important class, with
fiber skeletal arrangements usually being dendritic or re-
ticulate. Spicules can be embedded within the fibers or
echinate (protrude from) the exterior of the fibers. The
overall pattern of spicule and fiber distribution within a
sponge is termed the skeletal arrangement, with many
defined categories (for example, a dendritic or a reticu-
late arrangement, but there are several other possible
patterns). These different skeleton types are represented
in the ontology.
Cell types
We decided to keep cell types within PORO, rather than
add them to the central OBO cell type ontology (CL)
[30], as these are relatively few in number and are largely
specific to sponges. Examples include bacteriocyte
(PORO_00001062), actinocyte (PORO_0000107) and
choanocyte (PORO_0000003). The latter is of particular
interest to evolutionary biologists due to their proposed
homology to choanoflagellates. Many sponges lack true
epithelia with basement membranes, so we introduce ast ac sg ox
st
ac
Figure 2 Ontology visualization showing a portion of the spicule hier
visualization is drawn using the OBO-Edit Graph View plug-in for Protégé 4
an acanthostyle (ac), a strongyle (sg), and an oxea (ox).class epithelioid cell (PORO_0000004) rather than re-
using the Cell Ontology class for epithelial cell. How-
ever, many of the gene products required for epithelia
are found in Porifera [31], and Homoscleromorpha have
basement membranes and true epithelia [29], so in these
cases use of the CL class may be justified. Figure 3
shows some of the cell types in PORO, together with the
tissue layers in which they are located.
Chemical entities and proteins
We reuse classes from the Chemical Entities ontology
CHEBI [32] for chemical structures of relevance  for
example, calcium carbonate (CHEBI_3311), biogenic
silica (CHEBI_64389) and aragonite (CHEBI_52239). In
some cases, these are connected from other parts of the
ontology via a composed from relation. For example,
calcareous spicule and calcium carbonate exoskeleton
are composed of calcium carbonate. Many sponge tax-
onomists use biochemical markers and lipid profiles as
descriptors, so we anticipate extending this part of the
ontology in the near future.
Qualities and traits
We include qualities and traits used by sponge taxono-
mists in the ontology. For example, under relationship
to substrate we have sessile (PORO_0000526), rooted
(PORO_000050) and endolithic (PORO_0000284). In
future versions of the ontology some of these terms will
be contributed back to the PATO phenotype and trait
ontology [33].
Applications and future directions
Studies of poriferan systematics have increased consider-
ably in recent times, with the number of researchers
studying sponges doubling in the past two decades [29].
The application of molecular approaches to sponge tax-
onomy has revolutionized and considerably improved
our understanding of the diversity and complexsg ox
archy (with many terms omitted for space reasons). The graph
. Inset are examples of spicules including, from left to right, a style (st),
Figure 3 A subset of PORO illustrating the three layers of structures comprising a whole organism and some of the cell types that
comprise these layers. The mesohyl is a gelatinous layer sandwiched between the external pinacoderm and internal choanocyte-lined surface
(choanoderm). We re-use the CARO class portion of tissue for these layers, although some debate exists about whether these features constitute
true tissues.
Thacker et al. Journal of Biomedical Semantics 2014, 5:39 Page 5 of 8
http://www.jbiomedsem.com/content/5/1/39evolutionary history of this group [34,35]. However, the
pace at which molecular systematics generates informa-
tion about the phylogenetic diversity of sponges has not
been matched by a corresponding acceleration of our
understanding of the morphological and functional di-
mensions of this biological diversity. The incorporation
of an ontological approach to organize and connect
structural, functional, genetic, and gene expression con-
cepts will allow us to improve this situation.
The phylum Porifera contains over 8,000 accepted spe-
cies [1], but at least 6,000 additional species are thought
to exist based on surveys of museum collections [36].
When integrating morphological and molecular datasets,
most studies of sponge systematics find high support for
morphological classifications at the species level, indicat-
ing the ability of morphological characters to distinguish
between sponge species [29,34,37-39]. However, there is
often a lack of resolution at the genus and family levels,
suggesting that morphological characters are often
homoplasic [29,37]. This low phylogenetic resolution
within orders is not surprising since there are relatively
few morphological characters available for analyses and
since these characters can be phenotypically plastic. Re-
cently, Morrow et al. [40,41] demonstrated that some
homoplasic characters may actually represent distinct
morphological traits that are described by a single term
(e.g., acanthostyle, Figure 2).
A major question in the development of multi-species
anatomy ontologies is whether ontological terms should
be designed with an assumption of the homology of ana-
tomical structures [20]. In constructing PORO, we took
an explicitly pragmatic approach and made noassumptions that these ontological terms refer to evolu-
tionarily homologous characters. Although there are
many known instances of homoplasy throughout sponge
systematics [37,40,41], our current goal is to reflect ana-
tomical terms as they have been used in recent and his-
torical literature. For example, the term actine refers to
the ray of a spicule [2]. For sponges bearing calcareous
spicules, actines do not contain an axial filament, while
for sponges bearing siliceous spicules, actines do contain
an axial filament [42,43]. Although it is clear that actine
is referring to a feature of two evolutionarily distinct
types of spicules, the concept of actine, that of a ray [2],
provides a practical term when describing this feature.
In future studies, we plan to use the PORO ontological
framework to describe homoplasic characters and hope
to provide a higher degree of resolution when describing
particular anatomical features. Greater precision in nam-
ing morphological features might allow sponge biologists
to create less ambiguity in character states, yielding less ho-
moplasy. While the question of how much homology to
build into an ontology is debated [44], it is common for
multi-species ontologies to include structures that are not ex-
plicitly determined to be homologous, and we have previ-
ously used this strategy for other anatomical ontologies [25].
A practical concern when identifying sponges is poor
specimen preservation. In some cases, critically import-
ant morphological features can be difficult to determine,
yielding difficulty in assigning taxonomic names using
existing, primarily bifurcating, identification keys. By in-
tegrating the morphological ontology with taxonomy, we
hope to enable the creation of polytomous identification
keys that can function with incomplete data sets. By
Thacker et al. Journal of Biomedical Semantics 2014, 5:39 Page 6 of 8
http://www.jbiomedsem.com/content/5/1/39using the Porifera ontology to annotate images of sponge
morphology, we will facilitate the proper identification
of anatomical features.
In the future, we plan to extend the core ontology to
include extinct taxa. For example, Archaeocyatha are
fossil sponges from the Cambrian era that lack spicules.
Their functional biology is only deduced from a theoretical
model [45], but they appear to share many characters with
modern Porifera. Kerner et al. [46] standardized descrip-
tions of the morphological characters of Archaeocyatha,
building a descriptive knowledge base of illustrated and
clearly defined terms. We are currently adding these char-
acters into the Porifera Ontology and explicitly connecting
anatomical terms between fossil and modern taxa. By link-
ing the skeletal elements of Archaeocyatha to those of
sponges, we hope to enhance our understanding of the
functional biology of Archaeocyatha as an analog of
sponges.
It is important to note that PORO is not a complete
database, knowledge base, or application in its own
right  however, PORO can form the terminological and
deductive knowledge backbone of such a system. PORO
is complementary to classification aids such as SPON-
GIA [26], which is a powerful expert system able to infer
the species of a sample based on the answers to ques-
tions concerning descriptive characters. Indeed, in con-
structing that system, the authors noted:
The simple work of character definition in the
domain model turned out to be a non-trivial task. A
thesaurus of terms for sponges that was to contain an
important consensus among the European experts in
sponge systematics was in the pipeline and its
preliminary versions were available to us. However,
the current vocabulary of our expert was not always
standardized according to the previous consensus [26].
PORO could be used directly as the domain model
used in systems such as SPONGIA and its successors.
One intriguing possibility is the encoding of the taxo-
nomic classification rules of SPONGIA directly in OWL,
allowing the use of modern Description Logic reasoners.
Given that OWL is expressive enough to encode classifi-
cation rules involving conjunction and disjunction of ei-
ther symbolic or quantitative characters, it may seem
that this would be easily achieved. However, one challenge
is that the MILORD II framework used by SPONGIA
makes use of many-valued logic, reasoning with uncer-
tainty and non-monotonic reasoning, all of which are out-
side the scope of OWL. One research possibility would be
to combine Bayesian and ontological reasoning, as has
been done in disease classification [47].
Finally, a major effort is underway by the Next-Generation
Phenomics research team [48] to automate the process ofderiving character matrices from published species de-
scriptions. As part of our future work, we will use the
ontology to annotate text and to standardize the names of
morphological characters across various research groups
over the past 200 years of sponge taxonomy. In addition,
the Phenomics team is seeking to automate character rec-
ognition using image processing software. It will be crucial
to annotate reference images with terms from the ontol-
ogy to calibrate this novel imaging system.
Conclusions
There are a number of ontologies covering taxa such as
plants, fungi and bilaterians, but the Porifera ontology is
the first ontology dedicated to a non-bilaterian meta-
zoan. Because many terms used in sponge taxonomy
and systematics have Porifera-specific meanings, we cre-
ated the structure of the Porifera ontology from existing
resources, primarily the Thesaurus of Sponge Morphology
[2]. We will revise and expand PORO to accommodate
new concepts and relationships as we use the ontology
to build character matrices for modern and fossil taxa.
By accelerating our ability to describe and understand
sponge morphology, we seek to reconcile differences be-
tween morphological and molecular approaches to pori-
feran systematics. We hope that this integrative approach
to taxonomy and systematics will inspire investigators
working with invertebrate and microbial taxa to add value
to their morphological datasets by placing the characters
used to describe additional taxonomic groups into an
ontological framework.
Methods
Bottom-up ontology development
When building an anatomy ontology, it is possible to
take a top-down approach or a bottom-up approach
(or some combination thereof ). With a top-down ap-
proach, the creator starts with upper-level categories
and gradually introduces more specific classes. With a
bottom-up approach, the creator starts with the terms of
interest (which are typically more specific) and builds
them into a hierarchy, gradually working-up to the root.
With PORO we took a bottom-up approach. We
started with the online version of the Thesaurus of
Sponge Morphology [2] and used a Perl script to generate
a skeleton ontology in OBO format. This was adjusted
using a text editor and OBO-Edit [49], and then trans-
lated into OWL and edited using the Protégé 4 ontology
editor [50] (http://protege.stanford.edu). The translation
retained the textual definitions obtained from the the-
saurus, as well as annotations on the definitions referen-
cing the source material. These are represented in the
OWL ontology as axiom annotations.
For Protégé editing, we make use of a number of plug-
ins, including one for annotating images (https://github.
Thacker et al. Journal of Biomedical Semantics 2014, 5:39 Page 7 of 8
http://www.jbiomedsem.com/content/5/1/39com/balhoff/image-depictions-view), OBO-Edit style pro-
ductivity assistance tools (https://github.com/balhoff/obo-
actions) and the OBO-Edit Graph View plugin for Protégé
(http://code.google.com/p/obographview/).
The editors of the ontology met through meetings or-
ganized by the Phenotype Research Coordination Net-
work (RCN), where the ontology was reviewed and
biologists were provided with training in ontology build-
ing and reasoning.
We make use of the HermiT reasoner as part of the
ontology development process [51]. Due to the use of
OWL features such as cardinality constraints, we cannot
use the faster reasoners that operate over restricted pro-
files of OWL, but due to the current relative small size of
the ontology, reasoning can be performed dynamically.
As well as using reasoning within Protégé, we run rea-
soner checks as part of an automated build process,
using the OBO Ontology Release Tool (http://code.google.
com/p/owltools/wiki/OortIntro) executed within a Con-
tinuous Integration server [52]. This server also checks for
common problems that can occur during Protégé editing,
such as duplicate labels, equivalent classes, or classes hav-
ing multiple text definitions.
Availability
PORO is always available in OWL from the OBO Library
permanent URL http://purl.obolibrary.org/obo/poro.owl.
The content of the ontology is available under a CC-BY
license (http://creativecommons.org/licenses/by/2.0). Fur-
ther details can be obtained from the project website
https://code.google.com/p/porifera-ontology/.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
CJM and RWT created the initial version of the ontology from the Thesaurus
of Sponge Morphology [2]. All authors contributed to the design and content
of the ontology. All authors contributed to the manuscript. All authors read
and approved the manuscript.
Acknowledgements
We acknowledge the support of the Phenotype Ontology Research
Coordination Network (NSF-DEB-0956049) for supporting ES and organizing
the meetings that brought the developers together. This work was also
supported by grants from the U.S. National Science Foundation, Division of
Environmental Biology [grant numbers 0829986 and 1208310 awarded to
RWT]. CJM was supported by the Director, Office of Science, Office of Basic
Energy Sciences, of the U.S. Department of Energy under Contract No.
DE-AC02-05CH11231. We thank the three anonymous reviewers for their
detailed comments on a previous version of this manuscript.
Author details
1Department of Biology, University of Alabama at Birmingham, Birmingham,
USA. 2Museo Margarita, Boca de Rio 6304, Venezuela. 3CR2P, UMR 7207
CNRS-MNHN-UPMC, Département Histoire de la Terre, Muséum National
dHistoire Naturelle, Bâtiment de Géologie, CP48, 57 rue Cuvier, 75005 Paris,
France. 4Department of Medical Informatics and Clinical Epidemiology,
Oregon Health & Science University, Portland, USA. 5Genomics Division,
Lawrence Berkeley National Laboratory, Berkeley, CA, USA.Received: 1 July 2013 Accepted: 22 July 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18
http://www.jbiomedsem.com/content/5/1/18
RESEARCH Open Access
Exploitation of semantic methods to cluster
pharmacovigilance terms
Marie Dupuch1,2,3*, Laëtitia Dupuch4, Thierry Hamon5,6 and Natalia Grabar1
Abstract
Pharmacovigilance is the activity related to the collection, analysis and prevention of adverse drug reactions (ADRs)
induced by drugs. This activity is usually performed within dedicated databases (national, European, international...), in
which the ADRs declared for patients are usually coded with a specific controlled terminology MedDRA (Medical
Dictionary for Drug Regulatory Activities). Traditionally, the detection of adverse drug reactions is performed with data
mining algorithms, while more recently the groupings of close ADR terms are also being exploited. The Standardized
MedDRA Queries (SMQs) have become a standard in pharmacovigilance. They are created manually by international
boards of experts with the objective to group together the MedDRA terms related to a given safety topic. Within the
MedDRA version 13, 84 SMQs exist, although several important safety topics are not yet covered. The objective of our
work is to propose an automatic method for assisting the creation of SMQs using the clustering of semantically close
MedDRA terms. The experimented method relies on semantic approaches: semantic distance and similarity
algorithms, terminology structuring methods and term clustering. The obtained results indicate that the proposed
unsupervised methods appear to be complementary for this task, they can generate subsets of the existing SMQs and
make this process systematic and less time consuming.
Introduction
The development of new drugs has allowed the treat-
ment of many diseases that were previously considered
incurable and with potential fatal outcomes for patients.
However, this major therapeutic advance is limited by the
toxicity of some drugs that may also be dangerous for
patients. Tominimize the risks associated with drug use, it
is necessary to detect as early as possible the adverse drug
reactions (ADRs) that may have been unnoticed during
clinical trials. This is the role of regulatory authorities and
of pharmacovigilance units within pharmaceutical labo-
ratories and hospitals. The main source of knowledge for
pharmacovigilance is based on the reporting of the ADRs
by health professionals and patients. These case reports
are recorded in pharmacovigilance databases. To facili-
tate the analysis of those data, ADRs are coded using a
controlled vocabulary, usually MedDRA (Medical Dictio-
nary for Drug Regulatory Activities) [1]. The detection of
*Correspondence: dupuchm@hotmail.fr
1CNRS UMR 8163 STL; Université Lille 1&3, F-59653 Villeneuve dAscq, France
2Centre de Recherche des Cordeliers, Université Pierre et Marie Curie - Paris6,
UMR_S 872, Paris F-75006, France
Full list of author information is available at the end of the article
new pharmacovigilance alerts, or signal detection, is typi-
cally based on a manual review of case reports by experts,
and more recently in some countries by data mining tech-
niques [2,3]. MedDRA is a fine-grained vocabulary with
over 80,000 terms and it has been shown that the grouping
of similar MedDRA terms (i.e., Hepatitis infectious, Hep-
atitis infectious mononucleosis, Hepatitis viral) is often
necessary in the process of the signal detection [4,5]. It
may allow indeed to detect the toxicity of a drug more
quickly.
TheMedDRA terms are structured into five hierarchical
levels (Table 1). From the highest to the lowest, these lev-
els are: System organ class (SOC), High level group term
(HLGT), High level term (HLT), Preferred term (PT), and
Low level term (LLT). The hierarchical organization of
the MedDRA terminology is clearly oriented on the divi-
sion by organ system, i.e. among the SOCs we can find for
instance Musculoskeletal and connective tissue disorders,
Hepatobiliary disorders, Psychiatric disorders andCardiac
disorders. In Table 1, we indicate also examples of terms
belonging to these five hierarchical levels. In the majority
of cases, hierarchical levels have the subsumption is-a
relations between them. For instance, in Table 1, the PT
© 2014 Dupuch et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited.
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 2 of 14
http://www.jbiomedsem.com/content/5/1/18
Table 1 Structure of MedDRA: five hierarchical levels of MedDRA, number of terms per level and some examples of
the terms
Level Expanded form Nb terms Examples
SOC System Organ Class 26 Musculoskeletal and connective tissue disorders
HLGT High Level Group Terms 332 Joint disorders
HLT High Level Terms 1,688 Arthropathies NEC
PT Prefered Terms 18,209 Arthritis
LLT Lowest Level Terms 66,587 Arthritis, Arthritis aggravated, Atrophic arthritis
Joint inflammation, Finger arthritis
Total 86,842
Arthritis is-a HLT Arthropathies NEC, while the HLT
Arthropathies NEC is-aHLGT Joint disorders. The situ-
ation is different when we consider the relations between
PTs and their LLTs [6]: these are no more subsumption
relations but identical or subsumption relations instead,
as the LLTs may be synonym or subordinate to their
PTs. Thus, in Table 1, the LLT Arthritis is identical to its
PT term Arthritis, although other LLTs such as Arthritis
aggravated, Atrophic arthritis, Joint inflammation, Finger
arthritis, Knee arthritis are subordinated to this PT.
A first method to group the MedDRA terms is based
on the hierarchical levels in MedDRA: HLT (High Level
Terms), HLGT (High Level Group Terms) or SOC (System
Organ Class) [7,8]. However, it was observed that some
safety topics are orthogonal to these hierarchical levels
(their terms may belong to different SOCs), which led to
the development of the Standardized MedDRA Queries
(SMQs) containing the MedDRA terms in connection
with a safety topic [9] and independently from their SOCs.
For example, the Haemorrhage SMQ contains MedDRA
terms related to bleeding in all parts of the body: it groups
terms from a large set of SOCs. The SMQs are devel-
oped internationally by experts looking manually in all
the MedDRA terms relevant to each SMQ. There are cur-
rently 84 SMQs that do not cover the entire drug-induced
set of safety topics (Haemorrhage, Hepatic disorders, Sys-
temic lupus erythematosus, Convulsions...). This situation
leads us to propose methods for automating the cluster-
ing of the terms when MedDRA provides no grouping
category appropriate for a given safety topic. The lists of
MedDRA terms may then be presented for the selection
to the experts.
Other work on the automatic clustering of pharma-
covigilance terms relies on a specific resource ontoEIM.
ontoEIM stands for ontology and Événements Iatrogènes
Médicamenteux (Adverse Drug Effects in French) [10].
This resource is created through the projection of Med-
DRA on the terminology SNOMED CT (Systematized
Nomenclature of Medicine - Clinical Terms) [11]. The
projection is performed on the basis of the UMLS (Uni-
fied Medical Language System) [12], in which several
terminologies are already merged and aligned, includ-
ing MedDRA and SNOMED CT. The ontoEIM resource
has been exploited to build groupings through the hier-
archical subsumption [10,13]. Precision observed is high
while the recall is extremely low, which is due to the fact
that the SMQs contain terms from different SOCs. In
other experiments, the ontoEIM resource has been used
in combination with the semantic distance algorithms and
applied to a subset of the MedDRA terms [14]. The same
approach has been applied to a subset of WHO-ART
(WHO Adverse Reaction Terminology) terms [15]. In the
WHO-ART related experiment, the obtained groupings
demonstrated interesting results because several types
of semantic relations were detected between the terms
(synonyms, antonyms, physiological functions or abnor-
malities, associated symptoms, abnormal laboratory tests,
pathologies and their causes, close anatomical localiza-
tion, degrees of severity, and heterogeneous groupings),
although these groupings were not compared with the
SMQs.
Objectives
We address the problem of grouping the MedDRA phar-
macovigilance terms in a way that reflects coherent and
medically sound safety topics. Although the MedDRA
vocabulary is structured according to specific organ-based
semantic characteristics of the terms, this organization
does not fully capture important semantic relationships
among terms.We aim to explore how to group these terms
in a way that directly reflects the intuitions captured in
manually created SMQs. More precisely, our objective is
to work on semantic methods for the automatic creation
of groupings of the MedDRA terms. We propose to adapt
and to combine two strategies: semantic distance and sim-
ilarity algorithms, and terminology structuring methods.
Special attention is paid to the merging and comparison
between these two methods and evaluation of the gener-
ated term clusters. In order to measure the ability of our
methods to produce clusters similar to the existing SMQs,
we evaluate the generated clusters by taking these existing
SMQs as a gold standard. Our method relies on two main
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 3 of 14
http://www.jbiomedsem.com/content/5/1/18
assumptions: (1) the MedDRA terms can be used for the
automatic creation of groupings of terms; (2) the combi-
nation of the semantic methods provide complementary
results.
Methods
The method is organized in four main steps (Figure 1): (1)
computing the semantic distance and similarity between
the MedDRA terms using ontoEIM, (2) computing the
semantic relations from a flat list of the MedDRA
terms with the terminology structuring methods, (3)
clustering the MedDRA terms, (4) and evaluating the
obtained clusters against the SMQs. For the implementa-
tion, we exploit Perl and R (http://www.r-project.org) lan-
guages, and several Natural Language Processing (NLP)
tools.
Data
Wemake use of several types of material.
MedDRA terms
The MedDRA PT terms (n = 18,209) are exploited either
as a flat list of terms, in which case the semantic relations
between them are computed with terminology structuring
methods, or through the ontoEIM resource [10], in which
case the semantic relatedness between them is computed
with semantic distance and similarity algorithms. We
work with the PT terms because they are used for build-
ing the SMQs and for coding the pharmacovigilance case
reports. The ontoEIM resource attempts to improve the
MedDRA structuring in two ways: the structure of Med-
DRA terms becomes similar to the structure in SNOMED
CT which makes it more fine-grained (the hierarchy is
modified and enriched, and contains up to 14 hierarchi-
cal levels); and the MedDRA terms receive formal def-
initions (decomposition into their semantic primitives).
Thus, in Table 2, the MedDRA ADR terms Abdominal
abscess and Pharyngeal abscess are defined on two axes
(Disorders and Body structure). For instance, Pharyngeal
abscess is semantically decomposed into the Disorder ele-
ment Abscess morphology and Body structure element
Neck structure. The names of the formal definition ele-
ments correspond to the names of the hierarchies of the
SNOMED CT. Within the ontoEIM, we have three hier-
archical trees (Figure 2): one for the MedDRA terms and
one for each axis of the formal definitions. The ontoEIM
resource is used with the semantic similarity and distance
algorithms.
Lexical resources
Three kinds of lexical resources are involved in the
methods: (1) synonyms extracted from the UMLS (n =
228,542); (2) synonyms acquired from three biomedical
terminologies thanks to their compositionality [16] (n =
28,691); (3) synonyms from WordNet [17] (n = 45,782).
Lexical resources provide pairs of synonyms such as
{accord, concordance}, {pain, ache}, {aceperone, acetabu-
tone}, {adenazole tocladesine} or {bleeding, haemorrhage}.
ADR terms Body structure Disorders
Abscess morphology
Abdominal cavity structure
Abdominal abscess
Pharyngeal abscess
Neck structure
(Rada et al, 1989) (Zhong et al, 2002)
Chodorow, 1998)
(Leacock & 
fo
rm
al
 d
ef
in
iti
on
s +
?
Flat list of
the MedDRA
terms
Radius
HAC
POS?tagging
Syntactic analysis
Genia tagger
Ogmios platform
YaTeA
Detection of
hierarchical relations
Lexical inclusion
Faster
Detection of
synonymy relations
Synoterm
FasterLexical resources Strongly connected components
Connected components
clusters
Merging the
SMQs
Baseline
Experts
ontoEIM resource
Clustering the MedDRA termsComputing the semantic distance and similarity
Pre?processing Term structuring
Evaluation
Terminology structuring methods
Clustering within directed graphs
Semantic distance and similarity methods
Figure 1 General schema of the method. Figure 1 presents the general schema of the proposed methods. The methods consist into four main
steps: application of the semantic similarity and distance methods, application of the terminology structuringmethods, clustering of the
semantically similar terms and evaluation of the obtained results.
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 4 of 14
http://www.jbiomedsem.com/content/5/1/18
Table 2 Example of a formal definition for the MedDRA
terms Abdominal abscess and Pharyngeal abscess
MedDRA terms Disorders Body structure
Type of abnormality Anatomical localization
Abdominal abscess Abscess morphology Abdominal cavity structure
Pharyngeal abscess Abscess morphology Neck structure
The terms are semantically decomposed into their elements from Disorders and
Body structure axes.
These resources are used with the terminology structur-
ing methods.
StandardizedMedDRAQueries (SMQs)
We use the 84 SMQs (2010 version) as the gold stan-
dard for the evaluation of the generated clusters of terms.
The SMQs contain MedDRA terms relevant to a given
safety topic. These terms usually belong to different SOCs.
For instance, the Angioedema SMQ contains terms from
the Immune system disorders SOC (Systemic allergic reac-
tion, Allergic oedema, Sulfonamide allergy, Type I hyper-
sensitivity), Skin and subcutaneous tissue disorders SOC
(Angioedema, Cholinergic urticaria, Urticaria idiopathic,
Acute angio oedema), Eye disorders SOC (Chemosis, Con-
junctival oedema, Edema eyelid), etc. The size of the
SMQs goes from 47 terms (Scleral disorders SMQ) up
to 8,036 terms (Malignancies SMQ). The SMQs can be
composed of a flat list of terms or can be hierarchically
structured.
Experimental approach
Computing the semantic distance and similarity
Several semantic distance and similarity algorithms can
be applied within structured terminological resources
[18-21]. In our work, we also rely on this type of approach.
In this case, the algorithms count the number of edges
(links) between the two terms in order to compute the
relatedness of these terms. The simplest algorithm [18]
counts the edges between terms and aims to find the
shortest path between them. Thus, on the Figure 2, we
show an excerpt from a terminological graph. When we
compute the shortest path between the nodes Pharyngeal
abscess and Abdominal abscess, we follow the path within
the ADR hierarchy and obtain the shortest path equal to
four edges. In addition to the path length, other criteria
may be taken into account: hierarchical depth of terms
[22,23], information content [24], the nearest common
parent [25], etc. Besides the computing of the semantic
closeness between two terms or words, these algorithms
have been used in different contexts such as word-sense
disambiguation [22], information retrieval [23,26], gene
annotation [27], and terminology enrichment and adap-
tation [28,29]. A review of the semantic measure and
similarity algorithms common within the biomedical area
has appeared [30].
In our work, we separately exploit three algorithms to
compute the semantic distance and similarity between
two terms t1 and t2: (1) the Rada semantic distance
[18] relies on the computing of the shortest path sp; (2)
the LCH Leacock and Chodorow semantic similarity [20]
relies on the shortest path sp and on the maximal depth
found within the terminology; (3) the Zhong semantic
distance [23] relies on the absolute depth of terms and
on their closest common parent. Semantic distance and
similarity are computed between the MedDRA terms but
also between the elements of their formal definitions (D
and B) to make the semantic representation of the terms
more fine-grained. To illustrate, lets consider Abdominal
abscess and Pharyngeal abscess terms from Figure 2. The
weight of edges is set to 1 because all relations are of the
same kind (hierarchical subsumption), and the value of
Figure 2 Computing the shortest paths between two terms. Figure 2 presents the principle for the computing of the shortest paths sp
between two MedDRA terms (Abdominal abscess and Pharyngeal abscess) and between the elements of their formal definitions (axis Disorders and
Body structure). Blue nodes are inherited from MedDRA, red nodes from SNOMED CT.
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 5 of 14
http://www.jbiomedsem.com/content/5/1/18
each shortest path corresponds to the sum of weights of
all its edges. For this pair of terms we obtain the following
shortest paths sp: spADR = 4, spB = 10 and spD = 0. The
unique semantic distance between theMedDRA terms for
each semantic distance measure is computed as follows:
?x ? {Rada, LCH ,Zhong},
?
i?{ADR,D,B}
Wi ? sdx(t1i, t2i)
?
j?{ADR,D,B}
Wj
,
where {ADR,D,B} respectively correspond to the Med-
DRA ADR terms, and the axes Disorders D and Body
structure B; t1 and t2 are two MedDRA ADR terms;Wi is
the coefficient associated with each of the three axes (the
value is set to 1 for B and ADR and to 2 forD to reflect the
importance of the latter [31]); and sd(t1, t2) is the seman-
tic distance between t1 and t2, computed on a given axis
with one of the three semantic distance measures {Rada,
LCH, Zhong}. For the example above, the unique semantic
distance is 3.5. According to the tested parameters (three
semantic distance measures and MedDRA terms with or
without their formal definitions), we build six symmetric
matrices with the MedDRA terms from ontoEIM.
Term structuringmethods
The terminology structuring provides methods for the
detection of semantic relations between terms. Two
strategies may be distinguished: those which rely on the
internal analysis of the terms and those which rely on the
contexts within which the terms occur. Because we are
working on the terms out of their context, we exploit the
terminology structuring methods which rely on the inter-
nal analysis of the terms. These methods are applied to a
flat list of 18,209 MedDRA PTs. They lead to the detec-
tion of hierarchical subsumption and synonymy relations
between these terms. The terms are pre-processed: the
POS-tagging is done with Genia tagger [32] and the syn-
tactic analysis with the YATEA shallow parser [33]. Three
methods are then applied for the acquisition of seman-
tic relations: lexical inclusions, morpho-syntactic variants
and compositionality.
Lexical inclusion and hierarchy The basic statement on
lexical inclusion hypothesis [34] states that when a given
term is lexically included in another term there is a seman-
tic subsumption between them. This hypothesis is well
verified in the biomedical area [35,36].
We distinguish three steps within this approach:
 the terms are syntactically analyzed into head and
expansion components. For instance, on Figure 3, the
syntactic analysis of the term muscle pain results in
two components: head component pain and
expansion component muscle;
 the syntactic and semantic relation is then
established between a given term and its head
head
component component
expansion
pain muscle
Figure 3 Syntactic analysis of terms for the induction of
hierarchical relations. Figure 3 presents the syntactic analysis of the
termmuscle pain, its decomposition into head and expansion
components, which is then used for the induction of hierarchical
relations between this term and its head component.
component. For instance, the term on Figure 3
provides the relation between muscle pain (the whole
term) and pain (the head component of the term).
With these specifications, the identified relations are
hierarchical: the long term muscle pain is the
hierarchical child of the short term pain. Indeed,
muscle pain conveys a more specific information;
 parent and child terms have to be MedDRA terms,
otherwise the identified relations are removed.
With the applied specifications of this approach, the
identified relations are induced from lexical and syntac-
tic information conveyed by the analyzed terms. Besides,
these specifications guarantee that the identified relations
correspond to the hierarchical subsumption. In fact, we
do not allow the induction of other kinds of relations.
For instance, if relations between the whole terms and
their expansion components were allowed, the identi-
fied relations would be associative, such as localization
for example from Figure 3: muscle pain is localized in
muscle.
Morpho-syntactic variants Weworkwith Faster [37] for
the identification of morpho-syntactic variants between
the PT terms. This tool uses several transformation
rules, such as insertion (cardiac disease/cardiac valve dis-
ease), morphological derivation (artery restenosis/arterial
restenosis) or permutation (aorta coarctation/coarctation
of the aorta). Each transformation rule is associated with
hierarchical or synonymy relations: the insertion intro-
duces a hierarchical relation (cardiac valve disease is
more specific than cardiac disease), while the permuta-
tion introduces a synonymy relation. When several trans-
formations are involved, such as in gland abscess and
abscess of salivary gland (combination of permutation
(synonymy) and insertion (hierarchy) rules), the hierarchi-
cal relation prevails.
Compositionality and synonymy The synonymy rela-
tions are acquired in two ways:
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 6 of 14
http://www.jbiomedsem.com/content/5/1/18
1. The synonymy relation is established between two
simple MedDRA terms if this relation is provided by
the lexical resources.
2. The identification of synonym relations between
complex terms relies on the semantic
compositionality [38]. Compositionality appears to
be a common characteristics of the biomedical terms
[16,39,40].
In our work, we consider that two complex terms are
synonyms if one of their components at the same
syntactic position are synonyms and the other
components are identical or also synonyms. For
instance, given the synonymy relation between pain
and ache provided by the lexical resources, the terms
muscle pain and muscle ache are also identified as
synonyms [41] (Figure 4).
Three transformation rules are applied: on the head
component like in the given example, on the
expansion component, and on head and expansion
components.
We perform several experiments: each medical syn-
onymy resource is used individually and then combined
withWordNet.
Clustering of terms
During the clustering step, it is important to distinguish
between disjoint and non disjoint clusters: with disjoint
clusters a given term may belong to at most one cluster,
while with non disjoint clusters there is at least one term
that belongs to more than one cluster. We give advan-
tage to the non disjoint clusters because they suit better
the specificity of our objectives: one MedDRA term may
belong to several SMQs.
For clustering the terms on the basis of their semantic
distance and similarity, we use two clustering methods:
hierarchical ascendant classification HAC and Radius R
method. With HAC, the method first chooses the best
centers for clusters and then builds the hierarchy of terms
by progressively merging the smaller clusters into bigger
ones to finally obtain one unique cluster. The obtained
dendrogram is then segmented into k disjoint clusters.
With the R radius approach, every MedDRA term is con-
sidered as a possible center of a cluster and its closest
terms are clustered together with it. This method gener-
ates non disjoint clusters.
For clustering of terms with the computed hierarchical
and synonymy relations, the relations are considered as
directed graphs: the terms are the nodes of the graph while
the hierarchical relations are the directed edges. We par-
tition these directed graphs in a way that each directed
sub-graph correspond to a set of vertices such as at leat
one vertix can reach the others by a directed path. Hence,
the generated components are non disjoint clusters. To
improve the coverage of these clusters, we add the syn-
onyms: if a term has a synonymy relation with the term
from a cluster then this term is also included in this clus-
ter. The initial graph is then augmented with two edges
going from and to the synonyms.
Finally, we perform two more steps to deduplicate and
merge the clusters:
 Separately for each method (semantic similarity and
terminology structuring), we compute whether
smaller clusters are included into larger clusters and
we merge those clusters which have at least 80%
overlap between them.
 Between the clusters computed by the two methods
(semantic similarity and terminology structuring),
two clusters provided by these methods and which
have at least 80% overlap between them are also
merged together.
Evaluation of the generated results
We first evaluate the correctness of the generated seman-
tic relations, which is done manually by a computer
scientist.
We then perform quantitative and qualitative evalua-
tion of the generated clusters. The quantitative evaluation
is performed thanks to their comparison with the SMQs.
A cluster is associated to the SMQ with which it has
the maximal F-measure. For the setting of the thresholds
of the semantic distance and similarity algorithms and
their evaluation, we perform a ten-fold cross-validation:
the data are partitioned into ten subsets, one subset is
used for the setting up the methods while the remain-
ing nine subsets are used for the evaluation. This process
is done ten times with a different training subset each
head
component component
expansion head
component component
expansion
pain muscle ache muscle
Figure 4 Syntactic analysis of terms for the induction of synonymy relations. Figure 4 presents the syntactic analysis of the termsmuscle pain
andmuscle ache, their decomposition into head and expansion components, which is then used for the induction of synonymy relations between
these two terms.
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 7 of 14
http://www.jbiomedsem.com/content/5/1/18
time. Three classical measures are then computed: preci-
sion P (percentage of the relevant terms clustered divided
by the total number of the clustered terms), recall R
(percentage of the relevant terms clustered divided by
the number of terms in the corresponding SMQ) and F-
measure F (the harmonic mean of P and R). The final
evaluation values are computed with the thresholds which
provide the best results the most frequently during the
cross-validation step. We evaluate the clusters from each
method separately and after their merging. A qualitative
evaluation is done by a medical expert: we perform a
failure analysis of our methods. As for the baseline, we
chose the most frequently used approach for the group-
ing of theMedDRA terms, which relies on the hierarchical
structure of MedDRA: the exploitation of the hierarchical
subsumption of the PTs through the HLT MedDRA level
[7,8,42].
Results
The 7,629 MedDRA terms from ontoEIM have been pro-
cessed through the three semantic distance and similarity
algorithms. An excerpt from the generated matrices is
presented in Table 3: for instance, the distance between
Gastric ulcer and Gastrointestinal ulcer is 1, while the
distance between Gastric ulcer and Biopsy tongue is 10,
which reflects the semantics of the terms from these two
pairs (the first pair of terms is semantically closer than the
second pair). The flat list of 18,209 MedDRA terms has
been processedwith the terminology structuring methods
for the detection of hierarchical and synonymy relations.
The results for the terminology structuring methods are
presented in Table 4. We can observe that the num-
ber of the acquired hierarchical relations reaches up to
4,000. The number of the acquired synonyms is lower
(nearly 2,000), while the impact of the WordNet resource
is very low (37 and 60 relations). The percentage of the
MedDRA PT terms involved in the generated hierar-
chical relations is 32%. It reaches up to 40% when the
synonymy is also considered. With semantic distance,
all the terms from ontoEIM, 51% of the MedDRA PTs,
are used.
Table 5 indicates the number of clusters and their
size according to the strategies and methods (semantic
distance, terminology structuring and merging of the
results provided by these two methods). This table shows
that semantic distance method provides the majority of
the clusters, and that number of clusters and their size
increase with the merging of the methods (semantic
distance and terminology structuring). With the cross-
validation, we tested several parameters and determined
the best thresholds: with the Radius clustering 4 for Rada,
4.10 for LCH and 0.02 for Zhong; with theHAC clustering
300 classes. With these thresholds, the number of clusters
and their size become larger. We apply these best thresh-
olds to generate the final set of clusters to be evaluated
and analyzed by the expert. The impact of the best thresh-
olds on the clusters varies across the SMQs, but the global
average results are improved. For the terminology struc-
turing methods the best results are obtained with lexical
inclusions, morpho-syntactic variants and synonyms.
The generated semantic relations and clusters have been
evaluated via a comparison with the existing SMQs, with
the baseline, and through an analysis provided by a medi-
cal expert and a computer scientist. The key observations
are that the proposed methods outperform the baseline
and that the merging of the methods improves the results.
We have also observed several limitations of the meth-
ods and results. We discuss the performed analysis and
evaluations in the following section.
Discussion
Limitations of the ontoEIM resource
The ontoEIM resource is unique of its kind, but currently
it suffers from incompleteness: only 51% of the Med-
DRA PTs are aligned with the SNOMED CT terms. The
main reason for this is that ontoEIM integrates the align-
ments between these two terminologies which are already
proposed by the UMLS. The integration of additional
alignments [43,44] is planned but requires an important
expertise of pharmacovigilance experts. Moreover, the
recent development of this resource [45] is oriented to
the maintenance of the MedDRA hierarchical structure
and on some of the existing SMQs. These two points (use
of the MedDRA hierarchical structure and description of
some existing SMQs) are not suitable for the methods we
designed.
Table 3 Semantic distancematrix
Gastric ulcer Venooclusive liver Reflux gastritis Biopsy tongue Gastrointestinal
disease ulcer
Gastric ulcer 0 5 3 10 1
Venooclusive liver disease 5 0 7 11 6
Reflux gastritis 3 7 0 12 5
Biopsy tongue 10 11 12 0 11
Gastrointestinal ulcer 1 6 5 11 0
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 8 of 14
http://www.jbiomedsem.com/content/5/1/18
Table 4 Acquisition of semantic relations (hierarchical subsumption and synonymy) between theMedDRA terms
Relationships Methods Number of relations
Hierarchical relations Lexical inclusion 3,366
Hierarchical relations Morpho-syntactic variation with Faster 743
Synonymy relations Compositionality with 3 biomed terminologies 1,879
Synonymy relations Compositionality with 3 biomed terminologies and WordNet 1,939
Synonymy relations Compositionality with the simple UMLS synonyms 190
Synonymy relations Compositionality with the simple UMLS synonyms and WordNet 227
Synonymy relations Morpho-syntactic variation with Faster 100
Correctness of the semantic relations
A manual analysis of the generated hierarchical relations,
done by a computer scientist, indicates that these relations
are usually correct: the syntactic constraints guarantee
correct propositions. Nevertheless, we observed a small
number of syntactic ambiguities. They appear within 144
pairs (5%) with maximal syntactic heads and correspond
to pairs like: {anticonvulsant drug level, drug level}, {blood
smear test, smear test}, {eye movement disorder, move-
ment disorder}. Thus, within the first pair, there is an
ambiguity on drug as two dependencies seem possible:
{anticonvulsant drug level, drug level} as proposed with
the maximal syntactic head analysis or {anticonvulsant
drug level, level} (analysis provided with the minimal syn-
tactic head). In our work, we give preference to the syntac-
tic analysis with maximal syntactic heads. But whatever
the performed syntactic analysis, the semantic relations
remain correct. Nevertheless, we will see that, although
the generated semantic relations are deemed correct, the
relevance of these relations and of the terms they link is
not always perfect to the building of the SMQs. Indeed,
some of the terms are seen to be relevant to the SMQs
while others do not, which may be due to the difference
existing between the linguistically observable semantics of
the relations and their domain or medical validity.
Quantitative evaluation of the generated clusters through
their comparison with the SMQs.
In Table 6, we indicate the average values of Precision,
Recall and F-measure obtained with eachmethod individ-
ually (semantic distance and terminology structuring) and
Table 5 Clustering of terms: number of clusters and
their size
Strategy Clusters of terms
#clusters Interval Mean
Semantic distance 7,564 [2; 1,354] 132.67
Terminology structuring 748 [1; 119] 3.82
(hierarchical+synonymy)
Merging (semantic distance + 7,684 [1; 1,354] 130.75
hierarchical + synonymy)
whenmerged. The average precision is usually higher than
45%, although the recall is lower especially with the ter-
minology structuring methods. This is due to the fact that
the clusters generated with our methods are smaller than
the SMQs and show their different aspects. In this table,
we can also see that the merging of the methods allows to
improve the average performance of the generated clus-
ters (Recall and F-measure), although we lose one percent
in Precision.
In Figures 5, 6 and 7, we indicate the evaluation results
obtained against all the 84 SMQs for the three evaluation
measures: Precision (Figure 5), Recall (Figure 6) and F-
measure (Figure 7). Each figure shows the performance of
the tested methods (terminology structuring and seman-
tic distance). The x axis represents the 84 reference SMQs,
the y axis the evaluation results. On the whole, we can
observe that precision is usually higher than recall, and
that there is an important variation across the SMQs.
In our previous experiments [46], we gave advantage to
precision, while in the current experiment F-measure is
advantaged, which improves the global results by eight
points. When we look closer at these Figures, we can
observe for example that, in Figure 5, the terminology
structuring method has the highest precision (with over
30% of the clusters showing 100% precision), while the
semantic distance method shows the lowest precision.
The situation is different with recall in Figure 6: the ter-
minology structuring method has the lowest values, while
the semantic distance method has the highest values. In
Figure 7, we can see that the merging of the methods very
often outperforms the semantic distance methods. These
figures also point out that there is a great variability across
the SMQs, while currently we use the same setting of the
methods independently from the SMQs.
Table 6 Evaluation results against the gold standard
(84 SMQs): average values
Methods Precision Recall F-measure
Semantic distance 45.2 32.4 36.9
Terminology structuring 68.4 12.2 18.7
Merging 44.2 36.5 40.0
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 9 of 14
http://www.jbiomedsem.com/content/5/1/18
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60  70  80
Pr
ec
is
io
n
SMQs
sem distance
merging
term structuring
Figure 5 Evaluation of the clusters generated with the proposed
methods (84 SMQs): Precision. Figure 5 presents the evaluation
Precision values obtained further to the evaluation of the clusters
generated with the proposed methods (semantic distance,
terminology structuring and merging) against the gold standard data
(84 SMQs).
We also performed an additional analysis of the clusters
generated with the terminology structuring which shows
the following contribution of the generated semantic rela-
tions:
1. the hierarchical relations form the basis of the
clusters: they correspond to 96% of the involved
terms and show 69% precision. Only three clusters
do not contain hierarchical relations;
2. the Faster relations are involved in 50% of clusters
and show a precision between 75 and 85%;
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60  70  80
R
ec
al
l
SMQs
sem distance
term structuring
merging
Figure 6 Evaluation of the clusters generated with the proposed
methods (84 SMQs): Recall. Figure 6 presents the evaluation Recall
values obtained further to the evaluation of the clusters generated
with the proposed methods (semantic distance, terminology
structuring and merging) against the gold standard data (84 SMQs).
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60  70  80
F?
m
ea
su
re
SMQs
sem distance
term structuring
merging
Figure 7 Evaluation of the clusters generated with the proposed
methods (84 SMQs): F-measure. Figure 7 presents the evaluation
F-measure values obtained further to the evaluation of the clusters
generated with the proposed methods (semantic distance,
terminology structuring and merging) against the gold standard data
(84 SMQs).
3. one third of the clusters contains synonymy relations,
and their precision varies between 55 and 69%;
4. the relations acquired with the UMLS resources are
involved in 14% of clusters while their precision is
only 38%;
5. the WordNet-based relations involve only six terms
(such as those involved in the relations {heart
syndrome, nerve degeneration} and {heart injury,
nerve damage}). The whole impact of the WordNet
synonyms is almost null. Moreover, the involved
terms are either proposed by other more
contributory methods or do not correspond to
correct propositions. The most interesting (and
correct) relation is {intestinal gangrene,
gastrointestinal necrosis}. It is unique to the
WordNet resource output.
On the whole, observations of the impact of the meth-
ods and resources on their contribution correspond to the
expected results but provide also with surprises. Thus, the
highest precision is observed with the morpho-syntactic
Faster relations: these are based upon the morphological
variations of the terms and usually conveyminor semantic
modifications ({abdomen, abdominal}, {infect, infection}).
The synonymy relations may involve greater semantic
variations (such as in {sepsis, infection} or {abdominal,
intestinal}) and this explains their less impressive but still
acceptable precision. Moreover, the synonym terms may
have a contextual semantic value [47], i.e. be valid in
some but not in all the contexts. As a matter of fact, the
pairs {sepsis, infection} and {abdominal, intestinal} have
been acquired from terms considered as synonyms in the
existing terminologies. However, these synonyms are not
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 10 of 14
http://www.jbiomedsem.com/content/5/1/18
deemed to be correct for the creation of the SMQs. Finally,
the hierarchical relations convey yet greater semantic vari-
ation (the hierarchical child terms are semantically more
specific than the parent terms), although their precision
is higher than the precision of the synonym relations.
Moreover, the generated hierarchical relations participate
very actively in the creation of the clusters of terms. As
we previously observed, the generated hierarchical rela-
tions bring the majority of terms in the clusters. This
is a surprising observation: we did not expect to receive
such a great contribution from the hierarchical relations.
Another surprising observation is related to the poor con-
tribution of the synonymy relations from WordNet and
those extracted directly from the UMLS: both their cover-
age and precision are weak and they are weakly involved
in the creation of the clusters.
Comparison with the baseline
Our baseline is the most common method utilized for the
grouping of the PT terms within MedDRA: their hier-
archical subsumption through the HLT terms. Among
the 1,688 HLTs and 84 SMQs, 46 of them have either
direct (Thrombocytopenias (SMQ) and Thrombocytope-
nia (HLT)) or non ambiguous correspondences (Renal
failure and impairment (SMQ) and Acute renal failure
(HLT)). We use these 46 SMQs as gold standard with the
baseline hierarchical subsumption and with our methods.
These 46 SMQs are a subset of the whole set of the 84
SMQs. Similarly to the results presented in the previous
paragraph, the average performance on the baseline set
is indicated in Table 7, while the detailed performance
per evaluation measure is indicated on Figures 8 (Preci-
sion), 9 (Recall) and 10 (F-measure). We can observe that
on average, the baseline approach can be compared with
the terminology structuring method, although the base-
line performance is lesser. The comparison with other
experiments points out that precision is higher with the
baseline (although very close with the one provided by
the semantic distance), while recall and F-measure are
notably improved with other methods. The figures also
show that the proposedmethods outperform the baseline.
These are positive observations which clearly indicate that
the proposed methods contribute to the state of the art.
Table 7 Evaluation results for the baseline and the
proposedmethods (46 SMQs): average values
Methods Precision Recall F-measure
Baseline 60.3 9.2 14.9
Semantic distance 46.0 33.9 34.1
Terminology structuring 71.1 11.8 18.9
Merging 41.0 45.6 37.3
Figure 8 Comparison of the generated clusters with the baseline
(46 SMQs): Precision. Figure 8 presents the precision values
obtained further to the comparison of the baseline with the proposed
methods: exploitation of the MedDRA hierarchical structure and of
the hierarchical subsumption of the PT terms through their HLT
terms. We consider the 46 SMQs which have equivalent HLT terms.
Qualitative evaluation with an expert
In Table 8, we indicate examples of seven clusters:
Angioedema, Embolic and thrombotic events, arterial,
Haemodynamic oedema, effusions and fluid overload,
Periorbital and eyelid disorders, Peripheral neuropathy,
Haemolytic disorders andAgranulocytosis. This table indi-
cates the number of terms in the SMQs and in the
corresponding clusters (clu), as well as the number of
common terms between them (com) and the performance
(Precision P, Recall R and F-measure F) when computed
against the reference data Reference and also after the
 0
 20
 40
 60
 80
 100
 0  5  10  15  20  25  30  35  40  45
R
ec
al
l
SMQs
sem distance
term structuring
merging
baseline
Figure 9 Comparison of the generated clusters with the baseline
(46 SMQs): Recall. Figure 9 presents the recall values obtained
further to the comparison of the baseline with the proposed
methods: exploitation of the MedDRA hierarchical structure and of
the hierarchical subsumption of the PT terms through their HLT
terms. We consider the 46 SMQs which have equivalent HLT terms.
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 11 of 14
http://www.jbiomedsem.com/content/5/1/18
 0
 20
 40
 60
 80
 100
 0  5  10  15  20  25  30  35  40  45
F?
m
ea
su
re
SMQs
sem distance
term structuring
merging
baseline
Figure 10 Comparison of the generated clusters with the
baseline (46 SMQs): F-measure. Figure 10 presents the F-measure
values obtained further to the comparison of the baseline with the
proposed methods: exploitation of the MedDRA hierarchical structure
and of the hierarchical subsumption of the PT terms through their HLT
terms. We consider the 46 SMQs which have equivalent HLT terms.
manual analysis performed by the expert (Manual). The
results obtained with different strategies are indicated: the
semantic distance sd, the terminology structuring struc,
as well as the merging merg of semantic distance and
terminology structuring. We can observe that the perfor-
mance of the methods varies a lot across the presented
SMQs and clusters. Usually, the terminology structur-
ing provides a higher precision and lower recall than
the semantic distance measures. The semantic distance
and merging approaches generate bigger clusters: they
lead to increased recall but they decrease the precision.
Usually, the F-measure takes advantage and is improved.
The manual evaluation by the expert accepts additional
terms, which allows to have a more complete picture of
the performance of the proposed methods. This expert
evaluation leads also to increased precision, recall and
F-measure.
We performed a detailed qualitative analysis of seven
SMQs and clusters with the medical expert.
For instance, the SMQ Angioedema contains 52 terms
which mean signs and symptoms of angioedema. The
semantic distance algorithm provides a cluster with 56
terms, among which 36 do not belong to this SMQ. Three
Table 8 Evaluation results against the gold standard and further to themanual analysis of the expert
SMQs Number of terms Reference Manual
SMQ clu com P R F P R F
Angioedemasd 52 56 20 36 38 37 41 44 42
Angioedemastruc 52 31 19 61 36 45 61 36 45
Angioedemamerg 52 41 21 51 40 45 71 48 57
Embolic and thrombotic events...sd 132 140 48 34 36 35 39 41 40
Embolic and thrombotic events...struc 132 13 12 92 9 16 92 9 16
Embolic and thrombotic events...merg 132 140 48 34 36 35 47 46 46
Haemodynamic oedema, effusions...sd 36 56 13 23 36 28 38 58 50
Haemodynamic oedema, effusions...struc 36 31 13 42 36 39 84 72 78
Haemodynamic oedema, effusions...merg 36 41 15 37 42 39 81 92 86
Periorbital and eyelid disorderssd 39 44 22 50 56 53 52 59 55
Periorbital and eyelid disordersstruct 39 4 4 100 10 18 100 10 18
Periorbital and eyelid disordersmerg 39 45 22 48 56 52 78 46 58
Peripheral neuropathysd 31 58 16 27 51 36 45 84 59
Peripheral neuropathystruct 31 2 2 100 6 12 100 6 12
Peripheral neuropathymerg 31 58 16 28 52 36 60 80 69
Haemolytic disorderssd 26 27 12 44 46 45 66 69 67
Haemolytic disordersstruct 26 3 3 100 11 20 100 11 20
Haemolytic disordersmerg 26 27 12 44 46 45 78 81 79
Agranulocytosissd 29 25 7 28 24 26 32 27 29
Agranulocytosisstruc 29 13 9 69 31 42 77 34 47
Agranulocytosismerg 29 11 9 81 31 45 77 34 47
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 12 of 14
http://www.jbiomedsem.com/content/5/1/18
of them (Injection site urticaria, Cervix oedema and Injec-
tion site swelling) could be included in the SMQ because
they are caused by drugs and are indeed the symptoms
of angioedema. Eight more terms (i.e., Solar urticaria,
Urticaria thermal, Urticaria contact) are true false posi-
tives because they are not related to angioedema. Finally,
other terms, although they mean oedemas, are not caused
by drugs. Thus, according to the expert, three of the 36
false positives should be considered for the inclusion in
the SMQ. As for the terminology structuring method, it
provides a cluster with 31 terms, among which 12 do not
belong to the SMQ. According to the expert, this eval-
uation is correct: the term Injection site oedema has a
too broad meaning (although this SMQ seems to con-
tain other broad terms, such as Gingival injury and Skin
lesion), while 11 other terms mean oedemas not caused
by drugs. With the merging we improve the performance:
we obtain two more true positives (Oedema peripheral,
Generalized oedema), while the false positives remain the
same. The results are different because the merging is not
supervised (it is based upon the intersection between the
clusters): the clusters may be different when considered
separately for each method and when considered through
their merging. As a matter of fact, this is precisely what
happens with the Angioedema SMQ: during the merging
step, the clusters selected are different from those selected
during the evaluation of the individual methods, and we
gain one new true positive term.
For the SMQ Embolic and thrombotic events, our meth-
ods provide 92 false positives with the semantic distance
and one with terminology structuring. The analysis of
these terms is very similar to what we observe for other
SMQs: some of the proposed terms should be considered
for inclusion in the SMQ (such as Iliac artery stenosis,
Hepatic artery stenosis, Vertebral artery stenosis, Cere-
bellar ataxia, Penile vascular disorder) because they are
very close to the already included terms, other terms
have a too broad meaning to efficiently contribute to the
SMQ (Peripheral ischaemia, Chest injury, Ischaemia or
Infarction). Finally, some other terms (Mesenteric artery
stenosis...) are true false positives. Among the false neg-
atives of the Angioedema SMQ, we find terms such as
Wheezing, Drug hypersensitivity, Swollen tongue, Penile
oedema, and among the false negatives of the Embolic and
thrombotic events SMQ, we find terms such as Venous
occlusion, Splenic infarction, Subclavian artery thrombo-
sis. The main reasons of the false negatives are: (1) with
the semantic distance and similarity algorithms, in addi-
tion to the fact that only 51% of the MedDRA terms are
included in the ontoEIM resource, when the terms are
included they may be too distant in this resource, (2)
with the terminology structuring, themethods may be not
exhaustive enough to detect all the lexical and syntactic
regularities within the terms.
Conclusions and perspectives
We combined several strategies and methods for the
clustering of the MedDRA terms with similar or close
meaning. We performed a comparison of the results
obtained and analyzed their complementarity. A ten-fold
cross-validation was carried out in order to test different
parameters and select those which positively influence the
results. Although the automatic creation of the SMQs is a
difficult task, our results indicate that the automaticmeth-
ods may provide a basis for the creation of the SMQs.
The current evaluation has been done against the existing
SMQs, but we expect we can apply the same method for
the creation of new SMQs with similar performance. Our
methods generate clusters which are smaller than the cor-
responding SMQs and which show their different aspects.
For this reason, the precision of the clusters is often high,
while their merging leads to the improvement of their
completeness.
Our future work will address the current limitations of
our methods and results. The material, and more partic-
ularly the ontoEIM resource, is being improved thanks to
a better alignment with the SNOMED CT [43,44]. More-
over, the future studies will lead to the identification of
other parameters which influence the quality of clusters
and also of other factors and more robust methods for the
merging of clusters [48-50]. Also, we would like to address
the points related to the complementarity of the clus-
ters and their potential hierarchical dependencies. As we
observed, the performance varies according to the SMQs
and it appears that different strategies should be used for
different SMQs, while currently we apply the same set-
tings of the methods to all the SMQs. We plan to perform
an exhaustive analysis of the nature of semantic relations
which can be observed within the SMQs, which will allow
to propose other methods and to reduce the current false
negatives within the clusters. An alternative method will
consist into the exploitation of corpora for the detection
of other semantic relations among the terms. In addition,
we intend to carry out a more detailed evaluation of the
generated clusters. This addresses particularly the impact
of the generated clusters on the exploring of the pharma-
covigilance databases (such as the FDA database) and on
the signal detection tasks. The very first results of this
type of evaluation (not presented in the article) are very
promising because they lead to an improvement of the
signal detection by comparison with the results obtained
with the SMQs.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MD implemented and performed the semantic distance and similarity
computing, the clustering experiences and the quantitative evaluation; LD
had the responsibility to evaluate the obtained clusters; TH adapted the
terminology structuring methods, generated the results with these methods
Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 13 of 14
http://www.jbiomedsem.com/content/5/1/18
and evaluated their correctness; NG led the work, proposed the methodology,
participated in the evaluation and designed the article. All the authors
participated in the writing of the article. All authors read and approved the
final manuscript.
Acknowledgments
The presented research was conducted as part of the PROTECT consortium. It
has received support from the Innovative Medicine Initiative Joint Undertaking
(www.imi.europa.eu). The authors also acknowledge the support of the
French Agence Nationale de la Recherche (ANR) and the DGA, under the
TecSan grant ANR-11-TECS-012. The authors are also thankful to the reviewers
for their patience and remarks which improved the quality of this work.
Author details
1CNRS UMR 8163 STL; Université Lille 1&3, F-59653 Villeneuve dAscq, France.
2Centre de Recherche des Cordeliers, Université Pierre et Marie Curie - Paris6,
UMR_S 872, Paris F-75006, France. 3INSERM, U872, Paris F-75006 France.
4Université Toulouse III Paul Sabatier, F-31062 Toulouse, France. 5LIMSI-CNRS,
BP133 Orsay, France. 6Université Paris 13, Sorbonne Paris Cité, France.
Received: 10 April 2012 Accepted: 1 January 2014
Published: 16 April 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42
http://www.jbiomedsem.com/content/5/1/42
SOFTWARE Open Access
Executing SADI services in Galaxy
Mikel Egaña Aranguren1,2*, Alejandro Rodríguez González1 and Mark D Wilkinson1
Abstract
Background: In recent years Galaxy has become a popular workflow management system in bioinformatics, due to
its ease of installation, use and extension. The availability of Semantic Web-oriented tools in Galaxy, however, is
limited. This is also the case for Semantic Web Services such as those provided by the SADI project, i.e. services that
consume and produce RDF. Here we present SADI-Galaxy, a tool generator that deploys selected SADI Services as
typical Galaxy tools.
Results: SADI-Galaxy is a Galaxy tool generator: through SADI-Galaxy, any SADI-compliant service becomes a Galaxy
tool that can participate in other out-standing features of Galaxy such as data storage, history, workflow creation, and
publication. Galaxy can also be used to execute and combine SADI services as it does with other Galaxy tools. Finally,
we have semi-automated the packing and unpacking of data into RDF such that other Galaxy tools can easily be
combined with SADI services, plugging the rich SADI Semantic Web Service environment into the popular Galaxy
ecosystem.
Conclusions: SADI-Galaxy bridges the gap between Galaxy, an easy to use but static workflow system with a wide
user-base, and SADI, a sophisticated, semantic, discovery-based framework for Web Services, thus benefiting both
user communities.
Keywords: Galaxy, Web services, SADI, RDF, SPARQL, OWL
Background
There is a growing global movement towards representa-
tion of bioinformatics data and knowledge using contem-
porary syntaxes and semantic languages approved by the
World Wide Web Consortium (W3C) [1], like Resource
Description Framework (RDF) [2] and Web Ontology
Language (OWL) [3]. Major bioinformatics resources
making their data available using these formats include
UniProt [4], EBI [5], and soon, the DNA Databank of
Japan [6]. Beyond these core providers, there are also
large integrated warehouses of bioinformatics data in
RDF format including, most significantly, Bio2RDF [7],
which integrates critical bioinformatics resources such as
dbSNP [8], OMIM [9], and KEGG [10], and NCBI eutils
*Correspondence: mikel.egana@ehu.es
1Biological Informatics, Centre for Plant Biotechnology and Genomics (CBGP),
Technical University of Madrid (UPM), Campus of Montegancedo, 28223
Pozuelo de Alarcón, Spain
2Genomic Resources, Department of Genetics, Physical Anthropology and
Animal Physiology, Faculty of Science and Technology, University of Basque
Country (UPV/EHU), Sarriena auzoa z/g, 48940 Leioa - Bilbo, Spain
[11], which wraps NCBI databases as resolvable RDF
resources.
This wealth of resources brings the inevitable require-
ment for tools that support the flow of native RDF data
through a formal bioinformatics analysis pipeline. The
Semantic Automated Discovery and Integration (SADI)
project has established design-patterns for bioinformatics
resources that wish to natively consume and produce RDF
data [12], and there are SADI plug-ins to several popular
data workflow and exploration environments, including
Taverna [13] and the IO Informatics Sentient Knowledge
Explorer [14]. While Taverna is a rich and full-featured
environment for constructing and editing complex work-
flows, Galaxy [15] is showing itself to be a favorite of
bench-biologists due to its relative simplicity compared
to Taverna, and the familiarity it brings biologists by
exposing the tools they commonly use in a manner that
they can quickly interpret and work with. As such, it was
desirable to bring support for SADI-based, RDF-native
data and analysis tools into the Galaxy environment. Here
we describe SADI-Galaxy - a set of tools that retrieve and
© 2014 Egaña Aranguren et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the
Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,
distribution, and reproduction in any medium, provided the original work is properly credited. The Creative Commons Public
Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this
article, unless otherwise stated.
Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 2 of 11
http://www.jbiomedsem.com/content/5/1/42
wrap SADI Semantic Web Services in a manner that
allows them to be included in Galaxy workflows.
Implementation
Galaxy is a Web server, written in Python, that offers a
very usable interface for the typical bioinformatics com-
putational analyses (Figure 1). A user can store data,
and analyse it using a variety of tools, sending the out-
put of one tool as input to the next. This process is
stored in a chronological history from which workflows
can be extracted, providing an easy-to-reproduce abstrac-
tion of common steps. Data, histories and workflows are
Web-shareable and can be imported and exported. A
Galaxy tool is, typically, a wrapper for a terminal exe-
cutable program. Since such wrappers are defined by
an XML file describing the inputs and outputs of the
tool as well as its Web interface [16], creating Galaxy
tools from pre-existing executables is not technically
demanding.
SADI is a standards-compliant set of lightweight design
patterns for publishing bioinformatics data and analy-
sis services on the Semantic Web. It uses Semantic Web
technologies at every level of the Web services stack.
In particular, a SADI service describes its interface in
OWL, and then both consumes and produces RDF data
that match that OWL logical description. Finally, SADI
requires that the output data is semantically connected
to the input data by a meaningful relationship. As such,
workflows of SADI services output unbroken chains of
RDF Linked Data [17]. SADI services are catalogued in a
publicly-accessible database (registry), and queries against
that database will be used in this study to find, and retrieve
the interface definitions for, SADI services of interest
to any given Galaxy user; the retrieved service defini-
tions will become templates for the Galaxy wrapper,
and thus the services can be accessed through these
wrappers.
SADI-Galaxy consists of two parts (Figure 2): the
Core, and the Tool-generator. SADI-Galaxy Core
includes three Galaxy tools that are installed in the same
manner as any other Galaxy tool [18], and they can be
used on their own. These tools are:
 SADI generic client. This tool is able to
execute any SADI service, given the services URI and
an RDF input file that is compatible with the service.
The RDF output of the service is stored as any Galaxy
output. Automated reasoning is used to check if the
RDF input is compliant with the SADI Services input
OWL class; i.e. whether the RDF instance is inferred
to be a member of the input OWL Class, effectively
providing up-front, low-cost automated workflow
validation.
 RDF Syntax Converter. This tool is able to
convert an RDF file into a variety of formats, most
importantly including Tab Separated Values (TSV)
format (three columns for subject, predicate and
Figure 1 Galaxy interface. The Galaxy web interface is divided in different views, listed at the top: Analyze data, Workflow, Shared Data,
Visualization, Admin, Help, and User. The frontpage is the Analyze data view, shown in this figure, with three columns: available tools (left),
current tool (center), and history of loaded data and executed tools (right). In this example the logged in user is working in a history named Mikel
mPuma testing: different datasets have been loaded (Steps 1, 3, 5) and a tool has been executed (The execution of tool Create ace TOC
from SAM, using the result of step 5 as input, has resulted in the dataset stored in step 9). The tool ClustalW is selected to be executed next,
using the dataset of step 1 as input, and the result will appear in the history as step 10. (Some steps have been deleted from the history).
Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 3 of 11
http://www.jbiomedsem.com/content/5/1/42
Figure 2 SADI-Galaxy Core tools and SADI services as Galaxy tools. Galaxy Analyze data view (only left column shown) resulting from the
installation of the SADI-Galaxy Core (SADI generic client, RDF Syntax Converter, and Merge RDF Graphs, under SADI COMMON UTILITIES)
and a number of specific SADI services, retrieved by a SPARQL query through the SADI-Galaxy Tool-generator (Under SADI SERVICES; only a
few shown).
object), so that non-RDF-based Galaxy tools can
consume SADIs output.
 Graph Merge. This tool is able to merge the output
of different SADI services into a single RDF graph for
downstream processing.
SADI-Galaxy then offers an additional, more advanced
functionality through the Tool-generator (Figure 3). The
Tool-generator adds the ability to query a SADI Service
registry, using arbitrary query parameters, to retrieve a set
of matching SADI services. These Service-specific tools
will then be deployed in Galaxy, alongside the Core tools
described above. The Tool-generator, at the code-level, is
simply a command line executable (A Shell script) that
reads one or more SPARQL [19] queries and executes
them against a SADI registry. The matching service URIs
retrieved by the query are then used to generate, for each
service, a Galaxy compliant wrapper and install it as a new
Tool.
The SPARQL queries used by the Tool-generator can
be tuned in order to install a concrete set of services
matching, for example, a particular research objectivea.
Query examples are provided in the SADI-Galaxy bundle,
as well as a query generator that is able to produce param-
eterised queries from a base query (e.g. for different SADI
service publishers). The default query can be seen in
Figure 4.
The Galaxy tools created by the Tool-generator utilize
the Cores generic SADI client, but pre-configure many
of the parameters of that generic tool. Therefore, rather
than requiring users to know, for example, the address and
data-types of a service, each is provided as an indepen-
dent Galaxy tool, enabling simple invoking and storing of
desired SADI tools as bookmarks. These bookmarks
Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 4 of 11
http://www.jbiomedsem.com/content/5/1/42
Figure 3 SADI-Galaxy Tool-generator. The Tool-generator is a Shell script that reads a SPARQL query (top, center) and generates XML files (an
XML file for each SADI service Galaxy Tool). The XML files are copied to the appropriate location on the Galaxy server and appear as regular tools in
the Galaxy tools menu, under SADI SERVICES.
also enable Galaxys tool-search function to explore SADI
Service descriptions. In this way, the same SADI Service
can be invoked either through the generic client (if the
URI is known by the user) or through the corresponding
Galaxy tool (if the URI is unknown and/or the service is
used frequently).
Resultsb
A simple example
To demonstrate how SADI-Galaxy can be used to trans-
form data to RDF, in order for SADI services to consume
it, and how different SADI services can be combined as
regular Galaxy tools, a simple use case was envisioned,
and captured in the workflow depicted in Figure 5. The
aim of the workflow is to obtain the UniProt entry asso-
ciated with a PDB entry [20], that is, to obtain informa-
tion of the protein whose 3D structure is described in
PDB. The workflow starts from a TSV file (as it is cus-
tomary in Galaxy) and the TAB2RDF toolc, part of the
SPARQL tools tool-set [21], is used to transform it to
the RDF/XML syntax that a SADI service can consume.
The file is submitted to the SADI service pdb2uniprot
to obtain the UniProt ID, which is returned as an RDF
file. The RDF file is submitted to another SADI service,
uniprotInfo, to obtain all the information about that
UniProt entry, also as an RDF file. This final RDF file can
be converted to a TSV file with RDF2TAB, or queried with
SPARQL-Galaxy to obtain concrete information about
the protein [22].
Galaxy as a complex SADI client
This second use case will reveal the more detailed func-
tionality provided by SADI-Galaxy, including registry
searching, reasoning, and RDF format conversions that
allow linking between SADI tools and other Galaxy tools.
The premise of this use-case is that a researcher is inter-
ested in retrieving all of the information about a specific
protein, that can be obtained from any SADI service, and
Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 5 of 11
http://www.jbiomedsem.com/content/5/1/42
Figure 4 Tool-generator default SPARQL query. In this query a set of filters for retrieving SADI service URIs is defined: the service (?s) must be a
member of the classes serv:serviceDescription and sadi:Service (Lines 11 and 12); the service must be provided by an organisation
(serv:providedBy ?org, line 13); the service must add an OWL property predicate to the output (sadi:decoratesWith ?out .
?out owl:onProperty ?prop, lines 17 and 18); etc. The query returns around 250 active SADI services from the default registry. Other
queries can be defined, as long as the ?s variable is used for service URIs. For example, in line 15, the "wilkinsonlab.info" value can be
used, instead of the variable ?pub, to retrieve SADI services provided by wilkinsonlab.info.
then to integrate and query it, as shown in Figures 6, 7,
8, 9, 10. The first step is to generate Galaxy Tools for
all services that can consume data that complies with
(i.e. is logically inferred to be a member of) the OWL
class UniProt_Record [23]. This is accomplished by
using SADI-Galaxy to execute the SPARQL query shown
in Figure 6. As a result, the researcher obtains a Galaxy
interface in which only the SADI services relevant to
their investigation are presented, all pre-configured and
ready to invoke their individual SADI services as shown
in Figure 7. The researchers starting data is first retrieved
from the Biomart central server [24] using the Galaxy
standard facility Get data: Biomart central server (Top
of Figure 8). The input - a list of UniProt IDs -, is manip-
ulated with the Galaxy default text manipulation tools to
generate an RDF file the SADI services can consume (Left-
hand of Galaxy workflow in Figure 8). An interface to
execute each SADI service is made available to the user by
simply clicking on the services name (No need to know
the services URI, as shown in Figure 7). Since the out-
puts from all SADI services (Center of Galaxy workflow
in Figure 8) are all RDF, merging them is trivial (Right-
hand of Galaxy workflow in Figure 8) and results in an
integrated dataset that can be queried (Figure 9) to obtain
results that, in turn, resolve to actual resources on theWeb
(Figure 10).
This process, in which the user integrates disparate
information produced by many different SADI services,
can be executed any time new UniProt IDs are obtained,
or any time new SADI services that can consume UniProt
entries are published in the registry (This can be accom-
plished by simply running the Tool generator with the
same query periodically, without having to know when
the registry is updated, i.e. which new services have been
added).
Discussion
SADI-Galaxy is inspired by the Galaxy Web Services
Extensions (GWSE) tool [25,26], which is able to dynam-
ically load SAWSDL/WSDL web services as Galaxy tools.
SADI-Galaxy focuses on SADIs SemanticWeb-compliant
services, rather than SAWSDL/WSDL services, and there-
fore for the first time brings Semantic Web resources
into the native Galaxy environment. In addition, the two
extensions have slightly different behaviors. Where the
WS Extensions tool dynamically loads new Galaxy tools,
the ability to execute such dynamic loading is not part of
the standard Galaxy distribution. The SADI-Galaxy code
conforms strictly to the Galaxy specification, thus works
in any Galaxy off-the-shelf installation. After a restart,
the SADI-Galaxy generic client becomes available in the
left-hand column of the Galaxy interface, and individ-
ual SADI services may be dynamically loaded into that
tool by providing the service URI. To simplify this task
even further, SADI-Galaxy allows the option of query-
ing the SADI service registry to discover the URIs of
desired services, thus making it possible to automatically
Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 6 of 11
http://www.jbiomedsem.com/content/5/1/42
Figure 5 Galaxy workflow for use case A simple example. Top: Galaxy Workflow view interface; bottom: simplified version, with detailed
depiction of files, including RDF triples (not all the triples shown). The workflow starts with a TSV file containing the information that will be sent as
input to the SADI service pdb2uniprot (Input dataset). The file is processed to convert it to a column format Galaxy can recognise
(Convert) and then transformed to RDF with the Tab-to-RDF and RDF format tools from the SPARQL tools tool-set. The RDF file is
submitted to the pdb2uniprot SADI service using the SADI services generic caller and the output RDF is sent to the
uniprotInfo SADI service, also with the SADI services generic caller. The output RDF from the uniprotInfo SADI service can
be converted to a TSV file with RDF Syntax Converter or queried with SPARQL (Execute an SPARQL query against an RDF
file) to obtain concrete information (Also in TSV format). Note that triples are added to the RDF input as a result of executing a SADI service, in
pdb2uniprot and uniprotInfo.
Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 7 of 11
http://www.jbiomedsem.com/content/5/1/42
Figure 6 SPARQL query to retrieve SADI services for
UniProt_Record. This query, when executed by SADI-Galaxy,
retrieves the URIs of all the services that have UniProt_Record
as input class (Lines 10 to 12): the services consume RDF data
containing instances that are inferred to be members of
UniProt_Record when automated reasoning is applied, i.e.
instances that satisfy the restrictions defined in the OWL Class
UniProt_Record.
generate a large number of desirable Galaxy tools, a pow-
erful feature that is not available within theWS Extensions
tool.
Another system that has features comparable to SADI-
Galaxy is Tavaxy [27], which is a standalone server that
is a mediator between Galaxy and Taverna. Where
Tavaxymakes it possible to mix Taverna and Galaxy work-
flows, SADI-Galaxy concentrates on bringing Semantic
Web Services - already available within Taverna - into
the Galaxy environment. A similar result could be
achieved by first building a workflow of SADI services
in Taverna, then importing that workflow into Tavaxy in
order to add the Galaxy services; however, that process
is far from seamless. It is more desirable to provide
native access to the thousands of SADI resources from
within the Galaxy environment itself, than to require
Galaxy users to use foreign tools such as Taverna and
Tavaxy.
We suggest SADI-Galaxy as the minimum infras-
tructure that marks the point of intersection between
SADI Services and Galaxy, which can now act as the
core codebase upon which more powerful functional-
ity is constructed. In particular, we expect two major
developments in the near future: discovery, where given
an RDF input, Galaxy is able to infer and automati-
cally select the appropriate SADI Tool; and adding tools
dynamically (as WS Extensions already does), once a
consensus has been reached by Galaxy developers on
how to implement a standard function for dynamic tool
loading.
Conclusions
The simplicity and predictability of the SADI Service
design patterns - effectively, to simply consume and pro-
duce raw RDF over HTTP POST - has allowed us to
create a highly generic service invocation infrastructure
that would have been extremely difficult using other Web
service frameworks where, often, a client needs to know
significantly more about the data schema and service
invocation process. Building a framework that focuses on
semantics, rather than syntax - not only for the data itself,
but also for the messaging infrastructure - means that the
Figure 7 Galaxy interface for getKEGGIDFromUniProt SADI service. On the left column, available SADI services that comply with
UniProt_Record are listed. On the right column, one of them, getKEGGIDFromUniProt, is selected in order to invoke it: only the RDF data
must be provided (In this case, the RDF is produced by RDF format in step 69 of current history). When the Execute button is clicked,
automated reasoning will be used to validate the RDF data against the UniProt_Record OWL Class, as noted by the Input OWL Class menu
item in the What it does section of the tool interface.
Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 8 of 11
http://www.jbiomedsem.com/content/5/1/42
Figure 8 Galaxy workflow for use case Galaxy as a complex SADI client. In order to obtain the input for the workflow (Bottom), data is
imported directly from a BioMart server, without having to upload the data to the Galaxy server (Top). The data, a list of UniProt IDs, is then
converted to suitable RDF (Compliant with UniProt_Record) using the standard Galaxy text manipulation tools (Bottom workflow, left-hand).
The resulting RDF is then sent to different SADI services, generated by SADI-Galaxy, that can consume it (Bottom workflow, middle); each service is
executed as shown in Figure 7. The output of the services is merged with the Merge Graphs tool (Bottom workflow, right-hand), and queried with
SPARQL Galaxy (Executed as shown in Figure 9) to generate the results shown in Figure 10.
client can be largely agnostic so as to how to invoke any
service, making the necessary decisions on an ad hoc basis
at invocation-time. For example, in SADI-Galaxy, when
combining different SADI services, the RDF of the inter-
mediate steps can just as easily be consumed by any other
RDF-based tool.
Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 9 of 11
http://www.jbiomedsem.com/content/5/1/42
Figure 9 SPARQL Galaxy. This query takes as input the merged output of all the SADI services and finds the proteins that are encoded by a KEGG
gene and are related to SNPs, to produce the results shown in Figure 10.
Galaxys easy-to-use platform for storing data, pro-
grams for analysing data, and the resulting workflows is
an ideal ecosystem within which to provide SADIs data
retrieval and analysis functionalities to our target end-
users in a very familiar and straightforward manner. We
hope that, by providing SADI services within the widely-
used Galaxy platform, we can encourage the more rapid
adoption of these powerful new Semantic Web technolo-
gies, with SADI-Galaxy acting as the de facto interface
between these two projects.
Figure 10 SPARQL query results. Galaxy interface for HTML results of query from Figure 9 (Top). The links point to actual resources: this is shown
by the resolved KEGG gene entry (Bottom), obtained as the result of clicking on a link.
Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 10 of 11
http://www.jbiomedsem.com/content/5/1/42
Availability and requirements
 Project name: SADI-Galaxy.
 Project home page: http://github.com/mikel-egana-
aranguren/SADI-Galaxyd.
 Operating system(s): UNIX-based (GNU/Linux,
Mac OS X, *BSD, etc.).
 Programming language: Java, Python, Shell Script,
and Sed.
 Other requirements: a working Galaxy server
(http://galaxyproject.org/).
 License: General Public License (GPL), version 3.
Endnotes
aDifferent SADI tool-sets can be added
simultaneously to the Galaxy tools menu, by executing
the Tool-generator sequentially (with different SPARQL
queries) and by editing the title in the Galaxy tools menu
each time a new tool set is added (The default title is
SADI SERVICES, as shown in Figure 2). This way a
researcher can organise SADI services in meaningful
groups, having all the groups available in the same
Galaxy interface.
bThe use cases can be explored and executed in the
following Galaxy page (Also available in the http://biordf.
org:8983 Galaxy server through shared data and then
published pages): http://biordf.org:8983/u/mikel-
egana-aranguren/p/sadi-galaxy-jbms-use-cases. In order
to reproduce a use case a user must be created in the
Galaxy server and the history and the workflow of the
use case imported, so that the first item of the history can
be used as input for the workflow.
cA locally modified version of SPARQL tools was
used, adding the possibility of rendering user-defined
namespaces. A patch has been submitted to the original
author for inclusion on the Galaxy tool shed repository;
the modified version can be obtained at http://github.
com/mikel-egana-aranguren/SPARQL_tools_tab2rdf.
dSee also TAB2RDF (http://github.com/mikel-egana-
aranguren/SPARQL_tools_tab2rdf), SADI generic
client (http://toolshed.g2.bx.psu.edu/repos/mikel-
egana-aranguren/sadi_generic/) and SPARQL Galaxy
(http://toolshed.g2.bx.psu.edu/repos/mikel-egana-
aranguren/sparql_galaxy/).
Abbreviations
RDF: Resource description framework; OWL: Web ontology language; SADI:
Semantic automated discovery and integration; SPARQL: SPARQL protocol
and RDF query language; TSV: Tab separated values.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MEA developed SADI-Galaxy and ARG developed the SPARQL query engine to
retrieve the SADI services. MDW contributed with test-cases, wrote and
revised portions of the manuscript, and supervises the Biological Informatics
laboratory within which this work was executed. All authors read and
approved the final manuscript.
Acknowledgements
Mikel Egaña Aranguren is funded by the Marie Curie Cofund programme (FP7)
of the European Union and the Genomic Resources Group of the University of
Basque Country. Alejandro Rodríguez González and Mark D. Wilkinson are
funded by the Isaac Peral Programme of the CBGP-UPM.
Received: 12 December 2013 Accepted: 19 September 2014
Published: 22 September 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Lingutla et al. Journal of Biomedical Semantics 2014, 5:50
http://www.jbiomedsem.com/content/5/1/50SOFTWARE Open AccessAISO: Annotation of Image Segments with
Ontologies
Nikhil Tej Lingutla1, Justin Preece2, Sinisa Todorovic1, Laurel Cooper2, Laura Moore2 and Pankaj Jaiswal2*Abstract
Background: Large quantities of digital images are now generated for biological collections, including those developed
in projects premised on the high-throughput screening of genome-phenome experiments. These images often carry
annotations on taxonomy and observable features, such as anatomical structures and phenotype variations often
recorded in response to the environmental factors under which the organisms were sampled. At present, most of these
annotations are described in free text, may involve limited use of non-standard vocabularies, and rarely specify precise
coordinates of features on the image plane such that a computer vision algorithm could identify, extract and annotate
them. Therefore, researchers and curators need a tool that can identify and demarcate features in an image plane and
allow their annotation with semantically contextual ontology terms. Such a tool would generate data useful for inter and
intra-specific comparison and encourage the integration of curation standards. In the future, quality annotated image
segments may provide training data sets for developing machine learning applications for automated image annotation.
Results: We developed a novel image segmentation and annotation software application, Annotation of Image
Segments with Ontologies (AISO). The tool enables researchers and curators to delineate portions of an image into
multiple highlighted segments and annotate them with an ontology-based controlled vocabulary. AISO is a freely
available Java-based desktop application and runs on multiple platforms. It can be downloaded at http://www.
plantontology.org/software/AISO.
Conclusions: AISO enables curators and researchers to annotate digital images with ontology terms in a
manner which ensures the future computational value of the annotated images. We foresee uses for such
data-encoded image annotations in biological data mining, machine learning, predictive annotation, semantic
inference, and comparative analyses.
Keywords: Image annotation, Semantic web, Plant ontology, Image segmentation, Plant anatomy, Web
services, Computer vision, Image curation, Machine learningBackground
Annotation of Image Segments with Ontologies (AISO)
is an interactive tool which allows users to segment and
annotate a digital image  such as those produced with
digital photography or from scanned prints  with
ontology terms. An ontology is a controlled and struc-
tured vocabulary of agreed-upon labels (terms) that
represent the knowledge of the types of entities within a
given domain [1]. Labeling image data with ontology
terms imbues it with semantic meaning, which makes it* Correspondence: jaiswalp@science.oregonstate.edu
Equal contributors
2Department of Botany and Plant Pathology, 2082 Cordley Hall, Oregon State
University, Corvallis, OR 97331-2902, USA
Full list of author information is available at the end of the article
© 2014 Lingutla et al.; licensee BioMed Centra
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.possible to computationally infer relationships amongst
different images and parts of images. The use of ontol-
ogies has gained increasing importance as the number,
complexity, and size of biological data sets have in-
creased [2]. AISO was developed in response to a need
within the biology community for a streamlined tool that
enables consistent and structured labeling of digital im-
ages. A shift in research focus towards high throughput
phenotyping [3,4] requires specialized tools that bring
consistency to the image annotation process. AISO an-
notates images with ontology terms and taxonomy labels
via lightweight web services, allowing users to select and
annotate image segments.
Many photo-editing and illustration software packages
enable the ad hoc editing of an image, but any highlightingl. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Lingutla et al. Journal of Biomedical Semantics 2014, 5:50 Page 2 of 6
http://www.jbiomedsem.com/content/5/1/50and labeling utility requires thorough knowledge of the
softwares illustration capabilities (i.e. layering, boundary
detection) and does not include the structured integration
of scientific data. For example, any labels applied to hand-
illustrated segments superimposed onto an image would
have to be individually constructed and associated with a
particular portion of an image. AISO simplifies this func-
tionality and requires only a few input gestures and clicks
to identify and label segments. The resulting structured
image and ontology annotation allows for consistent
extraction techniques, enabling future database storage,
active learning, and semantic inference functionalities. Re-
searchers are thus empowered to construct meaningful
image data sets drawn from their laboratories, online
image archives, and publications.
Implementation
Software architecture
AISO is a multi-platform, Java desktop application ex-
tending the source code of the Interactive Segmentation
Tool (IST) [5], originally developed for comparing the per-
formance of image segmentation algorithms. The user
interface was constructed using the Standard Widget
Toolkit [6], an open-source Java package. The ontology
terms are provided through a light-weight Plant Ontology
web service [1], and returns data in the JSON format [7].
Species names are provided via the uBio web service [8] in
XML. Annotation data  segments, labels, and curation
details  are all saved into a compressed ZIP archive,
which contains the original image, binary segment data
files, segment mask images, and an XML file storing
segment coordinates and other curation metadata. An
example of the contents and structure of an AISO XML
metadata file is available in the Help document, which
may be accessed from the application menu.
Choice of segmentation algorithm
Segmentation algorithms are created with different ap-
plication domains in mind and the computer vision re-
search community is generally focused on segmenting
images of the human body and the built environment.
Segmenting anatomical images of biological specimens,
such as plants, presents a number of challenges that
have received scant attention in the literature. Plants
contain curvilinear and asymmetric forms, textures, and
spatial orientations that make identification and classifi-
cation more difficult for computer vision algorithms. We
chose the Interactive Graph Cuts (IGC) segmentation
algorithm because markups have a local effect, thereby
avoiding major global deformations in the segmented
area. This has great value in plant images that contain
densely grouped features, such as many similar, overlap-
ping leaf structures. The IGC algorithm also is more ac-
curate in extracting foreground objects, and includes aresponsiveness that allows the user to iteratively refine
the segments. The average time required for a user to at-
tain optimal object and boundary accuracy for an image,
and the average total time spent annotating each image
are much lower when compared to other algorithms [9].
Results and discussion
Segmenting and annotating images
After opening an image file in AISO (Figure 1A; allowed
formats: JPEG, PNG, GIF, and BMP), the user works in
two alternating modes: Segmentation and Labeling. The
default Segmentation mode operates in a foreground/
background paradigm and allows the user to mark the
foreground of an image by drawing red lines with the
mouse or trackpad (left-click and drag); the back-
ground is denoted with blue lines (right-click and drag).
Once both the foreground and the background are des-
ignated with these mark-up lines (Figure 1B,C), AISO
will execute its IGC segmentation algorithm [10], identi-
fying and extracting borders circumscribing the fore-
ground mark-up and visually highlighting the area
(Figure 1C). Edges in an image can be extracted wher-
ever there is a detectable change in the lightness or
darkness of adjacent pixels. The user may further refine
their segmented area by adding foreground mark-up
lines (red) outside of the initial area to expand the dis-
cernable boundary, or by applying background mark-up
lines (blue) across the initial area to exclude nearby
image content from the segment. Once satisfied with a
particular segment, the user must form that segment by
pressing the Form segment button, thereby fixing it as
a new layer overlaid on top of the original image. Fol-
lowing segmentation, the user then enters into Labeling
mode by pressing the Labeling Mode button to assign
an ontology term to a segment. The user may search for
a Plant Ontology term by typing its name in the
dropdown box labeled Annotate. The PO terms are
provided through a web service [1] which requires an
Internet connection. The dropdown box will display
auto-completed term suggestions when the user presses
the down arrow on their keyboard. After selecting a
term in the dropdown, the user should press the As-
sign button to associate their selected PO term with the
currently selected segment. The assigned ontology term
will appear on a segment when the user selects that seg-
ment and hovers over it with the pointer (Figure 1D).
The user may also assign a taxonomic name to the en-
tire annotated image via the uBio namebank search
web service [8] and enter additional curation metadata
(Figure 1E).
The user can save annotated image files into a custom
ZIP package, and may also re-open previously annotated
images for continued editing. The original image is al-
ways preserved and viewable. When saving annotations,
(A) (B)
(E)
(C) (D)
Figure 1 Screenshots of AISO demonstrating the segmentation process. (A) Open an image file in AISO (e.g. a Chamerion angustifolium
flower) (B) Mark the foreground (red line): the aspect of the image you would like to segment. (C) Mark the background (blue line): the area
of the image you want to ignore. (NOTE: Auto-segmentation occurs after the background is marked; further refinement of both foreground and
background is possible.) (D) Label the new image segment with an ontology term selected from the integrated web service query interface.
(E) Screenshot of the annotation panel of AISO showing a selected ontology term, its definition and synonyms, and designated species
(provided via web services), as well as curator, collection, and comment information entered by the user.
Lingutla et al. Journal of Biomedical Semantics 2014, 5:50 Page 3 of 6
http://www.jbiomedsem.com/content/5/1/50the user may optionally export an HTML file containing
a web-enabled version of the annotated image, which al-
lows the user to easily share their work in other media
platforms. For example, manuscript authors could sub-
mit annotated images along with other supplementary
data, to enhance the collection of ontology-based image
data for comparative analyses and machine learning.
Annotated images could thereafter be used in online
resources and publications, or placed in a file archive or
database for future analysis.
Case studies
AISO was used to segment and label small library of
botanical images (provided by Dennis Wm. Stevenson;
New York Botanical Gardens), both as a test of AISOs
capabilities and as the beginnings of a training set of
segmented image data for a potential future active-learning system. One such image of the floral structures of
Galanthus elwesii, commonly known as Giant Snowdrop,
was segmented and labeled with the Plant Ontology
terms plant ovary (PO:0009072), style (PO:0009074),
tepal (PO:0009033), anther (PO:0009066) and perianth
(PO:0009058) (Figure 2). Each segment was generated
with only a few user-directed mouse or trackpad strokes,
followed by a quick auto-complete search for the appro-
priate Plant Ontology term. Each segments overlay is
assigned a different preset color to help distinguish it from
any other segments in the same image.
Additionally, AISOs capabilities have been applied to
paleo-botanical images of fossilized lauraceous flowers
from the Eocene epoch (Figure 3). In this particular
case, Plant Ontology terms stamen (PO:0009029),
carpel (PO:0009030), and tepal (PO:0009033) were ap-
plied to user-identified segments in a cross-section image.
(A)
(B) (C) (D) (E)
Figure 2 Screenshots and insets of AISO displaying annotated segments of Galanthus elwesii (giant snowdrop) flowers. The represented
segments are (A) perianth (PO:0009058), (B) multiple instances of anther (PO:0009066), (C) tepal (PO:0009033), (D) style (PO:0009074) and (E) plant
ovary (PO:0009072).
Lingutla et al. Journal of Biomedical Semantics 2014, 5:50 Page 4 of 6
http://www.jbiomedsem.com/content/5/1/50This particular example highlights the IGC algorithms lo-
calized border-detection capabilities when constructing an
image segment.
Comparison to existing software
AISO brings together disparate image segmentation and
semantic labeling functionalities found in existing software
and merges them into a user-friendly, science-focused
package. Hollink et al. [11] developed an application inter-
face for annotating whole images with ontology terms, but
it lacked an image segmentation feature. Conversely, Shao
et al. [12] developed image segmentation capabilities with-
out segment-specific semantic tagging features. Semantic
Image Annotator [13], built as an extension to the web-
focused Semantic MediaWiki platform, allows users to de-
fine rectangular areas on an image file and tag those areas
with semantic labels, but does not provide dynamic image
segmentation. Koletsis and Petrakis dissertation work [14]
includes an algorithm named Semantic Image Annota-
tion which automatically annotates images with ontologyterms based on a training data set of similar images, but
this approach also lacks a segmentation feature.
Future development
Future enhancements to AISO include extending web
service support for multiple ontologies, such as those
developed by OBO Foundry [15] members and model
organism databases. Enhancements would also include
enabling automated segmentation based on active
learning, and adding support for high-resolution im-
ages (10120 megabytes).
Conclusions
AISO allows researchers and curators to interactively
segment images and assign semantic annotations to
those segments. This annotation capability gives biolo-
gists the opportunity to enhance the computational
value of their own image data. Data-enriched images can
be used to mine biological data sets, train machine
learning software, and generate conclusions via semantic
(A)
(B)
Figure 3 Cross section of a fossilized lauraceous flower from the Eocene epoch (~55 MA) showing a single carpel, stamen, and tepals.
The original image (A) has been segmented and annotated using Plant Ontology terms in AISO (B). Note the automated color differentiation
between segments. The annotation labels in this figure have been enlarged for readability.
Lingutla et al. Journal of Biomedical Semantics 2014, 5:50 Page 5 of 6
http://www.jbiomedsem.com/content/5/1/50
Lingutla et al. Journal of Biomedical Semantics 2014, 5:50 Page 6 of 6
http://www.jbiomedsem.com/content/5/1/50inference. We believe that the existing functionality of
AISO, combined with our future efforts in active learning,
will provide a powerful tool for the biology community
and for scientific journals interested adding annotated im-
ages and associated metadata to their publication pipeline.Availability and requirements
Project name: Annotation of Image Segments with
Ontologies (AISO).
Project home page: http://www.plantontology.org/software/
AISO.
Operating system(s): Platform-independent (Mac OS X,
Linux, Windows).
Programming language: Java.
Other requirements: An Internet connection, the Java
Runtime Environment (JRE).
License: Creative Commons (Attribution-NonCommercial-
NoDerivs 3.0 Unported).
Any restrictions to use by non-academics: No.
Abbreviations
AISO: Annotation of image segments with ontologies; IGC: Interactive graph
cuts; IST: Interactive segmentation tool; JSON: Javascript object notation;
PO: Plant ontology; XML: Extensible markup language.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
PJ and ST conceived the project. PJ and ST co-supervised NL for his masters
thesis project. NL and JP designed and wrote the software enhancements
(building on Kevin McGuiness work; see acknowledgements), co-authored
the manuscript, wrote user manuals, coordinated testing, and provided
user support and training. JP coordinated the software development and
technical architecture of the new features. LC curates and coordinates the
Plant Ontology project; tested the AISO software and mentored students in
annotation of images, and contributed to the manuscript. LM tested the
AISO software and created the annotated images presented herein. NL,
JP, LC, ST, and PJ edited the manuscript. All authors read and approved the
final manuscript.
Acknowledgements
We thank Kevin McGuinness at Dublin City University for sharing the original
IST source code, which we modified extensively to build AISO, and for
advising our development efforts. We are also grateful to Dr. Dennis Wm.
Stevenson at the New York Botanical Gardens for his permission to use
digital images of botanical species in the segmentation testing of AISO. We
also thank Brian Atkinson and Dr. Ruth A. Stockey at Oregon State University,
for their contribution of the paleo-botanical image and subsequent segmentation
and annotation, as described in the Case Studies and Figure 3. This work was
financially supported by the National Science Foundation (NSF) of USA award
IOS:0822201. PJ and ST were also supported by Oregon State University (OSU).
The funders had no role in the software design, analysis or preparation of the
manuscript.
Author details
1School of Electrical Engineering and Computer Science, Kelley Engineering
Center, Oregon State University, Corvallis, OR 97331-2902, USA. 2Department
of Botany and Plant Pathology, 2082 Cordley Hall, Oregon State University,
Corvallis, OR 97331-2902, USA.
Received: 13 September 2014 Accepted: 26 November 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24
http://www.jbiomedsem.com/content/5/1/24
RESEARCH Open Access
Terminology extraction frommedical texts in
Polish
Ma?gorzata Marciniak* and Agnieszka Mykowiecka
Abstract
Background: Hospital documents contain free text describing the most important facts relating to patients and their
illnesses. These documents are written in specific language containing medical terminology related to hospital
treatment. Their automatic processing can help in verifying the consistency of hospital documentation and obtaining
statistical data. To perform this task we need information on the phrases we are looking for. At the moment, clinical
Polish resources are sparse. The existing terminologies, such as Polish Medical Subject Headings (MeSH), do not
provide sufficient coverage for clinical tasks. It would be helpful therefore if it were possible to automatically prepare,
on the basis of a data sample, an initial set of terms which, after manual verification, could be used for the purpose of
information extraction.
Results: Using a combination of linguistic and statistical methods for processing over 1200 children hospital
discharge records, we obtained a list of single and multiword terms used in hospital discharge documents written in
Polish. The phrases are ordered according to their presumed importance in domain texts measured by the frequency
of use of a phrase and the variety of its contexts. The evaluation showed that the automatically identified phrases
cover about 84% of terms in domain texts. At the top of the ranked list, only 4% out of 400 terms were incorrect while
out of the final 200, 20% of expressions were either not domain related or syntactically incorrect. We also observed
that 70% of the obtained terms are not included in the Polish MeSH.
Conclusions: Automatic terminology extraction can give results which are of a quality high enough to be taken as a
starting point for building domain related terminological dictionaries or ontologies. This approach can be useful for
preparing terminological resources for very specific subdomains for which no relevant terminologies already exist. The
evaluation performed showed that none of the tested ranking procedures were able to filter out all improperly
constructed noun phrases from the top of the list. Careful choice of noun phrases is crucial to the usefulness of the
created terminological resource in applications such as lexicon construction or acquisition of semantic relations from
texts.
Background
Terminology extraction is the process of identifying
domain specific phrases (terms) based on the analysis
of domain related texts. It is a crucial component of
more advanced tasks like: building ontologies for specific
domains, document indexing, construction of dictionar-
ies and glossaries. The subject has been undertaken
quite often, particularly in the context of molecular biol-
ogy terminology. In particular, the Medline abstracts
database was frequently used as a data source for pro-
tein and gene names, [1,2]. The biomedical domain is
*Correspondence: mm@ipipan.waw.pl
Institute of Computer Science PAS, Jana Kazimierza 5, 01-248 Warsaw, Poland
changing so rapidly that manually prepared dictionar-
ies are becoming outdated very quickly. In more sta-
ble domains, like clinical medicine, a lot of terminology
also exists which is used locally and which is not listed
in any dictionaries. For many languages, medicine and
biomedicine terminology is covered by several sources
like those available in UMLS [3], e.g. MeSH or SNOMED,
but there are still a lot of domain related expressions
occurring within clinical texts which are not included
there. Moreover, there are a number of languages (like
Polish), whose medical linguistic resources are underde-
veloped. In particular, for the Polish language there are
no computer dictionaries, except MeSH, with medical
© 2014 Marciniak and Mykowiecka; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the
Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,
distribution, and reproduction in any medium, provided the original work is properly credited.
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 2 of 14
http://www.jbiomedsem.com/content/5/1/24
vocabulary or terminology, nor is there a SNOMED
translation.
This lack of resources and the need for keeping up to
date resources describing rapidly changing subdomains
has lead to exploring the idea of automatic terminol-
ogy extraction. Several different approaches to this task
are discussed in [4]. It may be observed in the research
reported there that, regardless of the detailed assump-
tions undertaken in the particular solutions, terminology
extraction usually consists of two steps. The first one iden-
tifies candidates for the terms, and is usually supported
by linguistic knowledge. The second step, based on statis-
tics, involves ranking and filtering candidates according
to some measure of their relative domain importance.
Although the general scheme of term extraction is quite
stable, the specificity of a particular natural language, the
domain of interest, the size of data available and the acces-
sibility of language processing tools, can all influence the
results. Until now, there has been no single strategy which
can precisely select terms from non terms and which
has proved to be best for all the domains and languages
tested.
Automatic extraction of phrases from texts makes it
possible not only to prepare a list of domain related
terms, but also to identify the exact ways in which they
are expressed in context. These results can be used later
on to help create a domain ontology and in specify-
ing the information that may be extracted from docu-
ments with rule based methods, see [5]. While writing
extraction rules we just have to describe all the identi-
fied phrases. Assigning one semantic concept or ontology
class to all lexical paraphrases requires the normalisa-
tion step on which all variants are grouped together.
In [6] the normalisation procedures are described. The
authors consider the conflation of orthography and inflec-
tional variants, as well as lexical synonyms, structural
variants of phrases, and recognition of acronyms and
abbreviations.
What is common to all domain vocabularies is that
the vast majority of terms are noun phrases. Although
in some approaches verbal phrases are also taken into
account [7], terminology vocabularies usually contain
nominalised versions of such terms. Extracting can-
didates for domain terms can be based on simple
n-grams, e.g. [8], but in most approaches, linguistic infor-
mation is used. Usually only small shallow grammars
are defined [9], but sometimes more elaborate linguistic
processing is performedin [7] the terminology extrac-
tion was carried out on fully syntactically parsed texts.
While extracting domain terminology we are interested
in compound terms which describe precise concepts,
e.g. kos´c´ ramienia humerus, the concepts attributes,
e.g. powie?kszone we?z?y ch?onne enlarged lymph nodes or
relationships between two concepts, e.g. z?amanie kos´ci
przedramienia humerus fracture. These phrases are not
only expressing certain domain important concepts or
events but can also be used later on to build up a domain
model in which we can include the knowledge that lymph
nodes can be enlarged and that the bone can be broken.
Recognition of complex expressions can entail recogni-
tion of shorter phrases which are part of these longer
ones.
At the initial stage of candidate selection, the longest
sequences matching the set of defined rules are identi-
fied. If we are to order phrases using weights based on
the number of times they appear in text, we should also
analyse phrases which occur inside others. For exam-
ple, an occurrence of lewa nerka prawid?owa left kidney
normal should also be counted as an occurrence of the
phrases: nerka kidney, lewa nerka left kidney and nerka
pra-wid-?owy kidney normal. Another decision to be
made is whether to count the occurrences of all nested
phrases or only those which occurred at least once as
a separate phrase. It may happen that a term which is
very important does not occur even once in a given data
set.
The preselected set of phrases constitute input data
for the term selection algorithm which usually assigns
each phrase a numerical value approximating the rela-
tive likelihood that the phrase will constitute a domain
term. One of the most popular ranking methods, designed
specially for recognising multiword terms, is the C/NC
method proposed in [9]. This method takes into account
phrase occurrences both in isolation and nested inside
longer ones, and the different contexts of their appear-
ances. In this method every phrase is assigned a C-value,
which is computed on the basis of the number of times it
occurs within the text, its length, and the number of dif-
ferent contexts it takes (within noun phrases in which it
occurs).
The definition of the C-value coefficient is given below
(p  is a phrase under consideration, LP is a set of phrases
containing p), r(LP)  the number of different phrases in
LP, l(p) = log2(length(p)).
C-value(p) =
??
?
l(p) ? (freq(p) ? 1r(LP)
?
lp? LP freq(lp)), if r(LP) > 0,
l(p) ? freq(p), if r(LP) = 0
(1)
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 3 of 14
http://www.jbiomedsem.com/content/5/1/24
Long phrases tend to occur more rarely than shorter
ones so the multiplication by the logarithm of length
moves them towards the leading positions. If a nested
phrase occurs in one context only, its C-value is set to 0 as
it is assumed to be incomplete. If a nested phrase occurs in
a lot of different contexts, the chance that it may constitute
a domain term increases.
A popular modification of the method was aimed at
extending the ranking procedure for phrases of the length
1 which originally all get a 0 value. For this purpose, the
logarithm of the length for one word phrases (used in the
original solution) was replaced with a non-zero constant.
In [10], where this method was applied to Spanish texts,
the authors initially set this constant to 0.1, but finally set
it to 1, arguing that otherwise one word terms would be
located too low on the ranking list.
Comparisons to other term extraction methods, per-
formed in [11] among others, showed that in the
biomedical domain termhood-basedmethods outperform
unithood-based methods where termhood is defined as a
degree that a linguistic unit is related to domain-specific
concepts, and unithood as a degree of strength or stabil-
ity of syntagmatic combinations and collocations [12]. In
[4] the C-value method, which is based on frequency mea-
sure, was judged to be better suited for term identification
than mutual information or the Dice Factor describing the
degree of association measures.
The C-value obtained using the equation cited above
reflects only the relationships between the terms them-
selves. The results can be improved on the basis of the
contexts in which the terms occur within texts. In [9]
it was suggested that appearing in the same context as
highly ranked terms should increase the rank of the candi-
date term. For example, the frequent statement niepraw-
id?owy twór abnormal formation is ranked high, while the
rare one nieprawid?owy cien´ abnormal shadow has much
lower a C-value. Both phrases occurred in the same sin-
gular context: stwierdzono found. On this basis, the low
mark of the second term can be increased. The idea is
realised by the NC coefficient which is counted according
to the following equation in which t is a candidate term,
Ct is a set of distinct contexts of t, ft(b) is the frequency of
b occurring as a context of t andweight(b) = t(b)/nwhere
t(b) is the number of terms the context word b occurs with
and n is the total number of the terms considered.
NC-value(t) = 0.8 ? C-value(t)
+ 0.2 ?
?
b?Ct
ft(b) ? weight(b) (2)
In the original solution contexts were just strings of
wordforms surrounding the given phrase within the text.
The authors of [10] proposed using lemmas of the sur-
rounding words instead of their forms for processing
Spanish, which has different forms of adjectives and nouns
according to number and grammatical gender.
Applying the C/NC scheme or another ranking proce-
dure we get an ordered list of the potential terms. We
expect that phrases which are not domain relevant or lin-
guistically incorrect are located low on this list and we
are not interested in the exact value of the C/NC coeffi-
cient of a particular term. Finally, a cut-off value according
to a coefficient value or a position on the list is chosen
at the final processing stage. A set of phrases which are
located above this cut-off constitute the final result of
the terminology extraction task. The different extraction
methods can be compared on the basis of a percentage of
the selected phrases judged as not being terms during the
evaluation stage.
Results and discussion
The term extraction procedure was conducted on two
sets consisting of discharge reports from two wards of a
Polish children hospital: the allergies and endocrine ward
(further referred to as o1) and the surgical ward. They
consisted respectively of about 78,000 tokens, and over
360,000 tokens. The analysed texts were very concise as
physicians reported only the most important facts there.
Thus, it occurred that the great majority of the extracted
nominal phrases were domain related. But not all of them
were equally useful for the given domain, and a shal-
low grammar also resulted in extracting some sequences
which were not correct phrases at all. Thus, the order-
ing of the results was still an important task. The C/NC
method proved able impose an ordering which located
important phrases at the beginning of the ranked list,
while incorrect phrases were moved towards its end.
The defined grammar together with the procedure of
identifying nested phrases identified more than 4100 dif-
ferent nominal phrases (nested or independent) in the o1
set, more than 7100 in the surgery set and more than
14150 in the both sets combined together. This means
that about 1350 of them occurred in both sets (about one
third of the smaller set). The number of phrases extracted
using the shallow grammar and the distribution of their
length and frequencies are given in Tables 1 and 2. About
20% of these phrases are singular words; the largest group
of phrases has two elements (38%) while only about 5%
have 5 or more words. The average phrase length is equal
to 2.5. More than half of the phrases occurred exactly
once, while less than 10% of them occurred more than
10 times.
Table 3 shows the distribution of the C-value. About one
third of phrases got a 0 value because they always had the
same context (within a phrase as its nested subphrase).
The remaining 70% of phrases contained correct clinical
terms located both at the top of the list as well as close
to the bottom of the list. Medical terms which occurred
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 4 of 14
http://www.jbiomedsem.com/content/5/1/24
Table 1 Distribution of phrase lengths
Phrase Data Common
length o1 surgery o1+surgery nb % from o1 in surg.
?
4156 11354 14156 1354 32.58
1 1381 2219 2880 720 52.14
2 1644 4212 5403 453 27.55
3 801 2941 3605 137 17.10
4 242 1301 1511 32 13.22
5 68 476 534 10 14.71
> 5 20 205 223 2 10.00
Max 12(8) 5(7) 12(8) 0 -
very few times in isolation got a very low positive C-
value, e.g. anestezjolog anaesthetist, torbielka small cyst.
They cannot be differentiated by the method from nouns
such as kwiat flower or chodnik pavement which also
occurred within the data. The positive effect of counting
occurrences of nested phrases can be observed for ostry
dyz?ur emergency service, for example, which occurred in
isolation only once, but was used 82 times in 6 different
contexts and classified in 148th place.
The answer to the question whether to count occur-
rences of nested phrases which never occur in isolation is
not clear. One of the examples of the successful recogni-
tion of such a term is kos´c´ ramienna humerus. Another
example is miedniczka nerki renal pelvis which also did
not occur in isolation but had 15 occurrences in 6 dif-
ferent contexts and was located in 705th place. However,
the strategy of promoting nested phrases on the basis of
the occurrences of the phrases they are part of, can some-
times lead to undesirable results. The phrase infekcja dróg
tract infection never occurred alone but had 11 differ-
ent contexts and was located very high (216) in spite of
being an incorrect (truncated) phrase. An extreme exam-
ple of such a phrase which gained a very high C-value is
karta informacyjna leczenia treatment information card
being a subsequence of the phrase karta informacyjna
leczenia szptialnego hospital treatment information card.
Table 2 Distribution of phrase frequencies
Phrase Data
freq o1 surgery o1+surgery
?
4156 11354 14156
=1 2272 7120 8211
210 1417 4076 4572
1150 325 922 969
51100 71 115 157
1011000 71 168 217
1000- 0 28 30
Table 3 Standard C-value distribution
Terms Data
freq o1 surgery o1+surgery
?
4156 11354 14156
C= 0 1110 3458 4163
C> 0 3046 7896 9993
0<C< 1 893 1509 1936
C= 1 565 1301 1708
C> 1 1588 5086 6349
1<C<= 2.5 898 2842 3531
C> 2.5 690 2244 2818
In surgical data it occurred 1164 times in this phrase and
once in a longer phrase poprzednia karta informacyjna
leczenia szpitalnego previous hospital treatment informa-
tion card. For the C-value counting algorithm this meant
there were two different contexts in which this phrase
appeared, and resulted in the sixth top value for a phrase
which did not occur in the data and is probably not used
at all.
The equation for C-value promotes sequences which
have different contexts but, in the case of nested phrases,
it may be possible that all these contexts describe a super
phrase. e.g. for klatkasubst (cage, case, frame) there are
several context super phrases like: klatkasubst pier-
siowaadj chest, USG klatki piersiowej chest ultrasound,
RTG klatki piersiowej chest RTG, zdje?cie klatki pier-
siowej chest picture, klatka piersiowa prawid?owa chest
normal, but all these are contexts for the term klatka
piersiowa chest and should not promote klatka as an
independent term. This word is ambiguous and is rather
rarely used alone with respect to klatka piersiowa chest.
The accepted solution (named as C1) relies on counting
super phrases which differ only in the words adjacent to a
given term.
The distribution of the C1-value is given in Table 4.
For the C1-value method the phrase: karta informacyjna
leczenia treatment information card, which occurred
Table 4 C1-value distribution
Terms Data
freq o1 surgery o1+surgery
?
4156 11354 14156
C= 0 2843 4140 4933
C> 0 2843 7214 9223
0<C< 1 775 1243 1625
C= 1 581 1339 1757
1<C<= 2.5 843 1487 3227
C> 2.5 644 2068 2614
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 5 of 14
http://www.jbiomedsem.com/content/5/1/24
only as the nested phrase and has only one context,
obtained the proper 0 C1-value. The proposed strategy,
however, did not eliminate all unfinished phrases and
yielded only a slight lowering of their score, e.g. from 28th
place down to 45th forUSG jamy USG of cavity in the list
for surgical data. The high ranking of this phrase on the
terminology list is a result of it being part of the follow-
ing two phrases: USGbrev:nw jamysubst:gen brzusznejadj:gen
(used 377 times alone and 51 as a nested phrase) and less
common USGbrev:nw jamysubst:gen brzuchasubst:gen (used 3
times alone). Both phrases have the same English equiv-
alent: USG of abdominal cavity. Moreover, the phrase
USG jamy was recognised once in isolation because of a
spelling error in the word brzusznej abdominal.
C1 coefficients are by definition usually lower than the
original C-values. However, the changes in the ranking
order are not very large. For o1 data, of the top 600 ele-
ments 20 received a C1-value equal to 0. Only two of
them were good medical terms, the rest were incomplete
phrases like the one described above and were correctly
suppressed. For surgical data, these extreme changes were
even smaller4 in 600 top phrases got a 0 C1-values, one
of them is a correct medical term. In the entire surgi-
cal data, 119 terms which had a non-zero C-value got a
0 C1-value, 46 of them were incorrect phrases. For the
previously given example, infekcja dróg, we got 4 contexts
instead of 11, the coefficient value was lowered by about
20%, but the position changed only by 20. Similarly, for
the very frequent phrase USG jamy the change, equal to
about 40% of coefficient value, resulted in a small change
in position (of 17 places).
In order to identify terminology that may not be related
to the medical domain, we compared the terminology
extracted from medical data with phrases extracted from
the general corpus of the Polish language (National Cor-
pus of Polish (NKJP) [13])processed and ranked using
the same tools. Then we compared terminology identi-
fied in NKJP and medical data: surgery and o1 separately.
Table 5 shows how many terms are recognised in both
corpora (NKJP and the medical one) and the number of
terms that have a higher C1-value in the NKJP data. This
comparison gives only a general overview as the sizes of
the compared corpora are different. The longest common
Table 5 Comparison with general corpus
Terms o1 Surgery
Common with NKJP 791 1155
1-word 680 969
Multi words 111 186
C1-value greater in NKJP 431 546
1-word 374 477
Multi words 57 69
phrase has four words and there is only one in both cor-
pora infekcja górnych dróg oddechowych upper respiratory
tract infection. Multi-word terms that have a C1-value
higher in the NKJP data account for about 2% of multi-
word terms for o1 data and less than 1% for surgery
data. Moreover, most multi-word terms with a higher
C1-value in NKJP are related to the medical domain,
e.g.: poradnia zdrowia psychicznego mental health clinic,
przewód pokarmowy gastrointestinal tract, oddzia? inten-
sywnej terapii intensive care unit. But, of course, there
are also terms that are common in everyday language like:
numer telefonu telephone number, drugie danie second
course or wycieczka autokarowa bus trip. The compari-
son shows that in hospital documents there are very few
phrases that are frequently used in the corpus of general
Polish. Moreover, the common phrases are usually related
to medicine. So, this stage turned out not to substantially
influence the results.
Finally we ordered the terms according to the C1/NC
method. Tables 6 and 7 shows the leading terms for both
data sets.
To check if the changes introduced by the NC correc-
tion method were significant we used the top 300 as a
set of terms whose contexts were taken into considera-
tion while calculating the NC coefficient. Unfortunately,
clinical notes mostly contain noun phrases and a lot of
terms just have punctuationmarks as their contexts. Thus,
reordering phrases according to the NC values did not
introduce many changes. In fact, most corrections only
caused a difference of no more than 20 places. The bigger
differences were seen only at the bottom of the list where
they are not very important, as usually, the end of the list
is not taken into account as a source of domain terms. The
possible explanation of this minor positive effect is the rel-
atively small size of the available data, as some phrases
from this 300 element list occurred little more than 15
times.
Manual evaluation
We performed two tests to evaluate the results of the
extraction procedure. The first test was aimed at checking
the completeness of the initial list of all considered nom-
inal phrases. It involved the manual identification of ter-
minology in documents and checking how many of these
terms were present in the full list of terms before truncat-
ing it. The o1 documents were approximately two times
longer, so we randomly selected two (1667 tokens) and
four (2074 tokens) documents for the evaluation respec-
tively. The test was performed by two annotators. The
results are given in Tables 8 and 9. As is evident from the
information in the tables, about 85% of phrases indicated
by the annotators are common for both of them. The lists
of extracted terms contain above 80% of phrases indicated
by the annotators.
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 6 of 14
http://www.jbiomedsem.com/content/5/1/24
Table 6 Top 20 phrases in o1 data
Phrase C1/NC Full Nested
karta informacyjna leczenia
szpitalnego hospital treatment
185.60 116 0
information card
morfologia krwi full blood
count
124.00 155 4
wynik badania examination
result
114.04 118 27
masa cia?a body mass 107.82 122 17
stan ogólny general
condition
102.66 75 62
uk?ad kielichowo-miedniczkowy
poszerzony widened
pyelocalyceal system
102.17 55 0
pediatria ogólna general
paediatrics
93.60 117 0
oddzia? alergologii allergy
ward
93.60 117 0
kod pacjenta patient code 92.80 116 0
USG jamy brzusznej ultrasound
of the abdominal cavity
92.14 66 10
lekarz prowadza?cy attending
physician
91.28 114 0
ordynator oddzia?u head of
hospital department
91.28 114 0
badanie ogólne general
examination
79.51 93 9
RTG klatki piersiowej chest
X-ray
78.14 52 12
nerka prawid?owej wielkos´ci
kidney of normal size
74.81 59 0
pe?cherzyk z?ó?ciowy prawid?owy
normal gall bladder
73.54 58 0
uk?ad kielichowo-miedniczkowy
pyelocalyceal system
69.35 4 59
pe?cherz moczowy wype?niony
filled bladder
62.56 42 11
klatka piersiowa chest 58.80 1 87
badanie examination 55.20 35 665
The second test indicated how many medical phrases
were at the top, in the middle and at the bottom of the lists
of terms ordered from the highest to the lowest score of
their C1/NC-value. The phrases were judged by the same
two annotators, as to whether they belong to the termi-
nology or not. The results of the evaluation are given in
Tables 10 and 11. In the top part of the lists, the great
majority of terms (about 88%) is judged to be domain
related by both annotators. The percentage of badly struc-
tured terms is below 10%. The proportion of badly struc-
Table 7 Top 20 phrases in surgical data
Phrase C1/NC Full Nested
karta informacyjna leczenia
szpitalnego hospital treatment
1862.40 1164 1
information card
oddzia? chirurgiczno-urazowy
surgical and casualty ward
1332.80 833 0
badanie ogólne general
examination
1030.95 1170 112
wynik badania examination
result
964.56 1167 43
oddzia? chirurgii surgical ward 943.26 1179 3
kod pacjent patient code 931.20 1164 0
zalecenie lekarskie medical
recommendation
924.80 1156 0
zastosowane leczenie applied
treatment
735.22 919 1
odp?yw pe?cherzowo-moczowodowy
vesicoureteral reflux
678.09 124 317
pe?cherz moczowy bladder 662.48 325 525
wskaz´nik protrombinowy
prothrombin ratio
609.60 762 1
stan ogólny dobry good general
condition
526.40 414 0
grupa krwi blood group 520.80 649 4
USG jamy brzusznej ultrasound of
the abdominal cavity
511.34 377 51
uk?ad kielichowo-miedniczkowy
pyelocalyceal system
508.30 67 267
karta informacyjna information
card
470.00 1 1173
wsteczny odp?yw
pe?cherzowo-moczowodowy
vesicoureteral reflux
468.70 238 14
leczenie szpitalne hospital
treatment
466.40 0 1166
stan ogólny general condition 430.81 222 422
nerka prawid?owej wielkos´ci kidney
of normal size
410.84 324 1
tured terms in the other two sets is evidently higher which
proves that the C/NC ranking method moves bad terms
toward the end of the list. However, as can be seen, even
the last section of the list contains 6082% of domain
terms.
Table 8 Phrases in o1 texts
1st annot. 2nd annot. Common
nb of phrases 241 235 208
nb of extr. phr. 199 190 175
% of extr. phr. 82.5 80.0 84.1
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 7 of 14
http://www.jbiomedsem.com/content/5/1/24
Table 9 Phrases in surgery texts
1st annot. 2nd annot. Common
nb of phrases 163 164 138
nb of extr. phr. 134 136 116
% of extr. phr. 82.2 82.9 84.0
Comparison with MeSH
MeSH is a controlled biomedical vocabulary that was cre-
ated to index articles from biomedical journals and to
make literature searches easier. Thus, for example, the
data contains the following terms: kidney and gallblad-
der but does not contain the phrases: left kidney or
normal gallbladder which are used in hospital documen-
tation but do not function as keywords in journal papers.
Experiments in applying MeSH to clinical data were done
for English [14] and Swedish [15], UMLS resources were
used for information extraction in French [16,17], German
[18], and Dutch [19]. A better source of data that contains
clinical terminology is SNOMED but it is not translated
into Polish. As there are no other publicly available elec-
tronic resources of Polish medical terminology we com-
pared the results obtained in the task with the terminology
represented in the Polish MeSH thesaurus. We performed
the experiment on the version available from http://
www.nlm.nih.gov/mesh/ updated in 2012 which contains
26581 main headings and 17638 synonyms. The data is
being created in the GBL (Central Medical Library) in
Warsaw.
The extracted terms have simplified base forms which
cannot be directly compared with the thesaurus that con-
tains terms in their nominative base form. There are three
possible solutions to this problem. The first one is to con-
vert the terminology from simplified base forms into cor-
rect grammatical phrases and check them in MeSH. The
second approach consists in converting MeSH data into
simplified base forms. The third approach is to compare
the simplified formswith data inMeSHusing approximate
string matching.
We tested the first and the last method described
above to perform a comparison of the top ranked sur-
gical ward terminology with the MeSH thesaurus. We
Table 10 Phrases considered as terms in o1 documents
C1/NC - o1
1st annot. 2nd annot.
Domain General Bad Domain General Bad
nb % nb % nb % nb % nb % nb %
top200 176 88 19 9.5 5 2.5 178 89 14 7 8 4
middle100 88 88 5 5.0 7 7.0 83 83 8 8 9 9
end100 75 75 18 18.0 7 7.0 82 82 10 10 8 8
Table 11 Phrases considered as terms in surgery
documents
C1/NC - surgery
1st annot. 2nd annot.
Domain General Bad Domain General Bad
nb % nb % nb % nb % nb % nb %
top400 353 88.3 28 7.0 19 4.7 348 87.0 27 6.7 25 6.3
middle200 136 68.0 11 5.5 43 21.5 145 72.5 14 7.0 41 20.5
end200 127 63.5 33 16.5 40 20.0 121 60.5 35 17.5 44 22.0
wanted to test only medical terminology so we selected
353 terms that underwent positive manual verification
by the first annotator. 52 terms (15%) are present in the
MeSH thesaurus in their exact form, while 90 (25.5%)
exact forms are nested in other terms. The method for
approximate string matching performed on the simpli-
fied forms increased the number of recognised terms to
106 (30%). 9 terms recognised by the method using exact
forms were not recognised by the last method. Almost
all these phrases contain gerunds whose lemma forms
differ significantly from the words, e.g: leczenieger szpi-
talneadj hospital treatment has a simplified base form
leczyc´ szpitalny. Finally, we tested the approximate string
matching method on the set of terms consisting of gram-
matical phrases. In this case 119 (34%) terms gave positive
results.
The results presented in this paper are worse than the
results discussed in the paper [20]. In that experiment
from 1987, manually extracted terminology from hospi-
tal documents was compared with the English MeSH. The
authors concluded that about 40% of these phrases were
present in MeSH. The results we obtained are even worse
and they show that the Polish MeSH is not large enough
for the evaluation of clinical terminology extracted from
hospital documentation, so in this task it cannot serve as
a source of normalised terminology.
Results for simplified grammar
Finally, we tested whether the precision of the extrac-
tion grammar influences the results. We performed an
experiment in which we changed the grammar used for
phrase identification in such a way that it relied only
on information about part of speech and did not take
into account gender, number and case agreement. Pol-
ish taggers are not very reliable in assessing detailed
values of morphological tags, especially for domain spe-
cific text, while preparation of correction rules is time
consuming. However, neglecting this information results
in the extraction of many phrases that are syntactically
incorrect. The experiment performed on the surgical
data resulted in obtaining 13591 candidates (compared to
11354). Although the results (see Table 12) obtained for
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 8 of 14
http://www.jbiomedsem.com/content/5/1/24
Table 12 Comparison of the results for different grammars
for surgery documents
C1 - surgery
Original grammar Simplified grammar
Domain General Bad Domain General Bad
nb % nb % nb % nb % nb % nb %
top400 353 88.3 28 7.0 19 4.7 350 87.5 19 4.75 31 7.75
next400 331 82.8 19 12.5 50 12.5 310 77.5 15 3.75 75 18.75
the first 400 terms were good  87.5% of terms were clas-
sified as domain related (in comparison to 88.3% obtained
with the original grammar), but in the next 400 places the
changes were more significant: only 77.5% of the terms
were domain related while 18.75% were badly structured
(82.8% and 12.5% for the original grammar). These results
confirm the hypothesis that better initial selection of can-
didates has a positive impact on the final results of the
chosen method of terminology ranking.
Conclusions
The analysis of the results obtained in the automatic
terminology extraction showed that the top part of the
terminology list contains phrases that refer almost unex-
ceptionally to the most frequent domain related concepts
described in the data. The extracted terms may help to
create a domain ontology and, most importantly, they
reflect the variety of phrases that are used in everyday
hospital practice. The method can be useful for preparing
terminological resources for very specific subdomains for
which no relevant databases already exist.
Clinical texts contain practically only domain specific
knowledge and almost all correct phrases extracted by the
grammar are domain related. Thus, the standard method
of filtering the results by comparing the occurrences of
phrases to their frequencies in the general corpora can-
not improve the results. As multiword expressions are less
likely to be ambiguous for some domains, general data
can be used as an additional source of information about
possible contexts.
The C-value approach turned out to be useful for rec-
ognizing terms being subsequences of other phrases. The
performed evaluation showed that none of the tested
ranking procedures were able to filter out all improp-
erly constructed noun phrases from the top of the list, so
the processing stage consisting in choosing noun phrases
turned out to be very important to the usefulness of the
created terminological resource.
In particular, the comparison of the obtained results
with manually extracted terminology from selected docu-
ments showed that proper morphological tagging is very
important to the selected approach. The application of the
NC part of the C/NC method to the clinical data does not
significantly change the order of terms, so the NC step is
not very useful if the aim is to collect all possible domain
related phrases, but can help in selecting those that are
most important in a particular domain.
Methods
Text characteristics
We analysed two sets of data containing hospital discharge
documents. They were collected from two wards of a
childrens hospital. The first set of data consisted of 116
documents (about 78,000 tokens) relating to patients with
allergies and endocrine diseases. The second data set con-
tained 1165 documents from a surgical ward (more than
360,000 tokens). The documents were originally written
in MS Word. They were converted into plain text files
to facilitate their linguistic analysis. During conversion,
information serving identification purposes was substi-
tuted with symbolic codes. The vocabulary of the clinical
documents is very specific, and significantly differs from
general Polish texts. Inmedical data there are many abbre-
viations and acronyms, some of them are in common
use: RTG X-ray or godz (godzina) hour, but many of
them are domain dependent. For example, por. in everyday
language means porównaj compare, but in the medical
domain it is more often the abbreviation for poradnia
clinic. Some abbreviations are created ad hoc, e.g., in
the phrase babka lancetowata ribwort plantain the word
lancetowata ribwort is abbreviated to lan or lanc. These
abbreviations cannot be properly recognised out of con-
text. Moreover, many diagnoses or treatments are written
in Latin, e.g., immobilisatio gypsea immobilisation with
gypsum.
Another problem in analysing clinical data is misspelled
words. As the notes are not meant to be published, the
texts are not very well edited. Despite the spelling cor-
rection tool being turned on, some errors still occurred,
mainly in words missed from the standard editor dic-
tionary like echogenicznos´ci echogenicity misspelled as
echiogenicznos´ci, echogenicznosci and echogenicznos´a?ci.
Grammatical errors are infrequent but most utterances
are just noun phrases, not complete sentences. Thus, our
observations concerning the overall linguistic character-
istics of Polish clinical data are consistent with those
described by Kokkinakis and Thurin for Swedish [15].
The first level of the linguistic analysis of data is its
segmentation into tokens. At this level we distinguish:
words, numbers and special characters. Words and num-
bers cannot contain any special characters. Words may
contain digits, but they do not start with digits. So,
the string 12mm is divided into 2 tokens: 12number
and mmword, while the string B12 is treated as one
word.
In the next step of data processing we annotated the
data with morphological information. Each word was
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 9 of 14
http://www.jbiomedsem.com/content/5/1/24
assigned its base form, part of speech, and complete mor-
phological characteristics. The annotation is done by the
TaKIPI tagger [21] that cooperates with the Morfeusz
SIAT morphological analyser [22] and the Guesser mod-
ule [23] that suggests tags for words that are not in the
dictionary.
To correct Guessers suggestions and some systematic
tagging errors, we manually prepared a set of global cor-
rection rules that work without context, see [24], so they
were only able to eliminate some errors, e.g. replace very
unlikely interpretations of homonyms. We also prepared
a list of the most common abbreviations, which were
assigned the appropriate full form as their lemma. Finally,
we (automatically) removed improperly recognised sen-
tence endings after abbreviations, and added the end of
sentence tags at the ends of paragraphs.
Phrase selection
In this work we decided only to analyse nominal phrases
and put verbal constructions aside. The internal syntac-
tic structure of nominal phrases that constitute terms
can vary, but not all types of nominal phrases structures
are likely to characterise terminological items. In Polish,
domain terms most frequently have one of the following
syntactic structures:
 a single noun or an acronym, e.g. angiografia
angiography, RTG X-ray;
 a noun followed (or, more rarely, preceded) by an
adjective, e.g. granulocytysubst oboje?tnoch?onneadj
neutrofils, ostryadj dyz?ursubst emergency service;
 a sequence of a noun and another noun in genitive,
e.g. biopsjasubst:nom tarczycysubst:gen biopsy of thyroid;
 a combination of the last two structures, e.g.
gazometriasubst:nom krwisubst:gen te?tniczejadj:gen
arterial blood gasometry.
The syntactic rules become more complicated as one
wants to take additional features of Polish nominal
phrases into account:
 word order: as Polish is a relatively free order
language, order of phrase elements can vary;
 genitive phrase nesting: the sequences of genitive
modifiers can have more than two elements, e.g.
wodonerczesubst:nom niewielkiegoadj:gen
stopniasubst:gen dolnegoadj:gen uk?adusubst:gen
podwójnegoadj:gen nerkisubst:gen prawejadj:gen mild
hydronephrosis of the duplicated lower collecting
system of the right kidney;
 coordination: some terms include coordination (of
noun or adjectival phrases), eg. USG naczyn´ szyjnych
i kre?gowych ultrasound of the carotid and vertebral
vessels, zapalenie mózgu i rdzenia inflammation of
brain and medulla;
 prepositional phrases: there are also terms like
witaminy z grupy B vitamins of the B group which
include prepositional phrases inside.
In our work we account for all of the nominal phrase
types described above, except those including preposi-
tional phrases and nominal coordination. To recognise
them, we defined a shallow grammar consisting of a cas-
cade of six sets of rules being regular expressions. The
rules operate on the data annotated with a part of speech
and the values of morphological features. The results
obtained by applying a set of rules on one level were used
as the input for the subsequent set. The rules are cited in
Table 13 in a format slightly modified for this presenta-
tion; in particular, this format does not include the output
Table 13 The sets of rules for recognizing noun phrases
Set Rules
I N subst | ger
NC (foreign_subst | foreign) +foreign?+foreign?
NC brevnpun,nw| brevnpun,nw
NC brevpun,nw + .? | brevpun,nw + .?
NC brevnpun,nphr| brevnpun,nphr
NC brevpun,nphr + .? | brevpun,nphr + .?
AJ 2 adv?+(adjC,G,N| ppasC,G,N)
AC brevadjw,npun| brevadjw,pun + .?
CN i
II A AJ+adv?
A3 AC + - + AJC,G,N
A3 adja + - + AJC,G,N
AC AC + -+AC
N NC,G,N+AJC,G,N| AJC,G,N+NC,G,N
NZ subst(lemma=to/co/obra?b/kierunek/cel/czas/
moz?liwos´c´/podstawa/cia?g/cecha/...)
AZ IR(lemma=aktualny/daleki/gdy/pewien/wzgla?d/
ten/inny/sam/niektóry/wczesny/...)
III ADJP A
ADJP AC,G,N?+AC,G,N?+AC,G,N?+AC,G,N?+CN+AC,G,N
ADJP AC,G,N?+AC,G,N?+AC,G,N?+AC,G,N+AC?
IV NB2 NC+ADJP
NB2 AC+N
NB N+AC
NB ADJPC,G,N+NC
NB ADJPC,G,N?+NC,G,N+ADJPC,G,N?
V NG NBgen?+NBgen?+NBgen?+NBgen?+CN?+NBgen
NG NB
VI X NG+NGgen?+ADJPC,G,N?
X NG+NC
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 10 of 14
http://www.jbiomedsem.com/content/5/1/24
part of the rules. Indexes describe values of morphological
features. Names in lowercase correspond to the respective
feature values, capitalized names correspond to variables
referring to case (C, C2), gender (G, G2) or number
(N, N2).
The Polish tagset is quite detailed (over 1000 actually
used tags) and contains around 30 word classes. This set,
for our purposes, was extended by the foreign tag used
for Latin or English words used in discharge summaries.
Words which can build up a nominal phrase can be from
one of the following categories: subst (noun), ger (gerund),
foreign_subst, foreign, and brev:pun:nw, brev:pun:nphr,
brev:npun:nw, brev:npun:nphr (abbreviation/acronym of a
noun or noun phrase requiring or not requiring a period
afterwards). The first two types of these core elements
inflect and they are assigned to the N class. Foreign words
and abbreviations do not inflect but they can also be mod-
ified by adjectives. These words cannot be a source of
gender, number or case values and are assigned the cate-
gory NC. Foreign names frequently consist of more than
one element, so sequences of up to three foreign words
are also accepted by the grammar (we do not analyse
the internal structure of Latin or English sequences). The
first set of rules also includes rules for identifying basic
adjectivesinflective (AJ) and non-inflective (AC) which
can possibly be modified by adverbs. The X notation is
used to mark cases in which the morphological descrip-
tion of the resulting phrase should be copied from the Xth
element of the rule and not from the first one (e.g. case,
gender and number of an adjective phrase consisting of an
adverb and an adjective should be the same as those of the
adjective).
In the second set of rules, adverbs can be attached to
adjectives which are in front of them (but only if there is
no adjective after themthis more preferable attachment
is covered by the first set of rules). There are also rules for
special types of Polish complex adjectivesconstructions
like pe?cherzowo-moczowodowy vesico-ureteric contain-
ing a special form of an adjective ending with -o followed
by a hyphen and an adjective. The last two rules of the sec-
ond set are defined specially for the procedure of nested
phrases borders identification procedure (special rules
are responsible for not constructing nested phrases which
include adjectives but do not include the nouns they
modify).
The third set of rules describes compound adjecti-
val phrases, the fourth one combines adjectival phrases
with nouns, the fifth describes sequences of genitive
modifiers, and the last one combines genitive modifiers
and optional adjectival modifiers which can occur after
genitive ones. There is also a rule which allows for a
non-inflective noun as a last phrase element. This rule
accounts for acronyms used at the end of noun phrases,
but it turned out that due to the lack of punctuation
it was responsible for recognising improperly structured
phrases.
Applying such a general set of rules to our data would
result in a subset of phrases which we considered non-
domain terms. These were phrases beginning with mod-
ifiers describing that a concept represented by a subse-
quent nested phrase was occurring, desired or expected,
e.g. (w) trakciesubst choroby during illness. To eliminate
such phrases we defined a set of words which were to be
ignored during phrase construction. Rules for recognis-
ing them (and assigning NZ or AZ category) were added
to the first set. These words belong to the following three
classes:
 general time or duration specification, e.g. czas time,
miesia?c month;
 names of months, weekdays;
 introductory/intension specific words, e.g. kierunek
direction, cel goal, podstawa base, cecha feature
(22 words more).
In the results presented in this paper, only some types
of normalisation of the extracted terms described in [6]
are completed. We recognise morphological variants of
terms. Domain abbreviations and acronyms that have a
unique interpretation were extended and thus matched
with their full versions. This cannot always be done in
a straightforward manner, as there are many abbrevi-
ations/acronyms that can be correctly interpreted only
in context. Moreover, discharge documents do not con-
tain definitions of abbreviations or acronyms, and many
acronyms are created from English phrases (e.g. MCV
Mean Corpuscular Volume) so it is impossible to adapt
the method proposed in [25] for acronym recognition,
which was based on analysing acronym definitions.
Identification of nested phrases and term weighting
In order to apply the C-value method, the operation
of identifying phrases nested within other phrases is
crucial. In our solution, borders of nested phrases are
introduced by the grammar. As a nested phrase we take
every fragment of a nominal phrase which is recog-
nised by any of the grammar rules as being a noun
phrase itself. For example, pe?cherzyksubst z?ó?ciowyadj gall
bladder usually occurs with an adjective describing its
condition e.g, pe?cherzyksubst z?ó?ciowyadj prawid?owyadj
normal gall bladder, or kos´c´subst ramiennaadj humerus
occurs with information indicating the left or right
side. Recognising the first exemplary phrase results
in identifying two candidates: pe?cherzyksubst z?ó?ciowyadj
prawid?owyadj and pe?cherzyksubst z?ó?ciowyadj but not
z?ó?ciowyadj prawid?owyadj as this is not a noun phrase.
The original work in which the C/NC method was pro-
posed concerned Englisha language with little inflec-
tion and a rather stable noun phrase structure. Thus,
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 11 of 14
http://www.jbiomedsem.com/content/5/1/24
the authors did not have to pay a lot of attention to
defining how they compared phrases and counted the
number of different contexts. They compared word forms.
However, for highly inflectional languages, like Polish, dif-
ferent forms of a word can vary significantly, making a
decision on term equality harder. Because of this, find-
ing repeated nested phrases also cannot be done by just
matching the strings. For example, the following nomi-
nal phrase in the nominative (which is traditionally con-
sidered a basic form): zakaz?eniesubst:gen wirusemsubst:dat
grypysubst:gen influenza virus infection is written in the
genitive as: zakaz?eniasubst:gen wirusemsubst:dat grypysubst:gen
influenza virus infection. In this latter phrase we ought
to recognise the term zakaz?enie wirusem grypy and three
nested phrases: wirus grypy, wirus and grypa. None of
them directly matches the considered phrase. The first
one matches the basic (nominative) form, but the nomi-
native form of the nested phrases does not match either
the genitive or nominative form of the entire phrase. This
proves that lemmatisation of the entire phrase does not
solve the problem.
To overcome this difficulty we decided to transform
the identified phrases into simplified base forms, being
sequences of lemmas of phrase elements. In the cited
example, such a simplified lemma is: zakaz?enie wirus
grypa infection virus influenza. In this sequence all the
above nested terms (converted into their simplified base
forms) can be found easily.
Our approach is much simpler and more robust than a
formally correct one. It allows not only for easier recog-
nition of nested phrases but also helps in cases where
establishing a correct basic form can be difficult for shal-
low rules. For example, the correct lemma for the phrase
okresowegogen badaniagen ogólnegogen moczugen should be
okresowe badanie ogólne moczu periodic general exam-
ination of urine (periodic urinalysis) but could possibly
also (syntactically) be okresowe badanie ogólnego moczu
periodic examination of general urine. Introducing arti-
ficial base forms we avoid this difficulty. Simplified base
forms allow us also to join phrases with various abbrevi-
ations of the same word like babka lan and babka lanc
with their full formbabka lancetowata ribwort plan-
tain (from patch tests). As proper lemmatisation of all
phrases is also prone to tagging errors, our approach is
much easier and more robust than a formally correct one.
The lemmatisation approach explained above means
that sometimes semantically different phrases have the
same simplified base forms.
This may happen due to:
 phrases with genitive modifiers occurring in different
numbers e.g. zapalenie ucha ear inflammation and
zapalenie uszu ears inflammation are both
converted into the singular;
 the adjectives in different degrees (small, smaller)
having the same base forms, e.g. miednica ma?a small
pelvis (more frequently written as ma?a miednica
where ma?a small refers to its size) and miednica
mniejsza (mniejsza smaller indicates anatomic part)
lower pelvis;
 negated and positive forms of adjectival participles,
e.g. powie?kszony/niepowie?kszony increased/not
increased, both have the lemma powie?kszyc´ inf
increase.
 gerunds and participles having infinitives as their
base forms, so e.g.: phrases usunie?cieger
kamieniasubst:gen removing stone (an operation) and
usunie?typpas kamien´subst:nom removed stone
(description of the stone) have the same simplified
base form usuna?c´inf kamien´subst .
After normalisation of the recognised phrases consist-
ing in their transformation into simplified forms we have
to decide on a way of differentiating contexts. The C-value
coefficient greatly depends on the way for counting the
number of different contexts in which a nested phrase
occurs. In comparison to [9], we introduced slight modifi-
cations to the way of computing this number. In the orig-
inal solution all different sequences consisting of different
initial words and different final words were counted. For
example, if we consider a set of four terms:
 powie?kszenie [we?z?ów ch?onnych] lymph nodes
enlargement
 powie?kszenie [we?z?ów ch?onnych] krezkowych
mesenteric lymph nodes enlargement
 znaczne powie?kszenie [we?z?ów ch?onnych]
significant lymph nodes enlargement
 powie?kszenie [we?z?ów ch?onnych] szyji neck lymph
nodes enlargement
the number of context types for we?z?ówsubst:pl:gen
ch?onnychadj:pl:gen lymph nodes would be four. But this
method of context counting obscures the fact that the
close context of we?z?ów ch?onnych does not change that
much. To account for this phenomenon, one may count
only the one word context of any nested phrase.
While choosing this option one has still many pos-
sibilities to combine right and left contexts. We tested
three approaches: the first one was to count pairs of left
and right full contexts combined together; in the second
approach we counted different words in both left and
right contexts grouped together. However, the best results
were obtained for the third option in which we took the
maximum from different left and right words contexts
counted separately. So, in the above example, the left con-
text is empty as the same word powie?kszenie enlargement
appears in all phrases. This version is called C1. For our
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 12 of 14
http://www.jbiomedsem.com/content/5/1/24
example the number of different contexts calculated using
these methods would be accordingly:
4: powie?kszenie, powie?kszenie-krezkowych,
znaczne-powie?kszenie, powie?kszenie-szyji ;
3: powie?kszenie, krezkowych, szyji ;
2: krezkowych, szyji.
We counted the C-value for all phrases including those
of length 1. However, we set l(p) in the equation (1) to 0.1
not to 1 like [10]. We observed that although one word
terms constituted only 19% of the first 1000 terms in the
o1 data, while on the entire list there were 33% of them
(14% and 19% respectively for surgical data), many of the
one word terms occurred only once (34% and 37% respec-
tively). Setting l(p) for one word phrases to 1 result in 46%
of the first 1000 terms to be of length 1.
For the results obtained using the C1 coefficient, we
applied the full C/NC method to take the external terms
context into account. For calculating the NC coefficient
we used one word contexts which were adjectives, nouns
and verbs which occurred immediately before or imme-
diately after any term which was in the top 300 positions
according to its C-value coefficient.
Depending on the goal, requiring the imposition of
greater stress on the recall or precision of the results, the
smaller or larger top part of the list ordered by the NC
value can be taken as a resulting terminology resource.
Manual evaluation
The manual evaluation was performed by two annotators:
one was a paediatrician specialising in allergology and pul-
munology, the secondwas involved in the experiment, had
a computer background and had experience in linguistic
and medical data processing.
The two annotators were only given very general
instructions to mark a phrase which they thought of as
being important in clinical data and which did not include
prepositions. The basic problem of this task was to decide
what kind of phrases constituted terminology. Sometimes
only the boundaries of the phrase indicated by the annota-
tors were different, e.g: in the phrase na ca?ym ciele on the
whole body only cia?o body was recognised by the first
annotator, while the second annotator included the word
ca?e whole. Moreover, both annotators had a tendency
to indicate phrases that contained coordinations of nouns
which were not covered by the grammar, e.g:Wyniki pod-
stawowych badan´ morfotycznych i biochemicznych krwi i
moczu The results of basic morphotic and biochemical
blood and urine examinations. The first annotator recog-
nised 42 terms in the o1 data that were absent from the
automatically prepared list for the following reasons: lack
of grammar rules recognising the coordination of nom-
inal phrases  6 errors; lack of other grammar rules 
8; tagging errors  11; problems with rules containing
abbreviations and their tagging  10; phrases contain-
ing time expressions and introductory/intension specific
words (e.g: week, goal, direction)  6.
For the second evaluation experiment for the o1 data
we took the top 200 terms, and randomly selected 100
terms from the middle of the list (C1/NC-value ? (1.0,
2.5 ?) and 100 from the bottom part of the list (C1/NC-
value ? ?0.0, 1.0?). For surgery data we evaluated the 400
topmost terms and 200 terms from the middle and bot-
tom part of the lists. Then, the phrases were judged by
the same two annotators, as to whether they belonged to
the terminology or not. Not all phrases from the top part
of the lists were classified as terms. Despite attempts to
eliminate semantically odd phrases like USG jamy USG
of cavity and infekcja dróg infection of tract (only in the
o1 data) they still appear in the top part of the lists as
they are often in the data and cavity and tract are part
of several well established phrases. Another problem was
caused by abbreviations attached to correct phrases like
uraz g?owy S head injury S where S is a part of the ICD-
10 code of the illness S00 written with a space between
S and 00. Our grammar does not exclude such contrac-
tions as it is possible that an abbreviation is at the end of
a phrase, e.g: kontrolne badanie USG control ultrasound
examination.
Comparison of simplified terms with MeSH
Below we describe three possible solutions for comparing
our list of simplified base forms of terms with terminol-
ogy in MeSH that contains correctly structured nominal
phrases in the nominative case. We applied the first and
the last method of term forms matching as described
below.
The first one is to convert the terminology from sim-
plified base forms into correct grammatical phrases and
check them in MeSH. We have to take into account
that the general Polish morphological dictionary does not
recognise about 18.8% of word-tokens in clinical data,
see [24]. In general, the automatic generation of correct
base forms from simplified ones is error prone, but the
construction of medical phrases is more restricted than
for literary language so the results are better. We per-
formed this task with the help of phrases extracted from
clinical data, in which we identified fragments that are sta-
ble like genitive complements. This solution significantly
decreases the role of unknown words. For example in the
phrase wirussubst:sg:nom Epsteinasubst:sg:gen-Baarsubst:sg:gen
Epstein-Barr virus the part Epsteinasubst:gen-Baarsubst:gen
has the same form in all inflected forms of the whole
phrase. So it is possible to copy this part from the phrase
extracted from the data. We have to take into account
that some of the terminology in Polish MeSH is nom-
inal phrases in the plural, e.g. the above phrase is in
Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 13 of 14
http://www.jbiomedsem.com/content/5/1/24
plural form in MeSH:Wirusysubst:pl:nom Epsteinasubst:sg:gen-
Baarsubst:sg:gen Epstein-Barr viruses. This problem can be
overcome by generating both singular and plural forms.
This will account for medical plurale tantum phrases like
drogisubst:pl:nom moczoweadj:pl:nom urinary tract that now
are improperly lemmatised to a phrase in the singular
drogasubst:sg:nom moczowaadj:sg:nom.
We converted the selected 353 terms into their correct
base forms. For the following 11 terms, their base forms
were corrected manually as they were unknown to the
morphological dictionary and should be inflected: uro-
dynamiczny urodynamic, przype?cherzowy paravesical,
detromycynowy chloramphenicol and podpe?cherzowy
bladder outlet and compound words pe?cherzowo-
moczowy vesicoureteral (4 terms) and miedniczkowo-
moczowodowy pelvi-ureteric (3 terms).
The second approach consists in converting MeSH data
into simplified base forms. This method also has disad-
vantages as 42% of words contained in MeSH are not
represented in the general Polish dictionary that we used
for the annotation of our data and which was used to
annotate the NKJP corpus [13]. Converting MeSH termi-
nology into simplified base forms does not solve all prob-
lems either. For example, Polish MeSH does not contain
the phrase: chirurgiasubst naczyniowaadj vascular surgery
but it contains zabiegisubst chirurgiczneadj naczynioweadj
vascular surgery operations. The English equivalent of
the last phrase contains the first phrase but this is not
true of the Polish version. The simplified form of the
first phrase chirurgia naczyniowy is not contained in the
simplified version of the last phrase zabieg chirurgiczny
naczyniowy as the strings chirurgia and chirurgiczny are
different.
The third approach is to compare the simplified forms
with data in MeSH using approximate string matching.
To apply this method we perform a sort of stemming by
removing suffixes indicating cases of nouns and adjec-
tives. Then we apply the Levenshtein distance measure
which takes into account the position of a non-matching
letter in the analysed word. Words are more similar if dif-
ferences are found nearer to the end of the word than
to the beginning. For each word from a phrase in ques-
tion we find a set of similar words. Then we look for
MeSH terms that contain one similar word for each phrase
element.
Abbreviations
adj: Adjective; brev: Abbreviation; ICD: International Classification of Diseases;
gen: Genitive; ger: Gerund; MeSH: Medical Subject Headings; NKJP: National
Corpus of Polish; nom: Nominative; nphr: Noun phrase; npun: No punctuation;
nw: Noun word; pl: Plural; pun: Punctuation; POS: Part of Speech; sg: Singular;
SNOMED: Systematized Nomenclature of Medicine; subst: Substantive; UMLS:
Unified Medical Language.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MM performed data pre-processing, developed the evaluation scheme of the
results, and scrutinised the entire experiment. AM defined the shallow
grammar, implemented C/NC methods, and took part in the evaluation. The
whole paper was written and corrected by both authors. Both authors read
and approved the final manuscript.
Acknowledgements
The research was supported partially by the POIG.01.01.02-14-013/09 project
which is co-financed by the European Union under the European Regional
Development Fund. The authors would like to thank Beata Gosk (pediatrician)
for consultations and taking part in the evaluation of the terminology, and
Piotr Rychlik for implementing the terminology comparison functions and the
Levenshtein distance measure to the Polish MeSH. This article has been
published as part of the Semantic Mining of Languages in Biology and
Medicine (SMLBM) thematic series of the Journal of Biomedical Semantics. An
initial version of the article was presented at the 5th International Symposium
on Semantic Mining in Biomedicine.
Received: 1 July 2013 Accepted: 13 May 2014
Published: 31 May 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27
http://www.jbiomedsem.com/content/5/1/27RESEARCH Open AccessWhy functions are not special dispositions:
an improved classification of realizables for
top-level ontologies
Johannes Röhl1 and Ludger Jansen2*Abstract
Background: The concept of function is central to both biology and technology, but neither in philosophy nor in
formal ontology is there a generally accepted theory of functions. In particular, there is no consensus how to
include functions into a top-level ontology or whether to include them at all.
Methods: We first review current conceptions of functions in philosophy and formal ontology and evaluate them
against a set of criteria. These evaluation criteria are derived from a synopsis of theoretical and practical
requirements that have been suggested for formal accounts of functions. In a second step, we elucidate in
particular the relation between functions and dispositions.
Results: We argue that functions should not be taken as a subtype of dispositions. The strongest reason for this is
that any view that identifies functions with certain dispositions cannot account for malfunctioning, which is having
a function but lacking the matching disposition. As a result, we suggest a cross-classification of realizables with
dispositions supervening on the physical structure of their bearer, whereas both functions and roles also have some
external grounding. While bearers can survive the gain, loss and change of roles, functions are rigid properties that
are essentially connected to their particular bearers. Therefore, Function should not be regarded as a subtype of
Disposition; rather, the classes of functions and dispositions are disjoint siblings of Realizable.
Keywords: Function, Disposition, Role, Process, Realizable, Artefacts, Top-level ontology, BFOBackground
The ascription of functions is central to biology as well
as to psychology, technology and engineering. However,
realizable entities like functions, dispositions and roles
are notoriously difficult to understand and there is no
consensus how to model them within a top-level ontol-
ogy. The more general debates in the philosophy of
biology and technology also offer several theories of
function with their respective advantages and shortcom-
ings. Because of the diversity, plurality and ambiguity of
function concepts in, e.g., engineering, some authors
have claimed that there are many different function con-
cepts which are only connected by family resemblances
[1]. Even if this is true, it will nevertheless be useful for the
representation of scientific statements about functions to* Correspondence: ludger.jansen@uni-muenster.de
Equal contributors
2Philosophisches Seminar, Universität Münster, 48143 Münster, Germany
Full list of author information is available at the end of the article
© 2014 Röhl and Jansen; licensee BioMed Cen
Creative Commons Attribution License (http:/
distribution, and reproduction in any medium
Domain Dedication waiver (http://creativecom
article, unless otherwise stated.focus on some of the more important uses of the word and
fix these within a formal ontological framework. One par-
ticular challenge is whether there can be an overarching
meaning of the term function both for biological and arte-
factual functions.
Another challenge we will address concerns the rela-
tionships between functions and other kinds of entities
like dispositions and roles in formal ontology. That these
relations are not at all clear is witnessed by BFO, the
Basic Formal Ontology [2,3]: BFO versions up to 1.1.1
contain the categories Disposition, Function and Role as
jointly exhaustive and pairwise disjoint children of the
category Realizable, but in the transition to the new
version BFO 2 it is planned to position Function as a
subtype of Disposition (cf. Table 1 for more details).
In this paper, we will try to meet these challenges. For
this purpose, we will review different philosophical the-
ories of functions and evaluate them with respect to a
set of desiderata for function theories. In the remaindertral Ltd. This is an Open Access article distributed under the terms of the
/creativecommons.org/licenses/by/4.0), which permits unrestricted use,
, provided the original work is properly credited. The Creative Commons Public
mons.org/publicdomain/zero/1.0/) applies to the data made available in this
Table 1 Definitions of the children of realizables in BFO
1.1.1 and 2 (Graz release)
Definition in BFO 1.1.1 [2] Definition in BFO 2 [9]
Disposition = A realizable entity
that essentially causes a specific
process or transformation in the
object in which it inheres, under
specific circumstances and in
conjunction with the laws of
nature. A general formula for
dispositions is: X (object) has the
disposition D to (transform, initiate
a process) R under conditions C.
b is a disposition means: b is a
realizable entity & bs bearer is
some material entity & b is such
that if it ceases to exist, then its
bearer is physically changed, &
bs realization occurs when and
because this bearer is in some
special physical circumstances,
& this realization occurs in virtue
of the bearers physical make-up.
Function = A realizable entity the
manifestation of which is an
essentially end-directed activity of a
continuant entity in virtue of that
continuant entity being a specific
kind of entity in the kind or kinds
of contexts that it is made for.
A function is a disposition that
exists in virtue of the bearers
physical make-up and this physical
make-up is something the bearer
possesses because it came into
being, either through evolution
(in the case of natural biological
entities) or through intentional
design (in the case of artefacts),
in order to realize processes of a
certain sort.
Role = A realizable entity the
manifestation of which brings
about some result or end that is
not essential to a continuant in
virtue of the kind of thing that it
is but that can be served or
participated in by that kind of
continuant in some kinds of
natural, social or institutional
contexts.
b is a role means: b is a realizable
entity & b exists because there
is some single bearer that is in
some special physical, social, or
institutional set of circumstances
in which this bearer does not
have to be & b is not such that,
if it ceases to exist, then the
physical make-up of the bearer
is thereby changed.
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 2 of 16
http://www.jbiomedsem.com/content/5/1/27of this section, we will give some background informa-
tion on the current state-of-the-art representation of
functions, reviewing work on this topic published by the
research groups that developed the top-level ontologies
BFO, DOLCE and GFO [2,4,5].
Functions as realizables in BFO
In the older versions of BFO, Function, Disposition and Role
are sibling subclasses of the class Realizable dependent
continuant [6]. This common superclass implies that in-
stances of any of these three classes share the following
characteristics:
 They are continuants, i.e. they are wholly present at
every time of their existence.
 Like qualities, they are (specifically) ontologically
dependent on an independent continuant (some
material thing or system) that is their bearer.
 They are realizable, i.e. they are by definition
connected to certain types of processes such that
instances of such a process type can be realizations
of the realizable entity in question.
 When they are realized, their bearers are participants
of their realization processes, i.e. of the processes they
are roles, dispositions or functions for.Note that realizables do not need to be (always or
ever) realized [7], as, e.g. in the case of a safety mechan-
ism, the function of which will only be realized if certain
conditions obtain (and they may never obtain). Also note
that several other types of realizables are conceivable
like, e.g., propensities, tendencies, abilities, capacities,
virtues and vices [8]. Arp and Smith [6] conceptualise
the specific differences between functions, roles and dis-
positions as follows (cf. Table 1): The realizations of
functions and dispositions take place in virtue of the
bearers physical makeup, whereas a role is optional:
It does not reflect the intrinsic structure of their bearer
but a natural, social or institutional set of circum-
stances. Functions are distinguished from dispositions
by the additional condition that the function bearer pos-
sesses the physical structure that grounds the function
because of how it came to be there in the first place: In
the case of artefactual functions by intentional design
and production or in the case of biological functions by
a history of evolutionary selection. In BFO 2 the relation
of functions and dispositions was revised: functions are
now a subclass of dispositions [9], as detailed in Table 1.
Although we sympathize with BFO as a top-level ontol-
ogy and will later on use its other categories, especially
the fundamental disjoint classes of continuants and
occurrents and their relations, we find that its treatment
of functions is in need of some improvement and clarifi-
cation and we will later suggest a way to do so.
Functions and flows in DOLCE
While the top-level ontology DOLCE [4] neither in-
cludes functions nor dispositions or roles in its core ver-
sion, there are several suggestions for a formalisation of
engineering functions within the DOLCE framework
[10-12]. However, their formalisation starts from a very
specific technical approach that focuses on flows of
materials, energy or signals. Functions are then, basic-
ally, what relates certain input and output flows. While
being useful in engineering, this approach is mostly or-
thogonal to the debates on the functions of biological
entities. One main goal of these authors seems to be the
integration of the flows in the DOLCE ontology and
they accordingly classify different sorts of flow as various
types of process-like entities in DOLCE (states, pro-
cesses etc.) and characterise their dependencies and
relationships with the continuants involved (which are
called endurants in DOLCE). The relationships be-
tween functions, dispositions and roles that concern us
in the present paper are not discussed. We will not dis-
tinguish between different types of processes as inputs
or outputs for functions for two reasons: First, DOLCEs
subtypes of processes are heavily description-relative and,
hence, linguistic artefacts. Second, this model is not quite
as natural for biology as for applications in engineering. We
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 3 of 16
http://www.jbiomedsem.com/content/5/1/27can state that the function of the eye is to see without being
able to specify input and output flows. The point is not that
one could not conceptualise seeing in terms of flows of
visual and neurological signals, but that it is not necessary
to understand the function of biological organs in these
terms. Consequently, analyses of functions in the philoso-
phy of biology do not normally refer to input and output
flows (though it can be argued that such ideas are highly
relevant to certain strands of psychology or functional the-
ories of the mind). Let us go through an example: The
function of an animals legs is locomotion, so the actual
locomotive process of the animal is the realization of the
function and this motion could also be called an output.
But what is the input? Is it some neurological signal from
inside the animal or some stimulus from the environment?
Or is it a flow or change of energy in some cell or neural
pathway? In order to understand that the function of legs is
locomotion, the question about the input seems to be ir-
relevant. In any case, such a fine-grained approach with re-
spect to input or output flows does not add anything to the
main question of our paper. We take up this point again in
the Methods section.
Work from the GFO-Group
An elaborate and somewhat complicated model of func-
tions has been developed by researchers from the
OntoMed group that has produced the General Formal
Ontology (GFO) [13,14]. According to GFO, like in
BFO, functions have realizations which are usually pro-
cesses, but unlike in BFO, these realizations could also
be continuant entities. Burek et al. characterise a func-
tion by three function determinants, in particular:
(1) the preconditions for the realization of this
function,
(2) the goal affected or brought about by the function
(or its realization, respectively),
(3) the functional item which corresponds to the role
of the material bearer of the function as the
participant in the realization of the function.
In addition to functions and realizations, they intro-
duce realizers. A realizer is the actual particular entity
that plays the role of the functional item. Furthermore,
they distinguish the realization of a function (usually a
process) from the goal which is a state of the world
that is reached by means of the realization starting from
the precondition requirements. The authors illustrate
these components using the example of oxygen trans-
port in the human body. The realization of this function
is an actual process of oxygen transportation and it has
as precondition the presence of some oxygen at location
A and as goal state the presence of some oxygen at
location B. The functional item is denoted by thenominalised role term oxygen transporter and in the
case of oxygen transports in the human body this role
is played by instances of red blood cells, which are,
thereby, realizers of this function. Thus, according to
this approach, roles are, to some extent, always in-
volved when we deal with functions and roles are expli-
citly dependent on role contexts. However, note that
these roles are different from BFO roles and should
not be confused with them.
Burek et al. also distinguish dispositional function
from actual function where the latter applies only to
functions that are actually realized. This seems to be re-
dundant as it ignores that this modal feature is captured
already by the distinction between functions as realiz-
ables and their realization processes. Despite its elabor-
ate apparatus, this approach does not help much to
distinguish functions from roles and dispositions be-
cause all three terms are used to characterise functions.
Elsewhere Burek seems to conceive of functions as a
subclass of dispositions, stating that only the dispositions
for effects of an item which are related to some (pre-)
defined system of functions and goals are its functions
[14]. This statement could be taken to mean that func-
tions should be understood as a special kind of dispos-
ition. In any case, it seems that intentions of agents (like
designers and users of an artefact) choose the functions
of an item and functions are characterised as intentional
entities ([14] definition 67, p.157) and, therefore, agent-
dependent (or community-dependent) and subjective.
Altogether, this account seems to be better suited to
technical functions. Still, many of its features are per-
fectly compatible with the BFO account, which we try to
improve upon. The central structure of functions as
having bearers and being realized in processes with the
bearers as participants is very similar. BFO refrains from
some subtleties like the explicit consideration of the
preconditions for the realization of a function or the dis-
tinction between a goal-state and the realization as a
process leading to a goal state. A possible reason for
abstracting from these distinctions is the actual talk
about functions in much of biology (and the philosophy
of biology) where the process of blood-pumping as a
realization of the hearts function is normally not con-
trasted with a distinct goal state like, e.g., the distribu-
tion of nutrients and oxygen in the body achieved by the
blood-pumping. For many organic functions, the pre-
conditions for realization are often implicitly presup-
posed. For example, the function of the heart is to pump
blood and in order to realize this function it has to be
part of a living body, be connected to arteries and veins,
and enough blood to be pumped has to be present. All
of these are, of course, features of a physiological organ-
ism. Biologists do describe these from the point of view
of the whole organism and not for each organ separately.
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 4 of 16
http://www.jbiomedsem.com/content/5/1/27To spell these conditions out would be a tedious and
possibly endless endeavour: In order to play a part in
locomotion from A to B, legs must be part of an organ-
ism situated at A, but not at B, there must not be a hin-
drance between A and B, it must be possible to be
located at B, etc. All this is true, but of no specific inter-
est to the biologist. To conclude, the approach by Burek
et al. seems to be compatible with the BFO approach in
its main features, but does not help much with the prob-
lems of distinguishing and relating functions, roles and
dispositions.
Methods
We will proceed in two steps in this paper. First, we survey
philosophical theories of functions and evaluate them
against a set of requirements. The theories evaluated are
mostly theories of biological functions, but we also look at
some theories for artefact functions. In the discussion sec-
tion we take up the results of this survey and take a closer
look at the relation between functions and dispositions.
The requirements used as evaluation criteria will be
detailed in this section. In the literature, several lists of
requirements or criteria for a theory of functions have
been suggested. As will become clear, we will not use all
these criteria, but need to pick out a coherent subset of
criteria that we will use in this paper.
We extract from Artiga [15] for biological and from
Houkes and Vermaas [16] for artefactual functions the
following list of adequacy criteria for function theories
(somewhat adapting them to our own terminology):
(1) Teleology: The function should have a central role
in the explanation of the existence of the function
bearer [15].
(2) Restriction: Proper or essential functions of a thing
can be distinguished from its accidental functions
or transient effects [15,16].
(3) a. Normativity: The performance of a function can
be evaluated as better or worse according to a
norm given (at least implicitly) by the function
ascription [15].
b. Malfunctioning: A thing can have a function,
although it fails to perform according to this
function occasionally or even permanently. Failure to
properly perform according to ones function can be a
case of malfunctioning or of non-functioning [16].
(4) a. Avoidance of epiphenomenalism: Functions
should be determined by current performance of its
bearer, not mainly by causally inert historical facts
like its (evolutionary or cultural) history or a mere
ascription by its producers, users, or observers [15].
b. Support: The physical structure of the function
bearer supports its function, even in the cases of
accidental functions or dysfunction [16].(5) Innovation: Novel functions can be ascribed
correctly to innovative artefacts [16], but also to
newly evolved organisms.
There are some tensions between these desiderata [17]
which are also acknowledged by the authors who pro-
posed them. Artiga argues that the normativity criterion
and avoidance of epiphenomenalism cannot be satisfied
at the same time because (3) implies that function as-
criptions should somehow be independent from the
actual properties or activities of the function bearer
while (4a) explicitly denies this [15]. Likewise, Houkes
and Vermaas state a tension between the malfunction
criterion (3b) and the support criterion (4b) for their
theory of artefact functions [16]. They claim to have
solved this for artefactual functions. This solution is
based on, firstly, an optimistic view of the rationality of
designers, users and other ascribers of functions who
would not, or so they assume, assign unsupported func-
tions and, secondly, on a history of maintenance and re-
pair that makes a distinction between non-function and
malfunction easier. But it might well be that not all of
the criteria can be satisfied simultaneously and that
some may have to be relaxed in any theory of functions.
Normativity is one of the most crucial, but also most
contested desiderata for functions. To assert that some-
thing has a function or that a function can be performed
better or worse, we need some kind of normative dimen-
sion. Franssen points out that, as a biological concept, it
would have to be naturalist and, thus, cannot be norma-
tive in the full-blown moral sense [18], but for the nor-
mative dimension of functions it should suffice that it
accounts for the fulfilling (or missing) of purpose in a
naturalist way [19]. Franssen suggests that we could
understand the normativity of functions in a deflationary
way; that is, in terms of the rational expectations we
have with respect to the functions of organs or artefacts.
However, this kind of normativity ascription is too weak
because it lays no special claim to functions in particu-
lar. We form these rational expectations also with re-
spect to a dropped stone that is supposed to fall
(thereby obeying a law of nature). Furthermore, Franssen
argues that, in biology, functions are attributed more
widely than the associated normative evaluations [18].
Many traits or behaviours of living beings can be de-
scribed as having functions in the evolutionary sense, but
are usually not evaluated in the way organs (as quasi-tools
of an organism) are. However, we can ignore these border-
line cases and still acknowledge that we need the norma-
tivity in the case of organs or more generally for all
subsystems of organisms to which functions are ascribed
and which are evaluated with respect to their perform-
ance, as it is clearly the case in medical contexts. These
use-cases show that we do need to deal with the
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 5 of 16
http://www.jbiomedsem.com/content/5/1/27normative aspect and, therefore, we will retain this criter-
ion at some expense of criterion (4).
We supplement these criteria with some requirements
for an ontology of functions proposed by Burek [14]:
(6) Continuants: Functions should not be conflated
with their realizations. While functions (and
other realizables like dispositions and roles) are
continuants, i.e. existing wholly at any point
of time during their existence, their realizations
are processes that stretch out in time.
(7) Bridging of Domains: An ontology of functions
should account for both artefacts (devices) and
non-artefacts.
We regard it as an asset of a theory of function if it
can account for biological and engineered function as
uniformly as is permissible given the differences between
the two domains. While (6) is fulfilled by all approaches
in our survey, it is acted against by such a prominent
system as the Gene Ontology [20], which does not
clearly separate functions and activities because the term
Molecular function is used both in the sense of an activ-
ity and in the sense of a capability: Molecular function
is defined as the biochemical activity (including specific
binding to ligands or structures) of a gene product. This
definition also applies to the capability that a gene prod-
uct (or gene product complex) carries as a potential.
[21] That functions themselves can sometimes be
processes has also been suggested by Kitamura and
Mizoguchi [22], who talk about processes as actual
functions. However, what they call an actual function
corresponds rather to functioning than to function, i.e.
to what we (with BFO) call a realization of a function.
Only their capacity function corresponds to what we
call function proper. Thus, while they use the term
function for processes, they do not claim processes
are functions in the same sense as the functions as-
cribed to devices.
Hence, we strongly support (6) and (7), but we will
not consider the following desiderata (8) and (9), which
have also been suggested by Burek [14]:
(8) Process functions: Processes can be bearers of
functions.
(9) Decomposition: An ontology of functions should
support functional decomposition, i.e. the analysis
in terms of sub-functions.
Criterion (8) is contentious, as it is in direct contradiction
to our preferred framework BFO where only independent
continuants can be bearers of functions. We do acknow-
ledge that natural language does, in fact, attribute functions
to processes like in the following examples:A. His knocking at the door had the function to cause
someone to open the door.
B. The pumping has the function to keep the blood
circulating.
However, surface grammar might be deceiving. Ex-
ample (A) is a description of an intentional action and
the agent knocking at the door performs this action be-
cause of a certain purpose. This shows that it is not ne-
cessary to ascribe functions to processes because this
comes down to the ascription of intentions or plans of
persons participating in these processes. In example (B),
the function to keep the blood circulating can, as well,
be ascribed to the heart, which is an independent con-
tinuant, as BFO requires. The heart can also be said to
have the function to pump blood and given the hearts
canonical location within a circulatory system, it fulfils
the former function by realizing the latter: It keeps the
blood circulating by pumping it. Hence, in these cases,
we have an instrumental or causal relation between two
processes. While this is an important feature to be mod-
elled, we hesitate to deal with it together with the func-
tions of independent continuants.
We do not consider (9), as decomposition is orthog-
onal to the relationship between functions and disposi-
tions and, thus, not relevant to the topic of this paper.
Decomposition seems to be of interest mainly in the
technical domain and is hardly discussed for bio-
functions in applied ontology, but it can, of course, be
extended to this domain. This is nicely shown by the
heart example that we just discussed: The heart sup-
plies oxygene to the body by circulating the blood, and
it keeps the blood circulating by pumping it (cf. the
hierarchies of biological functions in [23]). Of course,
composition of functions cannot be analogous to the
mereological composition of the whole by its parts be-
cause the function of the whole is not simply the sum
of the functions of its parts. Nevertheless, a function of
a whole often depends on the functions of its parts, but
the composition relation for functions is not straight-
forward and would be a different topic. While func-
tional decomposition is compatible with our approach,
it is not relevant for our present purposes. For these
reasons, we will use the criteria (1)(7) only.
Results
Survey of philosophical theories of functions
The concept of a function has provoked a lively debate
in the philosophy of biology, of mind and of engineering.
Several accounts have been proposed to illuminate the
nature of biological functions; see [24] and [25] for re-
cent contributions to this debate. We will now survey
the most important suggestions and evaluate them with
respect to the desiderata given above.
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 6 of 16
http://www.jbiomedsem.com/content/5/1/27Among the many different conceptions and theories of
functions, three main classes of theories may be distinguished.
Like Franssen [18] and Artiga [15], we start by looking at (a)
causal contribution theories of functions and (b) etiological ac-
counts. We add to this by looking at (c) intentional accounts
and two further developments of the intentional account,
namely (d) the ICE theory that combines intentional, causal
and etiological elements, and the (e) fictionalist account that
extends the intentional account to biological functions.
Causal contribution theories of functions
Causal contribution theories take criterion (4) as suffi-
cient for the ascription of a function. They are also called
dispositional [15], causal role [26], goal-contribution
[27], and systemic [28] accounts of functions. They all
have in common that the function of a thing is linked to
the present causal contribution of the function bearer in a
certain context. The most straightforward is the simple
causal role analysis. According to this analysis, X has func-
tion F simply means that X does causally contribute to
some output O of a complex system S [26,27]. A well-
known problem of this account is that it is extremely broad
and admits many unintuitive functions: It implies, e.g. that
clouds could be ascribed the function to produce rain be-
cause they undoubtedly have a central causal role in the
production of rain (more examples and further criticism in
[27]). Thus, the simple causal role account fails the teleology
criterion and no distinction between essential and accidental
functions is possible. To ameliorate this, further conditions
have to be added in order to narrow down possible functions
of a thing. As functions are intuitively connected either with
some intention, as in artefactual functions, or a (not neces-
sarily intended or conscious) goal in biological functions,
Boorses general goal-contribution approach [27] could be
considered the minimal core of the concept of a function
with system S, system part X and goal G:
X performs function Z in the G-ing of S at t if and only
if at t, the Z-ing of X is a causal contribution to G.
But this still does not satisfy all of the criteria. First,
Boorses definition does not capture functions as such,
but only their realizations. Moreover, Boorse-functions
could be performed only once or accidentally and fulfil
this definition which would usually not be what we
mean when we ascribe a function. (A formal-ontological
characterization of such a goal-contribution approach
has been sketched in [29]). Boorse still has to rely on
distinctions like the normal function of a type as opposed
to accidental functions or deviations of single tokens to
avoid those counterintuitive accidental functions. We will
come back to the type/token relation as a source of norma-
tivity below. An even more challenging problem for this ac-
count is posed by malfunctioning because if X does notperform Z (i.e. if function Z is not actually realized) and no
actual causal contribution takes place, we cannot ascribe a
function at all. Therefore it is not clear how malfunctioning
should be handled within this framework.
The concept of a systemic function, as suggested by
Mizoguchi et al. [28], is very similar. They introduce a
systemic context that structures a system into a nested
hierarchy of subsystems and components and assigns a
specific behaviour to the system and its components in
order to define an objects systemic function:
An object A performs a systemic function within a
systemic context C, if and only if there is a system S
such that:(1) C is a systemic context for S,
(2) according to C, A is a component of a subsystem of S,
(3) the goal of this subsystem is to realize the goal of C, and
(4) some behaviours of A play the (functional) role
determined by C.
This definition can be applied both to biological and
to technical functions. To give a biological example, a
systemic context C for the human liver (=A) would be
the human digestive system (goal: digestion of food and
extraction of nutrients) and within the subsystem of fat
digestion the function of the liver is the production of
bile. The point is that the systemic function is context-
dependent in a specific way, i.e. via a system its bearer is
part of. Hence, the function of a thing can change de-
pending on the system it is a component of. In an earlier
paper [22], Kitamura and Mizoguchi discuss the example
of a heat-exchanging device. This device shows as char-
acteristic behaviour a process of heat transfer that leads
to a temperature change in some fluid. This behaviour
can serve different functions: It can either have the
function of heating by giving heat or the function of
cooling by removing heat. Kitamura and Mizoguchi,
therefore, call the role of the behaviour which the device
plays in the respective teleological context (here: heat
exchange) the actual function of the device. Depending
on context, this will be heating or cooling. Their expression
actual function corresponds to what we call realization
of a function and their capacity function corresponds to
our function. If we follow the idea that the realization gets
its role depending on context, we could say that the device
has the capacity (=disposition) for the realization heat-
exchanging, but can have the (capacity) function of heating
or cooling depending on the context. Thus, the context-
dependence of the realization of a function can also be
claimed for the function as a realizable entity.
In this way, the proper function is fixed by the respect-
ive context and can be distinguished from accidental
functions. Therefore, the accidental/essential distinction
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 7 of 16
http://www.jbiomedsem.com/content/5/1/27can be maintained by making functions context-dependent:
all systemic functions are essential with respect to the
systemic context [28] (italics in the original). However,
it is not clear how the systemic context can be fixed
without recourse to other factors. Intentional design is
such a factor which Kitamura and Mizoguchi [22] take
as a standard example for such a context.
To summarize: The causal contribution approaches
can account well for the support criterion because func-
tions are closely tied to the dispositions of their bearers
and their realizations. They have no problem with novel
functions because nothing is said about the history of
the function bearer; they avoid epiphenomenalism be-
cause the actual performance is central. On the other
hand, the problem of accidental and essential functions
can only be dealt with by embedding the bearer in a sys-
tem, which, in turn, may lead to the problem of deter-
mining the proper context of a function and, thus, to
circularity, making the proper context depend on the
proper function and vice versa. A central problem for
this account is the possibility of malfunctioning, for if
function is taken to be identical with the actual causal
contribution of the function bearer, it is not clear how
something can both have a function and not perform ad-
equately and it seems very hard to distinguish malfunc-
tion from the absence of function. We will come back to
this later, but we already note here that many authors do
not employ the idea of realizables and, therefore, do not
distinguish as sharply between the having of a function
(or a disposition) and the actual taking place of the re-
spective realization process. In the approach that takes
functions as realizable entities, it is clear from the outset
that something may have a function without actually or
ever performing it. This does justice to the fact that a
function ascription is not tied to actual performance
and, thus, rejects the causal role criterion as sufficient
for a function ascription, but this alone is not sufficient
to explain malfunction.
Etiological theories of functions
As an alternative to causal contribution theories, etio-
logical accounts of biological functions have been sug-
gested [30,31]. While the causal theories of functions
which we discussed so far focus on the effects of func-
tions, etiological theories focus on their causes: The core
idea of etiological theories of functions is that a function
of X plays a relevant role in an explanation of why X
exists in the first place. In the present section we will
discuss only those etiological accounts that refer to non-
intentional causes, while we will discuss intentional
accounts of functions in the next section. In non-
intentional etiological accounts, a function is taken to be
dependent on the history of the biological kind whose
instances are bearers of the function in question (or, assome prefer, on the history of matching reproductively
established families [30]), i.e. on the series of evolu-
tionary precursors of a present function bearer. It is
evolutionary selection that causally explains the exist-
ence of the functional parts in the first place. Thus we
can phrase the etiological account of biological func-
tions as follows:
Instances of a biological kind X have the function to
do F if and only if the F-ing of the instances of X has
in the past been causally responsible for the positive
selection of X and, thus, indirectly for the present
existence of instances of X.
Such an etiological approach can deal with the most
salient difficulties of the causal contribution accounts.
Essential functions can be distinguished from accidental
ones by referring to the selection history of the trait; fur-
thermore, this history is determined by salient effects of
earlier versions of the trait, so no external goal or con-
text seems necessary. Since performance of a function is
clearly distinguished from having a function, something
can have a function (due to its history), but actually be
malfunctioning in the present [15]. However, recall
Franssens criticism of the normativity of evolutionary
functions mentioned in the methods section. Many traits
of an organism could be considered to be functional be-
cause of their evolutionary history, but would not qualify
as functions in the everyday sense and are usually not
evaluated with respect to norms. Franssen also sees a
proliferation problem if, e.g., foxes are ascribed the func-
tion to hunt, eat and, thus, control the population of
rabbits because they evolved in this way [18].
In any case, etiological accounts have problems with
respect to the avoidance of epiphenomenalism and with
novel functions: Functionality is exclusively determined
by evolutionary development, and such historical facts
are causally inert. Moreover, due to the necessity of hav-
ing a selection history, there cannot be any functions in
the first generation of a biological type, although the
actual structure of the organs would be functional in
the everyday sense, which is very counterintuitive. In
addition, a certain body part may acquire new uses and
functions during the evolutionary history of a species
while the early history and, hence, the reasons for its
existence remain the same [27]. Sea turtles, for ex-
ample, use their flippers to bury their eggs in the sand;
this seems to be one of the functions of their flippers.
During the evolutionary history of the turtles, however,
the flippers developed as a means for locomotion and
only later acquired the digging function [32]. Peter
Godfrey-Smith has argued that functional explanation
has to focus on the recent history of species rather
than on its distant evolutionary origins [33]. One of his
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 8 of 16
http://www.jbiomedsem.com/content/5/1/27arguments is that otherwise there would be no differ-
ence between functional explanations and evolutionary
history, which are two distinct tasks according to
Tinbergens famous four questions that biology has to
answer [34]. Hence, if functions tell us anything about
the survival value of a certain trait, the recent history
of a species seems to be more important than its dis-
tant evolutionary past.
Furthermore, adaptionism is no longer seen as the
only possibility how biological differentiations and func-
tions can arise [35]. More generally, according to etio-
logical accounts, the actual performance of a body part
should not be relevant because the evolutionary history
is the only thing that matters. Thus, functions seem to
be mere epiphenomena of some causal history whereas,
intuitively and methodologically, the essence of a func-
tion lies in what a thing can and is supposed to do now,
which can be understood and discovered independently
of how the thing came to be there.
Intentional accounts of functions
As a third group of theories of functions we consider
intentional accounts. They are tailored to accounting for
the design functions of artefacts: A screwdriver can be
said to have the design function to drive screws because
it is produced with the plan to be used for this purpose.
A design function, then, is not a property that inheres in
the functional artefact, but it is the content of an ascrip-
tion by an agent or a group of agents involving a plan
about the future use of this artefact (or of artefacts of
this type). They are made for a certain purpose, their
function. Let us call this the planning account of design
functions. On this account, the truth-maker of a func-
tion ascription is a plan. Houkes et al. have given an
action-theoretic analysis of use and design claiming that
the designers intentional plans are, in a sense, prior to
both the design of an artefact and its use by a prospect-
ive user [36]. According to this approach, both use and
design functions of artefacts are dependent on the de-
signers (or the clients) ends and on his plan for achiev-
ing these ends. On such an account, artefacts are to be
described as objects playing a role in the contexts of
both use and design, contexts that are mediated by the
communication of a user plan [36]. (The more elabor-
ate ICE theory of technical artefacts by Houkes and
Vermaas [16] will be discussed in the next section as a
further development).
Intentional accounts fare well with regard to the tele-
ology criterion: Artefacts are produced in order to fulfil
the function ascribed to them by their designer. They
can distinguish well between essential and accidental
features and they can account for normativity and
innovation because all of these can be based on the in-
tentions and expectations of the designer.On the downside, design functions are grounded in
designers function ascriptions and not in the physical
structure of artefacts. Thus, an artefact could have any
function independently of its physical structure and dis-
positions. (It will, though, not be able to realize its func-
tion unless it possesses a corresponding disposition to
do so). One option to ameliorate this is to demand ra-
tionality of the designer: A rational designer would have
justified (although, not necessarily true) beliefs about the
components and their working together when he as-
cribes a function to the system, and would not assign
functions not supported by the structure and disposi-
tions of the components. However, it is not easy to see
how this approach can be transferred to biology, for it
seems to presuppose a rational and intentional Creator
and looks almost like intelligent design theory, outrightly
rejected by many (though not all) biologists, philoso-
phers and theologians [37]. (We will later discuss a fic-
tionalist approach that tries to overcome this problem.)
The intentional account can easily be modified in
order to deal with the so-called use functions of artefacts
(which would be classified as roles within the BFO
framework). Use functions are directed at those activities
that users actually use things for. (Cf., e.g., [28] for more
on the distinction between design function and use func-
tion and [17] for problems of this distinction.) If I use
my screw driver to open my paint cans, it has the use
function to open paint cans. It has not been produced
for this purpose; hence, the use function can differ from
its design function, though it might be just the same.
Moreover, one and the same thing can have many differ-
ent use functions at different occasions. This account
can also be extended to biomedical entities: If someone
uses digitalis to kill his wife, he has a certain action plan
that involves the participation of both a probe of digitalis
and his wife with a certain intended outcome.
The ICE theory
Houkes and Vermaas have developed a function theory
for artefacts they call ICE theory, reflecting the fact
that they include Intentional, Causal and Evolutionary
elements in their account [16]. The main intentional
element (I) is a use plan for the artefact that reflects the
designers and prospective users intentions. The causal
aspect (C) shows in their identification of function as-
criptions with ascriptions of physico-chemical capaci-
ties or, in our terminology, dispositions that ensure the
support of the function ([15]; p. 100):
A designer justifiably ascribes the disposition to V as
a function to an artefact x relative to a use plan p for
x, and relative to an account A, if and only if (I) the
designer believes both that (I1) x has the capacity to
V and that (I2) p leads to its goals due to, in part, xs
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 9 of 16
http://www.jbiomedsem.com/content/5/1/27capacity to V; and (C) the designer can justify these
beliefs on the basis of A.
The evolutionary aspect of the ICE theory is very weak
and only relevant for function ascriptions by passive,
non-designing users (as opposed to designers) and their
use of artefacts because they need to have warranted in-
formation (by testimony) about the designers beliefs (I1)
and (I2) and this is transmitted historically. The au-
thors claim that this account does well according to
their criteria, i.e. to (2) Restriction, (3b) Malfunctioning,
(4b) Support and (5) Innovation. Due to the specifica-
tions of the originally intended use plan, accidental func-
tions can be excluded and novel functions are no
problem; and on account of the identification of func-
tions with dispositions, there is no threat of epiphenom-
enalism: Since designers and users need to have the
justified belief that the thing in question actually has this
disposition in order to ascribe the corresponding func-
tion, the support criterion is satisfied and cases of
wishful thinking are excluded. Note that this is an epi-
stemic understanding of the support that the physical
structure lends to functions. Although for many stand-
ard cases of function ascriptions the support criterion
can be satisfied in this way, such rationality constraints
do not completely exclude mad scientists and strange
function ascriptions. They cannot guarantee that func-
tion ascriptions are always veridical, and it could hap-
pen that function ascriptions are rationally justified
with respect to the available information but false.
Malfunctioning is still considered a difficult case for the
ICE-theory by its authors. In many cases of malfunctioning,
users ascribing an ICE-function may be wrong but justified
because they are not aware that the artefact in question has
lost the respective disposition. Users can also be fully aware
of the lack of capacity because they know the artefact to be
broken, but they assume that repair is possible and planned.
Intuitively, we would ascribe the function to the broken
artefact, but, according to the ICE definition, this would
not be justified. Houkes & Vermaas distinguish having a
disposition from its performance and use this for other
cases of non-performance because necessary conditions for
the realizations are not met, as in the case of a car with an
empty fuel tank. Overall, they find it necessary to refer to a
background of maintenance and repair to distinguish mal-
functioning from non-functioning. Some damage of arte-
facts responsible for their malfunction can be repaired, so
these are cases of (temporary) malfunction. However, in
some instances, repair is not technically feasible; in such
cases an artefact would definitely have lost its function.
The fictionalist account
Another modification of the intentionalist approach is the
fictionalist approach [38-40]. In pre-Darwinian biology,organisms and their parts were described as if they, too,
were something created  either, allegorically spoken, by a
personified Nature or by God as a creator. In the latter case,
ascribing functions to biological entities could be conceived
of as reading the mind of God before the act of creation
and as a reconstruction of the reasoning underlying His
creation; we mentioned this option in the context of the
pure intentional account. This is no longer a viable account
for modern science and much of the discussion about
biological functions can be understood as finding some
substitution for this intentional model. In the former alle-
gorical case, we have something like an as-if parlance,
which can be found, e.g. in Aristotle: Although Aristotle re-
jects the idea that the universe or life had a beginning in
time, he often says that Nature has well organised her crea-
tures [41]. We suggest to read this as a fictionalist or as if 
way to talk about biological function, meaning: Were this
plant or animal brought about by Mother Nature (a very
intelligent designer), she would have done so for good
reasons. Hence, the planning account can be upheld for
biological functions with a small modification: The truth-
maker of the ascription of a biological function is no actual
plan, but a plan within the fiction of Mother Nature design-
ing her creatures. The fictionalist account simply invites
taking the intentional approach seriously as a model, i.e. a
certain kind of fiction [42,43], without an ontological com-
mitment to the existence of Mother Nature as a real person
with beliefs and desires. That is, Mother Nature is the func-
tional equivalent to extensionless mass-points in mechanics
or rational utility maximizers in economics: They do not
need to exist to make the theory in question an explanatory
or predictive success. This way, the fictionalist account
allows extending the intentional account to biological
functions, while retaining the plain intentional account
for artefacts.
The fictionalist account does well with respect to our
list of criteria. Like in the original intentional account,
functions feature in a teleological explanation (though
some transfer is needed from the fictive design model to
evolutionary reality). The fictionalist account can distin-
guish between accidental and essential functions; and it
allows for normativity and malfunctioning. Novel func-
tions can be accounted for if we situate the fictive designer
at a later stage in evolutionary history faced with different
evolutionary challenges. We can also give slightly more
optimistic answers regarding the threat of epiphenomenal-
ism and the support for the actual performance of a cer-
tain body part, as its physical structure is the most
important evidence for the function of a body part and
any informed judgement about the function of a part must
account for how its structure supports this function. Fi-
nally, there is now a viable bridge from biological func-
tions to artefactual functions, as intentional accounts fare
well with the latter anyway.
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 10 of 16
http://www.jbiomedsem.com/content/5/1/27Discussion
Formal ontological and philosophical accounts of
functions
As should be clear now, the philosophical theories fare
differently with respect to the criteria set out above
(Table 2), and they do not always distinguish sharply be-
tween functions, roles and dispositions. However, BFO
draws on the different function theories for its distinc-
tions between the types of realizables. To begin with,
BFO functions and BFO dispositions are determined
by their causally relevant internal structure; this fits to
the causal-role account, as it is said that the realization
of a (biological) function helps to realize the charac-
teristic physiology and life pattern for an organism of
the relevant type [6]. The difference between dispositions
and functions is founded on a historical (evolutionary) or
intentional (design) component, respectively. Similarly,
these intentional and historical criteria are used in BFO 2
as the specific difference of functions as opposed to non-
functional dispositions. Thus, BFO functions are essential
features of their bearers because of either an evolution-
ary or an intentional component, whereas functions
according to a mere causal-role account would prob-
ably be classified as dispositions within BFO. What is
called optional, accidental or use functions in the
general debate are roles in BFO because they are not
essential to their bearers. In fact, it is essential to BFO-
roles that their bearers are not essentially playing
these roles; this is particularly clear in the case of social
roles.
Function and homology
When explicating the distinction between functions,
roles and definitions in BFO, Arp and Smith define bio-
logical functions as follows:
A biological function is a function which inheres
in an independent continuant that is (i) part of
an organism and (ii) exists and has the physical
structure it has as a result of the coordinated
expression of that organisms structural genes. [6]Table 2 Evaluation of philosophical function theories
Causal role accounts Etiological a
(1) Teleology ? +
(2) Accidental/essential ? +
(3) Normativity/Malfunctioning ? +
(4) Support/No epiphenomalism + ?
(5) Novel functions + ?
(6) Continuants + +
(7) Bridging of domains + +Philip Lord [44] points out that several prima facie bio-
functions are not captured by this definition because of its
restriction to organism parts on the one hand and to gene
expression as grounds for the structure on the other hand.
His counterexamples are molecular functions and functions
of whole organisms. Lord gives an alternative definition:
A biological function is a realizable entity that
inheres in a continuant which is realized in an
activity, and where the homologous structure(s) of
individuals of closely related and the same species
bear this same biological function. [44]
Lord claims that his definition is recursive rather than
circular, despite the occurrence of the word function
in the definiens. While this can be considered to be a
problem on its own, his suggestion is also open to coun-
terexamples when it comes to more recently acquired
functions like the sea turtles flippers, which are used to
bury their eggs in the sand (see above), but have homo-
logues in other species that have (only) the function of
locomotion. Lord realizes this when trying to distinguish
functions from roles: A human can walk on his hands,
but the function of the human hand is not walking as
most humans do not walk on their hands, whereas the
hands homologous structures in other primates do have
this function. Therefore in humans, the hand may have
the role, but does not have the function walking. Hence
he concludes that among the instances of realizables that
are realizables for the same type of process can be both
roles and functions depending on the species the realiz-
ables bearer belongs to. This presents a problem for the
distinction between functions and roles. We will come
back to this distinction in due course, but it should be
noted that this problem exists for Lord mainly because
of the reference to homologues in other species in his
function definition. To put it bluntly: Had evolution
stopped after the first species, according to Lords defin-
ition, there would not have been any biological function
at all. If we look only at humans, we would not suppose
that the function of the human hand is to walk with, butccounts Intentional accounts ICE account Fictionalist account
+ + +
+ + +
+ + +
? + +
+ + +
+ + +
? ? +
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 11 of 16
http://www.jbiomedsem.com/content/5/1/27that it can assume such a role under certain circum-
stances for appropriately trained individuals. Therefore,
it seems that connecting functions by definition with ho-
mologues in other species (and their respective func-
tions) does introduce more difficulties than it solves,
even if homologues are of huge importance heuristically.
Function vs. role
Let us in more detail review the differences between
functions and roles. Lord [44] states that in actual bio-
medical ontologies, this distinction between functions
and roles is hardly observed. Lord cites OBI, the Ontol-
ogy for Biomedical Investigation [45], as an example that
ignores this distinction. The context dependence does
not seem sufficient to distinguish functions from roles.
Dumontier has claimed that on the level of proteins and
molecules the distinction between functions and roles
becomes redundant: The difference between functions
and roles is not particularly obvious in molecular sys-
tems and may in fact be redundant. For instance, the
function of an enzyme is to catalyze a reaction [] Every
time a protein executes such functionality, it necessarily
realizes the enzyme role [46]. Consequently, Dumontier
uses only roles in his analysis of molecular reaction. This
seems appropriate because the very same type of protein
can have the enzyme role in one reaction type and a dif-
ferent (e.g. substrate) role in a different reaction type. In
our approach, we would ascribe several dispositions to
the protein as prerequisites to the different realizations
that would correspond to the roles of enzyme etc., so
the protein would have dispositions and roles, but no
functions. Also, subrelations of the hasParticipant rela-
tion could be employed when the different participants
of a reaction are classified according to their roles like
hasSubstrate, hasProduct or hasCatalyst. Note that
the inverse relation participatesIn and its subrelations
cannot be used for this purpose because the same type
of molecule can have different roles in reactions and,
thus, it will generally not be true that all molecules par-
ticipate in some reaction type.
Accordingly, many so-called functions in biomedical
ontologies are, strictly speaking, roles. Also the so-called
use functions of artefacts are roles. To come back to our
function criteria, roles clearly fail the first two, Teleology
(1) and Restriction (2). They fail the Teleology criterion
because a role-bearer is not created for the playing of a
mere role. This seems obvious in the case of artefact use
cases not intended by the designer, i.e. using a chair to
stand on to change a lightbulb, or social roles. Similarly,
rabbits have not been selected for being food for foxes,
though they may serve the role of fox food. Roles also
fail the essential-accidental distinction; as can be seen by
the examples given so far, roles are all accidental in a
sense. The other criteria are less clear. Malfunctioningcan, in some cases, be applied to roles, especially to so-
cial roles. In other cases, there is no plausible norm for
the role to be evaluated against, as in cases of accidental
use or misuse, because an artefact that is explicitly used
for an unintended purpose will not be expected to per-
form optimally. Roles also usually must have some phys-
ical support in the dispositions and abilities of their
bearers to have successful realizations. For example, a
screwdriver can only serve the role as a makeshift chisel,
because of its shape, its hardness etc.
Functions vs. dispositions
Now, what does our review tell us about the relation be-
tween functions and dispositions? A common philosoph-
ical position thinks of dispositions as a certain type of
properties [7,8,47]. According to this position, a dispos-
ition is a causal property that is linked to a realization,
i.e. to a specific behaviour or process which the individ-
ual that bears the disposition will exhibit under certain
circumstances or as a response to a certain trigger. Some-
thing is water-soluble if it can dissolve in water. In this fash-
ion, dispositions establish a link between independent
continuants (stable things) and occurrents (processes). The
fundamental connection is the following: Continuant type
S has disposition type D for a realization type P and, in case
some token p of P occurs as the realization of an instance
of D, then an instance s of type S is both the bearer of the
disposition d and a participant of this process instance p.
The category of dispositions is often treated as a special
kind of dependent continuants that are linked to a process
of realization by a respective formal relationship [6-8].
Some, but not all of the positions surveyed above are
explicit about the relations between functions and dispo-
sitions, and those that are do not agree on this matter.
While Cummins writes that if something functions as a
pump in a system [] then it must be capable of pump-
ing and that to attribute a function to something is, in
part, to attribute a disposition to it ([26]; p. 7578),
Millikan states that a things having a function has to
do not with its powers but with its history ([31]; p. 17).
On a first superficial view, the idea that functions are, in
fact, dispositions seems to be at least compatible with
most of the positions reviewed: According to the causal
role accounts, functions can be identified with disposi-
tions quite easily. Millikans resistance notwithstanding,
other authors and BFO 2 combine an etiological account
with the claim that functions are evolutionarily acquired
dispositions [9,33]. Intentional accounts can argue that
functions are specially designed dispositions and the au-
thors of the ICE account actually state that function
ascriptions are special kinds of disposition ascriptions.
More scrutiny, however, will show that the criteria (3a)
and (3b), i.e. normativity and malfunctioning, require us
not to subsume functions under dispositions.
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 12 of 16
http://www.jbiomedsem.com/content/5/1/27Are functions dispositions?
We have seen that among others, both BFO 2 and
Houkes and Vermaas conceive of functions as special
dispositions (or of function ascriptions as special ascrip-
tions of dispositions). We will now apply some of the
criteria for functions above to try to capture also the
prima facie differences between functions, roles and dis-
positions. Dispositions can be blocked or incompletely
realized, but their bearers are not evaluated in a norma-
tive fashion. We usually do not evaluate the realization
of a disposition like inflammability; although, we cer-
tainly have rational expectations about the perform-
ance of the bearer of a disposition and rely on them in
our intentions. Suppose that someone wants to start a
campfire to get warmth and all available wood is wet. In
such a case, the wood will only burn with difficulty and
one will in fact evaluate the available wood negatively
with respect to its inflammability, but only because one
ascribed a use function (i.e. a certain role) to the wood
before. The having of a disposition need not explain the
existence of the disposition-bearer, so they generally fail
with respect to the teleology criterion, although there
might be some cases where the having of a disposition
does explain the existence of its bearer. There are mate-
rials (like alloys or textiles) that are specifically created
for their dispositions because the artefacts made from
the material will need those dispositions to fulfil the
functions they are intended for. Thus, in these cases,
some chunk of the alloy comes into existence because it
was designed to have a certain disposition. Could this be
taken as an argument for functions as special disposi-
tions? We do not think so. The teleological dimension,
like the normative one, applies to dispositions only in
the case of artefacts or in connection with intentional
use; that is, in contexts that have a teleological compo-
nent (usually intentional) anyway.
Peter Kroes has also argued against seeing functions as
a type of dispositions because dispositions lack the nor-
mative dimension we need for functions [48]. Although
his argument relies on Carnaps dated analysis of dispo-
sitions, it does show the salient difference and could
probably be adapted within a more sophisticated analysis
of dispositions.
Dispositions are usually ascribed because of their
realization (their performance). They can be distin-
guished from functions according to the criterion (4a)
because there is no tension between current perform-
ance and normativity for dispositions. Other than func-
tions, dispositions do not face the threat of being mere
epiphenomena.
One further central difference between dispositions on
the one hand and functions and roles on the other hand
seems to lie in their context-dependence. Continuants may
lose or acquire dispositions, but not without fundamentalchanges within the bearer. In contrast, many functions can
be performed by different types of bearers and an object
may have different functions in different contexts without
any change in itself. Chopsticks, for example, have the func-
tion to support eating. Similar sticks found in the woods do
not have any such function, though they may have the very
same physical structure and, hence, the same dispositions.
Dispositions, that is, are purely internally grounded, while
the function of the chopsticks is a historical property due
to the way this artefact has been produced. At the other
end of the spectrum, social functions and roles are exter-
nally grounded; that is, they are dependent on the respect-
ive context, relational and historical properties and mostly
independent from the physical structure of their bearers.
Biological functions like those of organs, enzymes etc. lie
somewhat in between these extremes, as an entity can usu-
ally perform several functions in a certain range of contexts.
They are objective systemic functions in the sense men-
tioned above and not merely ascribed by an agent; their
context-dependence is fixed by the functional hierarchy of
the respective physiological system. An organ like the liver
has many functions, like the production of bile, glycogen
storage, cholesterol synthesis etc., but all these are fixed by
the respective physiological systems in which the liver and
its products are functionally involved. They are not as arbi-
trary or flexible as the screwdriver that can serve the use
functions (i.e. roles) of a can opener or a weapon.
Do functions depend on dispositions?
For all of these reasons, we should assume that functions
are not identical to dispositions. Nevertheless, even if
they are distinct entities, functions could ontologically
depend on dispositions. The support criterion suggests
indeed that, in some sense, functions could be based
on dispositions. On the intentional account, functions of
artefacts are clearly independent from the dispositions of
their bearers; due to the fallibility of human designers,
the one could easily occur without the other. From the
point of the fictionalist extension of the planning account
to biological functions, however, the existence of a function
implies the existence of the corresponding disposition in
typical cases if we transfer the usual assumptions of Gods
omniscience and benevolence to our fictional designer.
Nonetheless, even if a biological function is typically ac-
companied by a disposition, this concurrence is not univer-
sal, as proven by malfunctioning.
In our view, the dispositions are part of the internal
structure of a thing; they determine whether it can fulfil
the respective function in a given context. Johansson
[49] calls this the substratum of a function. While the
function itself is independent from its substratum, its
realization depends on its existence. This dependence
can be a generic one because sometimes different dispo-
sitions or structures can ground the same function: E.g.
Table 3 A new cross-classification of realizables
Internally grounded
(= non-optional given
the physical structure)
Externally grounded
(= optional given the
physical structure)
Essential
(= non-optional
given the bearer)
Essential disposition Function
Accidental
(= optional given
the bearer)
Accidental disposition Role
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 13 of 16
http://www.jbiomedsem.com/content/5/1/27the cooling function of a cooler can be implemented in
different technical setups [49].
As we know biological functions only through their ac-
tual realizations, we would have no reason to ascribe
them unless instances of a certain kind typically dis-
played that behaviour and, a fortiori, possessed a corre-
sponding substratum disposition. How would we know
the biological function of, say, a heart, if hearts did not
typically have the disposition to pump blood and did not
typically realize this function? Thus, there should be
some evidential connection between the function and
the disposition of the organ. This way, we can meet the
Support requirement (4) by giving it an epistemic inter-
pretation: The discovery and ascription of a function is
epistemically supported by the preliminary discovery and
ascription of dispositions in the same or other instances
of a certain species.
On the other hand, many diseases like, e.g. heart insuf-
ficiency, are characterized by the very contrast between
functions and the lack of corresponding dispositions and
so is malfunctioning in general. Malfunctioning artefacts
or diseased organs are characterised by the loss of the
disposition to fulfil their function. E.g. a lung with a car-
cinoma will still have the function to serve as an oxygen
provider for the body, but the function may no longer
be realized because the corresponding disposition (to be
able to serve as an oxygen provider for the body) is no
longer present. Such an account of malfunctioning works
because (and only if) the function is ontologically inde-
pendent of the disposition. From this point of view, the task
of medicine is to restore the disposition matching to the
function, such that the organ would be (fully) functional
again. In such a fashion one can also account for healing
processes: A healing process, then, consists of restoring a
disposition where there is a function without its corre-
sponding disposition. We conclude that the corresponding
disposition is only necessary for the realization of a func-
tion, not for the function itself. Since in biological (and
many artefactual) cases we can evaluate the perform-
ance of token functions with respect to what is a nor-
mal realization for the function type and because the
normal realization is dependent on the corresponding
disposition, we have a correspondence of function and
disposition at the type level or for prototypical tokens.
However, this is to be distinguished from a token-level
dependence of the function on the corresponding dis-
position. If we want to accommodate malfunctioning,
we should reject the latter.
As a source of the normativity of functions, we can iden-
tify the type membership of an instance of a function
bearer. Being a token of a type involves an evaluative di-
mension [32,50,51]. Therefore, in function ascription, the
attribution of a function to a type of entity takes preference
over the ascription to a token. A token is supposed to havea function and be able to perform it successfully because it
belongs to a certain type.
A further reason not to treat functions as special dis-
positions is the following difficulty: In formalised fash-
ion, type-level relations like hasFunction are usually
defined by universal quantification over their instances
using the corresponding token-level-relation [52]. As
said above in the normal or paradigmatic case, a func-
tion of an entity comes along with the disposition to
perform this very function, but as we want to allow for
the possibility of malfunctioning tokens that have lost
the corresponding disposition and, consequently, per-
form the function insufficiently or not at all, we cannot
assert a token-level dependence of disposition and func-
tion for all instances of a type. One option here would
be to distinguish between canonical and non-canonical
entities within a type of function-bearers. Such a distinc-
tion is used, e.g. with respect to anatomical structures in
the BioTop ontology [53]. If one follows this approach,
all instances of the canonical type have the correspond-
ing disposition and do (or would) function adequately,
so for this subtype the standard definitional procedure
works. The instances of the non-canonical subtype will
not be ascribed the disposition, but only the function.
Recommendations
We can summarize the discussion by suggesting a new
classification schema for the three realizables function,
disposition and role. It concurs with BFO 1.1.1 in treat-
ing functions as siblings of dispositions rather than spe-
cial dispositions as in BFO 2. It makes use of two
independent criteria:
 Structure [6]: Does the realizable supervene on the
internal structure or is it externally grounded?
 Rigidity [54]: Is the realizable essential or accidental
to its bearer?
Structure and Rigidity correspond to two flavours of
(non-)optionality. While Structure deals with (non-)option-
ality given the physical structure of the bearer, Rigidity deals
with (non-)optionality given the essence of the bearer (i.e.
given the kind of thing it is). Thus, we end up with a cross
classification of realizables presented in Table 3.
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 14 of 16
http://www.jbiomedsem.com/content/5/1/27We will start the elucidation with Structure. A realizable
can be optional given a certain physical structure of its
bearer. All realizables that are externally grounded, i.e.
grounded in some context, are optional in this sense, e.g. all
roles. In contrast, dispositions are internally grounded,
based on the bearers physical structure and, therefore, not
optional given the bearers physical structure. A few clarifi-
cations seem in order: Internal grounding does not require
that there is a one-to-one-correspondence between a dis-
position type and a structural type on which it is based.
Various instances of one and the same type of dispositions
can be based on quite different physical structures. Fragility,
for example, may be based on the molecular structure of
dry wood as well as on the structure of glass, respectively.
Like many other dispositions, fragility can be structurally
constituted in multiple ways, but in all of these ways the
dispositions base is a structure literally internal to the
bearer of the disposition, and it is independent of the con-
text or the realization conditions.
There is a debate whether dispositions need laws of na-
ture to get connected to their realizations or whether laws
of nature are actually based on dispositions, so the latter
are ontologically more basic than laws [55]. We will not
enter this debate here, but we note that even if dispositions
depend on laws of nature (whatever laws of nature are), this
will not be an ontological dependence that would force us
to reject the claim that dispositions are internal to their
bearers.
Second, a realizable can be optional given the essence
of its bearer. Roles are optional in this sense, and since a
bearer can gain and lose dispositions, some dispositions
are also optional in this way. Note that in this case they
are not optional given the physical structure, but op-
tional given the bearer: The same bearer can survive the
acquisition of such a disposition (by learning, training,
modification, etc.) and it may lose it without ceasing to
be (by forgetting, wear out, modification, etc.), although
the bearer has to change its physical structure in order
to gain or lose dispositions. However, not all dispositions
are optional for a given bearer. Some dispositions, like
the disposition of a proton to attract electrons, are es-
sential: Losing this disposition would imply that the pro-
ton ceases to be a proton, i.e. that it ceases to exist.
There might be only a few essential dispositions and
they might be restricted to the domain of fundamental
physics. For this reason, we acknowledge their existence,
but do not encourage the introduction of separate onto-
logical categories for accidental and essential disposi-
tions, respectively. Functions, too, are essential for their
bearers: Given the essence of being a heart, it is not op-
tional to have the function to pump blood; and given the
essence of being a screwdriver, it is not optional to have
the function to manipulate screws. This is why screw-
drivers are made; it is their origin, without which theywould not be the things they are. Without these functions
they would not be hearts or screwdrivers, but rather
something else. This notion of essentiality for functions
seems to be shared by Kitamura and Mizoguchi with re-
spect to artefacts [22]. In a similar way, both the etio-
logical and the system account can argue that the bearers
of biological functions come into existence in order to ful-
fil these functions; the fictionalist account will assert similar
things relative to the respective design fiction.
Functions are externally grounded. We argued that
there are good arguments not to treat functions as dis-
positions, nor to make functions dependent on disposi-
tions. This distinction is our central disagreement with
the BFO 2 suggestion discussed above. We also define
roles in a rather narrow way (following BFO, but diver-
ging from [13,28,56] and others): On our account, roles
are never essential for its bearer. Hence, we think that
assigning an essential breather or eater role to a hu-
man being is a loose way of speaking and not to be taken
ontologically serious. Breathing and eating are processes,
not functions or roles. True, humans have to participate
in breathing and eating processes on a regular basis, but
there is no need to add to it by postulating a breather
role or an eater role for human beings. One could treat
participation (or rather the property of being a possible
and viable participant) in processes as a role in a very
general sense. Starting from the original BFO suggestion,
however, the category name Role is used in a more spe-
cific way that is distinct from participation. In our view,
participation is an ontological relation [57] (which would
be modelled as an object property in OWL), whereas a
role is a realizable; that is, a (specifically) dependent
continuant. The realization of a role, of course, in-
volves the role-bearer standing in the participation
relation of a realization process of the type that is im-
plied by that role. One of the main points of introdu-
cing roles is to have the option to model these less
strict relationships with a modal character that cannot
be modelled by stating the participant relation which
only concerns the actual participation. Therefore, the
so-called use functions (cf. above) are to be classified
as roles in our classification scheme, in agreement with
the BFO conception of roles.
Conclusions
We surveyed several accounts for the analysis of func-
tions, both for biological functions and for engineered
functions, and we evaluated them with respect to a list
of criteria compiled from the literature. While some of
these theories treat functions as a subclass of disposi-
tions, this cannot account for the normativity connected
with function ascriptions and for the possibility of mal-
functioning. We suggest a new classification of realiz-
ables by means of two independent criteria yielding four
Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 15 of 16
http://www.jbiomedsem.com/content/5/1/27subcategories: essential dispositions, accidental disposi-
tions, functions and roles. Treating functions as a cat-
egory in its own right solves the problems of normativity
and malfunctioning.
On this account, functions are not only disjoint from
dispositions, they are also ontologically independent
from dispositions. Functions are, however, normally
and mostly accompanied by corresponding dispositions.
Moreover, there cannot be a realization of a function with-
out there being a realization of a matching disposition. It is
for these reasons, why it is so difficult to distinguish be-
tween these categories. Malfunctioning, however, requires
them to be distinct categories: It happens in case a function
is present but the corresponding disposition is lacking.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
Both authors contributed equally to the present work. Both authors read and
approved the final manuscript.
Acknowledgements
First research for this paper was supported by DFG grant JA 1904/2-1 within
the project GoodOD. Many thanks to Andrew Spear who provided us with a
recent draft of the follow-up version of [3] and to Amrei Bahr, Niels Grewe,
Ulrich Krohs, Philip Lord and audiences at OBML 2012 and anonymous
referees for critical and helpful comments. For the publication of this paper,
we acknowledge support by Deutsche Forschungsgemeinschaft and Open
Access Publication Fund of the University of Münster.
Author details
1Institut für Philosophie, Universität Rostock, 18055 Rostock, Germany.
2Philosophisches Seminar, Universität Münster, 48143 Münster, Germany.
Received: 25 February 2014 Accepted: 23 April 2014
Published: 2 June 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Haendel et al. Journal of Biomedical Semantics 2014, 5:21
http://www.jbiomedsem.com/content/5/1/21RESEARCH Open AccessUnification of multi-species vertebrate anatomy
ontologies for comparative biology in Uberon
Melissa A Haendel1*, James P Balhoff2,3, Frederic B Bastian4,12, David C Blackburn5, Judith A Blake6,
Yvonne Bradford7, Aurelie Comte4,12, Wasila M Dahdul3,8, Thomas A Dececchi8, Robert E Druzinsky9,
Terry F Hayamizu6, Nizar Ibrahim10, Suzanna E Lewis11, Paula M Mabee8, Anne Niknejad4,12,
Marc Robinson-Rechavi4,12, Paul C Sereno10 and Christopher J Mungall11Abstract
Background: Elucidating disease and developmental dysfunction requires understanding variation in phenotype.
Single-species model organism anatomy ontologies (ssAOs) have been established to represent this variation.
Multi-species anatomy ontologies (msAOs; vertebrate skeletal, vertebrate homologous, teleost, amphibian AOs) have
been developed to represent natural phenotypic variation across species. Our aim has been to integrate ssAOs and
msAOs for various purposes, including establishing links between phenotypic variation and candidate genes.
Results: Previously, msAOs contained a mixture of unique and overlapping content. This hampered integration and
JOURNAL OF
BIOMEDICAL SEMANTICS
Balhoff et al. Journal of Biomedical Semantics 2014, 5:45
http://www.jbiomedsem.com/content/5/1/45SOFTWARE Open AccessAnnotation of phenotypic diversity: decoupling
data curation and ontology curation using
Phenex
James P Balhoff1,2*, Wasila M Dahdul3, T Alexander Dececchi3, Hilmar Lapp1, Paula M Mabee3 and Todd J Vision1,2Abstract
Background: Phenex (http://phenex.phenoscape.org/) is a desktop application for semantically annotating the
phenotypic character matrix datasets common in evolutionary biology. Since its initial publication, we have added
new features that address several major bottlenecks in the efficiency of the phenotype curation process: allowing
curators during the data curation phase to provisionally request terms that are not yet available from a relevant
ontology; supporting quality control against annotation guidelines to reduce later manual review and revision; and
enabling the sharing of files for collaboration among curators.
Results: We decoupled data annotation from ontology development by creating an Ontology Request Broker
(ORB) within Phenex. Curators can use the ORB to request a provisional term for use in data annotation; the
provisional term can be automatically replaced with a permanent identifier once the term is added to an ontology.
We added a set of annotation consistency checks to prevent common curation errors, reducing the need for later
correction. We facilitated collaborative editing by improving the reliability of Phenex when used with online folder
sharing services, via file change monitoring and continual autosave.
Conclusions: With the addition of these new features, and in particular the Ontology Request Broker, Phenex users
have been able to focus more effectively on data annotation. Phenoscape curators using Phenex have reported a
smoother annotation workflow, with much reduced interruptions from ontology maintenance and file
management issues.
Keywords: Annotation, Phenotypes, Ontology, Curation, Systematics, Character matrixBackground
Phenex [1] is a desktop application for creating seman-
tic annotations within the phenotypic character matrix
datasets common in evolutionary biology. Using terms
from a user-configurable set of ontologies, free-text
character state descriptions can be annotated using the
EntityQuality methodology for ontologically describ-
ing phenotypes [2,3]. In addition, taxon entries can be
annotated with identifiers from a taxonomy ontology
[4]. Phenex exploits the ability of the NeXML file format
[5] to attach arbitrary metadata, including ontological
expressions, to phylogenetic data elements, to embed
these ontology annotations within traditional character* Correspondence: balhoff@nescent.org
1National Evolutionary Synthesis Center, Durham, NC, USA
2Department of Biology, University of North Carolina, Chapel Hill, NC, USA
Full list of author information is available at the end of the article
© 2014 Balhoff et al.; licensee BioMed Central
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.matrix data. Phenex was developed as part of the Phenos-
cape project [6], and it has been used to connect the mor-
phological diversity of vertebrates to model organism
phenotypes via common ontological semantics [7,8].
Since Phenex was initially described, we have added
several substantial new features that collectively aim to
address major bottlenecks in the efficiency of the pheno-
type curation workflow. Specifically, three main issues
led to the development of the following three features:
(1) Ontology Request Broker (ORB). When using Phenex
for data annotation, curators would invariably encounter
the need to add new terms to the ontologies being used
to most accurately characterize the anatomy descrip-
tions presented to them. Using earlier versions of
Phenex, data curators frequently had to switch tasks
between data annotation and ontology development.
That is, they paused data annotation, and changedLtd. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Balhoff et al. Journal of Biomedical Semantics 2014, 5:45 Page 2 of 5
http://www.jbiomedsem.com/content/5/1/45software to a term tracker where they made a formal
term request, or to an ontology editor where they added
the desired term, deferring the formal term request. For
the rich, interoperability-promoting, and community-
developed ontologies used in Phenoscape, formal term
requests often entail lengthy discussions. This process
proved a major limitation to annotation efficiency, as a
single paper could require dozens of individual requests.
To address this, we decoupled data annotation from
ontology development by creating an Ontology Request
Broker (ORB) component in Phenex, building on the
recently developed Provisional Term service provided
by the National Center for Biomedical Ontology (NCBO)
BioPortal as part of its application programming interface
(API) [9]. The Phenex ORB component allows curators
to submit requests for provisional terms directly from
the curation interface and use them in data annotation.
The requested terms can be reviewed asynchronously
for inclusion in the appropriate ontology as official
terms. Data files are later automatically updated to
reference the appropriate permanent term ID.
(2) Collaborative editing capability. Large-scale cur-
ation projects like Phenoscape typically employ multiple
personnel that participate in data curation, prompting
the need that curators be able to edit data files collab-
oratively, even if not necessarily concurrently. Phenos-
cape curators had previously been using, with mixed
success, a software version control system (Subversion
[10]) in order to ensure that each curators copy of the
project files was up to date with respect to changes
made by others and to avoid conflicting edits. However,
many biologists are not accustomed to installing or
using version control software, and its ease of use varies
between operating systems. In contrast, file sharing
services such as Dropbox [11] can satisfy collaborative
editing requirements while being mostly transparent to
the user, and they are already widely used by biologists.
To mitigate the risk of curators inadvertently overwriting
changes made to a file by another user, we added two basic
features to Phenex: file change monitoring and continual
autosave. Phenex now immediately notifies the user
when the file they are currently editing changes and
offers to reload it, and the continual autosave decreases
the likelihood of a user having potentially conflicting yet
unsaved edits.
(3) Annotation consistency constraints. Phenotype
descriptions often use the full expressivity of natural
language, and as a consequence there are often several
or even many ways to compose ontological annotations
to represent the semantics of a description. If left
unchecked, the resulting variability can hinder inter-
operability for automated integration and reasoning across
annotations. To promote annotation consistency among
curators, Phenoscape has, as have others, establishedcuration guidelines [8]. Aside from consistency, the quality
of annotations may also be affected by ontology terms
applied in incorrect contexts, such as inapplicable taxo-
nomic group, or inconsistently with its definition, such
as using a quality term defined as inhering in a single
entity to annotate a relationship between two entities
and vice versa. To better report and identify problematic
or missing annotations, Phenex now features a Consistency
Review panel, which allows curators to obtain a growing
variety of quality control reports, as well as an Annotation
Checker panel, which provides the same reports for the
currently selected annotation.
Implementation
The version of Phenex described here has been archived
at http://dx.doi.org/10.5281/zenodo.12370. Phenex is de-
veloped using the Java Swing graphical interface toolkit.
It is built on the application framework developed for
the OBO-Edit ontology editor [12], which provides the
ontology object model, ontology reading capabilities,
and configurable interface layout.
Ontology Request Broker (ORB)
The Ontology Request Broker relies on three main compo-
nents: a term request interface and web service client
within Phenex, the Provisional Class web services provided
by the NCBO BioPortal [13], and a standalone web user
interface for updating requested provisional terms with
permanent identifiers [14]. Before a user can issue
provisional term requests, they must enter their BioPortal
user ID and API key into a Phenex configuration panel.
The Phenex term request client provides an entry panel
that allows the user to request a provisional term with
the given name and description, optionally with suggested
superclass and synonyms. Phenex sends the request
(via HTTP POST) to the Representational State Transfer
(REST) [15] based BioPortal web service API, and receives
in return a new unique URI to be used as a class identifier.
Phenex adds this new term to the current ontology session,
which makes it available for use in annotations. Annota-
tions using provisional IDs are saved in output NeXML files
in the same manner as any other term reference.
At application launch time, Phenex issues a HTTP
GET request to the BioPortal API for all provisional
terms submitted by the configured user. This allows
Phenex to add all provisional terms previously requested
by the user to the users ontology session for use in an-
notations. In addition, for any provisional terms in the
BioPortal database that have since the initial request
been associated with permanent IDs, Phenex automatic-
ally migrates any annotations using those terms to use
the permanent ID instead. This is accomplished by
marking those terms as obsolete within the Phenex
ontology session, and adding to their metadata a
Balhoff et al. Journal of Biomedical Semantics 2014, 5:45 Page 3 of 5
http://www.jbiomedsem.com/content/5/1/45replaced_by annotation with the permanent ID as its
value. Phenex updates any loaded data by following
replaced_by relationships, whether these are due to
normal ontology changes that are part of their regular
maintenance, or due to provisional term resolutions. To
enable collaborative usage of provisional terms, and to
avoid duplicate requests within the project, Phenoscape
curators use a shared BioPortal user ID.
Phenoscape curators assign permanent IDs to provisional
terms using a separate ORB manager web interface
[14], which is a simple client-side web application im-
plemented using AngularJS [16] and the BioPortal web
services API.
Collaborative editing support
Phenex monitors the file from which the currently open
document was loaded for changes. The implementation
makes use of the jpathwatch Java library [17]. Any
changes to the file that are not due to Phenex saving the
document are reported to the user, giving the user the
opportunity to immediately reload the file, or to ignore
the changes. If the user has unsaved changes, reloading
the file would result in these changes being discarded.
To minimize the likelihood of this situation, Phenex also
provides a user-configurable option to autosave the file
after any change is made. Phenex autosave is built on its
previously existing undoredo support.
Consistency review
Annotation consistency rules are implemented by the
AnnotationConsistencyChecker class within the Phenex
source code. This class contains several rules focused
tightly on the EntityQuality model and the semantic
implications of the structure of the Phenotype and Trait
Ontology (PATO) [18]. For example, annotations must
include both an entity and a quality term; PATO rela-
tional qualities, which are qualities that relate between
two entities rather than inhering in a single one, must be
provided with an additional related entity; and entities
descending from Gene Ontology biological process
(GO:0008150) must be used only with descendants of
process quality (PATO:0001236). Phenex displays errors
or warnings in two locations. The Annotation Checker
panel displays any issues found for the currently edited
phenotype annotation; this panel continuously updates its
status as the user edits the annotation. The Consistency
Review panel displays a list of errors for all annotations in
the dataset; additionally it notes if there are incompletely
annotated characters, which are characters for which some
but not all states have annotations.
Results and discussion
As a result of these new features in Phenex, Phenoscape
curators using the tool report a smoother workflow, withfewer interruptions for tasks related to ontology main-
tenance or file synchronization.
The most impactful of these new features is the
Ontology Request Broker. Prior to its implementation,
curators were required to manually keep track of unfin-
ished annotations, while separately contributing miss-
ing terms to the relevant ontology. Submitting a new
term to a community-developed ontology can be a
complex process, possibly involving lengthy discussion
with the expert community and ontology editors. The
Ontology Request Broker in Phenex provides an anno-
tation workflow that effectively decouples ontology
editing from annotation work (Figure 1). When a miss-
ing term is encountered, the user can simply request it,
without leaving Phenex, and receive a temporary iden-
tifier that they can immediately use within annotations.
The term request consists of metadata such as the sug-
gested label, superclass, definition, and possible syno-
nyms. Later, without having to interrupt a data curation
session, they can review their requested terms and
manage the community vetting process for eventually
adding those terms to the relevant ontologies.
Not every provisional term request necessarily results
in an ontology term addition. In some cases, later review
of the request reveals that a suitable term does exist
already, but for some reason was not discovered. In this
case the URI of the existing term can simply be entered
as the permanent identifier for the provisional term. The
failure to discover the already existing term may indicate
that a naming variant is missing from the terms syno-
nym annotations, and the ontology editor may then
choose to add the respective synonym.
JOURNAL OF
BIOMEDICAL SEMANTICS
Clark et al. Journal of Biomedical Semantics 2014, 5:49
http://www.jbiomedsem.com/content/5/1/49RESEARCH Open AccessA use case study on late stent thrombosis for
ontology-based temporal reasoning and analysis
Kim Clark1, Deepak Sharma2, Rui Qin2, Christopher G Chute2 and Cui Tao2,3*Abstract
In this paper, we show how we have applied the Clinical Narrative Temporal Relation Ontology (CNTRO) and its
associated temporal reasoning system (the CNTRO Timeline Library) to trend temporal information within medical
device adverse event report narratives. 238 narratives documenting occurrences of late stent thrombosis adverse
events from the Food and Drug Administrations (FDA) Manufacturing and User Facility Device Experience (MAUDE)
database were annotated and evaluated using the CNTRO Timeline Library to identify, order, and calculate the
duration of temporal events. The CNTRO Timeline Library had a 95% accuracy in correctly ordering events within
the 238 narratives. 41 narratives included an event in which the duration was documented, and the CNTRO
Timeline Library had an 80% accuracy in correctly determining these durations. 77 narratives included documentation
of a duration between events, and the CNTRO Timeline Library had a 76% accuracy in determining these durations.
This paper also includes an example of how this temporal output from the CNTRO ontology can be used to verify
recommendations for length of drug administration, and proposes that these same tools could be applied to other
medical device adverse event narratives in order to identify currently unknown temporal trends.Introduction
The Clinical Narrative Temporal Relation Ontology
(CNTRO) [1] and its associated temporal reasoning
framework (CNTRO Timeline Library) [2,3] can be used
to facilitate an efficient and semi-automated temporal
analysis of events documented within a narrative. Previ-
ously it has been shown how CNTRO can be combined
with LifeFlow [4] software developed by the University
of Maryland, which is capable of visualizing event se-
quences, such that it is possible to see patterns in the
order of events within several narratives [5]. CNTROs
ability to correctly answer temporal-related questions re-
garding specific events that have occurred within a nar-
rative has also been previously demonstrated [6]. The
goal of this present paper is to illustrate how CNTRO
(referring to both the ontology and its associated Time-
line Library) can be used to analyze temporal properties
of events documented across multiple narratives. In this* Correspondence: cui.tao@uth.tmc.edu
2Division of Biomedical Statistics and Informatics, Mayo Clinic, Rochester, MN,
USA
3School of Biomedical Informatics, University of Texas Health Science Center
at Houston, Houston, TX, USA
Full list of author information is available at the end of the article
© 2014 Clark et al.; licensee BioMed Central Lt
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.example, CNTRO is able to verify a recommendation for
length of drug administration.
The Food and Drug Administration (FDA) requires
notification of all medical device adverse events that are
associated with malfunction, serious injury, or death [7].
Events leading up to the device failure are compiled and
reported within a narrative text, which is made publi-
cally available through the MAUDE (Manufacturer and
User Facility Device Experience) database [8,9]. Analysts
at the Center for Devices and Radiological Health (CDRH)
read the event histories of each narrative to identify poten-
tial trends that may exist, which includes temporal pat-
terns (similar sequences of events, similar durations of or
between events, similar time/date stamps of event occur-
rences, etc.) [10]. However with 80,000 to 120,000 device-
related adverse events reported annually to the FDA [11],
this approach to trend identification is time consuming,
expensive, and the potential exists for a missed trend iden-
tification. An automated temporal analysis of adverse
event narratives would lead to faster identification of pat-
terns and/or earlier prediction of a future failure, which
could be used to drive improvements into the next gener-
ation of medical devices.
Automating temporal analysis of events within a narra-
tive is a complex problem. A computer program cannotd. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Clark et al. Journal of Biomedical Semantics 2014, 5:49 Page 2 of 9
http://www.jbiomedsem.com/content/5/1/49create a timeline of events and answer time-related ques-
tions by querying information directly from a narrative
without semantic annotation and inference. Human ex-
perts can understand temporal relationships through the
use of words such as before, after, during, follow-
ing, etc. and appreciate that 1 year, 12 months, and 365
days are approximately equivalent even though differences
in granularity are used. To allow for a machine-under-
standable data representation and exchange of temporal
information automatically, the CNTRO System uses a
Semantic-Web [12] based framework to apply relation-
ships between events within natural language narratives
through the use of the RDF (Resource Description Frame-
work) triple representation [1]. An RDF triple consists of a
subject, an object, and a predicate, which indicates the re-
lationship between the subject and the object [10].
Consider the following example. 60 days after stent
implantation, antiplatelet therapy was discontinued in
preparation for a splenectomy surgery. In this example,
stent implantation is identified as the subject, antiplate-
let therapy discontinuation is identified as the object,
and after is identified as the predicate. A temporal re-
lationship is created between stent implantation and dis-
continuation of antiplatelet therapy using a temporal
offset of 60 days.
The computer program now understands that stent
implantation occurred first, and discontinuation of anti-
platelet therapy occurred second. It also understands
that the time delay between these two events was 60
days. Additionally, there is an inference that because an-
tiplatelet therapy was stopped, it had to have started at
some point prior. The CNTRO framework then creates
a timeline for events and provides a programmatic query
interface to access the timeline information. This makes
it possible for the time-related information to now be
queried in an automated manner. In our particular ex-
ample, we could ask questions such as: Which event oc-
curred first? How long after stent implantation was
antiplatelet therapy administration discontinued?
Many previous efforts have been attempted to model
temporal information within computer-based systems.
Ontologies such as Time ontology [13] and the SWRL
Temporal ontology [14] can formally model temporal in-
formation in general and connect with semantic rea-
soners for inferring new temporal relations based on
semantics defined within the ontologies. These ontol-
ogies only focus on structured data with absolute time
information, however, and therefore cannot precisely
capture the temporal information expressed in human
language [1]. In clinical narratives, many temporal fea-
tures are expressed in relative (e.g. next Friday) or am-
biguous (e.g. early last week) ways. Ignoring this data
will forgo valuable information that could be otherwise
leveraged in clinical research. Models such as the HL7time specification [15] and the TimeML model [16] offer
a way to represent temporal information form semi-
structured or unstructured narratives. These approaches,
however, do not provide the formal semantic definition
capacities for domain knowledge as ontologies do. In
clinical narratives, temporal information is often not ex-
plicitly expressed, but rather needs to be inferred before
the data can be further analyzed. Without a reasoning
component, it is difficult to resolve a relatively complete
patient history for profound clinical studies [17]. There-
fore, we believe that the CNTRO system is necessary as
it provides a formal ontology in OWL with well-defined
semantics for the time domain and enables semantic-
web [12] based temporal reasoning.
Methods
The CNTRO system
CNTRO [1] is an OWL ontology designed to model
temporal relations among clinical events. Figure 1 shows
the ontology overview. It models clinical events, tem-
poral entities (including time instants, time intervals, re-
peated time periods, and durations), time granularity
(minute, hour, day, month, year), temporal relationships,
and time uncertainties in the semantic web notation. In
order for users to annotate events and time-related in-
formation using CNTRO semantics, a Protégé plug-in,
Semantator [18], was developed to interface with CNTROs
temporal reasoning framework. Semantator provides users
two modes: manual annotation mode and semi-automatic
annotation mode. In the manual annotation mode, the
users can view the domain ontology, the document to be
annotated, and the annotated result in the same environ-
ment. For semi-automatic annotation, we have linked
Semantator with Natural Language Processing tools that
support automatic named entity recognition. Users can
browse, revise, and save the annotation results at anytime.
Semantator has been used to create gold standard annota-
tions for evaluating the reasoning output in our project.
The annotated information is stored as an OWL/RDF file
or an RDF triple. The CNTRO Timeline Library is then
used to infer temporal information not explicitly expressed
in the original narrative [2]. The CNTRO Timeline Library
contains a rule-based normalizer that automatically con-
verts different temporal expressions into standard formats
such as the XML dateTime format. It also leverages the se-
mantic definitions in the ontology (e.g. OWL DL axioms,
property transitivity and inversions) to support temporal
relation inference. These OWL features were handled by
the Pellet OWL reasoner. In addition, the Timeline Li-
brary contains a set of Java functions for answering a list
of time-related questions, such as when a particular event
happened, chronological sequence of events, durations of
events, durations between events, temporal relations be-
tween events, and sorting a set of events on the timeline.
Figure 1 CNTRO overview.
Clark et al. Journal of Biomedical Semantics 2014, 5:49 Page 3 of 9
http://www.jbiomedsem.com/content/5/1/49The Timeline Library first calls the reasoner to infer new
temporal relations. It then considers all the temporal rela-
tions among the events, normalized timestamps of the
events, and durations of the events to compute new time
stamps of events if possible. It can also calculate the dur-
ation of an event given the start and end time of the event
and the duration between two events given the time
stamps of them. After all the possible inferences and cal-
culations are done, it tries to sort a given set of events
based on all the above information.
Late stent thrombosis adverse event identification
Late Stent Thrombosis (LST) adverse event narratives
were used to demonstrate how the CNTRO system and
its automated temporal relation reasoning can be used
to verify drug therapy duration recommendations. Al-
though the exact mechanism or mechanisms of LST are
not known, it has been observed to occur less frequently
when dual antiplatelet therapy has been administered
over a period of time [19,20]. Current guidelines recom-
mend the administration of dual antiplatelet therapy for
3 to 6 months following drug-eluting stent implantation,
unless the patient is not at high risk for bleeding, in
which case therapy is recommended for 12 months [21].
The CNTRO System was used to evaluate the order of
events within each narrative and query both the duration
in which antiplatelet therapy was administered and the
duration between initial stent implantation and the oc-
currence of late stent thrombosis.
Narratives used in this study were obtained from med-
ical device adverse event reports documented within the
MAUDE database. 238 adverse event reports were iden-
tified in which late stent thrombosis occurred, defined
either as late within the report or by a duration of 6
months between stent implantation and the occurrenceof thrombosis. These narratives were then manually an-
notated using Semantator by an expert.
Adverse event narrative annotation
We created a domain ontology which includes common
events that occur after stent implantation was created
with specific normalized event types. The domain ontol-
ogy is relative to simple comparing to the CNTRO. It
only defines the set of events we what to monitor for
our use case. These events were then imported into
CNTRO for temporal relationship modeling. The follow-
ing events were included: initial stent implantation, fol-
low up stent implantation(s), start and stop time points
of antiplatelet therapy administration, unrelated surger-
ies occurring after stent implantation, late stent throm-
bosis, myocardial infarction, admission to the emergency
room, and patient death. Events such as guide wire in-
sertion are required for all stenting procedures; therefore
annotation of these events would not be beneficial and
were therefore not performed. Life-saving events follow-
ing the thrombosis detection were also not annotated
within the narratives as the focus of the application of
CNTRO was based on verifying the recommended dur-
ation of drug administration and not the potential to
survive following an occurrence of thrombosis.
Start and stop point of the antiplatelet therapy were an-
notated to determine the duration of therapy. Unrelated
surgeries which occurred between stent implantation and
identification of thrombosis, myocardial infarction, ad-
mission to the emergency room and patient death were
all annotated as events to verify the Event Order and In-
ferred Relationship functions of the CNTRO Timeline
Library.
Annotations were performed using Semantator. The
first step in the annotation process involves identification
Clark et al. Journal of Biomedical Semantics 2014, 5:49 Page 4 of 9
http://www.jbiomedsem.com/content/5/1/49of the individual events. As shown in Figure 2, after each
event is created, the text turns color specific to each event
type.
After the events are created, temporal relationships be-
tween the events can be defined through annotation, see
Figure 3. A relationship connects two events and can indi-
cate that the events occurred or began at the same time,
or that one event occurred or began before another event.
If the duration between the events is known, this is anno-
tated after the relationship has been defined, see Figure 3.
If a specific event has a duration, this information is anno-
tated as well.
CNTRO timeline evaluation
For each annotated narrative, the CNTRO Timeline Li-
brary creates a matrix that visually shows the temporal
relationships between the events, which is a simple way
to track, view, document, and evaluate the accuracy of
CNTRO system timeline computations. Each annotatedFigure 2 Annotating an event.event is included within the matrix. The matrix indicates
which events occur at the same time, and then orders
the remaining events on a timeline as applicable. Figure 4
shows a sample matrix. Figure 4(a) shows a partial com-
plaint file with three events and their corresponding tem-
poral constraints highlighted. Figure 4(b) shows the event
descriptions. Figure 4(c) shows the annotated (asserted)
temporal relations between events (e.g., EVENTID-3
EQUAL EVENTID-1). Figure 4(d) shows both asserted
and inferred temporal relations between events. Based on
the annotation result, the reasoner knows the timestamp
of EVENTID-1 and EVENTID-3 as well as the fact that
EVENTID-3 and EVENTID-1 happened at the same
time. It can therefore infer the timestamp of EVENTID-3
which is the same as the one for EVENTID-1. Then
based on the timestamps, it can infer the temporal rela-
tions among the three events. Finally Figure 4(e) shows
the timeline bucket that includes a set of sorted timeline
entries.
Figure 3 Annotating a relationship.
Clark et al. Journal of Biomedical Semantics 2014, 5:49 Page 5 of 9
http://www.jbiomedsem.com/content/5/1/49The annotations of the Late Stent Thrombosis Adverse
Event Narratives were reviewed using these matrices and
compared against gold standard results, in which events
were manually recorded in timeline order from two ex-
ports reading each narrative. The timeline accuracy was
assessed by comparing the gold standard results to the
CNTRO Timeline Library results. All conflicting results
between CNTRO and the gold standard were reviewed
among the human experts to determine if the conflict
resulted from an error in the gold standard result, an
error in manually annotations, or an error in the reason-
ing component of CNTRO.
CNTRO duration evaluation
Durations can be computed for an individual event, be-
tween two events, or between an event and a timestamp.
CNTRO first determines if start and end time infor-
mation exists for an event to calculate the duration. If
one of these pieces of information is missing, the pro-
gram then computes it by either using a duration anno-
tation, Antiplatelet therapy was administered for two
months (the antiplatelet therapy event is defined here
with a duration of 2 months) or uses a temporal relation
to another event with a relative time stamp, Antiplatelet
therapy was started in May 2006. In July 2006, the pa-
tient underwent prostrate surgery. Antiplatelet therapy
was stopped the day before surgery. In this secondexample the occurrence of antiplatelet therapy starting
and stopping each have a time stamp, and CNTRO in-
fers that antiplatelet therapy was administered for 2
months based on the duration between the start and end
times. In some cases, the duration of a pair of events
cannot be calculated directly (the two events are not dir-
ectly connected through the RDF graph), but need to go
through one or more intermediate events. In this case,
the above two functions need to be called iteratively
until the duration of the two events are calculated.
The adverse event narratives for late stent thrombosis
could describe durations in days, months, and/or years.
Month was the most frequent granularity used in the
complaint data, followed by years, and then days. To be
able to compare data from different narratives, the dur-
ation granularity was normalized to Month for this use
case as this was the most frequently used granularity,
and estimating durations reported in years by number of
days would likely increase the noise within the data. The
durations calculated by CNTRO were compared to man-
ual calculations to determine accuracy.
Application of CNTRO temporal analysis
To provide an example of how the CNTRO system can
potentially be used to evaluate temporal properties
within narrative data, survival analysis was performed
using the narratives that specified both a duration of
Figure 4 Sample evaluation matrix (a) original document (partial); (b) event description; (c) asserted temporal relations; (d) asserted
and inferred temporal relations; (e) timeline bucket.
Clark et al. Journal of Biomedical Semantics 2014, 5:49 Page 6 of 9
http://www.jbiomedsem.com/content/5/1/49antiplatelet therapy and time from stent implantation to
late stent thrombosis (or in which a duration could be
inferred) to examine therapeutic guidelines for antiplate-
let administration duration. Note that as this data comes
from the FDA MAUDE Database, all records within the
example ended up with an event of late stent throm-
bosis. Data of patients who have not had a late stent
thrombosis occurrence are not easily accessible; there-
fore this example is purely illustrative of the CNTRO
systems capability. Similarly, because the data used
within this analysis comes from adverse event files indi-
cating thrombosis occurred, no patient data requires
censoring.
Late Stent Thrombosis adverse event files were divided
into two different groups based on how long antiplatelet
therapy was administered in patients following implantationof a drug-eluting stent. Using current antiplatelet therapy
recommendations, any adverse event narrative specifying
that antiplatelet medication was administered for less than
6 months was segregated into the Shorter Duration of An-
tiplatelet Therapy group. Any adverse event narrative indi-
cating that antiplatelet medication was administered for 6
or more months was segregated into the Longer Duration
of Antiplatelet Therapy group. Adverse event narratives
that did not provide information specifying how long anti-
platelet therapy was prescribed were excluded from the
analysis.
Results
CNTRO timeline and duration evaluation
238 adverse event narratives included at least two events,
such that a timeline could be created within CNTRO for
Clark et al. Journal of Biomedical Semantics 2014, 5:49 Page 7 of 9
http://www.jbiomedsem.com/content/5/1/49system evaluation. For each narrative, the CNTRO system-
inferred timeline was evaluated with a gold standard result.
The CNTRO system was capable of correctly ordering
each event in all but 8 of the narratives. This resulted in
an overall CNTRO timeline accuracy of 95%. There were
41 adverse event narratives that included enough informa-
tion such that the duration of antiplatelet therapy was
known. The CNTRO automatic reasoning system had an
80% accuracy in inferring and/or calculating this duration
of an event. There were 77 adverse event narratives that
included enough information such that the duration be-
tween stent implantation and identification of late stent
thrombosis was known. The CNTRO Automatic reason-
ing system had a 76% accuracy in inferring and/or calcu-
lating this duration between events. An evaluation of the
errors and discussion of possible enhancements to the
CNTRO system is included within the Discussion section.
Late stent thrombosis adverse event temporal pattern
analysis
Within this paper, the CNTRO system was used to con-
firm what has been previously identified as a temporal
pattern within the late stent thrombosis adverse event in
a semi-automated manner, which is more efficient than
through manual observation. The common event pattern
within late stent thrombosis adverse events (stent im-
plantation, administration of antiplatelet therapy, discon-
tinuation of antiplatelet therapy, late stent thrombosis)
was shown by CNTRO system through timeline identifi-
cation of events. This result shows that the CNTRO sys-
tem has the potential to be applied across multiple
adverse event failure modes to identify new trends that
have previously not been observed.Figure 5 Survival analysis of shorter duration of antiplatelet therapy
late stent thrombosis adverse events.There were 36 adverse events that specified both the
duration between drug-eluting stent implantation and
occurrence of late stent thrombosis, and the duration of
antiplatelet therapy. These 36 reports were used to exe-
cute a survival analysis. Although this represents only a
limited subset of late stent thrombosis events and does
not include patient information for those who have not
had late stent thrombosis, the data can still be used for
illustration purposes of CNTROs temporal analysis cap-
abilities. Late Stent Thrombosis adverse event files were
divided into two different groups based on how long an-
tiplatelet therapy was administered in patients with an
implanted drug-eluting stent. Adverse event narratives
that did not provide information specifying how long an-
tiplatelet therapy was prescribed were excluded from the
analysis. 14 adverse events reported that antiplatelet
therapy was administered for 6 months or less following
initial stent implantation. 22 adverse events reported
that antiplatelet therapy was administered greater than 6
months.
Survival analysis with Kaplan-Meier curve and log-
rank test was performed in Minitab. The median time to
LST is 27.3 months for longer antiplatelet therapy group
and 14.6 months for shorter antiplatelet therapy group,
respectively. The p-value of log-rank test is 0.029, which
indicates a significant association between duration of
antiplatelet therapy and time to LST. Figure 5 supports
that on average, late stent thrombosis occurs later in pa-
tients who continued to take antiplatelet therapy longer
than 6 months. Although this is a retrospective observa-
tional study of a subset of LST cases only, the finding is
consistent and supports guidance for use of longer anti-
platelet therapy [1]. This example validates the CNTRO(group 1) and longer duration of antiplatelet therapy (group 2) in
Clark et al. Journal of Biomedical Semantics 2014, 5:49 Page 8 of 9
http://www.jbiomedsem.com/content/5/1/49Systems ability to confirm known temporal trends and
verify drug administration duration recommendations.
Discussion
Error analysis
Although the CNTRO system can provide relatively
good results for our use case, there are still limitations
in the system. First, the evaluation results work well with
the MAUDE reports because these reports are relatively
short and simple compared to other clinical narratives
such as clinical notes. Second, since the purpose of this
study is to evaluate CNTROs representation and reason-
ing capacities, the reports were annotated manually.
Many ambiguities and uncertainties were resolved dur-
ing the annotation process. Nevertheless, this study pro-
vides promising results and valuable analysis for us to
continue develop the CNTRO system.
The CNTRO system was able to order the event se-
quences for 95% of the narratives. The reasoner failed due
to different interpretations of time intervals and back-
ground assumptions in the manual annotation. Comput-
ing the order of two events is difficult when using start or
finish temporal relations when both the start and end
times cannot be annotated. For example, a narrative might
specify that antiplatelet therapy began at the time of stent
implantation, and specify that it occurred for a period of 2
months. The temporal relation of the event1 (antiplatelet
therapy) and event2 (stent implantation) depends on
whether the start and end times of the events can be com-
pared. When considering the start time, the two events
start at the same time (event1 starts event2). The system
cannot infer the relationship by the end time since the
duration of stent implantation is not specified, given that
it occurs at a single point in time. Given the assumption
that the stent implantation procedure cannot last for 2
months, we can infer that event1 ends after event2. This
kind of background knowledge needs to be further speci-
fied in the domain ontology so that the CNTRO system
can infer the correct order. Additionally, patient death
inherently is known to be the last event in a patient-care
timeline. This kind of inherited order needs to be incorpo-
rated in the domain ontology so that the sequence of
events can be correctly inferred.
For duration inference, there are three major reasons
the program failed to return the correct results. (1) An-
notation ambiguities: some narratives contain duration
information in an ambiguous way such as in range (e.g.,
2-3 month), or in different levels of granularity (e.g.,
two month and ten days) that the program cannot
automatically process. We are working on expanding the
ontology so that it can cover ranges. In addition, we are
adding more functions to the reasoner so it can normalize
durations in different levels of granularity. (2) Long series
of events: sometimes the duration calculation involves along series of events. The program sometimes fails when
there are many intermediate events between the start and
the end events. This is usually due to one or more inter-
mediate events were not annotated by the ontology and
therefore were not included during the reasoning process.
3) Temporal relation granularity: an annotator can specify
the level of granularity over a temporal relation. For ex-
ample, we can specify that the granularity of event1 be-
fore event2 is day. This means that the temporal
relation was compared on the granularity of day, which
implies that although event1 was before event2, but they
happened on the same day. This assumption was not pro-
grammed in the CNTRO reasoning system yet, and
caused errors when calculating the duration between
event1 and event3. For example, we know that Event3
may have occurred 183 days after event2, but without the
assumption that event1 and event2 happened on the same
day, the system cannot infer the duration between event1
and event3. The CNTRO reasoner needs to be updated to
handle level of granularity on temporal relations.
Areas for improvement of MAUDE database for temporal
analysis
There were some weaknesses identified regarding the
use of adverse event narratives from the MAUDE data-
base. The MAUDE database does not have selectable
fields for Device Manufacturer or Brand Name. Due to
the free text fields, there are a variety of spellings and
misspellings for both the Device Manufacturer and
Brand Name which may have resulted in a missed late
stent thrombotic adverse event based on how these
fields were used to sort complaints. The level of detail in
some adverse event narratives was very limited and the
duration between stent implantation and stent throm-
bosis may not have been documented. Additionally, due
to patient privacy some time stamps were removed mak-
ing the duration between stent implantation and stent
thrombosis unknown. It is possible that late stent throm-
bosis occurred in some patients but the complaint narra-
tives were filtered out due to not being able to classify the
event as late. Late stent thrombosis adverse events may
also have been missed while filtering from the files if a
different term was used within the narrative as there is
no searchable failure mode within MAUDE specific to
thrombosis.
Future CNTRO applications
Of interest in recent literature is a current investigation
into understanding whether there is a link between in-
complete stent apposition (ISA) (separation between the
stent strut and the vessel wall) and late stent thrombosis.
Stent which are not adequately apposed following im-
plantation are referred to as acute ISA, and may be due
to incorrect stent sizing or inadequate expansion of the
Clark et al. Journal of Biomedical Semantics 2014, 5:49 Page 9 of 9
http://www.jbiomedsem.com/content/5/1/49stent. Inadequate stent apposition identified at a later
point in time is referred to as late ISA. Late ISA can ei-
ther be persistent, meaning that it was the result of inad-
equate stent expansion, or acquired, meaning the vessel
becomes enlarged, or plaque or thrombosis in-between
the stent and wall dislodged creating space, or the stent
recoiled. There will likely be future studies attempting to
link late stent thrombosis with either persistent or ac-
quired ISA. The CNTRO system could be of value in
this investigation to determine if there is a correlation of
post-dilation frequency with late stent thrombosis or a
relationship between the change in apposition and the
duration between discontinuation of antiplatelet therapy
and thrombus formation.
Conclusion
Although the CNTRO system was able to provide rela-
tively good results for this use case, there are still limita-
tions in the system. First, the evaluation results work
well with the MAUDE reports because these reports are
relatively short and simple compared to other narratives
such as clinical notes. More CNTRO system evaluation
needs to be performed using complex electronic health
record data. Second, since the purpose of this study is to
evaluate CNTROs representation and reasoning capaci-
ties, the reports were annotated manually. The current
manual annotation method is not practical for long-term
use, and an automatic annotation process is currently
under development. Third, many ambiguities were re-
solved during the annotation process. Uncertainty rea-
soning is currently being incorporated into the CNTRO
system to resolve these ambiguities. In spite of these lim-
itations, this study provides promising results and valu-
able analysis to support continuing the development of
the CNTRO system.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
KC led the data annotation and analysis, and drafted the manuscript. DS
contributed to the CNTRO reasoner implementation and result evaluation.
RQ is the statistician of the project. CGC contributed on the ontology design.
CT led the overall study design. All co-authors participated in writing, reviewing,
discussion, and editing of the manuscript. All authors read and approved the
final manuscript.
Acknowledgments
This research is partially supported the National Library of Medicine of the
National Institutes of Health under Award Number R01LM011829. We thank
Ms. Donna Ihrke for her help on annotating the files.
Author details
1Boston Scientific Corporation, Maple Grove, MN, USA. 2Division of
Biomedical Statistics and Informatics, Mayo Clinic, Rochester, MN, USA.
3School of Biomedical Informatics, University of Texas Health Science Center
at Houston, Houston, TX, USA.
Received: 20 June 2014 Accepted: 13 November 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Younesi et al. Journal of Biomedical Semantics 2014, 5:31
http://www.jbiomedsem.com/content/5/1/31RESEARCH Open AccessCSEO  the Cigarette Smoke Exposure Ontology
Erfan Younesi1, Sam Ansari2*, Michaela Guendel1, Shiva Ahmadi1, Chris Coggins3, Julia Hoeng2,
Martin Hofmann-Apitius1 and Manuel C Peitsch2Abstract
Background: In the past years, significant progress has been made to develop and use experimental settings for
extensive data collection on tobacco smoke exposure and tobacco smoke exposure-associated diseases. Due to the
growing number of such data, there is a need for domain-specific standard ontologies to facilitate the integration
of tobacco exposure data.
Results: The CSEO (version 1.0) is composed of 20091 concepts. The ontology in its current form is able to capture
a wide range of cigarette smoke exposure concepts within the knowledge domain of exposure science with a
reasonable sensitivity and specificity. Moreover, it showed a promising performance when used to answer domain
expert questions. The CSEO complies with standard upper-level ontologies and is freely accessible to the scientific
community through a dedicated wiki at https://publicwiki-01.fraunhofer.de/CSEO-Wiki/index.php/Main_Page.
Conclusions: The CSEO has potential to become a widely used standard within the academic and industrial
community. Mainly because of the emerging need of systems toxicology to controlled vocabularies and also the
lack of suitable ontologies for this domain, the CSEO prepares the ground for integrative systems-based research in
the exposure science.
Keywords: Exposure, Cigarette smoke, Environmental risk, Ontology, Knowledge representationBackground
Recently, there has been an increased focus in systems
toxicology on systems-oriented methodologies that
emphasize the understanding on the biological impact of
chemical exposures with increased mechanistic granularity
[1,2]. In particular, a recent report by the US National
Research Council Committee on Toxicity Testing and
Assessment of Environmental Agents advocates for a shift
away from toxicological assessment at the level of apical
endpoints towards the understanding of the effects of an
exposure on toxicity pathways [3]. Moreover, the Food
and Drug Administration (FDA) recently describes a
system-based omics-approach to discover pulmonary
biomarkers and to improve the evaluation of tobacco
products [4]. This indicates a growing recognition that
exposure science should be considered as an integrated
part of a systematic approach for risk assessment [5].* Correspondence: sam.ansari@pmi.com
Equal contributors
2Philip Morris International R&D, Philip Morris Products S.A., Quai Jeanrenaud
5, 2000 Neuchâtel, Switzerland
Full list of author information is available at the end of the article
© 2014 Younesi et al.; licensee BioMed Centra
Commons Attribution License (http://creativec
reproduction in any medium, provided the orTo assess biological responses to environmental
exposure, a systems-based approach attempts to apply an
integrative strategy. A systems-based approach integrates
a continuous model from the starting point of exposure to
disease outcome [6]. A typical limitation in systems
approaches is the lack of standards for harmonization of
heterogeneous data types that are experimentally obtained
from different resources. Such data types often have
various structures, formats and annotations, which
adversely affect the degrees of their interoperability and
flexibility for integrative methods. Standard terminologies
and proper contextual information are necessary for data
sharing, reuse, and integration [7]. Recently, biomedical
ontologies have emerged in support of systems approaches
by facilitating the annotation of bio-simulation models
and flexible access to knowledge [8]. The main purpose of
ontologies is to organize data and information of a
particular knowledge domain in a structured, controlled,
and standard manner. Thus the data can be shared among
scientists in different research areas or accessed and
interpreted using different computational tools. The core
of any ontology is a controlled vocabulary that attempts to
describe a unified definition for all terms and concepts inl Ltd. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly credited.
Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 2 of 11
http://www.jbiomedsem.com/content/5/1/31a particular subject area [9]. A good example is the Gene
Ontology (GO) that provides a controlled vocabulary
describing the roles of genes and their products in various
organisms [10].
At the heart of systems toxicology is the understanding
of signaling pathways perturbed by biologically active
substances and the identification of those that have the
potential to cause adverse health effects in humans. This
requires integrating OMICs data with in vitro and in vivo
toxicological endpoints. The goal of systems toxicology is
therefore to link disease susceptibility at the molecular
level to environmental stress or toxicant effect at the
clinical level. Despite advances in various aspects of
toxicogenomics, semantic representation of toxicological
data and endpoints is still in its infancy. A variety of tools,
platforms, and workflows coexist but each uses its own set
of terms and ontologies, a challenge for data exchange.
Hardy et al. [11] in their review provide an overview of
existing toxicology vocabularies and ontologies that are
currently being used in predictive toxicology initiatives
and applications [11].
Recently, the toxicology OpenTox ontology has been
developed to support standard representation of
relations between chemical and toxicological datasets
and experiments by unified terms. It is part of the
OpenTox framework, which aims at unifying access to
toxicity data, predictive networks, and validation procedures
[12]. One of the advantages of the OpenTox ontology is the
combination of several related ontologies that cover
common information for chemical compounds, chemical
datasets, algorithms, models, assays, in vivo studies, and
toxicological endpoints. Moreover, when integrated in a
semantic environment, the OpenTox ontology service
facilitates registering new resources, remote access, and
searching datasets using SPARQL. However, the OpenTox
remains a high-level ontology and does not include concept
granularity for the majority of its components in particular
for the domain of environmental exposure.
Lately, the exposure ontology (ExO) has been proposed
to provide the missing link between exposure science and
various environmental health disciplines, including
toxicology [13]. The main advantage of the ExO is that it
provides the first semantic template for representation of
exposure information around the following four root
concepts: exposure stressor, exposure receptor, exposure
event, and exposure outcome. Although the current
version of the ExO includes very general and high-level
concepts to cover the breadth of the exposure knowledge
domain, it still lacks sufficient granularity that is required
to capture detailed information. Besides, the ExO is not
compliant with the proposed upper-level ontology
standards such as the Basic Formal Ontology (BFO) [14]
or the Descriptive Ontology for Linguistic and Cognitive
Engineering (DOLCE) [15], which makes its integrationwith existing or new ontologies semantically more
difficult. Furthermore, Thomas et al. [16] describe the
use of a Smoking Behavior Risk Ontology (SBRO) to
represent risk models for phenotypes associated to
tobacco smoking behavior [16]. However, the scope of
their ontology is limited to nicotine pharmacokinetics,
pharmacodynamics, nicotine dependence, and clinical
smoking cessation outcomes.
Exposure to tobacco smoke is considered an environ-
mental risk factor to human health and it is involved
in the initiation and progression of several respiratory
diseases including chronic obstructive pulmonary dis-
eases (COPD) and lung cancer [17,18]. Elimination or
minimization of exposure to cigarette smoke provides a
clear opportunity to prevent related diseases. Although
experiments that measure exposure to environmental
tobacco smoke follow  to a large extent  the typical
protocols used in toxicology experimental settings, no
semantic framework capturing information specific to the
domain of cigarette smoke exposure risk is available.
In response to the need for semantic representation of
the environmental exposure knowledge domain with
particular focus on the cigarette smoke exposure risk,
the Cigarette Smoke Exposure Ontology (CSEO) was
developed.
Results
Purpose of the cigarette smoke exposure ontology
The development of an ontology starts by defining its
domain and scope. The scope of the CSEO was defined
based on the potential application of the ontology in the
domain of environmental exposure and was focused on
exposure to cigarette smoke. Since setting a proper
scope helps draw boundaries to the knowledge domain
included in the ontology, the CSEO is intended to
include all concepts and terms that represent processes
and elements involved in conducting cigarette smoke
exposure experiments, in association with cigarette-smoke
related diseases (Figure 1).
The scope of the ontology revolves around the exposure
experiment concept and covers description of sampling
and experimental factors, test items, test systems, exposure
condition, and link to diseases. These are the main
concepts to be included in the CSEO by following the life
cycle of ontology building, as described in the Methods
section. Axiomatisation of concepts in the CSEO is based
on the axioms provided in the BFO and ExO. For example,
the description of an exposure follows the lines of the
exposure event class in the ExO. We have, furthermore,
enriched the ExO classes with extra classes that make the
ontology more specific to cigarette smoke rather than just
to exposures in general. The reason for choosing these
concepts is that they represent the major players in systems
toxicology studies conducted in the domain of smoke
Figure 1 High-level schematic representation of the CSEO scope. The scope of CSEO was designed around the key concept of exposure
experiment and its substantial elements.
Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 3 of 11
http://www.jbiomedsem.com/content/5/1/31exposure. Most exposure experiments follow a similar
routine summarized as follows: the design, factors, and
protocols of an experiment must be defined before
conducting the experiment. This is often the case for
exploratory systems-based approaches and lesser the case
for validated assays. The two main components of an
experiment are often a test system and test item, where the
test system describes the exposure receptor (e.g., a clinical,
in vivo, or in vitro setup), and the test item describes the
exposure stressor (e.g., chemical compounds, cigarette
smoke, and its characterization). Both of these components
require terms that clearly specify the items. These two
components interact in an exposure experiment and their
interaction is described by the exposure conditions, for
example, exposure transport path, frequency, and doses.
The exposure condition, therefore, connects the test
system and the test items under the experiment description.
The exposed test system itself includes sampling proce-
dures, which are bound to various endpoint measurements.
In the case of systems-based approaches, the sampling
procedures cover a large number of procedures. The
sampling of the test items together with the endpoint
measurements leads to an outcome, which may be
associated with respiratory system diseases.
The main purpose of the ontology is to support annota-
tion of experimental data sets such as the details of the
experiment and its design, description of test item, test
system, as well as the exposure path to outcomes.
Additional file 1 shows an example on the use of CSEO to
annotate experiments. GeneChip Microarray experi-
ments generate high-throughput transcriptomic data
that can be reused for other research topics than the
originally designed experiment. Therefore, the FGED(Functional Genomics Data) society created standards
to exchange these and other similar data types related to
functional genomics. These standards not only include the
format of exchange but also the minimum requirements
for experimental annotation so that experimental data can
be correctly reproduced and reused. The exchange file
format is called MAGE-TAB [19], which includes an
IDF file for the definition of the investigation, a SDRF
file for the specification of each sample, and an ADF
file for the specification of the microarray analyte layout.
This file format is supported by the repository ArrayExpress
[20] and gives open access to a large number of functional
genomics datasets.
While MAGE-TAB defines the exchange format, there
is another standard that describes the required annotation
level, MIAME [21] the Minimum Information About
a Microarray Experiment. Additional file 1 shows an
example of the SDRF file that is MAGE-TAB and
MIAME compliant. Each row indicates the biological
samples with annotations and protocols for biological
sample transformation. The data model starts with a
subject, which is an animal model including additional
information about type, strain, and gender. When a protocol
applies, the biomaterial is changed, here from an untreated
animal to a treated animal. The treatment is further
described with the exposure item, brand, smoking regimen,
nicotine concentration, exposure path, and exposure
duration. The next protocol defines a post-exposure
treatment and affects only part of the samples. After
all exposures, the animal is dissected into organ parts
that are described by the next protocol. The organ
part is now further defined as frozen alveolar tissue area
from left lung of each animal. The next protocols define
Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 4 of 11
http://www.jbiomedsem.com/content/5/1/31lysis in this tissue and the extraction of RNA that is
hybridized on a GeneChip. The SDRF file ends with the
reference to the raw data file names, processed data file
name, and a summary of all experimental factor values.
All protocols are defined in the IDF file (not shown).
MAGE-TAB requires the use of ontology defined
terms. The ontology resource is specified with location
and version in the IDF. Yellow marked columns in
Additional file 1 show the CSEO annotations that
cover a large fraction of the SDRF file and ensure rich
and proper annotation. The annotation level of this file is
much richer than the MIAME requirement and supports
the reproducibility and reusability of experimental data.
Furthermore, conceptualizing and organizing this
knowledge domain in the form of an ontology allows
efficient augmentation of biological knowledge retrieval
and extraction. Therefore, the sensitivity to which
biological mechanisms are modulated in response to
different risk factors posed by smoking toxicants in
the lungs can be captured.
Framework and architecture of the CSEO
The CSEO was designed to be compliant with the Basic
Formal Ontology (BFO). The BFO was adopted to define
the upper-level standard architecture. The BFO is designed
to support development of domain ontologies for scientific
research [22]. On the other hand, the ExO is the only
existing and intuitive semantic framework used by the
exposure science community that provides a good
template for plugging in subdomain ontologies related
to the exposure domain. Therefore, the ExO superclasses
were used as root concepts for the CSEO. Accordingly,
the CSEO populates the ExO for the concepts of theFigure 2 Schematic representation of the main ontology classes and
blue: is-a relations; yellow: ExO: is_associated_with; orange: ExO: interacts_w
violet: ExO: interacts_with.cigarette smoke risk subdomain and also complies
with requirements of the OBO Foundry and RO
(Relation Ontology). Figure 2 depicts the architecture
of the CSEO in relation to BFO and ExO and its main
classes. Such an architecture is expected to incorporate
provenance into the CSEO so that concepts can be traced
back to their corresponding upper-level classes in ExO
and BFO.
The CSEO comes in two different versions: the main
CSEO version is a BFO-compliant ontology, and the
second version is a controlled vocabulary version,
hereafter referred to as lexical version. The CSEO-BFO
version consists of the BFO top-level hierarchy into
which the adjusted ExO hierarchy was plugged. The
CSEO classes were organized underneath these layers as
a third layer of granularity. This is the so-called
computer-readable format of the CSEO, which
represents the formal ontology. The lexical version, on
the other hand, forms the so-called expert-readable
format and does not claim to be a standard-adhering
ontology in itself. Instead, it is an access point to the
CSEO classes that is intuitive and easy to navigate for
medical and biological experts. This lexical version
supports the creation and review of the ontology by
various experts within the field. It, furthermore, creates
a categorization of ontology classes and terms into
context categories inside the knowledge domain. This is
usable also for context-sensitive text mining i.e., it
contains a branch that collects all terms related to
exposure outcomes (including terms which are not
necessarily exposure types) compared to the CSEO-BFO
version where they have to be collected manually. Both
versions are available on the CSEO dedicated wiki website.class provenance between BFO, ExO, and CSEO. Arrow legend:
ith_an_exposure_stressor_via; brown: MGED: has_experiment_ design;
Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 5 of 11
http://www.jbiomedsem.com/content/5/1/31Three-dimensional evaluation of the CSEO
Structural measure
Measurement of the structural dimension of the ontology
reflects the organizational patterns of the concepts in
the ontology. The first draft of CSEO (version 1.0) is
composed of 20091 concepts, including the BFO and
ExO classes. Additional file 2 provides several metrics
on structural properties of the ontology. These metrics
include breadth, which relates to the cardinality of paths;
depth, which relates to the cardinality of paths in a
graph; tangledness, which relates to multi-hierarchical
nodes; and fanout factor, which relates to the dispersion
of nodes.
As shown in Additional file 2, the high number of
classes and leaves together with high values for average
width and the fanout factor, point towards a broad
coverage of concepts by the ontology whereas the values
for depth show specificity of the concept types to the
domain of cigarette smoke exposure risk. The tangledness
factor of 0.71 indicates the presence of multi-hierarchical
nodes in the ontology (i.e. categories having multiple
parents). This is beneficial when greater crosslinking of
the domain concepts is desired. Different relation types
from RO were used to relate concepts in the CSEO
including part_of , precedes, has_participant, etc. Figure 2
illustrates the relational view of the second-level concepts
in the CSEO.
Functional measure
Measuring the functional dimension of the ontology
indicates how well the conceptualization of the ontology
captures the semantic space of the knowledge domain. The
lexicalized ontology was used to calculate precision, recall,
and F-score values (69.23, 77.81, 73.26, respectively).
The result of this evaluation shows that the ontology
in its current form is able to capture a wide range of
concepts related to cigarette smoke exposure in the
knowledge domain of exposure with a reasonable sensitivity
and specificity towards manual curation. The F-score of
above 73% reflects the quality output of the ontological
search in the published knowledge domain of cigarette
smoke exposure risk.
Usability profile
Usability profile of an ontology is defined by the extent of
user-friendliness of the ontology in terms of easy navigation,
knowledge accessibility, and meta-information availability.
Navigation of the CSEO and its user interface has been
facilitated using the WebProtégé software, which provides a
web-based access to the content of the ontology without
the need for software installation [23]. By following the
hyperlink provided on the wiki website under CSEO
access, the user is directed to the WebProtégé page in
which clicking CSEO launches the formal BFO-compliantontology whereas clicking CSEO-Expert Readable hyperlink
launches the hierarchy of controlled vocabulary underlying
CSEO. The search field makes it possible to search for any
CSEO-related concept and locate it in the tree (Figure 3).
Feedbacks can be provided through the same portal and a
dedicated team will process them.
To increase the level of efficiency in accessing different
views (subdomains) of the ontology, the ExO root concepts
were used for further classification of the CSEO instants.
By this means, tracking exposure-specific concepts for
users becomes easier and more efficient. Meta-information
(i.e. annotations including synonyms, definition, and
reference) is provided for each concept in the CSEO
to enable users accessing relevant information.
Since a proper documentation is needed to ensure
direct access and efficient usability of the ontology, a
wiki environment was created that contains instructions for
using the ontology, documentation on purpose and scope of
the ontology, and information about interfacing to the
ontology. The wiki is accessible through the following
hyperlink in FireFox and Safari browsers: https://publicwiki-
01.fraunhofer.de/CSEO-Wiki/index.php/Main_Page.
Use-case scenario: answering competency questions by
experts
Ontology-driven information retrieval and extraction
systems will guide analysis of literature in precisely
answering complex scientific questions [24]. The lexicalized
form of the CSEO was used to automatically retrieve
and extract domain specific knowledge related to
cigarette smoke exposure risk from PubMed abstracts
(see Methods). Experts in the knowledge domain of
cigarette smoke exposure risk were asked to design
several complex questions to be posed to the ontology.
The following questions were considered to test the
performance of the ontology:
 What are the potential effects of the toxicity
induced by tobacco smoke constituents on smokers?
 Which toxicological studies are available that
measure total particulate matter in electrically
heated cigarettes?
 Which documents report on the use of experimental
mouse models for investigating the effect of
cigarette smoke exposure on the risk of COPD?
Queries were formulated in the SCAIView environment
using the CSEO terminology. SCAIView displays named
entities by markup of the text (e.g. PubMed abstracts).
The key feature of SCAIView is the possibility to perform
ontological search in biomedical text using concept
hierarchies and synonyms associated with each concept in
the ontology. While using the ontology in SCAIView, the
hierarchical organization of the ontology was preserved by
Figure 3 Illustration of term search and navigation through the CSEO.
Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 6 of 11
http://www.jbiomedsem.com/content/5/1/31transforming the ontology OWL file into an XML tree
structure. Subsequently, retrieved documents were
manually checked for containing correct answers to
the posed competency questions. Table 1 summarizes these
queries, their corresponding retrieval rate, and reference to
the relevant documents that contain correct answers
to competency questions. Titles of both relevant and
irrelevant abstracts are listed in Additional file 3.
These results indicate that application of the CSEO-
derived terminology to the semantic literature search
leads to retrieval of highly relevant publications containing
the correct answer to the posed competency question.
Moreover, highlighted CSEO concepts (terms) by
SCAIView allow users to detect and extract knowledge
statements, as illustrated in Figure 4. The CSEO termin-
ology can be accessed through the SCAIView search
engine under: www.scaiview.com/scaiview-academia.html.
Discussion
The CSEO covers relevant concepts in the field of
systems-based toxicology assessment and includes manyTable 1 Answering competency questions using CSEO-driven
Query (22.03.2013) No. of retrieved
docs:
(([CSEO: Smoke Constituent]) AND [CSEO: Toxicity])
AND [CSEO: Tobacco]
21
([CSEO: Electrically heated cigarette]) AND [CSEO:
Total Particulate Matter]
7
(([CSEO: Mouse model]) AND [CSEO: Cigarette Smoke
Exposure]) AND [MeSH Disease: Pulmonary Disease
Chronic Obstructive]
9terms from the conventional toxicology assessment. Thus,
the CSEO enables users to capture and integrate exposure
information from the beginning of the experiment to the
point of outcome measurement. Compared to other
relevant ontologies, the CSEO covers a large number
of concept classes including the 44 external ontologies.
Additionally, the CSEO uses semi-automated methods for
the term extraction and evaluation and therefore ensures
good coverage of the knowledge domain.
Another advantage of the CSEO over the existing
related ontologies is the enrichment of high-resolution
concepts that extends the higher-level exposure ontology
in areas where existing ontologies are particularly weak.
For instance, the CSEO describes mouse and rat strains
that are commonly used in exposure experiments,
includes human anatomy with a dedicated subclass to
microanatomy of the respiratory system, and articulates
staging of progressive diseases. Moreover, the CSEO can
be used for text mining and knowledge discovery purposes
because the CSEO is a lexicalized ontology that supports
ontology-driven information retrieval and extraction assemantic search in PubMed abstracts
No. of relevant
docs:
PMIDs of relevant documents:
17 (80.95%) 14521141 [25], 1188959 [26], 18848577 [27], 21651432
[28], 17661226 [29], 2002748 [30], 12857635 [31],
19330121 [32], 14698566 [33], 11731039 [34],
18383128 [35], 16859820 [36], 21651433 [37],
21417965 [38], 2165143 1[39], 15072838 [40] ,
18464053 [41]
7 (100%) 12975773 [42], 12975774 [43], 14698566 [33], 12975771
[44], 18590791 [45], 12975772 [46], 16963170 [47]
9 (100%) 20133926 [48], 19017996 [49], 23044435 [50], 22279084
[51], 18988919 [52], 21700603 [53], 20228194 [54],
19491340 [55],16510458 [56]
Figure 4 An example of highlighted CSEO terms in the PubMed abstracts as appears in the SCAIView environment. The highlighted
terms guide users to informative statements and facilitates their detection, quality check and extraction.
Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 7 of 11
http://www.jbiomedsem.com/content/5/1/31described in the application scenario. Finally, the ability to
use the CSEO in different systems may be facilitated
by the BFO upper-level ontology. Thus, various sub-
ontologies relevant to exposure can be integrated with
the ExO-CSEO structure under the BFO framework.
Similar to other ontologies, the CSEO suffers from the
sparse granularity and misclassification of concepts in
some parts of the ontology. Other shortcomings common
to all ontologies such as missing concepts, lack of standard
definitions, and incompleteness of synonym lists should be
addressed by engagement of the research community and
inclusion of their feedback in the process of ontology
enrichment. To facilitate the community contribution, a
website has been prepared with the aim of collecting users
feedback and providing access to the latest version of the
ontology. With the public release of the ontology, it is
hoped to reach out to the broader community and collect
feedback and comments, which will be integrated in the
future versions of the CSEO and be used to improve the
ontology. With the version 1.0 of the CSEO, the ontology
is sufficiently established to be useful for the scientific
community. Furthermore, the project team will continue
to review articles, abstracts, and other resources relevant
for the domain and to extract novel terms and synonyms.
New releases of the CSEO will be announced and made
available through the NCBOs bioportal.
Conclusions
With the creation of the CSEO including relevant terms
for describing exposure experiments, it can serve as a
powerful glossary for definition finding and relationship
visualization, facilitating the right use of terms. The
CSEO has the potential to grow in the future and be used
as a dictionary for various processes such as controllinginternal documents (e.g. Excel Workbooks) or efficient use
of Laboratory Information Management Systems (LIMS).
This functionality can be used for the identification of
relevant information (internally or publicly) or for the
extraction of relevant knowledge statements.
Methods
Defining scope of the CSEO
To define the scope, a qualitative survey was performed
involving various experts in the domain of environmental
exposure. Experts in toxicology, molecular biology, and
clinical pathology fields in PMI were consulted and asked
for their input on the concept classes that they deem
as necessary to describe the knowledge domain of
environmental exposure from their viewpoint. Based on
this input, boundaries of the knowledge domain to be pre-
sented by CSEO was determined as depicted in Figure 1.
Resources and tools
Different resources were used for construction of the
ontology (Additional file 4). General and common
concepts, for which an established ontological definition
exists, were captured. 44 publicly available ontologies
listed in Additional file 4 were re-used and the relevant
terms/classes/concepts were selectively integrated in the
CSEO along with their annotations. Specialized terms were
collected from various contributors mainly used for internal
process and workflow tracking in systems, such as Labora-
tory Information Management Systems (LIMS). Literature
sources either were searched by keywords (e.g. smoke, tox-
icity, cigarette, tobacco in PubMed) or were recommended
by experts (e.g. CORESTA publications or handbooks).
Additionally, relevant publicly available abstracts, a number
of relevant full-text articles, as well as The Handbook of
Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 8 of 11
http://www.jbiomedsem.com/content/5/1/31Cigarette Smoke Toxicity by David Bernhard were
reviewed. Here, relevant text bodies were manually an-
notated, relevant terms were extracted and enriched
with synonyms and integrated into the ontology.
The Protégé 4.2 (Build 276) [57], developed and
maintained by The National Center for Biomedical
Ontology together with its inbuilt HermiT 1.3.3 reasoner
[58] were used to construct the ontology. The Knowtator
plugin [59] was used for manual annotation of abstracts
inside the Protégé environment. The text-mining tool
ProMiner [60] was utilized for named entity recognition
of ontology terms in PubMed abstracts and results
were integrated with SCAIView [61] for context-sensitive
visualization of query results.
Ontology development and evaluation process
During the process of ontology building, a hybrid ap-
proach combining both bottom-up and top-down
methods was adopted so that the ontology was popu-
lated at the level of superclasses and subclasses simul-
taneously. The development of the CSEO was
accomplished in four phases according to the common
life cycle of the ontology building [62].
Phase I: Knowledge acquisition and conceptualization
Concepts were extracted from previously identified
resources (see Additional file 4). Resources wereFigure 5 Mapping resources used for generating the ontology contenclassified into two groups based on their contents: struc-
tured content and unstructured content. Concepts from
structured contents such as tables, ontologies, and lists
were integrated automatically whereas concepts from
unstructured contents such as free text of publications
were manually inspected and extracted with the help of
annotation tools. Figure 5 describes the cardinal map-
ping of resources to the ontology contents. All concepts
in the ontology were annotated by additional informa-
tion including synonym(s), definition(s), and reference
(s). In the BFO version of the CSEO, relationships
among concepts were defined based on the standard re-
lation types in the Relation Ontology (RO) [63] and were
checked using the HermiT reasoner.
Phase II: Terminology analysis and concept enrichment
Transformation of the ontology OWL format into a
dictionary file was achieved using a Java script. The
script extracts concept names and the corresponding
synonyms from the ontology OWL structure and assigns
unique identifiers to each concept. This dictionary
was incorporated into ProMiner for named entity
recognition. In a subsequent step, the major super-
class concepts were used as keywords for queries in
PubMed. Five hundred relevant abstracts were chosen
from the result list of each concept search. After
compiling all abstracts, the corpus was randomlyts to their corresponding branches in the CSEO.
Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 9 of 11
http://www.jbiomedsem.com/content/5/1/31divided into a training set (250 abstracts) and test set
(250 abstracts) using the randomization command in
Linux. To create the reference gold standard, suitable
annotation guidelines were developed so that annota-
tors are guided to keep the breadth and depth of the
ontology in mind. For enrichment purposes (here
optimizing both the ontology concepts and the corre-
sponding dictionary), the training set was analyzed for
false-negative entities, which  after individual expert
evaluation  was added to the ontology. Classes were an-
notated both manually and automatically by mapping them
to external ontologies. For this purpose, the National Cen-
ter for Biomedical Ontology (NCBO) was used [64]. CSEO
classes were manually annotated with equivalent external
ontology classes using an annotation property. These anno-
tations were then used to automatically retrieve synonym
information via the NCBO services. The evaluation process
required the performance comparison between automatic-
ally and manually annotated text from the same set.
Phase III: Evaluation
A metric-based approach evaluating the ontology was
used in three dimensions after the completion of the
ontology [65]. Structural evaluation was performed by
calculating features such as depth, breadth, and other
topological features. To evaluate the functional quality
of the ontology in terms of measuring the boundaries of
the knowledge domain it captures, precision, recall,
and F-score values were calculated. Precision is the
number of true positives (TP) divided by the sum of
TP and false positives (FP). Recall is the number of
TP divided by the number of results that should
have been returned (true positives (TP) + false nega-
tives (FN)). The F-score = 2 × (precision × recall)/
(precision + recall). These values were derived from
the longest string match found between automatically
annotated words using ProMiner and the human-curated
gold standard annotation for each abstract in the selected
corpus [66].
Phase IV: Visualization of concepts through the text
The ontology was integrated into the SCAIView literature
mining and visualization environment.
Additional files
Additional file 1: MAGE-TAB SDRF file with CSEO classes.
Additional file 2: CSEO ontology metrics.
Additional file 3: Titles of retrieved PubMed abstracts for
answering competency questions in Table 1.
Additional file 4: Resources used for construction of CSEO.
Competing interests
Authors declare no competing interests.Authors contributions
EY conceived of the study, carried out ontology construction studies,
participated in anntation and evaluation, and drafted the manuscript. SA
conceived of the study, carried out data collection, participated in ontology
construction and evaluation, and helped to draft the manuscript. MG
performed ontology formalization, dictionary generation and technical
evaluation. SA performed ontology construction and participated in
ontology annotation and evaluation. CC participated in stakeholder
engagement. JH participated in the design of the study and coordination.
MHA and MCP conceived of the study and participated in its design and
coordination. All authors read and approved the final manuscript.
Acknowledgements
The authors wish to thank PMI internals Walter Schlage, Sandra Wagner,
Pavel Pospisil, Michel Rotach, Regina Stabbert, Rodolphe Gualandris, Kishor
Lad, Eva Garcia, Jacques-Antoine Duret, and Carole Mathis for their terminology
contribution and review. Moreover, we would like to acknowledge external
collaborators Prof. Gerhard Scherer from ABF GmbH, Mehran Sharifi from
Labstat, Jacqueline Miller from JT International SA, Mark Ballantyne from
Covance Laboratories Ltd, and Carolyn Mattingly from NC State University.
Authors wish to thank Theo Mevissen, Juliane Fluck, and Bernd Müller for their
assistance in setting up text-mining version of the ontology, as well as Ashutosh
Malhotra and Stephan Springstubbe for further support on the ontology
generation.
Author details
1Fraunhofer Institute for Algorithms and Scientific Computing SCAI, Schloss
Birlinghoven, 53754 Sankt Augustin, Germany. 2Philip Morris International
R&D, Philip Morris Products S.A., Quai Jeanrenaud 5, 2000 Neuchâtel,
Switzerland. 3Carson Watts Consulting, 1266 Carson Watts Rd, King, NC
27021-7453, USA.
Received: 14 July 2013 Accepted: 3 July 2014
Published: 10 July 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Vaz et al. Journal of Biomedical Semantics 2014, 5:43
http://www.jbiomedsem.com/content/5/1/43
RESEARCH Open Access
TypOn: the microbial typing ontology
Cátia Vaz1,2*, Alexandre P Francisco1,3, Mickael Silva4, Keith A Jolley5, James E Bray5, Hannes Pouseele6,
Joerg Rothganger7, Mário Ramirez4 and João A Carriço4
Abstract
Bacterial identification and characterization at subspecies level is commonly known as Microbial Typing. Currently,
these methodologies are fundamental tools in Clinical Microbiology and bacterial population genetics studies to track
outbreaks and to study the dissemination and evolution of virulence or pathogenicity factors and antimicrobial
resistance. Due to advances in DNA sequencing technology, these methods have evolved to become focused on
sequence-based methodologies. The need to have a common understanding of the concepts described and the
ability to share results within the community at a global level are increasingly important requisites for the continued
development of portable and accurate sequence-based typing methods, especially with the recent introduction of
Next Generation Sequencing (NGS) technologies. In this paper, we present an ontology designed for the
sequence-based microbial typing field, capable of describing any of the sequence-based typing methodologies
currently in use and being developed, including novel NGS based methods. This is a fundamental step to accurately
describe, analyze, curate, and manage information for microbial typing based on sequence based typing methods.
Keywords: Ontology, Knowledge representation, Microbial typing methods
Introduction
It is widely known that different strains from a given
bacterial species may have distinct phenotypic behaviors,
such as a higher capacity to cause invasive disease, to
asymptomatically colonize the host or to present resis-
tance to antimicrobials [1]. Such distinguishing charac-
teristics can be usually attributed to lineages identified at
the level of the genotype. Microbial typing refers to the
methodologies used to identify these lineages and define
them at sub-species level. Microbial typing has impor-
tant implications in several health related fields such as
surveillance of infectious diseases, outbreak investigation
and control, identification of pathogen reservoirs, and
studies on pathogenesis [2]. Traditionally these method-
ologies were based on characterizing a limited number
of markers. These markers can be phenotypic character-
istics, such as the presence of certain structures on the
bacterial surface [3], or genetic characteristics, such as
the presence on the bacterial genome of DNA sequences
*Correspondence: cvaz@cc.isel.ipl.pt
1INESC-ID, R. Alves Redol 9, 1000-029 Lisboa, Portugal
2Instituto Superior de Engenharia de Lisboa, Instituto Politécnico de Lisboa, R.
Cons. Emídio Navarro 1, 1959-007 Lisboa, Portugal
Full list of author information is available at the end of the article
that are recognized and cleaved by specific enzymes, gen-
erating band patterns by gel electrophoresis [4]. More
recently, due to the low cost and increasing availability of
DNA sequencing technologies, the development of typ-
ing methods became focused on the use of DNA sequence
information.
Although these methods revolutionized microbial typ-
ing, through the creation of novel, unambiguous and
easily understandable nomenclatures for human use,
the existing databases still lack interfaces for machine-
readable formats, which can be used for automated data
submission and querying. An ontology describing the
concepts and relationships for sequence-based typing
methods can thus provide a powerful tool in the field.
Sharing data annotated in a common language, and in
a semantically rich machine-readable format, will allow
a better integration of the data existing in databases of
sequence-based typing methods, epidemiological infor-
mation and the novel NGS data being produced [5]. Fur-
thermore, it can facilitate comparison of different typing
schemas, and allow users to mine, in an effective way, the
ever-growing public data.
In this paper, we describe the design of TypOn, a
microbial typing ontology. TypOn was developed from a
© 2014 Vaz et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction
in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 2 of 11
http://www.jbiomedsem.com/content/5/1/43
previous prototype ontology [6], and focuses on sequence-
based typing methods, including novel NGS methodolo-
gies. We discuss the connection of TypOn to existing
ontologies, how to use it to annotate data already publicly
available, and the methods to effectively query it.
Domain description
Several typing methods have been used in outbreak
detection and epidemiological surveillance ranging from
phenotypic methods to fragment based methods and
sequence based methods [5,7].
Multilocus Sequence Typing (MLST) [8,9] is a widely
adopted methodology to type several diferent species of
microorganisms. This method is based on determining
the sequence of internal fragments of multiple (typically
seven) loci encoding proteins with housekeeping func-
tions. By a locus we understand a specific location in
the chromosome, where different sequences occupying a
given locus define distinct alleles of that locus. Each set of
loci defining an MLST scheme is proposed by a group of
researchers that usually also provide evidence supporting
its discriminatory power and suitability for its intended
purpose. When applying the methodology, the set of alle-
les identified at the loci considered define a sequence type
(ST), a key identifier with this methodology. The loci cho-
sen are usually different for each species, although some
species may share some or even all loci in their MLST
schemas. The number of loci can be also variable and
can be greater or smaller than the seven loci more com-
monly adopted. MLSTs large appeal for the community
was the reproducibility and portability of results, which
allowed the deployment of databases for several bacte-
rial species [10-12]. The strain nomenclature developed
by MLST facilitated the global tracking and immediate
comparison of microbial strains in clinical and research
settings.
Another sequence-based method that derived its suc-
cess from a common nomenclature and the ease of
strain classification, was spa typing for Staphylococcus
aureus [13], an important pathogen that is one of the
major causes of nosocomial infections [14]. This method
is based on repeat sequences present at a single locus, the
spa gene. These repeats are short sequences of DNA (in
the case of spa about thirty nucleotides) that, although
sharing consensus characteristics, can be variable in their
sequence. Their number is also variable and, in the case of
spa typing, this is expressed by a string of numbers repre-
senting the identity and number of repeats present at each
spa allele. An updated list of identified repeats and spa
types can be found at http://spaserver.ridom.de/.
Multilocus Variable Number of Tandem Repeats Anal-
ysis (MLVA) [15] is a method that is based on the num-
ber of repeat patterns present on several defined locus
that are, similarly to MLST, defined in a schema. Several
schemas are also available in multiple websites such
as http://www.mlva.eu/, http://www.mlva.net/ or http://
www.miru-vntrplus.org/.
With the advent of Next Generation Sequencing (NGS)
technologies, and the ability to produce a draft genome
sequence of a microbial strain in a couple of days instead
of weeks or months, researchers can use this informa-
tion to classify the strain according to new or previously
available sequence-based typing methods. Furthermore,
novel typing methods are being developed that are able to
probe tens, hundreds or even thousands of different loci
across the genome [16], effectively extending the MLST
concepts. An example of such a method is ribosomal
MLST [17]. Other whole genome methodologies probe
the genome for Single Nucleotide Polymorphisms (SNPs)
when compared to reference genomes [18,19] in order to
discriminate strains.
The main goal of typing methods is the characteriza-
tion of individuals existing in a given sample. The sample
under study can be recovered from sick or healthy sub-
jects or directly from the environment. The first process
is the isolation of the microorganisms to be character-
ized from the sample collected. Each individual, or in the
case of bacteria, each colony isolated from the micro-
bial population then becomes an isolate, referring to
the process of isolating it from a sample that contains
many microorganisms potentially representing distinct
species. In particular each isolate can then be identified
at subspecies, species, or genus level, identifying it as
belonging to a given taxonomic unit, i.e., taxon. More-
over, since most microorganisms reproduce asexually the
subsequent propagation of this isolate in the laboratory as
an axenic culture, would be expected to generate a clonal
population.
Each isolate can be associated with typing information
and ancillary details. The isolate can have the nucleotide
sequence of its genetic material determined and multi-
ple typing characteristics defined through different typing
methods, such as MLST, spa typing or MLVA [15]. There
are several categories of typing methods. For instance,
although MLST and spa typing are both genotypic meth-
ods, MLST is a multi-locus typing method while spa
typing is a single-locus typing method.
For the specific case of multi-locus typing methods,
several schemas have been defined as indicated above.
Each schema is characterized by the (possibly ordered)
set of loci selected for a given taxon, usually defined at
the species level. Each schema is then administrated by
one or a group of microbiologist, or by an institution, and
each isolates ancillary data and respective typing informa-
tion are deposited in public repositories and validated by
a curator. Different schemas can be defined for the same
taxon and an ontology will facilitate understanding the
relationships between different schemas. Ancillary data
Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 3 of 11
http://www.jbiomedsem.com/content/5/1/43
include information about the place where the microor-
ganism was isolated, information about environment or
host, and other possible contextual details. Later on, we
will discuss how this information can be added and anno-
tated in the context of TypOn.
We also note that data ownership is a particular delicate
issue in the surveillance of communicable diseases and, as
we will discuss later, the approach proposed in this paper
allows a straightforward implementation of the agreed
upon policies. The sensitive nature of the information
and the ethical issues associated can be also safeguarded
through the application of suitable access control lev-
els based on the ownership structure embedded in the
ontology.
Microbial typing ontology
TypOn is an OWL ontology for describing microbial typ-
ing, focusing on sequence based methods. Such a descrip-
tion will provide a universal framework for the exchange
of information on the microbial typing field, allowing
the integration of data coming from the numerous and
disparate online databases.
In the next Section, we describe and discuss the TypOn
ontology and its suitability for describing knowledge in
the microbiology typing methods domain. Later, we illus-
trate how to annotate and query microbial typing data and
information using TypOn.
Ontology modeling
The main concepts and properties defined in TypOn
are depicted in Figures 1 and 2. The ontology, which
is an extended version of a previous prototype ontol-
ogy [6], is available at http://purl.phyloviz.net/ontology/
typon. In this current version, we added new con-
cepts and refined existing ones, based on comments
made by domain experts, including microbiologists and
industry partners. The aim of TypOn is to represent
knowledge about any of the currently used sequence-
based microbial typing methods. TypOn can be reused
as well as expanded, whenever new requirements and
new technologies demand it. For backwards compatibil-
ity, older versions can still be used by explicitly stat-
ing the TypOn version. For instance, version 20140606
may be accessed and referenced as http://purl.phyloviz.
net/ontology/20140606/typon. This ontology has been
developed in the context of the Patho-NGen-Trace
project http://patho-ngen-trace.eu/project/ as a way to
standardize microbial data exchange between online
databases and current software using those databases.
Ongoing developments, new versions, as well as use
cases and examples, can be found in the project repository
https://bitbucket.org/phyloviz/typon, and documented in
the project wiki https://bitbucket.org/phyloviz/typon/
wiki. The base URL for TypOn, http://purl.phyloviz.net/
ontology/typon, redirects transparently to the last stable
version of the ontology, in RDF/XML format, located in
the master branch in the development repository.
The current version of TypOn has 44 classes, including
those imported from other ontologies, and 47 properties,
as shown in Table 1.
Although we will discuss examples of bacterial typing,
we believe that TypOn is equally applicable to typing
methods used to characterize any microorganism. Natu-
rally, this implies that other loci, in addition to those used
in the examples, will have to be entered into the ontology
and new schemas will have to be defined. The ontology
offers a flexible framework with which the existing and
future sequence typing methods can be described and the
examples are meant to illustrate its application as well
as its flexibility. As described above, the first process in
Figure 1 The Isolate concept and its object properties. Dashed lines represent object properties and solid lines represent subclass relations, e.g.,
STAllele is-a Allele. Properties hasLocus and isOfLocus have cardinality of exactly one. All other properties do not have any cardinality restriction.
Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 4 of 11
http://www.jbiomedsem.com/content/5/1/43
Figure 2 Typing Information hierarchy and its object properties. Dashed lines represent object properties and solid lines represent subclass
relations, e.g., Genotypic is-a Sequence-based Typing Information. Property hasSchema has cardinality of exactly one. All other properties do not have
any cardinality restriction.
microbial typing is the recovery of the microorganisms to
be characterized from the sample collected and, thus, Iso-
late is a main concept in TypOn and it is characterized by
several properties.
Figure 1 shows an overview of the Isolate class and its
related concepts and properties.
Each Isolate when identified at subspecies, species, or
genus level, belongs to a certain Taxon, a relation that
we express through the property isFromTaxon. The Taxon
Table 1 Statistics concerning concepts and properties
either defined in TypOn or reused from other
ontologies/vocabularies
Ontology Concepts
Object Data
properties properties
Microbial Typing 24 21 25
Ontology (TypOn)
Basic Formal 8 1 0
Ontology (BFO) [20]
Sequence Ontology 6 0 0
(SO) [21]
Environment Ontology 2 0 0
(EnvO) [22]
Ontology for Biomedical 1 0 0
Investigations (OBI) [23]
Uniprot Core 1 0 0
Ontology (UNIPROT) [24]
Friend of a 1 0 0
Friend (FOAF) [25]
The DBpedia 1 0 0
Ontology [26]
concept is reused from the Uniprot Core Ontology [27]
for classifying life forms. Moreover, we define that each
Isolate is an Organism, a concept that is reused from
the Ontology for Biomedical Investigations (OBI) [28].
We also know the Place from where each Isolate was
recovered, which we describe through the property iso-
latedAt. As with the Taxon concept, the Place concept is
reused from another ontology, in this case the DBpedia
ontology [29]. One can also define the environment mate-
rial or system where a given Isolate was collected. These
concepts are already found in the environment ontol-
ogy (EnvO) [22] and, hence, we reused them and we add
the properties isolatedOnMaterial and isolatedOnSystem,
relating these concepts with the concept Isolate. Each Iso-
late, can have Sequence information, i.e. the nucleotide
sequence of its genetic material, and Typing Information.
The property hasTypingInformation commonly has cardi-
nality higher than one for each Isolate, since several typing
methods, such as MLST or MLVA, can be applied to the
same Isolate. Later we will present an example of an isolate
with both MLST and spa typing information.
In this context, it is important to note that TypingIn-
formation is the root of a class hierarchy (see Figure 2).
This hierarchy can be extended by including new, and
already known, typing techniques, such as phenotypic
information related to antibiotic susceptibility. In particu-
lar, we are able to distinguish different categories of typing
methods. Let us considerMLST and spa Typing. Both are
Genotypic methods, but the first is a Multilocus method
while the latter is a Single Locusmethod.
Let us consider the concepts Locus, Allele, Schema and
MLST, as depicted in Figure 3. In MLST we can have
Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 5 of 11
http://www.jbiomedsem.com/content/5/1/43
Figure 3MLST typing information. Dashed lines represent object properties and solid lines represent subclass relations. Property isOfLocus has
cardinality of exactly one. All other properties do not have any cardinality restriction.
several typing schemas administrated by some Agent, a
concept reused from FOAF ontology [25], for instance the
database curator, and composed by a set of Schema Parts.
Such schemas are represented through the class Schema,
which has associated the property hasSchemaPart. The
Schema Part concept allows us to identify a particular
Locus and provide the index order for that locus in the
underlying schema through object property hasLocus and
data property index, respectively. In the case of MLST,
each locus identifies a region within the coding sequence
of an housekeeping gene. Thus, the object property hasLo-
cus associated to the Schema Part concept has cardinality
of exactly one.
As depicted in Figure 1 and indicated above, each Iso-
latemay have been characterized by more than one typing
method. In the case of MLST, this kind of typing infor-
mation can be subject to different schemas, resulting in
different sequence types, which are characterized by the
alleles found at each locus. Therefore, in our ontology,
we associate to each MLST both a schema and a set of
observed alleles through properties hasSchema and hasI-
dentifiedAllele, respectively. The property hasSchema has
cardinality of exactly one and, hence, MLST and MLVA
instances must be related with one and only one given
Schema. Notice that STAllele is an Allele (see Figure 1)
defined in a MLST Schema, disjoint from Spa Allele and
VNTAllele. So, we only associate to MLST typing infor-
mation the concept of STAllele. This is necessary given the
different ways in which the distinct alleles are defined in
these typing methods.
The entity Spa Allele is used in the context of spa typ-
ing. Each spa typing information has amatching spa Allele,
which corresponds to a specific sequence of repeats found
as a result of the amplification of the locus of the spa gene
of Staphylococcus aureus. In Figure 2 we can observe that a
given Spa Allele has repeat profile parts, i.e., an entity that
stores the index order of a given Spa Repeat in a given Spa
Allele. Notice that each Spa Repeat may occur more than
once in a given Spa Allele and that different Spa Alleles
can have distinct number of repeats.
Notice also the difference between properties hasI-
dentifiedAllele and hasDefinedAllele relating respectively
MLST and Locus concepts with the Allele concept. One
could imagine that alleles associated to a given MLST
instance could be obtained through the defined Schema,
since property hasDefinedAllele allows to relate loci and
alleles. But this is not the case. A Locus may have associ-
ated many alleles, with each of them belonging to many
MLST instances, and hence we cannot identify the allele
belonging to a particular MLST instance. That can be
accomplished through property hasIdentifiedAllele which
relates each MLST instance with its identified alleles.
Although we did not add those kind of assertions in our
ontology, we can still use this information to assert that
the identified alleles for a given MLST instance are suf-
ficient against a given Schema. We could even infer for
which schemas a given MLST instance provides enough
information.
It is also important to note that, by knowing only the
Locus, it is possible to identify the Taxon that it belongs
to, using the isOfTaxon property (see Figure 1).
Additional information for each class, such as sample
collection date and other id, are described through data
properties. For instance, the class Allele has data proper-
ties such as id and date entered. The class Isolate has data
properties such as sample collection date and date entered.
Whenever possible, we reuse concepts from and estab-
lish relations with other ontologies as indicated in Table 1.
An Agent is a concept imported from the FOAF ontol-
ogy [25] and it can be a person, a group of persons or
even an institution. In TypOn it allows the description
of a person or a group of persons who have curated the
Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 6 of 11
http://www.jbiomedsem.com/content/5/1/43
information about the isolate, who have submitted that
information to the database and who own the isolate.
These relations are described by the object properties
curated by, sent by and owned by, respectively. Reusing
the Agent concept is extremely useful because it will allow,
for instance, the use of TypOn together with the Web
Access Control ontology [30] for defining access control
levels in microbial typing databases, an important issue as
mentioned above. Several applications in defining access
control have been widely discussed and are well known to
the research community [31,32].
All TypOn concepts were derived from the Basic Formal
Ontology (BFO) [20], the Ontology for Biomedical Inves-
tigations (OBI) [23] and the Sequence Ontology (SO) [21]
to ensure upper-level interoperability with other ontolo-
gies. In order to avoid huge imports in TypOn we have
used OntoFox [33] to query and import only relevant con-
cepts in top level ontologies. These are then imported as,
and are available at, https://bitbucket.org/phyloviz/typon/
raw/master/imported.owl.
TypOn was also submitted to the BioPortal (https://
bioportal.bioontology.org/), hosted by the National Cen-
ter of Biomedical Ontologies (NCBO), being also available
at http://bioportal.bioontology.org/ontologies/TYPON.
Concepts such as Schema and Typing Information are
qualities (BFO_0000019), i.e., a categorical property. As
discussed, we have classified an Isolate as an organism
(OBI_0100026). More details in Figure 4.
Figure 5 depicts the Typon concepts that are related to
the Sequence Ontology. Notice that we define Locus as
a region (SO:0000001), since it is a named region on a
genome.
Another example is the Spa Repeat concept which is
a repeat unit (SO:0000657). Both Locus and Spa Repeat
are also generically dependent continuant (BFO_0000031)
since region (SO:0000001) and repeat unit (SO:0000657)
concepts are subclasses of BFO_0000031 as defined in the
sequence ontology (SO). Thus these concepts are related
to both SO and BFO (see Figures 4 and 5).
Use case
In this section, we illustrate how we can represent typ-
ing information annotated with the TypOn ontology. Our
example makes use of data regarding the characteriza-
tion of a Staphylococcus aureus isolate for the purpose
of this case. We will use the Turtle language [34] in the
description of our isolate.
We can represent the isolate named Sa66296 as follows:
@prefix dbpedia:
<http://dbpedia.org/resource/> .
@prefix typon:
<http://purl.phyloviz.net/ontology/typon#> .
:Sa66296
rdf:type typon:Isolate ;
rdfs:label "Sa66296"@en ;
typon:sampleCollectionDate
"2013-05-06T00:00:00Z"^^xsd:dateTime;
typon:typon#dateEntered
"2014-02-19T00:00:00Z"^^xsd:dateTime;
typon:isolatedAt dbpedia:Lisbon ;
typon:hasTypingInformation
:mlst105 ,
:spa_t002 ;
typon:isFromTaxon
<http://www.uniprot.org/taxonomy/1280>.
This is an instance of typon:Isolate labelled as
Sa66296. rdfs:label is an instance of rdf:Property that
may be used to provide a human-readable version
for the name of a resource. We further specify that
it has two pieces of typing information mlst105 and
Figure 4 Relationships among TypOn, the Basic Formal Ontology (BFO), and the Ontology for Biomedical Investigations (OBI). The lines
represent subclass relations.
Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 7 of 11
http://www.jbiomedsem.com/content/5/1/43
Figure 5 Relationship between TypOn and the Sequence Ontology (SO). The lines represent subclass relations.
spa_t002 (instances of typon:MLST and typon:spaTyping,
respectively). Thus, these pieces of typing information are
other individuals annotated with our ontology. We can
keep track of the dates when the isolate was collected and
when the isolate was entered into the system using proper-
ties sampleCollectionDate and dateEntered, respectively.
We also describe the origin of the isolate, with the indi-
vidual labelled Lisbon which has type dbpedia-owl:Place.
Note that this isolate was recovered in Lisbon, Portugal,
represented as a resource in DBpedia. Figure 6 depicts the
isolate information.
The individual labelled mlst 105 represents the
sequence based typing method with a schema defined by
the sequence of seven housekeeping loci, represented as
follows:
@prefix typon:
<http://purl.phyloviz.net/ontology/typon#> .
:mlst105
rdf:type typon:MLST ;
rdfs:label "mlst 105"@en ;
typon:ST "105"^^xsd:string ;
typon:hasIdentifiedAllele
:arcc1 ,
:aroe4 ,
:glpf3 ,
:gmk_4 ,
:pta_12 ,
:tpi_1 ,
:yqil28 ;
typon:hasSchema
:schema1 .
Note that the individual labelled schema1 identifies
the seven housekeeping loci, using individuals of type
typon:SchemaPart for keeping track of the index of each
locus:
@prefix typon:
<http://purl.phyloviz.net/ontology/typon#> .
:schema1
rdf:type typon:Schema ;
rdfs:label "schema 1"@en ;
typon:PubMedID "10698988"^^xsd:string ;
typon:hasSchemaPart
:schema_part_1 ,
:schema_part_2 ,
:schema_part_3 ,
:schema_part_4 ,
:schema_part_5 ,
:schema_part_6 ,
:schema_part_7 .
Let us now describe the individual that represent the
arcc locus, the first locus in the considered schema as
described by the individual labelled schema_part_1:
@prefix typon:
<http://purl.phyloviz.net/ontology/typon#>.
:schema_part_1
rdf:type typon:SchemaPart ;
rdfs:label "schema part 1"@en ;
typon:index "1"^^xsd:int ;
typon:hasLocus :arcc .
:arcc
rdf:type typon:Locus ;
rdfs:label "arcc"@en ;
typon:hasDefinedAllele :arcc1 ;
typon:isOfTaxon
<http://www.uniprot.org/taxonomy/1280> .
:arcc1
rdf:type typon:STAllele ;
rdfs:label "arcc 1"@en ;
typon:id "1"^^xsd:int ;
typon:isOfLocus :arcc ;
typon:hasSequence :sequence1 .
Note that the identified alleles of the loci can be
directly obtained from the individual labelled mlst105.
Furthermore all possible defined alleles can be obtained
from the respective loci. It is also possible to obtain the
locus that is associated to each allele, namely by prop-
erty typon:isOfLocus. Figure 7 summarizes the represen-
tation of the MLST typing information concerning the
isolate in our example. The complete example is available
Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 8 of 11
http://www.jbiomedsem.com/content/5/1/43
Figure 6 The isolate Sa66296 and its object and data properties. Dashed lines represent object properties and solid lines represent subclass
relations. Described properties do not have any cardinality restriction.
at https://bitbucket.org/phyloviz/typon/raw/master/test/
Sa66296.ttl.
Annotating data
In this section we discuss how to annotate a large dataset
with the TypOn ontology and how to perform queries.
We started by writing a D2RQ mapping for the data
available for Neisseria spp, one of the databases avail-
able in our local BIGSdb [16] installation. D2RQ [35,36]
is a mapping language and platform for treating non-
RDF relational databases as virtual RDF graphs, aiming
to expose RDBs on the semantic web. The mapping
allows us to reuse existing vocabularies and ontologies,
such as TypOn, to map relational schemas, such as the
one underlying BIGSdb. We note that our mapping is
not exhaustive and that we just annotated part of the
data with TypOn ontology. The mapping is available
at https://bitbucket.org/phyloviz/typon/raw/master/test/
BIGSdb_d2r_mapping.ttl.
Even though the D2RQ web app provides a SPARQL
endpoint, it turns out that queries may take a long time to
complete, causing high loads in the underlying database,
and the web app may also become irresponsive. To over-
come this issue we used the tool dump-rdf available with
D2RQ to dump all triples and we uploaded them to a local
instance of Virtuoso, which among other functionalities
includes a highly efficient triple store (http://virtuoso.
openlinksw.com/). Hence, a more responsive SPARQL
endpoint is available at http://data.phyloviz.net/sparql,
where we should select http://rest.phyloviz.net/neisseria/
as the default graph. All resources at http://rest.phyloviz.
net/neisseria/ are also dereferenceable through rewrite
rules against the SPARQL endpoint.
Let us consider a few SPARQL queries for illustra-
tion purposes. Imagine that we wanted to define a new
MLST schema that includes loci carB, glnA, and rpiA.
How can we find all isolates for which we already have
typing information under this new schema? It turns
out that we can answer this question with a SPARQL
query:
PREFIX typon:
<http://purl.phyloviz.net/ontology/typon#> .
PREFIX uniprot:
<http://purl.uniprot.org/core/> .
PREFIX neisseria:
<http://rest.phyloviz.net/neisseria/> .
SELECT (str(?isolate_id) as ?isolate)
?species
?carB
?glnA
?rpiA
FROM <http://rest.phyloviz.net/neisseria/>
WHERE {
?isolate_res
typon:isFromTaxon ?taxon;
Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 9 of 11
http://www.jbiomedsem.com/content/5/1/43
Figure 7MLST typing information for isolate Sa66296. Dashed lines represent object properties. Instancemlst 105 of typon: MLST is related with
instance schema 1 of typon: Schema through object property typon: hasSchema with cardinality of exactly one. Instancemlst 105 is also related with
several instances of typon: Allele, e.g., instance aroe 4, through typon: hasIdentifiedAllele. Instance schema 1 is related with several instances of typon:
SchemaPart, e.g., instance schema part 2, through property typon: hasSchemaPart. Each instance of typon: SchemaPart is related with an instance of
typon: Locus, e.g., instance schema part 2 is related with instance aroe, through property typon: hasLocuswith cardinality of exactly one. Each instance
of typon: Allele is related with an instance of typon: Locus, e.g., instance aroe 4 is related with instance aroe, through property typon: isOfLocus with
cardinality of exactly one. Each instance of typon: Locus is also related with an instance of typon: Allele, e.g., instance aroe is related with instance aroe
4 through property typon: hasDefinedAllele.
typon:hasAllele ?carB_res ;
typon:hasAllele ?glnA_res ;
typon:hasAllele ?rpiA_res ;
typon:isolateId ?isolate_id .
GRAPH ?taxon {
?taxon uniprot:scientificName ?species;
uniprot:rank uniprot:Species.
}
?carB_res
typon:isOfLocus
neisseria:resource/loci/carB ;
typon:id ?carB .
?glnA_res
typon:isOfLocus
neisseria:resource/loci/glnA ;
typon:id ?glnA .
?rpiA_res
typon:isOfLocus
neisseria:resource/loci/rpiA ;
typon:id ?rpiA .
}ORDER BY ?species ?carB ?glnA ?rpiA
We can then submit this query to our endpoint at http://
data.phyloviz.net/sparql and our results include:
isolate species carB glnA rpiA
...
058-24 Neisseria lactamica 22 18 29
09002S1 Neisseria lactamica 22 18 42
...
92001 Neisseria meningitidis 1 1 1
154 Neisseria meningitidis 1 1 1
H1964 Neisseria meningitidis 1 1 1
...
As another example, we may be interested in exploring
the variability at the third locus in any MLST schema in
our dataset, but only for isolates of Neisseria polysaccha-
rea found in Canada. Taking into account the relation-
ships defined in TypOn and Uniprot, we can retrieve this
variability as follows through a federated query:
PREFIX typon:
<http://purl.phyloviz.net/ontology/typon#> .
PREFIX uniprot:
<http://purl.uniprot.org/core/> .
PREFIX rdfs:
<http://www.w3.org/2000/01/rdf-schema#> .
PREFIX xsd:
<http://www.w3.org/2001/XMLSchema#> .
SELECT DISTINCT
?isolate
?schema
?locus
?allele_id
WHERE {
SERVICE
<http://beta.sparql.uniprot.org/sparql> {
?taxon
uniprot:scientificName
"Neisseria polysaccharea".
}
?isolate
typon:isFromTaxon ?taxon;
typon:isolatedAt
<http://dbpedia.org/resource/Canada>;
typon:hasAllele ?allele .
?schema typon:hasSchemaPart ?spart .
?spart
Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 10 of 11
http://www.jbiomedsem.com/content/5/1/43
typon:hasLocus ?locus_res ;
typon:index "3"^^xsd:int .
?locus_res rdfs:label ?locus .
?allele
typon:isOfLocus ?locus_res ;
typon:id ?allele_id .
}
By submitting this query to http://data.phyloviz.net/
sparql, we obtain
isolate schema locus allele_id
db:isolates/ db:schemes/1 aroE 286
5194
db:isolates/ db:schemes/1 aroE 289
5195
where the prefix db: stands for http://rest.phyloviz.net/
neisseria/resource/.
Final remarks
TypOn provides the basic concepts needed to establish
the vocabulary and the semantic relationships for differ-
ent sequence-based typing methods, and it is designed to
allow further expansion. It was defined based on three
different approaches to sequence-based typing: using the
DNA sequence information directly, using the sequence
of repeats in a DNA sequence, and for MLVA, using the
number of repeats in a locus. Since these three approaches
can be used to define many of the existing typing meth-
ods, TypOn can be easily expanded to encompass the
newer multilocus typing techniques that are appearing
based on NGS technologies, defined by expansion of
the MLST concepts to larger numbers of genes [37] or
by Single Nucleotide Polymorphism approaches, where
each position on the genome can be viewed as a locus
and the nucleotide present as an allele. Other advan-
tages of this ontology is that it can provide a consistent
link with legacy microbial typing techniques and pro-
vide a way to describe and annotate the evolution of
specific typing schemas. This will be of paramount impor-
tance, if schemas that will be constructed by grouping
loci from existing schemas or adding new loci, are to
be designed and represented in an accurate way. This
ontology is the first stepping stone on the implementa-
tion of a semantic web approach for the data repositories
in this field. It lays the foundation for a common lan-
guage that can be used to integrate and link data from
different typing databases and for a complete merging of
microbial typing withmicrobial genomics. Using the strat-
egy discussed in the previous section (Annotating data),
a SPARQL endpoint is already deployed for the Pubmlst
MLST databases at http://pubmlst.org/sparql. This end-
point accesses data annotated using TypOn for MLST
databases for 75 distinct bacterial species that are hosted
at Pubmlst.org and further 29 species hosted externally
to Pubmlst.org. A RESTful API is also being developed to
facilitate data access without requiring the SPARQL end-
point. Future work will focus on expanding the ontology
and creating and deploying RESTful APIs to perform not
only custom querying but also automated submission and
curation of data for authenticated users, in order to speed
up and distribute the curating process, and ensure better
quality and reproducibility of data in the field of microbial
typing.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
All authors contributed to the development of the ontology. CV, APF, JAC, MR
and MS created/edited textual definitions of ontology terms. CV, APF, MR and
JAC have wrote and edited the manuscript. CV, MS and JAC have contributed
with the individuals that exemplify the ontology. APF has annotated data in
our local BIGSdb and JAC has helped him in the SPARQL queries examples. All
authors discussed, read and approved both the ontology and the manuscript.
Acknowledgements
The work presented in this paper made use of data available at MLST.net [10],
PubMLST [11] and Institut Pasteur MLST Databases [12]. This study was partly
supported by the European Community grant FP7-278864-2
(PathoNgenTrace, http://www.patho-ngen-trace.eu/), and by national funds
through FCT  Fundação para a Ciência e Tecnologia, under projects
PTDC/EIA-CCO/118533/2010, EXCL/EEI-ESS/0257/2012, and
PEst-OE/EEI/LA0021/2013.
Author details
1INESC-ID, R. Alves Redol 9, 1000-029 Lisboa, Portugal. 2Instituto Superior de
Engenharia de Lisboa, Instituto Politécnico de Lisboa, R. Cons. Emídio Navarro
1, 1959-007 Lisboa, Portugal. 3Instituto Superior Técnico, Universidade de
Lisboa, Av. Rovisco Pais 1, 1049-001 Lisboa, Portugal. 4Instituto de
Microbiologia, Instituto de Medicina Molecular, Faculdade de Medicina,
Universidade de Lisboa, Av. Prof. Egas Moniz, 1649-028 Lisboa, Portugal.
5Department of Zoology, University of Oxford, Oxford, UK. 6Applied Maths NV,
Keistraat 120, 98308 Sint-Martens-Latem, Belgium. 7Ridom GmbH, Mendelstr.
11, D-48149 Münster, Germany.
Received: 20 June 2014 Accepted: 6 October 2014
Published: 18 October 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Karystianis et al. Journal of Biomedical Semantics 2014, 5:22
http://www.jbiomedsem.com/content/5/1/22RESEARCH Open AccessMining characteristics of epidemiological studies
from Medline: a case study in obesity
George Karystianis1,2, Iain Buchan2,3 and Goran Nenadic1,2*Abstract
Background: The health sciences literature incorporates a relatively large subset of epidemiological studies that
focus on population-level findings, including various determinants, outcomes and correlations. Extracting structured
information about those characteristics would be useful for more complete understanding of diseases and for
meta-analyses and systematic reviews.
Results: We present an information extraction approach that enables users to identify key characteristics of
epidemiological studies from MEDLINE abstracts. It extracts six types of epidemiological characteristic: design of
the study, population that has been studied, exposure, outcome, covariates and effect size. We have developed a
generic rule-based approach that has been designed according to semantic patterns observed in text, and tested it
in the domain of obesity. Identified exposure, outcome and covariate concepts are clustered into health-related
groups of interest. On a manually annotated test corpus of 60 epidemiological abstracts, the system achieved
precision, recall and F-score between 79-100%, 80-100% and 82-96% respectively. We report the results of applying the
method to a large scale epidemiological corpus related to obesity.
Conclusions: The experiments suggest that the proposed approach could identify key epidemiological characteristics
associated with a complex clinical problem from related abstracts. When integrated over the literature, the extracted
data can be used to provide a more complete picture of epidemiological efforts, and thus support understanding via
meta-analysis and systematic reviews.
Keywords: Text mining, Epidemiology, Key characteristics, Rule-based methodologyBackground
Epidemiological studies aim to discover the patterns and
determinants of diseases, and other health related states
by studying the health of populations in standardised
ways. They are valuable sources of evidence for public
health measures and for shaping of research questions in
the clinical and biological aspects of complex diseases.
Nevertheless, the increasing amount of published litera-
ture leads to information overload, making the task of
reading and integrating relevant knowledge a challenging
process [1-3]. For example, there are more than 23,000
obesity-related articles reporting on different epidemio-
logical findings, including almost 3,000 articles with
obesity/epidemiology as a MeSH descriptor in 2012, with* Correspondence: g.nenadic@manchester.ac.uk
1School of Computer Science, University of Manchester, Kilburn Building,
Oxford Road, Manchester, UK
2Health e-Research Centre, Manchester, UK
Full list of author information is available at the end of the article
© 2014 Karystianis et al.; licensee BioMed Cen
Creative Commons Attribution License (http:/
distribution, and reproduction in any mediummore than 15,000 such articles in the last 10 years.
Therefore, there is a need for systems that enable the
extraction of salient epidemiological study features in
order to assist investigators to reduce the time required
to detect, summarise and incorporate epidemiological
information from the relevant literature [4].
Epidemiology is a relatively structured field with its
own dictionary and reporting style, deliberately written
in a typical semi-structured format in order to standardize
and improve study design, communication and collabor-
ation. The standard characteristics in most epidemiological
studies include [5]:
 study design - a specific plan or protocol that has
been followed in the conduct of the study;
 population - demographic details of the individuals
(e.g., gender, age, ethnicity, nationality) participating
in an epidemiological study;tral Ltd. This is an Open Access article distributed under the terms of the
/creativecommons.org/licenses/by/4.0), which permits unrestricted use,
, provided the original work is properly credited.
Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 2 of 11
http://www.jbiomedsem.com/content/5/1/22 exposure - a factor, event, characteristic or other
definable entity that brings about change in a health
condition or in other defined characteristics;
 outcome - the consequence from the exposure in the
population of interest;
 covariate - a concept that is possibly predictive of
the outcome under study;
 effect size - the measure of the strength of the
relationship between variables, that relates outcomes
to exposures in the population of interest.
In this paper we present a system that enables the
identification and retrieval of the key characteristics
from the epidemiological studies. We have applied the
system to the obesity epidemiological literature. Obesity
is one of the most important health problems of the 21st
century [6], presenting a great public health and eco-
nomic challenge [7-9]. The rapid and worldwide spread
of obesity has affected people of all ages, genders, geog-
raphies and ethnicities. It has been regarded as a multi-
dimensional disorder [10], with major behavioural and
environmental determinants, with genetics playing only
a minor role [7].
Related work
In the last decade, a significant amount of research has
been performed on the extraction of information in the
biomedical field, especially on the identification of bio-
logical [11,12] and clinical concepts [13,14] in the litera-
ture. In clinical text mining, several attempts have been
made to extract various kinds of information from case
studies and clinical trials in particular [1-4,15-23]. For
example, De Bruijn et al. [22] applied text classification
with a weak regular expression matcher on randomized
clinical trial (RCT) reports for the recognition of key trial
information that included 23 characteristics (e.g. eligibility
criteria, sample size, route of treatment, etc.) with overall
precision of 75%. The system was further expanded to
identify and extract specific characteristics such as primary
outcome names and names of experimental treatment
from journal articles reporting RCTs [4], with precision of
93%. However, they focused solely on RCTs and especially
on randomized controlled drug treatment trials. Hara and
Matsumoto [1] extracted information about the design of
phase III clinical trials. They extracted patient population
and compared associated treatments through noun phrase
chunking and categorisation along with regular expression
pattern matching. They reported precision for popula-
tion and compared treatments of 80% and 82% respect-
ively. Hansen et al. [2] worked on RCTs identifying the
numbers of the trial participants through a support vec-
tor machine algorithm with 97% precision, while Fizman
et al. [19] aimed to recognize metabolic syndrome risk fac-
tors in MEDLINE citations through automatic semanticinterpretation with 67% precision. However, to the best of
our knowledge, there is no approach available for recognis-
ing key information elements from various types of epi-
demiological studies that are related to a particular health
problem.
Methods
Our approach involved the design and implementation
of generic rule-based patterns, which identify mentions
of particular characteristics of epidemiological studies in
PubMed abstracts (Figure 1). The rules are based on pat-
terns that were engineered from a sample of 60 epi-
demiological abstracts in the domain of obesity. Mentions
of six semantic types (study design, population, exposures,
outcomes, covariates and effect size) have been manually
identified and reviewed. Additionally, a development set
with additional 30 abstracts was used to optimise the per-
formance of the rules. These steps are explained here in
more details.
1. Abstract selection and species filtering. In the
first step, abstracts are retrieved from PubMed using
specific MeSH terms (e.g. obesity/epidemiology
[mesh]). They are checked by LINNAEUS, a species
identification system [24], to filter out studies based
on non-human species.
2. Building of dictionaries of potential mentions. In
the second step, a number of semantic classes are
identified using custom-made vocabularies that include
terms to detect key characteristics in epidemiological
study abstracts (e.g. dictionaries of words that indicate
tudy design, population totals, etc.  a total of
fourteen dictionaries). We also identify mentions of
Unified Medical Language System (UMLS) [25]
terms and additionally apply the Specialist lexicon
[26] in order to extract potential exposure, outcome,
covariate and population concepts. Finally, epidemio-
logical abstracts are processed with an automatic
term recognition (ATR) method for the extraction of
multi-word candidate concepts and their variants
[27,28]. Filtering against a common stop-word list
(created by Fox [29]) is applied to remove any
concepts of non-biomedical nature.
3. Mention-level application of rules. In the third
step, rules are applied to the abstracts for each of
the six epidemiological characteristic separately. The
rules make use of two constituent types: frozen
lexical expressions (used as anchors for specific
categories) and specific semantic classes identified
through the vocabularies (identified in step 2), which
are combined using regular expressions. The frozen
lexical expressions can contain particular verbs,
prepositions or certain nouns. Table 1 shows the
number of rules created for each of the six
Figure 1 The four steps of the approach applied to epidemiological abstracts in order to recognise key characteristics. Linnaeus is used
to filter out abstracts not related to humans; Dictionary look-up and automatic term recognition (ATR) are applied to identify major medical
concepts in text; MinorThird is used as an environment for the rule application and mention identification of epidemiological characteristics.
Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 3 of 11
http://www.jbiomedsem.com/content/5/1/22characteristics with some typical examples. As a
result of the application of rules, candidate mentions
of epidemiological concepts are tagged in text. We
used MinorThird [30] for annotating and
recognizing entities of interest.
4. Document-level unification. Finally, in cases
where several candidate mentions for a single
epidemiological characteristic were recognised in
a given document, we also unified them to get
document-level annotations using the following
approach: if a given mention is part of a longer
mention, then we select only the longer. Mentions
that are not included in other mentions (of the
same type) are also returned. In addition, where
applicable (i.e. for exposures, outcomes and covariates),
these mentions are mapped to one of the 15 UMLS
semantic groups (Activities and Behaviors, Anatomy,
Chemicals and Drugs, Concepts and Ideas, Devices,
Disorders, Genes and Molecular, Geographic Areas,Living Beings, Objects, Occupations, Organizations,
Phenomena, Physiology and Procedures). We decided
to perform the mapping to high-level UMLS semantic
groups to assist epidemiologists in the application of
an epidemiological sieve, which could help them
decide whether or not to include abstracts for more
detailed inspection. For example, highlighting different
types of determinant (e.g. demographic vs. lifestyle)
would be useful for considering the completeness
and relevance of factors in a particular study by
emphasizing possible connections between the
background of the exposure and/or the outcomes.
Results
Evaluation
We evaluated the systems performance at the document
level by considering whether selected spans were cor-
rectly marked in text. We calculated precision, recall
and F-score for each of the characteristic of interest
Table 1 Examples of rules for recognition of study design, population, exposure, outcome, covariate and effect size in
epidemiological abstracts
Characteristic
(number of rules)
Examples Identified span (in bold)
Study design
(16 rules)
Rule [@st a(types)]
Methods: This was a cross-sectional
study of 214 overweight/obese 
cross-sectional study
Population
(119 rules)
Rule a(totals) re((of|on|in)) [@stats a(clusters)]
Sibling study in a prospective cohort
of 208,866 men from 
cohort of 208,866 men
Rule @multiple re(with|in|on)? [a(clusters) re(with|without) @multiple]
bone mineral density in patients
with type 2 diabetes
bone mineral
density
in patients with type 2
diabetes
Exposure
(134 rules)
Rule a(relations) eq(between) [@multiple] eq(and) @multiple
 and analyze the association
between body mass index and
blood pressure in 
association Between body mass
index
and blood pressure
Rule [@multiple] a(be) a(related) a(with) eq(onset)? eq(of)?
Short sleep duration is associated
with onset of obesity
Short sleep
duration
is associated with onset of
Outcome
(100 rules)
Rule @factors eq(of) [@multiple]
Cardiovascular and disease related
predictors of depression
predictors of depression
Rule @multiple a(be) a(adverbs) a(related) a(with) [@multiple]
Conclusions coffee intake is inversely
associated with t2dm in Chinese.
coffee intake is inversely associated with t2dm
Covariate
(28 rules)
Rule a(adj) eq(for) [@multiple]
 after adjusting for age, smoking
status, and clinical history of
diabetes mellitus.
adjusting for age, smoking status, and
clinical history of diabetes
mellitus.
Rule eq(including) [@multiple] eq(as) @synonyms
 including visceral adipose tissue
(vat) and subcutaneous adipose
tissue (sat) as covariates.
including visceral adipose
tissue (vat) and
subcutaneous adipose
tissue (sat)
as covariates
Effect size
(15 rules)
Rule @multiple [a(preva) a(be) @perce]
Hernia prevalence was 32.4% Hernia prevalence was 32.4%
Rule @multiple @or @ci
 more likely to have elevated
blood pressure (or = 9.05, 95%
ci: 1.44, 56.83)
elevated blood
pressure
(or = 9.05, 95% ci: 1.44, 56.83)
The rule components in square brackets are the extracted spans that denote the key characteristic; the rest of the rule (if any) specifies the context. The rules
use explicit matching of spans (e.g. eq(onset)), regular expressions (re) for matching specific verbs or prepositions (e.g. re((of|on|in))), various vocabularies that
contain single (e.g. a(types)  matching words that indicate the conduction of a study (e.g. study, analysis, review)) and multiword terms (e.g. @st, a vocabulary
of epidemiological study designs (e.g. case control)). totals contains words that suggest the participant population; stats is a dictionary that contains numbers and words
that express numeric values (e.g., one hundred); clusters includes the variations that a population sample can be described (e.g., men, patients, individuals); multiple contains
single or multi-word biomedical concepts (e.g., depression, type 2 diabetes); relations is a dictionary with single words that describe an association between concepts
(e.g., relationship, link, association); factors contains single or multi-word terms that describe risk factors (e.g., risk factors, predictors); or is a dictionary that contains noun
phrases in which the effect size odds ratio can be expressed, including the ways in which its numeric value is presented (e.g., odds ratio = 1.34, or = 2.56); ci follows a
similar pattern for confidence interval with its assigned numeric value e.g., (95% ci = 0.91, 95% ci: 4.36, 5.48).
Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 4 of 11
http://www.jbiomedsem.com/content/5/1/22using the standard definitions [31]. In order to create an
evaluation dataset, 60 abstracts were randomly selected
from the PubMed results obtained by query obesity/
epidemiology[mesh] and manually double-annotated for
all the six epidemiological characteristics by the first author
and an external curator with epidemiological expertise.The inter-annotator agreement of 80% was calculated on
the evaluation dataset by the absolute agreement rate [32],
suggesting relatively reliable annotations.
Table 2 shows the results on the evaluation set, with
to the results obtained on the training and development
sets for comparison (Tables 3 and 4). The precision and
Table 2 Results, including true positives (TP), false
positives (FP), false negative (FN), precision (P), recall (R)
and F-score on the evaluation set
. Evaluation set (60 abstracts)
TP FP FN P R F
Study design 12 0 1 100.0 92.3 95.9
Population 35 1 4 97.2 89.7 93.3
Exposure 45 8 11 84.9 80.3 82.5
Outcome 73 19 13 79.3 84.8 82.4
Covariate 17 2 0 89.4 100.0 94.4
Effect size 65 2 10 97.0 86.6 91.5
All classes (micro) 247 32 39 88.5 86.3 87.4
All classes (macro) 91.3 88.9 90.0
Micro averages are calculated across all different document level mentions;
macro averages are calculated across different characteristics.
Table 4 Results, including true positives (TP), false
positives (FP), false negative (FN), precision (P), recall (R)
and F-score on the development set
. Development set (30 abstracts)
TP FP FN P R F
Study design 11 1 2 91.6 84.6 88.0
Population 36 4 4 90.0 90.0 90.0
Exposure 59 4 0 93.6 100.0 96.7
Outcome 65 13 1 83.3 98.4 90.2
Covariate 13 3 0 81.2 100.0 89.6
Effect size 50 17 5 74.6 90.9 81.9
All classes (micro) 234 42 12 84.7 95.1 89.6
All classes (macro) 85.7 93.8 89.5
Micro averages are calculated across all different document level mentions;
macro averages are calculated across different characteristics.
Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 5 of 11
http://www.jbiomedsem.com/content/5/1/22recall values ranged from 79% to 100% and 80% to
100%, with F-measures between 82% and 96%. The best
precision was observed for study design (100%). How-
ever, despite having a relatively large number of study
design mentions in the training set (38 out of 60), the
development and evaluation sets had notably fewer men-
tions and therefore the precision value should be taken
with caution. Similarly, the system retrieved covariate
characteristic with 100% recall, but again the number of
annotated covariate concepts was low. The lowest preci-
sion was observed for outcomes (79%), while exposures
had the lowest recall (80%). With the exception of study
design that saw a little increase (7.7%), recall decreased
for the rest of the characteristics when compared to the
values on the development set. On the other hand, effect
size had a notable increase in precision, from 75%
(development) to 97% (evaluation). Overall, the micro
F-score, precision and recall for all the six epidemiologicalTable 3 Results, including true positives (TP), false
positives (FP), false negative (FN), precision (P), recall (R)
and F-score on the training set
. Training set (60 abstracts)
TP FP FN P R F-score
Study design 37 5 1 88.0 97.3 92.5
Population 94 10 5 90.3 94.9 92.6
Exposure 104 21 14 83.2 88.1 85.5
Outcome 125 26 8 82.7 93.9 88.0
Covariate 13 4 0 76.4 100.0 86.6
Effect size 41 5 9 89.1 82.0 85.4
All classes (micro) 414 71 37 85.3 91.7 88.4
All classes (macro) 84.9 92.7 88.4
Micro averages are calculated across all different document level mentions;
macro averages are calculated across different characteristics.characteristics were 87%, 88% and 86% respectively, sug-
gesting reliable performance in the identification of epi-
demiological information from the literature.
Application to the obesity corpus
We applied the system on a large scale corpus consisting
of 23,690 epidemiological PubMed abstracts returned
by the obesity/epidemiology[mesh] query (restricted to
English). We note that a number of returned MEDLINE ci-
tations did not contain any abstract, resulting in 19,188
processed citations. In total, we extracted 6,060 mentions
of study designs; 13,537 populations; 23,518 exposures;
40,333 outcomes; 5,500 covariates and 9,701 mentions of
effect sizes.
Table 5 shows most frequent study types in obesity
epidemiological research. The most common epidemio-
logical study designs are cohort cross-sectional (n = 1,940;
32%) and cohort studies (n = 1876; 31% of all recognizedTable 5 The most frequent study designs extracted from
the obesity epidemiological literature
Study design Frequency %
Cross-sectional 1,940 32.0
Cohort 1,876 30.9
Review 678 11.1
Population/epidemiological 521 8.5
Case control 341 5.6
Observational 191 3.1
Non randomized controlled 109 1.7
Non randomized 109 1.7
Qualitative descriptive 95 1.5
Qualitative 49 0.8
Frequency is the number of documents, and the last column presents the
share within the entire set.
Table 6 The most frequent exposures extracted from the
obesity epidemiological literature
Exposures Frequency %
Obesity 2,450 10.4
Body mass index 1,351 5.7
Overweight 531 2.2
Age 394 1.6
Waist circumference 291 1.2
Physical activity 289 1.2
Hypertension 256 1.0
Metabolic syndrome 240 1.0
Body weight 218 0.9
Type 2 diabetes 206 0.8
Gender 193 0.8
Smoking 186 0.7
Abdominal obesity 135 0.5
Insulin resistance 128 0.5
Mortality 117 0.4
Adiposity 116 0.4
Weight gain 108 0.4
Diet 98 0.4
Childhood obesity 92 0.3
Weight loss 89 0.3
Waist to hip ratio 82 0.3
Education 79 0.3
Childhood 79 0.3
Socioeconomic status 75 0.3
Ethnicity 75 0.3
Depression 70 0.2
Central obesity 69 0.2
Pregnancy 67 0.2
Race 66 0.2
Blood pressure 66 0.2
Overweight/obesity 59 0.2
CVD risk factors 59 0.2
Height 55 0.2
Morbidity 54 0.2
Leptin 52 0.2
Birth weight 49 0.1
Asthma 49 0.1
Bariatric surgery 48 0.1
Physical inactivity 47 0.1
Family history 45 0.1
Frequency is the number of documents, and the last column presents the
share within the entire set.
Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 6 of 11
http://www.jbiomedsem.com/content/5/1/22studies), whereas there were only 109 (1.7%) randomized
clinical trials. Tables 6, 7, 8, 9, 10 and 11 present the most
frequent exposures, outcomes and covariates along with
their UMLS semantic types.
Discussion
Compared to other approaches that focused specifically
on randomized clinical trials, our approach addresses a
significantly more diverse literature space. We aimed at
extracting key epidemiological characteristics, which are
typically more complex than those presented in clinical
trials. This is not surprising because clinical trials are
subject to strict regulations and are reported in highly
standardised ways. Although this makes it difficult to
compare our results with those of others directly, we still
note that our precision (79-100%) is comparable to
other studies (67-93%). The overall F-score of 87%
suggests that a rule-based approach can generate reli-
able results in epidemiological text mining despite the
restrained nature of the targeted concepts. Here we
discuss several challenges and issues related to epi-
demiological text mining, and indicate the areas for fu-
ture work.
Complex and implicit expressions
Despite having relatively reliable annotations (recall the
inter-annotator agreement of 80%), epidemiological ab-
stracts feature a number of complex, varying detail and
implicit expressions that are challenging for text mining.
For example, there are various ways in which population
can be described: from reporting age, sex and geograph-
ical region to mentioning the disease the individuals are
currently affected with or that are excluded from the
study (e.g. The study comprised of 52 subjects with his-
tologically confirmed advanced colorectal polyps and 53
healthy controls [PMID  21235114]). Even more com-
plex are the ways in which exposures are expressed,
given that these are not often explicitly stated in text as
exposures but rather part of the context of the study.
Similarly, identification of covariate concepts is challen-
ging as only a small number of covariates are explicitly
stated in text.
Finally, out dictionary coverage and focus were quite
limited by design: we focused on biomedical concepts,
but other types of concepts may be studied as determi-
nants and outcomes, or being mentioned as covariates
(e.g., high school environmental activity). While these
have been addressed by application of ATR, more gen-
eric vocabularies may need to be used (see below for
some examples).
Error analysis on the evaluation dataset
Our approach is based on intensive lexical and ter-
minological pre-processing and rules to identify the key
Table 7 Distribution of UMLS semantic groups assigned
to exposures
Semantic group Frequency %
Disorders 8,700 36.9
Concepts/ideas 4,635 19.7
Physiology 3,969 16.8
Procedures 1,611 6.8
Activities/behaviors 1,285 5.4
Living beings 1,030 4.3
Chemicals/drugs 857 3.6
Objects 368 1.5
Genes/molecular 344 1.4
Anatomy 252 1.0
Phenomena 180 0.7
Geographic areas 145 0.6
Occupations 73 0.3
Devices 30 0.01
Organizations 21 0.0
Other 16 0.0
Table 8 The most frequent outcomes extracted from the
obesity epidemiological literature
Outcomes Frequency %
Obesity 5,220 12.9
Overweight 2,058 5.1
Type 2 diabetes 1,379 3.4
Body mass index 1,084 2.6
Hypertension 728 1.8
Cardiovascular disease 712 1.7
Metabolic syndrome 659 1.6
Mortality 460 1.1
Insulin resistance 297 0.7
Childhood obesity 289 0.7
Coronary heart disease 260 0.6
Death 250 0.6
Health 225 0.5
Waist circumference 211 0.5
Abdominal obesity 209 0.5
Smoking 194 0.4
Physical activity 193 0.4
Weight gain 181 0.4
Morbidity 180 0.4
Cvd risk factors 175 0.4
Weight 162 0.4
Adiposity 161 0.3
Overweight/obesity 155 0.3
Asthma 127 0.3
Blood pressure 122 0.3
Dyslipidemia 116 0.2
Body weight 110 0.2
Stroke 101 0.2
Central obesity 98 0.2
Depression 95 0.2
Weight loss 94 0.2
Underweight 91 0.2
Chronic diseases 91 0.2
Hypercholesterolemia 88 0.2
Cancer 86 0.2
Survival 85 0.2
Cardiovascular risk 85 0.2
Atherosclerosis 81 0.2
Coronary artery disease 78 0.1
Inflammation 68 0.1
Frequency is the number of documents, and the last column presents the
share within the entire set.
Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 7 of 11
http://www.jbiomedsem.com/content/5/1/22epidemiological characteristics. The number of rules de-
signed for obesity can be considered relatively high (412),
given that they were engineered from relatively small train-
ing (and development) datasets. On one hand, the number
of rules for study design (16), covariate (28) and effect size
(15) were rather small in comparison to others e.g., popula-
tion (119), indicating the existence of generic expression
patterns that can identify concept types from more generic
epidemiological characteristics (such as study design or ef-
fect size). However, disease-related concepts often include
a variety of determinants along with a number of outcomes
of various nature (e.g. anatomical, biological, disease-
related, etc.). Therefore, on the other hand, the task of rec-
ognizing these epidemiological elements (e.g., outcomes,
exposures) through a rule based approach is not an easy
task and requires a number of rules to accommodate dif-
ferent types of expression. We briefly discuss the cases of
errors for each of the characteristic below.
Study design
Due to the limited number of study design mentions
(only 13) in the evaluation set, the high values of preci-
sion, recall and F-score should be taken with caution.
There were no false positives in the evaluation data set.
However, it is possible that in a larger dataset, false posi-
tives could appear if certain citations report more than
one mention of different study types. In addition, study
designs without specific information can be ambiguous
and thus were ignored (e.g. Metabolic and bariatric
surgery for obesity: a review [False Negative]).
Table 9 Distribution of UMLS semantic groups assigned
to outcomes
Semantic group Frequency %
Disorders 21,809 54.0
Concepts/ideas 7,277 18.0
Physiology 3,810 9.4
Procedures 1,697 4.2
Living beings 1,616 4.0
Activities/behaviors 1,413 3.5
Chemicals/drugs 990 2.4
Anatomy 577 1.4
Objects 314 0.7
Genes/molecular 265 0.6
Phenomena 250 0.6
Geographic areas 137 0.3
Occupations 102 0.2
Organizations 36 0.0
Devices 28 0.0
Other 16 0.0
Table 10 The most frequent covariates extracted from
the obesity epidemiological literature
Covariates Frequency %
Age 1,066 19.3
Gender 631 11.4
Body mass index 346 6.2
Smoking 260 4.7
Education 160 2.9
Race 117 2.1
Physical activity 108 1.9
Alcohol consumption 83 1.5
Ethnicity 70 1.2
Type 2 diabetes 67 1.2
Race/ethnicity 60 1.0
Obesity 58 1.0
Waist circumference 53 0.9
Income 43 0.7
Hypertension 42 0.7
Socioeconomic status 39 0.7
Height 36 0.6
Marital status 33 0.6
Demographics 32 0.5
Parity 27 0.5
Smoking status 25 0.5
Energy intake 25 0.5
Lifestyle 22 0.4
Educational level 20 0.3
Birth weight 20 0.3
Weight 17 0.3
Maternal age 17 0.3
Family history 17 0.3
Exercise 16 0.2
Depression 15 0.2
Total energy intake 14 0.2
Region 13 0.2
Insulin resistance 13 0.2
Occupation 12 0.2
Family income 12 0.2
Blood pressure 12 0.2
Adiposity 11 0.2
Social class 10 0.1
Gestational age 10 0.1
Area 10 0.1
Frequency is the number of documents, and the last column presents the
share within the entire set.
Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 8 of 11
http://www.jbiomedsem.com/content/5/1/22Population
An analysis of false positives reveals that rules relying on
the identification of prepositional phrases associated
with populations (e.g. among and in) need more specific
presence of patient-related concepts. False negatives in-
cluded 3,715 deliveries or 895 veterans who had bar-
iatric surgery, which are referring to births and a
specific demographic respectively, but our lexical re-
sources did not contain those. Nevertheless, the F-score
for the population type was the second best (93%),
showing that a rule-based approach can be used to iden-
tify the participants in epidemiological studies. An inter-
esting issue arose in the identification of population
associated to meta-analyses. For example, the mention
included 3 studies involving 127 children was identified
by patterns but it is clear that a specific approach would
be needed for meta-analysis studies.
Exposures and outcomes
While outcomes are often explicitly mentioned in text
as such, exposure concepts are not, which makes the
identification of exposures a particularly challenging
task. Still, the use of dictionaries containing biomed-
ical concepts for identification of potential mentions
proved useful for capturing exposure concepts. How-
ever, dictionary-based look-up also contributed to in-
correct exposure candidates that were extracted from
non-relevant contexts. On the other hand, two frequent
causes of errors could be linked to missing concepts
from our dictionaries (e.g. late bedtimes or costs)
Table 11 Distribution of UMLS semantic groups assigned
to covariates
Semantic group Frequency %
Physiology 2,381 43.2
Concepts/ideas 1,044 18.9
Disorders 783 14.2
Activities/behaviors 591 10.7
Living beings 232 4.2
Procedures 184 3.3
Chemicals/drugs 112 2.0
Geographic areas 41 0.7
Occupations 34 0.6
Objects 29 0.5
Phenomena 26 0.4
Genes/molecular 17 0.3
Anatomy 17 0.3
Other 4 0.0
Organizations 4 0.0
Devices 1 0.0
Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 9 of 11
http://www.jbiomedsem.com/content/5/1/22and relatively complex exposure expressions (e.g. level
of PA during leisure).
An important source of errors was the confusion be-
tween exposures and outcomes, given they both refer to
similar (semantic) types whose instances can  in different
studies  be either exposure or outcome, and thus their
role can be easily misinterpreted as an outcome rather than
a studied determinant (and vice versa). We noted that rules
such as association between < exposure > and < outcome>
or <exposure > associated with < outcome> generated
encouraging results i.e., a number of TPs. This was
not surprising: when a clinical professional is studying
the relationship between two concepts, he explores the
link between an exposure and an outcome, which the
above patterns capture. Still, sometimes these patterns
would match links irrelevant to exposure/outcome re-
lationships (e.g. relationship between race and gender).
Cases like these result in the generation of both false pos-
itives and false negatives. Overall, a sentence-focused rule
based method may struggle to understand a concepts
role in a given case, and a wider context might need to
be considered.
Covariates
Covariates had only a limited number of identified
spans, hence any conclusion regarding the systems per-
formance is at most indicative. Still, the results could pro-
vide an initial indication that (at least explicit) covariate
mentions could be detected with good accuracy, despite
some false positives (e.g. a generic mention potentialconfounders was identified as a covariate in  after ad-
justment for potential confounders).
Effect size
The rules designed to recognize effect size spans were
based on the combination of numerical and specific
lexical expressions (e.g. relative risk, confidence
interval). A relatively high recall (87%) revealed that
this approach returned promising results, with only a
small number of mentions being ignored by the system,
but with high precision. False negatives included expres-
sions that included multiple values (e.g.,  increased
risks of overweight/obesity at the age of 4 years (odds ratio
(95% confidence interval): 15.01 (9.63, 23.38)),  bmi
statistically significantly increased by 2.8% (95% confi-
dence interval: 1.5% to 4.1%; p < 0.001) ).
Application to the obesity corpus
Although we had relatively good recall in both the devel-
opment and evaluation datasets, the experiments with
the entire obesity dataset have shown that the system ex-
tracted epidemiological information only from a limited
number of documents. We have therefore explored the
reasons for that.
Study design
We identified study type from only around 40% of proc-
essed articles (each tagged as obesity/epidemiology). To
explore whether those missed study design mentions
are due to our incomplete dictionaries and rules, we
inspected 20 randomly selected articles from those that
contained no identified study type, and we identified the
following possible reasons:
 No mention of study design: while the article
presents an epidemiological context, no specific
epidemiological study had been conducted (and thus
there was no need to specify study design)  this
was the case in almost 2/3 of the abstracts with no
study design;
 Summarised epidemiological studies: articles
summarizing epidemiological information but
without reporting a specific conducted study and its
findings (15% of the abstracts);
 Other study designs: studies including comparative
studies, surveys, pilot studies, follow-up studies,
reports, reviews that were not targeted for identifi-
cation (20% of the abstracts).
We note that we can see a similar pattern in the evalu-
ation dataset (which was randomly selected from the
obesity corpus). Importantly, for the majority of ab-
stracts in the evaluation dataset, if the system was able
to detect the study type, all other epidemiological
Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 10 of 11
http://www.jbiomedsem.com/content/5/1/22characteristics have been extracted with relative success,
providing a complete profile of an epidemiological study
(data not shown).
Covariates
Only 5,500 confounding factors were recognised. To ex-
plore the reason for so many articles not having covari-
ates extracted, a random sample of 20 abstracts in which
no covariate concept was identified was investigated.
None of the studied abstracts contained any covariate
mentions. Most abstracts used only generic expressions
(e.g., after adjustment for confounding factors, after
controlling for covariates) without specifying the re-
spective concepts. We note that we only processed ab-
stracts and it seems likely that covariates may be defined
in full-text articles.
Effect size
Similar observations to the ones made for the covariate
characteristic were noted for the effect size mentions
(only 9,701 mentions were extracted). We explored a
sample of 20 abstracts in which no effect size was recog-
nised. As many as 60% of the abstracts did not report
any observed effect size between the studied exposures
and outcomes due to the nature of the conducted study
(e.g. pilot study, systematic review, article). We failed,
however, to get effect size mentions in 40% of cases,
mainly because of mentions that contained coordinated
expressions (e.g. The prevalence of hypertension was
considerably higher among men than among women
(60.3% and 44.6%, respectively; PMID 18791341) or
statistical significance data, which are not covered by
our rules.
Outcomes
As opposed to other characteristics, the number of
recognised outcome concepts was more than double the
number of abstracts. This is not a surprise, as most
of the epidemiological studies include more than one
outcome of interest. In addition, with the current system,
we have not attempted to unify synonymous terms (unless
they are simple orthographic variants).
Conclusions
We presented a generic rule based approach for the
extraction of the six key characteristics (study design,
population, exposure(s), outcome(s), covariate(s) and ef-
fect size) from epidemiological abstracts. The evaluation
process revealed promising results with the F-score ran-
ging between 82% and 96%, suggesting that automatic
extraction of epidemiological elements from abstracts
could be useful for mining key study characteristics and
possible meta-analysis or systematic reviews. Also, ex-
tracted profiles can be used for identification of gaps andknowledge modelling of complex health problems. Al-
though our experiments focused on obesity mainly for the
purpose of evaluation, the suggested approach of identify-
ing key epidemiological characteristics related to a particu-
lar clinical health problem is generic.
Our current work does not include identification of
synonymous expressions or more detailed mapping of
identified terms to existing knowledge repositories, which
would allow direct integration of the literature with other
clinical resources. This will be the topic for our future
work. Another potential limitation of the current work is
that we focused only on abstracts, rather than full-text arti-
cles. It would be interesting to explore if full-text would
improve the identification (in particular recall) or it would
introduce more noise (reducing precision).
Availability and requirements
Project name: EpiTeM (Epidemiological Text Mining)
Project home page: http://gnode1.mib.man.ac.uk/
epidemiology/
Operating system(s): Platform independent
Programming language: Python
Other requirements: MinorThird
License: FreeBSD
Any restrictions to use by non-academics: None
Abbreviations
ATR: Automatic term recognition; FN: False negatives; FP: False positives;
P: Precision; R: Recall; RCT: Randomized clinical trial; TP: True positives;
UMLS: Unified Medical Language System.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
The study was conceived and designed by IB and GN. GK implemented the
system, provided the data and performed the experiments. All authors read
and approved the final manuscript.
Acknowledgements
We would like to thank Katherine McAllister (Institute of Population Health,
University of Manchester) for the annotation of the datasets. This work was
partially supported by the UK Medical Research Council via a PhD grant to
GK. GN and IB are partially-supported by the Health e-Research Centre (HeRC)
grant. GN acknowledges support from the Serbian Ministry of Education and
Science (projects III44006; III47003).
This article has been published as part of the Semantic Mining of Languages
in Biology and Medicine (SMLBM) thematic series of the Journal of Biomedical
Semantics. An initial version of the article was presented at the 4th International
Symposium on Languages in Biology and Medicine (LBM) in 2011.
Author details
1School of Computer Science, University of Manchester, Kilburn Building,
Oxford Road, Manchester, UK. 2Health e-Research Centre, Manchester, UK.
3Centre for Health Informatics, Institute of Population Health, University of
Manchester, Manchester, UK.
Received: 9 April 2014 Accepted: 15 April 2014
Published: 19 May 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Hoehndorf et al. Journal of Biomedical Semantics 2014, 5:15
http://www.jbiomedsem.com/content/5/1/15
REVIEW Open Access
Thematic series on biomedical ontologies in
JBMS: challenges and new directions
Robert Hoehndorf1*, Melissa Haendel2,3, Robert Stevens4 and Dietrich Rebholz-Schuhmann5,6
Abstract
Over the past 15 years, the biomedical research community has increased its efforts to produce ontologies encoding
biomedical knowledge, and to provide the corresponding infrastructure to maintain them. As ontologies are
becoming a central part of biological and biomedical research, a communication channel to publish frequent updates
and latest developments on them would be an advantage.
Here, we introduce the JBMS thematic series on Biomedical Ontologies. The aim of the series is to disseminate the
latest developments in research on biomedical ontologies and provide a venue for publishing newly developed
ontologies, updates to existing ontologies as well as methodological advances, and selected contributions from
conferences and workshops. We aim to give this thematic series a central role in the exploration of ongoing research
in biomedical ontologies and intend to work closely together with the research community towards this aim.
Researchers and working groups are encouraged to provide feedback on novel developments and special topics to
be integrated into the existing publication cycles.
Introduction
This editorial will explore the expectations linked to
a growing infrastructure around biomedical ontolo-
gies. Since they become an integral part of bio-
logical and biomedical research for the annotation
of data  its integration, analysis, and visualization
[1]  the demand for a place arises in which the scientific
community can be made aware of new ontologies, major
updates to existing ontologies, development and updates
to ontology-based tools, and the discussion of ontology-
based methods. The JBMS thematic series on Biomedical
Ontologies, and the annual JBMS Ontology Issue, will
fill these gaps and establish a hub of information about
biomedical ontologies and their scientific applications.
The role of ontologies in biological and biomedical
research has steadily increased in conjunction with the
increase in quality and quantity of data that is being col-
lected in all areas of biology. Not only is the number
of ontologies increasing, their size growing, their rele-
vance in biomedical research rising and they penetrate
*Correspondence: leechuck@leechuck.de
Contributed equally
1Department of Computer Science, Aberystwyth University, Llandinam
Building, SY23 3DB Aberystwyth, UK
Full list of author information is available at the end of the article
more areas of biology and biomedicine; ontologies have
also begun to play a key part in the interpretation of
the biomedical data as well as inspire the development
of new tools for end users and new analysis methods for
biomedical scientists. As a result, data integration and
interoperability has become a relevant cost factor in the
execution of big data projects and has been acknowledged
by national and international projects, for example by the
Elixir initiative, which aims to establish a biomedical IT
infrastructure across Europe [2]. The development and
application of ontologies will be an integral part of such
an infrastructure for the main reason that data interoper-
ability requires tools to explicitly describe the semantics
of terms used to characterize the features of data, and
ontologies are widely used to fill this role.
Which ontology did youmean?
There has been considerable debate in the ontology
research community as to what constitutes an ontology
in biology [3-5] and what properties an ontology should
have. Traditional axes of classification for ontologies
include the expressivity of the language used to develop
and distribute the ontologies, the applications for which
the ontologies are intended (i.e. who uses the ontology and
how) and the domain covered by the ontology. Arguments
pertain
© 2014 Hoehndorf et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication
waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise
stated.
Hoehndorf et al. Journal of Biomedical Semantics 2014, 5:15 Page 2 of 6
http://www.jbiomedsem.com/content/5/1/15
(a) To the degree of formality of the language used to
express the information in an ontology, i.e., whether
a formal language such as the Web Ontology
Language (OWL) [6] is used or a graph-based
representation without explicit formal semantics,
(b) To the complexity of the ontology description, i.e.,
whether rich axioms and relations are used or
whether a taxonomy, accompanied with textual
definitions of classes in an ontology, is sufficient,
(c) To the interpretation of what constitutes a class or
relation in an ontology, i.e., whether a class in an
ontology refers to something in the world or to a
mental construct, and
(d) To the orthogonality of the content, i.e., what
content has been incorporated from other ontologies
and for which purposes.
Depending on the intended applications, artifacts called
ontologies are developed with any combination of these
properties.
In the JBMS thematic series on Biomedical Ontolo-
gies, we employ a broad interpretation of ontology and
include artifacts that primarily provide vocabularies for
the purpose of data annotation as well as formal theo-
ries that provide a rich representation of certain aspects of
biomedicine. To annotate data within a database, a taxon-
omy of classes with labels and textual definitions is often
sufficient, while more expressive formal constructs would
be required if the ontology is developed to verify data
integrity.
Representing ontologies
The annotation of research data using an ontology enables
integration of data both within a database and across
multiple databases [1]. Ontologies provide a controlled
set of classes together with an explicit (formal or infor-
mal) representation of their meaning, a hierarchy between
these classes and complex axiom patterns (relations)
[7] between the classes, and ontologies facilitate data
integration when shared across multiple databases. The
taxonomic relations allow integration through general or
specific aspects even if exact matches between data items
can not be identified; and axioms between classes serve as
complex relations that facilitate further data integration.
Today, most biomedical ontologies are developed in
shared formal languages, either the OBO Flatfile Format
[8] or the Web Ontology Language (OWL) [6]. Both lan-
guages are tightly coupled and thus allow translations
between them [9,10] so that the OBO Flatfile Format can
now be considered to be a fragment of OWL [8].
The expressivity of a biomedical ontology is determined
by the particular subset of OWL that is being used to
formulate the ontologies, and serves as a major distin-
guishing factor. It characterizes the knowledge that can
be expressed (such as whether the ontology may contain
contradictions) and determines the complexity of gen-
eral tasks such as querying the ontology and categorizing
data with the ontology. OWL 2 comprises three emerg-
ing profiles (OWL EL, OWL QL and OWL RL) apart
from OWL DL [11]. The OWL EL profile forms a subset
which (a) allows to specify a taxonomy between classes
(i.e., to state that one class is the subclass of another), (b)
existential restrictions (i.e., to state that instances of one
class must stand in a relation to some instance of another
class), and (c) disjointness of classes (i.e., to state that two
classes cannot share any instances), and has been found
useful for a significant number of biomedical ontologies
[12-15].
Domains of ontologies and their applications
Ontologies are of particular importance in domains in
which large volumes of data are being generated, and the
emergence of high-throughput technologies has increased
the importance of ontologies in some domains. In the
1990s, research on discovering gene functions in diverse
organisms required ameans to standardize gene functions
for comparison within and across multiple organisms:
this need induced the development of the Gene Ontol-
ogy (GO), which turned into one of the most important
resources in genomics research [16]. In a similar way, the
Sequence Ontology (SO) [17] emerged as a response to
the availability of more and more sequencing data, and to
provide compatibility between different data formats for
biological sequences and their features.
Different anatomy ontologies specify the organismal
components for multiple species, and  on a smaller scale
of granularity  the developmental relations and features
of cell types are characterized by the Celltype Ontology
[18]. Phenotype ontologies are also available for multi-
ple species and are widely used for the annotation of
the abnormalities observed in mutagenesis experiments
[19-21] as well as for the characterization of diseases and
drug effects [22].
Further domains covered comprise chemical entities to
annotate drugs and theirs biological activities [23], struc-
tures, and pharmaceutical applications [23,24] for data
interoperability [25], and ontologies for experimental set-
tings, e.g., the BioAssay Ontology [26], the Experimental
Factor Ontology [27], the eagle-i ontology [28] and the
Ontology of Biomedical Investigations [29], capture the
biomedical metadata to characterize experiments. Simi-
larly, ontologies for environmental conditions denote data
samples and their surroundings upon their encounter
[27,30]. Ontologies are also being used to annotate and
classify journal articles [31,32], pathways [33], and specific
biological entities [34].
Ontologies, together with their annotations, are exten-
sively used in the analysis of biomedical data, for example
Hoehndorf et al. Journal of Biomedical Semantics 2014, 5:15 Page 3 of 6
http://www.jbiomedsem.com/content/5/1/15
in the form of Gene Set Enrichment Analysis (GSEA) [35]
for the interpretation of gene expression datasets. GSEA
makes use of the structure of the Gene Ontology to iden-
tify statistically over- or under-represented classes based
on gene expression observed in two biological states. Sim-
ilar methods are also applied to other ontologies such as
the Human Disease Ontology [36], the Neuro Behavior
Ontology [37], or even the full set of ontologies contained
in BioPortal [38].
Another analysis method relying on ontologies is to
compare data items and identify meaningful biologi-
cal relations between them based on semantic similarity
[39]. This approach has been applied to identify protein-
protein interactions [40], classify chemicals [41], suggest
candidate genes involved in diseases [42,43] and repur-
pose drugs [44,45]. When applying semantic similarity to
compare two data items, the choice of ontology deter-
mines the kind of similarity that is revealed: using GO
will provide functional similarity, chemical entities from
ChEBI will provide chemical structure similarity, and
using phenotype ontologies will result in phenotypic simi-
larity.
The integration of multiple ontologies  in particular
from different domains  can reveal relations between
annotated data items. For example, anatomy ontologies
for cross-species comparisons  linking homologous or
analogous anatomical structures  can be used to trans-
fer and compare annotations for multiple species [15,46].
For this purpose, the UBERON anatomy ontology [15] was
developed. It enables cross-species phenotype representa-
tions that have been applied to deciphering humanGWAS
data based on comparisons with mouse model pheno-
types [47] as well as the prioritization of candidate genes
and drug targets based on data from model organisms
[42,44,48,49].
Additionally, the rich axiom systems of some ontolo-
gies help to verify and classify data according to con-
straints on biological entities expressed in the ontologies.
One example of such an application has been the clas-
sification of proteins using ontologies [50], in which an
ontology provides rules according to which decisions
about the protein family are made. The same, or sim-
ilar, constraints expressed in ontologies can be used to
verify data, i.e., determine whether a data item com-
plies with the constraints expressed in the ontology or
not [51].
Themain challenges for research in biomedical
ontologies
Evaluation of ontologies and the development of a robust
research methodology
Establishing effective methods to evaluate ontologies 
both qualitatively and quantitatively, if possible  towards
fitness for a purpose is a major challenge in ontology
research [52]. Determining the best ontology for a given
purpose becomes important, and criteria such as the
ontology structure, formality, its complexity, its coverage,
as well as the amount of data annotated with it con-
tribute to this decision. Effective methods for evaluation
are particularly required for domains in which multiple
ontologies overlap in their content and intended appli-
cations, such as for human diseases where ICD, MeSH,
SNOMED CT, the Human Disease Ontology [53], the
Human Phenotype Ontology [22], the Unified Medical
Language System (UMLS) [54], and more specific ontolo-
gies such as the Infectious Disease Ontology [55], Malaria
ontology [56], etc. are being used.
The research methodology underlying the development
of biomedical ontologies will also improve when effective
evaluation criteria are being applied. The Ontology Sum-
mit [57] has addressed this need with the topic Ontology
Evaluation Across the Ontology Lifecycle in 2013, and
ontology evaluation featured prominently in panel dis-
cussions at the International Conference on Biomedical
Ontologies 2013 (ICBO) and will play a prominent role
at ICBO2014. The JBMS thematic series on Biomedi-
cal Ontologies will follow the community discussions to
address ontology evaluation principles and methods, and
their instantiation in community-agreed guidelines and
standards.
Standards and Interoperability: Linked Data and beyond
Efficient reuse of ontologies, and the knowledge they con-
tain, in the organization of open, linked data possibly
accessible through multiple public interfaces (SPARQL
endpoints) from different data providers is another chal-
lenge [58]. The main task is to balance the complexity
of processing and querying ontologies, which commonly
require the use of an automated reasoner, with the need
to efficiently query large, linked datasets. In particular
when multiple ontologies are used to annotate datasets
and automated reasoning over these ontologies provides
the means for finding relations between the classes in
these ontologies, the need for an infrastructure to support
combined queries over ontologies with queries over linked
data using SPARQL arises.
Recently, some applications have come forward in which
automated reasoning is used to answer complex queries
over ontologies and subsequently retrieve data [59-61].
At the same time, major providers of biological and
biomedical data such as the European Bioinformatics
Institute (https://www.ebi.ac.uk/rdf/) and UniProt (http://
beta.sparql.uniprot.org/) provide access to their content
through public SPARQL endpoints. In the future, we
expect exciting applications that combine reasoning over
ontologies in ontology repositories, such as the Ontol-
ogy Lookup Service [62], BioPortal [63] or OntoBee [64],
with (federated) SPARQL queries and provide a genuinely
Hoehndorf et al. Journal of Biomedical Semantics 2014, 5:15 Page 4 of 6
http://www.jbiomedsem.com/content/5/1/15
knowledge-driven way for exploring linked biomedical
data.
Knowledge-based analysis of biomedical data
Integration of ontologies  and the knowledge they
contain  in the analysis of biological and biomedical
data is yet another challenge. Ontologies have been suc-
cessfully integrated with biomedical analysis pipelines
[35,39,65,66]. However, these analysis methods mainly
exploit the ontologies taxonomy and often make use of
the axioms and constraints only implicitly.
Many ontologies contain a lot more information than
taxonomic relationships, and some recent work has begun
to exploit some additional information  disjointness
between classes in an ontology  to improve computa-
tion of semantic similarity [67]. How the rich information
that is further contained in formalized ontologies can be
incorporated in the analysis of biomedical data remains
a research question, and novel methods will likely appear
as the infrastructure and tool support around ontologies
evolves.
The JBMS thematic series on Biomedical
ontologies
The JBMS thematic series on Biomedical Ontologies will
provide the venue for publishing research about biomedi-
cal ontologies, their development, integration and quality
assurance. On a regular basis, we will have open calls for
papers on specific topics, and we welcome community
input for important challenges to address.
The annual JBMS Ontology Issue will become a central
part of the thematic series where we focus on ontologies
that have already been demonstrated to be useful for sci-
entific applications. The Ontology Issue is intended for a
wide audience of readers; it does not specifically target
researchers in ontology, but rather biological and biomed-
ical researchers who may want to apply ontologies in their
domain and require an overview over the currently avail-
able artifacts they can already use. In the Ontology Issue,
new ontologies can be described as well as updates to
existing ontologies. Updates in regular intervals produce
a better understanding of the progress in developing an
ontology and the major changes to its content, structure
and applications.
In the future, we aim to establish another regular call
in which ontology-based tools and applications will be
described and updates to these tools published. Addition-
ally, the thematic series will provide a venue to publish
conference and workshop papers, and interested work-
ing groups are encouraged to suggest special topics or to
contribute to existing publication cycles.
We aim to make the JBMS thematic series on Biomed-
ical Ontologies take a central role in the exploration of
current research in biomedical ontologies, and we intend
to work closely with the research community to achieve
this aim. All researchers are invited to express ideas and
demands, ask for feedback on topics, and provide sugges-
tions for novel developments.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
RH and DRS have drafted the first manuscript version. MH and RS have
contributed valuable improvements to the manuscript. All authors read and
approved the final manuscript.
Acknowledgements
The work of DRS is funded by the EU STREP project grant 296410 (MANTRA)
under the 7th EU Framework Programme within Theme Information Content
Technologies, Technologies for Digital Content and Languages
[FP7-ICT-2011-4.1].
Author details
1Department of Computer Science, Aberystwyth University, Llandinam
Building, SY23 3DB Aberystwyth, UK. 2OHSU Library and Department of
Medical Informatics, Oregon Health & Science University, Portland, Oregon,
USA. 3Department of Medical Informatics and Epidemiology, Oregon Health &
Science University, Portland, Oregon, USA. 4School of Computer Science, The
University of Manchester, Oxford Road, M13 9PL Manchester, UK. 5Department
of Computational Linguistics, University of Zürich, Binzmühlestrasse 14, 8050
Zürich, Switzerland. 6European Bioinformatics Institute (EMBL-EBI), Wellcome
Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK.
Received: 8 January 2014 Accepted: 9 February 2014
Published: 6 March 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Klein et al. Journal of Biomedical Semantics 2014, 5:11
http://www.jbiomedsem.com/content/5/1/11
RESEARCH Open Access
Benchmarking infrastructure for mutation text
mining
Artjom Klein1*, Alexandre Riazanov2, Matthew M Hindle3 and Christopher JO Baker1*
Abstract
Background: Experimental research on the automatic extraction of information about mutations from texts is
greatly hindered by the lack of consensus evaluation infrastructure for the testing and benchmarking of mutation text
mining systems.
Results: We propose a community-oriented annotation and benchmarking infrastructure to support development,
testing, benchmarking, and comparison of mutation text mining systems. The design is based on semantic standards,
where RDF is used to represent annotations, an OWL ontology provides an extensible schema for the data and
SPARQL is used to compute various performance metrics, so that in many cases no programming is needed to
analyze results from a text mining system. While large benchmark corpora for biological entity and relation extraction
are focused mostly on genes, proteins, diseases, and species, our benchmarking infrastructure fills the gap for
mutation information. The core infrastructure comprises (1) an ontology for modelling annotations, (2) SPARQL
queries for computing performance metrics, and (3) a sizeable collection of manually curated documents, that can
support mutation grounding and mutation impact extraction experiments.
Conclusion: We have developed the principal infrastructure for the benchmarking of mutation text mining tasks. The
use of RDF and OWL as the representation for corpora ensures extensibility. The infrastructure is suitable for
out-of-the-box use in several important scenarios and is ready, in its current state, for initial community adoption.
Introduction
Mutation text mining
The use of knowledge derived from text mining for men-
tions of mutations and their consequences is increasingly
important for systems biology, genomics and genotype-
phenotype studies. Mutation text mining facilitates a wide
range of activities in multiple scenarios including the
expansion of disease-mutation database annotations [1],
the development of tools predicting the impacts of muta-
tions [2,3], the modelling of cell signalling pathways [4]
and protein structure annotation [5,6]. The types of use-
ful text mining tasks specific to mutations range from the
relatively simple identification of mutation mentions [7,8],
to very complex tasks such as linking (grounding) iden-
tified mutations to the corresponding genes and proteins
[9-11], interpretation of the consequences of mutations in
*Correspondence: artjom.unb@gmail.com; bakerc@unb.ca
1Computer Science And Applied Statistics Department, University of New
Brunswick, Saint John, Canada
Full list of author information is available at the end of the article
proteins [12], or identifying mutation impacts [13,14] and
related phenotypes [15].
Although the demand for mutation text mining soft-
ware has lead to a significant growth of the experimental
research in this area, the development of such systems
and the publication of results is greatly hindered by the
lack of adequate benchmarking facilities. For example,
in developing a mutation grounding system [11] show-
ing an encouraging level of performance accuracy, 0.73,
on a homogeneous corpus of 76 documents, the authors
achieved only 0.13 on a heterogeneous corpus of larger
size. When the system was reimplemented (see [16]), the
authors encountered another challenge  the evaluation
of the new system by comparing it to the state-of-the-art
was practically unaffordable, despite the existence of sim-
ilar systems, due to the lack of consensus benchmarking
infrastructure.
Such challenges and evaluation issues are not unique or
specific for mutation text mining, and are also present in
other domains of biomedical text mining. In the follow-
ing subsection, we discuss benchmarking and evaluation
© 2014 Klein et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Klein et al. Journal of Biomedical Semantics 2014, 5:11 Page 2 of 13
http://www.jbiomedsem.com/content/5/1/11
difficulties in biological text mining in general, which are
also relevant to mutation text mining.
Benchmarking and evaluation challenges in biomedical
text mining
Benchmarks, in the form of annotated corpora and related
software utilities, are usually designed and created for
specific text mining tasks and support fixed, usually hard-
coded, evaluation metrics. Besides quantitative and qual-
itative characteristics  number of entities annotated,
distribution of annotation types, etc.  a corpus is char-
acterized by the format, annotation schema (semantics
of annotations, annotation types), and evaluation metrics
to calculate the performance of the text mining systems.
For example, the benchmark for MutationFinder [7] (one
of the most popular single point mutation extractors) is
in a custom tabular format, stores annotations and raw
text separately, has annotations of single point mutation
JOURNAL OF
BIOMEDICAL SEMANTICS
Tao et al. Journal of Biomedical Semantics 2014, 5:16
http://www.jbiomedsem.com/content/5/1/16EDITORIAL Open AccessA 2013 workshop: vaccine and drug ontology
studies (VDOS 2013)
Cui Tao1*, Yongqun He2 and Sivaram Arabandi3Abstract
The 2013 Vaccine and Drug Ontology Studies (VDOS 2013) international workshop series focuses on vaccine- and
drug-related ontology modeling and applications. Drugs and vaccines have contributed to dramatic improvements
in public health worldwide. Over the last decade, tremendous efforts have been made in the biomedical ontology
community to ontologically represent various areas associated with vaccines and drugs  extending existing clinical
terminology systems such as SNOMED, RxNorm, NDF-RT, and MedDRA, as well as developing new models such as
Vaccine Ontology. The VDOS workshop series provides a platform for discussing innovative solutions as well as the
challenges in the development and applications of biomedical ontologies for representing and analyzing drugs and
vaccines, their administration, host immune responses, adverse events, and other related topics. The six full-length
papers included in this thematic issue focuses on three main areas: (i) ontology development and representation,
(ii) ontology mapping, maintaining and auditing, and (iii) ontology applications.Introduction and background
Drugs and vaccines have been critical to prevent and
treat human and animal diseases. Work in both (drugs
and vaccines) areas is closely related - from preclinical
research and development to manufacturing, clinical trials,
government approval and regulation, and post-licensure
usage surveillance and monitoring. Many drug and vaccine
related ontologies have already been or are being developed
for different use cases and applications. The 2013 Vaccine
and Drug Ontology Studies workshop (VDOS 2013) work-
shop series aims to become an international forum for
researchers to identify, propose, and discuss solutions for
important research problems in ontology representation
and analysis of vaccine and drug formation and prepar-
ation, administration, function mechanisms, and induced
host immune responses. The immune responses can be
positive responses for prevention and/or treatment of a
disease, or can be negative responses, i.e., adverse events.
This workshop aimed to support the deeper understand-
ing of vaccine and drug mechanisms and effects.
VDOS 2013 was held on July 7, 2013, at Montreal, Qc,
Canada. This workshop was part of the fourth Inter-
national Conference on Biomedical Ontology (ICBO 2013).* Correspondence: cui.tao@uth.tmc.edu
1School of Biomedical Informatics, The University of Texas Health Science
Center, Houston, TX, USA
Full list of author information is available at the end of the article
© 2014 Tao et al.; licensee BioMed Central Ltd
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.The workshop attracted interest from many international
attendees, including paper presenters, senior academic and
government scientists, postdoctoral fellows, and graduate
students. After a rigorous peer review process (all submis-
sions have been reviewed by at least three independent
reviewers), six full-length papers and three short-length
papers were accepted for proceeding paper publications
and oral presentations in the workshop. After one add-
itional round of independent peer reviewing by the
workshop co-organizers and the journal editors, the
selected six full-length papers were extended and accepted
for publication in the current issue of the Journal of
Biomedical Semantics (JBMS).
The VDOS-2013 workshop is the 2nd in this series.
The first workshop of the series was organized as the
Vaccine and Drug Ontology in the Study of Mechanism
and Effect workshop (VDOSME 2012) [1] on July 21,
2012, at Graz, Germany, as part of the third International
Conference on Biomedical Ontology (ICBO 2012). For
this year, the name has been changed to Vaccine and
Drug Ontology Studies (VDOS) to reflect the expansion
in the scope to more than just mechanism and effect.
The workshop series also covers vaccine and drug-related
clinical data representation and analysis, including clinic-
ally reported vaccine and drug adverse events.. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Tao et al. Journal of Biomedical Semantics 2014, 5:16 Page 2 of 3
http://www.jbiomedsem.com/content/5/1/16Summary of selected papers in the thematic issue
The six papers selected for this thematic issue are extended
versions of the original full-length papers presented at
the VDOS 2013 [2-7]. These papers cover a wide range
of topics including ontology development and repre-
sentation, ontology mapping, maintaining and auditing,
and ontology applications.
In the area of ontology development and representation,
Lin and He introduced their Ontology of Genetic Suscep-
tibility Factors (OGSF) for representing the susceptibility
factors for post vaccination events using a formal onto-
logical mechanism [2]. OGSF is aligned with the Basic
Formal Ontology (BFO). This paper ontologically defines
two core OGSF terms genetic susceptibility and genetic
susceptibility factor and the design pattern for represent-
ing genetic susceptibility to a vaccine adverse event. Two
use cases related to the genetic susceptibility to adverse
events following vaccination of an influenza vaccine or
smallpox vaccine were studied using the OGSF ontology.
Hanna et al built the Drug Ontology (DrOn) to model
drug information for comparative-effectiveness research
[3]. DrOn is represented in OWL2 and covers drug infor-
mation derived from three different sources (RxNorm,
ChEBI, and PRO). Although DrOn was originally designed
for comparative-effectiveness research studies, it can also
service many other use cases in the biomedical domain.
Marcos and He [4] developed the Ontology of Vaccine
Adverse Events (OVAE). OVAE was built as an extension
of the Vaccine Ontology (VO) and the Ontology of Adverse
Events (OAE). It represents and classifies the adverse events
recorded in package insert documents of commercial vac-
cines licensed by the USA Food and Drug Administration
(FDA). OVAE will be very useful in supporting rational
VAE prevention and treatment and benefits public health.
In the area of ontology mapping, Winnenburg, et al.
tried to map the terms in (Anatomical Therapeutic
Chemical (ATC) and Medical Subject Headings (MeSH)
through the Unified Medical Language System (UMLS)
[5]. Both lexical-based and instance-based mapping were
performed, which yielded hundreds of new mappings
between the two terminology systems. The alignment
between ATC and MeSH can be critical for drug evalu-
ation and safety studies as well as for pharmacogenetic
research. On one hand, MEDLINE literature is indexed
using MeSH. On the other hand, adverse drug events are
usually analyzed in reference to ATC. In order to integrate
drug information from these different sources, a reliable
alignment between ATC and MeSH is very much desired.
In the area of ontology applications, Doulaverakis et al.
present a semantic framework to discover drug-drug and
drug-disease interactions [6]. They use SKOS to represent
the semantics of the medical classification for drug rele-
vant information derived from ICD-10, Unique Ingredi-
ent Identifier (UNII), ATC, and the International VirusTaxonomy (IVT). Rule-based reasoning approaches were
then used to identify drug recommendations. Zhang et al.
introduce a novel approach that combines ontologies and
network analysis technologies for identifying new associa-
tions among vaccines, genes, and diseases [7]. The authors
leverage data extracted from MEDLINE and represented
this information using Resource Description Framework
(RDF). This RDF graph can then be viewed as a network
to perform network analysis.
Workshop presentations and discussions
In the workshop, the six full-length papers described above
were orally presented. In addition, three short papers were
accepted for short oral presentations. Zhu et al. introduces
their work on building a drug and drug class network
derived from multiple drug terminological resources, such
as ATC, National Drug File Reference Terminology (NDF-
RT), RxNorm, and Structured Product Label (SPL) [8]. He
et al. investigated how to audit the redundancies caused
by importing top-level ontologies [9]. More specifically,
they studied the redundancies in Drug Discovery Investi-
gations ontology (DDI) when importing BFO. Hall et al.
introduces their software that supports extracting new
drug information from drug structured product labels
(SPL) to update the DrOn [10].
During the discussion session, we discussed two main
areas  (a) mapping between different drug models (6
papers), and (b) adverse events detection and analysis
(3 papers). The major focuses on drug ontologies them-
selves (including models that contribute greatly in this
area), their goals, and the challenges in aligning them,
show that some of the preparatory work still needs to
be done to continue the research into adverse events. A
number of different approaches to drug mapping were
discussed  lexical, ingredient based, using chemical
structures and by exploiting networks (e.g. via the UMLS).
While each provided their unique benefits, there was
concurrence on the need for using more than one
method to improve the quality of mapping. Furthermore,
these papers also demonstrated the need for enhancing
the definitions (logical and textual) of the terms and the
relations both for improved human understanding as well
as for better integration between them. The two papers
focusing on adverse event detection and analysis also
highlighted some of the gaps and shed light on areas
for further ontology development efforts. These studies
also demonstrated promising usages and advantages of
ontology in standardizing, integrating, and analyzing
adverse event data.
Overall, the VDOS 2013 workshop provided an ideal
platform for ontology researchers and users to present
and discuss the progresses and issues in the development
and applications of ontologies related to vaccines and
drugs. Positive feedbacks were obtained.
Tao et al. Journal of Biomedical Semantics 2014, 5:16 Page 3 of 3
http://www.jbiomedsem.com/content/5/1/16Competing interests
The authors declare that they have no competing interests.
Acknowledgements
As editors of this thematic issue, we thank all the authors who submitted
papers, the Program Committee members and the reviewers for their
excellent work. We appreciate the support and help from the ICBO 2013
meeting organizers. We are grateful for editorial reviews from Dr. Dietrich
Rebholz-Schuhmann from JBMS.
Author details
1School of Biomedical Informatics, The University of Texas Health Science
Center, Houston, TX, USA. 2Unit for Laboratory Animal Medicine, Department
of Microbiology and Immunology, and Center for Computational Medicine
and Bioinformatics, University of Michigan Medical School, Ann Arbor, MI,
USA. 3Ontopro LLC, Houston, TX, USA.
Received: 26 February 2014 Accepted: 17 March 2014
Published: 20 March 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Clark et al. Journal of Biomedical Semantics 2014, 5:28
http://www.jbiomedsem.com/content/5/1/28RESEARCH Open AccessMicropublications: a semantic model for claims,
evidence, arguments and annotations in
biomedical communications
Tim Clark1,2,3*, Paolo N Ciccarese1,2 and Carole A Goble3Abstract
Background: Scientific publications are documentary representations of defeasible arguments, supported by data
and repeatable methods. They are the essential mediating artifacts in the ecosystem of scientific communications.
The institutional goal of science is publishing results. The linear document publication format, dating from 1665,
has survived transition to the Web.
Intractable publication volumes; the difficulty of verifying evidence; and observed problems in evidence and
citation chains suggest a need for a web-friendly and machine-tractable model of scientific publications. This model
should support: digital summarization, evidence examination, challenge, verification and remix, and incremental
adoption. Such a model must be capable of expressing a broad spectrum of representational complexity, ranging
from minimal to maximal forms.
Results: The micropublications semantic model of scientific argument and evidence provides these features.
Micropublications support natural language statements; data; methods and materials specifications; discussion and
commentary; challenge and disagreement; as well as allowing many kinds of statement formalization.
The minimal form of a micropublication is a statement with its attribution. The maximal form is a statement with its
complete supporting argument, consisting of all relevant evidence, interpretations, discussion and challenges
brought forward in support of or opposition to it. Micropublications may be formalized and serialized in multiple
ways, including in RDF. They may be added to publications as stand-off metadata.
An OWL 2 vocabulary for micropublications is available at http://purl.org/mp. A discussion of this vocabulary along
with RDF examples from the case studies, appears as OWL Vocabulary and RDF Examples in Additional file 1.
Conclusion: Micropublications, because they model evidence and allow qualified, nuanced assertions, can play
essential roles in the scientific communications ecosystem in places where simpler, formalized and purely
statement-based models, such as the nanopublications model, will not be sufficient. At the same time they will add
significant value to, and are intentionally compatible with, statement-based formalizations.
We suggest that micropublications, generated by useful software tools supporting such activities as writing, editing,
reviewing, and discussion, will be of great value in improving the quality and tractability of biomedical
communications.
Keywords: Argumentation, Annotation, Data citation, Digital abstract, Scientific discourse, Scientific evidence,
Methods citation, Research reproducibility, Nanopublications* Correspondence: tim_clark@harvard.edu
1Department of Neurology, Massachusetts General Hospital, 55 Fruit Street,
Boston, MA 02114, USA
2Harvard Medical School, 25 Shattuck Street, Boston, MA 02115, USA
Full list of author information is available at the end of the article
© 2014 Clark et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited.
Clark et al. Journal of Biomedical Semantics 2014, 5:28 Page 2 of 33
http://www.jbiomedsem.com/content/5/1/28Introduction
During the past two decades the ecosystem of biomedical
publications has moved from a print-based to a mainly
Web-based model. However, this transition brings with it
many new problems, in the context of an exponentially
increasing, intractable volume of publications [1,2]; of sys-
temic problems relating to valid (or invalid) citation of sci-
entific evidence [3,4]; rising levels of article retractions
[5,6] and scientific misconduct [7]; of uncertain reproduci-
bility and re-usability of results in therapeutic develop-
ment [8], and lack of transparency in research publication
[9]. While we now have rapid access to much of the
worlds biomedical literature, our methods to organize,
verify, assess, combine and absorb this information in a
comprehensive way, and to move discussion and anno-
tation activities through the ecosystem efficiently, re-
main disappointing.
Computational methods previously proposed as solutions
include ontologies [10]; text mining [2,11,12]; databases
[13]; knowledgebases [14]; visualization [15]; new forms of
publishing [16]; digitial abstracting [1]; semantic annotating
[17]; and combinations of these approaches. However, we
lack a comprehensive means to orchestrate these methods.
We propose to accomplish this with a layered metadata
model of scientific argumentation and evidence.
Such a common metadata representation of scientific
claims, argument, evidence and annotation in biomedi-
cine should serve as an integrating point for the original
publication, subsequent annotations, and all other com-
putational methods, supporting a single framework for
activities in the nine point cycle of authoring-publishing-
consumption-reuse we discuss in the section on Use
Cases. This cycle can be thought of as an information
value chain in science. This means that each set of dis-
parately motivated and rewarded activities, carried out
by various actors, creates and passes along value to the
next, which consumes this value-added product as an in-
put. A metadata representation to support this value chain
would need to:
 serve as a common Web-friendly nucleus for
value-addition and extraction across the biomedical
communications ecosystem: understood, operated
upon and exchanged by humans and by computers,
as supplements to the linear documents they
characterize;
 enable more powerful use and sharing of
information in biomedicine, particularly through
integration and mashup to provide the most
relevant views for any social unit of researchers;
 enable the addition of value to the content while
providing a detailed provenance of what was done;
 support computational processing in a way that
complete papers in un-augmented linear naturallanguage cannot yet integrate well with existing
linear textual representations.
This paper introduces the micropublications semantic
metadata model. The micropublications model is adapted
to the Web, and designed for (a) representing the key ar-
guments and evidence in scientific articles, and (b) sup-
porting the layering of annotations and various useful
formalizations upon the full text paper.
This model responds to the nine use cases we present,
in which digital summarization of scientific argumentation
with its evidence and methodological support is required.
These use cases, for the most part, deal directly with the
scientific literature, rather than its processed reflection in
curated topical databases. They illustrate how and why
currently proposed statement-based approaches need
richer representation and how this model can play such
a role.
In this paper we present
 a Use Case analysis mapped to sets of common
activities in the biomedical communications
ecosystem, showing the potential value addition and
path to implementation of the proposed model for
each Use Case;
 a formal model of micropublications;
 illustrative examples instantiating the model for each
Use Case;
 notes on an interface to nanopublications and other
statement-based formalizations;
 discussion on how the model can support
reproducibility and verifiability in research; on
implementation in software; and relationship to
other work; and
 our Conclusions about the role of this model in
next-generation scientific publishing.
We also provide, in three separate files of Additional
Material:
1. detailed class, predicate and rule definitions;
2. a proposed Web-friendly representation, using
community ontologies, serialized in the W3C
Web Ontology Language, with a set of examples
in RDF; and
3. a comparison of micropublications to the SWAN
model.
Background
Beyond statement-based models
Statement-based models have been proposed as mecha-
nisms for publishing key facts asserted in the scientific lit-
erature or in curated databases in a machine processable
form. Examples include: Biological Expression Language
Clark et al. Journal of Biomedical Semantics 2014, 5:28 Page 3 of 33
http://www.jbiomedsem.com/content/5/1/28(BEL) statements [18]; SWAN, a model for claims and
hypotheses in natural language developed for the annota-
tion of scientific hypotheses in Alzheimers Disease (AD)
research [14,19-21]; and nanopublications [22-26], which
contribute to the Open PHACTS linked data warehouse
of pharmacological data [26].
What we mean by statement-based is that they con-
fine themselves to modeling statements found in scien-
tific papers or databases, with limited or no presentation
of the backing evidence for these statements. Some offer
statement backing in the form of other statements in the
scientific literature, but none actually has a complete rep-
resentation of scientific argument including empirical evi-
dence and methods. Of the three examples we mention,
 Nanopublications model only the indicated
statement;
 SWAN models a principal statement, or
hypothesis, with supporting statements, or
claims, from the same publication only, and
JOURNAL OF
BIOMEDICAL SEMANTICS
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6
http://www.jbiomedsem.com/content/5/1/6
RESEARCH Open Access
Synonym extraction and abbreviation
expansion with ensembles of semantic spaces
Aron Henriksson1*, Hans Moen2, Maria Skeppstedt1, Vidas Daudaravic?ius3 and Martin Duneld1
Abstract
Background: Terminologies that account for variation in language use by linking synonyms and abbreviations to
their corresponding concept are important enablers of high-quality information extraction from medical texts. Due to
the use of specialized sub-languages in the medical domain, manual construction of semantic resources that
accurately reflect language use is both costly and challenging, often resulting in low coverage. Although models of
distributional semantics applied to large corpora provide a potential means of supporting development of such
resources, their ability to isolate synonymy from other semantic relations is limited. Their application in the clinical
domain has also only recently begun to be explored. Combining distributional models and applying them to different
types of corpora may lead to enhanced performance on the tasks of automatically extracting synonyms and
abbreviation-expansion pairs.
Results: A combination of two distributional models  Random Indexing and Random Permutation  employed in
conjunction with a single corpus outperforms using either of the models in isolation. Furthermore, combining
semantic spaces induced from different types of corpora  a corpus of clinical text and a corpus of medical journal
articles  further improves results, outperforming a combination of semantic spaces induced from a single source, as
well as a single semantic space induced from the conjoint corpus. A combination strategy that simply sums the cosine
similarity scores of candidate terms is generally the most profitable out of the ones explored. Finally, applying simple
post-processing filtering rules yields substantial performance gains on the tasks of extracting abbreviation-expansion
pairs, but not synonyms. The best results, measured as recall in a list of ten candidate terms, for the three tasks are:
0.39 for abbreviations to long forms, 0.33 for long forms to abbreviations, and 0.47 for synonyms.
Conclusions: This study demonstrates that ensembles of semantic spaces can yield improved performance on the
tasks of automatically extracting synonyms and abbreviation-expansion pairs. This notion, which merits further
exploration, allows different distributional models  with different model parameters  and different types of corpora
to be combined, potentially allowing enhanced performance to be obtained on a wide range of natural language
processing tasks.
Background
In order to create high-quality information extraction sys-
tems, it is important to incorporate some knowledge of
semantics, such as the fact that a concept can be signified
by multiple signifiersa. Morphological variants, abbrevia-
tions, acronyms, misspellings and synonyms  although
different in form  may share semantic content to differ-
ent degrees. The various lexical instantiations of a concept
*Correspondence: aronhen@dsv.su.se
Equal contributors
1Department of Computer and Systems Sciences (DSV), Stockholm University,
Forum 100, SE-164 40 Kista, Sweden
Full list of author information is available at the end of the article
thus need to be mapped to some standard representa-
tion of the concept, either by converting the different
expressions to a canonical form or by generating lexical
variants of a concepts preferred term. These mappings
are typically encoded in semantic resources, such as the-
sauri or ontologiesb, which enable the recall (sensitivity) of
information extraction systems to be improved. Although
their value is undisputed, manual construction of such
resources is often prohibitively expensive and may also
result in limited coverage, particularly in the biomedi-
cal and clinical domains where language use variability is
exceptionally high [1].
© 2014 Henriksson et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 2 of 25
http://www.jbiomedsem.com/content/5/1/6
There is thus a need for (semi-)automatic methods that
can aid and accelerate the process of lexical resource
development, especially ones that are able to reflect real
language use in a particular domain and adapt to differ-
ent genres of text, as well as to changes over time. In
the clinical domain, for instance, language use in gen-
eral, and (ad-hoc) abbreviations in particular, can vary
significantly across specialities. Statistical, corpus-driven
and language-agnostic methods are attractive due to their
inherent portability: given a corpus of sufficient size in
the target domain, the methods can be applied with no or
little adaptation needed. Models of distributional seman-
tics, building on the assumption that linguistic items with
similar distributions in large bodies of linguistic data
have similar meanings, fulfill these requirements and have
been used to extract semantically similar terms from large
corpora; with increasing access to data from electronic
health records, their application in the clinical domain
has lately begun to be explored. In this paper, we present
a method that employs distributional semantics for the
extraction of synonyms and abbreviation-expansion pairs
from two corpora: a clinical corpus (comprising health
record narratives) and a medical corpus (comprising jour-
nal articles). We also demonstrate that performance can
be enhanced by creating ensembles of (distributional)
semantic spaces  both with different model param-
eter configurations and induced from different genres
of text.
The structure of this paper is as follows. First, we
present some relevant background literature on syn-
onyms, abbreviations and their extraction/expansion. We
also introduce the ideas underlying distributional seman-
tics in general and, in particular, the models employed in
this study: Random Indexing and Random Permutation.
Then, we describe our method of combining semantic
spaces induced from single and multiple corpora, includ-
ing the details of the experimental setup and the mate-
rials used. A presentation of the experimental results is
followed by an analysis and discussion of their implica-
tions. Finally, we conclude the paper with a summary and
conclusions.
Language use variability: synonyms and abbreviations
Synonymy is a semantic relation between two phono-
logically distinct words with very similar meaning. It is,
however, extremely rare that two words have the exact
same meaning  perfect synonyms  as there is often at
least one parameter that distinguishes the use of one word
from another [2]. For this reason, we typically speak of
near-synonyms instead; that is, two words that are inter-
changeable in some, but not all, contextsc [2]. Two near-
synonyms may also have different connotations, such as
conveying a positive or a negative attitude. To compli-
cate matters further, the same concept can sometimes be
referred to with different words in different dialects; for
a speaker who is familiar with both dialects, these can
be viewed as synonyms. A similar phenomenon concerns
different formality levels, where one word in a synonym
pair is used only as slang and the other only in a more
formal context [2]. In the medical domain, there is one
vocabulary that is more frequently used by medical pro-
fessionals, whereas patients often use alternative, layman
terms [3]. When developing many natural language pro-
cessing (NLP) applications, it is important to have ready
access to terminological resources that cover this variation
in the use of vocabulary by storing synonyms. Examples of
such applications are query expansion [3], text simplifica-
tion [4] and, as already mentioned previously, information
extraction [5].
The use of abbreviations and acronyms is prevalent in
both medical journal text [6] and clinical text [1]. This
leads to decreased readability [7] and poses challenges
for information extraction [8]. Semantic resources that
also link abbreviations to their corresponding concept, or,
alternatively, simple term lists that store abbreviations and
their corresponding long form, are therefore as important
as synonym resources for many biomedical NLP appli-
cations. Like synonyms, abbreviations are often inter-
changeable with their corresponding long form in some, if
not all, contexts. An important difference between abbre-
viations and synonyms is, however, that abbreviations are
semantically overloaded to a much larger extent; that is,
one abbreviation often has several possible long forms,
with distinct meanings. In fact, 81% of UMLSd abbrevia-
tions in biomedical text were found to be ambiguous [6].
Identifying synonymous relations between terms
The importance of synonym learning is well recognized in
the NLP research community, especially in the biomedical
[9] and clinical [1] domains. A wide range of techniques
has been proposed for the identification of synonyms
and other semantic relations, including the use of lexico-
syntactic patterns, graph-based models and, indeed, dis-
tributional semantics [10]  the approach investigated in
this study.
For instance, Hearst [11] proposes the use of lexico-
syntactic patterns for the automatic acquisition of
hyponymse from unstructured text. These patterns are
hand-crafted according to observations in a corpus. Pat-
terns can similarly be constructed for other types of lexical
relations. However, a requirement is that these syntactic
patterns are common enough to match a wide array of
hyponym pairs. Blondel et al. [12] present a graph-based
method that takes its inspiration from the calculation of
hub, authority and centrality scores when ranking hyper-
linked web pages. They illustrate that the central similarity
score can be applied to the task of automatically extract-
ing synonyms from a monolingual dictionary, in this case
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 3 of 25
http://www.jbiomedsem.com/content/5/1/6
the Webster dictionary, where the assumption is that
synonyms have a large overlap in the words used in their
definitions; they also co-occur in the definition of many
words. Another possible source for extracting synonyms
is the use of linked data, such as Wikipedia. Nakayama
et al. [13] also utilize a graph-basedmethod, but instead of
relying on word co-occurrence information, they exploit
the links betweenWikipedia articles (treated as concepts).
This way they can measure both the strength (the number
of paths from one article to another) and the distance (the
length of each path) between concepts: concepts close to
each other in the graph and with common hyperlinks are
deemed to bemore closely related than those farther away.
There have also been some previous efforts to obtain
better performance on the synonym extraction task by not
only using a single source and a single method. Inspiration
for some of these approaches has been drawn from ensem-
ble learning, a machine learning technique that combines
the output of several different classifiers with the aim
of improving classification performance (see [14] for an
overview). Curran [15] exploits this notion for synonym
extraction and demonstrates that ensemble methods out-
perform individual classifiers even for very large corpora.
Wu and Zhou [16] use multiple resources  a monolin-
gual dictionary, a bilingual corpus and a largemonolingual
corpus  in a weighted ensemble method that combines
the individual extractors, thereby improving both preci-
sion and recall on the synonym acquisition task. Along
somewhat similar lines, van der Plas and Tiedemann [17]
use parallel corpora to calculate distributional similarity
based on (automatic) word alignment, where a trans-
lational context definition is employed; synonyms are
extracted with both greater precision and recall com-
pared to a monolingual approach. This approach is, how-
ever, hardly applicable in the medical domain due to the
unavailability of parallel corpora. Peirsman and Geeraerts
[18] combine predictors based on collocation measures
and distributional semantics with a so-called compound-
ing approach, wherein cues are combined with strongly
associated words into compounds and verified against a
corpus. This ensemble approach is shown substantially
to outperform the individual predictors of strong term
associations in a Dutch newspaper corpus. In informa-
tion retrieval, Diaz and Metzler [19] report increased
performance gains when utilizing language models that
derive evidence from both a target corpus and an external
corpus, compared to using the target corpus alone.
In the biomedical domain, most efforts have focused
on extracting synonyms of gene and protein names
from the biomedical literature [20-22]. In the clinical
domain, Conway and Chapman [23] propose a rule-based
approach to generate potential synonyms from the Bio-
Portal ontology  using permutations, abbreviation gener-
ation, etc.  after which candidate synonyms are verified
against a large clinical corpus. Henriksson et al. [24,25]
use models of distributional semantics to induce unigram
word spaces and multiword term spaces from a large cor-
pus of clinical text in an attempt to extract synonyms of
varying length for SNOMED CT preferred terms. Zeng
et al. [26] evaluate three query expansion methods for
retrieval of clinical documents and conclude that an LDA-
based topic model generates the best synonyms. Pedersen
et al. [27] explore a set of measures for automatically
judging semantic similarity and relatedness among med-
ical term pairs that have been pre-assessed by human
experts. The measures range from ones based on thesauri
or ontologies (WordNet, SNOMED-CT, UMLS, Mayo
Clinic Thesaurus) to those based on distributional seman-
tics. They find that the measure based on distributional
semantics performs at least as good as any of the ontology-
dependentmeasures. In a similar task, Koopman et al. [28]
evaluate eight different data-driven measures of seman-
tic similarity. Using two separate training corpora, one
containing clinical notes and the other medical literature
articles, they conclude that the choice of training cor-
pus has a significant impact on the performance of these
measures.
Creating abbreviation dictionaries automatically
There are a number of studies on the automatic creation of
biomedical abbreviation dictionaries that exploit the fact
that abbreviations are sometimes defined in the text on
their first mention. These studies extract candidates for
abbreviation-expansion pairs by assuming that either the
long form or the abbreviation is written in parentheses
[29]; other methods that use rule-based pattern matching
have also been proposed [30]. The process of determin-
ing which of the extracted candidates that are likely to
be correct abbreviation-expansion pairs is then performed
either by rule-based [30] or machine learning [31,32]
methods. Most of these studies have been conducted for
English; however, there is also a study on Swedish medical
text [33], for instance.
Yu et al. [34] have, however, found that around 75% of
all abbreviations found in biomedical articles are never
defined in the text. The application of these methods to
clinical text is most likely inappropriate, as clinical text is
often written in a telegraphic style, mainly for documen-
tation purposes [1]; that effort would be spent on defining
used abbreviations in this type of text seems unlikely.
There has, however, been some work on identifying such
undefined abbreviations [35], as well as on finding the
intended abbreviation expansion among several possible
expansions available in an abbreviation dictionary [36].
In summary, automatic creation of biomedical abbre-
viation dictionaries from texts where abbreviations are
defined is well studied. This is also the case for abbrevia-
tion disambiguation given several possible long forms in
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 4 of 25
http://www.jbiomedsem.com/content/5/1/6
an abbreviation dictionary. The abbreviation part of this
study, however, focuses on a task that has not as yet been
adequately explored: to find abbreviation-expansion pairs
without requiring the abbreviations to be defined in the
text.
Distributional semantics: inducing semantic spaces from
corpora
Distributional semantics (see [37] for an overview of
methods and their application in the biomedical domain)
were initially motivated by the inability of the vector
space model [38]  as it was originally conceived  to
account for the variability of language use andword choice
stemming from natural language phenomena such as syn-
onymy. To overcome the negative impact this had on
recall in information retrieval systems, models of dis-
tributional semantics were proposed [39-41]. The the-
oretical foundation underpinning such semantic models
is the distributional hypothesis [42], which states that
words with similar distributions in language  in the
sense that they co-occur with overlapping sets of words 
tend to have similar meanings. Distributional methods
have become popular with the increasing availability
of large corpora and are attractive due to their com-
putational approach to semantics, allowing an estimate
of the semantic relatedness between two terms to be
quantified.
An obvious application of distributional semantics is
the extraction of semantically related terms. As near-
synonyms are interchangeable in at least some contexts,
their distributional profiles are likely to be similar, which
in turn means that synonymy is a semantic relation that
should, to a certain degree, be captured by these meth-
ods. This seems intuitive, as, next to identity, the highest
degree of semantic relatedness between terms is real-
ized by synonymy. It is, however, well recognized that
other semantic relations between terms that share similar
contexts will likewise be captured by these models [43];
synonymy cannot readily be isolated from such relations.
Spatial modelsf of distributional semantics generally
differ in how vectors representing term meaning are con-
structed. These vectors, often referred to as context vec-
tors, are typically derived from a term-context matrix that
contains the (weighted, normalized) frequency with which
terms occur in different contexts. Working directly with
such high-dimensional (and inherently sparse) data 
where the dimensionality is equal to the number of con-
texts (e.g. the number of documents or the size of the
vocabulary, depending on which context definition is
employed)  would entail unnecessary computational
complexity, in particular since most terms only occur in
a limited number of contexts, which means that most
cells in the matrix will be zero. The solution is to project
the high-dimensional data into a lower-dimensional
space, while approximately preserving the relative dis-
tances between data points. The benefit of dimensionality
reduction is two-fold: on the one hand, it reduces com-
plexity and data sparseness; on the other hand, it has
also been shown to improve the coverage and accuracy
of term-term associations, as, in this reduced (semantic)
space, terms that do not necessarily co-occur directly in
the same contexts  this is indeed the typical case for syn-
onyms and abbreviation-expansion pairs  will neverthe-
less be clustered about the same subspace, as long as they
appear in similar contexts, i.e. have neighbors in common
(co-occur with the same terms). In this way, the reduced
space can be said to capture higher order co-occurrence
relations.
In latent semantic analysis (LSA) [39], dimensionality
reduction is performed with a computationally expensive
matrix factorization technique known as singular value
decomposition. Despite its popularity, LSA has conse-
quently received some criticism for its poor scalability
properties. More recently, alternative methods for con-
structing semantic spaces based on term co-occurrence
information have been proposed.
Random indexing
Random indexing (RI) [44] is an incremental, scalable
and computationally efficient alternative to LSA in which
explicit dimensionality reduction is avoidedg: a lower
dimensionality d is instead chosen a priori as a model
parameter and the d-dimensional context vectors are then
constructed incrementally. This approach allows new data
to be added at any given time without having to rebuild the
semantic space. RI can be viewed as a two-step operation:
1. Each context (e.g. each document or unique term) is
first given a static, unique representation in the
vector space that is approximately uncorrelated to all
other contexts. This is achieved by assigning a sparse,
ternaryh and randomly generated d-dimensional
index vector: a small number (usually around 12%)
of +1s and ?1s are randomly distributed, with the
rest of the elements set to zero. By generating sparse
vectors of a sufficiently high dimensionality in this
way, the index vectors will be nearly orthogonali.
2. Each unique term is assigned an initially empty
context vector of the same dimensionality d. The
context vectors are then incrementally populated
with context information by adding the (weighted)
index vectors of the contexts in which the target
term appears. With a sliding window context
definition, this means that the index vectors of the
surrounding terms are added to the target terms
context vector. The meaning of a term, represented
by its context vector, is effectively the (weighted)
sum of all the contexts in which it occurs.
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 5 of 25
http://www.jbiomedsem.com/content/5/1/6
Randompermutation
Models of distributional semantics, including RI, generally
treat each context as a bag of wordsj. Suchmodels are often
criticized for failing to account for term order. Recently,
methods have been developed for building distributional
semantic models that store and emphasize word order
information [45-47]. Random permutation (RP) [46] is a
modification of RI that encodes term order information by
simply permuting (i.e., shifting) the elements in the index
vectors according to their direction and distancek from
the target term before they are added to the context vector.
For instance, before adding the index vector of a term two
positions to the left of the target term, the elements are
shifted two positions to the left; similarly, before adding
the index vector of a term one position to the right of the
target term, the elements are shifted one position to the
right. In effect, each term has multiple unique representa-
tions: one index vector for each possible position relative
to the target term in the context window. Incorporat-
ing term order information not only enables order-based
retrieval; it also constrains the types of semantic relations
that are captured.
Model parameters
There are a number of model parameters that need to be
configured according to the task that the induced seman-
tic spaces will be used for. For instance, the types of
semantic relations captured depends on the context def-
inition [43,48]. By employing a document-level context
definition, relying on direct co-occurrences, one models
syntagmatic relations. That is, two terms that frequently
co-occur in the same documents are likely to be about the
same general topic. By employing a sliding window con-
text definition, one models paradigmatic relations. That
is, two terms that frequently co-occur with similar sets of
words  i.e., share neighbors  but do not necessarily co-
occur themselves, are semantically similar. Synonymy is
a prime example of a paradigmatic relation. The size of
the context window also affects the types of relations that
are modeled and needs to be tuned for the task at hand.
This is also true for semantic spaces produced by RP; how-
ever, the precise impact of window size on RP spaces and
the internal relations of their context vectors is yet to be
studied in depth.
Method
The main idea behind this study is to enhance the
performance on the task of extracting synonyms and
abbreviation-expansion pairs by combining multiple and
different semantic spaces  different in terms of (1) type
of model and model parameters used, and (2) type of
corpus from which the semantic space is induced. In addi-
tion to combining semantic spaces induced from a single
corpus, we also combine semantic spaces induced from
two different types of corpora: in this case, a clinical
corpus (comprising health record notes) and a medi-
cal corpus (comprising journal articles). The notion of
combining multiple semantic spaces to improve perfor-
mance on some task is generalizable and can loosely be
described as creating ensembles of semantic spaces. By
combining semantic spaces, it becomes possible to benefit
from model types that capture slightly different aspects of
semantics, to exploit various model parameter configura-
tions (which influence the types of semantic relations that
are modeled), as well as to observe language use in poten-
tially very different contexts (by employing more than one
corpus type).We set out exploring this approach by query-
ing each semantic space separately and then combining
their output using a number of combination strategies
(Figure 1).
The experimental setup can be divided into the fol-
lowing steps: (1) corpora preprocessing, (2) construction
of semantic spaces from the two corpora (and from the
conjoint corpus), (3) identification of the most profitable
single-corpus (and conjoint corpus) combinations, (4)
identification of the most profitable (disjoint) multiple-
corpora combinations, (5) evaluations of the single-corpus
(including the conjoint corpus) and multiple-corpora
combinations, (6) post-processing of candidate terms, and
(7) frequency threshold experiments. Once the corpora
have been preprocessed, ten semantic spaces from each
corpus, as well as the conjoint corpus, are induced with
different context window sizes (RP spaces are induced
with and without stop words). Ten pairs of semantic
spaces are then combined using three different combina-
tion strategies. These are evaluated on the three tasks  (1)
abbreviations ? expansions, (2) expansions ? abbrevia-
tions and (3) synonyms  using the development subsets
of the reference standards (a list of medical abbreviation-
expansion pairs for 1 and 2 and MeSH synonyms for 3).
Performance is mainly measured as recall top 10, i.e. the
proportion of expected candidate terms that are among
a list of ten suggestions. The pair of semantic spaces
involved in the most profitable combination for each cor-
pus is then used to identify the most profitable multiple-
corpora combinations, where eight different combination
strategies are evaluated. The best single-corpus combi-
nations are evaluated on the evaluation subsets of the
reference standards, where using RI and RP in isolation
constitute the two baselines. The best multiple-corpora
combination is likewise evaluated on the evaluation sub-
sets of the reference standards; here, the results are
compared both to (1) semantic spaces induced from a
single corpus and the conjoint corpus, and (2) ensem-
bles of semantic spaces induced from a single corpus (and
the conjoint corpus). Post-processing rules are then con-
structed using the development subsets of the reference
standards and the outputs of the various semantic space
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 6 of 25
http://www.jbiomedsem.com/content/5/1/6
Figure 1 Ensembles of semantic spaces for synonym extraction and abbreviation expansion. Semantic spaces built with different model
parameters are induced from different corpora. The output of the semantic spaces are combined in order to obtain better results compared to
using a single semantic space in isolation.
combinations. These are evaluated on the evaluation sub-
sets of the reference standards using the most profitable
single-corpus and multiple-corpora ensembles. All eval-
uations on the evaluation subsets of the reference stan-
dards also include an evaluation of weighted precision,
see Eq. 1:
Weighted Precision :Pw =
?j?1
i=0 (j ? i) · f (i)?j?1
i=0 j ? i
where
f (i) =
{
1 if i ? {tp}
0 otherwise
(1)
and j is the pre-specified number of labels  here, ten,
except in the case of a dynamic cut-off  and {tp} is the set
of true positives. In words, this assigns a score to true pos-
itives according to their (reverse) ranking in the list, sums
their scores and divides the total score by the maximum
possible score (where all j labels are true positives).
Finally, we explore the impact of frequency thresholds
(i.e., how many times each pair of terms in the reference
standards needs to occur to be included) on performance.
Inducing semantic spaces from clinical andmedical corpora
Each individual semantic space is constructed with one
model type, using a predefined context window size and
induced from a single corpus type. The semantic spaces
are constructed with random indexing (RI) and random
permutation (RP) using JavaSDM [49]. For all semantic
spaces, a dimensionality of 1,000 is used (with 8 non-zero,
randomly distributed elements in the index vectors: four
1s and four -1s). When the RI model is employed, the
index vectors are weighted according to their distance
from the target term, see Eq. 2, where distit is the distance
to the target term. When the RP model is employed, the
elements of the index vectors are instead shifted accord-
ing to their direction and distance from the target term;
no weighting is performed.
weighti = 21?distit (2)
For all models, window sizes of two (1 + 1), four (2 + 2)
and eight (4 + 4) surrounding terms are used. In addition,
RI spaces with a window size of twenty (10 + 10) are
induced in order to investigate whether a significantly
wider context definition may be profitable. Incorporating
order information (RP) with such a large context window
makes little sense; such an approach would also suffer
from data sparseness. Different context definitions are
experimented with in order to find one that is best suited
to each task. The RI spaces are induced only from corpora
that have been stop-word filtered, as co-occurrence infor-
mation involving high-frequent and widely distributed
words contribute very little to the meaning of terms. The
RP spaces are, however, also induced from corpora in
which stop words have been retained. The motivation
behind this is that all words, including function words 
these make up the majority of the items in the stop-word
lists  are important to the syntactic structure of language
and may thus be of value when modeling order infor-
mation [45]. A stop-word list is created for each corpus
by manually inspecting the most frequent word types
and removing from the list those words that may be of
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 7 of 25
http://www.jbiomedsem.com/content/5/1/6
interest, e.g. domain-specific terms. Each list consists of
approximately 150 terms.
The semantic spaces are induced from two types of cor-
pora  essentially belonging to different genres, but both
within the wider domain of medicine: (1) a clinical corpus,
comprising notes from health records, and (2) a medical
corpus, comprising medical journal articles.
The clinical corpus contains a subset of the Stockholm
EPR Corpus [50], which encompasses health records from
the Karolinska University Hospital in Stockholm, Sweden
over a five-year periodl. The clinical corpus used in this
study is created by extracting the free-text, narrative
parts of the health records from a wide range of clini-
cal practices. The clinical notes are written in Swedish
by physicians, nurses and other health care professionals
over a six-month period in 2008. In summary, the cor-
pus comprises documents that each contain clinical notes
documenting a single patient visit at a particular clinical
unit.
The medical corpus contains the freely available subset
of Läkartidningen (19962005), which is the Journal of the
Swedish Medical Association [51]. It is a weekly journal
written in Swedish and contains articles discussing new
scientific findings in medicine, pharmaceutical studies,
health economic evaluations, etc. Although these issues
have been made available for research, the original order
of the sentences has not been retained due to copy-
right reasons. The sentences thus appear in a randomized
order, which means that the original texts cannot be
recreated.
Both corpora are lemmatized using the Granska Tagger
[52] and thereafter further preprocessed by removing
punctuation marks and digits. Two versions of each cor-
pus are created: one version in which the stop words are
retained and one version in which they are removedm. As
the sentences in Läkartidningen are given in a random
order, a document break is indicated between each sen-
tence for this corpus. It is thereby ensured that context
information from surrounding sentences will not be incor-
porated in the induced semantic space. Statistics for the
two corpora are shown in Table 1.
In summary, a total of thirty semantic spaces are
induced  ten from each corpus type, and ten from the
conjoint corpus. Four RI spaces are induced from each
Table 1 Corpora statistics
Corpus With stop words Without stop words Segments
Clinical ?42.5M tokens ?22.5M tokens 268,727 documents
(?0.4M types) (?0.4M types)
Medical ?20.3M tokens ?12.1M tokens 1,153,824 sentences
(?0.3M types) (?0.3M types)
The number of tokens and unique terms (word types) in the medical and clinical
corpus, with and without stop words.
corpus type (12 in total), the difference being the context
definition employed (1 + 1, 2 + 2, 4 + 4, 10 + 10). Six RP
spaces are induced from each corpus type (18 in total), the
difference being the context definition employed (1 + 1,
2 + 2, 4 + 4) and whether stop words have been removed
or retained (sw).
Combinations of semantic spaces from a single corpus
Since RI and RP model semantic relations between terms
in slightly different ways, it may prove profitable to com-
bine them in order to increase the likelihood of capturing
synonymy and identifying abbreviation-expansion pairs.
In one study it was estimated that the overlap in the out-
put produced by RI and RP spaces is, on average, only
around 33% [46]: by combining them, we hope to cap-
ture different semantic properties of terms and, ultimately,
boost results. The combinations from a single corpus type
involve only two semantic spaces: one constructed with RI
and one constructed with RP. In this study, the combina-
tions involve semantic spaces with identical window sizes,
with the following exception: RI spaces with a wide con-
text definition (10 + 10) are combined with RP spaces with
a narrow context definition (1 + 1, 2 + 2). The RI spaces
are combined with RP spaces both with and without stop
words.
Three different strategies of combing an RI-based
semantic space with an RP space are designed and evalu-
ated. Thirty combinations are evaluated for each corpus,
i.e. sixty in total (Table 2). The three combination strate-
gies are:
 RI ? RP30
Finds the top ten terms in the RI space that are
among the top thirty terms in the RP space.
 RP ? RI30
Finds the top ten terms in the RP space that are
among the top thirty terms in the RI space.
 RI + RP
Sums the cosine similarity scores from the two spaces
for each candidate term.
For the first two strategies (RI ? RP30 and RP ? RI30)
a two-stage approach is applied. First one type of model
is used (RI or RP) to produce an initial ranking of words
according to a given query. The other model type, trained
on the same corpus, is then used to re-rank the top
30 words produced by the first model according to its
internal ranking. The intuition behind this approach is
to see if synonyms and abbreviation-expansion pairs can
be detected by trying to ensure that the set of contex-
tually related words also have similar grammatical prop-
erties, and vice versa. In the third strategy (RI + RP),
we apply a straightforward summing of the generated
similarity scores.
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 8 of 25
http://www.jbiomedsem.com/content/5/1/6
Table 2 Overview of experiments conducted with a single semantic space
For each of the 2 corpora, 10 semantic spaces were induced.
RI spaces RI_20 RI_2 RI_4 RI_8
RP spaces RP_2 RP_2_sw RP_4 RP_4_sw RP_8 RP_8_sw
The induced semantic spaces were combined in 10 different combinations.
Combinations
Identical window size RI_2, RP_2 RI_4, RP_4 RI_8, RP_8
Identical window size, stop words RI_2, RP_2_sw RI_4, RP_4_sw RI_8, RP_8_sw
Large window size RI_20, RP_2 RI_20, RP_4
Large window size, stop words RI_20, RP_2_sw RI_20, RP_4_sw
For each combination, 3 combination strategies were evaluated.
Combination strategies RI ? RP30 RP ? RI30 RI + RP
For each of the two corpora and the conjoint corpus, 30 different combinations were evaluated. The configurations are described according to the following pattern:
model_windowSize. For RP, swmeans that stop words are retained in the semantic space. For instance,model_20means a window size of 10+10 was used.
Combinations of semantic spaces frommultiple corpora
In addition to combining semantic spaces induced from
one and the same corpus, a combination of semantic
spaces induced from multiple corpora could potentially
yield even better performance on the task of extracting
synonyms and abbreviation-expansion pairs, especially if
the terms of interest occur with someminimum frequency
in both corpora. Such ensembles of semantic spaces 
in this study consisting of four semantic spaces  allow
not only different model types and model parameter con-
figurations to be employed, but also allow us to capture
language use in different genres or domains, in which
terms may be used in slightly different contexts. The pair
of semantic spaces from each corpus that is best able to
perform each of the aforementioned tasks  consisting of
two semantic spaces  is subsequently combined using
various combination strategies.
The combination strategies can usefully be divided into
two sets of approaches: in the first, the four seman-
tic spaces are treated equally  irrespective of source
 and combined in a single step; in the other, a two-
step approach is assumed, wherein each pair of semantic
spaces  induced from the same source  is combined
separately before the combination of combinations is per-
formed. In both sets of approaches, the outputs of the
semantic spaces are combined in one of two ways: SUM,
where the cosine similarity scores are merely summed,
and AVG, where the average cosine similarity score is
calculated based on the number of semantic spaces in
which the term under consideration exists. The latter is
an attempt to mitigate the effect of differences in vocabu-
lary between the two corpora. In the two-step approaches,
the SUM/AVG option is configurable for each step. In
the single-step approaches, the combinations can be per-
formed either with or without normalization, which in
this case means replacing the exact cosine similarity
scores of the candidate terms in the output of each queried
semantic space with their ranking in the list of candi-
date terms. This means that the candidate terms are now
sorted in ascending order, with zero being the highest
score. When combining two or more lists of candidate
terms, the combined list is also sorted in ascending order.
The rationale behind this option is that the cosine sim-
ilarity scores are relative and thus only valid within a
given semantic space: combining similarity scores from
semantic spaces constructed with different model types
and parameter configurations, and induced from differ-
ent corpora, might have adverse effects. In the two-step
approach, normalization is always performed after com-
bining each pair of semantic spaces. In total, eight combi-
nation strategies are evaluated:
Single-step approaches
 SUM: RIclinical + RPclinical + RImedical + RPmedical
Each candidate terms cosine similarity score in each
semantic space is summed. The top ten terms from
this list are returned.
 SUM, normalized: norm(RIclinical) + norm
(RPclinical) + norm(RImedical) + norm(RPmedical)
The output of each semantic space is first normalized
by using the ranking instead of cosine similarity; each
candidate terms (reverse) ranking in each semantic
space is then summed. The top ten terms from this
list are returned.
 AVG: RIclinical + RPclinical + RImedical + RPmedicalcountterm
Each candidate terms cosine similarity score in each
semantic space is summed; this value is then averaged
over the number of semantic spaces in which the term
exists. The top ten terms from this list are returned.
 AVG, normalized:
norm(RIclinical)+ norm(RPclinical)+ norm(RImedical)+ norm(RPmedical)
countterm
The output of each semantic space is first normalized
by using the ranking instead of cosine similarity; each
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 9 of 25
http://www.jbiomedsem.com/content/5/1/6
candidate terms normalized score in each semantic
space is then summed; this value is finally averaged
over the number of semantic spaces in which the term
exists. The top ten terms from this list are returned.
Two-step approaches
 SUM?SUM: norm(RIclinical + RPclinical) +
norm(RImedical + RPmedical)
Each candidate terms cosine similarity score in each
pair of semantic spaces is first summed; these are
then normalized by using the ranking instead of the
cosine similarity; finally, each candidate terms
normalized score is summed. The top ten terms from
this list are returned.
 AVG?AVG:
norm
(RIclinical + RPclinical
countterm?source?a
)
+ norm
(RImedical + RPmedical
countterm?source?b
)
countterm?source?a + countterm?source?b
Each candidate terms cosine similarity score for each
pair of semantic spaces is first summed; for each pair
of semantic spaces, this value is then averaged over
the number of semantic spaces in that pair in which
the term exists; these are subsequently normalized by
using the ranking instead of the cosine similarity;
each candidate terms normalized score in each
combined list is then summed and averaged over the
number of semantic spaces in which the term exists
(in both pairs of semantic spaces). The top ten terms
from this list are returned.
 SUM?AVG:
norm(RIclinical +RPclinical)+ norm(RImedical +RPmedical)
countterm
Each candidate terms cosine similarity score for each
pair of semantic spaces is first summed; these are then
normalized by using the ranking instead of the cosine
similarity; each candidate terms normalized score in
each combined list is then summed and averaged over
the number of semantic spaces in which the term
exists. The top ten terms from this list are returned.
 AVG?SUM: norm
(RIclinical + RPclinical
countterm
)
+
norm
(RImedical + RPmedical
countterm
)
Each candidate terms cosine similarity score for each
pair of semantic spaces is first summed and averaged
over the number of semantic spaces in that pair in
which the term exists; these are then normalized by
using the ranking instead of the cosine similarity;
each candidate terms normalized score in each
combined list is finally summed. The top ten terms
from this list are returned.
Post-processing of candidate terms
In addition to creating ensembles of semantic spaces, sim-
ple filtering rules are designed and evaluated for their
ability to enhance performance further on the task of
extracting synonyms and abbreviation-expansion pairs.
For obvious reasons, this is easier for abbreviation-
expansion pairs than for synonyms.
With regards to abbreviation-expansion pairs, the focus
is on increasing precision by discarding poor suggestions
in favor of potentially better ones. This is attempted by
exploiting properties of the abbreviations and their cor-
responding expansions. The development subset of the
reference standard (see Evaluation framework) is used to
construct rules that determine the validity of candidate
terms. For an abbreviation-expansion pair to be consid-
ered valid, each letter in the abbreviation has to be present
in the expansion and the letters also have to appear in the
same order. Additionally, the length of abbreviations and
expansions is restricted, requiring an expansion to con-
tain more than four letters, whereas an abbreviation is
allowed to contain a maximum of four letters. These rules
are shown in Eq. 3 and Eq. 4.
For synonym extraction, cut-off values for rank and
cosine similarity are instead employed. These cut-off val-
ues are tuned to maximize precision for the best semantic
space combinations in the development subset of the ref-
erence standard, without negatively affecting recall (see
Figures 2, 3 and 4). Used cut-off values are shown in Eq. 5
for the clinical corpus, in Eq. 6 for the medical corpus, and
in Eq. 7 for the combination of the two corpora. In Eq. 7,
Cos denotes the combination of the cosine values, which
means that it has a maximum value of four rather than
one.
Exp ? Abbr =
{ True, if (Len < 5) ? (Subout = True)
False, Otherwise
(3)
Abbr ? Exp =
{
True, if (Len > 4) ? (Subin = True)
False, Otherwise
(4)
Synclinical=
{
True, if (Cos?0.60)?(Cos?0.40?Rank<9)
False, Otherwise
(5)
Synmedical =
{
True, if (Cos ? 0.50)
False, Otherwise (6)
Synclinical+medical =
{
True, if (Cos ? 1.9) ? (Cos ? 1.8 ? Rank < 6) ? (Cos ? 1.75 ? Rank < 3)
False, Otherwise (7)
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 10 of 25
http://www.jbiomedsem.com/content/5/1/6
Figure 2 Distribution of candidate terms for the clinical corpus. The distribution (cosine similarity and rank) of candidates for synonyms for the
best combination of semantic spaces induced from the clinical corpus. The results show the distribution for query terms in the development
reference standard.
Figure 3 Distribution of candidate terms for the medical corpus. The distribution (cosine similarity and rank) of candidates for synonyms for
the best combination of semantic spaces induced from the medical corpus. The results show the distribution for query terms in the development
reference standard.
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 11 of 25
http://www.jbiomedsem.com/content/5/1/6
Figure 4 Distribution of candidate terms for clinical + medical corpora. The distribution (combined cosine similarity and rank) of candidates
for synonyms for the ensemble of semantic spaces induced from medical and clinical corpora. The results show the distribution for query terms in
the development reference standard.
Cos: Cosine similarity between candidate term and
query term.
Rank: The ranking of the candidate term, ordered by
cosine similarity.
Subout: Whether each letter in the candidate term is
present in the query term, in the same order
and with identical initial letters.
Subin: Whether each letter in the query term is
present in the candidate term, in the same
order and with identical initial letters.
Len: The length of the candidate term.
The post-processing filtering rules are employed in two
different ways. In the first approach, the semantic spaces
are forced to suggest a predefined number of candidate
terms (ten), irrespective of how good they are deemed to
be by the semantic space. Candidate terms are retrieved by
the semantic space until ten have been classified as correct
according to the post-processing rules, or until one hun-
dred candidate terms have been classified. If less than ten
are classified as incorrect, the highest ranked discarded
terms are used to populate the remaining slots in the
final list of candidate terms. In the second approach, the
semantic spaces are allowed to suggest a dynamic num-
ber of candidate terms, with a minimum of one and a
maximum of ten. If none of the highest ranked terms are
classified as correct, the highest ranked term is suggested.
Evaluation framework
Evaluation of the numerous experiments is carried out
with the use of reference standards: one contains known
abbreviation-expansion pairs and the other contains
known synonyms. The semantic spaces and their var-
ious combinations are evaluated for their ability to
extract known abbreviations/expansions (abbr?exp and
exp?abbr) and synonyms (syn)  according to the
employed reference standard  for a given query term
in a list of ten candidate terms (recall top 10). Recall
is prioritized in this study and any decisions, such as
deciding which model parameters or which combina-
tion strategies are the most profitable, are solely based
on this measure. When precision is reported, it is cal-
culated as weighted precision, where the weights are
assigned according to the ranking of a correctly identified
term.
The reference standard for abbreviations is taken from
Cederblom [53], which is a book that contains lists of
medical abbreviations and their corresponding expan-
sions. These abbreviations have been manually collected
from Swedish health records, newspapers, scientific arti-
cles, etc. For the synonym extraction task, the reference
standard is derived from the freely available part of the
Swedish version of MeSH [54]  a part of UMLS  as
well as a Swedish extension that is not included in UMLS
[55]. As the semantic spaces are constructed only tomodel
unigrams, all multiword expressions are removed from
the reference standards. Moreover, hypernym/hyponym
and other non-synonym pairs found in the UMLS ver-
sion of MeSH are manually removed from the reference
standard for the synonym extraction task. Models of dis-
tributional semantics sometimes struggle to model the
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 12 of 25
http://www.jbiomedsem.com/content/5/1/6
meaning of rare terms accurately, as the statistical basis
for their representation is insufficiently solid. As a result,
we only include term pairs that occur at least fifty times
in each respective corpus. This, together with the fact
that term frequencies differ from corpus to corpus, means
that one separate reference standard is used for the eval-
uation of the clinical corpus and another is used for the
evaluation of the medical corpus. For evaluating combina-
tions of semantic spaces induced from different corpora,
a third  common  reference standard is therefore cre-
ated, in which only term pairs that occur at least fifty
times in both corpora are included. Included terms are
not restricted to form pairs; in the reference standard for
the synonym extraction task, some form larger groups of
terms with synonymous relations. There are also abbrevi-
ations with several possible expansions, as well as expan-
sions with several possible abbreviations. The term pairs
(or n-tuples) in each reference standard are randomly split
into a development set and an evaluation set of roughly
equal size. The development sets are used for identi-
fying the most profitable ensembles of semantic spaces
(with optimized parameter settings, such as window size
and whether to include stop words in the RP spaces) for
each of the three tasks, as well as for creating the post-
processing filtering rules. The evaluation sets are used for
the final evaluation to assess the expected performance of
the ensembles in a deployment setting. Baselines for the
single-corpus ensembles are created by employing RI and
RP in isolation; baselines for the multiple-corpora ensem-
bles are created by using the most profitable clinical and
medical ensembles from the single-corpus experiments,
as well a single space induced from the conjoint cor-
pus and an ensemble of semantic spaces induced from
the conjoint corpus. Statistics for the reference standards
are shown in Table 3. The differences in recall between
the different semantic spaces/ensembles, when evaluated
on the evaluation subset of the reference standards, are
tested for statistical significance. The exact binomial sign
test is used ([56], pp. 532535), assuming independence
between all query terms.
In addition to the automatic evaluation using the ref-
erence standards, a small manual evaluation is also car-
ried out on the synonym task. A random sample of 30
query terms (out of 135 terms in the Clinical + Medi-
cal reference standard) and their respective ten candidate
terms as suggested by the best combination of seman-
tic spaces is investigated and a manual classification of
the semantic relation between each of the candidate
terms and the target term is carried out. The candi-
date terms are manually classified as either a synonym,
an antonymn, a hypernymo, a hyponym or an alterna-
tive spelling (for instance rinitis/rhinitis) of the target
term.
Results
The experimental setup was designed in such a manner
that the semantic spaces that performed best in com-
bination for a single corpus would also be used in the
subsequent combinations from multiple corpora. Identi-
fying the most profitable combination strategy for each
of the three tasks was achieved using the development
subsets of the reference standards. These combinations
were then evaluated on separate evaluation sets con-
taining unseen data. All further experiments, including
the post-processing of candidate terms, were carried out
with these combinations on the evaluation sets. This
is therefore also the order in which the results will be
presented.
Combination strategies: a single corpus
The first step involved identifying the most appropriate
window sizes for each task, in conjunction with evalu-
ating the combination strategies. The reason for this is
that the optimal window sizes for RI and RP in isolation
are not necessarily identical to the optimal window sizes
when RI and RP are combined. In fact, when RI is used
in isolation, a window size of 2 + 2 performs best on the
two abbreviation-expansion tasks, and a window size of
10 + 10 performs best on the synonym task. For RP, a
semantic space with a window size of 2 + 2 yields the
Table 3 Reference standards statistics
Reference standard
Clinical corpus Medical corpus Clinical + Medical
Size 2 Cor 3 Cor Size 2 Cor 3 Cor Size 2 Cor 3 Cor
Abbr?Exp (Devel) 117 9.4% 0.0% 55 13% 1.8% 42 14% 0%
Abbr?Exp (Eval) 98 3.1% 0.0% 55 11% 0% 35 2.9% 0%
Exp?Abbr (Devel) 110 8.2% 1.8% 63 4.7% 0% 45 6.7% 0%
Exp?Abbr (Eval) 98 7.1% 0.0% 61 0% 0% 36 0% 0%
Syn (Devel) 334 9.0% 1.2% 266 11% 3.0% 122 4.9% 0%
Syn (Eval) 340 14% 2.4% 263 13% 3.8% 135 11% 0%
Size shows the number of queries, 2 cor shows the proportion of queries with two correct answers and 3 cor the proportion of queries with three (or more) correct
answers. The remaining queries have one correct answer.
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 13 of 25
http://www.jbiomedsem.com/content/5/1/6
best results on two of the tasks  abbr?exp and syn 
while a window size of 4 + 4 is more successful on
the exp?abbr task. These are the model configurations
used in the RI and RP baselines, to which the single-
corpus combination strategies are compared in the final
evaluation.
Using the semantic spaces induced from the clinical cor-
pus, the RI+RP combination strategy, wherein the cosine
similarity scores are merely summed, is the most success-
ful on all three tasks: 0.42 recall on the abbr?exp task,
0.32 recall on the exp?abbr task, and 0.40 recall on the
syn task (Table 4). For the abbreviation expansion task, a
window size of 2 + 2 appears to work well for both mod-
els, with the RP space retaining stop words. On the task of
identifying the abbreviated form of an expansion, seman-
tic spaces with window sizes of 2 + 2 and 4 + 4 perform
equally well; the RP spaces should include stop words.
Finally, on the synonym extraction task, an RI space with
a large context window (10 + 10) in conjunction with an
RP space with stop words and a window size of 2 + 2 is the
most profitable.
Using the semantic spaces induced from the med-
ical corpus, again, the RI + RP combination strategy
outperforms the RI ? RP30 and RP ? RI30 strategies:
0.10 recall on the abbr?exp task, 0.08 recall on the
exp?abbr task, and 0.30 recall on the syn task (Table 5)
are obtained. This combination outperforms the other
two by a large margin on the exp?abbr task: 0.08 recall
compared to 0.03 recall. The most appropriate window
sizes for capturing these phenomena in the medical
corpus are fairly similar to those that worked best with
the clinical corpus. On the abbr?exp task, the opti-
mal window sizes are indeed identical across the two
corpora: a 2 + 2 context window with an RP space that
incorporates stop words yields the highest performance.
For the exp?abbr task, a slightly larger context window
of 4 + 4 seems to work well  again, with stop words
retained in the RP space. Alternatively, combining a large
RI space (10 + 10) with a smaller RP space (2 + 2, with
stop words) performs comparably on this task and with
this test data. Finally, for synonyms, a large RI space
(10 + 10) with a very small RP space (1 + 1) that retains
all words best captures this phenomenon with this type of
corpus.
Using the semantic spaces induced from the conjoint
corpus, the RI ? RP30 combination strategy outperforms
the other two strategies on the abbr?exp task: 0.30 recall
compared to 0.25 and 0.23 (Table 6). On the exp?abbr
task, this and the RI + RP combination strategy perform
equally well, with 0.18 recall. Finally, on the synonym task,
the RI +RP performs best with a recall of 0.46. In general,
somewhat larger window sizes seem to work better when
combining semantic spaces induced from the conjoint
corpus.
The best-performing combinations from each corpus
and for each task were then treated as (ensemble) base-
lines in the final evaluation, where combinations of
semantic spaces from multiple corpora are evaluated.
Combination strategies: multiple corpora
The pair of semantic spaces from each corpus that
performed best on the three tasks were subsequently
employed in combinations that involved four semantic
spaces  two from each corpus: one RI space and one
RP space. The single-step approaches generally performed
better than the two-step approaches, with some excep-
tions (Table 7). The most successful ensemble was a
simple single-step approach, where the cosine similar-
ity scores produced by each semantic space were simply
summed (SUM), yielding 0.32 recall for abbr?exp, 0.17
recall for exp?abbr, and 0.52 recall for syn. The AVG
option, although the second-highest performer on the
abbreviation-expansion tasks, yielded significantly poorer
results. Normalization, whereby ranking was used instead
of cosine similarity, invariably affected performance neg-
atively, especially when employed in conjunction with
SUM. The two-step approaches performed significantly
worse than all non-normalized single-step approaches,
with the sole exception taking place on the synonym
extraction task. It should be noted that normalization was
Table 4 Results on clinical development set
Strategy
Abbr?Exp Exp?Abbr Syn
RI RP Result RI RP Result RI RP Result
RI ? RP30 RI_8 RP_8_sw 0.38 RI_8 RP_8 0.30 RI_8 RP_8 0.39
RP ? RI30 RI_20 RP_4_sw 0.35
RI_4 RP_4_sw
0.30
RI_8 RP_8
0.38RI_20 RP_4_sw RI_8 RP_8_sw
RI_20 RP_2_sw
RI + RP RI_4 RP_4_sw 0.42 RI_4 RP_4_sw 0.32 RI_20 RP_4_sw 0.40
RI_8 RP_8_sw
Results (recall, top ten) of the best configurations for each model and model combination on the three tasks. The configurations are described according to the
following pattern:model_windowSize. For RP, swmeans that stop words are retained in the model.
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 14 of 25
http://www.jbiomedsem.com/content/5/1/6
Table 5 Results onmedical development set
Strategy
Abbr?Exp Exp?Abbr Syn
RI RP Result RI RP Result RI RP Result
RI ? RP30
RI_4 RP_4_sw
0.08
RI_2 RP_2
0.03 RI_20 RP_4_sw 0.26
RI_20 RP_2 RI_4 RP_4
RI_20 RP_4_sw RI_4 RP_4_sw
RI_8 RP_8
RI_20 RP_2
RI_20 RP_2_sw
RI_20 RP_4
RI_20 RP_4_sw
RP ? RI30
RI_2 RP_2_sw
0.08
RI_2 RP_2
0.03 RI_8 RP_8_sw 0.24
RI_4 RP_4 RI_2 RP_2_sw
RI_4 RP_4_sw RI_4 RP_4
RI_8 RP_8 RI_4 RP_4_sw
RI_8 RP_8_sw RI_8 RP_8
RI_20 RP_2_sw RI_8 RP_8_sw
RI_20 RP_4 RI_20 RP_2
RI_20 RP_4_sw RI_20 RP_2_sw
RI_20 RP_4
RI_20 RP_4_sw
RI + RP RI_4 RP_4_sw 0.10 RI_8 RP_8_sw 0.08 RI_20 RP_2_sw 0.30
RI_20 RP_4_sw
Results (recall, top ten) of the best configurations for each model and model combination on the three tasks. The configurations are described according to the
following pattern:model_windowSize. For RP, swmeans that stop words are retained in the model.
always performed in the two-step approaches  this was
done after each pair of semantic spaces from a single cor-
pus had been combined. Of the four two-step combina-
tion strategies, AVG?AVG and AVG?SUM performed
best, with identical recall scores on the three tasks.
Final evaluations
The combination strategies that performed best on the
development sets were finally evaluated on completely
unseen data in order to assess their generalizability to
new data and to assess their expected performance in a
Table 6 Conjoined corpus space results on clinical + medical development set
Strategy
Abbr?Exp Exp?Abbr Syn
RI RP Result RI RP Result RI RP Result
RI ? RP30 RI_4 RP_4_sw 0.30 RI_4 RP_4_sw 0.18 RI_8 RP_8_sw 0.41
RI_20 RP_4_sw
RP ? RI30
RI_4 RP_4
0.23
RI_4 RP_4_sw
0.13
RI_8 RP_8
0.36
RI_4 RP_4_sw RI_8 RP_8_sw RI_8 RP_8_sw
RI_8 RP_8 RI_20 RP_2_sw RI_20 RP_2_sw
RI_20 RP_2 RI_20 RP_4_sw RI_20 RP_4_sw
RI_20 RP_4
RI + RP RI_2 RP_2_sw 0.25
RI_4 RP_4_sw
0.18 RI_8 RP_8_sw 0.46RI_8 RP_8_sw
RI_20 RP_4_sw
Results (recall, top ten) of the best configurations for each model and model combination on the three tasks. The configurations are described according to the
following pattern:model_windowSize. For RP, swmeans that stop words are retained in the model.
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 15 of 25
http://www.jbiomedsem.com/content/5/1/6
Table 7 Disjoint corpus ensemble results on clinical + medical development set
Strategy Normalize
Abbr?Exp Exp?Abbr Syn
Clinical Medical Clinical Medical Clinical Medical
RI_4 RI_4 RI_4 RI_8 RI_20 RI_20
RP_4_sw RP_4_sw RP_4_sw RP_8_sw RP_4_sw RP_2_sw
AVG True 0.13 0.09 0.39
AVG False 0.24 0.11 0.39
SUM True 0.13 0.09 0.34
SUM False 0.32 0.17 0.52
AVG?AVG 0.15 0.09 0.41
SUM?SUM 0.13 0.07 0.40
AVG?SUM 0.15 0.09 0.41
SUM?AVG 0.13 0.07 0.40
Results (P = weighted precision, R = recall, top ten) of the best models with and without post-processing on the three tasks. Dynamic # of suggestions allows the
model to suggest less than ten terms in order to improve precision. The results are based on the application of the model combinations to the development data.
deployment setting. Each evaluation phase involves com-
paring the results to one or more baselines: in the case
of single-corpus combinations, the comparisons are made
to RI and RP in isolation; in the case of multiple-corpora
combinations, the comparisons are made to semantic
spaces induced from a single corpus (as well as the con-
joint corpus) and ensembles of semantic spaces induced
from a single corpus (and, again, the conjoint corpus).
When applying the single-corpus combinations from
the clinical corpus, the following results were obtained:
0.31 recall on abbr?exp, 0.20 recall on exp?abbr, and
0.44 recall on syn (Table 8). Compared to the results on
the development sets, the results on the two abbreviation-
expansion tasks decreased by approximately ten per-
centage points; on the synonym extraction task, the
performance increased by a couple of percentage points.
The RI baseline was outperformed on all three tasks;
the RP baseline was outperformed on two out of three
tasks, with the exception of the exp?abbr task. Finally,
it might be interesting to point out that the RP base-
line performed better than the RI baseline on the two
abbreviation-expansion tasks, but that the RI baseline did
somewhat better on the synonym extraction task.
With the medical corpus, the following results were
obtained: 0.17 recall on abbr?exp, 0.11 recall on
exp?abbr, and 0.34 recall on syn (Table 9). Compared
to the results on the development sets, the results were
higher for all three tasks. Both the RI and RP baselines
were outperformed, with a considerable margin, by their
combination. However, the improvement in recall for the
combination method compared to the best baseline was
only statistically significant for the synonym task. In com-
plete contrast to the clinical corpus, the RI baseline here
outperformed the RP baseline on the two abbreviation-
expansion tasks, but was outperformed by the RP baseline
on the synonym extraction task.
When applying the disjoint corpora ensembles, the fol-
lowing results were obtained on the evaluation sets: 0.30
Table 8 Results on clinical evaluation set
Evaluation configuration
Abbr?Exp Exp?Abbr Syn
RI_4+RP_4_sw RI_4+RP_4_sw RI_20+RP_4_sw
P R P R P R
RI Baseline 0.04 0.22 0.03 0.19 0.07 0.39
RP Baseline 0.04 0.23 0.04 0.24 0.06 0.36
Clinical Ensemble 0.05 0.31 0.03 0.20 0.07 0.44
+Post-Processing (Top 10) 0.08 0.42 0.05 0.33 0.08 0.43
+Dynamic Cut-Off (Top ? 10) 0.11 0.41 0.12 0.33 0.08 0.42
Results (P = weighted precision, R = recall, top ten) of the best models with and without post-processing on the three tasks. Dynamic # of suggestions allows the
model to suggest less than ten terms in order to improve precision. The results are based on the application of the model combinations to the evaluation data. The
improvements in recall between the best baseline and the ensemble method for the synonym task and for the abbr?exp task are both statistically significant for a
p-value < 0.05. (abbr?exp task: p-value = 0.022 and synonym task: p-value = 0.002.) The improvement in recall that was achieved by post-processing is statistically
significant for both abbreviation tasks (p-value = 0.001 for abbr?exp and p-value = 0.000 for exp?abbr).
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 16 of 25
http://www.jbiomedsem.com/content/5/1/6
Table 9 Results onmedical evaluation set
Evaluation configuration
Abbr?Exp Exp?Abbr Syn
RI_4+RP_4_sw RI_8+RP_8_sw RI_20+RP_2_sw
P R P R P R
RI baseline 0.02 0.09 0.01 0.08 0.03 0.18
RP baseline 0.01 0.06 0.01 0.05 0.05 0.26
Medical ensemble 0.03 0.17 0.01 0.11 0.06 0.34
+Post-processing (top 10) 0.03 0.17 0.02 0.11 0.06 0.34
+Dynamic cut-off (top ? 10) 0.17 0.17 0.10 0.11 0.06 0.34
Results (P = weighted precision, R = recall, top ten) of the best semantic spaces with and without post-processing on the three tasks. Dynamic # of suggestions allows
the model to suggest less than ten terms in order to improve precision. The results are based on the application of the model combinations to the evaluation data.
The difference in recall when using the ensemble method compared to the best baseline is only statistically significant (p-value < 0.05) for the synonym task (p-value =
0.000).
recall on abbr?exp, 0.19 recall on exp?abbr, and 0.47
recall on syn (Table 10). Compared to the results on
the development sets, the results decreased somewhat on
two of the tasks, with exp?abbr the exception. The p-
values for the significance tests of the recall differences in
Table 10 are shown in Table 11. The two ensemble base-
lines were clearly outperformed by the larger ensemble of
semantic spaces from two types of corpora on two of the
tasks; the clinical ensemble baseline performed equally
well on the exp?abbr task.
Post-processing
In an attempt to further improve results, simple post-
processing of the candidate terms was performed. In one
setting, the system was forced to suggest ten candidate
terms regardless of their cosine similarity score or other
properties of the terms, such as their length. In another
setting, the system had the option of suggesting a dynamic
number  ten or less  of candidate terms.
This was unsurprisingly more effective on the two
abbreviation-expansion tasks. With the clinical corpus,
recall improved substantially with the post-processing fil-
tering: from 0.31 to 0.42 on abbr?exp and from 0.20
to 0.33 on exp?abbr (Table 8). With the medical cor-
pus, however, almost no improvements were observed for
these tasks (Table 9). For the combination of semantic
spaces from the two corpora, the improvements in recall
after applying post-processing on the two abbreviation
tasks are not statistically significant (Table 10).
With a dynamic cut-off, only precision could be
improved, although at the risk of negatively affect-
ing recall. With the clinical corpus, recall was largely
Table 10 Results on clinical + medical evaluation set
Evaluation configuration
Abbr?Exp Exp?Abbr Syn
Clinical Medical Clinical Medical Clinical Medical
RI_4 RI_4 RI_4 RI_8 RI_20 RI_20
RP_4_sw RP_4_sw RP_4_sw RP_8_sw RP_4_sw RP_2_sw
SUM, False SUM, False SUM, False
P R P R P R
Clinical space 0.03 0.17 0.03 0.19 0.05 0.29
Medical space 0.01 0.06 0.01 0.08 0.03 0.18
Conjoint corpus space 0.03 0.19 0.01 0.08 0.05 0.30
Clinical ensemble 0.04 0.24 0.03 0.19 0.06 0.34
Medical ensemble 0.02 0.11 0.01 0.11 0.05 0.33
Conjoint corpus ensemble 0.03 0.19 0.02 0.14 0.07 0.40
Disjoint corpora ensemble 0.05 0.30 0.03 0.19 0.08 0.47
+Post-processing (top 10) 0.07 0.39 0.06 0.33 0.08 0.47
+Dynamic cut-off (top ? 10) 0.28 0.39 0.31 0.33 0.08 0.45
Results (P = weighted precision, R = recall, top ten) of the best semantic spaces and ensembles on the three tasks. The results are based on the clinical + medical
evaluation set and are grouped according to the number of semantic spaces employed: one, two or four. The disjoint corpus ensemble is performed with and without
post-processing. A dynamic cut-off allows less than ten terms to be suggested in an attempt to improve precision. Results for tests of statistical significance are shown
in Table 11.
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 17 of 25
http://www.jbiomedsem.com/content/5/1/6
Table 11 P-values for recall results presented in Table 10
P-values, recall Medical Conjoint Clinical Medical Conjoint Disjoint
(synonym task) space corpus ensemble ensemble corp. ens. corp. ens.
Clinical space 0.011 1.000 0.057 0.885 0.003 0.000
Medical space - 0.004 0.000 0.000 0.000 0.000
Conjoint corpus - - 0.210 1.000 0.001 0.000
Clinical ensemble - - - 0.480 0.189 0.001
Medical ensemble - - - - 0.047 0.000
Conjoint corp. ens. - - - - - 0.041
P-values for the differences between the recall results on the synonym task for the semantic spaces/ensembles presented in Table 10. P-values showing a statistically
significant difference (p-value < 0.05) are presented in bold-face.
P-values for the post-processing and for the abbr?exp and exp?abbr are not shown in the table. However, for the significance level p-value < 0.05, there were no
statistically significant recall difference between the standard Disjoint Corpora Ensemble and the post-processing version for any of the three tasks (p-value = 0.25 for
abbr?exp and p-value = 0.062 for exp?abbr). When testing the recall difference between the pairs of semantic spaces/ensembles shown in Table 10 for the
abbr?exp task, there was only a significant difference for the pairs Medical Space vs. Clinical Ensemble (p-value = 0.039), Medical Space vs. Disjoint Corpora Ensemble
(p-value = 0.004) and Medical Ensemble vs. Disjoint Corpora Ensemble (p-value = 0.039). For the exp?abbr task, there were no statistically significant differences.
unaffected for the two abbreviation-expansion task, while
precision improved by 37 percentage points (Table 8).
With the medical corpus, the gains were even more sub-
stantial: from 0.03 to 0.17 precision on abbr?exp and
from 0.02 to 0.10 precision on exp?abbr  without hav-
ing any impact on recall (Table 9). The greatest improve-
ments on these tasks were, however, observed with the
combination of semantic spaces from multiple corpora:
precision increased from 0.07 to 0.28 on abbr?exp and
from 0.06 to 0.31 on exp?abbr  again, without affecting
recall (Table 10).
In the case of synonyms, this form of post-processing
is more challenging, as there are no simple properties of
the terms, such as their length, that can serve as indica-
tions of their quality as candidate synonyms. Instead, one
has to rely on their use in different contexts and grammat-
ical properties; as a result, cosine similarity and ranking
of the candidate terms were exploited in an attempt to
improve the candidate synonyms. This approach was,
however, clearly unsuccessful for both corpora and their
combination, with almost no impact on either precision
or recall. In a single instance  with the clinical corpus 
precision increased by one percentage point, albeit at the
expense of recall, which suffered a comparable decrease
(Table 8). With the combination of semantic spaces from
two corpora, the dynamic cut-off option resulted in a
lower recall score, without improving precision (Table 10).
Frequency thresholds
In order to study the impact of different frequency thresh-
olds  i.e., how often each pair of terms had to occur in
the corpora to be included in the reference standard  on
the task of extracting synonyms, the best ensemble sys-
temwas applied to a range of evaluation sets with different
thresholds from 1 to 100 (Figure 5). With a low frequency
threshold, it is clear that a lower performance is obtained.
For instance, if each synonym pair only needs to occur
at least once in both corpora, a recall of 0.17 is obtained.
As the threshold is increased, recall increases too - up to
a frequency threshold of around 50, after which no per-
formance boosts are observed. Already with a frequency
threshold of around 30, the results seem to level off. With
Figure 5 Frequency thresholds. The relation between recall and the required minimum frequency of occurrence for the reference standard terms
in both corpora. The number of query terms for each threshold value is also shown.
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 18 of 25
http://www.jbiomedsem.com/content/5/1/6
frequency thresholds over 100, there is not enough data in
this case to produce any reliable results.
Discussion
The results clearly demonstrate that combinations of
semantic spaces lead to improved results on the synonym
extraction task. For the two abbreviation tasks, most of the
observed performance gains were not statistically signifi-
cant. Combining random indexing and random permuta-
tion allows slightly different aspects of lexical semantics to
be captured; by combining them, stronger semantic rela-
tions between terms are extracted, thereby increasing the
performance on these tasks. Combining semantic spaces
induced from different corpora further improves perfor-
mance. This demonstrates the potential of distributional
ensemble methods, of which this  to the extent of our
knowledge  is the primary implementation of its kind,
and it only scratches the surface. In this initial study, only
four semantic spaces were used; however, with increas-
ing computational capabilities, there is nothing stopping
a much larger number of semantic spaces from being
combined. These can capture various aspects of seman-
tics  aspects which may be difficult, if not impossible,
to incorporate into a single model  from a large variety
of observational data on language use, where the contexts
may be very different.
Clinical vs. medical corpora
When employing corpus-driven methods to support lex-
ical resource development, one naturally needs to have
access to a corpus in the target domain that reflects
the language use one wishes to model. Hence, one can-
not, without due qualification, state that one corpus type
is better than another for the extraction of synonyms
or abbreviation-expansion pairs. This is something that
needs to be duly considered when comparing the results
for the semantic spaces on the clinical and medical cor-
pora, respectively. Another issue concerns the size of each
corpus: in fact, the size of themedical corpus is only half as
large as the clinical corpus (Table 1). The reference stan-
dards used in the respective experiments are, however, not
identical: each term pair had to occur at least fifty times
to be included  this will differ across corpora. To some
extent this mitigates the effect of the total corpus size and
makes the comparison between the two corpora fairer;
however, differences in reference standards also entail that
the results presented in Tables 8 and 9 are not directly
comparable. Another difference between the two corpora
is that the clinical corpus contains more unique terms
(word types) than the medical corpus, which might indi-
cate that it consists of a larger number of concepts. It has
previously been shown that it can be beneficial, indeed
important, to employ a larger dimensionality when using
corpora with a large vocabulary, as is typically the case
in the clinical domain [57]; in this study a dimensional-
ity of 1,000 was used to induce all semantic spaces. The
results, on the contrary, seem to indicate that better per-
formance is generally obtained with the semantic spaces
induced from the clinical corpus.
An advantage of using non-sensitive corpora like the
medical corpus employed in this study is that they are gen-
erally more readily obtainable than sensitive clinical data.
Perhaps such and similar sources can complement smaller
clinical corpora and yet obtain similar or potentially even
better results.
Combining semantic spaces
Creating ensembles of semantic spaces has been shown to
be profitable, at least on the task of extracting synonyms
and abbreviation-expansion pairs. In this study, the focus
has been on combining the output of the semantic spaces.
This is probably the most straightforward approach and
it has several advantages. For one, the manner in which
the semantic representations are created can largely be
ignored, which would potentially allow one to combine
models that are very different in nature, as long as one
can retrieve a ranked list of semantically related terms
with a measure of the strength of the relation. It also
means that one can readily combine semantic spaces that
have been induced with different parameter settings, for
instance with different context definitions and of differ-
ent dimensionality. An alternative approach would per-
haps be to combine semantic spaces on a vector level.
Such an approach would be interesting to explore; how-
ever, it would pose numerous challenges, not least in
combining context vectors that have been constructed dif-
ferently and potentially represent meaning in disparate
ways.
Several combination strategies were designed and eval-
uated. In both the single-corpus and multiple-corpora
ensembles, the most simple strategy performed best: the
one whereby the cosine similarity scores are summed.
There are potential problems with such a strategy, since
the similarity scores are not absolute measures of seman-
tic relatedness, but merely relative and only valid within
a single semantic space. The cosine similarity scores will,
for instance, differ depending on the distributional model
used and the size of the context window. An attempt
was made to deal with this by replacing the cosine sim-
ilarity scores with ranking information, as a means to
normalize the output of each semantic space before comb-
ing them. This approach, however, yielded much poorer
results. A possible explanation for this is that a measure
of the semantic relatedness between terms is of much
more importance than their ranking. After all, a list of
the highest ranked terms does not necessarily imply that
they are semantically similar to the query term; only that
they are the most semantically similar in this space. For
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 19 of 25
http://www.jbiomedsem.com/content/5/1/6
the multiple-corpora ensembles, the AVG strategy was
applied with the aim of not penalizing candidate syn-
onyms that only appear in one of the two corpora. It is
not surprising that this strategy was not successful given
the form of the evaluation, which consisted of suggesting
candidate synonyms that were known to occur at least 50
times in both corpora. The two-step approaches for the
multiple-corpora ensembles all included a normalizing
and/or averaging component, resulting in a lower recall
compared to the SUM strategy, probably for the same rea-
sons as when these strategies were applied in the one-step
approach.
To gain deeper insights into the process of combining
the output of multiple semantic spaces, an error analy-
sis was conducted on the synonym extraction task. This
was achieved by comparing the outputs of the most prof-
itable combination of semantic spaces from each corpus,
as well as with the combination of semantic spaces from
the two corpora. The error analysis was conducted on the
development sets. Of the 68 synonyms that were correctly
identified as such by the corpora combination, five were
not extracted by either of the single-corpus combinations;
nine were extracted by the medical ensemble but not by
the clinical ensemble; as many as 51 were extracted by the
clinical ensemble but not by its medical counterpart; in
the end, this means that only three terms were extracted
by both the clinical and medical ensembles. These results
augment the case for multiple-corpora ensembles. There
appears to be little overlap in the top-10 outputs of
the corpora-specific ensembles; by combining them, 17
additional true synonyms are extracted compared to the
clinical ensemble alone. Moreover, the fact that so many
synonyms are extracted by the clinical ensemble demon-
strates the importance of exploiting clinical corpora and
the applicability of distributional semantics to this genre
of text. In Table 12, the first two examples, sjukhem
(nursing-home) and depression show cases for which the
multiple-corpora ensemble was successful but the single-
corpus ensembles were not. In the third example, both
the multiple-corpora ensemble and the clinical ensemble
extract the expected synonym candidate.
There was one query term  the drug name omepra-
zol  for which both single-corpus ensembles were able
to identify the synonym, but where the multiple-corpora
ensemble failed. There were also three query terms for
which synonyms were identified by the clinical ensemble,
but not by the multiple-corpora ensemble; there were five
query terms that were identified by the medical ensem-
ble, but not by the multiple-corpora ensemble. This shows
that combining semantic spaces can also, in some cases,
introduce noise.
Since synonym pairs were queried both ways, i.e. each
term in the pair would be queried to see if the other
could be identified, we wanted to see if there were
cases where the choice of query term would be impor-
tant. Indeed, among the sixty query terms for which the
expected synonym was not extracted, this was the case
in fourteen instances. For example, given the query term
blindtarmsinflammation (appendix-inflammation), the
expected synonym appendicit (appendicitis) was given as
a candidate, whereas with the query term appendicit, the
expected synonym was not successfully identified.
Models of distributional semantics face the problem of
modeling terms with several ambiguous meanings. This
is, for instance, the case with the polysemous term arv
(referring to inheritance as well as to heredity). Distant
synonyms also seem to be problematic, e.g. the pair reha-
bilitation/habilitation. For approximately a third of the
synonym pairs that are not correctly identified, however,
it is not evident that they belong to either of these two
categories.
Post-processing
In an attempt to improve results further, an additional
step in the proposed method was introduced: filtering
of the candidate terms, with the possibility of extract-
ing new, potentially better ones. For the extraction of
abbreviation-expansion pairs, this was fairly straightfor-
ward, as there are certain patterns that generally apply
to this phenomenon, such as the fact that the letters in
an abbreviation are contained  in the same order 
in its expansion. Moreover, expansions are longer than
abbreviations. This allowed us to construct simple yet
effective rules for filtering out unlikely candidate terms
for these two tasks. As a result, both precision and recall
increased; with a dynamic cut-off, precision improved
significantly. Although our focus in this study was pri-
marily on maximizing recall, there is a clear incentive
to improve precision as well. If this method were to
be used for terminological development support, with
humans inspecting the candidate terms, minimizing the
number of poor candidate terms has a clear value. How-
ever, given the seemingly easy task of filter out unlikely
candidates, it is perhaps more surprising that the results
were not even better. A part of the reason for this may
stem from the problem of semantically overloaded word
types, which affects abbreviations to a large degree, par-
ticularly in the clinical domain with its telegraphic style
and where ad-hoc abbreviations abound. This was also
reflected in the reference standard, as in some cases
the most common expansion of an abbreviation was not
included.
The post-processing filtering of synonyms clearly failed.
Although ranking information and, especially, cosine sim-
ilarity provide some indication of the quality of synonym
candidates, employing cut-off values with these features
can impossibly improve recall: new candidates will always
have a lower ranking and a lower cosine similarity score
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 20 of 25
http://www.jbiomedsem.com/content/5/1/6
Table 12 Examples of extracted candidate synonyms
Query term: sjukhem (nursing-home)
Clinical Medical Clinical + Medical
Heartcenter (heart-center) Vårdcentral (health-center) Vårdcentral (health-center)
Bröstklinik (breast-clinic) Akutmottagning (emergency room) Mottagning (reception)
Hälsomottagningen (health-clinic) Akuten (ER) Vårdhem (nursing-home)
Hjärtcenter (heart-center) Mottagning (reception) Gotland (a Swedish county)
Län (county) Intensivvårdsavdelning (ICU) Sjukhus (hospital)
Eyecenter (eye-center) Arbetsplats (work-place) Gård (yard)
Bröstklin (breast-clin.) Vårdavdelning (ward) Vårdavdelning (ward)
Sjukhems (nursing-homes) Gotland (a Swedish county) Arbetsplats (work-place)
Hartcenter (hart-center) Kväll (evening) Akutmottagning (emergency room)
Biobankscentrum (biobank-center) Ks (Karolinska hospital) Akuten (ER)
Query term: depression (depression)
Clinical Medical Clinical + Medical
Sömnstörning (insomnia) Depressioner (depressions) Sömnstörning ( insomnia)
Sömnsvårigheter (insomnia) Osteoporos (osteoporosis) Osteoporos (osteoporosis)
Panikångest (panic disorder) Astma (asthma) Tvångssyndrom (OCD)
Tvångssyndrom (OCD) Fetma (obesity) Epilepsi (epilepsy)
Fibromyalgi (fibromyalgia) Smärta (pain) Hjärtsvikt (heart failure)
Ryggvärk (back-pain) Depressionssjukdom (depressive-illness) Nedstämdhet (sadness)
Självskadebeteende (self-harm) Bensodiazepiner (benzodiazepines) Fibromyalgi (fibromyalgia)
Osteoporos (osteoporosis) Hjärtsvikt (heart-failure) Astma (asthma)
Depressivitet (depressitivity) Hypertoni (hypertension) Alkoholberoende ( alcoholism)
Pneumoni (pneumonia) Utbrändhet (burnout) Migrän (migraine)
Query term: allergi (allergy)
Clinical Medical Clinical + Medical
Pollenallergi (pollen-allergy) Allergier (allergies) Allergier (allergies)
Födoämnesallergi (food-allergy) Sensibilisering (sensitization) Hösnuva (hay-fever)
Hösnuva (hay-fever) Hösnuva (hay-fever) Födoämnesallergi ( food-allergy)
Överkänslighet (hypersensitivity) Rehabilitering (rehabilitation) Pollenallergi (pollen-allergy)
Kattallergi (cat-allergy) Fetma (obesity) Överkänslighet (hypersensitivity)
Jordnötsallergi (peanut-allergy) Kol (COPD) Astma (asthma)
Pälsdjursallergi (animal-allergy) Osteoporos (osteoporosis) Kol (COPD)
Negeras (negated) Födoämnesallergi (food-allergy) Osteoporos ( osteoporosis)
Pollen (pollen) Astma (asthma) Jordnötsallergi (peanut-allergy)
Pollenallergiker (pollen-allergic) Utbrändhet (burnout) Pälsdjursallergi (animal-allergy)
The top ten candidate synonyms for three different query terms with the clinical ensemble, the medical ensemble and the disjoint corpus ensemble. The synonym in
the reference standard is in boldface.
than discarded candidate terms. It can, however  at
least in theory  potentially improve precision when
using these rules in conjunction with a dynamic cut-off,
i.e. allowing less than ten candidates terms to be sug-
gested. In this case, however, the rules did not have this
effect.
Thresholds
Increasing the frequency threshold further did not
improve results. In fact, a threshold of 30 occurrences
in both corpora seems to be sufficient. A high frequency
threshold is a limitation of distributional methods; thus,
the ability to use a lower threshold is important, especially
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 21 of 25
http://www.jbiomedsem.com/content/5/1/6
in the clinical domain where access to data is difficult to
obtain.
The choice of evaluating recall among ten candidates
was based on an estimation of the number of candidate
terms that would be reasonable to present to a lexicog-
rapher for manual inspection. Recall might improve if
more candidates were presented, but it would likely come
at the expense of decreased usability. It might instead
be more relevant to limit further the number of candi-
dates to present. As is shown in Figure 4, there are only
a few correct synonyms among the candidates ranked 6
10. By using more advanced post-processing techniques
and/or being prepared to sacrifice recall slightly, it is
possible to present fewer candidates for manual inspec-
tion, thereby potentially increasing usability. On the other
hand, a higher cut-off value could be used for evaluating a
system aimed at a user who is willing to review a longer list
of suggestions. An option for incorporating this difference
in user behavior would be to use an evaluation metrics,
such as rank-biased precision [58], that models the per-
sistence of the user in examining additional lower-ranked
candidates.
Reflections on evaluation
To make it feasible to compare a large number of seman-
tic spaces and their various combinations, fixed reference
standards derived from terminological resources were
used for evaluation, instead of manual classification of
candidate terms. One of the motivations for the current
study, however, is that terminological resources are sel-
dom complete; they may also reflect a desired use of lan-
guage rather than actual use. A manual classification on a
sample of one of the reference standards,Medical + Clini-
cal, was carried out on the synonym task in order to verify
this claim. The results in this study thus mainly reflect to
what extent different semantic spaces  and their com-
binations  are able to extract synonymous relations that
have been considered relevant according to specific termi-
nologies, rather than to what extent the semantic spaces
 and their combinations  capture the phenomenon of
synonymy. This is, for instance, illustrated by the query
term depression in Table 12, in which one potential syn-
onym is extracted by the clinical ensemble  depressivitet
(depressitivity)  and another potential synonym by the
medical ensemble: depressionsjukdom (depressive illness).
Although these terms might not be formal or frequent
enough to include in all types of terminologies, they are
highly relevant candidates for inclusion in terminologies
intended for text mining. Neither of these two terms
are, however, counted as correct synonyms, and only the
multiple-corpora ensemble is able to find the synonym
included in the terminology.
Furthermore, a random sample of 30 words (out of
135) was manually classified for the semantic relation
between each of the candidate terms in the sample, as
suggested by the best combination of semantic spaces
(the Disjoint Corpus Ensemble, see Table 10), and the
target term. In the reference standard for this sample,
33 synonyms are to be found (only three target words
have two synonyms; none have three or more). The best
combination finds only 10 of these reference synonyms
(exact match), which accounts for the low recall figures
in Table 10. However, a manual classification shows that
the same combination finds another 29 synonyms that
do not occur in the reference standard. Furthermore,
the Disjoint Corpus Ensemble also suggests a total of 15
hyponyms, 14 hypernyms and 3 spelling variants as can-
didate terms, which, depending on the context, can be
viewed as synonyms. Among the candidate terms, we also
find 3 antonyms, which shows the inability of the models
readily to distinguish between different types of semantic
relations.
In one instance, we also capture a non-medical sense of
a term while completely missing the medical sense. For
the target term sänka (erythrocyte sedimentation rate), 9
out of 10 candidate terms relate to the more general sense
of lowering something (also sänka in Swedish), with can-
didate terms such as rising, reducing, increasing, halving
and decreasing. None of these are included in the ref-
erence standard, which for this word only contains the
abbreviation SR (ESR) as a synonym.
In the case of the target term variecella, the reference
standard contains only the synonym vattkoppor (chick-
enpox), while the Disjoint Corpus Ensemble correctly
suggests the abbreviation VZV, as well as herpes and the
plural form varicellae (which is apparently missed by the
lemmatizer).
It is important to recognize that this type of manual
post-evaluation always bears the risk that you are too
generous, believing in your method, and thus (manually)
assign too many correct classifications  or, alternatively
that you are too strict in your classification in fear of being
too generous. Future studies would thus benefit from
an extensive manual classification of candidates derived
from data generated in clinical practice, beforehand, with
the aim of also finding synonyms that are not already
included in current terminologies but are in frequent
use. These could then be used as reference standards in
future evaluations.
The choice of terminological resources to use as ref-
erence standards was originally based on their appropri-
ateness for evaluating semantic spaces induced from the
clinical corpus. However, for evaluating the extraction
of abbreviation-expansion pairs with semantic spaces
induced from the medical corpus, the chosen resources 
in conjunction with the requirement that terms should
occur at least fifty times in the corpus  were less appro-
priate, as it resulted in a very small reference standard.
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 22 of 25
http://www.jbiomedsem.com/content/5/1/6
This, in turn, resulted in no significant differences for
either of the two the abbreviation tasks between the best
single space and the combination of medical spaces, or
between the conjoint corpus ensemble and the disjoint
corpus ensemble. When assessing the potential of using
semantic spaces for abbreviation-expansion tasks, more
focus should therefore be put on the results from the eval-
uation on the spaces created from the clinical corpus, as
the improvement in recall gained by post-processing was
statistically significant for both the abbr?exp task and the
exp?abbr task, as was also the improvement gained from
using an ensemble of spaces compared to a single corpus
space for the abbr?exp task.
For synonyms, the number of instances in the reference
standard is, of course, smaller for the experiments with
multiple-corpora ensembles than for the single-corpus
experiments. However, the differences between the single
space and the ensemble of spaces are statistically signifi-
cant. Moreover, when evaluating the final results with dif-
ferent frequency thresholds, similar results are obtained
when lowering the threshold and, as a result, including
more evaluation instances. With a threshold of twenty
occurrences, 306 input terms are evaluated, which results
in a recall of 0.42; with a threshold of thirty occurrences
and 222 query terms, a recall of 0.46 is obtained.
Future work
Now that this first step has been taken towards creat-
ing ensembles of semantic spaces, this notion should be
explored in greater depth and taken further. It would, for
instance, be interesting to combine a larger number of
semantic spaces, possibly including those that have been
more explicitly modeled with syntactic information. To
verify the superiority of this approach, it should be com-
pared to the performance of a single semantic space that
has been induced from multiple corpora.
Further experiments should likewise be conducted
with combinations involving a larger number of corpora
(types). One could, for instance, combine a professional
corpus with a layman corpus  e.g. a corpus of extracts
from health-related fora  in order to identify layman
expressions for medical terms. This could provide a useful
resource for automatic text simplification.
Another technique that could potentially be used to
identify term pairs with a higher degree of semantic simi-
larity is to ensure that both terms have each other as their
closest neighbors in the semantic subspace. This is not
always the case, as we pointed out in our error analysis.
This could perhaps improve performance on the task of
extracting synonyms and abbreviation-expansion pairs.
A limitation of the current study  in the endeavor to
create a method that accounts for the problem of language
use variability  is that the semantic spaces were con-
structed to model only unigrams. Textual instantiations
of the same concept can, however, vary in term length.
This needs to be accounted for in a distributional frame-
work and concerns paraphrasing more generally than
synonymy in particular. Combining unigram spaces with
multiword spaces is a possibility that could be explored.
This would also make the method applicable for acronym
expansion.
Conclusions
This study demonstrates that combinations of semantic
spaces can yield improved performance on the task of
automatically extracting synonyms. First, combining two
distributional models  random indexing and random
permutation  on a single corpus enables the capturing
of different aspects of lexical semantics and effectively
increases the quality of the extracted candidate terms, out-
performing the use of one model in isolation. Second,
combining distributional models and types of corpora 
a clinical corpus, comprising health record narratives, and
a medical corpus, comprising medical journal articles 
improves results further, outperforming ensembles of
semantic spaces induced from a single source, as well as
single semantic space induced from the conjoint corpus.
We hope that this study opens up avenues of exploration
for applying the ensemble methodology to distributional
semantics.
Semantic spaces can be combined in numerous ways. In
this study, the approach was to combine the outputs, i.e.
ranked lists of semantically related terms to a given query
term, of the semantic spaces. How this should be done is
not wholly intuitive. By exploring a variety of combination
strategies, we found that the best results were achieved by
simply summing the cosine similarity scores provided by
the distributional models.
On the task of extracting abbreviation-expansion pairs,
substantial performance gains were obtained by applying
a number of simple post-processing rules to the list of can-
didate terms. By filtering out unlikely candidates based on
simple patterns and retrieving new ones, both recall and
precision were improved by a large margin.
Lastly, analysis of a manually classified sample from the
synonym task shows that the semantic spaces not only
extract synonyms that are present in the reference stan-
dard. Equally valid synonyms not present in the reference
standard are also found. This serves to show that the refer-
ence standards, as most often is the case, lack in coverage,
as well as supports the fact that the semantic spaces can
be used to enrich and expand such resources.
Endnotes
aSignifiers are here simply different linguistic items
referring to the same concept.
bOntologies are formal descriptions of concepts and
their relationships.
Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 23 of 25
http://www.jbiomedsem.com/content/5/1/6
cThe words big and large are, for instance, synonymous
when describing a house, but certainly not when
describing a sibling.
dUnified Medical Language System: http://www.nlm.
nih.gov/research/umls/
eHyponyms are words that are subordinate to another
word, its hypernym. For instance, dog is a hyponym of
mammal, which in turn is a hyponym of animal.
fThere are also probabilistic models, which view
documents as a mixture of topics and represent terms
according to the probability of their occurrence during
the discussion of each topic: two terms that share similar
topic distributions are assumed to be semantically related.
gExplicit dimensionality reduction is avoided in the
sense that an initial term-context matrix is not
constructed, the dimensionality of which is then reduced.
The high-dimensional data is prereduced, if you will, by
selecting a much lower dimensionality from the outset
(effectively making this a parameter of the model).
hTernary vectors allow three possible values: +1s, 0s
and ?1s. Allowing negative vector elements ensures that
the entire vector space is utilized.
iOrthogonal index vectors would yield completely
uncorrelated context representations; in the RI
approximation, near-orthogonal index vectors result in
almost uncorrelated context representations.
jThe bag-of-words model is a simplified representation
of a text as an unordered collection of words, where
grammar and word order are ignored.
kAn alternative is to shift the index vectors according to
direction only, effectively producing direction vectors [46].
lThis research has been approved by the Regional
Ethical Review Board in Stockholm
(Etikprövningsnämnden i Stockholm), permission
number 2012/834-31/5.
mThe used stop word lists are available at http://people.
dsv.su.se/~mariask/resources/stoppord.txt (clinical
corpus) and http://people.dsv.su.se/~mariask/resources/
lt_stoppord.txt. (medical corpus)
nAntonyms are words that differ in one dimension of
meaning, and thus are mutually exclusive in this sense.
For instance, something cannot be both large and small
in size at the same time.
oHypernyms are words that are superordinate to
another word, its hyponym. For instance, animal is a
hypernym ofmammal, which in turn is a hypernym of
dog.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
AH was responsible for coordinating the study and was thus involved in all
parts of it. AH was responsible for the overall design of the study and for
carrying out the experiments. AH initiated the idea of combining semantic
spaces induced from different corpora and implemented the evaluation and
post-processing modules. AH also had the main responsibility for the
manuscript and drafted parts of the background and results description. HM
and MS contributed equally to the study. HM initiated the idea of combining
semantic models trained differently (Random Indexing and Random
Permutation) and was responsible for designing and implementing strategies
for combining the output of multiple semantic models. HM also drafted parts
of the method description in the manuscript and surveyed relevant literature.
MS initiated the idea of applying the method to abbreviation-expansion
extraction and to different types of corpora. MS was responsible for designing
the evaluation part of the study, as well as for preparing the reference
standards. MS also drafted parts of the background and method description in
the manuscript. VD, together with MS, was responsible for designing the
post-processing filtering of candidate terms. MD provided feedback on the
design of the study and drafted parts of the background and method
description in the manuscript. MD also carried out the manual evaluation, and
the analysis thereof. AH, HM, MS and MD analyzed the results and drafted the
discussion and conclusions in the manuscript. All authors read and approved
the final manuscript.
Acknowledgements
This work was partly (AH) supported by the Swedish Foundation for Strategic
Research through the project High-Performance Data Mining for Drug Effect
Detection (ref. no. IIS11-0053) at Stockholm University, Sweden. It was also
partly (HM) supported by the Research Council of Norway through the project
EviCare - Evidence-based care processes: Integrating knowledge in clinical
information systems (NFR project no. 193022). We would like to thank the
members of our former research network HEXAnord, within which this study
was initiated. We would especially like to thank Ann-Marie Eklund for her
contributions to the initial stages of this work. We are also grateful to Staffan
Cederblom and Studentlitteratur for giving us access to their database of
medical abbreviations. Finally, we would like to thank the three reviewers for
their insightful comments.
Author details
1Department of Computer and Systems Sciences (DSV), Stockholm University,
Forum 100, SE-164 40 Kista, Sweden. 2Department of Computer and
Information Science, Norwegian University of Science and Technology,
NO-7491 Trondheim, Norway. 3Faculty of Informatics, Vytautas Magnus
University, Vileikos g. 8 - 409, Kaunas, LT-44404, Lithuania.
Received: 4 June 2013 Accepted: 17 January 2014
Published: 5 February 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Sarntivijai et al. Journal of Biomedical Semantics 2014, 5:37
http://www.jbiomedsem.com/content/5/1/37DATABASE Open AccessCLO: The cell line ontology
Sirarat Sarntivijai1,2*, Yu Lin2, Zuoshuang Xiang2, Terrence F Meehan3, Alexander D Diehl4, Uma D Vempati5,
Stephan C Schürer5, Chao Pang6, James Malone3, Helen Parkinson3, Yue Liu2, Terue Takatsuki7, Kaoru Saijo7,
Hiroshi Masuya7, Yukio Nakamura7, Matthew H Brush8, Melissa A Haendel8, Jie Zheng9, Christian J Stoeckert9,
Bjoern Peters10, Christopher J Mungall11, Thomas E Carey2, David J States12, Brian D Athey2 and Yongqun He2*Abstract
Background: Cell lines have been widely used in biomedical research. The community-based Cell Line Ontology (CLO)
is a member of the OBO Foundry library that covers the domain of cell lines. Since its publication two years ago,
significant updates have been made, including new groups joining the CLO consortium, new cell line cells, upper
level alignment with the Cell Ontology (CL) and the Ontology for Biomedical Investigation, and logical extensions.
Construction and content: Collaboration among the CLO, CL, and OBI has established consensus definitions of cell
line-specific terms such as cell line, cell line cell, cell line culturing, and mortal vs. immortal cell line cell. A cell
line is a genetically stable cultured cell population that contains individual cell line cells. The hierarchical structure of
the CLO is built based on the hierarchy of the in vivo cell types defined in CL and tissue types (from which cell line
cells are derived) defined in the UBERON cross-species anatomy ontology. The new hierarchical structure makes it
easier to browse, query, and perform automated classification. We have recently added classes representing more
than 2,000 cell line cells from the RIKEN BRC Cell Bank to CLO. Overall, the CLO now contains ~38,000 classes of
specific cell line cells derived from over 200 in vivo cell types from various organisms.
Utility and discussion: The CLO has been applied to different biomedical research studies. Example case studies
include annotation and analysis of EBI ArrayExpress data, bioassays, and host-vaccine/pathogen interaction. CLOs utility
goes beyond a catalogue of cell line types. The alignment of the CLO with related ontologies combined with the use
of ontological reasoners will support sophisticated inferencing to advance translational informatics development.
Keywords: Cell line, Cell line cell, Immortal cell line cell, Mortal cell line cell, Cell line cell culturing, AnatomyBackground
Cell culturing dates back to as early as 1911 when Alexis
Carrel attempted to grow living cells outside an organ-
ism. The establishment of the first human cell line,
HeLa, in 1951 has since brought the fruitful develop-
ment of other cell lines from different organisms. Cell
lines have been commonly used in many aspects of bio-
medical research and experimentation. Mass production
of cell line culture of animal cells is fundamental to the
manufacture of viral vaccines and other products in bio-
technology such as enzymes, synthetic hormones, anti-
cancer agents, and immunobiologicals (e.g., monoclonal
antibodies, interleukins, and lymphokines). However, it* Correspondence: siiraa@umich.edu; yongqunh@med.umich.edu
1US Food and Drug Administration, Silver Spring, MD, USA
2University of Michigan, Ann Arbor, MI, USA
Full list of author information is available at the end of the article
© 2014 Sarntivijai et al.; licensee BioMed Cent
Commons Attribution License (http://creativec
reproduction in any medium, provided the orhas been realized that cell lines are often contaminated by
other lines  for example, the robust HeLa line has been
shown to have widely contaminated many cell lines [1-3].
In addition to the cross-contamination, other issues
exist in the domain of cell line representation. Due in
large part to a history of bottom-up naming practices,
cell line nomenclature has not been standardized or con-
trolled by any centralized authority. This has made man-
agement and tracking of cell line information a difficult
task, despite the existence of various public repositories
and indexed catalogues available for open access. More-
over, cell line related terms are loosely interchangeable
and inconsistently used across communities, such that
terms like primary cells, primary cell culture, and cell
line have become loaded with conflated and ambiguous
meaning. Confusion can also come from variability in
how cell lines are categorized. This results in part fromral Ltd. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly credited.
Sarntivijai et al. Journal of Biomedical Semantics 2014, 5:37 Page 2 of 10
http://www.jbiomedsem.com/content/5/1/37the wide range of methods for generating and modifying
cell lines confer diverse attributes used in their clas-
sification. As we move toward the establishment of a
centralized resource for cell lines, the ambiguity of cell
line-associated terms needs to be clarified.
Many of the challenges can be addressed by the devel-
opment of an ontology for cell lines, wherein the various
cell line attributes can be normalized and based on
agreement between users in the community. The dif-
ferent aspects of describing a cell line can be modu-
larized by their corresponding source organism and
anatomical part, modifications, and culturing methods,
or related diseases.
The Cell Line Ontology (CLO) is a community-based
ontology that covers the biological cell line domain. The
CLO was originally presented in the International Con-
ference on Biomedical Ontology (ICBO) in 2011 [4].
The original CLO was developed cooperatively by ontol-
ogy editors from the University of Michigan Cell Line
Knowledge base (CLKB) team, the EBI Functional Gen-
omics Production Team, Cell Ontology (CL) [5] team,
and the Bioassay Ontology (BAO) development team at
the University of Miami. Subsequently, the Cell Bank of
RIKEN BioResource Center (BRC) in Japan, the Ontology
for Biomedical Investigation [6], and Reagent Ontology [7]
joined the CLO development consortium. The CLO Con-
sortium aims to unify publicly available cell line data from
multiple sources to a standardized format based on a con-
sensus design pattern derived from the establishment of
CLO. This manuscript focuses on introducing recent up-
dates on the CLO development.
Construction and content
CLO statistics
The development of CLO follows the OBO Foundry
principles, including openness, collaboration, and use of
a common shared syntax [8]. As a result, the CLO devel-
opers have been working to establish a common under-
standing and agreement of key CLO-specific terms. The
CLO terms and definitions have been created and re-
fined with the input from participants who utilize the
CLO in their studies. In addition, the CLO also makes
use of existing ontologies via an OntoFox import strategy
[9]. The CLO is aligned with the Basic Formal Ontology
(BFO; http://www.ifomis.org/bfo) version 2 (Graz release)
by importing of all the class terms of BFO as its upper level
ontology. Note that the preparation for BFO version 2.0
alignment has also been implemented in the event that its
use becomes an OBO recommendation. Object properties
imported from the Relation Ontology (RO) are used to
represent relations in the CLO. Some terms from Spatial
Ontology (BSPO), SemanticScience Integrated Ontology
[10] and Information Artifact Ontology (IAO) [11] are also
imported as part of the top-level ontology manager. Table 1summarizes classes imported for the upper-level ontology
operations, along with the classes utilized from other exter-
nal ontologies to establish data integration and to support
automated reasoning.
In summary, CLO consists of over 38,000 terms for
various cell line cells. These are mostly cell line informa-
tion obtained from cell line records deposited at four cell
line resources: the ATCC and HyperCLDB cell lines
stored in the CLKB from the University of Michigan,
Corriell Cell Lines processed by EBI, and the cell lines
from the Cell Bank of RIKEN BioResource Center (BRC).
Almost 300 cell line entry descriptors of cell types were
imported from CL and over 1,300 anatomical entities were
imported from UBERON [12]. CLO currently contains in-
formation of cell lines derived from more than 350 species
(NCBITaxon entities). Biomedical experiment related
terms were imported from OBI and EFO [13]. When
applicable, components from the following resources: Gene
Ontology (GO), Phenotypic Quality Ontology (PATO),
Protein Ontology (PRO) [14], Chemical Entity of Biological
Interest (ChEBI), and Human Disease Ontology (DOID)
are imported into CLO based on available information in
the cell line cell records.
The CLO was developed using the format of W3C
standard Web Ontology Language (OWL2) (http://www.
w3.org/TR/owl-guide/). The latest CLO is available for
public view and download at http://code.google.com/p/
clo-ontology/. The latest version of the CLO is also avail-
able for visualization and downloading from Ontobee
(http://www.ontobee.org/browser/index.php?o=CLO) or
NCBOs BioPortal: (http://purl.bioontology.org/ontology/
CLO). The source code of CLO is open and freely available
under the Apache License 2.0.
Alignment of core domain concepts between CLO, OBI,
and CL
As part of the CLO refactoring process, a working group
was established between members of several key open
biomedical ontologies where cell line-related entities are
represented, including the Cell Ontology (CL) and the
Ontology for Biomedical Investigation [6]. The goal of
this group was to align modelling related to cultured
cells in accordance with OBO Foundry principles of or-
thogonality and re-use. One key outcome of this work
was the integration of inconsistent representations into a
single shared model. Classes representing key concepts
were implemented in the CL, CLO, and OBI - with the
CL as a home for high-level in vitro cell modelling (e.g.
cultured cell), the CLO as a home for more specific cell
line cell and cell line classes, and the OBI as a home for
experimental entities related to these cell lines (e.g. cell
line culture and establishing cell line classes). As a result,
each term has a single representation that is re-used be-
tween ontologies through established import mechanisms.
Table 1 Summary of ontology terms in CLO and major source ontologies used in CLO as of November 21st, 2013
Ontology Classes Object properties Annotation properties Total
CLO (Cell Line Ontology) specific 38453 2 0 38455
Imported upper-level ontologies
BFO (Basic Formal Ontology) 22 38 0 60
RO (Relation Ontology) 0 85 1 86
BSPO (Spatial Ontology) 0 18 0 18
SIO (SemanticScience Integrated Ontology) 0 3 0 3
IAO (Information Artifact Ontology) 17 2 17 36
Imported entities from other external ontologies
OBI (Ontology for Biomedical Investigation) 20 6 2 28
EFO (Experimental Factor Ontology) 149 1 1 151
CL (Cell Ontology) 269 0 0 269
UBERON 1315 0 0 1315
NCBITaxon (NCBI Taxonomy) 354 0 0 354
PATO (Phenotypic Quality Ontology) 22 0 0 22
GO (Gene Ontology) 299 0 0 299
PR (Protein Ontology) 5 0 0 5
DOID (Human Disease Ontology) 741 0 0 741
ChEBI (Chemical Entities of Biological Interest) 32 0 0 32
Total 41698 155 21 41874
The detail of total number of terms and the most recent update can be viewed at http://www.ontobee.org/ontostat.php?ontology=CLO.
Sarntivijai et al. Journal of Biomedical Semantics 2014, 5:37 Page 3 of 10
http://www.jbiomedsem.com/content/5/1/37A second key outcome of this alignment work was the
crafting of clear consensus definitions for common but
ambiguous domain terminology, including a careful
characterization of the term 'cell line' itself. Updated
definitions for a selection of key CLO classes resulting
from this work are detailed below.
A cell line is defined as a genetically stable and
homogenous population of cultured cells that shares a
common propagation history (i.e. has been successively
passaged together in culture). This view clarifies two key
confusions surrounding the term cell line. The first re-
lates to the scale at which the term applies, here refer-
ring to discrete experimental populations rather than
maximal collections representing an entire lineage (e.g.
the collection of all HeLa cells that exist). The second
concerns the criteria that establish when a cultured cell
population qualifies as a line. By applying cell line to
experimental populations with a shared culture history,
we define the term consistently with its most prevalent
usage in domain discourse, and in a way that is most fit
for data annotation needs, as it represents populations
that are actually cultured, experimented upon, and shared
in the practice of science.
A cell line cell is defined simply as a cultured cell that
is part of a cell line. This class is a child of CL:cultured
cell, defined as a cell in vitro that is or has been cultured
in vitro (Figure 1). Cell line cells, like cell lines, can existin active culture or stored in quiescence. The represen-
tation of each specific cell line (e.g. HeLa, HEK293) is
implemented at the scale of individual cells, such that
cell line cell is the root of the core CLO hierarchy of
cell line types.
A clonal cell line is defined as a cell line that derives
from a single cell that is expanded in culture. Feedback
from community experts and stakeholders initiated the
representation of this specific type of cell line as a key ex-
perimental resource with unique and valuable attributes.
Finally, a cell line culture represents an actual physical
culture of cell line cells that is an input to experimental
processes, and is comprised cell line cells and the media
along with any added components. This term is intended
to cover actively propagated cultures as well as those
kept frozen.
Through the alignment efforts summarized above, we
have increased the utility of the CLO as a community
resource for standardizing reference to domain concepts
and facilitating the exchange and discovery of cell line
related information.
Basic CLO cell line design pattern
The basic CLO design pattern represents organ anat-
omy, cell types, disease and pathology, source informa-
tion in the form of ownership and derivation where cell
lines are related to each other, and technical information
Figure 1 The top level CLO hierarchical structure and key ontology terms. Terms imported from other ontologies are indicated by the
ontology abbreviations inside parentheses. Terms without an identified source are CLO terms. All the arrows indicate the is_a relation except the
explicitly labelled has grain relation.
Sarntivijai et al. Journal of Biomedical Semantics 2014, 5:37 Page 4 of 10
http://www.jbiomedsem.com/content/5/1/37such as culture conditions (Figure 2A). A core design
pattern implemented for, cell line cell, describes deriv-
ation from an in vivo cell, which is in turn, part of some
anatomical structure, and in turn, anatomical structure
is part of a species of organism. In addition, diseases
borne by the source organism can be captured in this
pattern when known. An object property is model for is
defined to represent a relation between a cell line cell
and a disease where the cell line cell is a model system
for studying the disease. An example of applying this
general design pattern to a concrete example is shown
in Figure 2B where the design pattern is used to repre-
sent a HeLa cell transfected with luciferase reporter in
BAO. In this example, a HeLa cell transfected with a lu-
ciferase reporter derives from HeLa cell through a
stable transfection that is a cell line cell modification
process. The transfected HeLa cells are part of or speci-
fied input of cell line culturing process, specifically, ad-
herent cell line culturing process. This cell line cell also
inherits the pathological cell properties from HeLa cell
(as a cell line cell), which derives from some epithelial
cell that is part of  some uterine cervix, which is part of 
an organism Homo sapiens. This specific instance of
Homo sapiens also has disease of some cervical carcin-
oma, which is a carcinoma and HeLa cell is a cell line
model for studying this cancer. Relations of knowledge
in multiple domains are shown in this example of a cell
line cell annotation in BAO with the semantic infra-
structure provided by CLO. Modification of a cell line
cell can give rise to another derived cell line cell, which
is also described by derives from relation. A cell line
cell is specified input of some special culturing process
(CLO:cell line cell culturing, a subclasss of OBI:maintaining
a cell line culture), which can differ from culturing one cellline cell to another (e.g., suspension cell line culturing
or adherent cell line culturing). The relation derives
from is not a transitive relation. The derives from re-
lation could be transitive if, and only if, there are no
confounding environmental and/or experimental con-
ditions that affect the cell lines characteristics (such
as cross contamination). The cell line cell culturing re-
flects a particular culturing condition or growth mode
(e.g. suspension or cell surface adhesion). A cell line is
supplied, maintained, or catalogued by a specific organi-
zation such as American Type Cell Culture (ATCC) that
has cell line repository role. Since relation terms such as
supply, own, or manage have not been fully developed
in any ontology, we have created a CLO-specific relation
reflecting this activity with the label is in cell line reposi-
tory. This object property designates the representation
of a particular cell lines information in such repository.
This is an update from the previous version that utilized
relation mentions to describe this activity. The relation
of class label mentions was used in CLOs previous re-
JOURNAL OF
BIOMEDICAL SEMANTICS
Laurenne et al. Journal of Biomedical Semantics 2014, 5:40
http://www.jbiomedsem.com/content/5/1/40
RESEARCH Open Access
Making species checklists understandable to
machines  a shift from relational databases to
ontologies
Nina Laurenne1*, Jouni Tuominen1, Hannu Saarenmaa2 and Eero Hyvönen1
Abstract
Background: The scientific names of plants and animals play a major role in Life Sciences as information is indexed,
integrated, and searched using scientific names. The main problem with names is their ambiguous nature, because
more than one name may point to the same taxon and multiple taxa may share the same name. In addition, scientific
names change over time, which makes them open to various interpretations. Applying machine-understandable
semantics to these names enables efficient processing of biological content in information systems. The first step is to
use unique persistent identifiers instead of name strings when referring to taxa. The most commonly used identifiers
are Life Science Identifiers (LSID), which are traditionally used in relational databases, and more recently HTTP URIs,
which are applied on the Semantic Web by Linked Data applications.
Results: We introduce two models for expressing taxonomic information in the form of species checklists. First, we
show how species checklists are presented in a relational database system using LSIDs. Then, in order to gain a more
detailed representation of taxonomic information, we introduce meta-ontology TaxMeOn to model the same content
as Semantic Web ontologies where taxa are identified using HTTP URIs. We also explore how changes in scientific
names can be managed over time.
Conclusions: The use of HTTP URIs is preferable for presenting the taxonomic information of species checklists. An
HTTP URI identifies a taxon and operates as a web address from which additional information about the taxon can be
located, unlike LSID. This enables the integration of biological data from different sources on the web using Linked
Data principles and prevents the formation of information silos. The Linked Data approach allows a user to assemble
information and evaluate the complexity of taxonomical data based on conflicting views of taxonomic classifications.
Using HTTP URIs and Semantic Web technologies also facilitate the representation of the semantics of biological data,
and in this way, the creation of more intelligent biological applications and services.
Keywords: Scientific name, Taxonomic concept, LSID, HTTP URI, Ontology, Semantic web, Linked data,
Species checklist
Background
Research on biodiversity requires integrating data from
distributed heterogeneous sources, such as scientific lit-
erature, observations, and biomedical resources. Data is
often presented using a variety of terms, vocabularies, and
languages, which presents a barrier to interoperability and
*Correspondence: nina.laurenne@helsinki.fi
Equal contributors
1Semantic Computing Research Group (SeCo), Department of Media
Technology, Aalto University, P.O. Box 15500, 00076 Aalto, Espoo, Finland
Full list of author information is available at the end of the article
makes data reuse and integration a challenge for both
human users and machines.
Scientific names are important for interlinking infor-
mation about taxa in all fields of the Life Sciences. A
taxon is a group of one or more organisms whose mem-
bers are considered evolutionarily related to one another;
a taxon typically has a name and rank, i.e., a species, genus,
etc. Taxon names are especially necessary when indexing
biological information and cataloguing biodiversity. The
nature of names, whether important or problematic, has
recently been re-examined by several researchers [1-6].
© 2014 Laurenne et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited.
Laurenne et al. Journal of Biomedical Semantics 2014, 5:40 Page 2 of 13
http://www.jbiomedsem.com/content/5/1/40
Difficulties arise when a particular taxon can be referred
to using multiple names, since scientists opinions differ
on how evolutionary units should be organised into classi-
fications. Also, researchers may use the same name with a
different meaning when referring to taxa. Well-conducted
taxonomic studies may be 250 years old and still use-
ful but in most cases, the perceived boundaries of taxa
have been revised several times after the original publica-
tion. Contrary to popular belief, a generally agreed-upon,
single taxonomy of organisms does not exist, and this
fact is directly reflected in the scientific naming system
through the various usages of names. For a taxonomist,
a scientific name is a label that mirrors an evolution-
ary hypothesis that is under continuous testing. There
will never be a commonly agreed upon single taxonomy
and there will always be multiple competing current tax-
onomic views. Nevertheless, efforts are made to provide
usable taxonomies for non-taxonomists.
Checklists are species catalogues where taxa are organ-
ised hierarchically according to an authors current view of
a classification. The coverage of a species checklist varies
from a geographically limited area to a worldwide list,
and it typically focuses on a particular organismal group.
An authors view of research results is thus inevitably
emphasised, which opens the lists to interpretation if
they lack sufficient taxonomic details. A regional species
list indexes taxa of a given area, but it can also contain
additional information. For example, Fauna Europaea [7]
and the Atlas of Living Australia [8] provide distribution
maps and visualisation tools. The database Encyclope-
dia of Life (EoL) [9] covers the whole world and has a
considerable amount of species information. Also, unlike
most resources, it supports multiple classifications since
data providers can upload differing taxonomies into the
system.
Checklists were previously only published in journals
(static lists), but up-to-date checklists (dynamic lists) are
increasingly available on the web. For example, the most
notable database, Catalogue of Life (CoL) [10], aims to
include all known species and currently contains nearly
1,352,112 species from 132 taxonomic datasets (2013
Annual Checklist). The database of zoological names
ZooBank [11] currently has 101,777 nomenclatural acts.
The Global Biodiversity Information Facility (GBIF) [12]
has made an effort to stabilise name usage by setting up
a Checklist Bank [13] for storing names and information
about them. The widely used Taxonomic Concept Trans-
fer Schema (TCS) [14] specifies the format (XML), in
which taxonomic information is presented when exchang-
ing data. Darwin Core (DwC) [15,16], created by Biodi-
versity Information Standards (TDWG) [17], is a stan-
dardised form of presenting biological information. The
metadata elements in DwC are not strictly defined as
the format and the element values are not fully specified.
This means that the interoperability of DwC records is
not achieved if the elements are not used in a consis-
tent way. For example, a taxon name may be a literal
value or referred to using a URI. Darwin Core Archive
(DwC-A) [18] is a data standard for producing a self-
contained dataset for sharing species-related data, such
as occurrence records and checklists. The CSV (Comma-
Separated Values) data files of an archive are organised
in a star-like manner, with one core data file and possible
extensions, e.g., for vernacular names or distribution data.
The scope of biomedical resources differs from check-
lists because the focus is on a gene or a cell level. Nev-
ertheless, the name question remains relevant because
scientific names are used for linking information. Cur-
rently, the National Center for Biotechnology Information
(NCBI) [19] provides a single robust consensus hierar-
chy of taxa constructed by experts, but NCBI ambi-
tiously seeks to build a topology based on monophyletic
groups, i.e., taxa derived from a common ancestor. NCBI
allows flexibility in the acceptance of informal names
and surrogate names can be used when contributing
data and searching for taxa [5]. The majority of the
submitted DNA sequences do not have a binominal sci-
entific name because specimens are not identified into
a species level at the time of submission or only sur-
rogate names are used [5]. The significance of DNA
sequence data is increasing due to the rapid development
of molecular methods that are applied in constructing
evolutionary hypotheses and barcoding biodiversity. Con-
sequently, descriptions of new species based on molecular
evidence result in an increased number of species in
checklists.
A major source for ambiguity in scientific names is that
they change over time. One of the most common types of
change concerns a Linnean binominal name combination.
The genus of a binominal name changes when a species is
moved to another genus. For example, the parasitic wasp
species moscaryi once belonged to the genus Tetraconus,
but as a result of a taxonomic revision that synonymised
two genera, its new name combination is Monomachus
moscaryi [20]. Synonymisation happens due to assess-
ments of the identity of types (i.e., typically a physical
specimen to which a scientific name is attached). If two
or more taxa are lumped, the older name remains valid
but with a changed taxonomic circumscription, and the
more recent names become its synonyms. Consequently,
there is more than one name pointing to the taxon, and
the taxonomic concept associated with the older name
changes. The opposite situation is the split of taxa, where
one taxon is divided into two or more taxa. The diver-
gence between a name and its meaning is characteristic
of taxonomy, because a scientific name does not neces-
sarily change despite the fact that taxon boundaries are
redefined. Researchers can also classify the same species
Laurenne et al. Journal of Biomedical Semantics 2014, 5:40 Page 3 of 13
http://www.jbiomedsem.com/content/5/1/40
in various ways, thus leading to the existence of multiple
name combinations.
Berendsohn [21] introduced the concept of a poten-
tial taxon, which is a scientific name with information
on a circumscription. He proposed the term secundum
(abbr. sec) be attached to a scientific name when refer-
ring to a particular taxonomic circumscription. This was
a concrete suggestion on how to interlink differing tax-
onomic views while continuing to retain the adequate
taxonomic information in databases [22]. Having infor-
mation on circumscriptions in databases is an improve-
ment, but machine-readable semantics need to be used in
order to enhance themachine-processability of taxonomic
information.
In this paper we present two models for describing tax-
onomic information in a machine-processable way. The
first model describes species checklists as a relational
database and the second one is further developed repre-
sentation of taxonomic information using Semantic Web
technologies. We explore the reasons for moving away
from relational databases towards semantic technology,
and we also discuss options for managing scientific names
as they change over time.
Towards semantic handling of biological names
A biologist understands the semantics of scientific names
by reading scientific literature, but computers require
explicit identifier systems and data models to process
semantics. It is obvious that persistent identifiers for taxa
should be used instead of ambiguous name strings to
increase the processability of scientific names. Using iden-
tifiers allows information to be connected unambiguously,
which enables interoperability between systems. Further-
more, there is a need to interlink taxa between the differ-
ent versions of checklists as they are updated. Otherwise,
data indexed using an earlier version of a checklist can-
not categorically be found using a later version of the
checklist.
Recognising taxa using identifiers
The most commonly used identifiers in biology are Life
Science Identifiers (LSID) [23]. An LSID consists of six
parts (Figure 1): the first two indicate that the type of
URN (Uniform Resource Name) is an LSID, the third
part expresses the authority, and the fourth specifies the
namespace (which specifies the type of an LSID, e.g., sci-
entific name, living thing, picture, or museum specimen),
the fifth points to the object ID, and the optional sixth part
is for versioning information. An LSID can be accommo-
dated to a single name or to a set of taxonomic details,
depending on its purpose [2,24]. For example, identi-
fiers are given to scientific names in the World Register
of Marine Species [25], but in the Catalogue of Life [10]
they are given to taxonomic concepts. The Universal Bio-
logical Indexer and Organizer (uBio) [26] has 11,106,374
namebank records where LSIDs are used for referring to
taxonomic concepts [6]. Also, an RDF (Resource Descrip-
tion Framework) representation [27] is provided but some
of the essential information is expressed as literals (a
classification, taxonomic rank and a typing of resources)
instead of URIs, which hampers machine-processability.
The data carried by an LSID is obtained using a specific
resolver. Locating the resolver via the Domain Name Sys-
tem (DNS) of the Internet requires that the resolver be
configured in a DNS SRV record (DNS service record) of
the domain used as the authority part of an LSID. LSIDs
can also be used without a resolver if they are presented as
HTTP URIs using an LSID HTTP proxy. According to the
TDWG guidelines for using identifiers, an LSID resolver
should return metadata about the requested resource in
RDF form [27]. The application of LSIDs in the Catalogue
of Life is thoroughly discussed by Jones et al. [2]. GBIF has
published recommendations for the adoption of LSIDs
and HTTP URIs [28,29].
The URN scheme applied to LSIDs is a URI scheme
standardised by the Internet Assigned Numbers Author-
ity (IANA) [30]. HTTP is also a URI scheme, but there
is a fundamental difference between URNs and HTTP
URIs. HTTP URIs are based on the DNS, where the global
uniqueness of identifiers is guaranteed by the DNS infras-
tructure, which also facilitates addressing and retrieving
information about HTTP URIs. In contrast to URNs, sep-
arate web services are not necessary to manage identifier
creation or resolve them for data retrieval because these
functions are already available in the infrastructure of the
web. As a result, HTTP URIs are used as the identifier
mechanism for the Semantic Web and Linked Data [31].
In addition, the form of an HTTP URI is flexible because
Figure 1 The structure of an LSID. An LSID of a cerambycid beetle species obtained from the Catalogue of Life database.
Laurenne et al. Journal of Biomedical Semantics 2014, 5:40 Page 4 of 13
http://www.jbiomedsem.com/content/5/1/40
it does not have strictly defined parts like LSIDs. HTTP
URIs allow linking data across the web on the basis of the
meaning of concepts that are identified with HTTP URIs,
which enables the creation of the Web of Data.
LSIDs were the first attempt to solve the name problem,
but due to the rapid development of Semantic Web tech-
nologies, the trend now favours standardised web technol-
ogy. Themain differences between LSIDs and HTTPURIs
are presented in Table 1. The technology applied does not
solve the problem of the divergence between a name and
its meaning, but it does provide an appropriate solution
for publishing and interlinking data in an interoperable
way on the web.
Both LSID- and HTTP URI-based checklists can be
published for humans via a user interface and for
machines as APIs (Application Programming Interface) to
provide access to the data in multiple ways. For example,
the user interface can be used to check a valid name for a
taxon and browse a classification. The same information
can be obtained using a specialised API, but more general
query interfaces can also be provided. In Linked Data, an
API for reading the RDF description or a human-readable
HTML page for a resource is typically provided, as well
as a general purpose endpoint service that can be queried
using the SemanticWeb query language SPARQL. In addi-
tion, checklists can be made available as downloadable
files [31].
Semantic modelling of taxonomies
On the Semantic Web, taxonomies are represented using
RDF resources, i.e., entities with URI identifiers, and
explicit relations between them. A relatively new approach
is to express taxonomic information as an ontology. The
first ontology model for a taxonomic classification was
presented by Schulz et al. [34], with taxa organised into
a single hierarchy. Franz and Peet [35] and Franz and
Thau [36] have offered further insight into the issues of
taxonomic ontology modelling. So far, a few taxonomic
ontologies have been published in the NCBO BioPortal
Table 1 Themain differences between LSIDs and HTTP
URIs
Life science HTTP URIs
identifiers
Standardised by Object Management Internet Engineering
Group [32] Task Force [33]
Reuse existing Defines a new Uses an
URI schemes URN subscheme existing scheme
Data retrieval/ Specific resolving Uses existing
dereferenceability service needed web technology
(DNS, web servers)
Structure of identifier Strict Flexible
Linked Data compatibility No Yes
[37-41] and the ONKI ontology service [42]. The most
comprehensive of them is the NCBI Organismal clas-
sification [41], which contains more than 352,000 taxa
in a single hierarchy. Common to the classifications in
the NCBO BioPortal is that the hierarchy is constructed
using subclassOf (isA) relations and presented in theOBO
ontology language [43]. TaxonConcept.org [44] tackles
the name problem of taxonomic information in prac-
tice and shows how to publish the information as Linked
Open Data. It also demonstrates how data from external
sources are integrated and investigates how to combine
taxonomic concepts with specimen data. However, some
of the important information about names are described
as literals, e.g., the classification of taxa. Also, the taxo-
nomic change types are not described (split or lump of
taxa). The Taxonomic Meta-Ontology TaxMeOn [45,46]
aims to respond to the practical needs of managing bio-
logical names over time, and it links taxonomic infor-
mation to names. This meta-ontology differs in that it
offers a greater level of detail and supports differing
classifications.
An increasing number of ontologies are available and
therefore ontology evolution has become an important
issue. The world  and our conceptualisation of it  is
continually changing, which makes ontology versioning
essential [45,47,48]. Existing data that refer to a concept
should be kept consistent when its meaning changes or
when it is removed from an ontology. Data described
using different versions of an ontology then can be inte-
grated by utilising mappings (alignments) between the
ontology versions [49]. Khattak et al. [50] document ontol-
ogy evolution by keeping a log of changes in concepts.
Small changes in an ontology are grouped into sets, which
can later be used to revert to previous stages. An alter-
native solution is to recognise concept changes instead of
versioning an ontology. Wang et al. [51] show how the
changes in concepts and their impacts can be identified
automatically by comparing the concepts both extension-
ally and intensionally in cases where they do not have fixed
identifiers.
Methods
In order to develop two models for presenting taxonomic
information in a machine-processable way, four design
principles were applied to satisfy the following conditions:
1. use as few terms as possible to express as much
information as possible in the schema of the model.
The taxonomic terminology and its usage is
established in biology, and the terms are used in
consistent way. As few new terms as possible are
introduced.
2. focus on a restricted domain, that is, scientific
species checklists including all taxonomic
Laurenne et al. Journal of Biomedical Semantics 2014, 5:40 Page 5 of 13
http://www.jbiomedsem.com/content/5/1/40
information and excluding any other taxon-related
information (e.g., distribution).
3. support information on various levels of granularity,
as the source material is heterogeneous in its level of
detail.
4. accept all views of taxonomy equally legitimate
regardless of the time they were disseminated.
The focus of the models is in representing the taxo-
nomic relations between taxa in a single checklist (clas-
sification, synonymy), in different checklists (mapping
taxonomic concepts) and in individual versions of a check-
list (managing taxonomic changes).
The datasets utilised in the study consist of 20 published
species checklists that cover mainly northern European
mammals, birds and several groups of insects and assem-
ble ca. 78,000 taxon names (Additional file 1). Twomodels
are applied to the same datasets. Namemappings between
the checklists are provided for eight families of papilionoid
and hesperioid butterflies.
Results
Taxonomic database
The main elements of the Taxonomic Database (Figure 2)
[52] are a binominal scientific name and a taxonomic
concept that connects the names that refer to the same
taxon. Each concept is identified with a concept LSID. In
addition, three other attributes are assigned to the sci-
entific name: 1) a reference to the original publication
(author name and year of publication) in which the taxon
description was first published, 2) a status of a name indi-
cating its validity in the checklist, and 3) a taxonomic rank
Figure 2 A simplified structure of the relational taxonomic
database. The boxes illustrate the tables of the database, and the
lines present the relations between them. LSIDs are given to
taxonomic concepts and scientific names (illustrated with a darker
colour). Taxonomic concepts are linked to each other using the
relations described in Table 3 and each concept is linked to a
scientific name. External LSIDs and common names are connected to
the concepts. An author reference, validity, and a taxonomic rank are
assigned to the scientific names.
expressing level in a hierarchical classification (species,
genus, etc.). A taxonomic hierarchy between scientific
names is constructed using a hierarchical part-of relation.
An LSID that is obtained from an external source can
be assigned to a taxon concept as an attribute. Common
names in multiple languages can be connected to the con-
cept, but no taxonomic rank can be specified for them.
In order to recognise the orthographic variants of scien-
tific names, LSIDs are accommodated to the names as
well.
A new LSID is given to a concept if it changes, such as a
taxonomic change, an addition or removal of a synonym,
or a change in relations between taxa. An LSID is assigned
to a new taxon when added to a dynamic checklist. LSIDs
are versioned in the case of minor changes using the
optional part of the identifier. The decision whether to
create a new object identifier of an LSID or a new version
is made by a maintainer.
Taxa can be searched using a complete or partial sci-
entific name via a user interface, and the system returns
a currently valid name and its synonyms. If the taxon
is found in other checklists, their interrelations are also
described. The information is also provided as an RDF
representation for machine consumption. Only the latest
versions of dynamic checklists can be seen in the system.
However, older ones are stored internally in the database.
Taxonomic concepts are linked on the basis of their
equivalence at a species level, but at higher levels the
alignment of taxa is based on the species content. For
instance, two species that have the same name and the
same authorship citation are linked as congruent by
default, but two genera are linked as congruent only if the
species belonging to the genera are the same. The rea-
sons for treating species and taxa above the species level
differently are debated in the Discussion.
Taxonomic meta-ontology
TaxMeOn is an ontology schema for biological names, and
here we present the part that describes species checklists.
Themodel is based on RDFS (RDF Schema) and some fea-
tures of OWL (Web Ontology Language); it contains 12
classes with 49 subclasses (excluding 61 subclasses of the
class TaxonomicRank) and 28 properties. The core classes
and their relations are illustrated in Figure 3.
The class TaxonInChecklist represents both a scientific
name and its concept. The relation rdfs:label expresses the
unominal name of a taxon which is 1) the last epithet of a
name combination, or 2) a name of a taxon at higher lev-
els, e.g., a family. The taxonomic hierarchy is constructed
using the relation isPartOfHigherTaxon.
JOURNAL OF
BIOMEDICAL SEMANTICS
Wang et al. Journal of Biomedical Semantics 2014, 5:36
http://www.jbiomedsem.com/content/5/1/36RESEARCH Open AccessStandardizing adverse drug event reporting data
Liwei Wang1,2*, Guoqian Jiang2, Dingcheng Li2 and Hongfang Liu2Abstract
Background: The Adverse Event Reporting System (AERS) is an FDA database providing rich information on
voluntary reports of adverse drug events (ADEs). Normalizing data in the AERS would improve the mining capacity
of the AERS for drug safety signal detection and promote semantic interoperability between the AERS and other
data sources. In this study, we normalize the AERS and build a publicly available normalized ADE data source. The
drug information in the AERS is normalized to RxNorm, a standard terminology source for medication, using a
natural language processing medication extraction tool, MedEx. Drug class information is then obtained from the
National Drug File-Reference Terminology (NDF-RT) using a greedy algorithm. Adverse events are aggregated
through mapping with the Preferred Term (PT) and System Organ Class (SOC) codes of Medical Dictionary for
Regulatory Activities (MedDRA). The performance of MedEx-based annotation was evaluated and case studies were
performed to demonstrate the usefulness of our approaches.
Results: Our study yields an aggregated knowledge-enhanced AERS data mining set (AERS-DM). In total, the
AERS-DM contains 37,029,228 Drug-ADE records. Seventy-one percent (10,221/14,490) of normalized drug concepts
in the AERS were classified to 9 classes in NDF-RT. The number of unique pairs is 4,639,613 between RxNorm
concepts and MedDRA Preferred Term (PT) codes and 205,725 between RxNorm concepts and SOC codes after
ADE aggregation.
Conclusions: We have built an open-source Drug-ADE knowledge resource with data being normalized and
aggregated using standard biomedical ontologies. The data resource has the potential to assist the mining of ADE
from AERS for the data mining research community.Introduction
Since the early 1990s, adverse drug events (ADEs) have
received considerable attention from researchers in qual-
ity and patient safety [1]. Although randomized clinical
trials (RCTs) are considered as a gold standard for deter-
mining the safety issues of drugs, it is generally recog-
nized that premarketing RCTs may not detect all safety
issues related to a particular drug in clinical practice [2].
The US Food and Drug Administration (FDA) Adverse
Event Reporting System (AERS) is one of the main re-
sources in post-marketed ADE detection based on data
mining techniques [3,4]. The main data mining metrics
used for ADE detection include the proportional report-
ing ratio (PRR), the reporting odds ratio (ROR), the in-
formation component (IC), and the empirical Bayes
geometric mean (EBGM) [5]. For example, Kadoyama* Correspondence: wlw@jlu.edu.cn
1Department of Medical Informatics, School of Public Health, Jilin University,
Jilin, China
2Department of Health Sciences Research, Mayo Clinic, Rochester, MN, USA
© 2014 Wang et al.; licensee BioMed Central L
Commons Attribution License (http://creativec
reproduction in any medium, provided the oret al. [6] used the above metrics and detected signals for
paclitaxel-associated mild, severe, and lethal hypersensi-
tivity reactions and docetaxel-associated lethal reactions.
Poluzzi et al. [7] detected drug-induced torsades de
pointes (TdP) signals of linezolid, caspofungin, posaco-
nazole, indinavir, and nelfinavir using ROR. However,
most of existing studies on the AERS were carried out
for a small number of drugs [6,8-10], and few studies
were focused on large-scale mining or on detecting the
etiology of ADE signals in terms of mechanism of action,
physiologic effect, or molecular structure of drugs [11].
We realize that potential of the AERS has not been fully
utilized, and one of main reasons for this is because
there is a lack of standardization among drug names.
In the AERS, drugs can be registered by arbitrary names,
including trade names, abbreviations, and even typograph-
ical errors since they are directly entered by health care
professionals (e.g., physicians, pharmacists, nurses, etc.) and
consumers (e.g., patients, family members, lawyers, etc.)
[5]. There is limited normalization effort for the AERS data.td. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly credited.
Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 2 of 13
http://www.jbiomedsem.com/content/5/1/36For example, for drug names AERS uses valid trade
names, if available, based on sources such as the Orange
Book [12] and other internal databases [13]. Otherwise,
the verbatim names are used, thus forming substantial
barriers for data integration for the purpose of ADE signal
detection. There have been some attempts in drug name
normalization when mining AERS, but typically it is either
unclear how the normalization was conducted or the
normalization was attempted only for a small number of
drugs [3,6,9-11,14-16].
In terms of ADE names, AERS does provide normal-
ized terms based on Medical Dictionary for Regulatory
Activities (MedDRA) preferred terms (PTs), though the
use of MedDRA requires a license. In this study, we have
demonstrated that MedDRA PT-based normalization ac-
tually enables the powerful data aggregation capability
when we link the PT terms to their corresponding
System Organ Class (SOC) categories.
In this study, we aim to produce an open-source AERS
data mining set (AERS-DM), which is normalized and
aggregated with two standard drug ontologies, including
RxNorm and the National Drug File-Reference Termin-
ology (NDF-RT), and one ADE terminology, MedDRA.
Methods
Resources
The FDA AERS database is a public database that in-
cludes 7 tables. Its structure is in compliance with the
international safety reporting guidance (ICH E2B) [17].
Information related to a single AERS report can be re-
trieved from those tables using a unique identifier (i.e.,
Individual Safety Report (ISR) number). Among them,
the DRUG table includes drug-related information such
as DRUG_SEQ (a unique number for identifying a
drug in a report), DRUGNAME, ROUTE (the route
of drug administration), and DOSE_VBM (verbatim
text for dose, frequency, and route, exactly as entered on
the report). The REAC table includes adverse event in-
formation using PTs in MedDRA, a medical terminology
adopted to describe adverse drug event [18]. The DEMO
table includes patient demographics and administrative
information of the events, including CASE (case num-
ber for identifying an AERS case) and FDA_DT (date
FDA received report).
RxNorm, released initially in 2004, is a standardized no-
menclature for clinical drugs and drug delivery devices
[19]. Since its creation, RxNorm has been increasingly rec-
ognized by the biomedical informatics community as an
emerging standard for clinical information exchange [20].
RxNorm is organized by concepts, in which each concept
consists of drug names sharing the same meaning at a spe-
cific level of abstraction and is assigned a concept unique
identifier (RxCUI). RxNorm also provides relationships
between concepts, as indicated in Figure 1, adapted fromPeters and Bodenreider [21]. For example, 'Diphenhydra-
mine Hydrochloride is the precise_ingredient_of Tylenol
PM. The description of all such relationships can be re-
trieved, see the Availability and requirements section for
the webpage. Data in RxNorm is distributed in Rich Re-
lease Format (RRF) tables, which is the default relational
format used by the National Library of Medicine (NLM).
NDF-RT was developed by the Veterans Health Admin-
istration, providing clinical information about medications,
and has been included in RxNorm. NDF-RT uses a de-
scription logic-based, formal reference model that groups
drugs and ingredients into the high-level classes for Chem-
ical Structure (e.g., Acetanilides), Mechanism of Action
(e.g., Prostaglandin Receptor Antagonists), Physiological
Effect (e.g., Decreased Prostaglandin Production), drug-
disease relationship describing the Therapeutic Intent (e.g.,
Pain), Pharmacokinetics describing the mechanisms of ab-
sorption and distribution of an administered drug within a
body (e.g., Hepatic Metabolism), and legacy VA-NDF clas-
ses for Pharmaceutical Preparations (VA Drug Class; e.g.,
Non-Opioid Analgesic) [22]. Figure 2 shows the NDF-RT
content model [23] together with an example showing drug
class information for BUTABARBTIAL NA 100MG CAP.
We utilized RxNorm and NDF-RT for normalizing
and aggregating drug information in AERS in consider-
ation of three reasons. First, these two ontologies are
publicly available medication ontologies that have been
intensively developed and used for drug data integration
[22,24,25]. Second, RxNorm aims to enable various sys-
tems using different standardized drug nomenclatures to
share and exchange data efficiently, which we believe
meets the requirements for meaningful use of the ADE
reporting data. In addition, since RxNorm only repre-
sents a nomenclature of drugs and does not contain
drug categorical information, we leveraged the categor-
ical information extracted from NDF-RT for medication
data aggregation. Third, as a part of the Unified Medical
Language System (UMLS), RxNorm and NDF-RT can
function as interoperable drug standards that can inte-
grate with other health data, such as electronic health
records (EHRs), so as to facilitate the semantic integra-
tion of the data in the health domain.
Finally MedDRA is a controlled terminology devel-
oped for reporting adverse events, related to drugs, to
regulatory agencies [26]. MedDRA has a hierarchical
structure with five levels: SOC, High-Level Group Term
(HGLT), High-Level Term (HLT), PT and Lowest-Level
Term (LLT). There are 26 classes (SOCs). PTs are the
main descriptors in MedDRA and are used in AERS. All
MedDRA terms are integrated UMLS.
Tool
In our study, we used a natural language processing
(NLP) tool, MedEx, to normalize AERS drugs. MedEx
Figure 1 Relations among RxNorm concepts, adapted from Peters and Bodenreider [21].
Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 3 of 13
http://www.jbiomedsem.com/content/5/1/36extracts medication information from clinical notes [27].
Besides MedEx, there are a number of other existing
NLP-based tools available that could be used for drug
normalization, including MedLEE [28], National Center
for Biomedical Ontology (NCBO) Annotator Web Service
[29], and Mayo cTAKES [30]. We chose to use the MedEx
because we consider it an optimized system for drug
normalization with a relative good performance, ranked
second in the 2009 i2b2 Medication Extraction challenge,
where the first-place system in that challenge is not
available for public use [31]. The evaluation showed that
MedEx performed well on identifying drug names, with
precision (97%), recall (88%) and F-measure (92%) for 50
discharge summaries and precision (95%), recall (92%)
and F-measure (93%) for 25 clinic notes, respectively [27].In this study, we used MedEx version 2.0. Input files for
MedEx included concrete information on drugs, and out-
put normalized data included RxNorm codes.
Data processing
Figure 3 presents an overview of the data processing flow
that contains three steps: de-duplication, drug normalization,
and data aggregation. In the de-duplication step, redundant
reports are removed. In the normalization step, MedEx is
applied to normalize AERS drugs to RxNorm codes. During
aggregation, adverse events are aggregated according to
MedDRA SOC and PT codes, and NDF-RTbased classi-
fication information for those drugs is obtained from
RxNorm. Two tables are formed; one stores the normalized
Drug-ADE information and the other stores the aggregated
Figure 2 NDF-RT content model and the example. Triangles denote hierarchies of related concepts, categorized in the rectangles within the
triangles. Taxonomic or ISA relationships (upward-pointing blue solid arrows) unify NDF-RT clinical drug concepts into a polyhierarchy, classified
both by their VA drug class and their generic ingredient(s). Various named role relationships (sideways-pointing amber dash arrows) define the
central drug concepts (green) from which they originate in terms of the reference hierarchy concepts (blue) pointed to. Role relationships are also
inherited into subsumed clinical drug concepts. Adapted from NDF-RT document [April 2012 Version] [23].
Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 4 of 13
http://www.jbiomedsem.com/content/5/1/36information of Drug-ADE. The data in the two tables can
be connected through the RxNorm codes. Those steps are
further detailed below.
Removing redundant AERS data
According to the FDAs recommended method for de-
duplication, for reports with the same CASE number,
we select the latest (most recent) report date (i.e.,
FDA_DT) in the AERS DEMO table. For reports with
the same CASE and FDA_DT values, we select the one
with the highest ISR number. Table 1 shows examples
of how to delete duplicate reports. We select the ISR
4275741 for CASE number 4047837, and ISR
7637797 for CASE number 8468457. Consequently,Figure 3 An overview of the data processing flow.the selected ISRs in the DEMO table are kept in DRUG
and REAC tables.
Normalizing AERS drug and ADE data
After de-duplication, we concatenate the following fields
in the DRUG table: DRUGNAME, ROUTE, and
DOSE_VBM, and the resulting strings are normalized
with MedEx. Compared to DRUGNAME alone, the
concatenated string gives more comprehensive informa-
tion about the corresponding drug, since the ROUTE
field can provide information such as Oral and the
DOSE_VBM field can provide information such as
Tablet. The results are mapped to the RxNorm code
RxCUI. For example, the concatenated string POTASSIUM
Table 1 Examples of duplicate reports
ISR CASE FDA_DT De-duplication
4269368 4047837 20040113 ×
4275741 4047837 20040121 ?
7637789 8468457 20110720 ×
7637797 8468457 20110720 ?
ISR is the unique number for identifying an AERS report. CASE is the
number for identifying an AERS case. "FDA_DT is the date FDA received
report. De-duplication indicates the positive or negative action.
Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 5 of 13
http://www.jbiomedsem.com/content/5/1/36CHLORIDE EXTENDED RELEASE TABLET EXTENDED
RELEASE TABLET ORAL 20 MEQ BID ORAL is nor-
malized to RxCUI 198116 (i.e., Potassium Chloride 20
MEQ Extended Release Tablet). Meanwhile, we map
PT entries of the REAC table to PT and SOC codes of
MedDRA.
Aggregating normalized drug and ADE data
The algorithm for classifying AERS drugs based on NDF-
RT includes two parts; the first part is to identify the cor-
responding NDF-RT concepts for those normalized AERSFigure 4 Traversal pathways for classifying AERS drugs based on NDFdrugs and the second is to extract the associated NDF-RT
classification information.
Specifically, RxNorm contains NDF-RT ingredients and
clinical drugs. Meanwhile, NDF-RT ingredients are con-
nected to their mechanisms of action, physiologic effects,
and therapeutics (indications and contraindications) and
the corresponding clinical drugs inherit those relations.
NDF-RT clinical drugs are also connected to their corre-
sponding VA drug classes. Therefore, if a given RxCUI
itself is an NDF-RT concept (i.e., one of its sources is
NDF-RT), we use it to find NDF-RT classification infor-
mation. Otherwise, we traverse the relations provided by
RxNorm to greedily identify the related NDF-RT ingredi-
ents and clinical drugs, and then extract the associated
classification information (see Figure 4).
We obtain PT and SOC codes in MedDRA for PT en-
tries in the AERS. For example, PT entries Anaemia of
chronic disease, Anaemia of malignant disease, and
Nephrogenic anaemia are mapped to corresponding PT
codes 10002073, 10049105, and 10058116. And these
codes are aggregated under the SOC code 10005329
(i.e., blood and lymphatic system disorders).-RT.
Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 6 of 13
http://www.jbiomedsem.com/content/5/1/36Experiments
For the experiments, we first gathered the AERS data that
are publicly available from 2004 to 2011 [32]. We used the
May 2012 release of RxNorm (which contains the map-
pings to NDF-RT) and the 14.1 version of MedDRA. We
ultimately produced an AERS-DM that is composed of
two tables, one containing the normalized Drug-ADE in-
formation and the other containing the aggregation infor-
mation of the Drug-ADEs. We then analyzed the statistics
of the normalization and aggregation for AERS data in the
AERS-DM. We also performed case studies to demon-
strate the usefulness of the AERS-DM.
We evaluated the normalization performance of MedEx
in two steps. For the first step, we randomly selected 200
unique input drug names before MedEx-based annota-
tions. We recruited three reviewers with medical back-
ground who manually reviewed the 200 drug names and
annotated them using the RxNorm codes. The version of
the RxNorm used in the evaluation was the same as that
included in MedEx. A gold standard was generated after
the reviewers achieved inter-agreements. Precision (P), Re-
call (R), and F-measure (F) were calculated for these se-
lected drug names, using P = TP/(TP + FP), R = TP/(TP +
FN), and F = 2PR/(P + R), in which TP stands for True
Positive, FP stands for False Positive, and FN stands for
False Negative. For the second step, we randomly selected
100 drug names that failed to be normalized. Then we
confirmed whether or not the drug names are included in
RxNorm. In addition, we checked to see if an unmatched
drug name is a valid domestic or foreign drug by using
two drug resources. The first resource is Drugs@FDA, a
drug dictionary providing FDA-approved brand and gen-
eric drug information [13], and the second resource is
Drugs.com, the largest independent drug information
website available on the Internet [33].
Additionally, we evaluated the validity of the algorithm
used for identifying the mappings between RxNorm codes0%
20%
40%
60%
80%
100%
120%
0 5
Percentage of  
unique AERS 
drug names  
covered by 
RxNorm 
Frequency (l
Figure 5 Distribution of unique drug names in AERS.and NDF-RT concepts. We randomly selected 20 AERS
drug names and manually checked the accuracy of the
mappings produced by our algorithms.
Results
General statistics of normalization and aggregation
After de-duplicating reports, according to the recom-
mended method in the download files provided by FDA
[34], the number of AERS records is reduced to 2,643,979
from the original 3,874,965. The number of unique verba-
tim drug names is reduced to 1,517,811 from the original
1,700,925.
For drug name normalization, 1,125,045 of 1,517,811
(74%) AERS unique drug names were normalized to
14,489 unique RxNorm concepts, of which 10,221 (71%)
were classified in NDF-RT.
For the ADE normalization, we mapped 14,740 existing
MedDRA PT terms in the AERS to MedDRA codes, ac-
counting for 76% of 19,294 total MedDRA PT terms.
These MedDRA PT codes were then mapped to 26 Med-
DRA SOCs.
Figure 5 shows the distribution of unique drug names in
the AERS (normalized with the single field DRUGNAME),
which follows Zifps Law. In other words, the more popu-
lar the drug names are, the higher the chance those drug
names are to be normalized by the RxNorm codes.
We classified the RxNorm concepts using the NDF-RT.
Table 2 shows the coverage of AERS drugs by NDF-RT
classes. For example, 5,823 RxNorm concepts were mapped
to 29 corresponding VA classes, accounting for 57% of all
classified RxNorm concepts and corresponding to 77% of
total AERS reports. Table 3 shows the results of MedDRA
PTcode aggregation by MedDRA SOCs.
Statistics of AERS-DM
The AERS-DM includes two tables, as discussed above.
There are 37,029,228 Drug-ADE records after de-10 15 20
og2) of unique drug name in AERS
Table 2 Coverage of AERS drugs by NDF-RT classes
Drug_class_type1 Drug_class, No.2 RxNormConcept, No. in the AERS3 a%4 b%5
Generic Ingredient Combinations 26 9,813 96 88
Chemical Ingredients Class 16 9,331 91 81
Therapeutic Intent 24 8,069 79 78
Mechanism of Action 7 8,061 79 81
Physiologic Effect 16 7,989 78 82
VA Class 29 5,823 57 77
FDA Established Pharmacologic Class (EPC) 66 3,049 30 46
Therapeutic Category 7 2,880 28 50
Pharmacokinetics 1 730 7 28
Total 192 10,221 100 100
1Drug_class_type represents various classes provided by NDF-RT as shown in Figure 2.
2Drug_Class, No. is the number of direct subordinate classes of each Drug_class_type.
3RxNorm concept, No. in the AERS shows the total number of RxNorm concepts in the AERS for each drug class type.
4a% indicates the percentage of classified RxNorm concepts for each Drug_class_type, calculated as the division of RxNorm concept, No. in the AERS by
10,221; a total of RxNorm concepts mapped to at least one NDF-RT class.
5b% indicates the percentage of classified AERS reports, calculated as the division of the number of AERS reports assigned with the Drug_class_type by the total
AERS report number (2,643,979).
Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 7 of 13
http://www.jbiomedsem.com/content/5/1/36duplication. The number of unique pairs between RxNorm
concepts and MedDRA codes is 4,639,613, and between
RxNorm concepts and SOC pairs after ADE aggregation,
205,725. Tables 4 and 5 show the top 10 most frequent
pairs (Therapeutic Category, PT and Therapeutic Category,
SOC), respectively.Evaluation results
The initial inter-annotator agreement on the 200 anno-
tated drug names was 62.8%, a low agreement rate, maybe
due to the various understandings in the rules for drug
name normalization from the annotation training. It in-
deed indicated the difficulty of the drug normalization in
AERS. Comparing MedEx-based annotations with the
gold standard generated from majority votes by three hu-
man reviewers, we calculated the performance measures.
TP, FP and FN were calculated as 138, 7 and 6 respect-
ively. Recall, Precision and F-measure were then calculated
as 95.8%, 95.2% and 95.5% respectively, which is compar-
able with performance measures in the original evaluation
of MedEx [27]. The slight difference may be caused by dif-
ferent evaluation contexts, and different drug name num-
bers in gold standards, with discharge summaries (377),
clinic notes (200) and AERS (200). The false positive was
low in the 200 annotated drug names, which was mainly
due to the false recognition of partial drug names, for ex-
ample, both drug names ADONA (CARBAZOCROME
SODIUM SULFONATE) and CLEXANE (HEPARNI-
FRACTION SODIUM SALT) were normalized to so-
dium (RxCUI 9853). The first, a foreign drug from Italy
and Japan, is out of the scope of RxNorm, the second is
covered by RxNorm but not identified. As a result both
are falsely mapped by MedEx.Table 6 shows the evaluation results of 100 drug
names failed to be normalized by MedEx. A large por-
tion of them (75) are due to problems associated with
AERS records, including names not covered by RxNorm
such as foreign drug names, typographical errors, un-
specified names (e.g., BLINDED PLACEBO), herbs (e.g.,
ALOEELITE), domestic drugs (e.g., PANHEPRIN), new
drugs (e.g., HIZENTRA, approved by the FDA in 2010),
and non-drugs (e.g., RADIATION THERAPY). A small
portion of them (25) that failed to be mapped are due to
MedEx. The evaluation results revealed several issues
related to drug name normalization. First, the public re-
lease of MedEx (version 2.0) is in an executable format,
which prevents the use of the latest version of RxNorm
for drug name normalization. For example, the drug
name SAPHRIS did not have a match using MedEx
because it is included in RxNorm in the 2012 version
but not in the 2008 version (used in MedEx). Second,
RxNorm does not contain foreign brand names since it is
intended to cover drugs prescribed in the United States.
Third, we found that some of the records in AERS contain
unspecified names. An example is the name BLINDED
PLACEBO, which is used as a drug name and makes the
normalization infeasible. We would suggest that incorpor-
ating thorough normalization at the point of data entry is
desired, so as to improve the data quality for data mining.
Manual evaluation shows the greedy algorithm used to
find mapping between RxNorm and NDF-RT is 100% valid.
Case studies for AERS-DM
As described in the section above, the study produced an
AERS-DM containing normalized and aggregated AERS
reporting data. To demonstrate the usefulness of the
AERS-DM, we used the AERS-DM to analyze the NDF-
Table 3 Aggregation of MedDRA PT codes by MedDRA SOCs
MedDRA SOC Code MedDRA SOC Terms PT Codes, No.1 AERS Reports, No. (%)2
10018065 General disorders and administration site conditions 595 875,070 (15.6)
10029205 Nervous system disorders 729 570,448 (10.2)
10017947 Gastrointestinal disorders 699 447,243 (8.0)
10022891 Investigations 2,748 407,270 (7.3)
10037175 Psychiatric disorders 453 329,198 (5.9)
10022117 Injury, poisoning, and procedural complications 697 311,952 (5.5)
10038738 Respiratory, thoracic, and mediastinal disorders 437 287,425 (5.1)
10028395 Musculoskeletal and connective tissue disorders 365 276,784 (4.9)
10040785 Skin and subcutaneous tissue disorders 390 273,674 (4.9)
10021881 Infections and infestations 1,361 267,741 (4.8)
Infections and infestations
10007541 Cardiac disorders 283 237,037 (4.2)
10047065 Vascular disorders 595 195,591 (3.5)
10027433 Metabolism and nutrition disorders 236 160,134 (2.9)
10038359 Renal and urinary disorders 288 132,055 (2.4)
10029104 Neoplasms benign, malignant and unspecified (incl. cysts and polyps) 1,293 129,227 (2.3)
10005329 Blood and lymphatic system disorders 218 122,276 (2.2)
10015919 Eye disorders 462 114,312 (2.0)
10042613 Surgical and medical procedures 1,176 89,005 (1.6)
10019805 Hepatobiliary disorders 162 79,067 (1.4)
10038604 Reproductive system and breast disorders 381 73,431 (1.3)
10021428 Immune system disorders 111 64,127 (1.1)
10041244 Social circumstances 189 48,077 (0.9)
10036585 Pregnancy, puerperium and perinatal conditions 182 36,959 (0.7)
10013993 Ear and labyrinth disorders 75 30,174 (0.5)
10010331 Congenital, familial, and genetic disorders 833 21,244 (0.4)
10014698 Endocrine disorders 129 18,053 (0.3)
1PT Codes, No. shows the number of PT codes for each SOC.
2AERS Reports, No. (%) shows the number of AERS reports and the percentage among the overall AERS report number for each SOC, where one report may
contain several SOCs associated with several drugs.
Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 8 of 13
http://www.jbiomedsem.com/content/5/1/36RT drug class Pharmacokinetics and their correspond-
ing ADE categories represented by the MedDRA SOCs.
Specifically, we joined the two tables in the AERS-DM
and retrieved the AERS reports under seven existing
pharmacokinetics classes. We then analyzed the data,
including their corresponding MedDRA SOC-based
ADE categories.
Figure 6 shows a profile of seven pharmacokinetics
classes in the AERS-DM. The bars represent the total
number of AERS reports for each individual pharmaco-
kinetics class and the line represents the total drug num-
bers under each pharmacokinetics class. This figure
illustrates that most drugs with pharmacokinetics class
information reported in the AERS are relevant to Renal
Excretion and Hepatic Metabolism. We also found
that the number of AERS reports is disproportional tothe number of drugs for some pharmacokinetics classes.
For example, the class Hepatic excretion contains
fewer drugs than the class Fecal excretion but has
more AERS reports. The result indicates that the drugs
in the Hepatic excretion class may be associated with
more AERS reports than the Fecal excretion class, and
interesting etiology knowledge may be found through
further mining with disproportionality metrics and other
data sources, including EHRs.
Figure 7 shows a profile of 26 MedDRA SOC-based
ADE categories that corresponds to the seven pharma-
cokinetics classes in the AERS. The bars represent the
total number of AERS reports for each individual SOC
and the line represents the total number of drugs associ-
ated with each SOC category. The figure illustrates that
the top five most frequent ADE categories are General
Table 4 Therapeutic category and PT pairs
Co-occurrence, No. Therapeutic Category, Codes (Terms) PT, Codes (Terms)
31,858 2225 (Central Nervous System Agent) 10028813 (Nausea)
30,824 2225 (Central Nervous System Agent) 10013709 (Drug ineffective)
24,922 882 (Antirheumatic Agent) 10022086 (Injection site pain)
23,633 882 (Antirheumatic Agent) 10028596(Myocardial infarction)
20,240 2095 (Cardiovascular Agent) 10028813 (Nausea)
18,435 2095 (Cardiovascular Agent) 10013968 (Dyspnoea)
15,695 4703 (Gastrointestinal Agent) 10028813 (Nausea)
11,709 4703 (Gastrointestinal Agent) 10012735 (Diarrhoea)
10,509 884 (Anti-infective Agent) 10028813(Nausea)
10,505 884 (Anti-infective Agent) 10037660 (Pyrexia)
10,022 988 (Antineoplastic Agent) 10012735 (Diarrhoea)
8,865 988 (Antineoplastic Agent) 10011906 (Death)
100 106571 (Diagnostic Agent) 10022061(Injection site erythema )
69 106571 (Diagnostic Agent) 10028813(Nausea)
Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 9 of 13
http://www.jbiomedsem.com/content/5/1/36disorders and administration site conditions, Nervous
system disorders, Gastrointestinal disorders, Investi-
gations, and Respiratory, thoracic, and mediastinal dis-
orders. Similarly, we found that the number of AERS
reports is disproportional to the number of drugs for
some MedDRA SOC categories. The result may reveal
some important areas of ADE surveillance for post-
marketing drugs if combined with prescription informa-
tion as the denominator.
Discussion
In addition to the significance described in the Introduction
section above, the present study was also partially moti-
vated by our previous work on building a standardizedTable 5 Therapeutic category and SOC pairs
Co-occurrence, No. Therapeutic Category, Codes (Terms)
133,936 2095 (Cardiovascular Agent)
90,959 2095 (Cardiovascular Agent)
194,311 2225 (Central Nervous System Agent)
175,172 2225 (Central Nervous System Agent)
129,953 882 (Antirheumatic Agent)
61,386 882 (Antirheumatic Agent)
76,007 4703 (Gastrointestinal Agent)
69,825 4703 (Gastrointestinal Agent)
65,105 884 (Anti-infective agent)
41,900 884 (Anti-infective agent)
53,478 988 (Antineoplastic Agent)
35,625 988 (Antineoplastic Agent)
403 106571 (Diagnostic Agent)
251 106571 (Diagnostic Agent)knowledge base of ADEs known as ADEpedia, in which
we intended to integrate and normalize known ADE
knowledge from disparate ADE datasets (e.g., the FDA
structured product labels, and the UMLS) [35,36]. We
consider that the FDA AERS reporting database is
another important data source for ADE knowledge
discovery and the normalization of both the drug and
ADE names is the first important task we need to tackle
with.
Drug and ADE aggregation
NDF-RT is a major national source of drug classification
information, providing multi-axial classifications such as
physiological effect, mechanism of action, etc. It has provedSOC, Codes (Terms)
10018065 (General disorders and administration site conditions)
10029205 (Nervous system disorders)
10018065 (General disorders and administration site conditions)
10029205 (Nervous system disorders)
10018065 (General disorders and administration site conditions)
10029205 (Nervous system disorders)
10018065 (General disorders and administration site conditions)
10017947 (Gastrointestinal disorders)
10018065 (General disorders and administration site conditions)
10029205 (Nervous system disorders)
10018065 (General disorders and administration site conditions)
10017947 (Gastrointestinal disorders)
10018065 (General disorders and administration site conditions)
10040785 (Skin and subcutaneous tissue disorders))
Table 6 Reasons for non-normalization of drug names by
MedEx
Reasons No.
MedEx reasons 25
AERS entry reasons 75
Foreign brand names 41
Typo 19
Unspecified names 8
Herbs 4
Uncovered domestic drug 1
New drug 1
Non-drug 1
Total 100
Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 10 of 13
http://www.jbiomedsem.com/content/5/1/36to be capable of representing medications in clinical set-
tings. For example, Rosenbloom et al. [37] investigated the
coverage of the Physiologic Effects hierarchy in NDF-RT
and found this category to be sufficient for classifying med-
ications. Zhu et al. [38] used the FDA Established Pharma-
cologic Class in NDF-RT to profile Structured Product
Labelling for clinical applications.
On the other hand, the applicability of NDF-RT and
RxNorm for clinical drug classification was explored, and
the imperfect mappings between RxNorm and NDF-RT
and incomplete drug classification were evidenced by sev-
eral studies [22,24,25]. Palchuk et al. [25] used the NDF-
RTs drug class tree to organize RxNorm into a hierarchy
and evaluated this mapping using data from EHRs. Pathak
et al. [22] investigated the applicability of RxNorm and
NDF-RT for representation and classification of medica-
tion data from EHRs using the NLMs NDF-RT web ser-
vices API for NDF-RT drug class assignment. Both of the
above studies were limited to the Drug Products by VA
Class hierarchy under Pharmaceutical Preparations,0
100000
200000
300000
400000
500000
600000
AERS 
report 
number 
(bar)
Figure 6 A profile of pharmacokinetics classes in AERS.with no consideration of the multi-axial hierarchies. In
addition, issues in mapping and classifying drugs from
RxNorm using the NDF-RTs multi-axial classification
were investigated by Pathak and Chute [24]. In the study,
they identified the issues in NDF-RT, including the lack
of coverage of drug classes (chemical structure, mech-
anism of action, physiologic effect, therapeutic intent,
and pharmacokinetics) for clinical drugs, and suggested
that the resolution would rely on the targeted improve-
ment of NDF-RT. Thus, the existing studies on the clas-
sification of RxNorm using NDF-RT are either about
multi-axial classification, based on the mapping be-
tween the two ontologies, or limited to VA class based
on EHR data.
In the context of standardizing the AERS data in this
study, we developed a systematic algorithm in which the
rich semantic connections within RxNorm were fully
utilized to build the mappings with the related concepts
in NDF-RT. The mappings were then used to aggregate
the AERS data under multi-axial classifications in NDF-
RT. Different from those related studies, the present
study focuses on real-world data in the AERS and pre-
sents a normalized AERS-DM data set together with all
corresponding drug classification information provided
by NDF-RT for large-scale data mining purposes. The
evaluation results show that the greedy algorithm for
maximum mapping to corresponding NDF-RT concepts
from RxNorm codes was valid. We believe that the map-
ping method developed in our study could be useful in
other similar context.
We found that the MedDRA PT terms coding ADEs
in the AERS changed over time. The used AERS data
ranged from 2004 to 2011, and during that time period
the MedDRA versions had been updated twice annually
[39]. Some ADE codes in the AERS from the older Med-
DRA edition had become obsolete and may hinder the0
100
200
300
400
500
600
Drug 
number
(line)
Figure 7 A profile of MedDRA SOC for pharmacokinetics classes in AERS.
Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 11 of 13
http://www.jbiomedsem.com/content/5/1/36identification of Drug-ADE pairs. In the future, we will
consider whether making the PT terms consistent over
the years before conducting ADE detection (i.e., building
a version control mechanism) may be useful for improv-
ing data quality.
In addition, other studies have identified a number of
issues related to the use of MedDRA. For example, the
hierarchy problems and semantic reasoning incapability
of MedDRA mitigate its usefulness for querying and
analyzing AERS data. SNOMED-CT, as the largest clin-
ical terminology, can complement these disadvantages,
with as many levels of hierarchy as are considered ap-
propriate, and the semantic consistency in relationships
[40]. In addition, using SNOMED-CT for ADE coding
can also achieve the integration of ADE data in the
AERS with other health data sources, including EHRs. A
few studies have demonstrated the mappings between
MedDRA and SNOMED-CT. For example, Bodenreider
[41] proposed the mappings by leveraging the structure
of SNOMED-CT for aggregation purposes. Mougin et al.
[42] proposed to improve the mapping through an auto-
matic lexical-based approach. A recent study [43] com-
pared three methods using the Ontology of Adverse
Events (OAE), MedDRA, and SNOMED-CT in classify-
ing the ADE terms associated with two vaccines. Among
the three methods,, the OAE method provided better
classification results. This initiative is inspiring in the
field of ADE detection for vaccines. However, given that
it is a newly emerging ontology with only 2723 terms,
the coverage of the OAE is very limited. For the AERSdata normalization and aggregation, we consider that the
widely used SNOMED-CT would be a better candidate as
an ADE terminology, and this will be one of our future
works.
Case studies
We demonstrated the usefulness of the AERS-DM pro-
duced by this study by analyzing the data set using the
pharmacokinetics class. There are other class dimen-
sions available for analysis, including physiologic effect,
mechanism of action, and VA class, all of which come
from the knowledge structure asserted in the NDF-RT.
We believe that the knowledge asserted in the standard
ontologies will enrich the AERS-DM and enable the
meaningful use of AERS data for ADE signal detection
and data mining.
Implication of study
Based on AERS-DM, more efficient data mining and ADE
detection in individual drugs would be achieved and facili-
tated. The reasons are three-fold as follows. First, having
enriched features of drugs and enlarged cohort informa-
tion, AERS-DM could provide potential explanations for
individual differences in ADEs. Second, with the capability
of large-scale ADE mining, comparative analysis of differ-
ent drug classes could be explored, thus accumulating
ADE evidence in the field of individual drugs. Third, pre-
senting more meaningful organization of drug and ADE
data with standard terminologies, AERS-DM could be a
platform for deeper mining by further connecting with
Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 12 of 13
http://www.jbiomedsem.com/content/5/1/36clinical notes, scientific literature, gene expression, pro-
teomics and pharmacogenomics data, and various other
ontologies. We believe that AERS-DM could be used to
explore the complex network among drugs and ADEs,
and such research would bear far-reaching significance in
terms of the study paradigm of ADEs.
Limitations
We used only two drug ontologies (RxNorm and NDF-RT)
and one ADE terminology (MedDRA) to normalize and ag-
gregate AERS data. We believe additional investigations of
other standard terminologies, such as SNOMED-CT would
be beneficial in exploring the potential of standardized
AERS reporting data in data mining.
Conclusion
In this study, we leveraged three biomedical ontologies?Rx-
Norm, NDF-RT, and MedDRA?for normalizing and aggre-
gating the AERS data and produced a standardized ADE
dataset referred to as AERS-DM. With the normalized
codes and aggregated features, the AERS-DM would be use-
ful for the research community in the data mining field. We
will continue to refine and optimize the AERS-DM and up-
date it periodically in the future. In addition, we will investi-
gate the integration of the AERS-DM data set with other
health data sources, such as EHR data, literature databases
(e.g., Semantic Medline [44]) and other ontologies (e.g.,
Drug Ontology [45]), for the purpose of promoting ADE
detection in individual drugs. Finally, we will leverage
SNOMED-CT for standardizing the AERS data.
Availability and requirements
Dataset name: AERS-DM
Dataset home page: http://informatics.mayo.edu/adepedia/
index.php/Download
Operating system(s): Platform independent
Other requirements: None
License: GPL
Any restrictions to use by non-academics: None
RxNorm Files: http://www.nlm.nih.gov/research/umls/
rxnorm/docs/rxnormfiles.html
RxNorm concept relationships: http://www.nlm.nih.
gov/research/umls/rxnorm/docs/2013/appendix1.html
Abbreviations
AERS: The Adverse Event Reporting System; NDF-RT: National Drug File-Reference
Terminology; MedDRA: Medical Dictionary for Regulatory Activities; PT: Preferred
Term; SOC: System organ class; HGLT: High-Level Group Term; HLT: High-level
term; LLT: Lowest-level term; ADEs: Adverse drug events; AERS-DM: AERS data
mining set; EHRs: Electronic health records; PRR: Proportional reporting ratio;
ROR: Reporting odds ratio; IC: Information component; EBGM: Empirical Bayes
geometric mean; RRF: Rich Release Format; NLM: National Library of Medicine;
RxCUI: RxNorm concept unique identifier; UMLS: Unified Medical Language
System.
Competing interests
The authors declare that they have no competing interest.Authors contributions
All co-authors are justifiably credited with authorship, according to the authorship
criteria. Final approval is given by each co-author. In detail: LW design,
development, analysis of data, interpretation of results, and drafting of the
manuscript; GJ  conception, interpretation of results, and critical revision of the
manuscript; DL  analysis of data; HL  conception, design, development,
interpretation of results, and critical revision of manuscript.
Acknowledgements
Liwei Wang is the recipient of a scholarship granted by the State Scholarship
Fund from the China Scholarship Council. The study was also supported by
NSF grant ABI:0845523 and NIH grant R01LM009959A1.
Received: 13 June 2013 Accepted: 23 July 2014
Published: 12 August 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Nichols et al. Journal of Biomedical Semantics 2014, 5:1
http://www.jbiomedsem.com/content/5/1/1DATABASE Open AccessNeuroanatomical domain of the foundational
model of anatomy ontology
B Nolan Nichols1*, Jose LV Mejino1, Landon T Detwiler1, Trond T Nilsen1, Maryann E Martone2, Jessica A Turner3,
Daniel L Rubin4 and James F Brinkley1Abstract
Background: The diverse set of human brain structure and function analysis methods represents a difficult
challenge for reconciling multiple views of neuroanatomical organization. While different views of organization are
expected and valid, no widely adopted approach exists to harmonize different brain labeling protocols and
terminologies. Our approach uses the natural organizing framework provided by anatomical structure to correlate
terminologies commonly used in neuroimaging.
Description: The Foundational Model of Anatomy (FMA) Ontology provides a semantic framework for representing
the anatomical entities and relationships that constitute the phenotypic organization of the human body. In this
paper we describe recent enhancements to the neuroanatomical content of the FMA that models cytoarchitectural
and morphological regions of the cerebral cortex, as well as white matter structure and connectivity. This modeling
effort is driven by the need to correlate and reconcile the terms used in neuroanatomical labeling protocols. By
providing an ontological framework that harmonizes multiple views of neuroanatomical organization, the FMA
provides developers with reusable and computable knowledge for a range of biomedical applications.
Conclusions: A requirement for facilitating the integration of basic and clinical neuroscience data from diverse
sources is a well-structured ontology that can incorporate, organize, and associate neuroanatomical data. We
applied the ontological framework of the FMA to align the vocabularies used by several human brain atlases, and
to encode emerging knowledge about structural connectivity in the brain. We highlighted several use cases of
these extensions, including ontology reuse, neuroimaging data annotation, and organizing 3D brain models.
Keywords: Data integration, Neuroanatomy, Neuroscience, Ontology, Brain atlas, Neuroinformatics, Information
retrieval, mriBackground
Large-scale human brain imaging initiatives are generat-
ing Big Data to characterize normal and neuropsychi-
atric brain structure and function. The Alzheimers
Disease Neuroimaging Initiative (ADNI, [1]), Human
Connectome Project (HCP, [2]), NKI-Rockland Sample
[3], and others provide researchers with unprecedented
access to massive amounts of shared neuroimaging data.
While tools and methods are available to support the
visualization [4-8], data management [9-13] and analysis
[14-17] of shared or privately collected neuroimaging* Correspondence: nolan.nichols@gmail.com
1University of Washington, Seattle, WA, USA
Full list of author information is available at the end of the article
© 2014 Nichols et al.; licensee BioMed Central
Commons Attribution License (http://creativec
reproduction in any medium, provided the ordata, these tools use different approaches to define, seg-
ment, and label neuroanatomical structures.
One important component of research in this domain
involves the development of digital brain atlases, which
provide both a template brain and neuroanatomical la-
bels in a standard coordinate system. Imaging data from
individual participants are aligned to the template and
brain region labels are propagated over to provide con-
text to observed features in the data (e.g., location of ac-
tivation foci). Atlases (i.e., the brain template and
anatomical label pair) can be based on manual or auto-
matically labeled brain regions that are derived using
volume-based or surface-based methods. Each brain
atlas develops, adopts, or refines an anatomical labeling
protocol [18-24]. As a result, the labeling protocols used
to define the boundaries of neuroanatomical regions canLtd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 2 of 13
http://www.jbiomedsem.com/content/5/1/1vary widely across brain atlases, so collections of ana-
tomical entities from different labeling schemes do not
always stand in a relation that allows a one-to-one map-
ping. Thus, data annotated with labels from different at-
lases are difficult to compare based on labels alone.
Previous efforts to reconcile terminologies in neuroim-
aging, referred to as the brain atlas concordance problem,
have taken both quantitative and qualitative approaches.
Taking a quantitative, bipartite graph approach, Bohland
et al. demonstrated that different brain atlas labeling
schemes lack a high degree of spatial concordance when
comparing labels that seemingly refer to the same anato-
mical structure [25]. Qualitative approaches, such as our
own, organize anatomical labels through synonymy, rela-
tions, and class hierarchies that provide practical utility
(e.g., information retrieval and data integration) without
resolving fine-grained spatial discrepancies. These ap-
proaches are complementary and both will be necessary in
identifying a satisfactory solution to the brain atlas con-
cordance problem.
To improve our symbolic model of the brain atlas con-
cordance problem the labeling protocols from each brain
atlas need to be made explicit. However, the labeling
protocols that define anatomical structure boundaries in
brain atlases are generally published in natural language
(i.e., as a manuscript) and lack the term definitions and
relationships provided by a machine-readable ontology
framework. Neuroimaging data and information encoded
by these terms cannot be accurately interpreted, com-
pared, correlated and applied across different studies. A
similar standardization issue faces the development of
white matter connectivity atlases [26,27], in which our
understanding of human brain connectivity is rapidly
evolving. As new white matter analysis methods and la-
beling protocols are developed, a proliferation of terms
to label newly identified structures in white matter at-
lases will likely occur.
A robust semantic framework is needed to explicitly
represent the anatomical labels from different atlases
using relationships that describe anatomical structure.
Our goal is to provide such a framework for human
neuroimaging that will facilitate the integration and
harmonization of data registered to standard coordinate
systems with labels for structures in human brain
atlases.
Approaches to labeling brain structures
We selected brain atlases that are widely used in the hu-
man brain mapping community and harmonized the
terms used in each atlas labeling scheme with the ana-
tomical structures modeled in the Foundational Model
of Anatomy (FMA) Ontology. In this section, we provide
a summary of brain labeling protocols that describe the
anatomical knowledge and spatial relationships encodedin the Talairach Daemon, Desikan-Killiany (i.e., FreeSur-
fer), and Anatomical Automatic Labeling (AAL) atlases,
as well as NeuroLex. We conclude with a proposal for
harmonizing all atlas and NeuroLex terms with classes
in the FMA ontology.
Talairach daemon labels
The Talairach Daemon (TD) is an information system
that provides a mapping between 3D coordinates (i.e.,
Talairach coordinates) and specific brain structure labels
[28]. It is a digital representation of the original Talair-
ach atlas [19] that is hierarchically organized into five
levels:
1. Hemisphere
2. Lobe
3. Gyrus
4. Tissue type
5. Cell type
For example, the label Right Cerebrum.Temporal Lobe.
Inferior Temporal Gyrus.Gray Matter.Brodmann area 20
represents a number of 3D coordinates in the Brodmann
area 20 cell-type level, the gray matter tissue-type level
and so on. While this approach has been broadly applied
in human brain mapping, there are limitations when nor-
malizing patient MRI scans due to natural morphological
differences between individuals.
Desikan-Killiany atlas
The Desikan-Killiany (DK, [29]) atlas is a gyral, surface-
based parcellation scheme for labeling anatomical MRI
scans. The anatomical labeling protocol was manually
applied to 40 MRI scans to build a template brain with
labels for 34 cortical regions of interest (ROI) per hemi-
sphere. This atlas is packaged with the FreeSurfer MRI
data analysis package [30] that provides researchers with
access to a variety of image processing tools that in-
cludes labeling anatomical ROIs with a predefined set of
terms.
Automated anatomical labeling
The Automated Anatomical Labeling (AAL) brain
atlas provides labels for 90 anatomical regions of
interest (45 per hemisphere) from a single participant
using magnetic resonance imaging (MRI) [20]. Ana-
tomical structures (45 per hemisphere) were identified
in a high-resolution MRI by manually tracing struc-
tures in each slice of a 3D volume. The AAL Toolbox
for the Statistical Parametric Mapping Matlab package
[17,31] provides researchers with a method for label-
ing brain regions in their data using the AAL proto-
col and corresponding vocabulary.
Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 3 of 13
http://www.jbiomedsem.com/content/5/1/1NIFSTD and NeuroLex
The Neuroscience Information Framework (NIF) stand-
ard ontologies (NIFSTD) are developed to provide a
consistent source of terminology for neuroscience con-
cepts [32]. NIFSTD is not a brain labeling protocol nor
is it tied to a particular spatial arrangement of brain re-
gions, but is a collection of brain region labels and
inter-relationships. Neurolex also represents a general
mammalian hierarchy of brain parts, with each struc-
ture assigned a taxon rank at which it is generally con-
sidered to hold, whereas other ontologies, like the
FMA, are more species specific. NIFSTD is a formal
ontology constructed through the import of commu-
nity ontologies with specific extensions for neuro-
science, covering the major domains of neuroscience
[32,33]. For community contributions, NIF maintains
the Neurolex lexicon, where each entity within the
ontology is exposed as a wiki page (http://neurolex.org),
built using the Semantic Media Wiki Platform. Entities
migrate from Neurolex into the more formal NIFSTD
ontologies [33].
An important feature of the project is to clearly and ex-
plicitly define all of the terms that are used to describe
data (e.g., anatomical terms, techniques, organism names).
The NIF gross anatomy module was largely based on the
NeuroNames hierarchy [34-36], re-coded in the Web
Ontology Language (OWL), but has been extensively
modified through contributions to Neurolex. Neurolex
serves as a community platform where those with minimal
knowledge of building ontologies can still contribute their
expertise. Through programs such as the Neuron Registry
project of the International Neuroinformatics Coordinat-
ing Facility (http://incf.org), Neurolex is growing into a
significant knowledge base for neuroscience. However,
NeuroLex does not currently provide the framework ne-
cessary to correlate the terms from different brain labeling
schemes.
Our approach: the Foundational Model of
Anatomy ontology
The Foundational Model of Anatomy Ontology (FMA) [37]
is an open source reference ontology for the domain of ana-
tomy that takes into account, at all biologically salient levels
of organization, the entities and their spatio-structural rela-
tions which constitute and form the structural phenotype of
vertebrates with a special emphasis on the human organism.
It is based on a unifying theory that explicitly defines ana-
tomy and its content from the structural point of view. In
particular, it provides a framework that can incorporate and
accommodate all entities under the purview of the anatomy
domain.
The FMA is implemented as a computable informa-
tion artifact and is primarily intended for developers of
terminologies and application ontologies [38] in clinicalmedicine and biomedical research that require anatom-
ical knowledge. Ontologists primarily value its merits be-
cause it is both broader and more fine-grained than
extant anatomy texts or terminologies. For example, the
FMA models both abstract, high-level concepts and leaf-
level, fine grained concepts such as Material anatomical
entity and Brodmann area 1 of left postcentral gyrus,
respectively. This approach is not entirely consistent
with the tradition-based representation of anatomy that
clinical practitioners and biomedical researchers are
taught in their training. Therefore, the benefits the FMA
offers to end users are best realized through derived ap-
plication ontologies [38,39] and biomedical software that
utilize anatomical knowledge.
The principled framework provided by the FMA is
flexible enough to capture the intended semantics of ter-
minologies developed for more specific purposes. For
example, application ontologies derived from the FMA
(e.g., RadLex [40,41]) can be used to reconcile prevalent
views of anatomy (e.g., radiologists or anatomy teachers)
with an ontological representation of biological struc-
ture. Thus, knowledge extracted from the FMA can be
abstracted to a level that is familiar to individuals in a
given domain. The FMA can also incorporate annota-
tions on anatomical entities that provide a mapping to
external knowledge sources (e.g., ontologies or brain
atlas terms), as well as a means to correlate between
mapped terms. A central goal of this paper is to demon-
strate how the FMA can be used to harmonize the grow-
ing number of neuroscience terminologies and provide a
framework and use cases for developing useful biomed-
ical applications.Construction and content
Authoring environment
The FMA information artifact is implemented in
Frames using Protégé, an authoring and editing envir-
onment created by members of the Stanford Biomedical
Informatics Group [42]. Currently, the master copy of
the FMA is stored in a relational MySQL database;
however, many major biomedical ontologies (e.g., those
in the OBO Foundry [43]) are now developed using
OWL. OWL is now the standard language for describ-
ing ontologies on the Web, and there are ongoing
efforts to translate the FMA into OWL. Previous at-
tempts succeeded in creating a version of the FMA in
OWL Full [44], and more recently a subset of the FMA
was converted into OWL 2 [45]. The migration of the
entire FMA into OWL 2 would greatly facilitate inte-
gration and interoperability with external ontologies
and Semantic Web-related technologies. A strategy for
this conversion is in early development, thus outside
the scope of this paper; however, even without this
Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 4 of 13
http://www.jbiomedsem.com/content/5/1/1migration, the current Frames version allows us to rec-
oncile existing neuroanatomy terminologies.
Enhancing neuroanatomy content in the FMA
The neuroanatomical content of the FMA was enhanced
with detailed modeling for cerebral hemisphere brain la-
beling schemes, cerebral sulci, white matter structures,
and neural connectivity relationships. These enhance-
ments were designed to support use cases in human
brain imaging by incorporating four major terminolo-
gies, described above, that are widely used for annotating
neuro-related data (i.e., Talairach, Desikan-Killiany, AAL,
NeuroLex). Our goal was to augment the FMA with the
spatio-structural properties needed to represent different
brain labeling schemes, while maintaining a single co-
herent framework. By accommodating different views
within the same framework, we can use the enhanced
FMA properties to correlate disparate brain labeling
schemes. In the next section we describe the extension
in more detail. Note that in this paper we represent
FMA classes in Courier New font and relationships in
bold italic.
Cerebral hemisphere labeling
For a given anatomical labeling protocol, the terms used
to label or annotate brain structures may refer to neuro-
anatomical entities at different levels of granularity or
using disparate features (e.g., morphological vs. cyto-
architectural) to define the boundaries of specific struc-
tures and their corresponding labels. This means that
there may not be a direct or one-to-one correspondence
between the terms from different atlases or terminologies.Figure 1 Protégé screen capture showing the slots for the different t
superior frontal gyrus maps to Talairach and DK (Freesurfer).However, by mapping these terms to the FMA, the onto-
logical structure of the FMA explicitly defines what en-
tities are represented by the terms and how they correlate
with one another according to the properties and spatio-
structural relationships established for them in the FMA.
In this section we provide a technical overview of how the
FMA was enhanced to accommodate and correlate differ-
ent brain labeling protocols.
To provide a mapping between different terminologies
we used Protégé to introduce property slots (e.g., source
names and unique identifiers) that link FMA classes to cor-
responding annotation terms (Figure 1). The labels used in
each labeling scheme were manually correlated with a cor-
responding FMA term. A list of potential mappings was
semi-automatically generated using direct string matching,
synonymy mapping, lexical mapping, or by interpreting the
symbols and abbreviations used in a given labeling protocol
(e.g. R for Right, ctx for cortex, etc.).
Mapping NeuroLex
Using the term matching approach described above, the
terms from NeuroLex were mapped to a subset of clas-
ses from the FMA that model neuroanatomical know-
ledge. For example, Right frontal lobe is a direct
string match between FMA and NeuroLex, Inferior
horn of the lateral ventricle in NeuroLex is
synonymous with Temporal horn of lateral ventricle in
the FMA, and Lateral occipital cortex in Neu-
roLex is a lexical match to Cortex of lateral occipital
gyrus in the FMA. The mappings were then manually in-
corporated into the FMA by adding specific NeuroLex
identifiers to the newly defined NeuroLex_ID property.erminologies. In the example, the FMA class Gray matter of right
Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 5 of 13
http://www.jbiomedsem.com/content/5/1/1Mapping Talairach
Talairach Daemon annotations explicitly represent five
levels of partonomy where the level of granularity for
each neuroanatomical entity is denoted by a period. For
example, the Talairach label Right Cerebrum.
Frontal Lobe.Superior Frontal Gyrus.Gray
Matter.Brodmann area 6 indicates a set of coordi-
nates located on Right Brodmann area 6, on the
Right superior frontal gyrus of the Right
frontal lobe in the Right cerebral hemisphere.
The actual neuroanatomical entity being represented here is
Brodmann area 6 of right superior frontal gyrus, which ex-
ists in the FMA and is therefore directly mapped to the cor-
responding Talairach term.
Where appropriate and necessary in the ontology, we
added new classes, properties and relations to complete
the mappings between FMA classes and the different an-
notation terms [46,47]. This is particularly true for ac-
commodating and reconciling different labeling schemes
for the cerebral cortex. For example, the Talairach
term Right Cerebrum.Frontal Lobe.Superior
Frontal Gyrus.Gray Matter.Brodmann area 6
refers to an area in the gray matter of the right su-
perior frontal gyrus that overlaps with Brodmann area
6. Whereas the gyrus is subdivided into regions based
on topographical surface landmarks, Brodmann areas
are regions defined on the basis of the underlying
cytoarchitecture or cellular and laminar organization.
Although both types of regional partitions are in the
FMA, neither Brodmann area 6 nor the gray matter of
the right superior frontal gyrus had been partitioned to
account for the overlap. We therefore reconciled both
morphological and cytoarchitectural schemes into the
FMA ontology with the following modeling pattern.
First, we created a class for the Gray matter of the
superior frontal gyrus, which overlaps with (i.e.,
has_regional_part) Brodmann areas 6, 8, 9, 10 and 11
(Figure 2). Second, we created a class for Brodmann area
6 and model overlaps with the gray matter of the precentral,
the superior frontal, the middle frontal, the inferior frontal
and the medial frontal using has_regional_part relations
(Figure 3).
Going back to our Talairach example above, we
mapped it to the new FMA class called BrodmannFigure 2 A listing of regional parts (i.e., Brodmann Areas) for the grayarea 6 of right superior frontal gyrus which
is_a Segment of Brodmann area 6 and a regional_-
part_of both Right Brodmann area 6 and Gray
matter of right superior frontal gyrus
(Figure 4). And following the transitive part_of relation
of Brodmann area 6 of right superior frontal
gyrus up the FMA part hierarchy reveals that all the
granularity levels implicitly encoded in the Talairach
label are explicitly represented in the part hierarchy of
the FMA (Figure 5). The latter is the kind of information
that the ontology can provide to facilitate automated
reasoning by any system.
Mapping Desikan-Killiany and AAL
The labels used in FreeSurfer with the Desikan-Killiany
(DK) atlas contain abbreviations and acronyms such as
Ctx, lh and wm which mean Cortex, Left hemi-
sphere and White matter, respectively. For example
the term ctx-lh-postcentral maps to Gray matter of
left postcentral gyrus. Many of the terms used
in DK (e.g. ctx-rh-inferiortemporal) and AAL (e.g. Tem-
poral_Inferior_Right) are customized and specific only to
their respective projects. Therefore, some semantic inter-
pretation is required to parse the meaning and interoper-
ate with other atlas terminologies. In the FMA we provide
a semantic framework that explicitly declares the intended
meanings of the terms used.
We identified anatomical entities (i.e., classes) in the
FMA that most closely correspond to a given brain atlas
label. We then elaborated on the properties associated
with each FMA class to provide additional relationships
that capture information necessary to correlate with the
labels from other brain atlases and NeuroLex (e.g., iden-
tifiers, preferred names, etc.). From the above examples,
ctx-rh-superiorfrontal from DK is mapped to the FMA
class Gray matter of right superior frontal
gyrus and Frontal_superior_right from AAL to FMA
class Right superior frontal gyrus.
Note that the structural entities represented by the dif-
ferent terms are at various levels of granularity, with
Talairach, FreeSurfer and AAL at the levels of Brodmann
area, gray matter of cortex, and gyrus, respect-
ively. Furthermore a laterality attribute is specified for all
three representations as opposed to NeuroLex, which doesmatter of the superior frontal gyrus.
Figure 3 A listing of regional parts (i.e., cortical gyri) that intersect Brodmann Area 6.
Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 6 of 13
http://www.jbiomedsem.com/content/5/1/1not require left/right attributes. However the FMA has
the framework to correlate all the entities based on their
ontological definitions and relationships as shown in
Figure 6. In this example, the Talairach term is mapped to
the FMA class Brodmann area 6 of right superior
frontal gyrus, a part_of Gray matter of right
superior frontal gyrus, the FMA class referenced
by DK, which in turn is a part_of the AAL mapped entity
Right superior frontal gyrus. The non-
lateralized NeuroLex classes are then mapped via is_a re-
lation to the lateralized entities represented in the other
terminologies (e.g., Right superior frontal gyrus
(AAL) is_a Superior frontal gyrus (NeuroLex)).
White matter and connectivity relationships
There are a growing number of human neuroimaging
techniques from the emerging field of connectomics
[2,48,49] that can describe white matter connectivity at an
increasing level of detail. Parallel ontological representa-
tions are required to capture and accommodate the newly
derived or updated knowledge models these methods pro-
vide. This is necessary to establish precise and reliable
structural-functional correspondence between disparateFigure 4 Part relationships of Brodmann area 6 of right superior fronrepresentations of white matter structures. Using new and
classic neuroanatomical knowledge we have enhanced the
FMA representation of white matter tracts and connectiv-
ity relationships between gray and white matter structures.Partonomy of white matter structures
We pursued a comprehensive spatio-structural representa-
tion of white matter tracts, particularly relating to parton-
omy and connectivity relationships. A good example
addressed by this approach relates to the common practice
of using the same term to represent both the entire tract
and its segments, as in the case of the Corticospinal
tract. In cases where only a very specific segment of the
tract is to be identified, the indiscriminate use of a non-
exclusive term for its annotation can lead to errors and
inconsistencies, especially when machine-processing is
involved. This can be avoided by properly declaring the
parts of a structure with unique terms assigned to each
part and only using the structure specific term for annota-
tion. Figure 7 illustrates this approach using the Corti-
cospinal tract (i.e., the complete structure) and all of
its named segments/parts.tal gyrus.
Figure 5 Correlation of Talairach label to part relationships of Brodmann area 6 of right superior frontal gyrus in the FMA.
Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 7 of 13
http://www.jbiomedsem.com/content/5/1/1Granularity of connectivity relationships
Connectivity between neuroanatomical entities entails
relationships at different levels of granularity. Numerous
terms have been used inconsistently to establish con-
nectivity relationships between neurons, between nerve
fibers or tracts and between gray matter structures.Figure 6 An example of how terms from brain atlases and vocabulari
corresponding class in the FMA hierarchy.In this paper we proposed and gave definitions to
specific connectivity types for different granular entities
(Table 1).
At the neuronal level, a neuron can synapse_with an-
other neuron or a muscle fiber or a gland cell. Connect-
ivity is at the subcellular level between the pre-synaptices (right, yellow) can be correlated by mapping to the
Figure 7 Regional partition of the corticospinal tract from the
brain to the spinal cord.
Table 1 White matter connectivity terms and definitions
Innervate A connectivity relation where a neurite of one neuro
a region of a muscle cell or a gland cell.
Synapse_with A connectivity relation where there is apposition be
postsynaptic membrane of one or more neurites of
form of neurotransmission is evident between them
Projects_to A connectivity relation where individual axons comp
synapse_with neurites or somas of a collection of ne
synonymous with terminate_in.
Projects_from A connectivity relation where individual axons comp
more brain regions. This relation may be synonymou
Sends_output_to A subproperty of project_to relation where neurotra
brain regions.
Receives_input_from A subproperty of project_from relation where neuro
brain regions.
Has_pathway A connectivity relation where a collection of neuron
located in B via axons comprising the fiber tract from
Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 8 of 13
http://www.jbiomedsem.com/content/5/1/1membrane of a neurite of a neuron with a post-synaptic
membrane of a neurite or soma of another neuron or
with a region of a muscle fiber or a gland cell. White
matter structures at the nerve or tract-level (i.e., collec-
tion of axons) projects_to and projects_from any region
of the neuraxis (i.e., a term referring to both brain and
spinal cord). For example, the Dorsal segment of
superior longitudinal fasciculus (i.e., SLF I)
projects_from Brodmann area 6 of superior
frontal gyrus and projects_to Brodmann area 5 of
superior parietal lobule. Finally, we created tern-
ary relationship types that model neural connectivity
between any two regions of the neuroaxis connected by
white matter. Gray matter structures receives_input_from
and sends_output_to other gray matter structures, as
shown in Figure 8 for Putamen.
The connectivity relationships we propose capture the es-
sential levels needed to express how information is commu-
nicated throughout the brain. However, several other efforts
are working on the issue of neural connectivity relations.
For example, the OBO Relation Ontology [50] proposes re-
lations such as has_fasciculating_neuron_projection and
axon_synapses_in. As additional relationships are defined,
the FMA will provide a framework to accommodate these
terms for further refinement of connectivity representation.
Cerebral sulci
Among immaterial anatomical entities, particular atten-
tion was directed to anatomical spaces such as the cere-
bral sulci. Sulci are defined in different contexts,
depending on the operational needs of the users. In
some labeling protocols, sulci are treated as 1-D lines
that serve as boundaries of gyri, whereas in surface-
based parcellation models they are spaces or grooves
that surround the gyri. The PALS-B12 atlas from Caret
[51], a significant labeling scheme of widespread utility,
involves the use of sulci to identify and contour buriedn synapses with a neurite or a region of the soma of another neuron or
tween the presynaptic membrane of a neurite of one neuron and the
another neuron or a region of a muscle cell or a gland cell and some
.
rising a fiber tract originating from one or more brain regions
urons located in one or more other brain regions. This relation may be
rising a fiber tract are parts of a collection of neurons located in one or
s with originate_in.
nsmission is sent from one brain region to one or more other
transmission is received by one brain region from one or more other
s located in brain region A sends_output_to a collection of neurons
brain region.
Figure 8 Connectivity relationships for the Putamen.
Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 9 of 13
http://www.jbiomedsem.com/content/5/1/1cortex among gyri. Here the term sulcus denotes a 3D
volume, the segments of gyri located in the furrows. We
disambiguated the representation of sulcus by treating it
as an anatomical space and for the area of the gyrus in
the sulcus, we regarded it as an anatomical structure
that is part of the gyrus and classified it as sulcal seg-
ment of a gyrus under Segment of gyrus of brain.
As shown in Figure 9, the middle frontal gyrus consists
of several regional parts or segments, one of which is
Sulcal segment of middle frontal gyrus and
the rest are parts of the gyrus that are externally visible.
With this approach, the entire middle frontal sulcus canFigure 9 Regional parts of the middle frontal gyrus. Sulcal
segment of middle frontal gyrus is the part located in the sulci.be modeled as belonging to two gyri  both the Sulcal
segment of middle frontal gyrus and Sulcal segment of
inferior frontal gyrus.
Utility and discussion
Extensions to the neuroanatomical axis of the FMA were
motivated by use cases in ontology development, knowledge
retrieval, and data integration. In this section we describe
several use cases for our work and discuss how reusable and
computable anatomical knowledge captured in the ontology
can be utilized to solve real-world problems.
Neuroanatomical knowledge reuse
The FMA is a reference ontology that can be imported into
other ontologies as a way to reuse curated knowledge about
anatomy [37]. RadLex [40] is an ontology composed of
standard terms for the domain of radiology, including im-
aging observations, characteristics, and techniques, as well
as diseases, radiology reporting terminology, and anatomy.
For anatomy, RadLex incorporates a subset of the FMA
that is relevant to the radiological scale of analysis [41].
RadLex also contains knowledge beyond anatomy that
enables additional radiology oriented use cases such as hu-
man brain imaging. For example, a digital brain label can
indicate the anatomical structure that a set of image coor-
dinates pertains to in a brain template, whereas RadLex
can be used to describe key aspects of a neurological im-
aging examination including modality, technique, visual
features, anatomy, findings, and pathology. By incorporat-
ing neuroanatomical content from the FMA, RadLex en-
ables rich dataset annotations and provides a means to
correlate and integrate the findings with other external
data and studies as discussed in the following section.
Ontological knowledge retrieval
To leverage the knowledge we encoded into the FMA
the ontology can be accessed using a query engine. For
this purpose we used the Query Integrator (QI) as an
Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 10 of 13
http://www.jbiomedsem.com/content/5/1/1underlying technology [52] to query the neuroanatom-
ical content of the FMA as represented in OWL-Full.
The QI is a Web-based query management and execution
system that enables queries over any Web-accessible data
or knowledge source (e.g. ontology). The QI supports
multiple query languages, including SPARQL [53] for RDF
[54] data sources. QI queries may be stored for reuse, exe-
cuted via RESTful Web services, and chained together to
form query pipelines. This latter capability allows the re-
sults of ontology queries to be joined with data queries to
answer more interesting questions than are possible based
on the data alone.
Dataset annotation and intelligent query
As reported in Turner et al. [47], the FMA was used to
annotate a large dataset of task-based functional MRI
(fMRI) signal activations in subjects with schizophrenia
and healthy subjects. The activation locations were an-
notated with neuroanatomical labels from the Talairach
Daemon [28,55]. These labels combined cytoarchitec-
tural labels from one method for labeling brain regions,
with morphological terms based on sulci and gyri. The
FMA was extended to include intersections between
label pairs when regions overlap. For example, within
areas covered by the label Inferior temporalFigure 10 Screenshot of the 3D model asset manager component of
and the selected asset set on the left (AAL - Brain). Scenes are generatgyrus exist areas covered by the label Brodmann
area 20. Therefore, part_of the Inferior temporal
gyrus is part_of Brodmann area 20, and part_of the
Inferior temporal gyrus is not in Brodmann
area 20. Conversely, Brodmann area 20 has parts
that are in the Inferior temporal gyrus and parts
of Brodmann area 20 which are in other gyri. This ex-
tension of the ontology in conjunction with a reasoning
engine allowed novel questions to be asked about the
data.
3D anatomical model management
In biomedical education and research, 3D surface models
are useful and commonplace. For example, a researcher
using the brain atlases described above may generate
neuroanatomical surface models from a patients MRI,
where each model is a different brain region. While these
models can be organized using naming conventions or dir-
ectory structures, it may be more meaningful to annotate
models using terms from the FMA. Similar to our work
on annotating tabular datasets, the knowledge in the FMA
can be queried and used to reason about which 3D models
to select and display in a 3D scene. We have developed a
prototype scene generation system that implements this
idea and will allow users to create Web-based 3D scenesthe scene generator displaying a list of asset sets on the right
ed from the selected asset set based on queries to the FMA.
Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 11 of 13
http://www.jbiomedsem.com/content/5/1/1using the results of queries over the FMA or data sets an-
notated with FMA identifiers (Figure 10).
The system provides access to several biomedical 3D
model sets, including models generated from the brain
atlases described above (e.g., AAL and DK) and allows
users to upload their own models. Scenes can be con-
structed by hand or using queries, and queries can be
shared between users and customized using parameters.
Since all scenes are rendered using WebGL, they can
easily be embedded within any website or Web-based
publication. For example, users of this system can access
knowledge in the FMA to generate a scene showing all
portions of the brain with blood supply from the middle
cerebral artery or all structures connected by a given
white matter tract. Similarly, scenes can be generated to
display different model sets of the same structures such
as 3D models of the left hemisphere in the DK
(Figure 11) and AAL anatomical labeling schemes
(Figure 12).
This tool currently provides users with the ability to
manage 3D model sets, query annotated models, and
visualize query results as 3D scenes. As this tool matures
it will also incorporate support for building scenes from
connectivity relationships, visualizing the correlation be-
tween model sets, and displaying volumetric anatomical
images. In addition, our tool provides model management
features that facilitate sharing and reuse of model sets.
These tools are also useful in validating and exploring the
FMA and other terminologies, as well as model sets basedFigure 11 A view of the DK left hemisphere parcellation with
the left precuneus selected. A link to this scene can be found
at: http://purl.org/sig/docs/neurofma-jbs.
Figure 12 A view of the AAL left hemisphere labeling with the
left superior temporal gyrus selected. A link to this scene can be
found at: http://purl.org/sig/docs/neurofma-jbs.on these. Misalignments, missing structures, and issues
with coordinate sets readily become apparent in query-
driven scenes and, in many cases, can be remedied from
within the tool and re-exported for use elsewhere. Simi-
larly, visualizations can be composed that relate the struc-
tures defined in different terminologies to one another. As
mentioned, this system is currently under development;
we anticipate publishing and releasing it to the commu-
nity in the near future. Links to any publications associ-
ated with this tool will be available on the supplementary
materials page (http://purl.org/sig/docs/neurofma-jbs).
Conclusions
We demonstrated that the framework provided by the
FMA ontology can be extended to accommodate and cor-
relate the terms used in three human brain labeling
schemes and NeuroLex. We then used the enhanced FMA
to highlight use-cases for neuroanatomical knowledge re-
use and retrieval. The FMA was found to sufficiently cap-
ture and clarify the relationships between different
Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 12 of 13
http://www.jbiomedsem.com/content/5/1/1anatomical labeling schemes necessary to fulfill the use
cases.
As a result, the disciplined and principled approach in
the FMA lays the foundation for:
1. An ontology-based standard for anatomical data
annotation
2. Queries that can use the ontology to infer
anatomical relationships in data
3. Data visualization systems that incorporate
anatomical knowledge
4. A meta-atlas that harmonizes different brain
labeling protocols
5. A unifying anatomical framework for integrating a
variety of biomedical data
While this effort advances state-of-the-art knowledge
representations of human neuroanatomy, further work is
needed to address the brain atlas concordance problem.
Symbolic representations of anatomical labeling schemes
alone are not sufficient to model the spatial information
in brain atlases. Computational frameworks, such as
proposed by Bohland, et al. [25], provide quantitative
measures of spatial concordance but do not address the
issue of lexical mappings. A hybrid framework that inte-
grates quantitative information with ontologies would
offer a more comprehensive solution to reconciling
neuroanatomical labeling schemes. Additionally, the
purely structural approach taken by the FMA only ac-
commodates anatomical descriptors, and the need for
functional divisions of the brain calls for future develop-
ment of a functional brain labeling ontology.Availability and requirements
Latest Release:
Foundational Model Explorer (Online):
http://sig.biostr.washington.edu/projects/fm/FME
Frames:
http://sig.biostr.washington.edu/projects/fma/release/
index.html
OWL-Full:
http://sig.biostr.washington.edu/share/downloads/fma/
FMA_Release/alt/v3.2.1/owl_file/fma_3.2.1_owl_file.zip
Licensing:
Creative Commons Attribution 3.0: http://creative-
commons.org/licenses/by/3.0Abbreviations
AAL: Automated anatomical labeling; FMA: Foundational model of anatomy;
fMRI: Functional magnetic resonance imaging; MRI: Magnetic resonance
imaging; NIF: Neuroscience information framework; NIFSTD: Neuroscience
information framework standard ontology; OBO: Open biomedical
ontologies; QI: Query integrator; RO: Relations ontology; ROI: Region of
interest; TD: Talairach daemon.Competing interests
The authors report no competing interests with the work described in
this manuscript.
Authors contributions
BNN provided expertise on brain atlases, extracted connectivity information
from the literature, and coordinated use case efforts. JLVM is a primary
author of the FMA and manually incorporated terms into the FMA using
Protégé. TTN is the primary developer of the model management and
visualization application. MEM provided expertise on NIF, NeuroLex, and
neuroanatomy. JAT provided expertise on neuroanatomical dataset
annotation and brain imaging. DLR provided expertise on RadLex. JFB
oversaw all research activities and provided expertise on the FMA and QI
application. All authors reviewed, edited, and approved of the manuscript.
Acknowledgements
This work was supported in part by RC4 NS073008-01 (BNN , JFB), the
National Academies Keck Futures Initiative (BNN), 1R01MH084812-01A1 (JAT),
RSNA-NIBIB HHSN268200800020C (DLR, JAT, JFB, LTD, JLVM) and DoD/
USAMRMC GRANT10362280 (JFB, TTN). This work was conducted using the
Protégé resource, which is supported by grant GM10331601 from the
National Institute of General Medical Sciences of the United States National
Institutes of Health.
Author details
1University of Washington, Seattle, WA, USA. 2University of California San
Diego, San Diego, CA, USA. 3Mind Research Network, Albuquerque, NM, USA.
4Stanford University, Stanford, CA, USA.
Received: 4 July 2013 Accepted: 24 December 2013
Published: 8 January 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Doulaverakis et al. Journal of Biomedical Semantics 2014, 5:13
http://www.jbiomedsem.com/content/5/1/13
RESEARCH Open Access
Panacea, a semantic-enabled drug
recommendations discovery framework
Charalampos Doulaverakis1*, George Nikolaidis2, Athanasios Kleontas2,3 and Ioannis Kompatsiaris1
Abstract
Background: Personalized drug prescription can be benefited from the use of intelligent information management
and sharing. International standard classifications and terminologies have been developed in order to provide unique
and unambiguous information representation. Such standards can be used as the basis of automated decision
support systems for providing drug-drug and drug-disease interaction discovery. Additionally, Semantic Web
technologies have been proposed in earlier works, in order to support such systems.
Results: The paper presents Panacea, a semantic framework capable of offering drug-drug and drug-diseases
interaction discovery. For enabling this kind of service, medical information and terminology had to be translated to
ontological terms and be appropriately coupled with medical knowledge of the field. International standard
classifications and terminologies, provide the backbone of the common representation of medical data while the
medical knowledge of drug interactions is represented by a rule base which makes use of the aforementioned
standards. Representation is based on a lightweight ontology. A layered reasoning approach is implemented where at
the first layer ontological inference is used in order to discover underlying knowledge, while at the second layer a
two-step rule selection strategy is followed resulting in a computationally efficient reasoning approach. Details of the
system architecture are presented while also giving an outline of the difficulties that had to be overcome.
Conclusions: Panacea is evaluated both in terms of quality of recommendations against real clinical data and
performance. The quality recommendation gave useful insights regarding requirements for real world deployment
and revealed several parameters that affected the recommendation results. Performance-wise, Panacea is compared
to a previous published work by the authors, a service for drug recommendations named GalenOWL, and presents
their differences in modeling and approach to the problem, while also pinpointing the advantages of Panacea.
Overall, the paper presents a framework for providing an efficient drug recommendations service where Semantic
Web technologies are coupled with traditional business rule engines.
Keywords: Ontologies, Decision support, Rule-based reasoning, Drug recommendations
Background
One of the health sectors where intelligent information
management and information sharing compose valuable
preconditions for the delivery of top quality services is
personalized drug prescription. This is more evident in
cases where more than one drug is required to be pre-
scribed, a situation which is not uncommon, as drug inter-
actions may appear. The problem is magnified by the wide
*Correspondence: doulaver@iti.gr
1Centre for Research and Technology Hellas, Information Technologies
Institute, Thessaloniki, Greece
Full list of author information is available at the end of the article
range of available drug substances in combinationwith the
various excipients in which the former are present.
If one takes into account that there exist more than
18,000 pharmaceutical substances, including their excip-
ients, then it is clear that the continuous update of
health care professionals is remarkably hard. Over this,
the extensive literature makes discovery of relevant infor-
mation a time consuming and difficult process, while the
different terminologies that appear between sources add
more burden on the efforts of medical professionals to
study available information.
Semantic Web technologies can play an important role
in the structural organization of the available medical
information in a manner which will enable efficient
© 2014 Doulaverakis et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication
waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise
stated.
Doulaverakis et al. Journal of Biomedical Semantics 2014, 5:13 Page 2 of 10
http://www.jbiomedsem.com/content/5/1/13
discovery and access. Research projects funded for
enabling Semantic Web technologies in the diagnosis and
therapeutic procedures exist such as REMINE [1], PSIP
[2], NeOn [3] and Active Semantic Documents [4] or
works such as [5], but they dont fully address the prob-
lem of automated drug prescription using drug-drug and
drug-disease interactions.
Rule-based approaches have been proposed for address-
ing issues relating to biomedical ontologies research. It
is common for ontologies written in expressive Seman-
tic Web languages such as OWLa, to not be able to
handle all requirements for capturing the knowledge in
several biomedical and medicine domains. As a method
for enriching the expressiveness of ontology languages,
researchers have proposed the use of rules which act upon
the defined ontological knowledge. According to [6], rules
are helpful in the following situations relating to biomed-
ical ontologies: defining standard rules for chaining
ontology properties, bridging rules for reasoning across
different domains, mapping rules for defining map-
pings between ontologies entities and querying rules for
expressing complex queries upon ontologies. The author
gives a thorough review of RuleMLb and SWRLc, the two
major ontology rule languages, the available rule forma-
tion tools and the reasoners. Golbreich et al. [7] makes
use of the outcomes of the previous paper to showcase
the need for rules in biomedical applications with a use
case of a brain anatomy definition, where a brain struc-
ture ontology is defined in OWL but rules describing the
relationships between the properties and entities that are
needed for correct annotation of MRI images. Another
work citing the need for semantically enriched rules,
where an ontology is coupled with SWRL rules for anno-
tating pseudogenes and answering research questions, has
been proposed in [8]. All the above papers present the
need for extending ontologies with rules in order capture
the knowledge of complex biomedical domains.
The paper presents Panacea, a semantic-enabled sys-
tem for discovering drug recommendations and inter-
actions. Panacea is based on experiences and lessons
drawn from the development of GalenOWL [9], a sim-
ilar system which had Semantic Web technologies in
its core. As such, Panacea can be considered the evo-
lution of GalenOWL in terms of design and scalability.
Panacea makes use of established and standardized med-
ical terminologies together with a rich knowledge base
of drug-drug and drug-diseases interactions expressed as
rules. Panacea is implemented having in mind scalabil-
ity, completeness of results and responsiveness in query
answering.
Standard terminologies and semantic web
Standard terminologies and classifications in the med-
ical domain have been developed in order to support
information sharing and exchange and to enable a com-
mon expression of key concepts. Such is the case for
example for the ICD-10d (International Classification of
Diseases) index of theWorld Health Organization (WHO)
where it is used to classify diseases and other health prob-
lems recorded on many types of health and vital records
across many countries. The classification is also used for
storing and retrieval of diagnostic information and for
the compilation of national statistics reports by the WHO
members.
On the other hand, ontologies and the Semantic Web
enable a common representation and understanding of
knowledge. Ontologies can effectively capture a domains
knowledge by specifying the definitions of terms by
describing their relationships with other terms. A rea-
soner can be employed upon an ontology in order to
uncover implicitly defined information while the expres-
siveness of ontologies can be further enriched by formu-
lating rules in standard rule languages, such as RuleML
or SWRL that are mentioned above, thus not sacrificing
interoperability.
Panacea aims to combine and make use of the benefits
of standard terminologies and Semantic Web technolo-
gies by enabling inference and rule-based reasoning on
ontologies that have been expressed using the medical
standards.
Methods
Architecture and functional design
The purpose of Panacea is to provide drug prescription
recommendations based on a patients medical record, i.e.
advise physicians to prescribe medications according to
the drugs active substance indications and contraindica-
tions. For details regarding the initiative that triggered
development of Panacea and the initial medical and phar-
maceutical data that were available, the reader is encour-
aged to read [9].
Panacea follows a layered reasoning process which is
depicted in Figure 1. During the start-up of the system,
the medical terminologies, namely ATCe (Anatomical
Therapeutic Chemical), UNIIf (Unique Ingredient Identi-
fier), ICD-10, ICTVg (International Virus Taxonomy) and
custom encodings, are transformed to semantic entities,
using an appropriate vocabulary, and the initial ontol-
ogy is constructed. The ontology binds to a reasoner to
infer relations such as inheritance and unions. This pro-
cess is performed once offline during initialization and the
knowledge base is available to the system for further uti-
lization. In order to get recommendations in Panacea, a
patient instance with the appropriate medical record data
is created and fed to the knowledge base. The reasoning
process enriches the patient instance with inferred knowl-
edge, thus making it explicit. On this enriched instance,
and by utilizing a different reasoning process, the set of
Doulaverakis et al. Journal of Biomedical Semantics 2014, 5:13 Page 3 of 10
http://www.jbiomedsem.com/content/5/1/13
Figure 1 Panacea framework architecture and data flow.
medical rules is applied upon. The result of this final stage
of rule-based reasoning is the recommendations list which
can be retrieved through SPARQLh querying.
A key characteristic of the suggested architecture is
that, regarding second level reasoning, the framework can
utilize any rule-based reasoner or rule engine. Since all
the inferred knowledge of the medical definitions and
patient data is materialized in the knowledge base, the
medical rules can be expressed and loaded in an appro-
priate rule engine. The rule engine could be an ontology
reasoner or a business rule manager with appropriate cus-
tomizations in the data structures. This approach helps in
bringing together the best of both worlds: semantic and
meaningful representation of data using Semantic Web
technologies and the maturity of traditional rule engines
in efficiently handling complex and large amounts of
rules.
Use case scenario
In order to demonstrate the benefits of the proposed
semantic recommendation system, a use case regarding
a possible scenario is described below. The codings in
the parentheses represent the corresponding ICD-10 and
ATC codes of diseases and drugs, respectively.
An elder man visits his family doctor complaining
for pain in his right lower back and abdominal region
which is accompanied with fever. After appropriate clin-
ical examination, he is diagnosed with right pyelonephri-
tis (ICD-10: N11.0). According to the patients medical
history, he is suffering from chronic atrial fibrillation
(ICD-10: I48.2) for which he receives clopidogrel (ATC:
B01AC04), vertigo (ICD-10: H81.49) for which he receives
cinnarizine (ATC: N07CA02), high arterial blood pressure
(ICD-10: I10) for which he receives candesartan (ATC:
C09CA06) and amlodipine (ATC: C08CA01), and dia-
betes mellitus (ICD-10: E11.9) for which he receives met-
formin (ATC: A10BA02) and sitagliptin (ATC: A10BH01).
For the new condition of pyelonephritis that was diag-
nosed, the treating doctormust decide a number of things.
Regarding the prescription for treating this new disease,
the doctor has to decide which active substances to pre-
scribe in order to treat the resulting inflammation, the
cause of the inflammation, the back and abdominal pain
and the resulting fever.
However, before a decision is made the following fac-
tors regarding the patients medical history should also be
considered:
 There should be a check for drug-drug interaction
that the patient is taking, before the onset of the new
condition (the pyelonephritis).
 There should be a check for drug-disease interaction
of the drugs that the patient is already prescribed
with the new condition.
 The new prescription has to be verified that it will
not have adverse effects or interactions with the
previously prescribed medication and with the
patients medical history.
Doulaverakis et al. Journal of Biomedical Semantics 2014, 5:13 Page 4 of 10
http://www.jbiomedsem.com/content/5/1/13
It is clear that the task for the doctor can be hard and a
misjudgment could lead to wrong prescriptions. Using an
automated drug recommendation system can minimize
this risk. The recommendation system will use the input
data and the pharmaceutical rules in order to propose a
treatment that will be safe for the patient.
Semantic transformations
Panacea is built on top of international standards of
medical terminology in order to represent medical and
pharmaceutical information. The following standard ter-
minologies are used:
ICD-10: International Classification of Diseases. It is
used in Panacea for unique identification of diseases
thus uniquely identifying drug indications and
contraindications related to diseases. The latest 2010
version was used in this work.
UNII: Unique Ingredient Identifier. Used for the
identification of active ingredients found in drugs. In
Panacea it is used for uniquely identifying drug
indications and contraindications related to
ingredients. The 2013 index was used.
ATC: The Anatomical Therapeutic Chemical
Classification is used for the classification of drugs. In
Panacea it is used in similar fashion to UNII. The
latest 2013 index was used.
ICTV: The International Committee on Taxonomy
of Viruses indexing is used for the classification of
viruses. In Panacea it is used in order to uniquely
drug indications and contraindications related to
viruses. The latest 2012 release was used.
Besides these international standards, a number of
domain classifications have been declared and used in
order to enhance the usability of the system or to repre-
sent data that are not included in the standards. These
classifications act as supplementary to the standards.
Substance: As the use of encodings for drug ingre-
dients is not convenient for humans, the identification
of active substances is done using its common name
JOURNAL OF
BIOMEDICAL SEMANTICS
Pesquita et al. Journal of Biomedical Semantics 2014, 5:4
http://www.jbiomedsem.com/content/5/1/4RESEARCH Open AccessThe epidemiology ontology: an ontology for the
semantic annotation of epidemiological resources
Catia Pesquita1,3*, João D Ferreira1,3, Francisco M Couto1,3 and Mário J Silva2,3Abstract
Background: Epidemiology is a data-intensive and multi-disciplinary subject, where data integration, curation and
sharing are becoming increasingly relevant, given its global context and time constraints. The semantic annotation
of epidemiology resources is a cornerstone to effectively support such activities. Although several ontologies cover
some of the subdomains of epidemiology, we identified a lack of semantic resources for epidemiology-specific
terms. This paper addresses this need by proposing the Epidemiology Ontology (EPO) and by describing its
integration with other related ontologies into a semantic enabled platform for sharing epidemiology resources.
Results: The EPO follows the OBO Foundry guidelines and uses the Basic Formal Ontology (BFO) as an upper
ontology. The first version of EPO models several epidemiology and demography parameters as well as
transmission of infection processes, participants and related procedures. It currently has nearly 200 classes and is
designed to support the semantic annotation of epidemiology resources and data integration, as well as
information retrieval and knowledge discovery activities.
Conclusions: EPO is under active development and is freely available at https://code.google.com/p/epidemiology-
ontology/. We believe that the annotation of epidemiology resources with EPO will help researchers to gain a
better understanding of global epidemiological events by enhancing data integration and sharing.Background
Epidemiology is the study of the factors influencing the
occurrence and distribution of health-related states or
events in specified populations, and the application of
this knowledge to control health problems [1]. It is a
multi-disciplinary subject that integrates diverse areas of
knowledge, such as medicine, biology, statistics, social
sciences and geography.
Epidemiology is becoming increasingly data-intensive,
considering the large volumes of data generated by bio-
medical research and by the recent explosion of mobile
phone and Internet usage - which contains epidemiologi-
cally relevant behaviors, such as disease symptoms reports
[2], and also the data created by large-scale computational
simulations and models of disease transmission and
spread [3,4]. To handle these challenges, epidemiology
needs to embrace the new scientific methodology desig-
nated as the fourth paradigm, whereby vast troves of data* Correspondence: cpesquita@di.fc.ul.pt
1LASIGE, Campo Grande, Lisboa, Portugal
3Universidade de Lisboa, Lisboa, Portugal
Full list of author information is available at the end of the article
© 2014 Pesquita et al.; licensee BioMed Centra
Commons Attribution License (http://creativec
reproduction in any medium, provided the orare collected, analyzed, validated and visualized [5]. Ontol-
ogies are crucial to support this new paradigm, since
they provide the means to semantically describe epi-
demiological resources, supporting their categorization
and sharing.
Consider the following example: a research team is
building a model for herd immunity in populations where
a measles vaccine can be administered. To achieve this,
they need data on measles incidence rates and vaccination
rates in different populations/locations over time, as well
as other parameters, such as birth rate, factors influencing
vaccination (e.g. legal frame, income and education level
of parents), transmission mode and secondary attack rate
(i.e. the number of cases of an infection that occur among
contacts within the incubation period following exposure
to a primary case in relation to the total number of ex-
posed contacts). These data can then be used to fit the
parameters of their model. Traditionally, to collect the data,
researchers would conduct extensive literature searches
to find a set of relevant scientific articles, read them to
extract the relevant information and/or contact the authors
to request access to the datasets directly. The epidemiologyl Ltd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Pesquita et al. Journal of Biomedical Semantics 2014, 5:4 Page 2 of 7
http://www.jbiomedsem.com/content/5/1/4community has not yet adopted the practice of publicly
sharing datasets in open databases [6], which further hin-
ders the collection of pertinent data. However, epidemi-
ology is a domain where timeliness is crucial. For instance,
when facing a new pandemic, laboratories need to be able
to produce new vaccines very quickly, and public health
officials need to understand the disease and its spread so
they can issue recommendations to the population to ef-
fectively contain the pandemic and diminish its impact.
To make data collection more efficient and effective,
epidemiological resources need to be easily searchable
and retrievable, which can be achieved by semantic-
enabled platforms for sharing epidemiological resources.
An approach is supporting the annotation of datasets with
ontological concepts, so that the semantics encoded in
ontologies can be used to find relevant resources. For in-
stance, resources that do not refer to measles, but to other
typical childhood diseases with the same transmission
mode can very well be of interest to extract parameters
for the measles herd immunity model.
The only currently available ontology specifically intended
for epidemiology is integrated into the BioCaster Global
Health Monitor [7], a news filter created with the aim of
providing an early warning monitoring station for epi-
demic and environmental diseases. However, the 2,000
classes of the BioCaster ontology are insufficient to
provide enough coverage and granularity for a full se-
mantic annotation of epidemiological resources. For
instance, there is no class for vaccine, and diseases are
direct instances of Human Disease or Avian Disease,
which are direct subclasses of Disease, highlighting the
complexity of modeling these domains [8]. However, in
such a multidisciplinary domain as epidemiology, several
key areas have already been described in existing ontol-
ogies, including, among others, the Disease Ontology
[9], Infectious Disease Ontology (IDO) [10], Symptom
Ontology [11], Vaccine Ontology [12] and the Pathogen
Transmission Ontology (TRANS) [11]. In previous work,
we have outlined a Network of Relevant Ontologies for
Epidemiology (NERO) [13]. We found that while some
concepts are fully covered by these ontologies, others are
not, in particular the specific epidemiological concepts
that are seldom used outside this domain, such as, for in-
stance, parameters like exposure ratio or attack rate.
Consequently, a new ontology that covers these specific
epidemiology concepts, while reusing and complementing
relevant existing ontologies in related domains is needed.
Bearing this in mind, we have created the Epidemiology
Ontology (EPO), which aims at covering the areas of epi-
demiology not well described by other quality ontologies,
particularly those related with metrics, parameters and
models. EPO currently covers epidemiological and demo-
graphical parameters, for which there was very little cover-
age in surveyed ontologies, as well as transmission ofinfection, complementing classes from the TRANS ontol-
ogy. In future versions, the scope of EPO will be expanded
to include all parameters that influence epidemic pro-
cesses, in articulation with existing and in development
ontologies for public health and medical surveillance.
In this paper, we describe the current state of EPO and
how it is related to other ontologies relevant for the epi-
demiological domain. We also explain how EPO is being
used to annotate epidemiological resources in a platform
for epidemiological resource sharing, where it supports
data querying and integration, and provide examples of
how it could also be used for annotation of other databases
and literature. The current version of EPO has 190 classes,
of which 118 are newly created and 33 are imported from
two relevant OBO foundry candidate ontologies, IDO and
TRANS. EPO uses the Basic Formal Ontology (BFO) [14]
as an upper ontology, and IAO [15] as a source of annota-
tion properties, further supporting its interoperability with
other OBO foundry ontologies and candidate ontologies.
We have submitted EPO to the OBO Foundry [16], as
well as to the BioPortal site of the National Center for
Biomedical Ontologies (NCBO) [17]. EPO is freely avail-
able at https://code.google.com/p/epidemiology-ontology/.
Results
Modelling
We used the Dictionary of Epidemiology (DoE) [1] in
the creation the EPO. The Dictionary of Epidemiology is
a well-established reference that captures the nomencla-
ture commonly used in epidemiology. Most class labels,
synonyms and definitions in EPO correspond to diction-
ary entries or sub-entries.
In the current version of EPO, we have focused our
modeling activity in three major areas: demographic pa-
rameters, epidemiological parameters and transmission
of infection.
Although some resources contain a few demographic
parameters, such as MeSH [18] and NCI Thesaurus [19],
we have found that the majority of such parameters are
not represented in hierarchical vocabularies or ontol-
ogies. Likewise, the coverage for epidemiological param-
eters was also quite sparse. However, there are several
resources that model transmission of infection, in-
cluding the Pathogen Transmission Ontology (TRANS)
with 25 classes fully dedicated to transmission of infection,
the Host Pathogen Interaction Ontology [20], Influenza
Ontology [21] and NCI Thesaurus. Nevertheless, TRANS
models transmission of infection types only, and it does so
in a different fashion from the DoE, with a different hier-
archical organization and definitions. Consequently, we
chose to include classes for transmission of infection in
EPO in accordance with the entries in the DoE. Whenever
an equivalent class was present in TRANS we imported it,
but used the label and definition from the DoE as editor
Figure 1 A representative portion of EPO. This diagram represents a portion of EPO and how EPO classes are related to each other and to
other ontologies classes. Unlabeled arrows represent subclass relationships, and labeled arrows represent relations imported from RO. The
ontology for each class is identified by its prefix.
Table 1 Statistics of EPO specific and imported classes
and properties
Ontology Number of classes
or properties
Epidemiology Ontology (EPO) 118
Infectious Disease Ontology (IDO) 19
Pathogen Transmission Ontology (TRANS) 14
Basic Formal Ontology (BFO) 38
Relation Ontology (RO) 4
Information Artifact Ontology (IAO) 7
OBOInOWL 1
Phenotypic Quality Ontology (PATO) 1
Total 202
Pesquita et al. Journal of Biomedical Semantics 2014, 5:4 Page 3 of 7
http://www.jbiomedsem.com/content/5/1/4preferred label and definition, which resulted in reusing
14 TRANS classes, for a total of 21 transmission of infec-
tion types modeled in EPO. These classes are organized in
single inheritance, in up to five levels, increasing the
granularity level given by TRANS by two levels, but also
widening its scope by including classes for the participants
in the transmission of infection process. These include
classes imported from IDO as well as EPO-specific classes,
which are linked to their respective transmission type via
participates_in relations (see Figure 1. for a relevant por-
tion of EPO).
Furthermore, EPO also contains 17 classes dedicated
to transmission of infection-related processes, such as
isolation, containment and eradication, to name a few.
These classes are particularly relevant for the description
of public health procedures and their impact on epi-
demic events. Their articulation with transmission of in-
fection types in describing epidemiological resources will
allow the elucidation of the relations between these pro-
cedures and the mode of transmission.
In the demographic and epidemiological parameters
branches we currently have 36 and 21 classes, respectively.
These are organized in a multiple inheritance structure,
with classes being both subclasses of either demography
parameter or epidemiology parameter, as well as of their
specific parameter type, like rate. To the best of our
knowledge, there were no suitable ontologies from whichto import classes in these areas, since the very few
terms that exist are poorly defined and structured. How-
JOURNAL OF
BIOMEDICAL SEMANTICS
González et al. Journal of Biomedical Semantics 2014, 5:46
http://www.jbiomedsem.com/content/5/1/46SOFTWARE Open AccessAutomatically exposing OpenLifeData via SADI
semantic Web Services
Alejandro Rodríguez González1, Alison Callahan2, José Cruz-Toledo3, Adrian Garcia1, Mikel Egaña Aranguren4,
Michel Dumontier2 and Mark D Wilkinson1*Abstract
Background: Two distinct trends are emerging with respect to how data is shared, collected, and analyzed within
the bioinformatics community. First, Linked Data, exposed as SPARQL endpoints, promises to make data easier to
collect and integrate by moving towards the harmonization of data syntax, descriptive vocabularies, and identifiers,
as well as providing a standardized mechanism for data access. Second, Web Services, often linked together into
workflows, normalize data access and create transparent, reproducible scientific methodologies that can, in
principle, be re-used and customized to suit new scientific questions. Constructing queries that traverse
semantically-rich Linked Data requires substantial expertise, yet traditional RESTful or SOAP Web Services cannot
adequately describe the content of a SPARQL endpoint. We propose that content-driven Semantic Web Services
can enable facile discovery of Linked Data, independent of their location.
Results: We use a well-curated Linked Dataset - OpenLifeData - and utilize its descriptive metadata to automatically
configure a series of more than 22,000 Semantic Web Services that expose all of its content via the SADI set of
design principles. The OpenLifeData SADI services are discoverable via queries to the SHARE registry and easy to integrate
into new or existing bioinformatics workflows and analytical pipelines. We demonstrate the utility of this system through
comparison of Web Service-mediated data access with traditional SPARQL, and note that this approach not only simplifies
data retrieval, but simultaneously provides protection against resource-intensive queries.
Conclusions: We show, through a variety of different clients and examples of varying complexity, that data from the
myriad OpenLifeData can be recovered without any need for prior-knowledge of the content or structure of the
SPARQL endpoints. We also demonstrate that, via clients such as SHARE, the complexity of federated SPARQL queries
is dramatically reduced.
Keywords: OpenLifeData, Bio2RDF, SADI, Semantic web services, SPARQL, SHARE, Sentient knowledge explorer, GalaxyBackground
Data integration is an ongoing challenge for biological
informaticians, and is often a study unto itself, with
numerous research groups worldwide approaching the
problem from a variety of perspectives [1]. Integration is
difficult for a variety of reasons, generally broken into
the three core issues of syntax, structure, and semantics
[2]. In addition, assigning and using unique identifiers
for data items and concepts is an essential requirement in
biology and elsewhere, and forms an equally disruptive
barrier to successful integration [3]. Syntactic barriers* Correspondence: markw@illuminae.com
1Centro de Biotecnología y Genómica de Plantas, Universidad Politécnica de
Madrid, Madrid, Spain
Full list of author information is available at the end of the article
© 2014 González et al.; licensee BioMed Centr
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.include issues such as binary or textual format, and
free-text or structured text; structural barriers involve such
things as flat-file formats, and XML Schema; semantic
barriers include inconsistent naming, naming conflicts
(multiple things with the same name, or multiple names for
the same thing) or insufficiently defined names; and
finally identification issues involve non-unique identifiers,
identifiers that can only be interpreted within a particular
scope (e.g. in the context of a given database), non-opaque
identifiers, and unstable or unpredictable identifiers.
The Semantic Web Initiative [4] has recently emerged
with technologies and frameworks aimed at solving at
least some of these problems. In particular, the Resource
Description Framework (RDF [5]) is an entity-relationship
data model that is, in principle, machine-readable andal Ltd. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
González et al. Journal of Biomedical Semantics 2014, 5:46 Page 2 of 12
http://www.jbiomedsem.com/content/5/1/46capable of representing any concept or data entity. RDF
also proposes several approved syntaxes, aimed at
maximizing machine-readability. Importantly, a query
language has been developed for RDF - the SPARQL
Protocol and RDF Query Language (SPARQL [6]) - and a
protocol for exploring and retrieving the RDF stored in
SPARQL endpoints on the Web is now well-established
and, in our experience, highly consistent from implemen-
tation to implementation.
With RDF as its core, the Linked Data initiative [7]
proposes several best practices that dramatically improve
the discoverability and integration of data on the Web.
First, all data entities and relationships must be identified
by a Uniform Resource Identifier (URI), which guarantees
uniqueness on a global scale. Second, URIs should
resolve to data and metadata using the most common
Web protocol, HTTP. Third, URIs should resolve to
useful/informative information, and resource providers
should offer this information in a variety of syntaxes
that can be selected by HTTP content-negotiation; in
particular, Linked Data resources should provide a means
of retrieving the data in the form of RDF. Finally, this RDF
should contain labeled links between that piece of data,
and other pieces of data also identified by resolvable URIs,
where the links indicate the relationship between the two
data elements and are, themselves, resolvable URIs. With
the ability to retrieve, share, and re-use these relationship
definitions, we begin to move towards the semantic
aspect of the Semantic Web.
Attempts to unify semantics have long been a focus of
biomedicine. The medical world has engaged for centuries
in the development of nosologies for naming and
classifying diseases. Within the bioinformatics community,
ontologies have become widely adopted in the past
decade, with the most prominent of these being the Gene
Ontology [8]. While such ontologies generally focus on
consistent and sensible human-readable names, they have
dedicated less attention to the unique identification of the
concept - names in ontologies are not guaranteed to be
globally unique, nor are concepts guaranteed to be
uniquely named, and can appear in multiple ontologies.
However, as these ontologies became encoded using the
rules of Linked Data, aspects of these problems were also
solved. Concepts became globally and uniquely identified
by resolvable URIs, and shared concepts could be referred
to by URI from one ontology to another. Moreover, mod-
ern ontologies use of the Web Ontology Language
(OWL) [9] description logic to define the meaning of
the links in Linked Data - effectively, the precise
nature of the relationship between one data entity and
another - enabled machines to automatically traverse these
linkages in a meaningful way.
Two key issues remained problematic, however, even with
Linked Data. First there was no widely-used mechanism toensure the stability and predictability of URIs representing
data and concepts - for example there was no way to
predict the URI for the Protein Data Bank record of
the Arabidopsis UFO protein, and even if this URI
were determined, it might not be the same from one
day to the next. As a result, individual Linked Data
resources could not reliably link out to other Linked
Data resources, because the URIs were unpredictable
and unstable. Data tended to remain siloed even in
the Linked Data world because links generally pointed
inward, rather than outward, as a result of this instability
and unpredictability. Second, the structure of what was
returned when the URI to a piece of data was resolved was
also not sufficiently predictable, and not consistent from
site to site, even for the same type of data. While Linked
Data is a significant improvement over XML Schema with
respect to the predictability of its data structures, there
were still no guidelines for how to arrange the relationships
between pieces of data, or even what those relationships
could/should be. It was these remaining problems
that became the focus of the Bio2RDF project.
Bio2RDF is an open source project that uses Semantic
Web technologies to create a sustainable infrastructure
for publishing biological data in a manner that eases the
task of data integration [10-12]. Bio2RDF scripts convert
heterogeneously formatted data (e.g. flat-files, tab-delimited
files, dataset specific formats, SQL, XML etc.) into RDF.
Bio2RDF follows a set of basic conventions to generate
and provide Linked Data which are guided by Tim
Berners-Lees design principles and a set of community-
established guidelines and practices. Specifically, entities,
their attributes and relationships are named using a
simple convention to produce Internationalized Resource
Identifiers (IRIs) that are highly predictable in their
structure, while statements are articulated using the
lightweight semantics of RDF Schema (RDFS) and Dublin
Core. Bio2RDF, however, did not reliably implement all of
the requirements of well behaved linked data, such as
HTTP content-negotiation, and had somewhat limited
expressivity in its relationships as a result of using the
semantics of RDF Schema. OpenLifeData provides
customized services over Bio2RDF SPARQL endpoints. Its
goal is to provide alternative user interfaces and application
programming interfaces to Linked Open Data beyond what
Bio2RDF currently does. OpenLifeData enriches Bio2RDFs
RDFS semantics to OWL expressivity, implements rich
HTTP content-negotiation, and utilizes query-rewriting to
resolve OpenLifeData IRIs and SPARQL queries against
the Bio2RDF SPARQL endpoints.
OpenLifeData data is accessed by users either via the Web,
through resolution of a URI to an HTML-representation of
its data content in their browser, or by the submission of a
SPARQL query to one of the OpenLifeData endpoints.
While the data-types and relationships within each endpoint
González et al. Journal of Biomedical Semantics 2014, 5:46 Page 3 of 12
http://www.jbiomedsem.com/content/5/1/46can be determined by manual exploration of the endpoint,
SPARQL queries must nevertheless be constructed
manually, and then posed against the appropriate endpoint
(s). Extracting OpenLifeData Linked Data, therefore, remains
a non-trivial task for even experienced bioinformaticians.
The 2014 release of OpenLifeData (based on Release 3 of
Bio2RDF) developed a scheme to provide a pre-computed
summary, or index, of the contents of each OpenLifeData
SPARQL endpoint in order to reduce the computational
load required for exploratory queries and enable new
applications. Summary metrics were pre-computed,
including number of triples, number of unique subjects,
number of unique predicates, number of unique objects,
list and frequency of unique types, list and frequency of
unique predicates, list and frequency of unique subject,
predicate-unique object tuples, list and frequency of
instances of subject type, predicate, and instances of
unique object type, and finally number of links to other
datasets. These indexes make it easier to determine the
structure and content of each OpenLifeData endpoint,
and moreover, the structures are highly consistent from
endpoint to endpoint.
Semantic Automated Discovery and Integration (SADI)
[13] is a set of design principles for exposing Web Services
in a manner that simplifies their integration with other
Semantic Web resources. Described simply, SADI Services
are Web-based tools that consume a particular type of
data, and return another type of data that is explicitly
related to that input. For example, you could send DNA
sequences to a SADI tblastx service, and it would
give you back Protein sequences that are connected to the
original DNA sequence by the hasProteinHomologyTo
relationship. Expressed more concretely, SADI services
consume and produce RDF data, where instances of an
input OWL class, represented in RDF, are submitted to the
service by HTTP POST, and RDF instances of an output
OWL class are returned in response. The constraint SADI
places on these data is that the output class must be a
specialization of the input class such that the input
instances are related to the new service-generated
data nodes through ontologically-defined relations.
The result of chaining SADI services together, therefore, is
an unbroken network of well-formed and ontologically-
grounded Linked Data, which can be explored and
traversed using standard tools such as SPARQL.
SHARE (Semantic Health And Research Environment)
[14,15] is a SADI client that combines: a registry of the
input and output OWL classes for all known SADI
services, a service discovery and invocation API, an
automated workflow design and enactment engine, and a
logical reasoner. While other components of SHARE are
discussed in detail in the previously-referenced papers, it
is relevant to this manuscript that service discovery is
achieved by indexing all known SADI services in theSHARE registry, such that input types, output types, and
the properties that link them, are all rapidly searchable.
This registry is made publicly available as a SPARQL
endpoint, where the data model of the registry follows that
of the myGrid serviceDescription [16] ontological class.
The similarity between the input-type, property,
output-type signature of a SADI Web Service, and the
subject-type, predicate, object-type indexes of the
OpenLifeData endpoints provides a natural mechanism
through which these two initiatives could be combined,
such that OpenLifeData becomes discoverable and access-
ible via SADI. At the NBDC/DBCLS BioHackathon 2013
we proposed that it should be possible to automatically
generate (a) formalized definitions of SADI services, (b)
SPARQL queries to retrieve the service-appropriate
data from the OpenLifeData endpoints, and (c) the
SADI service code to serve that data, all by simply parsing
the OpenLifeData indexes. This manuscript describes
the realization of that vision. For the remainder of the
manuscript will use the short name OpenLifeData2SADI
as a convenient way of referring to the project as a whole.
Implementation
OpenLifeDatas content summaries are provided as RDF
[17]. We utilize the Jena [18] Java libraries to parse the
[Subject type - predicate - Object type] (SPO) triple
patterns in these indexes, and additional indexes created
specifically for this project, to generate sets of three
configuration files used by OpenLifeData2SADI to
serve each data-type within OpenLifeData. The first
file contains two OWL ontological classes, describing the
input and output data for the service. These ontologies
are published on the Web such that the input and output
class URIs are resolvable through HTTP GET. The second
file is a summary containing the URIs of the input and
output OWL Classes for that service, the human-readable
class names, and the URI and name of the RDF predicate
that links the two classes. Finally, a third file is generated
that contains a SPARQL query template that, when
filled-in with data and executed against the appropriate
OpenLifeData endpoint, retrieves the output data
appropriate for that service. We now describe in additional
detail how each of these steps is undertaken.
Parsing the indexes
Each OpenLifeData dataset is served from its own
SPARQL endpoint, and contains data within a specific
namespace (e.g. sgd for Sacharromyces Genome Database,
or ncbigene for the NCBI Gene). The content of each
endpoint has been pre-indexed, using VoID (Vocabulary of
Interlinked Datasets), where the index captures all unique
data-type/predicate/data-type triples for that endpoint. For
example, one of the index triples for the HGNC endpoint
is Gene Symbol/x-omim/Gene. The Java collector first
González et al. Journal of Biomedical Semantics 2014, 5:46 Page 4 of 12
http://www.jbiomedsem.com/content/5/1/46parses the information provided in the OpenLifeData in-
dexes to obtain two parameters: SPARQL Endpoint URL
and Namespace - effectively, the location of each dataset,
and the domain/scope of that dataset. In principle, each
endpoint could be interrogated to retrieve all SPO patterns
by executing the following SPARQL query:This would be sufficient to gather all information
necessary to create SADI services that output resource
nodes (URIs); however, at this time, OpenLifeData does
not index the large component of data that exists as
literal values (numbers and strings). As such, to be fully
comprehensive, we execute an iterative set of queries
over each endpoint which gathers all subject-types, then
the predicates associated with each subject-type, and
finally the object type that is connected by each
predicate, including the cases where the object is a
literal value. To further enrich the semantics, we then
do federated queries over multiple end-points in an
attempt to determine more specific details about the
object types. For example, the omim dataset includes links
to entities in the hgnc dataset, but considers all of these to
be Resources - a generic term for something that exists
in another dataset. Through our federated queries, we can
determine that these hgnc Resources represent, for
example, Genes, or SNPs, and thereby we are able to
construct semantically richer descriptions of what the
SADI services will consume/produce.
The queries we execute (in template form) are as
follows:
Get Subject-typesGet Predicate-typesGet Object-typesGet Data-typesFederated Query for object typesConfiguration file creation
After retrieving all SPO patterns for each endpoint,
OpenLifeData2SADI then builds the files needed to
automatically configure the SADI Service; each SPO
triple pattern becomes its own Service, where the service
consumes data of the Subject type, and returns all
triples from that endpoint matching the SPO pattern
for that Subject. For each Service, three configuration
files must be created:
Input and output ontology classes
Using the Java OWL API [19] we create ontology classes
based on the pattern of each SPO in each endpoint;
these classes describe the OWL properties required
for/provided by the Input and Output of the service
respectively.
In OpenLifeData URIs the class/predicate identifier, and
namespace are separated either by the hash (#) or colon (:)
characters. Since we intend that OpenLifeData2SADI
services should make sense to both machines and
humans, an attempt is made to construct a human-readable
name for each class and property. The code first attempts
to resolve the URI to retrieve its rdf:label, and this label, if
available, is used as the human readable class/property
name in the final configuration file for that service. If no
label can be retrieved, the hash or colon separator is used to
split a name from the rest of the URI and this is used as the
human-readable name. While not entirely successful, this is
González et al. Journal of Biomedical Semantics 2014, 5:46 Page 5 of 12
http://www.jbiomedsem.com/content/5/1/46our best attempt at automatically building services that have
accurate human-readable descriptions.
The input class (generically called Subject_Class in
this discussion) is defined in OWL, simply, as the rdf:
type of the Subject of the SPO triple, as defined by
OpenLifeData. It contains no other axioms or restrictions.
The class representing the output of the service is then
defined as the Subject_Class with an additional property
defined by the Predicate of the SPO triple, where the
range of that predicate is defined by the Object data-type
component of the SPO triple. This is represented in
Manchester Syntax as follows:Logically, therefore, the Service output is a subclass of
the Service input (Subject_Class), as is typical for all SADI
services. A similar approach is taken for OpenLifeData
predicates with Literal value ranges. The resulting ontology
is then saved to the local filestore with the naming
convention ./<namespace>/<subject_predicate_object > .owl
and this is published on the Web such that the URIs in that
ontology resolve correctly.
In a second phase, the process above is duplicated, but
in this second iteration, the owl:Inverse of the Predicate
is used, and Subject and Object are reversed. This allows
us to automatically create SADI services that traverse
the OpenLifeData in either orientation, and thus behave
in a manner akin to conventional SPARQL, where either
Subject or Object may be bound in a constraint clause
of the query.
Configuration file
This file contains parameters required to properly con-
figure the SADI service such that it (a) serves the
appropriate data using the appropriate descriptors,
and (b) provides its own metadata in a form that is
comprehensible to humans. The Java code that creates
these configuration files requires a single argument - the
root URL to the final location of the ontologies (created
above) on the Web. The configuration file contains the
following parameters:
 INPUTCLASS_NAME: The name of the input
class after removing the namespace. In cases
where the class name is opaque or numerical,
an attempt is made to resolve the class URI to
its full OWL-RDF definition, and retrieve the
label property, such that the class name is
human-readable.
 INPUTCLASS_URI: The URI of the input class
 OUTPUTCLASS_NAME: The name of the output
class. This is the same for all services, but conflicts
are avoided since each output class name exists in aunique namespace (ontology); every output class is
named #ServiceOutput.
 OUTPUTCLASS_URI: The Web-resolvable URI of
the output class. This URI is generated by the con-
catenation of the root URL, the namespace of the
OpenLifeData dataset, the path and name of the
ontology file, and the generic class-name
#ServiceOutput.
 PREDICATE_NAME: The name of the predicate. As
with the input class name, an attempt is made to
retrieve the human-readable label of the predicate if
it appears that the predicate is somehow opaque or
numerical.
 PREDICATE_URI: The URI of the predicate.
 ORIGINAL_ENDPOINT: The URL of the original
endpoint indexed by OpenLifeData.
 GENERIC_ENDPOINT: The endpoint that should
be queried by the SADI service using SPARQL.
OpenLifeData is duplicated in several locations; the
preferred location to query would be the value of
this field.
 OUTPUT_CLASS: The rdf:type of the data that will
be added during the service execution.
The resulting file is written to the local filestore in
the same folder as the ontology file, with the naming
convention./<namespace>/<subject_predicate_object > .cfg.SPARQL query file
The third file generated by OpenLifeData2SADI contains
the SPARQL query that should be executed within the
business logic of the SADI Web service. The content of
this query is service specific, but follows the pattern:where < PREDICATE_NAMESPACE > is replaced with
the namespace of the predicate provided by the SPO,
and < predicate > element is replaced with the local
name of the predicate (the component after the # or :
character). %VAR is left in the query template, and will be
substituted by the SADI service at run-time, based on the
input data.
In the case of the SPARQL query, there is no difference
between the forward Predicate and the inverse predicate.
Inverse predicates do not exist in the OpenLifeData
SPARQL endpoints, but rather are simply defined in the
OWL logic that defines the entities and relationships
in those endpoints. As such, we rely on logical rea-
soning to determine that an inverse invocation can be
solved equally well by a forward query; thus the
González et al. Journal of Biomedical Semantics 2014, 5:46 Page 6 of 12
http://www.jbiomedsem.com/content/5/1/46query that serves both forward and inverse services is
identical.
SADI service implementation
To serve the OpenLifeData data, a single Perl script
using the standard SADI::Simple code libraries act as the
SADI Service Daemon for all services. The script listens
for HTTP calls to URLs of the form:In this URL, SADI is the name of the OpenLifeData2SADI
Service script, while the additional path information
(namespace and service name) are used as keys to access
the configuration file and SPARQL query file appropriate
for that service, as described above. The SADI Perl script
parses these files, and configures itself to be capable of:
HTTP GET:
 Returning the complete service interface definition,
represented as an owl:Individual of the mygrid
ontology ServiceDescription Class, as per the SADI
design patterns.
HTTP POST:
 Parsing the input data, which arrives in RDF syntax
as owl:Individuals of that services Input OWL Class.
 Executing the SPARQL query, extracted from the
configuration files, against the correct OpenLifeData
endpoint for that service, using each of the
incoming owl:Individuals to fill the query variables
for that particular invocation.
 Constructing owl:Individuals compliant with the
class definition of that services Output OWL Class,
and passing this data back to the caller.
This is all accomplished using the normal SADI service
template [13]. The key difference is that the Services
interface template retrieves its values from a dynamic
look-up of data from the configuration files, rather than
being hard-coded into the service.
Service registration
Two scripts were written to automate the registration and
deregistration of the full suite of OpenLifeData2SADI.
The registration code and deregistration code are available
in the Perl folder of the GitHub project (see Availability
and Requirements section). They operate by querying all
of the configuration files (for registration) or all of the
existing SHARE registry entries (for deregistration) and
triggering the registry to call GET on each service end-
point. The registry functions by creating a service if it findsa valid service description document at that endpoint, or
deregistering a service if it does not. Therefore, in the case
of registration, the SADI script should be installed on the
designated service endpoint first, in order to respond to
the registry calls. In the case of deregistration, the SADI
Service code should be removed prior to running the
deregistration script.
Workflows of Bio2RDF services
The establishment of the OpenLifeData2SADI suite of
services made it possible to more easily explore the
interconnections between OpenLifeData endpoints. In
order to generate an exhaustive list of these connections,
to assist third-parties in building novel exploration tools,
the following query was issued which creates a list of all
valid service-output to service-input pairs within the set of
OpenLifeData services (note that the PREFIX directives in
this example are shared for all queries in this manuscript,
and will not be repeated in later examples):Since this, in principle, represents the complete set of
potential workflow connections that could be constructed
within these services, we chose to formally represent the out-
put of this query as an abstract workflow template, using the
Open Provenance Model for Workflows (OPMW) Abstract
Template ontology [20,21]. Those interested in generating a
copy of this abstract template for their own exploration can
simply execute the OpenLifeData2SADI2OPMW.pl script in
the GitHub project, which will generate a copy based on the
contents of the public SHARE registry. A copy generated
at the time of writing is also available in the projects Git
repository (see Availability section).
Provenance
Provenance of data is becoming increasingly import-
ant as datasets get larger, more dispersed over the
Web, and as data gathering and analyses become more
automated. The OpenLifeData2SADI project has selected
the NanoPublication [22] conventions and model for
González et al. Journal of Biomedical Semantics 2014, 5:46 Page 7 of 12
http://www.jbiomedsem.com/content/5/1/46passing provenance information to the client, along
with the results of their service invocation. As with
all SADI services, this is achieved through normal
HTTP content negotiation. If the client passes an Accept:
application/n-quads HTTP header, the OpenLifeData2-
SADI service will respond by returning three named
graphs, constructed according to the NanoPublication
specifications. One graph contains the service output,
the second contains the metadata describing the service
and, for example, its name, description, and URL, and the
third describing the date and time the NanoPublication
was generated.Results and discussion
At this time there are more than 22,000 OpenLifeData2-
SADI services from 26 independent endpoints, and more
will be generated as OpenLifeData expands into new data-
types. These services are discoverable through simple
queries against the SHARE registry, or through a variety
of client applications. We now demonstrate the utility
of the OpenLifeData2SADI application by a series of
walkthroughs, where the process of discovery, execu-
tion, and chaining-together of SADI-wrapped OpenLi-
feData services is described in more detail and
compared to the interrogation of OpenLifeData dir-
ectly via SPARQL.
We will start with a small fragment of RDF data repre-
senting a Human Gene Naming Committee (HGNC) Gene
Symbol:Discovery of OpenLifeData2SADI services
Discovery of services is generally accomplished by
executing a SPARQL query against the SHARE registry
[23]. Discovery of the OpenLifeData2SADI services
can be accomplished by a wide variety of query structures,
but in this example we will query for services that
consume OpenLifeData HGNC Gene Symbols and have
approved-name somewhere in the services descriptive
text. The query is:This returns a single result, which is the URL for ser-
vice hgnc_vocabulary_Gene-Symbol_hgnc_vocabular-
y_approved-name_string.Invocation of OpenLifeData2SADI services
Invocation of a discovered OpenLifeData data retrieval
service simply consists of sending the data to the service
endpoint using HTTP POST. This can be accomplished
with widely available tools such as Unix curl. Below, the
sample HGNC Gene Symbol record described earlier, is in
the file sampledata_hgnc.rdf. Curl is then used to invoke
the service, as follows:the result of this service invocation is the output data,
containing the approved name from the OpenLifeData
HGNC endpoint:Client applications
We do not expect that our users will typically discover
or access OpenLifeData2SADI services via SPARQL queries
or the command-line. More commonly, the same discovery
and invocation interactions presented in their raw form
above are presented to the user graphically via one of the
SADI plug-ins or client applications; nevertheless, discovery
and invocation happens the same way as described above,
regardless of the client. We believe that this simple
standardization provides a very low barrier-to-adoption for
new users and tool-developers who wish to gain access to
the myriad OpenLifeData resources.
There are a wide range of graphical clients capable of
executing SHARE registry queries in response to the users
contextual needs, or in some cases, fully automatically.
We will now present several of these applications,
showing how OpenLifeData services can be accessed and
chained-together within these diverse clients.
The list of services we will use for this demonstration
are:
(1) Gene-Symbol_approved-name
(2) Gene-Symbol_x-omim
(3) Gene_gene-function
(4) Gene_article
(5) Gene_x-mgi
(6) Gene_x-uniprot
Services (1) and (2) link an HGNC resource to its
approved name and a linked OMIM entry, services
(3), (4), and (5) link an OMIM resource to a gene
function description, its associated PubMed entries,
González et al. Journal of Biomedical Semantics 2014, 5:46 Page 8 of 12
http://www.jbiomedsem.com/content/5/1/46and its associated Mouse Genome Informatics (MGI)
Gene, while service (6) links an MGI gene identifier to its
associated UniProt identifier. The template workflow
connecting these services in a biologically-meaningful way
is shown in Figure 1.
IO informatics knowledge explorer
The SADI plug-in to the IO Informatics Knowledge
Explorer [24,25] (KE) provides menu-driven access to
the SHARE registry through a context menu that
appears when right-clicking a piece of biological data on
the KE canvas. In Figures 2A and B we show the same
sample data from the examples above, loaded into the
Knowledge Explorer. A right click reveals the Find
SADI Services menu option, which then initiates a
search based on the data-type that was selected. Here we
have chosen the approved-name service from the resulting
services menu by clicking the selection box. In Figure 2C
the approved name for HGNC:7 has been added as new
information to the canvas. Figure 2D shows the final result
after a series of OpenLifeData2SADI services have been
executed, following the workflow path in Figure 1.
SHARE
The SHARE client [14] is one of several SADI clients
capable of chaining multiple services together. We will
utilize this client to emphasize the fact that, as a result
of exposing OpenLifeData data as SADI services, it is no
longer necessary to know which data exists in which of
the 26+ OpenLifeData SPARQL endpoints. In this use
case we imagine that a researcher has studied some
human condition, has narrowed-down to a specific gene
list of interest, and now wants to know more about those
genes, their functions, and whether or not the proteins
might be suitable drug targets based on known proteinFigure 1 A workflow of OpenLifeData2SADI services, numbered
as in the list of services above, and the output data that
will result.information from their respective Mouse homologues.
Diagrammatically, the workflow is as shown in Figure 2
(using the service numbers and starting-data from above).
The SHARE interface is at http://dev.biordf.org/
cardioSHARE. SHARE exposes SADI Web Services as if
they were combined into a single, global, SPARQL
endpoint. The SHARE SPARQL query that will invoke
the workflow from Figure 2 is:Note that it was not necessary to know which endpoint
contained which data elements, nor to use service
queries to federate over these endpoints. This is important
when considering the complex structure of federated
SPARQL queries, where it is necessary to know the location
of the endpoint, and in some cases, the named-graph
that must be queried. For example, the equivalent
SPARQL query over the OpenLifeData endpoints, would
be as follows:As such, we believe that OpenLifeData2SADI makes
the exploration across the more than 20 OpenLifeData
data endpoints considerably more straightforward.
Galaxy
The Galaxy [26] workflow environment is very popular
among life scientists, yet to date, we know of no Galaxy
workflow that accesses OpenLifeData or Bio2RDF data.
This is likely due to the lack of life science tools and
services that deal with RDF-formatted data at all, and
the lack of a straightforward template for mapping data
between a workflow and a SPARQL query (and back
again). The SADI Galaxy plugin [27,28] provides SADI
Figure 2 Discovery and invocation of OpenLifeData2SADI services using the SADI plugin to the Sentient Knowledge Explorer. A. Data
nodes respond to a right-click with a context menu item Find SADI Services. B. a set of services capable of consuming nodes of that type are
discovered and presented in a menu-like manner. C. the result of selecting the approved-name service from the menu. D. the output after
iteratively invoking all 6 of the services from the example service list (effectively, manually executing the workflow in Figure 1).
González et al. Journal of Biomedical Semantics 2014, 5:46 Page 9 of 12
http://www.jbiomedsem.com/content/5/1/46services as normal Galaxy tools [29], thus making it
straightforward to chain OpenLifeData services together
in the Galaxy environment. Figure 3 shows the same
workflow as above, created within the Galaxy workbench.
In order to reproduce the workflow, it is necessary to
create, for yourself, a user on our Galaxy server [30] and
import the history and workflow [31,32]. The first item of
the history can be used as the input to the workflow to
reproduce the results reported here.
Limitations and scalability
The use of SADI to expose data in SPARQL endpoints
clearly adds a certain amount of overhead with respect to
both execution-time and computational load; however, it
is difficult to directly compare the two scenarios because(a) speed and load depend on the client, and web service
clients are significantly different from one another, and
from SPARQL clients; (b) the time (and knowledge)
required to manually construct each desired SPARQL
query, compared to the automated dynamic discovery
of appropriate SADI services, is not considered in a
head-to-head comparison of their respective execution
times, and (c) It is considerably easier to optimize the
execution plan for a SPARQL query, versus a service
workflow. Nevertheless, a direct comparison of the
federated query entered into the SHARE client (above)
versus the equivalent federated SPARQL query entered
into the Virtuoso web-based query interface, showed
execution times of 34-39 seconds for SHARE compared
to 2-3 seconds for Virtuoso. Thus, while the overhead of
Figure 3 A workflow of OpenLifeData2SADI services in the Galaxy workbench environment. This workflow is an instantiation of the
template workflow in Figure 1.
González et al. Journal of Biomedical Semantics 2014, 5:46 Page 10 of 12
http://www.jbiomedsem.com/content/5/1/46the Web Service solution is, in this case, significant, we
feel it is still within reason for a user-interface, given the
difficulty a user would face in creating and debugging
SPARQL queries. Similarly, we would argue that the
dynamically-generated, menu-driven interface provided by
the KE plugin is orders of magnitude faster than the user
having to manually type each SPARQL query into the KE
SPARQL interface.
Conclusions
In this work we attempted to address four distinct
problems:
The first is that, since most bioinformatics workflows
combine a variety of different kinds of Web Services
together with local processors to execute the data retrieval
and analysis, it is highly desirable to expose SPARQL
endpoints in a discoverable manner akin to Web Services.
Current approaches to exposing SPARQL endpoints as ser-
vices result in services with low discoverability and incom-
plete (or even absent) descriptions of what will be returned
from a service invocation. By exposing the contents of
OpenLifeData as SADI Web Services, it becomes straight-
forward to integrate these endpoints into popular workflow
environments such as Taverna [33] or Galaxy, and more
importantly, the service interface, and the data that passes
through the service, is explicitly semantically defined.Second, the discovery of data in Bio2RDF (or any
SPARQL endpoint) has, historically, required considerable
prior knowledge and often trial-and-error exploration
until an appropriate SPARQL query has been constructed.
By enhancing the semantics of Bio2RDF, and indexing all
of its semantically rich entity-relationships in the recent
release of OpenLifeData, it became possible to expose all
of this data as SADI services that are registered in the
SHARE registry. It is now straightforward to discover,
through highly predictable SPARQL queries, which
Bio2RDF endpoints contain data of interest, and what the
nature of that data is. Moreover, because the registry
query is predictable, it is trivial to make a comprehensive
map of all entity-to-entity connections within the entire
OpenLifeData universe, even spanning between separate
OpenLifeData endpoints, and this was made publicly
available as an abstract workflow template following
the Open Provenance Model for Workflows (OPMW)
ontology [20,21].
Third, SPARQL endpoints are a highly granular approach
to bioinformatics data publishing akin to publicly exposing
the SQL interface to a relational database. Historically,
there have been very few core bioinformatics data
hosts who allowed such fine-grained access to their
databases, primarily because of the potential for users
to submit resource-hungry queries (either accidentally,
González et al. Journal of Biomedical Semantics 2014, 5:46 Page 11 of 12
http://www.jbiomedsem.com/content/5/1/46or on purpose). Indeed, this has already been identified as
a problem with respect to queries against the UniProt
SPARQL endpoint [34]. While losing the potential for
query optimization, exposing RDF data via SADI Services
has several advantages over exposing RDF triple-stores as
SPARQL. First, because SADI services can be executed in
a multiplexed manner, and asynchronously, bulk data
requests could be easily managed over the available
compute-resources at the host site. This is difficult to
achieve with existing SPARQL endpoints. Second, because
the Service exposes the RDF data via a wrapper around a
simple SPARQL query that is guaranteed to be correct, it
becomes impossible for the data request to be malformed
or resource-consuming (beyond what the provider allows).
Thus, the data provider is better-shielded from misuse
or abuse.
Finally, in these early days of Linked Data publishing in
the life sciences, Linked Data resources can, justifiably,
change as the data providers adapt their models, publishing
procedures, publisher metadata and even endpoint
locations and access restrictions to accommodate new
behaviors or concerns. As such, SPARQL queries that are
successful one day, may not be successful the next. By
serving OpenLifeData through SADI Services, which are
dynamically discovered by clients that automatically
configure themselves for correct access, we provide an
API that is much more resilient to underlying change
than a pure SPARQL interface would be; updating the
OpenLifeData2SADI behavior is simply a matter of
changing the configuration file, or (in the worst case)
updating one simple piece of code, compared to all users
of the data being required to update their own software.
For all of these reasons, we feel that the availability
of OpenLifeData2SADI will dramatically enhance the
access to, and utility of, OpenLifeData for all biologists.
This, in turn, will hopefully spur a more rapid adoption of
both Linked Data and Semantic Web Services throughout
the life science data provider community.
Availability and requirements
The Java code and Perl scripts for this project are available
on the Wilkinson laboratory Github, at https://github.
com/wilkinsonlab/OpenLifeData2SADI under the Apache
version 2 license. Indexing the OpenLifeData endpoints
can be executed in either Java or Perl. Ontology and
configuration file creation requires Java 6, Jena, and
the OWL API. Serving the data as Web Services is
accomplished by running the indexer (in Java or Perl),
building the configuration files, and deploying the SADI
Perl script, with core dependencies on the SADI::Simple
and RDF::Query::Client libraries from CPAN. The repository
also contains the output files from the most recent
execution of the OpenLifeData2SADI2OPMW.pl script, and
these are free to use in any way.Abbreviations
DBCLS: Database center for life science; HTTP: Hypertext transport protocol;
NBDC: National bioscience database center; OPMW: Open provenance model
for workflows; OWL: Web ontology language; RDF: Resource description
framework; SPARQL: SPARQL Protocol and RDF Query Language;
SADI: Semantic automated discovery and integration; SPO:
Subject-predicate-object; VoID: Vocabulary of interlinked datasets.
Competing interests
The authors declare they have no competing interests.
Authors contributions
MDW and MD conceived of, and planned, the project. MD, AC, and JCT
created the indexes of the OpenLifeData dataset, and enriched the
semantics of the underlying Bio2RDF data such that it could be optimally
represented using SADI Semantic Web Services. ARG wrote the Java
codebase and created the configuration files. MDW wrote the Perl SADI
service script and the Perl version of the OpenLifeData indexer. MEA created
the SADI Galaxy plugin, tools, and workflow. AG updated the Knowledge
Explorer plugin as depicted in Figure 2. All authors read, corrected, and
approved the manuscript.
Authors information
MD is the lead investigator of the OpenLifeData project and is Associate
Professor at Stanford University. MDW is the founder and leader of the SADI
project, Issac Peral Distinguished Researcher at the Center for Plant
Biotechnology and Genomics, and Director of the FBBVA-UPM Chair in
Biological Informatics at the Universidad Politécnica de Madrid. JCT, AC,
AG, MEA and ARG are, or have been, researchers in these collaborating
laboratories during the execution of this project.
Acknowledgements
MDW is supported by the Isaac Peral and Marie Curie COFUND Programmes
of the Universidad Politécnica de Madrid, Centre for Plant Biotechnology and
Genomics UPM-INIA. ARG is supported by the Isaac Peral programme, UPM.
Portions of this work have been funded by the Fundación BBVA. We wish to
express deep gratitude and appreciation to the National Bioscience Database
Center (NBDC) and Database Center for Life Science (DBCLS) in Tokyo, Japan,
and Dr. Toshiaki Katayama who has been organizing annual BioHackathon in
Japan since 2008; The idea for OpenLifeData2SADI emerged as a result of
the face-to-face contact facilitated by these extraordinary events.
Author details
1Centro de Biotecnología y Genómica de Plantas, Universidad Politécnica de
Madrid, Madrid, Spain. 2Center for Biomedical Informatics Research, Stanford
University, Stanford, CA, USA. 3Department of Biology, Carleton University,
Ottawa, ON, Canada. 4Genomic Resources Group, University of the Basque
Country (UPV-EHU), Bilbao, Spain.
Received: 17 July 2014 Accepted: 7 November 2014
Published: 19 November 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Malone et al. Journal of Biomedical Semantics 2014, 5:25
http://www.jbiomedsem.com/content/5/1/25
DATABASE Open Access
The Software Ontology (SWO): a resource for
reproducibility in biomedical data analysis,
curation and digital preservation
James Malone1, Andy Brown2, Allyson L Lister2, Jon Ison1, Duncan Hull2, Helen Parkinson1 and
Robert Stevens2*
Abstract
Motivation: Biomedical ontologists to date have concentrated on ontological descriptions of biomedical entities
such as gene products and their attributes, phenotypes and so on. Recently, effort has diversified to descriptions of
the laboratory investigations by which these entities were produced. However, much biological insight is gained from
the analysis of the data produced from these investigations, and there is a lack of adequate descriptions of the wide
range of software that are central to bioinformatics. We need to describe how data are analyzed for discovery, audit
trails, provenance and reproducibility.
Results: The Software Ontology (SWO) is a description of software used to store, manage and analyze data. Input to
the SWO has come from beyond the life sciences, but its main focus is the life sciences. We used agile techniques to
gather input for the SWO and keep engagement with our users. The result is an ontology that meets the needs of a
broad range of users by describing software, its information processing tasks, data inputs and outputs, data formats
versions and so on. Recently, the SWO has incorporated EDAM, a vocabulary for describing data and related concepts
in bioinformatics. The SWO is currently being used to describe software used in multiple biomedical applications.
Conclusion: The SWO is another element of the biomedical ontology landscape that is necessary for the description
of biomedical entities and how they were discovered. An ontology of software used to analyze data produced by
investigations in the life sciences can be made in such a way that it covers the important features requested and
prioritized by its users. The SWO thus fits into the landscape of biomedical ontologies and is produced using
techniques designed to keep it in line with users needs.
Availability: The Software Ontology is available under an Apache 2.0 license at http://theswo.sourceforge.net/; the
Software Ontology blog can be read at http://softwareontology.wordpress.com.
Background
We report on the Software Ontology (SWO) [1,2], an
ontology for describing the software used within compu-
tational biology, which includes bioinformatics resources
and any software tools used in the preparation and main-
tenance of data. Development of the SWO is motivated
by the growing interest in the recording and reproducibil-
ity of biomedical investigations [3,4]. Reproducibility is as
important for computational investigations of data as it
*Correspondence: robert.stevens@manchester.ac.uk
2School of Computer Science, University of Manchester, Oxford Road,
Manchester, M13 9PL, UK
Full list of author information is available at the end of the article
is for investigations in the wet laboratory [5,6]. In order
to understand research results presented from data analy-
sis investigations or perform new analyses based on these
results, it is important to know whence the data came,
how they were analysed and with what tools. In a recent
Science paper, Peng [7] suggested that making research
that uses computational methods reproducible requires
much greater attention to detailing the software as part
of the experimental process. Gentleman et al [5] state the
need for reproducibility by combining analysis code with
the data; e.g., using BioConductor packages to analyze
MicroArray data. However, for reproducibility, the version
of the BioConductor packages, R and any associated soft-
ware that may have an influence on the outputs would
© 2014 Malone et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited.
Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 2 of 13
http://www.jbiomedsem.com/content/5/1/25
need to be known - and even the hardware upon which it
was run, as all of these can have an influence on the results
obtained.
The growing use of workflows as a means of analyz-
ing biological data [8-10] and as a means of recording
and exchanging method [11] has provided one avenue for
the recording of method. There has also been a move to
automatically describe the provenance of computations
(including the actual run of a workflow), and ontologies
have been provided to support this recording [12]. An
ontology such as the SWO provides the vocabulary and
identifiers for the software aspects of such automatically
recorded provenance.
As well as the reproducibility angle, describing software
and the data it consumes and produces is important
for search for software and construction of applica-
tions and workflows. Registries such as BioCatalogue [13]
describe Web services used in bioinformatics accord-
ing to data consumed and produced, and the func-
tional units of the services involved and so on. These
semantic descriptions can be then used for search and
retrieval. Similarly, automated or semi-automated work-
flow construction depends on descriptions of the services
[14].
An ontology of software can have impact in all of
these areas by providing the means to describe soft-
ware used, the data consumed and produced, its versions
and so on. The scope of SWO is thus broad; it needs
to cover not only bioinformatics, but any tools used in
the management, analysis and presentation of biologi-
cal data. Prima facie, the SWO needs to cover, but is
not limited to, this range of software, and descriptions
of its objectives (for what it is used), the data it con-
sumes and produces, the algorithms it implements to
achieve these objectives, its version, and some aspects
of its project details. This software would include, for
example, spreadsheets, word-processors, databases, as
well as the bespoke desktop and services on the Web
used by bioinformaticians within computational biology.
A rich description of such a broad range of software
used in life science investigations implies a similarly broad
scope for an ontology of software. To date, however,
attempts to produce such an ontology have not been
convincing, although some promising efforts have been
made:
 DOAP (Description Of A Project) [15] describes a
software project (home page, developers, language,
etc.), rather than the software itself. There is an
overlap in scopethe home page and developers are
features that the SWO will need, but the SWO aims
to model the software artifact itself, not the project.
 OWL-S [16] is a domain neutral and general
mechanism for describing Web services, such that
they can be discovered, composed and invoked. Of its
three aspects, OWL-Ss profiling mechanism comes
within the scope of the SWO, but the grounding for
the automatic invocation of a Web service is outside
the SWOs scope. In addition, its focus on only Web
services precludes its model from our use.
WSMO [17] (Web Services Modeling Ontology) has
a similar remit and again is not suitable for the
SWOs aims.
 Ontologies of data mining tools cover much of the
same ground as the SWO, but are obviously restricted
to data mining. As data mining tools are used within
bioinformatics and computational biology, we aim to
include such descriptions into the SWO, and we
incorporate portions of DMOP [18] into the SWO.
 The Ontology of Biomedical Investigations (OBI) [19]
has a broad remit of enabling scientists to accurately
and precisely describe how a biomedical investigation
was planned and performed, including devices,
samples, sample preparation, etc. As its remit is
broad, it has also spun out ontologies such as the
Information Artifact Ontology (IAO) [20] to allow
descriptions of information such as digital documents
associated with an investigation. Descriptions of
software used in an investigation could fit within
OBIs remit, but the SWO has been deliberately kept
separate as its scope is greater than biomedical
investigations themselves. However, the SWO is built
to be broadly inline with OBI and IAO.
 EDAM [21] is a vocabulary for describing data related
concepts in bioinformatics including types of data,
identifiers, formats, operations and topics of broad
biological areas. EDAM is limited to these concepts
and as such does not cover software and related
software items like algorithms and licenses.
The SWO aims at a wider scope of describing any
software used in the pursuit of computational
biology; this includes spreadsheets, database systems,
XML parsers, ontology development environments
and the like. EDAM is now a subset of the SWO and
has been integrated into the SWO.
 The Bioinformatics Resource Ontology (BRO) [22]
has a similar remit to EDAMresource description
and discovery. However, it models resources very
broadly, capturing concepts concerning research
infrastructures, people, funding etc. It also conflates
various aspects of software, such as algorithm and
implementation.
 The myGrid Ontology [23], though focusing on
bioinformatics software resources, makes many of
the same distinctions as the SWO, but has not been
actively maintained for a long-time. EDAM covers
the concepts included in myGrid and, with the SWO,
supersedes and replaces the myGrid ontology.
Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 3 of 13
http://www.jbiomedsem.com/content/5/1/25
Bio-ontologies now cover a broad range of life science
entities from biological sequences (SO) [24] to the func-
tional attributes of gene products (GO) [25], and from
cells (CTO) [26] to gross anatomy (Uberon) [27] and
phenotype (PATO) [28]. We now also have ontologies
describing small molecules and their roles that participate
in many biological processes (ChEBI) [29]. Added to this
are descriptions of biomedical investigations such as the
OBI [19], the Experimental Factor Ontology (EFO) [30]
and the BioAssay Ontology [31]. The SWO fits neatly and
independently into this ontology landscape in its role as
an ontology concerned with the description of resources
used in the investigation of biomedical phenomena, rather
than the biomedical phenomena themselves.
User stories
The principal use for the SWO is in the description of
resources used in storing, managing and analyzing data.
Our SWO workshops produced a broad range of what
in agile development are termed user stories for the
SWO [32] and we highlight a few here:
 Describing the software used in the analysis of gene
expression data. With the development of
next-generation high throughput sequencing
techniques, a slew of new software packages for
analyzing this data has emerged. Understanding how
results have been produced requires knowledge of the
software used. This is an important consideration for
the European Bioinformatics Institute (EBI), which
hosts such data resources as ArrayExpress [33] where
describing how data were analysed is important.
 The eagle-I project [34] concerns the collection of
descriptions of biomedical resources and services
available at various sites in North America such that
consumers can find those relevant to their work. The
eagle-I consortium require descriptions of data types
and formats, software names, programming languages
and licensing information to describe these resources.
 The European consortium under the BioMedBridges
project [35] wish to collect descriptions of Web
services and software tools used both within the
consortium and more widely in Europe in a Tool and
Service Registry [36]. They wish to answer user
questions about software attributes such as data types
and formats, licenses, and developer and source code
information and function. Queries require structure
for faceting such that higher-level categories in the
ontology can be used to drill down to subsets of
interest.
 The British Library and UK National Archives
require software descriptions to assist in the curation
of digital artifacts for preservation purposes. Data
curated using specific versions of a piece of software
can produce varied results, so being able to describe
attributes such as version and data formats are of
great importance.
Materials andmethod
The Software Ontology has adapted agile software engi-
neering methods into the ontology engineering pro-
cess [1]. Agile methods offer a number of principles that
aim to keep users involved in the process of developing
software and enable rapid response to changing require-
ments whilst also building in consistent quality control
checks [32,37]. Specifically, the SWO project focused on
the following agile principles [38] and adapted them to
ontology development:
 The SWOs users, domain experts, and ontology
engineers are all active contributors throughout the
process;
 Close engagement with users meant the introduction
of requirements gathering and ontology modelling
sessions as iterative activities, each iteration (sprint)
resulting in a new increment;
 Acknowledgment that requirements can evolve and
priorities can change throughout the engineering
lifecycle;
 Encouragement of self-organised and
cross-functional teams of developers;
 Testing is an integral part of development and
happens all the time;
 Provision of regular and frequent builds to the
participants for discussion, testing and refinement
(and ultimately agreement).
Applying these principles requires a number of events
to take place in order to deliver information to other
events in a cyclic manner, though events can be run in
parallel. The agile ontology engineering method can be
summarised as follows:
 Requirements gathering. Requirements are
captured from stakeholders by identifying key areas
of interest, eliciting competency questions [39] and
desirable features of the ontology. Activities are
driven by user stories, in the form of competency
questions, and card sorting exercises.
 Requirements prioritisation. Prioritisation of
requirements has two components, both of which are
adapted from agile engineering techniques. The first
is the estimation of the complexity of implementing a
particular requirement using planning poker [32].
The second component allows participants to
collectively rank requirements by bidding on
individual requirements of most interest to their
needs based on the Buy a Feature method [40].
Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 4 of 13
http://www.jbiomedsem.com/content/5/1/25
 Implementation of the Top requirements. The
implementation of the ontology focuses on the
prioritised requirements that were bought from the
previous event. Separating the ontology components
into modules allows concurrent development to
occur from co-located or distributed developers.
Additional content is gathered from stakeholders
using methods such as template completion via tools,
such as Populous [41] - a tool for creating ontologies
using a spreadsheet like interface, which are well
suited to large-scale concept collection.
 Evaluation of product. At the end of each iteration,
the ontology is evaluated against competency
questions [39]. Defined classes based on competency
questions act as queries within the ontology and are
used to demonstrate delivery of a particular feature to
stakeholders.
The SWO project conducted three face-to-face work-
shops between 2011-2012 (see [42] for details), dur-
ing which the method outlined above was applied [1].
The first workshop (WS1) was used primarily to gather
requirements and potential content, since there was no
ontology to evaluate at that point. The second (WS2) and
third (WS3) workshops took place four months and 12
months later and were used to both evaluate form and
content, as well as to generate new content for the SWO.
There were 18 participants in WS1, 14 in WS2 and 17
in WS3. Seven of these participants attended all three
workshops. Participants represented a user base under
the broad heading of digital curation and preservation,
with more specific areas including archiving organiza-
tions, software sustainability, library services, astronomy,
life science and pharmaceutical research.
Spreadsheets and populous
Throughout the project, spreadsheets created using the
Populous tool [41] were used to collect specific software
descriptions from the community. Populous is a tool that
allows cell values to be connected to ontology parts such
that each row becomes a description for an ontology class
following a specified template. In this way, members of
the community did not have to learn new technology or
ontology languages to contribute directly to the ontol-
ogy; instead they simply worked in a familiar spreadsheet
environment.
Testing competency questions via DL queries
The testing component of the method concerns the use
of competency questions phrased as description logic
axioms executed as queries (DL queries). An ontology in
OWL should be able to satisfy competency questions pre-
cisely and this can be tested using the description logic
aspects of the language.
In the testing phase a DL query is formulated which
represents a question of interest, e.g. which software can
take as input image data in the JPG format images. If the
DL query is not producing the desired results then the
ontology needs further refinement and a further itera-
tion occurs. Testing using DL queries in this way is a test
after [43] approach since test driven approaches require a
test to be written before the encoding. This is not suitable
for ontology development in most current environments
since writing a DL query as a test to be executed before
development requires testing infrastructure that, as yet,
does not exist in most environments.
Results
What should be modelled in the SWO?
WS1 resulted in a set of requirements that the ontology
was required tomatch; these were sorted into 15 groups of
features, each groups label became a feature for modelling
in the ontology. In addition, there were 91 competency
questions aligned to these features (see Table 1 for the fea-
ture groups and [44] for the competency question groups).
For instance, the group Function contained sticky notes
containing can the software perform XML editing? and can
the software be used for word processing? It is worth not-
ing that a question could also fall into multiple groups,
for instance can the software perform XML editing? falls
into both Function and, by implication, Data/format fea-
ture groups since the software would need to be able to
parse XML. That all of the competency questions could
be aligned to a feature group, and conversely that each
feature group contained competency questions, provided
a validation of the process, since an orphaned question
might suggest a missing category or an empty feature
group.
The list of features for the SWO gained from the work-
shops are shown in Table 1 along with whether or not
they were bought, i.e. were prioritised in the user priori-
tisation sessions. From a modelling perspective, bought
features were a combination of both simple concepts and
more complex components; some features were deemed
important but too costly to model in a way suited to
customers needs, such as modelling the hardware upon
which software is run. One interesting result of the pri-
oritisation event is that the users initially suggested that
some features, such as algorithm, were ranked highly, but
following effort estimation suggesting this was very costly
to represent, the feature was not bought. Some features
which were discussed as important remained so after
prioritisation and were duly bought, such as data and
function.
In a second prioritisation event, the exercise was
repeated. The algorithm component of software (origi-
nally not prioritised) was consideredmore important than
had previously been determined and was added to the list
Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 5 of 13
http://www.jbiomedsem.com/content/5/1/25
Table 1 The feature groups identified by the workshop participants
Feature Definition Bought? Example competency question
Software The software itself Yes What is the name of the software?
Data Data that the software consumes and pro-
duces
Yes Will it render a gif format image?
Function The task the software is used to do, some-
times called objective
Yes Does this software provide XML editing?
Algorithm The specific instructions as part of software
to perform a given task
No What is the normalization algorithm used in
this software?
Configure parameters Parameters required to run the software; set-
tings
No What are setting needed to run this analysis?
Life cycle Stage of maturity of a piece of software No Does the software meet the ISO-4 standard?
Version The version information Yes What is the latest version of this software?
Supplier Developer and/or maintainer of software Yes Who developed this software?
Dependencies Other pieces of software or libraries required
to run it
No What are the dependencies for using OWL-
API?
Interface Modes of interaction with the software Yes Is there a Web API for Blast?
Source code location URL or otherwise of source code Yes Where can I get the code?
Cost of ownership Cost to purchase but also to run No Is it free?
Platform Which platform is required to run software No Will the software run on Ubuntu?
License What license and usage restrictions exist for
a given software
Yes What software can I use for my task which is
under the Apache 2 license?
Architecture Architectural structure of the software, such
as peer-to-peer
No Is the software client-server?
Those features that were bought were then prioritised for inclusion in the ontology.
of features. This became apparent after the initial exam-
ples failed to answer some of the competency questions
regarding software that implements a given algorithm.
Since there was a small amount of additional extra effort
available, algorithm was included in some descriptions of
software added more recently to the SWO.
The ontology
The ontology was authored in the Web Ontology Lan-
guage (OWL) [45] using the schema shown in Figure 1
as a guide for the top-level distinctions made in describ-
ing software. As of Release 1.1 in December 2013, the
SWO contained 3 777 classes, 50 object properties, 5 data
properties and 114 individuals. Table 2 shows the number
of classes under each major division in the SWO. Ini-
tially, addition of software used in bioinformatics to the
SWO was driven by the needs of the ontologys authors
and client projects. Latterly, however, a more systematic
approach has been adopted; we are using results of a sur-
vey of Genome Biology and BMC Bioinformatics with
BioNERDS [46], a named entity recogniser for bioinfor-
matics software and databases. This survey provided a list
of software and databases ranked by the number of docu-
ments in which those resources were mentioned. We took
the top 50 resources and removed the databases and any
obviously spurious entries to leave only software (database
management systems such as mySQL are software, but for
the SWO a reference to database content, such as SWISS-
PROT, does not count as software). Genome Biology gave
27 software names, and BMC Bioinformatics 25 names
out of the top 50 resources in each case. These correlate
to 47.5% of the total document level mentions within the
top 50 in Genome Biology, and 53.7% in BMC Bioinfor-
matics. In this way we expect to be able to make the SWO
cover the main software used in bioinformatics and com-
putational biology (the list of software is available in the
supplementary data).
The SWO is separated into discrete ontology modules
that are combined to produce software descriptions. Sep-
arating the different aspects of software in this way allows
for both concurrent development and reuse of those com-
ponents useful for other projects, for instance the orga-
nizations module for an ontology describing biomedical
instruments and license module for an ontology of liter-
ature. Figure 2 illustrates the different OWL module files
and which components of the SWO they contain.
For describing the ontology we use the following font
conventions: classes and object properties. The SWO
is axiomatised as follows; the class Software is natu-
rally the focus of attention. A class of software may be
Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 6 of 13
http://www.jbiomedsem.com/content/5/1/25
license clause
software 
interface
has_clause
data data format 
specification
has_
format_
specification
has_specified
_data_input
has_specified
_data_output
algorithm
software
implementis_encode
d_in
organization
software 
publishing process
software 
development 
process
has_
participant
has_
participant
license interfaceprogramming 
language
version
has_version
has_license has_interface
information 
processing
is_
    
executed_
Figure 1 The SWOs schema.
described in terms of the data it takes as input, the data
it produces as output, the objective or processing task it
is designed to meet, licensing restrictions that apply to
using the software (and so on). Few of these properties
are universally true of software (there is software that, at
the granularity at which the SWO is represented) takes
no data as an input), so using restrictions to represent
these notions is not desirable. The only restriction on
Software is that it is executed in some process. A typical
piece of software would be described as follows:
Table 2 The number of classes or individuals under each
major division in the SWO; these are things that describe
domain content, rather than ontology infra-structure
SWO division Number Example classes or individuals
Software 512 Blast, Excel, Endnote, Clustal
Data 1168 heatmap, sequence alignment
(protein), 2D PAGE image
Data Format 434 XLS, RDF-XML, BAM, JPEG
Information processing 608 Phylogenetic tree construction,
spreadsheet editing, ontology
engineering,
protein structure analysis
Algorithm 159 ANOVA, Chi-square, t-test
Organization 78 Agilent Technologies, Adobe
Systems, Bioconductor, SAS
Institute Inc.
Programming language 46 C++, Java, MATLAB language, Ruby
Software license 30 Apache license v2, GNU GPL, MIT
License
 A property has specified data input links a software to
its input data while has specified data output
similarly links software to its output data. the
specified part of these property names seeks to
capture the choice inherent in, for instance, data
inputs by enabling statements such as software x is
specified to be able to take data types p, q and r as
inputs, without saying each and every instance of
software actually does so. The tree of data is presently
fairly flat, with some structure separating image data
from much of the other data.
 Data format is separated from the data itself and is
related via a property has format specification which
can be used to specify that the data has a certain
syntax, such as XML or SVG. In the SWO we make a
distinction between data and the format of the data.
These are easily conflated, but useful to pull apart as
one type of data can be presented in many formats.
Perhaps the easiest example is that of image data;
here the data is the symbols or values that represent
the meaning of the data; the format, however, is the
syntax that governs the encoding and decoding of
that data. Thus data are the symbols upon which a
computer (typically via software) performs
operations. the format specifies how the data are to
be encoded. So, for images, image data is image data
(some symbols), but an image file could be encoded
in PNG, JPEG, PDF, etc, and, in some cases,
inter-converted, preserving the image data itself.
 The algorithm section of the SWO captures
algorithms which a piece of software implements.
Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 7 of 13
http://www.jbiomedsem.com/content/5/1/25
versiondata
data format 
specification
algorithmorganization
software publishing 
process
software 
development 
process
license clause
license interface
software
programming 
language
information 
processing
Figure 2 The SWO s ontology consists of several modules which are used to compose software descriptions.
 The property is executed in links a software to the
information processing class in which it is
executed. Information processing can be seen here as
the task for which the software is being used to help
accomplish. For instance, differential
expression analysis and ontology
engineering.
 Software can have a version using the has version
property. Versioning is complex and is discussed in
more detail below.
 Licenses are also described in the SWO as types of
software license. Software licenses are
described in terms of license clause classes via
the has clause property which capture specific
licensing aspects such as how software can be used,
redistributed, extended or modified.
 Interfaces to software, such as APIs and graphical user
interfaces, are described in software interface
and related to software by the property has interface.
 Software is encoded within a programming language
and this is represented via the property is encoded in
the programming language part of the ontology.
 Organizations involved in developing or publishing
software are captured as individuals under the
organization class and related to software as the
output of either via the software development
process or the software publishing
process.
 The SWO also contains datatype properties for
connecting, for instance documentation locations to
software with has documentation, homepage for
software has website homepage and a download URL
with has download location.
If we consider the example ofMicrosoft Excel 2007 this
is described in the SWO as follows:
 Excel is specified to be able to take as input any data
in various data formats such as the XLS spreadsheet
format and the tab delimited format.
has specified data input some
(data and (has format specification some XLS spread-
sheet format))
has specified data input some
(data and (has format specification some tab delim-
ited file format))
 Similarly, Excel is also specified as inputting and
outputting data in various data formats such as the
XLS spreadsheet format or as a tab delimited format.
has specified data input some
(data and (has format specification some XLS spread-
sheet format))
has specified data input some
(data and (has format specification some tab delim-
ited file format))
 Excel 2007 is versioned as Excel Microsoft 2007.
has version value Microsoft 2007 version
 It has a proprietary commercial license.
has license some Proprietary commercial soft-
ware license
 It has a graphical user interface.
has interface some Graphical user interface
 Excel 2007 is both developed and published by
Microsoft.
output of some
(software development process and (has partici-
pant value Microsoft))
output of some
(software publishing process and (has partici-
pant value Microsoft))
 Excel can be used to edit spreadsheets.
is executed in some spreadsheet editing
For Excel, the specified inputs and outputs are Data, as
spreadsheets can have content of a more or less arbitrary
type. The SWOhas not attempted to represent all possible
Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 8 of 13
http://www.jbiomedsem.com/content/5/1/25
formats for software such as Excel. Instead those data for-
mats that are necessary for the annotations and searches
for the SWOs use cases are prioritised. In line with many
ontology projects, the SWO is largely driven by the needs
of its users.
BLAST 2.2.26 is described as follows:
 BLAST 2.2.26 can take as input, for example, DNA
sequence data in FASTA format or in GenBank
format.
has specified data input some
(DNA nucleotide sequence and (has format specifica-
tion some FASTA format))
has specified data input some
(DNA nucleotide sequence and (has format specifica-
tion some GenBank format))
 It has several interfaces including a command line
interface and web based.
has interface some command-line interface
has interface some web user interface
 This version is developed by the NIH.
output of some
(software development process and (has partici-
pant value NIH))
 Blast can be used to perform multiple sequence
alignment. It can also be used to perform pairwise
sequence alignment.
is executed in some multiple sequence alignment
is executed in some pairwise sequence alignment
 It can be downloaded from ftp://ftp.ncbi.nih.gov/
blast/executables/
has download location value ftp://ftp.ncbi.nih.gov/
blast/executables/
The SWO makes a distinction between an item of soft-
ware and a software suite; this is MS Word 2010 as
opposed to MS Office 2010 that is a bundle of several
MS products including MS Word. A software suite is a
piece of software in its own right, as it provides a thin
wrapper around the bundled softwareeven if this is
just for presentational reasons. The SWO describes MS
Office by using the property has part to relate the software
components. For example, MS Office 2001:
has part some Microsoft Excel 2002
has part some Microsoft Word 2001
Software licences
Several competency questions focused on licensing issues
such as is the software open source or available with-
out restrictions on derivatives. To capture this, software
licenses were given parts, as mentioned above, which were
license clauses. This way, a license can be described by
attaching the relevant clause components which enables
questions to be asked over these components. Figure 3
illustrates an example of a defined class that uses the
same logic to infer types of software licenses that have
clauses that indicate the software is open source. The
highlighted class, GNU project Free Software License
Type is described as follows:
software license
and (has clause some Source code available)
and (has clause some
(Distribution unrestricted
or Distribution with notices))
Figure 3 Inferring open source software licenses from the ontology.
Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 9 of 13
http://www.jbiomedsem.com/content/5/1/25
Software versions
The version name class is used to describe individu-
als which are a specific version name for a given piece of
software. These versions are then related to the class of
software with which they are associated using has_version.
The versions name is captured in the RDFS:label anno-
tation of the given individual.
Competency questions for software versions required
not only a record of what version name was attributed
to a given software instance but also which versions pre-
ceded and proceeded a given piece of software. There are
two forms of the question: Find all previous versions and
find the version prior to the one in hand (and similarly
for subsequent versions). This is a list of versions and we
use the pattern described in [47]. The directly following
and preceding version individuals are asserted via the
properties directly followed by and directly preceded by.
These properties have the super-properties followed by
and preceded by, which are transitive (if A is preceded
by B and b is preceded by C, then A is preceded by C).
In OWL the sub-property implies the super-property, so
the chain of transitive links is maintained automatically.
This means that both forms of the competency question
for versions can be answered. The variant of asking for
the version n back in the chain would be answered with
an expression like directly preceded by some
directly preceded by version x for the
version two versions back in the list. In addition software
which has a dual licensing form (often for branding) can
also be captured. In Manchester OWL this appears as
follows:
Microsoft Excel 2007
has_version value Microsoft 2007 version
Microsoft 2007 version
directly_preceded_by Microsoft 2003 version
directly_followed_by Microsoft 2010 version
We can now perform the query by using the two transi-
tive parent properties which will allow us to get, for exam-
ple, all predecessors. Continuing the example, for versions
ofMicrosoft Excel which came before this current version,
in Manchester OWL:
Microsoft Excel and (has_version some (version nameor num-
ber and (followed_by value Microsoft 2007 version)))
which when asked of the SWO returns the classes
Microsoft Excel 2002 and Microsoft Excel
2003.
Merging SWO and EDAM
The SWO has a broader scope of software than EDAM,
but both broadly model software in the same way. As such,
EDAM is a subset of the SWO, we have been merging
EDAM into the SWO. Although much of EDAM is now
merged into the SWO, there is still an ongoing process of
refactoring to align these fully. Full details of the merge
procedure can be found on the SWO blog [42]. The
process to date can be summarised as follows:
1. Modifications to the underlying annotations within
EDAM were performed to align the structure of the
ontologies more closely.
2. The native OBO format was converted to OWL.
3. High-level EDAM hierarchies were merged into the
SWO structure.
Annotations and Conversion to OWL
A number of annotations were added to the EDAM
ontology in preparation for its conversion to OWL and,
ultimately, merging with the SWO. These included:
 The addition of the definition_editor annotation
property from EFO to all classes with definitions,
providing authorship in a manner in line with the
method already employed within the SWO.
 The addition of the EDAM idspace to all properties
(and usages of those properties) within the OBO file
as the automated conversion to OWL creates an
incorrect OWL-based namespace,
 The addition of appropriate alternative annotations
to the converted OWL files, as the OWLAPI 4.1 does
not convert some annotations to OWL correctly,
 The automatic conversion of EDAM to OWL using
Protege 4.1s conversion feature, which makes use of
the underlying OWLAPI. While the Protege 4.1
conversion process between OBO and OWL is
straightforward, some manual changes were required
[48]. The merge of EDAM into SWO and therefore
into OWL will render this process unnecessary in the
future.
Merging
There are four high-level EDAM terms: Data, Format,
Operation and Topic. These terms and their hierar-
chies are in the process of being manually merged with
the SWO. The initial stages of this have been previously
described in [49]. In this process, each high-level EDAM
term is compared against the SWO and either added as
a subclass to an appropriate point (where no equivalent
class exists) or formally axiomatised as equivalent to a
pre-existing SWO class.
EDAMs Format and Data have been fully merged,
and can be found within the SWO as equivalent classes
to data format specification and data, respec-
tively. EDAMs Topic class describes broad domains or
fields of interest and has no equivalent class within the
SWO, and has been added without any modifications as a
child of the SWOs information class.
Initially it appeared that the EDAM Operation class
would be a good match for the SWO Objective
Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 10 of 13
http://www.jbiomedsem.com/content/5/1/25
hierarchy. EDAM Operation describes tasks, such
as data annotation or classification in much the
same way as SWO objective. However EDAMs
Objective, defined as information describing the
intended outcome of running a process, does not match
the SWOs Operations modelling of the whole pro-
cess (inputs, outputs, process and outcome). As the def-
inition of EDAM Operation class fitted better under
process in the SWO, Operation has been merged
with information processing (a child of process
in the SWO) and the two classes have been axiomatised as
being equivalent.
If EDAMs Operation had been simply placed under
process in the SWO, then the SWO Objective
and newly-enhanced process hierarchies would have
contained many similarities. For example, the EDAM
sequence analysis class within Operation
has many similarities with the SWO classes within
Objective such as molecular sequence
analysis. As such, Operation was first merged with
the SWO information processing, then the SWO
Objective hierarchy was refactored as part of the
process hierarchy, and finally the Objective class
itself was deprecated (for further details see [50]).
An additional issue arose with the EDAM class
Parameter. Parameter was considered a class of data
in EDAMwhereas the contextual nature of whether or not
something is a parameter would suggest it is a role in the
SWO. The class metadata is a type of data in the SWO
but in EDAM this is a type of report.
There is also a use of asserted multiple hierarchies in
EDAM, for example BioXSD (format) class is an asserted
subclass of five other classes; Alignment format
(XML), Raw sequence format, Sequence
feature annotation format, Sequence
record format and XML. The SWOhierarchy enforces
a single axis of asserted classification and multiple classi-
fications are built by inference following a normalisation
style approach [51]. EDAM did not have this strict con-
straint during its development, so in the merged SWO
and EDAM asserted polyhierarchy exists, however, refac-
toring is ongoing to remove any remaining asserted
polyhierarchy.
Some of this integration can be seen in Figure 3.
The shared EDAM upper level classes with the SWO,
such as data (Data in EDAM) and data format
specification (Format in EDAM) can be seen here.
Equivalence axioms were placed between classes where
integration was clear (i.e. the ontologies referred to the
same concept but with different URIs).
The SWOs polyhierarchy
The polyhierarchy produced and maintained in the SWO
by this approach produces an ontology in which software
is described along many dimensions. These dimensions
are those captured in the properties and divisions within
the SWO. As well as license and version above, soft-
ware can also be classified along the other dimensions
previously described, such as:
 The data it takes as input:
software and has specified data input some data
 The data it takes as output:
software and has specified data output some data
 The process the software supports:
software and is executed in some information processing
 The data format supported by the software
software and has specified data input some (data and (has
format some data format specification))
and similarly for specified data outputs
 Algorithms implemented
software and implement some algorithm
 Programming language
software and is encoded in some programming language
 Developer of software
output of some (software development process and
(has participant value Microsoft))
These dimensions can be combined in arbitrary forms,
e.g., Information processing task, inputs and outputs.
Defined classes instantiating these classifications are not
numerous within the SWO; instead these queries would
be deployed at time of use from within software applica-
tions using the SWO.
The SWO applied
BioMedBridges software registry
The Tools and Data Services Registry [36] is a catalogue
of the prevalent bioinformatics tool and data resources,
including the Web services, portals and applications used
by scientists within the BioMedBridges research infras-
tructures. The registry, which is developed in a sustainable
way by ELIXIR [52], requires a detailed description of
software and resources. The vocabulary for this descrip-
tion is provided by the SWO and EDAM, and includes
the type of software and software interface, topic (gen-
eral scientific domain), function, types of input and output
data, data formats, software maturity, supported platform,
language, license and cost. The registry is built using a
federated curation model in which software descriptions
are harvested from key providers and other registries,
Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 11 of 13
http://www.jbiomedsem.com/content/5/1/25
working with these partners to ensure annotations are
made at source. For example, the registry will include
content from BioCatalogue [53], which will also be anno-
tated using the SWO and EDAM.
eagle-I
The eagle-I network is a US$15 million NIH-funded
project with the aim of facilitating biomedical research by
creating a network of research resources repositories [34].
More than 50,000 resources which include biomedical
data, software, databases and services - are listed and
more are added every week. The Software Ontology
plays an important role within the eagle-Is applica-
tion ontology which is used for indexing and searching
these resources. This includes the discovery of resources
based on data sets and formats, licenses and software
function.
Gene Expression Atlas Data
The Gene Expression Atlas has produced an RDF repre-
sentation [54] which describes summaries of whether or
not a gene is differentially expressed given a particular
condition, e.g. human liver. As part of these descriptions,
SWO and EDAM classes are used to capture which soft-
ware analysis packages were used to produce the summary
information and to type data resources which link to this
gene expression data, such as an Entrez Gene Database
Reference. SWO was also applied to the RDF export of
this data into the new EBI RDF Platform [55] wherein
the statistical packages used to generate the results were
typedwith SWOclasses enbling querying over the specific
software.
Evaluating the SWO
Our evaluation of the SWO took two forms:
1. Testing by competency questionsDo we meet the
tests as supplied by the competency questions set by
our customers?
2. CoverageDoes it it contain terms required to
annotate with?
As described above, the SWO has been used to describe
software in several settings. The informal feedback from
users browsing the ontology is that the SWO has the
appropriate shape and talks about the right features for
customers tasks. The BMB project has, however, raised
the issue of describing the platform upon which the soft-
ware is capable of running or was run in a particular
setting. This was raised as an important issue in the SWO
workshops, but a complete description was deemed too
costly to be bought. A similar missing feature is the cost
of software; again, this was raised in the workshops, but
was not a high enough priority to be bought. Cost covers
many facetstheres the monetary cost, but there is also
the cost of use and maintenance. Monetary cost is rela-
tively straight-forward to model, but the other costs are
highly subjective. Our current thoughts are to use a rich
description of licences to imply whether or not a software
is free to use and form slightly more complex axioms to
cover the case when the software is free to a subset of
users, for example free to academics and such like. This
latter modeling of cost is now being built into the latest
versions of the SWO.
As well as the features described, the in-use evalua-
tion naturally reveals a lack of content; the software that
needs to be described is not present. As previously men-
tioned, as well as direct submissions from the community,
the SWO has more recently been evaluating against the
BioNERDS list of software mentions in biomedical liter-
ature and is looking to improve to 100% coverage of the
top 200 within the next 6 months. The dynamic and fluid
nature of software availability and development within the
bioinformatics community is an ongoing issue and is not
unique to the SWO. The SWO has reused, where pos-
sible, reference bio-ontologies such as the OBO Relation
Ontology and Information Artifact Ontology and has con-
sulted with various other ontology consortia on the model
used to describe software. This has helped to populate
some small areas of the ontology more quickly than oth-
ers, though generally much of what is in the SWO does
not exist within these reference ontologies, reinforcing the
need for an ontology like the SWO.
Our on-going testing and ontologising to pass failed
tests works as a tactic in ontology development. However,
frameworks for doing this are only nascent. The processes
used in the SWO aspire to follow similar methods to those
used in the development of production ontologies such
as the EFO at the EBI [30]. Here, continuous integration
systems are used to test each commit of a version of an
ontology such that potential bugs are caught early.
Discussion and conclusions
An ontology of software is necessary for the description
of the data that are now central to the pursuit of life sci-
ence research. Just as we need ontologies to specify the
biomedical entities that are discovered through our sci-
ence, we also need a description of how those entities were
discoveredboth in the wet lab and the dry computa-
tional analysis of the data produced by those biomedical
investigations. The SWO fits into this ontological land-
scape.
Descriptions of a softwares information processing
tasks, the data it consumes and produces, together with
the format of those data, are central to the SWO. In addi-
tion to the core areas, the SWOdescribes many peripheral
but useful concepts including software developers and
their organizational background as well as software ver-
sions, locations, and licensing. To create an ontology that
Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 12 of 13
http://www.jbiomedsem.com/content/5/1/25
is complete in any of these areas is ambitious. For instance,
it is not feasible to describe the universe of softwares
information processing tasks. Instead, the SWO takes the
stance of doing what is necessary for the job in hand; our
Agile approach should help in keeping the SWO fit for
purpose. Nevertheless, the SWOs conceptual framework
seeks to be able to accommodate the changes necessary to
keep it fit for purpose.
The work to integrate with EDAM has enriched the
SWO with additional concepts in the areas of bioinfor-
matics resources and Web services. In the context of
wider biomedical investigations, the SWO with EDAM
should play a significant role in annotating experimen-
tal protocols, alongside complementary ontologies such as
OBI.
Biomedical ontologies typically focus on biological and
medical entities which introduces its own levels of com-
plexity, particularly placing knowledge into the context
of evolution. Biomedical software faces different com-
plexities; evolution is replaced with the diversities of
human design and practice. It is clear that this varia-
tion introduces difficulties in making biomedical analyses
both describable and reproducible but this requires more
than just the appropriate ontologies to be available. There
needs to be a paradigm shift towards both releasing all
data associated with investigations and in describing the
components in sufficient detail that they are understand-
able and reproducible. This issue only becomes more
salient in the age of so called Big Data, lest we face the
problems we already encountered when interpreting the
current archive ofMediumData [56]. This requires a com-
bination of elements including tooling, funding and the
treatment of metadata as a first class citizen. An ontology
of software will play an important role in achieving this
aim.
The SWO has been developed under the Apache 2.0
open source license and is open to collaboration from
external bodies. Already, several groups are making edits
to the ontology and we hope to increase this number with
additional members of the community. New user groups
have recently emerged such as the new CLI-mate [57] tool
and we intend to support these activities.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
JM, AB, ALL, JI, DL, HP and RS contributed content to the SWO. JI is lead
developer of EDAM. JM and RS managed the SWO project and organised user
workshops. All authors read and approved the final manuscript.
Acknowledgements
Work on the Software Ontology has been funded by the JISC SWORD project
and EPSRC grant EP/C536444/1. We acknowledge funding from European
Molecular Biology Laboratory, European Bioinformatics Institute (EMBL-EBI).
We would like to thank everyone that attended the SWO workshops for their
invaluable contributions.
Author details
1EMBL-EBI, Wellcome Trust Genome Campus, Cambridge, CB10 1SD, UK.
2School of Computer Science, University of Manchester, Oxford Road,
Manchester, M13 9PL, UK.
Received: 20 June 2013 Accepted: 19 April 2014
Published: 2 June 2014
INTRODUCTION Open Access
Selected papers from the 16th Annual
Bio-Ontologies Special Interest Group Meeting
Larisa N Soldatova1*, Philippe Rocca-Serra2, Michel Dumontier3, Nigam H Shah3
From Bio-Ontologies Special Interest Group 2013
Berlin, Germany. 20 July 2013
* Correspondence: larisa.
soldatova@brunel.ac.uk
1Brunel University, London, UK
Abstract
Over the 16 years, the Bio-Ontologies SIG at ISMB has provided a forum for vibrant
discussions of the latest and most innovative advances in the research area of
bio-ontologies, its applications to biomedicine and more generally in the organisation,
sharing and re-use of knowledge in biomedicine and the life sciences. The six papers
selected for this supplement span a wide range of topics including: ontology-based data
integration, ontology-based annotation of scientific literature, ontology and data model
development, representation of scientific results and gene candidate prediction.
Summary of selected papers
In 2013, the SIG received 26 submissions, including 15 papers, 5 flash updates and 6 poster
abstracts. 7 papers and 6 flash updates (some papers were converted to flash updates) were
selected for presentation at the meeting, out of which 6 appear in this supplement. The six
papers selected for this supplement are extended versions of the original papers and flash
updates presented at the 2013 SIG. The papers include research on such classic but never-
theless crucially important problems as ontology-based data integration [1-3], ontology-
based annotation of scientific literature [1-4], ontology and data model development
[2,3,5], representation of scientific results [5] and gene candidate prediction [6].
Bölling et al in the paper titled SEE: structured representation of scientific evidence in
the biomedical domain using Semantic Web techniques present an RDF/OWL based
approach for detailed representation of scientific evidences [1]. Knowledge in biomedicine
is context-dependent and based on a variety of evidences obtained by experimental obser-
vations, inferences from other results, different interpretations, and modeling approaches.
Bölling et al suggest RDO (the Reasoning and Discourse Ontology) - a lightweight OWL
vocabulary for the representation and recording of how scientific claims are made and
how they are related to each other. It provides computationally accessible representations
of evidence-related information such as the materials, methods, assumptions and informa-
tion sources used to establish a scientific finding. The proposed approach is demonstrated
on the case study of evidence gathered in the literature regarding a claimed source of the
enzyme glutamine synthetase. SEE resources, including the RDO ontology, are available
from http://purl.org/see.
Soldatova et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):I1
http://www.jbiomedsem.com/content/5/S1/I1 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2014 Soldatova et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
The paper titled Statistical algorithms for ontology-based annotation of scientific lit-
erature by Chakrabarti et al. reports on a probabilistic framework for annotating
BrainMap literature using the Cognitive Paradigm Ontology (CogPO) [2]. This frame-
work exploits hierarchical information, dependences and restrictions available in the
ontology. At present, articles in the BrainMap repository are annotated manually
according to CogPO definitions and it is a time and efforts intensive process that pre-
sents the major bottleneck for the whole repository. The proposed annotation frame-
work would enable (semi-) automated solutions for the annotation of BrainMap
literature. The proposed stochastic approaches for literature annotation were tested
against the gold standard - the annotation by human subject matter experts, and
yielded encouraging results.
Merrill et al in their paper Semantic Web repositories for genomics data using the
eXframe platform addresses the critical task of the integration of genomic databases
and data re-use [3]. They developed the second generation of the eXframe platform
that supports the creation of online repositories to deposit genomics data as Linked
Data. The eXframe platform provides a built-in SPARQL (Sparql Protocol and RDF
Query Language) endpoint to query the data. The platform uses biomedical ontologies,
e.g. OBI (the Ontology for Biomedical Investigations), DO (Disease Ontology), ChEBI
(Chemical Entities of Biological Interests) ontology, to enable interoperability of the
produced repositories. The platform also provides support for accessing data using
popular statistical programming language R. The platform has been successfully tested
through the case study of the Stem Cell Commons project of the Harvard Stem Cell
Institute. eXframe is freely available at: https://github.com/mindinformatics/exframe.
Oellrich et al. in the paper titled The influence of disease categories on gene candidate
predictions from model organism phenotypes analyse Exomisers performance with
respect to disease categories provided by Orphanet [4]. Exomiser is a tool previously
developed by the authors to narrow down gene candidate lists that have been identified in
exome analyses using cross-species phenotype comparisons amongst other sources of evi-
dence. Oellrich et al. show that the prediction results depend on the organism and when
automatically predicting disease gene candidates careful consideration is required as to
which organism to apply for the predictions. For each disease category, they investigated
the ten most common clinical phenotypes. Oellrich et al. found, for example, that the per-
formance for zebrafish for nearly all disease categories is much more dependent on the
disease category than it is for the mouse. The authors conclude that smarter tools capable
of taking into account the differences between species and accumulate predictions are
required.
The paper Evolving BioAssay Ontology (BAO): modularization, integration and
applications by Abeyruwan et al. outline the work on the development of common
reference metadata terms and definitions required for the reporting of information
about low- and high- throughput drug and probe screening assays and results [5]. The
authors have created BAO to support effective integration, aggregation, retrieval, and
analyses of drug screening data. Abeyruwan et al. employed a modular approach for
the development of BAO with domain-level components separated from structural
components. The main components include bioassay, assay biology, assay method, assay
format, assay endpoint and assay screened entity. BAO is sufficient to enable modeling
of result profiles (signatures) generated in panel and profiling assays, for example those
Soldatova et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):I1
http://www.jbiomedsem.com/content/5/S1/I1
Page 2 of 3
in the LINCS (the Library of Integrated Network-based Cellular Signatures) project.
The authors have leveraged BAO in software tools, such as the Semantic Web software
applications BAOSearch, LIFE, and the BioAssay Research Database (BARD). BAO is
available at http://bioassayontology.org.
Tatum et al. in their paper titled Preserving sequence annotations across reference
sequences present an RDF data model for describing sequence annotation instances
within an established ontological framework that fits common practice of working with
reference sequences and different versions of genome assemblies [6]. Tatum et al. created
the Reference Sequence Ontology to provide a mechanism for linking annotation
instances to different reference sequences. They also investigated how sequence annota-
tions using different reference sequences can be semantically linked and identified three
types of reference sequence relationships that are crucial for data integration. Tatum et al.
present a working data model of sequence annotations that can be preserved across differ-
ent reference sequence assemblies. The ontology of Reference Sequence Annotation is
available at http://purl.bioontology.org/ontology/RSA.
Competing interests
The authors declare that they have no competing interests.
Acknowledgements
As editors of this supplement, we thank all the authors who submitted papers, the Program Committee members and
the reviewers for their excellent work. We are grateful for help from Sarah Headley from BioMed Central in putting
this supplement together.
This article has been published as part of Journal of Biomedical Semantics Volume 5 Supplement 1, 2014: Proceedings
of the Bio-Ontologies Special Interest Group 2013. The full contents of the supplement are available online at http://
www.jbiomedsem.com/supplements/5/S1.
Authors details
1Brunel University, London, UK. 2University of Oxford, Oxford e-Research Centre, UK. 3Stanford University, CA, USA.
Published: 3 June 2014
