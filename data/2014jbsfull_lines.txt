JOURNAL OFBIOMEDICAL SEMANTICSChaudhri et al. Journal of Biomedical Semantics 2014, 5:51http://www.jbiomedsem.com/content/5/1/51RESEARCH Open AccessComparative analysis of knowledge representationand reasoning requirements across a range of lifesciences textbooksVinay K Chaudhri1*, Daniel Elenius1, Andrew Goldenkranz2, Allison Gong3, Maryann E Martone4, William Webb5and Neil Yorke-Smith6,7AbstractBackground: Using knowledge representation for biomedical projects is now commonplace. In previous work, werepresented the knowledge found in a college-level biology textbook in a fashion useful for answering questions.We showed that embedding the knowledge representation and question-answering abilities in an electronictextbook helped to engage student interest and improve learning. A natural question that arises from thissuccess, and this papers primary focus, is whether a similar approach is applicable across a range of life sciencetextbooks. To answer that question, we considered four different textbooks, ranging from a below-introductory collegebiology text to an advanced, graduate-level neuroscience textbook. For these textbooks, we investigated the followingquestions: (1) To what extent is knowledge shared between the different textbooks? (2) To what extent can the sameupper ontology be used to represent the knowledge found in different textbooks? (3) To what extent canthe questions of interest for a range of textbooks be answered by using the same reasoning mechanisms?Results: Our existing modeling and reasoning methods apply especially well both to a textbook that iscomparable in level to the text studied in our previous work (i.e., an introductory-level text) and to a textbookat a lower level, suggesting potential for a high degree of portability. Even for the overlapping knowledgefound across the textbooks, the level of detail covered in each textbook was different, which requires thatthe representations must be customized for each textbook. We also found that for advanced textbooks, representingmodels and scientific reasoning processes was particularly important.Conclusions: With some additional work, our representation methodology would be applicable to a range oftextbooks. The requirements for knowledge representation are common across textbooks, suggesting that ashared semantic infrastructure for the life sciences is feasible. Because our representation overlaps heavily withthose already being used for biomedical ontologies, this work suggests a natural pathway to include suchrepresentations as part of the life sciences curriculum at different grade levels.Keywords: Ontology, Textbook knowledge, Knowledge representation, Reasoning, Question answering,Semantic infrastructure* Correspondence: Vinay.Chaudhri@sri.com1SRI International, Menlo Park, CA 94025, USAFull list of author information is available at the end of the article© 2014 Chaudhri et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public DomainDedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article,unless otherwise stated.Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 2 of 19http://www.jbiomedsem.com/content/5/1/51BackgroundUsing knowledge representation is now commonplaceacross a range of biomedical projects [1-3]. This usageis evidenced by the success of the National Center ofBiomedical Ontologies, which, as of 2014, publishes anddisseminates more than 350 ontologies [4]. Despite thiswidespread application of knowledge representation inbiomedical projects, further significant value could bereaped: The Journal of Nucleic Acids Research cataloguesthousands of databases that could substantially benefit ifthey were accompanied by an explicit ontology [5]. Weanticipate that knowledge representation will play a cru-cial role in future biomedical research, especially forexploiting, leveraging, and understanding big data.During an artificial intelligence (AI) project called ProjectHalo, we developed an intelligent textbook technology thatleverages an explicit ontology and a question-answeringsystem, and that helps students learn better [6]. Obviousoverlaps exist between the technologies used in ourproject and the methods that are commonplace for bio-medical ontologies [7,8]. This convergence presents anunprecedented pathway for synergy between work on on-tologies and life sciences education. If textbook knowledgecould be represented and encoded in an educational con-text, as we propose here, then it could eventually be morewidely incorporated into biomedical projects, thus com-plementing the existing knowledge resources.Our work on the intelligent textbook [6] focused onan introductory college-level biology textbook calledCampbell Biology [9]. We encoded substantial portionsof Campbell and then used this knowledge representa-tion as a basis for an intelligent textbook called Inquire,which enables students to explore topics across multiplelevels of organization and to pose their own questions,which are then answered by machine reasoning. Theintelligent textbook is a powerful learning tool that bothgives students information such as definitions and de-scriptions of terms, and enables them to explore structure,function, and concepts across different levels of biologicalorganization.The current papers focus is on investigating the ques-tion: To what extent can a generic methodology for cap-turing textbook knowledge be developed that is applicableacross a range of life sciences textbooks? We have brokenthis high-level question into three sub-questions: (1) Towhat extent is knowledge shared between the differenttextbooks? (2) To what extent can the same upper ontol-ogy be used to represent the knowledge found in differenttextbooks? (3) To what extent can the questions of inter-est for a range of textbooks be answered by using thesame reasoning mechanisms? A desired outcome is quan-tifying the extent to which the already developed methodsapply to different textbooks and quantifying any differ-ences or novel requirements across textbooks. Thesequestions are important, because if we could apply thesame methodology to textbooks at both lower and highergrade levels, then this generalizability would enable mak-ing semantics integral to science textbooks. The answersto these questions will also be informative to others asthey seek insights both into generic techniques for ontol-ogy design and into the requirements that differ acrossdomains.For the remainder of this section, we give an overviewof our project, review the prior work on knowledge repre-sentation, describe the ontology, and provide the rationalefor the textbooks that were selected for comparison. Wefollow that by a description of our methods and results.Context of Project HaloProject Halo was an AI project funded by Vulcan, Inc.,with the goal of creating a system called Digital Aristotlethat could answer questions on a wide variety of sciencetopics. SRI International participated in this project from20032013 [6,10,11]. During this period, we advancedthe state of the art in knowledge base (KB) systems byenabling domain experts with little background in know-ledge representation to author knowledge that could beused for answering questions. This works results areembodied in a knowledge-authoring system called AURA[11]. To demonstrate the scalability of the approach, weused AURA to encode substantial fractions of CampbellBiology [9], which resulted in the knowledge base KBBio 101 [12]. A team of biologists trained in AURA buthaving no background in knowledge representation per-formed the encoding work. We designed a knowledge-factory process that the biologists used to systematicallyconvert the textbook content into KB Bio 101 [12]. Al-though accurately assessing the total effort invested inthe encoding is difficult, we estimate that the effort wasat least twelve person years. KB Bio 101 represents asubstantial fraction of Campbell Biology and containsmore than 100,000 axioms [13].We incorporated KB Bio 101 into an electronic textbookapplication called Inquire, which helps students with read-ing and homework problem solving [6]. An evaluation ofInquire with students showed the practical utility of in-corporating a KB into an electronic textbook, as theInquire students exhibited higher scores than did the con-trol group and received no grades D or F, while these lowergrades were seen in the control group. A video basedon Inquire won the best video award at the annual confer-ence of the Association for Advancement of ArtificialIntelligence (AAAI) in 2012a.Knowledge representation in AURAThe AURA knowledge-authoring system uses KnowledgeMachine (KM) as its knowledge representation and rea-soning engine [14]. KM supports standard representationalChaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 3 of 19http://www.jbiomedsem.com/content/5/1/51features such as classes; individuals; class-subclass hier-archy; disjointness; slots; slot hierarchy; necessary and suf-ficient properties; and deductive rules. The representationin KM can be formally understood as first-order logic withequality. Uniquely, KMs representation supports graph-structured class descriptions. We illustrate KM in thefollowing example.Suppose we wish to represent the statement: Every cellis an entity that has a ribosome and a chromosome as itsparts. We can express this statement in first-order logicas follows. (We implicitly assume that the statements holdover all times).Axiom A1:? x : Cell xð Þ?? y1; y2 : Entity xð Þ?has?part x; y1ð Þ ? has?part x; y2ð Þ? Ribosome y1ð Þ ? Chromosome y2ð ÞNext, suppose we wish to represent: Every eukaryoticcell has as parts a ribosome, a nucleus, and a eukaryoticchromosome such that the chromosome is inside thenucleusb. We can capture this statement in first-orderlogic as follows:Axiom A2:? x : Eukaryotic?Cell xð Þ?? y1; y2; y3 : Cell xð Þ? has?part x; y1ð Þ ? has?part x; y2ð Þ? has?part x; y3ð Þ ? is?inside y2; y3ð Þ? Ribosome y1ð Þ ? Eukaryotic?Chromosome y2ð Þ? Nucleus y3ð ÞIn the class definition of a Eukaryotic-Cell, specifyingthe is-inside relationship between the Chromosome andthe Nucleus violates the tree model property [15]. Inmodels satisfying tree model property, each node has (atmost) a unique direct predecessor, and in general, it is agood indicator of decidability. To see how the validmodels for A2 violate the tree model property, we createa directed graph as follows: each variable in the axiom isrepresented by a node, and a directed edge exists be-tween the nodes representing a variable x and a variabley if they both participate in the same predicate such thatx appears in the first position and y appears in the sec-ond position. (Because DLs are limited to binary predi-cates, we limit our discussion to only binary predicates.)For a graph for axiom A2, the node y3 has two incomingedges from x and y2, and thus, violates the tree modelproperty. DL systems achieve decidable reasoning bylimiting the representation to only allow tree models,and this limitation is well known [16]. Active research isin progress to address this limitation [17-20].Next, suppose we wish to explicitly state the inherit-ance relationships in our representation by asserting thata Eukaryotic-Cell inherits a Chromosome and Ribosomefrom a Cell, and further, by specifying the inheritedChromosome as a Eukaryotic-Chromosome. We can cap-ture such relationships if we rewrite A1 and A2 by usingSkolemization, a well-known technique to approximateexistential variables in the antecedent of an axiom [21].With Skolemization, in an axiom of the form ? Y1 Yn ?X ?, the existential variable X can be removed and re-placed everywhere in ? with the function term f(Y1Yn), where f is a new function symbol that does notoccur anywhere else in the axiom. The rationale for sucha substitution is that, for any query, the original axiom isunsatisfiable if and only if the transformed axiom isunsatisfiable [21]. This implies that a query with an ori-ginal axiom in the KB can be answered if and only if itcan be answered when posed against the KB with theSkolemized version of the same axiom. However, fromthe point of view of logical entailment, the SkolemizedKB is stronger than the original one, which is why wesay that Skolemization only approximates existentialquantification and is not equivalent to it. Skolemizationof A1 and A2 enables referring to the Skolem functionsintroduced in them outside the scope of the existentialquantifier. In the Skolemized versions of axioms A1 andA2 shown below, we can see that A4 refers to the Skolemfunctions introduced in A3.Axiom A3:? x : Cell xð Þ?Entity xð Þ?has?part x; f cell1 xð Þð Þ ? has?part x; f cell2 xð Þð Þ? Ribosome f cell1 xð Þð Þ ? Chromosome f cell2 xð Þð ÞAxiom A4:? x : Eukaryotic?Cell xð Þ? Cell xð Þ ?has?part x; f ecell1 xð Þð Þ ? has?part x; f ecell2 xð Þð Þ? has?part x; f ecell3 xð Þð Þ? Eukaryotic?Chromosome f ecell3 xð Þð Þ? Nucleus f ecell1 xð Þð Þ ? Ribosome f ecell2 xð Þð Þ? is?inside f ecell3 xð Þ; f ecell1 xð Þð Þ? f ecell3 xð Þ ¼ f cell2 xð Þ ? f ecell2 xð Þ ¼ f cell1 xð ÞThe equality statement used in A4 proves to be a power-ful tool that explicitly shows the inheritance relationship.In some cases, equality statements can be inferred. For ex-ample, if a cardinality constraint asserts that a Cell hasexactly one Chromosome, then one can deductively con-clude that the Eukaryotic-Chromosome must be the sameas the inherited Chromosome. However, associating suchconstraints is incorrect in many situations, as is the casefor a Eukaryotic-Cell.More details about our approach to knowledge repre-sentation [22] and reasoning are available in previouslypublished papers [23-25]. We have translated KB Bio 101into multiple different formats including Web OntologyLanguage Version 2 (OWL2) functionalc, answer set pro-gramming, and the Thousands of Problems about TheoremChaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 4 of 19http://www.jbiomedsem.com/content/5/1/51Proving syntaxd. The translation into OWL2 is lossy, as itcannot fully capture the graph structures represented inthe KB; the other translations are non-lossy. These trans-lations are available through our websitee, and an OWLversion is available through BioPortalf.Upper ontology in AURAAURA uses an upper ontology called Component Libraryor CLIB [26]. CLIB is a linguistically motivated ontologydesigned to support representation of knowledge for au-tomated reasoning. CLIB uses four simple, upper-leveldistinctions: (1) Entity (things that are); (2) Event (thingsthat happen); (3) Relation (associations between things);and (4) Role (ways in which entities participate in events).A unique feature of CLIB is that it provides a vocabu-lary of actions for modeling biological processes. An Ac-tion is a subclass of Event. In CLIB, the class Action has42 direct subclasses, with 147 subclasses in all. Examplesof direct subclasses include Attach, Impair, and Move.Other subclasses include Move-Through (which is a sub-class of Move) and Break (which is a subclass of Damage,which is a subclass of Impair). To ensure generality, thesesubclasses were developed by consulting lexical resources,such as WordNet [27]; the Longman Dictionary of Con-temporary English [28]; and Rogets Thesaurus [29].CLIB provides semantic relationships to define theparticipants of an action. These relations are based on acomprehensive study of case roles in linguistics [30] andinclude agent, object, instrument, raw-material, result,source, destination, and site. (The syntactic and se-mantic definitions that we developed for these relationsare available elsewhere [31].) As an example, we considerFigure 1 A simplified view of the structure of Biomembrane represenquantified, and every other node (shown in gray) is existentially quantified.Biomembrane, there exists an instance of Phospholipid-Bilayer and an instanthe instance of Glycoprotein is-inside the instance of Phospholipid-Bilayer. Tinstance-instance relationships [33].the definition of raw-material. The semantic definition ofraw-material is any entity that is consumed as an input toa process. The syntactic definition of raw-material is ei-ther it is the grammatical object of verbs such as to useor to consume, or the word using precedes it.CLIB also provides the vocabulary needed to definethe relationships that exist between entities, and betweenentities and events, and to associate properties with bothentities and events. For example, the most frequent rela-tionships help define the structural relationships thatexist between entities [32]. We use such relationships forrepresenting structure: has-part, has-region, material,element, and possesses. We have developed detailed defi-nitions and guidelines for their usage. For example, we saythat X has-region Y if Y is a region of space or a Spatial-Entity defined only in relation to X. The complete defini-tions of the CLIB concepts and relationships are availableonlineg.As an illustration of the use of CLIB, in Figure 1, weshow a simplified representation of the structure of aBiomembrane. From the representational point of view,the graph in Figure 1 represents an existential rule of thesort seen in axioms 1 and 2. In this figure, the node shownin white is universally quantified, and every other node,shown in gray, is existentially quantified. Therefore, we canread a portion of Figure 1 as follows: for every instance ofBiomembrane, there exists an instance of Phospholipid-Bilayer and an instance of Glycoprotein that are in has-partrelationship to it, and further the instance of Glycoproteinis-inside the instance of Phospholipid-Bilayer. In thecontext of the relationships used in biomedical ontol-ogies, our usage of has-part and other relationshipsted in AURA. The Biomembrane node (shown in white) is universallyWe can read a portion of this figure as follows: for every instance ofce of Glycoprotein that are in has-part relationship to it, and furtherhe usage of has-part and other relationships corresponds toChaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 5 of 19http://www.jbiomedsem.com/content/5/1/51corresponds to instance-instance relationships [33]. Thearrows go from the first argument of a predicate to the sec-ond argument. For example, an arrow from Biomembraneto a Phospholipid-Bilayer labeled as has-part correspondsto the predicate has-part(b,p), where b is an instance of aBiomembrane, and p is an instance of a Glycoprotein.The numbers on some of the edges indicate cardinalityconstraints. For example, the instance of Phospholipid-Bilayer in Figure 1 has exactly two phospholipid layersthat are in a has-region relationship to it. In Figure 2,we show the functions of a Biomembrane. A portion ofthis figure can be read analogously to Figure 1 as fol-lows: for every instance of a Biomembrane, there exists afunction Block in which the agent is a Hydrophobic-Core, the object is a Hydrophilic-Compound, and aninstrument is a Fatty-Acid-Tail. More details about ourrepresentation of functions are available elsewhere [32].Reasoning in AURAThe KM system [14] provided the core reasoning ser-vices for AURA. KMs reasoning combines description-logic-style classification [34] with backward chainingon rules. We extended KMs basic reasoning with sev-eral higher-level reasoning methods to answer ques-tions [24,25]. AURA also contained a natural languageprocessing interface that processed an input English ques-tion and converted it to a formal representation for evalu-ation by the reasoner [11]. We list below several abstractquestion templates, each followed by an example of itsinstantiation. A detailed formalization of different rea-soning processes in AURA has been published else-where [24,25]. To make this paper self-contained, wefollow each question either by giving a high-level de-scription of how that question was formalized or byspecifying a logical query that could be evaluated bya general-purpose reasoner.Figure 2 Functions of Biomembrane. The top half of this figure can be reaof chemical entities that it is permeable to, and that this movement is througQ1. What are the R of X? (e.g., What are the parts of acell?)Q1 is a very common and basic form of query withnumerous variations. Because the relevant knowledge toanswer Q1 is in the form of axioms such as A1, theformalization of Q1 contains a premise that extends theKB to KB by creating a sample instance of Cell. For ex-ample, for the class Cell, and corresponding to the axiomA1, KB will contain the individual c1. By the applicationof A1, KB is further extended by adding r1 and ch1 suchthat they are instances of Ribosome and a Chromosome,respectively, and by adding the assertions (has-part c1 r1)and (has-part c1 ch1), which are conclusions derived byusing A1. To answer Q1, we query for all literals matching(has-part c1 ?x), returning c1 and ch1 as answers. In morecomplex examples, query evaluation can involve inheritinginformation from super-classes and applying multiplerules.Some instantiations of Q1 leverage the relation hier-archy in the KB. For example, What is the structure ofa cell? Here, the word structure maps to the has-struc-ture relationship in our ontology, which has four sub-relations: has-part, has-region, material, and possesses.For the values returned for each of these relationships,the system further retrieves spatial relationships tocomplete the structural description.In more complex forms of Q1, further constraints on thevalues returned can exist. For example, consider: What doesX do during Y? Assuming that we are interested in thosesteps such that X is a raw-material, those steps must alsosatisfy an additional constraint that they must be sub-stepsof Y. Here, steps correspond to the phases of a process.Q2. What are the subclasses of X? (e.g., What are thesubclasses of a eukaryotic cell?)d as follows: every Biomembrane has a function to allow Move-Throughh its Hydrophobic-Core, which is a region of its Phospholipid-Bilayer.Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 6 of 19http://www.jbiomedsem.com/content/5/1/51Q2 is an example of a taxonomic query that queriesfor all subclass relationships for a class. In AURA, thisquery is answered by traversing the class-subclass hier-archy. Other queries similar to Q2 are: What are thesuper-classes of X? Is X a subclass of Y?Q3. How many X does a Y have for a relation R? (e.g.,How many chromosomes does a human cell haveas its part?)Q3 queries for the cardinality constraints on the has-partrelationship for a human cell. AURA answers this query bya straightforward lookup of cardinality constraints.Q4. Describe X? (e.g., Describe a Cell?)To answer Q4, AURA computes all the facts knownabout a class. The facts about a class include taxonomic re-lationships (i.e., its super-classes and subclasses as com-puted in Q2); its relation values (as computed in Q1); andits cardinality constraints (as computed in Q3). AURAevaluates Q4 by issuing Q1, Q2, and Q3 as sub-queries,and then organizes the results in a concept descriptionpage.Q5. What is the difference/similarity between X and Y?(e.g., What is the difference/similarity between anintegral protein and a peripheral protein?)AURA computes the answer to Q5 in three steps: (1)computing descriptions of X and Y as explained in Q4,(2) computing the similarities and differences betweenthe two descriptions, and (3) then summarizing the re-sults. We have described the details of the computationsin a previous paper [25].AURA supports more specific forms of Q5. Forexample: What are the structural differences between Xand Y?; What is the difference between the size of Xand size of Y?; etc.Q6. What is the relationship between X and Y? (e.g.,Whatis the relationship between DNA and a gene?)Here, we are interested in computing how the individualinstances of X and Y are related to each other. For ex-ample, how is an individual instance of a DNA-Moleculerelated to an individual instance of a Gene. One possibleanswer to this question is that a DNA-Molecule has as itspart a DNA-Strand, which in turn, has as its part a Gene.To answer Q6, AURA first creates an individual instanceof X and recursively computes its relation values (as inQ1) until it encounters an instance of Y. In general, mul-tiple such relationships exist in the KB that should beranked in the order of interest. AURA uses a variety ofheuristics to limit the search process (for example, firstsearching the taxonomic relationships, preferring struc-tural relationships, etc.).AURA supports several questions that leverage thecomputation supported in Q6. Examples include: Whatare structural relationships between X and Y?; X is to Yas A is to what?; and Why is it important that X hasproperty Y? To answer the question X is to Y as A is towhat?, AURA first computes a path between X and Y,and then starting from A, traverses the same path to de-termine the answer [32]. An example formulation of thequestion Why is it important that X has property Y? isHow does the selective permeability of membranes facili-tates its function? To answer this question, AURA com-putes a path that begins from the permeability of amembrane and ends at the function of the Membrane,and that involves the relation facilitates [32].We have implemented these reasoning methods inAURA and have extensively tested them. In the first stageof testing, we conducted a trial with students studyingfrom Inquire. This initial test was done for the chapter onmembranes. The results showed that the question tem-plates were useful to the students, as the students usingthe facility achieved higher scores than the students study-ing from traditional methods, validating the choice ofquestion templates [6]. Once the question templates werevalidated, we instantiated them for the first eleven chap-ters. The test suite for each chapter was spread across thecontent of the chapter and consisted of approximately 150questions each. We executed the questions against AURA,and the domain experts rated the answers for correctness.From the 1,836 questions that we tested, the system cor-rectly answered 1,540 questions, giving an overall correct-ness score of approximately 85%. These results showed avery high degree of system competence for answeringquestions. (For example, IBMs Watson system that wonthe television game show Jeopardy! had a passing rate inmid-seventies [35].)Textbooks used for comparisonWe chose to compare four textbooks spanning a range ofbreadth and depth of coverage (i.e., scope) based on the fol-lowing rationale: choose one textbook comparable toCampbell, one textbook at a grade level lower, one textbookat a grade level higher, and one textbook at the advancedgraduate level. Specifically, we used (1) Raven, which rep-resents a textbook with a similar scope to Campbell [36];(2) Levine, which offers both less breadth and depth thanCampbell, and is used in a lower-division undergraduate,non-major course [37]; (3) Alberts, which has a narrowerbreadth, but a greater depth than Campbell, and is used inan upper-division undergraduate class in cell biology, andis considered a reference text for cellular and molecularbiologists [38]; and (4) Kandel, which targets the specificChaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 7 of 19http://www.jbiomedsem.com/content/5/1/51field of neuroscience, and is therefore, narrower in breadthbut has greater depth than Campbell, and also containsadditional topics such as cognitive scienceh [39]. Kandel isa textbook written for advanced undergraduates, graduatestudents, and medical students studying neuroscience, aspecialized field that is largely biological, but also concernsitself with psychology and cognitive science. Kandel differsfrom the other textbooks in that different authors who areexperts in their respective fields contributed most of theindividual chapters. This approach may lead to a less-uniform treatment across the book than the other text-books, which are each written by a small team of authors.Data comparing the relative lengths of these four text-books is summarized in Table 1 below.Goals of researchWe divided the high-level goal of investigating to whatextent do our current process and methodology for cap-turing the semantics of textbook knowledge generalize toa range of life sciences textbooks into the following threemore-specific questions: (1) To what extent is knowledgeshared between the different textbooks? (2) To whatextent can the same ontology be used to represent theknowledge found in different textbooks? (Based on ourwork with Campbell Biology, we were aware of many ofCLIBs limitations, especially because, from an AI perspec-tive, fully capturing natural language text is an extremelydifficult problem. Our goal here was to quantify the extentto which we could represent knowledge by using the exist-ing CLIB vs. extending it to address any new require-ments as we model different textbooks.) (3) To whatextent can the questions of interest for a new textbook beanswered by using the reasoning mechanisms alreadyavailable in AURA? Because the foundational set of ques-tions is expected to be similar in all domains, we expectedgood generality, but we wished to quantify it against eachtextbook.MethodsWe now consider our methods for answering each ofthe three specific questions introduced in the previoussection.Table 1 Data on page length and chapters in selectedtextbooksTextbook Pages Chapters Pages/chapterCampbell 1263 56 23Raven 1298 57 23Levine 1034 45 30Alberts 1728 45 69Kandel 1316 67 20Domain analysisThe goal of domain analysis is to answer the question:To what extent is knowledge shared between the differ-ent textbooks? More specifically, we were interested inunderstanding whether KB creation for each new bookshould start from scratch or some knowledge from onebook could be shared from another. Answering thisquestion for the topics that appear in one textbook butnot in another is straightforward. Therefore, we selectedthe topics of action potential and membrane structure,which appeared in each of the four textbooks. The teamundertook a coarse analysis of the selected material andselected a few paragraphs for detailed analysis. The teamcompiled information such as the length of coverage, theactual biological content covered, figures, and the typeof language used for describing the material. Such com-parison gave us insight into the commonality of know-ledge across different textbooks, and that informationguided us as to what extent we could share the domain-specific content across the KBs for different textbooks.Knowledge representation analysisThe goal of the knowledge representation analysis was toanswer the question: To what extent can the same upperontology be used to represent the knowledge found in dif-ferent textbooks? Next, we give an overview of the AURAknowledge-engineering process that was the basis of therepresentation analysis, we provide an approach for deal-ing with subject matter consensus, and we introducecategories of representation requirements.AURA knowledge-engineering processWe used an already established knowledge-engineeringprocess to represent the content of a textbook [31] as thebasis of this analysis. This process has two distinct phases:(1) representation design and (2) knowledge encoding. Forthe representation-requirements analysis, we performedonly the representation design phase, which includes thefollowing three steps: (1) determining relevance: analyzeeach sentence in the textbook for its relevance for answer-ing questions; (2) writing universal truths (UTs): for eachrelevant sentence, paraphrase it as a universally true state-ment about a specific entity or an event; and (3) develop-ing action items for encoding: for each universally truestatement, identify the concepts and relations that will beused for representing it.We illustrate the above process by considering an ex-ample sentence: Many cells, including most prokaryotes,also produce a strong supporting layer around the mem-brane known as a cell wall. Multiple UTs can be derivedfrom this sentence. One UT is: Many cells produce a cellwall. The use of word many is also indicative of the factthat there are some exceptions to this UT. To handle suchexceptions, our knowledge-engineering process dictatesChaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 8 of 19http://www.jbiomedsem.com/content/5/1/51that this statement should be further specialized for cells(for example, plant cells always produce a cell wall). Thus,the UT will be reformulated as All plant cells produce acell wall.Our general strategy to deal with exceptions is finding aclass for which that statement is applicable as a universaltruth. We ignore any exceptions that cannot be dealt withby using such a strategy. With the CLIB ontology, the UTunder consideration will be represented by asserting thatevery Plant-Cell is an agent of a process called Synthesis-of-Cell-Wall, which has a result of Cell-Wall which is-part-of the Plant-Cell. Here, agent, result, and is-part-ofare relations from the CLIB ontology. As a second ex-ample, consider the UT: Every plant cell has a cell wallthat is a strong supporting layer. This UT will be repre-sented by asserting that every Plant-Cell has-part a Cell-Wall that has-function a Support that has an object thePlant-Cell itself, and has an intensity value of strong.Here, has-function, object, and intensity are relations inthe CLIB ontology. As a final example, consider the fol-lowing sentence: A protoplast is a plant cell without a cellwall. The UT for this sentence will be: Every protoplastis a cell without a cell wall. Clearly, the sentence frag-ment every protoplast is a plant cell cannot be univer-sally true in our representation, because in that case,Protoplast will inherit all the properties of a Plant-Cell in-cluding a Cell-Wall. We will define Protoplast as a sub-class of Cell in our class hierarchy. The relationshipbetween a Plant-Cell and a Protoplast will be captured byother means.Another central feature of AURAs knowledge-engineeringprocess is the division of labor between knowledge engi-neers and domain experts: the knowledge engineers haveaccess to the full power of the representation languagewhich, as was explained earlier, is comparable to first-order logic with equalitybut the domain experts createonly new classes, declare classes to be disjoint, specifycardinality constraints, and, most importantly, authorexistential rules of the sort visualized in Figures 1 and 2.Achieving consensus among domain expertsOur approach to achieving consensus among the differ-ent domain experts working on the project is driven bythe following observations: (1) Even for biological know-ledge at the level of an introductory college course, notwo textbooks are exactly the same. (2) A textbook suchas Campbell has a large number of reviewers who areable to approve the content of the textbook. (3) Despitethe differences in the textbooks, the students can beevaluated using a common test, and their answers can berated. The key lessons that we drew from the textbook-authoring process is to aim for a process in which theproject experts could review a representation and havean objective test for evaluating the knowledge in thesystem. We developed an extensive set of knowledge-engineering guidelines that prescribe how the domainexperts should go about capturing textbook knowledge[31,40]. Just as a textbook undergoes a review process, therepresentations undergo a review process that ensures anadequate application of the guidelines. This review doesnot mean that a representation meets an experts personalview on how the knowledge should be modeled, but ra-ther ensures that the established encoding guidelines areadequately applied. Question and answer pairs stated inEnglish provide a natural objective test to check the ad-equacy of the representation in the same way as studentscan be objectively tested on an exam.Inventory of representation requirementsThe representation requirements can be put into twocategories: (1) requirements that are already supported inCLIB and (2) requirement that are not currently sup-ported. When we cannot model a universal truth in astraightforward manner by using the constructs availablein the CLIB, we note this as a new KR requirement. Thenew KR requirements are strongly dependent on the stateof CLIB at the time of the analysis. For answering thequestion of whether the same upper ontology could beused across multiple textbooks, however, the primary issueis the applicability of the representations supported inCLIB and the commonality of each new requirementacross different textbooks.KR requirements can arise due to the following rea-sons: (1) The knowledge can be represented by using thecurrent features of the representation language andCLIB, but no established knowledge-engineering guide-lines exist to handle it. We refer to the challenges arisingdue to this reason as process issues. (2) Representing theknowledge requires intervention from a knowledge engin-eer to extend the upper ontology. We refer to the issuesarising due to this reason as requiring knowledge-engineersupport. (3) Representing the knowledge is a topic ofcurrent and future research, and the current research hasnot yet been incorporated into the project. We refer tosuch issues as requiring research and application. We nowgive an inventory of the KR requirements that were en-countered during the process, and we indicate into whichof the above three categories each requirement fell.Negative informationWe say that a UT has a negative information KR issue ifit cannot be modeled by using any of the four existingmethods for handling negative information: (1) disjoint-ness between classes, (2) cardinality constraints, (3) rela-tions with negative meaning, and (4) negative values. Asan illustration, consider the following sentence fromRaven: Because these chains are nonpolar, they do notform hydrogen bonds with water, and triglycerides areChaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 9 of 19http://www.jbiomedsem.com/content/5/1/51not water-soluble. Here, we can state that a polar mol-ecule is disjoint from a nonpolar molecule (to capturethe nonpolarity), and we can assign a value of insolubleto the property solubility-in-water. In principle, onecould introduce a slot with negative meaning (for example,does-not-form, or use a qualified number constraint onall Create processes in which Nonpolar-Chains participatethat asserts that the result contains exactly zero Hydrogen-Bonds). However, no established methodology exists re-garding which approach to use. Therefore, dealing with theexample of negative information considered here is aprocess issue.Missing relationshipsWe say that a UT cannot be expressed because of a missingrelationship if the necessary relationship is missing fromthe vocabulary. An issue already known based on our workwith Campbell is the lack of certain spatial relationships.As an illustration of this issue, consider the following sen-tence from Raven: Although the distribution of membranelipids is symmetrical in the ER where they are synthesized,this distribution is asymmetrical in the plasma membrane,Golgi apparatus, and endosomes. Here, we need a newrelation to capture asymmetrical distribution. Missing rela-tionships require knowledge-engineer support.Inability to state graded quantifiersRecall that whenever the textbook uses words such asmany, most, typically, etc., our KE strategy is tofind a more-specific subclass for which the statement isuniversally true. This strategy breaks down when thetextbook does not contain information about such a spe-cific subclass. For example, consider the following sentencefrom Levine: Most prokaryotes and many eukaryotes havecell walls. The main difference between this sentence andthe sentence: Many cells, including most prokaryotes, alsoproduce a strong supporting layer around the membraneknown as a cell wall, which we considered earlier, is thethat Levine does not offer any specific examples of cellsthat do contain cell walls, so we cannot apply our KE strat-egy that worked for the earlier sentence. Whenever weencounter such a situation, we label it as an inability tostate graded quantifiers, and it is a research and applica-tion issue.Modeling biological models and reified statementsThe textbooks frequently describe models and theoriesabout natural phenomena. The statements about modelsare not universally true statements, but instead arecontextual statements that hold true only in the context ofthat model. As an illustration, consider the following state-ment from Alberts: These regions cannot be identified inhydropathy plots and are only revealed by x-ray crystallog-raphy, electron diffraction (a technique similar to x-raydiffraction but performed on two-dimensional arrays ofproteins), or NMR studies of the protein's three-dimensionalstructure. Here, the presence of the regions is contextualto a particular set of techniques. Such knowledge can becaptured in AURA, but the relevant guidelines have notbeen developed yet, and therefore, it is a process issue.Property value comparisonA need frequently exists to compare property values. TheCLIB ontology contains several comparison operators forproperties, but we saw some examples where none of theexisting operators were directly applicable to some sen-tences in the new textbooks. For example, consider the fol-lowing sentence from Raven: However, at the end of eachaction potential, the cytoplasm contains a little more so-dium and a little less K than it did at rest. Here, weneed qualitative operators to capture relationships suchas little more and little less. This issue requiresknowledge-engineer support.CausationThe notion of causality associated in the context of pro-cesses where causal relationships of events are of primaryinterest is already supported in CLIB. The textbook veryoften explains things by using the words such as be-cause, causes, etc. We use the category label of caus-ation to capture such issues as the current CLIB does notprovide support to model such information. For example,consider the following sentence from Alberts: The shapeand amphiphilic nature of the phospholipid moleculescause them to form bilayers spontaneously in aqueousenvironments. This KR requirement requires both researchand application.DisjunctionA need arises to capture two or more alternatives in aUT that cannot be modeled by another means. Forexample, consider the following sentence from Alberts:Hydrophilic molecules dissolve readily in water becausethey contain charged groups or uncharged polar groupsthat can form either favorable electrostatic interactions orhydrogen bonds with water molecules. This KR require-ment requires both research and application.ConditionalityCapturing a conditional statement in a UT that cannot bemodeled by another means is sometimes necessary. Ourgeneral approach for capturing conditional statements hasbeen using the class hierarchy. We create a new class, andthe if part of the condition becomes a sufficient propertyfor that class, while the else part of the conditionbecomes the necessary properties of that class. Such anapproach works for most situations; but in some cases, itleads to unnatural classes, and thus is undesirable. ForChaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 10 of 19http://www.jbiomedsem.com/content/5/1/51example, consider the following sentence from Alberts:This change of state is called a phase transition, and thetemperature at which it occurs is lower (that is, the mem-brane becomes more difficult to freeze) if the hydrocarbonchains are short or have double bonds. Here the condi-tionality is between the temperature and the properties ofhydrocarbon chains. If we model this knowledge by usingsufficient properties, then creating unnatural classes, suchas phase transition for short hydrocarbon chains, wouldbe necessary. Handling this requirement is a process issue.PossibilityMany sentences make statements of the form A can B,without necessarily stating that A always does B. Werefer to the representation needs of such sentences aspossibility. For example, consider the following sen-tence from Alberts: The free hydroxyl group contrib-utes to the polar properties of the adjacent headgroup, as it can form hydrogen bonds with the headgroup of a neighboring lipid, with a water molecule,or with a membrane protein. Dealing with this KRrequirement is a research and application. Initial stepsin this direction could be undertaken by using researchresult on representing dispositions [41].Data interpretationIn the advanced textbooks, figures are shown thatcontain representative data. The text then describes theform of the data and what conclusions either were orcould be derived from this data. Thus, the figures arenot just meant to illustrate a model but also to teachstudents how the actual data led to a set of conclusions.As an illustration, consider the following sentence fromAlberts: In a normal unclamped axon, an inrush ofNa + through the opened Na channels produces thespike of the action potential; inactivation of Na chan-nels and opening of K channels bring the membranerapidly back down to the resting potential. Dealingwith this requirement is a research and applicationissue.Science as a processParticularly in Kandel and also in Alberts, many of thebiological concepts are presented in the context of theprocess of science, i.e., scientists go through a process oftesting, interpreting data, and developing hypothesesthat are then tested again. For example, consider thefollowing sentence from Kandel: A simple interpretationof these results is that the depolarizing voltage step se-quentially turns on active conductance channels for twoseparate ions: one type of channel for inward current andanother for outward current. Dealing with this require-ment is a research and application issue.Qualitative number constraintOur current representation approach enables quantita-tive number constraints. We saw several examples in thetextbooks where the constraint values are qualitative, andno other encoding approach sufficed. For example, con-sider the following example from Raven: Mammalianmembranes, for example, contain hundreds of chemicallydistinct species of lipids. Dealing with this requirementrequires knowledge-engineer support.Mathematical reasoningCLIB provides two different representations to facilitatemathematical reasoning: (1) simple qualitative relationshipssuch as direct proportionality and (2) reasoning with math-ematical equations. However, Kandel presents more com-plicated equations beyond CLIBs current representationaland reasoning capabilities. Kandel also includes derivationsof mathematical formulas that cannot be represented byusing current capabilities. For example, consider the follow-ing sentences from Kandel: When tetraethylammonium isapplied to the axon to block the K+ channels, the totalmembrane current lm, consists of lc, lv and lNa. This outwardcurrent reaches a plateau that is maintained for the dur-ation of the pulse (Figure nine-3B). Dealing with this re-quirement requires knowledge-engineer support.Vagueness/ambiguityAdvanced textbooks cover frontiers of our knowledge,and hence, this vagueness or ambiguity is not due topedagogical presentation. However, it can lead to a uni-versally true statement that is relevant but too vague toproperly encode. These sentences are found across alltextbooks, and seem to be more common in Alberts.(For example: Membrane attachment through a singlelipid anchor is not very strong, however, and a secondlipid group is often added to anchor proteins more firmlyto a membrane.) Dealing with this requirement requiresresearch and application.Other issuesWe use the KR category of other issues for representationproblems that do not clearly fit into any of the previous cat-egory. For example, consider the following sentence fromRaven: From this simple molecular framework, a large var-iety of lipids can be constructed by varying the polar organicgroup attached to the phosphate and the fatty acid chainsattached to the glycerol. Here the author is trying to con-vey the salient variance between different phospholipids.Certain aspects of this knowledge are easily captured assufficient properties, but that approach may not always beenough, especially to answer a question of the form Howcan you get different instances of a phospholipid? For thepurposes of answering similarity and difference questions,Table 2 Data on the length of description of actionpotential and membrane potentialAction potential Membrane structureTextbook Pages Images Sentences Pages Images SentencesCampbell 7 7 91 6 12 160Raven 10 9 58 6 5 91Levine 2 2 37 2 1 17Alberts 14 14 20 12 18 270Kandel 20 16 280 4 1 75Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 11 of 19http://www.jbiomedsem.com/content/5/1/51and relationship questions, a representation based onsufficient properties is adequate.Reasoning requirements analysisThe goal of the reasoning requirements analysis wasanswering the question: to what extent can the questionsof interest for a new textbook be answered by using thereasoning mechanisms already available in AURA? Wewanted to confirm that as we move across textbooks, wewould not have to develop new sets of reasoning methodsfor answering questions. To perform the analysis, thedomain-expert team developed sample questions aboutmembrane structure and action potential for each of thefour textbooks. The overall guidance was to focus on thekinds of questions that a student studying from the bookmight have. The biologists had access to the examples ofeducationally useful questions that we had previouslydeveloped for Campbell. Some variability in the style anddifficulty of questions potentially exists, because we didnot have a mutual validation of question sets authored bydifferent biologists. The possibility also exists that webiased their question-authoring effort by showing themthe questions from the prior effort on Campbell. However,because the questions from the previous effort receivedextensive feedback from multiple teachers and students,we believe that they were a good guideline for this exercise.The domain-expert team and the knowledge-engineeringteam jointly analyzed the questions.The questions stated in English needed to be translatedinto the question templates supported by the system. Suchtranslation is done by AURAs question-understandingmodule [42]. In many cases, the English statement of aquestion is not very helpful for determining the computa-tion that must be performed in answering that question.For example, consider the question: How does the pos-ition of the gates in gated proteins cause the blocking of themovement of ions across the membrane? We can re-formulate this question as: What is the causal relation-ship between the position of the gates in gated proteins andthe blocking of the movement of ions across the mem-brane? Another formulation of the same question is:How are the position of the gates in gated proteins andthe blocking of the movement of ions across the membranecausally related? In AURA, both of these formulationswill be handled by using Q6, in which we search forthe causal relationships between the two entities inthe question, and we expect the answer to be containedin the retrieved path. To develop such reformulations, theknowledge engineers must extensively rely on their know-ledge of AURA to determine whether a given question inthe corpus could be translated into one of the existingtemplates. This approach introduces some imprecisioninto the analysis, but this is unavoidable without undertak-ing the actual implementation.Results and discussionWe now consider the results of our analysis of domainknowledge, and representation and reasoning require-ments, for the four textbooks.Results and discussion on domain knowledgeanalysisWe first analyze the two topics that we chose for compari-son: action potential and membrane structure, and thenoffer conclusions based on the analysis.In Table 2, we summarize data about the length of de-scription of the different topics across the five textbooks.To the extent that different textbooks emphasize differ-ent levels of detail, the corresponding KBs need to matchthat level of detail. To make this observation concrete, weconsider below specific example comparisons of contentacross the three textbooks.Campbell covers membrane structure in greater depththan Levine, Raven, or Kandel, but is limited in itsdescription of the molecular structure of phospholipids.Raven and Alberts devote more detail to the molecularstructure of phospholipids. Levine introduces lipids buthas no mention of their more specific forms, such asglycolipids, which are mentioned in the other textbooks.In Kandel, membrane structure is not a major topic (itis more a topic in general biology than in neuroscience).Campbell describes equilibrium potential by providing adefinition and presenting an equation for the mathemat-ical model known as the Nernst equation, along with twoexamples using this equation. Raven provides a similaramount of information to Campbell, but omits any exam-ples using the Nernst equation. Alberts provides a defin-ition, derives the Nernst equation, and shows severalexamples. Kandel provides the greatest breadth and depthfor membrane potential, and devotes an entire chapter(Chapter 8) to the passive electrical properties of theneuron that are important for understanding the influenceof neuronal structure and other properties on short andlong-range signaling. Kandel also covers the contributionof different types of membrane channels to the signalingproperties of different parts of the neuron.Next, we consider the biological themes that occurinconsistently across our sample of textbooks: evolution,Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 12 of 19http://www.jbiomedsem.com/content/5/1/51disorders/disease, scientific uncertainty, and animal models.Campbell, Raven, and Levine do not mention evolution inthe context of action potential, but Alberts and Kandeldiscuss evolution of action potential function and structureof membrane proteins, respectively. Although Campbelland Raven omit discussion of disease, Levine, Alberts, andKandel provide examples of diseases that affect normalfunctioning of action potentials. The presentation ofscientific uncertainty also varies considerably across text-books. Raven omits any mention of scientific uncertaintyin the context of action potential, while Campbell andLevine simply report its existence. Alberts suggests thatscientists will resolve uncertainty without exception, butKandel presents scientific inquiry with respect to actionpotential as an iterative process with some degree of un-certainty. Animal models for the study of action potentialare not described in Levine, and a single experimentalmodel is described in both Raven and Campbell. AlthoughKandel describes a single experimental model, the giantsquid axon, this text also emphasizes experimental tech-niques and their specific role in elucidating aspects of theaction potential. Alberts describes multiple experimentalmodels for the study of action potential.The examples above suggest a great deal of common-ality as well as differences in how different topics are de-scribed across the textbooks. For example, on the topicof membrane structure, the KB for Levine will containfar fewer terms than the other KBs (e.g., terms such asGlycolipid would need to be omitted.) Similarly, the KBfor Alberts and Raven will provide a much more detailedaccount of phospholipid structure than the KB forTable 3 Observed knowledge representation issuesCategory of KR issue Occurs in textbooksLevine RavenNegative information xSpatial relation x xMissing slot (other than spatial relation) xInability to state graded quantifiers x xBiological models and reified statements x xProperty-value comparison xCausationDisjunction xConditionalityPossibilityData interpretationScience as a processQualitative number constraint xMathematical reasoningVagueness/ambiguity xOther xCampbell. Similarly, while the Nernst equation willexist in all the KBs, the example associated with its use(as in Alberts), and a description of electrical properties (asin Kandel), will be specific to the KBs for those textbooksonly. Differences in how to handle evolution, uncertainty,diseases, and animal models can have major repercussionsin KB design.Our analysis above suggests that a great deal of com-monality across textbooks can be leveraged in creating aKB for each of them. At the minimum, the experience andrepresentation approaches developed for one textbook cancontribute toward a faster design of representations for adifferent textbook. Our analysis does not provide sufficientinformation about whether the domain-specific axiomwriting for the textbook for a new KB should begin fromscratch or should reuse the axioms from the previousones. Clearly, some reuse should be possible, but the ex-tent of reuse and its cost effectiveness is an open question.Further, our analysis provides concrete examples of wherethe textbooks have substantial differences requiring repre-sentation design that is specific to that textbook.Results and discussion on knowledge representationrequirementsIn Table 3 below, we summarize all the KR issues alongwith the textbooks for which the issue was encountered.The column labeled as New issue indicates an issuethat we have not encountered or so far addressed in ourwork with Campbell Biology.In Table 4 below, we show the results that indicate thenumber of UTs for each of the textbooks that could notOccurs inCampbell?Newissue?Alberts Kandelx x x Nox x x Nox x x Nox x Nox x x Nox Nox x Nox x Nox x Nox x Nox x Yesx x x Nox Nox x Nox x x Nox NoTable 4 KR requirements by category, for the topic action potentialCategory of KR/KE issue Number of UTs affected (%)Levine Raven Alberts KandelNegative information 3 (8%) 6 (6%) 3 (2%)Spatial relation 3 (2%)Missing slot (other than spatial relation) 4 (8%) 3 (3%) 3 (2%) 6 (7%)Inability to state graded quantifiers 1 (1%) 2 (2%)Modeling biological models and reified statements 1 (1%) 3 (3%)Property-value comparison 3 (3%)CausationDisjunction 1 (3%)ConditionalityPossibility 11 (8%)Data interpretation 11 (8%) 4 (4%)Science as process 8 (9%)Qualitative number constraintMathematical reasoning 3 (3%)Vagueness/ambiguity 1 (1%) 2 (1%)OtherTotal 8 (21%) 13 (13%) 35 (26%) 26 (30%)Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 13 of 19http://www.jbiomedsem.com/content/5/1/51be adequately represented for the topic of action poten-tial. For each UT that could not be represented, we iden-tify a knowledge representation category to indicate thenature of requirement. We next explain these results foreach of the textbooks.For Levine, approximately 20% of UTs for action po-tential had new KR requirements. In addition, negativeinformation that could not be adequately encoded oc-curred for action potential, and we encountered oneinstance of disjunction that could not be adequatelyencoded. The issues of lack of specificity and models didnot arise for action potential for this text. For the Raventextbook, 13% of the UTs were problematic. For Alberts,approximately 25% of UTs presented new KR requirementsfor action potential. In addition, the new KR requirementof data interpretation arose. In Kandel, approximately 30%of UTs presented new KR requirements. Thus, like Levinebut unlike Raven and Alberts, Kandel presented propor-tionally more issues for action potential. For example, datainterpretation issues and science as process issues arosefrequently. Further, for action potential, Kandel containedsentences outside AURAs current mathematical represen-tation and reasoning capabilities.In Table 5 below, we show our results of how well wecould represent the topic of membrane structure foreach of the four textbooks. Detailed explanations follow.For Levine, we encoded approximately 85% of UTs with-out any facing any new requirements. The most commonnew KR requirements were missing relations (namely,spatial relations), and the inability to identify a sufficientlyspecific concept, as illustrated in the earlier example.Raven exhibits a greater percentage and breadth of newKR requirements than Levine. Nearly 35% of UTs did havenew KR requirements. The most common KR require-ments were, again, specificity of concepts and missingslots. A common requirement for Raven was representingbiological models. Raven (and the other textbooks) hadseveral examples of negative information of a form thatcannot be represented with AURAs current capabilities.Alberts has a similar percentage of new KR requirementsto Raven and a greater breadth. Again, more than 30% ofUTs posed some new KR requirement. Further require-ments come from conditionality, causation, and possibility.Because Alberts is a research-oriented textbook, it de-scribes topics at the limit of current biological knowledge.This leads to the greater number of UTs with the KR issuesof vagueness compared to other textbooks. For Kandel,more than 80% of UTs were encoded without facing anynew KR requirement, and no new requirements arose thatdid not arise for another textbook, except the need torepresent knowledge about science as a process. Hence, interms of number of issues, Kandel proved amenable to ourKE process despite its more advanced nature.Let us now consider how these results address the ques-tion: to what extent can the same upper ontology be usedto model knowledge across a range of life science text-books? The results in Table 3 suggest that all the require-ments that were identified for the new textbooks, with theexception of data interpretation, were also requirementsfor Campbell. This finding is strong evidence in supportTable 5 KR issues by category, for the topic membrane structureCategory of KR/KE issue Number of UTs affected (%)Levine Raven Alberts KandelNegative information 3 (1%) 17 (7%) 2 (2%)Spatial relation 3 (4.5%) 17 (7%) 15 (6%) 2 (2%)Missing slot (other than spatial relation) 6 (2.5%) 8 (3%) 4 (4%)Inability to state graded quantifiers 6 (9%) 28 (12%) 10 (4%)Modeling biological models and reified statements 1 (1.5%) 19 (8%) 1 6 (7%)Property-value comparisonCausation 2 (1%)Disjunction 1Conditionality 5 (2%)Possibility 3 (1%)Data interpretationScience as process 2 (2%)Qualitative number constraint 1Mathematical reasoningVagueness/ambiguity 2 15 (6%)Other 3 (1%)Total 10 (15%) 79 (33%) 77 (31%) 16 (18%)Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 14 of 19http://www.jbiomedsem.com/content/5/1/51of the claim that if these requirements were supported inan upper ontology, such ontology would be applicableacross multiple textbooks. From Table 3, we also see thatspatial relationships and biological models are the require-ments that occur most uniformly across the textbooks,followed by negative information, graded quantifiers, andscience as a process. These constitute high-priority areasfor extending the CLIB ontology.From Tables 4 and 5, we see that the existing upperontology enabled us to capture at least 67% of all the UTsacross all topics and across all the textbooks. In somecases, the coverage was as high as 87%. Based on these re-sults, we can conclude that CLIB already provides a goodfoundation for representing knowledge across the range oflife science textbooks considered here.Results and discussion on reasoning requirementsRecall that our high-level question regarding reasoningrequirements was: To what extent can the questions ofinterest for a new textbook be answered by using thereasoning mechanisms already available in AURA? Wegave an overview of the current questions supported inan earlier section.To answer the above question, we assembled a suite ofnew questions for each of the four textbooks and putthem into two different categories: (1) answerable withexisting system capabilities, or minor extensions of them,supposing that the requisite concepts are encoded; and(2) require new reasoning capabilities, or major extensionsof existing capabilities, or beyond anticipated feasiblereasoning, or contingent on significant new research.We will now present the results of our analysis andwill illustrate the questions that fall into each of thesecategories.In Table 6 below, we summarize the overall analysis ofquestions about action potential and membrane structure.Across the four textbooks on average, we observe thatfor action potential, approximately 85% of the questionsare category 1 (existing capability), and 15% are category2 (representational extension or significant reasoning re-quirements). For membrane structure, nearly 90% of thequestions are category 1 (existing capability), and 10%are category 2 (representational extensions or significantreasoning requirements).From each of the four textbooks, we now give examplequestion forms that could be answered by using the exist-ing capability. For each question form, we give an examplequestion, its model answer if provided, and a reformula-tion of the question. Because each of these question formscan be answered by using the existing capability (or aminor extension of it) through the given reformulations,new question templates are not required.? Question template in English: What is the role of X(in context Y)?? Example instantiation from the sample question set:In the equivalent electrical circuit model, whatcellular element serves as the resistor? [Kandel]Table 6 Reasoning requirements analysis for action potential and membrane structureAction potential Membrane structureTextbook Questions Existing Research Questions Existing ResearchRaven 16 12 4 21 17 4Levine 53 43 10 32 25 7Alberts 19 16 3 48 47 1Kandel 50 43 7 29 26 3Total 138 114 22 130 115 15The column labeled as Questions indicates the total number of questions considered in the analysis. The column labeled as Existing indicates the number ofquestions that could be handled by using existing capabilities in AURA, and the column labeled as Research indicates the number of questions that cannot behandled by the current capabilities in AURA and that require further research.Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 15 of 19http://www.jbiomedsem.com/content/5/1/51? Question template in English: Why is it importantthat X has property Y?? Example instantiation from the sample question set:Why is it important that membranes are selectivelypermeable? [Levine]? Reformulate as: How does the selectivepermeability of membranes facilitate its function?? Question template in English: What kinds of X arecommon in Y?? Example instantiation from the sample question set:What kinds of lipids are common in cellmembranes? [Levine]? Reformulate as: What are the lipid parts of a cellmembrane?? Question template in English:What does X do during Y?? Example instantiation from the sample question set:What is the sodium potassium pump doing duringan action potential? [Levine]? Reformulate as: What does sodium potassium doduring an action potential?? Question template in English: What features of Xaffects its role in Y?? Example instantiation from the sample question set:What features of the voltage-gated sodium channelaffect its role in an action potential? [Raven]? Reformulate as: What is the relationshipbetween a voltage-grated sodium channel andaction potential? (This reformulation isapproximate as it does not specifically ask forthe relationship to role in the action potential.)We now consider example questions that require newquestion templates. For each, we give an example ques-tion template and its instantiation.? Question template in English: What is the importanceof X?? Example instantiation from the sample question set:What is the importance of plasma membranefluidity? [Alberts]? Question template in English: What aspects of X canbe seen by Y? (where Y is a inspection technique,instrument, or process)? Example instantiation from the sample question set:What aspects of the plasma membrane can be seenby transmission electron microscopy (TEM)? [Raven]? Question template in English: What properties of Xcontribute to the property Z of Y?? Example instantiation from the sample question set:What characteristic of phospholipids contributesmost to the membrane-forming properties of thesemolecules? [Alberts]? Question template in English: Given that X does Y,why does Z also not do Y?? Example instantiation from the sample question set:Given that the sodium-potassium pump results in anet transport of positive ions from the inside of the cellto the outside, why don't negative ions also leave thecell to balance out the charge difference? [Raven]? Question template in English: Which strategy does Xuse to achieve Y?? Example instantiation from the sample question set:Vertebrate systems generally rely on what adaptivestrategy for increasing the rate of axonalconduction? [Kandel]The quantitative results in Table 6 support the conclu-sion that a large fraction of the questions in the testsuite assembled by the domain experts (greater than85%) for a new textbook could be answered by using thereasoning mechanisms already available in AURA. Thisfinding is an extremely positive result that attests to thegenerality of the already-implemented reasoning mecha-nisms. However, we would like to emphasize that giventhe bias introduced by exposing the domain experts tothe existing capabilities, we should not take these resultsto conclude that the existing capabilities could answergreater than 85% of all possible questions posed againstthese textbooks. These results are applicable to only to aspecific style of educationally useful questions that haveChaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 16 of 19http://www.jbiomedsem.com/content/5/1/51been found helpful in our work on the intelligent text-book. These results show that such questions have a highdegree of generality and applicability across the range oftextbooks considered in this analysis.Comparison to related work and broader impactsIn this section, we relate the work presented here to re-lated efforts in modeling knowledge by using OWL andother biomedical ontology development efforts. We alsocomment on how our work can be exploited by others.Most of the representation features used in AURA arealso found in OWL (for example, classes; class-subclassrelationships; disjoint statements between classes; domain;range; qualified number constraints; etc.). Our work tocapture graph-structured knowledge of the sort illustratedin axioms A1A4 is closely related to recent efforts toextend OWL to capture graph-structured descriptions[17]. Others have recognized the need to support graph-structured descriptions to capture chemical structures[16], and active research is underway to address it [17-20].KB Bio 101 already contains several hundred examples ofcomplex concepts that utilize such graph-structured repre-sentation [13], such as the ones shown in Figures 1 and 2.One possible technique to achieve decidable reasoning in aKB with graph-structured descriptions is to avoid certainkinds of cyclical dependence among concepts [17], butno empirical evaluation exists of such a technique on arealistic, large-scale dataset. KB Bio 101 is an excellentcandidate data set for undertaking such evaluation. Moregenerally, KB Bio 101 can be used as a dataset for testingtechniques for ontology modularization, ontology map-ping, ontology evaluation, development of ontology designpatterns, etc.In several prior publications, we related the represen-tations supported in CLIB with the ones adopted for bio-medical ontologies (for example, in [32], we describe ourrepresentation for structure and function; in [43], we de-scribe representation of roles; and in [44], we describe therepresentation of genetic entities). Gene Ontology or GO[45] is a closely related community-wide effort that sup-ports molecular-level and cellular-level representations forgene function. Because life science textbooks cover know-ledge at organismal, species, and population levels, thescope of knowledge represented in KB Bio 101 is muchbroader than the knowledge represented in GO.A unique feature of our ontology that none of the otherbiomedical ontologies supports is a vocabulary of processclasses (e.g., Move, Attach, Release, etc.) and their detaileddefinitions using semantic relationships (e.g., agent,object, source, destination, etc.). Due to lack of suchvocabulary, ontologies such as GO define functions usingonly textual strings and functions are not compositionallydefined to capture their complete meaning. The CLIBapproach to modeling processes and their participantscan be readily exploited by biomedical ontologies toachieve a much greater depth of knowledge capture forbiological functions.A driving use case for GO, and a major contributor toits success, has been its use in annotation projects. Thequestion templates Q1Q6 introduced in our work canprovide another compelling use case for exploiting GOand other biomedical ontologies. Although Q1Q6 weredriven by the needs of education applications, similarreasoning can be useful for biological discovery applica-tions such as [46].Many educational innovations begin at the graduatelevel, and slowly find their way to undergraduate andprecollege-level education. Therefore, perhaps, the mostimpactful way to exploit this work is using it as an ex-ample to start incorporating biomedical ontologies intoundergraduate and high-school-level curricula for life sci-ence education. Future life sciences graduates will need toroutinely use ontology resources, and some of these gradu-ates will need to help create new ones. However, ontologiesare not yet a standard part of the life sciences curriculum.Students are not normally exposed to ontologies unlessthey enter a graduate program in bioinformatics. Webelieve that now is the time to begin making training informal languages and their ontological commitments anintegral part of the life sciences curriculum. Wider use ofontologies in the life sciences will lead to better under-standing and communication of knowledge by teachersand students. Such explicit usage of ontologies is differentfrom the methods used by search tools such as Google,which are excellent for retrieval but do little to improveour understanding of the subject matter.ConclusionsWe present our conclusions for each of the three majoranalyses presented here: (1) domain knowledge require-ments, (2) knowledge representation requirements, and(3) reasoning requirements. We acknowledge at the out-set that our conclusions are based on the data gatheredfor the topics of action potential and membrane struc-ture. Our generalized conclusions are based on the hy-pothesis that these data could be generalized to otherbiological topics in the textbook.The results of our domain requirements analysis showthat, as expected, the Levine textbook, which is aimed at alower instructional level than Campbell, presents materialfrom a more general perspective, omitting details thatCampbell and Raven include. Likewise, the textbooksaimed at a higher instructional level than Campbell presentdetails that Campbell does not. The breadth and depth ofcoverage for action potential and membrane structureappear most similar between Campbell and Raven. We alsofound that the textbooks for instruction levels higher thanCampbell and for a specific field of biological sciences doChaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 17 of 19http://www.jbiomedsem.com/content/5/1/51not cover the broad range of knowledge in Campbell butinstead rely on Campbells prerequisite biology knowledge,and build on a fraction of this foundation. For example,Kandel provides considerably less breadth on the topic ofmembrane structure compared to Campbell. The details ofmembrane structure are likely omitted from Kandel be-cause the authors deem such information as prerequisite ornot germane to the sub-discipline of neuroscience. Our re-sults suggest that the modeling effort invested in represent-ing any of these books will reduce the cost of doingadditional books. Because the considered textbooks vary indetail, and in their choice of the aspects of biology know-ledge to emphasize, the KB for each of these textbooks alsomust be customized and made specific to that particulartextbook.The results of our knowledge representation require-ment analysis showed that the knowledge-engineeringprocess used for Campbell appears to be effective acrossthe range of considered textbooks. We encountered nomajor surprises regarding modeling issues: most ofthe issues that we saw in these textbooks also exist insome form for Campbell. We confirmed that the stud-ied textbooks that were written for the same grade level(i.e., Campbell and Raven) were comparable in theirknowledge content and representation requirements. Wefound an increase in presentations of theories, models,and history in the higher-level textbooks, which is ex-pected as the textbooks for the higher grade levels arecloser to the frontiers of knowledge. For example, Kandeldescribes the experiments that are used to test a model orhypothesis, and the reasoning process that was used tosupport or refute that model. Our overall conclusion wasthat our existing representation tools are applicable formodeling knowledge across the range of considered text-books, and that the new requirements identified here willhave broad applicability to multiple textbooks.Based on the reasoning requirements analysis, wecan conclude that a majority of the biologist-authored,educationally useful questions for each of the textbookscan be adequately addressed by using extensions toAURAs current capabilities. This assertion is true becauseall the textbooks had the same foundational set of ques-tions and were all based on the same foundational biology.The reasoning patterns of relationship questions and com-parison questions seem to be directly applicable acrossmultiple textbooks. We also found that the answers forone textbook may contain vocabulary or detail that isunexpected at a different grade level. For example, Levinedoes not use the term phospholipid bilayer. In Kandeland Alberts, most answers are with respect to models andcannot be considered as universally true. We definitelycannot conclude that the existing question templates areadequate for the space of all questions that the readers ofeach of the textbooks might want to ask. Our previouswork with Campbell also showed us that the existingtemplates are inadequate for capturing all the reasoningpatterns.A possible way forward is aligning the presented repre-sentations and approach with the methods that arealready commonplace in biomedical research, and thenstart incorporating those representations in life sciencestextbooks. As an example, consider the representationfor Kandel. Especially for a field as broad as neurosci-ence, different groups will need to be engaged for differ-ent parts of a text like Kandel. In fact, in this textbook,different experts author different chapters to ensure thatthe content aligns with current thinking in the field.Efforts are underway through projects like the InternationalNeuroinformatics Coordinating Facilityi, the Blue BrainProjectj, and the Neuroscience Information Frameworkkto create a semantically unified body of broad neurosci-ence knowledge. When textbook knowledge is comple-mented with resources like these, the enhanced version isnot only useful for biomedical research but can also serveas a valuable education tool.Undertaking textbook knowledge representation as pro-posed here will profoundly shift the way we think of lifescience education. The semantic representations wouldserve as a conceptual mathematics that computers couldrigorously reason over. Exposure to such representationsas part of a life science education will likely instill grad-uates with an increased level of rigor in learning andworking with biological concepts. The time is nowripe to introduce these techniques at all levels of biol-ogy education, so that students are well prepared for thecomputational thinking [47] that is both so vital topractitioners in todays knowledge economy and indis-pensable for researchers pursuing advanced biomed-ical discoveries.Endnotesahttp://www.aaaivideos.org/2012/inquire_intelligent_textbook/bAlmost every universally true statement in biologyhas an exception. For example, there are eukaryotic cellsthat do not have a nucleus.chttp://www.w3.org/TR/owl2-syntax/dhttp://www.tptp.orgehttp://www.ai.sri.com/~halo/public/exported-kb/biokb.htmlfhttps://bioportal.bioontology.org/ontologies/AURAghttp://www.ai.sri.com/~halo/public/clib/20130328/clib-tree.htmlhThe 5th edition of Kandel appeared when our studywas underway.ihttp://incf.orgjhttp://bluebrain.epfl.chkhttp://neuroinfo.orgChaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 18 of 19http://www.jbiomedsem.com/content/5/1/51AbbreviationsAURA: Automated User Centered Reasoning and Acquisition System;CLIB: Component Library; GO: Gene Ontology; KB: Knowledge Base;KE: Knowledge Engineering; KM: Knowledge Machine; KR: KnowledgeRepresentation; OWL: Web Ontology Language; UT: Universal Truth.Competing interestsThe authors declare that they have no competing interests.Authors contributionsVKC was the technical leader for the project and was responsible forformulating, directing, and managing the project, and for writing the finalreport. DE served as a knowledge engineer and conducted the knowledgerepresentation and reasoning requirements analysis. A. Goldenkranz servedas a domain expert and performed the domain analysis tasks for Levin. A.Gong served as a domain expert and performed the analysis tasks for Raven.MM served as a domain expert and performed the analysis tasks for Kandel.WW served as a domain expert and performed the analysis tasks for Alberts.NYS served as a knowledge engineer, conducted the knowledgerepresentation and reasoning requirements analysis, and helped in writingthe report. All authors read and approved the final manuscript.Authors informationDr. Vinay K. Chaudhri is a program director in the Artificial Intelligence (AI)Center at SRI International. His research focuses on the science andengineering of large knowledge base systems and spans knowledgerepresentation and reasoning, question answering, knowledgeacquisition, and innovative applications. His most recent work has been oncreating an intelligent textbook in biology that answers a students questionsand leads to significant learning gains. He has co-edited a volume on theTheory and Application of conceptual modeling, and two special issues ofAI Magazine  one on Question Answering Systems, and another onApplication of AI to Contemporary and Emerging Education Challenges.He has taught a course on Knowledge Representation and Reasoning atStanford University. He holds a Ph.D. in Computer Science from Universityof Toronto where he was a Connaught Scholar. He also holds a Masters inIndustrial and Management Engineering from Indian Institute of TechnologyKanpur, and a Bachelors degree in Mechanical Engineering fromNational Institute of Technology, Kurukshetra. He is a senior memberof AAAI.Daniel Elenius is a Computer Scientist at the Computer Science Lab at SRIInternational. He holds an MS in computer science and engineering fromLinköping University, Sweden. He has developed several reasoning systems,including a policy reasoner for the DARPA neXt Generation (XG) program, aprobabilistic fault propagation analysis tool for the DARPA META program,and a system that reasons about hierarchical tasks and resource assignmentsfor the DoD ONISTT and ANSC projects. His research interests includeautomated reasoning, knowledge representation, and the semantic web.Andrew Goldenkranz is a biology teacher at Monta Vista High School inCupertino, California. He has helped position the Inquire application (an iPadapp for AURA) so that it is useful for teaching students studying fromCampbell Biology.Allison Gong studies marine biology, particularly invertebrate zoology, andteaches biology at the community college and university levels in California.She teaches marine biology, zoology, and evolution to science majors andscience-phobes alike. Her interests in marine biology focus on marineinvertebrate life histories, larval biology, and ecology of the rocky intertidal. Sheholds a PhD in biology from the University of California.Maryann E. Martone received her BA from Wellesley College in biologicalpsychology and her PhD in neuroscience in 1990 from the University ofCalifornia, San Diego, where she is currently a professor in the Departmentof Neuroscience. She is the principal investigator of the NeuroinformaticsFramework project, a national project to establish a uniform resourcedescription framework for neuroscience. Her recent work has focused onbuilding ontologies for neuroscience for data integration. She has completedher tenure as the US scientific representative to the InternationalNeuroinformatics Coordinating Facility (INCF), where she still heads theprogram on ontologies. MM recently joined FORCE11, an organizationdedicated to advancing scholarly communication and e-scholarship, asExecutive Director.William Webb is an expert in wildlife biology and has taught biology coursesto community college students for five years across multiple campuses.Dr. Webb has community college teaching experience in diverse topicswithin biology, including general education courses such as generalbiology and health science, in addition to majors courses such as humananatomy and physiology and animal biology. He holds a PhD in wildlife sciencefrom the University of Washington.Neil Yorke-Smith is an assistant professor at the American University of Beirut,Lebanon and a visiting scholar at St Edmunds College, Cambridge. Hisresearch interests include intelligent agents, planning and scheduling,constraint-based modeling, intelligent user interfaces, and their real-worldapplication to managerial decision-making. He holds a PhD in optimizationfrom Imperial College London, UK.AcknowledgementsVulcan Inc. and SRI International funded this work. We thank the AURAdevelopment team for providing the context for this effort. We thank Prof.Craig Heller for his comments on an early version of this manuscript. Finally,we thank Ilinca Tudose for her work on the representation of Biomembranethat is used in this paper.Author details1SRI International, Menlo Park, CA 94025, USA. 2Monta Vista High School,Cupertino, CA, USA. 3Cabrillo College, Aptos, CA, USA. 4University ofCalifornia, San Diego, CA, USA. 5Foothill Community College, Los Altos Hills,CA, USA. 6American University of Beirut, Beirut, Lebanon. 7University ofCambridge, Cambridge, UK.Received: 23 May 2014 Accepted: 26 November 2014Published: 18 December 2014JOURNAL OFBIOMEDICAL SEMANTICSDiallo Journal of Biomedical Semantics 2014, 5:44http://www.jbiomedsem.com/content/5/1/44RESEARCH Open AccessAn effective method of large scale ontologymatchingGayo DialloAbstractBackground: We are currently facing a proliferation of heterogeneous biomedical data sources accessible throughvarious knowledge-based applications. These data are annotated by increasingly extensive and widely disseminatedknowledge organisation systems ranging from simple terminologies and structured vocabularies to formal ontologies.In order to solve the interoperability issue, which arises due to the heterogeneity of these ontologies, an alignment taskis usually performed. However, while significant effort has been made to provide tools that automatically align smallontologies containing hundreds or thousands of entities, little attention has been paid to the matching of large sizedontologies in the life sciences domain.Results: We have designed and implemented ServOMap, an effective method for large scale ontology matching. Itis a fast and efficient high precision system able to perform matching of input ontologies containing hundreds ofthousands of entities. The system, which was included in the 2012 and 2013 editions of the Ontology AlignmentEvaluation Initiative campaign, performed very well. It was ranked among the top systems for the large ontologiesmatching.Conclusions: We proposed an approach for large scale ontology matching relying on Information Retrieval (IR)techniques and the combination of lexical and machine learning contextual similarity computing for the generation ofcandidate mappings. It is particularly adapted to the life sciences domain as many of the ontologies in this domainbenefit from synonym terms taken from the Unified Medical Language System and that can be used by our IR strategy.The ServOMap system we implemented is able to deal with hundreds of thousands entities with an efficientcomputation time.Keywords: Ontology matching, Life sciences ontologies, Entity similarity, Information retrieval, Machine learning,Semantic interoperabilityIntroductionWith the wide adoption of Semantic Web technologies,the increasing availability of knowledge-based applicationsin the life sciences domain raises the issue of findingpossible mappings between the underlying knowledgeorganisation systems (KOS). Indeed, various terminolo-gies, structured vocabularies and ontologies are used toannotate data and the Linked Open Data Initiative isincreasing this activity. The life sciences domain is veryprolific in developing KOS ([1-4] are examples of suchresources) and intensively using them for different pur-poses including documents classification [5] and codingsystems to Electronic Health Records [6].Correspondence: gayo.diallo@u-bordeaux.frUniversity Bordeaux, ISPED, Centre INSERM U897, F-33000 Bordeaux, France© 2014 Diallo; licensee BioMed Central Ltd. ThCommons Attribution License (http://creativecreproduction in any medium, provided the orOne of the key roles played by these KOS is providingsupport for data exchanges based on a common syntaxand shared semantics. This particular issue makes thema central component within the Semantic Web, theemerging e-science and e-health infrastructure.These KOS, which are independently developed at thediscretion of various project members, are heteroge-neous in nature, arising from the terminology used, theknowledge representation language, the level of semanticsor the granularity of the encoded knowledge. Moreover,they are becoming more complex, large and multilingual.For instance, the Systematized Nomenclature of MedicineClinical Terms (SNOMED-CT) [7], a multiaxial, hier-archical classification system that is used by physiciansand other healthcare providers to encode clinical healthinformation, contains more than 300,000 regularly evolvingis is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly credited.Diallo Journal of Biomedical Semantics 2014, 5:44 Page 2 of 19http://www.jbiomedsem.com/content/5/1/44concepts. Each concept is designated by synonymousterms, sometimes by several. Another example is theInternational Classification of Diseases (ICD), the WorldHealth Organizations standard diagnostic tool for epi-demiology, health management and clinical purposes usedto monitor the incidence and prevalence of diseases andother health issues. The current ICD-10 version con-tains more than 12,000 concepts designated with termsin 43 different languages including English, Spanish andFrench.There is a clear need to establish mappings betweenthese different KOS in order to make inter-operablesystems that use them. For instance, the EU-ADR pro-ject [8] developed a computerised system that exploitsdata from eight European healthcare databases andelectronic health records for the early detection ofadverse drug reactions (ADR). As these databases usedifferent medical terminologies (ICD-9, ICD-10, ReadCodes, International Classification of Primary Care) toencode their data, mappings are needed to translate aquery posed to the global system into queries under-standable for the different data sources. Performingmanual mappings between all the mentioned resourcesis not feasible within a reasonable time. Generallyspeaking, the data integration domain [9], the semanticbrowsing of information domains [10] and web ser-vices composition [11] are areas where the matching ofknowledge resources is usually performed.There is therefore a crucial need for tools which areable to perform fast and automated mapping computa-tion between entities of different KOS and which canscale to large ontologies and mapping sets. Significanteffort has been expended in the ontology alignment/matching domain. A matching system is defined by theOntology Alignment Evaluation Initiative (OAEI) [12]as a software program capable of finding mappingsbetween the vocabularies of a given set of input ontol-ogies [13]. Formally, given two ontologies, a mapping isa 4-tuple [14]:< id; e1; e2; r >such that: id is an identifier for the given mapping; e1 and e2 are entities, i.e. classes and properties ofthe first and second ontology, respectively; r is a relation, e.g. equivalence (=), subsumption (?),disjointness (?) between e1 and e2.Some metadata, including a confidence value, w (usually? [0, 1]), are often associated with the mapping.In the following section we will briefly give an over-view of different approaches and systems in line with theapproach we propose in this paper. In particular, we willreview approaches which use a space reduction strategyfor large scale ontology matching and machine learning-(ML) based matching and briefly present systems evalu-ated recently for the largest task in the context of theinternational OAEI campaign. We will further discuss,in Discussion, systems for matching ontologies in thebiomedical domain.Related workOntology matching is an active research area. Existingontology matching systems use terminological, structuraland semantic features for the computation of candidatemappings (please see [14-16] for a complete survey).Despite the advances achieved in matching relativelysmall size ontologies, the large scale matching problemstill presents real challenges to tackle, due to the com-plexity of such a task. These challenges include effi-ciency issues in term of space and time consumption,the use of background knowledge, user involvementand the automated evaluation of the matching system[14,17]. Therefore, approaches for ontology matchinghave been proposed in the literature including cluster-ing and blocking strategies (reduction of search space),ML- based matching (in particular for reusing existingalignments or combing results for parallel matches),interactive alignment (taking into account the user) andthe use of specialised background knowledge (in particularfor the life sciences domain).A structure-based clustering approach for the matchingof large ontologies is introduced in [18]. The idea is topartition each input schema graph into a set of dis-jointed clusters before identifying similar clusters in thetwo schema graphs to be matched. The COMA++ system[19] is finally used to solve individual matching tasks andcombine their results. Hamdi et al. provide TaxoMap [20],a tool which is based on the implementation of thepartition-based matching algorithm proposed in [21] tofind oriented alignment from two input ontologies.TaxoMap provides one-to-many mappings between singleconcepts and establishes three types of relationships:equivalence, subclass and semantically related relation-ships. The semantically related relationships denote anuntyped link indicating the closeness of two concepts.Hu et al. [21] address the issue of aligning large ontol-ogies by proposing a partition-based block approach forthe matching of large class hierarchies. Their matchingprocess is based on predefined anchors and uses struc-tural affinities and linguistic similarities to partitionsmall block input class hierarchies. In contrast to thesedivide-and-conquer methods, Wang et al. [22] use twokinds of reduction anchors to match large ontologiesand reduce time complexity. In order to predict ignor-able similarity calculations, positive reduction anchorsDiallo Journal of Biomedical Semantics 2014, 5:44 Page 3 of 19http://www.jbiomedsem.com/content/5/1/44use the concept hierarchy while negative reductionanchors use locality of matching. A partial referencealignment strategy is used in [23] in order to partitionontologies to be aligned, computing similarities be-tween terms and filter mapping suggestions. To testthe approach, alignments provided by OAEI and fromprevious evaluation of the SAMBO system [24] areused.On the other hand, Nezhadi et al. use an ML approachto combine similarity measures of different categories inorder to align two given ontologies [25]. Their evaluationof different learning classifiers  K Nearest Neighbor,Support Vector Machine (SVM), Decision Tree (DT)and AdaBoost  on real life (small) ontologies for bib-PROCEEDINGS Open AccessSEE: structured representation of scientificevidence in the biomedical domain usingSemantic Web techniquesChristian Bölling1*, Michael Weidlich2, Hermann-Georg Holzhütter1From Bio-Ontologies Special Interest Group 2013Berlin, Germany. 20 July 2013* Correspondence: christian.a.boelling@gmail.com1Institute of Biochemistry, CharitéUniversitätsmedizin Berlin, Berlin,GermanyAbstractBackground: Accounts of evidence are vital to evaluate and reproduce scientificfindings and integrate data on an informed basis. Currently, such accounts are ofteninadequate, unstandardized and inaccessible for computational knowledgeengineering even though computational technologies, among them those of thesemantic web, are ever more employed to represent, disseminate and integratebiomedical data and knowledge.Results: We present SEE (Semantic EvidencE), an RDF/OWL based approach fordetailed representation of evidence in terms of the argumentative structure of thesupporting background for claims even in complex settings. We derive designprinciples and identify minimal components for the representation of evidence. Wespecify the Reasoning and Discourse Ontology (RDO), an OWL representation of themodel of scientific claims, their subjects, their provenance and their argumentativerelations underlying the SEE approach. We demonstrate the application of SEE andillustrate its design patterns in a case study by providing an expressive account ofthe evidence for certain claims regarding the isolation of the enzyme glutaminesynthetase.Conclusions: SEE is suited to provide coherent and computationally accessiblerepresentations of evidence-related information such as the materials, methods,assumptions, reasoning and information sources used to establish a scientific findingby adopting a consistently claim-based perspective on scientific results and theirevidence. SEE allows for extensible evidence representations, in which the level ofdetail can be adjusted and which can be extended as needed. It supportsrepresentation of arbitrary many consecutive layers of interpretation and attributionand different evaluations of the same data. SEE and its underlying model could be avaluable component in a variety of use cases that require careful representation orexamination of evidence for data presented on the semantic web or in otherformats.Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1 JOURNAL OFBIOMEDICAL SEMANTICS© 2014 Bölling et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction inany medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.BackgroundScientific evidence, as a concept, can be defined as information that is relevant to assessthe likelihood that a particular scientific idea is correct. Representation of the corre-sponding evidence is therefore key to evaluating hypotheses and assessing claims con-tained in scientific articles, databases or any other repository of scientific information.Biomedical knowledge is often highly context-dependent and based on evidence obtainedfrom the skilful combination and evaluation of individual results, involving, among otheraspects, a range of model organisms, diverse experimental and computational techniques,different forms of interpretation, and various inference schemes. Consequently, all thoseaspects - the materials, methods and information sources used, the observations made,the reasoning employed and the context-specific assumptions made - are important forcomprehensive evidence accounts. Likewise, when data, often from disparate sources, isintegrated to study complex biological systems an account of the evidence that was usedto infer a models properties and those of and among its components is critical for cor-rect and transparent understanding of that model.Scientific findings are now routinely published as resources on the World WideWeb. Besides electronic versions of natural language texts more and more informationfrom both new and legacy sources becomes available through databases [1] and webservices [2] which provide through structured formats and interfaces consolidatedviews of and programmatic access to biomedical data. Semantic web technologies andstandards in particular offer by virtue of their well-defined semantics and broad applic-ability potent means for the computational integration and analysis of biomedical datafrom heterogeneous and distributed sources on a large scale [3-5]. Accordingly, theResource Description Framework (RDF, [6]) is increasingly employed to represent anddisseminate new and legacy biomedical data [7,8] and biomedical ontologies specifiedin the Web Ontology Language (OWL, [9]) are being developed to encode domain-specific knowledge and annotate data from biomedical investigations [10-12]. As withany other means for communicating scientific results, findings encoded in semanticweb formats need to be accompanied by an account of how they have been establishedto evaluate their relevance. Towards this end different models, tools and methods havebeen proposed: for representing and evaluating research hypotheses [13,14], contextua-lization [15], models of discourse [16], of argument [17], extended means for annota-tion [18,19], or specific container formats [20]. There is, however, currently nodedicated model supporting a coherent, extensible and semantic-web compatible repre-sentation of all those aspects routinely considered by a researcher inspecting theevidence for a given scientific finding, i.e. a representation of (i) the experimental andcomputational methods and settings that were used to establish the observationalresults and process the data, (ii) the reasoning including additional findings andassumptions used to infer the result in question, and (iii) information sources andagents through which the corresponding views were communicated and propagated.Here we introduce SEE (Semantic EvidencE), an RDF/OWL based approach for pro-viding detailed, extensible and computationally accessible accounts of evidence even incomplex settings. SEE is designed to enable the fabric of observations, methods,assumptions, and inferences examined by researchers to evaluate the evidence for aclaim to be formally represented along with their sources using semantic web techni-ques. Evidence is captured in terms of the argumentative structure of the supportingBölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 2 of 22background for a claim i.e., by a coherent representation of claims, of the entities theclaims are about, of the argumentative relations between the claims and of claim pro-venance. SEE accommodates nested layers of interpretation and attribution and differentevaluations based on the same data. We demonstrate its application in a case study that istypical for the task of collecting, representing and evaluating evidence for systems biologyapproaches such as genome-scale metabolic network reconstruction by providing anexpressive account of evidence for the location of the enzyme glutamine synthetase.ResultsOverview of the SEE approachThe SEE approach for representing evidence consists of providing (i) a formal representa-tion of scientific claims, their provenance and the argumentative structure used to justifythem by other claims, (ii) a formal representation of claim content and (iii) a coherentintegration of the two. SEE relies on an abstract model for the representation of claims,provenance and argumentative structure specified in the Reasoning and DiscourseOntology (RDO), a lightweight OWL vocabulary developed for this purpose. Claim con-tent e.g., what is claimed regarding the properties of biological entities or the results andmethods of an investigation is represented in RDF graphs by using appropriately definedsemantic web resources and design patterns which as a best practice should, if possible, bere-used from existing domain ontologies. The connection between claims as representa-tional primitives and their content relies on named RDF graphs [21] which enable pointingto collections of RDF-triples or OWL-axioms serialized as such.After outlining general requirements and design principles for representation of evi-dence we describe the RDO. We then demonstrate the application and design patterns ofthe SEE approach in a case study generating an expressive representation of evidencereported in the literature for the location of the enzyme glutamine synthetase.Deriving design principles and requirements for representation of evidenceWe posit two design principles for the representation of evidence and explain theirrationale in the following:DP1: Representation of evidence amounts to representation of claims and argumentativestructure.DP2: Evidence relations in the sense of A is evidence for B obtain between thethings being claimed.Accounts of evidence are directed towards the justification of scientific claims. The SEEapproach is based on the notion that scientific claims put forward possible, more or lesslikely scenarios and outcomes - states of affairs [22] - as being accurate descriptions of asubject of scientific inquiry. Something is evidence for a certain state of affairs, if and onlyif it gives reason to believe that this state of affairs in fact obtains [23]. A pairing of evi-dence and what it is claimed to be evidence for therefore corresponds to the set of pre-mises and the conclusion of an argument in which the truth of the premises alleges togive reason to believe the conclusion is true. Therefore the evidence used by authors oragents to justify a claim, possibly using further unstated background assumptions, can bemirrored by an argumentative structure having the claim as its conclusion. Typically, whatis used to justify the authors conclusions within this argumentative structure are claims inthemselves accepted as true on the basis of observations or inferences of the same or ofBölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 3 of 22other investigators. SEE, therefore, models evidence relations in the sense of A is evidencefor B specifically as relations between claims.We derive two additional requirements:DP3: A researchers assessment of the evidence for a finding usually includes evaluationof which materials and methods were used, what kind of data was obtained and whichproperties were observed, inferred or assumed to establish the finding. Consequently, arepresentation of the materials, methods, data items and other elements forming thesubject of a claim should be part of a computationally accessible evidence representation.In RDF and OWL the subject of a claim, a state of affairs, must be expressed, using appro-priately defined resources, as (one or more) triples and axioms, respectively. It followsthen, in accordance with DP2 that in an RDF/OWL-based representation of evidence thatincludes claim subjects the representation of evidential relationships should operatebetween claim subject representations, i.e. between sets of RDF-triples and/orOWL-axioms.DP4: Representation of claims and hence representation of evidence must take intoaccount claim provenance, in particular through which source and by which agents theclaims were made. Knowing which agent made the claim is crucial for evaluating inde-pendence and reproducibility. Tracking the original source of a claim provides a naturalreference point for all subsequent representations of the claim and its supporting back-ground and for re-evaluation of the claim within the original context in which it wascommunicated.We therefore identify as minimal components for modelling evidence elements repre-senting (i) scientific claims and the argumentative structure used to justify them by otherclaims, (ii) the subjects of the claims i.e. that what is claimed with regard to a subject ofinquiry, (iii) the agents making the claims and arguments, (iv) the sources in which claimswere originally made e.g., the original scientific articles or database records.Reasoning and Discourse Ontology (RDO)Based on the foregoing we developed an abstract model for representation of evidence interms of claims, their argumentative structure and their provenance. It is specified here asthe Reasoning and Discourse Ontology (RDO) using the Web Ontology Language (OWL).This section outlines the core classes and properties of RDO. Full, formal specification ofall RDO constructs is provided in the ontology file provided as additional file 1.The typical scenario that underlies the constructs defined in RDO is the following:Agents (e.g., individual scientists) make claims on particular occasions (e.g., as authorsof a published scientific article) about a subject of inquiry. The subject of the claim - i.e. what is claimed - is communicated in some linguistic form, often as part of a morecomprehensive report (e.g., a scientific article) authored by the agents. Claims areusually justified by other claims the subject of which has been accepted as true, usuallyon the basis of yet other claims. RDO (Figure 1) rests on the distinction of a claim, itssubject and the linguistic form in which this subject is communicated and is centeredaround the concept of an assertion [24]: instances of the class assertion (couriertypeface denotes OWL classes, courier in italics denotes OWL properties)represent particular claims made by particular agents on a particular occasion that aparticular proposition, the subject of the claim, is true. Propositions, in our model, arerepresented by the class proposition and taken to represent the semantic contentBölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 4 of 22of contextualized lexical entities formulated in some natural or artificial language [25].The lexical entities by which the subject of a claim and propositions and reports ingeneral are formulated are represented using the class text. Further core classes arereport representing accounts intended to accurately describe an event or situation.Thus, scientific journal articles or database records as typical sources of assertions areexamples of a rdo:report. Agent is used to represent individual persons, corporatebodies or information processing devices as roleplayers in the creation of reports orassertions. RDO specifies various properties to represent the relations betweeninstances of these classes (Figure 1). In particular, argumentative structure is capturedby the property is inferred from which relates an instance of assertion toanother if and only if the former is, directly or indirectly, inferred from the latter (andpossibly other premises).Application: representation and evaluation of evidence for a source of glutaminesynthetaseIntroducing the case studyWe applied SEE to generate a computationally accessible, expressive and extensibleaccount of evidence gathered in the literature regarding a claimed source of the enzymeglutamine synthetase (GS). We have chosen this particular test case because obtaining reli-able information on location of enzyme activities is a subject area of particular importancefor systems biology approaches such as the reconstruction of cell-type specific [26] ororganism-level [27] metabolic networks. Furthermore, it embodies the typical task ofacquiring knowledge on a subject of inquiry by extracting and combining evidence fromdifferent sources.Figure 1 Core classes and properties in RDO. Boxes denote labelled classes, arrows denote labelledproperties, direction of arrows denotes property domains and ranges. Asterisk: property has domain andrange-specific sub-properties. The color code used here is also used in subsequent figures.Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 5 of 22Starting point is our evaluation of a scientific journal article [28] (referred to as Meister1985 in the following) authored by Alton Meister which asserts in the second paragraphof the text, among other things, that the enzyme glutamine synthetase (GS) was isolatedfrom rat liver. This assertion is based, by way of citation, on the contents of another articleby Tate, Leu and Meister [29] (referred to as Tate 1972 in the following). In Tate 1972the isolation of GS from rat liver is reported. The finding is reported to be based on aninvestigation which involved, among other things, extraction of rat livers, protein purifica-tion and g-glutamyl hydroxamate synthesis (g-GHS) assays. In the following we show howthis context is formalized using the SEE approach to yield a detailed formal account of theevidence presented through these articles for rat liver as source of GS. In doing so, weillustrate various design patterns used in SEE for representing the relevant items. Forclarity assertion instances will be indexed as A1, A2, and so forth.Representing the evidenceFigure 2 shows how the assertion from Meister 1985 that GS was isolated from ratliver is represented using RDO, exemplifying the design pattern used to represent therelations between a particular assertion and its subject and provenance: The articleFigure 2 Representation of assertions with subject and provenance. Assertion instances arerelated to proposition instances representing the subject of the assertion by asserts, to the agentsmaking the assertion by is_assertion_made_by and to the reports in which they are made byis_assertion_made_in. See text for additional relations among assertions, agents, reports and textualrepresentations. Assertion and proposition labels reflect the graph representation of assertion subjects (seetext). Color code of assertion and proposition labels indicates the structured representation of assertionsubjects (yellow: class, blue italics: property, red: Manchester syntax restriction keyword). Circle shows theindex by which the assertion is referred to in the text.Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 6 of 22itself, Meister 1985, is classified as instance of report annotated with a uniformresource locator (URL) providing its digital representation. The second paragraph ofMeister 1985 constitutes a report_part. It is expressed as the English language textas which it is written and which is represented as an instance of text. The originaltext is linked to it via the data property has_lexical_structure. Meisters claimthat glutamine synthetase was isolated from rat liver contained in this paragraph isrepresented by an instance of assertion (A1) labelled as ! some GS-enzyme isolatedfrom some rat liver ! AM to indicate the assertion subject in a concise, human read-able manner (formalization of assertion subjects is described below). A1 is related to acorresponding instance of proposition identifying the subject of the claim, to aninstance of agent representing Alton Meister, and to said report part by the prop-erties asserts, is_assertion_made_by and is_assertion_made_in,respectively.Claims which reiterate previous findings are represented as assertions on the same sub-ject made by the respective agents. Formally, the reiterating claim is represented as anassertion instance which is linked to the source assertions by is_directly_in-ferred_from and linked to the same proposition instance as the source assertionsby asserts. Each assertion can be linked to its corresponding agents and reports. Appli-cation of this design pattern to our case study is shown in Figure 3: The fact that Meistersassertion (A1) reiterates what Tate & co-workers have asserted on the isolation of GSfrom rat liver (A2), is represented by a relation of the former to the latter via is_direc-tly_inferred_from and by sharing the same proposition instance via asserts.The argumentative structure within and across the publications is represented as aseries of assertion instances and is_directly_inferred_from relations withadditional links to represent assertion subjects and provenance (Figure 4). Theassertion instances linked to A2 reflect the results and the reasoning of the authorsat various steps of their investigation based on a careful analysis of the internal argu-mentative structure of Tate 1972. Specifically, Tate et al.s main conclusion that GS-enzyme was isolated from rat liver (A2) is essentially based on asserting that (A3)there is a biological sample (labelled sample-1) which has GS-activity, that (A4) anyGS-activity is borne by some GS-enzyme and that (A5) sample-1 was isolated fromsome rat liver (precise definitions for GS-enzyme, GS-activity in the context of thecase study are detailed in additional file 2). The joint use of A3, A4 and A5 to infer A2Figure 3 Representation of claims which reiterate previous findings. The fact that Meisters assertion(A1) reiterates what Tate & co-workers have asserted on the isolation of GS from rat liver (A2), isrepresented by a relation of the former to the latter via is_directly_inferred_from and bysharing the same proposition instance via asserts. Each assertion instance is linked to itscorresponding agents and reports. Color code as in figure 1.Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 7 of 22is made explicit by using the has_conjunctive_part property to link them to thesame composite assertion instance which in turn is related to A2 using theis_directly_inferred_from property. This pattern is used whenever an asser-tion is inferred from more than one premise. A3, the assertion that sample-1 has GS-activity is justified in turn by asserting that (A6) it was input to a particular assay(labelled assay-1), that (A7) this assay produced a particular result, data item 1, andthat (A8) this data item is a measurement of some GS-activity. A8, in turn, is justifiedby asserting that (A9) the data item is output of assay-1, that (A10) this assay was a g-GHS assay, and that (A11 & A12) this type of assay is suited to measure GS-activity.Some assertions are not further justified, either because they reflect factual descriptionsin Tate 1972 (A9, A10), represent general assumptions of the authors (A11) or areexpressions of terminological domain knowledge (A12, A4). A5 exhibits a similar justi-fication trail, as shown in Figure 4. Full, formal representation of the argumentativestructure for the test case is provided in additional file 2.The prevalent pattern in SEE for recording individual and logically relevant steps of aninvestigation is for any such step to link its outcomes (data or material), the techniquesused to produce these outcomes, and their objectives as exemplified in the compositeassertions comprising assertions A9-A12 and A15-A20 (Figure 4). In A9-A12, for example,the experimental process type (g-GHS assay) is linked to the objective of its application(GS-activity measurement) and in turn to the quality that is intended to be determined(GS-activity). Generally, the relations between these ontologically different entities are notFigure 4 Representation of argumentative structure within and across publications. Theargumentative structure used to justify Meisters claim on the isolation of GS from rat liver (A1) isrepresented as a series of assertion instances linked by is_directly inferred_from relations.Dashed line separates assertions made in Meister 1985 and Tate 1972. Dashed-dot boxes indicate compositeassertions with their component assertions placed inside signifying the has_conjunctive_partrelations. Author initials tags of the assertions made by Tate et al. are omitted, as are links to propositions,reports, authors and texts. Color code as in figure 1.Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 8 of 22trivial and not one-to-one (one objective can consist of the determination of several quali-ties recognized in a scientific domain, a certain quality can be the subject of inquiry in sev-eral objectives). However, in this particular case the objective and quality are narrowlydefined and directly correlated.Representation of assertion subjectsThe representation of argumentative structure and claim provenance as an interrelated setof assertion instances described so far is complemented by a structured representationof what is asserted in each assertion, the assertion subject. To this end each assertioninstance is linked to a corresponding proposition instance the IRI (InternationalizedResource Identifier) of which identifies a named RDF graph. This graph provides a struc-tured representation of the assertion subject using appropriately defined resources (Figure5). This setup enables querying the elements forming the assertion subject. In assertionA10, shown in Figure 5 as an example, it is asserted by Tate and co-workers that the parti-cular assay they performed was a g-GHS assay. The representation of this statement as agraph identified by the IRI of the proposition instance linked to the assertioninstance representing A10 enables to access the entities A10 is about: the particular assay,its asserted type, and the typing relation itself. Full specification of all propositions asnamed graphs in the context of the case study is provided in additional file 3.Figure 5 Representation of assertion subjects as named graphs. In SEE structured, queryablerepresentations of assertion subjects are provided as named graphs. A) The structured representation of thesubject of an assertion can be accessed as the RDF graph identified by the IRI of the proposition instancerelated by the asserts property to the assertion instance representing the assertion. B) Structuredrepresentation of the subject of assertion A10 asserting that the particular assay performed by Tate et al.(:assay-1) was a g-GHS assay (:gamma_ghs_assay). C) TriG representation of the graph shown in B.Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 9 of 22To generate the graph representations of the assertion subjects, the natural languageexpressions of the assertions identified in the Meister 1985 and Tate 1972 reports were for-malized in RDF using appropriately defined resources (see additional files 2, 3 and 4). Mostassertion subjects could be formalized in a straightforward manner applying OWL 2 RDF-based semantics [30]. The principal claim that glutamine synthetase was isolated from ratliver which is the common subject of assertions A1 and A2 was formalized in RDF byinstantiating the class gs_enzyme and is_isolated_from some rat_liver (shownas :proposition-1 in additional file 3). This exemplifies instantiation of the OWL-class(A and related_to some B) as a design pattern for formalization of statements whichcan, in natural language, be represented in the form some A related to some B (A and Bdenoting OWL-classes used to represent the types A and B, respectively and related_todenoting an OWL-property used to represent the relation among some of their instances).Labels of assertion and corresponding proposition instances are directlyderived from the graph representation of the assertion subject (see methods section).In particular, the label some A related_to some B is used for propositioninstances that represent statements of the form some A related to some B by applyingthe design pattern described above.Representing consecutive layers of interpretation and own conclusionsWe use the test case to specify additional design patterns to represent activity of acurator or generally of a third party evaluating a scientific report. Our representationof the evidence in the Meister 1985 and Tate 1972 reports is the result of the interpre-tation by another agent (Christian Bölling - CB). This can be explicitly represented inSEE using its familiar design pattern for propositions and assertions. For example, theclaim that Tate et al. indeed assert that the assay they performed was a g-GHS assay intheir 1972 publication can be represented as an assertion instance in its own right,made by another agent, CB (Figure 6). This pattern allows for representing arbitrarymany consecutive layers of interpretation or attribution.So far the presented account consists of assertions attributed to the authors of theMeister 1985 and Tate 1972 reports, i.e. a representation of what these authors assert.SEE also provides the resources to append own conclusions. For example, an agent, CB,could upon evaluation of the claims made by Tate et al. conclude for himself that GSwas indeed isolated from rat liver. This is represented as an assertion instance in itsown right (A30, labelled ! some GS-enzyme isolated from some rat liver ! CB). It islinked to the corresponding proposition via asserts and the assertions made by Tateet al. via is_directly_inferred_from. We describe two semantically different pat-terns to make this connection. In pattern 1 assertion A30 is linked to assertion A2(Figure 7). In pattern 2 (Figure 8) A30 is linked to a new composite assertion thatinvolves two more curator assertions (A31, A32) and A4 as a representation of termino-logical domain knowledge. A31 and A32 are linked by is_directly_inferred_-from to composite assertions reflecting factual descriptions of data and proceduresgiven in Tate 1972. There is a subtle, yet important difference in meaning between thesetwo representations. In pattern 1 CBs conclusion is based on Tate et al.s assertion onthe same subject, i.e., it is based on the author statement itself and does not necessarilyimply an affirmation of how Tate et al. reached their conclusion. In pattern 2 the curatorinference is based on factual descriptions in Tate 1972, i.e., it affirms the conclusions ofTate et al. as own conclusions on the basis of the reported experimental results.Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 10 of 22Evaluation of a given set of data might also lead to conclusions different from thoseof the authors. Such alternative interpretations can be represented using SEE. Forexample, one might dispute that g-GHS assays are suited to measure GS-activity (EC6.3.1.2). The g-GHS assay works by measuring the formation of L-g-glutamyl hydro-xamate rather than glutamine [31]. Tate et al. assert as the objective of its applicationGS-activity measurement, accepting the formation of the hydroxamate under theconditions of the assay as a proxy for the formation of glutamine and the actual reac-tion mechanism. Assertion A11 using the property achieves_objective reflectsthis acceptance by Tate et al.. Alternatively, a third party could assert that g-GHSassays merely achieve the less specific objective of measuring g-glutamyl transferase(GGT) activity (EC 2.3.2.2) (Figure 9, assertion A45). In this case the data reportedby Tate et al. can still be used to infer that rat liver is a source of GGT-enzyme(Figure 9, assertion A40).Figure 6 Representation of consecutive layers of interpretation. Consecutive layers of interpretationcan be represented as assertions the subject of which is about other assertions. A) CBs assertion that Tateand co-workers assert that assay-1 was a gamma-GHS assay in their 1972 report is represented as anassertion instance linked to a proposition instance whose named graph representation relates theassertion instance A10 to the Tate 1972 report via the is_assertion_made_in property. B) TriGrepresentation of the graph shown in A. In combination with the RDF dataset shown in Figure 5C this isan example of a named graph referencing a named graph via the corresponding assertion andproposition instances. Color code as in figure 1.Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 11 of 22Evaluating the test case evidence representationThe test case evidence representation that was created using the RDO constructs anddesign patterns was evaluated in terms of its potential to answer, within the confines ofthe case study, a list of competency questions reflecting different aspects of the evidencea researcher investigating glutamine synthetase knowledge would be interested in:Q1: Which locations of GS have been asserted?Rat liver.Q2: Where has rat liver GS been reported?The Meister 1985 and Tate 1972 reports.Q3: Do the assertions made in these reports pertain to independent observations?No. Meisters assertion is based on Tate et al.s assertion. Moreover, some of theauthors of the two reports are identical.Q4: Is there experimental evidence and where is it described?Yes. In the Tate 1972 report.Figure 7 Representation of curator activity: inference from author statement . The pattern torepresent inference from author statement is illustrated here by linking curator assertion A30 (shown inbold) to assertion A2 of the Tate et al. argumentation signifying inference of A30 on the basis of an authorstatement of Tate et al. on the same subject. Color code as in Figure 1.Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 12 of 22Q5: Which observations and techniques were used for establishing rat liver as GS source?1. extraction of a protein sample from rat liver (technique: TLM purification)2. that sample has GS-activity (technique: g-GHS assay)Q6: Did Tate et al. really make these observations and conclusions? Who created thisaccount of their findings?Christian Bölling.Based on the SEE design patterns, these questions could be formulated as SPARQL[32] queries and successfully answered (see additional file 5). In each of Q1-Q6 thestructured representation of assertion subjects as named graphs, besides the other SEEdesign patterns, is used to identify assertions which are relevant to answer the query.For answering Q1 assertions are identified whose subjects graph representationincludes a graph pattern indicative for the isolation of GS from some location (Figure10A). For answering Q3, pairs of assertions are identified whose subjects share thesame graph representation and where one is inferred from the other (Figure 10B).Figure 8 Representation of curator activity: inference from experimental evidence. The pattern torepresent inference from evaluation of the reported experimental evidence, in contrast to inference basedon author statement (Figure 7), is illustrated here by linking curator assertion A30 to a new compositeassertion involving curator assertions A31 and A32. These are, in turn, linked to the composite assertionsA9-A12 and A15-A20, respectively. These composite assertions reflect data and procedures reported in Tate1972. Taken together this graph therefore represents a curator conclusion (A30) based on the affirmativeoutcome (A31, A32) of the evaluation of the data and procedures reported in Tate 1972. Note that A2, theprincipal conclusion of Tate et al. is unrelated to the new composite assertion. Curator assertions and theirlinks to the Tate et al. argumentation are shown in bold. Color code as in Figure 1.Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 13 of 22The following evidence-related information can be queried exploiting property chainsand other axioms defined for the RDO constructs:- all assertions which are directly or indirectly used to infer a given assertion- all assertions made in a given report- all assertions made by a given agent- all assertions on the same subject- all agents making assertions on a given subjectFor the corresponding queries see additional file 5. As an example, in Figure 11 theobject property assertions inferred for assertion A1, Meisters assertion that GS wasisolated from rat liver, are shown. These inferences, simply derived in Protégé 4 withHermiT 1.3.8 as a reasoner include all assertions which A1 is directly or indirectlyinferred from and all reports and texts A1 is based on.DiscussionSEE designSEE offers a tangible interpretation of the concept of evidence in terms of the argu-mentative structure of the supporting background for a claim. It rests on theFigure 9 Representation of curator activity: alternative interpretations of reported data. Alternativeinterpretations of reported data can be represented as assertions that are made by a third party and linkedto assertions reflecting factual descriptions of the reported data. Based on CBs assertion that g-GHS assaysmeasure GGT activity (A45) - rather than GS-activity - it is inferred that GGT-enzyme has been isolated fromrat liver (A40). The corresponding inference chain relies on a number of new curator assertions (A41-A46)and their combination into composite assertions but re-uses assertions on the quantitative data obtainedand the procedures conducted by Tate et al. Curator assertions and new inference links are shown in bold.Color code as in Figure 1.Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 14 of 22Figure 10 Competency questions SPARQL queries. A) SPARQL query to identify all asserted locations ofGS (Q1). This query identifies patterns in which an assertion (_:q11) has a subject (_:q12) which includes agraph pattern indicative for the isolation of GS from some location. B) SPARQL query to identifydependency of assertions on the same subject. The query identifies assertions (_:q31, _:q33) which sharethe same subject (?proposition) and are inferred from one another.Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 15 of 22disctinction between claims as such (assertion), their subjects (proposition) and thelinguistic form in which these subjects are communicated (text). As a consequence ofthis design evidential relations (as in A is evidence for B) can be represented consis-tently as relations between assertions. This means that statements of the form thisdataset / experiment / publication / method is evidence for B are regarded as figura-tive expressions. Instead, the relation between a dataset, an experiment or a publicationand the state of affairs it is claimed to be evidence for is represented indirectly throughrelations between assertions the subjects of which relate to the entities in question.The advantage of this design is that it enables a coherent representation not only ofextensive argumentative networks but also of arbitrary many layers of consecutiveFigure 11 Inferred object property assertions. Inferred object property assertions for Meisters assertionthat GS was isolated from rat liver (A1).Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 16 of 22interpretations and alternative evaluations of the same observations or informationsources. RDO offers clear, formally defined types and relations for representing claims,their subjects, their linguistic representations, related information sources and agentson the basis of well established concepts from epistemology and the philosophy of lan-guage [22,24,25,33,34]. The case study examples suggest that the SEE design principlesand their implementation in RDO are capable of correctly representing, in a computa-tionally accessible and coherent form, the entire evidence trail for a claim needed toevaluate its relevance including observational data, research techniques, assumptionsand information sources.SEE represents argumentative structure at its foundational level of premises beingused to infer a conclusion using the is_directly_inferred_from property andits transitive superproperty is_inferred_from. This allows for a coherent repre-sentation of different argument forms and larger rhetorical structures which can bemapped onto their underlying assertions.SEE aims to capture arguments as they are presented in their sources rather than toevaluate their quality or to categorize them. How conclusive an argument is will typi-cally depend on agent background knowledge or application-dependent requirements.The SEE design enables users to evaluate evidence according to their own, possiblydomain- and application-specific criteria.SEE-based accounts could also be used alongside specified rules, or argument formsconsidered as acceptable by individual researchers or within specific domains ofinquiry which could then be leveraged to automatically infer new assertions on thebasis of the already asserted information.With regard to the extraction of assertion subjects and a specific argumentativestructure from a natural language text SEE relies in its current form on a heuristicapproach leveraging expert domain knowledge to identify assertions and formalizethem in OWL. As OWL is a subset of first order logic there may be statements fromnatural or artificial languages which cannot directly be translated into OWL, constrain-ing the formalization of assertion subjects in SEE. It is, however, not clear which actuallimitations arise from this theoretical constraint for the representation of evidence inspecific use cases. The test case presented here suggests that within a specified domainof discourse, using appropriate constructs and design patterns, the relevant contents ofthe statements made originally in a context-rich narrative format such as a scientificjournal article can be adequately formalized.Formalization of natural language statements is an important prerequisite for compu-tational approaches to data evaluation. For applications that can forego this need thestatements can be represented in their original form as texts or referenced by links tothe original information sources. Both are by default designed to be provided in SEE asreference points for evaluation.The presented design patterns make SEE-based accounts of evidence extensible. Thisdesign is in line with the open world assumption on which RDF and OWL as knowledgerepresentation languages operate. The particular argumentative structure and level ofdetail presented in the case study are based on heuristics reflecting domain-specificrequirements to understand how an enzyme was characterised. This representation can beextended or shortened as required. For example, details on the protein purification processperformed by Tate et al. or indeed any other detail that becomes relevant for theBölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 17 of 22evaluation of the presented evidence could be appended to the existing assertions. Like-wise, as we have demonstrated, alternative views and conclusions can be accommodated.On the other hand, for applications which only require information on claim provenance,only the source publications of the main claims could be represented.Evidence typesEvidence type schemes provide a useful shorthand categorization of research techni-ques used to establish a claim. SEE could be aligned with any categorization ofresearch techniques and hence evidence type scheme to characterise the evidence foran assertion. Essentially, SEE provides a platform to define custom, extensible evidencetypes and apply them as needed. For example, the evidence for rat liver as a source ofGS in the test case could be characterised as experimental evidence as based on adirect assay as based on a g-GHS-assay or as based on a g-GHS assay, protein puri-fication involving Sephadex chromatography, and samples from Sprague-Dawley ratsdepending on the level of accuracy desired.The flexibility and extensibility of the SEE approach may also be useful to character-ise evidence where several techniques have been combined to establish a scientificresult or evidence is characterised in combination with claim provenance. We illustratethis with a comparison to the Gene Ontology (GO) evidence codes which are meant toreflect the type of work or analysis described in the cited reference which supports theGO term to gene product association [35]. GO evidence codes consist of a collectionof terms arranged in a hierarchical format. In this taxonomy the terms representingjustifications based on author statements (TAS, NAS) are unrelated to those representingexperimental techniques (EXP and child terms). Consequently, GO associations markedas being made on the basis of an author statement are usually not qualified with respectto how this author statement came about. In contrast, as demonstrated in the casestudy, using SEE any author statement can be extensively qualified in terms of theexperimental evidence or other author statements it is directly or indirectly based on.Use casesRepresentations which use SEE or its underlying model could be productive in a varietyof use cases requiring careful examination or recording of evidence, e.g.,- providing supporting background information for biomedical knowledge bases,- creating digital abstracts of research publications,- adding a claim-level perspective on research publications which could be used bypublishers, in bibliographic databases and in personal bibliography managers,- providing open linked data which can be integrated on an informed basis usingvarying, application specific evidence criteria.Related and future workThe SWAN biomedical discourse ontology [16] developed in the context of theSemantic Web Applications in Neuromedicine (SWAN) project offers a formal modelof scientific discourse based on two different classes of statements; swan:hypothesisand swan:claim. Claim subjects are to be represented in natural language and the reso-lution of their supporting background is confined to the document level. TheBölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1http://www.jbiomedsem.com/content/5/S1/S1Page 18 of 22Annotation Ontology (AO) [18] has been implied as a means to provide formalizedaccounts of claims and their supporting background conceptualized as annotations anddocument parts, respectively. While it is possible in this way to relate individual ontol-ogy terms to parts of documents, the AO semantics and use cases suggest that itsmain application area is representation and support of annotations of documentsrather than representation and evaluation of extensive, possibly nested, networks ofclaims. Nanopublications have been proposed as a container format to encode andpublish individual assertions using Semantic Web and Linked Data principles [36].PROCEEDINGS Open AccessSemantic Web repositories for genomics datausing the eXframe platformEmily Merrill1*, Stéphane Corlosquet1*, Paolo Ciccarese1,2, Tim Clark1,2,3, Sudeshna Das1,2*From Bio-Ontologies Special Interest Group 2013Berlin, Germany. 20 July 2013* Correspondence:mmerrill@partners.org;scorlosquet@gmail.com;sdas5@partners.org1Massachusetts General Hospital,Partners Research Building, 65Landsdowne St, Cambridge, MA,02139, USAAbstractBackground: With the advent of inexpensive assay technologies, there has been anunprecedented growth in genomics data as well as the number of databases inwhich it is stored. In these databases, sample annotation using ontologies andcontrolled vocabularies is becoming more common. However, the annotation israrely available as Linked Data, in a machine-readable format, or for standardizedqueries using SPARQL. This makes large-scale reuse, or integration with otherknowledge bases very difficult.Methods: To address this challenge, we have developed the second generation of oureXframe platform, a reusable framework for creating online repositories of genomicsexperiments. This second generation model now publishes Semantic Web data. Toaccomplish this, we created an experiment model that covers provenance, citations,external links, assays, biomaterials used in the experiment, and the data collected duringthe process. The elements of our model are mapped to classes and properties fromvarious established biomedical ontologies. Resource Description Framework (RDF) datais automatically produced using these mappings and indexed in an RDF store with abuilt-in Sparql Protocol and RDF Query Language (SPARQL) endpoint.Conclusions: Using the open-source eXframe software, institutions and laboratoriescan create Semantic Web repositories of their experiments, integrate it withheterogeneous resources and make it interoperable with the vast Semantic Web ofbiomedical knowledge.BackgroundThere has been a rapid cost reduction per megabase of genomic information obtained,beating Moores law [1] many-fold [2,3], resulting in an exponential growth of geno-mics data, especially next generation sequencing data [4]. Standards to unambiguouslydescribe the experimental details are required to facilitate the understanding, qualitychecking, reusing, reproducing and integrating the data. The bioinformatics communityhas responded to the challenge and several standards have been developed over theyears. The first standard to be published provided requirements for the MinimumInformation About a Microarray Experiment (MIAME) [5]. Several other standardswere published as new technologies evolved and then the Minimum Information forBiological and Biomedical Investigations guideline was proposed for reporting all typesMerrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3http://www.jbiomedsem.com/content/5/S1/S3 JOURNAL OFBIOMEDICAL SEMANTICS© 2014 Merrill et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction inany medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.of biomedical experiments [6]. The major public repositories of genomics experiments,Gene Expression Omnibus (GEO) [7] and ArrayExpress [8], are compliant with thesestandards.While standards addressed the need for uniform experiment representation, controlledvocabularies, terminologies and ontologies were developed to describe the samples, assaysand other experimental details in an unambiguous manner. For example, the Ontology forBiomedical Investigations (OBI) [9] provides a model for biomedical experiments withclasses that describe elements of the experimental investigation process. The ExperimentalFactor Ontology (EFO) [10] was developed as an application ontology to describe thegenomics data in ArrayExpress [8]. In addition several ontologies and vocabularies havealso been developed to describe biological specimens such as the organism, tissue, celltype, disease state. These include the Cell Ontology (CL) [11], the Foundation Model ofAnatomy (FMA) [12], Disease Ontology (DO) [13] among numerous others.Several repositories of genomics data have adopted the MIAME or MIBBI standards andare leveraging these biomedical ontologies to provide consistent annotation of experi-ments. A few examples from diverse domains include the Gemma repository - a resourcefor sharing, reuse and meta-analysis of microarray data [14], Chemical Effects in BiologicalSystems (CEBS) database that contains data of interest to environmental health scientists[15] and Oncomine an integrated database and mining platform for oncology data mine[16]. Although these resources make use of ontologies to represent experimental data in astandardized manner, the annotations are not machine-readable by other software andthus integration with other knowledge resources remain a challenge.Meanwhile, Semantic Web [17] technologies such as Linked Data, Resource DescriptionFramework (RDF) and SPARQL are increasingly being used in the bioinformatics commu-nity to respond to the knowledge integration needs [18]. Semantic Web allows one toquery across disparate resources using a single flexible interface. For example, the Bio2RDFproject successfully applies Semantic Web technologies to create a mashup of key publiclyavailable databases using a common ontology and normalized Uniform Resource Identifiers(URI) [19,20]. Cheung et al. demonstrate the use of Semantic Web technologies for afederated query in the neuroscience domain [21]. There are several other examples acrossvarious biomedical domains that demonstrate the power of Semantic Web technologies.However, surprisingly there has been no wide spread adoption of Semantic Webtechnologies for experiment repositories, where queries using domain ontologies canhelp bridge different disciplines, for important applications such as translationalmedicine. Recently the European Bioinformatics Institute (EBI), recognizing this urgentneed, has released an RDF platform that includes a SPARQL endpoint for the GeneExpression Atlas [22], a database that summarizes gene expression from ArrayExpressexperiments[23]. However, it doesnt provide reusable software that can be used byother institutions to house and query their genomics data.To address this gap, we developed eXframe as a reusable software platform to buildgenomics repositories that automatically produce Linked Data and a SPARQLendpoint. Our platform is based on an open source content management system anduses existing biomedical ontologies to produce Semantic Web data enabling intero-perability with the other resources. The code is freely available and application isdemonstrated with a repository of stem cell data.Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3http://www.jbiomedsem.com/content/5/S1/S3Page 2 of 12ImplementationIn this section we describe the implementation of eXframe and how it automaticallygenerates Linked Data.FrameworkThe eXframe software framework [24] enables creation of web-based genomics experi-ment repositories. It is based on an open source content management system, Drupal[25], with modifications to support genomic experiment data. In this paper, we report are-factored second generation of eXframe, which produces Linked Data and a SPARQLendpoint for querying it. The revised version also includes an updated experimentmodel that has been generalized to support various types of biomedical experiments aswell as an upgrade to Drupal 7.We have defined content types (e.g. experiments, assays, biomaterials and bibliographiccitations) as well as their relationships as first class objects in Drupal. These predefinedcontent types are packaged as Drupal features and available for use within eXframe. Allcontent types and their fields are mapped to appropriate ontologies and vocabularies asdescribed in the following section. Using these mappings, the Drupal RDF modules [26]are used to produce RDF as well as a SPARQL endpoint. Data can also be exported inother standard formats such as ISA-Tab [27]. A simple schematic of the architecture isshown in Figure 1. The software also includes a basic theme (colors, fonts and style) forthe website. Any group or institution that uses eXframe can customize the content types,theme or ontology mappings.Data modelThe main content type within eXframe is an experiment. It describes the experimentand its meta-data including title, description, contributors, design, citations, and linksFigure 1 eXframe architecture . Overall schematic of eXframe architecture displaying the majorcomponents.Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3http://www.jbiomedsem.com/content/5/S1/S3Page 3 of 12to external resources such as GEO [28] and ArrayExpress [8]. The experiment content typeis mapped to the OBI investigation class obo:investigation. The experiments publicationmeta-data is represented using the Dublin Core ontology [29]. However, we are currentlyevaluating the PAV ontology [30] as it provides more detailed and precise provenanceinformation. For example, the Dublin Core ontology specifies the relation dc:date; but doesnot provide precise information as to whether the date is the submitted date, publisheddate or last updated date. The researchers that conducted the experiment are repre-sented as Drupal users with a profile and mapped to foaf:Person in the FOAF ontology[31]. While we do not specify the principal investigator (for the sake of simplicity), onecould use VIVO [32] to do so. Bibliographic citations are represented using the Drupalbiblio module and mapped to the bibliographic ontology, BIBO [33]. These classes andmappings are illustrated in Figure 2.The experiment class also describes the overall protocol; measurement type andincludes the experimental-factors, which can be exploited by bioinformaticians for dataanalysis. Experiments are composed of assays represented by the bioassay content type.Figure 2 Data Model. Data model outlining the relationship between the experiment, its assays andbiomaterials. The Drupal content types are indicated as green circles with the mapping listed underneath.Arrows indicate the relationships.Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3http://www.jbiomedsem.com/content/5/S1/S3Page 4 of 12The bioassay content type is mapped to obo:bioassay and specifies the technology plat-form used and other assay details. Bioassays are typically performed on several repli-cates specified by the replicate content type and mapped to efo:replicate (OBI onlymodels replicate design and analysis). Each replicate is associated with the biologicalmaterial on which the assay is conducted and is specified by the biomaterial contenttype. Thus technical replicates reference the same biomaterial, whereas biological repli-cates reference the unique materials used for the assay. The assays have raw data astheir output. Data transformations and analyses conducted on the raw data are cur-rently not represented, but are included in future plans for the system.Biomaterial is deeply annotated using Drupal Taxonomies and mapped to variouscontrolled vocabularies and ontologies. In the eXframe default package, the organism,tissue type, cell type, disease state and chemical treatment taxonomies are mapped toNCBI Taxonomy (NCBITaxon) [34], FMA [12], CL [11], Disease Ontology (DO) [13]and Chemical Entities of Biological Interest Ontology (ChEBI) [35] terms, respectively.EFO [10], NCI Thesaurus [36] or Breda Tissue Ontology (BTO) [37] is also used toincrease coverage when required. Biomaterial properties and their mappings are config-urable and can be easily customized to a particular domain as required. The mappingsof the main content types (experiments, bioassay, citation, biomaterials etc.) to ontolo-gies are configured in PHP code, in a single file (an excerpt of which is shown inFigure 3). Attributes of the experiment, bioassays, and biomaterials that can be definedvia structured vocabularies are stored as Drupal taxonomies. For example, Cell Type,an attribute of the biomaterial, is represented as taxonomy. Each term in the taxonomyis mapped to a class or classes in external ontologies. Thus, Fibroblast a term in theCell Type taxonomy, is easily added, edited and mapped to ontologies through theweb interface.Linked data & SPARQL endpointWe use the Drupal RDF modules to produce RDF using the mappings discussed above.RDF generated using the Drupal modules [26] is indexed into an RDF store poweredby the ARC2 PHP library [38]. A SPARQL endpoint is also published by this RDFstore. The RDF indexer in Drupal is designed to be backend-agnostic and allow forany RDF store to be plugged in. Were using ARC2, which is sufficient for our needs,but other stores can be used depending on the size of the dataset, or particularSPARQL features that might be needed.Some of the data in the repository is kept private until the researchers publish theirwork. To maintain privacy, we utilize two stores: one of which solely contains theFigure 3 Ontology mapping code. Excerpt from exframe.entity_rdf.inc showing how Drupal classes aremapped to external ontologies.Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3http://www.jbiomedsem.com/content/5/S1/S3Page 5 of 12public data, and whose SPARQL endpoint is publicly available; the other which con-tains the entire data and is kept secure using an API key. The secure, administrativeendpoint is used by R scripts (described in the next section) to access data for queryand analysis by members who have access authorization. The other benefit of havingdecoupled stores is that we have the flexibility of optimizing the performance and scal-ability of each store independently from the other.R IntegrationWe wanted to provide programmatic access to the repository data to retrieve experi-mental information in a manner that is independent of the Drupal database schema.The R statistical programming language [39] and platform is a popular tool for analyz-ing genomics data. Thus, we decided to provide support for accessing RDF data andthe SPARQL endpoint using R. The publicly available R packages to access RDF dataare not yet fully featured; for example the SPARQL package doesnt supportDESCRIBE queries. Hence the RDF package that does support DESCRIBE statementswas used to provide information about the resources. Using the package, first theexperiment RDF is used to obtain information about the assays, and then the assaysprovide information about the biomaterial (See relationships in Figure 2). The RDFpackage also had problems; it is hindered by UTF8 encoding issues. The resulting Rscripts included in the eXframe package produce data structures compatible for analy-sis with R packages such as BioConductor [40,41].ResultsCase study: Stem Cell CommonsStem Cell Commons (SCC) is a project of the Harvard Stem Cell Institute (HSCI) tofreely share biomedical data, tools and resources within the research community [42].Our platform, eXframe, was first implemented independently for the Blood genomicsprogram at HSCI, and then later extended to support all researchers at the Institute, asthe repository of Stem Cell Commons. Data from both the previously developed BloodGenomics store and the Stem Cell Discovery Engine (SCDE) [43] was merged into theeXframe-based SCC database.Genomics datasets are actively curated into the database; currently the repository con-tains over 200 datasets from 20 laboratories representing 4 organisms and 119 different celltypes and 39 tissue types. Results based on approximately half of the datasets (86) havebeen published in scientific journals, and these datasets are therefore available to the public.All bioassays and samples have been deeply annotated with ontologies. First we usedthe OBI ontology [9] for the main entities (experiment, biomaterial and assays) asdescribed in the data model section. Dublin Core [29] and FOAF [31] were used for themetadata and researcher respectively. The ontologies used to annotate the biomaterialsare listed in Table 1. All the Stem Cell Commons public data is available as Linked Dataas well as a SPARQL endpoint as described in the next sections.RDF generationRDF for the experiment, bioassay and biomaterials are automatically generated usingthe Drupal RDF modules as described previously. A screenshot of actual RDF outputfor an experiment curated in the Stem Cell Commons is depicted in Figure 4. It is aMerrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3http://www.jbiomedsem.com/content/5/S1/S3Page 6 of 12next-generation sequencing experiment performed by a HSCI researcher and measuresDNA methylation (using bisulphite sequencing) in the leukemia cell line K562, repro-grammed leukemia cell lines (LiPS) and the human embryonic stem cell line H1. FromFigure 4, we see how the Dublin Core ontology provides the provenance informationJOURNAL OFBIOMEDICAL SEMANTICSWinnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30http://www.jbiomedsem.com/content/5/1/30RESEARCH Open AccessA framework for assessing the consistency ofdrug classes across sourcesRainer Winnenburg and Olivier Bodenreider*AbstractBackground: The objective of this study is to develop a framework for assessing the consistency of drug classesacross sources, such as MeSH and ATC. Our framework integrates and contrasts lexical and instance-based ontologyalignment techniques. Moreover, we propose metrics for assessing not only equivalence relations, but also inclusionrelations among drug classes.Results: We identified 226 equivalence relations between MeSH and ATC classes through the lexical alignment, and223 through the instance-based alignment, with limited overlap between the two (36). We also identified 6,257inclusion relations. Discrepancies between lexical and instance-based alignments are illustrated and discussed.Conclusions: Our work is the first attempt to align drug classes with sophisticated instance-based techniques, whilealso distinguishing between equivalence and inclusion relations. Additionally, it is the first application of aligningdrug classes in ATC and MeSH. By providing a detailed account of similarities and differences between drug classesacross sources, our framework has the prospect of effectively supporting the creation of a mapping of drug classesbetween ATC and MeSH by domain experts.Keywords: Drug classes, MeSH, ATC, Instance-based mapping, Lexical mappingBackgroundMotivation and objectivesDrug classes provide a convenient mechanism for organizingdrugs in terms of chemical structure (e.g., Sulfonamidesagroup of compounds that contain the structure SO2NH2),function (e.g., Anti-Bacterial Agentsoften referred to as an-tibiotics), mechanism of action (e.g., Hydroxymethylglutaryl-CoA Reductase Inhibitorsa group of drugs, also calledstatins, which block an enzyme involved in the productionof cholesterol in the liver), metabolism (e.g., inhibitors ofCYP2C9drugs that block an enzyme from the CytochromeP450 protein family, which is involved in the metabolismof drugs, such as ibuprofen and fluoxetine, and whose ac-tivity is influenced by other drugs, such as rifampicin andfluconazole), and adverse events (e.g., drugs that induceQT prolongationthe antimalarial drug halofantrine slowsdown ventricular repolarization, which predisposes tocertain types of arrhythmias). The interested reader isreferred to [1] for more details about drug classes.* Correspondence: obodenreider@mail.nih.govLister Hill National Center for Biomedical Communications, National Libraryof Medicine, Bethesda, MD, USA© 2014 Winnenburg and Bodenreider; licenseof the Creative Commons Attribution Licensedistribution, and reproduction in any mediumDomain Dedication waiver (http://creativecomarticle, unless otherwise stated.Several drug classifications have been developed fordifferent purposes. For example, the Anatomical TherapeuticChemical (ATC) classification of drugs supports pharmacoe-pidemiology, while the Medical Subject Headings (MeSH)is oriented towards the indexing and retrieval of thebiomedical literature [2,3]. Moreover, sources tend toprovide different lists of drug classes, and such liststend to be organized in different ways according to thepurpose of a given source. For example, the ATC usesa complex classificatory principle, in which the firstsubdivision is primarily anatomical (i.e., distinctionbased on the target organs or anatomical systemse.g.,cardiovascular system drugs vs. dermatologicals), followedby a therapeutic subdivision (i.e., therapeutic intent of thedrugs in each anatomical groupe.g., antibacterial drugsvs. antiviral drugs), followed by a chemical subdivision(i.e., distinction between the structural and functionalcharacteristics of drugs within a therapeutic subgroupe.g., macrolides, such as erythromycin, vs. fluoroquinolones,such as ciprofloxacin, among the antibacterial drugs). Onthe other hand, MeSH maintains two parallel classifica-tions, one based on chemical structure (e.g., ciprofloxacinis represented under fluoroquinolones), and one based one BioMed Central Ltd. This is an Open Access article distributed under the terms(http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,, provided the original work is properly credited. The Creative Commons Publicmons.org/publicdomain/zero/1.0/) applies to the data made available in thisWinnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 2 of 14http://www.jbiomedsem.com/content/5/1/30functional characteristics, including mechanism of action,physiologic effect and therapeutic use. (e.g., ciprofloxacinis linked to the mechanism of action Topoisomerase II In-hibitors and to the therapeutic use Anti-Bacterial Agents).In contrast to ATC, MeSH does not make distinctionsbased on the target anatomical location of the drug(e.g., there are two Fluoroquinolones classes for oph-thalmological use vs. for systemic use in ATC, but onlyone Fluoroquinolones class in MeSH).Ideally, drug classes with similar names should havesimilar members and drug classes with similar membersshould have similar names. In practice, however, the samename can be used to refer to different classes. For ex-ample, in ATC, Fluoroquinolones refers to both a set ofophthalmological drugs (8 members) and a set of systemicdrugs (20 members), while, in MeSH, it refers to over 50chemical compounds with similar structural properties. Inthe absence of an authoritative reference for drug classes,the task of determining when two classes are equivalentacross sources remains extremely challenging. At the sametime, the use of multiple classifications is often required inapplications. This is increasingly the case as the use ofATC for pharmacovigilance is on the rise (e.g., [4]).The objective of this study is to develop a frameworkfor assessing the consistency of drug classes across sources,leveraging multiple ontology alignment techniques. Thisframework is meant to assist experts in the curation of amapping between drug classes across sources. We presenttwo applications of this framework, one to the alignment ofdrug classes between MeSH and ATC, and the other to theintegration of MeSH and ATC drug class hierarchies. Toour knowledge, this work represents the first effort to aligndrug classes between MeSH and ATC using a sophisticatedinstance-based alignment technique. Moreover, we proposemetrics for assessing not only equivalence relations be-tween classes, but also inclusion relations.Application of ontology alignment techniques to drug classesThe broad context of this study is that of ontology alignment(or ontology matching). Various techniques have been pro-posed for aligning concepts across ontologies, including lex-ical techniques (based on the similarity of concept names),structural techniques (based on the similarity of hierarchicalrelations), semantic techniques (based on semantic similaritybetween concepts), and instance-based techniques (based onthe similarity of the set of instances of two concepts). Anoverview of ontology alignment is provided in [5]. The maincontribution of this paper is not to propose a novel tech-nique, but rather to apply existing techniques to a novelobjective, namely aligning drug classes between MeSHand ATC. To this end, we use lexical and instance-basedtechniques, because the names of drug classes and the listof drugs that are members of these classes are the maintwo features available in these resources.Lexical techniquesLexical techniques compare concept names across on-tologies and are a component of most ontology align-ment systems [5]. When synonyms are available, theycan be used to identify additional matches. Matchingtechniques beyond exact match utilize edit distance ornormalization to account for minor differences betweenconcept names.As part of the Unified Medical Language System (UMLS),linguistically-motivated normalization techniques have beendeveloped specifically for biomedical terms [6]. UMLSnormalization abstracts away from inessential differences,such as inflection, case and hyphen variation, as well asword order variation. The UMLS normalization techniquesform the basis for integrating terms into the UMLSMetathesaurus, but can be applied to terms that are notin the UMLS. For example, the ATC class Thiouracils(H03BA) and the MeSH class Thiouracil (D013889) matchafter normalization (ignoring singular/plural differences).Lexical techniques typically compare the names of con-cepts across two ontologies as provided by these ontologies.However, additional synonyms can be used, for example,synonyms from the UMLS Metathesaurus. In other words,we leverage cosynonymy similarity for matching drug clas-ses. In this case, although the ATC class Anticholinesterases(N06DA) and the MeSH class Cholinesterase Inhibitors(D002800) do not match lexically, both names are cosyno-nyms, because they are found among the synonyms of theUMLS Metathesaurus concept C0008425.While there have been attempts to map individual drugsfrom ATC to concepts in the UMLS and MeSH throughlexical techniques, [7] note that these techniques are notappropriate for the mapping of drug classes.Instance-based techniquesAlso called extensional techniques, instance-based tech-niques compare classes based on the sets of individuals(i.e., instances) of each class. While instance-based tech-niques are also available in many ontology alignment sys-tems, the applicability of this technique is limited, becausemost biomedical ontologies consist of class hierarchies, butdo not contain information about instances. Here, however,individual drugs (e.g., atorvastatin) are the membersnotsubclassesof drug classes (e.g., statins). In other words,drug classes have individual drugs as instances, not sub-classes and are therefore amenable to instance-basedalignment techniques.Several methods have been proposed to implementinstance-based matching. [8] decompose these methods intothree basic elements: (1) A measure is used for evaluatingthe association between two classes based on the proportionof shared instances. Typical measures include information-based measures (e.g., Jaccard similarity coefficient) and stat-istical measures (e.g., log likelihood ratio). (2) A threshold isWinnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 3 of 14http://www.jbiomedsem.com/content/5/1/30applied to the measures and pairs of classes for whichthe measure is above the threshold are deemed closelyassociated and mapping candidates. (3) Hierarchical re-lations in the two ontologies to be aligned can also beleveraged by deriving instance-class relations betweeninstances of a given class and the ancestors of this class.In other words, in addition to asserted classes (i.e., theclasses of which individual drugs are direct members),we also consider inferred classes (i.e., the classes ofwhich asserted classes are subclasses). For example,the class asserted in MeSH for the drug atorvastatin isHydroxymethylglutaryl-CoA Reductase Inhibitors (i.e.,statins), whose parent concepts include AnticholesteremicAgents. Therefore, the class Anticholesteremic Agents isan inferred drug class for atorvastatin.To our knowledge, our work is the first attempt to aligndrug classes with instance-based techniques (i.e., beyondname matching), and the first application of aligning drugclasses in ATC and MeSH. Moreover, while most ontologyalignment systems mainly consider matches betweenequivalent classes, we are also interested in identifyingthose cases where one class is included in another class.Related work on drug classes, MeSH and ATCIn previous work, we compared drug classes betweenthe National Drug File-Reference terminology (NDF-RT)and SNOMED CT from the perspective of semanticmining [9]. We also used an instance-based alignmenttechnique, but only considered overlap between classes,not inclusion. Lexical alignment of the classes was notperformed. Overall, we found that the overlap betweenNDF-RT and SNOMED CT classes was very limited. In[10], we mapped selected drug classes between NDF-RTand ATC through lexical techniques, observed the limi-tations of lexical techniques for the alignment of drugclasses (also noted by [7]), and argued that the alignmentcould be improved by identifying mappings between thedrugs in these classes.As part of the EU-ADR project, [11] extracted adversedrug reactions from the biomedical literature and mappedMeSH drugs to ATC through the UMLS. However, theirmapping was limited to individual drugs and did notinclude drug classes. The alignment of drug classes isone element of the broader integration of drug informationsources in systems, such as the one developed by [12].However, the preliminary version of their system integratesATC, NDF-RT, RxNorm and the Structured Product labels,but not MeSH or the biomedical literature.ResourcesOur framework leverages several knowledge sources.In addition to ATC and MeSH, the two sources of drugclasses we propose to align and from which we extractinformation about drug-class membership, we also takeadvantage of RxNorm for aligning and normalizing indi-vidual drugs, and of the UMLS Metathesaurus as a sourceof synonymy for the lexical mapping of drug class names.Anatomical Therapeutic Chemical Drug ClassificationSystem (ATC)The ATC is a clinical drug classification system developedand maintained by the World Health Organization (WHO)as a resource for drug utilization research to improve qual-ity of drug use [2]. The system is organized as a hierarchythat classifies clinical drug entities at five different levels: 1stlevel anatomical (e.g., C: Cardiovascular system), 2nd leveltherapeutic (e.g., C10: Lipid modifying agents), 3rd levelpharmacological (e.g., C10A: Lipid modifying agents, plain),4th level chemical (e.g., C10AA: HMG CoA reductaseinhibitors), and 5th level chemical substance or ingre-dient (e.g., C10AA05: atorvastatin). The 2013 versionof ATC integrates 4,516 5th-level drugs and 1,255 druggroups (levels 1-4). We refer to these drug groups asATC classes.Medical Subject Headings (MeSH)The Medical Subject Headings (MeSH) is a controlledvocabulary produced and maintained by the NLM [3]. Itis used for indexing, cataloging, and searching the bio-medical literature in the MEDLINE/PubMed database,and other documents. The MeSH thesaurus includes26,853 descriptors (or main headings) organized in 16hierarchies (e.g., Chemical and Drugs). Additionally,MeSH provides about 210,000 supplementary conceptrecords (SCRs), of which many represent chemicals anddrugs (e.g., atorvastatin). Each SCR is linked to at leastone descriptor through a heading mapped to relation(e.g., atorvastatin is associated with Heptanoic Acids andPyrroles). These descriptors mapped to generally denotethe chemical structure of the drug. While most chemicaldescriptors provide a structural perspective on drugs,some descriptors play a special role as they can beused to annotate the functional characteristics of drugdescriptors and SCRs through a pharmacologic actionrelation (e.g., atorvastatin is linked to the mechanism ofaction Hydroxymethylglutaryl-CoA Reductase Inhibitorsand to the therapeutic use Anticholesteremic Agents).MeSH 2013 is used in this study.RxNormRxNorm is a standardized nomenclature for medicationsproduced and maintained by the U.S. National Libraryof Medicine (NLM) [13]. RxNorm concepts are linkedby NLM to multiple drug identifiers for commerciallyavailable drug databases and standard terminologies,including MeSH and ATC. (While RxNorm integratesdrugs and classes from ATC and drugs from MeSH, itdoes not integrate classes from MeSH.) RxNorm servesWinnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 4 of 14http://www.jbiomedsem.com/content/5/1/30as a reference terminology for drugs in the U.S. TheAugust 2013 version of RxNorm used in this study inte-grates 10,108 substances, including ingredients (IN) andprecise ingredients (PIN). Ingredients generally representbase forms (e.g., atorvastatin), while precise ingredients tendto represent esters and salts (e.g., atorvastatin calcium).RxNorm also represents clinical drugs, i.e., the drugsrelevant to clinical medicine (e.g., atorvastatin 10 MGOral Tablet). The relations among the various drug entitiesare represented explicitly in RxNorm (e.g., betweeningredients and precise ingredients, and between in-gredients and clinical drugs). NLM also provides anapplication programming interface (API) for accessingRxNorm data programmatically [14].Unified Medical Language System (UMLS)The UMLS is a terminology integration system created andmaintained by the National Library of Medicine (NLM)[15]. The UMLS Metathesaurus integrates over 150 ter-minologies, including MeSH, but not ATC. Synonymousterms across terminologies are grouped into conceptsand assigned the same concept unique identifier. TheMetathesaurus provides a comprehensive set of synonymsfor biomedical concepts, including drug classes, and isoften used for integrating terminologies beyond its own(e.g., [16]). Therefore, the UMLS is a useful resource formapping class names from ATC to drug class conceptspresent in the source vocabularies of the Metathesaurus.NLM provides an application programming interface(API) for accessing UMLS data programmatically. Version2013AA of the UMLS is used in this studya.MethodsOur framework for assessing the consistency of drug classesacross sources (here MeSH and ATC) uses techniques foraligning drug classes based on their names and druginstances as depicted in Figure 1. It can be summarizedas follows. Having established a reference list of drugsand drug classes, we compare the drug classes betweenMeSH and ATC based on their names (lexical align-ment, Figure 1, right) and on the individual drugs theseclasses contain as members (instance-based alignment,Figure 1, left). Toward this end, we leverage similarityRxNormINs/PINsMeSHdrugsATCdrugs ATC (classes)MeSH (classes)instance-based alignmentlexicalalignmentFigure 1 Alignment of ATC and MeSH classes.measures to compare the set of drugs in a class to theset of drugs in another class from the dual perspectiveof equivalence and inclusion. Finally, we compare thealignments obtained by the two approaches.Establishing a common reference for drugs, drug classesand drug-class membersDrugsAs of August 2013, both ATC and MeSH are integratedin RxNorm. We consider all MeSH drugs present inRxNorm, regardless as to whether they correspond todescriptors (also called main headings) or SupplementaryConcept Records (SCR) in MeSH. Our starting set of ATCdrugs consists of 5th-level ATC entities, from which weexclude combination drugs, often underspecified andunlikely to be represented in MeSH.As a result of the integration of MeSH and ATC intoRxNorm, the same RxNorm identifier is assigned to anATC drug and to the equivalent drug in MeSH. Individ-ual drugs in MeSH and ATC correspond to ingredients(IN) and precise Ingredients (PIN) in RxNorm. In orderto facilitate the comparison of individual drugs betweenMeSH and ATC, we normalize the drugs by mappingeach precise ingredient to its corresponding ingredient.We restrict our set of drugs to drugs of clinical relevanceby filtering out those ingredients that are not associatedwith any clinical drugs in RxNorm. The set of individualdrugs described here constitutes the set of eligible drugsfor this study.Drug classesIn order to minimize the number of pairwise compari-sons between MeSH and ATC drug classes, we excludebroad, top-level classes from MeSH and ATC, forwhich the alignment would not be meaningful anyway.In practice, we exclude the 14 ATC classes of level 1(anatomical classification). Similarly, we exclude thetop-level descriptors of the Chemicals and Drugs hier-archy (i.e., D01-D27) in MeSH, as well as the top-level ofthe pharmacological action descriptors (PharmacologicActions, Molecular Mechanisms of Pharmacological Action,Physiological Effects of Drugs, and Therapeutic Uses).Additionally, we exclude 167 of the 1,241 ATC classes(2nd4th level) corresponding to drug combinations,because combination drugs are often underspecified inATC. We define drug combination classes in ATC asclasses that contain combination (case-insensitive) in theirlabels or have ancestor classes with combination in theirlabels (e.g., G03EA: Androgens and estrogens are excludedalong with their ancestor class G03E: ANDROGENSAND FEMALE SEX HORMONES IN COMBINATION).Finally, we further exclude from MeSH and ATC anyclasses that are not connected to any eligible individualdrug (as defined above), directly or through a subclassWinnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 5 of 14http://www.jbiomedsem.com/content/5/1/30(e.g., A03AC: Synthetic antispasmodics, amides withtertiary amines contains three drugs (dimethylamino-propionylphenothiazine, nicofetamide, tiropramide), ofwhich none are eligibleb). The set of drug classes de-scribed here constitutes the set of eligible drug classesfor this study.Drug-class membershipAs mentioned earlier, the relation between a class andits drug members can be either direct (i.e., asserted) orindirect (i.e., inferred). In ATC, we consider as direct re-lations the relations asserted between 5th-level drugsand their 4th-level chemical classes. We infer drug-classrelations between 5th-level drugs and the correspondingATC classes at the 3rd and 2nd level. For example, as il-lustrated in Figure 2, the drug temafloxacin (J01MA05)is a member of the chemical class Fluoroquinolones(J01MA - asserted), the pharmacological class QUINOLONEANTIBACTERIALS (J01Minferred, 3rd level), and thetherapeutic class ANTIBACTERIALS FOR SYSTEMIC USE(J01inferred, 2nd level). Level-1 classes are ignored.Figure 2 Individual drugs and drug classes in RxNorm, MeSH and ATCExtracting drug-class membership relations from MeSHis a more complex process, because drugs can be repre-sented at different levels (descriptor or supplementaryconcept record), structural classes and functional classesare represented by different types of descriptors, and drugsare related to classes through various kinds of relationships.Relations between drugs (descriptors or SCRs) and func-tional classes (i.e., descriptor from the pharmacologicalactions hierarchy) are asserted through a pharmacologicaction relationship. Relations between an SCR drug andits heading mapped toc constitute the asserted relationsto structural classes, as do relations between a descriptordrug and its direct parent. We infer drug-class relationsbetween any drug and all the ancestors (direct or indirect)of the descriptors corresponding to their structural andfunctional (asserted) classes.For example, as illustrated in Figure 2, the SCRtemafloxacin has Anti-Bacterial Agents as pharmacologicalaction and Fluoroquinolones as heading mapped to. Fromthese asserted classes, we infer membership to Anti-InfectiveAgents (from Anti-Bacterial Agents) and to Quinolones,.Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 6 of 14http://www.jbiomedsem.com/content/5/1/30Quinolines, and Heterocyclic Compounds, 2-Ring (fromFluoroquinolones). Top-level classes are ignored.Aligning drug classesLexical alignmentWe leverage the UMLS (synonyms and lexical match-ing features) for aligning drug classes by their names.In practice, we consider equivalent classes those MeSHand ATC classes, whose names map to the same UMLSconcept. If both MeSH and ATC were integrated in theUMLS, we would only have to extract all UMLS con-cepts to which both a MeSH class and an ATC classare mapped. Since MeSH is integrated in the version ofthe UMLS used in this study, but ATC is not, we mapATC classes to the UMLS in order to link them to theequivalent classes in MeSH. More precisely, we use theExactString and NormalizedString search function ofthe UTS API 2.0 to establish mappings between thenames of the ATC classes and UMLS concepts. We usenormalization only when the exact technique does notresult in a match. We then associate the ATC class to aMeSH class through the UMLS concept to which theyboth map (e.g., H03BA: Thiouracils to D013889: Thiouracilthrough UMLS concept C0039957).Instance-based alignmentWe assess the similarity between two classes based onthe individual drug members (instances) they share. Inpractice, we perform a pairwise comparison between allATC classes and all MeSH classes, asserted and inferred.We define two scores for identifying equivalence and in-clusion relations between ATC and MeSH classes.Equivalence Score (ES) The Jaccard coefficient (JC) is ameasure of the similarity between two sets, for examplebetween the set of drugs in a given ATC class (A) and ina given MeSH class (M). However, many drug classesonly contain a small number of drugs, and, in this case,a small number of shared drugs between classes canyield relatively high Jaccard values. In order to reducethe similarity of pairs of classes with small numbers ofshared drugs, we use a modified version of the Jaccardcoefficient, JCmod, as suggested in [8],JC A;Mð Þ ¼ amaþmþ amES A;Mð Þ ¼ JCmod A;Mð Þ ¼ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiam am?0:8ð Þpaþmþ amwhere am represents the number of drugs common toA and M, and a +m + am the total number of uniquedrugs in both classes.Inclusion Score (IS) The Jaccard coefficient measuresthe similarity between the two classes, but does not reflectwhether one class is included in the other. Because of thedifference in organization and granularity between classesin ATC and MeSH, a given ATC class may not have anequivalent class in MeSH, but can be included in anotherMeSH class (e.g., C07AA: Beta blocking agents, non-selectiveincluded in D000319: Adrenergic beta-Antagonists). Suchinclusion relations are crucial for a comprehensive align-ment of the drug classes. We introduce a metric for findingfine-grained (child) classes that are included in coarse(parent) classes. This metric combines two elements. Thefirst one measures the intensity of the one-sidedness, i.e.,the extent to which the instances outside the intersectionare not distributed between both sides, but rather belong toonly one of the two classes. The second element measuresthe coverage of the finer-grained (child) class by thecoarser (parent) class.IS is calculated as follows:IS A;Mð Þ ¼ 0; for Cp A;Mð Þ ¼ 0 and Cc A;Mð Þ ¼ 0IS A;Mð Þ ¼ a?maþmammin amþ a; amþmð Þ ; otherwisewhere am represents the number of drugs common to Aand M, and a and m the number of drugs specific to Aand M, respectively.For example, if A contains 10 drugs and M contains20 drugs and if the two classes share 9 drugs, IS(A,M)= 0.75,providing a strong indication that A is included in M.More generally, a value of IS close to 0 indicates thatthe drugs that are not shared by the two classes areevenly distributed between the ATC and MeSH class, i.e., there is no inclusion relation between the classes. Incontrast, a value of IS close to 1 (in absolute value) indi-cates that the parent class contains most of the drugsthat are not shared by the two classes and that the childclass has a small proportion of specific drugs. The IS(A,M)score varies between ?1 and 1, and a score of 1 correspondsto the inclusion of A in M, while a score of ?1 correspondsto the inclusion of M in A.Selecting classes with the best equivalence and inclusionrelations A given class in ATC or MeSH may have bothequivalence and inclusion relations to classes from theother terminology. Moreover, it may have more thanone equivalence relation and often has multiple inclu-sion relations. We propose an approach for selectingthe best equivalence and inclusion relations for a givenclass. We heuristically determined 0.5 to be a reasonablethreshold for both ES and IS. Therefore, none of thepairs of classes with ES or IS values lower than 0.5 willbe considered for equivalence or inclusion, respectively.For a given class Cc, the class Ce selected as the bestequivalent class is the one with the highest ES. In con-trast, the class Cp selected as the best inclusion class isnot necessarily the one with the highest IS, because theTable 1 Selection of the ATC and MeSH classes suitablefor the instance-based alignmentATC MeSHCandidate drugs in terminology 2,730 4,153Corresponding drug entities in RxNorm (IN, PIN) 2,239 5,274Drug entities after normalization of PINs to INs 2,215 4,112Restriction to clinically-significant ingredients 1,706 2,339Restriction to clinically-significant ingredientspresent in both terminologies1,685 1,685Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 7 of 14http://www.jbiomedsem.com/content/5/1/30class with the highest IS is most likely a very broad class.IS favors large parent classes, while the best parent classis the smallest parent class that covers a large propor-tion of the child class. Therefore, we select as the bestinclusion relation the first pair among the best candi-date equivalence pairs for which IS is above the thresh-old of 0.5. Although it might seem counterintuitive toselect inclusion pairs among the candidate equivalencepairs, the high ES is consistent with the requirement forcoverage of the child class by the parent class.Usually the best equivalence and inclusion pairs aredifferent, but not always. For instance, the mapping be-tween two very similar classes, where one class containsa few specific drugs, might have both IS and ES abovethe threshold. Different use cases may call for differentstrategies for determining the best equivalent and inclu-sion pairs. For instance, while our strategy considersboth scores, ES and IS, when they are above the thresh-old, an alternative strategy could be to choose one scoreover the other based on max(ES, IS).Assessing the consistency between lexical andinstance-based alignmentsWe hypothesize that classes with similar drugs shouldhave similar names and classes with similar names shouldcontain similar drugs. We compare the results of thelexical and instance-based alignment methods and assesstheir consistency. We expect the lexical alignment to iden-tify equivalence classes, not class inclusion. Therefore, pairsof classes identified through the lexical alignment (LEX+)and identified as equivalent through the instance-basedalignment (EQ+) are considered consistent, as are the pairsof classes neither identified through the lexical alignment(LEX-) nor identified as equivalent through the instance-based alignment (EQ-). Conversely, pairs of classes identifiedthrough the lexical alignment (LEX+) but not identified asequivalent through the instance-based alignment (EQ-) areconsidered inconsistent, as are the pairs of classes not identi-fied through the lexical alignment (LEX-) but identified asequivalent through the instance-based alignment (EQ+).ResultsEstablishing a common reference for drugs, drug classesand drug-class membersDrugsAs shown in Table 1, we retrieved from RxNorm 2,239Ingredients (IN) and Precise Ingredients (PIN) that aremapped to 2,730 unique drugs in ATC, and 5,274 that aremapped to 4,153 drugs in MeSH. After normalization toINs, we selected 2,215 INs for ATC and 4,112 for MeSH.Finally, after restricting the RxNorm INs to those that areclinically relevant, we selected 1,706 INs for ATC and2,339 for MeSH. Of these, 1,685 drugs are present in bothATC and MeSH.Drug classesFrom the 1,255 ATC classes (1st4th level) we excluded14 ATC classes at the 1st level (anatomical classification)and 167 classes corresponding to drug combinations,leaving 1,074 classes eligible for the lexical alignment.We further excluded 81 empty classes without anydrug (ATC contains empty classes by design), and 159classes containing only drugs that cannot be mapped toRxNorm. The final set of ATC classes eligible for theinstance-based alignment, A*, contains 834 drug classes,of which 558 are considered asserted (4th level) and 276inferred (2nd3rd level).In MeSH, we identified 1,516 descriptors as drug classesfor the eligible drugs, including 1,223 asserted classes and293 inferred classes. These classes constitute the set ofMeSH classes eligible for both the lexical and the instance-based alignment, M*. We classify 403 of the drug classes inM* as functional classes, i.e., their descriptors are located inthe Chemical Actions and Uses [D27] sub-tree in MeSH,and 1,113 as structural classes.Drug-class membershipFor the 1,685 eligible drugs in MeSH, we established15,122 drug-class pairs, of which 4,759 are assertedand 10,363 inferred. For the eligible drugs in ATC, weestablished 6,368 drug-class pairs, of which 2,140 areasserted and 4,228 inferred.Aligning drug classesLexical alignmentFor the 1,074 eligible ATC classes, we were able to retrieve226 mappings to descriptors from the Chemicals and Drugs([D]) tree in MeSH. We found 18 mappings for therapeuticclasses (2nd level), 43 for pharmacological classes (3rd level),and 165 for chemical classes (4th level). Of the 226 map-pings, 99 are to pharmacological actions (functional classes)in MeSH, whereas 127 are to other descriptors at variouslevels of the MeSH hierarchy (structural classes).Instance-based alignmentEquivalence and inclusion scores Of the 834 ATCclasses eligible for instance-based alignment (|A*| = 834),828 (99%) could be associated with at least one MeSH class.Table 3 Characterization of the associations betweenATC and MeSH classes based on scores for equivalenceand inclusionATC to MeSH Best equivalence>.5 <.5 TotalBest inclusion >.5 148 (17%) 580 (70%) 728 (87%)<.5 1 (1%) 99 (12%) 100 (13%)Total 149 (18%) 679 (82%) 828 (100%)MeSH to ATC Best equivalence>.5 <.5 TotalBest inclusion >.5 120 (9%) 390 (30%) 510 (39%)<.5 45 (3%) 762 (58%) 807 (61%)Total 165 (12%) 1,152 (88%) 1,317 (100%)Table 4 Consistency between lexical and instance-basedWinnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 8 of 14http://www.jbiomedsem.com/content/5/1/30Of the 1,516 eligible drug classes in MeSH (|M*| = 1,516),1,317 (87%) could be associated with at least one ATC class.We conducted a pairwise comparison of all ATC classeswith all MeSH classes (|A*| x |M*| = 1,264,344). For the26,842 pairs that had at least one drug in common, we cal-culated the equivalence (ES) and inclusion (IS) scores. Asshown in Table 2, 223 pairs (<1%) had an ES ? .5 and wereconsidered equivalent (EQ+), and 6,257 pairs (23%) had anIS ? .5 and were considered in inclusion relation (IN+). Ofnote, there were 108 pairs with both strong equivalence andinclusion relations (EQ+ and IN+). The remaining 20,470pairs were considered unrelated, absent any strong equiva-lence or inclusion relations (EQ- and IN-).Classes with strong equivalence and inclusion relationsA given class in ATC or MeSH may have more than onestrong relation to a drug class from the other terminology.We determined the best equivalence and inclusion map-pings (not mutually exclusive) for each of the 828 ATC and1,317 MeSH classes with shared drugs, respectively.As shown in Table 3 (top), 828 ATC classes had some re-lation (equivalence or inclusion, but not necessarily strong)to a MeSH class. Of these, we identified 149 ATC classes(18%) with at a strong equivalence relation to MeSH, allbut one of which also showed a strong inclusion to someMeSH class (albeit not necessarily the same as the equiva-lent class). A strong inclusion relation to MeSH was foundfor 728 (87%) of these ATC classes. On the other hand,1,317 MeSH classes had some relation to an ATC class.Of these, we identified 165 MeSH classes (12%) with astrong equivalence relation to ATC, most of which alsoshowed a strong inclusion relation to some ATC class. Astrong inclusion relation to ATC was found for 510 (39%)of these MeSH classes (Table 3, bottom). The 1,317 MeSHclasses linked to ATC include 374 functional classes (28%)and 943 structural classes (72%). Overall, a strong relation(equivalence or inclusion) was found between 729 ATCclasses in ATC and the 555 MeSH classes.Assessing the consistency between lexical andinstance-based alignmentsThe results of the comparison between the lexical andinstance-based alignments are shown in Table 4. Weperformed the comparison on the cross-product of the 834Table 2 Analysis of the instance-based alignment betweenATC and MeSH classesequivalence vs. inclusion relationsInclusion relationYes (IN+) No (IN-) TotalEquivalence relation Yes (EQ+) 108 115 223No (EQ-) 6,149 20,470 26,619Total 6,257 20,585 26,842eligible ATC and 1,516 MeSH classes (1,264,344 pairs). Ofthe 226 pairs of equivalent classes between ATC and MeSHidentified through the lexical alignment, 36 (16%) were con-firmed through the instance-based approach (LEX+/EQ+),of which 14 were also categorized as inclusion relations.Not surprisingly, no equivalence relation was identified byeither approach for the bulk of the pairs from the cross-product between ATC and MeSH classes. A total of 313 in-consistencies between the two alignment approaches wereidentified, including 126 pairs identified exclusively by thelexical alignment (LEX+/EQ-), and 187 pairs specific to theinstance-based alignment (LEX-/EQ+). This finding dis-proves our initial hypothesis that classes with similar nameshave similar drugs and vice versa. Of note, 64 pairs ofequivalent classes identified through the lexical alignmentwere not amenable for processing by the instance-basedalignment, because at least one class of the pair did notcontain any eligible drug.DiscussionAnalysis of similarities and discrepancies between lexicaland instance-based alignmentsAs illustrated through a few examples throughout thissection, our framework facilitates the comparison of drugclasses across sources and reveals inconsistencies in theclasses, as well as deficiencies in the alignment techniques.alignments of drug classes (italics values denoteinconsistencies)Lexical alignmentYes (LEX+) No (LEX-) TotalInstance-basedalignmentYes (EQ+) 36 187 223No (EQ-) 126 1,263,995 1,264,121Total 162 1,264,182 1,264,344No data 64Total LEX+ 226Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 9 of 14http://www.jbiomedsem.com/content/5/1/30Valid mappingsWe identified an equivalence relation between the 4th-levelATC class Tetracyclines (J01AA) and the MeSH descriptorTetracyclines (D013754). The two classes share nine drugs.The MeSH class has one extra drug (meclocycline), which isin a different class in ATC (Antiinfectives for treatment ofacne), because, although structurally similar, it is not usedsystemically but topically. Jaccard similarity is high (0.86).This (equivalence) mapping is also identified by thelexical technique (exact match). Of note, the inclusionscore (1 in absolute value) is also high, because there is onlyone drug that is not in common, which is - automatically -located on only one side of the intersection.Erroneous lexical mappingsWe identified an inclusion mapping between the 4th-level ATC class Fluoroquinolones (S01AE) and the MeSHdescriptor Fluoroquinolones (D024841). Although the twoclass names are identical, which would suggest an equiva-lence relation, our mapping is identified as an inclusion,with seven drugs in common, one drug specific to theATC class and eleven drugs specific to the MeSH class. Infact, the ATC class is the specific class of fluoroquinolonesfor ophthalmic use (S01AE), in contrast to the class offluoroquinolones for systemic use (J01MA)d. The fluor-oquinolones used for eye disorders are (almost) a subsetof all fluoroquinolones and the ATC class S01AE is ap-propriately characterized as being included in the MeSHclass for fluoroquinolones. This example also constitutesan erroneous lexical mapping, since lexical mappings areexpected to reflect equivalence relations.Missing instance-based mappingsMany ATC and MeSH classes share only one or veryfew drugs, making it difficult to assess equivalence or inclu-sion with confidence. For example, the 4th-level ATC classSilver compounds (D08AL) and the MeSH descriptor SilverCompounds (D018030) share only one drug (silver nitrate),where Silver Compounds (D018030) contains another drug(silver acetate), which is in RxNorm but not in ATC. Themodified version of the Jaccard coefficient has a score of0.22 in this case, which is below our threshold of 0.5 forequivalence. However, we classified the ATC class D08ALas being included in the MeSH class Silver Compounds.During this failure analysis, we discovered that someMeSH drugs did not have a pharmacological actionassigned to them as we expected. For example, while pyr-antel is listed as Antinematodal Agents, oxantel is note.The MeSH editorial rules require that a certain number ofarticles assert a given pharmacologic action for it to berecorded in MeSH. Because of these missing pharmaco-logic actions, the 3rd-level ATC class ANTINEMATODALAGENTS (P02C) fails to be mapped to the MeSHpharmacological action Antinematodal Agents (D000969),the Jaccard similarity being below the threshold (0.37).As mentioned earlier, some ATC classes only containdrugs that cannot be mapped to MeSH through RxNorm,which we used to bridge between the two. Such classesmay be amenable to lexical alignment, but cannot bealigned through their instances. Similarly, some drugentities and biologicals (e.g., vaccines) are less well stan-dardized than most common drugs. For this reason, theinstance-based alignment may not be able to align theseclasses, when simple lexical techniques can. For example,the instance-based method fails to align the two classesEpoxides (L01AG) and Epoxy Compounds (D004852) be-cause the ATC class does not contain any eligible drug(the only instance, etoglucid (L01AG01), is not listed asa clinical drug in RxNorm).Missing lexical mappingsDespite the use of UMLS synonymy and normalization,the lexical alignment fails to identify a mapping betweenthe 3rd-level ATC class POTASSIUM-SPARING AGENTS(C03D) and the MeSH pharmacological action Diuretics,Potassium Sparing (D062865). In contrast, the instance-based alignment identifies an equivalence mapping withhigh Jaccard similarity (0.72). This finding is consistent withthe conclusions of [7].Further characterization of equivalence andinclusion relationsEven when considering only strong relations and thebest inclusion relations between ATC and MeSH classes,it is difficult to give a detailed account of the direction-ality of the relations, and the distribution between struc-tural and functional classes. Some salient findings aresummarized in Table 5. For example, we found 223(strong) equivalence relations between 149 unique ATCclasses and 165 unique MeSH classes, distributed al-most evenly between structural and functional classes inMeSH. When restricting the analysis to the best inclu-sion relations, more ATC classes (728) are found to beincluded in some MeSH class, than MeSH classes (510)are in some ATC classes. And fewer functional classes(146) than structural classes (364) in MeSH are includedin some ATC class.For almost all drug classes in ATC that have anequivalence mapping to a drug class in MeSH, there isalso at least one inclusion mapping to a broader classin MeSH. There is only one exception. The class Drugsused in diabetics (A10) is equivalent to HypoglycemicAgents (D007004), which is already at the highest level weconsider in MeSH (we ignore its parent class PhysiologicalEffects of Drugs because it is too general). In contrast,there are 45 classes in MeSH that are equivalent to ATCclasses but are not included in another class in ATC. ForTable 5 Detailed analysis of the mapping between ATCand MeSH classesStructural vs. functional classesType ofrelationDirection # strongrelations# uniqueATC classes# uniqueATC classesEquivalence(all)ATC-MeSH (all) 223 149 165ATC-MeSH (St) 115 77 84ATC-MeSH (Fn) 108 86 81Inclusion(all)ATC toMeSH (all)4914 728 650MeSH (all)to ATC1343 358 510Inclusion(best)ATC toMeSH (all)1267 728 483ATC toMeSH (St)597 559 275ATC toMeSH (Fn)670 657 208MeSH (all)to ATC568 264 510MeSH (St)to ATC406 211 364MeSH (Fn)to ATC162 102 146Details of the instance-based alignment between functional (Fn) and structural(St) classes in ATC and MeSH.Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 10 of 14http://www.jbiomedsem.com/content/5/1/30example, Antiparkinson Agents (D00978) maps to the 2ndlevel class Anti-Parkinson Drugs (N04) in ATC. Becausewe exclude 1st level classes in ATC, there is no parentclass in ATC which would include the drug of the MeSHclass Antiparkinson Agents. Conversely, the ATC classAnti-Parkinson Drugs (N04) is included in the higher levelclass Central Nervous System Agents (D002491) in MeSH,which is a parent of Antiparkinson Agents.The alignment between ATC classes and MeSH classescan be further characterized, especially in order to ac-count for concomitant occurrences of a strong inclusionrelation to a structural class and to a functional class. Asshown in Table 6, of the 505 strong equivalence and bestTable 6 Detailed analysis of the mapping between ATCand MeSH classesequivalence vs. inclusion relationsATC to MeSH To astructuralclass onlyTo afunctionalclass onlyTo both astructural and afunctional classTotalEquivalencerelation only0 1 0 1Both equivalenceand bestinclusion relations1 8 58 67Best inclusionrelations only50 75 312 437Total 51 84 370 505Analysis of concomitant equivalence and best inclusion relations between ATCand MeSH classes, when structural and functional classes in MeSH areconsidered separately.inclusion relations to structural and functional clas-ses in MeSH, the most frequent situation is the con-comitant occurrence of inclusion to both a structuraland a functional class. Of note, there is only one casewhere an equivalence relation occurs without a con-comitant inclusion relation.Application of the framework to the alignment ofimportant drug classesOne typical use case for the alignment of drug classesis to find equivalent classes in reference sources for agiven class (e.g., to find which class best representsmacrolides in MeSH and ATC). In order to illustratehow our approach supports the alignment of drugclasses between MeSH and ATC, we applied ourframework to a set of clinically relevant drug clas-ses. We used the set of high-severity, clinically sig-nificant drugdrug interactions created by [17], inwhich most drugs are categorized in reference todrug classes.We extracted all 13 drug classes from the list of veri-fied critical drugdrug interactions discussed in theirpaper (Table 7). We first performed a lexical mappingto identify these 13 classes in MeSH and ATC (usingnormalized string matches against the UMLS). Onlyin six cases did the lexical mapping approach retrieveclasses in both classifications. In another six cases, wewere able to retrieve the class in either ATC or MeSH.The class QT prolonging agents was not found in eithersource.For each drug class that we retrieved through lexicalmapping, we used our instance-based approach to deter-mine the best corresponding class in the other termin-ology. Table 8 shows the strength of the mappings interms of equivalence and inclusion. There is only one case(HMG CoA reductase inhibitors) where the two lexicalmatches also correspond to the best equivalent classesbased on the drug instances. For five other classes wefound equivalent class pairs starting from one lexicalmatch. For four classes we could not find equivalentmappings across the two classifications, but inclusion map-pings instead. Finally, three classes were left unmapped.(Two of these classes were underspecified as evidenced bythe mention [and] derivatives in their name. The last one,QT prolonging agents, was not represented in either source,which is often the case for drug classes defined in referenceto adverse effects [18]).This application illustrates the effectiveness of ourframework to support a clinical expert in the curation ofan alignment of drug classes between MeSH and ATC.It helps identify lexically similar classes in these twosources, but, more importantly, it helps identify whichclass of the other source is most closely related to agiven class. This feature enables experts to verify if theTable 7 Lexical mapping to ATC and MeSH for 13 clinically relevant drug classesDDI class ATC class lexical match MeSH class lexical match Best correspondingclass in ATCBest correspondingclass in MeSHTriptans - Tryptamines (D014363) Selective serotonin (5HT1)agonists (N02CC)-Proton pump inhibitors Proton pump inhibitors(A02BC)Proton pump inhibitors(D054328)- 2-Pyridinylmethylsulfinyl-benzimidazoles (D053799)HMG CoA reductase inhibitors HMG CoA reductaseinhibitors (C10AA)Hydroxymethylglutaryl-CoAReductase Inhibitors (D019161)- -Tricyclic antidepressants - Antidepressive agents,Tricyclic (D000929)Non-selective monoaminereuptake inhibitors (N06AA)-Protease inhibitors Protease inhibitors (J05AE) Protease inhibitors (D011480) - HIV Protease inhibitors(D017320)Narcotic analgesics - Narcotics (D009294) OPIOIDS (N02A) -Selective serotoninreuptake inhibitors (SSRIs)Selective serotoninreuptake inhibitors(N06AB)Serotonin uptake inhibitors(D017367)Selective serotonin reuptakeinhibitors (N06AB)Serotonin uptake inhibitors(D017367)MAO inhibitors MAO inhibitors (C02KC) Monoamine oxidaseinhibitors (D008996)Monoamine oxidase inhibitors,non-selective (N06AF)Benzylamines (D001596)Macrolides Macrolides (J01FA) Macrolides (D018942) Macrolides (J01FA) Macrolides (D018942)Azoles - Azoles (D001393) Imidazole and triazolederivatives (D01AC)-Amphetamine derivatives - Amphetamines (D000662) - -Ergot alkaloids and derivatives Ergot alkaloids(C04AE, G02AB, N02CA)Ergot Alkaloids (D004876) - Ergotamines (D004879)QT prolonging agents - - - -Lexical mapping to ATC and MeSH (columns 2-3) for 13 clinically relevant drug classes, along with their corresponding class in the other source obtained throughinstance-based mapping (columns 4-5). Italicized classes denote best corresponding pairs of classes.Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 11 of 14http://www.jbiomedsem.com/content/5/1/30equivalence suggested through lexical mapping is alsosupported by a large proportion of shared drugs be-tween these two classes. For example, the original classProton pump inhibitors is mapped lexically to Protonpump inhibitors in ATC and to Proton Pump Inhibitorsin MeSH. The best corresponding class in MeSH for theATC class Proton pump inhibitors, however, is not ProtonPump Inhibitors, but rather 2-Pyridinylmethylsulfinyl-benzimidazolesf. Moreover, in many cases, the originalclass can only be mapped lexically to either MeSH orATC. In these cases, the instance-based mapping offersa solution for finding which class of the other sourcehas the best correspondence. For example, the originalclass Tricyclic antidepressants can only be mappedlexically to the class Antidepressive Agents, Tricyclic inMeSH. However, the instance-based mapping identifiesthe ATC class Non-selective monoamine reuptake inhibitorsas a potential equivalence.While exploring mappings for these 13 clinically signifi-cant drug classes, we actually found no cases where the bestcorresponding classes in MeSH and ATC had exactly thesame members. Here are some reasons why. As mentioned earlier, the classificatory principlesused by ATC and MeSH are different. For example,Azoles represents a broad structural class in MeSH,whereas ATC splits azole drugs into several classesbased on their therapeutic use (e.g., antibacterialsand antimycotics). Some drugs appear to be missing from ATC,because of differences in the scopes of MeSH andATC. Such drugs include dietary supplements(e.g., red yeast rice), veterinary drugs (e.g., manymacrolides exclusively marketed for veterinary use),drugs of abuse (e.g., heroin) and drugs that onlyexist in combinations (e.g., lopinavir and ritonavir,but not lopinavir alone). Even though they are present in MeSH, some drugsappear to be missing from MeSH classes, because ofmissing relations to a drug class. For example, theclass assigned to tipranavir is Anti-HIV Agents,while most of the drugs from the same ATC classare (more appropriately) in the MeSH class HIVProtease Inhibitors. In many cases, the name of an ATC class isunderspecified, i.e., derives part of its meaning fromits position in the hierarchy. As a consequence, thelexical mapping of such class names is likely to pointto a broader class in MeSH. For example, the ATCclass Protease inhibitors is under the class AntiviralsTable 8 Best corresponding classes in ATC and MeSH for 13 clinically relevant drug classesDDI class ATC class MeSH class DrugscommonDrugs onlyin ATCDrugs onlyin MeSHES. IS Rel.Triptans Selective serotonin(5HT1) agonistsTryptamines 7 0 1 0.82 ?1 EqProton pump inhibitors Proton pump inhibitors 2-Pyridinylmethylsulfinyl-benzimidazoles5 1 0 0.76 1 EqHMG CoA reductase inhibitors HMG CoA reductaseinhibitorsHydroxymethylglutaryl-CoA Reductase Inhibitors8 0 2 0.76 ?1 EqTricyclic antidepressants Non-selective monoaminereuptake inhibitorsAntidepressive agents,Tricyclic10 2 2 0.69 0 EqProtease inhibitors Protease inhibitors HIV Protease inhibitors 8 3 1 0.63 0.44 EqNarcotic analgesics OPIOIDS Narcotics 15 3 11 0.50 ?0.48 EqSelective serotoninreuptake inhibitors (SSRIs)Selective serotoninreuptake inhibitorsSerotonin uptakeinhibitors6 0 8 0.40 ?1 InMAO inhibitors Monoamine oxidaseinhibitors, non-selectiveMonoamine oxidaseinhibitors3 0 5 0.32 ?1 InMacrolides Macrolides Macrolides 8 0 21 0.26 ?1 InAzoles Imidazole andtriazole derivativesAzoles 11 1 147 0.07 ?0.90 InAmphetamine derivatives - - -Ergot alkaloids and derivatives - - -QT prolonging agents - - -Best corresponding classes in ATC and MeSH for 13 clinically relevant drug classes, with the equivalence (ES) and inclusion (IS) scores from our frameworksmetrics, and the relation, equivalence or inclusion, between the two classes (Rel).Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 12 of 14http://www.jbiomedsem.com/content/5/1/30for systemic use, which means that it represents notall protease inhibitors, but only those that are usedto treat viral infections (which, in practice, meansHIV infections.)g In contrast, the MeSH classProtease Inhibitors truly represent all drugs, whosemechanism of action is to block some proteaseenzyme. Therefore, despite the similarity of theirnames, the ATC class Protease inhibitors is actuallyincluded in the MeSH class with the same name,and the best equivalence in MeSH for the ATC classProtease inhibitors is actually the class HIV ProteaseInhibitors. Differences in granularity between MeSH and ATCclasses are also responsible for some of thediscrepancies observed in the mapping between thetwo sources. For example, the MeSH classMonoamine Oxidase Inhibitors is not found in ATC,which provides three more specific classes instead(Monoamine oxidase inhibitors, non-selective,Monoamine oxidase A inhibitors, Monoamineoxidase B inhibitors).Application of the framework to the integration of theMeSH and ATC classificationsThe equivalence and inclusion relations obtained throughour framework can be combined in order to integrate thehierarchical structures of two drug classifications, such asMeSH and ATC. These additional relations create bridgesacross the original classifications, yielding an emerginghierarchy that combines both of them. As an illustration,we integrated the classes related to alkylating agents inMeSH and ATC. As depicted in Figure 3, all 4th-levelclasses under Alkylating Agents (L01A) in ATC have in-clusion mappings to Antineoplastic Agents, Alkylatingand Alkylating Agents in MeSH. The 3rd-level ATC classAlkylating Agents (L01A) itself is found to be equivalent tothese two classes in MeSH and is included in their parentclasses, Antineoplastic Agents and Toxic Actions, respect-ively. The 2nd-level ATC class Antineoplastic Agents (L01)can be regarded as equivalent to one of these parents,namely Antineoplastic Agents, although the equivalencescore ES is slightly under the threshold of 0.5. Such arepresentation helps users make sense of the similaritiesand differences in the organizational structure of theclassifications.Limitations and future workThe purpose of this framework is to provide a set ofmethods for assessing the consistency of drug classesacross sources. While we believe our framework willfacilitate the curation of an alignment of drug classesbetween two sources, it is beyond the scope of thiswork to provide such a reference alignment. Moreover,different reference alignments will most likely beAlkylang AgentsAnneoplasc Agents, AlkylangNoxaeOther alkylang agents L01AXNitrogen mustard analogues L01AAAlkyl sulfonates L01ABNitrosoureas L01ADEthylene imines L01ACALKYLATING AGENTS L01AANTINEOPLASTIC AGENTS L01 Anneoplasc Agents Toxic AconsNitrogen Mustard Compounds D009588Butylene Glycols D002072 / Mesylates D008698Aziridines D001388 / Triethylenephosphoramide D013721Nitrosourea Compounds D009607Funconal(PA)Structural(MH)has_PA*InclusionEquivalenceHierarchical  relaonasserted by terminology* Below the thresholdFigure 3 Integration of MeSH and ATC through the equivalence and inclusion relations obtained through our framework.Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 13 of 14http://www.jbiomedsem.com/content/5/1/30required for different use cases, as different applica-tions require different degrees of confidence.As part of this framework, we have developed equiva-lence and inclusion scores, for which we have deter-mined thresholds heuristically. We have not, however,fully investigated the impact of increasing or loweringthese thresholds on the quality of the alignment. We planto do so in future work.Another limitation is that we have only applied ourframework to one pair of drug classifications, MeSH andATC. However, our framework is amenable to aligningany pairs of classifications for which instance-leveldata are available. We plan to revisit our earlier workon NDF-RT and SNOMED CT classes to demonstratethe generalizability of our approach.As mentioned earlier, the instance-based alignment canbe applied only to those classes for which both MeSH andATC have drug members. This has been shown to be alimitation. On the other hand, the lexical alignmentcan still be used on these classes.The UMLS Methesaurus relies for a large part on lexicalsimilarity for determining synonymy among terms. Withthe recent inclusion of ATC in the UMLS Metathesaurus(in version 2013AB of the UMLS), it would no longer benecessary for us to perform the lexical alignment of ATCclasses to MeSH classes, since we could simply derive itfrom the UMLS, where synonymous terms from varioussources are given the same UMLS concept unique identi-fier. However, as discussed earlier, the lexical similarity ofclass names does not always reflect equivalence and ourinstance-based mapping remains an important alternativemethod for comparing classes.ConclusionsTo our knowledge, our work is the first attempt to aligndrug classes with sophisticated instance-based techniques,while also distinguishing between equivalence and inclusionrelations. Additionally, it is the first application of aligningdrug classes in ATC and MeSH. Moreover, this is the firstsystematic investigation of the consistency between lexicaland instance-based alignment techniques for these twodrug resources. We believe that the proposed frameworkwill effectively support the curation of a mapping betweenATC and MeSH drug classes by providing a detailed ac-count of the interrelations between the two resources.EndnotesaATC was integrated for the first time in version 2013ABof the UMLS released after this study was completed.Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 14 of 14http://www.jbiomedsem.com/content/5/1/30bNone of these drugs are currently available on theU.S. market.cIf the SCR is mapped to a drug, rather than a structuralclass descriptor, we associate it with the structural class ofthis drug descriptor instead.dWhen ATC was integrated into the UMLS Metathe-saurus, new terms were created for ambiguous classessuch as Fluoroquinolones, which appears at several loca-tions in the ATC hierarchy with slightly different mean-ings (e.g., Fluoroquinolone antiinfectives, ophthalmologicfor S01AE and Fluoroquinolone antibacterials, systemicfor J01MA).eThe pharmacological action Antinematodal Agents foroxantel was not present in MeSH 2013, but was addedto MeSH in the 2014 edition.fUpon investigation, it appears that some proton pumpinhibitor drugs, such as esomeprazole, were missing alink to the class Proton Pump Inhibitors in the 2013 ver-sion of MeSH. This was corrected in the 2014 version.gWhen ATC was integrated into the UMLS Metathe-saurus, the new term Protease inhibitors, direct actingantivirals was created for the underspecified class Proteaseinhibitors (J05AE).Competing interestsThe authors declare that they have no competing interests.Authors contributionsRW and OB conceived the project and contributed equally to performingthe acquisition, analysis, and interpretation of data and to the writing of themanuscript. Both authors read and approved the final manuscript.AcknowledgementsThis work was supported by the Intramural Research Program of the NIH,National Library of Medicine (NLM). This work was also supported by theOffice of Translational Sciences, Center for Drug Evaluation and Research atthe Food and Drug Administration (FDA) through an interagency agreementwith NLM (XLM12011 001). The authors want to thank Fred Sorbello, AnaSzarfman, Rave Harpaz and Anna Ripple for useful discussions.Received: 3 December 2013 Accepted: 4 February 2014Published: 9 July 2014RESEARCH Open AccessThe pathway ontology  updates andapplicationsVictoria Petri1*, Pushkala Jayaraman1, Marek Tutaj1, G Thomas Hayman1, Jennifer R Smith1, Jeff De Pons1,Stanley JF Laulederkind1, Timothy F Lowry1, Rajni Nigam1, Shur-Jen Wang1, Mary Shimoyama1,4,Melinda R Dwinell1,2, Diane H Munzenmaier1,2, Elizabeth A Worthey1,3 and Howard J Jacob1,2,3AbstractBackground: The Pathway Ontology (PW) developed at the Rat Genome Database (RGD), covers all types ofbiological pathways, including altered and disease pathways and captures the relationships between them withinthe hierarchical structure of a directed acyclic graph. The ontology allows for the standardized annotation of rat,and of human and mouse genes to pathway terms. It also constitutes a vehicle for easy navigation between geneand ontology report pages, between reports and interactive pathway diagrams, between pathways directlyconnected within a diagram and between those that are globally related in pathway suites and suite networks.Surveys of the literature and the development of the Pathway and Disease Portals are important sources for theongoing development of the ontology. User requests and mapping of pathways in other databases to terms in theontology further contribute to increasing its content. Recently built automated pipelines use the mapped terms tomake available the annotations generated by other groups.Results: The two released pipelines  the Pathway Interaction Database (PID) Annotation Import Pipeline and theKyoto Encyclopedia of Genes and Genomes (KEGG) Annotation Import Pipeline, make available over 7,400 and31,000 pathway gene annotations, respectively. Building the PID pipeline lead to the addition of new terms withinthe signaling node, also augmented by the release of the RGD Immune and Inflammatory Disease Portal at thattime. Building the KEGG pipeline lead to a substantial increase in the number of disease pathway terms, such asthose within the infectious disease pathway parent term category. The drug pathway node has also seenincreases in the number of terms as well as a restructuring of the node. Literature surveys, disease portaldeployments and user requests have contributed and continue to contribute additional new terms across theontology. Since first presented, the content of PW has increased by over 75%.Conclusions: Ongoing development of the Pathway Ontology and the implementation of pipelines promote anenriched provision of pathway data. The ontology is freely available for download and use from the RGD ftp site atftp://rgd.mcw.edu/pub/ontology/pathway/ or from the National Center for Biomedical Ontology (NCBO) BioPortalwebsite at http://bioportal.bioontology.org/ontologies/PW.Keywords: Biological pathway, Ontology, Pipeline, Pathway annotations, Pathway diagrams* Correspondence: vpetri@mcw.edu1Human and Molecular Genetics Center, Medical College of Wisconsin,Milwaukee, WI, USAFull list of author information is available at the end of the articleJOURNAL OFBIOMEDICAL SEMANTICS© 2014 Petri et al.; licensee BioMed Central Ltd. This is an open access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly cited.Petri et al. Journal of Biomedical Semantics 2014, 5:7http://www.jbiomedsem.com/content/5/1/7BackgroundIntroductionThe Pathway Ontology (PW) originated and is being de-veloped at the Rat Genome Database (RGD) [1]. Its goalis to cover any type of biological pathway, including al-tered and disease pathways, and to capture the relation-ships between them within the hierarchical structure ofa controlled vocabulary or ontology. The building of bio-logical ontologies as directed acyclic graphs (DAG) andthe use of structured or controlled vocabularies was firstadvanced and implemented by the Gene Ontology (GO)project [2,3]. Many bio-ontologies have been developedsince [4], as witnessed by the ever-growing number sub-mitted to and made available at the National Center forBiomedical Ontology (NCBO) BioPortal [5,6]. Severalontologies, including the Pathway Ontology, are beingdeveloped at RGD ([7], in the Biomedical Ontologiesthematic series of the Journal of Biomedical Semantics).Within the structure of a DAG, terms have defined rela-tionships to one another and a particular term can havemore than one parent. This means that there can bemore than one path in the ontology tree from a broader,more general parent term to a more specialized childterm. Within the tree structure, terms are nodes whosenames designate the class(es) they represent and whichare connected by edges that represent the relationship(s)between them. In PW, a node is the network/pathwayclass it stands for, and its features and aspects are cap-tured in the definition. A pathway is a set of inter-connected reactions and interactions whose delineationand scope are used as a model for exploring and study-ing, describing and understanding the working of andrelationships between biomolecules within a context.The categories or types of pathways are conceptualizedand referenced in the scientific literature and repre-sented in pathway databases such as the KyotoEncyclopedia of Genes and Genomes (KEGG), thePharmacogenomics Knowledge Base (PharmGKB), theSmall Molecule Pathway Database (SMPDB) and Wiki-Pathways, among others [8-11].The pathway ontology structureThe first of the main five nodes of the ontology, the meta-bolic node, contains networks/pathways that stand for/rep-resent the set of reactions underlying the transformation ofcompounds. The set of reactions/interactions underlyingthe coordinated responses that maintain the cellular/tissueand/or organ/organismal status quo and homeostasis areplaced under the regulatory node. The set of reactions/in-teractions initiated or triggered by a binding/molecularinteraction/conformational change event are found underthe signaling node. The set or sets of interactions whereone or more are deviant and represent the systems perturb-ation(s) fall under the disease node. Finally, the set or setsof reactions/interactions representing the systems responseto and handling of treatment(s) geared towards dealing withthose perturbation(s) are housed in the drug node. Thus,the main nodes of the Pathway Ontology are: metabolic,regulatory, signaling, disease and drug pathway (Figure 1A).Two types of relationships are being used in the ontology:is_a and part_of. For instance, insulin and glucagon arepeptide hormones whose signaling - insulin signaling path-way and glucagon signaling pathway, are children terms inan is-a relationship to the parent term peptide and proteinhormone signaling pathway. The two signaling pathwayswhich are initiated in response to high levels of circulatingglucose  insulin signaling pathway, or low  glucagon sig-naling pathway, and whose engagement of intracellular cas-cades aims at restoring the normal physiological levels ofglucose, are also in a part-of relationship to the glucosehomeostasis pathway term, along with other pertinentterms. Insulin also plays important roles in energy homeo-stasis. In the brain, insulin (and leptin) act to increase theexpression of appetite-decreasing Pomc while decreasingthe expression of appetite-stimulating Agrp genes. Thepeptide and protein hormone signaling pathway term is inturn a child of the more general term hormone signalingpathway, as other classes of compounds with very differentphysico-chemical properties can also act as hormones. Forinstance, the steroid hormones and the eicosanoids which,as the names suggest, are hormones, are lipid molecules.The signaling pathways they initiate are children of thelipid hormone signaling pathway term which in turn, is asibling of peptide and protein hormone signaling pathwayand child of hormone signaling pathway terms (Figure 1B).The nodes are not disjoint and a given pathway class canbe the child of terms residing in different nodes, as the ex-amples of insulin and glucagon signaling above show. Thepeptide and protein hormone signaling pathway and theglucose homeostasis pathway are both parents of the sig-naling pathways of insulin and glucagon, albeit with differ-ent relationships to their children; the two parent terms arewithin the signaling and regulatory nodes, respectively. Theenergy homeostasis pathway term is also a parent of insu-lin signaling and like glucose homeostasis, it is within theregulatory node (Figure 1C).The pathway and the process concepts, although attimes interchangeably used, are distinct. A pathway conveysthe idea of a set of interacting molecules, of the reactionsand interactions underlying its functioning. A process onthe other hand, conveys the idea of the end result, the con-clusion of a plan of action, whether the consequence of thecombined work that the set of reactions and interactionsproduces, in the case of a simpler one, or in the case of amore complex one, the combined work of pathways thatcontribute to or in some fashion modulate the end result.At the same time, a given pathway can participate in and/or regulate several processes [12]. In the Biological ProcessPetri et al. Journal of Biomedical Semantics 2014, 5:7 Page 2 of 12http://www.jbiomedsem.com/content/5/1/7(BP) ontology of GO there are metabolic and other processterms that map to KEGG pathways and to terms in PW.For instance, the formation of a fatty acid molecule is thefatty acid biosynthetic process term in GO; it is the fattyacid biosynthetic pathway term and the fatty acid biosyn-thesis entry in PW and at KEGG, respectively. While thephrasing is similar in GO, PW and KEGG, the term repre-sents a process in GO, a pathway in PW and the KEGGdatabase. KEGG is a primary source for metabolic pathwaysand projects such as databases and ontologies that in somefashion represent metabolism are going to exhibit a sharing,or an overlapping of terms/entries naming, but not an over-lapping of concepts and/or contexts. Likewise, there are sig-naling pathway terms in BP that relate to similar terms inthe signaling pathway node of PW and map to entries inpathway databases such as KEGG and others. However, thepositions of and relationships between such terms are dif-ferent, as are the perspectives of the two ontologies.Disease and altered pathwaysThe provision of terms for the altered versions of path-ways and the representation of disease pathways and dia-grams as collections of altered pathways are unique toPW and its use at RGD. An altered pathway is onewhere defects in one or several components of the path-way affect its normal functioning with potential implica-tions for a diseased phenotype. The severity of analtered pathway or the convergence of several alteredpathways can overcome the ability of the system to ad-just and is manifested in the diseased state. ViewingACBFigure 1 The pathway ontology main nodes and positions of selected terms. A. The five nodes of the Pathway Ontology. B. The term lipidhormone signaling pathway in the ontology showing the parent, siblings and children terms. C. The term insulin signaling pathway in theontology showing the position of the term within the tree. Insulin signaling pathway is in a part_of relationship to the glucose and energyhomeostasis pathway terms within the regulatory node and in an is_a relationship to peptide and protein hormone signaling pathway termwithin the signaling node.Petri et al. Journal of Biomedical Semantics 2014, 5:7 Page 3 of 12http://www.jbiomedsem.com/content/5/1/7diseases from a network- rather than a gene-centric per-spective, from the systems level of pathway cross-talkand alterations within, is an approach increasingly beingconsidered [13-15].As an example, a large-scale study carried out on anumber of pancreatic tumors identified several sets ofgenes that were altered in the majority of tumors. Ofthese, many were associated with core signaling path-ways and altered in 67% to 100% of tumors [16]. Perhapsnot surprisingly, these are pathways important forgrowth and proliferation and in some cases, also knownto be oncogenic (Figure 2). What may be intriguing isthe relatively large number of altered pathways and oneis tempted to wonder/speculate whether it is this num-ber and the combinations that result from it, that over-come the ability of the system to adjust and/or recoverand render the condition intractable. The pancreaticcancer pathway diagram presents the main pathways al-tered in the condition with the culprit genes showncolor coded. Additional links to a list of miRNAs(microRNAs) aberrantly expressed in pancreatic tumorsand to the Cancer Portal at RGD are provided (seeFigure 2).Pathway annotations, interactive pathway diagrams,pathway suites and suite networksThe use of the ontology allows for the standardized an-notation of rat, human and mouse genes to pathwayterms. Generally, annotations are made for the term ra-ther than on a gene-by-gene basis; thus, what is beingtargeted for annotation is the pathway itself  like theontology the overall pathway curation process isnetwork-centered [12,17]. Importantly, the ontology pro-vides the navigational means to access pathway annota-tions, interactive pathway diagrams, pathway suites andsuite networks as well as a variety of tools, from manyentry points. A pathway suite is a collection of pathwaysthat revolves around a common concept or is globallyFigure 2 Pancreatic cancer pathway diagram. The interactive pathway diagram page for the pancreatic cancer pathway. The alteredpathways associated with the condition are shown as gray rectangles that link to the ontology report(s) for the those terms. Culprit genes withinthe pathways are shown color-coded (default is red). The icon for the microRNAs (miRNA) with potential roles in pancreatic cancer links to a pagewhere several down- and up-regulated miRNAs are shown with some targets listed and with links to their report pages in RGD and the microRNAdatabase (MiRBase). The icon for the condition links to the Cancer Disease Portal in RGD.Petri et al. Journal of Biomedical Semantics 2014, 5:7 Page 4 of 12http://www.jbiomedsem.com/content/5/1/7related. If two (or more) pathway suites relate in somefashion, they constitute a suite network. For instance,the Glucose Homeostasis Pathway Suite Networkbrings together the suite dedicated to the various meta-bolic pathways involving glucose and the one dedicatedto the contributing signaling and regulatory pathways.Together, the pathway ontology, the pathway annota-tions and the graphical representations of pathways,constitute the elements of the Pathway Portal [12,17,18],an important project at the Rat Genome Database[19,20]. Pathway, along with disease, phenotype and bio-logical process, are the major concepts around whichthe Disease Portals are built and are entry points toaccess the data they contain. The Disease and PathwayPortals can be accessed from the main homepage ofRGD (Figure 3A). The Pathways entry point leads tothe Molecular Pathways link which houses the collectionof interactive pathway diagrams and suites that RGDpublishes. This entry point also provides access to path-way related publications by members of RGD as well asother information and data links (Figure 3B).An ontology search, accessed through the Functionentry point (see Figure 3A), brings up all the ontologiesthat have terms which contain the keyword(s) used. Se-lection of an ontology will show the terms containingthe keyword(s) with the option to search the tree or viewABFigure 3 Pathway portal data access. A. Rat Genome Database homepage with the main entry points to its content; the Pathways andFunction entry points described in the text, are circled. B. Accessing the Pathways entry point and entries within.Petri et al. Journal of Biomedical Semantics 2014, 5:7 Page 5 of 12http://www.jbiomedsem.com/content/5/1/7the annotations. Selecting the branch icon to the left ofa term brings up a browser result showing the parent,siblings and children of the term. The browser has beendeveloped at RGD and recently updated to indicatewhether interactive pathway diagrams are available ornot for terms and/or their children in the form of aboxed D of darker or paler green color, respectively(see Figure 1A-B). Any dark green D box links to thatinteractive diagram page. In addition, if the searchedterm has a diagram, a small icon will be shown in theterm entry, to the right of the term description; it willalso link to the diagram page. [The boxed A inFigure 1A-B denotes the presence of annotations].Selecting a term brings up an ontology report page withthe GViewer tool  a genome-wide view of rat chromo-somes with genes annotated to the term, a tabular list ofgenes annotated to the term by species with links to re-spective gene report pages and a diagram showing thepaths to the root term in the ontology tree. If there is aninteractive pathway diagram for the chosen term, anicon is present at the top of the page to the right of thediagram and it links to the pathway diagram page.Every diagram page consists of several sections. Thefirst provides an in-depth, expandable description of thepathway and the diagram itself whose objects link totheir report pages in RGD (genes, chemicals, pathways)or other websites. Beneath that is a tabular list of anno-tated genes by species with each entry linking to its re-port page and other links. As applicable, the alteredversion of the pathway and additional elements in thediagram can also be found in this section. The next sec-tion contains tabular lists of genes in the pathway thathave been annotated to disease, other pathway andphenotype terms with links to corresponding reportpages. The user has the option of toggling betweenterms and genes and can follow links to ontology reportpages for terms and to gene report pages for genes.PROCEEDINGS Open AccessEvolving BioAssay Ontology (BAO):modularization, integration and applicationsSaminda Abeyruwan1, Uma D Vempati2, Hande Küçük-McGinty1, Ubbo Visser1, Amar Koleti2, Ahsan Mir2,Kunie Sakurai3, Caty Chung2, Joshua A Bittker5, Paul A Clemons5, Steve Brudz5, Anosha Siripala6, Arturo J Morales6,Martin Romacker6, David Twomey6, Svetlana Bureeva7, Vance Lemmon2,3, Stephan C Schürer2,4*From Bio-Ontologies Special Interest Group 2013Berlin, Germany. 20 July 2013* Correspondence: sschurer@med.miami.edu2Center for Computational Science,University of Miami, 1320 S. DixieHighway, Gables One Tower, 33146Coral Gables, FL, USAAbstractThe lack of established standards to describe and annotate biological assays andscreening outcomes in the domain of drug and chemical probe discovery is a severelimitation to utilize public and proprietary drug screening data to their maximumpotential. We have created the BioAssay Ontology (BAO) project (http://bioassayontology.org) to develop common reference metadata terms and definitionsrequired for describing relevant information of low-and high-throughput drug andprobe screening assays and results. The main objectives of BAO are to enableeffective integration, aggregation, retrieval, and analyses of drug screening data.Since we first released BAO on the BioPortal in 2010 we have considerably expandedand enhanced BAO and we have applied the ontology in several internal andexternal collaborative projects, for example the BioAssay Research Database (BARD).We describe the evolution of BAO with a design that enables modeling complexassays including profile and panel assays such as those in the Library of IntegratedNetwork-based Cellular Signatures (LINCS). One of the critical questions in evolvingBAO is the following: how can we provide a way to efficiently reuse and shareamong various research projects specific parts of our ontologies without violatingthe integrity of the ontology and without creating redundancies. This paper providesa comprehensive answer to this question with a description of a methodology forontology modularization using a layered architecture. Our modularization approachdefines several distinct BAO components and separates internal from externalmodules and domain-level from structural components. This approach facilitates thegeneration/extraction of derived ontologies (or perspectives) that can suit particularuse cases or software applications. We describe the evolution of BAO related to itsformal structures, engineering approaches, and content to enable modeling ofcomplex assays and integration with other ontologies and datasets.Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5 JOURNAL OFBIOMEDICAL SEMANTICS© 2014 Abeyruwan et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.BackgroundIntroduction to BAO and the domainThe development of novel small molecule therapeutics (drugs) typically begins with theidentification of suitable compounds with desirable biological activity in simple modelsystems such as a purified protein that is a validated disease target or a cell related toa disease or disease state. Target-based and cell-based phenotypic high-throughputscreening (HTS) are among the most important approaches to identify new hits andleads from large compound libraries [1,2]. Innovations in assay design and technologi-cal advances in detection and throughput have dramatically increased the size anddiversity of HTS datasets generated in pharmaceutical companies and in publicresearch projects. Examples of NIH-funded large-scale screening programs in whichwe have been participating include the Molecular Libraries Program (MLP) [3] and theLibrary of Integrated Network-based Cellular Signatures (LINCS) program [4]. In theMLP, a large library (up to 430, 000 compounds) has been screened in over 600 probeprojects to develop novel tool and drug compounds. This data is deposited in Pub-Chem [5] and is also being curated and made available for structured analysis in theBioAssay Research Database (BARD) [6]. The LINCS project, in contrast to traditionalscreening, generates extensive signatures of cellular responses consisting of thousandsof results for any perturbation (such as small molecule drugs) to enable the develop-ment of better system-level disease models. Examples of LINCS screening results andassays include Landmark gene expression signatures (L1000), Kinome-wide bindingaffinities (KINOMEscan), phenotypic profiling across 1,000 cell lines, and many others,covering omics and HTS data. LINCS results are currently available via participatingcenters and can be queried and explored via the LINCS Information FramEwork(LIFE) developed by our group [7]. Several other publicly accessible resources ofscreening data exist, for example ChEMBL, a database that contains structure-activityrelationship (SAR) data curated from the medicinal chemistry literature [8], the Psy-choactive Drug Screening Program (PDSP), which generates data from screening novelpsychoactive compounds for pharmacological activity [9], or Collaborative Drug Dis-covery (CDD), a private company enabling drug discovery research collaborations [10].Despite being publicly available, current data repositories suffer from structural, syntac-tic, and semantic inconsistencies, complicating data integration, interpretation and analy-sis. As one of the largest and first repositories of public drug screening data, PubChem,has been essential to illustrate the need for clear metadata standards to describe drug andchemical probe discovery assays and screening results [11]. To address these prevailingissues; we have previously developed the first version of the BioAssay Ontology (BAO)[12]. This first version was developed iteratively based on domain expertise and availableassay data, primarily from the MLP, which we annotated using evolving versions of BAO.Since the first release of BAO, we have engaged with several more groups in publicresearch projects and in pharmaceutical companies and the biomedical ontology commu-nity. We aligned the organization of BAO with existing efforts as much as possible, mostimportantly at the Novartis Institutes of BioMedical Research, and we have significantlyextended the terminology and axioms in BAO to cover a broader range of assays andrelated concepts. One of our objectives in redesigning BAO was to introduce an upper-level ontology to facilitate alignment and integration with other biomedical domain ontol-ogies and to provide a more formal ontology development framework. However, a criticalAbeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 2 of 22requirement was to maintain a native organization of BAO that is meaningful to endusers and which enables straight-forward incorporation into software systems, such as ourpreviously developed BAOSearch application [13]. This led us to a formal, structural, andfunctional modularization of BAO, which we describe here. We also provide a generalsolution to defining profile and panel-type assays in which many results are generated inparallel, such as those in the LINCS project. Meanwhile, BAO has been applied in severalnew projects, most importantly BARD, which also contributed to extending and improv-ing BAO further.Semantic Web technologies have become increasingly popular to integrate biome-dical research information; a prominent example is the Bio2RDF project [14]. Inaddition to open-world integration of diverse omics and high-throughput drugscreening data, Semantic Web technologies provide capabilities for inference reason-ing with many potential benefits over traditional systems [15]. Only very recentlyhowever, have large public drug screening datasets been made available as ResourceDescription Framework (RDF) format. One such resource is ChEMBL, whose RDFmodel leverages BAO to describe the results [16]. A large initiative to develop anintegrative solution to diverse drug discovery data is the Open Pharmacological Con-cepts Triple Store (Open PHACTS) consortium [15]. Because of increasing adapta-tion of Semantic Web technologies in drug discovery data management, it wascritical to develop BAO as a formal Description Logic (DL) ontology implemented inWeb Ontology Language (OWL). We show modeling examples illustrating BAOsemantic inference capabilities to identify mechanistically related assays in absence ofsuch explicit annotation.Description logicDescription logic (DL) contains a set of decidable constructs from the first-order predi-cate logic, and it is the corner stone for the development of OWL DL ontologies inknowledge representation [17]. The computational complexity of a given DL dependson the constructs that are being used, and they are traditionally represented with dif-ferent complexity classes. Attribute Language with Complement (ALC) provides thepreliminary DL constructs with classes, roles, and individuals. The formal syntax ofALC is defined as follows (as a convention, we indicate conceptualization by capitalletters (e.g., C, D) or sans serif letters (e.g., Thing, bioassay), sets by bold face letters(e.g., C, I), and functions by lower case letters (e.g., fC,fI)). Let A be a named atomicclass, and, without loss of generality, let R be an abstract role. The class expressions(concepts or concept expressions) C, D are recursively constructed by: C, D ¬ A | ? |? | ?C | C ? D | C ? D | ?R.C | ?R.C, where, ? is the top concept, ? is the bottomconcept, the symbols for conjunction, disjunction, and negation are given by ?, ?, and? respectively, and ? and ? represent the universal and existential quantifier. ALCDL knowledge bases consist of two groups: (1) TBox provides statements about theterminological knowledge; and (2) ABox provides the statements about the assertionalknowledge about individuals. These statements are also known as axioms in descrip-tion logic. For class expressions C and D, the TBox statements are of the form C ? Dor C ? D, where ? denotes the equivalences among classes and ? constructs the sub-sumption or general class inclusion (GCI) axioms. On the other hand ABox consists ofaxioms of the form C(a) and R(a, b), where R is a role, and, a, b are individuals.Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 3 of 22ALC DL has been extended toSROIQ(D) DL with the following syntactic con-structs: {a} | ?R.Self | ? nR.C | ? nS.C, where, {a} represents nominals, ?R.Self relatesan individual to itself, and n ? ?+ with ? nR.C and ? nS.C provide the qualified cardin-ality restrictions. SROIQ(D) DL introduces an RBox with general role inclusionaxioms of the form R1 . . . R2 ? R, which provides the meaning that concatenation ofR1, . . . , R2 is a subrole of R. In addition, there exists constructs to represent transitive,symmetric, asymmetric, reflexive, irreflexive, functional, inverse functional, and disjointroles and concepts. It is to be noted that roles can either be abstract or concrete.The interpretation of DL is given by the direct model-theoretic semantics. Theclasses, roles, and individuals are given symbols from mutually disjoint sets of C, R,and I respectively. There exists another set called the domain of interpretation, ?,which contains entities for resources, individuals, or single objects. Using the domainof interpretation, the individuals, classes, and roles are interpreted by functions fI : I??, fC : C ? 2?, and fR : ? 2?×? respectively. The complex classes and role expressionsare interpreted by an extended interpretation function, .I , such that the interpretationfaithfully capturers the structure of the knowledge base. If a model exists, then theknowledge base is satisfiable, and the implicit knowledge (logical consequence) isentailed though an inference procedure. DL logic uses efficient tableau algorithms toinfer subsumption, class equivalence, class disjointness, global consistency, class consis-tency, instance checking, and instance retrieval.DL provides an appropriate trade-off between expressivity and scalability in practice.The complexity of DL is dominated by the data complexity, which is NP-hard forSROIQ(D) DL ABox and N2ExpTime-complete for the combined TBox, RBox, andABox. Modern SROIQ(D) DL reasoners such as the (1) tableau-based FaCT++ [18]and Pellet [19] reasoners; and the (2) hyper-tableau HermiT [20] reasoner, use intelli-gent heuristics and optimization methods to perform inferencing as efficiently as possi-ble. The reader is referred to [21,22] for a comprehensive discussion on SROIQ(D)DL syntax, semantics, deduction procedures, and model construction.Results and discussionBAO 2.0 native organization and main componentsThe new BAO 2.0 formally describes perturbation bioassays in the domain of drug andprobe discovery, such as small molecule HTS assays and screening results for the purposeof categorizing the assays and outcomes by concepts that relate to the screening modelsystem (format), assay method, the biology interrogated in the assay (such as a protein tar-get or biological process), the detection method (how does the assay work), and types ofresults (endpoints). BAO 2.0 is organized into several major sections, which include multi-ple levels of subcategories of subsumption class hierarchies. A number of specific objectproperty relationships were created to connect the classes and develop a knowledgerepresentation.The main categories in BAO 2.0, titled components, include bioassay, assay biology,assay method, assay format, assay endpoint, assay screened entity (Figure 1). Each ofthese component classes includes the subsumption trees of terms corresponding to thecategory and additional trees of related terms to describe each of the main componentsproperly and formally. In BAO 2.0, we incorporated a slightly different pattern fromBAO 1.6, since we were interested in making BAO 2.0 compatible with the existingAbeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 4 of 22upper-level and other domain-level ontologies. The BAO 2.0 categories also lend BAOto its native structures that is most useful to users, for example to annotate assays orto implement a user interface in a software application. We describe briefly the mainclass hierarchies of BAO 2.0 corresponding to the above components (Figure 1): BAO assay bioassay component includes the bioassay subsumption tree, and sev-eral other classes to describe assays, including assay kit, bioassay type, and bioassayspecification, which contains terminology trees to describe various details about abioassay and its context. The class hierarchy bioassay includes the list of the bioas-says and their formal description, e.g., cell cycle assay, enzyme activity assay. Bioas-says are organized roughly by their application (what the assay is used for). Theclass hierarchy assay kit includes the reagents and their cocktails that are commer-cially available to perform the different chemical reactions that encompass an assay(i.e., out of the box, ready to run assays). The information in bioassay specificationis similar to BAO 1.6. BAO assay format component includes the assay format subsumption tree todescribe the biological model system; a conceptualization of assays based on thebiological and/or chemical features of the experimental system. BAO assay method component includes terminologies to describe how the assayis performed, most importantly assay method and physical detection method. Italso includes computational method, instrument, and relevant other material entityassay ingredients. The class hierarchy assay method includes assay design methodFigure 1 BAO 2.0 main classes with some relationships between them. Six main components (shadedclasses) are used to formally describe bioassays by terms related to bioassay, biology, screened entity, assaymethod, format, and endpoint. The most important classes and their relations as shown including bioassay,measure group, biological macromolecule, screened entity, assay method (specifically assay design methodand physical detection method), assay format, and endpoint. There exists complex interactions amongthese entities. OWL DL 2 (SROIQ(D)), the decidable subset of the first-order-predicate logic provides theinterpretations, models, and logical consequences.Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 5 of 22and assay supporting method; assay design method describes how a biological per-turbation of the model system is translated into a detectable signal. The class hier-archy computational method contains various methods that are based on theapplication of information technology to chemistry and biology. The physical detec-tion method hierarchy includes the method (technology) used to detect the signalthat corresponds to the perturbagen in the assay environment and enabled by theassay design method. Class instrument consists of instruments used for detection/readout from an assay and their components, e.g., FLIPR, ViewLux plate reader,PHERAstar, etc; software lists the types of software that are used in the variousinstruments, e.g., image analysis software, which is a component of the high con-tent screening (HCS) platforms. BAO assay biology component includes various class hierarchies to describe thebiology of the assay including biological process, biological macromolecule, cell linecell, cellular component, cell phenotype, anatomical entity, disease, function, organ-ism. Many of these are mapped to external sources (vide infra). To describe thebiology of a simple binding assay for example, a biological macromolecule proteinwould have the biological role target. Many other role classes exist (vide infra). Theclass function includes the physiological function of biological macromolecules, e.g.,protein binding, kinase activity. This module was imported from the Gene Ontol-ogy (GO). The class cellular phenotype encompasses both the molecular character-istics of a cell and the (morphological) shape and structure of a cell and its parts. BAO assay screened entity component includes screened entity, which is the che-mical or biological entity that is tested/screened in the assay. The screened entitytypically modulates the function of the (known or unknown) biological macromole-cule with the role of a target. The most important screened entity for BAO is theclass small molecule, that contains compounds that are tested in the process ofdeveloping chemical probes and drugs, which is the primary domain of BAO. BAO assay endpoint component includes subsumption trees to describe the assayresult or endpoint and other required information to quantitatively or qualitativelyexpress the biological perturbation measured in a bioassay, such as units of measure-ment (imported from UO), and other details to interpret the results in the context ofthe assay methodology and the biology, such as as the mode of action of the pertur-bagen that the endpoint characterizes, or the signal direction and endpoint actioncorrelation of the assay. More details about the class endpoint are described below. Additional classes that were not assigned to any one of the main BAO compo-nents are organization, people, role, and quality: Organization includes, for examplemanufactures of assay kits, instruments, etc., or screening center where assays areperformed. People include the individuals who are involved in performing scientificresearch, such as assay development, compound screening, chemical synthesis, etc.Role describes the action that an entity performs in a given context; an entity canhave more than one role, e.g., target, perturbagen. BAO 2.0 has imported rolesfrom the Chemical Entities of Biological Interest (ChEBI) ontology and we haveadded some missing classes. Quality lists the characteristics that inhere in an entityof biological origin, namely, organism, cell, and molecule or a physical entity, e.g.,intensity, optical quality. Most of the terms in this class were imported from thePhenotypic Quality Ontology (PATO); missing ones were added to BAO.Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 6 of 22 BAO properties include both the object and data types that are required to createrelationships among the different concepts in BAO 2.0. These properties were eitherimported from the Relationship Ontology (RO), where available or created in BAO 2.0.Upper level ontology structure and aligning external ontologiesSince, there are several advantages of using upper level ontologies (ULOs), BAO 2.0makes use of the Basic Formal Ontology (BFO) and OBO Relations Ontology (OBO-RO) as its upper level ontologies. We have used the current release of BFO ontology(http://purl.obolibrary.org/obo/bfo.owl), which is also tightly coupled with OBO-ROontology (http://purl.obolibrary.org/obo/ro.owl). Figure 2 shows the main categories ofBFO and examples of corresponding BAO 2.0 classes. BFO conceptualization abstractlyrepresents objects, entities, and relations in our domain of discourse, and it is substan-tially used in biomedical ontologies compared to other OWL version of ULOs such asSUMO (http://www.ontologyportal.org/SUMO.owl) or DOLCE (http://www.loa.istc.cnr.it/ontologies/DLP 397.owl). The advantage of using an ULO is that it allows integra-tion of existing domain ontologies, by grounding them on a formally rigid ontologicalframework [23,24]. We make available a development instance of the BAO BFO ver-sion. Figure 2 also illustrates external ontologies, components of which we currentlyuse in BAO (see Methods). Their alignment with BAO is facilitated in part by the BFOstructure [25,26]. One important mid level ontology is the Ontology for BiomedicalInvestigations (OBI) [27]. We have previously outlined the different focus of BAO vs.OBI [12]. However, this is not to say that they are incompatible; alignment is one ofthe future tasks required to evolve BAO further. We have created a version of BAOFigure 2 BAO 2.0 makes use of BFO as the upper-level ontology and incorporates several externalontology modules. BAO classes were mapped under appropriate BFO concepts. The BFO framework alsofacilitates alignment to external ontology modules. Blue boxes are examples of BAO classes categorized byBFO (black rectangles). External ontologies used in BAO are shown as red boxes and labels.Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 7 of 222.0 that contains BFO and OBO-RO as ULOs (bao_complete_bfo_dev, which is adevelopment version) and another one without them (bao_complete, released).Bao_complete_bfo_dev simplifies alignments to external ontologies and is targetedto the ontology development community while bao_complete is targeted to the drugand probe screening community and developers of software applications (such as ourBAOSearch application). We emphases the fact that the BAO-to-BFO alignment is basedon our knowledge and understanding of BFO and OBO-RO structures, and BAOmechanisms. The alignment is an ongoing process, and we have a community wide bugreporting system to uses to provide feedback to provide semantically better alignments.bao_complete_bfo_dev and bao_complete are targeted towards different usersgroups, the latter is more amenable to perform on large-scale analysis of the chemicalbiology data without the additional constraints imposed by BFO and OBO-RO.BAO 2.0 modular architecture and implementationThe modularization implementation is described in detail in Methods. Our modulari-zation approach is illustrated in Figure 3. The modularization framework uses a layeredarchitecture and uses the modeling primitives, vocabularies, modules and axioms.Vocabularies only contain terms (classes with subsumption only). Module layers enablecombining vocabularies in flexible ways to create desired ontology structures or sub-sets. Axioms are separate files that do not contain any classes or properties. Classesand relationships are imported (directly or indirectly) from module and/or vocabularyFigure 3 BAO 2.0 ontology modularization framework. The framework uses a layered architecture toabstract complexities from different sources. It provides modeling primitives of vocabularies, modules,axioms, and perspectives to develop heterogeneous ontologies.Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 8 of 22files. The above mentioned classes in BAO 2.0 were created as separate vocabularyfiles. They were then imported into the bao_core file. BAO core only containsaxioms incorporating BAO classes and BAO properties. In our modularizationapproach we separate external and internal sources. External modules (compare Figure2) are generated as described in Methods. Overlap among external and internal classesand properties (i.e., those required in BAO core) are resolved using combinator mod-ules, that is, external classes and properties are mapped (equivalence or subsumption)to corresponding BAO classes and properties. This approach assures that BAO coreremains stable and independent from external sources that may change. The completeBAO includes external axioms and imports BAO core (indirectly importing all voca-bularies and properties) and external modules (bao_complete file). Using thisapproach we also generated the BFO version of BAO. All internal and external vocabu-lary, module and axiom files are available via the BAO website (http://bioassayontol-ogy.org). Figure 4 shows the current implementation of the modularization illustratingvocabularies, intermediate modules, ontology axioms, BAO internal and externalsources and their mappings.Modeling assays and results using BAO 2.0In addition to the BAO modularized design and systematic construction, we also tried tomake the definitions of concepts in BAO consistent. We especially defined bioassays withtheir essential components such as assay design method, endpoints, measure groups, andmolecular participants. Figure 1 illustrates how assays are modeled by specifying informa-tion related to the biology (such as target and/or biological process), assay format, assaymethod (including assays design method and physical detection method, screened entityand endpoint (result) as described above. The BAO 2.0 architecture allows a more flexibledefinition of bioassays, for example the same biomolecule can participate in assays indifferent roles and functions. Important classes include: target: The target concept is defined by using the relationships has participantand has role. That is because targets are biological entities (i.e., participants) ofassays that are playing the role target. Assays may have single or multiple targetsdepending on the assay type. biological process: A large number of assays are designed to measure outcomes ofbiological processes. Thus, based on the assay in study, we have written axioms forthese information in the assay definitions. screened entity: This concept refers to a molecular entity with the role screenedentity role.Figure 4 BAO 2.0 ontology modularization framework implementation. BAO 2.0 modularizationframework provides effective software engineering methods to build complex ontologies. Shown are thecurrent vocabularies, modules, and axiom files also indicating internal vs. external sources.Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 9 of 22 participants: Every assay has at least one participant, usually more. While axiomizingthe assays, we try to define the particular roles that these participants play in the differ-ent assays. However, when we are not certain about the roles, we choose not to putaxioms in order to avoid false reasoning cases. assay design method: Every assay has an assay design as the underlying method togenerate a detectable signal and could correlate with the strength of the perturba-tion of the biological model system by the screened entity. physical detection method: An assay design method, generating a type of signal islinked to a corresponding detection method (the physical principle of detecting thesignal), which is typically performed by a detection instrument.The concepts listed above along with various other classes are used while modelingthe concepts bioassay, measure group, and endpoint.We had previously introduced the concept measure group to link multiple endpointsto the same bioassay [28]. We have now generalized this model so that measure groupcan be derived from one or more measure groups. This allows the formal and iterativeconstruction of more complex assays and endpoints that are derived from multiplemeasurements (Figure 5). The axiomatization was done in a way that infers measuregroup as a subclass of bioassay (compare Figure 1). The axiomatization was motivatedFigure 5 Graphical illustration of BAO 2.0 measure group class definition. The class measure group isused to group and link one or more sets of experimental results to one bioassay. By definition one assay canhave multiple measure groups. The measure group contains overlapping axioms with the bioassay, whichallows the reasoner to infer that the measure group is acting like an equivalent class of the bioassay; i.e., measure group is inferred as subclass of bioassay. Shown is an example of kinase concentration-responseprofiling panel assay, in which compounds are tested at m concentrations against n kinase targets.Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 10 of 22by pragmatic considerations for the workflows and perspectives for organizing andanalyzing the assay results, which is the core focus of BAO. It may be argued, thatoperationally it is not formally an assay; however that is not in conflict with the BAOperspective. It should be noted that BAO measure groups and results remain asso-ciated with their corresponding subclasses of bioassay, whose instances are procedu-rally, methodologically, and materially real. To understand better the relations betweenthe concepts measure group and endpoint we explore them in more depth: The class measure group is a concept to group and link one or more (different)sets of experimental results to one bioassay. A bioassay can have multiple measuregroups. A measure group contains overlapping axioms with the bioassay, whichallows the reasoner to infer that the measure group is acting like an equivalentclass of bioassay. This equivalence cannot simply be asserted. The measure group,in addition to holding the assay component metadata for each reported endpoint,also provides flexibility to generate different derived endpoints, e.g., IC50 (gener-ated from several response values at different concentrations, i.e., concentration-response), or profile endpoints (e.g., a kinase panel assay). This can be formallydone via derived measure groups, in cases where we have multiple measure groupthat vary in one parameter (such as concentration or kinase target). The class endpoint, alternatively called result, is a quantitive or qualitative represen-tation of a perturbation (change from a defined reference state of the model system)that is measured by the bioassay. An endpoint consists of a series of data points, onefor each perturbing agent employed by the assay. Every endpoint is obtained by usingat least one measure group. For each endpoint, there exists a unit and a value, whichis a number (e.g., float, which makes this concept a data property, and the concept isaxiomized using a data property as opposed to an object property). For example, for aconcentration endpoint (e.g., IC50), there exists a concentration unit and a concentra-tion value, which is a float number (data property, not functional). Assays could havesingle or multiple endpoints depending on the assay type.Endpoints are not used to handle the different measurements in the same assay. Thatis axiomized through the measure group concept. They may vary due to parameterssuch as time, concentration, target, and so on, or combinations. The formal definitionsallow us to create individuals for different endpoints that might be using the samemeasure groups, i.e., results are measured once and different methods are applied onthese measurements to find different derived endpoints. We can group different mea-sure groups to define intermediate results. We can create profile endpoints and wecan define profiles of intermediate aggregated measure groups (Figure 5). An endpointindividual is associated with a specific measure group and a specific compound combi-nation and has a specific value and unit.In BAO 2.0, endpoints are classified into several categories; the most important onesare concentration endpoint (which includes concentration response endpoint),response endpoint, protein substrate and ligand constant, and physical property end-point. The class mode of action defines the functional effect and physical binding char-acteristics of the screened entity on the target using the subclasses ligand functionmode of action (inhibition, activation, etc.) and ligand binding mode of actionAbeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 11 of 22(reversible, irreversible, competitive, etc). Each endpoint is associated with a mode ofaction, e.g., IC50 and percent activation have inhibition and activation as the functionalmode of action, respectively. The class signal direction defines how the functionaleffect of the perturbation corresponds to the intensity of the detected signal, i.e.,increase or decrease with activation or inhibition. This is important to identify suitablecounter screens; for example if the detected perturbation results in signal decrease in acell-based assay, cytotoxic compounds may be detected as actives. The class endpointaction correlation defines if the endpoint value corresponds to increased or decreasedfunctional effect (inhibition, activation). Both signal direction and endpoint action cor-relation are required to formally interpret the results, because the same perturbation(e.g., inhibition of substrate-protein binding by a competing ligand) may be measuredvia a different molecular entity with the role measured entity (e.g., substrate-boundprotein or ligand-bound protein) and the effect can be expressed in different ways(e.g., normalized as remaining percent activity or percent inhibition). Further, depend-ing on the assay design method, the same perturbation in the same model system mayresult in increased or decreased signal.Application to model LINCS profiling and panel assays and resultsThe concepts bioassay, measure group, and endpoint as described above enable theformal definition of panel and profiling assays such as those routinely run in theLINCS program. An effective modeling solution is relevant, because of the emphasis ofLINCS to operate on result profiles and signatures, in contrast to individual endpoints.We define a panel assay as the parallel, spatially separate implementation of severalidentical assays, but that vary in one parameter (other than the screened entity),typically the target. A popular example is a kinase panel, for example the DisoveRxKINOMEscan assay that is also run at LINCS and in which compounds are screenedagainst over 450 kinases in parallel. Similar to a panel assay, a profiling assay can gen-erate a large number of readouts for any given tested compound, but all results areobtained from the same physical experiment, i.e., the same well. Such assays are alsocalled multiplexed assays and rely on sophisticated assay methods and/or detectiontechnologies that enable the detection of many signals in parallel, such as flow cytome-try, mass spectrometry or imaging. One example also run at LINCS is the L1000 tran-scriptional profiling assay (vide supra). As illustrated in Figure 5, our approach wouldalso allow to define concentration response (e.g., IC50) kinase profiling assays via itera-tive aggregation of sets of measure groups corresponding to two parameters, namely mscreening concentration (values) and n kinase targets. The first aggregation by screen-ing concentration (e.g., via curve fitting) defines the IC50 endpoint for each kinase andthe second aggregation defines an IC50 kinase profile endpoint. An actual example ofsuch a assay is the ActivX Biosciences KiNative assay, which is also run in the LINCSprogram. We have modeled several LINCS assays including KINOMEScan assay, tran-scriptional response profiling assay, cell cycle state assay. The specific instances ofthese assays including hundreds of kinase targets, transcribed genes, cell lines, etc wasimplemented in an application ontology and these assays and screening results areavailable in our LIFE software system [7].An example of a phenotypic cell-based LINCS assay is the cell cycle state assay. It isalso described in BAO 2.0. In the LINCS project, several small molecules that areAbeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 12 of 22known to function as kinase inhibitors were tested on cancer cell lines for their abilityto arrest the mitotic cell cycle. This assay was modeled in BAO as follows: the assaydesign method is S phase assessment or M phase assessment method. The presence ofthe markers, namely, EdU and anti-MPM-2 antibody, indicates that cells have entered/completed S phase and M phase, respectively. Hoechst 33342 was used to stain nucleifrom all cells to obtain the total cell count in the assay. The detection method is fluor-escence microscopy and the measured entity is DNA. The assay readout parametersare intensity parameter and counting parameter. The intensity of EdU and MPM2were measured in the nucleus and cytoplasm, respectively. The counts of Hoechst33342, EdU and MPM2 positive cells were reported after the threshold to signal inten-sity of each marker was applied. The endpoint was derived from the assay readoutparameters after normalizing with the assay controls. The endpoint for this assay ispercent apoptotic cells, percent mitotic cells, percent interphase cells, percent DNAreplicated cells, percent G2 arrested cells, and/or percent mitotic arrested cells. Thecellular phenotype or its disposition is obtained by quantifying cells which are positivefor each of these markers.Categorizing mechanistically related assays by inferenceBAO 2.0 contains detailed description of a range of common HTS assay, including thecategories: binding assay, cell cycle assay, cell viability assay, cytotoxicity assay, enzymeactivity assay, gene expression assay, redistribution assay, and signal transduction assay.The essential information that was described for each assay type includes format,method (including assay design), detection, endpoint, and molecular and cellular enti-ties and their roles, qualities and functions describing the biology of the system orwhich are key components involved in the assay design or detection methods. Wehave previously shown how promiscuous frequent hitter compounds (undesired assayartifacts) can be deconvoluted and categorized mechanistically based on detailedknowledge about the assays and their related design and detection methods [29]. How-ever, using the previous version of BAO (1.6) these assays were not yet defined in away that formalizes all necessary knowledge about their commonalities. This meansthat previously, in order to perform mechanism-based cross assay analysis, somehuman expert knowledge was required to identify and categorize related assays beyondtheir asserted annotations.BAO 2.0 provides a framework that enables automated classification of assays intomeaningful categories of interest, for example to aid in identifying common assay arti-facts and their likely mechanism of action. We illustrate this using several relatedassays: luciferase reporter gene assay, cell viability ATP quantitation assay, cytochromeP450 enzyme activity assay, kinase activity assay, and luciferase enzyme activity assay.Of these, the reporter gene and cell viability assays are cell-based, while the others arebiochemical assays. The modeling of these assays is illustrated in Figure 6. All assaysuse a different assay design method. Therefore they cannot be identified as mechanisti-cally related based on that annotation alone. The physical detection method chemilu-minescence is the same for all assays, but it is too generic to classify the assays bymechanisms that underlie artifacts, because luminescence can be generated by manymethods. However, among these examples, all assays perform (in different ways) theluciferase-catalyzed chemical reaction of luciferin and ATP forming oxyluciferin andAbeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 13 of 22light (luminescence) and thus luciferase and ATP participate in all these assays,although in different roles. For example in the reporter gene assay the amount ofexpressed luciferase is quantified by the intensity of light (luminescence) produced inthe presence of substrates, ATP, and luciferin. In the viability assays the proportion ofliving cells is quantified by measuring ATP content, again by the same reaction (withATP as the limiting reagent in the role measured entity). Similarity ATP-coupledassays measure the residual amount of ATP (e.g., after a kinase reaction) by a coupledluciferase reaction. The P450 luciferin-coupled assay mentioned above measures theamount of luciferin generated after detoxification by cytochrome P450 enzyme activity.Luciferase enzyme activity assays quantify the biochemical luciferase enzyme activityby the intensity of light, again using the same chemical reaction. In BAO2.0 we mod-eled these assays with the necessary formalism to enable the reasoning engine to cate-gorize the assays as mechanistically related. As an example, Figure 7 shows theasserted TBox of the assay design method ATP quantitation using luciferase and ATPcoupled enzyme activity measurement method and the inferred TBox in which the lat-ter is classified as a subclass of the former. For illustrative purposes we defined a classof all assays with an assay design method in which luciferase participates (in any role).The axioms and the asserted and inferred hierarchies are shown in Figure 8. All assaysmentioned above are inferred as assays that use luciferase, thus illustrating how BAOformal assay definitions enable a classification based on the mechanistic principle ofthe assay (assay design method). This in turn classifies the assay based on likely com-mon artifacts (e.g. compounds that stabilize or inhibit luciferase) [29]. Figure 8 alsoshows the justification for classifying the assays mentioned above under this category.Collaborative development and application of BAO to annotate assaysWe had previously annotated (using BAO 1.6) a large set of assays from PubChem [28]and made these annotations searchable in BAOSearch [13], which is a Semantic Webapplication. These annotations were now mapped to BAO 2.0 and expanded to includeadditional information such as bioassay type, cell culture conditions, DNA constructFigure 6 Conceptual modeling of different luciferase assays. Shown are bioassay, assay designmethod, physical detection method and participants (molecular entities with a specified role in the assay).Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 14 of 22Figure 7 Examples of luciferase assay design methods . Shown are the asserted TBox of ATPquantitation using luciferase and ATP coupled enzyme activity measurement method and the inferredTBox in which the latter is classified as a subclass of the former.Figure 8 An example that infers all bioassays in the ontology that use luciferase. This exampleprovides asserted and inferred hierarchies for bioassays that use luciferase as a participant. It also providesjustification for luciferase reporter gene assay being a subclass of bioassay uses luciferase.Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 15 of 22details, roles, functions and qualities of molecular entities participating the the assays.297 luciferase assays (vide supra) containing 328 measure groups are among the anno-tated assays, including the ones mentioned above and others. As explained above, theformalization of assays in BAO allows us to retrieve these assays based on a participantsuch as luciferase or ATP, even though these molecular entities were not explicitlyannotated. A large project in which BAO has been applied and which in-turn significantlyinfluenced the evolution of BAO is BARD [6]. In BARD, all MLP data, consisting of over600 probe discoveries, are curated and annotated using controlled terms and organizedinto probe projects. BARD makes these data searchable in various ways and enables inte-grative analysis. During the development of BARD, data curation and annotation, thedevelopment of new terminology, and evolution of BAO has occurred in parallel. Thedevelopment of terminologies and ontologies to annotate assays at Novartis also influ-enced our work in the BARD and BAO projects and highlights its relevance. We alsoapplied BAO to define LINCS assays (vide supra); we make LINCS data searchable andexplorable via the LIFE [7], which leverages Semantic Web technologies to integrate andsearch diverse data types. Our RegenBase project [30] also leverages BAO. BAO is alsoexplored in the ChEMBL and PubChem projects, where BAO endpoints are used in RDFschema [16], and at PubChem. As another example from the pharmaceutical industry, aresearch group at Astra Zeneca is using BAO to annotate assays in the context of theOpen PHACTS project (personal communications).ConclusionsWe have developed BAO 2.0 as a reference for standard metadata terms and definitionsrequired to describe relevant information of low and high-throughput drug and probescreening assays and results to enable effective data integration, aggregation, retrieval,and analyses. BAO 2.0 has been developed collaboratively to provide wider scope indescribing and modeling diverse and complex assays. BAO is extended significantly withregard to the previous version using domain knowledge and data annotated in BARDand by other collaborators. We have described a flexible layered architecture to developand integrate plethora of modules from established biomedical ontologies and upperlevel ontologies. Our modularization approach defines several integral distinct BAO 2.0modules and separates internal from external modules and domain-level from structuralcomponents. This approach facilitates the generation/extraction of derived ontologies (orperspectives) that suit a particular use case or software application. We have generalizedBAO to enable modeling of result profiles (signatures) generated in panel and profilingassays, for example those in the LINCS project. BAO leverages OWL DL(SROIQ(D)) to capture and formalize knowledge about assays and screening resultsand to enable computational systems to utilize knowledge. This enables the classificationof assays and screening results into categories that relate to the assay model system, thebiology (e.g., protein target or process), how a signal is generated and how it is detected,and screening results. We demonstrated inference reasoning capabilities of BAO to clas-sify assays into categories that relate to how the assay works. This offers the potential toidentify common promiscuous frequent hitters and their possible mechanism of action.We have leveraged BAO in software tools, such as the Semantic Web software applica-tions BAOSearch, LIFE, and the BARD system. We continue to develop and expandBAO further with the goal to establish a standard to report chemical biology assays andAbeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 16 of 22their results. For example, to better describe the pharmacology of GPCRs, we recentlydeveloped a GPCR ontology framework [31]. We are also expanding BAO further todescribe high-content phenotypic assays. BAO is currently used in several public andprivate screening projects and evaluated by a number of organizations and projects. Theparticipation of groups from industry and academia to develop and use BAO illustratesthe utility of the product as well as increasing public-private collaboration in pre-competitive areas, such as the development of standards and ontologies. BAO 2.0 is freelyavailable from the BAO project website (http://bioassayontology.org) and the NCBOBioPortal. Additional file 1 contains BAO 2.0 ontologies as well as the examples illustratedin this manuscript.MethodsBAO 2.0 development approachBAO 2.0 was developed from BAO 1.6. It was performed in the following steps: First,upper level classes were created to include the various entities that participate in abioassay, and their roles and qualities. Second, vocabulary files were created by movingthe individual upper classes to the respective files. Third, all the vocabulary files wereimported into a single file, called bao core (see Modularization below). Fourth, theupper level ontology, BFO was imported into bao complete and the various vocabularyfiles (either intact or separated as required) were moved to the respective upper levelontology classes. The process of importing external ontology modules, including objectproperties are described in detail below.Generating and processing external ontology modulesBAO is currently using excerpts from eleven external ontologies (including BFO):(1) Gene Ontology (GO); (2) Cell Line Ontology (CLO); (3) Unit Ontology (UO);(4) NCBI Taxonomy (NCBITaxon); (5) Human Disease Ontology (DOID); (6) ChemicalEntities of Biological Interest (ChEBI); (7) UBERON (a comparative anatomy ontology);(8) Phenotypic Quality Ontology (PATO); (9) Information Artifact Ontology (IAO); (10)Relationship Ontology (RO); and (11) Basic Formal Ontology (BFO). The workflow forextracting external ontologies is as follows: Domain experts provide the list of conceptsof interest and their ontology IDs. Based on these lists and the expression level of theexternal ontologies, we either use Java programs with OWL API to extract modulesfrom the external ontologies of interest or we use the online tool OntoFox [32] toextract the concepts of interest. Several of the ontologies listed above are taxonomies,where we use OntoFox to avoid overlapping efforts and/or redundant code. Currentlywe use OntoFox for the ontologies listed below: (1) GO; (2) CLO; (3) NCBITaxon;(4) DOID; (5) ChEBI; (6) PATO; and (7) UBERON.BAO modularization implementationThe Web Ontology Language (OWL) [33] Description Logic (DL) - provides a rich set ofconstructors to model a domain of discourse. The DL expressivity comes with a substan-tial computational cost, as the state-of-the-art DL reasoners costs 2NExpcomplete (e.g., [22]). When the size of the ontology increases (number of axioms), the computationalcost increases exponentially. In order to manage the size complexity, we provide a modu-larization methodology that preserves the required expressivity, yet being able to scalewith the size of the ontology. Figure 3 shows the basic structure of the methodology.Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 17 of 22The proposed modularization framework uses concepts from Directed AcyclicGraphs (DAG)s [34]. First, we determine the abstract horizon between TBox andABox. TBox contains modules, which define the conceptualization without dependen-cies. These modules are self contained and well defined with respect to the domain ofdiscourse. In these modules we provide concepts, relations, and individuals. The indivi-duals are restricted to nominals, therefore they only act to close the class expressions.Figure 3 shows the main components of the framework: the top left boxes are physicalfiles, i.e., each of them is an .owl file. They contain parts of our ontology, e.g., the topleft file may contain everything of the domain of discourse that we think is necessaryand important. We can have n of these modules.Second, once the n modules are defined and if those modules have interdependentaxioms, they are provided with another ontology (or module), which imports thenecessary modules. At this level one could create any number of gluing modules,which import other modules without dependencies or with dependencies. At this level,the modules depends only on the modules of discourse. So, they all are combined inanother physical .owl file, which we may call bao core (c.f., 3). The purpose of thiscore file is that it not only combines all of the submodules together (by referring toconcepts from other physical files), it also is self-contained. This means that there isno outside term or relationship in this file.Third, at this level we can design modules that import modules from our domain ofdiscourse, and also from third party ontologies. Third party ontologies could be large,therefore a suitable module extraction method (e.g., OWL API) can be used to extractonly part of those ontologies (vide supra). An example would be using a BFO term ora RO relationships. We would model this in the bao axiom level. We can have one baoaxiom file or multiple files, each may be modeled for a different purpose, e.g., tailoredfor various research groups. Thus, bao Axiom 1 . . . n as seen in Figure 3. Once theseontologies are imported, the alignment takes place. The alignments are defined forconcepts and relations using equivalence or subsumption DL constructs. The align-ment depends on the domain experts best guesses.Forth, release the TBox based on the modules created from the third phase. Dependingon the end-users, the modules are combined without loss of generality. With thismethodology we make sure that we only send out physical files that contain our (and theabsolute necessary) knowledge.Fifth, at this level, the necessary modules ABoxes (again 1 . . . n ABoxes) are created.ABoxes can be loaded to a triple store or to a distributed file system (Hadoop DFS[35]) in a way that one could achieve pseudo-parallel reasoning.Finally, using modules, we define views on the knowledge base. These are files that con-tain imports (both direct and indirect) from various TBoxes and ABoxes modules for theend-user. It can be seen as a view, using database terminology. In essence, we will be ableto tailor these views based on the modules that we need. We expect that this methodologywill speed-up the loading process, since only the necessary modules are loaded rather thanevery file that imports thousands of unnecessary and possibly redundant terms (e.g., dueto potential loops in the imports). Therefore, BAO modules: (1) modify, expand, andmaintain BAO independently; (2) use BAO in related efforts, such as knowledge reporting,more efficiently; (3) expand and synchronize BAO concepts in related efforts (e.g., BARD,Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 18 of 22LIFE, RegenBase, etc.); (4) reuse parts of BAO for different projects; (5) use other ontolo-gies easily and without effecting BAO in order to support community efforts; and (6) pro-vide transparent mapping of BAO to upper ontologies.In summary, our methodology is as follows: (1) different files for different modulesshould be created and each module should contain all the concepts as a taxonomy file;(2) after the n modules are created as taxonomies, the core owl file should combinethese modules; (3) the axioms related to the core ontology terms should be added tothe core owl file; (4) once the core owl file is created that has nothing but the ontol-ogys native concepts and axioms created by the native concepts, the third level filethat has external ontologies should be created. The external ontologies can be addedby using different combinations and related axioms can be added to the ontology atthis level; (5) after creation of the one or more owl files that link different externalontologies and contain related axioms, individuals related with the ontology are added/loaded in the next level; (6) the view file that contains imports (both direct and indir-ect) from various TBoxes and ABoxes is created from user specifications. Based on thisframework and methodology we have modeled the BAO 2.0. Figure 4 provides a completedescription of the current vocabularies, modules, and axioms files and their connectionsdeveloped for the ontology.Our modularization framework differs significantly from existing methodologies: Indecision making, a state represents a situation in which decisions should be made. Anontology provides a basic framework to represent situations in which decisions aremade. Depending on the layer in which the decisions needs to be made, a state canrepresent from low-level signals to high-level mental abstractions. Therefore, stateabstraction provides the basis in which layer-wise decisions are made. OWL ontologiesprovides mechanisms such as owl:import to represent state abstractions. But this hasnot been explicitly studied in large scale ontologies. Our modularization frameworkassesses the capabilities of OWL ontologies to represent state abstractions in differentcomplexities.Ontology interpretation provides mechanism to represent vocabularies for classes,roles, and individuals. Without any other assumption, the interpretations of these entitiesprovides the state of the system. These entities are analogues to low-level sensationsfrom perceptions. Our modularization framework captures these representations in thevocabulary layer. One can use these representations for tasks such as to populate drop-down menus in a web-application etc. These representations are at its basic levels andthe system does not assume any constraints. Having provided constraints leads to OWLontologies to represent state abstractions.In order to provide additional information related to basic entities, the next step is toenrich the state with constraints. In first-order-predicate logic, constraints are providedby axioms. Therefore, in OWL ontologies, we use axioms as the method to provide theconstraints, hence, the state abstractions. The modules in the modularization frame-work provides different constraints. The modules are connected though the owl:imports mechanism, and the constraints are provided by OWL constructs available inSROIQ(D) description logic. Therefore, at each layer, do-main experts provideaxioms for the best of their knowledge. The modularization framework provides hardboundaries in which, a domain user can extract constraints. This partially addressesAbeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 19 of 22one of the problems in ontologies: axioms extraction, which has NP-hard complexity.The hard boundaries provides decision points where abstract state of another systemshould have been extracted, for example, one can extract whole BAO abstract statewithout reference to a upper-level ontology such as BFO.The proposed modularization framework provides basic steps to implement ourknowledge base reporting (KBR) application. It needs to infer knowledge from massiveABoxes with parallel reasoning using frameworks such as Map-Reduce. KBR needs deci-sion layers in which to knowledge to be reported, and our modularization frameworkprovides those decision points.In artificial intelligence, state representations and state abstractions are an open pro-blem. OWL ontologies, with respect to first-order-predicate logic, provides methods torepresent knowledge, but, to our knowledge the state abstraction is not discussedwidely. Our modularization framework addresses these problems and possibilities inwhich state abstraction can be generalized.Assay annotations: terminology alignment, reformatting and processingAs described in our earlier publication [12], assays from PubChem were annotatedusing BAO 1.6 terminology. These existing annotations were mapped to correspond-ing BAO 2.0 classes and annotations were expanded including cell culture condi-tions, DNA construct, quality, role and function of molecular entities. In addition,the object properties and data properties were refined; many were imported fromthe RO. Cell line, gene and protein names were standardized by importing thenomenclature from CLO or specific repository, NCBI or HUGO, and UniProt,respectively. In total, 1,000 assays in the PubChem database were annotated usingBAO 2.0. These are leveraged in BARD; however, BARD includes all assays andresults generated by the MLP screening centers (>6, 000) and organizes them byprobe projects (>600). In the process of annotating assays, new terms were collectedand subsequently mapped or added to BAO manually (after expert review). In addi-tion, we incorporated terms from some of the Novartis ontology modules and termsrequested by other collaborating group (e.g., Astra Zeneca). Assay annotations werecaptured in a spread sheet with column headers that correspond to BAO classes orrelations. For the luciferase assays, we translated the columns headers for the mostimportant annotations and their contents into triples (by mapping column headersto corresponding relations) and loaded them into a RDF triple store as previouslydescribed. Figure 8 shows an example where we have used BAO 2.0 to infer allbioassays that use a method in which Luciferin 4-monooxygenase is a participant.We have defined the equivalent class bioassay uses luciferase as bioassay ? ?hasassay method (assay design method n ?has participant Luciferin 4-monooxygen-ase). OWL DL reasoners infer that cell viability ATP quantitation assay, cytochromeP450 enzyme activity assay, kinase activity assay, luciferase enzyme activity assay,and luciferase reporter gene assay are indeed luciferase assays. Figure 8 provides jus-tification for luciferase reporter gene assay being a subclass of bioassay uses lucifer-ase. This allows us to identify assays that are annotated with any of these assaydesign methods as assays that use luciferase, which is relevant to identify assay arti-facts across various different bioassays.Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5http://www.jbiomedsem.com/content/5/S1/S5Page 20 of 22Additional materialAdditional file 1: BAO 2.x build 2866. BAO version 2.x (build 2866) is a collection of OWL files that describe ourdomain of discourse. These files are serialized in OWL XML file format. In order to view and perform reasoning ofBAO, we recommend of using a standard OWL editor that support SROIQ(D) DL constructs, such as, Protégé-4.3open source ontology editor and knowledgebase framework (http://protege.stanford.edu). In addition, up-to-dateBAO releases are freely available from the BioAssay Ontology (BAO) project website (http://bioassayontology.org/wp/bao) and the NCBO BioPortal.Competing interestsThe authors declare that they have no competing interests. The views presented in this paper do not necessarilyrepresent or reflect those of the funding organizations.Authors contributionsSA and UDV contributed equally to the presented work. UDV, SA, AM, KS, PAC, JAB, AS, AJM, MR, SB, VL, and SCSdeveloped the ontology (terminology, definitions, properties). UDV mapped BAO concepts to BFO. SA, HK, UV, andSCS developed the modularization method and OWL DL implementation. SA and HK developed the ontologysupporting tools. HK, UDV, AJM, PAC, SB, SA, and SCS provided the ontology alignment with external ontologies andBARD. SA, UDV, HK, AK, DT, and SB developed ontology applications (assay annotation, software implementation). CC,VL, and SCS conducted ontology releases, QC, support, and CC maintains the BAO website. SCS envisioned anddesigned the BAO project. SA and SCS wrote the paper with contributions from UDV, HK, UV, and VL.AcknowledgementsThis work was funded by the National Institutes of Health (NIH) National Human Genome Research Institute (RC2-HG005668 and RC2-HG005668-02S1, awarded to SCS and VL), the NIH LINCS program via the National Heart, Lung,and Blood Institute (U01-HL111561 and U01-HL111561-02S1, awarded to SCS), the National Institute of NeurologicalDisorders and Stroke (R01-NS080145, awarded to VL, Prof John Bixby and SCS ). JAB, PAC, and SB were supported aspart of the NIH RoadMap Molecular Libraries Initiative (U54-HG005032, awarded to Prof Stuart L. Schreiber). We alsoacknowledge resources from the Center of Computational Science of the University of Miami.DeclarationsThe publication costs for this article were funded by the Center of Computational Science of the University of Miami.This article has been published as part of Journal of Biomedical Semantics Volume 5 Supplement 1, 2014: Proceedingsof the Bio-Ontologies Special Interest Group 2013. The full contents of the supplement are available online at http://www.jbiomedsem.com/supplements/5/S1.Authors details1Department of Computer Science, University of Miami, 1365 Memorial Drive, 33146 Coral Gables, FL, USA. 2Center forComputational Science, University of Miami, 1320 S. Dixie Highway, Gables One Tower, 33146 Coral Gables, FL, USA.3The Miami Project to Cure Paralysis, 1095 NW 14th Terrace, 33136 Miami, FL, USA. 4Department of Molecular andCellular Pharmacology, University of Miami School of Medicine, 1120 NW 14th Street, CRB 650 (M-857), 33136 Miami,FL, USA. 57 Cambridge Center, Cambridge, MA 02142, MA, USA. 6Novartis Institutes for BioMedical Research, 250Massachusetts Avenue, 02139 Cambridge, MA, USA. 7Thomson Reuters, 5901 Priestly Drive, Suite 200, 92008 Carlsbad,CA, USA.Published: 3 June 2014PROCEEDINGS Open AccessPreserving sequence annotations across referencesequencesZuotian Tatum1,4*, Marco Roos1,2, Andrew P Gibson1, Peter EM Taschner1, Mark Thompson1, Erik A Schultes1,Jeroen FJ Laros1,3From Bio-Ontologies Special Interest Group 2013Berlin, Germany. 20 July 2013* Correspondence: z.tatum@lumc.nl1Department of Human Genetics,Center for Human and ClinicalGenetics, Leiden University MedicalCenter, Einthovenweg 20, 2333 ZCLeiden, the NetherlandsAbstractBackground: Matching and comparing sequence annotations of different referencesequences is vital to genomics research, yet many annotation formats do not specifythe reference sequence types or versions used. This makes the integration ofannotations from different sources difficult and error prone.Results: As part of our effort to create linked data for interoperable sequenceannotations, we present an RDF data model for sequence annotation using theontological framework established by the OBO Foundry ontologies and the BasicFormal Ontology (BFO). We defined reference sequences as the common domain ofintegration for sequence annotations, and identified three semantic relationshipsbetween sequence annotations. In doing so, we created the Reference SequenceAnnotation to compensate for gaps in the SO and in its mapping to BFO, particularlyfor annotations that refer to versions of consensus reference sequences. Moreover,we present three integration models for sequence annotations using differentreference assemblies.Conclusions: We demonstrated a working example of a sequence annotationinstance, and how this instance can be linked to other annotations on differentreference sequences. Sequence annotations in this format are semantically rich andcan be integrated easily with different assemblies. We also identify other challengesof modeling reference sequences with the BFO.BackgroundSequence annotations and their relationship with reference sequencesSequence annotations are information artifacts that add biologically meaningful informa-tion to specific locations on genomic, gene, transcript or protein sequences. For example:1) Gene OR4F5 is located on human chromosome 1 (build hg19), from position69090 to 70008.2) Substitution of C by T at location 178 of transcript reference sequenceNM_004006.2 results in nonsense variant Gln60* in protein reference sequenceNP_003997.1.Tatum et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S6http://www.jbiomedsem.com/content/5/S1/S6 JOURNAL OFBIOMEDICAL SEMANTICS© 2014 Tatum et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction inany medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Sequence annotations are only meaningful if the reference sequence is known. However,specifying a stable reference is not necessarily straightforward. Before the Human GenomeProject, Locus Specific Databases (LSDB) were recommended for storing and sharing genecentric variant annotations [1]. To date, the most popular platform for storing these tran-script variants is the Leiden Open-source Variation Database v.2 (LOVD2) [2]. In eachLOVD2 instance, a stable transcript sequence is chosen as the reference sequence ofeach gene. Variants are annotated with descriptions of sequence variations and positionsaccording to the chosen transcript sequence. There are many advantages of using gene/transcript centric annotation approach. First, the length of a gene is much shorter than alocus/chromosome, therefore maintaining the sequence content is much easier. Secondly,it limits annotations mainly to the protein coding regions of the genome, therefore focus-ing more on easy to predict phenotypic effects. However, LSDBs typically limit descrip-tions of DNA variants to a single transcript, even when multiple transcripts may beaffected. Depending on which transcript is used, the variant description may look verydifferent. To calculate the location of a variant based on a different reference sequence, anexternal conversion tool has to be used for the position conversion [3]. Disambiguation ofthe variant description is an essential step in the context of data integration andpreservation.However, not all biological questions are locus specific. As sequencing technologiesadvanced in the past 15 years, more and more studies are omics focused, requiring astable and complete reference genome [4]. The Human Genome project was com-pleted in April 2003, followed by the release of human genome assembly NCBI35/hg17 inMay 2004. Sequence gaps and assembly errors were removed and newly discovered genes,(non-coding) transcripts and proteins were annotated with every new release up toGRCh37/hg19 (February, 2009) [5]. As reference sequences are revised, it becomesincreasingly difficult to track and compare annotations. Researchers today share theirresults of genome-wide genomic and epigenetic studies in publications and databases, butthey often fail to mention the exact version of the reference genome sequence. Moreover,many popular annotation file formats do not explicitly ask for reference sequence versioninformation. It is up to the user to embed this information in the file description throughnatural language. Consequently, when using these formats to exchange data for computa-tional analysis and data integration, essential metadata is too easily lost. For example, theENCODE Project Consortium [6] has effectively shared their data by publishing them asannotation tracks in the UCSC genome browser [7]. However, these annotation tracks useBrowser Extensible Data (BED) format, which does not explicitly state the referenceassembly version within the file. To propagate current annotations to the forthcomingGRCh38/hg20 and alternative genome assemblies, it is crucial to preserve annotationswith their respective reference sequence versions.A Semantic Web approach to data integrationA possible approach to exposing sequence variation annotations in a computer accessibleformat is provided by Sematic Web languages and tools [8]. It effectively removesthe boundaries between annotating data, linking data, and making data machine readable[9-11]. By representing data and metadata in Resource Description Framework (RDF) andusing shared ontologies in RDF and Web Ontology Language (OWL), mismatchesbetween database schemas and the identity of its content can be addressed [12,13].Tatum et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S6http://www.jbiomedsem.com/content/5/S1/S6Page 2 of 13A zfirst attempt for mutation data was presented by Zappa and coworkers, who produceda mutation database for TP53 as Linked Open Data [14]. They followed the principles ofLinked Data [15] and applied various existing ontologies to achieve optimal interoperabil-ity. However, they did not address the problem of integrating mutation data that wereannotated using different reference sequences. They did not model genomic locations ofannotations in detail, which makes querying this dataset difficult.Ontological framework for data integration across resourcesFormal ontologies play an important role in semantic data integration between informa-tion systems [16,17], bringing conceptual coherence, stability, and scalability to the applieddomain, which can greatly increase data interoperability [17,18]. The Open Biological andBiomedical Ontologies (OBO) Foundry provides a suite of orthogonal interoperable ontol-ogies to aid knowledge integration in the biomedical domain [19]. To take advantage ofthe OBO Foundry ontologies, we have chosen Basic Formal Ontology (BFO) [20] as ourupper ontological framework for data modeling [20]. Other ontologies in OBO that arerelevant to this paper include the Information Artifact Ontology (IAO) [21], the SequenceOntology (SO) [22], the Ontology for Genetic Interval (OGI) [23], and the RelationOntology (RO) [24].Previous efforts on modeling biological sequences and sequence annotations in the OBOcommunity have taken primarily a biological viewpoint. Thus, sequences refer to biologi-cal molecules, and sequence annotations refer to features defined with respect to biologi-cal process [22,25]. The SO focuses on creating a set of consistent vocabularies thatdescribe the biological functions of these sequences and defining the biological relation-ships between these sequences [22]. OGI models the biological physical sequence byadopting the realism approach from BFO, and further contributes to this model by addingspatial topological relationships between sequences [23]. However, Hoehndorf et al.pointed out a gap between this biological model and information systems that are used tostore sequence annotations [26]. To bridge this gap, they have proposed three views ofbiological sequences: molecular, syntactic, and abstract. Molecular sequences are DNA andRNA molecules as well as proteins. Syntactic sequences are strings like ACAC and repre-sent the arrangement of the molecules in the molecular sequences. Abstract sequencesrepresent an equivalence class of sequence tokens or representations. They point out thatwithout such a clear distinction data integration is hampered. Indeed, the SO communityacknowledged the lack of distinction that is made by biologists between abstract, syntactic,and molecular sequences. Bada and Eilbeck proposed a strategy of separating SO into twoparallel ontologies: one for molecular sequences, the other with abstract sequences(abstract in a broader sense than meant by Hoehndorf). The former would be an extensionof the Molecular Sequence Ontology while the SO would focus more on the abstractsequences referring to sequences, and parts of sequences [27]. However, this newalignment strategy is still under discussion.Beyond the OBO Foundry there are additional relevant ontologies applicable tosequence annotation. The Feature Annotation Location Description Ontology (FALDO)is the latest effort to address the void of describing sequence annotations from theinformation systems perspective [28]. It is designed to be general enough to describeannotations with various level of location complexity, but not addresses issues such asthe meaning of or the evidence of the location.Tatum et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S6http://www.jbiomedsem.com/content/5/S1/S6Page 3 of 13Aim of this paperOur aim is to create an RDF data model for describing sequence annotation instanceswithin an established ontological framework that fits our practice of working with refer-ence sequences and different versions of genome assemblies. We provide a mechanismfor linking annotation instances to different reference sequences. We also present someof the challenges in aligning our approach with current OBO Foundry ontologies.Results and discussionDescribing sequence annotation instancesOur starting point for modeling sequence annotations was the BED format, a widely usedtable-based format for sequence annotations that is easy to use and efficient to store (seeFigure 1). It typically consists of rows with a reference (e.g. a chromosome identifier), startand end position on that reference, and a value for the annotation. Most UCSC genomebrowser annotations can be downloaded as BED tracks. We started by deriving our RDFmodel from the BED format: (i) we identified the desired upper ontological framework forthe domain of interest; (ii) we converted data in the BED track to RDF triples; (iii)we further transformed the resulting triples by adding class definitions and ontologymappings to the final model. We describe these steps below:Upper ontological frameworkWe chose to use the BFO (version 1.1) as our top-level ontological framework. Weaugmented BFO with a minimal Reference Sequence Annotation (RSA) ontology tocapture classes and predicates, and defined alignment strategies for RSA with OBO.Data transformation to triplesAs a preparative step, we first created annotation instances that closely matched our ori-ginal data format. We created a naive model for sequence annotation to directly trans-late the information in the BED file with the addition of the reference assembly name(Figure 2). Predicates linking the resource and its property values were derived from theBED format description. At this stage, we used rdfs:Literal to capture concepts withoutfurther ontological grounding (i.e., rdf:type relations). This data-centric approach tosemantic modeling is similar to the syntactic conversion that is often used for integra-tion of non-RDF resources, where table values are converted to literals, and table namesand headers to classes and properties without any further semantic modelling [29].Figure 1 BED file examples. RefSeq transcript annotation in BED format on genome builds hg19 (a) andhg18 (b). The second line contains the start and end positions of the NM_001005484 transcript encodedby the OR4F5 gene that differ per assembly. Note that the BED file header line does not explicitly state thereference sequence information. The submitter can only embed this information in the track descriptionthrough natural language.Tatum et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S6http://www.jbiomedsem.com/content/5/S1/S6Page 4 of 13These naive models usually have limited semantic depth, such that finding commonelements for integration with other data sources can be difficult. Therefore, the model isoften linked to a more sophisticated, or personal model. In our case, we used the naivemodel as a starting point in the modeling process, replacing it step by step by a moreprecise model (Figure 3). Content of rdfs:Literals from the naive model were thusconverted to owl:instances, and class definitions were added. Below, we discuss our deri-vation of the new model step-by-step, while explaining the placement of new RSAclasses and predicates, the reuse of existing ontologies, and potential problems withOBO alignment. An RDF representation of the final model is shown as follows:@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .@prefix rsa: <http://rdf.biosemantics.org/ontologies/rsa#> .@prefix hg19: <http://rdf.biosemantics.org/data/genomeassem-blies/hg19#> .@base <http://rdf.biosemantics.org/examples/sequence_annota-tion#> .:transcript a rsa:SequenceAnnotation ;rsa:refseqID NM_001005484";rsa:isAnnotatedAt :location .:location a rsa:AnnotationLocation ;rsa:start 69090"^^xsd:int ;rsa:end 70008"^^xsd:int ;rsa:mapsTo hg19:chr1 ;Figure 2 Naive model. Naïve transformation of a BED sequence annotation. Predicates used in this modelare placeholders and replaced in a later stage.Figure 3 Semantic model. A sequence annotation instance after semantic transformation.Tatum et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S6http://www.jbiomedsem.com/content/5/S1/S6Page 5 of 13rsa:hasOrientation rsa:forwardJOURNAL OFBIOMEDICAL SEMANTICSHastings et al. Journal of Biomedical Semantics 2014, 5:38http://www.jbiomedsem.com/content/5/1/38RESEARCH Open AccessEvaluating the Emotion Ontology through usein the self-reporting of emotional responses atan academic conferenceJanna Hastings1,2*, Andy Brass3,4, Colin Caine3, Caroline Jay3 and Robert Stevens3AbstractBackground: We evaluate the application of the Emotion Ontology (EM) to the task of self-reporting of emotionalexperience in the context of audience response to academic presentations at the International Conference onBiomedical Ontology (ICBO). Ontology evaluation is regarded as a difficult task. Types of ontology evaluation rangefrom gauging adherence to some philosophical principles, following some engineering method, to assessing fitnessfor purpose. The Emotion Ontology (EM) represents emotions and all related affective phenomena, and should enableself-reporting or articulation of emotional states and responses; how do we know if this is the case? Here we use theEM in the wild in order to evaluate the EMs ability to capture peoples self-reported emotional responses to asituation through use of the vocabulary provided by the EM.Results: To achieve this evaluation we developed a tool, EmOntoTag, in which audience members were able tocapture their self-reported emotional responses to scientific presentations using the vocabulary offered by the EM. Wefurthermore asked participants using the tool to rate the appropriateness of an EM vocabulary term for capturing theirself-assessed emotional response. Participants were also able to suggest improvements to the EM using a free-textfeedback facility. Here, we present the data captured and analyse the EMs fitness for purpose in reporting emotionalresponses to conference talks.Conclusions: Based on our analysis of this data set, our primary finding is that the audience are able to articulatetheir emotional response to a talk via the EM, and reporting via the EM ontology is able to draw distinctions betweenthe audiences response to a speaker and between the speakers (or talks) themselves. Thus we can conclude that thevocabulary provided at the leaves of the EM are fit for purpose in this setting. We additionally obtained interestingobservations from the experiment as a whole, such as that the majority of emotions captured had positive valence,and the free-form feedback supplied new terms for the EM.Availability: EmOntoTag can be seen at http://www.bioontology.ch/emontotag; source code can be downloadedfrom http://emotion-ontology.googlecode.com/svn/trunk/apps/emontotag/ and the ontology is available athttp://purl.obolibrary.org/obo/MFOEM.owl.*Correspondence: hastings@ebi.ac.uk1Cheminformatics and Metabolism, EMBL  European Bioinformatics Institute,Wellcome Trust Genome Campus, Hinxton CB10 1SD, UK2Swiss Center for Affective Sciences, University of Geneva, Geneva, SwitzerlandFull list of author information is available at the end of the article© 2014 Hastings et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedicationwaiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwisestated.Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 2 of 17http://www.jbiomedsem.com/content/5/1/38BackgroundOntology evaluation is the assessment of how good anontology is for one or multiple purposes [1]. Biomedi-cal ontologies are being developed to address multiplerequirements in biology andmedicine including standard-isation, data annotation and statistical analysis [2]. Ontol-ogy evaluation is recognised to be a difficult problem [1],with modes of evaluation ranging from conformance tosome philosophical principle [3,4], adherence to a speci-fied method [5], conformance to a corpus of text or data[6], to fitness for purpose for a given task [7].Formal evaluations of biomedical ontologies are rareand this paper presents an evaluation of an ontologyvocabularys ability to make the distinctions necessary ina field of interest. To this end, we report on an evalua-tion of the suitability of the Emotion Ontology (EM, [8,9])in use for the self-reporting of emotional experiencesat an academic conference. As the ontology has previ-ously been described in [9] we do not here repeat thatmaterial. Rather, we focus on describing our experimentin which in order to assess the emotional vocabularyof the EMs fitness for purpose for the self report-ing of emotional experience, we used the ontologysvocabulary to capture an audiences emotional responsesto academic presentations at the International Confer-ence on Biomedical Ontology (ICBO) [10] that washeld in Graz, Austria in July 2012. We conducted thisevaluation through the development of a tool, EmOn-toTag, by means of which audience members wereable to capture their emotional responses to the sci-entific presentations using the vocabulary offered bythe EM.An ontology makes distinctions between entities in afield of interest. In our case, the EM makes distinctionsbetween types of emotions, such as being bored or inter-ested. In an academic conference we can assume thatneither talks nor the audience are homogeneous in emo-tional response provoked or elicited. In the biomedicalontology community in particular, there are well knowncontentious approaches to ontology engineering [3,11].Thus we can expect that different talks will provoke differ-ent emotional responses and that audience members wilhave a range of differing emotional responses to talks atICBO 2012. From this, our null hypothesis (H0) is:The EM will not enable audience members toarticulate their emotional response to a talkappropriately such that we can cluster the audience bytheir response to a talk. We will find that people andtalks are not able to be distinguished by thedescriptions of emotional responses.If the null hypothesis is rejected, we may expect confer-ence participants to be able to use the EM to articulatean emotional response to a talk and that talks and theaudience can be partitioned by emotional response.To test this hypothesis, we allowed audience membersto give their emotional responses to talks using the vocab-ulary drawn from the ontology, and also asked them torate how appropriate an EM vocabulary term was forarticulating an emotional response.Examples of the phrases that were used to capture emo-tions during the conference include I feel interested,I feel bored and I think that this is being caused super-naturally. The rating given by users as to how easy it wasto use the EM to capture their emotions ranged from 1(it was difficult to capture the emotion being experienced)to 5 (easy to capture the emotion being experienced). Wealso asked participants to suggest improvements to theontologys content using a free-text feedback facility; theaim here was to capture emotions that participants feltthey could not articulate using the EM.We were also able to collect the following information: Which vocabulary terms were used and with whatfrequency; The numbers of terms used per talk; The time at which a term was used and by which(anonymous) audience member; The number of people participating in the study; The strength of emotional response to talks.While the evaluation of ontologies in use in applicationshave been conducted before (as discussed in [1,12]; for arecent example evaluating the Gene Ontology in use see[13]), we believe that the approach we have followed ofcombining the use of an ontology in an application withthe simultaneous rating of the ease of use of the ontologysvocabulary for that application is a novel technique thatcould have applicability outside the scope of the presentinvestigation.The Emotion OntologyCapture of emotional experience is a component of a vari-ety of different research and application scenarios. Forexample, self-reported emotional experiences are oftencaptured to monitor mood fluctuations between clinicalvisits in the clinical treatment of depression and bipo-lar disorder [14,15]. Self-reported emotional experiencesmay also be useful in the assessment of response to soft-ware tools, new products, or audience response to aca-demic presentations. Various tools have been developedthat allow capture of emotional experience in the con-text of specific application needs (e.g. [16,17]). However,there has been no agreement on shared identifiers for theunderlying structure of the emotional domain such thatannotations could be compared between different toolsHastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 3 of 17http://www.jbiomedsem.com/content/5/1/38and across different projects that employ different levelsof specificity.Ontologies provide a flexible hierarchically organisedstructure for defining entities and vocabulary within adomain [18], and have been highly successful in enablingstandardisation in biomedical contexts [2]. Reflected ingeneric ontology languages such as the Web OntologyLanguage (OWL, [19]), ontologies are computable andsupported by many open source libraries across multi-ple languages, thus are suitable for implementation intoa wide range of different tools. Reuse of a shared ontol-ogy across multiple tools enables subsequent aggrega-tion and comparison between annotations arising fromheterogeneous projects [20], as has been amply illus-trated by successful applications of the Gene Ontologyproject [21,22].The Emotion Ontology (EM) is an ontology beingdeveloped for the domain of the emotions and allrelated affective phenomena [8,9]. The ontology aims toaddress diverse requirements arising from the full rangeof disciplines involved in research into affective phe-nomena, including psychology, psychiatry, neuroscience,biomedicine and the life sciences. Such applicationsinclude standardised data annotation for aggregationacross databases, meta-analyses of primary researchresults, mapping across disciplines for translation of pri-mary research into candidate therapeuticals, semanticsearching and querying of literature and databases suchas implemented by the Neuroscience Information Frame-work [23], and automated text analysis for addressing thesemi-automatic curation of the vast quantities of scien-tific literature [24]. We have previously used the ontologyin the automatic detection of emotions in the text of sui-cide notes, with potential application to the analysis of thediary writings of suicide-risk patients to assist in suicideprevention measures [25].The EM currently consists of distinct branches for emo-tions and related phenomena. As is documented in themetadata of the ontology, the vocabulary included inthe EM ontology has largely been drawn from [26] andthe vocabularies used in the GRID cross-cultural project[27]. The upper-level structure of the ontology, asreported in [9], distinguishes emotions proper as complexprocesses, for example anger or fear, from other physio-logical and mental processes that may form a part of anemotion process, including cognitive appraisal processesand subjective feelings. From this complex structure, weidentified three branches of the ontology that we believedto be of relevance for the self-reporting of emotionalexperiences: appraisals (cognitive judgements that maytrigger emotions), subjective feelings (inner awareness ofaffective feelings), and emotions proper.For example, anger is an emotion defined in the EM asAnger is a negative emotion, characterised by feelings ofunpleasantness and high arousal, in the form of antago-nistic feelings and action tendencies, and fear is definedas An activated, aversive emotion that motivates attemptsto cope with events that provide threats to the survivalor well-being of organisms. Characterised by feelings ofthreat and impending doom, and by an urge to get outof the situation. Feeling restless is a subjective feelingdefined in the EM as The subjective emotional feeling ofrestlessness, a state of not being calm, of an agitation to dosomething.The other branches of the ontology, includingbehavioural responses to emotions such as facial expres-sions and physiological responses to emotions such as anincreased heart rate, were excluded from this experimentby virtue of these not being appropriate to the use case ofself-reporting of emotional experience.MethodsWe used self-reporting of emotional response to thetalks at ICBO 2012 and our approach had the followingcomponents:1. Design and implementation of a Web application(EmOntoTag) that enables users to anonymouslylogin, tag their emotional response to an ICBO2012 presentation using terms from the EM, andrecord how appropriate they felt an EM term wasat articulating an emotional response.2. Obtaining permission from presenters for theirpresentation at ICBO to be included in this study.3. Running the experiment during the ICBO conferenceon July 2325 2012, and analysis of the data obtained.Design and development of EmOntoTagWe conceived a tool that would be light-weight and ableto run in any Web browser to enable the broadest rangeof conference attenders to participate in the experiment.The primary requirement that we identified for this toolwas that it would allow the user to self-report their cur-rent emotional experience with a minimum of overhead,such as technical terminology or excessive clicking. Fur-thermore, in order to address the hypothesis we forcedthe user to capture how well the EMs vocabulary was ableto capture the respondants emotional response as a rat-ing attached to every record they made of their emotionalexperience with this tool.AWeb tool was implemented in the Python language ontop of a MySQL database. This was subsequently wrappedwith the Jython JavaPython bridge to enable deploymentin a Tomcat Web application server. All source code forthe implementation of EmOntoTag is available from therepository hosted at [8]. EmOntoTag can be accessed viahttp://www.bioontology.ch/emontotag with login guest,for which responses will not be recorded.Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 4 of 17http://www.jbiomedsem.com/content/5/1/38In order to anonymise the users of the tool, while stillcontrolling access in order to ensure that we could distin-guish different users responses, we prepared anonymousrandom access codes printed on sheets of paper whichwere handed out to conference participants by the con-ference organisers. Only the anonymous random codeswere stored against the tags in the underlying database. Anexample of the sheet handed to conference participants isavailable as Additional file 1.After obtaining an access code and logging into the tool,users were presented with the list of available presenta-tions linked to information about the conference schedule.The tool had a register of the conference schedule and wasable to direct users to the currently ongoing presentation(at least according to the conference schedule). Presenta-tions were indexed by author names and by presentationtitle. Only those presentations for which the presenteragreed in advance to participate in the experiment wereenabled in the tool; for those presentations for which thepresenter did not agree, the tool showed a message thatthe presentation was not available for tagging.For each presentation, the users were offered a responsecapture interface that allowed them to articulate theiremotional response using the vocabulary from the under-lying EM ontology. As described in [9], EM distinguishesemotions proper, i.e. full complex emotional experi-ences associated with an object, from subjective feelings,which are simpler feelings and which dont necessarilyhave an object, and appraisals, which are the cognitive(thought) component of emotions which are viewed insome theories to be the triggers of emotions [28]. As adesign choice to enable natural emotional expression,options were provided to the user in the context of sen-tence completion, where the allowed sentences beganwithI feel and I think. I feel was used as the precursorto the selection options from the emotion and subjec-tive feeling branches of the ontology, while I think wasused as the precursor to the appraisal (cognitive) branchof the ontology. To accommodate the fact that the labelsfor emotion terminology in the ontology were in the nounform, e.g. fear, additional synonyms were added to theontology that would fit better in the context of a sentence,e.g. afraid.Examples of the sentences that were available for expres-sion of emotions include: I feel interested , I feel despairing, I feel calm and I think that this is familiar.The full list of options that were provided in the drop-down selections in the tool interface are provided inTable 1.Options for sentence completion were presented in arandom sequence, not sorted alphabetically, in order toavoid bias towards certain terms. This almost certainlyreduced usability of EmOntoTag, but our desire to avoidtoo much bias over-rode this usability issue. Figure 1shows a screenshot of the EmOntoTag user interface forselection of an EM sentence during the experiment.Having specified a sentence describing the emotionalexperience at that moment, the user was required torate how well the vocabulary provided by the EM cap-tured their emotional response. They were also ableto offer a strength of response for the EM sentenceused. All the users previously captured sentences forthat talk were displayed in a table lower on the screen,indexed by the time of capture, and it was possible todelete previous sentences, to allow for the correction oferrors. Deleted sentences were not used in the subsequentanalysis.It was also possible to use a separate free text input fieldto record requests for content for the EM or problemswith the EmOntoTag user interface. We used this as themeans to gather information about possible extensions tothe EM.Obtaining permission to run the experimentWe obtained permission from the presenters beforeincluding their presentations in the experiment. Presen-ters were contacted individually by email in order torequest permission to include their presentation in theexperiment, and their permission was sent by reply email.Only the scientific presentations were included in theexperiment, together with the two invited keynote talks.The response was overwhelmingly positive; from the 26papers and 2 keynote talks: all but one paper presen-ter gave their permission for inclusion in the experiment.Audience participation was anonymous, with no realisticway of tracing alphanumeric login codes to any individ-ual. Actual participation in recording emotional responsewas voluntary. All data were stored securely. We will notreport here on which responses were made for which par-ticular talks, with the exception of a selected talk for whichwe obtained specific permission.Experiment execution and data analysisWe enabled the EmOntoTag software on the weekendbefore the conference was due to start, and announcedand explained the experiment during the opening ses-sion of the conference on the morning of July 23rd 2012.While the tool is still available online to enable inter-ested parties to examine the interface, the cutoff date forresponsses which we included in the experiment was set at27th July 2012, i.e. 2 days after the conference closed. Thisexcluded two extremely late tags, but allowed for slowresponses.Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 5 of 17http://www.jbiomedsem.com/content/5/1/38Table 1 Emotion Ontology vocabulary used in theexperimentEmotion Ontology vocabularyEmotion Subjective feeling AppraisalPrefix: I feel Prefix: I feel Prefix: I thinkSurprised Out of control This is not expectedHappy Good I am being treated justlyMastery At ease This is not predictablepleasurePassionately In control This is not being deliberatelyloving causedSensory Exhausted This is dangerouspleasureDisgusted Energetic A response is needed urgentlyGrieving Tired This is being deliberately causedFurious Restless This is not dangerousAmused Weak This is expectedDespairing Bad I am not at the centre of attentionJealous Strong There are consequences and theyare unavoidableEmbarrassed Nervous I am being treated unjustlySerene Calm This is not familiarTerrified Alert This is not important for my goalsIrritated This is familiarProud This is being caused by chanceInterested This is pleasantSad I have irrevocably lost somethingimportantElated This is against my idealsLoving This is predictableStressed This is being caused by meSexualpleasureThis has undesirable consequencesAestheticpleasureThis is in line with my idealsCompassionate This is unpleasantEuphoric There are consequences but theyare avoidableSocialpleasureThis is being caused supernaturallyAnxious This is important for my goalsEnraged This has desirable consequencesBored This is not suddenContemptuous This is being caused by someoneelsePleasure A response is needed but noturgentlyAshamed This is suddenPanicked I am at the center of attentionTable 1 Emotion Ontology vocabulary used in theexperiment (Continued)HatefulAngryContentedDisappointedGuiltyJoyfulAfraidCompassionately lovingThe table gives a listing of the vocabulary drawn from the Emotion Ontologythat was provided to users of the EmOntoTag tool during the conference.The dataset was analysed using the R statistical analy-sis package and Matlab. We aggregated all the data into adata table indexed by anonymous user ID, ontology termID, time of response, talk ID, strength of response, andappropriateness of the EMs content. Furthermore, theontology terms were grouped by their valence into threecategories  positive, negative and neutral.To test the hypothesis that emotional response can bepartitioned into those for talks and those for the audience,we took these raw data and created two tables:1. One capturing users by the EM terms they had used;2. One capturing talks by how they were described.These tables were then normalised (so that the sum ofentries was one) to allow for the variation in number ofEM terms used to describe talks and in the numbers oftalks to which different users had responded.These two tables were then analysed using a princi-pal components strategy to determine which linear com-binations of terms described the greatest variation inresponses of people and EM terms. This gave us a setof eigenvalues and eigenvectors which could be used todescribe the data.ResultsRaw data are not provided in order to protect confidentiality.Number of respondants and EM terms usedThe total number of EM terms captured in the experimentwas 553, spread across the 27 presentations that agreedto participate in the study (25 paper presentations and 2keynotes). Of these, all 27 had at least 4 EM terms cap-tured in the experiment, and the largest number of termscaptured against one talk was 67. There were 35 distinctusers from the 80 registered conference attenders (44%)who captured EM terms during the experiment. Of these,the range of numbers of responses was large, with themost active user providing 78 terms and the least activeHastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 6 of 17http://www.jbiomedsem.com/content/5/1/38Figure 1 Screenshot of EmOntoTag facility for capturing emotional experience.user providing one term. The number of responses peruser is shown in Figure 2.The full set of counts for users and usages per term typeis given in Table 2.Ease of articulating emotional responseThe rating of how well the EM vocabulary allowed theuser to articulate their emotional response had a mean of3.42, with standard deviation 1.12. This significantly dif-fered from the median of 3; i.e. the users reported, onaverage, that the vocabulary did allow them to capturetheir emotions well(t = 8.7324, df = 552, p < 2.2?16).Our result can further be decomposed by groupingthe responses per ontology term type. There were threedifferent types of ontology term used in this experi-ment: appraisal (thoughts), subjective feeling and emo-tion. Of these, in fact, the emotions have the highestmean and the highest significance, while the thoughtscategory, with a mean of 3.14, was not significantly dif-ferent from the median of 3 (Thoughts: mean = 3.14,t = 1.6859, df = 187, p = 0.09348; Feelings: mean= 3.37,t = 2.7126, df = 66, p = 0.008505; Emotions: mean =3.60, t = 9.7564, df = 297, p < 2.2?16).Strength of responseThe mean of the strength of response is 3.07, standarddeviation 0.92. In contrast to the appropriateness of anEM term, the mean strength of response does not dif-fer significantly from the median of 3 (two-tailed t =1.8007, df = 552, p = 0.0723).Valence of responsesThe ontology terms were separated into three categoriesaccording to their valences: positive, negative, and neu-tral. Neutral was used for emotions such as surprisewhichare known to have either positive or negative valence, andHastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 7 of 17http://www.jbiomedsem.com/content/5/1/38Figure 2 The counts of responses per participating user.for many of the appraisal categories in which the sameapplied.Based on this division, we found that the majority ofresponses were positive (300 positive, 139 negative, 114neutral). Figure 3 shows the counts of terms used perontology type and per valence.Positive responses were also found to have been ratedas stronger, i.e., having a greater strength of responserating. Indeed, while no significant effect was detectedfor the overall rating of strength of response and alsonot for the neutral or negative response groups, thepositive responses were significantly stronger than themean (mean of strength of response (neutral): 2.991228;mean (positive): 3.186667 (t = 3.6273, df = 299, p =0.0003366); mean (negative): 2.884892).The positive responses also obtained a slightly bet-ter appropriateness score than the negative or neutralresponses (mean of appropriateness (neutral): 3.210526;mean (positive): 3.386667; mean (negative): 2.654676).The negative mean is significantly different to the posi-tive mean: t = 2.3365, df = 253.678, p = 0.02024, whilethe positive mean is not significantly different to that forneutral responses (p = 0.1734). The negative mean isalso significantly different to that for neutral responses:t = 2.9837, df = 235.493, p = 0.003147.Usage of ontology termsOf the total of 89 ontology terms that were included in thestudy, 67 were actually used. The most commonly usedterm was interested, with 86 occurrences. The distribu-tion of counts per ontology term is shown in Figure 4.The number of EM terms used per talk varies, withthe highest being 67 and the lowest 4. There was a spikein responses during the first talk (58) and thereafterfewer in general for subsequent talks, with spikes at thetwo keynotes and a resurgence in the last day. The twokeynotes gave sharp increases in the number of EM termscaptured relative to the remainder of the talks (67 and 30),which also makes sense given that the time the speakerswere talking was much longer. The second day had thelowest number of responses, with a bit of a revival on thelast day for the last three talks.The mean number of EM terms used per talk was 20.48,median 17 and standard deviation 15.48.Can talks and audience members be distinguished by EMterms?An analysis of the eigenvalues (scree plots) showed thatthe data could not be readily embedded in a low dimen-sional space. For both data sets, the first 5 eigenvectorscombined only captured 75% of the variation in the data.This is unsurprising in a domain with such inherent com-plexity as an academic audiences emotional responseto a series of research presentations; in contrast, anembedding into a low-dimensional space would have beensurprising.However, looking at the EM terms that dominated thefirst five eigenvectors in these two sets shows that therewere differences between talks and the audience. Theordering of key emotional terms needed to account for thevarience in the talks and the audience are shown in Table 3in descending order of strength (including the counts ofusage).Free text commentsThe free text comments yielded several suggestions ofterms that were missing from the ontology at the time ofthe experiment. These were:1. Curious (requested twice)2. Concerned3. Dubious4. Worried5. Confused (requested four times)Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 8 of 17http://www.jbiomedsem.com/content/5/1/38Table 2 Number of users and tagsTerm id Term label Number of tags Number of users Valence Type33 Interested 86 28 Positive Emotion42 Happy 30 18 Positive Emotion166 Bored 26 13 Negative Emotion68 This is expected 25 12 Neutral Thought44 Amused 22 12 Positive Emotion64 This is familiar 20 13 Neutral Thought73 This is important for my goals 19 12 Positive Thought12 Annoyed 19 8 Negative Emotion70 This is pleasant 17 13 Positive Thought95 This is in line with my ideals 17 11 Positive Thought86 This has desirable consequences 16 13 Positive Thought11 Irritated 16 7 Negative Emotion80 Tired 14 8 Negative Feeling111 Restless 13 8 Negative Feeling37 Mastery pleasure 11 3 Positive Emotion47 Contented 10 6 Positive Emotion114 Calm 9 6 Neutral Feeling41 Proud 9 4 Positive Emotion66 This is predictable 8 7 Neutral Thought51 Disappointed 8 6 Negative Emotion84 This is not being deliberately caused 8 4 Neutral Thought74 This is not important for my goals 7 7 Neutral Thought65 This is not familiar 7 6 Neutral Thought32 Surprised 7 5 Neutral Emotion43 Serene 7 4 Positive Emotion124 Nervous 6 6 Negative Feeling79 Good 6 6 Positive Feeling71 This is unpleasant 6 6 Negative Thought35 Pleasure 6 5 Positive Emotion101 I am being treated justly 6 4 Positive Thought34 Joyful 6 4 Positive Emotion121 Alert 6 3 Positive Feeling30 Despairing 5 5 Negative Emotion109 Energetic 5 5 Positive Feeling67 This is not predictable 5 4 Neutral Thought92 There are consequences but they are avoidable 4 4 Neutral Thought96 This is against my ideals 4 4 Negative Thought107 At ease 4 4 Positive Feeling52 Compassionate 4 4 Positive Emotion56 Sad 4 3 Negative Emotion46 Euphoric 4 3 Positive Emotion83 This is being deliberately caused 4 2 Neutral ThoughtHastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 9 of 17http://www.jbiomedsem.com/content/5/1/38Table 2 Number of users and tags (Continued)39 Aesthetic pleasure 3 3 Positive Emotion87 This has undesirable consequences 3 2 Negative Thought69 This is not expected 3 2 Neutral Thought36 Sensory pleasure 3 1 Positive Emotion40 Sexual pleasure 2 2 Positive Emotion28 Anxious 2 2 Negative Emotion54 Embarrassed 2 2 Negative Emotion62 This is sudden 2 2 Neutral Thought19 Disgusted 2 2 Negative Emotion76 This is being caused by me 1 1 Neutral Thought81 This is being caused supernaturally 1 1 Neutral Thought93 There are consequences and they are unavoidable 1 1 Neutral Thought102 I am being treated unjustly 1 1 Negative Thought110 In control 1 1 Positive Feeling116 Out of control 1 1 Negative Feeling13 Furious 1 1 Negative Emotion50 Passionately loving 1 1 Positive Emotion55 Ashamed 1 1 Negative Emotion90 A response is needed but not urgently 1 1 Neutral Thought112 Exhausted 1 1 Negative Feeling26 Afraid 1 1 Negative Emotion78 This is being caused by someone else 1 1 Neutral Thought89 A response is needed urgently 1 1 Neutral Thought106 I have irrevocably lost something important 1 1 Negative Thought119 Weak 1 1 Negative Feeling9 Angry 1 1 Negative EmotionResults table with count of usages per term, with distinct users, valence and type. The table gives the numbers of users and tags for each of the tag types that wasused by participants in the experiment.6. Distracted or unfocused7. Indifferent, emotionally neutral or feeling nothing(requested three times)8. Expectant or anticipative9. Hopeful10. Inspired (requested twice)11. Intrigued12. Schadenfreude.As a concrete outcome of this experiment, almost allof these missing emotional terms have been added to theontology. The exceptions are distracted or unfocused,which was deemed not to be an emotion term per se butrather having relevance to attention, which will be cov-ered in the context of the broader Mental Functioningontology project [24], and emotionally neutral or feel-ing nothing, which again was not considered to be anemotion but rather the absence of an emotion. The lattercase, emotionally neutral, should however be added as anoption provided by the user interface in subsequent ver-sions of the EmOntoTag tool. Additionally, amused wasrequested, despite this term actually being available in thelist of options.Some suggestions were received via the free text com-ment facility for alternative phrasing for certain of thelisted emotions and feelings, for example why cant I justsay pleased? and feeling of mastery better than masterypleasure. These suggestions have been incorporated intothe ontology by updating the tag display synonyms forthe relevant terms.A small number of comments related to the usabil-ity of the EmOntoTag tool employed in the experiment,specifically: Sometimes I get the red warning, This fieldis required, sometimes not, for what is apparently thesame behaviour, and Why did you not list the emotionsalphabetically?. The choice of unsorted presentation ofHastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 10 of 17http://www.jbiomedsem.com/content/5/1/38Figure 3 Counts of response by valence and by ontology term.selection options was done to avoid bias, though it doeshave an obvious usability penalty. By verbal communica-tion, another comment that we received as feedback onthe usability of the tool was that it was not optimised forsmart phone and other smaller screens. These enhance-ments will be incorporated into subsequent versions of thetool.Finally, several comments requested that the appraisalor thought list was not specific enough because it did notallow the specification of the actual cause of the emotionin question. The appraisal list included generic appraisalcomponents such as I think that there will be consequencesor I think that this is being supernaturally caused. Partici-pants, on the other hand, used the free text to request theability to express the specific cause of their emotion, forexample I feel bored because I have heard this all before, Iwas surprised that what I thought was an important aspectof the topic was missing from the talk, and I was afraidhe would run out of time. Those causes that are reflectedin the vocabulary of the EM are derived from those thathave been found to be fairly generic (i.e. applicable acrossmultiple scenarios) in the cross-cultural GRID project thatinvestigated the meaning of emotion terms [27]. Clearly,the EM cannot include a vocabulary for all the possi-ble scenarios and objects that can cause an emotion. Theaccurate description of the objects of the emotion andthe way that these objects are intricately linked to thetype of the emotion will be the subject of future EmotionOntology development.Exploring the emotional response to one talkWe were given permission by one of the presenters toreveal the results of the audiences emotional responseto his/her talk. A bar chart summarising the responsesFigure 4 Counts of tags for each ontology term used (non-zero occurrence).Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 11 of 17http://www.jbiomedsem.com/content/5/1/38Table 3 Terms that most describe users compared with terms that most describe talksTalks partition Audience partitionTerm Type Valence Id Count Term Type Valence Id CountInterested Emotion Positive 33 86 This is pleasant Thought Positive 70 17Restless Feeling Negative 111 13 Happy Emotion Positive 42 29Bored Emotion Negative 166 26 This is familiar Thought Neutral 64 20Annoyed Emotion Negative 12 19 Interested Emotion Positive 33 86Happy Emotion Positive 42 29 This is expected Thought Neutral 68 24Amused Emotion Positive 44 22Eigenvectors of terms that most describe the audience compared with terms that most describe talks; ordered in descending strength. The table gives the terms thatbest describe users and the terms that best describe talks.to the talk can be seen in Figure 5. Fifteen participantsresponded; the highest number of responses was 19 (par-ticipant 125) and the lowest was 1 response (partici-pant 79, who displayed mastery pleasurethe feeling ofmastery of the subject). Figure 6 shows the spread ofemotional responses during the talk and just after the talk.DiscussionWhilst we may have the emotional terms used by partici-pants, we do not know the motivation for the articulationof that emotion. So, the discussion that follows is some-what speculative. Also, tying reporting events to events inthe talks themselves risks identification of the talks, so thisdiscussion is limited to generalities.The main part of our null hypothesis that the EM willnot enable audience members to articulate their emo-tional response. . . can be rejected. This is given sup-port by the scores for the rating of how well the givenvocabulary sentences captured the emotion the audiencemember wanted to express; these rating scores were sig-nificantly higher than the median value. This alone indi-cates that the EM is sufficient to allow emotions to bearticulated.The following terms from the EM vocabularyinterested, happy, amused, this is familiar, this isexpected, bored, (all with a count greater than or equalto 20) are terms most responders have used for mosttalks. These are emotional responses one would expectFigure 5 The EM terms used to articulate the emotional response to one of the ICBO 2012 talks; y-axis are the terms used and x-axis is thenumber of times each term was used.Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 12 of 17http://www.jbiomedsem.com/content/5/1/38Figure 6 A time-line of emotional responses to the sample talk; the EM terms are put into bins and displayed as tags, the size of which isproportional to the number of times the tag was used. The general area of the EM is indicated by colour: appraisals in blue, emotions in red andfeelings in green. Valence is indicated by shading  darker for negative and lighter for positive. Exact times and durations are obscured to avoid thetalk being identified.to dominate in an academic conference, with an audi-ence interested and sometimes bored, with much that isfamiliar or expected, with a good deal of happiness andamusement thrown into the mix.The principle components strategy was used in the sec-ond part of our hypothesis, that the responses using theEM would be sufficient to cluster emotional responsesabout talks and the audience. The PCA determined whichlinear combinations of tags described the greatest varia-tion in responses about the audience and talks, and gave usa set of eigenvalues and eigenvectors which could be usedto describe the data. Table 3 (above) shows six EM termsthat partition to talks and five EM terms that partition tothe audience. The key emotional terms (identifiers shownin parentheses) needed to describe the talks were: inter-ested (33), restless (111), bored (166), annoyed (12),happy (42) and amused (44).Whereas those that best described the audience were:this is pleasant (70), happy (42), this is familiar (64),interested (33) and this is expected (68).As the order matters, terms this is pleasant (70) andhappy (42) are the EM terms that strongly describe theaudience, while the terms interested (33) and restless(111) were important for distinguishing the talks.The six talk terms are five emotions and one feel-ing; with three positive valence emotions (interested,amused and happy) and two negative emotions (boredand annoyed) and one negative valence feeling (restless).The five EM terms for the audience (two emotionsand three thoughts) were all either positive or neu-tral; two positive emotions (interested and happy) andone positive feeling (this is pleasant), with two of thethoughts being neutral (this is familiar and this isexpected).Two of the EM emotions (interested and happy) par-tition to both talks and audience (with interested beingstrongly in the talks partition) suggesting that overallthe talks and audience provoke or cause happiness andinterest. The audiences emotional responses are eitherpositive or neutral and the EMs feelings are associatedwith the people articulating the emotions. Much of theICBO content may well be either expected or familiarin some form to the audience; this is to be expected atmost conferences  this is the kind of thing that wouldbe expected from X. The general positive response of theaudience wil be discussed further below.The talks partition is, perhaps, more interesting; heresome negative terms appear as well as interested andhappy  we have annoyed, bored and restless. Thatany negative terms appearing are associated with the talks,rather than the audience, is reassuring; it is the talks thatcause annoyance, boredom and restlessness  while it isthe audience that feel that the talk is familiar, is expectedand is pleasant.That terms partition sensibly between talks and audi-ence, with each having high counts, together with thepartition being readily explicable leads us to believe thatthe EM has the ability to enable the articulation of anemotional response (at least in this situation).Our PCA analysis shows that the EM is sufficient toallow discrimination between audience members emo-tional responses to a conference talk. We can see emo-tional tags that are associated more strongly with theaudience and emotional tags that are associatedmore withthe talk itself. Overall, as the EM terms partitioningmakessense in the context of an academic conference, it indi-cates that the EM is competent to support conferenceattenders to articulate their emotional response to a talkand thus further supports our hypothesis.The commonest tags used were interested, used 86times by all users at some point in the conference, andhappy, used 29 times by 17 participants in 18 talks,Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 13 of 17http://www.jbiomedsem.com/content/5/1/38presumably reflecting a general level of contentment withthe conferences material (contented was used 10 timesby 6 participants in 5 talks; other tags of this kind can beseen in Table 2). Interested is the dominant emotionalresponse from the audience, which may well be expectedin a conference about ontologies, which an audience ofontologists has chosen to attend.Bored was used 26 times by 13 users in 14 talks; arelated set of tags, angry, was used once; annoyed wasused 19 times by 8 participants in 13 talks; furious wasused once; irritated was used 16 times by 7 participantsin 10 talks. The tags are related in the context of the com-munity itself: irritation, fury, annoyance, and anger maybe felt by those with entrenched opinion in opposition tothose of the speaker  or about bad science, though thetwo may not be unrelated in the minds of the participants.Members of the biomedical community will be familiarwith the divisions that exist within the community on fun-damentals of ontologies [3,11] and these divisions mighthave been reflected in the participants responses. In addi-tion, a conference in which all talks are of an equally highquality and are equally highly appreciated will be rare ornon-existent.This is expected was used 24 times by 11 participantsin 13 talks. The straight-forward interpretation is that theparticipants in question were hearing what they expectedfrom the speaker in question, either positively or nega-tively. On the positive interpretation, this tag could begrouped with the feeling of pride (if this is expressed as,for instance, a result of ones work or oneself being men-tioned in a positive light). Amusement was a responsearticulated by 12 participants, 22 times in 11 talks.The most straight-forward interpretation of this is thattalks contained an element of humour and the audienceresponded to this humour. Participants may have articu-lated amusement as schadenfreude (an emotion that wasrequested as an addition to the EM). However, taking themost straight-forward interpretation of this tag, we canobserve that ICBO had a reasonable amount of humour inits talks.Some of the tags not used were compassionately loving,contemptuous, guilty, terrified, I am at the centre ofattention. It is posssible to conceive of ways in whichthese unused tags could have been used  a person sin-gled out in a talk may feel to be the centre of attention,or being guilty of an ontological crime highlighted bya speaker. Others, such as being contemptuous are per-haps not required in this context when being angry isavailable, despite the obvious differences. The tag sex-ual pleasure was used once each by two participants. Ifa true reflection of response to either speaker or topicit may be disturbing, but it may also have been used injestdespite this, the EM still enabled the emotion to bearticulated.There is a strong tendency towards EM terms with apositive valence being used. There are at least four posiblefactors involved:1. The anonymity of reporting should allow negativeas well as positive emotional responses to bereported;2. Factions within the biomedical ontology communityand variability in the quality of the presented workshould mean there are negative emotional responsesto talks;3. Basic ideas of reduction in cognitive dissonance [29]may incline reported emotions to be positive; that is,audience members need to justify to themselves theirpresence at the conference  an individual giving abroadly negative emotional response would suggesthe or she had attended the wrong conference.Similarly, acquiescence bias [30] leads to individualstending to respond yes or positively to questions orsituations.4. The ICBO audience is self-selecting and will bepre-disposed to liking talks about biomedicalontologies; so, in spite of factionalism in thecommunity, most people will be emotionallypositive most of the time.Points three and four seem to have out-weighed pointsone and two. In addition, point one may not have beenstrong enough to overcome the need to reduce cognitivedissonance (point three).We can speculate that there weremore negative emotional responses than were reportedand the reduction in cognitive dissonance works partic-ularly well at the level of reporting. However, from thereported evidence, the emotional response to ICBO talksis overwhelmingly positive.We described the emotional response to one talk indetail. Linking responses to times or events in the fea-tured talk may break confidentiality, so the descriptionbelow is only at the most general level. Participant 125gave many responses (in time order): joyful, this is inline with my ideals, contented, this is in line with myideals, pleasure, good, this is important for my goals,this is pleasant, contented, this is expected, amused,contented, tired, proud, I am being treated justly, Iam being treated justly, euphoric, this is in line withmy ideals and contented. From this we may infer thathe or she found the talk in line with their thinking andwe could speculate that the participant was mentioned orhis/her work was mentioned. Participants 23, 40 and 123also expressed a similar emotional profile. For example,participant 40 was interested, amused, energetic, andthought this is pleasant.Amused, I feel contented, good and interested areamong the most frequent emotional responses to this talk,Hastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 14 of 17http://www.jbiomedsem.com/content/5/1/38which, assuming these 15 responses are indicative of theaudience as a whole, means this talk was well receivedemotionally (though, as discussed, one may suspect thatnegative responses are less likely to be expressed). Therewere eight amused responses from eight participants andseven of these were spread throughout the talk, suggest-ing an even level of amusement; one of the participantsbeing amused two hours after the talk, suggesting eithersustained amusement or a tardy response.In contrast, Participant 6 had a different profile andwas bored, bored, tired, restless and angry during thecourse of this talk. The response to this talk was gener-ally positive emotionally, but the ability to discriminatebetween participants (as shown in the earlier analysis) isexhibited.Related workEmotion self-reportingMood or emotion monitoring via questionnaires and self-reporting has been used in mental healthcare contexts,and more recently mobile phones have been adopted toserve that purpose [15]. Morris et al. [31] describe amobile phone application developed to allow the self-monitoring of emotional state. The application promptedusers to self-report their emotions several times a day,giving them a scale on which they could set either a sin-gle dimensional rating with different emotion types ora multidimensional Mood Map rating which allowedusers to select a point on a valence vs. arousal graph.The single dimensional scales were offered for the emo-tion types happiness, sadness, anxiety, and anger. Theseemotion types are all present in the EM ontology. Com-pared to our tool, their tool offered a reduced numberof distinct emotion types, with easier usability (using atouch interface). They coupled this experience samplingapplication with a mobile therapy utility that offered cog-nitive behavioural therapy via questions and suggestedthought exercises designed to improve the well-being ofthe user through altering their reaction to common stres-sors. Reid et al. [32] developed a mobile phone appli-cation for self-reporting that offers a questionnaire tousers on their mood, experiences and level of stress attimes throughout the day. The application prompts usersto collect data on mood and stress levels at four ran-dom times per day, and allows notes to be capturedabout locations and activities, correlated with usage ofsubstances such as alcohol and cannabis. Mood captureused Likert scales in which adjectives indicating increas-ing degrees of the relevant mood were displayed on thephone screen rather than numbers. The focus was on neg-ative moods, offering scales for angry, sad, tired, stressed,and anxious moods. Mood or emotion tagging smart-phone applications that are commercially available andmay be recommended for self-monitoring in cases ofbipolar disorder include MoodTrak [33] and the T2 moodtracker [34]. MoodTrak allows free-text description of thepresent emotion being experienced coupled with a starrating (15) that ranks the mood from positive to neg-ative. Tracking of moods is done online and a graphingfacility shows a history. However, no private option isavailable, raising difficulties for confidential or sensitiveusage scenarios. The use of free text to capture the nameof the emotion being experienced hinders subsequent har-monization for research purposes of heterogeneous dataarising from different users. The T2 mood tracker offersvariable scales along which a rating can be selected. Pre-loaded scales include anxiety, stress, depression, braininjury, and general well-being. However, the scales arecustomizable.It is our belief that an ontology such as the EM couldbenefit such applications as these by providing agreed-onstandard categories for emotion self-reporting, localiz-ing the vocabulary management function (which includestranslation management) in one central community-agreed facility. However, many of the applications cur-rently used for emotion capture allow only a veryrestricted vocabulary of emotion types, thus not captur-ing the broad range of different types of emotion that canbe experienced and reported on, but also not requiringmuch by way of vocabulary management. Others use freetext to enable the widest range of emotion types to bereported, but this approach sacrifices the facility for laterdata aggregation across different users and even differ-ent tools and may hinder subsequent interpretation andanalysis.Ontology evaluationTartir et al., 2010 [12] distinguish several broad tech-nical approaches to evaluation of ontologies, includinglogic-based approaches that use the knowledge encodedin axioms in the ontology to check for unsatisfiableclasses, and feature-based approaches that rely on metricsabout the content of the ontology, such as the percent-age of classes that lack textual definitions. According toBrank et al. [1], ontology evaluation approaches can bedivided into 1) those that compare the ontology to a goldstandard, 2) those that compare the ontology to a sourceof data about the domain being modeled in the ontol-ogy, 3) those which involve human assessment accordingto a predefined set of criteria, and 4) those which involvethe use of the ontology in a given application togetherwith an evaluation of the results. Our approach followsthe fourth strategy in that grouping, namely we haveimplemented an application that makes use of the ontol-ogy and built into the application the infrastructure toevaluate the ontology in use for that application. Simi-larly, [13] use a task-based approach to evaluate the GeneOntology in use for the enrichment analysis of gene setsHastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 15 of 17http://www.jbiomedsem.com/content/5/1/38resulting from microarray experiments. As highlightedin [35],Since users of ontologies will benefit from somethingthat ontologies can do, research in applied ontologyhas to be measured based on how well ontologies dotheir tasks.Our evaluation of the EM is notable as a rare exampleof a designed study on an ontologys fitness for purpose.Most evaluations of biomedical ontologies tend to fall intothe first and third groups above, if they are evaluated atall, and thus the work presented here is a contribution intothe field of evaluation of biomedical ontologies.LimitationsIn [1], several limitations of the ontology-evaluation-in-application-use paradigm are raised. Firstly, they point outthat the evaluation of an ontology in a particular appli-cation can yield a result that only has scope for similartasks. That is, in the context of the EM, our evaluation inuse for self-reporting of emotional experiences in a con-ference can only inform the applicability of the ontologyfor self-reporting tasks (at academic conferences) and notfor other types of use to which the ontology may be put.This is a fair point, and one that we are happy to concedeas a limitation of the present study.Secondly, they raise the concern that the ontology con-tribution to the overall application might be minimalcompared to the remainder of the implementation, andthat it can be difficult to separate the contribution ofthe ontology alone from the other aspects of the appli-cation. While our application design tried to minimisethe effect of all but the most direct aspects of the appli-cation aside from the available vocabularies, we agreethat there nevertheless might have been some impact onour results due to non-ontology-related aspects of theapplication. For example, the order in which words werepresented in the selection boxes might have had someinfluence on the selected results and on the experiencedease of use. To control for these effects, however, weexplicitly asked users to rate how easy it was for themto express their feelings using the vocabulary provided,rather than how easy the application as a whole wasto use.They further report that this paradigm cannot easilybe used to compare different ontologies unless a singleapplication can be reused with different pluggable ontolo-gies. It was not our objective in the present study tocompare different ontologies in the emotional domain,but we do believe that the application we have designedwould be able to accommodate different sources of vocab-ulary should such an experiment be conducted in thefuture.One clear limitation of our study is that only the vocab-ulary offered in the ontology has been evaluated, ratherthan the logical or hierarchical structure of the EM. Ourfindings thus only relate to the vocabulary component ofthe ontology, and a separate evaluation would be neededfor the other aspects of the ontology. However, the studydoes show that the EM largely contained the vocabularynecessary for the participants to articulate their emotions.Another limitation is that the study only evaluates theusers self-assessed reports of their emotions, that is, thestudy makes no attempt to calibrate the reporting ofemotions that the users provided with any objective psy-chometric evaluation of the emotions they were actuallyundergoing at that time. Our findings are thus only rele-vant to the self-reporting of self-assessed emotions, andnot to the objective measuring of emotions as mightbe required in clinical settings. As the evaluation wasintended to reveal whether the EM was sufficient to allowparticipants to self-report their emotional response, asopposed to revealing the true state of emotions at theICBO 2012 conference, we do not see this as a significantissue in this evaluation.A further limitation of the environment in which ourstudy was conducted was that internet difficulties andpower failures might have prevented some participantsfrom recording their emotion effectively and the associ-ated rating at the time that they would have liked. Wedid allow for post-hoc capture of tags to get aroundthis problem, and we did see a spike of captures late atnight (around 11pm) that was probably explained by thisphenomenon. We did not optimise the appplication foruse on mobile devices; this could have eased use andincreased the number of users. Also, we were asking alot of the conference attenders  many responses formany talks.Finally, the emotional words from the EM that wereused in the experiment themselves had implicit strengthswhich were not exposed in the analysis or correlated withthe stated strength of response. This information was notavailable as an annotation in the EM ontology; however, itmay be added in a future release.ConclusionWe find that the vocabulary provided by the leaves of theEmotion Ontology is suitable for use for the self-assessedself-reporting of emotional experiences in a conferencesetting. We evaluated the EM in the wild in this set-ting and found that the EM can be used to discriminatebetween and articulate emotional experiences of audi-ence members. We have released the EmOntoTag toolas open source for community adoption, adaptation andreuse in similar scenarios in future projects. We under-stand this experiment as a contribution to the ontologyevaluation domain from a perspective of use-case drivenHastings et al. Journal of Biomedical Semantics 2014, 5:38 Page 16 of 17http://www.jbiomedsem.com/content/5/1/38evaluation, and we have been able to enhance the ontol-ogy based on the feedback and comments we receivedduring the course of this experiment. Ontology evaluationis recognised to be a hard task that is not often per-formed and this work is a contribution of a formal exper-iment designed to evaluate the ability of an ontologysvocabulary to make the distinctions necessary in a fieldof interest.Future work will involve further adapting the ontologyto allow more comprehensive descriptions of the con-text and causes for a particular emotional experience, andevaluating the ontology for use in the self-reporting ofemotions in more clinical contexts, e.g. to facilitate emo-tional monitoring in the treatment of patients with mooddisorders. There are a broad range of application sce-narios in which the self-reporting of emotions might berelevant  almost any situation that has human involve-ment  and the EM is a candidate vocabulary for suchapplications.Additional fileAdditional file 1: Sample user handout for ICBO conferenceparticipants. Conference participants were given a handout together withtheir conference pack that detailed the experiment that we wereconducting and assigned each user a different, unique, random accesscode for the system. An example of these handouts is included as asupplementary file.Competing interestsThe authors declare that they have no competing interests.Authors contributionsRS conceived and supervised the project, and negotiated permission for theexperiment to be conducted during the ICBO conference. CC developed thetool that was used in the experiment. AB, RS and JH analysed the results. CJhelped with the interpretation of results. JH provided guidance on the UIdevelopment of the tool used, and drafted the initial version of themanuscript. All authors contributed to, and have read and approved, the finalversion of the paper.AcknowledgementsWe thank the ICBO scientific organisers, in particular Stefan Schulz and RonaldCornet, and all participating ICBO presenters for agreeing to allow theexperiment to be conducted during the conference. We are grateful to allparticipants who recorded their emotions and rated the ease of using theontology for this task during the conference. We thank Aitor Apaolaza Llorentefor assistance in creating one of the Figures in the manuscript. Furthermore,we acknowledge support from EPSRC grant EP/C536444/1 and EPSRC grantEP/J014176/1.Author details1Cheminformatics and Metabolism, EMBL  European Bioinformatics Institute,Wellcome Trust Genome Campus, Hinxton CB10 1SD, UK. 2Swiss Center forAffective Sciences, University of Geneva, Geneva, Switzerland. 3School ofComputer Science, University of Manchester, Oxford Road, Manchester M139PL, UK. 4Faculty of Life Sciences, University of Manchester, Oxford Road,Manchester M13 9PL, UK.Received: 10 December 2013 Accepted: 14 August 2014Published: 3 September 2014JOURNAL OFBIOMEDICAL SEMANTICSTorii et al. Journal of Biomedical Semantics 2014, 5:3http://www.jbiomedsem.com/content/5/1/3RESEARCH Open AccessDetecting concept mentions in biomedical textusing hidden Markov model: multiple concepttypes at once or one at a time?Manabu Torii1*, Kavishwar Wagholikar2 and Hongfang Liu2AbstractBackground: Identifying phrases that refer to particular concept types is a critical step in extracting informationfrom documents. Provided with annotated documents as training data, supervised machine learning can automatethis process. When building a machine learning model for this task, the model may be built to detect all typessimultaneously (all-types-at-once) or it may be built for one or a few selected types at a time (one-type- ora-few-types-at-a-time). It is of interest to investigate which strategy yields better detection performance.Results: Hidden Markov models using the different strategies were evaluated on a clinical corpus annotated withthree concept types (i2b2/VA corpus) and a biology literature corpus annotated with five concept types (JNLPBAcorpus). Ten-fold cross-validation tests were conducted and the experimental results showed that models trainedfor multiple concept types consistently yielded better performance than those trained for a single concept type.F-scores observed for the former strategies were higher than those observed for the latter by 0.9 to 2.6% on thei2b2/VA corpus and 1.4 to 10.1% on the JNLPBA corpus, depending on the target concept types. Improvedboundary detection and reduced type confusion were observed for the all-types-at-once strategy.Conclusions: The current results suggest that detection of concept phrases could be improved by simultaneouslytackling multiple concept types. This also suggests that we should annotate multiple concept types in developing anew corpus for machine learning models. Further investigation is expected to gain insights in the underlyingmechanism to achieve good performance when multiple concept types are considered.Keywords: Natural language processing, Information storage and retrieval, Data mining, Electronic health recordsBackgroundConcept mention detection is the task of identifyingphrases in documents that refer to particular concepttypes. Provided with documents annotated with conceptphrases as training data, supervised machine learningcan be used to automate concept mention detection. Inthe biological domain, sets of annotated documents havebeen developed and made publicly available over theyears [1,2]. Similarly in the clinical domain, annotatedclinical notes have been recently released to the researchcommunity through pioneering efforts [3,4]. These an-notated data sets have promoted application of machine* Correspondence: torii@udel.edu1Department of Radiology, Georgetown University Medical Center,Washington, DC, USAFull list of author information is available at the end of the article© 2014 Torii et al.; licensee BioMed Central LtdCommons Attribution License (http://creativecreproduction in any medium, provided the orlearning methods to concept mention detection in theclinical domain [5-8].When the detection task involves two or more targetconcept types, there is an option to build one machinelearning model for all types (all-types-at-once strategy)or to build multiple models each tackling one type (one-type-at-a-time strategy). The former strategy may havean advantage in exploiting dependency among concepttypes. In this work, we posed a question if these strat-egies have impacts on detection performance. We foundthis question important in two ways. First, it is useful toknow if one strategy is better than the other in terms ofthe detection performance. Second, when a new corpusis developed, the results of the current study may en-courage us to annotate additional concept types in orderto potentially enhance detection of the target concepttype. With current ongoing efforts on corpus development. This is an open access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly cited.Table 1 Descriptive statistics of the corporai2b2/VA corpus JNLPBA corpusDocuments 349 2,000Tokens 260,570 492,301Concept phrases Problem 11,968 Protein 30,269Test 7,369 DNA 9,530Treatment 8,500 Cell Type 6,710Cell Line 3,830RNA 951Torii et al. Journal of Biomedical Semantics 2014, 5:3 Page 2 of 6http://www.jbiomedsem.com/content/5/1/3in the clinical domain, we believe this would be a timelyquestion to pose.In this study, we used two kinds of annotated corpora.The one is a clinical corpus released in the 2010 i2b2/VA natural language processing (NLP) shared-taskchallenge [4] and the other is a biological literaturecorpus released in the Joint Workshop on NaturalLanguage Processing in Biomedicine and its Applica-tions (JNLPBA) [9]. The two corpora are different interms of writing styles as well as concepts presented andannotated, while they share challenges in identifying bio-medical concepts, such as difficulty in detecting propernames that may not have initial capital letters and inprocessing ambiguous acronyms and abbreviations. Thebest performing system in the i2b2/VA challenge andthat in the JNLPBA workshop achieved, respectively,F-scores of 0.852 and 0.726 on the evaluation corpora.These and the other top-ranked systems in the work-shops used various machine learning methods, includ-ing Hidden Markov Model (HMM), Support VectorMachine (SVM), and Conditional Random Field (CRF),along with various techniques and resources. Our inter-est in this work is to compare all-type-at-once and one-type- (or a-few-types-) at-a-time strategies, and not toaim for the best performance on these corpora by ex-ploring rich domain features. To focus on this goal, weemployed HMM that uses features internal to input text.MethodsExperimental designOne strategy we considered in building a concept detec-tion system was to train one machine learning modelthat covered all concept types. An alternative strategytested was to build separate models for different concepttypes. An HMM program implemented in the LingPipesuite [10] was used to train these models. Detection per-formance was measured with F-score, the harmonicmean of precision (the number of correctly extractedphrases divided by the number of all extracted phrases)and recall (the number of correctly extracted phrases di-vided by the number of all phrases to be extracted). Weconducted 10-fold cross-validation tests and calculatedthe average F-score.DataDescriptive statistics of the two data sets used in our ex-periments are shown in Table 1. The first data set usedwas a training corpus in the 2010 i2b2/VA NLP shared-task challenge [4]. This data set was made availablethrough our participation in the shared-task challengeand, hence, no additional ethical approval was requiredfor the current study. This corpus consists of 349 clinicaldocuments, including 268 discharged summaries fromthree institutions and 81 progress notes from oneinstitution. The documents were manually annotatedwith three concept types: Problem, Test, and Treatment.These annotations (spans of concept phrases) do notoverlap each other in text, except for eight annotationsthat we excluded in the current study.The second data set used was a training corpus of theBio-Entity Recognition Task in the JNLPBA workshop,which was publicly available online. The corpus consistsof 2,000 abstracts of biology research articles retrievedfrom the MEDLINE database using the search terms(Medical Subject Headings) of human, blood cells andtranscription factors [9]. It is the same document set asthe GENIA version 3.02 corpus, but the thirty six con-cept types originally annotated in the corpus were sim-plified to five types for the shared-task workshop:Protein, DNA, Cell Type, Cell Line, and RNA. There isno overlap among annotated concept phrases in thiscorpus.Detection strategiesOne or a few concept types at a timeIn this strategy, independent detection tasks were as-sumed for subsets of the target concept types. For eachsubtask, the BIO notation was used [11]. Each token inthe corpus was assigned one of the labels, B_ConceptType,I_ConceptType, and O, representing a token being the Be-ginning of a concept phrase, Inside of a concept phrase, orOutside of a concept phrase. For example, in order to in-dicate Problem phrases in the i2b2/VA corpus, the threelabels, B_Problem, I_Problem, and O, were used.All concept types at onceIn this strategy, a single detection task was assumed forall the target concept types. For example, given the threeconcept types in the i2b2/VA corpus, one HMM modelwas built using the seven labels, B_{Problem, Treatment,Test}, I_{Problem, Test, Treatment}, and O.Machine learning methodConcept mention detection was often tackled as a se-quence labeling problem [4,9]. Input text is viewed as asequence of tokens and the task is defined as assignmentTorii et al. Journal of Biomedical Semantics 2014, 5:3 Page 3 of 6http://www.jbiomedsem.com/content/5/1/3of each token with an appropriate label to demarcatespans of tokens referring to target concept types. Weused a sequence labeling program, named CharLmRes-coringChunker, from the LingPipe suite [10,12]. Thisprogram was chosen because it exploits features internalto text and the performance is not affected by additionalexternal resources and parameters associated with them.Also, this program runs fast and it was desirable in con-ducting cross-validation tests. A model trained with thisprogram first extracts candidate concept phrases using afirst-order Hidden Markov Model (HMM). In HMM,the likelihood of a sequence of labels is calculated basedon the two types of probabilities, the transition probabil-ities and the emission probabilities, learned from thetraining data set. In the implementation of the LingPipesuite, the emission probabilities that capture the relationbetween observed words and corresponding labels arecalculated using character language models. Transitionprobabilities that capture the ordering of labels assignedto words are calculated using a bigram model. As for la-bels to demarcate phrases, instead of using BIO labelsgiven as inputs to the program, enriched BMEWO+ rep-resentation is used internally [13]. Namely, B of BIO isdivided into W (a token of a single-word concept) and B(beginning of a multi-word concept), I into M and E(Middle or End of a multi-word concept), and similarlyO into {B, M, E, W}_O, where {B, E, W}_O is further di-vided based on the type of the neighboring concept.Candidate concept phrases extracted by an HMM modelare rescored using another level of character languagemodels to identify the best candidates. We varied theTable 2 Comparison of detection performanceTPi2b2/VA Problem All-at-once 964One-at-a-time 932Test All-at-once 582One-at-a-time 551Treatment All-at-once 653One-at-a-time 625JNLPBA Protein All-at-once 2,373One-at-a-time 2,251DNA All-at-once 581One-at-a-time 527Cell Type All-at-once 496One-at-a-time 455Cell Line All-at-once 233One-at-a-time 212RNA All-at-once 36One-at-a-time 33character n-gram size in our experiments, but the ex-perimental results exhibited the same trends across thedifferent choices of the size n and they did not affect ourconclusion. Therefore, we chose to report the results forn = 50 that generally yielded good performance. In train-ing the two kinds of models involved, the model for can-didate phrase detection and that for their rescoring,eighty and twenty percent of sentences in the trainingdata were used, respectively.Results and discussionTable 2 shows the performance of HMM models trainedusing the all-types-at-once and the one-type-at-a-timestrategies. As stated in the Methods section, we con-ducted ten-fold cross-validation tests on the two corporaand the detection performance was measured with theaverage F-score. Figure 1 shows how the detection per-formance varies when a-few-types-at-a-time was em-ployed for all the three concept types annotated in thei2b2/VA corpus. As for the JNLPBA corpus that is anno-tated with five concept types, there are many combina-tions for a few types to be selected for the strategy andhence we report on selected combinations for a single tar-get type, Protein, in Figure 2. As seen in the figures as wellas in the table, for every concept type annotated in thetwo corpora, the F-score was the highest when all concepttypes were considered simultaneously, and the lowestwhen each type was tackled individually. The differencesin the F-scores were statistically significant at the 0.01alpha level using the two-tailed paired t-test. We inspectederrors in one-type-at-a-time that were correctly handledFP FN Prec. Rec. F-score267 231 0.783 0.806 0.794244 264 0.792 0.779 0.785114 153 0.835 0.791 0.813112 185 0.831 0.748 0.787139 196 0.823 0.769 0.795138 223 0.818 0.737 0.775840 653 0.739 0.784 0.761752 775 0.749 0.744 0.747270 371 0.683 0.610 0.644339 425 0.609 0.553 0.580167 174 0.748 0.740 0.744168 215 0.730 0.678 0.703102 149 0.695 0.610 0.649180 170 0.543 0.554 0.54824 59 0.594 0.383 0.46218 62 0.640 0.345 0.447Figure 1 Detection performance for the 2010 i2b2/VAchallenge corpus. The horizontal axis shows incremental sets oftypes, including the selected target type (e.g., Problem in the topfigure), and the rightmost set corresponds to the all-at-once setting.The reported F-scores are for the selected target type.Figure 2 Detection performance for the JNLPBA corpus. Thehorizontal axis shows incremental sets of types, including theselected target type, and the rightmost set corresponds to the all-at-once setting. The reported F-scores are for the selectedtarget type.Torii et al. Journal of Biomedical Semantics 2014, 5:3 Page 4 of 6http://www.jbiomedsem.com/content/5/1/3in all-types-at-once, anticipating that the latter would takeadvantage of multiple concept types to identify targetphrases. We noticed three major error patterns, and oneof them, type confusion, explicitly involves multiple con-cept types. In the following description of the error pat-terns, we use examples of the Problem type, but similarinstances were observed for the other concept types con-sidered in the experiments.Type confusionIn one-type-at-a-time, phrases not of the target type may befalsely detected as target type phrases, e.g., <Treatment(durg)> for <Treatment (procedure)> where the latterTreatment phrase was falsely detected as Problem, whenProblem alone was tackled.Boundary errorsWe observed that boundary detection was degradedin one-type-at-a-time. Such cases included simple errors,e.g., His melanomaProblem where the word His wasmissed when Problem type was tackled alone, and alsoerrors involving more complex syntactic patterns, e.g.,his <Problem> and <Problem> where the first Problemphrase (and the word his) was missed. Over extensionof boundaries was also observed for one-type-at-a-time, but majority of its boundary errors were underextension.No detectionConcept phrases correctly identified in all-types-at-oncewere sometimes totally missed in one-type-at-a-time, e.g.,The patient had no further complaintsProblem where theProblem phrase was not detected at all when Problemtype was tackled alone.In our review, type confusion was observed less thanwhat we anticipated. For example, when Problem typewas tackled alone, across ten folds, there were 42phrases falsely detected as Problem (false negatives) thatwere correctly identified as Test (8 phrases) and Treat-ment (34 phrases) when all the types were tackled simul-taneously. Meanwhile, there were 439 Problem phrasesthat were correctly identified when all the types weretackled but were not identified either partially (199 casesTable 4 Time to train and apply HMM models on thei2b2/VA and JNLPBA corpora1A set of types considered Training(sec)Application(sec)i2b2 Problem, Test, Treatment 619 42Problem, Treatment 763 41Problem, Test 879 42Problem 1,117 43JNLPBA Protein, DNA, Cell Type, Cell line, RNA 3,010 88Protein, DNA, Cell Type, Cell line 3,812 92Protein, DNA, Cell Type 4,292 98Protein, RNA 4,694 100Protein 4,763 981The experiments were conducted on a server with six-core AMD Opteron2.8 GHz processors running CentOS 2.6. The reported times are the average often runs in ten-fold cross-validation.Torii et al. Journal of Biomedical Semantics 2014, 5:3 Page 5 of 6http://www.jbiomedsem.com/content/5/1/3of boundary errors) or fully (240 cases of no detection)when Problem type was tackled alone. Note, however,counting and interpretation of such error types involvessubtlety when more closely relevant concept types aredensely annotated as in the JNLPBA corpus becauseboundary errors and type confusion errors coincide fre-quently. We summarize the numbers of error instanceson the i2b2/VA corpus in Table 3. We initially expectedthat different outputs would be observed among casesinvolving different concept types, e.g., <Test> demon-strated <Problem>, where we might imagine that therecognition of the Test phrase affects that of the Prob-lem phrase or vice versa. We, however, encounteredsuch instances rarely, e.g., <Test> revealed <Problem>and <Test> showed <Problem>, in which the Problemphrases were not detected when Problem alone wastackled. The detection mechanism in the all-concept-types-at-once strategy needs to be examined to under-stand the advantage it has.In selecting these detection strategies, another im-portant consideration is the time to train and apply de-tection models. As shown in Table 4, it took more timeto train a model using the one-type-at-a-time strategy.Training of an HMM model does not requireoptimization unlike other popular machine learningmethods, such as SVM and CRF, and the increase in thenumber of target types may not incur extra trainingtime. However, reduction in the training time for all-types-at-once was not expected. That may be attributedto smaller per-type data structures used in all-types-at-once, compared to larger per-type data structures inone-type-at-a-time. The size of the model file wassmaller for all-concept-types-at-once, compared to thatfor one-type-at-a-time, e.g., 159 MB for all-types-at-once and 255 MB for Problem in one run of ten-foldcross-validation.Review of individual errors and analysis of run timemade us pay attention to the implementation of theHMM program and the impacts of model parameters in-volved, such as pruning of n-grams in the model andsmoothing of probabilities. We explored a wide range ofn-gram sizes to test if the choice of the tagging strategy,but it was difficult to explore all the parameters simul-taneously, e.g., the n-gram size, the smoothing param-eter, and the pruning parameter. Further investigation isrequired to gain insight in the combination of differentTable 3 Additional errors introduced in one-type-at-a-timeon the i2b2/VA corpusType confusion Boundary error No detectionProblem 42 199 244Test 50 92 299Treatment 47 266 113parameters, as well as the use of different machine learn-ing paradigms other than HMM.ConclusionsIn this study, we compared all-types-at-once and one-type-at-a-time strategies in applying HMM taggers on aclinical corpus released in the 2010 i2b2/VA NLP chal-lenge workshop and a biological literature corpus releasedin the JNLPBA workshop. We also tested a-few-types-at-a-time in building a model. The experimental result showsthat tackling multiple concept types at once could im-prove concept mention detection performance. Whenbuilding a new corpus, which has become an imminentagenda particularly in the clinical domain, we should con-sider annotating multiple concept types. The current re-sults are limited to one machine learning method, butnotably the best performing systems in the i2b2/VA chal-lenge and the NLPBA workshop employed all-types-at-once for Semi-Markov CRF [14] and HMM with SVM[15]. Further investigation is expected to test variousmachine learning methods for these different detectionstrategies.Availability of supporting dataThe clinical corpus used in this research was a trainingdata set in the Fourth i2b2/VA Shared-Task and Work-shop Challenges in Natural Language Processing forClinical Data. Information of this data set is found athttps://www.i2b2.org/NLP/Relations/.The biology literature corpus used in this research wasa training data set for the Bio-Entity Recognition Task inthe Joint Workshop on Natural Language Processing inBiomedicine and its Applications. The data set is avail-able at http://www.nactem.ac.uk/tsujii/GENIA/ERtask/report.html.Torii et al. Journal of Biomedical Semantics 2014, 5:3 Page 6 of 6http://www.jbiomedsem.com/content/5/1/3Abbreviationsi2b2: Informatics for integrating biology and the bedside; CRF: Conditionalrandom field; FN: False negative; FP: False positive; HMM: Hidden MarkovModel; JNLPBA: Joint Workshop on Natural Language Processing inBiomedicine and its Applications; NLP: Natural Language Processing;SVM: Support Vector Machine; TP: True positive.Competing interestsThe authors declare that they have no competing interests.Authors contributionsHL conceived the original idea of the study and coordinated the resourcesfor the experiments. MT designed the experiments and carried them outwith KW. All authors participated in the analysis of the experimental results.MT drafted the manuscript, and KW and HL significantly revised it. Allauthors read and approved the final manuscript.AcknowledgementsThis article has been published as part of thematic series Semantic Miningof Languages in Biology and Medicine of Journal of Biomedical Semantics.An early version of this paper was presented at the Fourth InternationalSymposium on Languages in Biology and Medicine (LBM 2011), held inSingapore in 2011. De-identified clinical records used in this research wereprovided by the i2b2 National Center for Biomedical Computing funded byU54LM008748 and were originally prepared for the Shared Tasks forChallenges in NLP for Clinical Data organized by Dr. Ozlem Uzuner, i2b2 andSUNY. The authors also thank all the other researchers and developers whomade their software resources and annotated corpora available to theresearch community. The authors acknowledge the funding from NationalScience Foundation (ABI: 0845523) and the National Institute of Health(R01LM009959A1).Author details1Department of Radiology, Georgetown University Medical Center,Washington, DC, USA. 2Department of Health Sciences Research, Mayo ClinicCollege of Medicine, Rochester, MN, USA.Received: 5 September 2012 Accepted: 26 November 2013Published: 17 January 2014JOURNAL OFBIOMEDICAL SEMANTICSWu et al. Journal of Biomedical Semantics 2014, 5:32http://www.jbiomedsem.com/content/5/1/32RESEARCH Open AccessBioBenchmark Toyama 2012: an evaluation ofthe performance of triple stores on biologicaldataHongyan Wu1, Toyofumi Fujiwara2, Yasunori Yamamoto1, Jerven Bolleman3 and Atsuko Yamaguchi1*AbstractBackground: Biological databases vary enormously in size and data complexity, from small databases that contain afew million Resource Description Framework (RDF) triples to large databases that contain billions of triples. In thispaper, we evaluate whether RDF native stores can be used to meet the needs of a biological database provider. Priorevaluations have used synthetic data with a limited database size. For example, the largest BSBM benchmark uses 1billion synthetic e-commerce knowledge RDF triples on a single node. However, real world biological data differs fromthe simple synthetic data much. It is difficult to determine whether the synthetic e-commerce data is efficient enoughto represent biological databases. Therefore, for this evaluation, we used five real data sets from biological databases.Results: We evaluated five triple stores, 4store, Bigdata, Mulgara, Virtuoso, and OWLIM-SE, with five biological datasets, Cell Cycle Ontology, Allie, PDBj, UniProt, and DDBJ, ranging in size from approximately 10million to 8 billion triples.For each database, we loaded all the data into our single node and prepared the database for use in a classical datawarehouse scenario. Then, we ran a series of SPARQL queries against each endpoint and recorded the execution timeand the accuracy of the query response.Conclusions: Our paper shows that with appropriate configuration Virtuoso and OWLIM-SE can satisfy the basicrequirements to load and query biological data less than 8 billion or so on a single node, for the simultaneous accessof 64 clients.OWLIM-SE performs best for databases with approximately 11 million triples; For data sets that contain 94 million and590 million triples, OWLIM-SE and Virtuoso perform best. They do not show overwhelming advantage over eachother; For data over 4 billion Virtuoso works best.4store performs well on small data sets with limited features when the number of triples is less than 100 million, andour test shows its scalability is poor; Bigdata demonstrates average performance and is a good open source triplestore for middle-sized (500 million or so) data set; Mulgara shows a little of fragility.BackgroundSemantic Web encodes information from theWorldWideWeb in a machine-readable syntax to make web infor-mation automatically recognizable and processable bycomputers [1]. Semantic Web, which is about commonformats for integration and combination of data drawnfrom diverse sources [2], facilitates the integration of het-erogeneous data on the World Wide Web by applying*Correspondence: atsuko@dbcls.rois.ac.jp1Database Center for Life Science, Research Organization of Information andSystems, 178-4-4 Wakashiba, Kashiwa, Chiba 277-0871, JapanFull list of author information is available at the end of the articleformal ontologies to specify the semantics of the dataexplicitly [3]. Semantic Web has unleashed a revolution ofdata publication and interconnection [4].Semantic Web has gained significance in the life sci-ences. Due to the success of the Human Genome Project(HGP) [5] and high-throughput sequencing, a large quan-tity of biological data is available to the scientific commu-nity via the Internet [4]. One challenge posed by biologicaldatabases is the diversity of data types, which includesequence (e.g., NCBIs GenBank [6]), microarray geneexpression (e.g., SMD [7] and GEO [8]), pathway (e.g.,BIND [9]), and proteomic data (e.g., PeptideAtlas [10]).These diverse data types are highly heterogeneous both© 2014 Wu et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited.Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 2 of 11http://www.jbiomedsem.com/content/5/1/32in structure and semantics [11]. However, the complex-ity of a disease cannot be explained without referring tomultiple biological databases. For example, to understandParkinsons disease requires both neuroscience informa-tion as well as mapping of gene expression across thewhole brain [12,13]. Semantic Web provides a way tointegrate heterogeneous data source.Life and health science communities [14] have maderemarkable progress as early adopters of Semantic Webtechnologies [15]. For example, the UniProt knowledge-base [16] is one of the core public databases in the lifesciences. UniProt connects more than 150 molecular biol-ogy and chemoinformatics databases and integrates, inter-prets, and standardizes data from numerous resourcesto achieve the most comprehensive catalogue of pro-tein sequences and functional annotations. As anotherexample, the Protein Data Bank Japan(PDBj) [17] acceptsand processes PDB entries that are deposited mainlyfrom Asian and Oceanic researchers and maintains acentralized archive of macromolecular structures in col-laboration with other wwPDB [18] members, includingthe RCSB-PDB [19], the BMRB [20] in the US, and thePDBe [21] in Europe.The popularity of Semantic Web has accelerated therapid development of one of its core techniques, the triplestore. A triple store [22] is designed to store and retrievetriples, which is a statement relating one object to another.This paper evaluates the performance of five native triplestores on biological data.Our evaluation was motivated by a project that is sup-ported by the Japan Science and Technology Agency tointegrate data in the life sciences. Our aim is to evaluatewhether RDF native stores can meet the needs of a biolog-ical database provider. Existing benchmarks, such as theLehigh University Benchmark (LUBM [23]) and the BerlinSPARQL Benchmark (BSBM [24]), use a data generator toproduce synthetic e-commerce knowledge data, and thelargest database on a single machine generated by sucha data generator includes 1 billion triples. However, realworld biological data differs from the simple syntheticdata much. The UniProt data has 164 owl classes anduses more than 180 properties, while SP2Bench [25] usesonly 23 properties, and BSBM [24] uses a similar numberof properties and only 8 classes. Due to one RDF tripleincluding only one property, 180 properties may theoreti-cally need the times of join over 180. This means that boththe graph and queries in UniProt are significantly differentin form to the generated data in either SP2Bench or BSBM.In addition each instance in RDF may differ much fromeach other even in the same class, which makes RDF flexibleto express heterogeneous data, and therefore to pick up aset of instances covering all 180 properties and 164 classeswill take a lot of effort. It is quite difficult to guarantee theconclusions drawn from synthetic benchmarks or otherfields are applicable to biological data. The biological databenchmark, Cell Cycle Ontology [26] uses real biologi-cal data. However, it includes only 10 million triples. Weused five groups of real biological data set ranging from10 million to 8 billion to make sure that the data was scal-able and variable enough. Due to hardware requirementof running a datastore of the UniProt and DDBJ size thereare few if any dependable public benchmark results i.e.fully describing the disk system and software used. Thereare no reports for single node installations with datasetsizes of more than 1 billion nodes. Our target is to verifyapplicability of a triple store for biological databases.For this evaluation, we used biological databases, CellCycle Ontology, Allie, PDBj, UniProt, and DDBJ con-taining as many as 8 billion triples. Biological databasesare also characterized by diverse and sparse data, whichmay impact performance. We evaluated the load andquery costs of five popular triple stores: 4store, Bigdata,110100100010000Cell CycleOntologyAllie PDBj UniProt DDBJMillionlogFigure 1 The size of the data sets. The five biological data sets that were used in our evaluation, with sizes ranging from 11 million toapproximately 8 billion triples.Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 3 of 11http://www.jbiomedsem.com/content/5/1/32Table 1 The loading cost for each triple storeTriple store Cell Cycle Allie PDBj UniProt DDBJOntologyOWLIM-SE (min) 3 22 140 3770 7750Virtuoso (min) 4 47 92 3508 47594store (min) 2 12 4834 X XBigdata (min) 3 272 1158 X XMulgara (min) 10 86 X X XMulgara, Virtuoso, and OWLIM-SE. To the best of ourknowledge, we evaluated the largest scale of real biologicaldata possible on a single node.MethodsTriple storeWe selected five native triple stores. Three of them wererecommended by the Bioinformaticians in the interna-tional symposium Biohackathon 2011, who had usedor tested these triple stores for their biological data.4store was used in the Cell Cycle Ontology [26]. Mul-gara was used as an internal triple store in DDBj.OWLIM-SE has been applied as UniProt triple store.Virtuoso showed good performance in BSBM and DBpe-dia SPARQL Benchmark. Bigdata, a complete free opensource triple, performed averagely well in BSBM and sup-ported most of inference functions and could run inboth single node and cluster mode. It could be a poten-tially good candidate to customize ones own triple store.Neither Jena TDB nor Jena SDB showed attractive per-formance in [26], in which both of them worked worsethan 4store and Virtuoso. Sesame showed bad load perfor-mance in BSBM Version 1. We evaluated the triple storesusing their newest versions as of June 30, 2012.4store4store [27,28] is a RDF/SPARQL store that is written inC and designed to run on UNIX-based systems. 4storecan be run on a single machine or networked clusters. Weevaluated 4store version 1.1.4.BigdataBigdata [29] is designed as a distributed database archi-tecture that runs on clusters of hundreds to thousands ofcommodity machines. However, Bigdata can also run inhigh-performance single-server mode. Bigdata supportsRDFS and limited OWL inference. Bigdata is open-sourcesoftware that is written in Java. We evaluated versionRWSTORE_1_1_0.OWLIM-SEOWLIM-SE [30,31] is a member of the OWLIM fam-ily (OWLIM-Lite, OWLIM-SE, OWLIM-Enterprise, andOWLIM on Amazon AWS), which provides native RDFengines that are implemented in Java and deliver full per-formance through both Sesame and Jena. Beginning withversion 4.3, OWLIM-SE supports SPARQL 1.1 Federation.OWLIM-SE also supports the semantics of RDFS, OWL2 RL, and OWL 2 QL. OWLIM-SE is available by com-mercial license only. We evaluated OWLIM-SE version5.1.5269.MulgaraMulgara [32] is an open-source triple store that is writ-ten in Java. Mulgara provides a SQL-like language shell,iTQL (Interactive Tucana Query Language), to query andupdate Mulgara databases. Mulgara supports RDFS andOWL inference. In addition, Mulgara also provides aSPARQL query parser and query engine. We evaluatedMulgara version V2.1.13.VirtuosoVirtuoso [33,34] provides a triple storage solution for RDFon RDBMS platforms. Virtuoso is a multi-purpose dataserver that supports RDBMS, RDF, and XML. Virtuosooffers stored procedures to load RDFXML, ntriples, and110100100010000Cell Cycle OntologyAllie PDBj UniProt DDBJminsOWLIM-SEVirtuoso4storeBigdataMulgaralogFigure 2 The loading cost of each triple store. The loading cost for each triple store for each data set. A missing value indicates that we failed toload the data set.Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 4 of 11http://www.jbiomedsem.com/content/5/1/32Table 2 The space cost to load the data for each triple storeTriple store Cell Cycle Allie PDBj UniProt DDBJOntologyOWLIM-SE 3.7G 8.2G 27G 213G 513GVirtuoso 0.84G 6.4G 30G 308G 538G4store 2.2G 14.7G 66G X XBigdata 0.78G 6.2G 34G X XMulgara 2.4G 15.8G X X Xcompressed triples. Virtuoso also supports SPARQL aswell as limited RDFS and OWL inference. Virtuoso can berun on both standalone and clusteredmachines. The stan-dalone triple store server is available through both opensource and commercial licensing. We evaluated Virtuosoversion 6.4 commercial because we found some bugs inthe open source version.Data setWe chose five typical biological data sets to evaluate. Thenumber of triples in these data sets ranging from 10 mil-lion to 8 billion. The data were available as either a setof large files, such as uniprot.rdf.gz, uniparc.rdf.gz, anduniref.rdf.gz in the UniProt data set, or a set of small files,e.g., 77,878 files in the PDBj data set. Figure 1 shows thedata size for each data set. Their formats and downloadaddresses are as follows:Cell Cycle Ontology [26]: .rdf format, 11,315,866 trip-les, from http://www.semantic-systems-biology.org/. Wedownloaded the data on December 21, 2011.Allie [35,36]: .n3 format, 94,420,989 triples, fromftp://ftp.dbcls.jp/allie/. We used the data published onDecember 12, 2011.PDBj [37]: .rdf.gz format, 589,987,335 triples, 77,878files, from ftp://ftp.pdbj.org/XML/rdf/. We downloadedthe data on December 19, 2011.UniProt [38]: .rdf.gz format, 4,025,881,829 triples,including 3 larger files, uniprot.rdf.gz, uniparc.rdf.gz, anduniref.rdf.gz, and 7 smaller files, including citations.rdf.gz,enzyme.rdf.gz, journals.rdf.gz, etc. from ftp://ftp.uniprot.org/pub/databases/uniprot/. We used the version thatwas released in November 2011.DDBJ [39,40]: .rdf.gz format, 7,902,743,055 triples, 330files, from ftp://ftp.ddbj.nig.ac.jp/ddbj_database/ddbj/.We downloaded the data on December 20th, 2011.SPARQL QueryThe query use cases we used in this study were designedbased on the daily usage of the data set. These usecases reflected the main search functions in the websiteof each data set, http://www.semantic-systems-biology.org/biogateway/querying for Cell Cycle Ontology, http://allie.dbcls.jp/ for Allie, http://beta.sparql.uniprot.org/ forUniProt, http://legacy.pdbj.org/index.html for PDBj, andhttp://www.ddbj.nig.ac.jp/searches-e.html for DDBJ.The SPARQL queries in our benchmark includedqueries aimed at retrieving one record as well as largerresult sets. Our queries included as many as 11 joins. Dif-ferent types of queries have a large impact on the querystore performance. The same query written in two dif-ferent ways can produce radically different query times.In addition, the designed queries considered the perfor-mance of many functions, including join, orderby, filter,distinct, union, optional, count, limit, and offset. SectionAdditional file 1SPARQL Query shows the detailedqueries that we tested.BenchmarkLoad timeWe searched for the best performance for each triplestore. We imported the data with default parametersas well as several empirically improved settings andidentified the best configuration (please see the sectionTable 3 The queries for Cell Cycle OntologyEndpoint case1 case2 case3 case4 case5 case6 case7 case8 case9 case10OWLIM-SE (ms) 121 9 2740 5 149 1722 3 39 25 1Virtuoso (ms) 24 2 23280 3 42500 13073 5 7562 41 24store (ms) 56 18 1236 13 33 64 22 67 2035 7Bigdata (ms) 282 35 3247 13 52 3320 11 93 47 10Mulgara (ms) 1294 20 2207 9 343 2325 32 58 33 4Endpoint case11 case12 case13 case14 case15 case16 case17 case18 case19OWLIM-SE (ms) 6 47 2 1 52779 7 4 24 17Virtuoso (ms) 120 19 5 1 56058 46 15 16 167214store (ms) 6 1563 8 7 X X X X 15Bigdata (ms) 20 27 5 6 18126 X X X 30Mulgara (ms) 14 X 9 6 X X X X 38Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 5 of 11http://www.jbiomedsem.com/content/5/1/32Figure 3 The results of the Cell Cycle Ontology query evaluation. The detailed query results for all 19 queries that were submitted to the CellCycle Ontology database for each triple store. For queries 16, 17, and 18, only the performance data for OWLIM-SE and Virtuoso are reportedbecause the other triple stores failed to execute these queries.Additional file 2Configuration for our optimal con-figuration). We tested each triple store with the bestconfiguration twice and reported the load time as the aver-age cost over the two tests. Every time we cleared the filesystem memory cache, deleted the previous database andthen loaded the data on an empty store.Disk space requirementThe disk space requirement is the total disk storage thatis used to load the data set for each triple store. Wereport the disk space requirement as the size of the wholedirectory that was used by the data repository.Query response timeWe executed the whole query sequence for every triplestore and recorded the query response time. We did thisfive times. Considering some unsteady factor (such as thesystem cache situation) may incur a higher query responsetime cost, we removed the highest one and reported thequery response time as the average cost of the remainingfour queries. In this paper, we present only the averagecost; details about the five time costs for each triple storecan be found at our website [41].To evaluate simultaneous executions with multi-clients,we sequentially picked up the queries successfully exe-cuted by all the tested triple stores (e.g., we used the 14queries without case 12, 15, 16, 17, 18 in Cell Cycle Ontol-ogy data set) to form five query mixes, and then executeeach query mix five times with 1, 4, 8, 64 clients, respec-tively, for each data set and triple store.Wemeasured theirtime cost.Query soundnessWe checked whether the triple store was able to returnquery results with the default query setting. For a querythat neither gave a result nor provided an error messagein one hour, we would report it failed the query. If aquery failed, we reported the unsupported clause or errormessage. In addition, for a query with limit predicate,we checked whether the demanding or maximum size wasreturned. For queries asking for returning all the results,we examined the result size of each triple store. If theresult size that some triple store returned was smaller, wetuned its configuration and performed the query again totry to return more results until its maximum results werereturned.EnvironmentOur evaluation focused on the data store on a singlemachine and a single end-user query. We used an Intel(R)Xeon(R) CPU E5649@ 2.53GHz with a 12 core hyper-threaded system (24 virtual cores), 64G of RAM, three 2TSCSI disk storages, an ext3 file system, CentOS release 5.7,and JDK 1.6.0_26.ResultsLoad timeWe conducted performance tuning to determine the bestperformance for every triple store. We found that Virtu-oso having one stream per core to load the data was agood performance point which would keep all parts ofthe system busy. The results showed that Virtuoso hadbetter performance with parallel loading on a multi-coremachine when a set of small files with multiple threadswas uploaded. Therefore, to test UniProt loading, we usedTable 4 The queries for AllieEndpoint case1 case2 case3 case4 case5OWLIM-SE(ms) 136 1530 1091 31 78942Virtuoso (ms) 23 1413 152 95 272994store (ms) X 217 X X 65128Bigdata(ms) 365 690 1779 98 38523Mulgara(ms) 373 121 X X XWu et al. Journal of Biomedical Semantics 2014, 5:32 Page 6 of 11http://www.jbiomedsem.com/content/5/1/32Figure 4 The results of the Allie query evaluation. The detailed query results for all five queries that were submitted to the Allie database foreach triple store. A missing value indicates that the query failed for that triple store.the Virtuoso procedure language to split all of the filesinto a set of smaller files that were each composed of200,000 triples, and we used 12 loading threads in ourtest. This splitting cost an additional 17 hours of runtime(The last load time for Virtuoso in Table 1 includes this 17hours). For some triple stores, such as OWLIM-SE, per-formance was improved by adjusting the JVM parameters(e.g., -Xmx, -Xms, etc.). However, other triple stores, suchas Mulgara, were not influenced by adjusting the JVMparameters. For details about our loading approaches,please see our website [41].Table 1 shows that OWLIM-SE and Virtuoso are ableto finish all of the loading tasks. The X mark in thetable indicates a failure to load the data set. Using 4store,the time cost to load approximately 100 million triplesin the Allie data set to 500 million triples in the PDBjdata set increased 400-fold. Therefore, we did not eval-uate the performance of 4store on UniProt or DDBJbecause of its poor scalability. Mulgara failed to loadPDBj with the error message Unable to load file: Ille-gal character ABSA_(A^2); Unexpected XAExceptionerror occurred when loading UniProt and DDBJ. Bigdatahad difficulty in loading all of the UniProt data, such thatthe loading process almost stopped when the loaded triplenumber exceeded 3.5 billion. However, Bigdata was able toload the data easily when the triple number was less than3 billion. Figure 2 illustrates the loading cost for each dataset.In addition, the format (such as the data set is composedof a big file or a set of small files) of the data set affectedthe performance of the triple store. The time to load PDBj,UniProt, and DDBJ was less for Virtuoso compared withOWLIM-SE, while the time to load Cell Cycle Ontologyand Allie was greater for Virtuoso than OWLIM-SE. Thisdifference may be partly due to the format of the data;the three former data sets were composed of many smallfiles. The triple number of DDBJ was nearly two timesthat of UniProt (7.9 billion compared with 4.0 billion,respectively). However, using Virtuoso, the loading costfor DDBJ was much less than two times the loading costfor UniProt. Virtuoso demonstrated good performancewhen loading multiple small files with multiple threads.Our experiment on OWLIM-SE 4.3 [42] demonstratedthat OWLIM-SE 4.3 took less time to load DDBJ com-pared with UniProt, which also suggests that the formatof the data set (e.g., multiple small files) affects the per-formance of a triple store. The difference can partly comefrom the data sets themselves since we use five differentreal data sets.Disk space requirementTable 2 shows the space that was consumed when load-ing the data set for every triple store. When loading eachdata set we cleared the database and then loaded the dataon an empty store. Therefore the presented space is justwhat the data set occupied. The experiment shows that thespace used by OWLIM-SE increased slowest as the datasize increased. 4store, Bigdata andMulgara were relativelypoor.Table 5 Query for PDBjEndpoint case1 case2 case3 case4OWLIM-SE(ms) 72 2 162 7Virtuoso (ms) 147 2 2 1384store (ms) 1025 1274 131 1524Bigdata(ms) 190 14 35 54Mulgara(ms) X X X XWu et al. Journal of Biomedical Semantics 2014, 5:32 Page 7 of 11http://www.jbiomedsem.com/content/5/1/32110100100010000case1 case2 case3 case4msOWLIM SEVirtuoso4storeBigdataMulgaralogFigure 5 The results of the PDBj query evaluation. The detailed query results for all 4 queries that were submitted to the PDBj database for eachtriple store. The performance of Mulgara is not reported because Mulgara failed to load PDBj.Query response time and query soundnessCell cycle ontologyTable 3 shows the query performance for the Cell CycleOntology data set. The X mark indicates a query thatfailed, and boldface shows the fastest response for eachquery. It is the same to the following tables. Both Virtuosoand OWLIM-SE demonstrated sound query ability. BothVirtuoso and OWLIM-SE completed all of the queries.The query soundness of 4store depended on the set-ting of the parameter Softlimit. In the first query, withSoftlimit equal to 5000, 4store was able to return all 53results. However, when Softlimit was equal to 1000, 4storereturned only 17 results. In addition, 4store, Bigdata andMulgara could not support the count() function in queries16, 17, and 18. Mulgara returned a zero result for query 15(the result size should be 7354) and an Unknown Con-straintExpression exception for query 12. 4store gave noresponse to query 15 in one hour.Figure 3 shows the corresponding bar chart for theCell Cycle Ontology results. For this smallest data set,Virtuoso responded faster than other triple stores forsome queries but was slowest for other queries, such asquery 5 and query 19. We say that Virtuoso has worstcases. OWLIM-SE performed best on this data set andhad no worst cases. Bigdata had average performance onthis data set. Although 4store had poor query sound-ness, the performance of 4store was distinctly better forsome cases, such as query 5 and query 6. Mulgara per-formed the worst of all of the triple stores on this dataset.AllieTable 4 shows the query performance for the Allie data set.Virtuoso, Bigdata, and OWLIM-SE demonstrated soundquery ability on this data set. 4store did not support thelang() function in queries 1, 3, and 4. Mulgara was unableto support the arbitrarily complexORDER BY clause.The triple number of this data set nearly 10-fold higherthan that of the Cell Cycle Ontology data set. Figure 4shows that Virtuoso and OWLIM-SE performed betterthan the other triple stores. For this data set, Virtuoso hadno worst cases. Bigdata had average performance on thisdata set. 4store was limited but performed well on query 2.PDBjVirtuoso andOWLIM-SE performed better than the othertriple stores on the PDBj data set, as shown in Table 5and Figure 5. However, neither of them had a signifi-cant advantage. 4store demonstrated sound query abil-ity, but the query performance on this data set was theworst of the five triple stores. Bigdata again displayedaverage performance. Mulgara failed to load the PDBjdata set, and therefore we could not present its queryperformance.Table 6 the queries for UniProtEndpoint case1 case2 case3 case4 case5 case6 case7 case8 case9 case10OWLIM-SE (ms) 931 1920 2627 142 61 89586 86380 674 994 1053Virtuoso(ms) 51 95 114 2 7 2206 34916 413 605 652Endpoint case11 case12 case13 case14 case15 case16 case17 case18OWLIM-SE (ms) 50 10 9 7 15037 32055 2818 8548Virtuoso (ms) 53 4 289 269 10631 9052 2 76Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 8 of 11http://www.jbiomedsem.com/content/5/1/32Table 7 The queries for DDBJEndpoint case1 case2 case3 case4 case5 case6 case7 case8 case9 case10OWLIM-SE (ms) 4783 4528 4867 12 25 4 470 1078 22 1Virtuoso(ms) 226 218 418 56 7 98 5 4 7 1UniProt and DDBJFor the two largest data sets, UniProt and DDBJ, Virtuosoperformed the best.We were able to completely load these two data sets onlywith OWLIM-SE and Virtuoso. Table 6 and Table 7 reportthat both Virtuoso and OWLIM-SE performed well on theUniProt and DDBJ data sets, respectively. However, Vir-tuoso performed better as the triple number increased.Figure 6 and Figure 7 are the corresponding bar charts forthe results of the UniProt andDDBJ data sets, respectively.Simultaneous executionTable 8 shows simultaneous executions withmulti-clients,1, 4, 8, and 64 clients respectively. Mulgara reported theerror Interrupted while waiting to acquire lock whendoing queries with over 2 clients.We only evaluated 4storewith Cell Cycle Ontology and Allie because it showedunsteady performance with multi-clients when data islarger. Virtuoso, OWLIM-SE and Bigdata finished thesimultaneous executions with good scalability.ConclusionsOur paper shows that with appropriate configuration Vir-tuoso and OWLIM-SE can satisfy the basic requirementsto load and query biological data less than 8 billion or soon a single node, for the simultaneous access of 64 clients.OWLIM-SE performs best for databases with approxi-mately 11 million triples, with no worst query cases; Fordata sets that contain 94 million and 590 million triples,OWLIM-SE and Virtuoso perform best in the five eval-uated triple stores, and they do not show overwhelmingadvantage over each other; For data over 4 billions Virtu-oso works best.As for other triple stores, (1) 4store performs well onsmall data sets (e.g. Cell Cycle Ontology) with limitedfeatures, and our test shows its scalability is poor; (2) Big-data demonstrates average performance on both loadingand querying and may be a good open source triple storefor middle-sized (500 million or so) data set; (3) Mulgarashows a little of fragility.Discussion and future workOur evaluation shows that both Virtuoso and OWLIM-SEare able to efficiently load and query data sets with up toapproximately 8 billion triples on a single machine. Thescalability of both Virtuoso and OWLIM-SE is good. Vir-tuoso has the best performance with parallel loading ona multi-core machine for sets of small files with multiplethreads. Although Virtuoso has some worst cases whenthe data set is very small, its performance improves as thenumber of triples increases. 4store performs the best onsmall data sets with limited features. The performance of4store worsens from Cell Cycle Ontology to PDBj as thesize of the data set increases, indicating that the scalabil-ity of 4store is poor. Bigdata had average performance onall data sets with acceptable loading and query costs. Big-data may therefore be a good open source triple store forsmaller data sets. BecauseMulgara failed to load several ofthe data sets that were tested, its query performance couldnot be demonstrated.Our results indicate that 4store can perform well onboth loading and querying data with limited features whenFigure 6 The results of the UniProt query evaluation. Only OWLIM-SE and Virtuoso were able to load the UniProt database.Wu et al. Journal of Biomedical Semantics 2014, 5:32 Page 9 of 11http://www.jbiomedsem.com/content/5/1/32Figure 7 The results of the DDBJ query evaluation. Only OWLIM-SE and Virtuoso were able to load the DDBJ database.the number of triples is less than 100 million. For datasets of moderate size (100million to 500million), VirtuosoandOWLIM-SE perform similarly. Of the five tested triplestores, Virtuoso performs best on data sets with severalbillion triples.The conclusions in our benchmark are basically con-sistent to BSBM when data size is less than 1 billion,however, not to all other benchmarks. Biological databenchmark [26] shows that OWLIM responded in rela-tively short time, 4store in moderate time and Virtuosowas slowest. Our benchmark shows some difference. Vir-tuoso performed best in many cases while it was slowestin some others. Our benchmark proves that Virtuoso hadgood scalability, while it could perform not well for smalldata. Another real-world data triple store benchmark [43]shows that Virtuoso was slowest to load the data. Ourbenchmark shows that Virtuoso worked faster in load-ing and querying as increasing the data size. In additionour evaluation shows that both Virtuoso and OWLIM-SEscaled well up to 8 billion in both loading and querying ona single node.Our detailed evaluation of the configurations of eachtriple store (please see the detailed configurations inAdditional file 2  Configuration for each triple store, orrefer to our website) demonstrated that the cost associ-ated with loading the data depends on multiple factors,including the server configuration (e.g., CPU, memory,hard disk, etc.), the system property (e.g., vm.swappiness,JVM, etc.), the application configuration (e.g., cachemem-ory in OWLIM-SE, etc.), and the data format and the sizeTable 8 Simultaneous executionTriple store Number of clients Cell Cycle Ontology Allie PDBj UniProt DDBJOWLIM-SE(ms)1 6,402 6,704 861 1,651,466 83,1794 8,474 13,967 1,041 1,911,144 89,6268 14,190 20,891 1,033 2,216,634 109,19564 120,126 159,211 2,286 6,058,957 442,181Virtuoso(ms)1 14,742 1,421 789 31,876 49,6244 22,459 7,189 1,168 50,953 5,2468 27,297 9,870 1,655 58,498 10,42664 194,850 55,366 8,496 905,697 35,8794store(ms)1 4,706 682 x x x4 15,825 1,413 x x x8 27,604 2,191 x x x64 237,246 15,288 x x xBigdata(ms)1 10,757 100,683 2,028 x x4 15,617 129,136 2,138 x x8 82,579 850,852 2,051 x x64 108,755 4,467,378 2,930 x xWu et al. Journal of Biomedical Semantics 2014, 5:32 Page 10 of 11http://www.jbiomedsem.com/content/5/1/32of the data set (e.g., DDBJ is nearly two times the triplesize of UniProt, but the loading cost when using Virtuosois two times less for DDBJ than UniProt, which indicatesthat the scaling is not proportional, etc.).For each database, several results were obtained byadjusting parameters that may significantly influence theperformance of each triple store. These parameters mayalso perform differently with different hardware and soft-ware platforms as well as with different data sets. Atest of all possible parameter combinations is difficultbecause some data sets, such as UniProt and DDBJ, maytake several days to load. Therefore, one limitation ofour evaluation is that we cannot guarantee that we havedemonstrated the best absolute performance of each triplestore.In the future, we will evaluate federated queries as wellas the inference ability of each triple store. The use caseswe used in this study were designed based on their dailyusage, including do join operations over 10 times, dif-ferent types of filter operations, and almost all of theclauses that are frequently used in the SPARQL queries.Some other special use cases can be designed to test thedetailed performance of each triple store, such as testsof PSO (in predicate-subject-object order) and POS (inpredicate-object-subject order) indices. In addition, thetriple stores themselves are also improving as newer ver-sions are released. For example, disk space requirementsand loading costs have been improved in OWLIM byintroducing compression and fixing bugs in the engine.Although Virtuoso 7 seems a mere major update to Vir-tuoso 6, the underlying technologies are very different.Virtuoso 6 is a row store database, but Virtuoso 7 adoptscolumn store technology, which makes them a totally dif-ferent performance. For Allie data set, Virtuoso 7 took 7minutes to import. As for five query use cases, it took 61,1107, 391, 71 and 5633 milliseconds, respectively. Com-pared with Virtuoso 6, response for use case 2, 4 and 5were faster, 1 and 3 were slower. However, we found thatthere are still some problems to use Virtuoso 7, such assystem crashed when uploading our DDBJ data with errorlog GPF:Dkpool.c:munmap failed. We will keep evaluat-ing new triple stores or versions and their clusters, andupdating the results in our website http://kiban.dbcls.jp/togordf/wiki.Additional filesAdditional file 1: SPARQL Query. This file includes the details of theSPARQL queries that we used in our evaluations [26].Additional file 2: Configuration. This file presents the modifiedparameters for each database.Competing interestsThe authors declare that they have no competing interests.Authors contributionsThe work presented in this paper was conducted in collaboration between allauthors. HW conducted the experiments, analysed the data, and drafted thepaper. TF, YY, and JB worked on data collection, use-case design, and softwaretuning as well as other related tasks. AY coordinated and managed the entireexperimental process. All authors have contributed to revisions to themanuscript and have approved the final version of the manuscript.AcknowledgementsThis work has been supported by the National Bioscience Database Center(NBDC) of the Japan Science and Technology Agency (JST). Jerven Bollemanwas supported by the Swiss Federal Government through the State Secretariatfor Education, Research and Innovation SERI and by the National Institutes ofHealth (NIH) grant 4U41HG006104-04.Author details1Database Center for Life Science, Research Organization of Information andSystems, 178-4-4 Wakashiba, Kashiwa, Chiba 277-0871, Japan. 2INTEC Inc, 1-3-3Shinsuna, Koto-ku, Tokyo 136-8637, Japan. 3Swiss-Prot group, SIB SwissInstitute of Bioinformatics, CMU, 1 Michel Servet, 1211 Geneva 4, Switzerland.Received: 13 May 2013 Accepted: 27 April 2014Published: 10 July 2014JOURNAL OFBIOMEDICAL SEMANTICSPalombi et al. Journal of Biomedical Semantics 2014, 5:20http://www.jbiomedsem.com/content/5/1/20RESEARCH Open AccessMy Corporis Fabrica: an ontology-based toolfor reasoning and querying on complexanatomical modelsOlivier Palombi1,2*, Federico Ulliana3, Valentin Favier1, Jean-Claude Léon2 and Marie-Christine Rousset3AbstractBackground: Multiple models of anatomy have been developed independently and for different purposes. Inparticular, 3D graphical models are specially useful for visualizing the different organs composing the human body,while ontologies such as FMA (Foundational Model of Anatomy) are symbolic models that provide a unified formaldescription of anatomy. Despite its comprehensive content concerning the anatomical structures, the lack of formaldescriptions of anatomical functions in FMA limits its usage in many applications. In addition, the absence ofconnection between 3D models and anatomical ontologies makes it difficult and time-consuming to set up andaccess to the anatomical content of complex 3D objects.Results: First, we provide a new ontology of anatomy called My Corporis Fabrica (MyCF), which conforms to FMA butextends it by making explicit how anatomical structures are composed, how they contribute to functions, and alsohow they can be related to 3D complex objects. Second, we have equipped MyCF with automatic reasoningcapabilities that enable model checking and complex queries answering. We illustrate the added-value of such adeclarative approach for interactive simulation and visualization as well as for teaching applications.Conclusions: The novel vision of ontologies that we have developed in this paper enables a declarative assembly ofdifferent models to obtain composed models guaranteed to be anatomically valid while capturing the complexity ofhuman anatomy. The main interest of this approach is its declarativity that makes possible for domain experts toenrich the knowledge base at any moment through simple editors without having to change the algorithmicmachinery. This provides MyCF software environment a flexibility to process and add semantics on purpose forvarious applications that incorporate not only symbolic information but also 3D geometric models representinganatomical entities as well as other symbolic information like the anatomical functions.BackgroundComputer modeling and simulation of the human body isbecoming a critical and central tool inmedicine but also inmany other disciplines, including engineering, education,entertainment. Multiple models have been developed, forapplications ranging from medical simulation to videogames, through biomechanics, ergonomics, robotics andCAD, to name only a few. However, currently availableanatomical models are either limited to very specific areasor too simplistic for most of the applications.*Correspondence: OPalombi@chu-grenoble.fr1Department of Anatomy, LADAF, Université Joseph Fourier, Grenoble, France2LJK (CNRS-UJF-INPG-UPMF), INRIA, Université de Grenoble, Grenoble, FranceFull list of author information is available at the end of the articleThe most generic models used to describe the anatomyare ontologies. Ontologies provide a unified view of adomain of interest resulting of a joint effort of a wholecommunity to standardize a common vocabulary witha clear semantics that can then be shared by users toannotate, index and retrieve data and tools.A lot of more or less specialized medical ontologieshave flourished recently. Most of them are grouped intothe Open Biological and Biomedical Ontologies foundry(OBO) [1]. For human anatomy, the reference domainontology is the Foundational Model of Anatomy (FMA)[2] which is a comprehensive description of the struc-tural organization of the body. Its main component is ataxononomy with more then 83000 classes of anatomical© 2014 Palombi et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedicationwaiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwisestated.Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 2 of 13http://www.jbiomedsem.com/content/5/1/20structures from the macromolecular to the macroscopiclevels. The FMA symbolically represents the structuralorganization of the human body.The complexity of human anatomy can make it difficultfor users to comprehend and interact with the anatomicalknowledge embedded. This complexity may explain thegap between available anatomical ontologies and poten-tial users [3]. In practice, anatomical concepts are usuallyused through the scope of other ontologies. For instancein SNOMED CT [4], anatomical concepts are linked tospecific diseases or symptoms. In fact, four of the 8 OpenBiological and Biomedical Ontologies (OBO) Foundryontologies and 39 of the sites other listed ontologies, coveraspects of the representation of anatomical knowledge.Whilst the OBO Foundry has a stated goal of creating asuite of orthogonal interoperable reference ontologies inthe biomedical domain [5], most of these ontologies havebeen developed to address a species-specific need artic-ulated by a community working with a particular modelorganism.Relations between anatomical structures and their func-tions appear to be a relevant knowledge. These structuraland functional relationships have been explicitly definedat the level of cells in [6] or at the level a whole organ-ism as the drosophila [7]. Also, Uberon [8] takes intoaccount functions in order to query multiple ontologies ofspecies.Yet, human body modeling relies on morphologicalcomponents on the one hand and functional and pro-cess descriptions on the other hand. This fundamentalinteraction between structures and functions has beenalready highlighted by Smith et al. [9,10]. This philosoph-ical approach relies on the idea that the link betweenanatomy and physiology must be formalized in an newreference ontology. We try to turn this concept intoaction.The ICF is an International Classification of Func-tioning, Disability and Health (ICF) [11] endorsed bythe World Health Organization since 2001. However, itsanalysis reveals that the current version of ICF exhibitsnon-conformances to many formal ontological principles[12,13]. The need for a formal description of anatomicalfunctions has been outlined in [14], with some guidelinesfor getting a separate ontology of anatomical functionsbased on an ontological analysis of functions in generalformal ontologies such as GFO [15] or Dolce [16].Another limitation is that, despite the proliferation ofspecialized or general ontologies, they are far from beingfully exploited, mainly because they are only seen as astandard common structured vocabulary used for anno-tating and navigating among resources. Yet, they alsocome with a formal logical semantics that makes themprocessable by machines through inference algorithms.However, until now, only few existing works in biomedicalontologies (e.g., [17-20]) take advantage of available auto-matic reasoners. Most of these works rely on ontologiesexpressed in OWL and use OWL reasoners that are basedon Description Logics [21]. OWL is one the standardsrecommended by the W3C for the Semantic Web, whichenables expressing sophisticated ontological constraints(using Description Logics constructors) but with a highcomputational complexity in the worst case. RDFS isanother W3C standard that is broadly used in particularin Linked Data. Whereas OWL is often seen as an exten-sion of RDF and RDFS, this is not exactly the case, mainlybecause RDF(S) offers interesting non-first-order featureswhich are not present in the Description Logics at thebasis of OWLprofiles, like the possibility of treating valuesboth as constants and as classes or properties. In the samespirit, the RDF query language SPARQLmakes possible toquery at the same time the data and the schema and allowsthat variables stand for classes and properties. This goesbeyond the first-order conjonctive queries typically con-sidered inDL-based settings. Recently, RDF-based seman-tic environments such as Jena (http://jena.apache.org/) orCwm (http://www.w3.org/2000/10/swap/doc/cwm) haveincluded logical rules to perform inferences on top of RDFdatasets. Logical rules and Description Logics are twoorthogonal decidable fragments of first-order logics thathave been extensively studied in knowledge representa-tion and in deductive databases. The interest of logicalrules (a la Datalog) is that they are easy to read andwrite for practitioners and they have a polynomial datacomplexity while allowing expressing complex interactionbetween properties and recursivity.When application software provides capabilities to dis-play/select 3D graphic entities, its interactive behaviorbecomes a mandatory feature to manage the graphic enti-ties attached to the digital objects taking part to thisapplication. In such software, selection functions havebeen under focus to provide users with efficient meansto reach the 3D content they are looking for. The mostcommon approaches for multiple object selection includeserial selection techniques that require the user to selectobjects one at a time, e.g. the ubiquitous ctrl + click (orshift + click) approach, and parallel selection techniquessuch as brushes, lassos, and selection shapes. However, asLucas et al. [22] point out, each has certain limitations,especially in 3D. For instance, multiple objects may be dif-ficult to distinguish, isolate, or even see due to occlusion,rendering size, environment clutter, and other display fac-tors. Requiring the user to adjust the view can be tedious,cumbersome, and even burdensome, especially when thenumber of objects to select is high, and may still fail tomake certain objects accessible. This is especially true fordisplay scenes with anatomical entities.Systems commonly address this issue with an indirectselection technique, that is, by allowing the user to specifyPalombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 3 of 13http://www.jbiomedsem.com/content/5/1/20the desired selection using an alternate representationsuch as a model tree or component list. Some systemsallow selection by common attribute or provide a moregeneral selection query or search [23]. Such indirect selec-tion techniques are useful, but are generally abstract andless intuitive than direct manipulation techniques. Theycan become cumbersome if the user has to browse a verylarge amount of entities, which is the case of anatomicalentities of the human body.In the case of man-made objects, recent advanceshave been made in this field using additional functionalinformation attached to the components of large digi-tal assemblies [24] using an ontology-based approach.This additional information provides the user with effi-cient means to select/process groups of components thatwould be otherwise tedious and error prone to identify[25]. Other interactive approaches, like Oh et al. [26]propose group selection with a dynamically computedhierarchy based on the notion of gravitational proxim-ity. Such approaches are less appropriate for rigid andexact specification of selections, particularly when objectsor components are frequently or always in contact withor intersecting each other, e.g. when managing sets ofanatomical entities. If man-made objects can take advan-tage of their modeling process to rely on concepts likegeometric constraints [27] and simple spatial structureslike repetitive placement of objects along lines or circles[28], such structures do not exist for anatomical enti-ties. It is therefore difficult to rely on spatial structures todisplay/select 3D anatomical entities.Our approach for supporting efficient navigation andselection of objects in 3D scenes of human body anatomyis to make explicit the anatomic and functional semanticsof 3D objects composing a complex 3D scene through asymbolic and formal representation that can be queriedon demand.In this context, our contribution is twofold: First, we address the lack of a formal description ofhuman body functions and we provide a newontology, called My Corporis Fabrica (MyCF),containing the following items: a taxonomy of anatomical functions conformto the ICF terminology; a taxonomy of anatomical structures based onFMA; and relations between them and with 3Dmodels, that make explicit how anatomicalstructures are composed, how they contributeto functions, and also how they can be relatedto 3D complex objects describingpatient-specific body parts, declared asinstances of appropriate mesh 3D modelsused for simulation or 3D rendering; Second, we equip MyCF with automatic reasoningcapabilities that enable model checking and complexqueries answering, and we show the added-value ofsuch a declarative approach for interactive simulationand visualization as well as for teaching applications.In particular, we provide new visualization/selectioncapabilities to manage and browse 3D anatomicalentities based on the querying capabilitiesincorporated in MyCF.Results and discussionMyCF is an ontology-based tool for automatic reasoningand querying on complex anatomical models. The core ofMyCF is a comprehensive anatomical ontology, the nov-elty of which is to make explicit the links between anatom-ical entities, human body functions, and 3D graphic mod-els of patient-specific body parts. It is equipped withinference-based query answering capabilities that are par-ticularly interesting for different purposes such as: automatic verification of the anatomical validity of3D models. Indeed, it is important to select thecorrect set of anatomical entities that contributes to asimulation, e.g. a simulation of movements where thecorrect bones, muscles, ligaments, . . . , are requiredto set up all the 3D and mechanical simulationparameters. These requirements are very close to theselection requirements described in the Backgroundsection. They can be regarded as equivalent to aselection operator; automatic selection and display of anatomical entitieswithin a 3D scene. Anatomical entities can varylargely in size, can be very close to each other or evenhidden by other anatomical entities. The use ofgeometric means to select useful sets of entities is notsuited whereas inference-based queries using humanbody functions can provide much more suited means.Such selection capabilities are particular relevant fordiagnosis for instance; training students on anatomical entities participatingto a certain body function. Here again, this purpose isclose to that of selection functions where theconnection between function and anatomical entitiesprovides new means to browse and highlight featuresof anatomical structures accessible in 3D.The first version of MyCF has been published in 2009[29]. This version was limited to anatomical entities. Thecurrent version of MyCF has been widely improved byadding (i) new anatomical entities with more details thanFMA for some body parts, (ii) almost 4000 human bodyfunctions, and (iii) the set of classes related to 3D mod-els. The current version of the ontology contains almost74000 classes and relations as well as 11 rules storedPalombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 4 of 13http://www.jbiomedsem.com/content/5/1/20in a deductive RDF triple-store using a Sesame server,and that can be queried with a remote-access facility viaa web server [30]. The ontology can be easily updated,just by entering or deleting triples and/or by modifyingthe set of rules, without having to change the reasoningalgorithmic machinery used for answering queries. It isthe strength of a declarative approach that allows a fine-grained domain-specific modeling and the exploitation ofthe result by a generic (domain-independent) reasoningalgorithm.MyCF features three distinct taxonomies linked by rela-tions and rules: Anatomical entities, such as knee, shoulder, andhand, denote parts of the human body, and give aformal description of canonical anatomy; Functional entities, such as gait, breath, and stability,denote the functions of the human body, and are thefundamental knowledge to explain the role of eachanatomical entity; Finally, 3D scenes with entities such as 3D-object,3D-scene define the content required to get 3D viewsof patient-specific anatomical entities described by3D graphical models related to anatomical entities.Figure 1 shows the top classes of the three taxonomiesas they are displayed by the Protégé editor. We now inturn describe each of the taxonomies ( of anatomical enti-ties, anatomical functions, and 3D objects respectively),the relations existing within and between them and theinference rules on which reasoning is performed.The taxonomy of anatomical entitiesThe taxonomy of anatomical entities of MyCF contains69000 classes at the moment. It is inherited from theFoundational Model of Anatomy (FMA) ontology, in thesense that we have extracted from FMA a lot of termsthat we have incorporated into MyCF. The correspon-dences between terms of the two ontologies are definedby means of the owl:sameAs relation. For example, to saythat mcf:Femur corresponds to fma:Femur, we use thetriple ?mcf:Femur, owl:sameAs, fma:Femur?. However, wehave skipped the top levels of the FMA taxonomy, thatwe have judged too general for our needs. For example, weskipped the FMA classes Physical anatomical entity andNon-physical anatomical entity declared in FMA as sub-classes of the top FMA class Anatomical entity. We alsoskipped the two FMA classes Material anatomical entityand Immaterial anatomical entity appearing in FMA assubclasses of Physical anatomical entity, and the two FMAclasses Postnatal anatomical structure and Developmen-tal anatomical structure appearing as subclasses of theFMA subclass Anatomical structure ofMaterial anatomi-cal entity.On the other hand, we have added to MyCF anatomi-cal classes that are not present in FMA but relevant forbiomechanical simulations and for 3D visualization suchas tendons and other anatomical entities, especially forfree limbs. Figure 2 illustrates the level of detail with whicha musculature is described in MyCF.We have also introduced the possibility of makingexplicit the left and right specializations of anatomicalentities. Many anatomical entities indeed can be special-ized into two symmetric anatomical entities, representingits left and right version. For instance, the entity knee isspecialized into the left knee and the right knee. Distin-guishing the right and left versions of a given anatomicalstructure may be very important, for instance when link-ing anatomy with 3D models for simulation purposes.This provides new and convenient means of interactingFigure 1 Protégé display of the top classes of the three MyCF taxonomies. The three top classes of MyCF are 3D entity, Anatomical entity(same as FMA) and Functional entity.Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 5 of 13http://www.jbiomedsem.com/content/5/1/20Figure 2 Description of musculature in MyCF anatomical taxonomy (an extract about tendons of sartorius).Muscles in MyCF are subdividedinto different parts. A particular focused has been made on tendons that play important roles in biomedical simulations.with a 3D scene containing graphical entities that can beeasily accessed, e.g. displaying the left knee. Our solutionto this specialization in a semi-automatic and systematicway is the following one: we add two new relations mcf:leftSubClassOf andmcf:rightSubClassOf, that we declare asspecializations of the rdfs:subClassOf relationbetween anatomical entities using rules that will bedetailed at the section The taxonomy of anatomicalentities. These relations are not topological, in thesense that they do not aim at describing the absoluteor relative displacement of an anatomical entity withrespect to another, like the ones proposed in [8].They aim at capturing the specialization of ananatomical concept, like the knee, in its left and rightdeclensions; for every anatomical entity for which we want todeclare its left and right specialization, e.g.knee_joint, we introduce two new names of classes,e.g. Left_knee_joint and Right_knee_joint, that wedeclare respectively as mcf:leftSubClassOf andmcf:rightSubClassOf of the anatomical entity. This isdone by adding two RDF triples. For instance, in thecase of the knee joint we have:? Left_knee_joint mcf:leftSubClassOf knee_joint ?,? Right_knee_joint mcf:rightSubClassOf knee_joint ?; we iteratively replicate all links, expressed as RDFtriples, between an anatomical entity and one of itssubclasses through the left and right specializations.For instance, in Figure 3, we report the result of theseoperations for the subtree of the anatomicaltaxonomy rooted in knee_joint.Classifying entities with these new properties bringsthe following advantages: by querying on SubClassOfproperty, one can navigate the anatomical entities regard-less of left/right parts, thus obtaining a more succinctrepresentation of the data; by querying on leftSubClas-sOf and rightSubClassOf properties, one can navigate thecomplete taxonomy of the left and right specializations ofan anatomical entity, if it is needed. Finally, we can eas-ily relate a 3D object to the corresponding left or rightspecializations of anatomical entities, which is efficient toprovide new means of selecting anatomical entities usingthe left and right concepts that are relevant in many 3Dapplications and biomechanical simulations.In addition to the generic rdfs:subClassOf relationand its subproperties mcf:rightSubClassOf and mcf:rightSubClassOf that are the basis of the tree structureof the anatomical taxonomy, we have introduced themcf:PartOf and mcf:InsertOn domain-specific relations.Then, we have declared in the form of RDF triples andrules that will be explained as knowledge on how anatom-ical entities are related by these two properties: The property mcf:PartOf is used to make explicit thesubparts of anatomical entities, which is an importantanatomical knowledge. For example, a joint is a partof the articular system (but joint is not a subclass ofan articular system), is declared by adding to theontology data base the RDF triple:? mcf:Joint mcf:PartOf mcf:Articular_System ?.Note that, like for FME (the explorer of FMA), themcf:PartOf relation can be chosen for defining thetree structure through which the user wants tovisualize the anatomical entities in 3D, as analternative to the tree structure defined by therdfs:subClassOf relation. The property mcf:InsertOn is used to specify attachpoints of anatomical entities. This knowledge isimportant in anatomy and also for biomechanicalsimulation purposes. For instance, the distal tendonof right sartorius is inserted on the Medial part ofproximal epiphysis of right tibia, is expressed byadding the RDF triple:? mcf:Distal_Tendon_Of_Right_Sartorius mcf:InsertOnmcf:Medial_part_of_proximal_epiphysis_of_right_tibia ?.In the current version of MyCF, there are 4000 RDFtriples involving the property mcf:PartOf, and 850 RDFtriples involving the propertymcf:InsertOn.Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 6 of 13http://www.jbiomedsem.com/content/5/1/20mcf:Knee_jointsmcf:Left_knee_joint mcf:Tibiofemoral_joint mcf:Patellofemoral_joint mcf:Right_knee_jointmcf:Left_tibiofemoral_jointmcf:Right_Tibiofemoral_jointmcf:Left_patellofemoral_jointmcf:Right_Patellofemoral_jointmcf:Knee_jointsmcf:Tibiofemoral_joint mcf:Patellofemoral_joint mcf:Right_knee_jointmcf:Right_Tibiofemoral_jointmcf:Left_knee_jointmcf:Left_tibiofemoral_jointmcf:Left_patellofemoral_jointmcf:Right_Patellofemoral_joint(a)(b)Figure 3 Example of left and right structures of the knee joint. Left-right structure of the knee-joint subclasses. (a) FMA taxonomy and (b)MyCFtaxonomy. The novel propertiesmcf:LeftSubClassOf andmcf:RightSubClassOf are drawn in red and blue, respectively.The taxonomy of anatomical functionsThe taxonomy of anatomical functions of MyCF is thetrue added-value of MyCF that distinguishes it from thestate-of-the-art anatomical ontologies. It contains 4000classes at the moment, most of the terms used to denotethem come from the ICF terminology. The anatomicalfunctions are structured using two relations: the generic rdfs:subClassOf relation betweenfunctions. For instance, the extension of the knee is asubclass of the simple movement function, isexpressed by the RDF triple:? mcf:Extension_Of_The_Knee rdfs:subClassOfmcf:Simple_Movement ? and the domain-specific relation mcf:IsInvolvedInwhich plays a role analogous to that of the partOfrelation between anatomical entities. For example,the eversion of the foot is involved in the mobility ofankle joints, is expressed by adding the RDF triple:? mcf:Eversion_Of_The_Foot mcf:IsInvolvedInmcf:Mobility_Of_Ankle_Joints ?Notice that the eversion of the foot is not a subclassof the mobility of ankle joints.In the current version of MyCF, there are 4000 RDFtriples specifying rdfs:subClassOf relations between func-tions, and 1300 RDF specifying mcf:IsInvolvedIn relationsbetween functions.The real added-value ofMyCF is to link the taxonomy offunctions with the anatomical taxonomy to make explicitthe functional roles of anatomical entities. Exploiting therelationships between anatomical and functional entitiesis decisive to retrieve the entities participating to somefunctions, and vice-versa. In particular, this is crucial formedical diagnosis and it is also of key importance to beable to display/select interactively 3D geometric entities.To address this issue, we have introduced two domain-specific relations, mcf:hasFunction and mcf:contributesToto describe how an anatomical entity contributes to agiven function. The former is used to denote that ananatomical entity, as a whole, realizes a given function.The latter is used to denote that an anatomical entitysimply contributes to the realization of a given function,but taken alone it may not be sufficient to execute thisfunction. The relation mcf:hasFunction relates an anatomicalentity with the function(s) that it realizes. Forinstance, we can declare that the function kneemovement is performed by the knee by the followingRDF triple:? mcf:Knee mcf:hasFunction mcf:Knee_Movement ?Similarly, we can make explicit the functions ofensuring sliding motion of articular surface andensuring transmission and amortization of chargesPalombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 7 of 13http://www.jbiomedsem.com/content/5/1/20of joint cartilages by the two following RDFtriples:? mcf:Joint_Cartilage mcf:hasFunctionmcf:Ensure_Sliding_Motion_Of_Articular_Surface ?? mcf:Joint_Cartilage mcf:hasFunction mcf:Ensure_Transmission_And_Amortization_Of_Charges ? The relation mcf:contributesTo is a weaker relationthan the relation mcf:hasFunction, which allows tospecify that a given anatomical entity contributes tothe realization of a given (set of) function(s). For instance,despite the fact that the toe does not have as functionthe body stability, the toe contributes to the bodystability. This can be expressed by the RDF triple:? mcf:Toe mcf:contributesTo mcf:Body_Stability ?Note that, as it will be explained in the next section,this triple can be declared or inferred by rules. Wewill also show how we express by a rule that therelation mcf:hasFunction is stronger than the relationmcf:contributesTo, which enables to infer that anyanatomical enity that is declared as having asfunction a given anatomical function, contributes tothis function a fortiori.In the current version of MyCF, there are 700 RDFtriples specifying mcf:hasFunction between anatomicalentities and anatomical functions, and 500 RDF specifyinga mcf:contributesTo relation between anatomical entitiesand anatomical functions.The taxonomy of 3D objectsThe taxonomy of 3D objects of MyCF is simple butmandatory to connect the anatomical entities and theirfunctions to graphic entities used to interact with these3D objects. It also illustrates well the declarative wayto connect additional knowledge for different purposesto a given ontology. Here, we want to connect to theanatomical ontology (patient-specific) to 3D geometricmodels displaying a body part, so that the different 3Dobjects contained in the scene are related to the anatomi-cal entities they describe, thus providing the user with newmeans to select/display these entities using the knowl-edge embedded in the taxonomies of anatomical entitiesand anatomical functions as well as their relationships.This is a new scheme to avoid the selection/display ofthese entities using purely geometry-based approaches.The proposed taxonomy aims at defining the smallestcontent enabling elementary tasks to display/select 3Dobjects though this can be enriched to refer to geometriccriteria for these tasks that would add other entities in thistaxonomy. Our approach for doing so consists in: Designing a taxonomy of geometric objects (shown inFigure 4) made of two classes respectively calledmcf:3D-scene and mcf:3D-object, a relation calledmcf:Contains having the class mcf:3D-model asdomain and the class mcf:3D-object as range, andfour relations respectively called mcf:Position,mcf:hasMesh, mcf:hasTexture and mcf:hasColourrespectively, in order to possibly relate each specific3D-object to a position matrix, a mesh file, a texturefile, and a color, linking this 3D geometry taxonomy to the anatomicaltaxonomy through two relations called respectivelymcf:Describes relating instances of the class mcf:3D-object to instances of the class fma:AnatomicalEntity, and mcf:Displays relating instances of theclass mcf:3D-scene to instances of fma:AnatomicalEntity or of mcf:Anatomical Function. declaring each new patient-specific 3D model that weacquire as an instance of the class 3D-scene, by amcf:3D_entityrdfs:subClassOfmcf:3D_scene mcf:3D_objectmcf:object_2rdf:type rdf:typemcf:containsmcf:containsrdfs:subClassOf"texture.jpg""mesh.obj"(255,255,0)rdf:typemcf:texturemcf:meshmcf:colormcf:scene_1 mcf:object_3Figure 4 3D taxonomy in MyCF. 3D taxonomy of MyCF is basic with only three classes. The individual, for instance, called object_3 is anmcf:3D_object that has a geometry (obj file) and a texture (jpg file) allowing a 3D visualization and interaction.Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 8 of 13http://www.jbiomedsem.com/content/5/1/20RDF triple: ? mcf:id rdf:type 3D-scene ? where mcf:iddenotes an identifier (e.g., an URI) where the file model-ing the 3D-scene is stored, and stating which body partor function it displays by an RDF triple, for instance:? mcf:id mcf:Displays mcf:Knee ? identifying all the 3D-objects segmented within the3D-scene and corresponding to anatomical entities asinstances of the class 3D-object, for which a numberof RDF triples are declared to specify that theyidentify 3D-objects contained in the 3D-scene fromwhich they have been extracted, and that theydescribe the corresponding anatomical entity.For instance, the 3D-scene displayed in Figure 5, storedin a file identified by mcf:id, in which the differentcoloured 3D-objects corresponding to muscles and boneshave been extracted by segmentation, would be describedin myCF by the following RDF triples:? mcf:id rdf:type mcf:3D-scene ? ? mcf:id mcf:Displays mcf:Leg ?? mcf:id mcf:Contains mcf:id1 ? ? mcf:id1 rdf:type mcf:3D-object ?? mcf:id mcf:Contains mcf:id2 ? ? mcf:id2 rdf:type mcf:3D-object ?? mcf:id mcf:Contains mcf:id3 ? ? mcf:id3 rdf:type mcf:3D-object ?....? mcf:id1 mcf:Describes mcf:Left_sartorius ?? mcf:id1 mcf:hasMesh ¨ ..\geometries\l_sartorius.obj¨ ?? mcf:id2 mcf:Describes mcf:Left_bicepsfemoris ?? mcf:id2 mcf:hasMesh ¨ ..\geometries\l_bicepsfemoris.obj¨ ?? mcf:id3 mcf:Describes mcf:Left_semimembranosus ??mcf:id2 mcf:hasMesh¨ ..\geometries\l_semimembranosus.obj¨ ?.....Figure 5 Example of a 3D scene containing complex 3Danatomical models. 3D-model of the proximal part of the left lowerlimb. Only the left sartorius is pointed out.Figure 6 summarizes the structure of MyCF ontologymade of its three taxonomies interrelated by relations.The inference rulesThe inference rules of MyCF express complex connec-tions between relations. They allow the ontology designerto declare part of his/her domain knowledge in the formof abstract rules. These rules capture in a very compactway implicit facts that can be made explicit on demandor at query time by an inference mechanism. This mecha-nism is automatic and consists in applying the rules on theexplicit facts declared and stored as RDF triples, in all thepossible manners satisfying the conditions of these rules.For each possible instantiation of the variables (denotedby a name starting by ?) appearing in the condition partof a given rule such that all its conditions are satisfied byexplicit facts, the new facts corresponding to the (appro-priately instantiated) conclusion of the rule are added.This saturation process is iterated as long as new facts canbe produced. The termination is guaranteed by the formof the rules that are considered. They correspond to saferules, also called Datalog rules: all the variables appearingin the conclusion of a rule also appears in the conditionpart. This contrasts with description logics axioms or withDatalog+? rules [31] in which we can infer that there exists(unknown) individuals verifying a given property.The rules that are considered in the current version ofMyCF are the following ones. It is important to note how-ever that adding, removing or modifying a rule is verysimple and does not impact the inference mechanism thatremains unchanged as long as the rules added that are safeones.The three following rules express the transitivity of thegeneric relation rdfs:subClassOf, as well as of the domain-specific relations mcf:PartOf between anatomical enti-ties and mcf:IsInvolvedIn between anatomical functions,respectively.IF? ?a rdfs:subClassOf ?c ?AND? ?c rdfs:subClassOf ?b ?THEN? ?a rdfs:subClassOf ?b ? (R1)IF? ?a mcf:PartOf ?c ?AND? ?c mcf:PartOf ?b ?THEN? ?a mcf:PartOf ?b ? (R2)IF? ?a mcf:IsInvolvedIn ?c ?AND? ?c mcf:IsInvolvedIn ?b ?THEN? ?a mcf:IsInvolvedIn ?b ? (R3)The three following rules express specializations of rela-tions: mcf:LeftSubClassOf and mcf:RightSubClassOf areboth two specializations of rdfs:subClassOf ; and the rela-tion mcf:hasFunction (between an anatomical entity andan anatomical function) is more specific (i.e., more pre-cise) than the relation mcf:contributesTo (between ananatomical entity and an anatomical function).Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 9 of 13http://www.jbiomedsem.com/content/5/1/20mcf:3D_entityrdfs:subClassOfmcf:3D_scene mcf:3D_objectmcf:object_1 mcf:object_2mcf:scene_1rdf:type rdf:type rdf:typemcf:containsmcf:containsmcf:Anatomical_entityrdfs:subClassOfmcf:Musclemcf:Sartoriusrdfs:subClassOfrdfs:subClassOfmcf:describesmcf:Functional_entityrdfs:subClassOfmcf:Simple_mouvement_of_knee_jointsmcf:Flexion_of_knee_jointrdfs:subClassOfmcf:Gaitrdfs:subClassOf mcf:isInvolvedInmcf:participatesTomcf:displaysFigure 6 The general structure of MyCF ontology (extract). The three taxonomies of MyCF are interconnected allowing a high level ofknowledge expression.IF? ?a mcf:LeftSubClassOf ?b ?THEN? ?a rdfs:subClassOf ?b ?(R4)IF? ?a mcf:RightSubClassOf ?b ?THEN? ?a rdfs:subClassOf ?b ?(R5)IF? ?a mcf:hasFunction ?b ?THEN? ?a mcf:contributesTo ?b ?(R6)Finally, the following rules express connections thathold in the domain of anatomy between the relationsrdfs:subClassOf and mcf:InsertOn, rdfs:subClassOf andmcf:IsInvolvedIn, rdfs:subClassOf andmcf:contributesTo,mcf:contributesTo and mcf:IsInvolvedIn, mcf:PartOf andmcf:InsertOn respectively.For example, the first rule says that if a given classrepresenting an anatomical entity ?a (e.g., Sartorius) is asubclass of an anatomical entity ?c (e.g., Muscle) that isknown to be inserted on an anatomical entity ?b (e.g.,Bone), then ?a is inserted on ?b (Sartorius inserts on aBone).IF? ?a rdfs:subClassOf ?c ?AND? ?c mcf:InsertOn ?b ?THEN? ?a mcf:InsertOn ?b ? (R7)IF? ?a mcf:IsInvolvedIn ?c ?AND? ?c rdfs:subClassOf ?b ?THEN? ?a mcf:IsInvolvedIn ?b ? (R8)IF? ?a mcf:contributesTo ?c ?AND? ?c rdfs:subClassOf ?b ?THEN? ?a mcf:contributesTo ?b ? (R9)IF? ?a mcf:contributesTo ?c ?AND? ?c mcf:IsInvolvedIn ?b ?THEN? ?a mcf:contributesTo ?b ? (R10)IF? ?a mcf:InsertOn ?c ?AND? ?c mcf:PartOf ?b ?THEN? ?a mcf:InsertOn ?b ? (R11)The point is that we can easily add rules crossing theanatomy domain and the 3D domain, to express, forinstance, conventional colors associated with the visual-ization of some organs (such as bones, muscles, and soon). The following rule expresses that the conventionalcolor for visualizing bones in anatomy is yellow:IF? ?x rdf:type mcf:3D-object ?AND? ?x mcf:Describes ?y ?AND? ?y rdfs:subClassOf mcf:Bone ?THEN? ?x mcf:hasColour yellow ? (R12)Querying: illustration by exampleIn the Figure 7, we illustrate a complete example fromquery to 3D visualization. Data are presented as a graphwith corresponding RDF triples on the bottom. The queryis explained in English and translated in SPARQL. Theanswers are used to select and highlight corresponding 3Dmodels in the 3D scene.ConclusionsWe have described MyCF with a particular emphasison its ontology structure, showing how the FMA ontol-ogy can be used as basis of the anatomical descriptionof human bodies and empowered with a taxonomy ofanatomical functions conforming to the ICF terminology.We have introduced new concepts that are particularlyuseful for checking the anatomical validity of 3D modelscontaining multiple anatomical entities, also for select-ing sets of anatomical entities on the basis of functionsrather than being bound to geometric approaches that arenot efficient enough to process complex 3D geometricconfigurations.These high level functionalities can be achieved thanksto the combination of different types of knowledge relatedto anatomy. The reasoning capabilities brought by theinference rules increase the power of realizing complextasks by reducing them to querying a knowledge baseimplemented as a deductive database.The main interest of this approach is its declarativ-ity that makes possible for domain experts to enrich theknowledge base at any moment through simple editorswithout having to change the algorithmic machinery.Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 10 of 13http://www.jbiomedsem.com/content/5/1/20Figure 7 Example of querying about anatomy of the sartorius. The graph on the left is a visual representation of data. The query about leftsartorius, translated in SPARQL, gives the bones on which the tendons of sartorius are inserted. In the final 3D scene the sartorius is showed aloneand the corresponding bones are highlighted in yellow.This provides MyCF software environment a flexibil-ity to process and add semantics on purpose for variousapplications that incorporate not only symbolic informa-tion but also 3D geometric models representing anatomi-cal entities as well as other symbolic information like theanatomical functions.The MyCF ontology is at the heart of the the MyCFBrowser: a tool for exploring anatomical 3D models [32].Further work will address the use of this environmentto feed a bio-simulation engine with the appropri-ate anatomical entities so that mechanical simulationscan be easily set up and extract the required geo-metric information from the 3D models of anatomicalentities.MethodsThrough the presentation of MyCF, we develop a noveland promising vision of ontologies equipped with infer-ence algorithms, that enables a declarative assembly ofdifferent models to obtain composed models guaranteedto be anatomically valid while capturing the complexity ofhuman anatomy.Methodology overview of the design of MyCF ontologyWe have designed a unifying representation frameworkto combine several types of structured knowledge aboutanatomy.For the types of anatomical knowledge for which ontolo-gies or terminologies exist, our approach is to enrichthem while remaining conform to them. The descriptionsin MyCFs ontology of the anatomical concepts and thehuman body functions are thus conform respectively tothe Foundational Model of Anatomy (FMA) [2] and tothe International Classification of Functioning, Disabil-ity and Health (ICF) [11]. In fact, MyCFs ontology bothenriches and links together two standard taxonomies thathave been developed separately and independently.For incorporating 3D models in MyCF ontology, inorder to follow a unifying approach, we have chosen todefine a taxonomy of 3D scenes and 3D objects, andto relate it to the taxonomies of anatomical entities andfunctions through relations.One particularity of MyCF is to use (generic and spe-cific) relations both to structure each taxonomy but also toestablish bridges between them. Inference rules are usedto express how relations interact.Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 11 of 13http://www.jbiomedsem.com/content/5/1/20We give now some details on our methodologicalchoices both for incorporating 3D models and inferencerules.3DmodelsWe want to be able to incorporate different types of 3Dmodels. Some 3D models can describe patient-specificbody parts acquired by CT (Computerized Tomography)or MRI (Magnetic Resonance imaging) scans. In thiscase, the 3D models are obtained by reconstruction usingclassic surface modeling techniques. The 3D models usedto illustrate this article are based on the Zygote humananatomy collections [33]. The resulting 3D models aremesh-based files associated with position matrices andtexture files for 3D view rendering. The storage and theprocessing of the files describing the 3D objects are spe-cially time and memory consuming. Our approach is todisconnect the identification of these files from their stor-age and processing, and to connect them to the ontologythrough their identifiers: each files identifier is declaredas an instance of a 3D scene capturing an anatomicalstructure, a body part, or a human body function in theontology. By segmentation, the 3D scene is decomposedinto components that are in turn declared as instances of3D objects describing the anatomical entities declared inthe ontology as parts of the given anatomical structure.Inference rulesWe have chosen the formalism of rules to expressproperties of relations (such as transitivity) but alsoproperties or constraints between domain-specific rela-tions. For instance, the following rule involving twodomain-specific relations (ContributesTo and IsIn-volvedIn) expresses that any anatomical entity ?Cparticipating to a function ?F that is involved in a function?F ? contributes to this function ?F ? too:IF? ?C mcf:ContributesTo ?F ?AND? ?F mcf:IsInvolvedIn ?F ?THEN? ?C mcf:ContributesTo ?F ?Such a rule is a compact formula that enables to inferas many instantiated facts as there exist pairs of facts sat-isfying its conditions. For example, using this rule, wecan infer that the muscle sartorius (but also the bicepsfemorus muscle) contributes to the function of move-ment of knee from the facts that sartorius (but also thebiceps femorus) contributes to the function knee flex-ion and that the function knee flexion is involved in thefunctionmovement of knee. Similarly, by using the samerule, we can infer that the different muscles (such as Ten-sor fascia lata, Rectus femoris, Vastus lateralis, Vastusmedialis, and Vastus intermedius) contributing to thefunction knee extension contribute too to themovementof knee since knee extension is involved in the move-ment of knee. This is a simple but powerful piece ofknowledge that can guide diagnosis by iteratively identify-ing the anatomical entities to check in case of dysfunctionof the movement of a knee. It can also help setting up anappropriate 3D scene or a biomechanical simulation. InFigure 8 Architecture of the MyCF environment. Overview of the architecture of MyCF.Palombi et al. Journal of Biomedical Semantics 2014, 5:20 Page 12 of 13http://www.jbiomedsem.com/content/5/1/20both cases, the interest is to select the anatomical enti-ties relevant to display them and meet the users needsor to select the anatomical entities to simulate a kneemovement. Rules can be very useful too for guiding imagesegmentation or image registration in medical imaging.For instance, a rule stating that every sinovial joint hasan articular capsule can guide automatic segmentation ofpatient-specific images.Semantic technologies used for building and exploitingMyCFIn order to make it easy to connect MyCF to the LinkedData cloud [34], we have followed the recommendationsof W3C and we have chosen the RDF(S) language forexpressing MyCF ontology.RDF [35] is a standard notation recommended by theW3C for the semantic Web composed of Web data and(simple) ontologies. RDF (Resource Description Frame-work) provides a simple language for describing annota-tions about Web resources identified by URIs. An RDFfact consists of a triple made of a subject, a predicate andan object. It expresses a relationship denoted by the pred-icate between the subject and the object. In a triple, thesubject, but also the predicate, are URIs pointing to Webresources, whereas the object may be either a URI or a lit-eral representing a value. RDFS is the schema language forRDF. It allows specifying a number of useful constraintson the individuals and relationships used in RDF triples.In particular, it allows declaring objects and subjects asinstances of certain classes. In addition, inclusion state-ments between classes and properties make it possible toexpress semantic relations between classes and betweenproperties. Finally, it is also possible to semantically relatethe domain and the range of a property to some classes.The point is that these constraints can be written in triplenotation, i.e., RDFS statements can be written using RDFas a notation. Therefore, a RDF data store can contain inthe same format triples expressing that a given acquisi-tion file (identified by a given URL u) is an instance of ananatomical structure (for instance the patella), and triplesdescribing knowledge known in the domain of anatomyabout this structure (for instance that the patella is acircular-triangular bone, and that it is part of the knee):? u rdf:type mcf:Patella ?? mcf:Patella rdfs:subClassOf mcf:CircularTriangularBone ?? mcf:Patella mcf:PartOf mcf:Knee ?As ontology editors, we have used Protégé [36] and Top-Braid Composer [37]. Protégé is supported by a strongcommunity of developers and academic, governmentand corporate users. The Protégé open source platformsupports modeling ontologies in a variety of formats via aweb client or a desktop client. TopBraid is a commercialtool specifically designed for RDF, which is also availableas free version.Finally, we have chosen to store and process the result-ing ontology as a deductive RDF triple-store using aSesame server. Sesame [38] is a de-facto standard frame-work for processing RDF data. This includes parsing,storing, inferencing and querying of/over such data. Itoffers an easy-to-use API that can be connected to allleading RDF storage solutions. Sesame fully supports theSPARQL [39] query language for expressive querying andoffers transparent access to remote RDF repositories usingthe exact same API as for local access. However Sesamecurrently has no built-in support for custom inferencerules. Therefore, we had to implement a rule engine ontop of it in order to enable sound and complete deduc-tive capabilities. This architecture is of course modularand adjustable. For instance, it is possible to change thetriple-store server, or to use an external reasoner support-ing Datalog rules for saturating the data. Figure 8 sketchesthe general architecture of the MyCF environment.Competing interestsThe authors declare that they have no competing interests.Authors contributionsOP conceived the original idea, contributed to the models (3D models andontologies), participated to the software development of a preliminary versionof My Corporis Fabrica, and supervised this project. FU contributed to theontological modeling and to the design of the overall architecture,implemented the rule engine, and is the main software developer. VF is themain developer of the taxonomy of anatomical functions. JCL contributed tothe models (3D models and ontologies). MCR contributed to the ontologicalmodeling and to the design of the overall architecture, and co-supervised thisproject. All authors read and approved the final manuscript.AcknowledgementsThis work has been partially supported by the LabEx PERSYVAL-Lab(ANR-11-LABX-0025-01) and by the project PAGODA (ANR-12-JS02-007-01).Author details1Department of Anatomy, LADAF, Université Joseph Fourier, Grenoble, France.2LJK (CNRS-UJF-INPG-UPMF), INRIA, Université de Grenoble, Grenoble, France.3LIG (CNRS-UJF-INPG-UPMF), Université de Grenoble, Grenoble, France.Received: 2 January 2014 Accepted: 23 April 2014Published: 6 May 2014JOURNAL OFBIOMEDICAL SEMANTICSDahdul et al. Journal of Biomedical Semantics 2014, 5:34http://www.jbiomedsem.com/content/5/1/34RESEARCH Open AccessNose to tail, roots to shoots: spatial descriptorsfor phenotypic diversity in the Biological SpatialOntologyWasila M Dahdul1,2*, Hong Cui3, Paula M Mabee1, Christopher J Mungall4, David Osumi-Sutherland5,Ramona L Walls6 and Melissa A Haendel7AbstractBackground: Spatial terminology is used in anatomy to indicate precise, relative positions of structures in anorganism. While these terms are often standardized within specific fields of biology, they can differ dramaticallyacross taxa. Such differences in usage can impair our ability to unambiguously refer to anatomical position whencomparing anatomy or phenotypes across species. We developed the Biological Spatial Ontology (BSPO) tostandardize the description of spatial and topological relationships across taxa to enable the discovery ofcomparable phenotypes.Results: BSPO currently contains 146 classes and 58 relations representing anatomical axes, gradients, regions,planes, sides, and surfaces. These concepts can be used at multiple biological scales and in a diversity of taxa,including plants, animals and fungi. The BSPO is used to provide a source of anatomical location descriptors forlogically defining anatomical entity classes in anatomy ontologies. Spatial reasoning is further enhanced in anatomyontologies by integrating spatial relations such as dorsal_to into class descriptions (e.g., dorsolateral placodedorsal_to some epibranchial placode).Conclusions: The BSPO is currently used by projects that require standardized anatomical descriptors forphenotype annotation and ontology integration across a diversity of taxa. Anatomical location classes are alsouseful for describing phenotypic differences, such as morphological variation in position of structures resulting fromevolution within and across species.Keywords: Anatomy, Spatial relationships, Position, Axes, Reasoning, BSPO, Ontology, PhenotypeBackgroundVariation among anatomical phenotypes, whether acrossspecies or between mutant and wildtype model organisms,frequently involves changes in position and orientation ofstructures. Among fish species, for example, the positionof the mouth may be ventral, dorsal, or terminal; bonyvertebral processes may be oriented laterally or medially;pelvic fins may be located posteriorly or anteriorly relativeto the abdomen. Computation across phenotypes thusrequires a vocabulary of positional terms to understandthe patterns of variation in the positioning of structures* Correspondence: wasila.dahdul@usd.edu1Department of Biology, University of South Dakota, Vermillion, SD, USA2National Evolutionary Synthesis Center, Durham, NC, USAFull list of author information is available at the end of the article© 2014 Dahdul et al.; licensee BioMed CentralCommons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.relative to others within and between organisms, and tounderstand the possible relationships to gene expressionand regulation. Positional terms have long been used inanatomy to describe the spatial aspects of the impressivediversity of organismal forms of both plants and animals.For example, positions in animals are often described inrelation to those of a bilaterally symmetrical animal(Figure 1). Accordingly, the primary or main axis is con-sidered the anterior-posterior (AP) axis, which extendslongitudinally from head to tail. The dorsal-ventral (DV)axis is recognized in that ventral typically faces toward,and dorsal away, from a substrate (meaning towards theground for land-dwelling organisms or towards the oceanor river/lake bottom for marine or aquatic organisms),whereas the left-right (LR) axis is defined in relation to aLtd. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,oralaboraldorsalventralrightanteriorposteriorleftrostralcaudalanterior posteriordorsalventralrightleftanterior posteriorA BCFigure 1 Comparison of primary organismal axes designated in a diversity of species and their representation in BSPO. In fishes(A) and in humans (B), anterior-posterior axis (narrow synonym rostral-caudal axis in humans) is shown in red, dorsal-ventral axis (narrowsynonym anterior-posterior axis in humans) shown in blue, and left-right axis shown in yellow. A cnidarian (sea anemone) (C) is bilaterallysymmetrical and has an oral-aboral axis, shown in orange.Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 2 of 13http://www.jbiomedsem.com/content/5/1/34plane running along the anterior-posterior midline. Wecreated the Biological Spatial Ontology (BSPO) to develop,define, and standardize terms that can be used to describespatial and topological relationships, at multiple biologicalscales from cells to whole organisms, and across diversetaxa.In the past two decades the developmental and geneticunderpinnings of positional axes have been investigatedfor model species, and highly conserved key patterningmolecules have been identified across widely divergenttaxa. Overlapping patterns of Hox gene expression, forexample, are required for organization along the AP bodyaxis in bilaterian animals [1]. Wnt/?-catenin expressionhas also been shown to determine primary body axisorientation in both bilaterian and non-bilaterian animals[2]. The DV axis is patterned by the chordinbonemorphogenetic protein (BMP) network and is conservedacross organisms as diverse as flies and humans (reviewedin [3]). Nodal signaling has been shown to control LRsymmetry, which also appears to have an ancient prebila-terian origin [4]. Within plants, homeobox genes, such asknotted-like homeobox (knox), also play a central role inspatial developmental patterns [5]. Although specificationof organismal axes may appear straightforward with respectto their application within model organisms (e.g., Arabi-dopsis, Caenorhabdites elegans, Drosophila, Danio rerio,Xenopus, mouse, etc.), there are taxon-specific differ-ences in the application of spatial terms, such as tohuman anatomy, that render the development of auniversal terminology complicated. Moreover, there arefundamental differences across the more than 35 animalbody plans (e.g., tapeworms, sea urchins) and variousplant growth forms (e.g., tree, shrub, herb, and thallus)that present some very difficult axes to interpret. Despitethese challenges, which we describe further below, thedevelopment of a set of spatial classes is necessary forquery and description of phenotypes across species.Here we describe the development of the BSPO, whichcontains 146 classes and 58 relations representinganatomical axes, gradients, regions, sections, sides, andsurfaces that apply to whole organisms and their parts.The BSPO is integrated with other ontologies and iscurrently used by projects that require standardizedspatial descriptors for anatomy ontologies, ontologyintegration, and phenotype annotation. For example,the free-text description anterodorsal margin of oper-cle can be represented formally as BSPO:anterodorsalmargin part_of some opercle (the latter class from anDahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 3 of 13http://www.jbiomedsem.com/content/5/1/34anatomy ontology). Because it is driven by researchneeds, the spatial terminology currently represented inBSPO is particularly developed for animals and, to a lesserextent, plants. However, BSPO is organized in a frame-work that is flexible enough to incorporate spatial termin-ology for other taxa (e.g., fungi).Results and discussionOntology organization and contentClassesa in BSPO represent various aspects of spatialorganization and are partitioned into categories for ana-tomical axes (14 classes), anatomical surfaces (12 classes),anatomical regions (81 classes, including margins andsides), anatomical gradients (6 classes), and anatomicalplanes (7 classes) (Figure 2). BSPO classes for anatomicalcompartment and anatomical compartment boundary(11 classes) refer to anatomical structures defined bylineage restriction [6] rather than by axial position, andthus these classes will be moved to the Common AnatomyReference Ontology (CARO) [7] in the future. Individualcompartments and their boundaries are typically namedwith respect to some axis. For example, most of the imagi-nal discs and embryonic segments of insects are bisectedby a boundary running medial to lateral that cells do notcross during development [8]. The regions of the discor segment anterior and posterior to the boundary arereferred to as anterior and posterior compartments,respectively, while the boundary is referred to as theanterior-posterior compartment boundary. BSPO pro-vides relationships that allow these compartments andanterior-posterior axisanis_aanterior sidestarts_axisposterior sidefinishes_axisanterior_toposterior_toopposite_tois_aanatomical sideis_aanterior marginoverlapsanatomical marginis_aanatomical regionis_aanterior regionoverlapsis_asurface_anatomical passesanatomical gradientdorsal-ventapproximately_perpendicular_toanterior-posterior gradienthas_axisis_ais_aanatomical structureis_amaterial anatomical entityis_ais_ais_aFigure 2 Organization of high-level spatial classes in BSPO and some(pink fill), anatomical axis (blue fill), anatomical plane (purple fill), and anawhite fill. Subclass (is_a) relations are shown in black and spatial relations inboundaries to be defined with respect to anatomicalaxes, in individual cases, but general classes such asanterior compartment seem of dubious usefulness andso will not be maintained in either BSPO or CARO.BSPO classes are linked by a rich set of 58 relationshiptypes. In addition to their logical relationships, all BSPOclasses have text definitions that are written as broadlyas possible to encompass taxonomic variability in bodyform. Synonyms are included where applicable and includecommonly used abbreviations for terms such as LR axisfor left-right axis.BSPO is open to all users and freely available in OBOand OWL formats at http://purl.obolibrary.org/obo/bspo.{ obo,owl }. BSPO can also be browsed online athttp://www.ontobee.org/browser/index.php?o=BSPO.Anatomical axesPrimary organismal axesAxes form the basis of the BSPO, with other concepts,such as relations and planes, defined in terms of theseaxes. In animals, three whole body axes are generallyapplicable (described below). Unlike the case in animals,there is generally no single primary organismal axis for awhole plant. Instead, axes are described for one or moremodular organs that compose a plant, such as shoots(stems and branches), roots, and phyllomes (leaves,petals, etc.) (see Axes of organism parts, below).In animals, the AP, DV, and LR axes (Figures 1A, B)are applicable to Bilateria and most of their descendants.The Bilateria include all metazoans except the spongesatomical axisanterior surfaceofanatomical surfaceis_aentity_throughimmaterial anatomical entityis_aanatomical planesagittal planeis_amidsagittal planeis_aral axisis_aorthogonal_toleft-right axisis_ais_a is_ais_aanatomical boundaryis_ais_aanatomical lineapical-basal axis relative to substrateis_aapical-basal axis relative to direction of growthis_aof their children. Anatomical region (green fill), anatomical gradienttomical surface (yellow fill). Parent classes from CARO are shown withorange.mouth anusFigure 3 An individual zooid of the colonial ectoproct Bugula.This species possesses a U-shaped gut and the location of the anusis adjacent to the mouth. Image based on illustration from theBIODIDAC image library.Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 4 of 13http://www.jbiomedsem.com/content/5/1/34(Porifera), placozoans, cnidarians, and ctenophores. Mostsponges are asymmetrical as adults, although an AP axishas been identified in their larvae [9]. Cnidarians areprimitively bilaterally symmetrical [10], with radial orbiradially symmetrical axes developing in more derivedmembers of the clade (Figure 1C). Genes regulatingbilaterian head development are expressed in the seaanemone at the larval aboral pole, indicating that theanterior, head-forming, region of bilaterians and theaboral region of cnidarians may have been derivedfrom the same domain of their last common ancestor[11]. Bilateral symmetry of the body, including an anteriorhead with an oral opening and a posteriorly extendedtrunk/tail with an anal opening, is thought to characterizethe common ancestor of Bilateria. Many textbook defini-tions of the three fundamental axes of bilaterians (AP, DV,LR) reference structures such as head, oral opening/mouth, anus, tail or gut that are not present in alllarval or adult bilaterians. Our definitions for these axesalso reference anatomical structures, but aim to use onlythe minimum that are those hypothesized to be presentbased on phylogenetic reconstruction of the ancestralbilaterian [12].In defining anterior-posterior axis in BSPO, we designateanterior as the end of the animal with a head. Interest-ingly, a head, however defined (e.g., based on concentrationof neurons [11], sensory structures, oral opening), hasbeen lost multiple times in development and evolution(e.g., adult tunicates, echinoderms, bivalve molluscs,ectoprocts, endoprocts), and as such the AP axis is hardor impossible to identify in these taxa. Although the oralopening/mouth is used as a proxy for an anterior end,it, as well, has been lost multiple times in various taxa(acanthocephalans, pogonophorans) or moved posteriorlyin others (flatworms such as planarians) [13]. On the otherend, criteria for recognizing posterior are conventionallyrelated to an anal opening at or near to the terminus ofthe body. However, given multiple independent lossesof an anal opening (e.g., gnathostomulids, some echi-noderms) and the many taxa with a U-shaped gut inwhich the anus is adjacent to the mouth (e.g., sipunculids,ectoprocts, entoprocts, some gastropods) (Figure 3), usinga digestive tract as a proxy for the longitudinal axis (AP)of the body is problematic. Interestingly, the U-shaped gutin some taxa is AP regionalized using highly conservedtranscription factors [14]. As Minelli [15] points out, manytaxa, such as those with a U-shaped gut, demonstratedissociation between an apparently evident elongate APsomatic body axis and a very different visceral axis. Infact, dramatic metamorphic development of many inverte-brates renders body axes very difficult to interpret (e.g.,[16]). Even in taxa with distinct head and tail ends, modifi-cations in body form can result in unconventional applica-tion of AP axis terminology. For example, seahorses, withtheir distinct upright posture, orient their AP axisperpendicular relative to the substrate rather than parallel[17]. Differential application of axes to the body and itsparts is necessary in cases where they have been dissoci-ated in development or evolution.Developmental and evolutionary changes to the DV axislikewise pose challenges for simple application of termin-ology. In BSPO, dorsal-ventral axis is defined as An axisthat is approximately perpendicular to the anterior-posterior axis and that extends through the horizontalplane of the body. An inversion of the DV axis occurredduring evolution resulting in correspondence between theventral side of arthropods and the dorsal side of verte-brates, as evidenced by phenotype (position of the neuralcord/tube) and inversion of the Chordin/BMP/Tolloidpathway markers [18].Another challenge in the application of the terminologyof the fundamental bilaterian axes (AP, DV) is that theseaxes are uniquely conflated in humans and other anthro-poid apes that are bipedal. In humans, superior is appliedDahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 5 of 13http://www.jbiomedsem.com/content/5/1/34to the head end (anterior) and inferior towards the feet.Anterior and posterior are applied to the human front(ventral) and back (dorsal), respectively (see also Axes oforganism parts, below). We have added human-specificterminology as synonyms to BSPO (Figure 1B) to assist inunambiguous reference to anatomical position whencomparing anatomy or phenotypes across species.In BSPO, the left-right axis is defined as An axis thatextends through an organism from left to right sides ofbody, through a sagittal plane, and it is orthogonal_tosagittal plane. The LR axis of many organisms is alsomodified in development and evolution. For example,flatfishes (order Pleuronectiformes) undergo a dramaticdevelopmental change in the LR body axis. In ontogeny,the left or right side of the body comes in contact withthe substrate, and one eye migrates to the other half ofthe head. Thus one side (left or right depending on thespecies) has two eyes and the side in contact with thesubstrate is referred to as "eyeless" or "blind". Descriptionof this modified anatomy requires specialized terms torefer to the blind side and eyed side of the fish. Otherstructures typically located along the midsagittal plane,such as the dorsal fin, are displaced horizontally. Reason-ing across flatfish and unmodified vertebrate eye morph-ologies may thus require specifying the spatial location(left or right side) of the blind or eyed side of theorganism.Despite the modifications to the primary axes wedescribe above, larvae and adults of many animal taxado in fact retain the ancestral bilaterian AP, DV, and LRaxes, and many conserved molecular and genetic deter-minants of these axes have been described in model or-ganisms. Few of the non-model taxa with the interestingdeviations from symmetry described above have beeninvestigated from a developmental or genetic standpoint,and thus much remains to be discovered and understoodabout axis specification.Several other primary organism axes are representedin BSPO. The medial-external axis extends from aninternal point towards the outside of the body or bodypart. This class is a superclass of medial-lateral axis(ML) and medial-radial axis (MR). In animals, the MLaxis applies to the left or right sides of a bilaterallysymmetrical animal. The oral-aboral axis (Figure 1C)is defined as the axis that extends from the oral open-ing to the furthest point in an organism that is directlyopposite. It is the major axis in cnidarians, cteno-phores, and echinoderms. During development, ananimal-vegetal axis (AV) is defined for most animaleggs, where the yolky (less rapidly dividing) end isvegetal and the less yolky (more rapidly dividing) endis animal. These terms are also often applied to thepoles (e.g., the animal pole) and the hemispheres (e.g.,the animal hemisphere).Axes of organism partsIn plants, as mentioned above, the axes primarily relateto organismal parts and are generally defined relative tothe direction of growth. The main axis of growth is typic-ally the apical-basal (AB) axis, which is determined by thegrowth of an apical meristem or apical cell. In BSPO, werefer to this axis as apical-basal axis relative to directionof growth to distinguish it from the apical-basal axis rela-tive to substrate (described below), which is applied toanimal bodies. In Figure 4A, which shows a seedling of avascular plant, an AB axis suggests a single straight linerunning through the center of the plant, from the tip ofthe root to the tip of the shoot apical meristem. However,even in this very simple plant, there are two AB axes, onefor the shoot system and one for the root. Most plantshave more complex, branching growth forms withmultiple AB axes. While one might describe theabstract, overall shape of a plant (e.g., an ellipsoid, cube,or pyramid) and define axes for that shape, those axeswould not necessarily relate to the actual axes along whichthe plant develops. The AB axis for a whole plant can beused only with the simplest of growth forms, such as anon-branching liverwort or fern thallus. In plants withsecondary growth (that is, growth that thickens axial or-gans such as stems), the medial-radial (MR) axis extendsfrom the center of the organ to the outside. The DV andML axes for a whole plant are also used with thalloidgrowth forms (whether branching or not), because theygrow roughly in a plane along the surface of the substrate(Figure 4B).In animals, the apical-basal axis relative to substrateis often applied to substrate-bound organisms such asPorifera, where the basal direction is towards the sub-strate. For bilaterian animals, this axis often refers to cellor tissue-level axes where one portion of the cell or tissueis adjacent to a substrate, such as a basal lamina or laminapropria, and the apical portion faces a lumen, for examplean intestinal epithelial cell with its microvilli facing thelumen of the intestine.In animals and plants, a proximal-distal axis (PD) isused to describe the position of parts in relation toattachment to another part, such that parts closer to theplane of attachment (e.g., the point where a leaf attachesto a branch) are proximal and those further away aredistal (Figure 4A). In animals, the terms proximal anddistal are often applied to outgrowths of the body, suchas limbs and other appendages such as antennae, para-podia, and feathers. In animals the regulatory genedistal-less has a role in specifying the PD axis, and it isexpressed in the distal portion of many appendages[19]. Proximal-distal terminology can also be appliedacross different levels of anatomical organization toorgans, tissues, and cells; for example, the proximal/distalepiphysis of femur, the proximal/distal collecting tubulebasalapicalapicalproximaldistaladaxialabaxialdorsalventralsuperiorinferioranteriorposteriorposteriorrostralcaudalcaudalADEdorsalanterior posterioranterioranteriorposteriorposteriorCventraldorsalventralbasalapicallaterallateralBmedialbasalapicalproximaldistalFigure 4 Axes applied to organism parts. In vascular (A) and non-vascular plants (B), the apical-basal axis relative to direction of growth(purple) runs in the direction of apical growth, in both shoots and roots. For lateral organs such as branches or leaves (A), the primary axis is theproximal-distal axis (green) and the adaxial-abaxial axis (pink). In plants or organisms with a thalloid growth form (B), the apical-basal axisrelative to direction of growth often runs parallel to the substrate, resulting in a dorsal-ventral axis that runs perpendicular to the substrate anda medial-lateral axis that is perpendicular to the apical-basal axis. C) Hippocampal pyramidal neuron, showing the application of the BSPOclasses apical-basal axis relative to substrate and proximal-distal axis to the whole cell or portions thereof. D) AP axes for the head, neck andtrunk of the giraffe. Note that these axis definitions delineate a bent version of the primary AP axis. E) AP axis of the human brain (double-headedred arrow) relative to the AP axis of the body (single red arrow). Note the use of superior and inferior to refer to structures relative to the substrate.Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 6 of 13http://www.jbiomedsem.com/content/5/1/34of the kidney, or the distal/proximal apical dendrite of aneuron (Figure 4C).In plants, proximal and distal should be applied toorgans or organ parts that do not develop from an apicalmeristem (and therefore have no AB axis) such as vascu-lar leaves, leaflets, petals, or sepals. The PD axis can alsobe used for organs with an apical meristem that branchfrom another organ, such as branches or lateral roots,but in these examples it is redundant with the AB axis.The adaxial-abaxial axis (AA) is also important forleaves and other types of phyllomes, with adaxial beingadjacent to the shoot axis (usually the top of the leaf )and abaxial being away from the shoot axis (usually thebottom of the leaf ). If a leaf or other organ is held hori-zontally, the adaxial-abaxial axis may be described asdorsal-ventral. The distribution of tissues varies alongthe AA axis in leaves, including characteristics of eachsurface, reflecting the different microclimates on theadaxial versus the abaxial sides of the leaf, such as sunexposure and humidity.Medial-external axes are also applied to parts of anorganism. The medial-radial axis in plants is used todescribe organs or organ parts that are roughly circularin cross-section, such as stems, roots, and petioles,while medial-lateral axis is used to describe laminar(flattened) plant parts such as many leaves and petalsor some shoot axes (e.g., cactus paddles) that expandthrough growth of marginal meristems [20]. Variationin the development of meristems along the AA, PD, orML axes results in much of the variation found in leafDahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 7 of 13http://www.jbiomedsem.com/content/5/1/34shapes. For example, some leaves that have a roundedcross-section, such as some species of Sanseveria, havean early developmental pattern in which growth ofeither the abaxial or adaxial leaf meristem is suppressed,to the effect that the opposite meristem (adaxial or abax-ial, respectively) grows around to cover the entire surfaceof the leaf.In cases where the application of axis terminology isdifficult, molecular determinants may be used as evidencefor spatial reference of body parts. For example, the regionof the fin or limb bud in vertebrates with a high concen-tration of sonic hedgehog (Shh) is posterior becauseShh posteriorizes the phenotype [21]. Note that thiscan apply to either portions of a given body axis, or tostructures that are not themselves part of a main bodyaxis (see also Anatomical gradients below).Axis terminology applied to substructures of an animalrequires reference to a main axis of the body, such asanterior or posterior, and sometimes the ancestralcondition of the body. For example, in the giraffe(Figure 4D), the AP axis is applied to several bodysegments (head, neck, trunk) and the DV axis is desig-nated as perpendicular to the AP axis for each of thesesegments. As a result, the DV axis of the neck is nearlyparallel to the AP axis of the trunk. Similarly, for humansand other bipedal anthropoids, the application of anorganismal head or brain axis is uniquely conflated withthe primary axis of the organism. In this case, the AP axis(often called rostral-caudal) of the human brain is atalmost a right angle to the AP axis of the rest of the body(Figure 4E).The traditional use of superior and inferior refersto parts that are the furthest or nearest to the substraterespectively. In BSPO, we define inferior side andsuperior side classes to support reference to the sub-strate. However, confusion can arise when these terms areapplied to homologous structures across species wherethey may not retain the same relationship to the substrate.For example, the human superior vena cava is furtherfrom the substrate than the inferior vena cava, but in themouse, these terms no longer reference differential distancefrom the substrate. For this reason, we do not recommendtheir use in defining axes or relations to axes, for structuresthat are likely to be compared across taxa.The BSPO does not yet have a complete terminologyfor describing the spatial dimensions of fungal anatomy,which could be integrated with existing anatomy ontol-ogies for fungi (Fungal Subcellular Ontology [22] andFungal Anatomy Ontology (FAO; http://purl.obolibrary.org/obo/fao.owl)). Nonetheless, BSPO can easily accom-modate the spatial terminology used to describe fungi,and some existing BSPO terms are applicable to fungalanatomy. For example, lateral is used in fungi, as inanimals, to refer to the side of the organism [23], andthe medial-radial axis can be used to describe cylin-drical structures in fungi such as the stem or stalk of amushroom. The dorsal-ventral axis and medial-lateralaxis used to describe thalloid plant structures (Figure 4B)could easily be applied to thalloid lichens. The termsadaxial and abaxial are used in fungi, similar to theirapplication in plants, to describe the side of an anatomicalstructure that is adjacent to or away from the long axis ofanother structure. While the abaxial-adaxial axis canbe used fairly generally to describe multiple types oforgans in plants, within fungi, adaxial and abaxialare restricted to describing the sides of basidiosporesin relation to the basidium, a specialized cell or organin the basidiomycetes [23].Relations along anatomical axesFifty-eight relations have been specified for use withBPSO terms. For each axis in BSPO we define a pair ofrelations specifying relative position along the axis. Forexample, for the DV axis we have the relations dorsal_toand its inverse ventral_to. An entity x is dorsal_to anentity y if x is further along the DV axis than y towardsthe dorsum. Each of these relations is also declared tobe transitive (i.e., if x is dorsal_to y, and y is dorsal_to z,then x is dorsal_to z). We also define non-transitiveversions of these relations, e.g., immediately_dorsal_toand immediately_ventral_to as subproperties of the tran-sitive forms. These are useful for specifying the order ofserially arranged, contiguous structures such as the tag-mata and segments of an arthropod body, the segments ofan arthropod leg, or internodes of a plant stem.Additional challenges in the application of anatomical axesAlthough the designation of the primary organism axesmay appear straightforward, pronounced developmentaland evolutionary changes in organ presence, morphology,and symmetry in many taxonomic groups have madethese axes biologically difficult to interpret and thus madeit correspondingly difficult to apply a standardized termin-ology. The evolutionary shift to pentaradial symmetry inthe adults of extant echinoderms, starfish, brittlestars, seaurchins, sand dollars, and crinoids is one of the mostspectacular examples. All echinoderm larvae are bilaterallysymmetrical, but upon metamorphosis, little or no traceof the larval AP axis remains in the pentaradial adult [24].Whether there are five AP axes, one central AP axis, ornone at all is still under molecular and genetic investi-gation. Similarly, the bilaterally symmetrical swimminglarvae of tunicates, with their characteristic chordatefeatures including pharyngeal arches and a post-analtail, metamorphose into sedentary sac-like adults withno apparent remnant of an AP axis. The tapeworm lacks aclear AP axis: adults lack a digestive tract (no mouth oranus) and neither end contains a concentration of neuronstransverse planeradialplanetangentialplanetransverse plane midsagittal planehorizontalplaneABFigure 5 Anatomical planes in BSPO. A) The three anatomicalplanes used to describe bilaterally symmetrical organisms are midsagittalplane (blue), horizontal plane (red), and transverse plane (purple).B) Anatomical planes used to describe wood (secondary xylem)anatomy. A transverse plane (purple), or cross-section, is perpendicularto the apical-basal axis relative to direction of growth in an axial organor to a proximal-distal axis in a lateral organ. A radial plane (green)follows the two dimensions specified by an apical-basal axis relative todirection of growth and a medial-lateral axis. A tangential plane(orange) is perpendicular to a radial plane.Dahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 8 of 13http://www.jbiomedsem.com/content/5/1/34that might be considered cephalic. Although the end withthe holdfast organ (scolex) is commonly consideredanterior, evidence including the manner of developmentof new segments and the positioning of testes relative toovaries within segments points to the opposite conclusion[15]. Biologically meaningful application of anatomicalposition terms requires further molecular and geneticunderstanding of the development of these taxa.Within plants, axis specification across species is fairlystraightforward because of the association between axesand developmental patterns (i.e., the apical-basal axisrelative to direction of growth is associated with apicalgrowth and the medial-radial axis is associated withradial growth). Nonetheless, unusual developmentalpatterns, such as the adaxialization of cylindrical leaves(described above under Axes of organism parts) canobscure the normal axes used to describe plant structures.Thus, it is the precise specification of spatial terminologythat allows for logical comparisons among forms thatdeviate from the norm.Anatomical planes and sectionsAnatomical investigation is frequently based on histologicalsections (i.e., anatomical planes) to support a better under-standing of three-dimensional structure. For example, longbefore the days of computerized image reconstruction,anatomists leveraged coronal, horizontal, sagittal,and parasagittal tissue sections to support inferredthree-dimensional representation of anatomical entitieswithin an animal. These are evident in numerous landmarkatlases such as The Rat Brain in Stereotaxic Coordinatesby Paxinos [25] and Kaufmans The Atlas of MouseDevelopment [26]. Even in more modern digital ap-proaches, reconstruction can happen only if the two-dimensional axes are accurately specified and registered(for examples, see [27]).Anatomical planes (Figure 5) are defined as perpen-dicular or parallel to an axis in BSPO. For example,sagittal plane is defined as Anatomical plane thatdivides a bilateral body into left and right parts, notnecessarily of even size and has relationships orthogo-nal_to left-right axis (Figure 2) and parallel_to anterior-posterior axis and dorsal-ventral axis. The use of BSPOcan aid integration and error-checking of section-basedviews through coordinates related to BSPO axes, based onthe logic within the ontology. For example, if a histologicalfeature is annotated to a particular structure that has inturn been declared to be located on the left side of theorganism, a right parasagittal section should not includesuch a structure.Traditional plant anatomy refers to three planes:transverse plane (or cross-section), radial plane, andtangential plane (Figure 5B). The transverse plane isused for plant parts that are both round or flattened incross-section, such as stems or leaves, whereas radialplane and tangential plane are generally used only withstructures that are roughly round in cross-section. Allthree planes are essential for the characterization andidentification of wood (secondary xylem found generallyin plant axes such as stems and roots), as woody tissuesappear different in each plane [28].The Foundational Model of Anatomy (FMA) ontology[29] for humans has an extensive classification of planes.These include horizontal anatomical plane, Frankfurtplane, and thoraco-abdominal plane. The FMA usesthese planes to demarcate the boundaries of organismsubdivisions such as the thorax. We cross-referenceFMA classes where they exist in the representation ofplanes in BSPO but focus on planes that are widelyapplicable across organisms.In addition to the relations along anatomical axes de-scribed above, we specify a number of relations relativeto anatomical planes. For example, relative to the sagittalplane, ipsilateral_to holds between two structures on thesame side of an organism; contralateral_to holds betweenDahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 9 of 13http://www.jbiomedsem.com/content/5/1/34two structures on the opposite sides of an organism. Incontrast to the standard axial relations, these are nottransitive, but they do hold the characteristic of beingsymmetric. If x is on the same side as y, then it must bethe case that y is on the same side as x.Anatomical topology: regions, sides, and marginsUnlike axes and planes that are immaterial, the ana-tomical topology classes in BSPO refer to the materialregions, sides, and margins of anatomical structures.These are labeled and defined in BSPO relative to theaxis classes. For example, subtypes of anatomical region(Figure 2) include anterior region, dorsal margin, andposterior side. These classes can be used to spatiallydefine anatomical structures relative to an axis of thewhole organism. Variation in the topology of homologousstructures across species is informative for phylogeneticinference. Examples include differences in surface fea-tures, such as the textured or smooth surface of cranialbones in catfishes [30], differences in the margins of skel-etal elements, such as the dorsal margin of the ilium inamniotes [31], and differences in the adaxial and abaxialregions of a leaf (Figure 4A).Anatomical sides are defined with the non-transitivesubproperties of part_of that specify which side of abisecting plane a structure is part of. Where these refer-ence the axes of the whole organism, they can apply to aside of either the whole organism or its substructures.For example, in_left_side_of can apply to the position ofthe heart relative to the whole organism, or apply to partof the heart, such as its left side. Where the referencedside only applies to some part of an organism, so do therelations. For example, in the long bones of limbs thathave proximal and distal sides, the proximal epiphysisof the femur can be defined as an epiphysis that isin_proximal_side_of the femur. We also define propertychains to propagate information about sides down thepartonomy, so that, for example, if X part_of Y and Yin_left_side_of heart then a reasoner can infer that Xin_left_side_of the heart.Some structures are not completely on one side or theother of a bisecting organismal plane but instead crossit. For example, the heart may asymmetrically span themidsagittal plane of an animal. For such cases, we definethe relation: intersects_midsagittal_plane_of. This relationapplies to midline structures such as the single unpairednostril of the hagfish, which is positioned along themidline of its head (median external naris EquivalentToexternal naris and intersects_midsaggital_plane_of somehead). This relation does not imply that the structure isunpaired, although this may often be the case. Structuresto which the intersects_midsaggital_plane_of does notapply stand in a in_lateral_side_of relation to the whole.For example, in most vertebrates, the naris (nostril) isbilaterally paired, and it is thus declared in UBERON (thecross-species metazoan Uber Anatomy Ontology) [32,33]as being in_lateral_side_of a head. This relation does notimply, however, that the structure is paired. To indicatewhether a structure is paired or unpaired, classes such asbilateral from the Phenotype and Trait Ontology (PATO)[34] can be used, although further work needs to be doneto connect these PATO classes to BSPO.Anatomical gradientsAnatomical gradients are defined in BSPO as Materialanatomical entity defined by change in the value of somequantity per unit of distance across some spatial axis.Note that these classes are defined as structures wherebythe differentiating characteristic is the distribution ofsome factor across a gradient. For example, Sonic hedge-hog (Shh) is expressed in a posterior to anterior gradientin the developing limb buds of vertebrates. The concen-tration of Shh is interpreted by the cells and influencesthe phenotypic outcome of digit morphology accordingto the gradient [21]. An anatomical gradient can also beapplicable to the whole organism, such as in the case ofearly anterior specification by bicoid, a maternal effectRNA that is translated in the fertilized egg and wasdiscovered in Drosophila melanogaster in the 1980s(see [35] for review). Anatomical gradient subclassesfor some of the primary organismal axes are includedin BSPO, for example, anterior-posterior gradient,which could be used to indicate the presence of thebicoid morphogen in the example above.BSPO and interoperability with other ontologiesThe classes and relations in BSPO uniquely representthe spatial aspects of anatomical entities and can be usedto create class expressions to enable spatial reasoning.UBERON simplifies the specification of spatial patternsin taxon-specific anatomy ontologies by doing this, e.g.,UBERON:forelimb BSPO:anterior_to some UBERON:hindlimb. Thus use of BSPO in UBERON can be lever-aged to infer spatial relations by new or existing anatomyontologies without those relationships. For example, fore-limb and hindlimb in the Xenopus Anatomy Ontology(XAO) [36] reference the UBERON classes for forelimband hindlimb, and therefore it can be inferred that aXAO:forelimb is anterior_to some XAO:hindlimb.Pre-composition using BSPO classes can enhance thedefinitions of some classes in anatomy ontologies thatrefer to the spatial aspects of structures. For example,the Plant Ontology [37,38], a unified vocabulary for allgreen plants contains a class for phyllome base that isdefined as The basal part of a phyllome, where it attachesto a shoot axis. Currently, only the relationship part_ofphyllome is specified in the ontology, but a more preciselogical definition could be created by specifying that aDahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 10 of 13http://www.jbiomedsem.com/content/5/1/34phyllome base is a BSPO proximal region that is part_ofa phyllome. The Gene Ontology (GO) also containsclasses that could be defined in terms of BSPO classesand relations. For example, GO:AP axis specification isdefined as The establishment, maintenance and elabor-ation of the anterior-posterior axis. The anterior-posterioraxis is defined by a line that runs from the head ormouth. This class could also be formally related to BSPO:anterior-posterior axis.Post-composition allows one to create classes that aremore granular than those available in an anatomy ontol-ogy while avoiding the complexity and potential unwieldi-ness to the ontology that may result from pre-composingvery specific classes [39]. Thus post-composing classes forthe regions, margins, and surfaces of structures neededfor annotation avoids creating a great number of pre-composed classes. BSPO is used by the Phenoscape pro-ject (Phenoscape.org; [40,41]) to create post-compositionsfor the annotation of morphological variation within andamong vertebrate species resulting from evolution. Forexample, the posterior location of a bony projection onthe cleithrumb (a shoulder girdle bone) is representedby combining the following anatomical and spatialclasses: anatomical projection part_of some BSPO:-posterior region and part_of some cleithrum. Spatialclasses are also used to specify the region of a structurethat varies in some quality; for example, BSPO:anteriormargin part_of some scapula is annotated as concaveor straight using quality classes from PATO. BSPO isalso used in post-composition for the annotation ofgene expression in ZFIN (zfin.org; [42]) and used toformally represent taxonomic species descriptions forwasps [43].PATO is an ontology of biological qualities that con-tains a number of relational qualities representing spatialconcepts. Formally there is a difference between thesespatial qualities and BSPO relations: PATO relationalqualities are classes (e.g., dorsal to) rather than relationsas in BSPO (e.g., dorsal_to). This difference manifestsitself in concrete ways when modeling the world usinglanguages such as the Web Ontology Language(OWL). For example, A is dorsal to B (where A and Bare instances) is asserted as a simple triple < A dor-sal_to B>. However, to refer to this dorsality relation-ship (e.g., to say that A is more dorsal to B than it is toC; or that this dorsality is caused by some genetic alter-ation), the relationship must be turned into an individ-ual, i.e., a relational quality (reified relation). Thesespatial quality classes in PATO could be pre-composedwith the relevant BSPO class. For example, dorsalizedis defined as a bearer's gross morphology containingonly what are normally dorsal structures. This classcould be formally defined by relating it to the BSPOclass dorsal region.Use of BSPO for text miningBSPO is useful at different levels for the naturallanguage processing of morphological descriptions. Fortext mining software such as CharaParser [44], whichis being developed to assist biocurators in annotatinganatomical phenotypes, BSPO can be used at the lexicallevel as a dictionary for identifying spatial classes in freetext descriptions. This most basic usage of the ontologymakes more complex uses possible.After spatial classes are identified at the syntactic level,BSPO is used to post-compose anatomical entities whenpre-composed classes from an anatomy ontology, suchas UBERON, are not available. For example, for the phraseanterior margin of maxilla, CharaParser would proposethe expression BSPO:anterior margin and part_of someUBERON:maxilla after it failed to find term variationssuch as anterior margin of maxilla, maxilla anteriormargin, or maxillary anterior margin in UBERON.Phrases such as anterior process of the maxilla arehandled similarly in that post-composition is consid-ered only when pre-composed classes/components arenot found in ontologies. In this case, CharaParser wouldpropose the post-composition: UBERON: anatomical pro-jection (synonym: process) and part_of (BSPO: anteriorregion and part_of UBERON: maxilla), along with otherpossible proposals.Sometimes additional domain knowledge is needed toannotate a phenotype that is not obviously spatially related.For example, the semantics of the phenotype clavicleblades articulate is built on the knowledge that clavicleblades are bilaterally paired structures. The BSPO in_left_-side_of and in_right_side_of relations (children of theBSPO relation in_lateral_side_of) can be used to explicitlydefine this type of structure. This makes it possible forCharaParser to use the ELK reasoner [45] to find all struc-tures that are bilaterally paired in UBERON by obtainingthe union of (BSPO:in_lateral_side_of some Thing) and(part_of some BSPO:in_lateral_side_of some Thing).When CharaParser processes qualities that are in therelation_slim of PATO, such as articulated with, it willunderstand that two entities are expected and then lookinto a list of bilaterally paired structures for possiblematches for the two entities (i.e., clavicle blade and(in_left_side_of some multi-cellular organism) and clavicleblade and (in_right_side_of some multi-cellular organ-ism)). Note that non-bilaterally paired structures can alsouse PATO relational qualities (e.g., frontal PATO: articu-lated with parietal).Towards formalization of BSPO relationsThe BSPO is represented in OWL, which provides a lim-ited number of constructs for characterizing relationshiptypes. We make use of characteristics such as transitivity,superproperties, and domain/range constraints to allowDahdul et al. Journal of Biomedical Semantics 2014, 5:34 Page 11 of 13http://www.jbiomedsem.com/content/5/1/34for limited forms of reasoning. For example, if A is an-terior to B, and B is anterior to C, then the transitivitycharacteristic of anterior_to entails that A is anterior to C.Similarly, we can also trivially infer that C is posterior_toA, using inverse axioms. Other more sophisticated formsof reasoning are not possible at this time. For example,the orthogonal_to relation has domain and range con-straints (it holds between an axis and a plane), but it hasno definitional axioms that capture the textual definitionof crossing a plane at a right angle. Spatial extensionsto OWL would be required to rigorously capture thismeaning, but it is not clear what the use case for theseadvanced types of reasoning would be. One possibilitywould be the integration of classic description logicqueries with geometric 3D model or anatomical atlasdata. For example, asking for all genes expressed inepithelial cells dorsal to a plane formed by bisecting aparticular organ. One possibility is to extend OWLusing custom datatypes  this is possible using a systemsuch as OWL-Eu [46]. For many practical scenarios, itmay be sufficient to encode the logic of the relationdirectly into the query engine. This is an area that wouldrequire further exploration.ConclusionsThe BSPO supports unambiguous usage of positionalterminology in the context of anatomical data and in thebuilding of anatomy ontologies. BSPO also serves as asource of classes and relations for post-composition ofanatomical entities, a requirement for the representationof morphological variation within and among species.To aid in its use, we include textual information indicatingthe taxon-appropriateness of different classes and rela-tionships in BSPO. In the future, we will also includetaxon constraints [47] and add additional constraintsencoded as OWL axioms.The BSPO provides an ontological representation ofanatomical position classes that can be used for spatialreasoning. For example, queries can be enabled to findstructures that are proximal to one another, or to comparelevels of phenotypic variation in dorsal vs. ventral regions.Particularly in light of the high level of conservation ingene pathways underlying these axes across species (e.g.,BMP gradients in dorsal-ventral patterning), the BSPO iscritical to enable interesting queries across phenotypes atdifferent anatomical positions.MethodsBSPO contains classes and relations (object properties inOWL) for the representation of anatomical axes, gradients,regions, planes, sides and surfaces (Figure 2). Spatial classesare classified along a single subclass hierarchy with upperlevel classes (e.g., material anatomical entity, immaterialanatomical entity) imported from CARO. Coordination ofclasses with a new CARO release is ongoing, and we antici-pate making a coincident new release of both ontologiessoon. Some relations (e.g., part_of) used in the BSPO aredefined in the Relations Ontology [48] and more specificrelations (e.g., posterior_to) are exclusively defined inBSPO. The specific relations in the BSPO currently lackhigher-level parents in the Relations Ontology. Some rela-tions in BSPO are used to relate anatomical region classesto those of anatomical axis, such as anterior side whichhas a starts_axis relationship to anterior-posterior axis(Figure 2). Other commonly used relations in BSPOinclude overlaps (e.g., anterior region overlaps anteriorside), and surface_of (e.g., anterior surface is a surface_ofanterior side). Note that we define relations textually butwe are unaware of a way to create a complete formaldefinition using OWL, which has limited capabilities forreasoning with relations.The original version of BSPO was derived from theFlyBase annotation qualifier section of the FlyBaseJOURNAL OFBIOMEDICAL SEMANTICSYamagata et al. Journal of Biomedical Semantics 2014, 5:23http://www.jbiomedsem.com/content/5/1/23RESEARCH Open AccessAn ontological modeling approach for abnormalstates and its application in the medical domainYuki Yamagata1*, Kouji Kozaki1, Takeshi Imai2, Kazuhiko Ohe2 and Riichiro Mizoguchi3AbstractBackground: Recently, exchanging data and information has become a significant challenge in medicine. Suchdata include abnormal states. Establishing a unified representation framework of abnormal states can be a difficulttask because of the diverse and heterogeneous nature of these states. Furthermore, in the definition of diseasesfound in several textbooks or dictionaries, abnormal states are not directly associated with the correspondingquantitative values of clinical test data, making the processing of such data by computers difficult.Results: We focused on abnormal states in the definition of diseases and proposed a unified form to describe anabnormal state as a property, which can be decomposed into an attribute and a value in a qualitative representation.We have developed a three-layer ontological model of abnormal states from the generic to disease-specific level. Bydeveloping an is-a hierarchy and combining causal chains of diseases, 21,000 abnormal states from 6000 diseases have beencaptured as generic causal relations and commonalities have been found among diseases across 13 medical departments.Conclusions: Our results showed that our representation framework promotes interoperability and flexibility of thequantitative raw data, qualitative information, and generic/conceptual knowledge of abnormal states. In addition, theresults showed that our ontological model have found commonalities in abnormal states among diseases across 13medical departments.Keywords: Ontology, Abnormal state, Disease, Property, Attribute, InteroperabilityBackgroundWith the development of newer technologies, data andinformation exchange have been required for severalapplications such as electronic health records (EHR) inmedicine. Such data and information include abnormalstates. However, abnormal states are difficult to sharebecause of their heterogeneity, caused by the variety ofgrain sizes, from the level of cells, tissue, and organs tothat of the entire human body. This results in diverserepresentations with little uniformity. BFO [1,2] andDOLCE [3] have contributed to the formalization of thequality description of entities. BFO provides E (Entity),P (Property) (e.g., <Eye (E), red (P)>) and DOLCE providesE (Entity), A (Attribute), V (Value) triple (e.g., <esophagus(E), length (A), short (V)>). However, we found that thereare more complicated forms of quality representationsin medicine. For example, hypertension is a compound* Correspondence: yamagata@ei.sanken.osaka-u.ac.jpEqual contributors1ISIR, Osaka University, 8-1 Mihogaoka, Ibaraki, Osaka, JapanFull list of author information is available at the end of the article© 2014 Yamagata et al.; licensee BioMed CentCommons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.concept, which has three elements: blood, pressure, andhigh joined to form one concept/word. Another exampleis hyperglycemia, composed of four concepts: blood,glucose, concentration, and high. Furthermore, in the caseof intestinal polyposis, it is unclear whether intestineor polyp should be considered as the entity.This motivated us to establish a common frameworkfor the representation of abnormal states supported bysound theories. In this study, we investigate the repre-sentation of abnormal states from the content-orientedview, which focuses on how to capture the content to berepresented, on the basis YAMATO [4].YAMATO has been built to target both high utilityand philosophical soundness while maintaining compati-bility with BFO and DOLCE. In brief, YAMATO has thefollowing characteristics:a) Quality-related concepts (dependent continuantentities) are divided into Property, Genericquality, and Quality value. Quality in BFO isidentical to Property in YAMATO.ral Ltd. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 2 of 14http://www.jbiomedsem.com/content/5/1/23b) Quality value is classified in a manner identical tothe classification in scales of measurement.c) The context dependency of Ordinal value isrepresented by using the theory of Role.d) Multiple kinds of informational entities aresymbolically represented.For the quality description, representations withboth EAV and EP formalisms are defined. Furthermore,PATO2YAMATO aims to integrate phenotype >descriptionsthat exist in different structured comparison contexts [5].It allows (1) the classification of quality values, in whichscales of measurements are properly represented; (2) strictmodeling of the context dependency of ordinal values;and (3) clear distinction between true values and mea-sured data. It provides the mapping of ontology termsof PATO [6] to YAMATOs framework and enablesthe interoperability of the quality framework betweendifferent top-level ontologies such as BFO and DOLCE. Forexample, in the YAMATO framework, PATO:0000582(increased weight) is defined as a Property that is a com-bination of Generic quality (Attribute), weight, and acontext-dependent Quality value (Attribute Value), heavy.The context-independent value is defined as a classWeight quality value.Another issue is that in several medical textbooks ormedical dictionaries, abnormal states in the definitionsof diseases have not been directly associated with thecorresponding quantitative values of clinical test data (e.g.,ischemia in ischemic heart disease or muscular weak-ness in muscular dystrophy), which makes their processingby a computer difficult.Furthermore, clinicians often deal with abnormal statesspecific to each disease only in a particular medical division,which makes it difficult to spread awareness regardingthe common nature of abnormal states. To address theseissues, we have been developing abnormality ontology forthe systematization of knowledge regarding abnormalstates, using ontological engineering, which represents aunified framework [7]. We focus on abnormal states inthe definitions of diseases, which should be referred toin several applications. In addition, we discuss the rep-resentation of the various abnormal states on the basisof ontological theories in a consistent manner.Our claim in this study is not isolated to adopting oneof the representational forms used in the existing re-sources. The aim of our work is to formalize and organizedifferent representations used in clinical medicine on thebasis of ontological theories, and to realize the interoper-ability between them. It's not a simple matter of the use ofexisting resources such as PATO, LOINC [8], and others.Unified theoretical considerations make the various repre-sentation forms interoperable, which enables the establish-ment of a consistent and computer understandable modelfor abnormal states that are used in the definitions ofdiseases and medical data.In this study, we first define abnormal states andexplain our representation model. Then, we introduce ourontology of abnormal states and demonstrate an applica-tion of our work. We have constructed a disease ontologyand captured a disease as one or more causal chains ofthe abnormal states in the human body [9]. Till date, clini-cians have described the causal chains of approximately21,000 abnormal states for approximately 6,000 diseasesacross 13 medical departments. Thus, we believe that theuse of our ontology will contribute to various clinicalapplications.ResultsDefinition of abnormal statesIn the human body, abnormal states are highly diverseand involve various grain sizes, from the level of cells,tissue, and organs to that of the whole organism. There-fore, to systematize the knowledge about abnormal states,it is important to clarify the essential characteristics of theabnormal states, and to conceptualize them in a consistentmanner.In this section, we focus on the abnormal states thatappear in the definition of diseases rather than in reality.A state is modeled as a time-indexed propertya that isassociated with an entity, and has the value of an attri-bute that changes with time [10]. For example, imaginethe state of hunger. It is represented by being hungryor not at some time point in time. We define Propertyas a characteristic that is inherent in an entity, having anattribute along with its value, such as being red: <color,red>. Properties are distinct from attribute values; for ex-ample, the Property hypertension is differentiated froman Attribute Value such as high as in blood pressure ishigh. An Attribute Value has three subclasses: categoricalvalue (e.g., viviparous/oviparous), quantitative value (e.g.,160 mmHg), and qualitative value (e.g., high/low, large/small, much/few). On one hand, the Attribute Value highcan be used for several attributes such as temperature,density, and velocity. On the other hand, the Propertyhypertension cannot be used for the values of the above-mentioned attributes and it has a set of attributes andvalues like < pressure, high >.In several textbooks and dictionaries, diseases havebeen defined in terms of abnormal states. For example,the definition of diabetes is Diabetes mellitus is charac-terized by chronic hyperglycemia with disturbances of[11]. Another disease myocardial ischemia is presentedas The term acute myocardial infarction should be usedwhen there is evidence of myocardial necrosis with acutemyocardial ischemia [12]. Therefore, we can say that adisease can be defined in terms of an assertion about thepatient being in an abnormal state or not.Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 3 of 14http://www.jbiomedsem.com/content/5/1/23In the medical domain, various types of representa-tions for abnormal states are used, and we conceptualizethese representations into three categories:(1) Quantitative representation (e.g., blood pressure is180 mmHg, blood glucose concentration is135 mg/dL).(2) Qualitative representation (e.g., blood pressure ishigh, blood glucose concentration is high).(3) Property representation (e.g., hypertension,hyperglycemia).Because the upper ontology YAMATO [4] has been care-fully designed to cover the property, quality, and quantityontologies, it supports our work on abnormal states.A quantitative representation is important for diagnosisbecause a concrete value should be identified by clinicalexamination for each patient. However, in the definitionof a disease, a property such as being hypertensive orbeing hyperglycemic is essential instead of quantitativedata. Thus, as our basic policy, we first capture theabnormal states as properties, represented by a tuplelike < Property (P), Property Value (Vp)>. The PropertyValue takes a binary value, i.e., <true/false>. For example,if the state stenosis exists, it is described as < stenosis,true>. In addition, when necessary, a Degree Value (Vd)can be used for describing the degree of the PropertyValue, such as < stenosis, severe >.Some readers may think that a property represented inthe above manner is extremely conceptual to be of prac-tical use because of the lack of a representation, whichwould give a more concrete meaning to data. Therefore,we specify a property by decomposing it into a tuple: <At-tribute (A), Attribute Value (V)>. The Attribute Valuecan be either a Qualitative Value (Vql) or a Quantita-tive Value (Vqt). For example, in a case of a qualitativerepresentation, stenosis (P) is decomposed into < crosssectional area (A), small (Vql)>, and in another case ofa quantitative representation, stenosis (P) is describedas a concrete value, e.g., <cross sectional area (A),5 mm2 (Vql)>. This approach contributes to promotingconsistency in representation, as well as the interoper-ability between the quantitative raw data and thegeneric/conceptual knowledge regarding abnormal states(see after the section Interoperability between propertiesand attributes).In clinical medicine, decomposition of some prop-erties cannot be achieved, because the precise mech-anisms in the human body have not yet been completelyuncovered. For example, in the case of nausea, propertyrepresentation could be nondecomposable. Whether suchabnormal states represented in terms of properties definedabove can be decomposed into a known attribute and itsvalue will depend on advances in medicine.Representation of abnormal statesBasic representationIn this section, we introduce our representation modelfor clinical abnormal states and show that we can appro-priately represent them in a consistent manner.Because an attribute cannot exist by itself but alwaysexists in association with an independent object, we needto identify the object (hereinafter referred to as targetobject). For example, in the case of gastrectasia, thetarget object of its attribute volume is the stomach.Accordingly, we introduce the Object to represent thetarget object of the attribute and decompose the prop-erty into a triple: <Object (O), Attribute (A), AttributeValue (V)>. This is our basic representation model forabnormalities. For example, gastrectasia is decomposedinto < stomach, volume, large > b (Table 1(a), row 1).Extended representationWe recognize that some properties may be difficult todecompose into the basic triple representation, such as aratio and what we call a meta-attribute, discussed below.Accordingly, we introduce a Sub-object (SO) to repre-sent a focused object (see next paragraph) as an extendedrepresentation, so that a property can be decomposed intoa quadruple: <Object (O), Sub-Object (SO), Attribute (A),Attribute Value (V) >.In the case of a ratio, in addition to identifying thetarget object with the ratio, it should represent for whatwill be focused on (focused object). Therefore, weintroduce a Sub-Object (SO) to represent a focused ob-ject. For example, the representation of hyperglycemia isa quadruple, <blood (O), glucose (SO), concentration (A),high (V)>, where the Object is blood and the Sub-Objectis glucose [Table 1(a), row 3].There appear to be different kinds of ratios dependingon what is focused on. As a result, the Object and Sub-Object vary according to the type of ratio. Our representa-tion model can represent all of them, as shown in Table 1(b). A detailed discussion can be found in a report byYamagata Y et al. [13].Furthermore, we show the representation of a meta-attribute. In the case of the property gastric polyposis,although color and size are attributes of polyps, manypolyps is not an attribute of polyps because it is notinherent in each polyp. Following the meta-attributeapproach in YAMATO, where, in the case of the roadis curvy, number of curves is identified as a meta-attribute of curves, and the road, which has manycurves, can be represented in terms of it (the numbercurves). Accordingly, we regard the number of polypsas a meta-attribute of polyps and the stomach can bedescribed in the same manner as a road. By introducingSub-object, the property gastric polyposis can bedecomposed into a quadruple < stomach (O), polyps (SO),Table 1 Representations of abnormal states(a) Representation Abnormal statesProperty (P)PropertyValue (Vp) Attribute (A)AttributeValue (V) Object (O) Sub-Object (SO)Basic representationGasrtectasia (gastric dilation) True Volume Large StomachNausea True PatientExtended representationHyperglycemia True Concentration High Blood GlucoseGastric polyposis True Number Many Stomach Polyp(b) Variant of Ratio Abnormal statesProperty (P)PropertyValue (Vp) Attribute (A)AttributeValue (V) Object (O) Sub-Object (SO) Ratiom/n (no unit) High m ratio True Ratio High The whole Focused m/nExample Hyperglycemia True Concentration High blood glucose Glucose/Bloodm/n (focused on m ofsame object)High m ratio True Ratio High Object m m/nExample High Albumin ratio True Concentration High Urine Albumin Albumin/Creatininem/n (focused on the ratioof same object)High m/n ratio True Ratio High Object m/nExample Increased A/G ratio True Ratio High Blood A/G Albumin/Globulin(a): Basic and extended representations (b): Representations of ratios.Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 4 of 14http://www.jbiomedsem.com/content/5/1/23number (A), many (V)>, where the stomach is identifiedas the Object, and the polyps as the Sub-object [Table 1(a), row 4].Interoperability between properties and attributesOur claim in this clause is the interoperability betweenthe abnormal states and data. Considering the interoper-ability between the abnormal states and clinical test data,the OP form itself may not be compatible with theobservational data.A large amount of clinical test data is stored in hospitals.To ensure the cross-compatibility between those data andthe abnormal states (described in EHR), the unified formshould be required for computer processing, so that anexchange mechanism between the OP and OAV isindispensable.The OAV form can deal with both the quantitative andqualitative representation of values. Most clinical test dataare quantitative, e.g., the arterial cross sectional area of24 mm2 and can be represented by OAVqt as < artery (O),cross sectional area (A), 24 mm2 (Vqt) > in our model.Notably, the quantitative value can be converted to aqualitative value such as small (Vql), with the thresholdgiven by each hospital.In the case of an abnormal state arterial stenosis, wecan guarantee the interoperability between the quantita-tive data and abnormal state by decomposing stenosisinto cross sectional area is small, which is representedas < artery (O), cross sectional area (A), small (Vql) >.Another example is the quantitative data blood glucoseconcentration level is 260 mg/dL. It is represented by <blood (O), glucose (SO), concentration (A), 260 mg/dL(Vqt)>, and the interoperability between the quantitativetest data and the abnormal states hyperglycemia used inthe definition of diabetes is realized via qualitative repre-sentation such as < blood (O), glucose (SO), concentration(A), high (Vql) > in the extended OSoAV form.Furthermore, another issue is the requirement of thedegree value of the property. Clinicians usually need totransform test data into abnormal states in the casereport. Imagine a case where a highly elevated value isobserved in the clinical test. A simple OP <O, P, true >does not satisfactorily capture such data, and thus thedegree of abnormal states may be needed. Therefore, weintroduce the Degree Value (Vd) like severe. Therefore,we can describe such data in terms of the degree valuein the OPVd form < blood (O), hyperglycemia, severe(Vd) > in a triplet like in the OAV form.We recommend that the degree value should onlyhave minimum variations such as mild/moderate/severefor representation because numerous degree values wouldlead to dispersion and destruction of the unified represen-tation. Taking the interoperability into account, it wouldbe considered preferable to decide how to set a thresholdfor determining the degree value of severe. However, be-cause such concrete threshold values tend to change withtime, such threshold values are left undetermined in thisstudy.Here we introduce Property Value (Vp). A Property hasa meaning of < hyperglycemia, existence > or < hypergly-cemia, true>. The value as existence/nonexistence ortrue/false should be independent of the degree value.However, because adding another form to the degreevalue would make the system of representation formsmore complex, we deal with the degree value (e.g.,mild/moderate/severe) as the specialization of the stateof hyperglycemia; we treat these values of existence/nonexistence or true/ false in the same manner asYamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 5 of 14http://www.jbiomedsem.com/content/5/1/23the degree value. Consequently, we use Vp to representboth Vd and Vp.The use of the value is e.g., a representation of thecondition of not being hyperglycemia, < hyperglycemia,false>, in latent diabetes. For this reason, we considerthat the true/false value is needed for computer processing.In conclusion, our model provided the following inter-operable representation forms:(1) The < OAV > form as clinical test data(2) The < OPV > form as abnormal states and(3) The extended form <OSoAV > as clinical test dataand < OSoPV > as abnormal statesConsequently, the OPV form is the same as the OAVform. Therefore, the OPV can be compliant with theunified representation (OAV triplet), which realizes theinteroperability between the data and abnormal states.Is-a Hierarchy of abnormality ontologyOntologically, the structural abnormality, dysfunction,pathological conditions, pathological processes, etc. arekinds of abnormal states. We propose abnormalityontology, which is a comprehensive ontology that coversall of the abovementioned concepts, and abnormal statesare defined as the top level.Clinicians work with strongly domain-specific know-ledge, which causes difficulties in finding common andgeneric knowledge across domains. A clear distinction be-tween the basic/generic and specific concepts is requiredto be made. To this end, we propose the following threelevels of abnormal states (Figure 1): Level 1: Generic abnormal states Level 2: Object-dependent abnormal states Level 3: Specific context-dependent abnormal statesLevel 1: generic abnormal statesLevel 1 defines very basic (or generic) concepts, whichdo not depend on any structural entity, i.e., object-independent states. Examples include deformation, add-itional/missing anatomical structures, translocation, anddysfunction, which are commonly found in several objects,and can be usable in several domains besides medicine,such as machinery, materials, and aviation.The top-level category of the generic abnormal states hasthree subclasses: structural abnormality, functional ab-normality, and other abnormality (Figure 2). A structuralabnormality is defined as an abnormal state associated withstructure. It has subcategories of material abnormality (e.g.,degeneration), shape abnormality (e.g., deformation), sizeabnormality, and conformational abnormality, such astopological abnormality (e.g., translocation), or structuraldefects (e.g., additional/missing structures) etc., while stillretaining the identity of the structural body in question.A functional abnormality is defined as an abnormalstate that is related to an impaired function and is classi-fied into hyperfunction and malfunction. Malfunction issubcategorized into dysfunction, function arrest, andhypofunction.Other abnormal states include parametric abnormal-ities, which are classified into increased or decreasedparameters, depending on whether or not the attributehas a higher or lower value than a threshold level.Examples included increased/decreased pressure orincreased/decreased weight.Our model has a recursive structure, in which the gen-eric abnormal states at Level 1 are referred to by Level 2object-dependent abnormal states.Level 2: Object-dependent abnormal statesLevel 2 defines object-dependent abnormal states. Thetop level concepts at Level 2 are dependent on genericstructures, such as wall-type structure, tubular struc-ture, and bursiform structure, which are common andare used in several domains. Level 2 has been developedby identifying the target object and specializing genericabnormal states at Level 1 with consistency. For example,by specializing small in area at Level 1, narrowing tube,where the cross-sectional area has become narrow, is de-fined at Level 2 this is further specialized in the definitionsoil pipe narrowing or tracheal stenosis.In the lower layer of Level 2, abnormal states that aredependent on medical domain-specific objects, such ashuman anatomical structures, are defined and designedto represent concepts at all required granularities in themedical domain. Here in general, one problem arises inhow fine the level of granularity needs to be supportedin our ontology. In the case of stenosis, the term, cor-onary artery stenosis in a specific organ (the coronaryartery) may be redundant. However, it is noteworthy thatthe abnormal states in one anatomical object can influ-ence the adjacent objects, which causes other abnormalstates. For example, although both are types of stenosis,coronary artery stenosis is different from rhinostenosisbecause the former causes myocardial ischemia and ische-mic heart disease, whereas the latter causes sleep apnea.Therefore, there is a need for distinct abnormal states atspecific organ levels.From an ontological engineering point of view, ourframework for modeling abnormal states is intended tocapture the abnormal states from generic to specificlevels, so as to provide abnormal states at the requiredgranularity of specific organ/tissue/cell layers in themedical domain.Here, the abnormal states of a specific object definedat Level 2 should be distinct from the disease-dependentFigure 2 Top-level categories related to abnormal states. Thetop level categories of abnormal states are classified into threesubclasses: structural abnormality, functional abnormality, andother abnormality such as parametric/nonparametric change andso on.Figure 1 Three-level ontological model of abnormal states. This figure shows an example of the three levels of structural abnormality of ourabnormality ontology. Level 1 defines generic concepts, which are object-independent states, e.g., small in area. Level 2 defines object-dependentabnormal states. States at the upper levels of Level 2 are dependent on generic structures, such as the narrowing tube and narrowing valve, whichare common and are used in several domains. Note that concepts at the lower level of the tree are specialized into medicine-specific concepts suchas vascular stenosis, arterial stenosis, and coronary artery stenosis. Level 3 defines disease-dependent concepts. For example, coronary artery stenosisin angina pectoris is defined as a constituent of the disease angina pectoris at Layer 3.Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 6 of 14http://www.jbiomedsem.com/content/5/1/23concepts at Level 3. For example, hyperglycemia is definedin a context-independent manner at Level 2, and this isreferred to in Level 3 concepts in various diseases, such asdiabetes, metabolic syndrome, and lipodystrophy.Currently, we are developing and enriching Level 2concepts to link each Level 3 concept to the uppercommon level concept.Level 3: specific context-dependent abnormal statesLevel 3 consists of context-dependent abnormal states,which refer to the Level 2 abnormal states, and are spe-cialized into specific disease-dependent ones. For example,rectal stenosis, which is dependent on the rectum atLevel 2, is defined as a constituent of Crohns disease atLevel 3; this is also defined as a cause or an effect of otherdiseases, such as rectal cancer, Hirschsprung disease, orintestinal tuberculosis.Application workCausal chains of diseaseWe have been developing a disease ontology, in which adisease is defined as a causal chain of abnormal states[9]. We divided the diseases into two major kinds: (1)ones where the etiological and pathological processesare well understood and (2) otherwise. Case (2) includesthe so-called syndromes, typically represented in termsof the criteria for diagnosis. Diseases of type (1) is identi-fied by its inherent etiological/pathological process(es).Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 7 of 14http://www.jbiomedsem.com/content/5/1/23In this paper, we deal only with type (1) diseases. Aftercareful examination of several diseases, we believe thatevery disease of type (1) should have a cue for identifica-tion. This means that we should be able to find the so-called main pathological/etiological condition(s), whichtheoretically characterize the disease to identify it. Weknow that diseases of type (2) necessarily employ criteriafor diagnosis to identify the disease because of the lack ofknowledge regarding etiological/pathological processes.In addition, we believe that we need a formulation fororganizing diseases in an is-a hierarchy in a disease model.According to the definition of a disease, this would consistof a causal chain(s), which consisted of nodes and links; adisease would be represented as a directed acyclic graph(DAG). We can introduce the is-a relation between dis-eases using the inclusion relationship between the causalchains as noted below.Is-a relation between diseases Disease A is a superclass of disease B if all the causal chains at the class levelof disease A are included in those of disease B. Theinclusion of nodes (disorders) is judged by consideringthe is-a relation between the nodes into account as wellas the sameness of the nodes.Core causal chain of a disease Causal chain(s) of adisease included in the chains of all its subclass diseasesis called the core causal chain of a disease.Derived causal chains of a disease Causal chains of adisease defined as possible causal chains of abnormalstates are called derived causal chains.For example, <myocardial stenosis?myocardial ische-mia > is the core causal chain for ischemic heart diseaseand <myocardial stenosis?myocardial ischemia?myo-cardial necrosis > is the core causal chain for myocardialinfarction. Here the core causal chain of Prinzmetal an-gina is defined as < coronary spasm?>, and if there aresome possible causes of spasm, e.g., smoking, it would beadded to the upstream of causal chains as < smoking(nicotine absorption through the respiratory tract)? cor-onary spasm? > as a derived causal chain (Figure 3).Till date, clinicians have described the causal chains ofdiseases and abnormal states. We have been using theseabnormal states to develop an is-a hierarchy of abnor-malities. Abnormal states used in disease definitions inthe ontology are defined as abnormal states at Level 3,where clinicians defined diseases in the respective med-ical departments. We collected all causal relationshipsfrom all disease concepts defined in the 13 medicaldepartments and combined the causal chains, includingthe same abnormal states. As a result, the generic causalchains that contain all causal relationships, includingapproximately 21,000 abnormal states from 13 medicaldepartments have been generated [14]. For example, weassume that a cardiovascular specialist in the division ofcardiovascular medicine describes coronary artery sten-osis and its causal chain as < coronary artery stenosis?myocardial ischemia?myocardial hypoxia > in ischemiccardiac disease. This can be linked with coronary arterystenosis in other diseases (e.g., hyperlipidemia) in otherdepartments (metabolic medicine). As a result, a genericcausal chain < accumulation of cholesterol? coronaryartery stenosis?myocardial ischemia?myocardial hyp-oxia > of hyperlipidemia can be obtained as a possiblecausal relationship of abnormal states in the disease.Discussion and related workDiscussionWe have introduced a unified form that represented anabnormal state as a time-indexed Property, and decom-posed it into its Attribute and Value. Furthermore, weintroduced the Sub-Object, which increases the flexibil-ity with consistency. A property representation has severaladvantages. First, it easily captures the essentials of eachdisease because of its abstract nature. Second, it is rela-tively insusceptible to a small parameter modification.Third, it allows for the distinction between a definition ofa disease and a diagnostic task that requires a quantitativerepresentation.Here, it should be noted that an abnormality can beexplained as some bodily feature that is not part of thehuman life plan (unlike pregnancy) [15]; however, makinga decision about whether or not a particular state is ab-normal is not the job of ontologists but medical experts,who are required to make decisions on the basis of theirmedical knowledge. For example, answering a questionabout whether or not high HDL cholesterol level is anabnormal state would not be a task for ontologists butfor medical experts; therefore, we do not discuss this issuein the present study.We demonstrated that our model is interoperablebetween the quantitative and qualitative data found inseveral medical records, and the conceptual knowledgeof the abnormal states in the definition of diseases.In this study, we do not deal with the concrete valuethat is to be set for the threshold because thresholdsmay tend to change with time; for example, the cutoffvalue of the fasting plasma glucose (FPG) level was revisedto 140 mg/dL in 1980 and to 126 in 1999 [16]. Therefore,we can freely change the threshold, and to do so isintrinsic. Nevertheless, even if the threshold changes,hyperglycemia will remain as < blood, glucose concen-tration, high >.Diversity and heterogeneous representation problemsof abnormal states are solved by a unified and consistentframework. However, in clinical DB, compound conceptsare often found in clinical terms. For example, bloodFigure 3 Types of ischemic heart disease constituted of causal chains. This figure shows a couple of causal chain-constituted ischemic heartdisease. Each node shows the abnormal states, and each link indicates the causal relation between the abnormal states. A core causal chain ofeach disease is colored differently: ischemic heart disease is orange, and the subclasses of the ischemic heart disease, myocardial infarction areyellow. Prinzmetal angina is also a subclass of the ischemic heart disease consists of a pink core causal chain, and by an upstream extensionsmoking is added in the derived causal chain. Organic angina pectoris is green and the accumulation of cholesterol is added to the derivedcausal chain, which is a possible cause of arterial sclerosis.Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 8 of 14http://www.jbiomedsem.com/content/5/1/23pressure is considered to be a compound concept, whichconsists of two elements blood (Object) and pressure(Attribute) joined for one meaning denoting an attribute.Other examples are WBC count and blood glucoselevel. Precisely speaking, these concepts should not bedealt with as an ontological matter but as a variation ofdata representation. Because medicine also requires anexchange of the real world data such as clinical test databetween hospitals or institutions, in the next step, we willdeal with them as a variation of data representation.We have developed an ontology of abnormal statesfrom generic to specific levels.We have confirmed that abnormal states in the definitionof 10 major diseases from three medical departments canbe described with our description framework and suc-ceeded in developing the is-a hierarchy from Level 1 toLevel 3 in our preliminary work.Till date, ontological engineers have defined Level 1concepts together with a major portion of the higherlevels of Level 2; clinicians have defined Level 3 concepts,including 21,000 abnormal states in 6,000 diseases inontology. We plan to reformulate all the abnormal statesat Level 3 in terms of our framework and complete thedevelopment of the middle concepts at Level 2 to linkboth the upper Level 1 and Level 3 abnormal states.Some readers might think Level 3 is unnecessary andshould be treated as the diagnostic instance level. How-ever, by introducing Level 3 concepts, it will providecontextual information in the specific disease and contrib-ute to the understanding of the background knowledgerelated to the underlying mechanisms of pathologicalprocess in the disease. Furthermore, Level 3 concepts areimportant for finding commonalities between the variousdiseases in terms of abnormal states. Therefore, we needto develop disease context-dependent levels as Level 3.Our ontology is able to distinguish the common con-cepts from specific ones. Such an ontological approachcontributes to finding commonalities not only acrossdiseases in one division but also across departments. Forexample, in cardiovascular medicine, coronary arterystenosis in ischemic heart disease has a commonalitywith pulmonary artery stenosis in the tetralogy of Fallotin that they have the same upper abnormal state arterialstenosis. In addition, it has a commonality with cerebro-vascular stenosis in brain infarction in cerebral surgery inthat they both have the same upper abnormal state vas-cular stenosis. A further commonality can be found withintestinal stenosis in the ileus in gastroenterologicalmedicine in that they have the same generic structure-dependent abnormal state narrowing tube. Therefore,finding commonalities across medical departments couldoffer a multidisciplinary perspective, allowing our methodto be applied to a wide range of research.In our application work, we have captured all 21,000abnormal states across the 13 medical departments withboth the is-a hierarchical structure of the abnormalityYamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 9 of 14http://www.jbiomedsem.com/content/5/1/23ontology, and a causal chain as a relationship betweendifferent classes of abnormal states that are influencedby each other. This allows us to integrate fragmentedknowledge of abnormal states, which might support theapplication of various kinds of medical knowledge, asfollows.(1) Conceptualization with little ambiguityIn the medical domain, there are quite a few ambigu-ous clinical terms with the same name but differentmeanings. One reason behind this is that clinicians useeach term in the context of specific diseases in their owndepartments. For instance, the medical term cardiachypertrophy is used in both a division of cardiovascularmedicine and metabolic medicine. The definition ofhypertensive heart disease in cardiovascular medicine in-dicates an increase in the thickness of the heart muscle,which results from a pressure overload caused by hyper-tension in the context of the heart. On the other hand, inglycogenosis II (Pompe diseases) in metabolic medicine, itimplies a glycogen accumulation in the heart muscle,which is caused by metabolic dysfunction (Figure 4). Be-cause our model can provide the appropriate upper levelsof concepts and can give contextual information, it ispossible to clarify their difference.Thus, our model can reveal the context of the mean-ings that is usually hidden in the implicit backgroundFigure 4 Examples of hypertrophy constituted of causal chains. This fhypertrophy is red. One usage is a constituent of a causal chain of the hypand the other is a constituent of a glycogenesis type II disease (Pompe dischain of each disease is yellow).knowledge of clinicians, and will contribute to making aclear distinction between different types of concepts.(2) Management of attributes by unified representationIf we allow clinicians to freely express the various attri-butes/abnormalities, it would lead to a lack of consistencyand interoperability. Our model solves this problem byproviding a unified representation model of attributes/abnormal states, as discussed in section Representation ofabnormal states, in which the attributes and properties aredifferentiated; the properties are decomposed into < attri-bute, attribute value>, as well as the advanced representa-tion for ratios and meta-attributes.(3) Quantitative assessment of commonalityTraditionally, abnormal states have been dealt with ina manner specific to each disease in a particular medicaldivision. Here, our model enables the capture of abnormalstates common to several diseases, i.e., those that are atthe first two levels and those that are disease-independent,which allows clinicians to overlook all abnormal statesacross medical departments.As a result, we can quantify and assess the degree ofcommonality of abnormal states between different medicaldepartments. In addition, it is possible to verify the com-monality of generic concepts by abstracting, or to findigure shows two different uses of cardiac hypertrophy. Each cardiacertensive heart disease in the cardiovascular department (upper figure),ease) in the metabolic disease department shown below (A core causalYamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 10 of 14http://www.jbiomedsem.com/content/5/1/23disease-specific abnormal states with no commonality toany disease in other departments. For example, esopha-gostenosis, which is a subclass of narrowing tube, maydemonstrate that it is specific to esophageal disease byshowing no commonality with other diseases, whereasvascular stenosis can be confirmed as being more commonby showing a higher rate of commonality across multiplediseases. Furthermore, our model may find commonalitiesof abnormal states that have always been treated as quitedifferent abnormal states in different departments.The clinicians treatment of the abnormal states in amanner specific to a disease and/or particular clinicaldivision may have caused fragmentation of the sameconcept into different ones that are treated as differ-ently. Because our approach finds commonalities in theorgan-independent abnormal states, we can clean upand deal with abnormal states more simply.Thus, our ontology will provide a clue to revealing thecontext embedded as background knowledge, which willallow us to compare abnormal states and evaluate theircommonalities across medical departments.Related workUpper ontologies such as BFO [1,2], DOLCE [3], andGalen [17] also deal with qualities and have contributed todealing with the semantics of data. BFO formalizes < Entity,Property > (e.g., <rose, red>), whereas DOLCE uses < Entity,Attribute, Value > formalization (e.g., <rose, color, red>),and Galen adopts < Entity, Property, Value > formalization,(e.g., <rose, redness, high>). Phenotypic Quality (PATO) [6]is an ontology of phenotypic qualities, where the de-scription was changed from < Entity, Attribute, Value > to< Entity, Property (Quality) > (e.g., <eye, red>) when theyemployed BFO. As explained in the Background section,the YAMATO ontology is an upper ontology in Japan [4],and offers interoperability among all of these descriptions,allowing us to handle all three kinds of descriptions in ourrepresentation model. Furthermore, PATO2YAMATOprovides the mapping of ontology terms of PATO toYAMATOs framework [5], and enables the interoper-ability of the quality framework between the differenttop-level ontologies such as BFO and DOLCE. In a prelim-inary study, the application has succeeded in making theconnection between rat or mouse phenotype data, and re-lated human abnormal states in the definition of diseases inthis study [18]. Because our extended model enables thecapture of commonalities of abnormal states across bio-logical species, it may contribute to translational researchlinking mouse experimental data and clinical research.Furthermore, on the basis of the ontological approach,if we make explicit the commonality and specificity ofabnormal states among multi-species, it should supporta comprehensive understanding of the basic commonmechanism or principles underlying organisms, and wouldlead to scientific discoveries by acquiring biomedical know-ledge through an interdisciplinary approach across species.In the medical domain, medical ontologies and stand-ard vocabularies, such as ICD-10 [19], SNOMED-CT [20],have been developed and extensively used in practice.However, they are largely based on legacy system termin-ologies, and thus have some ontological problems [21].SNOMED-CT is a comprehensive terminology, whichcontains more than 311,000 clinical terms. However, itis not compliant with any formal upper level ontology.SNOMED-CT allows for multiple inheritance that causesa messy situation in the classification of entities, despitethe fact that partitioning implies sibling classes aremutually disjoint, siblings at lower levels overlap eachother, which results in complex taxonomic graphs andmaintenance of the ontology difficult [20]. Furthermore,SNOMED-CT does not distinguish disorders from dis-eases. Not all disorders are diseases.Our ontological proposal will help avoid such problems.On the basis of YAMATO, we systematically defineabnormal states from the generic level (Level 1) to thespecific anatomical structure-dependent level (Level2). Furthermore, by specializing Level 2 concepts intoa disease-context (disease-specific) level (Level 3), wecan distinguish abnormal states from diseases.In our future plan, our ontology will be translated intoEnglish and be mapped with SNOMED-CT clinical terms.The mapping will evaluate the standard terminologies inline with fundamental ontology engineering and provideuseful information about causal relationships of abnormalstates in the definition of each disease.LOINC [8] provides the universal code names andclinical terms by decomposing them. However, becauseit focuses on the clinical observations, several of theabnormal states appearing in the definition of diseasesare out of the scope for LOINC.Although LOINC has < O (SO) A > like our model, itdoes not have Value (V). For example, a test for glucosetolerance about after 2 hours serum glucose for 100 g oralis represented by GLUCOSE^2H POST 100 G GLU-COSE PO:MCNC:PT:SER/PLAS:QN. The aim of LOINCis to standardize the vocabulary for the representation ofclinical test data and is useful for interoperability amongvarious data. However, our claim is not isolated to adopt-ing the OAV form. In order to realize the interoperabilitybetween the clinical test data and abnormal states, aQuantitative Value (Vqt) is needed in the representationform. Our model can deal with quantitative data in theOAV form; therefore, we can transform it into the OPform of abnormal states. As a result, our model has anability to maintain the interoperability between the clinicaltest data to abnormal states in diseases. Our model is notmerely a theoretical contribution. Only reutilizing theexisting resources cannot realize the interoperabilityYamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 11 of 14http://www.jbiomedsem.com/content/5/1/23between the various representation forms. We needmore sophisticated organization of related representationsincluding quantitative and qualitative data to exploit all ofthem in a consistent manner. To the best of our know-ledge, our model is the first to make such an exploitationpossible, which will contribute to medical practices.Our model contributes to the systematization of ab-normal states on the basis of ontological theory, and isable to distinguish between generic abnormal states,object-dependent ones, and disease-specific ones withunified representation. Moreover, the generic abnor-mal states are referred to the lower level of abnormalstates by specializing them into the required granularity.In the future, we plan to examine mappings to other datasets of representations of clinical observations such as inLOINC or MEDIS [22] that have been opened to thepublic by The Medical Information System DevelopmentCenter in Japan (MEDIS-DC) for interoperability. Thesemappings would provide a more comprehensive analysisof interoperability between the clinical observation dataFigure 5 Computational representation of abnormal states from genestates from small in area to ischemic heart disease specific coronary arterand the conceptual knowledge of abnormal states in thedefinition of diseases.OGMS, which uses BFO as an upper-level ontology[23], and DO [24] are both medical ontologies. However,they do not have causal relationships between the abnor-mal states in one disease. Our strategy will contribute toproviding a good resource for several medical researchersto analyze the causes of diseases from the viewpoint of thecausal relationships of the abnormal states. Collaborativeefforts in OBO Foundry have tried to coordinate variousontologies to support biomedical data integration [25]. Inthe next step, we plan to convert our abnormal stateontology into OBO format, and provide useful informa-tion about the causal relationships in diseases.In our practical application work, we published someparts of the causal chain in disease ontology as LinkedOpen Data (Disease Chain LOD) on the basis of ourRDF model [26]. It includes definitions of 2,103 diseasesand 13,910 abnormal states in six major medical depart-ments extracted from the disease ontology on May 11,ric to specific level. This figure shows the specialization of abnormaly stenosis using HOZO.Yamagata et al. Journal of Biomedical Semantics 2014, 5:23 Page 12 of 14http://www.jbiomedsem.com/content/5/1/232013. In addition, we have developed the visualizationsystem for disease chains called the Disease Chain LODViewer, which is available at http://lodc.med-ontology.jp/.Furthermore, a browsing system of disease chains withrelated information, which are obtained from other linkeddata or web service from external datasets (e.g., ICD 10,MeSH from DBPedia), is currently under development.ConclusionsWe proposed a representation model of abnormal statesdesigned in a unified manner. Our medical ontologyproject was started seven years ago. Since then, it has beenrefined and revised several times through discussion withboth ontologists and clinicians. Till date, we have appliedthis model to approximately 21,000 abnormal states fromapproximately 6000 diseases.We have demonstrated that our model has interoper-ability between quantitative and qualitative data and theconceptual knowledge of abnormal states in the definitionof diseases. With this model, we have been developing anontology of abnormal states from generic to specific levels.Figure 6 A visual editing tool for causal chains to define disease concdefinition of disease concepts. It visualizes the causal chains defined in a seIn the application we considered, we built disease chainsconsisting of causal relationships of abnormal states. Bycombining the disease chains and the ontology, we havecaptured all causal relations of the 21,000 abnormal statesin the 6,000 diseases across 13 medical departments.Although abnormal states have traditionally been con-sidered to be specific to each disease in a particular med-ical department, our approach has found commonalitiesamong abnormal states across medical departments.MethodsData sources for representation and ontologydevelopment of abnormal statesMedical doctors of The University of Tokyo Hospitaldescribed the disease ontology, and definitions of dis-eases were determined. Medical dictionaries [10,27] andJOURNAL OFBIOMEDICAL SEMANTICSVihinen Journal of Biomedical Semantics 2014, 5:9http://www.jbiomedsem.com/content/5/1/9SOFTWARE Open AccessVariation ontology: annotator guideMauno VihinenAbstractBackground: Systematic representation of information related to genetic and non-genetic variations is required toallow large scale studies, data mining and data integration, and to make it possible to reveal novel relationshipsbetween genotype and phenotype. Although lots of variation data is available it is often difficult to use due to lackof systematics.Results: A novel ontology, Variation Ontology (VariO http://variationontology.org), was developed for annotation ofeffects, consequences and mechanisms of variations. In this article instructions are provided on how VariOannotations are made. The major levels for description are the three molecules, namely DNA, RNA and protein.They are further divided to four major sublevels: variation type, function, structure, and property, and further up toeight sublevels. VariO annotation summarizes existing knowledge about a variation and its effects and formalizes itso that computational analyses are efficient. The annotations should be made on as many levels as possible. VariOannotations are made in reference to normal states, which vary for each data item including e.g. referencesequences, wild type properties, and activities.Conclusions: Detailed instructions together with examples are provided to indicate how VariO can be used forannotation of variations and their effects. A dedicated tool has been developed for annotation and will be furtherdeveloped to cover also evidence for the annotations. VariO is suitable for annotation of data in many types ofdatabases. As several different kinds of databases are in a process of adapting VariO annotations it is important tohave guidelines to guarantee consistent annotation.Keywords: Variation ontology, Annotation instructions, Systematics, Variation effects, Mutation, OntologyBackgroundVariations have different effects and mechanisms. Tocapture and describe the character of the variations atDNA, RNA and protein level, the Variation Ontology(VariO) was developed [1]. VariO annotations allow sys-tematic descriptions which can be used e.g. for searchesof complex queries also simultaneously from several da-tabases. Systematic descriptions have several benefits es-pecially for computational searches and analyses and forsoftware development.Variation information has been collected to various da-tabases. Locus specific databases (LSDBs) are for individ-ual genes or diseases and usually curated by experts inthe genes and diseases. There are currently thousands ofLSDBs available, mainly on the LOVD database manage-ment system [2]. Central databases like SwissProt [3]and ClinVar [4] contain information on large numbersCorrespondence: mauno.vihinen@med.lu.seDepartment of Experimental Medical Science, Lund University, BMC D10,SE-22184 Lund, Sweden© 2014 Vihinen; licensee BioMed Central Ltd.Commons Attribution License (http://creativecreproduction in any medium, provided the orof genes and/or proteins and variations in them. Additionaltypes of variation databases include national or ethnic da-tabases such as ETHNOS [5], chromosomal variation data-bases such as The Database of Genomic Variants archive(DGVa) [6], variation frequency databases including FIND-base [7], and databases dedicated on certain types of varia-tions or for an effect or mechanism, such as ProTherm,a database for protein stability affecting variations [8].Benchmark databases, such as VariBench [9], are dedi-cated for providing gold standard datasets for method de-velopers and assessors. All these databases would benefitfrom systematic descriptions. Human Genome VariationSociety (HGVS) nomenclature is widely used for namingvariations [10], however additional systematics would beneeded. HGVS and Human Variome Project (HVP) havereleased a number of recommendations [11], also for in-creased systematics, including recommendation to useVariO annotations [12]. The recent recommendations forLSDB establishment and curation emphasize the import-ance of systematics [13,14]. The goal of the GEN2PHENThis is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly credited.Figure 1 General structure of VariO. The ontology is designed forannotation of effects, consequences and mechanisms of variationsat the three molecular levels, DNA, RNA and protein. Each of thesehas further terms on four major sublevels: variation type, function,structure and property. Attributes are used to modify terms atstructure or property levels to further define the terms.Vihinen Journal of Biomedical Semantics 2014, 5:9 Page 2 of 8http://www.jbiomedsem.com/content/5/1/9project was to develop tools, data models and solutionsfor this domain [15].The HGVS variation nomenclature is a systematicnaming convention that is in use in some journals andnumerous databases. It provides guidelines and naming foralmost all variation types based on reference sequences.The Mutalyzer tool can generate the names automaticallyand performs a number of consistency checks [16].VariO is intended for the description of what is chan-ged in the variant in comparison to the normal or wildtype. Thus, it does not describe the properties of thewild type. The annotations are made in comparison to areference, which varies depending on the annotatedproperty including e.g. reference sequences, referencestates such as wild type enzyme activity and normal kin-etic properties. The application area of VariO is indescribing effects, consequences and mechanisms in di-verse data sources. These include all the different typesof variation databases mentioned above. In addition,it can benefit variation naming services [16], LSDB man-agement systems [2], data integrators, journals etc. AsVariO has been described previously [1], the goal of thisannotator guide is to explain how annotations are madeand used.VariO design principlesVariO terms have three biological molecules, DNA,RNA and protein, as the starting point. All these havefour major sublevels: variation type, function, structureand properties with more detailed sublevels. There arealtogether 8 levels of terms. The terms have a clear hier-archy and the organization of terms for DNA, RNA andprotein has a similar and coherent layout whenever ap-propriate. The terms for the three molecular levels areconsistent and related terms are used for related featuresat different levels. Suggestions for additional terms andupdates can be sent to the ontology developers.For visualization of terms, their definitions, relation-ships and paths, the AmiVariO browser is available atthe VariO website. Figure 1 indicates how the levels areorganized. When possible, variations are explained atDNA, RNA and protein levels. Each of these has sub-levels, out of which structure and property levels can befurther modified with attribute terms. The hierarchy ofthe terms has been designed to allow for a versatile andflexible annotation.VariO is intended for the description of all kinds ofvariations and situations. In Figure 2 the distinction be-tween terms of genetic and non-genetic origin is shown.Terms with genetic origin describe changes either inDNA or inherited from it to RNA and protein levels.The non-genetic terms, called variations emerging at theRNA or protein level, are for either biological or artificialmodifications that originate at the RNA or protein level.For example, RNA editing modifies a synthetized RNAchain and the variations are not coded in DNA.To keep the ontology compact, modifier attributesform the fourth major level. These terms are used tomodify the terms at the other levels. For example, quan-tity terms are used to modify other terms when the ef-fect is increased, decreased or is missing the quantity ofthe parameter, or when it is not changed. Instead of hav-ing separate terms for describing increase or decrease ofa feature, existing terms can be modified with attributeterms. This way the number of terms could be reducedconsiderably.VariO terms should be combined with other systemat-ics and ontologies. Evidence Ontology (ECO) terms [17]are used to describe the methods with which the annota-tions were obtained.VariO aims at describing any effect, consequence andmechanism, at any organism. The variations can be ofgenetic or non-genetic origin. The size of the variationdoes not matter, anything ranging from nucleotide oramino acid changes to chromosome or genome duplica-tions can be annotated. However, the annotations areposition based, even if the position means e.g. an entirechromosome.Example of VariO annotationThis example highlights a number of aspects of annota-tions with VariO. The annotations are used to explainFigure 2 Variation types for terms with genetic or non-genetic origin. The variation type annotations are made based on whether thevariation is of genetic or non-genetic origin. Genetic terms are used for alterations originating from the genetic material of the organism (DNA orRNA, depending on the organism), while terms with non-genetic origin are either artificial or originated from the processing of RNA or proteinmolecules without change in the corresponding DNA.Vihinen Journal of Biomedical Semantics 2014, 5:9 Page 3 of 8http://www.jbiomedsem.com/content/5/1/9additional features of the instances in a database in asystematic way. It may be tiring for a human reader tosee the same concept every time it is mentioned, how-ever, for computational analyses it is a blessing and facili-tates fast searches. For the annotation we are developing atool called VariOtator [18].Here is a real life example of variation in the AIREgene leading to the autoimmune polyendocrine syn-drome type 1 (APS-1) also called for APECED disease(autoimmune polyendocytopathy-candidiasis-ectodermaldystrophy), an autoimmune polyendocrine syndrome.AIRE, autoimmune regulator, is a transcriptional regulatorof tissue-specific antigens. Variations affect the regulation,leading to the production of self-reactive antigens. TheT >C variation leading to a L to P substitution in the homo-geneous staining region (HSR) domain is disease causing[19] (AIREbase [20,21] entry A0087). The functional andother aspects of APECED-causing variations were furtherstudied in [22].The genomic variant g.4789 T > C in the IDbase refer-ence sequence D0003 (http://structure. bmc.lu.se/cgi-bin/fetch_idrefseq.cgi?ac = D0003&format = embl, cross-reference to EMBL:AB006682) is annotated as follows.VariO:0128 variation affecting DNAVariO:0129 DNA variation typeVariO:0322 DNA variation classificationVariO:0135 DNA chain variationVariO:0136 DNA substitutionVariO:0313 transitionVariO:0314 pyrimidine transitionThe variation is of genetic originVariO:0128 variation affecting DNAVariO:0129 DNA variation typeVariO:0127 DNA variation originVariO:0130 DNA variation of genetic originThere is a pyrimidine transition of genetic origin.The effect to RNA sequence (IDbase reference sequenceC:0003 http://structure.bmc.lu.se/cgi-bin/fetch_idrefseq.cgi?ac = C0003& cross referenced to EMBL; AB006682)is, similar at DNA level, a pyrimidine transition, whichcauses a missense variation.VariO:0297 variation affecting RNAVariO:0306 RNA variation typeVariO:0328 RNA variation classificationVariO:0312 RNA substitutionVariO:0313 transitionVariO:0314 pyrimidine transitionVariO:0308 missense variationOn the protein level the reference sequence is UniProtentry O43918. A variation has different annotations atdifferent levels. In this example, the amino acid substitu-tion at protein level is annotated as DNA substitutionon DNA level, and on RNA level as RNA nucleotidesubstitution of type missense variation.The annotations are richer on protein level as the ef-fects of the variation affect protein function, structureand properties.Vihinen Journal of Biomedical Semantics 2014, 5:9 Page 4 of 8http://www.jbiomedsem.com/content/5/1/9VariO:0002 variation affecting proteinVariO:0012 protein variation typeVariO:0325 protein variation classificationVariO:0021 amino acid substitutionThe protein variation is due to change at the DNA level.VariO:0002 variation affecting proteinVariO:0323 protein variation originVariO:0013 protein variation of genetic originThis variant was shown to prevent transactivation, aprotein information transfer function.VariO:0002 variation affecting proteinVariO:0003 variation affecting protein functionVariO:0011 effect on protein information transferThe annotations for the structure include predicted ef-fects on the protein secondary and tertiary structure.Introduction of a proline in the middle of ?-helix affectsthe protein fold leading to conformational change.VariO:0002 variation affecting proteinVariO:0060 variation affecting protein structureVariO:0064 effect on protein 3D structureVariO:0070 effect on protein tertiary structureVariO:0079 effect on protein secondary structuralelementVariO:0080 effect on protein helixVariO:0082 effect on right handed protein helixVariO:0085 effect on alpha helixVariO:0002 variation affecting proteinVariO:0060 variation affecting protein structureVariO:0064 effect on protein 3D structureVariO:0070 effect on protein tertiary structureVariO:0073 effect on protein foldVariO:0074 protein conformational changeProperties are used for annotating various characteris-tics. As many properties should be annotated as data isavailable. The variation is disease-causing, which is indi-cated by the pathogenicity association attribute. In thecase of attributes, only the relevant attribute, not path toit is given. Attributes can be used to modify structureand property terms.VariO:0002 variation affecting proteinVariO:0032 variation affecting protein propertyVariO:0047 association of protein variation topathogenicity; VariO:0294 disease causingThe variant affects transactivation inactivating the pro-tein function. The property term (effect on protein activ-ity) is again modified by the attribute (missing).VariO:0002 variation affecting proteinVariO:0032 variation affecting protein propertyVariO:0053 effect on protein activity; VariO:292 missingEffects to protein interaction can be described in detail.Interaction attribute terms were derived from the ProteinInteraction ontology [23] but modified for the purpose ofVariO.VariO:0002 variation affecting proteinVariO:0032 variation affecting protein propertyVariO:0058 effect on protein interaction; VariO:0292missingThe variant prevents AIRE homomultimerization, whichis required for the transactivation activity, and is essentialfor the interaction with nuclear dots and cytoplasmic fila-ments. This can be further described by the attributes asfollows for the homomultimerization, i.e. interaction withanother protein molecule.VariO:0232 variation attributeVariO:0236 interactionVariO:0262 interactorVariO:0273 biopolymerVariO:0277 proteinand the interaction with nuclear dots and cytoplasmicfilaments are annotated as effects on protein complexes.VariO:0232 variation attributeVariO:0236 interactionVariO:0262 interactorVariO:0267 complexVariO:0269 protein complexEven more detailed descriptions would be possible withVariO, however, in this case as details are missing, the an-notations remain somewhat shallow. For instance, inter-action physical forces cannot therefore be annotated.The variant alters the subcellular localization of the pro-tein. Terms for the actual change to the compartment tar-geting are not provided as they have not been systematized.VariO:0002 variation affecting proteinVariO:0032 variation affecting protein propertyVariO:0033 effect on protein subcellular localizationThis example highlights how the experimental and pre-dicted results can be explained at multiple levels. Note thatVihinen Journal of Biomedical Semantics 2014, 5:9 Page 5 of 8http://www.jbiomedsem.com/content/5/1/9e.g. details of protein function are further explained bythe property terms. Above, attributes are used as simplemodifiers of quantity for protein activity and for diseasecausality, but these modifiers allow also more elabor-ate annotations as for interactions. The VariO annota-tion is modular and therefore any number of terms canbe used, whatever is needed to capture the type andeffects of a variant. More examples are available at http://variationontology.org/examples.shtml.How to get startedWhen annotating a variant one should combine all theexisting information about the variant and its effects.The steps from the variation identification to a func-tional annotation are depicted in Figure 3. In case ofcontradictory results try to obtain consensus. Databasesshould not reflect personal opinions, therefore the anno-tations have to be according to the approved standardsin the field. Once the data is available all relevant aspectsshould be described. This may require annotations atseveral sublevels, for example variation types can be fur-ther defined at structural level.Once the data is available the annotation process isrelatively straightforward. The annotation tool, VariOtator,Figure 3 Flowchart for annotation with VariO. All existinginformation for the variation should be available when startingannotation. Reference sequence is needed to indicate the positionof the variation. With this information can be generated thevariation type annotation. The effects on function, structure andproperty are annotated based on the obtained information. Themethods used for obtaining the results are indicated by EvidenceJOURNAL OFBIOMEDICAL SEMANTICSKalankesh et al. Journal of Biomedical Semantics 2014, 5:2http://www.jbiomedsem.com/content/5/1/2RESEARCH Open AccessThe languages of health in general practiceelectronic patient records: a Zipfs law analysisLeila R Kalankesh1,2, John P New3,4, Patricia G Baker5 and Andy Brass1*AbstractBackground: Natural human languages show a power law behaviour in which word frequency (in any largeenough corpus) is inversely proportional to word rank - Zipfs law. We have therefore asked whether similar powerlaw behaviours could be seen in data from electronic patient records.Results: In order to examine this question, anonymised data were obtained from all general practices in Salfordcovering a seven year period and captured in the form of Read codes. It was found that data for patient diagnosesand procedures followed Zipfs law. However, the medication data behaved very differently, looking much morelike a referential index. We also observed differences in the statistical behaviour of the language used to describepatient diagnosis as a function of an anonymised GP practice identifier.Conclusions: This works demonstrate that data from electronic patient records does follow Zipfs law. We alsofound significant differences in Zipfs law behaviour in data from different GP practices. This suggests thatcomputational linguistic techniques could become a useful additional tool to help understand and monitor thedata quality of health records.BackgroundA recent survey has shown that 90% of patient contactwith the National Health Service (NHS) in the UK isthrough General Practices and General Practitioners(GPs) [1]. Over 98% of the UK population is registeredwith a general practitioner and almost all GPs use com-puterised patient record systems, providing a unique andvaluable resource of data [2]. About 259 million GP con-sultations are undertaken every year in the UK. However,capturing structured clinical data is not straightforward[3]. Clinical terminologies are required by electronic pa-tient record systems to capture, process, use, transfer andshare data in a standard form [4] by providing a mechan-ism to encode patient data in a structured and commonlanguage [5]. This standard language helps improvesharing and communication of information through-out the health system and beyond [6,7]. Codes assigned topatient encounters with the health system can be used formany purposes such as automated medical decision sup-port, disease surveillance, payment and reimbursement of* Correspondence: abrass@manchester.ac.uk1School Of Computer Science, University of Manchester, Oxford Road,Manchester M13 9PL, UKFull list of author information is available at the end of the article© 2014 Kalankesh et al.; licensee BioMed CentCommons Attribution License (http://creativecreproduction in any medium, provided the orservices rendered to the patients [8]. In this work we arefocusing our attention specifically on the coding systemused predominantly by UK GPs, the Read codes.Read codes provide a comprehensive controlled vo-cabulary that has been structured hierarchically to pro-vide a mechanism for recording data in computerisedpatient records for UK GPs [9]. They combine the char-acteristics of both classification and coding systems [10].Most data required for an effective electronic patientrecord (demographic data, lifestyle, symptoms, history,symptoms, signs, process of care, diagnostic procedures,administrative procedures, therapeutic procedures, diag-nosis data, and medication prescribed for patient) can becoded in terms of Read codes [11]. Each Read Code isrepresented as 5-digit alphanumeric characters and eachcharacter represents one level in hierarchical structureof Read codes tree [12]. These codes are organised intochapters and sections. For example Read codes begin-ning with 09 are processes of care, those beginningwith A  Z (uppercase) are diagnosis, and those begin-ning a-z (lowercase) represent drugs (described furtherin the Methods section). Of some concern, however, isthe quality of the data captured in this way.ral Ltd. This is an open access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly cited.Table 1 An example of the 5-byte Read code that showshow the specificity of a term increases as a functionof depthDepth Read code Term1 G Circulatory system diseases2 G3 Ischaemic heart disease3 G30 Acute myocardial infarction4 G301 Other specified anterior myocardial infarction5 G3011 Acute anteroseptal infarctionIt is straightforward to examine the datasets to determine the range of termdepths that have been used in the coding process.Kalankesh et al. Journal of Biomedical Semantics 2014, 5:2 Page 2 of 8http://www.jbiomedsem.com/content/5/1/2At its heart, medical coding is a process of communi-cation, with clinical terminologies bridging the gap be-tween language, medicine and software [13]. Read codescan be thought of as a vocabulary for primary care medi-cine, providing words (terms) used to describe encoun-ters between GPs and patients. The GPs (annotators) areattempting to encode information regarding the consult-ation; information that the wider community then needsto decode. The bag of codes associated with a consult-ation can therefore be thought of a sentence made up ofwords from Read, a sentence written by a GP to conveyinformation to a range of different listeners.One of the best known and universal statistical behav-iours of language is Zipf s law. This law states that forany sufficiently large corpus, word frequency is approxi-mately inversely proportional to word rank. In fact,Zipf s law is considered as a universal characteristic ofhuman language [14] and as a wider property of manydifferent complex systems [15] as well as human lan-guages [16]. Zipf suggested that this universal regularityin languages emerges as a consequence of the competingrequirements of the person or system coding the informa-tion (speaker) compared with the person or system tryingto decode the information (listener). From the perspectiveof the speaker, it would be most straightforward for themto code the signal using high level, non-specific terms asthese are easy to retrieve. It is more difficult to code thesignal using very specific terms as this requires huntingthrough long lists and navigating deep into the termin-ology. The problem is very different for the listener. Forthem the problem is one of resolving ambiguity. If the datais coded using very specific terms then ambiguity is min-imal and interpreting the message is straightforward. Ifonly high level general terms are used, then it is muchharder to discern the meaning of the message. In any com-munication system there is therefore a tension between thework being done by the speaker and the listener. Indeed,some controversial recent papers have attempted to showthat Zipf s law emerges automatically in systems that sim-ultaneously attempt to minimise the combined cost of cod-ing and decoding information [16-18].Similar issues clearly arise in medical coding in whichthere needs to be a balance between the efforts requiredfrom the coder with those of the person interpreting andusing the data. Reaching a proper balance between com-prehensiveness and usability of clinical vocabularies isregarded as one of the challenges in the medical inform-atics domain [19].The hypothesis we are therefore exploring in this paperis whether a Zipfian analysis of medical coding data canprovide useful insights into the nature and quality ofdata. For example, we can ask where this balance liesacross different aspects of the data medically-codedcaptured in GP records, information about diagnosis,information about the medical procedures applied andmedication prescribed, and whether this balance is differ-ent across different general practices. We have thereforeperformed a computational linguistics analysis of a largecorpus of anonymised Read code data from GPs inSalford to see whether such analyses might have value inunderstanding and characterising coding behaviour anddata quality in electronic patient records. Salford is a cityin the North West of England with an estimated popula-tion of 221,300. The health of people in Salford is gen-erally worse than the English average, including theestimated percentage of binge drinking adults, the rate ofhospital stays for alcohol-related harm, and the rate ofpeople claiming incapacity benefit for mental illness. How-ever, the percentage of physically active adults is similarto the English average and the rate of road injuries anddeaths is lower.MethodsThe data setFor this study we took GP data from Salford. Data from2003 to 2009 was collected from 52 General Practicegroups from Salford. This data consisted of anonymisedpatient identifiers, anonymised GP practice identifiersand the set of Read codes collected. In total, the data setcontains over 136 million Read codes derived from34200 distinct codes. Ethical permission for this studywas granted through North West e-Health. Table 1 showsan example of a set of Read codes and demonstrates theway in which specificity increases with code depth.Zipfs law analysisMathematically, Zipf s law can be expressed as:f rð Þ ¼ r??where f(r) refers to the frequency of the word with rankr and a is the Zipf s law exponent. There are a numberof different ways in which this behaviour can be repre-sented mathematically - power law behaviour, Zipf s law,Paretos law - that can be demonstrated to be equivalentKalankesh et al. Journal of Biomedical Semantics 2014, 5:2 Page 3 of 8http://www.jbiomedsem.com/content/5/1/2[20]. For example, if P () is the proportion of wordsin a text with frequency  then Zipf s law can also beexpressed as:P ð Þe ?It is straightforward to show that ? and ? are related by:? ¼ 1þ 1?Figures in this paper have been presented in the formof the Pareto distribution (named after a nineteenth cen-tury Italian economist) as they provide the most con-venient form for calculating an accurate exponent. ThePareto distribution is expressed in terms of the cumula-tive distribution function (CDF):P X ? xð Þex?ka)c)DiagnosesFigure 1 The Pareto plots for the Salford data showing the cumulativfor the subset of the Read codes used in the Salford corpus. a) diagnodiagnosis and procedure codes could be effectively modelled, at least in pb). However, there was no range on which the medication data could be mwhere the distribution shape parameter, k, can be con-verted to the Zipf s law exponent (a) via:? ¼ 1kand to the power law exponent (?) as below:? ¼ 1þ kPareto plots and parameter estimations were calcu-lated using the Matlab packages plfit, plplot and, plpvadeveloped by Clauset and Shalizi [21]. These packagesattempt to fit a power law model to the empirical dataand then determine the extent to which the data reallycan be effectively modeled using a power law. Thesetools provide two statistics describing the data. The firstis a p-value that is used to determine the extent to whichthe power law model is appropriate. If the p-value isb)ProceduresMedicatione distribution function Pr(x) plotted as a function of frequency (x)sis codes; b) procedure codes; c) medication codes. The data forart of their range, by a power law (shown as the dotted lines in a andodelled by a power law, c).Kalankesh et al. Journal of Biomedical Semantics 2014, 5:2 Page 4 of 8http://www.jbiomedsem.com/content/5/1/2greater than 0.1 we can regard the power law to be aplausible model of our data. The second statistic pro-duced is ?, the exponent of power law.A number of Zipfian analyses were then performedon different subsets of the Read code data within theSalford corpus. In particular we looked at the subsets ofRead codes for codes to do with diagnosis, procedureand medication separately (Read codes used for diagno-sis start with an upper case character (A-Z), Read codesfor procedures begin with a number (09), and thosemedication with a lower case character (a-z) [22]). Wewere able to further subdivide the data into chaptersbased on the first letter of the Read code for more de-tailed analysis.We also performed a number of other simple analyses tocharacterise the Salford corpus. We first measured thetype-token ratio (TTR). The TTR is calculated by dividingthe types (the total number of different Read codes) by to-kens (total number of Read codes used), expressed as aFigure 2 Percentage of Read codes at each level of granularity as a fupercentage. In essence, this measure is equal to the numberof distinct terms (Types) in the corpus divided by the totalnumber of terms (Tokens) used [23]. A low TTR is a signalthat there is a lot of repetition in the terms used, a highTTR ratio is a signal that the vocabulary (distinct terms)used is rich. A second analysis examined the typical depthof the terms used from the Read codes in each of the sub-sets of data. In a final analysis we characterised the Readcode terminology itself, to how many terms at each levelthere were available to GPs in each chapter. We then re-peated this analysis in the Salford data looking at the set ofcodes that were actually used from this full set. From thiswe were able to determine the extent to which GPs did, ordid not, take advantage of the structure inherent in theterminology.ResultsIn the first analysis, the data was split by the three Readcode sections (diagnosis, procedure and medication) andnction of the Read code chapter.Kalankesh et al. Journal of Biomedical Semantics 2014, 5:2 Page 5 of 8http://www.jbiomedsem.com/content/5/1/2the Pareto distributions and power law exponents weredetermined. The Pareto plots for these data are shownbelow in Figures 1a to c. For these data sets, the valuesof the power law exponent for diagnosis, procedures,and medication were 1.66, and 1.68, and 1.94, with asso-ciated Type-Token Ratios (TTRs) of 2.7%, 0.32%, 0.35%respectively. However, the data in Figure 1c was not ef-fectively modelled by a power law (as determined by ap-value < 0.1) as there is no region of this curve thatcould be modelled by a straight line. A similar analysiswas performed on data from specific sub trees from thediagnosis chapters. In all cases we found clear Zipfianbehaviour (data not shown) for chapters in the diagnosisand procedure sections.It is evident from Figure 1c) that the medication codesdo not show Zipfian behaviour. We therefore exploredthe difference between the medication codes and othercodes from two perspectives: the depth of the codes pro-vided by the coding system itself for different categoriesof data (Figure 2), and the depth of codes used for0% 20% 40%0123456789ABCDEFGHJKLMNPQRSTUZabcdefghijklmnopqsuyMedicationsDiagnosesProceduresFigure 3 Percentage of Read codes at each level of granularity as a fudata set.describing different categories of data by doctors in prac-tice (Figure 3). In some chapters of Read codes, the hier-archies are deeper than in others. For example, the highestdepth of hierarchy for medication codes in the coding sys-tem is 4, whereas the highest depth of hierarchy for diag-nosis and procedure codes in the coding system is 5. It isinteresting to note that in the medication data all thecodes used had depth 4 and that there were no codes withdepths less than this. This contrasts sharply to the codesused in procedure and diagnosis which use a range ofdepths comparable to those provided in the Read codehierarchy. This is an indication that the medication datahave been encoded in such a way that information transfercan be maximised toward satisfying decoder needs (thespeaker has navigated to the roots of the hierarchy to en-code the information). It can be also interpreted that themedication Read Code r has been referred to the drug donly if r can be understood as referring to d by someoneother than the speaker (encoder) as a result of the com-munication act, an indexical reference system [24].60% 80% 100%Level 1Level 2Level 3Level 4Level 5nction of the Read code chapter as used by GPs in the SalfordKalankesh et al. Journal of Biomedical Semantics 2014, 5:2 Page 6 of 8http://www.jbiomedsem.com/content/5/1/2The data were then analysed as a function of the anon-ymised GP practice identifier. The typical values of ? inthe data ranged from 1.56 to 2.08. Percentage of typetoken ratio for aforementioned GP practices ranged from2.47% to 10.63%. This strongly suggests that the range ofcoding vocabulary used by different GP practices variesconsiderably in its richness and degree of repetition. Inmost of the graphs, two different regions could be recog-nised, a linear region on the left hand side (the moreuncommon terms) that fits the power law behaviour anda second region of higher frequency terms; the transi-tion between these region being the point at which thegraph deviates from the fitted line (Figure 4). A similarpattern has been observed in a Zipfian analysis of theBritish National Corpus (BNC) [25]. In the BNC cor-pus, the region of more commonly deployed codes wasdefined as a core vocabulary  the words commonlyused - and the region of less commonly used codes asa peripheral vocabulary  words more rarely used. A simi-lar interpretation can be made of the data from the medicalrecords. Despite difference in the value of exponents, allFigure 4 The Pareto plots for diagnosis Read codes used from six sepfigure we also show the measured values of ?, the measured Zipfs law expplots have one feature in common: average depth of codesin the region of core vocabulary is smaller (range 3.3-3.7)than that found in the regions of peripheral vocabulary(range 3.6-4.3). The analogy with language would be thatthe codes near the top of the Read code hierarchy consti-tute a core, commonly used, vocabulary, whereas the morespecialist terms found deeper in the hierarchy relate to amore peripheral and rarely used vocabulary.Discussion and conclusionsWithin the Salford corpus, the usage of Read codes fordiagnosis and process show a power law behaviour withexponents typical of those seen in natural languages.This supports the hypothesis being made in this paperthat there are overlaps between the processes involvedin describing medical data (terms chosen from a the-saurus to describe an encounter between a patient anda GP) and human communication (words chosen to de-scribe a concept to a listener). This was not only true ofthe complete data sets; it was also seen to be true of thedata from the specific chapters.arate GP practices from 20032006 (denoted as a to f). On eachonent, and the TTR, the type-token ratio.Kalankesh et al. Journal of Biomedical Semantics 2014, 5:2 Page 7 of 8http://www.jbiomedsem.com/content/5/1/2However, the story is not completely straightforward.There was one section of data captured by Read codesthat showed a very different behaviour, namely the medi-cation data. These data showed no evidence of Zipf s lawbehaviour and it would appear that the principle of reach-ing a balance between the encoding and decoding costshas broken down. The pattern of code use from the hier-archy of Read codes is very different for the medicationdata compared with process or diagnosis code. All Readcodes used by GPs for encoding the drug information isfrom the highest level provided by the hierarchy of ReadCode System. This would suggest that, in the case of medi-cation information, doctors attribute very high value tocreating minimal ambiguity in the message to the max-imum extent the coding system allows them. This is per-haps unsurprising as the prescription data are an input foranother health care professional in the continuum of care(pharmacist) and any ambiguity in the case of this sensitivedata could be harmful or fatal to a patient. The exactmatch between expression and meaning by someone otherthan encoder is critical. From this perspective, medicationdata seem to behave as an indexical reference in which anindexical expression e refers to an object o only if ecan be understood as referring to o by someone otherthan the speaker as a result of the communicative act.It is also the case that not all GPs use language in thesame way. It is known that capture of diagnosis informa-tion is very variable between different GP practices [26].At this stage, it is difficult to provide detailed explan-ation reasons for this. It could be that this reflects a dif-ference in the populations being served by each GP;however we do not have the information available to usin this study to allow us to address this. However, it issuggestive that this form of computational linguistic ana-lysis could provide useful information on the quality ofdata being captured from different GP surgeries. Thereis a significant body of work in language processinglooking at power law exponents and how they changewith different qualities of language, an analysis thatcould well have useful analogies for these data. At thisstage we do not have the information to determine theextent to which the signal mirrors the quality of the datacapture by the GPs, but this is clearly something thatwould warrant further study.Therefore, there are aspects of GP records that behavevery like a language and for which it would be appropri-ate to apply the methodologies of computational linguis-tics. Our hope is that the development of such methodscould provide important new tools to help assess andimprove the quality of data in the health service.AbbreviationsBNC: British National Corpus; CDF: Cumulative Distribution Function;GP: General Practitioners; NHS: National Health Service; TTR: Type-Token Ratios.Competing interestThe authors declare that they have no competing interests.Authors contributionsLRK performed the analyses and helped draft the paper. JPN and JP providedthe data sets for analysis and helped in the data interpretation. AB conceivedof the study, participated in its design and coordination and helped to draftthe manuscript. All authors read and approved the final manuscript.AcknowledgementsThis article has been published as part of thematic series Semantic Miningof Languages in Biology and Medicine of Journal of Biomedical Semantics.An early version of this paper was presented at the Fourth InternationalSymposium on Languages in Biology and Medicine (LBM 2011), held inSingapore in 2011.Author details1School Of Computer Science, University of Manchester, Oxford Road,Manchester M13 9PL, UK. 2Tabriz University of Medical Sciences, Tabriz, Iran.3Salford Royal NHS Foundation Trust, Stott Lane, Salford M6 8HD, UK.4School of Medicine, University of Manchester, Oxford Road, Manchester M139PL, UK. 5Northwest Institute for Bio-Health Informatics, University of Manchester,Oxford Road, Manchester M13 9PL, UK.Received: 3 September 2012 Accepted: 26 November 2013Published: 10 January 2014PROCEEDINGS Open AccessThe influence of disease categories on genecandidate predictions from model organismphenotypesAnika Oellrich1*, Sebastian Koehler2, Nicole Washington3, Sanger Mouse Genetic Project3, Chris Mungall1,Suzanna Lewis3, Melissa Haendel4, Peter N Robinson2, Damian Smedley1From Bio-Ontologies Special Interest Group 2013Berlin, Germany. 20 July 2013* Correspondence: ao5@sanger.ac.uk1Wellcome Trust Sanger Institute,Wellcome Trust Genome Campus,CB10 1SA Hinxton, UKAbstractBackground: The molecular etiology is still to be identified for about half of thecurrently described Mendelian diseases in humans, thereby hindering efforts to findtreatments or preventive measures. Advances, such as new sequencing technologies,have led to increasing amounts of data becoming available with which to addressthe problem of identifying disease genes. Therefore, automated methods are neededthat reliably predict disease gene candidates based on available data. We haverecently developed Exomiser as a tool for identifying causative variants from exomeanalysis results by filtering and prioritising using a number of criteria including thephenotype similarity between the disease and mouse mutants involving the genecandidates. Initial investigations revealed a variation in performance for differentmedical categories of disease, due in part to a varying contribution of the phenotypescoring component.Results: In this study, we further analyse the performance of our cross-speciesphenotype matching algorithm, and examine in more detail the reasons why diseasegene filtering based on phenotype data works better for certain disease categoriesthan others. We found that in addition to misleading phenotype alignmentsbetween species, some disease categories are still more amenable to automatedpredictions than others, and that this often ties in with community perceptions onhow well the organism works as model.Conclusions: In conclusion, our automated disease gene candidate predictions arehighly dependent on the organism used for the predictions and the diseasecategory being studied. Future work on computational disease gene prediction usingphenotype data would benefit from methods that take into account the diseasecategory and the source of model organism data.Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4http://www.jbiomedsem.com/content/5/S1/S4 JOURNAL OFBIOMEDICAL SEMANTICS© 2014 Oellrich et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction inany medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.BackgroundDespite many success stories in the identification of genetic causes for human heritablediseases, half of the currently described disorders with a presumed genetic etiology arestill without an identified molecular basis [1]. Although the identification of a noveldisease gene rarely leads to immediate, novel treatment options, clearly an understand-ing of the cellular pathways and networks affected by a genetic mutation is the basisfor developing improved treatment strategies and optimal genetic counseling. To sup-port the identification of genetic causes, and with that treatment of human heritabledisorders, biological as well as computational methods have been developed [2-7].However, none of the existing solutions is capable of providing reliable answers for alldiseases and improvements are still needed.Technology advances have led to solutions enabling rapid and cheap identification ofvariants in human genomes and exomes. However, these methods yield long lists of var-iants reflecting the fact that each individual harbours more than 30,000 variants identifi-able by exome sequencing, with typically 5% or more of variants not being listed indatabases of variants such as dbSNP. Typical bioinformatic filtering procedures removecommon variants and those deemed to be nonpathogenic, but are not able to narrow thesearch down to only a short list of candidates based only on the sequence variants.In a recent study, we presented the PHenotypic Interpretation of Variants in Exomes(PHIVE) algorithm that in addition to traditional variant filtering and evaluation alsoincludes the phenotype manifestations in individuals as well as the signs and symptomsof diseases [8]. It was shown that including phenotype information into the prioritisa-tion of candidate genes leads to an up to 54.1 fold improvement over methods purelybased on variant information. To assess the phenotypic suitability of a gene variant,PhenoDigms phenotype comparison algorithm was used [4]. The study also showedthat the performance of the PHIVE algorithm is influenced by the mode of inheritance(autosomal dominant vs. autosomal recessive) and by the class of mutation (nonsenseand missense mutations). However, our investigations did not include an evaluation ofthe characteristics of the diseases in question.Ongoing debates highlight that model organisms do not necessarily constitute idealfits for certain diseases [9], due to e.g. changes in gene expression [10], but can stillprovide valuable insights into a disease even though only part of the phenotypes maybe reproduced in a model organism [11,12]. To facilitate the linkage of model organ-isms and diseases, ongoing efforts such as the International Mouse PhenotypingConsortium (IMPC) and the Zebrafish Mutation Project (ZMP) record a range of pre-defined parameters that are not just restricted to phenotypes related to the diseasearea a researcher is studying [13].The number of automated disease gene candidate prediction tools using cross-speciesinformation is also increasing [14,4,6,5]. The aforementioned tools rely on the availabilityof logical definitions for phenotypes that allow their comparison across species [15].Typically, precision and recall measures for known gene-disease association are reportedto give an indication about the potential of the method and its suitability to the task ofdisease gene candidate identification. While Börnigen et al. worked on unbiased evalua-tion of the tools [16], to our knowledge, no further evaluation for performance of differ-ent disease categories has been undertaken. Tools that use model organism data for theprediction are limited not only to the availability of sufficient and unbiased experimentalOellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4http://www.jbiomedsem.com/content/5/S1/S4Page 2 of 14data, but are also restricted to disease areas where model organisms recapitulate thedisease and where the phenotype associated genes are orthologous.In this study, we analysed Exomisers performance with respect to disease categoriesprovided by Orphanet [17]. As the performance of Exomiser is influenced by the Phe-noDigm phenotype comparison algorithm, we based our experiments on the evaluationof PhenoDigm and its applicability to disease categories. Using known gene-diseaseassociations in the Orphanet and Online Mendelian Inheritance in Man (OMIM) data-bases [1], we identified areas for further improvements that will consequently influenceExomisers performance. Although we only currently use PhenoDigms mouse-basedpredictions in Exomiser, we plan to take advantage of zebrafish phenotypes amongstother model organism data in the future as part of our participation in the MonarchInitiative. Hence, we performed our assessment across both mouse and zebrafish data.One factor in the poorly performing disease categories was the sub-optimal imple-mentation of our approach for certain phenotype annotations, leading to missingphenotype alignments. These will be addressed in future releases. Other clinical pheno-types were not matched because they can not be accurately observed in the modelorganism in question. Interestingly, some perceptions of how well or how easily differentmodel organisms can be fitted to particular disease areas are mirrored in the evaluationresults. We conclude that automated prediction methods could potentially benefit fromtaking into consideration the categories of disease in which semantic model organismphenotype matching works best.Results and discussionExomiser provides functionality to filter and prioritize gene variant lists using our PHIVEalgorithm which combines phenotype comparisons from PhenoDigm in addition to allelefrequency and pathogenicity scores [8]. Our benchmarking of Exomiser was based on28,516 known disease-causing mutations from the Human Gene Mutation Database [18].Using Orphanets disease categorisation [19], we further divided Exomisers evaluationexome data sets by disease category. Figure 1 shows that Exomisers ability to identify thedisease causing genes using the PHIVE algorithm varies for the different disease cate-gories. All results fall into the range of 35 to 78%, with best performance in the gastroen-terological diseases category. Figure 1 also shows the performance of Exomiser if only thephenotype prioritization for genes is used but not allele frequency and pathogenicity. It isapparent that the phenotype score works better for some disease categories than forothers. For example, in the case of gastroenterological diseases the phenotype comparisonseems to contribute a lot to the identification of disease gene candidates while in the caseof surgical maxillo facial diseases, the contribution seems to be comparatively small. Notethat not all of Orphanets disease categories are represented due to the limited coverage inour evaluation set of 28,516 known disease-causing mutations.As the PHIVE algorithm combines PhenoDigm, allele frequency and pathogenicityprioritisation as well as some pre-filtering steps, evaluation of just the performance of thephenotype comparison is problematic. Therefore, we decided to further investigate theeffect of disease category on phenotype comparisons by just looking at the performance ofPhenoDigm. Due to the inclusion of these other steps in Exomiser, we do not expectobservations based on PhenoDigm performance for different disease categories to translatedirectly to Exomiser but should indicate potential categories where we may see enhancedOellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4http://www.jbiomedsem.com/content/5/S1/S4Page 3 of 14or reduced performance. In addition, PhenoDigm covers all of OMIM and Orphanet aswell as including zebrafish so we were able to perform a more extensive evaluation of theinfluence of disease categories on gene candidate predictions from model organismphenotypes.Analogous to the evaluation of Exomiser, we divided the diseases covered in Pheno-Digm into the disease categories provided by Orphanet and known gene-disease asso-ciations contained in Orphanet and OMIM. We then assessed precision and recallover the different ranks of diseases genes and determined the Area Under Curve(AUC) of the corresponding Receiver Operating Characteristic (ROC) curve. The AUCmeasures obtained for the individual disease categories and both the species (mouseand zebrafish) are presented in Table 1. AUC measures in mouse vary in the range of[0.774, 0.901] and in zebrafish in the range of [0.540, 0.835]. These results show thatAUC measures calculated over all diseases may mask disease categories that are per-forming well, e.g. zebrafish performs better than mouse for cardiac malformationsalthough the overall performance is much worse.Comparing the two model organisms, the most striking observations are first of all,that the performance for zebrafish for nearly all disease categories is reduced and sec-ondly, that performance is much more dependent on the disease category than it is forthe mouse. Given the species-divide between human and zebrafish compared tomouse, some of this reduced performance and increased variability maybe expected.Figure 1 Performance of PHIVE score in Exomiser by Orphanet disease category, together with justthe phenotype-based scores. The number of diseases tested for each category are shown in parentheses.Note that many diseases belong to multiple disease categories. The overall PHIVE performance and thecontribution of the phenotype score used in Exomiser is seen to vary with respect to the disease category.The highest contribution of the phenotype score is in the category of gastroenterological diseases, thesmallest in the category of abdominal surgical diseases.Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4http://www.jbiomedsem.com/content/5/S1/S4Page 4 of 14Although many of the organ systems and biological processes are similar in the zebrafish,some differences obviously exist that will affect certain disease categories more thanothers. However, much of the difference could also be due to focus of research using thesedifferent model organisms as well as varying technical difficulties with applying oursemantic comparison approach to the different phenotype ontologies used for human,mouse and fish.An additional factor, may be the extra difficulty of assigning orthology betweenhuman and zebrafish genes due to the greater evolutionary distance. In addition, manyTable 1 PhenoDigm performs best for urogenital diseases for mouse and cardiacmalformation for fish out of 31 disease categories.disease category* diseases(mouse)AUC(mouse)diseases(fish)AUC(fish)abdominal surgical 104 0.856 (0.336) 67 0.716(0.033)Allergic 5 - 0 -Bone 368 0.870 (0.002) 185 0.650 (0.110)cardiac 128 0.857 (0.138) 58 0.675(0.049)cardiac malformations 34 0.822 (0.221) 23 0.835 (1E-4)circulatory system 63 0.825 (0.239) 31 0.658 (0.417)developmental anomalies inembryogenesis943 0.852 (0.177) 475 0.673 (1E-4)endocrine 307 0.874 (0.029) 128 0.629 (0.382)eye 582 0.864 (0.034) 269 0.646 (0.147)gastroenterological 74 0.842 (0.391) 36 0.739(0.031)haematological 151 0.816 (0.151) 53 0.603 (0.215)hepatic 41 0.774 (0.011) 8 -immunological 134 0.843 (0.391) 36 0.540(0.014)inborn errors of metabolism 384 0.789 (5E-9) 91 0.646 (0.103)infectious 3 - 2 -infertility 41 0.817 (0.154) 18 0.635 (0.496)neurological 777 0.787 (2E-11) 328 0.630 (0.486)odontological 44 0.899 (0.078) 18 0.693 (0.161)otorhinolaryngological 150 0.890 (0.043) 74 0.731(0.015)renal 277 0.846 (0.479) 130 0.676(0.048)respiratory 65 0.808 (0.126) 35 0.594 (0.135)skin 418 0.852 (0.161) 154 0.636 (0.442)surgical maxillo facial 89 0.836 (0.367) 56 0.723 (5E-4)surgical thoracic 33 0.816 (0.176) 12 0.641 (0.364)systematic and rheumatological 68 0.832 (0.297) 14 0.592 (0.311)teratologic 1 - 1 -tumors 239 0.835 (0.388) 130 0.677(0.044)urogenital 62 0.901 (0.050) 31 0.608 (0.207)all diseases 3728 0.845 1558 0.630* disease categories according to Orphanet [19];  number of diseases falling into this category with phenotype data forthe orthologue(s) of the associated gene;  AUC, measured on disease-gene associations from OMIMs MorbidMap andOrphanet curation. Value in brackets shows the p-value of obtained this result compared to those obtained fromrandomly selecting the same number of diseases. Significant results (p value <0.05) are shown in bold. Note that onedisease may fall into different categories due to multiple systems affected by disease.Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4http://www.jbiomedsem.com/content/5/S1/S4Page 5 of 14of the genes are part of a genome duplication event in zebrafish so that one humandisease gene may correspond to two zebrafish genes and it may take disruption ofboth to recapitulate the clinical phenotypes.To investigate some of these issues we analysed the annotations and the correspondingPhenoDigm matches in more detail for the best, intermediate and worst performing dis-ease categories. In addition to the calculation of AUC measures, we further investigatedsix of the disease categories (three for each species) to obtain a better understanding ofthe shortcomings of either the method or the data. For each disease category, we investi-gated the 10 most common clinical phenotypes and their best matches in the modelorganism phenotypes.Investigation of mouse model prediction resultsTo identify reasons for the differences in performances with respect to the applied diseasecategories, we further investigated the following three categories of diseases: urogenital,hepatic and neurological diseases. We studied their annotations and the correspondingbest phenotype matches in mouse produced by PhenoDigm, together with potential biolo-gical reasons for differences in performance. The results for each of the three furtherinvestigated disease categories are shown in Additional File 1 and discussed in more detailin the following sections.Urogenital diseasesThe ten most frequently occurring clinical Human Phenotype Ontology (HPO) pheno-types in this category include expected urogenital phenotypes such as Cryptorchidismas well as others such as Short stature, Microcephaly and Cognitive impairment. Thelatter are due to the Orphanet classification allowing diseases to be assigned to multi-ple categories e.g. many diseases may be classified as both urogenital and neurologicalleading to a preponderance of both urogenital and neurological phenotypes when lookingat each individual category.The common urogenital and other types of clinical phenotypes all matched the expectedmouse phenotypes in Mammalian Phenotype Ontology (MP) (and their more specificchild terms when present) with the exception of Cognitive impairment. Cognitive impair-ment was the 10th most commonly observed clinical phenotype in this disease categoryand is obviously a more difficult phenotype to measure in a mouse model than physicalabnormalities such as Cryptorchidism and there is no directly corresponding MP equiva-lent term. Hence, PhenoDigm ends up matching numerous general abnormalities ofhigher mental function such as increased anxietyrelated response (MP:0001363) which willlead to Cognitive impairment not being an informative phenotype for selecting specificmouse model matches.The specific recall for most of the associated clinical phenotypes increases the likelihoodof mouse models being predicted that are relevant to the urogenital diseases. Assumingalso, that mouse models disrupting the known urogenital disease genes produce a pheno-copy of the disease, then performance of PhenoDigm would be expected to be good forthis category as was observed.Hepatic diseasesUseful animal models of liver disease have only very recently been identified [20], so it isperhaps not surprising that we found that this disease category to be the worst performingfor mouse. Despite the fact that mice do not necessarily constitute a good modelOellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4http://www.jbiomedsem.com/content/5/S1/S4Page 6 of 14organism to study hepatic diseases, we still investigated the phenotype matches togetherwith the predicted disease models for this disease category.In contast, to some of the other disease categories, the ten most frequently occurringphenotypes were all consistent for hepatic disease rather than some being a consequenceof Orphanets classification of certain diseases to multiple categories. Eight of these mostfrequent occurring clinical phenotypes recalled the expected mouse phenotype as the bestmatch in MP (and their child terms if they existed). For example, Figure 2a shows Pheno-Digm results in Hepatomegaly matching enlarged liver, liver hyperplasia and increasedliver weight as the best mouse phenotypes.The other two concepts mapped sub-optimally or produced completely misleadingresults. For example, Figure 2b shows how the HPO concept Pruritus is matched toabnormal skin physiology as the best hit in mouse as well as all its child terms dueto the lack of logical definitions for Pruritus (HP:0000989) and increased pruritus(MP:0010072). Although the 28 child terms include the ideal match, the additionalmatches will lead to non-specific mouse models being recovered. Finally, the bestmatches for the HPO concept Elevated hepatic transaminases (HP:0002910) wereincreased liver copper level and increased liver iron level based purely on increasedconcentrations of any object in the liver. Even though a corresponding MP conceptexists, increased circulating aspartate transaminase level (MP:0005343), the correctlogical definitions do not yet exist for PhenoDigm to have identified thisrelationship.Figure 2 Relationships between common HPO clinical phenotypes for the hepatic disease class andMP terms. (a) The HPO term for Hepatomegaly is identified as being equivalent to the MP term forenlarged liver via their logical definitions. This MP term and its children are identified as the best scoringmatches. (b) Lack of a logical definition for Pruritis leads to the best scoring MP matches being to thehigher level term of abnormal skin physiology and all of its multiple child terms, many of which having norelationship to pruritis.Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4http://www.jbiomedsem.com/content/5/S1/S4Page 7 of 14Although the performance could be improved if accurate logical definitions were createdfor the two poorly-mapped phenotypes, the fact that the others matched the expectedmouse phenotypes as the best hit suggests that the poor overall performance of Pheno-Digm in this disease category may be due to mouse mutants of hepatic disease genes notrecapitulating the same phenotypes.Neurological diseasesIn previous studies it has been found that there are sufficient commonalities betweenhumans and mice to determine disease gene candidates for some of the diseases belong-ing to the category of neurological diseases, e.g. diseases related to addiction [21].However, there are still differences between mice and humans related to gene structureand spatiotemporal expression patterns that may prevent mice being in general applicableto neurological diseases [12]. Despite a mouse model not faithfully recapitulating a humandisease, the mouse model may still provide insights into the origin of the disease [11].The ten most commonly occurring HPO annotations in this category included two thatdo not have a neurological basis but are due to Orphanets co-classification of diseases:Short stature and Scoliosis. However, both match the expected terms as the best hit in MPand their inclusion would therefore not be expected to account for the relatively poorperformance of PhenoDigm for this disease category.Looking at just the eight neurological phenotypes, four match the expected terms in MPand their child terms where present: Seizures, Muscular hypotonia, Microcephaly andNystagmus. The other four only match high level terms in MP and all their child terms asthe best scoring hits: Cognitive impairment, Intellectual disability, Global developmentaldelay and Hyperreflexia. These multiple matches lead to an imprecision when mousemodels are ranked according to their phenotype similarity with the disease. This poten-tially leads to noisy results as multiple models are associated that are not necessarily rele-vant for the disease but due to the misaligned phenotypes. These issues in semanticallymapping behavioural phenotypes may account for a large proportion of the poor perfor-mance in this disease category as opposed to underlying problems with using mice tomodel the biology of neurological diseases.Investigation of zebrafish model prediction resultsTo identify reasons for the differences in performances with respect to the applied dis-ease categories, we further investigated the following three categories of diseases: cardiacmalformations, immunological and bone diseases. We studied their annotations and thecorresponding matches from PhenoDigm and the results are summarised in AdditionalFile 1.Cardiac malformationsThe main reason for zebrafishs adoption as a model organism is the translucency of theorganism in the embryonic stage, allowing in vivo, non-intrusive visualisation of organs aswell as biological processes. Therefore, zebrafish are ideal model systems for studyingdevelopmental diseases and this may go some way to explaining why zebrafish outper-formed mouse as a model of the congenital cardiac malformations in our analysis.We observe that the most common clinical phenotype annotations in this diseasecategory are matched efficiently by PhenoDigm. For example, Abnormality of the aorta(HP:0001679) matches various more specific aortic abnormalities in the ZebrafishOellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4http://www.jbiomedsem.com/content/5/S1/S4Page 8 of 14Phenotype Ontology (ZP) annotations, whilst the clinical phenotypes Defect in theatrial septum (HP:0001631) and Ventricular septal defect (HP:0001629) are bestaligned with the zebrafish phenotype abnormally closed atrioventricular node. Tetral-ogy of Fallot (HP:0001636) in itself comprises four separate phenotypes and here onlymatches abnormally aplastic ventricular endocardium epithelium as the best hit whichis only a close association at best. In contrast, Patent ductus arteriosus (HP:0001643)does not match any sensible zebrafish phenotype but closure of the ductus arteriosison birth, allowing the lungs to get their own supply of blood, is known to be a specificaspect of air-breathing vertebrates [22].Many of the non-cardiac associated annotations seen in this disease category matchless well to the fish phenotypes but are also less commonly seen and the cardiacmatches alone appear to have been enough to efficiently recall the correct zebrafishmodels for most cardiac malformations. For example, no match to Cognitive impair-ment (HP:0100543) is retrieved and Microcephaly (HP:0000252) is aligned with abnor-mally decreased thickness cranial nerve VIII as the best match. Future investigation ofwhy zebrafish phenotypes such as abnormally hypoplastic head were not the best hitfor Microcephaly and some of the learning/memory fish phenotypes were not pickedup for Cognitive impairment may further improve recall.Immunological diseasesAlthough the zebrafish immune system closely approximates that of mammals, themain use of zebrafish in immunology comes from the fact that the embryonic stagealready has a fully competent innate immune system allowing contrasting studies withthe adaptive system [23]. One explanation for the poor performance in this diseasecategory is that human adult immune phenotypes from a mixture of innate and adap-tive responses are being compared to zebrafish embryonic innate phenotypes. Themost common clinical phenotypes seen in this category are Splenomegaly(HP:0001744) and Hepatomegaly (HP:0002240) and these match the expected ZPterms of abnormally increased size spleen and abnormally increased size liver as thebest scoring hits. However, other common clinical annotations such as Recurrent bac-terial or respiratory infections (HP:0002718, HP:0002205) are not matched to anythingin the zebrafish annotations beyond generalized immune system abnormalities. Recur-rent infections suggest a long-lasting loss of protective immunity due to a perturbancein the adaptive immune system and as described above, this would not be observed inthe embryonic zebrafish stages.Other common immunological annotations in this category include Lymphadenopa-thy (HP:0002716), Anemia (HP:0001903), Neutropenia (HP:0001875) and Thrombocyto-penia (HP:0001873) but none of these match the expected phenotypes in the zebrafishas the best scoring hit using our approach. The fact that zebrafish lack lymph nodesexplains the first one but there are fish annotated with abnormally present in fewernumbers in organism nucleate erythrocyte , abnormally present in fewer numbers inorganism neutrophil and abnormally present in fewer numbers in organism thrombo-cyte, so it would be expected that the clinical phenotypes should have recalled thesefish phenotypes. Investigation and restructuring of the underlying ontologies and/orlogical definitions to pick up these matches would presumably lead to an improvementin performance for this disease category.Oellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4http://www.jbiomedsem.com/content/5/S1/S4Page 9 of 14Bone diseasesDespite having a different skeletal organisation, zebrafish has recently emerged as a use-ful complementary model for bone research due to the ability to study in vivo, processessuch as osteogenesis and mineralization thanks to the existence of osteoblast-specificreporter lines [24].Using PhenoDigm, the performance was mid-range for this disease category relativeto the others. Looking at the most common skeletal clinical phenotypes we find thatsome match to the equivalent concepts in zebrafish whilst others warrant furtherattention. For example the most common clinical phenotype, Short stature(HP:0004322), is completely mis-matched to abnormally decreased height enterocyte asthe best match. Fixing our approach such that abnormally decreased length wholeorganism is the best match would probably lead to a dramatic increase in performance.Other matches such as Scoliosis (HP:0002650) with abnormally curved lateral vertebralcolumn, Micrognathia (HP:0000347) with abnormally aplastic dentary and Brachydac-tyly syndrome (HP:0001156) with abnormally aplastic pectoral fin skeleton are reason-able considering the evolutionary distance.ConclusionsExomiser is a tool to narrow down gene candidate lists that have been identified inexome analyses using cross-species phenotype comparisons amongst other sources ofevidence. Here we investigated the underlying PhenoDigm algorithm for different diseasecategories to understand where the approach is currently working well and to identifyareas for further improvement. We demonstrated that the phenotype comparisons workbetter for some disease categories than for others. Furthermore, the prediction resultsdepend on the organism and when automatically predicting disease gene candidatescareful consideration is required as to which organism to apply for the predictions.However, it is somewhat difficult to disentangle whether performance differences existdue to differences in biology, the annotation methods used for each species or the focusof annotations for mouse and fish.In addition to the identified biological restrictions that partially mirror community per-ceptions of how well the model organism can be fitted to human diseases, we showed thatthe underlying methodology still needs improvements. Even though a lot of work hasbeen done in this direction, more logical definitions are needed in addition to improvingthe quality of the existing definitions to improve semantic mapping between the species-specific phenotype ontologies. Future work, will focus on improving these definitions andwill undoubtably lead to improvements in the performance of PhenoDigm and Exomiser.Even with a perfectly aligned set of phenotype ontologies, our results highlight that itwill be dangerous to discount a model just because it does not perfectly match all theclinical phenotypes of the disease. For example, matches to clinical phenotypes such aslymphadenopathy were not seen in our assessment of the zebrafish results due to thelack of lymph nodes in fish rather than our alignment approach. In addition, differentareas of interest of the researchers who phenotype the models need to be taken intoaccount when using model organism to understand the genetic basis of disease i.e. par-ticular phenotypes may not have been assessed. In conclusion, smarter tools arerequired that take into account the differences between species and accumulateOellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4http://www.jbiomedsem.com/content/5/S1/S4Page 10 of 14predictions not only over multiple species but apply a sorting with respect to theapplicability of the species in the particular area of disease.MethodsBenchmark data: MorbidMap and OrphanetAssessing the performance of a gene prediction or prioritisation algorithm requiresbenchmark data containing established gene-disease associations. One database con-taining manually confirmed associations between human diseases and genes is OMIM[1]. The human-centric gene-disease associations from OMIM are available via adownload file called MorbidMap [25]. In addition we used the disease-gene associa-tions curated by Orphanet. Both OMIM and Orphanet have HPO annotations that canbe used by PhenoDigm and both were downloaded on 20 July 2013 and MouseGenome Database (MGD)s orthology file (see [26]) was used to convert the genes intomouse-specific gene identifiers that can be used for evaluation purposes. The finaldataset contained a total of 3,429 diseases associated with 2,662 unique genes, whichmapped to 2,772 orthologous genes in mouse and 1862 in fish.Generating prediction results with PhenoDigmPhenoDigm [4] uses phenotype descriptions of human heritable disease and individualanimal models to predict potentially gene candidates that may be causative for a dis-eases. The PhenoDigm algorithm uses a pairwise semantic similarity based on pheno-type ontology annotations, such as HPO or MP, and prioritises genes according to thissimilarity measure. Applying the PhenoDigm method, a database was generated con-taining all the results displayed in the online web interface [27]. Instead of regeneratingthe data, we used the data built from 20 July 2013 so that the results presented herecorrespond with the current publicly available data.Dividing diseases into sets according to Orphanet categorisationTo divide the disease into sets that are biologically meaningful, we downloaded the Orpha-net categorisation files from the Orphanet data download page [19] on 18 July 2013. Wedownloaded and processed 31 data files, one for each of the high level disease categoriesin the Orphanet categorisation. Each of the files contains a number of diseases that mayor may not be referenced to OMIM. Furthermore, a disease may not only be assigned toone category and instead be mentioned in multiple files. For example, X-linked myotubu-lar myopathy (OMIM:#310400) is categorised as a rare eye and neurological disorderbecause the most prominent symptoms include weakness, hypotonia and respiratoryfailure, as well as external ophthalmoplegia.We note here that the Orphanet web interface also provides a category of sucking/swallowing disorders. This disease category was not included here as no categorisationfile was provided on the Orphanet download page [19].Assessing PhenoDigms performance according to disease categoriesTo determine PhenoDigms performance, we applied ROC curves based on the gene-disease associations (see Benchmark data: MorbidMap and Orphanet ). We dividedthe diseases into sets according to Orphanets categorisation (see Dividing diseases intosets according to Orphanet categorisation) and consequently generated 31 evaluationOellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4http://www.jbiomedsem.com/content/5/S1/S4Page 11 of 14sets. PhenoDigms ranking was then compared using the 31 evaluation sets corre-sponding to each of the disease categories by determining true and false positivecounts individually for each rank. As true positive counts a gene that is associated to adisease in MorbidMap or Orphanet. Conversely, a gene that is not mentioned inMorbidMap or Orphanet for a particular disease counts as false positive. We notehere, that gene-disease associations may be counted as falsely identified connections,even though there is a relationship but it is not yet confirmed. However, we assumethat this number is relatively small compared to the large number of possible combina-tions of genes and diseases and assume that our evaluation procedure is still appropri-ate. As a consequence, the true predictive rates provided here may be lower than theyare in reality. To test the significance of each ROC analysis we performed 50 simulationsper disease category where a set of diseases of the same size as the evaluation set wasrandomly chosen. These simulations provided a mean and standard deviation for therandom distribution of scores for each evaluation set and these were used to calculate ap-value for the obtained result.Manual assessment of six disease categoriesFurther investigations into individual disease categories were necessary to identifypotential shortcomings in either method or data. We chose six categories of diseasesbased on the worst, best, and one intermediate AUC score for each species (mouseand zebrafish). Two curators assessed the phenotype matches to either mouse or fish(see Additional File 1) for the ten most frequently occuring phenotypes accumulatedover all the diseases falling into this category. The matches were assessed with respectto their biological correctness and whether they were sufficiently suitable to identifymodels from the respective organism.Additional materialAdditional file 1: Annotations and their matches for urogenetial, hepatic and neurological diseases withrespect to mouse and fish models. Excel sheet that contains the HPO annotations for the six furtherinvestigated disease categories: urogenital, neurological, hepatic, cardiac malformations, bone and immunological.In addition to the ten most frequent HPO annotations, we included the best scoring semantic matches to therespective model organism (either MP or ZP) as well as the frequency of this annotation.List of abbreviationsMouse Genetics Project (MGP), Mouse Genome Database (MGD), Online Mendelian Inheritance in Man (OMIM),Mammalian Phenotype Ontology (MP), Human Phenotype Ontology (HPO), Receiver Operating Characteristic (ROC),Area Under Curve (AUC), Standard Operating Procedure (SOP),Competing interestsThe authors declare that they have no competing interests.Authors contributionsAOE and DS designed the study, as well as implemented all required scripts. CM developed the PhenoDigm softwareand NW, MH contributed annotation datasets. PR and SK contributed to the analysis of the results. All authorscontributed to the final manuscript.AcknowledgementsThis work was supported by core infrastructure funding from the Wellcome Trust and National Institutes of Health(NIH) grant [1 U54 HG006370-01].DeclarationsPublication in this supplement was support by National Institutes of Health (NIH) grant [1 U54 HG006370-01]. Thisarticle has been published as part of the Journal of Biomedical Semantics, Volume 5 Supplement 1, 2013: ProceedingsOellrich et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S4http://www.jbiomedsem.com/content/5/S1/S4Page 12 of 14of the Bio-Ontologies Special Interest Group Meeting 2013. The full contents of the supplement are available online athttp://www.jbiomedsem.com/supplements/5/S1.This article has been published as part of Journal of Biomedical Semantics Volume 5 Supplement 1, 2014: Proceedingsof the Bio-Ontologies Special Interest Group 2013. The full contents of the supplement are available online at http://www.jbiomedsem.com/supplements/5/S1.Authors details1Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus, CB10 1SA Hinxton, UK. 2Institute for MedicalGenetics and Human Genetics, Universitaetsklinikum Charite, Augustenburger Platz 1, 13353 Berlin, Germany. 3BerkeleyBioinformatics Open-Source Projects, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, CA 94720 Berkeley,USA. 4Ontology Development Group, OHSU Library, Oregon Health & Science University, 3181 S.W. Sam Jackson ParkRd, OR 97239 Portland, USA.Published: 3 June 2014JOURNAL OFBIOMEDICAL SEMANTICSZhang et al. Journal of Biomedical Semantics 2014, 5:33http://www.jbiomedsem.com/content/5/1/33RESEARCH Open AccessNetwork-based analysis reveals distinct associationpatterns in a semantic MEDLINE-baseddrug-disease-gene networkYuji Zhang1*, Cui Tao2, Guoqian Jiang3, Asha A Nair3, Jian Su4, Christopher G Chute3 and Hongfang Liu3AbstractBackground: A huge amount of associations among different biological entities (e.g., disease, drug, and gene) arescattered in millions of biomedical articles. Systematic analysis of such heterogeneous data can infer novelassociations among different biological entities in the context of personalized medicine and translational research.Recently, network-based computational approaches have gained popularity in investigating such heterogeneousdata, proposing novel therapeutic targets and deciphering disease mechanisms. However, little effort has beendevoted to investigating associations among drugs, diseases, and genes in an integrative manner.Results: We propose a novel network-based computational framework to identify statistically over-expressedsubnetwork patterns, called network motifs, in an integrated disease-drug-gene network extracted from SemanticMEDLINE. The framework consists of two steps. The first step is to construct an association network by extractingpair-wise associations between diseases, drugs and genes in Semantic MEDLINE using a domain pattern drivenstrategy. A Resource Description Framework (RDF)-linked data approach is used to re-organize the data to increasethe flexibility of data integration, the interoperability within domain ontologies, and the efficiency of data storage.Unique associations among drugs, diseases, and genes are extracted for downstream network-based analysis. Thesecond step is to apply a network-based approach to mine the local network structure of this heterogeneousnetwork. Significant network motifs are then identified as the backbone of the network. A simplified network basedon those significant motifs is then constructed to facilitate discovery. We implemented our computationalframework and identified five network motifs, each of which corresponds to specific biological meanings. Threecase studies demonstrate that novel associations are derived from the network topology analysis of reconstructednetworks of significant network motifs, further validated by expert knowledge and functional enrichment analyses.Conclusions: We have developed a novel network-based computational approach to investigate the heterogeneousdrug-gene-disease network extracted from Semantic MEDLINE. We demonstrate the power of this approach byprioritizing candidate disease genes, inferring potential disease relationships, and proposing novel drug targets,within the context of the entire knowledge. The results indicate that such approach will facilitate the formulizationof novel research hypotheses, which is critical for translational medicine research and personalized medicine.* Correspondence: yuzhang@som.umaryland.edu1Division of Biostatistics and Bioinformatics, University of MarylandGreenebaum Cancer Center and Department of Epidemiology and PublicHealth, University of Maryland School of Medicine, Baltimore, MD, USAFull list of author information is available at the end of the article© 2014 Zhang et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited.Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 2 of 13http://www.jbiomedsem.com/content/5/1/33BackgroundA large amount of associations among biomedical en-tities are scattered in biomedical literature. Systematicanalysis of such heterogeneous data provides biomedicalscientists with unprecedented opportunities to infer novelassociations among different biological entities in thecontext of personalized medicine and translational re-search studies. MEDLINE (http://www.nlm.nih.gov/bsd/pmresources.html), for instance, currently contains morethan 22 million citations of biomedical literature. SemanticMEDLINE is a knowledge base consisting of associationsautomatically extracted from MEDLINE by integratingdocument retrieval, advanced natural language processing(NLP), and automatic summarization and visualization[1]. However, it is computationally challenging to performqueries directly from Semantic MEDLINE where associa-tions among different biomedical entities are very complexyet sparse. It is also very difficult to investigate those asso-ciations at a large scale. Advance informatics approacheshave the potential to fill gaps between knowledge needs oftranslational researchers and existing knowledge discoveryservices.In Semantic MEDLINE, biomedical entities and associa-tions are semantically annotated using concepts in theUnified Medical Language System (UMLS) [2]. The se-mantic information defined in the UMLS can be furtherleveraged to extract associations among concepts in spe-cific domains and identify domain patterns for specificstudies through advanced computational methods such asnetwork-based analysis.In the last decade, network-based computational ap-proaches have gained popularity and become a newparadigm to investigate associations among drugs, dis-eases, and genes. Applications of these approaches in-clude disease gene prioritization [3-5], identification ofdisease relationships [6,7] and drug repositioning [8,9].However, majority of these approaches focus on rela-tionships between only two kinds of entities (e.g., asso-ciation between gene and disease). For instance, Hu andAgarwar [10] created a human disease-drug networkbased on genomic expression profiles collected fromthe Gene Expression Omnibus (GEO) (http://www.ncbi.nlm.nih.gov/geo/). In total, 170,027 interactions betweendiseases and drugs were considered significant, including645 disease-disease, 5,008 disease-drug, and 164,374 drug-drug associations. These expression-based associationsamong diseases and drugs could serve as a backendknowledge base to facilitate discovery. Bauer-Mehrenet al. [11] developed a comprehensive disease-gene as-sociation network by integrating associations from sev-eral sources that cover different biomedical aspects ofdiseases. The results indicate a highly shared geneticorigin of human diseases. Functional modules were alsodetected in several Mendelian disorders as well as incommon diseases. To systematically analyze drug-disease-gene relationships, Daminelli et al. [12] proposed anetwork-based approach to predict novel drug-geneand drug-disease associations by completing incom-plete bi-cliques in the network. This approach holdsgreat potential for drug repositioning and discovery ofnovel associations. However, the analysis was limitedto only certain associations among drugs, genes, anddiseases (e.g., drug-disease and drug-gene associations). Anetwork-based investigation of all pair-wise associationsamong these entities is necessary to understand the com-plexity of existing associations and to infer novel associa-tions within the context of the whole knowledgebase.Network-based computational approaches enable usto analyze heterogeneous networks such as drug-disease-gene networks by decomposing them into small sub-networks, called network motifs (NMs) [13]. NMs arestatistically significant recurring structural patterns foundmore often in real networks than would be expected inrandom networks with the same network topologies. Theyare the smallest basic functional and evolutionarily con-served units in biological networks. Our hypothesis is thatNMs of a network are the significant sub-patterns thatrepresent the backbone of the network, which serves asthe focused portion out of thousands of nodes (e.g., drugs,diseases, and genes,) [14,15]. These NMs could also formlarge aggregated modules that perform specific functionsby forming associations in overlapping NMs.In this paper, we propose a network-based compu-tational framework to analyze the complex networkformed by a large amount of associations. We focus ona heterogeneous drug-disease-gene network derivedfrom Semantic MEDLINE and investigated underlyingassociations using network-based systems biology ap-proaches. Three case studies demonstrate that our ap-proach has potential to facilitate formulization of novelresearch hypotheses, which is critical for translationalmedicine research. In the following, we first presentMaterials and methods. We then describe the resultsand case studies in detail.Materials and methodsTo comprehensively investigate the integrated drug-disease-gene network formed by associations available inSemantic MEDLINE, we propose the following two-stepcomputational framework: (1) extraction and optimizationof drug-disease-gene network in Semantic MEDLINE; (2)network topology analysis of this heterogeneous networkat two levels: statistics and degree distribution of high-confidence association networks, and distinct pattern de-tection at NM level. In this section, we first describe thesteps to extract association network data from MEDLINEdatabase, followed by a description of the proposednetwork-based approach to investigate this heterogeneousZhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 3 of 13http://www.jbiomedsem.com/content/5/1/33drug-disease-gene association network. Figure 1 illustratesthe steps of the proposed approach.Data sources and preprocessingExtraction of association data from Semantic MEDLINESemantic MEDLINE currently contains more than 56million associations extracted from MEDLINE citationsand consists of eight tables, including concepts, conceptsemantic types, concept translations, predication, predi-cation arguments, and sentences. Data from differenttables need to be joined in order to obtain informationfor a particular association between two entities. Thedatabase contains an all-embracing joined table that pro-vides information about associations (source concept,predicate, and object concept), and their source PubMedIDs (PMIDs).We optimize and reorganize the relevant data inSemantic MEDLINE into the Resource DescriptionFramework (RDF) format. Based on the UMLS semantictypes and groups [16], we extract unique associationsamong drugs, diseases, and genes, and represent them insix views in relational database tables. We then use theWeb RDF transformation tool D2R server to convertthe six views into RDF triples through a D2RQ mappingovarian cancer5000 ci onsRetrievelSemaNetworkSummariza onSignificant NEvalua on Biological interpret on of NMs Core network inves onDegree distri onHub nodes analysis0 100 200 300 400 500 600 700 800020406080100120140DrugDegreeNodes0 200 400 600 800 1000 120005001000150020002500DiseaseDegreeNodesFigure 1 Overview of the network-based computational framework fofile (http://d2rq.org/d2r-server). This mapping file spe-cifies the mappings between those six relational data-base table schemas and the output RDF graphs [17]. Adetailed description of this approach is described in ourprevious work [18]. These six tables are used as prelim-inary association data resources including all uniqueassociations from Semantic MEDLINE.Data preprocessing using FDA-approved drugs in DrugBankSince the extraction accuracy of associations in SemanticMEDLINE is about 77% (precision is 76% to 96%, andrecall is 55-70%) [19], a filtering strategy is applied toextract high-confidence association data using the FDA-approved drug list from DrugBank, a database contain-ing drug information and the corresponding drug targetand treatment indication information [20]. As of July 312012, the database contains 1,578 FDA-approved drugentries, including 131 FDA-approved biotech drugs, and1,447 FDA-approved small molecule drugs. We extract as-sociations involving these FDA-approved drugs from eachdrug-related association table. After manually removinggeneric and nonsensical terms in the association tables(e.g., gene, homologous gene, and protein), we limit thedrug-drug, drug-gene, and drug-disease associations ton c Medline of rela onsOp miza onAssoci on TableNM Detec onMs Integrated Drug-gene-disease NetworkConfidence filteringr an integrated drug-disease-gene network.Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 4 of 13http://www.jbiomedsem.com/content/5/1/33those involved in the 1,578 FDA-approved drugs. Basedon the filtered drug-gene and drug-disease associations,we generate related gene and disease lists and then ob-tained gene-gene, disease-disease, and gene-disease associa-tions using these genes and diseases. This filtering strategyenables us to focus on associations related to FDA-approved drugs only in this study. These associations arethen analyzed by the proposed network-based approach.Network motif analysisNetwork motifs are topologically distinct subnetworkpatterns that are present more frequently in true net-works than in random networks [21]. They are usuallywell conserved and possess specific processing tasks insame types of networks. For example, in gene regulatorynetworks, the same set of network motifs have beenrepeatly identified in diverse organisms from bacteria tohuman [22]. The hypothesis is that network motifs wereindependently selected by evolutionary processes in aconverging manner and have characteristic dynamicalfunctions [23]. This suggests that network motifs serveas building blocks of in gene regulatory networks thatare beneficial to the organism.In this study, we extend network motif analysis to thedisease-drug-gene network. Six different types of associ-ations among drugs, diseases, and genes are integratedinto a heterogeneous disease-drug-gene network. In thisnetwork, nodes represent biomedical entities stored inthe RDF triples (i.e., diseases, drugs, or genes in subjectand object), and edges represent associations betweentwo biomedical entities (i.e., relationships in predicate).For simplicity, we consider all associations as undirec-tional association relationships in this study, discardingthe directionality and types in the original RDF graph. Inother words, as long as there is an association betweentwo nodes, we consider there is an edge between thesetwo nodes. We hypothesize that even within such sim-plified disease-drug-gene association network, networkmotifs in the network can (1) represent basic inter-relationships among diseases, drugs, and genes; (2) re-flect a framework in which particular functions areachieved efficiently. Specifically, we focus on three-nodenetwork motifs in this disease-drug-gene network sincethey are the building blocks for larger size network motifs(number of nodes > 3) [24]. All connected subnetworkscontaining three nodes in the association network are col-lated into isomorphic patterns [25], and the frequency ofthe patterns are counted. We also generated 1000 randomnetworks from the original network by switching edgesbetween vertices and preserving the number of edges be-tween types of nodes (i.e., disease, drug and gene). By thedefault of FANOMD algorithm, if the number of occur-rences for each pattern is at least five in the real network,which is significantly higher than randomized networks,the pattern is considered to be a network motif. Statisticalsignificance test is performed by computing the fraction ofrandomized networks in which the pattern appears at leastas often as in the interaction network [24]. The z score iscalculated using the following equation:Z ¼ Nreal? Nrandh i?randð1Þwhere Nreal is the number of times one three-node sub-network is detected in the real network, Nrand is themean number of times this subnetwork is detected in1000 randomized networks, and ?rand is the standard devi-ation of the number of times this subnetwork is detectedin randomized networks. The p value of a motif is the num-ber of random networks in which it occurs more often thanin the original networks, divided by the total number ofrandom networks. A pattern with p ? 0.05 is consideredstatistically significant. This network motif discovery pro-cedure is performed using the FANMOD tool [26].Construction of the core drug-disease-gene networkIt has been shown that in gene regulatory networks, foreach network motif, the majority of matches overlap andaggregate into homologous motif clusters [27]. Many ofthese motif clusters largely overlap with modules ofknown biological processes [28]. The clusters of overlap-ping matches of these motifs aggregate into a superstruc-ture that presents the backbone of the network and isassumed to play a central role in defining the global topo-logical organization. Accordingly, we aggregate matches ofsignificant network motifs into a core drug-disease-genenetwork. In this core network, we investigate the distribu-tion of the connectivity degree of different types of nodes.Nodes with significantly larger number of links in thenetwork are called hub nodes, which is critical in theinformation flow exchange throughout the entire network.ResultsAn integrated drug-disease-gene network reconstructedfrom Semantic MEDLINEWe constructed a drug-disease-gene network with thefollowing two steps:First, we extracted unique association data fromSemantic MEDLINE. Using a use-case driven databaseoptimization approach developed in our previous work[18], we extracted six different types of associationsfrom Semantic MEDLINE database. Table 1 shows basicstatistics of these six groups of associations. As illustratedin Table 1, the number of unique associations (the UniqueAssociation column) for each type of associations issignificantly less than the number of total associations(the Record column). Since the prediction accuracy ofSemantic MEDLINE is approximately 77% [29], we used aTable 1 Statistics of the six extracted association typesAssociation type Record in Semantic MEDLINE Unique associations Associations involvingFDA-approved drugsUnique entity numberDisease-Disease 2,516,049 843,221 1684 2,248Disease-Gene 206,155 111,117 21,444 5,954Disease-Drug 3,021,256 1,277,879 54,996 3,414Drug-Gene 398,572 248,491 3758 1,451Drug-Drug 4,780,394 1,900,576 266 382Gene-Gene 108,035 49,593 2169 2,792Total 11,030,461 4,430,877 84,317 7,24311This is the unique number of entities by summarizing all the associations.Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 5 of 13http://www.jbiomedsem.com/content/5/1/33filtering strategy to focus on associations involving FDA-approved drugs for downstream network-based analysis.Second, we constructed association related data involvingFDA-approved drugs. We applied the filtering strategydiscribed in the Materials and methods section to extractassociation data involving FDA-approved drugs from theunique association data set. As shown in the AssociationsInvolving FDA-approved Drugs column in Table 1, theassociation number of each table was further reduced.We used this focused association data to construct anintegrated disease-drug-gene network for downstreamnetwork-based analysis.Network topology analysis of the core drug-disease-genenetworkThe network motif analysis was performed on the inte-grated disease-drug-gene network obtained in SectionFigure 2 Degree distribution of three biomedical entities: drug, geneAn integrated drug-disease-gene network reconstructedfrom Semantic MEDLINE. Since the network containsthousands of associations among 865 drugs, 2791 genes,and 3578 diseases (Table 1), it is too complex for a directvisualization. We overcame this problem by identifyingenriched network motifs and interpreting them throughan enhanced visualization. Out of this heterogeneousnetwork consisting of 84,317 associations among 7,234 en-tities (including drugs, diseases, and genes), five significantnetwork motifs were identified. Figure 2 presents de-tailed statistics on these network motifs. The matches ofthese network motifs were extracted and number ofmatches for each network motif was counted (Num-ber of Matches column in Figure 2).Based on the network motifs identified in the analysis,we constructed a core disease-drug-gene network aggre-gated from significant network motif instances. We then, and disease.Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 6 of 13http://www.jbiomedsem.com/content/5/1/33investigated the degree distribution of different types ofentities in the integrated network. Figure 3 representsthe degree distribution of disease, drug, and gene nodesin the core drug-disease-gene network. All three distri-butions follow the power-law distribution, indicatingthat networks related to different types of nodes arescale-free. The majority of the nodes in the networkhave only a few (less than 10) links but few othernodes have a large number of links. Such distributionshave been observed in many studies of biological(A(B)Figure 3 Subnetworks extracted from NM 1. (A) Overview of the subneassociated with Malignant neoplasm of prostate and tumor growth. (C)infection and multicentric Castleman's disease.networks [30]. Our analysis demonstrates for the firsttime that in an integrated network consisting of het-erogeneous associations, the scale-free network struc-ture still holds. The hub nodes (i..e, the nodes have a largenumber of links) can provide scientists future researchdirections.Local network structure: from network to network motifThe five significant network motif patterns in Figure 2have strong biological meanings and could suggest)(C)twork, consisting of 126 diseases and 79 genes. (B) SubnetworkSubnetwork associated with communicable diseases, West Nile viralZhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 7 of 13http://www.jbiomedsem.com/content/5/1/33scientists future directions in their research field. Weprovided three case studies in the following sections toillustrate results based on three significant networkmotifs.Case study 1 - prioritization of disease genesWe first investigated whether the network motif analysiscould help prioritize disease genes based on the associa-tions between diseases and their surrounding genes. Oneexample is Network Motif 1 (NM 1) in Figure 2, inwhich two diseases that are associated with each otherare also associated with one common disease gene. Thisindicates that diseases identified to be associated in lite-rature are more likely to share same associated diseasegenes. To further investigate the relationships highlightedby NM 1, We extracted all associations relationshipsamong 126 diseases and 79 genes in NM 1. In total, thereare 71 disease-disease, 853 disease-gene, and 3 gene-geneassociations (Figure 4(A)) in this subnetwork, suggestingthat diseases that are associated with each other are morelikely to associate with a group of common disease genes.For instance in Figure 4(B), Malignant neoplasm of pros-tate shares all 35 associated genes with tumor growth.Similar findings have also been discovered in other studiesdemonstrating same functional modules/pathways beingaffected in similar diseases [6,31,32]. There are 10 genesonly associated to tumor growth in literature. Such in-formation will help scientists generate testable hypothesesof possible roles of these genes in prostate cancer research.Another example is shown in Figure 4(C), where commu-nicable diseases was identified to have common associ-ated genes with both West Nile viral infection andmulticentric Castlemans disease. Thirteen genes associ-ated only with communicable diseases can be consideredas candidate disease genes for the other two diseases andhelp scientists design future exploratory experiments. Thedetailed network information is presented in Additionalfile 1: File S1.Case study 2 - inference of disease relationshipsVery interestingly, we also identified another similardisease-gene network motif in our analysis (NM 4). Theonly difference between NM 1 and NM 4 is that NM 4doesnt have the associations between two diseasesthemselves. We extracted all associations among 2,664diseases and 1,122 genes in NM 4. In total, there are 860disease-disease, 17,242 disease-gene, and 310 gene-geneassociations in this subnetwork (Figure 5(A)). Based onthe guilt by association rule  diseases similar to eachother are more likely to be affected by the same genes/pathways, two diseases involved in the same NM 4 aremore likely to be similar/associated than other diseases[6]. For instance in Figure 5(B), Kidney Failure andskin disorder are associated with a group of fivecommon associated genes. A wide variety of differentskin disorders have been observed in patients with kid-ney diseases [33]. One example is the psoriasis disease.During the treatment of psoriasis with fumaric acidderivatives, patients could develop acute kidney failure[34]. In the subnetwork that consists of first neighborsof these two diseases, psoriasis is also included and hascommon associated genes with both kidney failure andskin disorder. Some genes in the network are associ-ated with one of these diseases only but not both. Toinvestigate enriched biological functions/processes, weperformed functional enrichment analysis on neighborgenes of three diseases with Ingenuity Pathway Analysis(IPA) Suite (http://www.ingenuity.com/). These genes areenriched in kidney-related disease categories (Table 2).Although a major portion of neighbor genes are relatedto skin disorder or psoriasis only, they have been an-notated with kidney related dysfunctions in the IPAdatabase. Given the fact that associations among thou-sands of diseases are complex yet incomplete, the in-ferred association relationships based on our networkmotif-based analysis can mine the significant networktopology properties of association networks and guidescientists to investigate significant association relation-ships in future experiments. The detailed network infor-mation is presented in Additional file 2: File S2.Case study 3  Drug repositioningNetwork Motif 2 (NM 2) suggests another associationpattern between diseases and drugs, in which two dis-eases associated with each other are targets for the samedrug. It has been shown by Suthram et al. [7] that diseaseswith significant correlations based on mRNA gene expres-sion data also share common drugs. This NM supportsthe hypothesis that similar diseases can be treated by samedrugs, allowing us to make hypotheses for drugs reposi-tioning purpose. We extracted all associations among 468disease and 162 drugs in NM 2. In total, there are 279disease-disease, 8,730 disease-drug, and 14 drug-drugassociations in this subnetwork (Figure 6(A)). We furtherinvestigated whether any drugs or diseases were hubnodes in this subnetwork. In Figure 6(B), AlzheimersDisease and nervous systems disorder are hub diseasessurrounded by 51 FDA-approved drugs. Both diseases areassociated with 20 common drugs, while nervous systemsdisorder has associations with additional 31 drugs. Thesedrugs can be considered repositioned for treatment ofAlzheimers Disease since it is a central nervous systemdisorder characterized by the presence of neurofibrillarytangles, neuritic plaques and dystrophic neurites in thebrain [35]. In Figure 6(C), we observed two hub drugssurrounding by 129 diseases, 16 of which have associa-tions with both drugs. Dobutamine is a sympathomimeticdrug used in the treatment of heart failure and cardiogenic(A)(B)Figure 4 Subnetworks extracted from NM 4. (A) Overview of the subnetwork, consisting of 2,664 diseases and 1,122 genes. (B) Subnetworkassociated with Kidney Failure and skin disorder.Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 8 of 13http://www.jbiomedsem.com/content/5/1/33(A)(B) (C)Figure 5 Subnetworks extracted from NM 2. (A) Overview of the subnetwork, consisting of 468 disease and 162 drugs. (B) Subnetworkassociated with Alzheimers Disease and nervous systems disorder. (C) Subnetwork associated with Dobutamine and Doxorubicin.Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 9 of 13http://www.jbiomedsem.com/content/5/1/33shock. Doxorubicin is a drug used in cancer chemother-apy. Chemotherapy side effects may increase the risk ofheart disease in cancer patients [36]. This series of under-lying connections can provide clinicians potential sideeffects related to certain drug treatment. This could takeyears to study in the clinic to identify such side effects.The results derived from our approach can serve asin silico exploratory analysis to guide such studies. Thedetialed network information is presented in Additionalfile 3: File S3.Three-gene network motif (NM 3) was also identi-fied in this heterogeneous network. This NM is a verycommon motif pattern in the protein-protein inter-action network or gene regulatory network [37,38], in-dicating that NM detection analysis of heterogeneousnetworks can identify significant NMs even enrichedTable 2 Enriched disease and disorder categories in IPA analysisCategory p-value MoleculesRenal Inflammation 6.62E-09 VEGFA,COL4A5,CD40LG,APCS,IL1RN,CLU,MYH9,COL4A4,VDR,ACTN4,NFKB1,TNF,FASRenal Nephritis 6.62E-09 VEGFA,COL4A5,CD40LG,APCS,IL1RN,CLU,MYH9,COL4A4,VDR,ACTN4,NFKB1,TNF,FASCongenital Heart Anomaly 3.41E-06 VEGFA,HSPG2,TRIM21,EDNRA,ECE1Liver Cirrhosis 4.13E-06 ADAM17,CD40LG,C5AR1,EDNRB,BSG,PTAFR,TNF,CCR7Glomerular Injury 5.22E-06 VEGFA,CLU,MYH9,ACTN4Cardiac Infarction 6.38E-06 PON1,BCL2L1,CD40LG,IL1RN,HSPA1A/HSPA1B,CLU,TNNI3,TNF,LRP1Renal Atrophy 7.66E-06 CD40LG,EDNRB,FGF23,EDNRA,VDR,AQP2Liver Damage 9.94E-06 BCL2L1,NLRP3,BSG,IL1RN,NFKB1,TNF,FASLiver Proliferation 1.75E-05 VEGFA,SOCS3,EDNRB,IL1RN,EDNRA,NFKB1,TNF,FASPulmonary Hypertension 3.13E-05 EDNRB,IL1RN,KIT,EDNRALiver Hepatitis 4.73E-05 BCL2L1,IL23A,TNF,CCR7,FASLiver Necrosis/Cell Death 6.57E-05 SOCS3,BCL2L1,CD40LG,IL1RN,HSPD1,NFKB1,TNF,FASCardiac Inflammation 6.64E-05 IL33,CLU,TNNI3,IL23A,NFKB1,TNFHeart Failure 6.76E-05 BCL2L1,CA2,TNNI3,VDR,NFKB1,TNF,AQP2,PRKCAHepatocellular Carcinoma 6.87E-05 VEGFA,CA2,BCL2L1,SOCS3,ADAM17,BSG,KEAP1,CLU,IGFBP3,S100A4,KIT,MKI67,TNFLiver Hyperplasia/Hyperproliferation 6.87E-05 VEGFA,CA2,BCL2L1,SOCS3,ADAM17,BSG,KEAP1,CLU,IGFBP3,S100A4,KIT,MKI67,TNFRenal Dysfunction 2.46E-04 BSG,FGF23,TNFCardiac Necrosis/Cell Death 3.17E-04 VEGFA,SOCS3,BCL2L1,S100B,HSPD1,TNF,LRP1,NAD+Cardiac Hypertrophy 5.67E-04 IL33,ADAM17,S100A6,HSPA1A/HSPA1B,FGF23,EDNRA,DMD,VDR,NFKB1,TNF,PRKCARenal Necrosis/Cell Death 5.83E-04 BCL2L1,HSPA1A/HSPA1B,IGFBP3,CLU,PAX2,NFKB1,TNF,FAS,PRKCALiver Inflammation 8.62E-04 IL1RN,FOXP3,NFKB1,TNF,FASKidney Failure 1.37E-03 VEGFA,SLC9A3,PKD2,MYH9,VDR,TNF,AQP2Cardiac Proliferation 1.66E-03 ADAM17,KIT,TNF,PRKCARenal Dilation 1.67E-03 EDNRB,EDNRA,AQP2Nephrosis 2.35E-03 CLU,ACTN4Liver Fibrosis 2.48E-03 VEGFA,SOCS3,EDNRB,PKD2,EDNRA,NFKB1,TNF,CCR7Renal Proliferation 2.48E-03 SOCS3,HSPG2,TJP1,HSPD1,TNF,CCR7Increased Levels of AST 3.13E-03 TNF,FASCardiac Fibrosis 4.69E-03 TNNI3,DMD,VDR,NFKB1,TNF,DIO3Increased Levels of Albumin 5.63E-03 VEGFALiver Regeneration 5.85E-03 SOCS3,IL1RN,TNFZhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 10 of 13http://www.jbiomedsem.com/content/5/1/33in a single type of associations in a heterogeneousassociation network.Comparisons of network motifs from different networksSince all five network motifs identified involve only twoout of three node types, we further investigated whetherthe networks involving only two node types can generatethe same NMs. To accomplish that, we performed NManalysis on disease-gene, disease-drug and gene networksrespectively. Not all NMs detected in the complete net-work can be detected in disease-gene, disease-drug andgene networks respectively (Additional file 4: File S4). Theresults indicate that although the NMs dont contain allthree different node types due to small NM size, theadditional associations still introduce additional informa-tion in the NM detection analysis.DiscussionLiterature mining approaches have been successful toextract associations among biological entities in the lastdecade. However, such information is usually large, com-plex and multidimentional, making it impossible for bio-medical researchers to directly investigate such data. Toleverage the gap between knowledge needs of translationalresearchers and existing knowledge discovery services, wehave proposed a network-based informatics approach toinvestigate the underlying relationships among differentbiological entities based on associations automaticallyID Network MotifFrequency[Original] Mean-Freq[Random] Standard-Dev[Random] Z-Score p-Value Number of MatchesNumber of Entities1 0.0096% 0.0035% 9.2982e-006 6.5 < 0.001 131126 diseases and 79 genes2 0.038% 0.025% 2.3404e-005 5.4 <0.001 522468 disease and 162 drugs3 0.0075% 0.0055% 7.5124e-006 2.7 0.008 103286 genes4 5.9% 5.1% 0.004215 2.2 0.026 811052664 diseases and 1122 genes5 0.032% 0.024% 4.1072e-005 2.0 0.032 437432 disease and 148 drugsFigure 6 Statistics of significant network motifs. Node color: black  drug, green  disease, red  gene. Edge color denotes the associationsbetween different biomedical entities: black  association between disease and disease, yellow - association between disease and gene, green -association between disease and drug, red - association between gene and gene.Zhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 11 of 13http://www.jbiomedsem.com/content/5/1/33extracted from literature. The proposed approach hasadvantages in several aspects.Our approach is one of the first attempts to investigatethe disease-drug-gene associations in an integrative man-ner. To demonstrate the superiority of NM analysis on theheterogeneous network, we performed NM analysis ondisease-gene, disease-drug and gene networks respectivelyand compared results with the ones derived from thecomplete disease-drug-gene network. Not all network mo-tifs detected in the complete network can be detected indisease-gene, disease-drug and gene networks respectively.The results indicates that although NMs doesnt contain allthree different node types due to their small size in thisstudy, the additional associations still introduce additionalinformation in the analysis. In addition, NM analysis ofsuch heterogeneous networks can extract and highlight thehotspots in the network, leading experts in different fieldsto generate testable hypotheses in their future research.We are aware that there are many other network ana-lysis approaches for both social networks and biologicalnetworks. These approaches are designed for differentpurposes. For instance, biological networks can be inter-rogated by their overall properties (e.g., average cluster-ing coefficient and overall distributions of node degrees),significant NMs, or clustered subnetworks/modules. Inthis work, we focus on identifying statistically significantthree-node NM patterns that can help infer novel disease-drug-gene relationships. The NM analysis can decomposethe whole heterogeneous network into smallest networkpatterns that recurrently discovered in the network, con-sidered as the backbone associations of diseases, drugs,and genes. For instance, in NM 1 instances in Figure 2,most of these NMs contain the first two same diseases,while the third gene is different. By extracting all the asso-ciations involving these two diseases from the original as-sociation network, we found that while these two diseasesshare a significant number of associated genes, they alsohave some unique associations with other genes respect-ively. Based on the assumption that similar diseases aremore likely to associate with same group (s) of genes orinvolve same biological processes, the genes associatedonly with one disease can be prioritized as candidateZhang et al. Journal of Biomedical Semantics 2014, 5:33 Page 12 of 13http://www.jbiomedsem.com/content/5/1/33disease genes of the second disease. Such inference couldonly be possible through NM level analysis by consideringsignificant network patterns (i.e., NMs) as well as theirneighborhood in the whole network. In addition, sincethese NMs are statistically significant subnetworks, theyrepresent the real signal from the network which usuallycontains considerable amount of false positive associa-tions, especially those from literature mining techniques.Due to the limitation of computational resource, we didntinclude the NMs with more than three nodes. We plan toextend our work to NMs with more nodes (i.e., >3) whenthe computational resource become available. We believethat the proposed network-based approach can comple-ment other existing network analysis methods and provideresearchers a unique way to look at these huge heteroge-neous networks.From our preliminary study [18], we found that SemanticMEDLINE lacks of gene-gene associations since suchinformation usually are illustrated in the main text ofliterature. Semantic MEDLINE contains gene-gene inter-action data from PubMed literature abstracts (Figure 2).We included all the associations in Figure 2 in our analysis.However, the number of gene-gene association in SemanticMEDLINE (2,169 high-confidence pairs) is relevantly smallcomparing to other public databases (e.g., HPRD [4]). Forinstance, we compared the gene-gene associations inSemantic MEDLINE with those in HPRD, a manuallycurated gene-gene association database in human [4].The overlap between these two databases is very small(about 10% associations of Semantic MEDLINE can befound in HPRD). HPRD contains many more associa-tions than Semantic MEDLINE (41,327 versus 2,169).Therefore, we believe that combining Semantic MED-LINE with other public resources (such as HPRD [39]and STRING [40]) will increase the coverage of asso-ciations and build a more comprehensive associationdatabase. Using linked data approach, it will be relativelyeasier to link our data graph with such databases.Conclusions and future workIn this paper, we proposed a network-based computa-tional framework to investigate integrated heterogeneousnetwork extracted from MEDLINE literature, includingassociations among three major entity categories: drug,gene, and disease. Five significant NMs were identifiedand considered as the backbone of the entire network.The potential biological meanings of each network motifwere further investigated. The results demonstrated thatthe proposed approach holds the potential to 1) prioritizecandidate disease genes, 2) identify potential disease rela-tionships, and 3) propose novel drug targets, within thecontext of the entire knowledge. We believe that suchanalyses can facilitate the process of inferring novel rela-tionships between drugs, genes, and diseases. One futuredirection is to develop module-based approaches tounderstand associations between different biomedicalentities. Modules are condensed subnetworks in a net-work. Modules identified in heterogeneous networksare a group of related diseases, drugs and genes, whichgives researchers a focused network view of the associ-ation relationships among these entities. Topology ana-lysis of heterogeneous networks using graphic theorycan also be applied in future studies, which can lead tothe identification of diseases/drugs/genes in the contextof association networks. Pathway level informationcould also be integrated in future analyses to extendcurrent association network.Additional filesAdditional file 1: Detailed network information derived from NM 1(Figure 4).Additional file 2: Detailed network information derived from NM 4(Figure 5).Additional file 3: Detailed network information derived from NM 2(Figure 6).Additional file 4: NM analysis of drug-gene, disease-drug, andgene-drug networks.Competing interestsThe authors declare that they have no competing interests.Authors contributionsYZ and CT led the study design and analysis, and drafted the manuscript. GJcontributed to the manuscript preparation and use case discussions. JS andAAN contributed to the association data extraction and reorganization, CGCand HL participated the design, provided support and manuscript editing. Allauthors read and approved the final manuscript.AcknowledgementsThis project was supported by the National Institute Health grant P30 CA134274-04 to the University of Maryland Baltimore, the National ScienceFoundation award 0937060 and the National Center for BiomedicalOntologies (NCBO) to C.T., and the National Institute of Health grantR01LM009959 and National Science Foundation award 0845523 to H.L.Author details1Division of Biostatistics and Bioinformatics, University of MarylandGreenebaum Cancer Center and Department of Epidemiology and PublicHealth, University of Maryland School of Medicine, Baltimore, MD, USA.2School of Biomedical Informatics, University of Texas Health Science Centerat Houston, Houston TX, USA. 3Division of Biomedical Statistics andInformatics, Department of Health Sciences Research, Mayo Clinic, RochesterMN, USA. 4Radiology Informatics Laboratory, Department of Radiology, MayoClinic, Rochester, MN, USA.Received: 2 February 2013 Accepted: 2 July 2014Published: 6 August 2014JOURNAL OFBIOMEDICAL SEMANTICSMagka et al. Journal of Biomedical Semantics 2014, 5:17http://www.jbiomedsem.com/content/5/1/17RESEARCH Open AccessA rule-based ontological framework for theclassification of moleculesDespoina Magka1*, Markus Krötzsch2 and Ian Horrocks1AbstractBackground: A variety of key activities within life sciences research involves integrating and intelligently managinglarge amounts of biochemical information. Semantic technologies provide an intuitive way to organise and siftthrough these rapidly growing datasets via the design and maintenance of ontology-supported knowledge bases. Tothis end, OWLa W3C standard declarative language has been extensively used in the deployment of biochemicalontologies that can be conveniently organised using the classification facilities of OWL-based tools. One of the mostestablished ontologies for the chemical domain is ChEBI, an open-access dictionary of molecular entities that supplieshigh quality annotation and taxonomical information for biologically relevant compounds. However, ChEBI is beingmanually expanded which hinders its potential to grow due to the limited availability of human resources.Results: In this work, we describe a prototype that performs automatic classification of chemical compounds. Thesoftware we present implements a sound and complete reasoning procedure of a formalism that extends datalogand builds upon an off-the-shelf deductive database system. We capture a wide range of chemical classes that are notexpressible with OWL-based formalisms such as cyclic molecules, saturated molecules and alkanes. Furthermore, wedescribe a surface less-logician-like syntax that allows application experts to create ontological descriptions ofcomplex biochemical objects without prior knowledge of logic. In terms of performance, a noticeable improvement isobserved in comparison with previous approaches. Our evaluation has discovered subsumptions that are missingfrom the manually curated ChEBI ontology as well as discrepancies with respect to existing subclass relations. Weillustrate thus the potential of an ontology language suitable for the life sciences domain that exhibits a favourablebalance between expressive power and practical feasibility.Conclusions: Our proposed methodology can form the basis of an ontology-mediated application to assistbiocurators in the production of complete and error-free taxonomies. Moreover, such a tool could contribute to amorerapid development of the ChEBI ontology and to the efforts of the ChEBI team to make annotated chemical datasetsavailable to the public. From a modelling point of view, our approach could stimulate the adoption of a different andexpressive reasoning paradigm based on rules for which state-of-the-art and highly optimised reasoners are available;it could thus pave the way for the representation of a broader spectrum of life sciences and biomedical knowledge.Keywords: Semantic technologies, Knowledge representation and reasoning, Logic programming and answer setprogramming, Datalog extensions, CheminformaticsBackgroundLife sciences data generated by research laboratoriesworldwide is increasing at an astonishing rate turningthe need to adequately catalogue, represent and indexthe rapidly accumulating bioinformatics resources into apressing challenge. Semantic technologies have achieved*Correspondence: magkades@gmail.com1Department of Computer Science, University of Oxford, Oxford, UKFull list of author information is available at the end of the articlesignificant progress towards the federation of biochemicalinformation via the definition and use of domain vocab-ularies with formal semantics, also known as ontologies[1-3]. OWL [4], a family of logic-based knowledge repre-sentation (KR) formalisms standardised by the W3C, hasplayed a pivotal role in the advent of Semantic technolo-gies. This is to a great extent thanks to the availabilityof robust OWL-based tools that are capable of derivingknowledge that is not explicitly stated by means of logicalinference. In particular, OWL bio- and chemo-ontologies© 2014 Magka et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly cited.Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 2 of 15http://www.jbiomedsem.com/content/5/1/17with their intuitive hierarchical structure and their formalsemantics are widely used for the building of life sciencesterminologies [5,6].Taxonomies provide a compelling way of aggregat-ing information, as hierarchically organised knowledgeis more accessible to humans. This is evidenced, e.g. bythe pervasive use of the periodic table in chemistry, oneof the longest-standing and most widely adopted classi-fication schemes in natural sciences. Organising a largenumber of different objects into meaningful groups facil-itates the discovery of significant properties pertaining tothat group; these discoveries can then be used to predictfeatures of subsequently detected members of the group.For instance, esters with low molecular weight tend to bemore volatile and, so, a newly found ester with low weightis expected to be highly volatile, too. As a consequence,classifying objects on the basis of shared characteristicsis a central task in areas such as biology and chem-istry with a long tradition of taxonomy use. Due to theavailability of performant OWL reasoners, life scientistscan employ OWL to represent expert human knowledgeand thus drive fast, automatic and repeatable classifica-tion processes that produce high quality hierarchies [7,8].Nevertheless, a prerequisite is that OWL is expressiveenough to model the entities that need to be classified aswell as the properties of the superclasses that lie higher upin the hierarchy.Two main restrictions have been identified in theexpressive power of OWL as hindering factors for the rep-resentation of biological knowledge [9,10]. First, due tothe tree-model property of OWL [11] (which otherwiseaccounts for the robust computational properties of thelanguage) one is not able to describe cyclic structures withadequate precision. Second, because of the open-worldassumption adopted in OWL (according to which missinginformation is treated as not known rather than false) it isdifficult to define classes based on the absence of certaincharacteristics. These limitations manifest themselvesamong othersvia the inability to define a broad range ofclasses in the chemical domain. For instance, one cannoteffectively encode in OWL the class of compounds thatcontain a benzene ring or the class of molecules that donot contain carbon atoms, i.e. inorganic molecules.These inadequacies obstruct the full automation ofthe classification process for chemical ontologies, suchas the ChEBI (Chemical Entities of Biological Interest)ontology, an open-access dictionary of molecular entitiesthat provides high quality annotation and taxonomicalinformation for chemical compounds [6]. ChEBI fostersinteroperability between researchers by acting as the pri-mary chemical annotation resource for various biologicaldatabases such as BioModels [12], Reactome [13] and theGene Ontology [5]. Moreover, ChEBI supports numer-ous tasks of biochemical knowledge discovery such asthe study of metabolic networks, identification of dis-ease pathways and pharmaceutical design [14,15]. ChEBIis manually curated by human experts who annotateand check the validity of existing and new molecularentries. Currently, ChEBI describes 36,660 fully annotatedentities (release 110) and grows at a rate of approxi-mately 4,500 entities per year (estimate based on previousreleases [16]). Given the size of other publicly availablechemical databases, such as PubChem [17] that containsrecords for 19 million molecules, there is clearly a strongpotential for ChEBI to expand by speeding up curat-ing tasks. ChEBI curating tasks span a wide range ofactivities such as adding natural language definitions andstructure information or classifying chemical entities bydetermining their position in the ChEBI taxonomy. Thusautomating chemical classification could free up humanresources and accelerate the addition of new entries toChEBI.As the classification of compounds is a key task of thedrug development process [18], the construction of chem-ical hierarchies has been the topic of various investigationscapitalising on logic-based KR [19-23], statistical machinelearning (ML) [24-26] and algorithmic [27-29] techniques.In KR approaches, molecule and class descriptions arerepresented with logical axioms crafted by experts andsubsumptions are identified with the help of automatedreasoning algorithms; in ML approaches a set of anno-tated data is used to train a system and the system isthen employed to classify new entries. So, KR approachesare based on the explicit axiomatisation of knowledge,whereas ML algorithms specify for new entries super-classes that are highly probable to be correct. As a con-sequence, the taxonomies produced using logic-basedtechniques are provably correct (as long as the modellingof the domain knowledge is faithful), but the statisticallyproduced hierarchies (although much faster) need to beevaluated against a curated gold standard. Algorithmictechniques involve the definition of imperative pro-cedures for determining classes of molecules. Theseapproaches are usually much quicker than logic-basedtechniques but have the disadvantage of requiring aprogrammer for defining new classes or for modifyingthe existing ones, as opposed to ontological knowledgebases that can be manipulated and extended by non-programmers. Here, we focus on logic-based chemicalclassification, which in certain cases can complement sta-tistical and algorithmic approaches [8,15].In previous work, we laid the theoretical foundationof nonmonotonic existential rules which is an expressiveontology language that is sound and complete and thatis suitable for the representation of graph-shaped objects;additionally, we demonstrated how nonmonotonic exis-tential rules can be applied to the classification ofmolecules [9]. The aforementioned formalism addressedMagka et al. Journal of Biomedical Semantics 2014, 5:17 Page 3 of 15http://www.jbiomedsem.com/content/5/1/17the expressivity limitations outlined above; however, theperformance of the implementationalthough faster thanprevious approacheswas not satisfactory (more than 7minutes were needed to classify 70 molecules under 5chemical classes on a standard desktop computer) failingthus to confirm practicability of the formalism.In the current work, we describe an improved prac-tical framework that relies on the same formalism butwith enhanced performance. Our contributions can besummarised as follows:1. We present a prototype that performs logic-basedchemical classification based on a sound, completeand terminating reasoning algorithm; we modelmore than 50 chemical classes and we show that thesuperclasses of 500 molecules are computed in 33seconds.2. We harness the expressive power of nonmonotonicexistential rules to axiomatise a variety of chemicalclasses such as classes based on the containment offunctional groups (e.g. esters) and on the exactcardinality of parts (e.g. dicarboxylic acids), classesdepending on the overall atomic constitution (e.g.hydrocarbons) and cyclicity-related classes (e.g.compounds containing a cycle of arbitrary length oralkanes).3. We present a surface syntax that enables applicationexperts to create ontological description of chemicalentities without prior knowledge of logic. The syntaxwe propose is closer to natural language than tofirst-order logic notation and is uniquely translatableto logical axioms.4. We exhibit a significant speedup in comparison withprevious ontology-based chemical classificationimplementations.5. We identify examples of missing and contradictorysubsumptions from the expert curated ChEBIontology that are present and absent, respectively,from the hierarchy computed by our prototype.Concerning future benefits, our prototype could formthe basis of an ontology-mediated application to assistbiocurators of ChEBI towards the sanitisation and theenrichment of the existing chemical taxonomy. Automat-ing the maintenance and expansion of ChEBI taxonomycould contribute to a more rapid development of theChEBI ontology and to the efforts of the ChEBI team tomake annotated chemical datasets available to the pub-lic. From a modelling point of view, our approach couldstimulate the adoption of a different and expressive rea-soning paradigm based on rules for which state-of-the-artand highly optimised reasoners are available; it could thuspave the way for the representation of a broader spectrumof life sciences knowledge.MethodsKnowledge base designThe reasoning task carried out using our methodology isthe identification of chemical classes for molecules, e.g.assigning water to the class of inorganic molecules or ben-zene to cyclic molecules. In this section we provide a high-level description of the knowledge base (KB) we built forthe purposes of our chemical classification experiments.We use the word classification to refer to the detectionof subsumptions between molecules and chemical classesrather than to the computation of the partial order forthe set comprising the chemical classes and moleculesw.r.t. the subclass relation. The KB consists of nonmono-tonic existential rules that formally describe molecularstructures and chemical classes; this representation cansubsequently be used to determine the chemical class sub-sumers of eachmolecule. For a formal definition of syntaxand semantics of nonmonotonic existential rules as wellas decidability proofs, we refer the interested reader to therelevant articles [9,30,31].For each chemical entity that we model using rules, wealso provide its axiomatisation in the surface syntaxa less-logician-like syntax which we designed and whichenables the ontological description of structured objectswithout the use of logic. Our surface syntax is in the samestyle of theManchester OWL syntax [32] and draws inspi-ration from a syntax suggested for OWL 2 rules [33].The main motivation for designing this syntax is to pro-vide a means for creating ontological descriptions in amore succinct way and without the use of special sym-bols. We have formally defined the surface syntax andits translation into nonmonotonic existential rules, butwe have not implemented an ontology editor that wouldallow to write axioms in the new syntax. Similarly, wehave not conducted experiments evaluating the use ofsurface syntax by application experts, but given that theManchester OWL syntax has been well received by non-logicians [32] and there is active development of toolsfor supporting more human readable ontology query lan-guages [34], we believe that the suggested syntax has thepotential to facilitate curating tasks. Since our main focusis to illustrate the transformation of molecular graphs andchemical class definitions into rules, we omit the technicaldetails and describe our methodology by means of run-ning examples. For a complete specification of the surfacesyntax including a BNF grammar and mappings to non-monotonic existential rules we provide an online technicalreport [35].Molecular structuresNext, we describe how a molfile can be converted into asurface syntax axiom and subsequently a rule that encodesits structure. We use as an example the molecule ofascorbic acid, a naturally occurring organic compoundMagka et al. Journal of Biomedical Semantics 2014, 5:17 Page 4 of 15http://www.jbiomedsem.com/content/5/1/17hasAtomsingledoublemoleculeascorbicAcid:oo ococcoccochFigure 1 Ascorbic acid representations.Molfile (left), molecular graph (top right) and description graph (bottom right) encoding the molecularstructure of ascorbic acid.commonly known as vitamin C. The molecular graph ofascorbic acid is depicted in the upper right corner ofFigure 1.Conceptually, the structure of ascorbic acid can beabstracted with the help of a directed labeled graph suchas the one that appears in the lower right corner ofFigure 1 and which in our framework is called descrip-tion graph (DG) [9]. The description graph of a moleculeis a labeled graph whose nodes correspond to the atomsof the molecule (nodes 113 for ascorbic acid) plus anextra node for the molecule itself (node 0) and whoseedges correspond to the bonds of the molecule (e.g. (1,7))plus some additional edges that connect the moleculenode with each one of the atom nodes (e.g. (0,1)); addi-tionally, the atom nodes are labeled with the respectivechemical elements (e.g. o for node 1) and the bond edgeswith the corresponding bond order (e.g. single for (1,7));finally, the molecule node is labeled with molecule andthe edges that connect the molecule node with each of theatom nodes are labeled with hasAtom. In order to sim-plify the depiction of the ascorbic acid DG in Figure 1a legend is used for the edge labels; all arrowless edgesare assumed to be bidirectional. In our setting, we fol-low the implicit hydrogen assumption according to whichhydrogen atoms are usually suppressed (excluding caseswhere stereochemical information is provided for theformed bond and hydrogens are explicitly stated as innode 13). Finally, we point out that both the nodes andthe edges can have multiple labels, allowing us to alsoencode molecular properties, such as charge values foratoms. The description graph of ascorbic acid can beconverted into the following surface syntax definition. Inthe rest of the text we use alphanumeric strings start-ing with a lower-case letter to denote predicates, thatis names of classes (e.g. ascorbicAcid) and properties(e.g. hasAtom).ascorbicAcidSubClassOfmolecule AND (hasAtom SOME Graph(Nodes(1 o, 2 o, 3 o, 4 o, 5 o, 6 o, 7 c, 8 c, 9 c,10 c, 11 c, 12 c, 13 h)Edges(1 2 single, 1 10 single, 2 7 double, 3 8 single4 9 single, 5 12 single, 6 11 single, 7 1 single8 7 single, 9 8 double, 10 9 single, 11 10 single12 11 single, 13 10 single)))Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 5 of 15http://www.jbiomedsem.com/content/5/1/17The surface syntax axiom above can next be trans-lated into the rule below. In fact we need a separaterule for each conjunct in the head but we use just onerule here to simplify the presentation; for the sake ofbrevity only one direction of the bonds appear and weshorten an expression of the form ?C1 . . . ? Cn with?ni=1Ci:ascorbicAcid(x) ? molecule(x) ?13i=1 hasAtom(x, fi(x))?6i=1 o(fi(x)) ?12i=7 c(fi(x)) ? h(f13(x))? single(f8(x), f3(x)) ? single(f9(x),f4(x)) ?i=1,9,11,13 single(f10(x), fi(x))?i=5,11 single(f12(x), fi(x))?i=1,8single(f7(x), fi(x)) ? single(f11(x),f6(x)) ? double(f2(x), f7(x))?double(f8(x), f9(x))The rule above is a typical first-order implication witha single atomic formula in the body and a conjunction ofatomic formulae in the head. Informally, the rule ensuresthat every time that the ascorbic acid molecule instanti-ated, its structure is unfolded according to its specifiedDG. Thus, triggering of the rule implies that (i) new termsthat correspond to the DGs nodes are generated (exclud-ing node 0), e.g. f1(x) represents atom node 1 (ii) eachnew term is typed according to the label of the rele-vant node with the help of a unary atomic formula (e.g.o(f1(x))) and (iii) each pair of terms with correspond-ing nodes connected in the DG is assigned the respec-tive label with the help of a binary atomic formula (e.g.single(f1(x), f7(x))). In order to ensure disjointness of theseveral molecular structures on the interpretation level,distinct function symbols are used in the rule of eachmolecule.General chemical knowledge and chemical classesBefore presenting the modelling of various chemicalclasses, we demonstrate how we can encode backgroundchemical knowledge with surface syntax axioms that cansubsequently be mapped to rules. Three such axiomsappear next.bond SuperPropertyOfsingle OR double OR triplecharged SuperClassOfpositive ORnegativehorc SuperClassOfh OR cExamples of such knowledge include the fact thatsingle and double bonds are kinds of bonds or thatatoms with positive or negative charge are charged; wecan also denote a particular class of atoms, e.g. atomsthat are hydrogens or carbons. The translation of theabove mentioned surface syntax axioms into rules appearsbelow.single(x, y) ? bond(x, y) negative(x) ? charged(x) h(x) ? horc(x)double(x, y) ? bond(x, y) positive(x) ? charged(x) c(x) ? horc(x)triple(x, y) ? bond(x, y)For our experiments, we represented 51 chemicalclasses using rules; we based our chemical modelling onthe textual definitions found in the ChEBI ontology [16].We covered a diverse range of classes that can be cate-gorised into four groups. For each class that we discuss, weprovide the surface syntax definition and its correspond-ing translation into one or more rules. Certain classeswith an intricate definition (such as the class of cyclicmolecules that appears later) are not expressible in sur-face syntax; these can be directly added as rules. Herewe show in full detail only a sample of the rules; thecomplete set of rules is available in Additional files 1, 2and 3 [36].Existence of subcomponents The great majority of themodelled chemical classes is defined via containment ofatoms, functional groups or other atom arrangements.Examples of this type include carbon molecular enti-ties, halogens, molecules that contain a benzene ring,carboxylic acids, carboxylic esters, polyatomic entities,amines, aldehydes and ketones. Next we show the surfacesyntax axioms that define the classes of carbon molecularentities, polyatomic entities, carboxylic acids and esters.In the following axioms we use the keyword GraphNLin contrast to the previously used Graph as our surfacesyntax grammar requires the use of the former when spec-ifying nodes that are either labeled with negative literalsor are specified to be disjoint.carbonEntity SuperClassOfhasAtom SOME cpolyatomicEntity SuperClassOfmolecule AND (hasAtom SOME GraphNL(DisjointNodes(1, 2)Edges()))heteroOrganicEntity SuperClassOfhasAtom SOMEGraphNL(Nodes (1c, 2NOT c NOT h)Edges (1 2 bond))Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 6 of 15http://www.jbiomedsem.com/content/5/1/17middleOxygenSuperClassOfo AND(bondSOME GraphNL(DisjointNodes (1, 2)Edges()))carboxylicAcidSuperClassOfmolecule AND (hasAtom SOME GraphNL (Nodes (1 c, 2 o, 3 o NOT middleOxygen NOT charged,4 horc)Edges (1 2 double, 1 3 single, 1 4 single)))carboxylicEsterSuperClassOfmolecule AND (hasAtom SOME Graph (Nodes (1 c, 2 o, 3 o, 4 c, 5 horc)Edges (1 2 double, 1 3 single, 1 5 single, 3 4 single)))One can find below the corresponding translationsinto rules. We define as carbon molecular entities themolecules that contain carbon; polyatomic entities arethe entities that contain at least two different atoms.Heteroorganic entities are the ones containing carbonatoms bonded to non-carbon atoms. Carboxylic acidsare defined as molecules containing at least one car-boxy group (a functional group with formula C(=O)OH)attached to a carbon or hydrogen; due to the implicithydrogens assumption we are not able to distinguishbetween an oxygen and a hydroxy group and, so, we needto specify that the oxygen of the hydroxy group is notcharged (NOT charged) and participates to only one bond( NOT middleOxygen). Similarly, carboxylic esters con-tain a carbonyl group connected to an oxygen ((C=O)O)which is further attached to two atoms that are carbon orhydrogen.Exact cardinality of parts Here we describe chemicalclasses of molecules with an exact number of atoms or offunctional groups. Examples include molecules that con-tain exactly two carbons, molecules that contain only oneatom and dicarboxylic acids, that is molecules with exactlytwo carboxy groups. The surface syntax axiom for thedefinition of molecules with exactly two carbons appearsnext.exactly2CarbonsSuperClassOfmolecule AND hasAtom EXACTLY 2 cThe translation into rules follows. One can read-ily verify that the surface syntax formulation is moredirect and intuitive than its equivalent translation intorules.molecule(x) ? hasAtom(x, y) ? c(y) ? carbonEntity(x)molecule(x) ? hasAtom(x, y1) ? hasAtom(x, y2) ? y1 = y2 ? polyatomicEntity(x)?2i=1hasAtom(x, zi) ? c(z1) ? notc(z2) ? noth(z2) ? bond(z1, z2) ? heteroOrganicEntity(x)?3i=1hasAtom(x, yi) ? o(y1) ?3i=2 bond(y1, yi) ? y2 = y3 ? middleOxygen(y1)molecule(x) ?4i=1 hasAtom(x, yi) ? c(y1) ? o(y2) ? o(y3) ?horc(y4) ? double(y1, y2) ? single(y1, y3) ? single(y1 , y4) ?notmiddleOxygen(y3) ? notcharged(y3) ? carboxylicAcid(x)molecule(x) ?5i=1 hasAtom(x, yi) ?i=1,4 c(yi) ?i=2,3 o(yi) ?horc(y5) ? double(y1, y2) ?i=3,5 single(y1 , yi) ? single(y3 , y4) ? carboxylicEster(x)Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 7 of 15http://www.jbiomedsem.com/content/5/1/17molecule(x) ?2i=1 hasAtom(x, yi) ? c(yi) ? y1 = y2 ? atLeast2Carbons(x)molecule(x) ?3i=1 hasAtom(x, yi) ? c(yi) ?3i=2 y1 = yi ? y2 = y3 ? atLeast3Carbons(x)atLeast2Carbons(x) ? not atLeast3Carbons(x) ? exactly2Carbons(x)Exclusive composition We next present classes ofmolecules such that each atom (or bond) they contain sat-isfies a particular property. These features are usually verynaturally modelled with the help of nonmonotonic nega-tion. Examples include inorganic molecules that consistexclusively of non-carbon atoms. In spite of the fact thatthere are many compounds with carbons considered inor-ganic, in this work we align our encoding with the ChEBIdefinition of inorganic molecular entities (CHEBI:24835),according to which no carbons occur in these entities;however, if the modeller wishes it, it is straightforward todeclare exceptions within our formalism using nonmono-tonic negation. Another example is the class of hydro-carbons which only contain hydrogens and carbons; alsosaturated compounds are defined as the compoundswhose carbon to carbon bonds are all single. The corre-sponding surface syntax axioms appear next.inorganicSuperClassOfmolecule AND hasAtom ONLY ( NOT c)hydroCarbon SuperClassOfcarbonEntity AND hasAtom ONLY (h OR c)unsaturatedSuperClassOfmolecule AND hasAtom SOME Graph ( Nodes (1 c, 2 c)Edges (1 2 double))unsaturatedSuperClassOfmolecule AND hasAtom SOME Graph (Nodes(1 c, 2 c)Edges (1 2 triple))saturatedSuperClassOfmolecule AND NOT unsaturatedPlease note that one can use more than one surface syn-tax axioms (and thus rules) to define classes that emerge asa result of different structural configurations, which is thecase for saturated molecules. Below we list the respectivetranslation into rules.molecule(x) ? notcarbonEntity(x) ? inorganic(x)hasAtom(x, z) ? notcarbon(z) ? nothydrogen(z) ? notHydroCarbon(x)carbonEntity(x) ? notnotHydroCarbon(x) ? hydroCarbon(x)molecule(x) ? hasAtom(x, z1) ? carbon(z1)hasAtom(x, z2) ? carbon(z2) ? double(z1, z2) ? unsaturated(x)molecule(x) ? hasAtom(x, z1) ? carbon(z1)hasAtom(x, z2) ? carbon(z2) ? triple(z1 , z2) ? unsaturated(x)molecule(x) ? not unsaturated(x) ? saturated(x)Cyclicity-related classes These chemical classes includethe category of molecules containing a ring of any lengthas well as other definitions that depend on the cyclicityof molecules, such as alkanes which are defined as satu-rated non-cyclic hydrocarbons. Assuming the (somewhatmore technical) definition of cyclic molecules, the surfacesyntax axiom for alkanes appears next.alkaneSuperClassOfsaturated AND hydroCarbon AND NOT cyclicThe corresponding rule translation follows.saturated(x) ? hydroCarbon(x) ? notcyclic(x) ? alkane(x)Determining subclass relationsFinally, we demonstrate how meaningful subsumptionscan be derived using a KB containing the rules outlinedin the previous two sections. In order to determine thesuperclasses of a certain molecule, we extend the KB witha suitable fact (i.e., a variable-free atomic formula) andwe examine the model that satisfies the KB under thestable model semantics (the addition of the fact and theexamination of the model is done automatically by ourimplementation). A formal definition of the stable modelsemantics is provided by Gelfond and Lifschitz [37]. Intu-itively, the stable model of a KB is the minimal set of factsthat are derived by exhaustively applying the existing rulesunder a particular rule order; a rule is applied if its posi-tive body can be matched to the so far derived facts andno atom of the negative body is in the already producedset of facts for the said matching.Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 8 of 15http://www.jbiomedsem.com/content/5/1/17Figure 2 Architecture of LoPStER. Stages of the classification process using LoPStER.The initially added fact is the molecule name predicateinstantiated with a fresh constant so that the rule thatencodes the structure of thatmolecule is triggered. For thecase of ascorbic acid, if we append the fact ascorbicAcid(a)to the previously described KB, we obtain the stable modelthat appears below.From the stable model atoms we can infer the super-classes of ascorbic acid, that is we deduce that ascor-bic acid isamong othersan unsaturated, polyatomic,heteroorganic, cyclic molecular entity that contains car-bon and a carboxylic ester. If there is no relevant atomfor a chemical class in the stable model, then we con-clude that the said class is not a valid subsumer, e.g.since carboxylicAcid(a) is not found in the stable model,carboxylic acid is not a superclass of ascorbic acid.Decidability checkTheKB discussed above contains rules with function sym-bols in the head, such as the rule used to encode themolecular structure of ascorbic acid. These rules mayincur non-termination during the computation of the sta-ble model due to the creation of infinitely many terms.In order to ensure termination of our reasoning pro-cess and thus decidability of the employed formalism,we perform a decidability check on the constructed KB.In a nutshell, the decidability check (also known and asmodel-summarising acyclicity [38]) involves transformingthe rules of the KB and inspecting the stable models of thetransformed KB for the existence of a special symbol. Ifthe KB passes the decidability check, then termination isguaranteed; this is the case for the types of KBs that wereStable model for ascorbic acidInput fact:ascorbicAcid(a)Stable model: ascorbicAcid(a), molecule(a), hasAtom(a, afi)for 1 ? i ? 13, o(afi)for 1 ? i ? 6,c(afi)for 7 ? i ? 12, h(af13), single(af8, af3), single(af9, af4), single(af12, afi)for i ? {5, 11},single(af10, afi)for i ? {1, 9, 11, 13}, single(af7, afi)for i ? {1, 8}, single(af11, af6), double(af2, af7),double(af8, af9), bond(af8, af3), bond(af9, af4), bond(af12, afi)for i ? {5, 11}, bond(af11, af6),bond(af10, afi)for i ? {1, 9, 11, 13}, bond(af7, afi)for i ? {1, 8}, bond(af2, af7), bond(af8, af9),horc(afi)for 7 ? i ? 13, carbonEntity(a), polyatomicEntity(a), heteroOrganicEntity(a),middleOxygen(af1), carboxylicEster(a), atLeast2Carbons(a), atLeast3Carbons(a),notHydroCarbon(a), unsaturated(a), cyclic(a)Stable model of the KB with the input fact ascorbicAcid(a) and the rules described in Methods; fi(a) isabbreviated with afi for 1 ? i ? 13.Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 9 of 15http://www.jbiomedsem.com/content/5/1/17previously described. Technical details of the aforemen-tioned condition are out of the scope of this text and canbe found in the relevant sources [38].Prototype implementationThe current section provides an overview of LoPStER(Logic Programming for Structured Entities Reasoner)the prototype we developed for structure-based chemicalclassification. The implementation is wrapped around theDLV system, a powerful and efficient deductive databaseand logic programming engine [39]. DLV constitutes theautomated reasoning component used by LoPStER forstable model computation of a rule set. Figure 2 depictsthe basic processing steps as well as the different filesthat are parsed and produced by LoPStER. LoPStER isimplemented in Java and is available online [36]; bothLoPStER and the rules modelling chemical classes areopen-source and released under GNU Lesser GPL. Next,we describe in more detail the several stages of execution.1. CDK-aided parsing. LoPStER parses the molfiles[40] of the molecules to be classified using theChemistry Development Kit Java library [41]. Themolfile is a widely used chemical file format thatdescribes molecular structures with a connectiontable; e.g. the molfile of ascorbic acid appears on theleft of Figure 1. For each molecule, a descriptiongraph (e.g. Figure 1 bottom right) representation isgenerated from its molfile according to atransformation as the one described for ascorbic acid.2. Compilation of the KB. For each molecule thedescription graph representation is used to produce aset of rules that encode the structure of the molecule,following the translation that was discussed in theprevious section. These rules along with theclassification rules and the facts necessary todetermine subclass relations are combined toproduce DLV programs (i.e. sets of rules) that arestored as plain text files on disk. In particular twokinds of DLV programs are created for eachmolecule, the program needed to perform thedecidability check as described before and theprogram needed to compute subclass relationsbetween the molecules and the chemical classes.3. Invoke DLV for decidability check. During thisstep, the model of the program, which was producedin the previous step for acyclicity testing, iscomputed. If the check is successful, then executionproceeds to the next stage; otherwise, the program isexited with a suitable output message.4. Invoke DLV for model computation. This is thestage where DLV is invoked to compute the stablemodel of the KB. Due to the check of the previousstep, the computation is guaranteed to terminate.5. Stable model storage. At this point, the stablemodel computed by DLV is stored in a file on disk toenable subsequent discovery of the subclass relations.6. Subsumptions extraction. This is the final phasewhere the stable model file is parsed in order todetect the superclasses of each molecule. All thesubsumee-subsumer pairs are stored in a separatespreadsheet file on disk.ResultsEmpirical evaluationIn order to assess the applicability of our implementa-tion, we measured the time required by LoPStER toperform classification of molecules. To obtain test datawe extracted molfile descriptions of 500 molecules fromthe ChEBI ontology. The represented compounds were ofdiverse size, varying from 1 to 59 atoms. Next, we inves-tigated the scalability of our prototype by altering twodifferent parameters of the knowledge base, namely thenumber of represented molecules and the type of mod-elled chemical classes. Initially, we constructed ten DLVprograms each of which contained rules encoding 50 · idifferent compounds, where 1 ? i ? 10, and rulesdefining the chemical classes (a sample of which was pre-viously described) excluding the cyclicity-related classes(48 classes in total). Next, we repeated the same construc-tion but this time including the rules for the cyclicity-related classes (51 classes in total). In the rest of thesection, we refer to the first setting as no cyclic and to thesecond as with cyclic.Additionally and in order to optimise the performance,we explored how classification times fluctuate dependingon the size of DLV programs. In particular, we parti-tioned the DLV programs into modules, we measuredclassification times for each module separately and wesummed up the times. Each module contains the factsand the rules describing a subset of the molecules rep-resented in the initial DLV program; the rules definingchemical classes are included in each one of the modules.Thus, the size of each module depends on the number ofencoded molecules. We tested modules of various sizes aswell as DLV programs without any partitioning for bothno cyclic mode and with cyclic mode. Modifying thesize of the module had a clear impact on the measuredtimes and performing classification with the modularisedknowledge base was always quicker than with the unpar-titioned one; we observed the shortest execution times formodule size 50 when testing in no cyclic mode and formodule size 20 when testing in with cyclic mode; the tim-ings we provide next refer to the aforementioned modulesizes.Table 1 summarises the classification times for the pre-viously described KBs. All the DLV programs that weretested passed the decidability check. The experimentsMagka et al. Journal of Biomedical Semantics 2014, 5:17 Page 10 of 15http://www.jbiomedsem.com/content/5/1/17Table 1 Timemeasurements for classificationNomolecules No of rules Time no cyclic Time with cyclic(sec) (sec)50 3614 4.81 7.85100 6832 3.41 8.69150 18072 4.25 9.97200 23746 4.55 11.88250 28502 6.60 18.71300 31892 8.27 20.63350 35046 8.14 22.58400 38095 9.30 24.23450 41536 9.94 29.68500 43629 10.40 32.79The first column is the number of molecules, the second is the number of rulesin the corresponding rule set and the third and fourth are measurements inseconds for no cyclic and with cyclic mode, respectively.were performed on a desktop computer (2GHz quadcoreCPU, 4GB RAM) running Linux. The first column dis-plays the number of molecules, the second column thenumber of rules contained in the corresponding DLV pro-gram and the third (fourth) column the time needed toperform classification in no cyclic (with cyclic) mode.We only display the number of rules for the no cyclicmode because there are only six more rules in the DLVprograms with cyclicity-related definitions. The classifica-tion experiments for each knowledge base were repeatedthree times and the results were averaged over the threeruns; also, the durations of Table 1 are inclusive, that isthey count the time spent from before the molfiles parsinguntil after the subsumptions extraction. Figure 3 depictsthe plots of the time intervals appearing in Table 1 bothwith regard to the number of molecules and the numberof rules contained in the respective DLV program.The performance results of Table 1 are encouraging forthe practical feasibility of our approach: the classificationof 500 molecules was completed in less than 33 seconsfor the suite of 51 modelled chemical classes. The dropin classification times between the 50 and 100 moleculescase is potentially due to JVM startup overhead. Onecan also observe that the rules encoding cyclicity-relatedclasses introduce a significant overhead for the classifica-tion times. In fact, it is the class that recognises moleculeswith cycles of arbitrary length that incurs the performancepenalty. The rules that encode the class of cyclicmoleculesneed to identify patterns that are extremely frequent inmolecular graphs; as a consequence, the amount of com-putational resources needed to detect ring-containingmolecules is much higher. However, since our class defini-tion for cyclic molecules detects compounds with cyclesof variable length, which is a significant property for theconstruction of chemical hierarchies, we consider thisoverhead acceptable.Discussion and related workConcerning expressive power, the current approachallows for the representation of strictly more chemicalclasses in comparison with other logic-based applica-tions for chemical classification. Villanueva-Rosales andDumontier [19] describe an OWL ontology of functionalgroups for the classification of chemical compounds; intheir work, they point out the inherent inability of OWL torepresent cyclic functional groups and how this impedesthe use of OWL in logic-based chemical classification. Asa remedy, Hastings et al. [21] employ an extension of OWL[42] for the representation of non-tree-like structuresand, thus, for the classification of molecular structures.However, the used formalism only allows for the iden-tification of cycles of fixed length and with alternatingsingle and double bonds. In the current approach we areFigure 3 Classification times. Curves of classification times with respect to number of molecules (left) and number of rules (right). The lower lineis for with cyclic mode and the upper for no cyclic mode.Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 11 of 15http://www.jbiomedsem.com/content/5/1/17able to recognise molecules containing cycles of both arbi-trary and fixed length and without requiring a particularconfiguration of bonds.Moreover, in both approaches outlined above theadopted open world assumption of OWL prevents onefrom defining structures based on the absence of cer-tain characteristics. In our approach we operate under theclosed world assumption which permits the definition ofa broad range of chemical classes that were not express-ible before such as the class of inorganic, hydrocarbonor saturated compounds. Finally and in comparison withprevious work [9], we take full advantage of the suggestedformalism by specifying a much wider range of chemi-cal classes and we do not require from the modeller aprecedence relation between the represented structures.In terms of performance, the classification resultsappear more promising than previous and related work.Hastings et al. [21] report that a total of 4 hours wasrequired to determine the superclasses of 140 molecules,whereas LoPStER identifies the chemical classes of 500molecules in less than 33 seconds. LoPStER is quicker incomparison with previous work too [9] where 450 secondswere needed to classify 70 molecules (two orders of mag-nitude faster). Please note that both cases discussed aboveconsidered a subset of the chemical classes used here.Regarding the significant change in speed, we identify thefollowing two factors that could explain it. First, DLV is amore suitable reasoner for our setting due to its bottom-up computation strategy as well as its active maintenanceteam and frequent releases. Second, we employ a moreefficient condition (model-summarising acyclicity [38]instead of semantic acyclicity [9]) in order to obtain termi-nation guarantees which allows for a more prompt decid-ability check. Finally, the classification times reported hereare slightly improved in comparison with a preliminaryversion of this paper due to some modelling optimisationsand the use of a recent new version of DLV.While conducting the experiments we discovered anumber of missing and inconsistent subsumptions fromthe manually curated ChEBI ontology; here we onlymention a few of them. As one can infer from themolecular graph of ascorbic acid appearing in the topright of Figure 1, ascorbic acid is a carboxylic ester aswell as a polyatomic cyclic entity. In spite of the factthat these superclasses were exposed by our classifica-tion methodology, we were not able to identify themin the ChEBI hierarchy. Figure 4 shows the ancestryof ascorbic acid (CHEBI:29073) in the OWL version ofthe ChEBI ontology; none of the concepts cyclic entity(CHEBI:33595), polyatomic entity (CHEBI:36357) or car-boxylic ester (CHEBI:33308) is encountered among thesuperclasses of ascorbic acid. Moreover, ascorbic acid isasserted as a carboxylic acid (CHEBI:33575) which is notthe case as it can be deduced by the lack of a carboxy groupin the molecular graph of ascorbic acid (the most com-mon tautomer of which appears in the top right corner ofFigure 1). We interpret the revealing of these modellingerrors as an indication of the practical relevance of ourcontribution.The chemical classification methodology that wepresent here is similar to other classification effortsbased on semantic technologies, such as classificationFigure 4 Ascorbic acid superclasses. Superclasses of ascorbic acid for the ChEBI OWL ontology release 102 as illustrated by the ChEBIgraph-based visualisation interface.Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 12 of 15http://www.jbiomedsem.com/content/5/1/17of proteins [7] or lipids [8]. Wolstencroft et al. use abioinformatics tool to extract composition informationfrom protein descriptions and subsequently translate thisinformation intoOWL axioms; these axioms are next usedto classify the proteins using a DL reasoner. Chepelevet al. use a cheminformatics tool to process lipid descrip-tions and produce annotated lipid specifications that arethen classified using an OWL ontology. The motivationof these two investigations is similar to ours, i.e. alle-viation of biocurating tasks; what distinguishes the twoapproaches from ours is the use of a different ontologylanguage and the role that this language plays during clas-sification. In particular, in our work we use nonmonotonicexistential rules instead of OWL which, unlike OWL, areable to capture cyclic structures. Also, in the sequenceof steps followed by our classification process we do notrely on a cheminformatics functionality to algorithmi-cally annotate the molecular descriptions, but instead theidentification of structural features forms integral part ofreasoning. The framework we suggested can be suitablefor the domains of lipids and proteins, as long as they arerestricted to structures of finite size; however empiricalevaluation would be needed to assess the suitability of theframework in practice. Regarding the application of ourprototype to ChEBI classification, it could be used to clas-sify ChEBI molecules under the chemical classes definedhere, but more curating effort would be needed to modelthe thousands of chemical classes that appear in ChEBI.In this work, we represent and reason about chemi-cal knowledge using an ontology language. However, themajority of axioms constituting the ontology, that is themolecule descriptions, are sourced through molfiles thatare parsed using cheminformatics libraries. The informa-tion provided by these files includes connectivity betweenatoms, types of atoms and bonds and charges of atoms.This information is converted into logical axioms thatare subsequently processed by an automated reasoningalgorithm to identify the chemical classes of themolecules. This approach has the advantage of allow-ing the knowledge modeller to define new classes in adeclarative way, that is without the need of writing codefor detecting their subsumees. However, a feature thatcould be detected using cheminformatics algorithms andbecome part of the ontology axioms is the existence ofring atoms. The benefits of such a modification could betwofold: it could considerably speed up the computationof all cyclicity-related classes (e.g. determining whether anatom is a ring atom can be done very quickly using theCDK library) and at the same time could allow for the def-inition of strictly more cyclicity-related classes, such ascarbocyclic compounds.An alternative approach could be to build rules fromchemical identifiers other thanmolfiles, such as InChi [43]or preferred IUPAC names [44]. In particular, InChi withits abilitiy to encode isotopical and stereochemical infor-mation (which can be critical for biological applications)could lead to richer chemical modelling. Also, widely usedchemical databases, such as ChemSpider [45], could beused as a resource for adding to rules information aboutmolecular properties.A category of molecules that our framework does notcover is tautomers. A tautomer is each of two or moreisomers that exist together in equilibrium, and are readilyinterchanged by migration of an atom (usually hydrogen)or group within the molecule. InChi handles tautomerismby allowing a compound to contain mobile hydrogenatoms, that is some hydrogens are marked as being ableto occur in different positions. This is an approach thatcould be adopted by our methodology too, if we extendedour formalism with the ability to represent disjunctivehasParticipantlocatedInreactantproductFigure 5 Transport reaction description graph.Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 13 of 15http://www.jbiomedsem.com/content/5/1/17hasPartlinkedFigure 6 Jasmonic acid description graph.Molecular graph of jasmonic acid (left) and description graph of jasmonic acid based on thefunctional groups partonomy (right).information. However, enriching nonmonotonic existen-tial rules with disjunction would require to alter the designand implementation of the reasoning algorithm, so treat-ing tautomers could be part of a future extension of ourframework.ConclusionWe presented an implementation that performs logic-based classification of chemicals and builds upon a soundand complete reasoning procedure for nonmonotonicexistential rules; our prototype relies on the DLV systemand is considerably quicker than previous approaches. Forour evaluation, we represented a wide variety of chem-ical classes that are not expressible with OWL-basedformalisms and described a surface syntax that couldenable cheminformaticians to define ontological descrip-tions of chemical entities intuitively and without the needto use first-order logic notation; additionally, our softwarerevealed subclass relations that are missing from theman-ually curated ChEBI ontology as well as some erroneousones. We demonstrated thus the capabilities of a datalog-based ontology language that displays a favourable trade-off between expressive power and performance for thepurpose of structure-based classification.Future researchFor the future it would be interesting to further applyour framework towards supporting classification of othercomplex biological objects. For instance, one can exploitthe expressive power of rules to represent biochemicalprocesses and infer useful relations about them. Figure 5depicts a description graph abstraction of a chemical reac-tion example discussed by Bölling et al. [46]. The processconsists of parts that are arbitrarily interconnected andcan thus be naturally modelled using our formalism. In thesame vein, our methodology could provide rigorous defi-nitions for the representation of lipid molecules that canbe systematically classified according to their structuralfeatures. Low et al. [47,48] introduced the OWL DLLipid Ontology which contains semantically explicit lipiddescriptions. One could achieve more accurate modellingby casting lipids in terms of rules that capture frequentcyclic patterns in a concise way; for example, Figure 6illustrates a description graph for jasmonic acidoneof the lipids encountered in the abovementioned OWLontology.Further work could involve the building of an ontologyeditor for the creation of surface syntax expressions andtheir automatic conversion into nonmonotonic existentialrules. We will also seek to extend our prototype to accom-modate subsumption between chemical classes so as togenerate a complete multi-level chemical hierarchy usingideas from our recent work [49,50]. We could extend ourformalism with numerical value restrictions [51] in orderto express e.g. classes depending on molecular weight.Moreover, it could be of interest exploring the integrationof our prototypewith Protégé [52], Life Sciences platforms[53] and chemical structure visualisation tools [54,55] aswell as defining a mapping of the introduced formalism toRDF [56].Additional filesAdditional file 1: Timemeasurements and produced hierarchy of theclassification experiments. Description of data: Full list of computedsubsumptions and time measurements for each of the five experimentsdiscussed in Empirical evaluation.Additional file 2: Logic programwithout cyclicity-related rules.Description of data: Set of rules modelling the chemical classes excludingthe cyclicity-related classes.Additional file 3: Complete logic program. Description of data: Set ofrules modelling all the chemical classes.AbbreviationsOWL:Web ontology language; ChEBI:Chemical entities of biological interest;W3C:World wide web consortium, KR:Knowledge representation; ML: Machinelearning; KB:Knowledge base; DG:Description graph; LoPStER:Logicprogramming for structure entities reasoner; RDF:Resource descriptionframework.Magka et al. Journal of Biomedical Semantics 2014, 5:17 Page 14 of 15http://www.jbiomedsem.com/content/5/1/17Competing interestsThe authors declare that they have no competing interests.Authors contributionsAll authors conducted research on the underlying decidability conditions fordatalog-based rules and jointly discussed the present paper and its maincontributions (surface syntax, chemical modelling, experimental setup). DMhas specified the surface syntax grammar, assembled the knowledge base,carried out the experiments and led the writing of the manuscript. MK and IHcontributed to the discussions and participated in the writing of themanuscript. All authors read and approved the final manuscript.AcknowledgementsWe would like to thank Dr Chris Batchelor-McAuley for answering ourchemistry questions and the anonymous reviewers of this article for providingJOURNAL OFBIOMEDICAL SEMANTICSPaul et al. Journal of Biomedical Semantics 2014, 5:8http://www.jbiomedsem.com/content/5/1/8RESEARCH Open AccessSemantic interestingness measures fordiscovering association rules in the skeletaldysplasia domainRazan Paul1, Tudor Groza1*, Jane Hunter1 and Andreas Zankl2,3AbstractBackground: Lately, ontologies have become a fundamental building block in the process of formalising and storingcomplex biomedical information. With the currently existing wealth of formalised knowledge, the ability to discoverimplicit relationships between different ontological concepts becomes particularly important. One of the most widelyused methods to achieve this is association rule mining. However, while previous research exists on applyingtraditional association rule mining on ontologies, no approach has, to date, exploited the advantages brought byusing the structure of these ontologies in computing rule interestingness measures.Results: We introduce a method that combines concept similarity metrics, formulated using the intrinsic structure ofa given ontology, with traditional interestingness measures to compute semantic interestingness measures in theprocess of association rule mining. We apply the method in our domain of interest  bone dysplasias  using the coreontologies characterising it and an annotated dataset of patient clinical summaries, with the goal of discoveringimplicit relationships between clinical features and disorders. Experimental results show that, using the abovementioned dataset and a voting strategy classification evaluation, the best scoring traditional interestingness measureachieves an accuracy of 57.33%, while the best scoring semantic interestingness measure achieves an accuracy of64.38%, both at the recall cut-off point 5.Conclusions: Semantic interestingness measures outperform the traditional ones, and hence show that they areable to exploit the semantic similarities inherently present between ontological concepts. Nevertheless, this isdependent on the domain, and implicitly, on the semantic similarity metric chosen to model it.IntroductionOver the course of the last decade, ontologies havebecome a fundamental building block in the knowledgeacquisition and capturing processes in the biomedicaldomain. Repositories such as BioPortal [1] or the OBOFoundry [2] currently offer a varied range of ontologies,in addition to tool support to visualise, query and inte-grate concepts hosted by these ontologies. Subsequently,this enables the construction of decision support meth-ods that use ontological background knowledge in orderto produce more accurate and more refined outcomes.*Correspondence: tudor.groza@uq.edu.au1School of ITEE, The University of Queensland, St. Lucia, Queensland 4072,AustraliaFull list of author information is available at the end of the articleOntologies provide structured and controlled vocabu-laries and classifications for domain specific terminolo-gies. Their adoption for annotation purposes provides ameans for comparing medical concepts on aspects thatwould otherwise be incomparable. For example, the anno-tation of a set of disorders (directly or via patient cases)using a particular ontology enables us to compare thesedisorders, by looking at the underpinning annotation con-cepts. The actual comparison can be done in an exactor inexact manner. More concretely, one may take intoaccount only those identical concepts that appear in allor some disorders, or may use a semantic similarity mea-sure that relaxes the constraint on identical concepts.Such a semantic similarity measure represents a functionthat takes two or more ontology concepts and returnsa numerical value that reflects the degree of similarity© 2014 Paul et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly cited.Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 2 of 13http://www.jbiomedsem.com/content/5/1/8between these concepts in a given ontology. This com-parison process represents a key aspect of typical datamining algorithms that form the core of any decisionsupport method. For example, two ontological concepts,such as HP:0004481 (Progressive macrocephaly) andHP:0004482 (Relative macrocephaly) from the HumanPhenotype Ontology (HPO) [3], would be treated differ-ently by any classical data mining algorithm because oftheir symbolic (i.e., lexical grounding) difference. How-ever, these two concepts, like any other two entities in anontology, are to a certain extent semantically similar  asimilarity that can be encoded via an existing or custom-made metric. Replacing exact matching with semanticsimilarity measures provides novel and exciting oppor-tunities in knowledge discovery and decision support onannotated datasets [4-6].Association rules [7] are valuable patterns that can dis-covered from annotated datasets. An association ruledenotes an implication relationship (or a directed co-occurrence) between two sets of items within a transac-tion. A widely used algorithm to discover such associationrules is Apriori [7]. However, regardless of the particu-lar algorithm used, the discovery process has two majorchallenges: (i) too many rules may be generated (the rulequantity problem); (ii) not all rules are necessarily inter-esting (rule quality problem). The solution to the rulequality problem relies on specifying an interestingnessmeasure [8-10] to encode the utility or significance of apattern. These measures are intended for selecting andranking patterns according to their potential interest andenables highly ranked rules to be immediately presentedor used for particular purposes.Existing work on interestingness measures takes intoaccount only exact matching [10]. Semantic similarities,however, enable novel ways of interpreting data items, andhence may lead to the identification of association rulesthat are otherwise not discoverable via exact matching. Inthis manuscript, we advance the state of the art by explor-ing the application of semantic similarities in widely usedinterestingnessmeasures in the context of association rulemining. In other terms, we aim to use existing taxonomicrelations to calculate so-called semantic interestingnessmeasures.The context of our research is provided by the SKELE-TOME project [11], which aims to create a community-driven knowledge curation platform for the skeletaldysplasia domain. Skeletal dysplasias are a heterogeneousgroup of genetic disorders affecting skeletal development.Currently, there are over 450 recognised bone dysplasias,structured in 40 groups. Patients with skeletal dysplasiashave complex medical issues including short stature,bowed legs, a larger than average head and neurologicalcomplications. Since most skeletal dysplasias are very rare(< 1:10,000 births), data on clinical presentation, naturalhistory and best management practices is sparse. To date,we have developed an ontology, the Bone Dysplasia Ontol-ogy (BDO) [12], and a series of decision support methods[6,13]. BDO has been built using the latest nosology ofbone dysplasias [14] that groups disorders according totheir overlapping clinical and genetic features. For exam-ple, Achondroplasia and Diastrophic dysplasia are similar,and are both part of the FGFR3 Group, because they sharea range of clinical features (i.e., short stature with veryshort arms and legs).Within this manuscript, we investigate both tradi-tional, as well as semantic interestingness measures in thecontext of association rule mining, to discover implicitrelationships between clinical features and disorders inskeletal dysplasia domain. The main contributions of thiswork are the following: (i) firstly, we analyse which ofthe existing traditional interestingness measures enables amore accurate discovery of association rules in the skele-tal dysplasia domain; (ii) secondly, we propose a seriesof interestingness measures based on semantic similaritymetrics using existing ontologies as background knowl-edge; and (iii) finally, we perform an extensive empiricalevaluation to measure the quality of the resulting rules,using an annotated dataset built on real patient data.At the same time, we show that, given an appropriatesemantic similarity metric, the semantic interestingnessmeasures outperform the traditional ones.As alreadymentioned, our work focuses only on skeletaldysplasias, and hence it investigates the efficiency of theabove-described methods only in this domain. However,the generic definition of a semantic interestingness mea-sure proposed in this manuscript is directly applicable inany other domain, while the rest of the research can beused as a guideline for choosing an appropriate domain-specific semantic similarity metric to be applied as part ofthe overall measure.BackgroundThis section provides an overview of the foundationalblocks of the experiments performed in the context ofour research. We start by introducing the Human Pheno-type Ontology and the Bone Dysplasia Ontology  i.e., theontologies used as background knowledge for the seman-tic similarity metrics. Then, we describe some of the basicnotions of semantic similarities, and finally, we discusssome of the traditional interestingness measures.Human Phenotype OntologyThe Human Phenotype Ontology (HPO) [3] has latelybecome the de facto controlled vocabulary to capture andrepresent clinical and radiographic findings. The ontol-ogy consists of around 9,000 concepts describingmodes ofinheritance, onset and clinical disease courses and pheno-typic abnormalities. This last category represents aroundPaul et al. Journal of Biomedical Semantics 2014, 5:8 Page 3 of 13http://www.jbiomedsem.com/content/5/1/895% of the ontology and it is the main subject of ourstudy. HPO structures phenotypic abnormalities in a hier-archical manner (via class-subclass relationships) fromgeneric (e.g., HP:0000929 (Abnormality of the skull) tospecific concepts (e.g., HP 0000256  Macrocephaly).For instance, HP:0001629 (Ventricular septal defect) is asubclass of the concept HP:0010438 (Abnormality of theventricular septum) in the sense that a ventricular septaldefect is a kind of abnormality of the ventricular septumand hence. every person with a ventricular septal defectcan also be said to have an abnormality of the ventricularseptum. This goes along the line of theTrue path rule [15],which states that an annotation with a particular conceptimplies the path from that concept to the root to be true,or more concretely, a valid annotation with all ancestorsof that concept.One obvious advantage of capturing phenotypic infor-mation using ontologies is that it enables the design ofassociation mining algorithms that can exploit the seman-tic relationships between concepts. For instance, an algo-rithm can be designed to support not only the patternsassociated with a concept like HP:0001671 (Abnormal-ity of the cardiac septa), but also those associated withits children, HP:0010438 (Abnormality of the ventricu-lar septum) and HP:0011994 (Abnormality of the atrialseptum).Bone Dysplasia OntologyThe International Skeletal Dysplasia Society (ISDS http://www.isds.ch/) Nosology lists all recognised skele-tal dysplasias and groups them by common clinical-radiographic characteristics and/or molecular diseasemechanisms. The Nosology is revised every 4 years byan expert committee and the updated version is usuallypublished in a medical journal. This is widely accepted asthe official nomenclature for skeletal dysplasias withinthe biomedical community, with the latest version beingpublished in 2010 [14].The Bone Dysplasia Ontology [12] aims to comple-ment the spectrum of existing ontologies and addressthe specific knowledge representation shortcomings ofthe ISDS Nosology. Its main role is to provide the scaf-folding required for a comprehensive, accurate and for-mal representation of the genotypes and phenotypesinvolved in skeletal dysplasias, together with their spe-cific and disease-oriented constraints. As opposed to theISDS Nosology, the ontology enables a shared conceptualmodel, formalised in a machine-understandable descrip-tion, in addition to a continuous evolution and a founda-tional building block for facilitating knowledge extractionand reasoning. Currently, the structure of the ontologyfollows closely the grouping of the disorders imposed bythe expert committee via the Nosology by using class-subclass relationships between the 40 groups and theirassociated bone dysplasia members. These groups arethen linked via the root concept Bone_Dysplasia.Semantic similarityAnnotations using Bio-ontologies allow us to compareconcepts on various aspects by using their intrinsicsemantic similarity. Semantic similarity represents thequantification of the degree of similarity between twoor more ontological concepts. For example, the annota-tion of two bone dysplasias with concepts emerging fromthe same ontology, e.g., HPO, enables their comparisonby looking at the semantic similarity between the con-cepts used for annotation. In addition to this implicit role,semantic similarity measures can also be used to discoverassociation rules in annotated datasets.In principle, there are two types of approaches forcomputing semantic similarity measures: node-based andedge-based. The former uses the nodes and their proper-ties as the data source whereas the latter uses the edgesbetween nodes and their associated types as data source.The node-based approaches usually rely on the notion ofInformation Content (IC) to quantify informativeness of aconcept. An IC value of a node is calculated by comput-ing the negative likelihood of its frequency in a large textcorpora (IC(c) = ?log(p(c))), with the intuition that themore probable is the appearance of a concept in a corpus,the less information it conveys.A large number of node-based measures have been pro-posed using Information Content as a central element,some of the most widely used being listed below, i.e.,Resnik [16], Lin [17] and Jiang and Conrath [18]. As a note,in the equations below,MICA denotes the Most Informa-tive Common Ancestor, i.e., the common ancestor of thenodes with the highest Information Content.Resnik : SIMRes(c1, c2) = IC(cMICA) (1)Lin : SIMLin(c1, c2) = 2 ? IC(cMICA)IC(c1) + IC(c2) (2)Jiang and Conrath : SIMJC(c1, c2)= 1 ? IC(c1) + IC(c2) ? IC(cMICA) (3)In the other category, i.e., edge-based approaches, Wu& Palmer [19] proposed a measure based on the lengthof the shortest path between the Least Common Ances-tor (LCA) and the root and on the length of shortest pathbetween each of the concepts and that common ancestor.DisW&P(c1, c2) = 2 ?N3N1+ N2+ 2 ?N3 (4)where, N3 is the length of path from LCA to the root; N1is the length of path from c1 to LCA; N2 is the length ofpath from c2 to LCA.Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 4 of 13http://www.jbiomedsem.com/content/5/1/8Association rule miningAssociation rules [7] provide knowledge in the form ofprobabilistic if-then statements, e.g., I ? Q. The headof the association rule (i.e., the if part  I) is calledantecedent, while the body (i.e., the then part Q) is calledconsequent. The antecedent and consequent of an asso-ciation rule are disjoint  they do not have any items incommon. To express uncertainty in association rules, i.e.,I ? Q with a certain degree of certainty, several metricscan be used, two of themost widely adopted being Supportand Confidence (discussed below). A set of associationrules aimed for classification is called predictive associ-ation rule set. A class association rule set is a subset ofassociation rules with the specified classes as their conse-quences. Predictive association rules form a small subsetof class association rules. Generally, mining predictiveassociation rules undergoes the following two steps: (i)Find all class association rules from a database, followedby (ii) Prune and organise the found class association rulesto return a sequence of predictive association rules.Traditional interestingness measures. As mentionedearlier, the rule discovery process is usually associatedwith two challenges, one of them being the rule qualityproblem, i.e., quantifying which of the discovered rules aremore interesting. Interestingnessmeasures play an impor-tant role in data mining, regardless of the kind of patternsbeing mined. They are intended for selecting and rank-ing patterns according to their potential interest to theuser. Below, we present a number of existing associationrules interestingness measures [10], which we have alsoapplied in our experiments. This set of measures rely onthe foundational Support and Confidencemetrics.Let T = {t1, t2, . . . , tn} be a database of n transactionswith a set of attributes (or items) I = {i1, i2, . . . , im}. Foran itemset IX ? I and a transaction t ? T , we say that tsupports IX if t has values for all the attributes in IX . ByTIX we denote the transactions that contain all attributesin IX .The Support of IX is computed asSupport(IX) = TIXn (5)or the fraction of transactions that include all attributes inIX .The Confidence of an association rule IX ? Q, where Qis also an itemset (Q ? I) and Q ? IX = ?, is defined by:Confidence(IX ? Q) = Support(IX ,Q)Support(IX) (6)or the ratio between the number of transactions thatinclude all items in the consequent (Q), as well as in theantecedent (IX)  namely, the Support of the union of IXand Q  and the number of transactions that include allitems in the antecedent (i.e., the Support of IX).Confidence alone may not be enough to assess thedescriptive interest of a rule, as rules with high confi-dence may occur by chance. Such spurious rules can bedetected by determining whether the antecedent and theconsequent are statistically independent. This inspired anumber of measures, including Lift, Conviction, Leverage,Jaccard, Cosine and Correlation Coefficient [8-10]. Weprovide their mathematical definitions in the followingsections.Materials andmethodsAnnotation datasetThe rare nature of bone dysplasias makes the data collec-tion particularly challenging. In 2002, the European Skele-tal Dysplasia Network (ESDN, http://www.esdn.org/) wascreated to alleviate, at least partly, the data sparsenessissue. At the same time it aimed to provide a collabo-rative environment to help with the diagnosis of skele-tal dysplasias and to improve the information exchangebetween researchers. To date, ESDN has gathered over1,200 patient cases, which have been discussed by its panelof experts. The ESDN case workflow consists of threemajor steps: (i) a patient case is uploaded and an initialdiagnosis is set by the original clinician that referred thecase; (ii) the panel of experts discusses the case until anagreement is reached; (iii) the panel of experts recom-mends a final diagnosis. Among the total number of cases,744 have a final bone dysplasia diagnosis (the remainingcases were not thought to be true bone dysplasias by theexperts), with a total of 114 different skeletal dysplasiascovered.Patient clinical summaries in ESDN are represented ina free text format. The language used within the ESDNclinical summaries suffers from several issues, such assynonymy (several terms having the same meaning) orhyponymy (one term beingmore specific than another). Inorder to be able to use this data, we extracted patient phe-notypes by annotating the text with corresponding termsfrom the Human Phenotype Ontology (HPO). The actualannotation process was performed using the NationalCentre for Biomedical Ontology (NCBO) Annotator [20],an ontology-based web service for annotation of tex-tual sources with biomedical concepts. A bone dysplasiaexpert (one of the co-authors) has manually validated theresulting HPO annotations to ensure their correctnessand to eliminate, in particular, false positives.As a remark,the false negatives resulted from the annotation processmay be under-estimated, and could not be validated sincewe were not able to perform a full-fledged annotation ofthe clinical summaries. The diagnosis associated with thepatient cases has also been annotated with concepts fromthe Bone Dysplasia Ontology (BDO). More concretely,the final diagnosis set by the panel of experts has beenconverted to the corresponding BDO concept.Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 5 of 13http://www.jbiomedsem.com/content/5/1/8In order to achieve realistic results using associationrule mining, from the 114 types of dysplasias present inthe ESDN dataset, we chose only those that were repre-sented by more than 10 patient cases. This has reducedour dataset to 394 annotated patient cases (i.e., around33% of the total number) diagnosed with 15 different bonedysplasias. The set features a total of 441 distinct pheno-types, with an average of 63.67 distinct phenotypes perdisorder and an average of 4.49 distinct phenotypes percase. The experiments described in this manuscript usethis dataset for training and testing purposes.Proposed approachOur goal is to discover association rules from anno-tated and diagnosed patient cases in order to observeco-occurrence relationships between clinical features anddisorders. In other words, we aim to find association rulesof the form {ICF } ? {IBD}, where ICF represents the setof clinical features of a patient and IBD is a bone dys-plasia diagnosis. From a conceptual perspective, ICF willcomprise annotations assigned to patient cases, or moreconcretely, HPO concepts. We have adapted the Apriorialgorithm by adding two constraints, required to matchour aim: (i) every desired itemset must have one set ofclinical features and a single dysplasia, and (ii) both candi-date itemsets and frequent itemsets can have at most onedysplasia item.Following the discovery of the desired itemsets, theseare partitioned into two components: a component con-taining the skeletal dysplasia and one containing the phe-notypes. A Boolean function that determines the type ofa component is used to perform this classification. Sub-sequently, we calculate the different traditional or seman-tic interestingness measures between the bone dysplasiacomponent and the phenotype set of the rule.Modelling traditional support in the context of semanticannotationsIf an itemset consists of the items I = {i1, i2, i3, . . . , im}for the reference concept RC and there are n transactionsin the knowledge base KB, Support is defined as the pro-portion of instances of the reference concept RC in theknowledge base which contain the itemset I.Support(I,RC,KB)= Number of instances of concept RC that contain the itemset IThe total number of instances of the concept RC(7)In our case, the reference concept (RC) is representedby the patient (P) and KB is annotated dataset. Below wepresent an example of traditional Support calculation.Let us consider the following set of clinical features rep-resented by HPO concepts (cf ? ICF ), in addition to abone dysplasia: cf1  HP:0008921 (Neonatal short-limb shortstature) cf2  HP:0008905 (Rhizomelic short stature) cf3  HP:0000772 (Abnormality of the ribs) cf4  HP:0000774 (Narrow chest) bd1  BDO:AchondroplasiaLet us also consider three reference concepts (i.e.,patients) p1, p2 and p3 and assume that the KB containsthe following itemsets: I(p1) = {Icf1(p1), Icf3(p1), bd1} I(p2) = {Icf1(p2), Icf4(p2), bd1} I(p3) = {Icf2(p3), Icf3(p3), bd1}where Icfx(px)={cfx|exhibits(px, cfx)}. Our goal is to com-pute the support of the itemset I(p)={Icf1 (p), Icf3(p), bd1}.We can quickly observe that there is one patient instancethat contains this pattern  i.e., p1. Since the totalnumber of patient instances is 3, traditional support isthen:Support(I, P,KB) = 13 = 0.33 (8)However, a close look at cf1 and cf2 in HPO reveals thatthese concepts are fairly similar (they have a direct com-mon ancestor in HP:0008873  Disproportionate short-limb short stature), but not exactly the same. cf3 and cf4are in a similar situation, with the parent of HP:0000774(i.e., HP:0005257  Thoracic hypoplasia) being a siblingof cf3. Unfortunately, traditional Support cannot leveragethis semantic similarity information as it relies on exactmatching. To overcome this issue, we propose an alter-native set of semantic interestingness measures (SemanticSupport, Semantic Confidence, etc.).Semantic similarity of itemsOur intuition is that by using semantic similarity mea-sures on patient findings (i.e., HPO concepts) we are ableto leverage and use the semantic relationships betweenphenotypes that cannot, otherwise, be acquired by typicaldata mining processes (due to their term-based match-ing process). As an example, if the background knowledgebase lists HP:0000256 (Macrocephaly) as a phenotype ofAchondroplasia and a new patient exhibits HP:0004439(Craniofacial dysostosis), we want to use the semanticsimilarity value between the two concepts to associatethe later to Achondroplasia with a certain probability.The semantic similarity between the concepts could beinferred, for example, via their most common ancestor HP:0000929 (Abnormality of the skull). Such an associa-tion is not possible when employing a typical data miningprocess since each term would be considered individu-ally and only in the context provided by the backgroundknowledge base.Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 6 of 13http://www.jbiomedsem.com/content/5/1/8In principle, a good semantic similarity measure needsto take into account the specific aspects of the targetdomain. There are, nevertheless, a series of requirements emerging also from the bone dysplasia domain and thestructure of HPO  that are generally applicable: Given two HPO concepts, we consider them to bemore similar if they are closer to each other (i.e., thepath between them is shorter). E.g., HP:0004481(Macrocephaly progressive) will be considered moresimilar to HP:0000256 (Macrocephaly) thanHP:0004488 (Macrocephaly at Birth ), because thedistance between HP:0004481 and HP:0000256is 1 whereas the distance between HP:0004481 andHP:0004488 is 2. Several strategies have been used in choosing thesemantic similarity function. Li et al. [21], in theirwork on modelling and capturing semantic similarityin WordNet, have employed an exponent function totransfer the path length between concepts into asimilarity value and have showed that the exponentialmeasure significantly outperforms traditionalsimilarity measures. Given that the design philosophyof HPO andWordNet are similar, we derive thesimilarity between two phenotypes as an exponentfunction of the path length between theircorresponding HPO concepts. The same rationale isvalid also for BDO. In order to be able to calculate the semanticinterestingness measures, semantic similarity needsto take values between 0 to 1. At the same time, anexact match should be signalled by a semanticsimilarity value of 1. The semantic similarity value of two concepts shouldbe dependent on the specificity of their LCA (i.e., itslocation in the overall hierarchy). More concretely,we consider the more specific LCA to be moreinformative. E.g., HP:0004439 (Craniofacialdysostosis) (as an LCA) should be considered moreinformative than HP:0000929 (Abnormality of theskull ), which is in this case, is its direct parent.In the following we describe a set of domain-orientedsemantic similarity functions that satisfy the above-listedrequirements.Domain-specific semantic similarity measures. If i1and i2 are two items, we define the semantic similaritybetween them as:SemSim(i1, i2) = Dist(LCA(i1, i2),Root)Dist(i1, i2) + Dist(LCA(i1, i2),Root)(9)where Dist(LCA(i1, i2),Root) is the length of path fromLCA(i1, i2) to the root and Dist(i1, i2) is a distancemeasure between i1 and i2 that depends on the underlyingtypes of the items.If the items under scrutiny are phenotypes, we defineDist(i1, i2) as shown in Eq. 10.Dist(i1, i2) =???2lx , if i1 = i20, if i1 = i2 = root1, if i1 = i2 = root(10)where lx is the shortest path between i1 and i2. Thisformula determines the semantic similarity of two HPOterms based on both the distance between these termsand the location of their LCA in the HPO structure. It canalso be observed that the larger the distance between theterms, the less similar they will be. Finally, if two conceptsare the same but do not denote the root, the value of thefunction is 0, while if they do denote the root, the value ofthe function is 1, to avoid the division by 0 case.In Eq. 10 the shortest path length is scaled by an expo-nential function to providemore weight to distance ratherthan depth. Furthermore, the base and the exponent ofthis power function aim to overemphasise the similaritybetween phenotypes when taking into account the HPOstructure. Generally, this similarity decreases faster thanthe distance. For instance, the distance between Macro-cephaly and Macrocephaly progressive is 1 and they arevery similar, while the distance between Abnormality ofSkull and Macrocephaly progressive is 3, with the formerbeing much more generic and different to Macrocephalyprogressive than any of the other macrocephalies.Similar to the phenotype distance described above, if weconsider two disorders using the Bone Dysplasia Ontol-ogy, we define the same Dist(i1, i2) as shown in Eq. 11 the semantic similarity equation remains unchanged (i.e.,as per Eq. 9).Dist(i1, i2) =???10lx?2, if i1 = i20, if i1 = i2 = root1, if i1 = i2 = root(11)where lx is again the shortest path between i1 and i2.The rationale behind Eq. 11 is the same as for Eq. 10 (seeabove), with the remark that the overall similarity betweendisorders decays at an even higher rate (with the distancein BDO) because of their coarse grained nature, which hasled to a fairly flat structure of the ontology. The structureof the ontology, and more concretely its maximum depth(i.e., 2), has influenced the constant (2) in the exponent ofthe formula (lx ? 2). The intuition is that concepts thatbelong to the same group, i.e., they are at the second levelin the hierarchy and the distance between them is 2 (viathe LCA), should receive the highest similarity, after theexact match.Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 7 of 13http://www.jbiomedsem.com/content/5/1/8Semantic supportGiven a knowledge base and an itemset, our goal is toautomatically derive a score that indicates the proportionof transactions in the knowledge base that contain theitemset at a semantic level, thus going beyond the exactmatching methods traditionally used for this task. Thisneeds to take into account the relations between items.We attempt to model the semantic support of an item-set as a function of the semantic similarity of the termspresent in the knowledge base and the itemset.If we consider a database T with n transactions{t1, t2, . . . , tn} and m items {i1, i2, . . . , im}, Semantic Sup-port of {i1, i2, . . . , ip} (p ? m) is calculated as follows:SemSupport(i1, i2, . . . , ip) = 1n ?n?q=1p?j=1argmaxv=1to|tq|||SemSim(ij , iv)||(12)The value of the Semantic Similarity (SemSim) rangesfrom 0 to 1 and so does the value of the Semantic Support.Semantic interestingnessmeasuresSemantic interestingness measures take into account howdata items are semantically related. To do so, it makes useof the underlying structure of the ontology that hosts thecorresponding items (e.g. generalisation, specialisation,etc). Hence, if we replace the traditional Support elementin the confidence calculation with Semantic Support weget Semantic Confidence. The same process can be appliedfor the other well-known interestingness measures, suchas lift, conviction, etc. Below we list the correspondingsemantic calculation for thesemeasures for an associationrule IX ? Q.SemConfidence(IX ? Q) = SemSupport(IX ,Q)SemSupport(IX ) (13)SemLift(IX ? Q) = SemConfidence(IX ,Q)SemSupport(Q) (14)SemConviction(IX ? Q) = 1? SemSupport(Q)1 ? SemConfidence(IX ? Q)(15)SemLeverage(IX ? Q) = SemSupport(IX ,Q)? SemSupport(IX ) ? SemSupport(Q)(16)SemJaccard(IX ? Q)= SemSupport(IX ,Q)SemSupport(IX ) + SemSupport(Q) ? SemSupport(IX ,Q)(17)SemCosine(IX ? Q) = SemSupport(IX ,Q)?SemSupport(IX) ? SemSupport(Q))(18)SemCorrelationCoeff (IX ? Q)= SemLeverage(IX ? Q)?S Supp(IX)?S Supp(Q)?(1? S Supp(IX) ? (1? S Supp(Q))(19)S Supp in Eq. 19 denotes Semantic Support.Experimental designWe have carried out a series of experiments with thefollowing goals: Firstly, we aim to analyse the accuracy of theresulting association rules when using existingtraditional interestingness measures; Secondly, we are interested in finding out the sameaccuracy, but when using the proposed semanticinterestingness measures; Finally, we aim to observe the difference between theaccuracies produced via the two methods.The quality of discovered rules depends on their ability todetermine the correct diagnosis. Tomeasure accuracy, wehave employed a voting strategy, which is described below.The purpose of evaluating the discovered rules is tounderstand the utility of the interestingness measures.Voting allows all firing association rules to contribute tothe final prediction. This strategy combines the associ-ations KF( px) that fire upon a new patient case px. Asimple voting strategy considers all the rules in KF( px),groups the rules by antecedent, and for each antecedentIX obtains the class corresponding to the rule with high-est confidence. We will denote the class voted by anantecedent Ii with a binary function vote(Ii, bd) that takesthe value 1 when Ii votes for disorder bd, and 0 for the anyother class  {bdn1, bd2, . . . , bdn} ? BD represent a set ofbone dysplasias. The disorder that receives the maximumvote is the most probable diagnosis for patient case x.TotalVote(bdi) =?Ii?antecedents(KF(px))Vote(Ii, bdi) (20)Weighted voting is similar to simple voting, however,each vote is multiplied by a factor that quantifies the qual-ity of the vote. In the case of association rules, this can bedone using one of the above defined measures.TotalVote(bdi) =?Ii?antecedents(KF(px))Vote(Ii , bdi) ?QVote(Ii , bdi)(21)In our case, QVote(Ii, bdi) is the quality of vote, or moreconcretely themaximum interestingness of that particularantecedent group.Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 8 of 13http://www.jbiomedsem.com/content/5/1/8We have performed individual experiments for each ofthe interestingness measures previously described, usingthe voting strategy. To assess their efficiency, we havecalculated the overall accuracy of the discovered associa-tion rules. In all experiments, we compute the predictionaccuracy as the overall percentage of correctly predicteddisorders at a given recall cut-off point (i.e., by taking intoaccount only the top K predictions for different values ofK, where K is the recall cut-off point). Hence, a successrepresents a correctly predicted disorder (the exact same,and not a sub or super class of it), while a miss representsan incorrectly predicted disorder. If N is the total numberof test cases and CP is the number of correctly predicteddisorders, then Accuracy = CP/N . This is expressed inpercentages in Tables 1, 2 and 3 in the Results section.As mentioned earlier in the manuscript our annotateddataset consisted of over 300 patience cases, with the clin-ical features annotated using HPO and the disorders usingBDO. In order to provide an accurate view over the pre-diction of the discovered rules, each experiment has beenperformed as a 5-fold cross validation with an 80-20 split(80% knowledge base, 20% test data). Tables 1, 2 and 3 liststhe resulted average accuracy at five different recall cut-offpoints.Within each experiment, we have used a relatively lowminimum Support of 5/N , where N is the total numberof cases, because we are interested in extracting both fre-quent and occasional associations. Every rule was able tocontribute to the voting. Controlling the number of rulesusing any minimum interestingness threshold can biasthe voting and hence, the overall result. Consequently, wehave not used this parameter to control the number ofrules. Finally, we have used a maximum itemset size of 10as the computational cost increases exponentially with theitemset size in the association rule mining process.ResultsIn this section we present and discuss the experimentalresults achieved using traditional and semantic interest-ingness measures. We start with the semantic similarityproposed in the previous sections and then compare itsresults against a series of classic semantic similarity mea-sures.Proposed semantic similarity metricIn order to observe the quality improvements broughtby semantic interestingness measures over the traditionalones, we have evaluated the discovered rules against realworld patient data. As already mentioned, we performedtwo sets of experiments. Firstly, we have compared andevaluated different traditional interestingness measures.Then, we performed the same experiment but by usingsemantic interestingness measures. This has enabled us toperform an overall comparison between the two types ofmeasures.Table 1 lists the experimental results for the traditionalmeasures. A first observation is that Confidence has theoverall best behaviour. At any recall cut-off point greaterthan 2 (K > 1) Confidence outperforms or scores simi-larly to the other measures. For example, it achieves anaccuracy of 46.58% for K = 2 and 53.42% for K = 3, bothwith 1.37% higher than the second scoring measure, Jac-card. The only exception appears for K = 1, where Jaccardoutperforms Confidence by 2.74%. A second, interesting,observation is that with the increase in the recall cut-offpoint, the measures reach a common ground, and hence,achieve the same performance  for K = 5, six of the sevenmeasures score the same accuracy (57.53%).Each of the measures we have considered in our exper-iments studies certain properties of the data. Conse-quently, the above-listed results enable us to reach abetter understanding of the underlying nature of the rela-tionships manifested by the data in our bone dysplasiaannotated dataset. For example, Confidence measures thelevel of causality (implication), while Jaccard measures thedegree of overlap among the given sets, or in our casespatient phenotypes. This leads to the conclusion that thebone dysplasia data seems to be governed more by causal-ity and overlap, rather than, for example, co-occurrence,which is described by Lift.Table 1 Experimental results on finding the quality of association rules, discovered using traditional interestingnessmeasuresTraditional Accuracy Accuracy Accuracy Accuracy Accuracyinterestingness measures K = 1 K = 2 K = 3 K = 4 K = 5Confidence 28.77 46.58 53.42 54.79 57.33Lift 26.03 36.99 42.47 49.32 57.53Conviction 28.77 43.84 46.58 49.32 57.53Correlation coefficient 27.40 36.99 45.21 52.05 57.53Cosine 28.76 43.84 49.31 54.79 58.90Jaccard 31.51 45.21 52.05 54.79 57.53Leverage 24.66 35.62 46.58 54.79 57.53The voting strategy has been used as classification method and the association rules have been used as background knowledge.Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 9 of 13http://www.jbiomedsem.com/content/5/1/8Table 2 Experimental results on finding the quality of association rules, discovered using semantic interestingnessmeasuresSemantic Accuracy Accuracy Accuracy Accuracy AccuracyInterestingness measures K = 1 K = 2 K = 3 K = 4 K = 5Semantic confidence 31.51 49.32 57.53 61.64 64.38Semantic lift 27.40 38.36 47.95 57.53 61.64Semantic conviction 32.88 43.84 53.42 56.16 58.90Semantic correlation coefficient 23.29 38.36 45.21 57.53 64.38Semantic cosine 31.51 47.95 52.05 57.53 61.64Semantic jaccard 34.25 46.58 56.16 61.64 64.38Semantic leverage 26.02 36.99 53.42 58.90 63.01The voting strategy has been used as classification method and the association rules have been used as background knowledge.Table 2 lists the experimental results for the semanticinterestingness measures. We can easily observe that theresults follow the same trend as in the previous exper-iment. Semantic Confidence has, again, an overall bestbehaviour for K > 1, outperforming Semantic Jaccard with1.37% for K = 2 (49.32%) and K = 3 (57.53%) and achievingthe same accuracy for K = 4 (61.64%) and K = 5 (64.38%).Semantic Jaccard achieves a better accuracy for K = 1,i.e., 34.25%, with 2.74% higher than Semantic Confidence.Finally, as in the previous experiment, we observe that theincrease in the recall cut-off point leads to a more uniformaccuracy across all measures, although slightly less alignedas they do not achieve the exact same accuracy.A comparative overview of the two types of measures ispresented in Table 3, where we can observe that semanticmeasures achieve better results than the traditional ones.Furthermore, the increase in the recall cut-off point leadsto a bigger difference in accuracy, from 2.74% for K = 1 to6.85% for K = 5.The main reason behind the increase in accuracy is theuse of similarity matching between terms. For instance,an ESDN patient diagnosed with Achondroplasia had thefollowing phenotypes: Rhizomelic short stature,Muscularhypotonia, Hypoplasia involving bones of the extremitiesandMalar flattening. The classifier using traditional con-fidence measures was not able to classify correctly thiscase, while the classifier using semantic confidence did.The semantic similarity employed by the latter foundan association between Rhizomelic short stature andAchondroplasia based on the more generic Short staturephenotype, which is common in Achondroplasia. ThisTable 3 Comparative overview of the experimental resultsachieved by the traditional and semantic interestingnessmeasuresInterestingness Accuracy Accuracy Accuracy Accuracy Accuracymeasures K = 1 K = 2 K = 3 K = 4 K = 5Traditional 28.77 46.58 53.42 54.79 57.53Semantic 31.51 49.32 57.53 61.64 64.38represents a clear example where the exact matching usedby traditional classifiers fails. Another similar instancewas in the case of a MED patient that exhibited thefollowing phenotypes: Pes planus (i.e., flat feet), Rhi-zomelic shortening and Frontal bossing. As in the previousexample, the classifier using traditional confidence failedto classify this instance correctly, while the one usingsemantic confidence did, based on the semantic similar-ity between Pes planus and the diverse feet abnormalitiesthat characterise MED.In order to have an accurate view over the classifica-tion results, we have checked the statistical significanceof the increase in accuracy at recall cut-off point 5. Thepurpose of this statistical significance testing was to assessthe performance of the classification using semantic rulesagainst the performance of the classification using tradi-tional rules, both on the ESDN dataset. Such a test wouldvalidate the observed increase in accuracy of 6.85% andwould show that it has not been obtained by chance.Since the comparison is between two differentapproaches on a single domain (skeletal dysplasias), wehave used the McNemars Chi-squared test with conti-nuity correction [22]. The null hypothesis was that thenumber of patient cases correctly classified by the classi-fier using semantic confidence but not by the one usingtraditional confidence is equal to the number of patientcases correctly classified by the classifier using traditionalconfidence but not by the one using semantic confidence.Table 4 shows the distribution of the 394 patient casesused in our experimental classification setting: (i) 205patient cases were correctly classified by both classifiers;(ii) 118 patient cases were misclassified by both classifiers;(iii) 51 patient cases were correctly classified using seman-tic confidence; and (iv) 20 patient cases were correctlyclassified using traditional confidence. From this data, theMcNemar test statistic with continuity correction is:?2McNemar =(|51? 20| ? 1)251+ 20 = 12.67 (22)Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 10 of 13http://www.jbiomedsem.com/content/5/1/8Table 4 Distribution of classification results in theMcNemars statistical significance testSemantic confidence basedclassifierPositive Negative TotalTraditional confidencebased classifierPositive 205 20 225Negative 51 118 169Total 256 138A McNemar test value of 12.67 corresponds to a p-value of 0.00037157, which provides strong evidence toreject the null hypothesis. We can, hence, conclude thatthe semantic interestingness measures we have proposedare able, with the help of the underlying domain ontolo-gies, to take advantage of the similarity matching betweenthe terms in the skeletal dysplasia domain.Classic semantic similarity metricsIn order to understand the role carried by the semanticsimilarity metric in the classification based on semanticinterestingness we have experimented with three classicsemantic similarities, defined earlier in the paper: Resnik,Lin and Wu & Palmer. The results achieved by each ofthese metrics are discussed below.Table 5 lists the experimental results achieved by thesemantic interestingness measures employing Resnik assemantic similarity. A first observation is that all measureshave performed uniformly, while from a comparative per-spective, they performed worse than exact matching andour proposed semantic similarity method. As in the previ-ous experiments, we observe that the increase in the recallcut-off point leads to a more uniform accuracy acrossall measures. The Resnik semantic similarity method isprimarily dependent on the frequency of the most infor-mative common ancestors. If any of the ancestors does notexist in the corpus, the similarity value becomes infinity,i.e., the concepts under scrutiny are completely dissimilar.In the case of our dataset, this is the main issue behindthe failure of the Resnik semantic similarity  being areal-world dataset, most patient cases will feature con-crete (very specific) phenotypes, while common ancestorsrepresent more generic/abstract concepts rarely found inclinical summaries. For example, the semantic similarityof Dolichocephaly and Full cheeks is ?, due to the factthat the frequency of all their ancestors (Abnormality ofthe head, Abnormality of head and neck and Phenotypeabnormality) in the patient cases is 0.The experimental results for the semantic interesting-ness measures using the second semantic similarity Lin  have led 0% accuracy on all measures and all fiverecall cut-off points  consequently we have have includedthem in a table. As in the case of Resnik, Lin is also heavilydependent on the IC of the common ancestors, and hencesuffers from the same issue discussed above. Anotherproblematic aspect of the Lin measure is that, in the con-text of the ESDNdata, it assigns higher similarity values topartial matches than to exact matches. A similarity valueof 1 is achieved when the concepts being measured arethe exact same  e.g., Short long bones. However, whenthe concepts are different and any of their ancestors ispresent in the underlying corpus, the similarity value will,usually, be greater than 1. This is because the frequencyof the ancestors (more abstract concepts) will be less thanthe frequency of the actual concepts and IC is inverselyproportional to frequency.For instance, the semantic similarity value betweenMacrocephaly and Hypoplasia involving bones of theextremities is 2.19 because the frequency of their mostinformative common ancestor  Abnormality of the skele-tal system is less than that of both concepts. The latteroccurs only 5 times in the corpus whereas MacrocephalyandHypoplasia involving bones of the extremities occur 41and 70 times, respectively. The Resnik measure is able toavoid this issue by treating exact and partial matches in thesamemanner  i.e., directly and only via the IC of themostinformative common ancestor and not by further diving itby the IC of the actual concepts. In an ideal scenario, exactmatches should assign higher similarity values that partialmatches.Table 5 Experimental results on finding the quality of association rules discovered using semantic Interestingnessmeasures that employed Resnik as semantic similaritymethodSemantic interestingness measures Accuracy Accuracy Accuracy Accuracy Accuracy(Employing Resnik) K = 1 K = 2 K = 3 K = 4 K = 5Semantic confidence 5.48 6.85 9.59 10.96 10.96Semantic lift 5.48 8.22 9.59 9.59 10.96Semantic conviction 2.74 6.85 9.59 9.59 10.96Semantic correlation coefficient 5.48 8.22 9.59 9.59 10.96Semantic cosine 5.48 8.22 9.59 10.96 10.96Semantic jaccard 5.48 8.22 9.59 9.59 10.96Semantic leverage 5.48 8.22 9.59 9.59 10.96Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 11 of 13http://www.jbiomedsem.com/content/5/1/8Finally, Table 6 lists the experimental results for thesemantic interestingness measures using the last seman-tic similarity  Wu & Palmer. We can observe thatthe results follow fairly closely the trend present in ourexperiments with the traditional interestingness mea-sures and the semantic interestingness measures employ-ing our proposed metric. Similarly to those results,there is an increase in accuracy with the increase inthe recall cut-off point, which also leads to a moreuniform accuracy across all measures. Semantic Con-fidence has an overall best behaviour for K > 1,while Semantic Leverage achieves a better accuracy forK = 1, i.e., 23.29%, with 2.74% higher than SemanticConfidence.TheWu & Palmer similarity score ranges between 0 and1, with 1 denoting an exact match and the rest of the val-ues being assigned based on the depth in the hierarchy anddistance between the concepts. This is the main reasonbehind its good performance  i.e., it uses only struc-tural distances instead of information content. It is, how-ever, biased more towards depth than the actual distancebetween concepts, or more concretely it is influenced bythe depth of the common ancestor of the concepts. Inthe case of out dataset, and using HPO as backgroundknowledge, this represents an issue because most com-mon ancestors are located at fairly uniform depths (dueto the inherent specificity of the terms) and, as such,do not provide enough variety for the final similarityscore.In conclusion, none of the classic semantic similari-ties perform better than the approach we have proposed:node-based similarities are heavily influenced by the pres-ence, or more precisely absence, of the common ancestorin the dataset (which leads to complete dissimilarity),while the edge-based similarity we have experimentedwith focuses more on the depth of the common ancestor,as opposed to the distance between the concepts, whichis more appropriate given our dataset and backgroundknowledge.Discussion and conclusionsMain findingsIn conclusion, based on the annotated bone dysplasiadataset, Confidence appears to be the best interesting-ness measure regardless of way in which is computed, i.e.,traditional or semantic. The use of semantics provides amarginal, but consistent, improvement in accuracy overtraditional measures. Since the semantic similarity relieson the structure of the underlying ontology, this improve-ment is heavily dependent on the reflection provided bythe domain ontology over the real domain knowledge.Limitations and generalisationEvery domain is governed by a set of rules. A good seman-tic similarity measure needs to take into account therules of the target domain. In our case, we have pro-posed and used two particular similarity measures, onetailored on the knowledge externalised by HPO and oneon the structure of bone dysplasias, provided by BDO.These semantic similarity measures are not necessarilydirectly applicable to other domains. Consequently, whilethe definition of semantic support is generic, in order toapply our approach in a different domain, an investigationis required to determine the most appropriate semanticsimilarity for that domain.RelatedworkThe literature contains a number of studies on usingassociation rule mining to identify relationships amongmedical attributes using biomedical ontologies [23-26].Kumar et al. [23] used association rules to indicatedependence relationships between Gene Ontology termsusing an annotation dataset and background knowl-edge. Myhre et al. [24], on the other hand, have focusedentirely on proposing an additional gene ontology layervia discovering cross-ontology association rules from GOannotations. However, none of these approaches use thebiomedical ontologies and, in particular, their hierarchicalstructure to compute interestingness measures. AnotherTable 6 Experimental results on finding the quality of association rules discovered using semantic Interestingnessmeasures that employedWu & Palmer as semantic similaritymethodSemantic interestingness measures Accuracy Accuracy Accuracy Accuracy Accuracy(EmployingWu and Palmer) K = 1 K = 2 K = 3 K = 4 K = 5Semantic confidence 20.55 35.62 36.99 42.47 54.79Semantic lift 13.70 26.03 28.77 39.73 52.05Semantic conviction 16.44 24.66 26.03 34.25 52.05Semantic correlation coefficient 20.55 28.77 32.88 39.73 43.84Semantic cosine 21.92 32.88 34.25 42.47 54.79Semantic jaccard 20.55 35.62 38.36 41.10 54.79Semantic leverage 23.29 30.14 32.88 38.36 45.21Paul et al. Journal of Biomedical Semantics 2014, 5:8 Page 12 of 13http://www.jbiomedsem.com/content/5/1/8set of existing research on applying association rule min-ing to biomedical ontologies includes studies on miningsingle level, multi-level and cross-ontology associationrules [27-29]. Carmona-Saez et al. [27], for example, minesingle level associations between GO annotations andexpressed genes from microarray data integrated withGO annotation information. However, as in the previouscase, the inherent information provided by the ontologystructure is not considered when computing the interest-ingness measures, and hence limit, to some extent, theknowledge discovered.Interestingnessmeasures play an essential role by reduc-ing the number of discovered rules and retaining onlythose with the best utility, in a post-processing step. Dif-ferent rule interestingness measures have different qual-ities or flaws. There is no optimal measure and one wayto solve this challenge is to try to find a good compro-mise. Research has been performed on finding optimalmeasures for different datasets [8,9], but by taking intoaccount only traditional interestingness measures.In summary, prior efforts in association rule miningapplied to datasets annotated with biomedical ontologyconcepts focus on mining normal, cross-ontology andmulti-level association rules, but leave out the use of thesemantic relationships between the target concepts fromthe computation of the interestingness measures.ConclusionConcepts defined and described by biomedical ontologies,e.g., the Human Phenotype Ontology, enable us to com-pare medical terms at a semantic level  a comparisonthat is otherwise not possible. Our research has focusedon the use of semantic relationships between patient phe-notypes, annotated by HPO entities, in the process ofmining association rules. In this manuscript, we have pro-posed a method that integrates concept similarity metricsinto the computation of traditional interestingness mea-sures, with application to finding association rules in thebone dysplasia domain. This method has been applied onan annotated patient dataset and used domain-specificsemantic similarities.Experimental results have led to the conclusion that, forour domain, Confidence is the most accurate measure,independently on the underlying computation method,i.e., traditional or semantic. On the other hand, SemanticConfidence was able to take advantage of structure of thedomain ontologies and of the custom semantic similarityto achieve better results (up to 6.85% better accuracyover the traditional Confidence). In conclusion, theseresults suggest that, given an appropriate domain-specificontology, semantic similarities are able to improve theefficiency of traditional interestingness measures in theassociation rule discovery process, hence enabling avaluable semantic interestingness measures framework.Competing interestsThe authors declare that they have no competing interests.Authors contributionsJH and AZ formulated the basic idea behind SKELETOME. JH coordinates theproject. TG leads the development of the project. RP and TG designed theexperiments. RP run the experiments. RP and TG analysed the experimentalresults. AZ provided the domain expertise. RP and TG wrote the manuscript.JH and AZ edited the manuscript. All authors read and approved the finalmanuscript.AcknowledgementsWe gratefully acknowledge the editor and anonymous reviewers whosecomments and advices have helped us improve our manuscript. This researchis funded by the Australian Research Council (ARC) under the Linkage grantSKELETOME  LP100100156 and the Discovery Early Career Researcher Award(DECRA)  DE120100508.Author details1School of ITEE, The University of Queensland, St. Lucia, Queensland 4072,Australia. 2Bone Dysplasia Research Group, UQ Centre for Clinical Research(UQCCR), The University of Queensland, Herston, Queensland 4006, Australia.3Genetic Health Queensland, Royal Brisbane and Womens Hospital, Herston,Queensland 4006, Australia.Received: 22 February 2013 Accepted: 21 January 2014Published: 5 February 2014JOURNAL OFBIOMEDICAL SEMANTICSHettne et al. Journal of Biomedical Semantics 2014, 5:41http://www.jbiomedsem.com/content/5/1/41RESEARCH Open AccessStructuring research methods and data with theresearch object model: genomics workflows as acase studyKristina M Hettne1, Harish Dharuri1, Jun Zhao3, Katherine Wolstencroft2,6, Khalid Belhajjame2, Stian Soiland-Reyes2,Eleni Mina1, Mark Thompson1, Don Cruickshank3, Lourdes Verdes-Montenegro5, Julian Garrido5, David de Roure3,Oscar Corcho4, Graham Klyne3, Reinout van Schouwen1, Peter A C t Hoen1, Sean Bechhofer2, Carole Goble2and Marco Roos1*AbstractBackground: One of the main challenges for biomedical research lies in the computer-assisted integrative study oflarge and increasingly complex combinations of data in order to understand molecular mechanisms. The preservationof the materials and methods of such computational experiments with clear annotations is essential for understandingan experiment, and this is increasingly recognized in the bioinformatics community. Our assumption is that offeringmeans of digital, structured aggregation and annotation of the objects of an experiment will provide necessarymeta-data for a scientist to understand and recreate the results of an experiment. To support this we exploreda model for the semantic description of a workflow-centric Research Object (RO), where an RO is defined as aresource that aggregates other resources, e.g., datasets, software, spreadsheets, text, etc. We applied this modelto a case study where we analysed human metabolite variation by workflows.Results: We present the application of the workflow-centric RO model for our bioinformatics case study.Three workflows were produced following recently defined Best Practices for workflow design. By modelling theexperiment as an RO, we were able to automatically query the experiment and answer questions such as whichparticular data was input to a particular workflow to test a particular hypothesis?, and which particular conclusionswere drawn from a particular workflow?.Conclusions: Applying a workflow-centric RO model to aggregate and annotate the resources used in a bioinformaticsexperiment, allowed us to retrieve the conclusions of the experiment in the context of the driving hypothesis, theexecuted workflows and their input data. The RO model is an extendable reference model that can be used by othersystems as well.Availability: The Research Object is available at http://www.myexperiment.org/packs/428The Wf4Ever Research Object Model is available at http://wf4ever.github.io/roKeywords: Semantic web models, Scientific workflows, Digital libraries, Genome wide association study* Correspondence: m.roos@lumc.nl1Department of Human Genetics, Leiden University Medical Center, Leiden,The NetherlandsFull list of author information is available at the end of the article© 2014 Hettne et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited.Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 2 of 16http://www.jbiomedsem.com/content/5/1/41BackgroundOne of the main challenges for biomedical research lies inthe integrative study of large and increasingly complexcombinations of data in order to understand molecularmechanisms, for instance to explain the onset and pro-gression of human diseases. Computer-assisted method-ology is needed to perform these studies, posing newchallenges for upholding scientific quality standards forthe reproducibility of science. The aim of this paper is todescribe how the research data, methods and metadatarelated to a workflow-centric computational experimentcan be aggregated and annotated using standard SemanticWeb technologies, with the purpose of helping scientistsperforming such experiments in meeting requirements forunderstanding, sharing, reuse and repurposing.The workflow paradigm is gaining ground in bioinfor-matics as the technology of choice for recording the stepsof computational experiments [1-4]. It allows scientists todelineate the steps of a complex analysis and exposethis to peers using workflow design and execution toolssuch as Taverna [5], and Galaxy [6], and workflow sharingplatforms such as myExperiment [7] and crowdLabs [8].In a typical workflow, data outputs are generated fromdata inputs via a set of (potentially distributed) computa-tional tasks that are coordinated following a workflowdefinition. However, workflows do not provide a completesolution for aggregating all data and all meta-data that arenecessary for understanding the full context of an experi-ment. Consequently, scientists often find it difficult (orimpossible) to reuse or repurpose existing workflowsfor their own analyses [9]. In fact, insufficient meta-datahas been listed as one of the main causes of workflowdecay in a recent study of Taverna workflows on myEx-periment [9]. Workflow decay is the term used whenthe ability to re-execute a workflow after its inceptionhas been compromised.We will be able to better understand scientific work-flows if we are able to capture more relevant data andmeta-data about them; including the purpose and contextof the experiment, sample input and output datasets, andthe provenance of workflow executions. Moreover, if wewish to publish and exchange these resources as a unit,we need a mechanism for aggregation and annotation thatwould work in a broad scientific community. SemanticWeb technology seems a logic choice of technology, givenits focus on capturing the meaning of data in a machinereadable format that is extendable and supports intero-perability. It allows defining a Web-accessible referencemodel for the annotation of the aggregation and theaggregated resources that is independent of how dataare stored in repositories. Examples of other efforts whereSemantic Web technology has been used for the biomed-ical data integration includes the Semantic Enrichment ofthe Scientific Literature (SESL) [10] and Open PHACTS[11] projects. We applied the recently developed ResearchObject (RO) family of tools and ontologies [12,13] to pre-serve the scientific assets and their annotation relatedto a computational experiment. The concept of the ROwas first proposed as an abstraction for sharing researchinvestigation results [14]. Later, the potential role for ROsin facilitating not only the sharing but also the reuse ofresults, in order to increase the reproducibility of theseresults, was envisioned [15]. Narrowing down to workflow-centric ROs, preservation aspects were explored in [16],and their properties as first class citizen structures thataggregate resources in a principled manner in [13]. We alsoshowed the principle of describing a (text mining) workflowexperiment and its results by Web Ontology Language(OWL) ontologies [17]. The OWL ontologies were custom-built, which we argue is now an unnecessary bottleneck forexchange and interoperability. These studies all contributedto the understanding and implementation of the conceptof an RO, but the data used were preliminary, and thestudies were focused on describing workflows with relateddatasets and provenance information, rather than fromthe viewpoint of describing a scientific experiment of whichworkflows are a component.A workflow-centric RO is defined as a resource thataggregates other resources, such as workflow(s), proven-ance, other objects and annotations. Consequently, an ROrepresents the method of analysis and all its associatedmaterials and meta-data [13,15], distinguishing it fromother work mainly focusing on provenance of researchdata [18,19]. Existing Semantic Web frameworks are used,such as (i) the Object Exchange and Reuse (ORE) model[20]; (ii) the Annotation Ontology (AO) [21]; and (iii)the W3C-recommended provenance exchange models[22]. ORE defines the standards for the description andexchange of aggregations of Web resources and providesthe basis for the RO ontologies. AO is a general modelfor annotating resources and is used to describe the ROand its constituent resources as well as the relationshipsbetween them. The W3C provenance exchange modelsenable the interchange of provenance information on theWeb, and the Provenance Ontology (PROV-O) forms thebasis for recording the provenance of scientific workflowexecutions and their results.In addition, we used the minimal information modelMinim, also in Semantic Web format, to specify whichelements in an RO we consider must haves, shouldhaves and could haves according to user-defined re-quirements [23]. A checklist service subsequently queriesthe Minim annotations as an aid to make sufficientlycomplete ROs [24]. The idea of using a checklist to per-form quality assessment is inspired by related checklist-based approaches in bioinformatics, such as the MinimumInformation for Biological and Biomedical Information(MIBBI)-style models [25].Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 3 of 16http://www.jbiomedsem.com/content/5/1/41Case study: genome wide association studiesAs real-world example we aggregate and describe theresearch data, methods and metadata of a computationalexperiment in the context of studies of genetic variationin human metabolism. Given the potential of geneticvariation data in extending our understanding of geneticdiseases, drug development and treatment, it is crucialthat the steps leading to new biological insights can beproperly recorded and understood. Moreover, bioinformat-ics approaches typically involve aggregation of disparateonline resources into complex data parsing pipelines.This makes this a fitting test case for an instantiatedRO. The biological goal of the experiment is to aid inthe interpretation of the results of a Genome-WideAssociation Study (GWAS) by relating metabolic traitsto the Single Nucleotide Polymorphisms (SNPs) that wereidentified by the GWAS. GWA studies have successfullyidentified genomic regions that dispose individuals todiseases (see for example [26], for a review see [27]).However, the underlying biological mechanisms oftenremain elusive, which led the research community toevince interest in genetic association studies of metaboliteslevels in blood (see for example [28-30]). The motivation isthat the biochemical characteristics of the metabolite andthe functional nature of affected genes can be combined tounravel biological mechanisms and gain functional insightinto the aetiology of a disease. Our specific experiment in-volves mining curated pathway databases and a specific textmining method called concept profile matching [31,32].In this paper we describe the current state of RO ontol-ogies and tools for the aggregation and annotation of acomputational experiment that we developed to elucidatethe genetic basis for human metabolic variation.MethodsWe performed our experiment using workflows developedin the open source Taverna Workflow Management Systemversion 2.4 [5]. To improve the understanding of the ex-periment, we have added the following additional resourcesto the RO, using the RO-enabled myExperiment [33]: 1)the hypothesis or research question (what the experimentwas designed to test); 2) a workflow-like sketch of the over-all experiment (the overall data flow and workflow aims); 3)one or more workflows encapsulating the computationalmethod; 4) input data (a record of the data that were usedto reach the conclusions of an experiment); 5) provenanceof workflow runs (the data lineage paths built from theworkflow outputs to the originating inputs); 6) the results(a compilation of output data from workflow runs); 7) theconclusions (interpretation of the results from the work-flows against the original hypothesis). Such an RO was thenstored in the RO Digital Library [34]. RO completenessevaluation is checked from myExperiment with a toolimplementing the Minim model [24]. Detailed descriptionof the method follows.Workflow developmentWe developed three workflows for interpreting SNP-metabolite associations from a previously publishedgenome-wide association study, using pathways fromthe KEGG metabolic pathway database [35] and GeneOntology (GO) [36] biological process associations fromtext mining of PubMed. To understand an association of aSNP with a metabolite, researchers would like to knowthe gene in the vicinity of the SNP that is affected by thepolymorphism. Then, researchers examine the functionalnature of the gene and evaluate if it makes sense given thebiochemical characteristics of the metabolite with whichit is associated. This typically involves interrogation ofbiochemical pathway databases and mining existing litera-ture. We would like to evaluate the utility of backgroundknowledge present in the databases and literature infacilitating a biological interpretation of the statisticallysignificant SNP-metabolite pairs. We do this by firstdetermining the genes closest to the SNPs, and thenreporting the pathways that these genes participate in.We implemented two main workflows for our experi-ment. The first one mines the manually curated KEGGdatabase of metabolic pathway and gene associationsthat are available via the KEGG REST Services [37].The second workflow mines the text-mining based data-base of associations between GO biological processesand genes behind the Anni 2.1 tool [31] that are availablevia the concept profile mining Web services [38]. We alsocreated a workflow to list all possible concept sets inthe concept profile database, to encourage reuse of theconcept profile-based workflow for matching againstother concept sets than GO biological processes. Theworkflows were developed following the 10 Best Prac-tices for workflow design [39]. The Best Practices weredeveloped to encourage re-use and prevent workflowdecay, and briefly consists of the following steps:1) Make a sketch workflow to help design theoverall data flow and workflow aims, and toidentify the tools and data resources required ateach stage. The sketch could be created using forexample flowchart symbols, or empty beanshellsin Taverna.2) Use modules, i.e. implement all executablecomponents as separate, runnable workflows tomake it easier for other scientists to reuse parts of aworkflow at a later date.3) Think about the output. A workflow has thepotential to produce masses of data that need to bevisualized and managed properly. Also, workflowscan be used to integrate and visualise data as well asHettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 4 of 16http://www.jbiomedsem.com/content/5/1/41for analysing it, so one should consider how theresults will be presented easily to the user.4) Provide input and output examples to show theformat of input required for the workflow and thetype of output that should be produced. This iscrucial for the understanding, validation, andmaintenance of the workflow.5) Annotate, i.e. choose meaningful names for theworkflow title, inputs, outputs, and for the processesthat constitute the workflow as well as for theinterconnections between the components, so thatannotations are not only a collection of static tagsbut capture the dynamics of the workflow.Accurately describing what individual services do,what data they consume and produce, and the aimsof the workflow are all essential for use and reuse.6) Make it executable from outside the localenvironment by for example using remote Webservices, or platform independent code/plugins.Workflows are more reusable if they can beexecuted from anywhere. If there is need to use localservices, library or tools, then the workflow shouldbe annotated in order to define its dependencies.7) Choose services carefully. Some services are morereliable or more stable than others, and examiningwhich are the most popular can assist with thisprocess.8) Reuse existing workflows by for example searchingcollaborative platforms such as myExperiment forworkflows using the same Web service. If a workflowhas been tried, tested and published, then reusing itcan save a significant amount of time and resource.9) Test and validate by defining test cases andimplementing validation mechanisms in order tounderstand the limitations of workflows, and tomonitor changes to underlying services.10)Advertise and maintain by publishing the workflowon for example myExperiment, and performingfrequent testing of the workflow and monitoring ofthe services used. Others can only reuse it if it isaccessible and if it is updated when required, due tochanges in underlying services.The RO core modelThe RO model [12,13] aims at capturing the elementsthat are relevant for interpreting and preserving theresults of scientific investigations, including the hypoth-esis investigated by the scientists, the data artefacts usedand generated, as well as the methods and experimentsemployed during the investigation. As well as these ele-ments, to allow third parties to understand the contentof the RO, the RO model caters for annotations thatdescribe the elements encapsulated by the ROs, as wellas the RO as a whole. Therefore, two main constructsare at the heart of the RO model, namely aggregationand annotation. The work reported on in this articleuses version 0.1 of the RO model, which is documentedonline [12].Following myExperiment packs [7], ROs use the OREmodel [20] to represent aggregation. Using ORE, an ROis defined as a resource that aggregates other resources,e.g., datasets, software, spreadsheets, text, etc. Specifically,the RO extends ORE to define three new concepts: i) ro:ResearchObject is a sub-class of ore:Aggregation whichrepresents an aggregation of resources. ii) ro:Resourceis a sub-class of ore:AggregatedResource representing aresource that is aggregated within an RO. iii) ro:Manifestis a sub-class of ore:ResourceMap, representing a resourcethat is used to describe the RO.To support the annotation of ROs, their constituentresources, as well as their relationship, we use the Anno-tation Ontology [21]. Several types of annotations aresupported by the Annotation Ontology, e.g., comments,textual annotations (classic tags) and semantic annotations,which relate elements of the ROs to concepts from under-lying domain ontologies. We make use of the followingAnnotation Ontology terms: i) ao:Annotation, which actsas a handle for the annotation. ii) ao:annotatesResource,which represents the resource(s)/RO(s) subjects to anno-tation. iii) ao:body, which describes the target of the anno-tation. The body of the annotation takes the form of a setof Resource Description Framework (RDF) statements.Note that it is planned for later revisions of the RO modelto use the successor of AO, the W3C Community OpenAnnotation Data Model (OA) [40]. For our purposes, OAannotations follows a very similar structure using oa:Annotation, oa:hasTarget and oa:hasBody.Support for workflow-centric ROsA special kind of ROs that are supported by the model iswhat we call workflow-centric ROs, which, as indicated bythe name, refer to those ROs that contain resources thatare workflow specifications. The structure of the workflowin ROs is detailed using the wfdesc vocabulary [41], and isdefined as a graph in which the nodes refers to steps inthe workflow, which we call wfdesc:Process, and the edgesrepresenting data flow dependencies, wfdesc:DataLink,which is a link between the output and input parameters(wfdesc:Parameter) of the processes that compose theworkflow. As well as the description of the workflow,workflow centric ROs support the specification of theworkflow runs, wfprov:WorkflowRun, that are obtainedas a result of enacting workflows. A workflow run isspecified using the wfprov ontology [42], which cap-tures information about the input used to feed theworkflow execution, the output results of the workflowrun, as well as the constituent process runs, wfprov:ProcessRun, of the workflow run, which are obtainedHettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 5 of 16http://www.jbiomedsem.com/content/5/1/41by invoking the workflow processes, and the input andoutputs of those process runs.Support for domain-specific informationA key aspect of the RO model design is the freedom touse any vocabulary. This allows for inclusion of verydomain-specific information about the RO if that servesthe desired purpose of the user. We defined new termsunder the name space roterms [43]. These new termsserve two main purposes. They are used to specify anno-tations that are, to our knowledge, not catered for byexisting ontologies, e.g., the classes roterms:Hypothesisand roterms:Conclusion to annotate the hypothesis andconclusions part of an RO, and the property roterms:exampleValue to annotate an example value for a giveninput or output parameter given as an roterms:Work-flowValue instance. The roterms are also used to specifyshortcuts that make the ontology easy to use and moreaccessible. For example, roterms:inputSelected associatesa wfdesc:WorkflowDefinition to an ro:Resource to statethat a file is meant to be used with a given workflowdefinition, without specifying at which input port or inwhich workflow run.Minim model for checklist evaluationWhen building an RO in myExperiment users are pro-vided with a mechanism of quality insurance by ourso-called checklist evaluation tool, which is built uponthe Minim checklist ontology [23,44] and definedusing Web Ontology Language. Its basic function is toassess that all required information and descriptionsabout the aggregated resources are present and complete.Additionally, according to explicit requirements defined ina checklist, the tool can also assess the accessibility ofthose resources aggregated in an RO, in order to increasethe trust on the understanding of the RO. The MinimFigure 1 An overview of the Minim model. An overview of the four commodel has four key components, as illustrated by Figure 1:1) a Constraint, which associates a model (checklist) touse with an RO, for a specific assessment purpose, e.g.reviewing an RO containing sufficient information beforebeing shared; 2) a Model, which enumerates of the set ofrequirements to be considered, which may be declared atlevels of MUST, SHOULD or MAY be satisfied for themodel as a whole; 3) a Requirement, which is the key partfor expressing the concrete quality requirements to anRO, for example, the presence of certain informationabout an experiment, or liveness (accessibility) of a dataserver; 4) a Rule, which can be a SoftwareRequiremen-tRule, to specify the software to be present in the operat-ing environment, a ContentMatchRequirementRule, tospecify the presence of certain pattern in the assesseddata, or a DataRequirementRule, for specifying data re-source to be aggregated in an RO.RO digital libraryWhile myExperiment acts mainly as front-end to users,the RO Digital Library [34] acts as a back-end, with twocomplementary storage components: a digital repositoryto keep the content, as a triple store to manage themeta-data content. The ROs in the repository can beaccessed via a Restful API [45] or via a public SPARQLendpoint [46]. All the ROs created in the myExperiment.org are also submitted to the RO Digital Library.Workflow-centric RO creation processBelow we describe the steps that we conducted whencreating the RO for our case study in an RO-enabledversion of myExperiment [33]. The populated RO isintended to contain all the information required to re-run the experiment, or understand the results presented,or both.ponents: a constraint, a model, a requirement, and a rule.Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 6 of 16http://www.jbiomedsem.com/content/5/1/41Creating an ROThe action of creating an RO consists of generating thecontainer for the items that will be aggregated, and get-ting a resolvable identifier for it. In myExperiment theaction of creating an RO is similar to creating a pack.We filled in a title and description of the RO at the pointof creation and got a confirmation that the RO had beencreated and had been assigned a resolvable identifier inthe RO Digital Library (Figure 2).Adding the experiment sketchUsing a popular office presentation tool, we made anexperiment sketch and saved it as a PNG image. Wethen uploaded the image to the pack, selecting the typeSketch. As a result, the image gets stored in the DigitalFigure 2 Screenshots from myExperiment illustrating the process of cbutton the user can enter a title and description (A), while pressing the cridentifier (B).Library and aggregated in the RO. In addition, an anno-tation was added to the RO to specify that the image isof type Sketch. A miniature version of the sketch isshown within the myExperiment pack (Figure 3).Adding the hypothesisTo specify the hypothesis, we created a text file thatdescribes the hypothesis, and then upload it to the packas type Hypothesis. The file gets stored in the DigitalLibrary and aggregated in the RO, this time annotated tobe of type Hypothesis.Adding workflowsWe saved the workflow definitions to files and uploadedthem to the pack as type Workflow. MyExperiment thenreating a Research Object placeholder. Before pressing the createeate button will result in a placeholder Research Object with anFigure 3 Workflow sketch. A workflow sketch showing that our experiment follows two paths to interpret genome wide association studyresults: matching with concept profiles and matching with KEGG pathways.Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 7 of 16http://www.jbiomedsem.com/content/5/1/41automatically performed a workflow-to-RDF transform-ation in order to extract the workflow structure accordingto the RO model, which includes user descriptions andmetadata created within the Taverna workbench. Thedescriptions and the extracted structure gets stored inthe RO Digital Library and associated with the workflowfiles as annotations.Adding the workflow input fileThe data values were stored in files that were thenuploaded into the pack as Example inputs. Such filesgets stored in the RO Digital Library and aggregated inthe RO, and as Example inputs.Adding the workflow provenanceUsing the Taverna-Prov [47] extension to Taverna, weexported the workflow run provenance to a file that weuploaded to the pack as type Workflow run. Similar toother resources, the provenance file gets stored in thedigital library with the type Workflow run, however asthe file is in the form of RDF according to the wfprov [42]and W3C PROV-O [22] ontologies, it is also integratedinto the RDF store of the digital library and available forlater querying.Adding the resultsWe made a compilation of the different workflow outputsto a result file in table format, uploaded to the pack astype Results. The file gets stored in the digital libraryand aggregated in the RO, annotated to be of the typeResults.Adding the conclusionsTo specify the hypothesis, we created a text file thatdescribes the hypothesis, and then uploaded it to thepack as type Hypothesis. The file gets stored in thedigital library and aggregated in the RO, annotated to beof type Conclusions.Intermediate step: checklist evaluationAt this point we checked how far we were from satisfy-ing the Minim model, and were informed by the toolthat the RO now fully satisfies the checklist (Figure 4).Annotating and linking the resourcesWe linked the example input file to the workflows thatused the file by the property Input_selected (Figure 5).In this particular case, both workflows have the sameinputs but they need to be configured in different ways.This is described in the workflow description field inTaverna.ResultsThe RO for our experiment is the container for theitems that we wished to aggregate. In terms of RDF, wefirst instantiated an ro:ResearchObject in an RO-enabledversion of myExperiment [33]. We thereby obtained aunique and resolvable Uniform Resource Identifier (URI)from the RO store that underlies this version of myEx-periment. In our experimental setup this was http://sandbox.wf4ever-project.org/rodl/ROs/Pack405/. It isaccessible from myExperiment [48]. Each of the subse-quent items in the RO was aggregated as an ro:Resource,indicating that the item is considered a constituentmember of the RO from the point of view of the scientist(the creator of the RO).Aggregated resourcesWe aggregated the following items: 1) the hypothesis(roterms:Hypothesis): we hypothesized that SNPs can beFigure 4 Screenshot of the results from the second check with the checklist evaluation service. The results from checklist evaluationservice show that the Research Object satisfies the defined checklist for a Research Object.Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 8 of 16http://www.jbiomedsem.com/content/5/1/41functionally annotated using metabolic pathway infor-mation complemented by text mining, and that this willlead to formulating new hypotheses regarding the role ofgenomic variation in biological processes; 2) the sketch(roterms:Sketch) shows that our experiment follows twopaths to interpret SNP data: matching with conceptprofiles and matching with Kyoto Encyclopedia of Genesand Genomes (KEGG) pathways (Figure 3); 3) the work-flows (wfdesc:Workflow): Figure 6 shows the workflowdiagram for the KEGG workflow and Figure 7 showsthe workflow diagram for the concept profile matchingworkflow. In Taverna, we aimed to provide sufficientannotation of the inputs, outputs and the functions ofFigure 5 Screenshot of the relationships in the RO in myExperiment.Research Object have been defined in myExperiment.each part of the workflow to ensure a clear interpretationand to ensure that scientists know how to replay theworkflows using the same input data, or re-run themwith their own data. We provided textual descriptionsin Taverna of each step of the workflow, in particular toindicate their purpose within the workflow (Figure 8);4) the input data (roterms:exampleValue) that we aggre-gated in our RO was a list of example SNPs derived fromthe chosen GWAS [28]; 5) the workflow run provenance(roterms:WorkflowRunBundle): a ZIP archive that containsthe intermediate values of the workflow run, together withits provenance trace expressed using wfprov:WorkflowRunand subsequent terms from the wfprov ontology. We thusThe relationships between example inputs and workflows in theFigure 6 Taverna workflow diagram for the KEGG workflow. Blue boxes are workflow inputs, brown boxes are scripts, grey boxes areconstant values, green boxes are Web services, purple boxes are Taverna internal services, and pink boxes are nested workflows.Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 9 of 16http://www.jbiomedsem.com/content/5/1/41stored process information from the input of the workflowexecution to its output results, including the informationfor each constituent process run in the workflow run,modelled as wfprov:ProcessRun. The run data is: 3 zipfiles containing 2090 intermediate values as separatefiles totalling 9.7 MiB, in addition to 5 MiB of provenancetraces; 6) the results (roterms:Result) were compiled fromthe different workflow outputs to one results file (seeresult document in the RO [49] Additional file 1). For15 SNPs it lists the associated gene name, the biologicalannotation from the GWAS publication, the associatedKEGG pathway, and the most strongly associatedFigure 7 Taverna workflow diagram for the concept profile mining winternal services, and pink boxes are nested workflows.biological process according to concept profile match-ing. Our workflows were able to compute a biologicalannotation from KEGG for 10 out of 15 SNPs and 15from mining PubMed. All KEGG annotations and mosttext mining annotations corresponded to the annotationsby Illig et al [28]. An important result of the text miningworkflow was the SNP-annotation rs7156144- stimu-lation of tumor necrosis factor production, which rep-resents a hypothetical relation that to our knowledge wasnot reported before; 7) the conclusions (roterms: Conclu-sion): we concluded that our KEGG and text mining work-flows were successful in retrieving biological annotationsorkflow. Blue boxes are workflow inputs, purple boxes are TavernaFigure 8 Taverna workflow annotation example. An example of an annotation of the purpose of a nested workflow in Taverna.Hettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 10 of 16http://www.jbiomedsem.com/content/5/1/41for significant SNPs from a GWAS experiment, and pre-dicting novel annotations.As an example of our instantiated RO, Figure 9 providesa simplified view of the RDF graph that aggregates andannotates the KEGG mining workflow. It shows the resultof uploading our Taverna workflow to myExperiment, asit initiated an automatic transformation from a Taverna 2Figure 9 Simplified diagram showing part of the Research Object foraggregated by the Research Object-enabled version of myExperiment. ShKEGG pathway mining workflow.t2flow file to a Taverna 3 workflow bundle, while extract-ing the workflow structure and user descriptions in termsof the wfdesc model [41]. The resulting RDF documentwas aggregated in the RO and used as the annotation bodyof a ao:Annotation on the workflow, thus creating a linkbetween the aggregated workflow file and its descriptionin RDF. The Annotation Ontology uses named graphs forour experiment. The Research Object contains the items that wereown is the part of the RDF graph that aggregates and annotates theHettne et al. Journal of Biomedical Semantics 2014, 5:41 Page 11 of 16http://www.jbiomedsem.com/content/5/1/41semantic annotation bodies. In the downloadable ZIParchive of an RO each named graph is available as a separ-ate RDF document, which can be useful in current RDFtriple stores that do not yet fully support named graphs.The other workflows were aggregated and annotated inthe same way. The RO model further uses commonDublin Core vocabulary terms [50] for basic metadatasuch as creator, title, and description.In some cases we manually inserted specified relationsbetween the RO resources via the myExperiment userinterface. An example is the link between input data andthe appropriate workflow for cases when an RO hasmultiple workflows and multiple example inputs. In ourcase, both workflows have the same inputs, but they needto be configured in different ways. This was described inthe workflow description field in Taverna which becomesavailable from an annotation body in the workflow uploadprocess.Checking for completeness of an RO: application of theMinim modelWe also applied Semantic Web technology for checkingthe completeness of our RO. We implemented a checklistfor the items that we consider essential or desirable forunderstanding a workflow-based experiment by annotat-ing the corresponding parts of the RO model with theappropriate term from the Minim vocabulary (Table 1).Thus, some parts were annotated as MUST have withthe property minim:hasMustRequirement (e.g. at least oneworkflow definition), and others as SHOULD have withthe property minim:hasShouldRequirement (e.g. the over-all sketch of the experiment). The complete checklistdocument can be found online in RDF format [51] and ina format based on the spreadsheet description of theworkflow [52]. We subsequently used a checklist servicethat evaluates if an RO is complete by executing SPARQLqueries on the Minim mappings. The overall result is asummary of the requirement levels associated with theTable 1 RO items checklistResearch object item Requirement RO ontology termHypothesis or ResearchquestionShould roterms: Hypothesis/roterms:Research QuestionWorkflow sketch Should roterms:SketchOne or more workflows Must wfdesc:WorkflowWeb services ofthe workflowMust wfdesc:ProcessExample input data Must roterms:exampleValueProvenance ofworkflow runsMust wfprov:WorkflowRunExample results Must roterms:ResultConclusions Must roterms:ConclusionRO items for a workflow-based experiment annotated with the appropriateterm from the Minim vocabulary.individual items; e.g. a missing MUST requirement is amore serious omission than a missing SHOULD (orCOULD) requirement. We justified the less strict require-ments for some items to accommodate cases when an ROis used to publish a method as such. We found that treat-ing the requirement levels as mutually exclusive (hencenot sub properties) simplifies the implementation ofchecklist evaluation, and in particular the generation ofresults when a checklist item is not satisfied.DiscussionIn this paper we explored the application of the SemanticWeb encoded RO model to provide a container datamodel for preserving sufficient information for researchersto understand a computational experiment. We foundthat the model indeed allowed us to aggregate the neces-sary material together with sufficient annotation (both formachines and humans). Moreover, mapping of selectedRO model artefacts to the Minim vocabulary allowedus to check if the RO was complete according to ourown predefined criteria. The checklist service can beconfigured to accommodate different criteria. Researchgroups may have different views on what is essential,but also libraries or publishers may define their ownstandards, enabling partial automation of the process ofchecking a submission against specific instructions toauthors. Furthermore, the service can be run routinelyto check for workflow decay, in particular decay relatedJOURNAL OFBIOMEDICAL SEMANTICSDietze et al. Journal of Biomedical Semantics 2014, 5:48http://www.jbiomedsem.com/content/5/1/48SOFTWARE Open AccessTermGenie  a web-application forpattern-based ontology class generationHeiko Dietze1*, Tanya Z Berardini2, Rebecca E Foulger3, David P Hill4, Jane Lomax3,David Osumi-Sutherland3, Paola Roncaglia2 and Christopher J Mungall1AbstractBackground: Biological ontologies are continually growing and improving from requests for new classes (terms) bybiocurators. These ontology requests can frequently create bottlenecks in the biocuration process, as ontologydevelopers struggle to keep up, while manually processing these requests and create classes.Results: TermGenie allows biocurators to generate new classes based on formally specified design patterns ortemplates. The system is web-based and can be accessed by any authorized curator through a web browser.Automated rules and reasoning engines are used to ensure validity, uniqueness and relationship to pre-existing classes.In the last 4 years the Gene Ontology TermGenie generated 4715 new classes, about 51.4% of all new classes created.The immediate generation of permanent identifiers proved not to be an issue with only 70 (1.4%) obsoleted classes.Conclusion: TermGenie is a web-based class-generation system that complements traditional ontologydevelopment tools. All classes added through pre-defined templates are guaranteed to have OWL equivalenceaxioms that are used for automatic classification and in some cases inter-ontology linkage. At the same time, thesystem is simple and intuitive and can be used by most biocurators without extensive training.Keywords: Ontology, Class generationBackgroundBiological ontologies such as the GeneOntology (GO) andthe Human Phenotype Ontology (HP) provide a rich set ofconstructs for describing biological entities such as genes,alleles and diseases. Like most data resources, ontologiesare rarely complete, and healthy ontologies are continuallygrowing and improving, as the state of knowledge pro-gresses. One process by which ontologies grow is fromrequests for new classes (terms) by biocurators. Theseontology requests can frequently create bottlenecks in thebiocuration process, as ontology developers struggle tokeep up with a deluge of requests.Historically the process used in projects such as the GOConsortium would be for ontology developers to workthrough a set of requests collected in an issue trackingsystem, and to manually add them to the ontology, usinga specialized Ontology Development Tool (ODT) such*Correspondence: hdietze@lbl.gov1Genomics Division, Lawrence Berkeley National Laboratory, 1 CyclotronRoad, 94720 Berkeley, CA, USAFull list of author information is available at the end of the articleas OBO-Edit [1]  see Figure 1. Sometimes the ontologydevelopers apply documented design patterns to guidethis process, particularly where collections of classes fol-low a common structure. For example, most classes in thedevelopmental process portion of the GO follow a consis-tent lexical form and relational structure as dictated in theGO developers documentation [2]. However, even withthis documentation in place, this has still largely been atime-consuming and error-prone manual process, espe-cially where ontology developers need to rearrange to theontology structure.Use of the Web Ontology Language (OWL), and inparticular providing computable definitions in the formof equivalence axioms can greatly assist in ontologydevelopment and maintenance through the use of OWLreasoners. However, reasoners do not in themselveshelp with the task of class generation. Furthermore,for many biological ontologies, the axioms necessaryfor reasoning have been added post-hoc [3,4] ratherthan prospectively at the time of class creation. Thiskind of retrospective axiomatization is inefficient but© 2014 Dietze et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproductionin any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 2 of 13http://www.jbiomedsem.com/content/5/1/48Figure 1 Conventional ontology class request workflow. General workflow for ontology class requests using a traditional issue tracker. A simpleclass request may take several days, for complex cases even longer.has in part been dictated by limitations of OBO-Edit.This can be partly circumvented by using an ODTthat fully supports OWL such as Protégé, but this toolcan be difficult for biocurators to use, and even in thehands of experts it can be time consuming to generatenew classes complete with axioms referencing externalontologies.Here we describe an application called TermGeniethat allows biocurators to generate new classes based onformally specified design patterns or templates. The sys-tem is web-based and can be accessed by any authorizedcurator through a web browser. Automated rules andreasoning engines are used to ensure validity, unique-ness and relationship to pre-existing classes. The systemmakes extensive use of OWL axioms, but can be easilyused without understanding these axioms. TermGe-nie is used extensively in the GO and is currently alsoin use for the Cell Type Ontology and for phenotypeontologies.ImplementationTo minimize the entrance barrier for biocurators andnon-experts, we provide TermGenie as a web applica-tion. The only requirement is a JavaScript enabled webbrowser. There are separate interfaces for separate tasks inTermGenie, one for class requests, and another forrequest review by ontology developers.Architectural componentsThe TermGenie application is based on client-serverarchitecture. The web client uses two JavaScript libraries(jQuery [5] and jQuery UI [6]) to implement the userinterface in the web browser. The server is written in Javaand accepts JSON messages in AJAX RPC calls from theclient via a Java servlet listener. Figure 2 illustrates therequired TermGenie components and the general work-flow for ontology class generation.The TermGenie server uses a set of modules and com-ponents to provide the required services for TermGenie.Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 3 of 13http://www.jbiomedsem.com/content/5/1/48Figure 2 Overview of TermGenie Components andWorkflow. (1) Retrieve existing templates for user selection; (2) Term generation processingand validation; (2a) Generate textual data and OWL axioms; (2b) Use reasoning to check for existing classes and new or changed relations; (3)Review of generated classes by the user in the web interface; (4) After review, assign permanent identifiers to the new classes; (5) Add the newclasses into the queue for review; (6) Senior ontology developers review the classes: accept, modify, obsolete; (7) Commit the changes to theontology; (8) Send confirmation e-mail to the user.This set includes modules for basic functionalities suchas loading ontologies, a persistence layer, reasoning, iden-tifier generation, access to version control systems, andsending e-mails. Some modules are used for more com-plex components, such as the term generation, requestsubmission, and review interface.Ontology loading and synchronizationFor ontology loading and the in-memory model,TermGenie relies on the OWL-API [7]. This Java libraryprovides an axiom-based ontology model with parsersand writers for many OWL serialization formats. Inaddition, we use a Java implementation for handlingOBO format [8], which also executes the conversion tothe OWL in-memory model from OBO format. Futureversions of the OWL-API will integrate the OBO formatlibrary, removing the need for this extra step. An impor-tant TermGenie feature is the support of ontology filehandling in a version control systems (VCS). CurrentlyTermGenie supports Subversion [9] and future supportfor Git [10] is planned. In addition, for a more efficientload of imports and files not in a VCS, TermGenie usesa local file cache for ontology files. The caching durationis a configuration parameter of each TermGenie instal-lation. TermGenie loads the required ontologies duringthe server start. To keep the ontologies up-to-date and insync with the source file, TermGenie periodically updatesthe VCS files and reloads the ontologies.Request queue and servicesAs shown in the workflow, TermGenie saves the requestedclasses for review in a queue. This request queue isseparate from the ontology file and requires a separatepersistent storage module. The persistence module isimplemented via the Java Persistence API using Open-JPA [11] as object-relational mapper and HSQLDB [12]as a simple embedded database for storage on disk. Thislightweight default implementation makes TermGenieindependent from more complex database setups andconfiguration issues. Because TermGenie does not pushthe requests to the ontology until they are reviewed,TermGenie provides additional services to access infor-mation about the pending request. Option one, thereis a separate TermGenie page, which list the currentlyDietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 4 of 13http://www.jbiomedsem.com/content/5/1/48pending and recently approved requests. This table isintended for users to quickly check their recent requests.Option two, there is a web service to query the status ofrequested class and whether it is an approved, pending,or unknown class identifier. This service is intended forthe integration of TermGenie in curation tools. CurrentlyProtein2GO [13] uses the service to verify the class iden-tifiers and prevent curators from entering invalid identi-fiers, while still allowing the immediate usage of newlygenerated classes.Sessions and user authenticationTermGenie uses Java servlets mainly as abstraction layer,but we make use of the built-in session handling mech-anism. The session is used to store the relevant tokensfor the authentication of users. For the authentica-tion, TermGenie currently relies on Persona [14] as alightweight service. Persona is a 3rd-party (non-profitand open source) protocol and service, which uses ane-mail address as primary identifier. It provides a conve-nient JavaScript client library and easy server-side calls fortoken verification. Once a TermGenie session has beenauthenticated, the authorization module uses the e-mailaddress as primary identifier to check whether the userhas the appropriate permissions for the requested opera-tion. TermGenie has different sets of permissions depend-ing on the tasks. For example, the submission of classesrequires a different set of permissions than the TermGeniemanagement console for administrators.Logic-based autocompletionAn important convenience feature for TermGenie usersis autocompletion. TermGenie uses a Lucene in-memoryindex to provide appropriate suggestions. To optimize thesuggested classes and restrict the classes for a template,TermGenie can be configured to use only a subset ofall available classes. For example, to create a subset forthe molecular functions in GO, the configured set justcontains the root class GO:0003674 (molecular_function).Using a reasoner, this set is then extended to include alldirect and indirect subclasses. The same configurationmechanism can also be used to allow the input of classesfrom multiple ontologies in an input field. For example,it is possible to use cell-type classes form the Cell TypeOntology and plant cell classes from the Plant Ontology.ConfigurationAll the different TermGenie components and modulesare configured and combined via Google Guice [15], alightweight dependency injection framework. TermGenieuses a combination of Java-based and compiler-checked defaults, configuration property files, andoptional command-line overrides to configure a specificTermGenie installation. For example, the Guice modulesare part of the Java configuration, creating a generic webapplication. The machine-specific details and secrets (e.g.passwords and private keys) are provided as a propertyfile to override the default parameters. The location of theproperty file is declared via a command-line parameter.This helps to avoid the problem of accidental release ofsensitive information into a public version control system.Templating systemThe core of TermGenie is template-based class gener-ation. The template-based approach allows the separa-tion of ontology design tasks, a fairly involved process,and standard class generation, a relatively straightforwardtask. For the former, the ontology developers extract orcreate, and test appropriate patterns for the generation ofnew classes based on the design principles of the ontology.A template consists of the OWL equivalent class axiomfor the formal definition and reasoning, label and textualdefinition building blocks and, if applicable, details forsynonym generation. These templates can then be usedby biocurators to generate desired standard classes with-out need for knowledge of the internal workings of theontology.In TermGenie each template is specified as a separateJavaScript function and file. During the generation theJavaScript code is executed by a Java-embedded JavaScriptengine. The embedded approach allows the use of nativeJava objects and functions, such as the ontology modeland reasoner checks, in Javascript calls without the needfor conversions. The Java layer also provides a set of func-tions intended to be used in the JavaScript code. These areshortcut functions for common tasks, such as the retrievalof a label for a given class. With these helpers it is possi-ble to create fairly compact JavaScript code for a template.This approach does not preclude the application to morecomplex operations and checks. Most of the validation,such as the search for existing classes and the inferenceof relations, is done in Java using standard OWL-APIreasoners.Every template has an associated XML-based configu-ration file. Amongst others, this configuration specifiesthe required and optional input fields, including detailson the relevant ontology subsets for the appropriate auto-complete suggestions. For an example of a template withits XML configuration, JavaScript code and resulting inputfields in TermGenie, see Figure 3. It should also be notedthat this particular example template is configured torequire exactly one ontology class as input. Other tem-plates can use up to three different input classes in theequivalent class axioms for a generated class.ReasoningTermGenie uses reasoning for two tasks: validation andrelation inference. Both tasks rely on the equivalent classDietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 5 of 13http://www.jbiomedsem.com/content/5/1/48Figure 3 Example template and configuration for TermGenie. (top-left) XML-based example template configuration for the Gene Ontologytemplate chemical_export. Includes declarations for required and optional input fields and corresponding JavaScript file; (top-right) Javascriptsnippet from the JavaScript file. for generating a class and OWL axioms; (bottom) Screenshot of the generated TermGenie input fields. Also showsautocompletion on ChEBI classes.axioms specified in the templates. For the validation,TermGenie asks the reasoner for equivalent named classesfor the given hypothetical new class. Similarly, for theinference and update of relations, we query the reasonerfor the direct super- and subclasses of the hypotheticalclass. This is done by declaring a new class using a newtemporary identifier and adding the corresponding equiv-alent class axioms. Next, TermGenie creates an up-to-datereasoner instance for the changed ontology. To preventunpredictable inferences, the ontology is checked forinconsistency and unsatisfiable classes. Once these checksare completed, the actual new-class-related queries aredone. After querying, the axiom changes are reverted andthe reasoner is discarded. The inferred direct subclassesare used to assert the most specific superclasses. In addi-tion the direct subclasses of the hypothetical new classare checked and their relations updated. This strategyallows the creation of not only the new leaf classes in theontology graph, but also new intermediate classes withan automatic update of relations for existing classes. Anexample of an inference using equivalent class axioms anda reference ontology is available in Figure 4.Using this workflow the reasoner creation and queryingare the most time-consuming steps of a TermGenie tem-plate request. In theory, TermGenie can use any OWL-API compliant reasoner, but the requirements for aninteractive web-application introduce a processing timelimit for the reasoner. We have experimented with multi-ple reasoners and chose ELK [16] as the most convenientcompromise for TermGenie.User workflowIn a typical workflow, the user begins by loading the rel-evant TermGenie web page, selecting and filling in therelevant template. After the class generation and valida-tion, the class is submitted for user review and approvaland assignment of a permanent identifier, see also Figure 5for a workflow diagram.On the user side, a TermGenie template consists of aset of required and optional input fields. TermGenie pro-vides autocompletion for ontology classes for appropriateinput fields. Before a user starts the new class generationand validation, a number of quick checks are executed.The checks include one for missing entries in requiredinput fields, such as a missing literature reference. Afterpassing these checks, the request is sent to the server.After the generation and reasoning step on the server,the users have the chance to review the proposed classes.They can also make modifications to textual parts if nec-essary (e.g. definition) or add additional synonyms. Thenext step is the submission of the generated classes forreview. As part of this process, a new permanent identifieris generated using a customizable identifier pattern andrange. To complete the submission step, the user must beDietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 6 of 13http://www.jbiomedsem.com/content/5/1/48Figure 4 Inferences for a class using a standard OWL reasoner. Reasoning example for a genus + differentia pattern for camptothecincatabolism in the GeneOntology. The class is defined by its genus catabolic process (GO:0009045) and differentia has_input camptothecin(CHEBI:27656). Following that definition, the class is a subclass of catabolic process. Using the additional axioms from ChEBI and the GeneOntology,a standard OWL reasoner can infer the more specific superclass alkaloid catabolic process (GO:0009822).logged in (authenticated), as the server will check for theappropriate permissions and will use the user metadatafor provenance information of the generated classes andrequested e-mail notifications.Ideally, after generating the identifier, a biocurator canimmediately use the generated identifiers for annotation.To facilitate this even while the identifier is not yet com-mitted to the ontology, we provide a web service to checkthe validity of class identifiers.Review processAfter a user has submitted their generated class requestsand generated the permanent identifiers, the requests areput into a queue for review by an ontology developer.During the review the ontology developer has the follow-ing three choices: approve, modify, or obsolete. There isno reject or delete option at that stage because a per-manent identifier has already been generated. In mostcases the classes can be approved without (or with min-imal) modifications since they rely on tested templates.Should a developer need more details, he/she may con-tact the original requester without making a decision, andkeep the request pending. The ontology developers canuse the e-mail information available from the provenanceinformation of each request.Once the ontology developer has determined whichclasses to commit to the ontology, he/she selects thecorresponding checkboxes and initiates the commit. Onthe server, the classes from the requests are first quicklychecked again. For the commit, the server uses the versioncontrol to create a clean checkout. From there TermGenieloads the ontology as a separate instance and appliesthe relevant changes. Depending on the original ontol-ogy file format this can either be OWL axioms or OBOterm frames. After writing the changed ontology as a file,TermGenie tries to commit the updated file into the VCS.When there is more than one request selected for committo the ontology, TermGenie processes each one separatelywith individual commits. This allows for a more fine-grained and aspect-oriented tracking of changes in theunderlying VCS. See also Figure 6 for the workflow duringthe review process.Results and discussionUsage in the Gene Ontology The pattern-based Ter-mGenie approach has been used for the Gene Ontologysince July 2010, with the current Java implementationin place since November 2012. During the period fromJuly 2010 until the end of June 2014, the Gene Ontologyinstance of TermGenie has been used to generate 4715classes, which represents 51.4% of all new classes createdin GO during that time. For a quarterly report of newclasses in GO see Table 1. The number of available tem-plates has been growing over time and currently stands at38 templates, see also Table 2 for a list of the templates.Many of these templates utilize an external ontology.As described before, TermGenie relies heavily on rea-soning for automatic classification and validation. Thisrequires that the ontology underlying a TermGenieinstance be sufficiently axiomatized with equivalent classaxioms. In the case of the Gene Ontology, with its consid-erable size and development history, a significant amountof time and effort was needed to introduce equivalentclass axioms into the ontology. The formalization of GODietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 7 of 13http://www.jbiomedsem.com/content/5/1/48Figure 5 TermGenie user workflow. To create a class in TermGenie, Biocurators go to the TermGenie website and select the relevant template fortheir request. The template consists of a set of required and optional input fields. TermGenie provides autocompletion for appropriate input fields.After passing some quick checks, the request is sent to the server, where generation and reasoning are executed. The results are send back and theusers have the chance to review the proposed classes. The next step is the submission of the generated classes for review. As part of this process, anew permanent identifier is generated using a customizable identifier pattern and range. Furthermore, the request is added to the review queue forfinal approval by the ontology developers.started in the early 2000s [17] and is still an ongoing task.It not only includes intra-ontology definitions [18], butalso makes use of existing other domain-specific ontolo-gies, such as the Chemical Entities of Biological Interest(ChEBI) ontology [19]. The most frequently referencedexternal ontology is ChEBI, but we also use the PlantOntology (PO), Cell Type Ontology, Phenotypic QualityOntology (PATO), and Uberon [4] to define class pat-terns in GO. One could argue that ontology formalizationis critical in creating a scalable and affordable long-term maintenance strategy because it supports automaticinferences and reasoning. The template-based formaliza-tion process helps to make implicit design patterns andassumptions explicit.Streamlining ontology development The template-based approach allows the separation of concerns androles between ontology engineering and everyday ontol-ogy class requests. Most of the ontology work for creatinga template can be done by the ontology developers andOWL experts during the design and test phase for eachof the templates. Once a pattern has been created and isavailable in a TermGenie web application, adding a newclass in that same pattern is vastly streamlined. The biocu-rators can quickly and safely create classes and permanentidentifiers on the website within minutes and return totheir annotation task. The effort for the final review by anontology developer for each class in TermGenie is min-imal as it relies on a pre-existing and tested solution.TermGenie also provides the convenient feature of e-mailnotifications.Bounds on complexity of composed classes Eventhough templates are usually tested and approved bythe ontology developers, one interesting issue for GeneOntology requests has come up. Some templates generateclasses of the same category as the input class (e.g. pro-cess involved_in process, or regulation of processes). ThisDietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 8 of 13http://www.jbiomedsem.com/content/5/1/48Figure 6 TermGenie workflow during a submitted class review by an ontology developer. After a user has submitted their generated classrequests, the requests are put into a queue for review by an ontology developer. During the review the ontology developer has the following threechoices: approve, modify, or obsolete. For the commit, the server uses the version control adapter to create a clean checkout. From there TermGenieloads the ontology as a separate instance and applies the relevant changes. After writing the changed ontology as a file, TermGenie tries to committhe updated file into the version control. After a successful commit the queue is updated and a confirmation e-mail is sent to the requester.means that it is possible to recursively compose classeswith definitions that unfold to a deeply nested hierarchy,with complex textual definitions and labels that imposea cognitive burden on users. Most of these classes arerequested for the annotation of complex biological pro-cesses and functions with a pre-composition strategy orlegacy systems with a simplistic annotation model (e.g.single unrelated annotations). From the formal point ofview these classes have a clear axiomatized definition andcan be unfolded into simpler annotations [20]. This kindof class request, although not very common in TermGe-nie, take longer to review as they often require furtherdiscussion andmodifications by ontology developers. Oneproposal has been to design a strategy to prevent the cre-ation of these multiply compounded classes and to insteadredirect users to the issue tracker instead of proceedingwith the request. The detection and redirection featurehas not yet been implemented.The most time-saving feature for biocurators is theimmediate creation of permanent identifiers. Therefore,during the review by an ontology developer, this leavesonly obsoletion as a way to reject a request. In theorythis could lead to higher number of unnecessary obsoletedclasses. However, this proved not to be an issue for theGene Ontology TermGenie instance. Only 70 requestedclasses have been obsoleted since inception, about 1.6% ofthe TermGenie requests.Non-templated class generation Biology and othercomplex subjects cannot always be axiomatized in a tem-platable way. Therefore, not all class requests can orshould be done using a template. To address this issue andat the request of the ontology developers, we added a free-form option to TermGenie. This allows very experiencedusers to quickly specify all the relevant details of a class,validate, and generate the new class using TermGenie. TheTable 1 TermGenie generated class counts in GO over timeQuarter 2010-III 2010-IV 2011-I 2011-II 2011-III 2011-IV 2012-I 2012-IITermGenie 139 154 236 254 307 175 255 806Manual 575 413 332 295 313 364 462 324Fraction 19.47% 27.16% 41.55% 46.27% 49.52% 32.47% 35.56% 71.33%Quarter 2012-III 2012-IV 2013-I 2013-II 2013-III 2013-IV 2014-I 2014-II TotalTermGenie 303 352 357 285 218 231 301 342 4715Manual 371 283 62 92 170 164 109 110 4439Fraction 44.96% 55.43% 85.20% 75.60% 56.19% 58.48% 73.41% 75.66% 51.51%Dietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 9 of 13http://www.jbiomedsem.com/content/5/1/48Table 2 Available templates for the geneontology termgenie instanceTemplate Input fields Equivalent class statementregulation: biological processregulation X:BP GO:0065007 and regulates some ?Xnegative_regulation X:BP GO:0065007 and negatively regulates some ?Xpositive_regulation X:BP GO:0065007 and positively regulates some ?Xregulation: molecular functionregulation X:MF GO:0065007 and regulates some ?Xnegative_regulation X:MF GO:0065007 and negatively regulates some ?Xpositive_regulation X:MF GO:0065007 and positively regulates some ?Xinvolved_in P:BP, W:BP ?P and part_of some ?Winvolved_in_mf_bp P:MF, W:BP ?P and part_of some ?Woccurs_in P:BP, C:CC ?P and occurs in some ?Cregulation_by R:GO:0050789, P:BP ?R and results_in some ?Ppart_of_cell_component P:CC, W: CC ?P and part_of some ?Wchemical_transport X:chebi GO:0006810 and transports or maintains localization of some ?Xchemical_transporter_activity X:chebi GO:0005215 and transports or maintains localization of some ?Xchemical_binding X:chebi GO:0005488 and has input some ?Xmetabolism_catabolism_biosynthesismetabolism X:chebi GO:0008152 and has participant some ?Xcatabolism X:chebi GO:0009056 and has input some ?Xbiosynthesis X:chebi GO:0009058 and has output some ?Xchemical_transmembrane_transport X:chebi GO:0055085 and transports or maintains localization of some ?Xchemical_transmembrane_transporter_activitytransmembrane transporter activity X:chebi GO:0022857 and transports or maintains localization of some ?Xsecondary active transmembrane transporter activity X:chebi GO:0015291 and transports or maintains localization of some ?Xuptake transmembrane transporter activity X:chebi GO:0015563 and transports or maintains localization of some ?Xtransmembrane-transporting ATPase activity X:chebi GO:0042626 and transports or maintains localization of some ?Xchemical_response_toresponse to X:chebi GO:0050896 and has input some ?Xcellular response to X:chebi GO:0070887 and has input some ?Xchemical_homeostasischemical homeostasis X:chebi GO:0048878 and regulates level of some ?Xcellular chemical homeostasis X:chebi GO:0055082 and regulates level of some ?Xchemical_import X:chebi GO:0006810 and imports some ?Xchemical_export X:chebi GO:0006810 and exports some ?Xchemical_import_into S:chebi, T:CC GO:0006810 and has target end location some ?T and imports some ?Scc_transport_from_totransport F:CC, T:CC GO:0006810 and has target start location some ?F and has target endlocation some ?Tvesicle-mediated transport F:CC, T:CC GO:0016192 and has target start location some ?F and has target endlocation some ?Tcc_transporttransport C:CC GO:0006810 and transports or maintains localization of some ?Cvesicle-mediated transport C:CC GO:0016192 and transports or maintains localization of some ?CDietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 10 of 13http://www.jbiomedsem.com/content/5/1/48Table 2 Available templates for the geneontology termgenie instance (Continued)chemical_transport_from_totransport X:chebi, [F:CC], [T:CC] GO:0006810 and transports ormaintains localization of some ?X [and hastarget start location some ?F] [and has target end location some ?T]vesicle-mediated transport X:chebi, [F:CC], [T:CC] GO:0016192 and transports ormaintains localization of some ?X [and hastarget start location some ?F] [and has target end location some ?T]cc_assembly_disassemblyassembly C:CC GO:0022607 and results_in_assembly_of some ?Cdisassembly C:CC GO:0022411 and results_in_disassembly_of some ?Cplant_development P:plant anatomical structure development and results in development of some?Pplant_formation X:plant anatomical structure formation involved inmorphogenesis and results information of some ?Xplant_maturation X:plant developmental maturation and results in developmental progression ofsome ?Xplant_morphogenesis X:plant anatomical structure morphogenesis and results in morphogenesis ofsome ?Xplant_structural_organization X:plant anatomical structure arrangement and results in structural organizationof some ?Xcell_apoptotic_process C:cell cell-type specific apoptotic process and occurs in some ?Ccell_differentiation C:cell GO:0030154 and results in acquisition of features of some ?Ccell_migration C:cell cell migration and alters location of some ?Cprotein_localization_toprotein localization C:CC GO:0008104 and has target end location some ?Cestablishment of protein localization C:CC GO:0045184 and has target end location some ?Cprotein_complex_by_activity A:MF GO:0043234 and capable_of some ?Asingle_multi_organism_processsingle-organism P:BP ?P and bearer of some PATO:0002487multi-organism P:BP ?P and bearer of some PATO:0002486biosynthesis_from T:chebi, F:chebi GO:0009058 and has output some ?T and has input some ?Fbiosynthesis_via T:chebi, V:chebi GO:0009058 and has output some ?T and has intermediate some ?Vcatabolism_to S:chebi, R:chebi GO:0009056 and has input some ?S and has output some ?Tcatabolism_via X:chebi, V:chebi GO:0009056 and has input some ?X and has intermediate some ?Vmetazoan_development X:Uberon anatomical structure development and results in development of some?XThe first column contains the template names and available templates variations. The second column lists the expected ontology inputs for the equivalent classstatement in the third column, with BP = GO:biological_process, MF = GO:molecular_function, CC = GO:cellular_component, chebi= chemical entity (CHEBI:24431),plant = plant anatomical entity (PO:0025131), cell = native cell (CL:0000003), Uberon = anatomical entity (UBERON:0001062).free-from workflow extends to the existing validation pro-cedures with additional checks. It searches for and warnsabout existing similar class names and synonyms for agiven class request. For example, a request for omega-some via free-form, produces a warning that a similarclass megasome already exists. In this case the warningcould be dismissed as the two classes refer to completelydifferent cell components. This additional check helps theontology developers to avoid the creation of redundantclasses.Due to the different use-case, this free-form template isimplemented as a separate tool in the TermGenie webapp,but shares many services (e.g., autocomplete, e-mail noti-fications) and adds requests to the common review queue.Furthermore, we use a different set of permissions torestrict the access of users to this template. Due to themore experimental nature of the requests via the free-form template, the obsoletion rate is slightly higher, with16 of 387 (4.1%) obsoleted requests.Evaluation of OWL reasoners for use in TermGenieBecause reasoning is a core task in TermGenie, weexperimented with multiple OWL-API compliant reason-ers. Currently, we haven chosen ELK [16] as the bestDietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 11 of 13http://www.jbiomedsem.com/content/5/1/48compromise for TermGenie. ELK is an OWL 2 EL pro-file [21] reasoner and provides a good trade off betweenresponse time and supported inference. Other testedreasoners include HermiT [22], JFact [23], Pellet [24],MORe [25] as full OWL compliant reasoners and jcel [26]as another OWL 2 EL compliant reasoner. In general allfull OWL2 reasoners proved to be too slow for usage inTermGenie. The other EL reasoner, jcel, is a viable alterna-tive, but ELK using multithreading out-performed jcel inthe initial classification step. A typical reasoning task forthe Gene Ontology and the required external ontologiesincludes about 415,000 logical axioms. Using ELK, we canrespond to a single request within a few seconds.TermGenie for other ontologies The TermGenie sys-tem was designed from the outset to be ontology-neutral.In addition to the Gene Ontology instance [27], we haveworked with the developers of other OBO Library ontolo-gies to create custom TermGenie instances.The OBO Cell Type Ontology (CL) [28] represents celltypes found in animals. One of the main uses of the CLis to rigorously describe samples collected as part of largenext-generation sequencing projects such as FunctionalAnnotation of Mammalian Genomes 5 (FANTOM5) andthe Encyclopaedia of DNA Elements (ENCODE), allow-ing analyses that yield insight into properties of differentcell types [29]. The ENCODE curators have found theCL instance of TermGenie useful as it provides a simpleweb-based way to generate new classes used to describesamples.The Ontology of Biological Attributes (OBA) [30] wascreated as a unified representation of traits (for exam-ple eye color) encompassing ontologies for describinganimals, plants and single-celled organisms. Many traitsfollow a trivial compositional pattern, encompassing asimple entity-attribute pattern, with the attribute beingtaken from the attribute subset of PATO, and the entitytaken from ontologies such as Uberon or PO. This ontol-ogy was originally created to be able to structure theregulation of biological quality branch of the GO, but ithas found uses in other areas. Curators in the MonarchInitiative project have used it to describe mouse strainphenotypes, andmost recently it has incorporated into theEncyclopaedia of Life (EOL) TraitBank [31] project.Ontologies of abnormal or variant phenotypes alsobenefit from a templated approach, as their classes canoften be described using an Entity-Quality combinatorialapproach, akin to that used inOBA. So far we have createdinstances for the Mammalian Phenotype Ontology [32]and the Human Phenotype Ontology (HP) [33], with plansto create instances for other species-specific phenotypeontologies. The HP instance was created in part to servethe needs of the NIH Undiagnosed Diseases Program(UDP), which is systematically describing the phenotypesof patients with undiagnosed diseases, so that phenotypecomparison algorithms can be used to assist the hunt forthe genomic underpinnings of these diseases. In this case,it is important that a diversity of ontology contributors canefficiently and effectively contribute to the HP. We believethat TermGenie will greatly facilitate contributions fromthe rare disease community.Note that in order for these instances to work effec-tively, it was first necessary for the respective developersto make their ontologies reasoner-ready by providingOWL equivalent class axioms, a process that has beenunderway for several years [3,34]. In contrast, once thenecessary OWL refactoring is complete, the configura-tion of the ontology-specific TermGenie instances takesabout a week, with most of the time spent on testing thetemplates.Comparison with other approachesCreating new classes in ontologies is a common task, onethat is typically done by a developer using an OntologyDevelopment Tool (ODT) such as OBO-Edit [1] or Pro-tégé [35]. These are both comprehensive, general purposeenvironments, and are not intended for use by annotatorsand biocurators without requisite training. In addition,both are desktop applications, requiring an installation onthe users machine. The limitations of desktop ontologydevelopment software, especially for collaborative work,led to the creation of WebProtégé, a web-based ontologydevelopment tool [36]. All three applications are pow-erful tools with steep learning curves and are usuallyintended for knowledge/ontology engineers and ontologydevelopers. They do not offer the separation of designand quick everyday use for non-experts. TermGenie is notintended to replace comprehensive ODTs; the pattern-based approach and ODTs complement each other in theontology development workflow. In fact the comprehen-sive ODTs are required during the template design andtesting.Other related work exists in the form of the TermGeneration plugin DOG4DAG [37]. It is available as anOBO-Edit and Protégé plugin. The tool allows proposal ofnew classes based on phrases extracted from a given textcorpus. The most common use case is to create or adddomain-specific vocabulary to an ontology. The best use isin early stages of ontology projects as it generates mostlylist of candidate classes. For amoremature and formalizedontology, a more axiomatized result is required.The Network-Extracted Ontology (NeXO) [38] is anexample of an orthogonal, data-driven approach to ontol-ogy generation. This approach takes as input a suffi-ciently large and dense network (i.e. gene and proteininteractions), and applies a clustering algorithm to gen-erate classes and relationships between these classes. Sofar, NeXO has been used to generate a yeast cellularDietze et al. Journal of Biomedical Semantics 2014, 5:48 Page 12 of 13http://www.jbiomedsem.com/content/5/1/48component ontology. It remains to be seen how well theapproach works for other portions of ontologies such asthe GO.Another template-driven class generation approach isQuick Term Templates [39]. There are multiple imple-mentations available for this approach: a MappingMasterplugin for Protégé, the OntoRat web application [40],or custom Perl code combined with spreadsheets. Eachimplementation still requires quite a bit of detailed knowl-edge of the ontology. One huge issue is the informa-tion flow back to the ontology developers, which alsoincludes the assignment of valid/permanent identifierand access control. In the TermGenie application thesedetails are controlled by the server and the built-in reviewmechanism.The Cellular Phenotype Ontology (CPO) is an ontologythat was entirely generated programmatically [41]. A cus-tom program was written using the java OWL-API togenerate a class from the cross-product of the cellularprocess branch of GO and a subset of PATO. This kind ofen-mass class generation is in contrast to the TermGenieapproach, in which biocurators flesh out a subset of thespace of all possible classes on an as-needed basis. Theresulting ontology is more compact and is arguably moreusable than one in which the entire space of terms isfleshed out in advance.Most recently, the Tawny-OWL framework provides anelegant and powerful way to generate an entire ontologyprogrammatically using a high-level declarative domain-specific language [42]. At this time, Tawny is difficult tointegrate into a conventional ontology development work-flow as it requires the source for the ontology to be storedas a Clojure program rather than in a non-programmaticformat such as OBO or OWL. However, we are workingwith the Tawny developers to explore ways to integrateour approaches.ConclusionTermGenie is a web-based class-generation system thatcomplements traditional ontology development tools. Allclasses added through pre-defined templates are guaran-teed to have OWL equivalence axioms that are used forautomatic classification and in some cases inter-ontologylinkage. At the same time, the system is simple and intu-itive and can be used by most biocurators without exten-sive training. Its use in the Gene Ontology has removed asignificant curation bottleneck, and has freed up ontologydevelopers from performing time-consuming repetitivetasks allowing them to work on high-level design issues.In the last 4 years the Gene Ontology TermGenie instancewas used to generated 4715 new classes, about 51.4% ofall new classes created. The immediate generation of per-manent identifiers proved not to be an issue with only 70(1.4%) obsoleted classes. TermGenie is now in use in otherprojects as well, including the Mammalian PhenotypeOntology, the Human Phenotype Ontology, the Cell TypeOntology and the Ontology of Biological Attributes.Availability and requirements Project name: TermGenie Project home page: http://termgenie.org Operating system(s): Platform independent Programming language: Java, JavaScript Other requirements: Java 6 or higher, Jetty 6 orhigher, Maven 3.0.x License: New BSD (BSD 3 Clause) Any restrictions to use by non-academics: noneAbbreviationsGO: GeneOntology; HP: Human Phenotype; ODT: Ontology Development Tool;OWL: Web Ontology Language; VCS: Version Control System; ChEBI: ChemicalEntities of Biological Interest; PO: Plant Ontology; CL: Cell Type Ontology;PATO: Phenotypic Quality Ontology; OBA: Ontology of Biological Attributes.Competing interestsThe authors declare that they have no competing interests.Authors contributionsCJM conceptualized and implemented the initial prototype. HD designed,implemented, and maintains the current TermGenie system. CJM, TZB, REF,DPH, JL, DOS, PR provided the ontology expertise, templates, and testexamples, and performed necessary ontology axiomatization and refactoring.All authors provided feedback, drove requirements and participated in testingand improving the system. All authors read and approved the final manuscript.AcknowledgementsWe thank all the GO curators who have provided feedback on the GOTermGenie instance. Thanks to Jen Hammock from the Encyclopaedia of Lifefor testing the OBA TermGenie instance.All authors are supported by the National Human Genome Research Institute(NHGRI) P41 grant 5P41HG002273-09 to the Gene Ontology Consortium. Inaddition, JL is funded by the European Molecular Biology Laboratory (EMBL),European Bioinformatics Institute Outstation (EMBL-EBI) core funds. Inaddition, HD and CJMs contribution was also supported by the Director,Office of Science, Office of Basic Energy Sciences, of the U.S. Department ofEnergy under Contract No. DE-AC02-05CH11231.Author details1Genomics Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Road,94720 Berkeley, CA, USA. 2The Arabidopsis Information Resource, PhoenixBioinformatics, 94063 Redwood City, CA, USA. 3European Molecular BiologyLaboratory, European Bioinformatics Institute (EMBL-EBI), CB10 1SD Hinxton,Cambridge, UK. 4Mouse Genome Informatics, The Jackson Laboratory, 04609Bar Harbor, ME, USA.Received: 28 August 2014 Accepted: 29 October 2014Published: 11 December 2014JOURNAL OFBIOMEDICAL SEMANTICSSaleem et al. Journal of Biomedical Semantics 2014, 5:47http://www.jbiomedsem.com/content/5/1/47RESEARCH Open AccessTopFed: TCGA tailored federated queryprocessing and linking to LODMuhammad Saleem1*, Shanmukha S Padmanabhuni2, Axel-Cyrille Ngonga Ngomo1, Aftab Iqbal2,Jonas S Almeida3, Stefan Decker2 and Helena F Deus4AbstractBackgroud: The Cancer Genome Atlas (TCGA) is a multidisciplinary, multi-institutional effort to catalogue geneticmutations responsible for cancer using genome analysis techniques. One of the aims of this project is to create acomprehensive and open repository of cancer related molecular analysis, to be exploited by bioinformaticianstowards advancing cancer knowledge. However, devising bioinformatics applications to analyse such largedataset is still challenging, as it often requires downloading large archives and parsing the relevant text files. Therefore,it is making it difficult to enable virtual data integration in order to collect the critical co-variates necessary foranalysis.Methods: We address these issues by transforming the TCGA data into the Semantic Web standard ResourceDescription Format (RDF), link it to relevant datasets in the Linked Open Data (LOD) cloud and further propose anefficient data distribution strategy to host the resulting 20.4 billion triples data via several SPARQL endpoints. Havingthe TCGA data distributed across multiple SPARQL endpoints, we enable biomedical scientists to query and retrieveinformation from these SPARQL endpoints by proposing a TCGA tailored federated SPARQL query processing enginenamed TopFed.Results: We compare TopFed with a well established federation engine FedX in terms of source selection and queryexecution time by using 10 different federated SPARQL queries with varying requirements. Our evaluation resultsshow that TopFed selects on average less than half of the sources (with 100% recall) with query execution time equalto one third to that of FedX.Conclusion: With TopFed, we aim to offer biomedical scientists a single-point-of-access through which distributedTCGA data can be accessed in unison. We believe the proposed system can greatly help researchers in the biomedicaldomain to carry out their research effectively with TCGA as the amount and diversity of data exceeds the ability oflocal resources to handle its retrieval and parsing.Keywords: Federated queries, SPARQL, TCGA, RDFBackgroundThe Cancer Genome Atlas [1] (TCGA) is an effort ledby the National Cancer Institute to characterize andsequence more than 30 cancer types from 9000 patientsat the molecular level. The goal is to analyse DNA forevery participant to discover abnormalities present in atumour sample that are peculiar to the oncogenic pro-cess and whether it affect progression and regression of*Correspondence: saleem@informatik.uni-leipzig.de1Universität Leipzig, IFI/AKSW,PO 100920, D-04009Leipzig,GermanyFull list of author information is available at the end of the articlethe tumours. Each cancer type published by TCGA hasthree levels. Level 1 is raw data, level 2 is normalizeddata, and level 3 is processed data. The analytics are per-formed on the level 3 data, which is also of our interestfor the work presented in this paper. TCGA is a valu-able resource for hypothesis-driven translational researchas all of its data results from direct experimental evi-dence. Analysis of such evidence within cancer researchhas led in recent years to clinically relevant findings in thegenetic mark-ups of different cancer types and is at the© 2014 Saleem et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproductionin any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 2 of 18http://www.jbiomedsem.com/content/5/1/47forefront of a coordinated worldwide effort towards mak-ing more molecular results from cancer analyses publiclyavailable [2].Big data research initiatives such as the InternationalCancer Genomics Consortia [3], the 1000genomes [4] andthe One Million Genomes project [5], the $10 MillionGenome Prize [6], and the remarkable drop in the cost ofgenome sequencing [7] will soon mean that the currentbioinformatics paradigm in which researchers downloadall the data, extract the interesting pieces and removethe rest, will no longer be feasible [8,9]. The rapid devel-opment of advanced statistical methods for analysingcancer genomics [10-12] further emphasizes the need toenable smooth online data collection and aggregation.As pointed out in [13], Large-scale genome character-ization efforts involve the generation and interpretationof data at an unprecedented scale that has brought intosharp focus the need for improved information technol-ogy infrastructure and new computational tools to ren-der the data suitable for meaningful analysis. A scalableand robust solution is therefore a critical requirement,whereby researchers can obtain a subset of big data theyare interested in by executing a query using a particularservice.In addition to the large semi-structured experimen-tal results available through TCGA and related projects,there is a significant number of unstructured and struc-tured biomedical datasets available on the Web. Mostof these datasets are critical towards annotating andintegrating the experimental results. Remote query pro-cessing and virtual data integration, i.e., transparent on-the-fly-view creation for the end user, can provide ascalable solution to both challenges. Due to the major-ity of TCGA data being available in text files (in tabularformat), it is difficult to query the contents of a par-ticular file or to enable virtual data integration. In thispaper, we have addressed above problems by applyingSemantic Web technologies and federated query process-ing. Semi-structured level 3 TCGA data were convertedinto Semantic Web standard format RDF such that itcould be queried and publicly accessed via SPARQL end-points. This choice of technology complies with the W3Crecommendation of integrating distributed and hetero-geneous data sources. There are currently a large num-ber of applications supporting SPARQL and RDF, bothacademic and commercial, and both SwissProt [14] andEBI [15] have made their databases available as SPARQLendpoints.In order to address the scalability issue while dealingwith big data, we propose an efficient data distributionstrategy and a TCGA tailored federated query engine(named TopFed) that leverages the data distribution alongwith the structure of triple pattern joins in a query forsmart source selection. The logistics of the proposedsolution will be assessed by comparison with a well estab-lished federation engine FedX [16].MotivationBefore TCGA,most cancer genomics studies have focusedon only one type of data or one cancer histology. TheCancer Genome Atlas project changes that paradigm bymaking available to oncologists and biomedical scien-tists a comprehensive compilation of raw and processeddata files on over 30 different cancer histologies and atseveral levels of Genomics (e.g. SNP, protein expres-sion, exon expression, sequences, methylation, etc.). Since2006, when the Cancer Genome Atlas first became avail-able, multiple studies were devised to exploit its data.Nevertheless, a means to easily exploit this cancer atlaslike one would exploit an atlas of planet Earth, doesnot yet exist. Part of the challenge is caused by a needto represent, organize and structure the 28.3 TB ofdata [17] available to the public in a way that can beeasily queried by computational/statistics tools. Furthercomplicating this task has been the growth of TCGAdata. Some institutions have access to the computationalresources necessary to provide a TCGA-synchronized andquery-able interface suitable to address the most complexquestions such as comparing methylation across cancerhistologies or correlating exon expression results withmethylation patterns regardless of cancer histology. Oneinstitution providing a tool and query language to exploitthis data is Memorial Sloan Kettering through its cBioportal [18]. However, the data must first be constrainedto the type of cancer before it can be exploited from abiological/molecular stand point. A second challenge iscaused by the applications of the data - not all data areuseful for all cancer researchers. Some researchers focuson a particular type of data, or a particular cancer histol-ogy, and therefore have little or no interest in hosting theentire Cancer Genome Atlas in a structured, query-ableform.The aim behind the work presented in this paper was todevelop the computational concepts - and devise a proto-type - that enable the exposure of TCGA as a distributed,semantically aware API (application programming inter-face). Although the data can be freely downloaded andanalyzed by anyone with a sufficiently powerful computer,the computational tools available nowadays do not enableexploring this atlas without significant effort involvedin selecting and downloading the data, mapping it togenomic coordinates and easily navigating to the sectionsof the genome that are relevant for understanding can-cer. For example, zooming into genomic regions known asCancer Hotspots or into the genomic coordinates whereoncogenes and tumour suppressors are encoded, requiresa combination of efforts including: 1) downloading thedata; 2) parsing the text files for relevant results; and 3)Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 3 of 18http://www.jbiomedsem.com/content/5/1/47mapping each file to the same set of genomic coordinates.On the other hand, fast, easy to use and integrated accessto the big data such as TCGA requires: 1) Representingdata in a format (e.g. RDF) amenable to integrated search;2) logically connect all data; 3) distributing data acrossmultiple locations (load balancing); and 4) supportinglinking and federated querying (collecting data frommorethan one location using a single query) with external datasources.TopFed is devised to address these requirements.Whereas requirements 1 and 2 are addressed using RDFand class level connectivity (see section TCGADataWorkflow), addressing requirements 3 and 4 relies on tech-niques that make the best use of the architecture ofthe Web to enable both redundancy when resources aredown and sharing the load of hosting this data acrossmultiple locations. As a proof of concept, TopFed linksdifferent portions of the Cancer Genome Atlas acrosstwo institutions, one at Insight Centre for Data Ana-lytics at NUIG in Ireland and other at the Universityof Alabama at Birmingham in United States. TopFed isdevised as a federation query engine that enables selec-tion of the appropriate endpoints necessary to addressan incoming query as well as optimization of the ser-vices discovery based on metadata about each endpoint.TopFed accepts queries in SPARQL, the same universal,standardized query language as each of the endpoints con-nected to it, making its functionality straightforward. Forexample, if someone is looking to query only one can-cer histology, they can direct their queries at the endpointhosting that data. However, if someone wants to exploitand compare multiple cancer histologies, the query canbe pointed at TopFed, which automates and optimizes thetask of discovering endpoints that contain the data nec-essary to address the question. To illustrate a typical usecase, we exemplify a genomic region query enabled byTopFed.Biological query exampleThis example makes use of the KRAS gene, a gene thatis commonly mutated and constitutively active in manycancer types, leading the cell to replicate DNA and makecopies of itself at a very fast pace. Genes with this typeof behaviour in the cell are commonly called oncogenes.When mutated, these genes become constitutive active,thus having the potential to cause normal cells to becomecancerous. Imagine that for five different cancer histolo-gies, we used TopFed to search for the methylation statusof the KRAS gene (chr12:25386768-25403863), and cre-ated a box plot comparing the values, shown in Figure 1.The query (given in Listing 1) executed on each of the fiveSPARQL endpointsa, resulting in five different samples.Figure 1 Biological query results.We used TopFed to search for the methylation status of the KRAS gene (chr12:25386768-25403863) across fivecancer histologies (hosted by five SPARQL endpoints) and created a box plot comparing the methylation values. The corresponding SPARQL queryto retrieve the required methylation values is given in Listing 1.Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 4 of 18http://www.jbiomedsem.com/content/5/1/47Listing 1 Query to retrieve averagemethylation values forthe KRAS gene and for all patient of a particular cancertypePREFIX t c g a : <h t tp : / / t c g a . d e r i . i e / schema/>PREFIX r d f : < h t tp : / /www. w3 . org /1999/02/22? rd f?syntax?ns#>PREFIX xsd : <h t tp : / /www. w3 . org /2001/XMLSchema#>SELECT DISTINCT ? p a t i e n t avg ( xsd : dec imal ( ?methylationKRAS ) ) as ? avgMethKRASWHERE{? pa t i en tURI t cga : bcr_ { p } a t i e n t _ { b } arcode ? p a t i e n t .? pa t i en tURI t cga : r e s u l t ? recordNo .? recordNo tcga : chromosome "12" .? recordNo tcga : p o s i t i o n ? p o s i t i o n .? recordNo tcga : be ta_ { v } a l u e ? methylationKRAS .FILTER ( xsd : dec imal ( ? p o s i t i o n ) >= 25386768 && xsd :dec imal ( ? p o s i t i o n ) < 25403863)}This query returns the average methylation results forthe KRAS gene of all patients in a particular cancerhistology. The results show a clear distinction betweensolid tumours and hematopoetic cancers. This differen-tial in the methylation values is not necessarily surprisingresults, given that blood cancers are known to be sig-nificantly different genetically from solid tumours. Whatis interesting and worth further exploring in these casesis the shape of the distribution: why Acute MyeloidLeukemia (AML) samples, a cancer of the myeloid ofblood cells, appear to have high methylation, effectivelycreating a bi-modal distribution? Exploring the prove-nance of this data may provide a clue for that - onehypothesis is that these samples were incorrectly diag-nosed as AML or it may be that these AML sam-ples are indeed genetically different - and thereforeshould not be treated with the same therapies as theothers. Since TopFed integrates both the clinical andgenomic parameters, exploring these different hypothe-sis is as easy as returning to the query and retrievingthe potentially relevant clinical parameters that couldexplain the difference. Exploring the same gene (KRAS)in another type of data (e.g. exon expression) couldalso help explain why these samples are different. SinceTopFed is aware of which SPARQL endpoints storeeach data property, it will appropriately select the correctsource for the data, thereby adding the extra parame-ters to the query sufficient to generate sufficiently robusthypothesis.Further exploring these examples is beyond the scope ofthis manuscript - however, we encourage our readers toexperiment themselves with their own hypothesis or witha different set of genes/genomic locations by changing thevalues for tcga:chromosome and tcga:position.We includean example of a query that could be used to retrieve theclinical parameters for the outlier patients (and comparewith non-outlier patients) in Listing 2.Listing 2 Query to retrieve averagemethylation values forthe KRAS gene, along with clinical data, for all AML outlierpatients. This query can be run athttp://vmlion14.deri.ie/node45/8082/sparqlPREFIX t c g a : <h t tp : / / t c g a . d e r i . i e / schema/>PREFIX r d f : < h t tp : / /www. w3 . org /1999/02/22? rd f?syntax?ns#>PREFIX xsd : <h t tp : / /www. w3 . org /2001/XMLSchema#>SELECT ? o u t l i e r s P a t i e n t ? avgMethKRAS ? p i o rd i a gn ?v i t a l s t a t ? a g e a t d i a g ? gender ? p r e t r e a tH i s t o r y ?e t h n i c i t y? r ace{{ SELECT DISTINCT ? o u t l i e r s P a t i e n t ( avg ( xsd :dec imal ( ? methylationKRAS ) ) as ? avgMethKRAS ) ?p i o rd i a gn ? v i t a l s t a t ? a g e a t d i a g ? gender ?p r e t r e a tH i s t o r y ? e t h n i c i t y ? r aceWHERE{? pa t i en tURI <ht tp : / / t c g a . d e r i . i e / schema / bcr_ { p }a t i e n t _ { b } arcode > ? o u t l i e r s P a t i e n t .? pa t i en tURI <ht tp : / / t c g a . d e r i . i e / schema / r e s u l t > ?recordNo .? recordNo tcga : chromosome "12" .? recordNo tcga : p o s i t i o n ? p o s i t i o n .? recordNo tcga : be ta_ { v } a l u e ? methylationKRAS .FILTER ( xsd : dec imal ( ? p o s i t i o n ) >= 25386768 && xsd :dec imal ( ? p o s i t i o n ) < 25403863)SERVICE <ht tp : / / vmlion14 . d e r i . i e / node42 /8081/spa rq l >{? pa t i en tURI t cga : bcr_ { p } a t i e n t _ { b } arcode ?o u t l i e r s P a t i e n t .? pa t i en tURI t c g a : p r i o r _ { d } i a g n o s i s ? p i o rd i a gn .? pa t i en tURI t cga : v i t a l _ { s } t a t u s ? v i t a l s t a t .? pa t i en tURI t cga : age_ { a } t _ { i } n i t i a l _ { p } a t h o l o g i c _ {d } i a g n o s i s ? a g e a t d i a g .? pa t i en tURI t cga : gender ? gender .? pa t i en tURI t cga : p re t r ea tment_ { h } i s t o r y ?p r e t r e a tH i s t o r y .? pa t i en tURI t cga : e t h n i c i t y ? e t h n i c i t y .? pa t i en tURI t cga : r a ce ? r ace .}}}FILTER ( ? avgMethKRAS > 0 . 0 5 )}Related workThe TCGA data has been widely used in the literature,but mostly in its raw form. Verhaakl et al. [19] use thegene expression results to describe a robust molecularclassification of TCGA Glioblastoma Multiforme (GBM)into Proneural, Neural, Classical, and Mesenchymal sub-types and integrate multidimensional genomic data toestablish patterns of somatic mutations and DNA copynumber. Other notable contributions [20-24], includingour own early analysis of DNA copy number variation inGBM [25] make use of the TCGA data for various impor-tant findings without, however, using more than one orpossibly two types of molecular data. To facilitate inte-grated analysis over all cancer types, Deus et al. developedan infrastructure using Simple Sloppy Semantic Database(S3DB) management model to expose clinical, demo-graphic and molecular data elements generated by TCGAas a SPARQL endpoint [26]. Robbins et al. [27] developedan engine to continuously index and annotate the TCGASaleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 5 of 18http://www.jbiomedsem.com/content/5/1/47data files using JavaScript in conjunction with RDF, andthe SPARQL query language. However, both [26] and [27]provide only file level provenance annotations withoutproviding structured access to actual contents containedin the files. Recently, Saleem et al. [28] presented a LinkedData version of the Cancer Genome Atlas Database foreffective cancer treatment. This work demonstrates threeuse cases namely target cancer treatment, mechanism-based cancer treatment, and survival outcome, where theLinked Data approach of integrating TCGA data wasused. More recently, a visualization of the integrationof the Linked TCGA cancer data with PubMed publica-tions is presented in [29,30]. The main aim behind thiswork is to foster serendipity through big data RDFization,continuous integration, and visualization. GenomeSnip,a visual analytics platform, which facilitates the intuitiveexploration of the human genome and displays the rela-tionships between different genomic features, is presentedin [31].Advances in federated query processing methods overthe Web of Data have enabled the application of feder-ated solutions for datasets, such as those from genomics.Quilitz and Leser [32] propose DARQ, which makesuse of service descriptions for relevant data sourceselection.Langegger et al. in [33] propose a solution similarto DARQ using a mediator approach, which continu-ously monitors the SPARQL endpoints for any datasetchanges and updates the service descriptions automati-cally. Umbrich et al. [34,35] propose a Qtree-based indexstructure that summarizes the content of data source forquery execution over theWeb of Data. Schwarte et al. [16]propose FedX, an index-free query federation for theWebof Data.SPLENDID [36] makes use of VOID descriptions alongwith SPARQL ASK queries to select the list of relevantsources for each triple pattern. Both FedX and SPLENDIDare able to handle more expressive queries as compared toprevious contributions.Other optimization techniques have also been attemp-ted. Li and Heflin [37] built a tree structure that supportedfederated query processing over heterogeneous sources.Kaoudi et al. [38] propose a federated query techniqueon top of distributed hash tables (DHT). Ludwig andTran [39] developed a mixed query engine that assumessome incomplete knowledge about the sources to selectand discover new sources at run time. Acosta et al. [40]present ANAPSID, an adaptive query engine that adaptsquery execution schedulers to SPARQL endpoints dataavailability and run-time conditions.Avalanche [41] gathers endpoint dataset statistics andbandwidth availability on-the-fly before the query federa-tion. Saleem et al. [42] presented DAW, a novel duplicate-aware federated query approach over the Web of Data.DAW makes use of the min-wise independent permuta-tions [43] and compact data summaries to extend existingSPARQL query federation engines in order to achieve thesame query recall values while querying less SPARQL end-points. Finally, HiBISCuS [44] is an efficient hypergraphbased source selection approach for SPARQL query fed-eration over multiple SPARQL endpoints. A fine-grainedevaluation of SPARQL endpoint federation systems isperformed in [45].All of the above SPARQL query federation approachesare more generic and usually over-estimate (explained inthe Source Selection sub-section below) the set of sourcescapable for answering a query. This over-estimation canbe expensive when data is large. In our case, the datain hand is also large and we need a TCGA optimizedfederation engine that selects close to optimal set of capa-ble sources. To this end, we propose TopFed, a TCGAtailed federated engine that make use of the intelligentdata distribution and join-aware source selection to min-imise the source over-estimation and provide fast queryresults.The main contributions of this paper are the following:1. We have proposed a Linked Data version of TCGAthat supports efficient data distribution andfederated SPARQL queries to integrate data frommultiple SPARQL endpoints efficiently by onlysending remote queries.2. We have published, to the best of our knowledge, thelargest RDF dataset (20.4 billion triples) and linked itto various datasets in the LOD cloud to enableannotation and enhancement with public knowledgebases as well as virtual data integration.3. We devised the basic architecture and logic rulesgoverning TopFed, a smart federated query enginefor virtual integration of the TCGA data frommultiple SPARQL endpoints that comply with theTCGA organizational schema. Further, we provideeasy to use utilities [46] in order to refine andtransform TCGA raw text files into RDF.4. We evaluate our approach against FedX using 10different SPARQL queries and show that our sourceselection algorithm, on average, selects less than halfsources compared to FedX (with 100% recall). Also,our average query processing time is one third incomparison to FedX.The remaining part of this paper is organized as follows:we present our methodology to refine, RDFize and linkthe TCGA data to LOD datasets in detail. Subsequently,we present a thorough evaluation of our approach againststate of the art approaches. We finally conclude the paperwith a discussion of our findings and an overview of futurework.Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 6 of 18http://www.jbiomedsem.com/content/5/1/47MethodsTransforming TCGA data to RDFThe process of transforming TCGA data into RDFb isshown in Figure 2. Given a TCGA text file, the first pro-cessing step is carried out by the Data Refiner, whichselects the specific fields [47] necessary for traditionalmolecular analysis algorithms. This step is necessary tominimize the size of the resulting RDF according to whatwe expect will be the most useful results. It is impor-tant to note that the above required fields for differenttypes of results may not be directly accessible throughraw text files. To this end, our Data Refiner makesuse of the annotations files [48] for the required fieldslookup. For example, methlylation annotation files areused to obtain chromosome and position values usingProbe_Name lookup. Finally, the refined text file is sent tothe RDFizer, which generates the resulting RDF dump inN3 format [49]. Our choice of N3 was due of its efficientspace consumption. The generated RDF dumpsc are thenuploaded to various SPARQL endpoints according to thedistribution rules shown in Figure 3.An example of the above RDFication process is shownin Figure 4, where part of rawmethylation result of patientTCGA-A2-A0CX is provided as input to the Data Refiner.The Data Refiner selects chrome, position, and beta_valueout of the five available columns. The selected columnsare commonly used for traditional molecular analysisalgorithms targeting methylation data. It is important toText FileData RefinerRDFizerSPARQL endpointRDFRDFFileFigure 2 TCGA text to RDF conversion process. Given a text file,first it is refined by the Data Refiner. The refine file is then convertedinto RDF (N3 notation) by the RDFizer. Finally, the RDF file is uploadedinto a SPARQL endpoint.note that Data Refiner also skipped the yellow highlightedline because beta_value is not available for that specificmethylation result. The refined text file is then passed toRDFizer that generates the RDF dump (N3 format). Thevalues d1...d8 show DNA methylation results from 1 to8. The use of this information is further explained in theSource Selection sub-section.The accuracy of the text to RDF conversion is 100%(to the best of our understanding) since our Data Refinerselects a predefined set of fields for different types ofresults. Further, it skips specific field values (such as NA,Null, Unknown, Not Reported etc.) during RDFication pro-cess as shown in the above example. Currently, we haveRDFized 27 cancer tumours and the statistics are shownin Table 1. We will RDFize new TCGA data once it isavailable through the TCGA data portal.Linking TCGA to the LOD cloudOne of the design principles of Linked Data [50] is theprovision of links to other data sources. Adding linksfrom TCGA to other knowledge bases is particularly cru-cial to ensure that the information already contained inother data sources can be easily (1) merged with the newTCGA data as well as (2) queried in combination withthe TCGA data by means of federated SPARQL queriesd.Moreover, links can facilitate other tasks such as cross-ontology question answering, data integration and dataanalytics. Yet, the sheer size of bio-medical knowledgebase available on the LOD cloud and of the TCGA knowl-edge base itself makes it impossible to use manual linkingto provide such cross knowledge-base links from TCGAto other data sources. We thus made use of the LIMESframework [51] for discovering links between TCGA andother knowledge bases. LIMES is a framework for linkdiscovery that provides time-efficient implementations ofseveral string and numeric similarity and distance mea-sures. The framework provides both means to define linkspecifications explicitly and machine-learning algorithmsfor finding link specifications in an unsupervised andsupervised fashion. Given that genes and chromosomeshave dedicated IDs that are used across several biomedi-cal knowledge bases, we used LIMES exactMatchmeasurefor linking.We focused on linking patient data and lookupdata with knowledge bases that describe genes and chro-mosomes. In particular, we linked TCGA to HGNC [52],OMIM [53] and Homologene [54]. Tables 2 and 3 providean excerpt of the links generated for the TCGA dataset,while Listing 3 provides an excerpt of the specificationsused for linking. The linking tasks were carried out on onekernel of a 2.3GHz i7 processor with 4GB RAM. Giventhat we used exact matches, we ensured that our link dis-covery achieves a precision of 100%. The recall of thelinking process is tedious to assess as it would requireassessing millions of links manually.Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 7 of 18http://www.jbiomedsem.com/content/5/1/47b1 b2 p1 p2 g1 g2 g3 p3 p4 g4 g5 g6 p5 p6 g7 g8 g9 C = {CNV, SNP, E-Gene, E-Protein, miRNA, Clinical} F = {Expression-Exon} M = {beta_value,  (CNV, SNP, E-Gene, miRNA,     E-Protein, Clinical)   Exon-Expression  Meth  D = {seg_mean, rpmmm, scaled_est, p_exp_val} C-2 = {{p  {E A G} {p = rdf:type  o F}} {{S-Join(p, E  F)  P-Join(p, E  F)} {!S-Join(p, M  B D  C)                !P-Join(p, M  B D  C) }}} C-3 = {{p  {M A}  {p = rdf:type  o B}} {{S-Join(p, M  B)   P-Join(p, M  B) } {!S-Join(p, E  F D  C)                !P-Join(p, E  F D  C) }}} C-1 = {{p  {D A G}  {p = rdf:type  o C}} {{S-Join(p, D  C)  P-Join(p, D  C) } {!S-Join(p, M  B E F)                !P-Join(p, M  B E F) }}} C-1  Category Colour = blueIF tumour lookup is successful    forward to corresponding leaf Else   broadcast to every one For each query triple t(s, p, o)  T A = {chromosome, result, code}  G = {start, stop} B = {DNA-Methy  E = {RPKM} Tumours  SPARQL endpoints C-2  Category Colour = pinkC-3  Category Colour = green1-16  17-33           1-5      6-11  12-16  17-22  23-27  28-33 1-4      5-8     9-12  13-16 17-20  21-24 25-27 28-30  31-33  Figure 3 TCGA data distribution/load balancing and source selection. The proposed data distribution and source selection diagram forhosting the complete Linked TCGA data.chromosome n beta_value16 28890100 0.4392713035849373 57743543 0.2451476653814617 15725862 0.04401610611963472 177029073 0.74134292703895311 93862594 0.029071382111447914 93813777 0.98555543668101918 11980953 0.010983200573291214 89290921 0.0104525957219692composite element REF gene_symbol chromosome n beta_valuecg00000292 ATP2A1 16 28890100 0.439271303584937cg00002426 SLMAP 3 57743543 0.245147665381461cg00003994 MEOX2 7 15725862 0.0440161061196347cg00005847 HOXD3 2 177029073 0.741342927038953cg00006414 ZNF425 7 148822837 NAcg00007981 PANX1 11 93862594 0.0290713821114479cg00008493 COX8C 14 93813777 0.985555436681019cg00008713 IMPA2 18 11980953 0.0109832005732912cg00009407 TTC8 14 89290921 0.0104525957219692@prefix  @prefix .@prefix @prefix -rdf-syntax-ns#type>.@prefix  @prefix @prefix  @prefix b:TCGA-A2-A0CX d: "TCGA-A2-A0CX". b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d1 . b:TCGA-A2-A0CX-d1 c: w: ; m: "16"; v: "28890100"; u: "0.439271303584937". b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d2 . b:TCGA-A2-A0CX-d2 c: w: ; m: "3"; v: "57743543"; u: "0.245147665381461". b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d3 . b:TCGA-A2-A0CX-d3 c: w: ; m: "7"; v: "15725862"; u: "0.0440161061196347". b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d4 . b:TCGA-A2-A0CX-d4 c: w: ; m: "2"; v: "177029073"; u: "0.741342927038953". b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d5 . b:TCGA-A2-A0CX-d5 c: w: ; m: "11"; v: "93862594"; u: "0.0290713821114479". b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d6 . b:TCGA-A2-A0CX-d6 c: w: ; m: "14"; v: "93813777"; u: "0.985555436681019". b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d7 . b:TCGA-A2-A0CX-d7 c: w: ; m: "18"; v: "11980953"; u: "0.0109832005732912". b:TCGA-A2-A0CX r: b:TCGA-A2-A0CX-d8 . b:TCGA-A2-A0CX-d8 c: w: ; m: "14"; v: "89290921"; u: "0.0104525957219692". Data RefinerRDFizerRefinedRDFizedRawFigure 4 Text to RDF conversion process example. An example showing the refinement and RDFication of the TCGA file.Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 8 of 18http://www.jbiomedsem.com/content/5/1/47Listing 3 Excerpt of the LIMES link specification for linkingTCGA and Homologene<SOURCE><ID>TCGA</ ID><ENDPOINT>dna_methylat ion450_Lookup . nt </ENDPOINT><VAR>?x </VAR><PAGESIZE>?1</PAGESIZE><RESTRICTION>?x rd f : t ype tcga?schema :dna_methy la t ion450_ lookup </RESTRICTION><PROPERTY> tcga?schema : Gene_Symbol AS lowerca s e</PROPERTY><TYPE>N?TRIPLE </TYPE></SOURCE><TARGET><ID>homologene </ ID><ENDPOINT>ht tp : / / homologene . b i o 2 rd f . org / s p a r q l</ENDPOINT><VAR>?y </VAR><PAGESIZE>10000 </PAGESIZE><RESTRICTION>? y a homologene : HomoloGene_Group</RESTRICTION><PROPERTY>homologene : has_gene_symbol AS lowerca s e</PROPERTY></TARGET><METRIC>exactmatch ( x . tcga?schema : Gene_Symbol ,y . homologene : has_gene_symbol ) </METRIC><ACCEPTANCE><THRESHOLD>1</THRESHOLD><FILE>dna_450_homologene_accepted . nt </ FILE><RELATION>tcga?schema : Homologene </RELATION></ACCEPTANCE>TCGA data workflow and schemaTo devise a fast, big data driven query federation engine,we started by exploiting how the various files and typesof data in TCGA are interconnected. To date, 23054 rawdata files from 28 cancer tumours have been collected,summing up to a total of 28.3 TB of data [55]. For eachlevel 3 data, we have identified three different types, i.e.,we RDFized level 3 data for each cancer type and furtherdefine 3 data types for each of the level 3 tumours dataof data. The resulting data are organized as a three layerarchitecture where layer 1 contains patient data, layer 2consists of clinical information and layer 3 contain resultsfor different samples of a patient. Each type of data isassigned to a different class in the RDFized version asdepicted in Figure 5. For each patient, tumour and blood-/normal tissue samples are collected and divided intodifferent portions upon which different protocols such asDNA, RNA and so on, are applied to extract the ana-lytes for the analysis of the sample. The extracted analytesare distributed across plates. All these plates contain-ing patients tumour and normal samples are shipped toGenome Characterization Centres (GCCs) and GenomeTable 1 Statistics for 27 tumours sorted by number of triplesTumour type Raw(GB) Refined(GB) RDF(GB) Triples(Million)Lymphoid Neoplasm Diffuse Large 0.37 0.20 0.83 35B-cell Lymphoma (DLBC)Cutaneous melanoma (UCS) 1.2 0.64 2.6 113Glioblastoma multiforme (GBM) 2.3 0.77 2.8 132Esophageal carcinoma (ESCA) 1.5 0.88 3.4 149Adrenocortical carcinoma (ACC) 1.6 0.90 3.6 158Pancreatic adenocarcinoma (PAAD) 2.6 1.1 4.5 200Kidney Chromophobe (KICH) 3.7 1.4 5.3 242Sarcoma (SARC) 3.8 1.5 5.9 267Cervical (CESC) 8.75 2.44 8.86 400.19Ovarian serous cystadenocarcinoma (OV) 8.2 2.4 8.7 410Rectal adenocarcinoma (READ) 8.07 2.25 9.04 413.31Papillary Kidney (KIRP) 10.40 2.90 10.4 469.65Stomach adenocarcinoma (STAD) 5.5 2.9 12 529Liver hepatocellular carcinoma (LIHC) 8.2 3.1 12 550Bladder cancer (BLCA) 12.16 3.39 12.3 556.38Acute Myeloid Leukemia (LAML) 14.85 4.14 15.1 684.05Lower Grade Glioma (LGG) 17.08 4.76 17.1 778.82Prostate adenocarcinoma (PRAD) 18.05 5.03 18.1 821.01Lung squamous carcinoma (LUSC) 20.63 5.75 20.5 927.08Cutaneous melanoma (SKCM) 23.22 6.47 23.2 1050.94Uterine Corpus Endometrial Carcinoma (UCEC) 13 5.98 24.2 1070Colon adenocarcinoma (COAD) 18 6.64 26 1175v Head and neck squamous cell(HNSC) 27.6 7.69 27.5 1245.37Lung adenocarcinoma (LUAD) 23 9.1 36 1611Kidney renal clear cell carcinoma (KIRC) 24 9.4 37 1658Thyroid carcinoma (THCA) 26 10.1 40 1796Breast invasive carcinoma (BRCA) 45 17 65 2959A total of 20.4 Billion triples.Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 9 of 18http://www.jbiomedsem.com/content/5/1/47Table 2 Excerpt of the links for the lookup files of TCGASource Target Class # links Runtime (ms)DNA27 HGNC Genes 23,181 154DNA27 Homologene Genes 27,654 193DNA27 OMIM Genes 15,171 158DNA450 Homologene Genes 489,643 5,710DNA450 OMIM Genes 212,284 429DNA27 HGNC Chromosomes 108,662 96DNA27 OMIM Chromosomes 16,039,535 8,055The source column shows the name of the look-up file that was linked to thetarget dataset named in the second column. The class column shows the type ofresources that were linked. The fourth column shows the number of links thatwere generated while the runtime column shows the time required by LIMES tocarry out the linking process in ms.Sequencing Centres (GSCs) that produce different datatype results which are shown in layer 3 (cf. Figure 5).The schema of the corresponding Linked TCGA is shownin Figure 6. We have included only important proper-ties from clinical data (e.g., drug, follow-up, radiationetc.) as the complete list of properties is around 300.This diagram is useful to understand the connectivitybetween the Linked TCGAdata and to formulate SPARQLqueries.Data distribution and load balancingA key property of the federation method described hereis the efficient distribution of the data among SPARQLendpoints to enable access to around 20 billion resultingtriples in a virtual integrated manner, i.e., the requireddata are transparently collected from different SPARQLendpoints. Proper load balancing among SPARQL end-points is also ensured to reduce the query execution time.To this end, we have divided each tumour data into threecategories, each of which is assigned a different colour blue, pink and green  as shown in Figures 3 and 7. Thegreen category contains only methylation results, pinkcontains expression exon results and all other data areTable 3 Excerpt of the links for themethylation results of asingle patientSource Target Class # links Runtime (ms)Methylation HGNC Chromosomes 97,530 205Methylation OMIM Chromosomes 14,407,269 6,095Gene expression HGNC Chromosomes 86,052 80Gene expression OMIM Chromosomes 12,535,829 4,679The source column shows the name of the patient file that was linked to thetarget dataset named in the second column. The class column shows the type ofresources that were linked. The fourth column shows the number of links thatwere generated while the runtime column shows the time required by LIMES tocarry out the linking process in ms.grouped in the blue category. The ratio of the sizes is 1:3:4for blue, pink, and green respectively.In order to achieve proper load balancing, if we allo-cate one SPARQL endpoint to the blue category data(smallest) then we must assign three SPARQL endpointsto pink and four SPARQL endpoints to the green cat-egory data. We propose 17 SPARQL endpoints to beassigned for the complete TCGA level 3 data (around33 tumours expected) distribution as shown in Figure 3.We assigned two SPARQL endpoints for blue, six end-points for pink and nine endpoints for green categorydata.Data are also balanced across each of the colouredcategory SPARQL endpoints according to cancer type(tumour). For example, in blue category, tumours1-16 are stored in the first blue SPARQL endpoint andthe remaining tumours (17-33) are stored in the secondblue SPARQL endpoint. It is important to note that wehave RDFized 27 tumours while in our data distribu-tion diagram we show 33 tumours. This is because weare expecting around 33 cancer tumours [56] data to bemade available by the TCGA data portal in the future.To achieve a similar size-oriented division, each of theSPARQL endpoints in the pink category contains eitherfive or six tumours data as shown in Figure 3 and eachof the first six SPARQL endpoints in the green categorycontain data for four tumours and each of the remain-ing three SPARQL endpoints contain three tumours data.Each of the three categories is used to create a con-ditional statement (labelled C-1, C-2, and C-3 given inListing 4), used by the federated engine for source selec-tion. For source selection, the predicates sets shown inFigure 3 (D, C, B, M, F, E, A and G) are also relevant.We further explain the decision model in Source Selectionsub-section.Listing 4 Conditions for colour category selectionC?1 = { { p ? {D ? A ? G} ? { p = rd f : t ype ? o ? C} }? { { S?J o i n ( p , D ? C) ? P?J o i n ( p , D ? C) }? { ! S?J o i n ( p , M ? B ? E ? F ) ? ! P?J o i n ( p , M? B ? E ? F ) } } }C?2 = { { p ? { E ? A ? G} ? { p = rd f : t ype ? o ? F } }? { { S?J o i n ( p , E ? F ) ? P?J o i n ( p , E ? F ) }? { ! S?J o i n ( p , M ? B ? D ? C) ? ! P?J o i n ( p , M? B ? D ? C) } } }C?3 = { { p ? {M ? A} ? { p = rd f : t ype ? o ? B } }? { { S?J o i n ( p , M ? B) ? P?J o i n ( p , M ? B) }? { ! S?J o i n ( p , E ? F ? D ? C) ? ! P?J o i n ( p , E? F ? D ? C) } } }TopFed federated query processing approachBefore going into the details of our federated query pro-cessing model shown in Figure 7, we first briefly explainTopFeds index which comprise of an N3 specificationfile and a Tissue Source Site to Tumour (TSS-to-Tumour)Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 10 of 18http://www.jbiomedsem.com/content/5/1/47Layer 1Layer 3: ResultsLayer 3: Results Layer 2: Clinicalpashipmentaliquotsampleslideprotocolsnp_resultexpression_protein_resultmiRNA_resultdna_meth _resultcopy_number_resultanalytefollow _Upradi ondrugexpression_gene_resultexpression_exon_resultexpression_protein_lookupmiRNA_lookupdna_methyla n_lookupexpression_exon_lookupexpression_gene_lookupFigure 5 TCGA class diagram of RDFized results. Each level 3 data is further divided into three layers where: layer 1 contains patient data, layer 2consists of clinical information and layer 3 contain results for different samples of a patient.hash table. The N3 specification file, shown in Listing 5,is devised based on the data distribution described inprevious section. It contains metadata relevant for datadistribution across SPARQL endpoints. For each SPARQLendpoint, its colour category, endpoint url, and the list oftumours data stored therein are specified. Moreover, thespecification file also contains the various sets of predi-cates. In addition, we also create a Tissue Source Site toTumour (TSS-to-Tumour [57]) hash table that containskey value pairs for TSS to tumour name. The TSS is theFigure 6 Linked TCGA schema diagram. The schema diagram of the Linked TCGA, useful for formulating SPARQL queries.Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 11 of 18http://www.jbiomedsem.com/content/5/1/47Query EngineParserFederator OpmizerIntegratorBlue SPARQL endpointsb1Pink SPARQL endpointsSpecificaonFileTSS-to-TumourHash TableResultsResultsSPARQL QuerySub-query Sub-queryGreen SPARQL endpointsb2 p1 p1 p6 g1 g1 g9Figure 7 TopFed federated query processing model. TCGAtailored federated query processing diagram, showing systemcomponents.location identifier from where the results of the differenttissues are obtained. This hash table was formed usingFile_Sample_Map files (containing file to patient bar-code entries) provided as meta data, with every TCGAarchive download via its Data Matrix portale. This metafile provides a list of patient barcodes belonging to aparticular cancer tumour. We extract the TSS part ofpatient barcodef and use this along with tumour nameas a hash entry. Both N3 specification file and TSS-to-Tumour hash table are used by our federated queryprocessor for efficient relevant data source (SPARQLendpoints) selection, which is explained in the nextsub-section.Listing 5 Part of the N3 specification file@pref ix t c ga : < h t tp : / / t c g a . d e r i . i e / schema / > .<h t tp : / / t c g a . d e r i . i e / s e t / setA > t cga : setName "A " ;t c ga : s e tE l ement s " r e s u l t " , " chromosome " , " bcr_ { p }a t i e n t _ { b } arcode " .<h t tp : / / t c g a . d e r i . i e / s e t / setE > t cga : setName "E " ;t c ga : s e tE l ement s "RPKM" .<h t tp : / / t c g a . d e r i . i e / s e t / setG > t cga : setName "G " ;t c ga : s e tE l ement s " s t a r t " , " s top " .<h t tp : / / t c g a . d e r i . i e / s e t / setM> tcga : setName "M" ;t cga : s e tE l ement s " p o s i t i o n " , " be ta_ { v } a l u e " .< h t tp : / / t c g a . d e r i . i e / endpoint / blue1 > t cga : c a t e go r y" b lue " ;t c g a : endpo intUr l " h t tp : / / 1 0 . 1 9 6 . 2 . 2 1 4 : 8 8 9 0 / s p a r q l" ;t c g a : containTumours "BLCA" , "CESC " , "HNSC" , " KIRP" , "LAML" .<h t tp : / / t c g a . d e r i . i e / endpoint / blue2 > t cga : c a t e go r y" b lue " ;t c g a : endpo intUr l " h t tp : / / 1 0 . 1 9 6 . 2 . 1 2 3 : 8 8 9 0 / s p a r q l" ;t c g a : containTumours "LGG" , "LUSC" , "PRAD" , "READ" , "SKCM" .<h t tp : / / t c g a . d e r i . i e / endpoint / pink1 > t cga : c a t e go r y" pink " ;t c g a : endpo intUr l " h t tp : / / 1 0 . 1 9 6 . 2 . 1 3 0 : 8 8 9 0 / s p a r q l" ;t c g a : containTumours "BLCA" , "CESC " , "HNSC " .Given a SPARQL query, it is first parsed and thensent to the federator that makes use of the N3specification file along with the TSS-to-Tumour hashtable, in order to find the relevant sources for eachof the triple pattern using Algorithm 1. The opti-mizer makes use of the source selection to generatean optimized sub-query execution plan. The opti-mized sub-queries are then forwarded to the relevantSPARQL endpoints. The results of each sub-query exe-cution are integrated and the final query result set isgenerated.Source selectionThe goal of the source selection is to find the optimallist of relevant sources (i.e., SPARQL endpoints) againstindividual query triple pattern. According to the distri-bution of Figure 3, if we can infer the category colourand tumour number for a triple pattern then we onlyneed to query a single endpoint for that triple pattern.For example, starting from the root node of Figure 3,we can go to the second level of the tree by knowingthe category colour (blue, pink, and green). Further, atsecond level, if we know the tumour number then wecan reach to a single SPARQL endpoint to query. Foreach query triple pattern, our source selection algorithmtries to get such information using the specification fileand type (star, path) of the join between the query triplepatterns.A star join between two triple patterns is formed ifboth of the triple patterns share the same subject. Con-sider the query given in Listing 6: the first two triplepatterns form a star join and the last four triple patternsform a second star join. A path join between two triplepatterns is formed if object of the first triple pattern isused as subject of the second triple pattern. For example,the second triple pattern form a path join with the thirdtriple pattern in the query shown in Listing 6. Moreover,every TCGA patient is uniquely identified by its barcodeof the format <TCGA-TSS-PatientNo>. For example,the patient barcode used in the first triple pattern ofthe Listing 6 query has a TSS identifier 18 and patientnumber 3406. This means we can infer tumour name/number from patient barcode using the TSS to tumourhash table.Listing 6 TCGA query with bound predicate{? s t c g a : bcr_ { p } a t i e n t _ { b } arcode "TCGA-18-3406" .? s t c g a : r e s u l t ? recordNo .? recordNo tcga : chromosome ? chromosom .? recordNo tcga : s t a r t ? s t a r t .? recordNo tcga : s top ? s top .? recordNo tcga : seq_ {m} ean ?mean .}Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 12 of 18http://www.jbiomedsem.com/content/5/1/47Algorithm 1 triple pattern source selectionRequire: Dblue = {b1, b2};Dpink ={p1, p2, ... p6};Dgreen ={g1,g2,... g9}; T = {t1, t2, ...tn}; tumourNo //data sources,query triple patterns, tumour number (can be null)1: for each bgp ? T do //each BGP in query2: for each ti ? bgp do //each triple pattern in BGP3: sources = null; c1Sources = null; c2Sources =null; c3Sources = null; type = null; s = subj(ti); p =pred(ti); o = obj(ti)4: if bound(s) then //if subject is bound5: catColour = s.getCategorycolour() //get cate-gory colour from subject6: tNo = s.getTumour() //get tumour from sub-ject7: if catColour = blue then8: sources = Dblue9: else if catColour = pink then10: sources = Dpink11: else if catColour = green then12: sources = Dgreen13: end if14: Si = sources.filter(tNo) //this will return a sin-gle capable source15: else if bound(p) then //if predicate is bound16: if C-1 then17: c1Sources = Dblue18: end if19: if C-2 then20: c2Sources = Dpink21: end if22: if C-3 then23: c3Sources = Dgreen24: end if25: sources = c1Sources ? c2Sources ? c3Sources26: if sources = null then27: sources = Dblue //only check for clinicalproperties28: end if29: if tumourNo = null then30: Si = sources.filter(tumourNo)31: else32: Si = sources33: end if34: else if !bound(p) ? !bound(s) then //if onlyobject is bound35: // prune selected sources with ASK queries36: for each si ? {Dblue ? Dpink ? Dgreen} do37: if ASK(si, ti) = true then38: Si = Si ? {si}39: end if40: end for41: end if42: return Si //reutrn the set of relevant sources fortriple pattern ti43: end for44: end forAs discussed in the Data distribution section, we havecategorized all SPARQL endpoints into three different cat-egory colours named blue, pink, and green. Our sourceselection algorithm (cf. Algorithm 1) requires the set ofSPARQL endpoints in each of the colour category andstores three different sets named Dblue,Dpink , and Dgreen.Moreover, it requires the tumour number tumourNo,which can be null and is obtained from the query as follow:if a triple pattern with predicate tcga:bcr_patient_barcodeand bound object containing the patient barcode form astar join with a triple pattern having predicate tcga:result,then by using the patient barcode value specified in theformer triple pattern can be used to get the requiredtumour number using TSS-to-Tumour hash table. Oursource selection algorithm runs for each basic graph pat-tern (BGP [58]) and for each individual triple pattern ofBGP as follow.If subject of the triple pattern is bound then we can getboth the category colour and tumour name from the sub-ject URI. The format of the TCGA URI is <http://tcga.deri.ie/Patient_barcode-ResultType>. The tumour namecan be obtained from Patient_Barcode and the categorycolour can be inferred from ResultType. For example, ifthe first character is e (shortcut for exon-expression), thenit belongs to the pink category. However, if the first char-acter is d (shortcut for dna-methylation), then it belongsto the green category and all other characters belong tothe blue category. Consider the query given in Listing 7:the tumour name can be obtained using hash table lookupfor TSS 18 and the colour category is pink.Listing 7 TCGA query with bound subject{<h t tp : / / t c g a . d e r i . i e /TCGA?18?3406?e266 > t cga : s t a r t? s t a r t .< h t tp : / / t c g a . d e r i . i e /TCGA?18?3406?e266 > t cga : s top ?s top .<h t tp : / / t c g a . d e r i . i e /TCGA?18?3406?e266 > t cga :RPKM ?rpkm .}Source selection for a triple pattern with only boundpredicate is more challenging. We have divided variouspredicates and classes of the TCGA data into different setsthat are shown in Listing 8. Set D contains all the pred-icates that uniquely identify the blue category and set Ccontains a list of classes specific to it. The sets B and Muniquely identify the methylation, i.e., the green categorywhile sets F and E are for the pink category. Sets A andG contain predicates that can be found in more than onecolour category. Starting from the root of the source selec-tion tree, if the condition C-1 given in Listing 4 holds thenall of the sources in blue category are relevant for thattriple pattern. This means that if predicate p of the triplepattern is set member of {D ? A ? G} or it is equal toSaleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 13 of 18http://www.jbiomedsem.com/content/5/1/47rdf:type and the object o belongs to set C and either thestar or path join between p and {D ? C} is true or the starand path join of p with {M ? B ? E ? F} is false, then all ofthe sources in the blue category are relevant.Listing 8 Predicate and class setsD = { seq_ {m} ean , r eads_ { p } er_ {m} i l l i o n _ {m} i r n a _ {m}apped , s c a l e d _ { e } s t imate , p r o t e i n _ { e } xp r e s s i on_{ v } a l u e }C = { copy_ { n } umber_ { r } e s u l t , snp_ { r } e s u l t ,e xp r e s s i on_ { g } ene_ { r } e s u l t , e xp r e s s i on_ { p }r o t e i n _ { r } e s u l t ,mirna_ { r } e s u l t , C l i n i c a l }B = { dna_ {m} e t h y l a t i o n _ { r } e s u l t }M = { be ta_ { v } a lue , p o s i t i o n }F = { exp r e s s i on_ { e } xon_ { r } e s u l t }E = {RPKM}A = { chromosome , r e s u l t , bcr_ { p } a t i e n t _ { b } arcode }G = { s t a r t , s top }Consider the third triple pattern of the query given inListing 6: the predicate chromosome is set member ofA, which means this predicate can be found in all of theendpoints. However, chromosome has a star join withseq_mean, which is unique for the blue category sources.Therefore, instead of selecting all of the sources (overes-timated as in FedX, SPLENDID etc.), TopFed will onlyselectDblue as relevant sources that can be further filtered,provided that the tumourNo given as input to Algorithm 1is not null. Similarly, C-2 holds for Dpink and C-3 holdsfor Dgreen relevant source selection. It is important to notethat more than one condition (C-1, C-2, C-3) can be truefor a triple pattern, therefore we check each of the threeconditions individually and make a union of the sourcesas given in line 24 of Algorithm 1. Further, if none of thecondition is true then we need to query the blue categorysources because we did not list many of the blue categorypredicates as they are numerous.For a triple pattern with bound object, we send SPARQLASK queries including the triple pattern to all of thesources and select sources that pass the test. This is sim-ilar to the source selection technique used in FedX for allthe triple patterns. Along with Algorithm 1, Figure 3 alsoprovides a visual demonstration of our triple pattern-wisesource selection.As an example, consider the query of Listing 6 andthe data distribution given in Figure 3. TopFed selectsone source for the first triple pattern because we canobtain tumour number from the given patient barcodeand this triple pattern only passes C-1. FedX selects threesources since every patient data can be found in each ofthe three colour categories exactly at one SPARQL end-point. For the second triple pattern, TopFed again selectsonly one source because C-1 only holds. However, FedXselects all of the 17 sources as predicate tcga:result canbe found in all of the endpoints. For each of the remain-ing triple patterns (3 to 6), TopFex selects only one sourceas tcga:seq_mean is unique for the blue category end-points and the others triple patterns (3 to 5) has starjoin with it. We have only two endpoints in blue cate-gory, which is filtered to one using the tumour numbergiven in triple pattern 1. FedX selects all of the 17 sourcesfor tcga:chromosome, eight sources each for tcga:start,tcga:stop, and two sources for last triple pattern. In total,TopFed selects only six sources while FedX selects 52 toanswer this query. Additionally, FedX also needs to send102 (6*17) SPARQL ASK queries. We want to emphasizethat we have replaced only source selection algorithm ofFedX. The join order optimization and the join implemen-tation remains the same.Results and discussionEvaluationThe goal of this evaluation is to support the claim thatTopFed selects a significantly smaller number of sourcesfor the same recall as FedX, thus achieving a good queryexecution performance for large datasets. We compareTopFed with the state-of-the-art approach for query fed-eration (FedX) both in terms of the total number ofsources selected and the execution time to achieve a100% recall, using 10 TCGA benchmark SPARQL queriesgof different shapes (i.e. star, path, and hybrid). A tex-tual description of all the benchmark queries is given inTable 4. FedX has been shown previously [36,45] to be theTable 4 Benchmark queries descriptionsQuery DescriptionQ1 Get the chromosome, start, stop and mean copy numbervalues of the patient TCGA-18-4721 for genome locations554268 to 5994290Q2 Get the chromosome, start, stop and mean exon-expressionvalues of all the TCGA patientsQ3 Get the chromosome, position and mean methylation valuesof all the TCGA patientsQ4 Get the chromosome, start and stop values of the TCGA patientTCGA-C4-A0F6Q5 Get the chromosome, start, stop values of all the TCGA patientsQ6 Get the chromosome, start, stop and miRNA values of the 20threcord of TCGA patient TCGA-AB-2821Q7 Get the chromosome, start and stop values of the TCGA patientTCGA-AB-2823 for mean sequence value of 0.0839Q8 Get the chromosome, start, stop, mean protein expression andmean exon-expression values of the TCGA patientTCGA-18-3410Q9 Get the chromosome, mean gene expression and meanmethylation values of the TCGA patient TCGA-C5-A1BFQ10 Get the chromosome, mean gene expression, mean exonexpression and mean methylation values of all theTCGA patientsThe corresponding SPARQL queries can be downloaded from http://goo.gl/UxUEXk.Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 14 of 18http://www.jbiomedsem.com/content/5/1/47fastest and more precise SPARQL federated query engine(to the best of our knowledge). Therefore, we evaluateTopFeds query performance by comparing it with FedX.TCGA benchmark setupTCGA benchmark data consists of genomic results from25 patients randomly selected from ten different tumourtypes and distributed across ten local SPARQL endpointswith the specifications given in Table 5. Furthermore, thebenchmark N3 specificationh file (used in the currentexperiments) assigns two, three, five SPARQL endpointsto the blue, pink, and green categories respectively.We have selected ten SPARQL queries based on expertopinion reflecting typical requests on TCGA data. Fur-ther, we have categorized our benchmark queries into fourdifferent quadrants as shown in Table 6. A single colourquery collects results from SPARQL endpoints listed inone of the three colour categories. A cross-colour querytargets more than one colour category results. A hybridquery contains both star and path joins between varioustriple patterns. Moreover, we can also obtain the tumournumber (to be used as input to Algorithm 1) from all ofthe hybrid queries. All of the benchmark data, includingbenchmark queries, can be found at the project website.Experimental resultsIn order to show the effects of source selection on perfor-mance (runtime + recall of sources selected), the numberof sources selected for each triple pattern of the query areadded (equation 1). Let mi equal the number of sourcescapable of answering a triple pattern ti and S is the totalnumber of available sources (10 in our benchmark). Then,for a query q with triple patterns {t1, t2, . . . , tn}, the totalnumber of sources selected (triple pattern-wise sourcesselected) is given in equation 1.total number of sources selected =n?t=1mt : 0 ? mt ? S(1)Table 5 Benchmark SPARQL endpoints specificationsSPARQL endpoint CPU RAM Hard diskvirtuoso-blue1 2.2 GHz, i3 4 GB 300 GBvirtuoso-blue2 2.6 GHz, i5 4 GB 150 GBvirtuoso-pink1 2.53 GHz, i5 4 GB 300 GBvirtuoso-pink2 2.3 GHz, i5 4 GB 500 GBvirtuoso-pink3 2.53 GHz, i5 4 GB 300 GBvirtuoso-green1 2.9 GHz, i7 16 GB 256 GB SSDvirtuoso-green2 2.9 GHz, i7 8 GB 450 GBvirtuoso-green3 2.6 GHz, i5 8 GB 400 GBvirtuoso-green4 2.6 GHz, i5 8 GB 400 GBvirtuoso-green5 2.9 GHz, i7 16 GB 500 GBTable 6 Benchmark queries distributionSingle Colour Cross-ColourStar 2 2Hybrid (star + path) 2 4The source selection results are shown in Figure 8.Overall, our source selection algorithm selects on aver-age less than half of the sources selected by FedX. This isdue to the possible overestimation of the sources by FedXwhile using SPARQL ASK queries for relevant sourceselection [16]. For example, any data source will likelymatch a triple pattern (?s, rdf:type, ?o). However, the samesources might not lead to any results at all once the actualmappings for ?s and ?o are included in a join evalua-tion. On the contrary, our source selection algorithm wasdesigned to resolve the join types between query triplepatterns specifically to avoid such overestimation (whichcan later greatly increase the query processing time asreflected in Table 7). Only in queries 5 and 10, TopFedselected sources are equal to FedX. The explanation forthis can be found in the amount of useful informationavailable in each query - both query 5 and query 10 aregeneric queries from which a tumour or a performance-improving colour category cannot be derived, becauseall logic conditions are exactly satisfied. Overall, TopFedselects the optimal (the actual required sources) num-ber of sources with 100% recall for all of the benchmarkqueries.We have performed a two-tailed heteroscedastic t-testbased on a sample of 10 (each query was run 10 times) tocompare the source selection execution time. The sourceselection execution time and the standard error (S.E)obtained are presented in Table 8. On average, our sourceselection algorithm only requires 17 msec per query. Thisis because our N3 specification file is much smaller (only43 lines) and we have created an in-memory Sesamerepository to load and access this file. For the first run, theFedX source selection execution time is much higher. Thisdelay is caused by the query engine sending a SPARQLASK query for each of the query triple patterns, and foreach of the sources. As explained above, FedX needs toissue 102 SPARQL ASK queries to perform source selec-tion for the query in Listing 6 and the data distributionin Figure 3. In order to minimise the number of SPARQLASK queries, FedX makes use of the cache to store theresult of the recent SPARQL ASK request. Every time aquery is issued, the engine first looks for a cache hit beforeissuing the actual SPARQL ASK query. To show the effectof the cache, we have rerun the same query 10 times afterthe first run and we have noticed a reasonable improve-ment. For a complete cached entries (100% cache hit),our source selection execution time is still comparableSaleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 15 of 18http://www.jbiomedsem.com/content/5/1/4705101520253035401 2 3 4 5 6 7 8 9 10 AvgFedX total number of sources selectedTopFed  total number of sources selectedFigure 8 Efficient source selection. Comparison of the TopFed and FedX source selection in terms of the total number of triple pattern-wisesources selected. Y-axis shows the total triple pattern-wise sources selected for each of the benchmark query given in X-axis.with FedX. It is important to note that all queries thatare not specific to a patient (i.e, queries 2, 3, 5, 10), theTopFed source selection time is small (less than 10 msec).The reason is that the tumour number cannot be inferredfrom these queries and as a result less computation (indexlookups) is required in the source selection Algorithm 1.In Table 7, we compare the execution time of TopFedand FedEx for all of the benchmark queries using a two-tailed heteroscedastic t-test based on a sample of 10. Itis important to mention that the query execution timewas measured when the first result was retrieved, i.e.,we did not iterate over all results. As an overall perfor-mance evaluation, the query execution time of TopFedis about one third to that of FedX. Specifically, TopFedsignificantly outperforms FedX in benchmark queries 2and 3 related to exon expression and methylation, respec-tively. These queries select the complete set of results forall of the 25 patients. TopFed is able to infer from thequery that the category colour should be pink and green,respectively, and issue the complete query to only theendpoints in the corresponding colour categories. In con-trast, FedX is not able to perform such pre-processing,hence issuing the query to all endpoints. As a result, ithas to collect results from all of the endpoints in the blue,pink, and green categories when only one of the cate-gories can produce results for each query. As an exampleof the FedX approach addressing query 2, the triple pat-tern (?recordNo, tcga:chromosome, ?chromosom) reliesTable 7 Comparison of average execution time for each query (based on a sample of 10)FedX(first run) FedX(cached) TopFedQuery no Execution time(msec) Execution time(msec) S.E Execution time(msec) S.E1 913 401.2 5.22 341.5* 5.602 81619 81170.7 655.93 866.5* 22.083 82271 81817.8 653.22 666* 27.124 1199 367.6 6.88 262.7* 7.355 80423 78723.5 459.43 78691.5 458.706 837 416.9 8.38 246.1* 3.567 921 399.6 4.41 248.1* 7.208 900 89 2.45 72.7* 1.529 950.3 76.8 2.16 63.3* 1.8910 912 63.6 1.99 49.6* 1.02Average 25094.53 24352.67 180.01 8150.8 53.60*Significant improvement.Saleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 16 of 18http://www.jbiomedsem.com/content/5/1/47Table 8 Comparison of source selection average execution time (based on a sampling of 10)FedX(first run) FedX(cached) TopFedQuery no Execution time(msec) Execution time(msec) S.E Execution time(msec) S.E1 530 11.7 0.35 28.1 0.982 487 11.4 0.67 5.2 0.573 470 11.9 0.78 5 0.424 510 12 0.52 23.6 1.575 473 9.8 0.65 4.8 0.296 371 9.9 0.38 21.7 0.687 521 10 0.39 24.4 0.768 483 9.5 0.45 29.5 0.869 496 9.8 0.39 20.1 0.9910 456 10.6 0.40 7.4 0.58Average 479.7 10.66 0.50 16.98 0.77on retrieving the results from all of the endpoints in boththe blue and green categories, only to return an emptyset of results, after making a star join with the triple pat-tern (?recordNo, tcga:RPKM, ?RPKM). We expect thatour approach will generally lead to much faster resolu-tion for queries of this nature, where a large number oftriples is retrieved for a specific colour category. Thisreflects the improvement that TopFeds engine is able todetermine those queries that will return empty sets priorto requesting the data. Although the benchmark query 5results in a very large set of triples, the execution time forboth systems is almost the same. As pointed out above,the reason for this is that the query is too generic andit is impossible to infer the category colour or tumournumber.ConclusionIn this work, we have published a Linked Data versionof TCGA data level 3 (to the best of our knowledge thelargest Linked Data dump anywhere) and further linkedit to the LOD cloud. This big data resource is designedto be used as infrastructure for biomedical and bioinfor-matics applications that analyse and query both the fileannotations but also the internal content of the patient-derived files of this key reference for molecular biologyand epidemiology of cancer.Anticipating usages that traverse to other related bigdata resources, we have also generated links to otherLOD data dumps such as HGNC, OMIM and Homolo-gene. We believe that this RDFication can greatly helpresearchers in the biomedical domain as the amount anddiversity of data exceeds the ability of local resourcesto handle its retrieval and parsing. The RDFized dataresource can be easily traversed from a modest machineto investigate a variety of measures at each position ofthe genome, across all types of molecular information,and across all cancer types, without the need to down-load the files and extract the pieces of information thatsatisfy the query. In fact, we would argue that this type ofanalysis will eventually be all but impossible for big dataresources like TCGA without RDFication and improvedfederation schemes such as those described in thispaper.The TCGA data dump (and what we expect will be thegenomics datasets in the future) is already too large to beeffectively handled by a single server. If the relationshipsbetween TCGA and other related resources are takeninto account, a smart data distribution framework thatdistributes the data among multiple SPARQL endpoints,such as the one reported here is, an absolute necessity.This framework, TopFed, is specifically designed as a fed-erated query processing engine that handles a collectionof physically distributed RDF data sources. The resultingvirtually integrated data resource was observed to enablesignificantly faster querying and retrieval (one third) thancurrent solutions, such as FedX. The TopFed source selec-tion algorithm achieves this result by considering themetadata about the data distribution with the type ofthe joins among query triples patterns. The substantialimprovements in efficient processing achieved, also in theuse of network traffic, suggests that the development ofsystems designed to process an individual patient clinicaldata to identify the drugs leading to better outcomes inrelated cohorts in TCGA-like resources (e.g., ICGC [59])is now at hand.One of our future aims is to develop an intelligent sys-tem, in which a cancer patients genomic data are used asinput to suggest effective drugs for treatment while com-paring against results from TCGA patients with the sameor similar cancer sub-types. In 2009, we contributed toCNViewer [60], a browser based tool that could be used,via oncologists uploading their own patients copy numberSaleem et al. Journal of Biomedical Semantics 2014, 5:47 Page 17 of 18http://www.jbiomedsem.com/content/5/1/47result, to calculate the Euclidean (or other) distance to allother patients with the same tumour type. With TopFed,not only we can calculate these distances using copy num-ber results, but in future work we expect to use aggrega-tion/correlation of molecular results to match and betterunderstand both the biology driving cancer and the mosteffective treatment for a patient given a set of geneticalterations.Availability of supporting dataThe TCGA data is available under the original TCGAData Use Certification Agreement [61] and TopFed sourcecode along with utilities are available under GNU GPL v3licence at the project home page https://code.google.com/p/topfed/.EndnotesaURLs of SPARQL endpoints hosting five cancerhistologies that are shown in Figure 1 can be found athttp://tcga.deri.ie/.bA step-by-step user manual is also available at: http://goo.gl/0oTAKV.cAvailable to download from: http://tcga.deri.ie/dumps/.dSee http://www.w3.org/TR/sparql11-query/ for moreinformation on federated queries based on SPARQL 1.1.eTCGA Data Matrix: https://tcga-data.nci.nih.gov/tcga/dataAccessMatrix.htm.fPatient barcode format: https://wiki.nci.nih.gov/display/TCGA/TCGA+Barcode.gBenchmark queries: http://goo.gl/UxUEXk.hTopFed index: https://topfed.googlecode.com/files/loadDistribution.n3.AbbreviationsTCGA: The cancer genome atlas; LOD: Linked open data; S3DB: Simple sloppysemantic database; GBM: Glioblastoma multiforme; RDF: Resource descriptionframework; DHT: Distributed hash tables; GCCs: Genome characterizationcentres; GSCs: Genome sequencing centres; TSS-to-Tumour: Tissue source siteto tumour; S.E: Standard error; ICGC: International cancer genome consortium.Competing interestsThe authors declare that they have no competing interests.Authors contributionsMS devised the system, tested it and wrote the manuscript, SSP devised thequeries and helped test the system, ACNN created links to LOD, JSA, SD andHFD participated in discussions and provided suggestions to improve thesystem. HFD wrote the biological motivation and supervised the work. AI usedthe system to generate RDF dumps of cancer tumours, hosted few tumoursRDF data to various SPARQL endpoints and contributed to the major revisionof manuscript. All authors read and approved the final manuscript.AcknowledgementsThe authors acknowledge support from the German Research Foundation(DFG) and Universität Leipzig within the program of Open Access Publishing.We would like to thank to Bade Iriabho from University of Alabama atBirmingham for uploading and maintaing all the data in the UAB servers.Further, we are thankful to Alex Parker from Foundation Medicine for providingvaluable feedback on the results of biological SPARQL query show in Figure 1.Author details1Universität Leipzig, IFI/AKSW,PO 100920, D-04009Leipzig,Germany. 2InsightCentre for Data Analytics, National University of Ireland (NUIG), Galway, Ireland.3Division Informatics, Department of Pathology, University of Alabama,Birmingham, USA. 4Foundation Medicine, Inc, Cambridge, MA 02141, USA.Received: 9 March 2014 Accepted: 3 November 2014Published: 3 December 2014JOURNAL OFBIOMEDICAL SEMANTICSAlirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35http://www.jbiomedsem.com/content/5/1/35RESEARCH Open AccessAutomated reasoning using abduction forinterpretation of medical signalsMarjan Alirezaie* and Amy LoutfiAbstractThis paper proposes an approach to leverage upon existing ontologies in order to automate the annotation of timeseries medical data. The annotation is achieved by an abductive reasoner using parsimonious covering theorem inorder to determine the best explanation or annotation for specific user defined events in the data. The novelty of thisapproach resides in part by the systems flexibility in how events are defined by users and later detected by thesystem. This is achieved via the use of different ontologies which find relations between medical, lexical andnumerical concepts. A second contribution resides in the application of an abductive reasoner which uses the onlineand existing ontologies to provide annotations. The proposed method is evaluated on datasets collected from ICUpatients and the generated annotations are compared against those given by medical experts.Keywords: Knowledge acquisition, Abductive reasoning, SensorIntroductionMedical monitoring of patients is becoming increasinglydevice supported and thus large volumes of high fre-quency data are generated from sensors that monitorphysiological parameters. While the use of such technolo-gies enables a continuous monitoring, the complexity andamount of data creates a challenge for the medical staffto provide interpretations. Furthermore, such interpreta-tions may be complex as sensor data is inherently uncer-tain, there may exist interdependencies between physicalparameters, and the data is voluminous and multivariate[1,2].Automated analysis and mining techniques have thepotential to support the medical staff in the interpreta-tion of the data. For time series data analysis this impliesa need for proper annotation of the signals with domaindependent knowledge in order to facilitate decision mak-ing and eventual diagnosis. The output generated by thealgorithms should ideally provide information that is com-patible with the knowledge and the terms used by healthpractitioners. In data-driven approaches [3] the labellingof data is limited to those pre-defined by the engineers*Correspondence: marjan.alirezaie@oru.seCenter for Applied Autonomous Sensor Systems (AASS), Dept. of Science andTechnology, Örebro University, SE-701 82, Örebro, Swedenimplementing the algorithms. On the other hand, knowl-edge driven approaches offer the possibility to moreexplicitly model the relations between higher level con-cepts and data. However, these techniques e.g. rule basedmethods, also require significant manual effort to encodedomain knowledge.At the same time, the amount of structured knowledgein the medical domain is rapidly increasing due in partby the Linked Data model. This model which is based onthe RDF model [4] allows bodies of knowledge that areindependently structured to be directly interlinked with-out any further customization efforts. For example,NCBOBioPortal [5] as a repository of biomedical ontologies con-tains more than 300 ontologies holding about 5 millionsclasses that cover medical concepts including the causesand symptoms of diseases. The rise of large and sharedmachine processable knowledge repositories provides anopportunity to automate the utilisation of information.In this paper, we propose a system which is able toreceive as input time series signals and generate as out-put an annotation of these signals. Domain knowledge isinputted into the system in a flexible manner allowing thepractitioners to express freely the terms and thresholdsthat are relevant for a particular physiological parame-ter i.e. an event. To enable flexibility, these expressionsare connected to a number of ontologies containing rela-tions between concepts expressed by the practitioner© 2014 Alirezaie and Loutfi; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use,distribution, and reproduction in any medium, provided the original work is properly credited. The Creative Commons PublicDomain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in thisarticle, unless otherwise stated.Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 2 of 16http://www.jbiomedsem.com/content/5/1/35and observations measured by the various sensors. Theontologies used are the Symptom ontology as one ofthe ontologies in BioPortal [5,6], WordNet [7] and theSemantic Sensor Network (SSN) ontologya. The symp-tom ontology provides the medical terms and definitionsdefined as concepts in a hierarchy of subsumption rela-tions which are used in the annotations of the sensordata. The WordNet ontology which consists of a lexicaldatabase of the English language enables finding relationsbetween the concepts in BioPortal and those defined bythe practitioner. The SSN ontology is used to link the spe-cific sensors to physiological parameters, and provide astandardized representation of sensors, observations andrelated concepts.The reasoning process used in this paper which finds therelations contained in the different ontologies, is abduc-tive. Abductive reasoning is chosen as it is non-monotonicand thus differs from deductive reasoning in that a logi-cally certain conclusion is not guaranteed. Rather, abduc-tive reasoning infers the best possible explanation givena set of observations. Techniques such as ParsimoniousCovering Theory (PCT) or diagnostic reasoners which areabductive are often used in the medical domain [8] asthey promote explicitation, and can contend with uncer-tainty by assessing the likelihood that a specific hypothesisentails a given conclusion [9].This paper whose main focus is more on the reason-ing method and its scalability and less on the auxiliarytechniques such as Natural Language Processing (NLP)used, evaluates the use of existing ontologies and abduc-tive reasoning to annotate sensor data from ICU patients.One benchmark dataset provided for use in 1994 AI inMedicine symposium submissions [10] and one datasetcollected at a local hospital (Section DataSets) are usedin the experimental analysis. The annotations generatedby the proposed approach are compared against the anno-tations made by experts. Also, the complexity of thereasoning method is evaluated.The paper begins with a description of related worksin Section Related work. The Linked Data model andAbductive Reasoning are then shortly introduced inSection Background. We explain the details of the frame-work in Section Method and then discuss the resultsof the reasoner and evaluate the frameworks output inSection Results and discussion. The paper ends with theconclusion and discussion in Section Conclusion.Related workIn the literature, research whose goal is to use knowl-edge driven methods to annotate time series data is foundin various fields in artificial intelligence that include sen-sor data enrichment [11,12], data stream annotation [13],symbol grounding [14,15], and semantic perception [16].Such works share the common feature where symbolicknowledge is integrated to the numeric data processing.Often high level symbolic knowledge is manually encodedbased on the requirements of the problem rather than(re)using existing knowledge already modelled in e.g.,ontologies (RDF graph model). For example, [16] and[17] have proposed reasoning techniques based on abduc-tive reasoning for data stream annotation using manuallyencoded knowledge. These works including [18] imple-mented in OWL use PCT for inferring the best possibleexplanation. However, the reasoner is restricted to gener-ate explanations with only one cause. The work presentedin [19] implements an automated reasoning which is sim-ilar to our work in the sense that the knowledge baseconsists of a RDF/OWL ontology. However, in our work,we propose an automated reasoning over external ontolo-gies modelled by different experts. Furthermore, the PCTbased reasoner in our work overcomes the constraint ofproviding an explanation containing more than one causefor the observations. This approach builds upon previouswork [20] and has formalized the reasoning process andextended the experimental evaluation.BackgroundIn this section we introduce preliminary features of theLinked Data model and abductive reasoning.Linked dataExploiting human knowledge for commonsense and auto-mated reasoning has always been a challenge. The fast-growing Webb which has traditionally been populatedwith HTML documents is known as the biggest reposi-tory of human knowledge in different domains. However,despite the fact that contents of this repository are acces-sible in the form of pages, due to the lack of semanticinterconnection among them, it is impossible for an arti-ficial agent to retrieve a specific concept. Therefore, thefirst step towards automatically using the content of Webpages is structuring these contents so that they becomeinterlinked and can be queried in different levels ofabstraction.Linked data which refers to a set of structured data,namely global data space, has become a paradigm pro-viding the transition from document oriented Web into aweb of interlinked data [21]. According to this paradigm,unstructured information represented in web pages ismapped into the RDF graph which is understood as aset of subject-predicate-object triples, T = (S ,P ,O) [4].Given U as a set of dereferenceable URIsc and L as a setof literals such as numbers or strings, the aforementionedRDF triple is defined as T ? U × U × (U ? L). In otherwords, all subjects and predicates are URIs and objectsare either a URI or a literal value. Similarly, stating the setQ = (V ? U) × (V ? U) × (V ? U ? L), where V as a setof variables is ranging over (U ? L), we can redefine theAlirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 3 of 16http://www.jbiomedsem.com/content/5/1/35triple T as an element of the query set Q. More specifi-cally, instead of feeding search engines with search terms,it is possible to fetch the desired set of triples by writinga query which is equivalent to the finite set of triples Q.Eventually, an answer for this query is simply achieved bybinding variables of the query triples into (U ? L).Different languages such as RDFS and OWL comply-ing with the Linked Data model, provide different levels ofexpressivity. Regardless of the implementation language,however, it is the uniformity and the integrability featuresof the Linked Data model that make the integration ofdifferent linked datasets straightforward.However, despite its unified structure, there are numberof issues with linked data that pose a challenge for auto-mated reasoning [22]. For instance, in order to query largesize linked datad, the query process needs to deal with theproblem of localizing relevant parts in linked data.In this paper, a biomedical repository called BioPor-tal [5] is used. Using a similar data model as the LinkedData model, BioPortal contains more than 300 ontologiesranging in subjects from anatomy, phenotype descrip-tion, to health [6]. Further details about dealing withthe aforementioned issue of size are discussed in SectionHypothesis extraction.Abductive reasoning with PCTReasoning processes are categorized into two maingroups, monotonic and non-monotonic reasoning.Monotonic reasoning including deductive reasoningimplies that inferring a new piece of knowledge doesnot change the set of already known information. Non-monotonic reasoning, on the other hand, states thatadding more knowledge can invalidate current conclu-sions. In diagnostic medical procedure where symptomsof a disease gradually emerge, monotonic reasoning dueto the permanence of its results, are less favourable. Sinceall the symptoms of a disease do not occur at a same time,the reasoner needs to be able to deal with incomplete datathroughout the reasoning process. Incompleteness mayalso extend to the high level models e.g. ontologies whichmay also be dynamically changing. A non-monotonicreasoning process whose set of answers can later beupdated is therefore useful in domains such as medicineand industrial diagnosis process [23].There are different models of non-monotonic reason-ing such as default reasoning, autoepistemic logic, beliefrevision and abductive reasoning [23]. In this work, weselected abductive reasoning with the ability of deriv-ing the best (most likely) explanations out of knownfacts. Abduction as the backbone of commonsense rea-soning and has increasingly been applied in diagnosissystems (medical domains) [24]. Diagnostic reasoning inparticular, is based on abductive logic and representsthe knowledge within a network of causal associations.The hypothesis-and-test approach of diagnosis reason-ing shows its non-monotonic behaviour where the set ofplausible causes of the observed behaviour can changewhenever the observation set extends. Parsimonious Cov-ering Theory (PCT) is a model of diagnostic reasoning [8]used in this work.PCT formalization is based on set theory and is definedwithin a quadruple T = (O,M,H, E), where O is the setof all observations which are either qualitative or quantita-tive objects;M states the set of all manifestations (events)detected over observations; subsequently, H contains allhypotheses defined as possible causes that are in rela-tions with expected events. Finally, E is the solution setindicating inferred explanations for items of M. Morespecifically, the inference process is about drawing E ? Has an explanation for elements of M ? O. However, theformalization is not complete in that it does not formal-ize the best explanation. For this, PCT suggests variouscriteria to select the final result set E . Two widely usedcriteria are: Set covering criterion is defined as a property of afunction f which is assumed to be a mapping from asubset ofH (set of all hypotheses) to a subset ofM(set of manifestations) so thatX is a possible cause forf (X ). An accepted conclusion w.r.t the set-coveringcriterion is set X ? H such that f (X ) = M. Minimum cardinality criterion is concerned aboutthe cardinality of the solution set. According to thiscriterion,R as a subset ofH is chosen as the solutionset if for all other covering subsets ofH, namely S ,|R| ? |S|.As previously mentioned, PCT is based on set the-ory. The eventual explanation is a subset of the of theHypothesis set for which aforementioned criteria hold.Selecting a subset poses an issue of the time complex-ity. Consequently, there are a number of techniques thataddress computational factors for making abductive rea-soning NP-Hard [25]. For instance, applying constraintsthat reduce the composite hypothesis set size as well asruling out criteria-violating candidates (and their superclasses) from the power set, can reduce the time complex-ity. Techniques used in this work are further discussed inSection Reasoner.MethodThe reasoner depicted in Figure 1 receives two primaryinputs, Hypothesis (H), andManifestation (M) which areseparately provided by the HypothesisExtractor and theManifestationExtractor processes, respectively. The out-put of the reasoner is called Explanation (E). Each com-ponent feeding the reasoner contains several modules thatAlirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 4 of 16http://www.jbiomedsem.com/content/5/1/35Figure 1 Sensor data annotation framework based on abductive reasoning.collaborate with ontologies including the SSN ontologyand theWordNet ontology.Considering the PCT quadruple T = (O,M,H, E)explained in Section Abductive reasoning with PCT,we then follow the reasoning process of the frameworkby mapping the main elements of PCT into outputs ofdifferent components.ConfigurationThe framework illustrated in Figure 1 is based on a config-uration file which is filled by the expert of the domain. Theconfiguration file contains details of (possible) behavioursof signals in which the expert is interested to monitor. Toillustrate the method in the paper, we will use a runningexample of configuration files shown in Figure 2, 3 and4. For instance, Figure 2 is about a situation where theexpert is interested to observe the heart rate, amountof oxygen saturation and blood pressure. There is alsoa section in the configuration file in which the expert,by setting a range of values, can specify a significantbehaviour for physiological terms.The SSN ontology is populated only with the contentsof the configuration file. There is an equivalent class orproperty in SSN, for each item (key/value pair) mentionedin the configuration file. The value of a key in the fileis used as a name of a class in SSN. Given FeatureOfIn-terest and Property as concepts defined in SSN and thefunction valueOf(key) which returns the value of a key inthe configuration file, the SSN ontology is populated asfollows:? F , ? P , ? B (F = valueOf (feature_of _interest),P = valueOf (property),B = valueOf (Behaviour),min = minValueOf (Behaviour),max = maxValueOf (Behaviour)?B_P  P  ssn:PropertyF  ssn:FeatureOfInterest  (?ssn:hasProperty. B_P)F_P_Sensor  ssn:Sensor  (?ssn:observes. P))B_P_SensorOutput  ssn:SensorOutput(?ssn:isProducedBy. F_P_Sensor)(?ssn:hasValue. B_P_Value)B_P_Observation  ssn:Observation(?ssn:observationResult. B_P_SensorOutput)(?ssn:FeatureOfInterest.F)B_P_Value  ssn:ObservationValueB_P_MinValue ? B_P_ValueB_P_MaxValue ? B_P_Value(B_P_MinValue,min) ? hasQuantityValue(B_P_MaxValue,max) ? hasQuantityValueAlirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 5 of 16http://www.jbiomedsem.com/content/5/1/35Figure 2 Configuration file sample I (related to an infant patient).For example, the SSN ontology populated with the con-tent of Figure 2 will contain the following axioms:Slow_Rate  Rate  ssn:PropertyHeart  ssn:FeatureOfInterest  (?ssn:hasProperty. Slow_Rate)Heart_Rate_Sensor  ssn:Sensor  (?ssn:observes. Rate))Slow_Rate_SensorOutput  ssn:SensorOutput(?ssn:isProducedBy. Heart_Rate_Sensor)(?ssn:hasValue. Slow_Rate_Value)Slow_Rate_Observation  ssn:Observation(?ssn:observationResult. Slow_Rate_SensorOutput(?ssn:FeatureOfInterest.Heart)Slow_Rate_Value  ssn:ObservationValueSlow_Rate_MinValue ? Slow_Rate_ValueSlow_Rate_MaxValue ? Slow_Rate_Value(Slow_Rate_MinValue, 0) ? hasQuantityValue(Slow_Rate_MaxValue, 157) ? hasQuantityValueThe configuration file allows expert to enter valueswhich denote either a normal or an abnormal behaviourin signals. For example, in Figure 2 and 4 we can findthe definition of abnormal and normal behaviours, respec-tively. In the experimental validation in Section Results,we show that the eventual explanations are not literallydependent on the content of the file. More specifically, thesignal explanation process results in same interpretationfor variations of terms used by the expert.Hypothesis extractionAccording to PCT, the Hypothesis set is defined as a setof facts that represent relations between expected eventsand their causes. The SemanticAnalysermodule (Figure 1)initializes the process resulting in the Hypothesis set. Thismodule collaborating with public ontologies is responsiblefor retrieving a hierarchy of related concepts formatted inRDF/OWL.Before going to the details of the Hypothesis Extraction,we first explain how we deal with localizing the relevantparts in Bioportal. The goal of the system is annotatingmedical signals that contain abnormal behaviours, (i.e.,symptoms of diseases). SemanticAnalyser queries for theterm symptom in the NCBO BioPortal. The results ofthis query is 21 records out of which 15 items belong to theSymptom ontology, as a sub ontology in BioPortal. There-fore, due to its high rank, the Symptom ontology is chosenas a reference ontology.The symptom ontology illustrated in Figure 5, has beenmodelled to capture signs and symptoms of diseases andprovides well-categorized medical symptoms in terms ofbody part names. Due to its structure, the symptom ontol-ogy is only used for retrieving the hierarchy of symptomconcepts modelled based on subsumption relations. Run-ning the following SPARQL querye, the SemanticAnalysermodule retrieves a hierarchy of symptoms in terms ofsubclasses of the symptom class:PREFIX owl: <http://www.w3.org/2002/07/owl#>PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>SELECT DISTINCT ?sub ?labSubFROM <http://bioportal.bioontology.org/ontologies/SYMP>WHERE {?super a owl:Class .?super rdfs:label ?label .?sub rdfs:subClassOf ?super.?sub rdfs:label ?labSub.FILTER regex(?label, "symptom")}Figure 3 Configuration file sample II (related to the same infant in Figure 2).Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 6 of 16http://www.jbiomedsem.com/content/5/1/35Figure 4 Configuration file sample III (related to an adult patient).SemanticAnalyser, then searches through the set ofsymptom classes in order to select relevant symptoms.The relevant symptoms are those ones that are relatedto parts of the body (feature_of_interest) observed bysensors e.g., heart and blood in case of the config-uration file in Figure 2, or heart, blood and res-piratory system in case of the configuration file inFigure 4. In order to find the relevant symptoms, eachsymptom type passes the two phases of tokenizingf andstemmingg. As shown in Figure 1, the SemanticAnal-yser module uses the WordNet ontology that containssynonym/pertainymh set of words and acquires the syn-onym/pertainym set of each token of a symptom type.Consequently, each symptom type (split into its tokens)is assigned with multiple synonym/pertainym lists corre-sponding to its tokens. The number of times that eachphysiological parameter (the feature_of_interest value)appears in the synonym set of each token is counted.Finally, a symptom type whose tokens have the highesttotal count is chosen as the top candidate which hasthe highest similarity to the feature_of_interest. Table 1shows all categorized symptom types along with the bodyparts name for different configuration files. For exam-ple, the cardiovascular system symptoms is chosen dueto its highest relevance to the heart as a feature_of_interest.The final Causes set shown in Figure 1 is the union ofall subclasses of the candidate symptom types returnedper each feature_of_interest. In Table 2, cause items asthe output of the SemanticAnalysermodule are listed. Thefirst 62 items and the total 89 items are considered ascauses related to configurations in Figure 2 and Figure 4,respectively. As we see in Table 2, each cause can be a sin-gle term (e.g., hypoxemia) or a combination of terms (e.g.,atrial fibrillation). The definition of each single causeterm is retrieved from either the Symptom ontology or theWordNet ontology (in case the former returns nothing) tobe replaced with the cause item.Figure 5 An excerpt of the symptom ontology: cardiovascular, hemic and respiratory symptoms.Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 7 of 16http://www.jbiomedsem.com/content/5/1/35Algorithm 1 Similarity MatrixRequire: Causesn×1, BehaviourListm×1, Sn×m = 0for i ? 1 to n dotree ? getGrammaticalTree(Cause[i])phrases ? getAllPhrases(tree)if size(phrases) > 0 thenfor k ? 1 to size(phrases) doJJ ? phrases[ k] .getAdjective();N1 ? phrases[ k] .getNoun1();N2 ? phrases[ k] .getNoun2();for j ? 1 to m doif BehaviourList[j].getBehaviour() ? SynSet(JJ)and BehaviourList[j].getProperty() ? SynSet(N1)and BehaviourList[j].getFeatureOfInterest() ?SynSet(N2) thenS[i, j] ? 1end ifend forend forend ifend forreturn S {//Similarity Matrix}The Hypothesis set is generated by the SignalMap-per module. The SignalMapper takes as input the setof causes. It selects a subset of these causes based onparameters mentioned in the configuration file. Specif-ically it looks at the terms used to define behaviours.For example, possible behaviours for a specific sig-nal are defined as fast, slow and irregular. TheSignalMapper concatenates the values of behaviour, fea-ture of interest and property to generate a list of phrasessuch as irregular heart rate, low oxygen saturation (seeTable 3). For those configurations where the expert statesthe normal behaviour, e.g., Figure 4, the term not isadded in front of the concatenated phrase, e.g. not nor-mal respiratory rate. For phrases preceded by not anantonym set is retrieved fromWordNet.As the next step towards generating the Hypothesisset, the SignalMapper process builds an n × m similar-ity matrix S, where n and m are the number of causeitems and the number of possible behaviours, respec-tively. The similarity matrix S which is initialized to zero,will hold the similarity values between elements of thesetwo lists (Algorithm 1). For calculating the similarity val-ues, the cause items need to be grammatically analysedi.For instance, for each cause item, grammatical roles ofits terms such as noun (NN) or adjective (JJ) areidentified. All causes will therefore have their own gram-matical tree by running the grammatical analysing processover rows of the matrix. In order to set the value of ele-ment si,j of matrix S, the process first needs to generatethe grammatical structure tree of the ith cause and thento check whether this cause is related to an behaviour j.For this, all adjectives (JJ) with their own substantives(NN) of the ith cause item are retrieved. Each substantive(called noun1) is also checked to see if it is related (e.g.,via a preposition or a connective) to another noun (callednoun2). If such a combination is found in a cause item, atthe next step, the synonym/pertainym sets of the adjec-tive, noun1 and noun2 are also retrieved to be checkedagainst the column side items. The value of si,j increases ifthe following three conditions are met: (SynSet(K) refersto the synonym/pertainym set of term K, cj refers to thejth column and ri refers to the ith row)Behaviour(cj) ? SynSet(Adjective(ri))Property(cj) ? SynSet(Noun1(ri))FeatureOfInterest(cj) ? SynSet(Noun2(ri))Table 1 List of symptoms retrieved from the symptom ontologyRelated to Figure 2 Related to Figure 4Symptom category Heart Blood Heart Blood RespiratoryAbdominal symptoms 0 0 0 0 0Head & neck symptoms 0 0 0 0 0Musculoskeletal system symptoms 0 0 0 0 0Neurological & physiological symptoms 0 0 0 0 0Reproductive system symptoms 0 0 0 0 0Skin & integumentary tissue symptoms 0 0 0 0 0Digestive system symptoms 0 0 0 0 0Cardiovascular system symptoms 1 0 1 0 0Hemic system symptoms 0 1 0 1 0Nervous system symptoms 0 0 0 0 0Nutrition, metabolism symptoms 0 0 0 0 0Respiratory system & chest symptoms 0 0 0 0 1Urinary system symptoms 0 0 0 0 0Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 8 of 16http://www.jbiomedsem.com/content/5/1/35Table 2 List of causes for three different symptom types# Cause Symptom group Body part1 Arrhythmia Cardiovascular SystemHeart2 Atrial fibrillation Cardiovascular System. . . . . . . . .30 Postphlebitic ulcer Cardiovascular System31 Hypoxemia Hemic SystemBlood. . . . . . . . .62 Cyanosis Hemic System63 Tachypnea Respiratory SystemRespiratory. . . . . . . . .89 Dyspnea Respiratory SystemFigures 6 and 7 illustrate two samples of a grammaticalstructure tree for two causes and their relations with twobehaviours. The matrix element referring to arrhythmiaand irregular heart rate will be set to 1 due to the match-ing terms found between them (Figure 6). Likewise, afterrunning the process of Algorithm 1, the matrix elementreferring to tachypnea and not normal respiratory rateis set to 1 (Figure 7).After calculating the elements value of the matrix S, theSignalMapper chooses non zero elements showing a rela-tion between causes and behaviours. The Hypothesis set(H) as the first input of the reasoner (Figure 1) is cre-ated by pairs of row-column items of non-zero elements inthe matrix S. Table 4(a) and Table 4(b) partially show tworetrieved Hypothesis sets based on the two configurationsin Figure 2 and Figure 4, respectively.Manifestation extractionTheManifestationExtractor component is responsible forthe signal analysis process. This component contains amodule called SignalAnalyser (Figure 1) which performsthe event detection process. Using the SSN ontologywhich is only populated with the configuration informa-tion, the Signal Analyser detects those parts of signals thatTable 3 Possible abnormal behaviours(a) Based on Figure 2 (b) Based on Figure 4# Abnormal behaviour # Abnormal behaviour1 Slow heart rate 1 Slow cardiac system pulse2 Fast heart rate 2 Rapid cardiac system pulse3 Irregular heart rate 3 Abnormal cardiac system pulse4 High blood oxygen 4 Not normal respiratory rate5 Low blood oxygen 5 Elevated blood pressure6 High blood pressure 6 Low blood pressure7 Low blood pressurecontain an abnormal behaviour mentioned in the config-uration. An event (or an abnormal behaviour detected ina signal) is defined based on threshold values set by theexpert of the domain according to sampling rate of signalsand the patient profile (age, gender, etc.). For example, inFigure 2, the Behaviour section related to heart showsthe range of heart rate values as < 157 AND > 175which is set by the expert to monitor the situation of aninfant to detect an irregular heart behaviour. The expertwould enter different values in case of an adult patient. Forinstance, the upper bound of the slow heart rate for aninfant is set to 157 (Figure 2) while the same behaviour foran adult patient is set to 60 (Figure 4).The applied data analysis method divides signals intoseveral segments. A segment is created based on the num-ber of events (set as threshold values defining a numericrange) detected in each signal. The division process isdone within an iterative process which looks for eventsin each signal and determines a set of temporal intervalsin which a number of events are included. The iterativeprocess starts by creating a temporal segment in the firstsignal whose length is set based on the minimum requirednumber of events in the signal. More precisely, the startingtime point of the initial segment is the same as that of thesignal, and its ending point is when theminimum requirednumber of events in this signal has been met. Detecting anew event affects the size and the number of created inter-vals. The iteration ends whenever the size of intervals donot change. At the end, these intervals are considered assegments. The reasoner will then be applied on each seg-ment separately. Therefore, the threshold values set by theexpert enables him/her to have some segments in which,for example, one signal has no event while the others do.Although the data analysis method can affect the even-tual interpretation results, it is the representation tech-nique which, in this work, is at focus. In Section Results,examples of the threshold values set for a configuration isgiven.The output of the ManifestationExtractor componentis a set of Manifestations (M) at each segment, which isa list of time points at which events are detected. TheManifestation set is the second input of the reasoner(Figure 1).ReasonerThe reasoner module is based on Parsimonious CoveringTheory (PCT) as an abductive reasoning method whosebasis is on the set theory. The main feature of this rea-soner is finding the best possible Explanations (E) forthe set of Manifestations (M) detected at each segmentof signals. More precisely, given the Hypothesis set (H)which is the set of the cause/abnormal_behaviour pairs,the reasoner calculates the power setk of the causes set.Final Explanations are those members of the power setAlirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 9 of 16http://www.jbiomedsem.com/content/5/1/35Figure 6 Grammatical parsing tree and relation between arrhythmia as a cause item and abnormal heart rate as a behaviour.(or subsets of the causes set) which do not violate thereasoners principles.The principles of the reasoner are defined within twocriteria: Covering and Minimality. According to the firstcriterion shown in (1), the reasoner nominates those sub-sets of the causes set (C) that are related to all Manifes-tations. In other words, the covering set indicates a setof subsets of causes with the aforementioned specifica-tion. Moreover, the concern of the minimality criterion(2) is the size of the selected subset. Complying withaforementioned criteria, the reasoner finds the best pos-sible explanations which are those covering subsets of thecauses (as part of the Hypothesis set) that are minimal interms of the cardinality. Algorithm 2 shows the details ofthe reasoner.Covering = {K ? C | ?m ? M, ? c ? K : (c,m) ? H}(1)Minimality = {c ? Covering |  ? d : (d ? c ? d ? Covering)}(2)Algorithm 2 Abductive ReasoningRequire: Causes, Observations, Relations{//Removing non-participant causes}relevantCauses? getRelevantCauses(Causes,Relations)explanations ?nullpowerSet ? getPowerSet(relevantCauses)for all ps in the powerSet doif isCovering(ps,Observations) thenif isIrredundant(ps,Observations) thenaddExplanation(ps, explanations)elseremoveSuperSet(ps, powerSet) {//Removing thesupersets of ps}end ifend ifend forreturn explanationsThe reasoning complexity, due to the power set calcu-lation, grows exponentially w.r.t the number of causes. InFigure 7 Grammatical parsing tree and relation between tachypnea as a cause and not normal respiratory rate as a behaviour.Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 10 of 16http://www.jbiomedsem.com/content/5/1/35Table 4 List of hypotheses(a) Hypothesis related to the configurations in Figure 2# Cause Abnormal behaviour1 Arrhythmia Irregular heart rate2 Bradycardia Slow heart rate3 Tachycardia Fast heart rate. . . . . .6 Hypertension High blood pressure7 Hypotension Low blood pressure. . . . . .18 Hypoxemia Low blood oxygen(b) Hypothesis related to the configurations in Figure 4# Cause Abnormal behaviour1 Arrhythmia Abnormal cardiac system pulse2 Bradycardia Slow cardiac system pulse3 Tachycardia Rapid cardiac system pulse. . . . . .6 Hypertension Elevated blood pressure7 Hypotension Low blood pressure. . . . . .20 Tachypnea Not normal respiratory rate21 Bradypnea Not normal respiratory rateorder to reduce the size of the power set, two steps indi-cated in Algorithm 2 are applied. The first step filters theset of causes by removing those causes that are not listedin pairs of the Hypothesis set. At the second step, superclasses are removed for elements of the power set wherethe minimality criterion is violated.The output of the reasoner is the set of Explanations forobservations.Results and discussionDataSetsIn order to evaluate the framework, we use two differentsets of multivariate medical data. The first dataset con-tains 12-hours of time-series data from a set of medicalsensors measuring heart rate, arterial pressure, and arte-rial oxygen saturation of an infant in an Intensive CaringUnit (ICU). This patient is suffering from several diseases,namely multiple liver abscesses, portal hypertensionand E. Coli sepsis, used as the ground truth for the eval-uation of the final explanations suggested by the reasoner.This package of data is the ICU data package providedfor use in 1994 AI in Medicine symposium submissions[10]. The second dataset also contains multivariate datafrom three sensors measuring heart rate, respiratory rateand blood pressure of an adult patient in a Critical CaringUnit (CCU) who is suffering from congestive heart fail-ure (CHF). This package is provided by the caring unitsection of a hospitall.ResultsIn this section, we discuss about the experiments whichare based on two different configurations and two dif-ferent datasets. The first experiment is related to theconfigurations in Figure 2 and the infant patient dataintroduced above. The second experiment is based on theconfigurations set in Figure 4 and the adult patient data.Finally, the scalability of the reasoner is also evaluated baseon different configuration parameters such as: numberof feature_of_interests (F ), size of the Causes set (|C|),number of abnormal behaviours (B) and distinct numberof causes in the Hypothesis set (|Hc|).Experiment IFigure 2 shows the configurations used in this experi-ment for monitoring the heart and blood situation of apatient. The properties of interest are rate (rate of heart),pressure (pressure of blood) and oxygen (amount ofoxygen in blood). As mentioned in Section Hypothe-sis extraction, in order to find the relevant symptoms,each symptom type listed in Table 1, is assigned withthe synonym/pertainym list of its tokens. For example,the set of tokens of the first symptom types (abdominalsymptoms) is [abdomen, symptom ]whose elements areassigned to their synonym/pertainym list:abdomen ?? { venter, stomach, belly}symptom ?? { indication, evidence, gesture, mark, point,...}Since there is no match between items of the abovelists and the two physiological parameters (heart andblood), the value of the abdominal symptoms item isset to zero. However, the 8th item, cardiovascular systemsymptoms, is tokenized as [cardiovascular, system, symp-tom]. Focusing on the first token, the synonym/pertainymlist is:cardiovascular ?? {cardiac, heart}The score of the item cardiovascular system symptoms,related to the heart, hence, increases to 1. The hemicsystem symptoms item, in a same way, gets 1 score sincethe pertainym of hemic is the term blood. Therefore,the selected symptoms indicated in Table 1 are those onesthat are related to the cardiovascular system and hemicsystem symptoms due to their highest similarity values tothe feature_of_interests set in the configuration file.The list of 62 cause items (|C| = 62) which are subclassesof the selected symptom types (cardiovascular systemsymptoms and hemic system symptoms) are only par-tially shown in Table 2. Furthermore, the list of all possiblebehaviours mentioned in the configuration file (Figure 2),that created by the SignalMapper module, is depicted inAlirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 11 of 16http://www.jbiomedsem.com/content/5/1/35Table 3(a). As we can see, the process of concatenatingbehaviour, feature_of_interest and property valuesresults in 7 phrases indicating different behaviours (B =7). For example, the first item, slow heart rate is gener-ated by concatenating the term slow as behaviour, theterm heart as feature_of_interest and the term rate asproperty.In order to achieve the Hypothesis set (H) whose ele-ments are the pairs of cause / abnormal_behaviour, theSignalMapper process, at its next step, creates a 62 × 7similarity matrix S initialized to zero. The updated valueof the element si,j will indicate the relation between theith cause and the jth behaviour. As mentioned before,for each cause item whose definitions has been retrievedfrom the symptom or the WordNet ontology, a grammat-ical structure tree holding the grammatical role of eachterm in the sentence, is generated. Finding the similaritybetween causes and abnormal behaviours implies a needfor checking if a similar phrase to an abnormal behaviouris detected within a cause item (Algorithm 1). For exam-ple, the first cause item (Table 2), arrhythmia, is definedas an abnormal rate of muscle contractions in the heart.As we see in the grammatical tree of this cause illustratedin Figure 6, there is an adjective (abnormal) whose sub-stantive (rate) is also related to a noun, heart (via apreposition, in). This cause item is found similar to thethird behaviour (irregular heart rate) since:Behaviour(c1) = abnormal ? SynSet(Adjective(r3) = irregular)Property(c1) = rate ? SynSet(Noun1(r3) = rate)FeatureOfInterest(c1) = heart ? SynSet(Noun2(r3) = heart)Therefore, the element s1,3 is set to 1. Follow-ing Algorithm 1, the similarity matrix S will finallycontain 18 non-zero values referring to 18 pairscause/abnormal_behaviour that creates theHypothesis set(|H| = 18) (Table 4(a)). Counting the number of causes,we find 11 distinct items out of 18 in this list (|Hc| = 11).Therefore, during the reasoning process, where the powerset of the causes set is generated, the reasoner needs todeal with the power set with the size of 211.The SignalAnalyser detects abnormal behaviours ofdata and represents them as items of the Manifesta-tion set (M) for each segment of data. The applied dataanalysis method divides signals into several segmentswhich as explained in Section Manifestation extraction,are defined based on the desired number of events ateach signal as well as the sampling rate of the signal.Figure 8 shows three signals related to the configurationsin Figure 2. The threshold value for the Arterial PressureFigure 8 Segmentation result over 12-hours data (the first dataset).Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 12 of 16http://www.jbiomedsem.com/content/5/1/35Signal has been set as 25 ? n ? 60 meaning that asegment needs to have at least 25 and at most 60 arte-rial pressure events. Similarly, the threshold values for theArterial O2 Saturation and the Heart rate are 2 ? n ?15 and 5 ? n ? 20, respectively. According to thesethreshold values, signals in Figure 8 are divided into 3segments.Given the two sets Hypothesis (H) and Manifestation(M), the reasoner separately provides inferred Explana-tions for each segment shown in Table 5. For the patientof the first dataset, 6 distinct diseases (explanations) havebeen found (Table 6(a)). By calculating the probability ofoccurrence for each disease, the soundness of the rea-soner outputs is evaluated. The Occurrence probabilityis defined as the ratio of the number of times a diseasehas been seen to the number of different explanationsobserved for a segmentm. According to Table 6(a), thefirst (hypertension) and the forth (Septic Shock) itemsare matched with the diseases mentioned in the patientprofile (portal hypertension and E. Coli sepsis) withthe probability of 100% and 33%, respectively. In addition,other items which are discovered by the reasoner but arenot mentioned in the patient profile such as tachycar-dia and hypertension are in the literature considered asa sign of Sepsis [26]. Therefore, if we also count thesecombinations as sepsis, as shown in Table 6(b), the truepositive diseases are the two first ones in the ordered list.The false negative case which exists in the patient pro-file but has not been inferred by the reasoner is liverabscesses. This liver dependent disease to be diagnosed,most likely requires other types of sensors information inorder to be detected.Experiment IIIn this section, we continue the experiments with thesecond dataset and present results of the reasoner for sit-uations where the expert uses the negation concept in theconfiguration file. As mentioned in Figure 4, the expertdecided to monitor the heart rate, the blood pressure andthe respiratory rate of the patient. Before going to thedetails, we examine the results of the HypothesisExtrac-tion component for this case.Table 5 Manifestations shown in Figure 8Seg# Manifestations Explanations1 Fast heart rate (Hypertension,hypoxemia,palpitation)Low blood oxygen (Hypertension, palpitation,hyperemia)High blood pressure (Hypertension,hypoxemia,septicShock)(Hypertension,hyperemia,septicShock)(Hypertension,hypoxemia,tachycardia)(Hypertension,hyperemia,tachycardia)2 Same as segment 1 Same as segment 13 Same as segment 1 Same as segment 1Table 6 Occurrence probability(a) (b)# Disease Probability # Disease Probability1 Hypertension 100% 1 Hypertension 66%2 Hypoxemia 50% 2 Septicshock 66%3 Hyperemia 50% 3 Hypoxemia 50%4 Septicshock 33% 4 Hyperemia 50%5 Palpitation 33% 5 Palpitation 33%6 Tachycardia 33%Candidate symptoms for the second dataset in Table 1are cardiovascular system, hemic system and respi-ratory system symptoms. The entire subclasses of thesethree concepts in the Synonym ontology contain 89 causes(|C| = 89) shown in Table 2. Moreover, for configura-tions in Figure 4, there are 6 possible abnormal behaviours(B = 6) (see Table 3(b)). One of these items, not normalrespiratory rate, is the phrase with negation for whichthe antonym set rather than the synonym set is retrievedfrom the WordNet ontology. SignalMapper, then, createsa 89 × 6 similarity matrix in order to prepare the Hypoth-esis set. Table 4(b) shows 21 relations (|H| = 21) out ofwhich 17 cause items (|Hc| = 17) are distinct. Therefore,the reasoner has only to deal with 217 elements of thepower set.Due to the threshold values set for the segmentationprocess, the signals which are the results of 4 days ofobservation with the sampling rate of once per hour, isdivided into 1 segment. Shown in Figure 9, the thresh-old values for the heart rate, respiratory rate and bloodpressure are set as, 1 ? n ? 25, 50 ? n ? 70 and30 ? n ? 55, respectively. The inferred Explanationsare shown in Table 7.It is worth mentioning that for the first dataset, sincethe cardinality of all inferred Explanations at each seg-ment were the same (3 items for each explanation), wedid not consider the minimality criterion. However, forthe second dataset, since the reasoner results in explana-tions with different sizes, the evaluation will be different.As shown in Table 7, the first two explanations holdsthe minimality criteria of the reasoner, heart failure anddyspnea. The first one is matched with CHF, the diseasethe patient is suffering from. Furthermore, the second one,dyspnea, is considered as amain sign of heart failure in theliterature [27].Experiment IIIThe purpose of the following experiment is to examinethe performance of the reasoner given various inputs.For example, given a larger hypothesis set, the reasonerspends more time on the processing of the power set cal-culation. In the following, we momentarily disregard theAlirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 13 of 16http://www.jbiomedsem.com/content/5/1/35Figure 9 Segmentation result over 4-days data (the second dataset).time for segmentation of the signals (as this is indepen-dent of the configurations) and we represent the reasoningtime for different configurations in order to study the scal-ability of the reasoner and the impact of the parameters ina configuration file on the reasoning performance.Recall that the final explanation is retrieved from theHypothesis set (H) which is as such extracted from theCause set (C). As said in Section Hypothesis extrac-tion, the cause items are the union of the subclasses ofthe candidate symptom types. The candidate symptomtypes are also chosen based on the feature_of_interestparameters mentioned in a configuration. For instance, inthe first experiment (Section Experiment I), due to the2 mentioned feature_of_interests in the configurationfile (F = 2), there were finally 2 symptom types cho-sen. Since, the number of subclasses for each symptomtype is not really specified, we consider it as a constantvalue for all types of symptoms. Therefore, the numberTable 7 Manifestations shown in Figure 9Seg# Manifestations Explanations1 Rapid cardiac system pulse (Heart failure)Not normal respiratory rate (Dyspnea)Low blood pressure (Anemia, apnea)(Anemia, tachycardia)(Apnea, hypotension)(Hypotension, tachycardia,tachypnea)of symptom types which is equivalent to the number offeature_of_interests (F ) indicated in the configuration,is considered as a significant parameter which affects thecardinality of the Cause set (|C|). The greater the parame-ter F , the larger the value of |C|.In experiment I: F = 2, |C| = 62,In experimentII: F = 3, |C| = 89,Since the input of the reasoning process is the Hypothe-sis set which is extracted from the Cause set, we focus onparameters affecting the distinct number of causes in theHypothesis set (|Hc|). The first parameter, is the size of theCause set (|C|) which is also dependent on the F parame-ter. Another parameter influencing |Hc|, is the number ofbehaviours (B).In Table 8, we listed the measured reasoning time (inmilliseconds)n for different configurations. The informa-tion of each row in Table 8 belongs to a configurationwhich is accumulated with a new configuration for its nextrow. In the following the summary of four configurationswhich are accumulated in order are given:I : II :feature_of_interest = Heart feature_of_interest = Bloodproperty = Rate property = OxygenBehaviours : Slow, Fast, Irregular Behaviours : High,LowIII : VI :feature_of_interest = Blood feature_of_interest = Respiratoryproperty = Pressure property = RateBehaviours : High,Low Behaviours : Slow, FastAlirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 14 of 16http://www.jbiomedsem.com/content/5/1/35Table 8 Reasoning time complexity (the unit of time is in milliseconds)|F | |C| |B| SimilarityMatrix_time |Hc| Reasoning_time Final_reasoning_time1 30 3 18 4 1 192 62 5 23 7 31 542 62 7 26 11 2146 21723 89 9 29 19 10301 10330The first row and first configuration uses only onefeature_of_interest (F = 1) and the number of causesretrieved from the symptom ontology is |C| = 30 (Table 2).The distinct number of causes in the Hypotheses is |Hc| =4 and is based on the 3 possible behaviours (B = 3). Thereasoning time for calculating the power set of causes inthe Hypothesis set is 1 ms. However, since the generationand the filtering process of the similarity matrix is neces-sary to reach to the final set, we consider the last columnof the table as the final reasoning time (19ms) which is thesummation of both the similarity matrix calculation timeand the reasoning time and by increasing the parameterF in the second row of Table 2 (F = 2) the growth of thenumber of behaviours (B = 5), we see the total reasoningtime also increased to 54 ms. In order to see the effect ofthe parameter B, we keep the same feature_of_interestsin the third row (F = 2 and therefore |C| = 62). By addingthe third configuration, the only parameter changes is thenumber of behaviours (B = 7), which results in a muchlonger reasoning time (2172 ms). Although the param-eter F influences the reasoning time, the effect of theparameter B on the reasoning process is stronger.The reasoning process due to the techniques explainedin Algorithm 2 (such as filtering the cause items based ontheir relations with events), is much more efficient than apure calculation of the power set of the Cause set. Never-theless, it still needs to deal with the power set calculationfor a smaller size of causes in the Hypothesis set, explain-ing an exponential trend in computation time. Therefore,the system configurations for higher scales matters. Forinstance, behaviours allow the system to reduce the num-ber of causes which are not relevant and results in asmaller size of H. At the same time, however, the highernumber of behaviours enables the system to accept morecause items during the similarity matrix filtering process,which results in a bigger size of H and consequently ahigher reasoning time. The number of behaviours givenin the configuration file is therefore the most influentialparameter in the reasoning time. In summary, accordingto the computational time represented in Table 8, the userin order to have a reasonable computational time, is rec-ommended not to define more than 3 behaviours for eachproperty of a feature_of_interest in a configuration file.Although the intensive care units (ICUs) depending onthe patient situation or medical specialty are divided intoseveral parts such as medical intensive care unit (MICU),surgical intensive care unit (SICU), etc., there are commonequipments in terms of monitoring critical physiologicalparameters [28]. For instance, instant monitoring of pulseoximetry, arterial blood pressure, oxygenation saturation,temperature along with using ventilators assisting the res-piratory systems are done by common wired sensors usedin any care units of emergency cases. Considering thetypical monitoring sensors in hospitals care units, thecomputational time of our approach applied on other realworld scenarios with in average 4 sensors and 3 gen-eral behaviours would be the same as what we discussedabove.ConclusionIn this paper, we have presented a framework which isable to annotate medical sensor data with labels con-taining probable causes pertaining to sensor events. Thisframework reduces the probability of losing the relevantcauses by retrieving a wide possibility of causes which arerelated to sensor data. At the same time, by pruning theretrieved concepts (removing irrelevant causes w.r.t theprobable events), the complexity of the reasoning processis reduced.The primary motivation to the presented work is havingthe data annotation process that is as automated as possi-ble. The process uses manually created configuration filewhich is filled by the expert of the domain and is basedon events which are likely to occur. Although the processof generating explanations of the data is dependent uponthe content of the configuration file, the expert is free topopulate this file using his/her own words. In other words,the eventual explanations, due to synonyms of terms con-sidered throughout the interpretation process, are literally(but not conceptually) independent of terms used by theexpert. Certain limitations in the system include the levelof complexity of the user defined configurations. In addi-tion, we chose to populate the SSN ontology with classesso as to provide the opportunity of a better classifica-tions of relevant classes for future purposes. For example,by creating the two classes Heart and Cardiac system asthe subclasses of the feature_of_interest class, the sys-tem will be able to, for some purposes in future, create aowl:sameas properties between them to introduce themas equivalent classes.Alirezaie and Loutfi Journal of Biomedical Semantics 2014, 5:35 Page 15 of 16http://www.jbiomedsem.com/content/5/1/35Furthermore, as discussed in Section Experiment III,the user of the system needs to consider the limitationin number of abnormal behaviour defined in the con-figuration to avoid the time complexity of the reasonerto increase. In addition, the filtering process in similar-ity matrix, where the relevant causes are chosen basedon their grammatical structure, can be further extendedtowards considering complicated situations that may befound in English definition of a cause.Although the use of the symptom ontology is limitedto the retrieval of subclasses, still, the existence of thisontology with its well-categorized structure was a positivefeature of the medical repository which provided read-able categories of symptoms in terms of different parts ofthe body. In order to extend the framework to be appli-cable to other domains (e.g., Meteorology or Geography),such a general ontology related to the domain is necessary.For this reason, the medical domain is the more promis-ing application domain for this approach. Considering therequirements of this framework in terms of the structureof knowledge, along with the reasoning issues over linkeddata such as data inconsistency or redundancymay help toefficiently develop and populate linked data for differentdomains.EndnotesaThis ontology developed by the W3C Semantic SensorNetworks Incubator Group (SSN-XG) describes sensors,observations, and related concepts [29].bThe size of the Web is 3.32 billion sites [30].cURIs return contents of a resource that they identify.dhttp://datahub.io/group/lodcloud (over 31 billiontriples),eThe last access date of the Bioportals SPARQLendpoint (http://sparql.bioontology.org) is on 27th July2014fThe process of splitting a sequence of strings into itselements (tokens or words).gThe process of reducing inflected words to their stem,base or root form.hIn WordNet 2.1 OWL, pertain is a property betweentwoWordSense concepts that indicates the relevant termfor a word [7]iIn this work we used StanfordParser [31] to analysephrases or sentences.jIt is useful to recall that the jth column refers toabnormal (or not + normal) behaviour that is composedof a behaviour (as an adjective), a feature_of_interest(as a noun) and a property (as a noun).kThe power set of a set is the set of all its subsets.lDue to the ethical concerns about the patients privacywe received this dataset as an anonymous patient profile.mSince all three segments are the same, the occurrenceprobability can be calculated for one segment and itsvalues can be generalized.nThe computational time has been done on a computerwhich has an Intel(R) Core(TM) i7-2620M CPU(2.70GHz), 64 bit, 4 cores,4 MB for the cache memory),12GB memory, and Linux kernel 3.8.0-44-generic.Competing interestsThe authors declare that they have no competing interests.Authors contributionsMA: modelling and developing different parts of the framework. AL:supervising in modelling the framework and revising the manuscript. Bothauthors read and approved the final manuscript.AcknowledgementsThis work has been supported by the Swedish National Research Council,Vetenskapsrådet, project nr 2010-4769, on cognitive olfaction.The evaluation of the framework presented in this article has also beenassisted by using the ICU data package provided for use in 1994 AI in Medicinesymposium submissions [10].Received: 9 April 2014 Accepted: 4 August 2014Published: 12 August 2014JOURNAL OFBIOMEDICAL SEMANTICSDumontier et al. Journal of Biomedical Semantics 2014, 5:14http://www.jbiomedsem.com/content/5/1/14DATABASE Open AccessThe Semanticscience Integrated Ontology (SIO)for biomedical research and knowledge discoveryMichel Dumontier1,4*, Christopher JO Baker2, Joachim Baran3, Alison Callahan4, Leonid Chepelev4, José Cruz-Toledo4,Nicholas R Del Rio5, Geraint Duck6, Laura I Furlong7, Nichealla Keath4, Dana Klassen8, James P McCusker9,Núria Queralt-Rosinach7, Matthias Samwald10, Natalia Villanueva-Rosales5, Mark D Wilkinson11 and Robert Hoehndorf12AbstractThe Semanticscience Integrated Ontology (SIO) is an ontology to facilitate biomedical knowledge discovery.SIO features a simple upper level comprised of essential types and relations for the rich description of arbitrary(real, hypothesized, virtual, fictional) objects, processes and their attributes. SIO specifies simple design patternsto describe and associate qualities, capabilities, functions, quantities, and informational entities including textual,geometrical, and mathematical entities, and provides specific extensions in the domains of chemistry, biology,biochemistry, and bioinformatics. SIO provides an ontological foundation for the Bio2RDF linked data for the lifesciences project and is used for semantic integration and discovery for SADI-based semantic web services. SIO isfreely available to all users under a creative commons by attribution license. See website for further information:http://sio.semanticscience.org.BackgroundBiomedical research is poised to enter an era of unprece-dented large scale data analysis powered by hundreds ofpublic biological databases and hundreds of millions ofpatient records. There is a real and urgent need to exploreeffective methods for biomedical data integration andknowledge management [1,2]. Semantic-based techno-logies, such as ontologies, offer a proven method toexploit expert-based knowledge in the analysis oflarge datasets through terminological reasoning such ascorrespondence, classification, query answering andconsistency checking [3-5].The Semantic Web effort, as pursued under the auspicesof the World Wide Web Consortium (W3C), provides aset of standards to facilitate the representation, publica-tion, linking, querying and discovery of heterogeneousknowledge using web infrastructure [6]. In particular,the Resource Description Framework (RDF) [7] enablestriple-based assertions about resources using web-friendlyidentifiers, RDF Schema (RDFS) [8] offers vocabulary tocreate terminological hierarchies, and the Web Ontology* Correspondence: michel.dumontier@stanford.edu1Center for Biomedical Informatics Research, Stanford University, Stanford,California, USA4Department of Biology, Carleton University, Ottawa, Ontario, CanadaFull list of author information is available at the end of the article© 2014 Dumontier et al.; licensee BioMed CenCreative Commons Attribution License (http:/distribution, and reproduction in any mediumLanguage (OWL) [9] assists in the construction and inter-pretation of ontologies as sophisticated logic-based ex-pressions to more precisely capture the meaning of typesand relations between entities. With dozens of high valuedatasets now available in RDF and hundreds of biologicalontologies expressed using OWL, there is a tantalizingopportunity to use these resources in knowledge discov-ery. Biomedical researchers have made use of SemanticWeb technologies to uncover curation errors in systemsbiology models [10], find putative disease-causing genes[11], identify aberrant pathways [12], and uncover alterna-tive drug therapies based on mechanism of action [13],among others [14]. These knowledge-based applicationsuse automated reasoning over a coherent knowledge baseoften crafted from multiple and different underlying rep-resentations. Ontology-design patterns offer a simple wayto guide users towards a uniform representation of know-ledge [15-17].With the goal of facilitating knowledge discoverythrough simple, but effective ontology-based data inte-gration, we developed the Semanticscience IntegratedOntology (SIO). SIO offers classes and relations to describeand relate objects, processes and their attributes with spe-cific extensions in the biomedical domain. Its relationscover aspects of spatial and temporal qualitative reasoningtral Ltd. This is an Open Access article distributed under the terms of the/creativecommons.org/licenses/by/2.0), which permits unrestricted use,, provided the original work is properly credited.Dumontier et al. Journal of Biomedical Semantics 2014, 5:14 Page 2 of 11http://www.jbiomedsem.com/content/5/1/14including location, containment, overlap, parthood andtopology; participation and agency, linguistic and symbolicrepresentation, as well as comparative and other informa-tion-oriented relations. Using straightforward mappings,we report on the substantial benefits afforded by SIO inthe retrieval of RDF-based linked data and automaticcomposition of OWL-described semantic web services.Although SIO development is driven by needs in thebiomedical domain, we show that SIO can be applied to abroader set of domains.This paper is organized as follows: we first describethe current state of the SIO OWL implementation, andthen we describe ontological foundations and essentialrelations in mereotopology, participation and reference.We then present three uses of SIO in knowledge repre-sentation and outline its use in the integration of dataand web services. We finish with a brief comparisonwith related work. As a matter of convention, we usesingle quotes to indicate labels, boldface to indicateclasses, and italics to indicate relations.The semanticscience integrated ontologyAs of November 2013, SIO (v1.0) is implemented as anOWL-DL ontology (SRIQ(D) expressivity) that comprisesof 1396 classes, 203 object properties, 1 datatype property,Figure 1 Selected portions of (A) class and (B) object property hierarc8 annotation properties, 7272 axioms, 1747 subClassOfaxioms, 43 equivalentClass axioms, and 209 subProper-tyOf axioms. English labels are provided using the rdfs:label annotation property while human readable, Englishlanguage definitions are provided using the Dublin Core(dc:) Metadata term dc:description. The ontology has max-imum depth of 41 subclasses while the average number ofchildren is 2. Figure 1 shows a slice of the class andobject-property hierarchies where entity is the top levelclass and is related to is the top level object property.Ontological foundationSIO adheres to a three-dimensional worldview that isfamiliar to most scientists  one that distinguishes be-tween processes and the objects that participate in them.An object is an entity that occupies space and is fullyidentifiable by its characteristics at any moment in timein which it exists. A process is an entity that unfoldsin time and has temporal parts. While an entity existsat and is located in some space and time (Figure 2B),these need not be real space or real time, but may insteadoccur in a hypothetical (propositional), virtual (electronic),or fictional (creative work) setting. A quality (intrinsicattribute), capability (action specification) or role (be-havior, right and obligation) may exist at some time in thehies in SIO.Figure 2 Key objects and relations in SIO. (A) Key SIO entities are objects, processes and their attributes (qualities, capabilities, roles,measurement values). Processes have objects as participants and may realize specific roles and capabilities. (B) Spatial and temporal qualificationof SIO entities is captured through a set of relations (is located in, exists at) and sub-relations (e.g. is contained in, is part of, measured at), while(C) information in the form of literals (string, numbers, dates) are captured as instances of information content entities which are associated withtheir specific objects or processes.Dumontier et al. Journal of Biomedical Semantics 2014, 5:14 Page 3 of 11http://www.jbiomedsem.com/content/5/1/14entity that bears it, but it is realized in a process inwhich it plays a critical role (Figure 2A). The value ofan informational entity such as a measurement value(quantity or position) is represented as a literal - string,number (integer, float, double), boolean or date - usingthe has value data property (Figure 2C).MereotopologySIO offers a number of mereotopological relations thatcan be used to describe one or more entities in terms oftheir spatial organization (Figure 3A). The parent relation'is located of ' is a transitive relation that holds true if thespatial or temporal region of one entity fully overlaps withthe spatial or temporal region of another entity. 'has part'is a relation that is reflexive in the sense that the whole isa part of itself, and is also transitive in that a componentof a part is also a component of the whole. Therefore, aquery on the has part relation will return the whole as ananswer. 'has proper part' is an irreflexive and asymmetricrelation that ensures that the whole is different fromand not one of its proper parts. 'has direct part' enablesusers to quantify the number of parts (via a cardinalityFigure 3 Relation hierarchies for (A) mereotopological relations, (B) prestriction) at a desired type granularity, which is nototherwise possible in OWL over the transitive has partrelation. 'has component part' may be used to indicate thatthe part is intrinsic to the whole, and that the removal ofthe part changes the identity of the whole, with the caveatthat there is no logic in OWL to directly infer this. 'con-tains' is a transitive relation in which the 3D spatial regionoccupied by entity A fully overlaps with the spatial regionoccupied by entity B, but it is not the case that A has B asa part. 'surrounds' is a relation that can be used to indicatethat A 'contains' B and either A 'is adjacent to' B or A 'isdirectly connected to' B.The next set of mereotopological relations allows one tospecify how the parts are positioned to one another. 'is con-nected to' is a symmetric, transitive relation that specifiesthat components either directly share a boundary (they aredirectly connected to each other) or that they are indirectlyconnected by a path of unbroken direct connections. 'isdirectly connected to' is a symmetric relation that indicatesthat two components share a boundary. Since this relationis non-transitive, we can use it in statements to quantifythe number of connections from one part to other kinds ofarticipatory relations, and (C) referential relations.Dumontier et al. Journal of Biomedical Semantics 2014, 5:14 Page 4 of 11http://www.jbiomedsem.com/content/5/1/14parts. 'is directly before' is a relation between entities placedon a dimensional axis in which the projection of the pos-ition of the first entity is numerically less than the projec-tion of the position of the second entity, and the entitiesare adjacent to one another. This is useful for indicatingthe spatial positioning of residues in linear biopolymerssuch as proteins or nucleic acids. A domain specific rela-tion such as is covalently connected to then enables one todescribe the atomic connectivity within a molecule such asmethane (Figure 4).Processes and participationSIO provides a set of relations to describe processes interms of their participants and their actions (Figure 3B).'has participant' indicates which entities participate in aprocess. 'has agent' specifies entities that directly or ac-tively participate in the process. 'has input' specifies en-tities at the start of the process. has parameter specifiesthose variables (and their values) used in the process.'has target' specifies entities that are modified during theprocess, but retain their identity. 'has substrate' specifiesentities that are consumed (or are sufficiently changedthat they lose their canonical identity). 'has product' speci-fies new entities formed as a result of a process. Relationssuch as has substrate, has target, has product are examplesof role-specialized relations. In SIO, more explicit role-based assertions can be formulated by stating that the roleof an entity is realized in the process. For instance, Figure 5shows a description of phosphorylation of an enzyme byATP in which substrate and product roles are realized.Figure 4 Exact description of a molecule of methane using mereotopSIO includes an OWL2 property chain [realizes o isrole of - > has participant] which enables an OWL2 DLreasoner to infer that entities having the realized role arealso participants of the process.Referential relationsReferential relations in SIO are used to indicate what anobject refers to or the nature of the mention of one entityby another (Figure 3C). At the top level, refers to enablesJOURNAL OFBIOMEDICAL SEMANTICSStenetorp et al. Journal of Biomedical Semantics 2014, 5:26http://www.jbiomedsem.com/content/5/1/26RESEARCH Open AccessGeneralising semantic categorydisambiguation with large lexical resources forfun and profitPontus Stenetorp1*, Sampo Pyysalo2,3, Sophia Ananiadou2,3 and Junichi Tsujii2,3,4AbstractBackground: Semantic Category Disambiguation (SCD) is the task of assigning the appropriate semantic category togiven spans of text from a fixed set of candidate categories, for example PROTEIN to Fibrin. SCD is relevant to NaturalLanguage Processing tasks such as Named Entity Recognition, coreference resolution and coordination resolution. Inthis work, we study machine learning-based SCD methods using large lexical resources and approximate stringmatching, aiming to generalise these methods with regard to domains, lexical resources and the composition of datasets. We specifically consider the applicability of SCD for the purposes of supporting human annotators and acting asa pipeline component for other Natural Language Processing systems.Results: While previous research has mostly cast SCD purely as a classification task, we consider a task setting thatallows for multiple semantic categories to be suggested, aiming to minimise the number of suggestions whilemaintaining high recall. We argue that this setting reflects aspects which are essential for both a pipeline componentand when supporting human annotators. We introduce an SCD method based on a recently introduced machinelearning-based system and evaluate it on 15 corpora covering biomedical, clinical and newswire texts and ranging inthe number of semantic categories from 2 to 91.With appropriate settings, our system maintains an average recall of 99% while reducing the number of candidatesemantic categories on average by 65% over all data sets.Conclusions: Machine learning-based SCD using large lexical resources and approximate string matching is sensitiveto the selection and granularity of lexical resources, but generalises well to a wide range of text domains and data setsgiven appropriate resources and parameter settings. By substantially reducing the number of candidate categorieswhile only very rarely excluding the correct one, our method is shown to be applicable to manual annotation supporttasks and use as a high-recall component in text processing pipelines. The introduced system and all related resourcesare freely available for research purposes at: https://github.com/ninjin/simsem.Keywords: Semantic category disambiguation, Approximate string matching, Lexical resources, Named entityrecognition, Domain adaptation, FreebaseBackgroundSemantic Category Disambiguation (SCD) is a key sub-task of several core problems in Natural Language Pro-cessing (NLP). SCD is of particular importance for NamedEntity Recognition (NER), which conceptually involvestwo sub-tasks that must be solved: detecting entity men-tions and determining to which semantic category a given*Correspondence: pontus@is.s.u-tokyo.ac.jp1Department of Computer Science, University of Tokyo, Tokyo, JapanFull list of author information is available at the end of the articlemention belongs. SCD is concerned with the latter, theselection of the appropriate semantic category to assignfor a given textual span from a set of candidate cate-gories (Figure 1). Other tasks that SCD is relevant toinclude coreference and coordination resolution. In coref-erence resolution [1], coreferring mentions must share thesame semantic category, and a method can thus excludecandidate mentions by having access to accurate seman-tic classifications. Also, by adding semantic informationabout the members of a coordinate clause, it is possible© 2014 Stenetorp et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited.Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 2 of 11http://www.jbiomedsem.com/content/5/1/26(a)(b)Figure 1 Example of the prerequisite for our task setting, demarked continuous spans as seen in (a) and the output, semantic categoriesassigned to the input spans as seen in (b). 2-comp-sys, Pro and +Regulation are used as short-hands for Two-component system, Proteinand Positive regulation respectively. Note the potential for partial overlap of different semantic categories as can be seen for the Protein andTwo-component system annotations.to resolve that the most likely meaning for a phrase suchas Tea or coffee and a sandwich is [[Tea or coffee]and a sandwich] rather than [[Tea] or [coffee and asandwich]] [2].We recently demonstrated that high-performance SCDsystems can be constructed using large-scale lexicalresources and approximate string matching for severalwell-established data sets [3]. However, a number of ques-tions regarding the applicability of these methods remainunanswered. First, this approach to SCD has only beenextensively evaluated for biomedical texts, which raisesthe question whether the advances made for the biomed-ical domain can readily be carried over to other domainssuch as clinical and newswire texts. Second, state-of-the-art NER and SCD systems typically rely on lexicalresources selected to suit the task being addressed [4,5]and one can thus expect performance to degrade if thesystem is moved to a new domain or language [6], but themagnitude of this effect for SCD has not been established.Third, while NER data sets are commonly annotated forshort, non-embedded text spans such as person names orprotein mentions, in a number of corpora annotations cancover long spans and be nested in complex structures [7].We would expect such annotations to pose issues for lex-ical resource matching strategies that rely on a mappingbetween the resource and the span being classified.There are several practical applications that involveSCD, such as the assignment of labels such as those ofICD-10 [8] to documents and the production of annota-tions to train information extraction systems [9]. For anymanual assignment task, there are cognitive limitations onthe number of distinct categories a human annotator canprocess before falling victim to degrading performance[10]. Automated systems could thus assist annotators bylimiting the number of categories presented to the user,excluding those that are clearly irrelevant; Figure 2 showsan illustration for a specific use-case. However, any anno-tation support system will be subject to close scrutiny,and an SCD system must thus have very high recall toavoid errors and rejection by users, while at the same time(a) (b)Figure 2 Examples of entity type annotations from [25], illustrating how the amount of visual and user-interface complexity (a) can bereduced using an SCD system (b). The relevant text span being annotated in both figures is heart which should be assigned the ORGANsemantic category.Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 3 of 11http://www.jbiomedsem.com/content/5/1/26limiting the number of categories presented to the highestdegree possible, even when the amount of training data islimited.In this work we extend our initial study [11] of the appli-cability of SCD for annotation support and as a pipelinesystem component, investigating whether SCD can begeneralised across domains and languages and the impactof lexical resource selection and differences in annotationcriteria.MethodsThis section discusses baseline methods, evaluation met-rics, feature sets, models, corpora and lexical resourcesused for the experiments.Previous work and baseline methodsAlthough SCD is central to NER and several other NLPtasks, there have been relatively few in-domain studiesinvestigating SCD as a stand-alone task. However, recentlya few publications have investigated this task in isolation.Cohen et al. [12] presented a fast and reliable approachfor associating a given textual span to one or severalontologies. The method was based on a set of manu-ally crafted rules and achieved an accuracy ranging from77.1% to 95.5% for determining the semantic categoryof a given annotation in a setting where each categorywas defined by reference to a domain ontology. In recentwork, [3] we introduced a machine learning-based SCDmethod that employed approximate string matching [13]of continuous textual spans to several large-scale lexi-cal resources. While the use of lexical resources such asdictionaries covering specific semantic categories is com-monplace in state-of-the-art NER systems [4,5], approxi-mate string matching was a novel aspect of the work. Weevaluated the method on several data sets and achievedresults ranging from 85.9% to 95.3% in accuracy. How-ever, although the overall best-performing model in thisstudy incorporated approximate string matching features,we failed to establish a clear systematic benefit of approx-imate, as opposed to strict, string matching for all datasets.Since our goal here is to evaluate the performanceof SCD for assisting other tasks such as manual textannotation, the approach of Cohen et al. has two limita-tions. First, it assumes that the semantic categories aredefined by ontological resources and therefore it cannotbe directly applied to annotation targets that do not matchavailable ontological resources. Second, unlike our previ-ously proposed approach, their approach does not provideranking or classification confidence. Since this makes themethod less suitable in a setting where it is necessary todynamically adjust the number of suggested categories, asis the case for annotation support, for the present studywe choose to extend our previous system.Task settingWe define an SCD task as follows: for a fixed set of can-didate semantic categories, given a text and a continuoustextual span in its context, assign the correct category tothe span. Figure 1 illustrates the style of annotation andthe possibility of overlapping and nested spans with dif-ferent semantic categories. The SCD task set-up is relatedto bothWord-sense Disambiguation [14] (WSD) and termgrounding (or normalisation) [15], but there are severalnoteworthy differences. The spans considered in WSDare inherently internally ambiguous (for example exploitcarrying the meaning of achievement, advantageous use,etc.), thus requiring the word sense to be mainly inferredby context. Further, SCD assumes a fixed set of cate-gories, while in WSD the senses are normally different foreach ambiguous word. In term grounding, entitymentionsare to be mapped to unique identifiers, typically definedwith reference to large resources such as Entrez Geneor Wikipedia, and each identifier represents only a smallnumber of entities or just a single one. The key differ-ence in this case is that as SCD is concerned with broadersemantic categories, each covering a large number of enti-ties, SCD methods can thus assume that the training datawill contain numerous instances of each possible category.In our previous work [3] we cast SCD as a (single-label)classification task, and Cohen et al. [12] considered itas a multi-label classification task. In this work we con-sider both the single-label classification setting as wellas a setting where we allow the method to suggest anynumber of categories for a given span, in part analo-gously to beam search with a dynamic width beam [16].Although in our data exactly one candidate category iscorrect for each span, this setting allows us to explorehigh-recall approaches while aiming to keep the numberof suggestions to a minimum.This setting matches our aim of reducing the cognitiveburden on a human annotator who has to determine thecorrect answer among multiple suggestions and allows usto study how well an SCD system can estimate its ownconfidence when passing on suggestions to either a humanannotator or another system.MetricsFor single-label classification, we report performanceusing accuracy, the fraction of cases where the system pre-dicts the correct label. When allowing the system to sug-gest multiple categories, we measure recall and ambiguityreduction. Recall is the fraction of cases where the correctcategory is present among the suggested categories, andambiguity is defined as the number of semantic categoriessuggested by the system. While both recall and (average)ambiguity give insight into the performance of the system,they are in a trade-off relation, similarly to how precisionis to recall, and should ideally be combined into a singleStenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 4 of 11http://www.jbiomedsem.com/content/5/1/26metric. To normalise the ambiguity metric with regard tothe number of categories, we define (relative) ambiguityreduction as follows:AmbiguityReduction = |Categories| ? Ambiguity|Categories| ? 1 (1)Here, we subtract one from the number of categoriesin the denominator to give the metric the same range asrecall ([0.0, . . . , 1.0]). We then straightforwardly combineaverage ambiguity reduction and recall into a harmonicmean.We train our model and produce learning curves withdata points using samples of [5%, 10%, . . . , 100%] of thetraining data. At each data point we take several randomsamples of the current data size and use the mean (?) ofthe performance over the samples to compensate for pos-sible sample variance. Results for eachmetric are providedas the mean of the data points of the learning curve, as iscommonly done to approximate the Area Under the Curve(AUC).Feature sets andmodelsOne of the primary differentiating factors between themachine learning models considered in our experimentsare the feature sets applied in training each model.Our three baseline models are defined by the follow-ing feature sets: INTERNAL (INT.), a comprehensive setof NER-inspired features derived solely from the textspan to be classified, GAZETTEER (GAZ.) features derivedfrom strict string matching look-ups of the span in eachof the applied lexical resources, and SIMSTRING (SIM.)features, representing an approximate matching variantof GAZETTEER calculated using the SimString approxi-mate string matching library [13]. These feature sets aredescribed in detail in our previous studies [3,17]. Thethree baseline methods are defined by the feature setcombinations INTERNAL, INTERNAL+GAZETTEER, andINTERNAL+SIMSTRING, abbreviated as INT., INT.GAZ.and INT.SIM., respectively.We extended our previous system described in [3] toallow it to determine the number of categories to proposeto optimise recall and ambiguity reduction as follows.The machine learning method applied in the system [18]provides probabilistic outputs, which can be used as indi-cators of the confidence the method has for each category.The system considers the categories ordered highest-confidence first, and returns the smallest set of categoriesso that the sum of the confidences for the set is equal toor greater than a threshold value. This threshold becomesan additional parameter for the system, controlling thetrade-off between ambiguity and recall. This will resultin a number of suggestions ranging from 1 to the totalnumber of categories in the data set. For example, for thecategories and confidences [PROTEIN 90%, CHEMICAL6%, ORGANISM 4%] and the confidence threshold 95%,the system would suggest PROTEIN and CHEMICAL, butnot ORGANISM. In our previous work, [11] we selected athreshold of 99.5% as this performed well for the evalua-tion on the development data sets, and we continued touse this threshold for our initial experiments here.CorporaFor evaluation, we initially included the six data sets usedin [3], listed above the mid-line in Table 1. While our pre-vious study found promising results for these data sets,they are all drawn from the biomedical domain, whichleft the generality of our method largely unsubstantiated.To argue that our method is applicable to other domains,we need to show this experimentally. To demonstrate theapplicability of the method, it is also necessary to evalu-ate against corpora containing more semantic categoriesthan the 17 covered by the EPI data set, the largest num-ber in the previously considered resources. To widen ourselection of annotated resources, we thus collected a totalof nine additional corpora, listed below the mid-line inTable 1 and presented in the following.To extend the coverage of domains, we included theI2B2 corpus [19] and the CoNLL-2002 data sets forTable 1 Corpora used for evaluationName Semantic categoriesEpigenetics and Post-Translational 17Modifications corpus [35] (EPI)Infectious Diseases corpus [22] (ID) 16Genia Event corpus [36] (GE) 11Collaborative Annotation of a Large 4Biomedical Corpus [37] (SSC)BioNLP/NLPBA 2004 Shared Task 5corpus [38] (NLPBA)Gene Regulation Event Corpus [39] (GREC) 64 (6)Multi-Level Event Extraction corpus [21] (MLEE) 52GeneReg corpus [40] (GREG) 10Gene Expression Text Miner corpus [41] (GETM) 3BioInfer [7] (BI) 119 (97)BioText [42] (BT) 2CoNLL-2002 Shared Task corpus, 4Spanish subset [20] (CES)CoNLL-2002 Shared Task corpus, Dutch 4subset [20] (CNL)i2b2 Medication Challenge corpus [19] (I2B2) 6OSIRIS corpus [43] (OSIRIS) 2Parenthesised values signify the actual number of categories after performingpre-processing steps so as to not suffer from data sparseness (GREC conversioninto SGREC [3]) or to compensate for ontological design decisions (BI). Themid-line indicates a cut-off between the above corpora used in previous work [3]and the corpora added to evaluate our approach for a variety of domains andcovering a large set of semantic categories.Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 5 of 11http://www.jbiomedsem.com/content/5/1/26Spanish and Dutch NER [20]. I2B2 stems from theclinical domain which, while related to the biomedicaldomain, involves a different set of semantic categories (e.g.DOSAGE and MEDICATION). The CoNLL-2002 data setsare both from the newswire domain, largely unrelated tothe previously considered domains, and additionally forlanguages other than English. They are thus expected topose new challenges, in particular in regards to the lexi-cal resources utilised. As mentioned above, the questionis still open as to whether our approach scales to a set ofcategories larger than the 17 of the EPI corpus. To addressthis issue, we acquired the MLEE [21] and BI [22] corporawhich contain 52 and 119 semantic categories each, rep-resenting increases of ?3× and ?7× respectively in thenumber of categories. Finally, we added four biomedicalcorpora not considered in previous work to increase thediversity of resources in this domain.Following initial corpus selection, we performed somepre-processing for a number of the resources, as follows.After inspecting the annotation guidelines for the BI cor-pus, we found that a core assumption of our task settingwas violated: mentions of entities of the three semanticcategories GENE, PROTEIN and RNA would be markedusing a single compound category (GENE, PROTEIN ORRNA) if they were not a participant of an event annota-tion. This is problematic for our experimental set-up sincewe do not seek to model whether targeted entity mentionsparticipate in events. Thus, we collapsed all entries forGENE, PROTEIN and RNA into the single GENE, PROTEINOR RNA category as a pre-processing step. Furthermore,BI allows for discontinuous span annotations, which alsoconflicts with the assumptions of our task setting. Wethus merged all discontinuous spans into single spans,removing any duplicate spans that were created in theprocess. Finally, to compensate for an ontological deci-sion to differentiate between state changes and processes(e.g. Phosphorylate compared to Phosphorylation) wemerged all paired types into single categories. After thesepre-processing steps had been carried out, we were leftwith 97 distinct semantic categories, a ?6× increasecompared to the largest number of categories consideredin our previous study. We also performed some neces-sary, but less involved, pre-processing steps for some othercorpora. In the case of BT, we removed the relational indi-cators for each span and used the two categories DISEASEand TREATMENT. For I2B2, we used the gold data anno-tated and released by the organisers of the shared task,leaving out the parts of the provided data submitted byshared task participants.All the data sets were randomly separated into train-ing, development and test sets consisting of 1/2, 1/4 and1/4 of the annotations respectively. The test set was kepthidden during development and was only used to gener-ate the final results. When reviewing annotation samplesand guidelines for the nine additional corpora, we foundsome cases that we anticipated would be problematicfor methods using our previously proposed feature sets.In particular, for compound noun-phrases (NPs) con-taining mentions of entities of several different semanticcategories, the classifier could potentially be confusedby matches to resources containing semantic categoriesunrelated to the entity referred to by the NP as a whole.As a concrete example, consider Complex of fibrin andplasminogen: the full span should be assigned the seman-tic category COMPLEX, while the semantic categories offibrin and plasminogen are PROTEIN. To address suchcases, we drew on the observation that the head word ofa noun-phrase commonly determines the semantic cat-egory of a span. Specifically, we constructed a set offeatures employing a simple heuristic-based noun-phrasehead-finding algorithm, extracting two span componentsof particular interest: the NP-head detected by the algo-rithm, and the Base-NP, approximated as the combina-tion of the NP-head and all preceding text in the span(Figure 3). These subspans were used in feature generationto define an extended NP feature set: for the INTERNALfeature set, we added binary features representing the textof the NP-head and Base-NP, and for the GAZETTEER andSIMSTRING feature sets, we performed look-ups againstall lexical resources using strict and approximate stringmatching respectively, in addition to the binary featuresfor the text of the NP-head and Base-NP. We will discussthe impact of these features for the various data sets in theResults section.Lexical resourcesAs a starting point, we adopt the collection of 170 lex-ical resources first gathered in [3]. These are partic-ularly suited for biomedical data as they were manu-ally selected with this single domain in mind. Since itwould be advantageous to use a general purpose col-lection of lexical resources rather than those selectedfor a specific domain, we also evaluate the data pro-vided by the Freebase project as a source of general-purpose lexical resources. The Freebase knowledge basecovers a wide range of domains, is multi-lingual innature, and has recently been utilised for several NLPFigure 3 Example of sub-string components used to generatethe NP-based features.Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 6 of 11http://www.jbiomedsem.com/content/5/1/26tasks [23,24]. Freebase is collaboratively curated by vol-unteers and contains millions of statements. However,not all of these are relevant to our experiments, as theknowledge base not only covers statements regardingsemantic categories but also information such as userdata. The project defines a set of 72 Commons cat-egories that have passed several community standardsand cover a wide array of topics such as ASTRON-OMY, GEOGRAPHY, GAMES, etc. We created 72 lexicalresources from the 15,561,040 unique entry names listedfor these Freebase categories, referred to in the followingas FB.Even though Freebase is a general-purpose resource,we anticipated some issues with the granularity of theCommons categories. In particular, the MEDICINEand BIOLOGY categories do not make any distinctionbetween, for example, DRUG and INFECTIOUS DISEASE,and ORGANISM and GENE, respectively. In order to allowfor a fair comparison to the manually selected biomedicaldomain lexical resources, we constructed an additional setof resources where these two categories anticipated to beproblematic were split into their sub-categories, giving atotal of 175 lexical resources. This set is referred to as FBXin the following.The GAZETTEER and SIMSTRING features are depen-dent on the choice of lexical resources, and we can thuscreate variants of these feature sets by using any of theabove-mentioned sets of lexical resources. For our exper-iments, we also defined in addition to the basic variantusing the 170 biomedical domain resources four modelsbased on the GAZETTEER and SIMSTRING in combinationwith the FB and FBX sets.Results and discussionThis section introduces and discusses the experimentaloutcomes. The experimental results are summarised inFigure 4, Table 2 and Additional file 1: Table S1. We firstinvestigate how our baseline models perform in regards toambiguity reduction and recall on the subset of corporaused in our previous work. Next, we proceed to eval-uate how the same models perform for additional datasets, focusing on performance for resources with largenumbers of semantic categories and those from domainswhich are either different but related (clinical) or largelyunrelated (newswire) to the biomedical domain. We thenevaluate the impact of utilising different lexical resourcesand evaluate the effectiveness of our proposed NP featureset. Lastly, we consider the effects of tuning the thresholdparameter that controls the trade-off between ambiguityand recall.Initial evaluation on biomedical corporaFor our initial investigations, we use the six corporaapplied in our previous study [3]. Figures 4a and 4b showthe lower end of the learning curves for ambiguity andrecall, and the results for the different evaluation metricsare given in the boxed upper left corners in Additionalfile 1: Table S1.We observe that the SIMSTRING model outperformsother baseline models in almost all cases where thereare non-trivial differences between the different models.We thus focus primarily on the SIMSTRING model inthe remainder of the evaluation. Our results are promis-ing for both the ambiguity and recall metrics. Ambiguityquickly drops to a manageable level of 23 remaining cat-egories for all corpora (Figure 4a), and the reduction inthe number of semantic categories is on average 60% overthe data sets (Additional file 1: Table S1c). The reduc-tion is most prominent for EPI, where the number ofcategories is reduced by ?95% even for the smallest train-ing set size considered. The positive results for ambigu-ity reduction are achieved without compromising recall,which stays consistently around or above ?99% for alldata sets (Figure 4b and Additional file 1: Table S1d). Thislevel is expected to be acceptable even for comparativelydemanding users of the system. In summary, we find thatFigure 4 Learning curves for ambiguity (a) and recall (b) for our initial ambiguity experiments.Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 7 of 11http://www.jbiomedsem.com/content/5/1/26Table 2 Results for the BT, GETM, I2B2 andOSIRIS data setsusing the Int.NP.Sim. model with a confidence threshold of95% for mean ambiguity reduction (?Amb.Red.), meanrecall (? Recall), and the harmonic mean of meanambiguity reduction and recall (H(?Amb.Red.,?Recall))Data set ?Amb.Red. ?Recall H(?Amb.Red.,?Recall)BT 78.00/+34.00 99.54/-00.31 87.46/+26.38GETM 88.50/+32.50 99.99/-00.01 93.89/+22.10I2B2 77.60/+42.60 98.14/-01.50 86.67/+34.87OSIRIS 78.00/+42.00 99.79/-00.21 87.56/+34.62The relative values are compared to the same model using a confidencethreshold of 99.5%.for a number of biomedical domain data sets the proposedapproach is capable of notably reducing the number ofproposed semantic categories while maintaining a veryhigh level of recall and that our SIMSTRING model outper-forms other baseline models.Impact of data set domain and number of categoriesWe next extend our evaluation to the additional nine cor-pora incorporated in this study. As this gives 15 corporain total, instead of considering performance metrics andlearning curves in detail for each, we will below focusprimarily on the summary results in Additional file 1:Tables S1a and S1b, giving accuracy and the harmonicmean of ambiguity reduction and recall. Among the nineadditional data sets, CES, CNS and I2B2 are of particularinterest regarding the ability of the approach to gener-alise to new domains; the former two are for languagesdifferent from English and from the newswire domain, acommon focus of NLP studies, and the latter from theclinical domain. Likewise, the MLEE and BI data sets,containing 52 and 97 semantic categories respectively,are suited for evaluating the ability of the approach togeneralise to tasks involving a large amount of semanticcategories.We first note that the SIMSTRING model performs wellfor all metrics for the biomedical domain MLEE, GREGand BI data sets. However, we observe several instancesof reduced performance with respect to the results of theinitial experiments for corpora of various domains. Forthe newswire domain CES and CNL data sets, we findsomewhat reduced accuracy and a low harmonic mean.The biomedical domain GETM, BT and OSIRIS corporaand the clinical domain I2B2 corpus show high accuracy,but share the low harmonic mean performance of theCES and CNL data sets. In all cases the poor results interms of the harmonic mean of ambiguity reduction andrecall is due to low ambiguity reduction; recall remainshigh in all instances, reaching a full 100% in numerouscases (Additional file 1: Table S1d). This suggests that themethod may have problems with its optimisation targetwhen the number of categories is small, a property sharedby all the above resources, overemphasising recall overambiguity. Additionally, for the out-of-domain data setsit is probable that our selection of lexical resources is apoor fit, a possibility evaluated specifically in the nextsection.In regards to data sets containing large sets of seman-tic categories, rather surprisingly both the MLEE and BIdata sets appear to pose little challenge to our approach,even though they both contain more than three timesthe number of categories considered previously. Theseresults suggest that, somewhat counter to expectation,the method appears to generalize well to large numbersof semantic categories, but poorly to small numbers ofsemantic categories.Lexical resource dependenceThe poor performance for the Spanish and Dutchnewswire corpora CES and CNL could potentially beexplaned by a mismatch between the data sets and theapplied lexical resources: the lexical resources originallyused in [3] were collected specifically for the biomedicaldomain, and using only English resources. This hypothesisis supported by the observation that the models rely-ing on lexical resources, SIMSTRING and GAZETTEER,performed poorly for these data sets, barely outper-forming or performing slightly worse than the strongbaseline of the INTERNAL model that does not utiliseany lexical resources. To test the hypothesis, we creatednew SIMSTRING and GAZETTEER model variants usingthe Freebase-based lexical resources FB and FBX. Theseare denoted in Additional file 1: Table S1 by a trailingparenthesis following the model name that contains theresource name (e.g. INT.SIM. (FB)).If we at first only consider the results of the FB-based models, we observe a considerable increase inperformance for the CES and CNL data sets by approxi-mately 45% points in mean accuracy and approximately1220% points in harmonic mean for the SIMSTRINGmodel (Additional file 1: Table S1a and Additional file 1:Table S1b). This effect is most likely due to namedentities annotated in these corpora, such as companynames, person names, and locations, now being listed inthe lexical resources and serving as strong features. Aninteresting observation is that although both the SIM-STRING and GAZETTEER models employ the same lexicalresources, the performance increase for the SIMSTRINGmodel greatly surpasses that of the GAZETTEER model.This result is largely analogous to what we have previouslydemonstrated for the biomedical domain, and suggeststhat the benefits of approximate string matching gener-alise also to the newswire domain and across languages.Although the effect of using the FB version of the Free-base data is positive for the CES and CNL data sets, thereStenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 8 of 11http://www.jbiomedsem.com/content/5/1/26is a notable drop in performance across the board fornearly all other data sets. At this point we should remem-ber that we have anticipated that the Freebase Commonscategories may be of limited value for specific domainsdue to their coarse granularity. We thus now also considerthe results of the FBX-based models that give a finer gran-ularity for the MEDICINE and BIOLOGY categories. ForSIMSTRING, using FBX as opposed to FB raises the aver-age accuracy over the data sets from 86.55% to 87.72% andthe average harmonic mean score from 60.40% to 64.79%.Further, SIMSTRING is shown to benefit more than thestrict string matching model GAZETTEER, which fails torealise a clear benefit from FBX as compared to FB. How-ever, for the biomedical domain corpora, performanceremains considerably lower than when using in-domainresources even for FBX.These results confirm the expectation that the per-formance of the approach is strongly dependent on thechoice of lexical resources, and suggest that while thelarge, general-purpose resource Freebase can be usedto derive lexical resources applicable across domains, itcannot match the benefits derived from using targetedresources curated by specialists in the domain relevant tothe corpus.Impact of noun-phrase head featuresAs noted in the introduction of the additional corpora, wewere concerned that annotated spans of text that covermentions of entities of multiple semantic categories maycause difficulties for our approach. This is in part due toour feature sets being inspired by features employed byNER systems, which frequently target short spans of textinvolving only single mentions of entities, such as propernames. To address this issue, we introduced the NP exten-sions of the feature sets of each model. In this section, wepresent results on the effectiveness of these features.We find that GAZETTEER and SIMSTRING benefit fromthe introduction of the NP features, while INTERNALshows mixed results depending on the metric. Interest-ingly, while GAZETTEER gains an average 0.60% pointsfor accuracy and 6.39% points for the harmonic mean,the respective gains are lower for SIMSTRING, at 0.46%points and 4.51% points. Following from what we haveobserved previously, we would expect that if approximatestring matching is more beneficial than strict matchingon the level of the whole string, it would also be so onsubsets of the same string. A possible explanation is thatwhile the GAZETTEER model previously had no accessto any substring matches in the lexical resources, theapproximate string matching model could make some useof this information even before the introduction of theNP features. Thus, it is possible that in allowing matchesagainst smaller regions of a given span, the use of approx-imate string matching to some extent relieves the needto perform detailed language-specific processing such ashead-finding.This evaluation demonstrated that the NP features areeffective for the GAZETTEER and SIMSTRING models,with their addition to the SIMSTRING baseline feature setproducing a model that outperforms all models in ourprevious work for a majority of the data sets for boththe accuracy and harmonic mean metrics. The resultingmodel, INT.NP.SIM., is our best model as-of-yet for theSCD task.Impact of confidence threshold parameterUntil now we have not addressed the low performancein terms of ambiguity reduction for the GETM, BT, I2B2and OSIRIS data sets. These are from the biomedical andclinical (I2B2) domains, but share the property of involv-ing only a small number of semantic categories: three inGETM and two in the others. One parameter we keptfixed throughout experiments was the confidence thresh-old that controls the number of suggestions proposedby our system and the trade-off between ambiguity andrecall. To investigate whether the setting of this param-eter could account for the low performance for theseresources, we lower the threshold from the value 99.5%,chosen based on experiments on the corpora used in ourprevious work [11], and instead use a threshold of 95.0%.This choice is motivated by a set of preliminary experi-ments on the development portions of all data sets. Wethen performed additional evaluation on the four above-mentioned corpora that had shown poor performance.We can observe that, as expected, performance in termsof ambiguity improves greatly (Table 2), roughly doublingin absolute terms. Further, this improvement is achievedwhile recall is preserved at a level of 98% or higher forall four data sets. In hindsight, this behaviour could beexpected on the basis of our observation of close to per-fect recall for the primary experiments for these four datasets.This experiment shows that while a high threshold cancause the system to err on the side of recall and fail toproduce a notable reduction in ambiguity for corpora witha low number of semantic categories, with an appropriatesetting of the threshold parameter it is possible to achieveboth high recall and a clear reduction in ambiguity also forsuch data sets.Conclusions and future workWe studied machine learning-based Semantic CategoryDisambiguation (SCD) methods using large lexicalresources and approximate string matching, focusing onthe ability of these SCD approaches to generalise tonew corpora, domains, and languages, their dependenceon factors such as the choice of lexical resources, andtheir applicability for annotation support tasks and asStenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 9 of 11http://www.jbiomedsem.com/content/5/1/26components in pipeline systems. Adapting an existingSCD method to a task setting allowing the system to sug-gest multiple candidates, we observed that performanceis dependent on the choice and granularity of lexicalresources and that resources with a low number of seman-tic categories and annotations involving mentions of mul-tiple entities posed specific challenges for the method.We demonstrated how these issues could be addressedand were able to show that a 65% average reduction inthe number of candidate categories can be achieved whilemaintaining average recall at 99% over a set of 15 corporacovering biomedical, clinical and newswire texts. We findthese numbers very promising for the applicability of oursystem and will seek to integrate it as a component forother systems to further verify these results.In future work, we hope to address a number of remain-ing questions. First, it should be verified experimentallythat our primary metric, the harmonic mean of ambiguityand recall, represents a reasonable optimisation target forSCD applications such as annotation support. By varyingthe trade-off between ambiguity reduction and recall andmeasuring the impact on actual human annotation time[25], we could empirically study the relationship betweenambiguity and recall for a given task. Furthermore, as wecould observe in our lexical resource experiments, theoptimal composition of lexical resources is dependent onthe data set. While we could have manually constructed anew collection of lexical resources to cover all the domainsin our experiments, this ad-hoc processes would poten-tially have to be repeated for each new data set we applyour method to. Instead, we propose to aim to automat-ically select the set of lexical resources optimal for eachdata set, which we believe to be more likely to result inlong-term benefits and to allow our method to be ben-eficially applied to novel tasks. By integrating automaticlexical resource construction and confidence parameterselection, we hope to be able to create a general-purporseSCDmethod applicable across tasks and domains withoutthe need for user intervention.The system used in this study as well as other resourcesare freely available for research purposes at https://github.com/ninjin/simsem.Availability of code, corpora and lexical resourcesThis section covers the availability and sources for thecode, corpora and lexical resources used in this work.In addition to assuring that those who have providedresources essential to this study are properly acknowl-edged, it aims to assist in the replication of the experi-ments presented in this paper.The code used for the experiments is available undera permissive license from https://github.com/ninjin/simsem. The lexical resources used were Freebase, pro-vided by Google and retrieved from https://developers.google.com/freebase/data on February the 9th of 2012,along with the 10 resources used to create dictionaries in[3], namely the Gene Ontology [26], the Protein Informa-tion Resource [27], the Unified Medical Language System(UMLS) [28], Entrez Gene [29], an Automatically gener-ated dictionary [30], Jochem [31], the Turku Event Corpus[32], Arizona Disease Corpus [33], LINNAEUSDictionary[34] and Websters Second International Dictionary from1934 (included in /usr/share/dict/web2 in the FreeBSD8.1-RELEASE). All of the above resources apart fromUMLS are freely available for research purposes withoutrestrictions. In UMLS, which to the best of our knowledgeis the largest collection of biomedical lexical resources to-date, some of the component resources are restricted evenfor research usage. Please see the UMLS license for furtherdetails.For our experiments we used the corpora originally usedin [3]. These were: the Epigenetics and Post-TranslationalModifications corpus [35], the Infectious Diseases cor-pus [22], the Genia Event corpus [36], the Collabora-tive Annotation of a Large Biomedical Corpus [37], theBioNLP/NLPBA 2004 Shared Task corpus [38] and theGene Regulation Event Corpus [39]. For this work wealso used the following corpora: the Multi-Level EventExtraction corpus [21], the GeneReg corpus [40], the GeneExpression Text Miner corpus [41], BioInfer [7], BioText[42], the Spanish and Dutch subsets of the CoNLL-2002Shared Task corpus [20], the i2b2 Medication Challengecorpus (I2B2) [19] and the OSIRIS corpus [43]. The abovecorpora are readily available for research purposes withthe exception of the I2B2 corpus, which due to its clinicalnature does not allow for redistribution and/or exposurebeyond researchers that have been explicitly authorised toutilise the data.Additional fileAdditional file 1: Table S1 Result tables for all data sets andmodels.The boxed results in the upper left corner signifies the results from [11],while the unboxed results are additions for the extension of the originalpaper. The best score(s) for each data set are underlined and scores whichare not statistically significantly different from the best result(s) with aP-value of 5% when using Fishers exact test are italicised [3].AbbreviationsThe followings abbreviations were used and introduced in this article.NER: Named entity recognition; NLP: Natural language processing; SCD:Semantic category disambiguation; WSD: Word sense disambiguation.Competing interestsThe authors declare that they have no competing interests.Authors contributionsPS and SP conceived of the methods and PS carried out the experiments andwrote the manuscript. SA and JT provided coordination and supervision of theoverall research. All authors read and approved the final version of themanuscript.Stenetorp et al. Journal of Biomedical Semantics 2014, 5:26 Page 10 of 11http://www.jbiomedsem.com/content/5/1/26AcknowledgementsWe would like to thank Mariana Neves of the Wissensmanagement in derBioinformatik (WBI) group at the Humboldt-Universität zu Berlin for providingus with format conversions for four of the corpora used in this work. We wouldalso like to thank Hubert Soyer for providing comments and feedback on thefinal version of the manuscript. Lastly we would like to thank the reviewers fortheir helpful comments, some of which helped define the direction of thiswork.This work was supported by Grant-in-Aid for Specially Promoted Research(MEXT, Japan) and the Royal Swedish Academy of Sciences.This article has been published as part of thematic series Semantic Mining ofLanguages in Biology and Medicine of Journal of Biomedical Semantics. Anearly version of this paper was presented at the Fourth InternationalSymposium on Languages in Biology and Medicine (LBM 2011), held inSingapore in 2011.Author details1Department of Computer Science, University of Tokyo, Tokyo, Japan. 2Schoolof Computer Science, University of Manchester, Manchester, UK. 3NationalCentre for Text Mining, University of Manchester, Manchester, UK. 4MicrosoftResearch Asia, Beijing, Peoples Republic of China.Received: 19 October 2012 Accepted: 3 April 2014Published: 2 June 2014JOURNAL OFBIOMEDICAL SEMANTICSLin and He Journal of Biomedical Semantics 2014, 5:19http://www.jbiomedsem.com/content/5/1/19RESEARCH Open AccessThe ontology of genetic susceptibility factors(OGSF) and its application in modeling geneticsusceptibility to vaccine adverse eventsYu Lin1,2,3* and Yongqun He1,2,3*AbstractBackground: Due to human variations in genetic susceptibility, vaccination often triggers adverse events in a smallpopulation of vaccinees. Based on our previous work on ontological modeling of genetic susceptibility to disease,we developed an Ontology of Genetic Susceptibility Factors (OGSF), a biomedical ontology in the domain ofgenetic susceptibility and genetic susceptibility factors. The OGSF framework was then applied in the area ofvaccine adverse events (VAEs).Results: OGSF aligns with the Basic Formal Ontology (BFO). OGSF defines genetic susceptibility as a subclass ofBFO:disposition and has a material basis genetic susceptibility factor. The genetic susceptibility to pathologicalbodily process is a subclasses of genetic susceptibility. A VAE is a type of pathological bodily process. OGSFrepresents different types of genetic susceptibility factors including various susceptibility alleles (e.g., SNP and gene).A general OGSF design pattern was developed to represent genetic susceptibility to VAE and associated geneticsusceptibility factors using experimental results in genetic association studies. To test and validate the designpattern, two case studies were populated in OGSF. In the first case study, human gene allele DBR*15:01 issusceptible to influenza vaccine Pandemrix-induced Multiple Sclerosis. The second case study reports geneticsusceptibility polymorphisms associated with systemic smallpox VAEs. After the data of the Case Study 2 wererepresented using OGSF-based axioms, SPARQL was successfully developed to retrieve the susceptibility factorsstored in the populated OGSF. A network of data from the Case Study 2 was constructed by using ontology termsand individuals as nodes and ontology relations as edges. Different social network analysis (SNA) methods werethen applied to verify core OGSF terms. Interestingly, a SNA hub analysis verified all susceptibility alleles of SNPsand a SNA closeness analysis verified the susceptibility genes in Case Study 2. These results validated the properOGSF structure identified different ontology aspects with SNA methods.Conclusions: OGSF provides a verified and robust framework for representing various genetic susceptibility typesand genetic susceptibility factors annotated from experimental VAE genetic association studies. The RDF/OWLformulated ontology data can be queried using SPARQL and analyzed using centrality-based network analysismethods.* Correspondence: yuln@med.umich.edu; yongqunh@med.umich.edu1Unit for Laboratory Animal Medicine, University of Michigan Medical School,Ann Arbor, MI 48109, USA2Department of Microbiology and Immunology, University of MichiganMedical School, Ann Arbor, MI 48109, USAFull list of author information is available at the end of the article© 2014 Lin and He; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited.Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 2 of 15http://www.jbiomedsem.com/content/5/1/19BackgroundGenetic susceptibility, also called genetic predisposition, isan increased likelihood or chance of developing a particulardisease (e.g., diabetes) or pathological bodily process (e.g.,infection) due to the presence of one or more gene muta-tions with or without a family history of an increased risk[1]. Genetic susceptibility is associated with all kinds of dis-ease and traits across the whole medical domain, such asinfectious diseases [2], alcoholism [3], cancer [4], and auto-immunity [5]. As a more specific example, human vaccin-ation may induce undesired adverse events, so calledvaccine adverse event (VAE), which may be manifested invarious forms of signs, symptoms and diseases [6]. TheVAE may appear in a small population but not in themajority of vaccinee population, indicating the genetic sus-ceptibility in the small population [7,8]. For example, com-pared to white children, the native American IndianApache children have significant impairment of their anti-body response to H. influenzae type b polysaccharide, thusthey may be prone to develop adverse events if adminis-tered a H. influenzae vaccine with H. influenzae type bpolysaccharide as its component [9]. Better understandingof genetic susceptibility factors to specific diseases willallow us design preventative and therapeutic measures toprevent and control the diseases in susceptible populations.Various kinds of genetic variations bear susceptibilities,e.g., linkage disequilibrium (LD, non-random associationbetween two or more loci) haplotype, a linkage region,genetic polymorphisms, alleles and so on. These variousgenetic variant entities are named genetic susceptibilityfactors by the authors. The allele that confers increasedsusceptibility may be inherited; whereas the disease itselfwill not. The single locus genotype is usually insufficientto cause a disease. A disease often appears when impairedexpressions of alleles at other gene loci and/or environ-mental factors co-exist [10]. Genetic susceptibility factorsmight not have obvious mutations. A genetically inheriteddisorder is more likely the consequence of a polygeniccombination of variants at several genes that might becommon in healthy humans. Moreover, the main deter-minants of susceptibility may be different in differentpopulations [11]. Furthermore, many environmental fac-tors may interact with genetic factors, and they contributeto a diseased outcome simultaneously [7,12]. Many appar-ently contradictory findings in disease-gene associationstudies associated with different study designs increase thecomplexity of the problem [13]. The sophisticated natureof genetic susceptibility makes it challenging to identifytrue genetic factors associated with human susceptibilityto a specific disease or a pathological bodily process.The general methodology to identify the genetic suscep-tibility to complex disease is a combination of linkage andassociation studies in biological experimental science. Atfirst, the family-based studies identify a linkage regioncontains several mega bases of DNA. To narrow downsuch a region to a susceptible gene (or genes), population-based casecontrol studies identify variants in linkagedisequilibrium with the susceptibility locus, which leadto define the genomic region responsible for the originallinkage signal [14]. Although the original linkage signalmay not be detectable in some studies, combination of dif-ferent levels of evidence from multiple studies may de-cipher true genetic susceptibility. At the post-genomicsera, it is possible to use high throughput Omics methodsto identify possible genetic variations that contribute to thegenetic susceptibility. The strategy of applying Omics andother methods to study host genetic variations and their ef-fects in vaccine-induced host immune responses (e.g.,VAEs) has been termed vaccinomics [12]. The notion ofgenetic susceptibility can be traced back to 1926 [15]. Sincethen, numerous literature reports of genetic susceptibilityhave been published. As of December 23, 2013, a PubMedsearch for genetic susceptibility has generated over119,900 hits. However, a database of general geneticsusceptibility factors is not yet available. As a first steptowards systematically collecting and studying genetic sus-ceptibility factors, there is a need to generate a consensus-based robust ontological framework for systematicallyrepresenting and studying such genetic susceptibility andthe genetic factors contributing to the susceptibility.A formal ontology is a set of computer- and human-interpretable terms and relations that represent entitiesin a specific domain and how these entities relate toeach other. Ontological terms are expressed in formallogic to support automated reasoning. Lin et al. have pre-viously developed an Ontology of Genetic SusceptibilityFactors to Diabetes Mellitus (OGSF-DM) intended to pro-vide a framework for genetic susceptibility to diseases [14].By using the TCF7L2 gene and its susceptibility to Type 2Diabetes (T2D) as an example, OGSF-DM formalizesthe basic definitions of genetic susceptibility and geneticsusceptibility factor. The ontology OGSF-DM is a virtualontology composed of three ontologies: the Ontology ofGenetic Disease Investigation (OGDI), which imports othertwo ontologies: the Ontology of Glucose MetabolismDisorders (OGMD) and the Ontology of GeographicalRegions (OGR). The previous study found out that es-sential variables impacting genetic susceptibility to diseasesinclude: genetic polymorphism, the population and geo-graphical location, the disease entities, and related statis-tical values (e.g., odds ratio and p-value) [14].The Open Biological and Biomedical Ontologies (OBO)Foundry community [16] has recently developed many on-tologies that overlap the scope of OGSF-DM. For example,within the OBO Foundry, the Ontology for BiomedicalInvestigations (OBI) that represents biological and clinicalinvestigations [17] overlaps with the scope of OGDI; theontology Gazetteer (GAZ) that describes environmentalLin and He Journal of Biomedical Semantics 2014, 5:19 Page 3 of 15http://www.jbiomedsem.com/content/5/1/19places [18] overlaps with OGR. However, the ontologicalmodeling of genetic susceptibility remains untouched.The original OGSF-DM was loosely aligned with BFO1.0 by denoting some classes as subclasses of continuantor occurrent. The structure of the three OGSF-DM ontol-ogies did not follow the OBO Foundrys principles [16],which makes it difficult to be integrated with other OBOFoundry ontologies. To leverage the reusability and inter-operability of the community developed ontologies, wehave found that the OGSF-DM would be better if beingrefined and focused more on the area of genetic suscep-tibility. We have thus proposed to develop a singleontology: the Ontology of Genetic Susceptibility Factors(OGSF), to represent various types of genetic suscepti-bility and genetic susceptibility factors supported bytextual conclusions given by genetic association studies.While the OGSF-DM modeled the genetic susceptibilityto a disease (i.e., diabetes mellitus) [14], genetic susceptibil-ity is not always associated with only disease. In BFO, a dis-ease is a subclass of disposition, which is positioned in thebranch of BFO:continuant. The genetic susceptibility isoften associated with the risk of a pathological bodilyprocess including a vaccine adverse event [19-21]. Thepathological bodily process as defined by the Ontology ofGeneral Medical Science (OGMS) as a process positionedunder the branch of BFO:occurrent [22]. Therefore, the dis-ease (a dependent continuant) and the pathological bodilyprocess (a BFO:occurrent) are located in two differentmajor branches of BFO. To more comprehensively repre-sent entities related to genetic susceptibility, it is requiredfor OGSF to represent pathological bodily processes suchas vaccine adverse events.In this paper, we introduce our development of a newversion of genetic susceptibility-focused ontology: theOntology of Genetic Susceptibility Factors (OGSF) byusing BFO 2.0 as its upper ontology. To illustrate theFigure 1 The OGSF hierarchy and key OGSF terms introduced in the pontology and verify our ontology design patterns, twovaccine adverse event-related genetic susceptibility casestudies were specifically analyzed. Our studies demon-strate that the OGSF successfully provides an ontologicalframework for systematically representing genetic suscep-tibility, genetic susceptibility factors, associated entitiesand relations.ResultsIn what follows, single quotes are used to refer to a spe-cific term within OGSF where appropriate. The numericalID following the prefix of ontology is given after the termis mentioned, which gives the indication of the terms re-source. Italics are used to indicate the axioms or proper-ties defined in the ontology.The new OGSF is aligned with BFOThe development of OGSF follows the OBO Foundryprinciples, including openness, collaboration, and useof a common shared syntax [16]. To align OGSF withBFO 2.0 version, we started with previously identifiedkey terms and render them using BFO's terms as par-ent terms (Figure 1). To enable the reusability of otherontologies, we have imported many related terms and re-lations from existing OBO foundry ontologies. For ex-ample, the terms vaccine (VO_0000001) and vaccination(VO_0000002) are adopted from the Vaccine Ontology(VO) [23,24]; the terms adverse event (OAE_0000001)and vaccine adverse event (OAE_0000004) are importedfrom OAE. The relations between these vaccine terms andVAE terms are defined in the newly generated OVAE [8].The vaccine related investigation is within the scope of theOBI, so that some OBI terms, such as investigation andtextual conclusion were imported into OGSF.In addition to the reuse of existing ontology terms, over60 OGSF-specific class and property terms exist. The twoaper.Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 4 of 15http://www.jbiomedsem.com/content/5/1/19OGSF core terms are: genetic susceptibility and geneticsusceptibility factor. The OGSF term genetic suscepti-bility (OGSF_0000000) is a subclass of BFO:disposition(BFO_0000016). The alternative term for genetic suscepti-bility is genetic predisposition. In BFO 2.0, the geneticand other risk factors for specific diseases are consideredas predispositions, i.e., they are dispositions to acquireother dispositions. The realization of such a predispositionconsists in processes which change the physical makeupof its bearer in such a way that parts of this bearer serveas the material basis for a disease [25]. Since the term pre-disposition is not included in current version of BFO 2.0,we assert OGSF genetic susceptibility as an immediatechild of BFO term disposition. The child terms of geneticsusceptibility include: genetic predisposition to disease oftype X (OGMS_0000033) and genetic susceptibility topathological bodily process (OGSF_0000001). The termthat reflects our use cases is genetic susceptibility to vac-cine adverse event (OGSF_0000010), which is a child termof genetic susceptibility to pathological bodily process.Another OGSF core term genetic susceptibility factor(OGSF_0000004) is a subclass of material entity (BFO_0000040). Any allele, gene, genotype, or haplotype may bea genetic susceptibility factor if a genetic association studysupports the association between any of those entities anda phenotype. The relation material basis of at some time(BFO_0000127) is formalized in BFO 2.0 to represent therelation between a material entity and a disposition [25].BFO 2.0 refers disposition to the potentials or powers ofthings in the world. Whenever a disposition exists, it is adisposition of something, namely its material bearer [25].This relation is adopted to represent the relation betweengenetic susceptibility factor and genetic susceptibility inOGSF. At the instance level, the same genetic susceptibil-ity factor bearing genetic susceptibility in a person has itsentire existence all the time. But this statement may notbe true at the class level. The same genetic susceptibilityfactor may contribute differently to the manifestation of adisease at different time periods and conditions. Such ameaning is reflected in the words at some time of the re-lation material basis of at some time.OGSF represents different types of genetic susceptibil-ity factors, including haplotypes, genes, single nucleotidepolymorphisms (SNPs), and alleles. A haplotype is a com-bination of DNA sequences at adjacent locations (loci) on achromosome that can be inherited together from a singleparent. A haplotype can describe a pair of genes on onechromosome or all genes on a chromosome from a parent.A haplotype can also refer to an inherited cluster of SNPsthat are variations at single positions in the DNA sequenceamong individuals. An allele is an alternative form of thesame gene or other genetic material that occupies a specificlocation on a chromosome. The Ontology for GeneticInterval (OGI) [26] defines different subclasses of alleleincluding allele of gene, allele of SNP and allele of haplo-type. Since every individual has two parents who each con-tribute one allele, genetic susceptibility factors can usuallybe represented by the notion of allele. Sometimes two ormore SNPs work together and contribute to genetic suscep-tibility. Two situations existed for this condition: the collab-orative SNPs from one haplotype, and the synergisticcombinations of SNPs from different haplotypes. Suchcases are represented as aggregate SNPs in OGSF. OGSFfully imports OGI, thus it inherits the OGIs allele classesand definitions. OGSF inherits the OGI classification ofhaplotype, genes and SNPs as material entities containingsequence information [27]. Different from OGI, the DNAsequences in the Sequence Ontology (SO) represents se-quence information itself [28]. The SO also does not dif-ferentiate different allele types. These are the reason whywe use OGI instead of SO in OGSF. A new relation is_al-lele_of_gene has been created to link allele of gene andgene. This relation is required for logical definition andcorrect reasoning in susceptibility allele of gene analysis asshown in our Case Study 2 described later in the paper.In total, OGSF contains over 600 class and propertyontology terms as shown on http://www.ontobee.org/ontostat.php?ontology=OGSF. In our VAE susceptibilityuse case studies, we have also generated many OGSFinstances as introduced later in this paper.Modeling genetic susceptibility to vaccine adverse eventAs defined in the Vaccine Adverse Event Reporting System(VAERS) and Ontology for Adverse Event (OAE), avaccine adverse event is an adverse event followingvaccination and does not necessarily assume a causalassociation [8,20,21]. However, a causal association be-tween administration of a specific vaccine and an ad-verse event in a particular population can be identifiedthrough systematic and statistical studies [7,12,29,30].Although a large number of studies have provided sup-porting evidences for asserting susceptibility factors (e.g.,susceptibility alleles) to vaccine adverse event outcomes,the results of these studies cannot be automatically proc-essed by computers. Our OGSF presentation aims to cre-ate a machine-interpretable ontological representation ofthese studies in order to analyze the results across studiesand search for possible causal associations.Figure 2 illustrates the design pattern of how OGSF isused to represent the association between a genetic sus-ceptibility factor and a vaccine adverse event (VAE)based on experimental studies reported in the literature.As shown in the figure, the genetic susceptibility factoris the material basis of genetic susceptibility. The gen-etic susceptibility to vaccine adverse event is realized inthe process of vaccine adverse event (OAE_0000004).In the vaccine case, the genetic susceptibility factor is apart of a human vaccinee carrying susceptibility alleleFigure 2 General design of OGSF representing genetic susceptibility to vaccine adverse event. Square boxes denote classes, and italicizedterms along the arrows denote relations.Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 5 of 15http://www.jbiomedsem.com/content/5/1/19for adverse event (OGSF_0000029), which actively par-ticipates in the vaccine adverse event. As a participantof a genetic association investigation (OGSF_0000016),a case group (OGSF_0000022) has a member of humanvaccinee carrying susceptibility allele for adverse event.A human vaccinee is vaccinated with a vaccine. Thevaccination occurs before (or is preceded by) a vaccineadverse event. As a specified output of the genetic asso-ciation investigation, the textual conclusion of geneticsusceptibility concludes the association between a geneticsusceptibility factor and a vaccine adverse event. Belowwe provide more specific details to introduce this OGSFdesign pattern.The direct linkage from susceptibility-related terms toVAE terms is often required in our OGSF modeling. Forexample, in OGSF, we need to link human vaccinee carry-ing susceptibility allele for adverse event (OGSF_0000029)to a vaccine. An object property term (ontological relation)reflecting such linkage is not available in existing ontol-ogies. However, VO defines a shortcut relation vaccineimmunization for host, which relates a vaccine with a vac-cinee [23]. The strategy of designing and using shortcut re-lations has been promoted by Mungall et al. to simplify thecomplex axioms involving nested class expressions to makeit triple-friendly for complex OWL ontologies [31]. In ourdesign, the inverse usage of this VO shortcut relation vac-cine immunization for host connects from human vaccineecarrying susceptibility allele for adverse event to vaccine.The term genetic association investigation (OGSF_0000016) is defined as: an investigation that aims to testwhether single-locus alleles or genotype frequencies (ormore generally, multi-locus haplotype frequencies) dif-fer between two groups of individuals (usually diseasedsubjects and healthy controls). Different types of thosestudies exist. For example, a case control genetic associ-ation study (OGSF_0000017) is a genetic association studythat contains two types of human study subject groups:case group and control group. The control group providesa background control in order to properly assess the resultsidentified from the case group study. In contrast, a case-only genetic association study (OGSF_0000036) includes acase group and does not have a control group to compare.The results obtained from a case-only genetic associationstudy provide sufficient evidence to detect an association[32]. However, they are often biased by pre-condition ofnon-independence between the genetic and environmen-tal factors in the population [33]. Another type of gen-etic association study is family-based genetic study(OGSF_0000041) that investigates family members whomay show different phenotypes. By analyzing entire ge-nomes of people with a disease (cases) and similarpeople without the disease (controls), a Genome-WideAssociation Study (GWAS or GWA study) examinesmany common genetic variants in different individualsto see the association between variant and a trait [7,12].Such a genome wide association study is a type of casecontrol genetic association study.A reported genetic susceptibility study typically includesa conclusion of the association between a genetic factorand a disease (or pathologic bodily process) under specificconditions. Such conclusion is required to be representedontologically. To represent the results from individualgenetic association studies as reported in different papers,we have added an OGSF term textual conclusion ofgenetic susceptibility to represent the textual conclu-sion of a genetic susceptibility study. Ontologically, aLin and He Journal of Biomedical Semantics 2014, 5:19 Page 6 of 15http://www.jbiomedsem.com/content/5/1/19textual conclusion of genetic susceptibility is asserted as aspecified output of  a genetic association investigation.There are three types of textual conclusion of genetic sus-ceptibility: positive textual conclusion of genetic suscepti-bility (OGSF_0000031), negative textual conclusion ofgenetic susceptibility (OGSF_0000032) and neutral text-ual conclusion of genetic susceptibility (OGSF_0000033).Using the vaccine adverse event example, a positive text-ual conclusion of genetic susceptibility means that a posi-tive conclusion is drawn based on a significant statisticalassociation of a genetic factor and a vaccine adverse eventas identified in a published paper. A negative textual con-clusion of genetic susceptibility denies such a possible as-sociation between a genetic factor and an adverse event asdeclared in a published paper. Sometimes, depending onthe data, an investigator might not be able to draw a de-finitive positive or negative conclusion on a genetic sus-ceptibility association. This situation is captured usingneutral textual conclusion of genetic susceptibility. Inaddition, OGSF also provides several datatype properties,such as hasOddsRatio and hasPvalue, to allow the repre-sentation of digital data for statistical evaluation of thetextual conclusion of genetic susceptibility (Figure 2).Use case studiesCase studies are used for two purposes: 1) to validate themodeling, 2) to test possible applications of the ontology.Below we represent two case studies reported from peer-reviewed journal articles using the OGSF framework.Figure 3 OGSF modeling of vaccine-associated multiple sclerosis. SquCase study 1: HLA allele DBR1*15:01 is genetic susceptibilityto Pandemrix related multiple sclerosis in a case report studyPandemrix is an influenza pandemics vaccine that is de-veloped by the company GlaxoSmithKline. The vaccinePandemrix is represented in the Vaccine Ontology (VO)with the VO ID: VO_0000410. Vrethem et al. reported theoccurrence of severe Multiple Sclerosis (MS) in a previouslyhealthy young male in association with the vaccination ofPandemrix [34]. In this study, a human DBR1*15:01 allele isresponsible for association with the Pandemrix-related MSadverse event. DBR1*15:01 is an allele of human leukocyteantigen (HLA) complex that encodes a MHC class II cellsurface receptor. The association of this allele with MSappears to be consistent with many previous reports onsituations other than vaccine adverse event [35,36].This genetic susceptibility case was represented inFigure 3 by following the general OGSF design pattern(Figure 2). For ontological modeling, it is critical to gener-ate description logic constraints and axioms to accuratelyrepresent human- and computer-interpretable knowledge.As an example, the basic information about DRB1*15:01can be ontologically represented as: DRB1*15:01 is subclass of allele of gene. DRB1*15:01 is subclass of (is_allele_of_gene someHLA DBR1 gene).In addition to the above basic logical definitions, geneticsusceptibility related to DBR1*15:01 can be identifiedbased on different studies. Case Study 1 is such a study,are boxes denote classes, and curved box denote instances.Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 7 of 15http://www.jbiomedsem.com/content/5/1/19which is represented as genetic association study_1(Figure 3). This study generated a specific output posi-tive conclusion of genetic susceptibility_1. This specificconclusion is about the class DBR1*15:01 and themultiple sclerosis AE. The instance of DBR1*15:01 isa part of the specific patient in the case study. Based onthis and many other case reports [34-36], we have gen-erated the OGSF representation at the class level: DRB1*15:01 is subclass of (part of continuant at alltimes that whole exists some (human vaccinee and(inverse (vaccine immunization for host) somePandemrix))) DRB1*15:01 is subclass of (material basis of atsome time some genetic susceptibility to vaccineadverse event) DRB1*15:01 is subclass of susceptibility alleleThis case study indicates that OGSF provides necessaryelements to represent genetic susceptibility and genetic sus-ceptibility factors associated with vaccine adverse events.Case study 2: genetic polymorphisms associated withadverse events after smallpox vaccination in multipleclinical trialsReif et al. reported that genetic polymorphisms in severalgenes encoding important immune factors, including en-zyme methylenetetrahydrofolate reductase (MTHFR), anTable 1 Statistical summary of genetic susceptibilityfactors with systemic adverse event following smallpoxvaccinationGSF& Allele Gene Odds ratio(confidentialinterval)P-value Study1 or 2rs1801133 SNP T MTHFR 2.3 (1.15.2) 0.04 1rs1801133 SNP T MTHFR 4.1 (1.411.4) 0.01 2rs9282763 SNP G IRF1 3.2 (1.19.8) 0.03 1rs9282763 SNP G IRF1 3.0 (1.18.3) 0.03 2rs839 SNP A IRF1 3.2 (1.19.8) 0.03 1rs839 SNP A IRF1 3.0 (1.18.3) 0.03 2Haplotype 1* G,A IRF1 3.2 (1.010.2) 0.03 1Haplotype 1* G,A IRF1 3.0 (1.09.0) 0.03 2Haplotype 2# T,C,A IL4 2.4 (1.05.7) 0.05 1Haplotype 2# T,C,A IL4 3.8 (1.014.4) 0.06$ 2Notes:&GSF stands for Genetic Susceptibility Factor.*Haplotype 1 contains G allele of rs9282763, and A allele of rs839 inIRF1 gene.#Haplotype 2 contains T allele of rs2070874, C allele of rs2243268, and A alleleof rs2243290 in IL4 gene.$In the second study, Haplotype 2 didnt achieve the significant level ofp value.The data of this table is summarized and curated from Reif. et al.s work on thegenetic analysis of adverse event after smallpox vaccination [37].immunological transcription factor (IRF1), and interleukin-4 (IL-4), were associated with adverse events after smallpoxvaccination [37]. In this report, two independent clinical tri-als were conducted as initial and replicating genetic associ-ation studies. Different from the Case Study 1 where anallele of gene is a susceptibility factor, susceptibility allelesof Single Nucleotide polymorphisms (SNPs) are the mater-ial basis of genetic susceptibility in this Case Study 2. Table 1lists all the SNPs (e.g., the A allele of rs839 SNP in the geneirf1), their associated genes, and the Odds Ratio and p-value from two clinical trials [37].The OGSF design pattern was applied to represent theinformation from these clinical trial studies (Figure 4).This figure does not include many linkages and axiomssimilar to those illustrated in Figure 3. Instead, Figure 4focuses on representation of statistics providing evidenceindicating the type of genetic associations to vaccineadverse events. In OGSF, the datatype property has-Size allows the recording of the size of a human studysubject group such as case group. The datatype prop-erties hasOddsRatio, hasPvalue and hasCI (confidenceinterval) link the corresponding data to specific textualconclusion of genetic susceptibility. The Odds Ratio,P-value, and confidential interval are used to measurethe association between genotypes and vaccine adverseevent [37]. The Odds Ratio represents the ratio that anoutcome will occur given an exposure, compared tothe odds of the outcome occurring in the absence ofthe same exposure [38]. Using these datatype proper-ties, the values of these measurements were capturedand represented within the ontology. For example, theconclusion of clinical trial 1 regarding the T allele ofrs1801133 SNP was supported by the statistical data:having an Odds Ratio of 2.3, a P-value 0.03, and a con-fidence interval of [> = 1.4, <=11.4]. These statisticalresults support a positive genetic association betweenthe allele of SNP and systemic adverse events of smallpoxvaccination [37].Since OGSF provides a framework to ontologicallyrepresent the complex data structure (including differ-ent variables and relations among these variables), therepresentation of the knowledge and data using OGSFsupports computer-assisted data integration and rea-soning. Such data sets can be queried efficiently usingSPARQL as described below.SPARQL queryThe SPARQL Protocol and RDF Query Language(SPARQL) is the query language and protocol for theResource Description Framework (RDF) data. RDF de-composes any knowledge into triples. Each RDF triplecontains three components: subject, predicate, and ob-ject [39]. OGSF is developed using the Web OntologyFigure 4 OGSF modeling of case study 2. Square boxes denote classes, and curved boxes denote instances.Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 8 of 15http://www.jbiomedsem.com/content/5/1/19Language (OWL) [40]. Both RDF and OWL are means toexpress increasingly complex information or knowledge,and both can be serialized in the RDF/XML syntax. RDF byitself has a limited capability for formal knowledge represen-tation. OWL adds ontological capability to RDF by definingthe components of RDF triples with formal computable firstorder description logic. So OWL provides more semanticrichness. In addition, the OGSF OWL document can beconverted to RDF format and queried by SPARQL.From the OGSF supported knowledge system, ourquestions are focused on: 1) the list of susceptibility fac-tors to a certain disease or pathological bodily process;2) the evidences, either supportive or negative, support-ing those susceptibilities. Using Case Study 2 as anexample, we designed a SPARQL query to identify thegenetic susceptibility factors to systemic adverse eventof smallpox vaccination and related statistical evidences.The SPARQL script developed to query against theOGSF ontology is provided as follows:This query was executed in the SPARQL plugin embed-ded with Protégé 4.3, build 304, and it could also be per-formed using the SPARQL endpoint (http://www.ontobee.org/sparql/index.php) in Ontobee [41], a linked data web-server where OGSF was deployed. The SPARQL executionretrieved five susceptibility factors to systemic smallpoxvaccine adverse event as shown in Additional file 1 andlisted below:1. T allele of rs1801133 SNP supported by 1 positiveevidence.2. G allele of rs9282763 SNP supported by 2 positiveevidence.3. A allele of rs839 SNP supported by 2 positiveevidence.4. haplotype 1 in IRF1 gene supported by 2 positiveevidence.5. haplotype 2 in IL4 gene supported by 1 positiveevidence, and 1 negative evidence.Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 9 of 15http://www.jbiomedsem.com/content/5/1/19The SPARQL query output is consistent with the resultsobtained from the paper (Table 1). Therefore, our evalu-ation confirms the value of OGSF ontology representationof genetic susceptibility knowledge and instance data set.Social network analysis and visualizationAfter an ontology is generated, it is often valuable butchallenging to determine which ontology terms are morecentral and carry more information than other terms inthe ontology. As an ontology defines terms and relations(object properties) between terms, an ontology can beviewed a social network. Specifically, the terms and rela-tions of an ontology can be viewed as a directed hyper-linked graph G = (V, E) with nodes v?V and edges e?E,where the nodes correspond to the terms or entities inan ontology, and a directed edge (p, q) ? E indicates therelation that links from p (i.e., the relations domain) toq (i.e., the relations range). Therefore, the methods usedfor social network analyses may be potentially used foridentifying key ontology terms as hubs or clusters ofontology terms [42]. In this study, we aimed to applyknown social network analysis methods to evaluate thestructure of the OGSF ontology and examine whetherOGSF was constructed effectively to represent key entitiesfor study of genetic susceptibility and genetic susceptibilityfactors as we designed.Social Network Analysis (SNA) is the sum of the toolsand methodologies of graph theory to analyze and thusdescribe structures of social networks [43]. Many SNAmethods also overlap with network analysis methodsfrom other domains such as literature mining-derivedgene network analyses [44]. Two questions have beenpre-designed for such social network analyses: Firstly,can the use case data support such identified centralterms in the network? Secondly, can different networkanalysis methods generate different results and insights?To address these questions, the data from Case Study 2were extracted using OntoGraf [45], and then visualizedand analyzed using social network visualization toolGephi [46]. The software was used to conduct the ana-lyses of the degree centrality, closeness centrality, andhubs and authority scores to measure the relative im-portance of a node within the network. The statisticalmeasurement data of these analyses are included inAdditional file 2.The first method of our network analysis was based onthe calculation of the degree centrality (Figure 5A). Thedegree centrality is simply the number of direct edgesthat an entity has in a network [43,44]. The network has24 nodes and 38 edges with an average degree of 1.538.Our analysis found that the two terms with the highestdegree centrality scores are systemic adverse event ofsmallpox vaccination, and haplotype 2 in IL4 gene.These two terms have the highest numbers of links toother terms. These findings are consistent with theknowledge stored in the ontology. However, the termhaplotype 2 in IL4 gene is not our intended core terms.This gives us insights that the degree measurement onlycannot verify the core terms of the current network.Secondly, we used the closeness centrality for networkexploration (Figure 5B). The closeness centrality mea-sures the average shortest path from a node to all othernodes. Specifically, the closeness centrality calculatesthe inverse of the farness that is the sum of a nodesdistances to all other nodes [47]. The more closenesscentrality a node is, the easier it can be reached byother nodes or reach out other nodes. The five ontologyterms that have the best closeness centrality scores andhave no out-reaching nodes are genetic susceptibilityto vaccine adverse event, systemic adverse event fol-lowing smallpox vaccination, IL4 gene, IRF1 gene,and MTHFR gene. The result is consistent with thedesign and construction of the ontology: the evidencelink to genetic susceptibility and vaccine adverse event,the variants link to genes. It is interesting that all the threegenes were identified together in this study.The third network analysis was based on the calcula-tion of the authority and hub scores [47,48] (Figure 6).The terms (nodes) that many other terms point to arecalled authorities. In contrast, the terms pointing to arelatively high number of authorities are called hubs.The authorities and hubs are a natural generalization ofthe eigenvector centrality that measures the influence ofa node in a network. The authority analysis has beenused for ranking web pages, and the data and ontologiesfrom the Semantic Web search [49]. Figure 6A showsthat top three authority centralized nodes: systemic ad-verse event of smallpox vaccination, genetic suscepti-bility of vaccine adverse event, and IL4 gene. Thereresults indicate: 1) the main focus of this piece of linkeddata is about systemic adverse event of smallpox vaccin-ation and genetic susceptibility; 2) IL4 gene carries moreinformation flow than others, for it is connected with twokinds (positive and negative) of evidence and a haplotypeof three SNPs in the network. Figure 6B shows nodes withhighest hub scores. Interestingly, these identified hubs areall the SNPs related to the adverse event concluded inCase Study 2.In summary, different network characteristics cal-culations reflect different dimensions of the ontologyknowledge. The closeness and authority centrality ana-lyses verified the core terms of the OGSF dataset incase study 2 are systemic adverse event of smallpoxvaccination and genetic susceptibility of vaccine ad-verse event. Interestingly, the hub analysis identifiedall the alleles of SNPs, and the closeness analysis de-tected all three hidden genes that are related to thosealleles of SNPs. It is noted that the genes instead of theFigure 5 Degree and closeness network analyses using Case Study 2 data modeled in OGSF. (A) Degree centrality. The size of a nodeindicates the degree of the node indicating the number of connections from the node. (B) Closeness centrality. The closeness centrality analysisidentified all three genes in the case study dataset. The visible nodes in the figure all have closeness centrality value equal to 0. The nodes in thefigure represent classes and instances contained in the case study. Those nodes displayed in the same color are clustered in the same group bythe modularization method of the software Gephi [46].Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 10 of 15http://www.jbiomedsem.com/content/5/1/19alleles of SNPs are usually found by direct literaturesearching. Based on these observations, our networkanalyses accurately identified ontology terms essentialfor representing genetic susceptibility and genetic sus-ceptibility factors.DiscussionIn this paper, we have introduced the development ofthe new version of the Ontology of Genetic Susceptibil-ity Factors (OGSF) and its usage for ontologically repre-senting genetic susceptibility to vaccine adverse events.The new OGSF is aligned with the BFO 2.0. OGSF im-ports many terms from existing ontologies and also in-cludes many new ontology terms. For the first time, wehave ontologically represented the genetic susceptibilityto a pathological bodily process (i.e., vaccine adverseevent). Two vaccine adverse event use cases were repre-sented and evaluated. The SPARQL and social networkanalyses were implemented to evaluate and analyze theOGSF contents and structure. Different social networkanalysis methods identified ontology terms with differ-ent types of importance in the ontology.OGSF emphasizes the classification of different geneticfactors and polymorphisms associated with susceptibilityto diseases or pathological bodily processes. Some suscep-tibility factors may be genotype or mutation, which can beexpressed using different allele classes. Moreover, OGSFhas several classes, such as susceptibility SNP interval, sus-ceptibility gene, and susceptibility haplotype to host thoseentities that is not allele per se. For example, in the con-structed network of our case study 2, the IL4 gene is thethird authoritative node but the first gene identified fromFigure 6 Authority and hub network analyses using Case Study 2 data modeled in OGSF. (A) Authority analysis. The top 3 node with thehighest authority score are systemic adverse event of smallpox vaccination, genetic susceptibility to vaccine adverse event, and IL4 gene.(B) Hub analysis. Hub nodes in this network are all the SNPs. All the visible nodes have the highest hub score of 0.08.Lin and He Journal of Biomedical Semantics 2014, 5:19 Page 11 of 15http://www.jbiomedsem.com/content/5/1/19the authority analysis (Figure 6A). From the SPARQLquery result, only haplotype 2 of IL4 gene is linked to twodifferent evidences: the positive conclusion from trial 1and the negative conclusion from trial 2. Moreover, thehaplotype 2 of IL4 gene is consisted of three SNPs that ismore than other haplotype in the network (Table 1). Thisstructure increases the ranking of IL4 gene in the author-ity analysis comparing to other genes. More interestingly,in another genetic susceptibility to smallpox vaccine ad-verse event study, a haplotype in IL4 gene is related with adecrease of the susceptibility to fever after vaccination[50]. This haplotype contains a SNP rs2243250 located inthe promoter region of IL 4 gene, where a C?T substitu-tion is associated with increased production of IL-4 [50].Searching the HaploReg database [51], this SNP is pre-dicted to be located in the same haplotype of IL4 gene in-troduced in Case Study 2. This example shows thecomplicated role that IL4 gene polymorphisms play in thesystemic adverse event triggered by smallpox vaccination.It also shows the importance of representing the increaseor decrease (resistance) of genetic susceptibility.In addition to the genetic susceptibility factors, manyother variables may also contribute to the manifestation ofa disease or a pathological bodily process outcome (e.g.,vaccine adverse event) [30]. For example, the human in-dividuals characteristics, such as race/ethnic identity,geographical region, and disease history, may also playan important role in the manifestation of an adverseoutcome. Different genetic study design, such as familystudy or population-based study, may lead to differentconclusions. To identify possible causality between agenetic susceptibility factor and a VAE, a statistical ana-lysis is often required. The sample size of human subjectsinvolved will also affect the statistical power of geneticassociation studies. Our integrative OGSF framework hasincorporated many statistical terms in order to measureLin and He Journal of Biomedical Semantics 2014, 5:19 Page 12 of 15http://www.jbiomedsem.com/content/5/1/19the robustness of the genetic association with a specificdisease or pathological outcome. The statistical measure-ment then gives foundations to support the true geneticassociation between genetic susceptibility factors and re-lated disease or pathological bodily process. Well-designedexperiments may be applied to verify the association.Different methods can be used for ontology evalua-tions [52]. A use case analysis is critical to evaluate thecorrectness, completeness, and utility of an ontology.Two use cases have been chosen and presented in thepaper to illustrate how OGSF is logically constructedand useful in representing genetic susceptibility to vac-cine adverse events. To further evaluate the ontologyutility in addressing specific questions, we designed andimplemented SPARQL queries to identify known gen-etic susceptibility factors to smallpox vaccine-inducedsystemic adverse events as shown in the second use case.Furthermore, different social network analyses were ap-plied to identify and verify the key ontology terms essentialin the topic.Although social network analysis (SNA) has been widelyused in the fields of web search and social studies, its ap-plication in ontology field is rare. SNA uses graph theories.Since ontologies can be considered as (labeled, directed)graphs, graph analysis techniques are promising tools forevaluating ontologies in many dimensions. Hoser et al.have applied SNA to analyze the structures of SuggestedUpper Merged Ontology (SUMO) and SWRC ontology[43]. Harth et al. and Hogan et al. have been developingsearch strategies using network-based approaches to minelinked data in semantic web respectively [49,53]. Theirstudies show that the SNA of a given ontology providesdeep insights into the structure of ontologies and know-ledge base. These ontology-related SNA studies treated allontology classes and relations as network nodes. Differentfrom this approach, our SNA analyses only consider ontol-ogy classes and their instances as nodes and make ontol-ogy relations (i.e., object properties) as edges. Our distincttreatment of ontology relations as edges makes sensessince these relations are designed to link different classesand their instances. Our SNA study found that thevisualization and social network analysis results usingthe Case Study 2 data provide better understanding ofontology designing and evaluation. Interestingly, ourSNA hub and closeness analyses generated two distinctsets of results. The hub analysis identified all five sus-ceptibility alleles of SNPs as top key terms while thecloseness analysis detected all three susceptibility genescollected in the Case Study 2. The SNA hubs are termsdirected to the high authority terms. Our identificationof all the SNPs as hubs is consistent with the notion thatthese SNPs are essential for the authority terms such assystemic adverse event of smallpox vaccination andgenetic susceptibility of vaccine adverse event. Thecloseness centrality measures how a node can be easilyreached by other nodes. As the genes have different sus-ceptibility variants (i.e., SNPs of genes), it makes sense thatthe genes have better closeness centrality scores than theirvariants. Since these genes are not directly defined as gen-etic susceptibility factors, the genes appear to be hiddenfactors that can be mined from the OGSF data. When weconsider the gene functions, the direct gene name extrac-tion gives more biological meaningful information thanthe variants themselves. These distinct observations sug-gest that different SNA analysis methods may identifyontology terms essential from different aspects.Other than OGSF, many other research projects alsofocus on establishing and cataloging the relation betweengenotypes and phenotypes. For example, the Databaseof Genotypes and Phenotypes (dbGaP) is a repositoryfor archiving, curating, and distributing the informationobtained from studies investigating the interactions ofgenotypes and phenotypes [54]. SNPedia is focused onthe medical, phenotypic and genealogical associations ofSNPs [55]. The Leiden Open (source) Variation Database(LOVD) provides open data of genetic variants curatedfrom published paper, and the disease association in-formation is included [56]. GWAS central (previouslycalled HGBASE, HGVbase and HGVbaseG2P) pro-vides a centralized compilation of summarized findingsfrom genetic association studies [57]. These resourcesprovide structured raw or curated information relatedto genotypes and phenotypes. However, unlike OGSF,these resources do not ontologically represent differentgenetic susceptibility types and genetic susceptibilityfactors with all necessary information and evidence as-sertions. OGSF is able to serve as an intermediate andan integrative layer between various evidence-basedmedicine applications and above existing structure dataresources and other unstructured data resources.Our study clearly shows that OGSF provides a robustplatform to support logical representation and analysisof genetic susceptibility and genetic susceptibility factors.Such platform will allow us to logically organize the know-ledge and data related to genetic susceptibility and geneticsusceptibility factors. With the well-organized informa-tion, it is then possible to generate automatic reasoningprograms to analyze the data, predict new knowledge ongenetic susceptibility, and support personalized medicineresearch. However, while the use case studies out of theliterature curation were meant for evaluating and validat-ing the OGSF framework, it would be a huge effort tomanually curate all the possible data available in the litera-ture. To improve the study of genetic susceptibility factors,it might help to devote more programing effort to select-ively integrate related data sources from openly accessibleresources such as the SNPedia [55] as introduced above.Advanced text mining programs may also be developed toLin and He Journal of Biomedical Semantics 2014, 5:19 Page 13 of 15http://www.jbiomedsem.com/content/5/1/19retrieved related information from unstructured literaturedata. Following these programming efforts, a large amountof manual curation may also be requested for expandingthe ontology and making it more useful. To achieve along-term goal of solving susceptibility issues, some spe-cific domains may initially be focused. We are looking forcollaborations for further applying OGSF for practicalusage for scientific domains.ConclusionsOriginated from previous OGSF-DM research [14], thenew Ontology of Genetic Susceptibility Factors (OGSF)is aligned with the framework of BFO 2.0 and developedto ontologically represents various genetic susceptibilitytypes, genetic susceptibility factors, and related entitiesand relations. OGSF has been used to represent geneticsusceptibility and susceptibility factors associated withvaccine adverse events as annotated from experimentalstudies. Our SPARQL and network evaluations haveshown that OGSF is able to provide a robust frameworkfor the representation and analysis of genetic susceptibil-ity knowledge and datasets. The social network analysisresults also demonstrated that key ontology terms crit-ical in different aspects can be detected with differentcentrality-based network analysis methods.MethodsOntology editingThe format of OGSF ontology is W3C standard WebOntology Language (OWL2) (http://www.w3.org/TR/owl-guide/). For this study, many new terms and logical def-inition were added into original OGSF [14] using theProtégé 4.3.0 build 304 OWL ontology editor (http://protege.stanford.edu/).Ontology term reuse and new term generationOGSF imports the whole set of the Basic Formal Ontology(BFO) [58]. To support ontology interoperability, termsfrom OBO Foundry ontologies, such as OBI, OAE, IAOand etc., are reused. For this purpose, OntoFox [59] wasapplied for extracting individual terms from external on-tologies. For those genetic susceptibility-specific terms, wegenerated new OGSF IDs with the prefix of OGSF_followed by seven-digit auto-incremental digital numbers.New OGSF terms created according to the intensive mod-eling from the use cases.Evaluation of OGSF by SPARQLUse case studies were designed based on literature survey.SPARQL was performed using the SPARQL query plug-inembedded with Protégé 4.3.0 build 304.Evaluation of OGSF by social network analysisGraphed data used for visualization was first extractedfrom OGSF using the OntoGraf plug-in [44]. After man-ual editing, the file (Additional file 3) was used as inputfor the network visualization software Gephi 0.8.2 beta(http://gephi.org) [45]. Gephi was also used to conductsocial network data analysis and visualization based onthe extracted data. The embedded algorithms in Gephiwere used to calculate the scores of degree, closeness[59], and hub and authority [46].Availability and accessThe website for OGSF project is available at http://code.google.com/p/ogsf/. As an OBO Foundry library ontol-ogy, OGSF has been deposited by default in the Ontobeelinked data server [41]. All OGSF terms can be browsedand searched via the Ontobee at http://www.ontobee.org/browser/index.php?o=OGSF. The source of the ontol-ogy is also deposited in the NCBO Bioportal: http://bioportal.bioontology.org/ontologies/3214.Additional filesAdditional file 1: Screen shots of SPARQL queries. (A) SPARQL queryused in Ontobee SPARQL query endpoint. The file includes the SPARQLquery script used in the Ontobee SPARQL query endpoint (http://www.ontobee.org/sparql/index.php) and its results as returned by the OntobeeSPARQL query server. (B) Additional File 4.png. Screen shot of ProtégéSPARQL query tab showing the SPARQL query result.Additional file 2: Network characteristic measurements of eachnode in the use case 2 graph. The file includes in-degree, out-degree,degree, authority, hub, modularity, clustering, strength, local clusteringcoefficient, eigenvector centrality, PageRank, eccentricity, closenesscentrality, betweenness centrality scores of the 24 nodes in the graph.The calculation was conducted by using Gephi software.Additional file 3: The input file used for network visualizationanalysis using Gephi software. The file includes the data of individualsand related classes of case study 2 extracted from OGSF ontology.AbbreviationsBFO: Basic formal ontology; FOAF: Friend of a friend project; HLA: Humanleukocyte antigen; GAZ: Gazetteer; IAO: Information artifact ontology;LD: Linkage disequilibrium; OAE: Ontology of adverse event; OBI: Ontologyfor biomedical investigations; OBO: Open biological and biomedicalontologies; OGDI: Ontology of genetic disease investigation; OGI: Ontologyfor genetic interval; OGMD: Ontology of glucose metabolism disorders;OGMS: of General medical science; OGR: Ontology of geographical regions;OGSF: Ontology of genetic susceptibility factors; OGSF-DM: Ontology ofgenetic susceptibility factors to diabetes mellitus; OVAE: Ontology of vaccineadverse event; OWL: Web ontology language; REO: Reagent ontology;SKOS: Simple knowledge organization system; SNA: Social network analysis;SNP: Single polymorphism nucleotide; SPARQL: SPARQL protocol and RDFquery language; SUMO: Suggested upper merged ontology; URI: Uniformresource identifier; VO: Vaccine ontology.Competing interestsThe authors declare that they have no competing interests.Authors contributionsYL: Primary OGSF developer, use case testing, social network analysis andvisualizing, drafting of manuscript. YH: OGSF developer, vaccine adverseLin and He Journal of Biomedical Semantics 2014, 5:19 Page 14 of 15http://www.jbiomedsem.com/content/5/1/19event domain expert, use case testing, and drafting of manuscript. Bothauthors read and approved the final manuscript.AcknowledgementsThis work has been supported by grant R01AI081062 from the NationalInstitute of Allergy and Infectious Diseases. We also appreciate the editorialreview of this manuscript by Dr. John Myles Axton. The publication fee waspaid by a discretionary fund from Dr. Robert Dysko, the director of the Unitfor Laboratory Animal Medicine (ULAM) in the University of Michigan.Author details1Unit for Laboratory Animal Medicine, University of Michigan Medical School,Ann Arbor, MI 48109, USA. 2Department of Microbiology and Immunology,University of Michigan Medical School, Ann Arbor, MI 48109, USA. 3Center forComputational Medicine and Bioinformatics, University of Michigan MedicalSchool, Ann Arbor, MI 48109, USA.Received: 17 August 2013 Accepted: 20 February 2014Published: 30 April 2014JOURNAL OFBIOMEDICAL SEMANTICSHe et al. Journal of Biomedical Semantics 2014, 5:29http://www.jbiomedsem.com/content/5/1/29RESEARCH Open AccessOAE: The Ontology of Adverse EventsYongqun He1*, Sirarat Sarntivijai1,2, Yu Lin1, Zuoshuang Xiang1, Abra Guo1, Shelley Zhang1, Desikan Jagannathan1,Luca Toldo3, Cui Tao4 and Barry Smith5AbstractBackground: A medical intervention is a medical procedure or application intended to relieve or prevent illness orinjury. Examples of medical interventions include vaccination and drug administration. After a medical intervention,adverse events (AEs) may occur which lie outside the intended consequences of the intervention. The representationand analysis of AEs are critical to the improvement of public health.Description: The Ontology of Adverse Events (OAE), previously named Adverse Event Ontology (AEO), is acommunity-driven ontology developed to standardize and integrate data relating to AEs arising subsequent tomedical interventions, as well as to support computer-assisted reasoning. OAE has over 3,000 terms with uniqueidentifiers, including terms imported from existing ontologies and more than 1,800 OAE-specific terms. In OAE, theterm adverse event denotes a pathological bodily process in a patient that occurs after a medical intervention.Causal adverse events are defined by OAE as those events that are causal consequences of a medical intervention. OAErepresents various adverse events based on patient anatomic regions and clinical outcomes, including symptoms, signs,and abnormal processes. OAE has been used in the analysis of several different sorts of vaccine and drug adverse eventdata. For example, using the data extracted from the Vaccine Adverse Event Reporting System (VAERS), OAE was usedto analyse vaccine adverse events associated with the administrations of different types of influenza vaccines. OAE hasalso been used to represent and classify the vaccine adverse events cited in package inserts of FDA-licensed humanvaccines in the USA.Conclusion: OAE is a biomedical ontology that logically defines and classifies various adverse events occurring aftermedical interventions. OAE has successfully been applied in several adverse event studies. The OAE ontologicalframework provides a platform for systematic representation and analysis of adverse events and of the factors(e.g., vaccinee age) important for determining their clinical outcomes.Keywords: Ontology of Adverse Events, OAE, Adverse event, Ontology, Vaccine, Drug, Vaccine adverse event, VAERS,Drug adverse event, Design patternBackgroundA medical intervention is a medical procedure or appli-cation intended to relieve or prevent illness or injury.The medical intervention can be an administration of adrug, a vaccine, a special nutritional product (for example,a medical food supplement), or it can be the use of amedical device. In the wake of a medical intervention,adverse events (AEs) may occur which lie outside theintended consequences of the intervention. These AEsare pathological bodily processes [1]. Severe AEs includetriggering of Guillain-Barre or Stevens-Johnson Syndromeparalysis and, in extreme cases, death. Such AEs may* Correspondence: yongqunh@med.umich.edu1University of Michigan, Ann Arbor, MI, USAFull list of author information is available at the end of the article© 2014 He et al.; licensee BioMed Central Ltd.Commons Attribution License (http://creativecreproduction in any medium, provided the orresult in hospitalization of the patient and requiring spe-cial care. Although typically having low incidence rates,they may impact the usage or regulation of vaccine, drug,or medical devices in the market. To monitor and investi-gate adverse events of various types, reporting systemshave been established to collect the relevant information.For example, the USA national vaccine safety surveillanceprograms include the Vaccine Adverse Events ReportingSystem (VAERS) [2] and the Food and Drug Administra-tion (FDA) Adverse Events Reporting System (FAERS) [3],established, respectively, for the spontaneous reporting ofvaccine and of drug-associated AEs.To improve representation and organization of adverseevent information, efforts have been undertaken over theyears to develop different vocabulary resources, includingThis is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly credited.He et al. Journal of Biomedical Semantics 2014, 5:29 Page 2 of 13http://www.jbiomedsem.com/content/5/1/29the Medical Dictionary for Regulatory Activities (MedDRA)[4], the Common Terminology Criteria for Adverse Events(CTCAE) [5], and the World Health Organization (WHO)sAdverse Reaction Terminology (WHO-ART) [6]. MedDRAis an adverse event coding vocabulary preferred by the FDAand utilized by VAERS and FAERS, as well as many clinicaltrials. CTCAE, a product of the USA National Cancer Insti-tute (NCI), is a standardised vocabulary used in assessingAEs associated with drugs for cancer therapy. WHO-ARTis a dictionary maintained by the WHO to serve as a basisfor rational coding of adverse reaction terms.While these resources have played a central role instandardizing and improving AE vocabulary use world-wide, their lack of text definitions and logical classificationhierarchies poses problems for automatic search and re-trieval and for computational analysis and aggregation [7].The Ontology for Adverse Events (OAE) is designed toaddress these issues by providing logically well-formeddefinitions and an associated structured classification.These definitions and classification function as theabstraction of the information from highly specificparticular (or instance) adverse events to more generaluniversals (or classes) that show commonalities oftennot obvious from individual data. As first illustrated in [8]and discussed also below, the application of OAE appearsto support reasonable classification and analysis of thevaccine adverse events (VAE) reported in the clinical VAEcase report system. MedDRA and other classical systemsfocus on the representation of the symptoms or diseasesthat are the adverse event outcomes of clinical findings.They, thus, do not take into account other elements (e.g.,patient age) of the process that leads from initial medicalintervention to subsequent outcomes. The OAE is de-signed to serve as a complementary resource that willfill this gap of treatment-clinical observation associationby providing a means of linking the content coded bythese systems to other relevant biological and clinicalinformation.Biomedical ontologies are consensus-based controlledvocabularies of entities and relations modelling a part ofthe biomedical world, which are represented in bothcomputer and human interpretable forms. They thus gofurther in providing support for computational analysis ofdata than the existing vocabulary resources. The AdverseEvent Ontology (AEO) was initially developed by transfer-ring those ontology terms representing vaccine adverseevents from the Vaccine Ontology (VO) [9,10]. The toplevel AEO adverse event representation was also partiallybased on the work conducted in the European ReMINEproject [11]. In our previous AEO paper [12], we definedthe term adverse event as: a pathological bodily processthat is induced by a medical intervention. This definitionin the previous version of the ontology of AEs assumed acausal association between an adverse event and a medicalintervention. A problem with this definition is that it doesnot align with the common usage of the term adverseevent in medical, pharmacological and public healthcontexts, where it is generally impractical to distinguishthe causal adverse consequences from all the bodily pro-cesses that unfold in a patient temporally subsequent toa given medical intervention. The FAERS and VAERSsystems thus state explicitly that they make no assump-tion of a causal relation between an adverse event and amedical intervention. The assumption of causality in ourpreceding ontology would imply too large a gap betweenthe ontology and actual practice. Above all, this assump-tion would make it difficult to use the term adverse eventto represent individual cases, since the existence of acausal relation is in many cases hard to verify. Further-more, due to a name conflict with the Anatomical EntityOntology that has the same abbreviation AEO, ourAdverse Event Ontology (AEO) was renamed the Ontologyof Adverse Events (OAE) in the Fall of 2011. In OAE,adverse event (OAE_0000001) is defined as to assumeno causal association, while those adverse events forwhich there is a causal association with an interventionare defined as a subclass of adverse event and named ascausal adverse event (OAE_0000003). The latter term is tobe used only when there is definitive evidence (includingbiological and statistical evidences) to assert such a causalassociation under specified conditions. We contend thatwith this change the OAE becomes more robust as a repre-sentation of the domain of adverse event reporting.In addition, other updates have been made to the OAEas compared to the original AEO. A large number of newOAE terms derived from a number of use cases have beenadded. Different ways of representing and analyzing thecausal association between AEs and medical interventionshave been classified and represented in OAE, and theontology has also been used in several studies, which willbe introduced in this paper.Breadth and ScopeThe OAE ontology is a community-based biomedicalontology in the domain of adverse events. OAE clearlydifferentiates adverse event and causal adverse event,with the latter a subtype of the former. A major effort inOAE is to represent ontologically various AEs on anatomiclocations and adverse outcomes (including symptoms,signs, and processes). OAE includes many logic definitionsformulated by using terms from existing ontologies (forexample the UBERON anatomy ontology). This strategylinks OAE with established ontologies and supportscomputer-assisted integration and reasoning. Since OAEdefines an adverse event as a process subsequent to amedical intervention, the ontology provides a logical firststep in the representation of this whole process. Such onto-logical definition allows the development and application ofHe et al. Journal of Biomedical Semantics 2014, 5:29 Page 3 of 13http://www.jbiomedsem.com/content/5/1/29new analysis methods to better understanding the mecha-nisms of adverse events associated with or induced bydifferent medical interventions. OAE also provides aframework for recording and analyzing the associationsrecorded on product labels for example between vaccineor drug administration and medically relevant events.The scope of OAE is very specific and should not beconfused with other relevant ontologies. OAE does nottarget adverse event reporting by following the patternof the existing Adverse Event Reporting Ontology(AERO) which focuses on the ontological representationof the vaccine AE data or information using the Brightonvaccine AE definitions [13]. Unlike OAE, AERO does notdefine adverse event as a pathological bodily process. Byfollowing the principle of ontological realism we arguethat the representation of data should be built as far aspossible on the real-world entities to which such data re-late. Finally, OAE is not an ontology of symptoms or signsas the indications of illness or diseases. The appearanceof various symptoms or signs (e.g., fever) is rather anoutcome of an adverse event, thus denoted by the suffixAE (e.g., a fever adverse event or fever AE).Authority and provenance of OAEOAE targets two communities: the adverse event com-munity and the OBO Foundry ontology community. Asconcerns the former, we have focused our ontology de-velopment on two important research communities,targeting vaccine adverse events and drug adverse events,respectively, and our team includes experts in both ofthese areas. For example, Dr. Yongqun He (co-author) is adomain expert in vaccinology, vaccine adverse events, andontology development [8,14]. Dr. Luca Toldo (co-author)is an expert in drug adverse events [15-17]. Expanding theOAE ontological analysis of VAERS data [8], Dr. SiraratSarntivijai is now expanding and applying OAE and ontol-ogy knowledge mapping to represent and analyze drug-associated AEs in her systems pharmacology research.Her OAE research has obtained strong support andcollaboration from clinical experts at the FDA Officeof Clinical Pharmacology. As an ontology in the OBOFoundry ontology library, the development of OAE fol-lows the OBO Foundry principles [18]. Dr. Barry Smith(co-author) is the founder of the Basic Formal Ontol-ogy (BFO) and also the one of the founders of the OBOFoundry. Our core development team has also in-cluded experts in semantics web (Dr. Cui Tao), medicalinformatics (Yu Lin, MD, PhD), software developer(Zuoshuang Xiang), and many students. Our developmen-tal effort has received technical supports from both theadverse event community and the OBO Foundry ontologycommunity, as demonstrated by positive feedbacks wereceived from three recent international adverse eventrelated workshops [19-21].As described above, the OAE was originally derivedfrom the vaccine adverse event branch of the VaccineOntology (VO) [9,10] and from the European ReMINEproject [11]. New OAE adverse event terms have beengenerated on the basis of clinical adverse event reportsin the VAERS [22] and the FDA [3]. We have referencedMedDRA in our OAE development by cross-referencingrelated MedDRA identifiers. The data models of adverseevent analysis provided by the Clinical Data InterchangeConsortium (CDISC) [23] were also referenced. Peer-reviewed journal articles have been used wherever possibleJOURNAL OFBIOMEDICAL SEMANTICSKuhn et al. Journal of Biomedical Semantics 2014, 5:10http://www.jbiomedsem.com/content/5/1/10RESEARCH Open AccessMining images in biomedical publications:Detection and analysis of gel diagramsTobias Kuhn1*, Mate Levente Nagy3, ThaiBinh Luong2 and Michael Krauthammer2,3AbstractAuthors of biomedical publications use gel images to report experimental results such as protein-protein interactionsor protein expressions under different conditions. Gel images offer a concise way to communicate such findings, notall of which need to be explicitly discussed in the article text. This fact together with the abundance of gel images andtheir shared common patterns makes them prime candidates for automated image mining and parsing. We introducean approach for the detection of gel images, and present a workflow to analyze them. We are able to detect gelsegments and panels at high accuracy, and present preliminary results for the identification of gene names in theseimages. While we cannot provide a complete solution at this point, we present evidence that this kind of imagemining is feasible.IntroductionA recent trend in the area of literature mining is theinclusion of images in the form of figures from biomed-ical publications [1-3]. This development benefits fromthe fact that an increasing number of scientific articlesare published as open access publications. This meansthat not just the abstracts but the complete texts includ-ing images are available for data analysis. Among otherthings, this enabled the development of query enginesfor biomedical images like the Yale Image Finder [4] andthe BioText Search Engine [5]. Below, we present ourapproach to detect and access gel diagrams. This is anextended version of a previous workshop paper [6].As a preparatory evaluation to decide which image typeto focus on, we built a corpus of 3 000 figures that allowsus to reliably estimate the numbers and types of images inbiomedical articles. These figures were drawn randomlyfrom the open access subset of PubMed Central and thenmanually annotated. They were split into subfigures whenthe figure consisted of several components. Figure 1 showsthe resulting categories and subcategories. This classifi-cation scheme is based on five basic image categories:Experimental/Microscopy, Graph, Diagram, Clinical andPicture, each divided into multiple subcategories. It shows*Correspondence: kuhntobias@gmail.com1Department of Humanities, Social and Political Sciences, ETH Zurich, Zürich,SwitzerlandFull list of author information is available at the end of the articlethat bar graphs (12.4%), black-on-white gels (12.0%), fluo-rescencemicroscopy images (9.4%), and line graphs (8.1%)are the most frequent subfigure types (all percentages arerelative to the entire set of images).We targeted different kinds of graphs (i.e., diagramswith axes) in previous work [7], and we decided to focusthis work on the second most common type of images:gel diagrams. They are the result of gel electrophore-sis, which is a common method to analyze DNA, RNAand proteins. Southern, Western and Northern blotting[8-10] are among the most common applications of gelelectrophoresis. The resulting experimental artifacts areoften shown in biomedical publications in the form ofgel images as evidence for the discussed findings such asprotein-protein interactions or protein expressions underdifferent conditions. Often, not all details of the resultsshown in these images are explicitly stated in the captionor the article text. For these reasons, it would be of highvalue to be able to reliably mine the relations encoded inthese images.A closer look at gel images reveals that they follow reg-ular patterns to encode their semantic relations. Figure 2shows two typical examples of gel images together witha table representation of the involved relations. The ulti-mate objective of our approach (for which we can onlypresent a partial solution here) is to automatically extractat least some of these relations from the respective images,© 2014 Kuhn et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited.Kuhn et al. Journal of Biomedical Semantics 2014, 5:10 Page 2 of 9http://www.jbiomedsem.com/content/5/1/10Figure 1 Categorization of images from open access articles of PubMed Central.possibly in conjunction with classical text mining tech-niques. The first example shows aWestern blot for detect-ing two proteins (14-3-3? and ?-actin as a control) in fourdifferent cell lines (MDA-MB-231, NHEM, C8161.9, andLOX, the first of which is used as a control). There aretwo rectangular gel segments arranged in a way to form a2× 4 grid for the individual eight measurements combin-ing each protein with each cell line. A gel diagram can beconsidered a kind of matrix with pictures of experimen-tal artifacts as content. The tables to the right illustratethe semantic relations encoded in the gel diagrams. Eachrelation instance consists of a condition, a measurementand a result. The proteins are the entities being measuredunder the conditions of the different cell lines. The resultis a certain degree of expression indicated by the darknessof the spots (or brightness in the case of white-on-blackgels). The second example is a slightly more complex one.Several proteins are tested against each other in a way thatinvolves more than two dimensions. In this case, the useof + and  labels is a frequent technique to denote thedifferent possible combinations of a number of conditions.Apart from that, the principles are the same. In this case,however, the number of relations is much larger. Only thefirst eight of a total of 32 relation instances are shown inthe table to the right. In such cases, the text rarely men-tions all these relations in an explicit way, and the image istherefore the only accessible source.BackgroundIn principle, image mining involves the same processesas classical literature mining [11]: document categoriza-tion, named entity tagging, fact extraction, and collection-wide analysis. However, there are some subtle differences.Document categorization corresponds to image catego-rization, which is different in the sense that it has todeal with features based on the two-dimensional spaceof pixels, but otherwise the same principles of automaticcategorization apply. Named entity tagging is different intwo ways: pinpointing the mention of an entity is moredifficult with images (a large number of pixels versus acouple of characters), and OCR errors have to be consid-ered. Fact extraction in classical literature mining involvesKuhn et al. Journal of Biomedical Semantics 2014, 5:10 Page 3 of 9http://www.jbiomedsem.com/content/5/1/10Figure 2 Two examples of gel images from biomedical publications (PMID 19473536 and 15125785) with tables showing the relationsthat could be extracted from them.the analysis of the syntactic structure of the sentences.In images, in contrast, there are rarely complete sen-tences, but the semantics is rather encoded by graphicalmeans. Thus, instead of parsing sentences, one has to ana-lyze graphical elements and their relation to each other.The last process, collection-wide analysis, is a higher-levelproblem, and therefore no fundamental differences can beexpected. Thus, image mining builds upon the same gen-eral stages as classical text mining, but with some subtleyet important differences.Image mining on biomedical publications is not a newidea. It has been applied for the extraction of subcellu-lar location information [12], the detection of panels offluorescence microscopy images [13], the extraction ofpathway information from diagrams [14], and the detec-tion of axis diagrams [7]. Also, there is a large amountof existing work on how to process gel images [15-19]and databases have been proposed to store the resultsof gel analyses [20]. These techniques, however, take asinput plain gel images, which are not readily accessiblefrom biomedical papers, because they make up just partsof the figures. Furthermore, these tools are designed forresearchers who want to analyze their gel images and notto read gel diagrams that have already been analyzed andannotated by a researcher. Therefore, these approaches donot tackle the problem of recognizing and analyzing thelabels of gel images. Some attempts to classify biomedicalimages include gel figures [21], which is, however, just thefirst step in locating them and analyzing their labels andtheir structure. To our knowledge, nobody has yet tried toperform image mining on gel diagrams.Approach andmethodsFigure 3 shows the procedure of our approach to imagemining from gel diagrams. It consists of seven steps: figureextraction, segmentation, text recognition, gel detection,gel panel detection, named entity recognition and relationextraction.aUsing structured article representations, the first step istrivial. For steps two and three, we rely on existing work.Themain focus of this paper lies on steps four and five: thedetection of gels and gel panels. In the discussion section,we present some preliminary results on step six of recog-nizing named entities, and sketch how step seven couldbe implemented, for which we cannot provide a concretesolution at this point.To practically evaluate our approach, we ran ourpipeline on the entire open access subset of PubMed Cen-tral (though not all figures made it through the wholepipeline due to technical difficulties).Figure extractionA large portion of the articles of the open access subsetof the PubMed Central database are available as struc-tured XML files with additional image files for the figures.We only use these articles so far, which makes the figureextraction task very easy. It would be more difficult,though definitely feasible, to extract the figures from PDFfiles or even bitmaps of scanned articles (see [22] andhttp://pdfjailbreak.com for approaches on extracting thestructure of articles in PDF format).Segmentation and text recognitionFor the next two steps  segment detection and sub-sequent text recognition , we rely on our previouswork [23,24]. This method includes the detection of lay-out elements, edge detection, and text recognition witha novel pivoting approach. For optical character recogni-tion (OCR), the Microsoft Document Imaging package isused, which is available as part of Microsoft Office 2003.Kuhn et al. Journal of Biomedical Semantics 2014, 5:10 Page 4 of 9http://www.jbiomedsem.com/content/5/1/10Figure 3 The procedure of our approach: (1) figure extraction, (2) segmentation, (3) text recognition, (4) gel detection, (5) gel paneldetection, (6) named entity recognition, and (7) relation extraction.Overall, this approach has been shown to perform bet-ter than other existing approaches for the images found inbiomedical publications [23].We do not go into the detailshere, as this paper focuses on the subsequent steps.Due to some limitations of the segmentation algorithmwhen it comes to rectangles with low internal contrast(like gels), we applied a complementary very simple rect-angle detection algorithm.Gel segment detectionBased on the results of the above-mentioned steps, we tryto identify gel segments. Such gel segments typically haverectangular shapes with darker spots on a light gray back-ground, or  less commonly  white spots on a darkbackground. We decided to use machine learning tech-niques to generate classifiers to detect such gel segments.To do so, we defined 39 numerical features for imagesegments: the coordinates of the relative position (withinthe image), the relative and absolute width and height,16 grayscale histogram features, three color features (forred, green and blue), 13 texture features (coarseness, pres-ence of ripples, etc.) based on [25], and the number ofrecognized characters.To train the classifiers, we took a random sample of500 figures, for which we manually annotated the gelsegments. In the same way, we obtained a second sam-ple of another 500 figures for testing the classifiers.b Weused the Weka toolkit and opted for random forest classi-fiers based on 75 random trees. Using different thresholdsto adjust the trade-off between precision and recall, wegenerated a classifier with good precision and anotherone with good recall. Both of them are used in the nextstep. We tried other types of classifiers including naiveBayes, Bayesian networks [26], PART decision lists [27],and convolutional networks [28], but we achieved the bestresults with random forests.Gel panel detectionA gel panel typically consists of several gel segments andcomes with labels describing the involved genes, proteins,and conditions. For our goal, it is not sufficient to justdetect the figures that contain gel panels, but we also haveto extract their positions within the figures and to accesstheir labels. This is not a simple classification task, andtherefore machine learning techniques do not apply thateasily. For that reason, we used a detection procedurebased on hand-coded rules.In a first step, we group gel segments to find con-tiguous gel regions that form the center part of gelpanels. To do so, we start with looking for segmentsthat our high-precision classifier detects as gel segments.Then, we repeatedly look for adjacent gel segments, thistime applying the high-recall classifier, and merge them.Two segments are considered neighbors if they are atmost 50 pixels apartc and do not have any text segmentbetween them. Thus, segments which could be gel seg-ments according to the high-recall classifier make it intoa gel panel only if there is at least one high-precision seg-ment in their group. The goal is to detect panels with highprecision, but also to detect the complete panels and notjust parts of them.We focus here on precision because lowrecall can be leveraged by the large number of available gelimages. Furthermore, as the open access part of PubMedCentral only makes up a small subset of all biomedicalpublications, recall in a more general sense is anywaylimited by the proportion of open access publications.As a next step, we collect the labels in the form of textsegments located around the detected gel regions. For aKuhn et al. Journal of Biomedical Semantics 2014, 5:10 Page 5 of 9http://www.jbiomedsem.com/content/5/1/10text segment to be attributed to a certain gel panel, itsnearest edge must be at most 30 pixels away from the bor-der of the gel region and its farthest edge must not bemore than 150 pixels away. We end up with a representa-tion of a gel panel consisting of two parts: a center regioncontaining a number of gel segments and a set of labelsin the form of text segments located around the centerregion.To evaluate this algorithm, we collected yet anothersample of 500 figures, in which 106 gel panels in 61 differ-ent figures were revealed by manual annotation.d Basedon this sample, we manually checked whether our algo-rithm is able to detect the presence and the (approximate)position of the gel panels.ResultsThe top part of Table 1 shows the result of the geldetection classifier. We generated three different classi-fiers from the training data, one for each of the thresholdvalues 0.15, 0.3 and 0.6. Lower threshold values lead tohigher recall at the cost of precision, and vice versa. Inthe balanced case, we achieved an F-score of 75%. To getclassifiers with precision or recall over 90%, F-score goesdown significantly, but stays in a sensible range. Thesetwo classifiers (thresholds 0.15 and 0.6) are used in thenext step. To interpret these values, one has to considerthat gel segments are greatly outnumbered by non-gelsegments. Concretely, only about 3% are gel segments.More sophisticated accuracy measures for classifier per-formance, such as the area under the ROC curve [29], takethis into account. For the presented classifiers, the areaunder the ROC curve is 98.0% (on a scale from 50% for atrivial, worthless classifier to 100% for a perfect one).The results of the gel panel detection algorithm areshown in the bottom part of Table 1. The precision is 95%at a recall of 37%, leading to an F-score of 53%. The com-paratively low recall is mainly due to the general problemof pipeline-based approaches that the various errors fromthe earlier steps accumulate and are hard to correct at alater stage in the pipeline.Table 2 shows the results of running the pipeline onPubMed Central. We started with about 410 000 articles,the entire open access subset of PubMed Central at thetime we downloaded them (February 2012). We success-fully parsed the XML files of 94% of these articles (forthe remaining articles, the XML file was missing or notwell-formed, or other unexpected errors occurred). Thesuccessful articles contained around 1 100 000 figures, forsome of which our segment detection step encounteredimage formatting errors or other internal errors, or wasjust not able to detect any segments. We ended up withmore than 880 000 figures, in which we detected about86 000 gel panels, i.e. roughly ten out of 100 figures. Foreach of them, we found on average 3.6 labels with recog-nized text. After tokenization, we identified about 76 000gene names in these gel labels, which corresponds to 6.8%of the tokens. Considering all text segments (including butnot restricted to gel labels), only 3.3% of the tokens aredetected as gene names.eDiscussionThe presented results show that we are able to detect gelsegments with high accuracy, which allows us to subse-quently detect whole gel panels at a high precision. Therecall of the panel detection step is relatively low, but withabout 37% still in a reasonable range. Asmentioned above,we can leverage the high number of available figures,which makes precision more important than recall. Run-ning our pipeline on the whole set of open access articlesfrom PubMed Central, we were able to retrieve 85 942potential gel panels (around 95% of which we can expectto be correctly detected).The next step would be to recognize the named entitiesmentioned in the gel labels. To this aim, we did a prelimi-nary study to investigate whether we are able to extract thenames of genes and proteins from gel diagrams. To do so,we tokenized the label texts and looked for entries in theEntrez Gene database to match the tokens. This look-upwas done in a case-sensitive way, because many names ingel labels are acronyms, where the specific capitalizationTable 1 The results of the gel segment detection classifiers (top) and the gel panel detection algorithm (bottom)Method Threshold Precision Recall F-score ROC areaSegmentsRandom forests0.15 0.439 0.909 0.592 ??? 0.9800.30 0.765 0.739 0.7520.60 0.926 0.301 0.455Naive Bayes 0.172 0.739 0.279 0.883Bayesian network 0.394 0.531 0.452 0.914PART decision list 0.631 0.496 0.555 0.777Convolutional networks 0.142 0.949 0.248Panels Hand-coded rules 0.951 0.368 0.530Kuhn et al. Journal of Biomedical Semantics 2014, 5:10 Page 6 of 9http://www.jbiomedsem.com/content/5/1/10Table 2 The results of running the pipeline on the openaccess subset of PubMed CentralTotal articles 410 950Processed articles 386 428Total figures from processed articles 1 110 643Processed figures 884 152Detected gel panels 85 942Detected gel panels per figure 0.097Detected gel labels 309 340Detected gel labels per panel 3.599Detected gene tokens 1 854 609Detected gene tokens in gel labels 75 610Gene token ratio 0.033Gene token ratio in gel labels 0.068pattern can be critical to identify the respective entity.We excluded tokens that have less than three characters,are numbers (Arabic or Latin), or correspond to com-mon short words (retrieved from a list of the 100 mostfrequent words in biomedical articles). In addition, weextended this exclusion list with 22 general words thatare frequently used in the context of gel diagrams, someof which coincide with gene names according to Entrez.fSince gel electrophoresis is a method to analyze genes andproteins, we would expect to find more such mentions ingel labels than in other text segments of a figure. By mea-suring this, we get an idea of whether the approach worksout or not. In addition, we manually checked the gene andprotein names extracted from gel labels after running ourpipeline on 2 000 random figures. In 124 of these figures,at least one gel panel was detected. Table 3 shows theresults of this preliminary evaluation. Almost two-thirdsof the detected gene/protein tokens (65.3%) were correctlyidentified. 9% thereof were correct but could be more spe-cific, e.g. when only actin was recognized for ?-actin(which is not incorrect but of course much harder to mapto a meaningful identifier). The incorrect cases (34.6%)can be split into two classes of roughly the same size: somerecognized tokens were actually not mentioned in thefigure but emerged from OCR errors; other tokens werecorrectly recognized but incorrectly classified as gene orPROCEEDINGS Open AccessStatistical algorithms for ontology-basedannotation of scientific literatureChayan Chakrabarti1*, Thomas B Jones1, George F Luger1, Jiawei F Xu1, Matthew D Turner1,2, Angela R Laird3,Jessica A Turner2,4From Bio-Ontologies Special Interest Group 2013Berlin, Germany. 20 July 2013* Correspondence: cc@cs.unm.edu1Department of Computer Science,University of New Mexico,Albuquerque, New Mexico, USAAbstractBackground: Ontologies encode relationships within a domain in robust datastructures that can be used to annotate data objects, including scientific papers, inways that ease tasks such as search and meta-analysis. However, the annotationprocess requires significant time and effort when performed by humans. Text miningalgorithms can facilitate this process, but they render an analysis mainly based uponkeyword, synonym and semantic matching. They do not leverage informationembedded in an ontologys structure.Methods: We present a probabilistic framework that facilitates the automaticannotation of literature by indirectly modeling the restrictions among the differentclasses in the ontology. Our research focuses on annotating human functionalneuroimaging literature within the Cognitive Paradigm Ontology (CogPO). We use anapproach that combines the stochastic simplicity of naïve Bayes with the formaltransparency of decision trees. Our data structure is easily modifiable to reflectchanging domain knowledge.Results: We compare our results across naïve Bayes, Bayesian Decision Trees, andConstrained Decision Tree classifiers that keep a human expert in the loop, in termsof the quality measure of the F1-mirco score.Conclusions: Unlike traditional text mining algorithms, our framework can modelthe knowledge encoded by the dependencies in an ontology, albeit indirectly. Wesuccessfully exploit the fact that CogPO has explicitly stated restrictions, and implicitdependencies in the form of patterns in the expert curated annotations.BackgroundAdvances in neuroimaging and brain mapping have generated a vast amount of scientificknowledge. This data, gleaned from a large number of experiments and studies, pertainsto the functions of the human brain. Given large bodies of properly annotated researchpapers, it is possible for researchers to use meta-analysis tools to identify and understandconsistent patterns in the literature. Since researchers often use jargon which is specificto a small sub-field to describe their experiments, it is helpful to tag papers with standar-dized descriptions of the experimental conditions of each papers accompanying study.Several repositories have been created with this effort in mind.Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2 JOURNAL OFBIOMEDICAL SEMANTICS© 2014 Chakrabarti et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.BrainMap (http://www.brainmap.org) is one of the largest and most widely used reposi-tories of neuroimaging results. The BrainMap software suite provides computational tool-sets, scientific data sets, and other informatics resources needed to explore the differentcognitive constructs underlying brain function in various disorders, such as the constella-tion of schizophrenia, bipolar disorder, depression, and autism [1]. Large-scale quantitativemeta-analyses demand the ability to easily identify studies using the same (or similarenough) experimental methods and subjects. The BrainMap method for describing experi-ments has evolved into a taxonomy composed chiefly of structured keywords that categor-ize the experimental question addressed, the imaging methods used, the behavioralconditions during which imaging was acquired, and the statistical contrasts performed.The Cognitive Paradigm Ontology (CogPO), compliant with the Basic Formal Ontology(BFO) [2], builds upon the BrainMap repository on the understanding that while theexperimental psychology and cognitive neuroscience literature may refer to certain beha-vioral tasks by name (e.g., the Stroop task or the Sternberg paradigm) or by function (aworking memory task, a visual attention task), the presentation of these paradigms in theliterature can vary tremendously and are most precisely characterized by the unique com-bination of the stimuli that are presented to the subject, the response expected from thesubject, and the instructions given to the subject. The prevalent use of different terminolo-gies for the same paradigm across different sub-specialities can hinder assimilation ofcoherent scientific knowledge. Discovering equivalence among these terminologies in astructured coherent fashion will facilitate richer information retrieval operations. TheBrainMap repository structure forms the backbone of the Cognitive Paradigm Ontology.It includes the keywords from BrainMap, as well as others, and explicitly represents theimplicit definitions and relationships among them [2]. This allows published experimentsimplementing similar behavioral task characteristics to be linked, despite the use of alter-nate vocabularies.Each piece of literature from the BrainMap repository is annotated according to theCogPO definitions. The process of annotation is traditionally undertaken by a humansubject matter expert, who decides the suitable annotation terms from the CogPOschema after reading the paper, while extracting descriptions of first PET and thenfMRI experiments, and storing each papers results in a standardized system for ease ofretrieval [2,3]. Unfortunately, this task is both time and effort intensive. It presents amajor bottleneck and cost to the whole process. As a result, even though the value ofthe BrainMap project has been proven, the number of publications in the literature faroutweighs the number of publications that have been included in the database [3]. Inthis study, we propose solutions for replacing this human only annotation step withautomated suggestions for the experimental paradigm terms.Text miningText mining methods have found application in identifying patterns and trends in richtextual data [4-6]. Text mining algorithms have also been extended to the problem ofmulti-objective multi-label classification where a variety of predictive functions can beconstructed depended on the required objective function including optimizing an F1-score [7] or minimizing the hamming loss [8]. F1 score is the geometric mean of therecall, a measure of the classifiers tendency to return all of the correct labels, andaccuracy, a measure of the tendency of labels returned by the classifier to be correct.Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2Page 2 of 15Hamming loss, on the other hand, gives a count of the number of false positives andfalse negatives a classifier identifies. Both of these distinct measures give an indicationof the classifiers ability to return high quality classifications.The performance of multi-objective multi-label classification can be further optimizedusing regret analysis [9]. The binary relevance method has been used to extend the solu-tion of multi-objective multi-label classification methods to larger datasets [10]. The mainalgorithms for multi-objective multi-label classification are generally classified under theumbrellas of problem transformation, algorithm adaptation, lazy learning, support vectormachine derived, ensemble methods, and label dependence exploitation [11]. SupportVector Machines and Self Organizing Feature Maps have been used to reduce the inher-ently high dimensionality of text mining problems [12] and have shown promising results[13]. Other, perception based techniques, like artificial neural networks and radial basisfunctions are useful in estimating classification functions for classes of problems withnon-linear and irregular decision boundaries [14].Latent Semantic Analysis works on the assumption that words that are close in meaningoccur close to each other in a document [15,16]. Using Singular Value Decomposition, thematrix representing word counts by paragraph from large document clusters are reducedto only preserve the similarity metric among documents. Documents can then be com-pared using projections and other distance metrics. K-means clustering partitions a corpusof documents in to clusters, where each cluster refers to similar documents [17]. There aremany variations on this theme. In fuzzy co-means clustering, each document may belongto more than one cluster defined by a fuzzy function [18,19]. Similarly, a variant of theclassic Expectation-Maximization algorithm assigns probabilistic distribution functionamong the clusters to each document [20].The NCBO Annotator takes free text and uses efficient concept-recognition techni-ques to suggest annotations from the Bio-Portal repository of ontologies [21]. TheNeuroscience Information Framework [22] uses ontological annotations of a broadvariety of neuroscience resources to retrieve information for user queries.However, most text-mining techniques do not leverage the hierarchical structuresencoded implicitly in an ontology. They consider the ontology terms as anchors forclustering or topic modeling techniques, but have no way to use the information thatthe terms may have exploitable relations to each other, either causal or hierarchical.These terms could just be a set of high entropy keywords for the algorithms to beequally effective. We present a framework that makes use of some of the hierarchicalinformation that is available from the ontology itself for the annotation task.Ontology-based annotation of documents has been an important application area fortext mining research [23]. Since the interdisciplinary nature of this text mining appliedto ontologies leads to overlap of terminology for both fields, we clarify the terms weuse here. We use categories to denote specific superclasses in CogPO (e.g., StimulusType), and labels to denote the leaf terms in each class, which are actually applied tothe abstracts (e.g., Flashing Checkerboard, which is a subclass of Stimulus Type).Dependencies refer to the explicit interaction between the ontology and the specific cor-pora, as captured by the expert-assigned annotations. This is an implicit function ofthe interrelationships between classes (categories of labels), leaf terms, the inherent(but not explicitly stated) logical restrictions in CogPO, and the manner in which thoserelationships are reified in a specific corpus by human annotators.Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2Page 3 of 15In previous work using a similar dataset, we evaluated a version of k-nearest-neigh-bor (kNN) for performing automated annotations [24,25]. We found that the perfor-mance was comparable with results on other textual annotation datasets, but fairlypoor for the multi-label aspects of the problem. Text mining algorithms have also beenapplied to the problem of multi-label annotation; the general case in which there aremore than two labels to choose from, and each paper can be best described by morethan one label [8,26].MethodsWe demonstrate techniques for automatic annotation of the neuroimaging literaturedriven by the Cognitive Paradigm Ontology.CorpusOur corpus consists of 247 human subject matter expert annotated abstracts that are partof the BrainMap database. We consider annotations in 5 distinct categories for eachabstract - Stimulus Modality (SM), Stimulus Type (ST), Response Modality (RM), ResponseType (RT) and Instructions (I). Each of these categories is comprised of several labels asdescribed in CogPO (Turner & Laird 2012) as shown in Figure 1. These human subjectmatter expert annotated abstracts serve as the gold standard against which we test ourstochastic approaches. Table 1 shows a component of the schema from CogPO that weconsider along with a subset of the labels. We only work on the abstracts, and not the fullpaper, because we want to interface our tool directly with the eUtils toolkit of PubMedthat can retrieve the text of abstracts in batch [27].Each abstract is annotated by at least one label from each of the SM, ST, RM, RT, orI categories, and possibly multiple labels from each. The average number of labels percategory per abstract ranged from 1.15 to 1.85 depending on the category. The humanFigure 1 CogPO annotations. We consider annotations from 5 distinct categories: Stimulus Modality,Stimulus Type, Response Modality, Response Type, and Instructions. A subset of the labels for eachcategory is shown here.Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2Page 4 of 15curators annotations model implicit dependencies as a result of the CogPO-corpusinteraction. These dependencies will be specific for each different corpus of abstracts.The CogPO ontology explicitly includes restrictions on the labels, e.g., a Tone as aStimulus Type label entails that the Stimulus Modality must include Auditory, or theInstruction label Smile entails Facial as the label for Response Modality. A flat textmining approach would be unable to make these distinctions, i.e., it would not be ableto tell that label a can change the probability of label b, in some other category. Ourapproach indirectly models this by learning patterns from the expert curated corpus.Naïve BayesNaïve Bayes is a probabilistic learning method, based on Bayes rule, which works surpris-ingly well on problems where a strong independence hypothesis assumption is not met.In fact, naïve Bayes also works well for supervised learning when the number of instancesin the training set is relatively small, which is our situation [25]. It has been extended tothe multi-label scenario using various transformation techniques [9]; we have also found ina comparison of text mining methods applied to this corpus that a naïve Bayes approachworks better than several others [25]. Therefore, we start with a naïve Bayes approach.The naïve Bayes technique across all categories and possible labels does not leveragethe dependencies between labels in different categories, which are implicitly encodedin the domain ontology. Traditional text-mining techniques consider the labels to beanchors for clustering or topic modeling techniques, but have no way to use the factthat the anchors may have implicit dependencies to each other and to object features.The features used to derive terms in traditional text mining are often a set of highentropy keywords [5]. Our framework does not explicitly model the interrelationshipsand restrictions in CogPO, but we exploit the fact that these relations and restrictionsdo exist and implicitly model the information that is encoded in the ontology. This isan important distinguishing characteristic of our stochastic approach.Table 1 Overview of key terms from the CogPO Ontology (adapted from [1]).Concepts Parent Class DefinitionStimulusRoleBFO: role The role of a stimulus in a behavioral experiment is attributed to theobject(s) that are presented to the subject in a controlled manner inthe context of the experiment.ResponseRoleBFO: role The role of response is attributed to the overt or covert behaviorthat is elicited from the subject in an experimental condition.Stimulus BFO: ObjectAggregate The object or set of objects, internal or external to the subject,which is intended to generate either an overt or covert response inthe subject as part of an experimental condition.Response BFO: ProcessAggregate The overt or covert behavior that is elicited from the subject in anexperimental condition.Instructions IAO:action specification,BFO: generically_independent_continuantInstructions are the information-bearing entity that sets up the rules fordesired behavior from the subjects. An explicit direction that guides thebehavior of the subject during the experimental conditions. Instructionsserve the function that they lay out what the response behaviorsshould be for any set of stimuli in the experiment.StimulusModalityBFO: Quality The quality of the sensory perception of an explicit stimulus.ResponseModalityBFO: FiatObjectPart Class of body parts used to perform the actions which can play therole of an overt responseWe consider only a subset of the Cognitive Paradigm Ontology as defined in [1]. We consider 5 classes, StimulusModality, Stimulus Type, Response Modality, Response Type, and Instructions.Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2Page 5 of 15In many ontologies, there are often different classes from which a label may bedrawn [1]. While naïve Bayes is able to assign certain features in a training sample tolabels in a single category, it is unable to learn about dependencies between labels andtheir associated attributes in different categories. Further, it is not possible for naïveBayes alone to increase or decrease its confidence in one label after it has beeninformed that some other label is a correct or incorrect annotation for the same sam-ple. Our method expands on naive Bayes by restricting training sets at each node inthe tree to only those training objects pertinent to that node. This allows us to takeadvantage of any underlying dependencies in the training set between labels of differ-ent categories, which would otherwise be hidden by building a separate classifier foreach category.Formal framework of naïve BayesThe framework which Naive Bayes requires to operate includes a set of items to beclassified whose classifications have already been obtained through some other process(usually a human annotator). Each item in this study, abstracts, which have beentagged with labels from the CogPo ontology, is then recast as a feature vector. In ourwork, this feature vector is a Boolean vector with one bit for every non-stop word inthe corpus. Each bit in an abstracts associated feature vector is set to true if the wordoccurs in the abstract and false otherwise. Figure 2. shows an overview of the naïveBayes method.More formally, we define the set of abstracts, the feature vector, and the set of fea-ture vectors (representing words from the corpus that are not stop words) as follows.Definition 1. The set of abstracts in the corpus is defined asD ={d |d is an abstract in the corpus}Definition 2. A feature is defined asF =< f |f is a feature representing a non ? stop word >Figure 2 Naïve Bayes. Naïve Bayes determines most probable labels in a category.Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2Page 6 of 15Definition 3. A feature vector is defined asV ={vc|vc =< bc1 . . . bcn >, bcj ={TRUE, fi ? dcFALSE, otherwise}By the previous definitions, the length or size of|vc| =??f ?? and |V| = |D| = number of abstractsDefinition 4. CogPO itself, as used in this study can be defined as the set of cate-gories Stimulus Modality, Stiumulus Type, Response Modality, Response Type, andInstruction.C = {SM, ST,RM,RT, I}Definition 5. Each category can be defined as a set of labels li. So for example,SM = {l1, l2, . . .}with li = Visual, l2 = Auditory, etcThe other 4 categories, ST, RM, RT, and I, can be similarly defined.Now we can explain the mechanism by which naive Bayes classifies each abstract.First, the classifier estimatesP(M(dc, lj)|bci = TRUE)or the probability that abstract c has label j given bit i in its feature vector is TRUE,by examining the gold standard corpus, extracting only those abstracts which have biti set to TRUE, and counting the frequency with which label j occurs in this set. This isdone for each label and for each of the feature bits. The classifier also estimatesP(M(dc, lj)|bci = FALSE))for each label and feature by a similar process.Next the classifier estimatesP(bci = TRUE|M(dc, lj))the probability that bit i in the feature vector of abstract x is true given that abstractx is labeled with label j, by flipping the above process around and examining onlythose abstracts which have label lj and counting the frequency with which bcj is set toTRUE in the annotated corpus. Similarly, the classifier then does this for the caseswhen bci is set to FALSE.Additionally, the classifier estimatesP(bci = TRUE)by simply looking at the frequency with which the ith bit of each abstracts featurevectors is true in the gold standard corpus. Similarly the classifier findsP(bci = FALSE) = 1 ? P(bci = TRUE)Lastly,P(M(dc, lj))Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2Page 7 of 15the probability that abstract c has label j, is estimated by counting the frequency ofthe occurrence of label j in the gold standard corpus.Given these four sets of valuesP(M(dc, lj) | bci) ,P (bci| M (dc, lj)) ,P (bci)andP(M(dc, lj))for each label and each feature bit we can estimateP(M(dc| lj) | Vc)or the probability that an abstract c is labeled with label j given its feature vector.Since, for any random variable A and B,P (A|B) = P (A ? B) /P (B)we know thatP(M(d|lj)|V) = P(M(dc| lj) and Vc)/P(Vc).The naive in naive Bayes comes from assuming that the probability of each bit beingtrue in the feature vector is independent of the state of every other bit in the featurevector. Therefore:P(M(d|lj) |V) = P(M (dc, lj) ? Vc)/P (Vc)? P (M (dc, lj)) ? I = 1 to |F| P(bci|M (dc, lj) /P (bci)Similarly, we calculate the probability for all the other labels in SM as well as ST,RM, RT, and I. We used binary relevance in a single category to solve the multi labelclassification problem. Our method takes the raw probability calculated by the Baye-sian classifier using the above equations for each label and accepts all labels thatreceive a probability greater than an arbitrary pre-defined cutoff a.Bayesian decision treesDecision trees are discrete models that can predict the output labels of samples in adata set, based on several input variables arranged in a tree-like structure with nodesand branches. Nodes in the tree represent a decision variable and the branches corre-spond to the next decision variable to be queried based on the outcome of the pre-vious decision variable. We use the Bayesian classifiers to make decisions about whichlabels to include at each node while traversing down the tree.Definition 6. BC,S is a Bayesian classifier trained on set S ?D over category C.Definition 7. If S is a training set and s ? S then label(s) is the set of correct labelsattached to item s.Definition 8. If t is a node in a tree T such that each node in T contains a label oran empty label, then Lt* is a set that contains the label of node t and all of the labelsof each ancestor of t, with no addition made if the label of a node is empty. In prac-tice, the root is the only node that will have an empty label, since on the root node,the naiveBayes algorithm will consider the entire training set.Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2Page 8 of 15Definition 9. T is a Bayesian Decision Tree if each node t of T consists of a categoryCt which is not the same category as any of the ancestors of t, and which is sharedamong the siblings and cousins of t ; a label lt which comes from the category of theparent of t and which is not shared with any of the siblings of t; and a multi-labelBayesian classifier BCt,St using definition 1. The training set St has the following restric-tion: ?s ? St , Lt* ? label(s). Finally, we require that the label of the root node beempty.Definition 10. If Bt is the Bayesian classifier associated with node t and I is an objectwhich maybe categorized by Bt, then Bt(I) is the list of all labels which Bt returns uponclassifying I.Definition 11. If l is a label and t is a node in a tree then Child(l, t) is the child of t,which contains label l.Building the Bayesian decision treeUsing these definitions, we construct a framework for annotating the neuroimagingabstracts with labels from the CogPO ontology categories of SM, ST, RM, RT, and I.We limit the training set on the naïve Bayes classifiers in the tree in order to leveragethe dependencies that exist between labels in different categories. Thus we change theunderlying probabilities of the Bayesian classifier to better fit any dependenciesbetween labels in different categories. This less is more approach helps the Bayesianclassifier to focus on attributes that are more important to the current node, as seen inFigure 3.Our approach uses conditional learning to boost accuracy and recall in automaticlearning systems. By conditional learning we mean that when the system classifies anabstract, it uses stochastic models (naive Bayes classifiers in this case) that were builtwith training data that is limited to only those training items that have labels that werealready determined to be pertinent to the abstract currently being labeled on a higherlevel of the decision tree Table 2.For example, consider an abstract that is being evaluated by this system and that hasalready been tagged by the system as having a Stimulus Modality of Auditory. Whenthe system reaches the Stimulus Type level of the decision tree, it will reach for a naivebayes classifier that has not been trained on the entire gold-standard data set. Instead itwill reach for a classifier which has been trained only on abstracts that were known toFigure 3 Less is More. The Bayesian Decision Tree limits the number of labels at each node. The pruningis done on the basis of the F1 micro score from the gold standard annotations. Thus the naïve Bayesprocess can be applied to a more concentrated set of abstract-label combinations resulting in moreaccurate annotations.Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2Page 9 of 15have Auditory as a label. This means that the underlying probabilities of various labelsfor Stimulus Type will change, making a label like Chord Sequences, a inherent Stimu-lus Type of Auditory more likely, and making a Stimulus Type of False Fonts, from Sti-mulus Modality Visual, less likely.It is important to note that this is not because the algorithm has been programmedto explicitly avoid the Stimulus Type label False Fonts when it encounters an abstractalready labeled Auditory. Instead this is due to the fact that it is implicitly the case inthe literature and given CogPO that the False Fonts label is mostly not compatiblewith the Auditory label, and human annotators, with their natural understanding ofboth the meaning of the literature and the ontology, capture this fact in their annota-tions. Our process merely retrieves this underlying implicit understanding from theannotations in the literature and then leverages that structure to aid in the annotationprocess.We asses the performance of our approach using the F1-micro score, based on preci-sion and recall [28]. In all our calculations, we set ? = 1F? =(1 + ?2) precision ? recall?2 ? precision + recallWe first construct 5 separate naïve Bayes classifiers for each of the 5 categories asformalized in section 2.2. Each classifier is then trained and tested on the entire corpusof abstracts using 10-fold cross-validation, and their F1-micro scores are calculated.Abstracts in the testing set are annotated with a label if the label had a probabilityscore greater than Fb = 0.1.Next we construct the Bayesian Decision Trees as formalized in the previous section.Given that we have 5 categories, we build all 120 possible BDTs. We annotate the cor-pus of abstracts using the BDTs with the criterion that if the probability of a label isgreater than 0.1 for some abstract, then that abstract is tagged with that label. Next weaggregate the labels across each of the 5 categories and calculate a mean F-score foreach category to determine the quality of the annotations for each instance of thecategory across all trees as seen in Figure 3.Table 2 High level description of the algorithm.Input Un-Labeled Item I Bayesian Decision Tree TOutput Label Vector in Multiple Categories LAlgorithmt = Root(T)SearchList = NULLwhile t ~= NULL doL = L : Bt(I)for l  Bt(I) doSearchList = SearchList : Child(l, t)end fort = SearchList[0]x : SearchList = SearchListend whilereturn LThis recursive program uses the Bayesian Decision Tree defined in Definition 9, along with Bayesian Classifier ofDefinition 10 and the child function of Definition 11 to label an unlabeled item. Unlike a normal naive Bayes classifierthat is trained on the whole training set, this algorithm steps through a decision tree whose every node contains aclassifier that is trained on a narrow subset of the original training set. This subset is limited to only those items whichare annotated with the labels of the ancestors of the current node.Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2Page 10 of 15Our approach can also be extended to the case in which the human subject matterexpert is in the classification loop and has an input to the automated annotation pro-cess, that is, the human subject matter expert is using our algorithm to more efficientlyannotate the set of abstracts. A human subject matter expert can usually determine thelabel for at least one of the categories with a quick glance at an abstract. For exmaple,if the abstract explicitly states that the experiment used a picture of faces as the stimu-lus, or that subjects pushed a button with their foot to respond. To model this, wetrained our BDTs with the condition that the root node has already been decided. Wecall this the Constrained Decision Tree (CDT). As a result we have trees rooted at SM,ST, RM, RT, and I, corresponding to the cases where the human expert assigns thelabel for that category. The rest of the tree is constructed exactly as before except that,when the mean f-score is calculated for each category across all possible CDTs, weremove the instances corresponding to the annotations assigned by the human subjectmatter expert since we do not want them to influence the results returned by ouralgorithm.Results and discussionsFigure 4. shows an overview of the entire process. The first task of the annotation pro-cess is handled by the naïve Bayes algorithm. The output of the naïve Bayes algorithmis then used by the Bayesian decision tree algorithm to calculate the annotation tags.Our results are shown in Figure 5. The error bars presented are twice the standarddeviation with respect to the mean of the F1-micro score for each category. F1-microscores for Stimulus Type (ST) and Instructions (I) are lower than in the other cate-gories because of the large number of labels they incorporate, leading to lower relativesample size for each label. Stimulus Modality (SM), Response Modality (RM), andResponse Type (RT) have fewer labels and thus produce better performance.For Response Modality (RM), Response Type (RT), and Instructions (I), the DecisionTree F1-micro score is slightly lower than that of the naïve Bayes because our samplesize constriction for the training sets at each level of the decision tree decreases preci-sion and recall for labels lower down in the tree, and any increases due to underlyingcorrelations are not sufficient to make up for this decrease. The Constrained DecisionTree always has a higher F1-micro score than the other methods because the guaran-tee of correct labels in the first category of each tree is leveraged through the cascadingcorrelations among labels in different categories further down the tree and the labelsdiscovered in the root nodes category.The combination of the stochastic representational power of the naïve Bayes with theexpressive simplicity of the Bayesian Decision Trees allows our automated classifier toachieve a significant improvement in the annotation of literature as compared to exist-ing string-matching tools like the NCBO Annotator. Not only are we able to annotateacross multiple categories, but our method also captures the implicit structural depen-dencies induced in the set of labels found in the gold standard labelled corpus. Ofcourse, this capture process will vary with the corpus to which it is applied, and a dif-ferent corpus for the same ontology being modeled by the same gold standard willproduce a different reification of the dependencies captured in the form of annotationsacross categories. Thus, instead of explicitly modeling the relationships between super-classes and classes directly from the ontology, we have developed a stochastic modelChakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2Page 11 of 15that can capture the effect of those superclass-class relationship indirectly from thespecific combination of human annotations and the corpus. Thus the same stochasticmeta-algorithm can be applied to solve similar automated annotation problems withdifferent ontologies, as well as a different gold standard for that ontology applied toseveral different corpora.Figure 4 Decision Trees. In this figure we can see an abstract going through a few steps of the annotationprocess for both a regular naive Bayes classifier trained on the gold standard corpus and a Bayesian decisiontree. The abstract classified by the naive Bayes classifier is classified without regard to decisions already madeby the classifier. Therefore, it is classified with the label False Font as its stimulus modality even though itsstimulus type was Auditory. By contrast, the when the Bayesian decision tree needs to identify a Stimulus Typeit uses a classifier trained on a set of abstracts which are all annotated with the label Auditory and thus picksChord Sequence as the abstracts Stimulus Type.Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2Page 12 of 15The constrained human-in-the-loop decision tree architecture further improves uponthe naïve Bayes results. When we fix the first node of the decision tree, there is a sig-nificant improvement in the annotation accuracy. This is a useful tool for aiding ahuman expert in annotation because the expert can usually select one annotation fromseveral categories with a quick skim of an abstract. Our technique can then annotatethe remaining categories with high accuracy. Although this approach does not elimi-nate the human expert from the loop, it complements their decision-making and hasthe potential to reduce the time and effort for the full annotation task.Conclusions and future workWe have demonstrated a stochastic framework for annotating BrainMap literature usingthe Cognitive Paradigm Ontology. Unlike text mining algorithms, our framework canmodel the knowledge encoded by the dependencies in the ontology, albeit indirectly. Wesuccessfully exploit the fact that CogPO has explicitly stated restrictions, and implicitdependencies in the form of patterns in the expert curated annotations. The advantage ofour pragmatic approach is that it is robust to explicit future modifications and additionsthat could be made to the relationships and restrictions in CogPO. Since we do not expli-citly model the relations and restrictions, but capture them implicitly from trainingpatterns, we do not have to make corresponding updates to our algorithm each timeCogPO is updated by humans. We merely need to have a correctly annotated body of work.The constrained decision tree architecture further improves upon the naïve Bayesresults. When we fix the first node of the decision tree, there is a significant improvementin the annotation accuracy. This is a useful tool for aiding a human expert in the annota-tion task.We next plan to apply our techniques to different ontologies with more complexstructures. We believe the modular nature of our framework will scale well to theseFigure 5 Comparison of Methods. F1 micro scores for the annotation returned for the Stimulus Modality,Stimulus Type, Response Modality, Response Type, and Instructions. The error bars are twice the standarddeviation.Chakrabarti et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S2http://www.jbiomedsem.com/content/5/S1/S2Page 13 of 15new ontologies. There is additional progress to be made in algorithmically learninggaps (missing labels) in the ontology. We speculate that our technique can find missingrestrictions and relations not explicitly defined in CogPO.Competing interestsThe authors declare that they have no competing interests.Authors contributionsCC and TBJ designed and implemented the experiments, the algorithms, and the formal framework. GFL and JAT arethe PIs of the project and secured the funding. GFL, as the computer science lead, coordinated the technical aspectsof the research. GFL and JAT supervised the development tasks of the project. MDT performed statistical testing andanalysis. ARL created the gold standard corpus. JFW implemented helper functions and other utilities.AcknowledgementsThis project is made possible by a collaboration agreement allowing comprehensive access to the BrainMap database,a copyrighted electronic compilation owned by the University of Texas. The authors thank Peter T. Fox for helpingwith this access. This research is supported by NIMH / NIH awards R56-MH097870, R01-MH084812, and R01-MH074457and by the Department of Computer Science of the University of New Mexico.DeclarationsThs article is published as part of a supplement Bio-Ontologies 2013. This supplement is supported by NIMH / NIHawards R56-MH097870, R01-MH084812, and R01-MH074457 and by the Department of Computer Science of theUniversity of New Mexico.This article has been published as part of Journal of Biomedical Semantics Volume 5 Supplement 1, 2014: Proceedingsof the Bio-Ontologies Special Interest Group 2013. The full contents of the supplement are available online at http://www.jbiomedsem.com/supplements/5/S1.Authors details1Department of Computer Science, University of New Mexico, Albuquerque, New Mexico, USA. 2Mind ResearchNetwork, Albuquerque, New Mexico, USA. 3Department of Physics, Florida International University, Miami, Florida, USA.4Department of Psychology and the Neuroscience Institute, Georgia State University, Atlanta, Georgia, USA.Published: 3 June 2014JOURNAL OFBIOMEDICAL SEMANTICSKatayama et al. Journal of Biomedical Semantics 2014, 5:5http://www.jbiomedsem.com/content/5/1/5REVIEW Open AccessBioHackathon series in 2011 and 2012:penetration of ontology and linked data in lifescience domainsToshiaki Katayama1*, Mark D Wilkinson2, Kiyoko F Aoki-Kinoshita3, Shuichi Kawashima1, Yasunori Yamamoto1,Atsuko Yamaguchi1, Shinobu Okamoto1, Shin Kawano1, Jin-Dong Kim1, Yue Wang1, Hongyan Wu1,Yoshinobu Kano4, Hiromasa Ono1, Hidemasa Bono1, Simon Kocbek1, Jan Aerts5,6, Yukie Akune3, Erick Antezana7,Kazuharu Arakawa8, Bruno Aranda9, Joachim Baran10, Jerven Bolleman11, Raoul JP Bonnal12, Pier Luigi Buttigieg13,Matthew P Campbell14, Yi-an Chen15, Hirokazu Chiba16, Peter JA Cock17, K Bretonnel Cohen18,Alexandru Constantin19, Geraint Duck19, Michel Dumontier20, Takatomo Fujisawa21, Toyofumi Fujiwara22,Naohisa Goto23, Robert Hoehndorf24, Yoshinobu Igarashi15, Hidetoshi Itaya8, Maori Ito15, Wataru Iwasaki25,Matú Kala26, Takeo Katoda3, Taehong Kim27, Anna Kokubu3, Yusuke Komiyama28, Masaaki Kotera29,Camille Laibe30, Hilmar Lapp31, Thomas Lütteke32, M Scott Marshall33, Takaaki Mori3, Hiroshi Mori34, Mizuki Morita35,Katsuhiko Murakami36, Mitsuteru Nakao37, Hisashi Narimatsu38, Hiroyo Nishide16, Yosuke Nishimura29,Johan Nystrom-Persson15, Soichi Ogishima39, Yasunobu Okamura40, Shujiro Okuda41, Kazuki Oshita8,Nicki H Packer42, Pjotr Prins43, Rene Ranzinger44, Philippe Rocca-Serra45, Susanna Sansone45, Hiromichi Sawaki38,Sung-Ho Shin27, Andrea Splendiani46,47, Francesco Strozzi48, Shu Tadaka40, Philip Toukach49, Ikuo Uchiyama16,Masahito Umezaki50, Rutger Vos51, Patricia L Whetzel52, Issaku Yamada53, Chisato Yamasaki15,36, Riu Yamashita54,William S York44, Christian M Zmasek55, Shoko Kawamoto1 and Toshihisa Takagi56AbstractThe application of semantic technologies to the integration of biological data and the interoperability ofbioinformatics analysis and visualization tools has been the common theme of a series of annual BioHackathonshosted in Japan for the past five years. Here we provide a review of the activities and outcomes from theBioHackathons held in 2011 in Kyoto and 2012 in Toyama. In order to efficiently implement semantic technologiesin the life sciences, participants formed various sub-groups and worked on the following topics: Resource DescriptionFramework (RDF) models for specific domains, text mining of the literature, ontology development, essential metadatafor biological databases, platforms to enable efficient Semantic Web technology development and interoperability, andthe development of applications for Semantic Web data. In this review, we briefly introduce the themes covered bythese sub-groups. The observations made, conclusions drawn, and software development projects that emerged fromthese activities are discussed.Keywords: BioHackathon, Bioinformatics, Semantic Web, Web services, Ontology, Visualization, Knowledgerepresentation, Databases, Semantic interoperability, Data models, Data sharing, Data integration* Correspondence: ktym@dbcls.jp1Database Center for Life Science, Research Organization of Information andSystems, 2-11-16, Yayoi, Bunkyo-ku, Tokyo 113-0032, JapanFull list of author information is available at the end of the article© 2014 Katayama et al.; licensee BioMed Central Ltd. This is an open access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly cited.Katayama et al. Journal of Biomedical Semantics 2014, 5:5 Page 2 of 13http://www.jbiomedsem.com/content/5/1/5IntroductionIn life sciences, the Semantic Web is an enabling technol-ogy which could significantly improve the quality and ef-fectiveness of the integration of heterogeneous biomedicalresources. The first wave of life science Semantic Webpublishing focused on availability - exposing data as RDFwithout significant consideration for the quality of thedata or the adequacy or accuracy of the RDF model used.This allowed a proliferation of proof-of-concept projectsthat highlighted the potential of Semantic technologies.However, now that we are entering a phase of adoption ofSemantic Web technologies in research, quality of datapublication must become a serious consideration. This isa prerequisite for the development of translational re-search and for achieving ambitious goals such as personal-ized medicine.While Semantic technologies, in and of themselves, donot fully solve the interoperability and integration problem,they provide a framework within which interoperability isdramatically facilitated by requiring fewer pre-coordinatedagreements between participants and enabling unantici-pated post hoc integration of their resources. Nevertheless,certain choices must be made, in a harmonized manner, tomaximize interoperability. The yearly BioHackathon series[1-3] of events attempts to provide the environment withinwhich these choices can be explored, evaluated, and thenimplemented on a collaborative and community-guidedbasis. These BioHackathons were hosted by the NationalBioscience Database Center (NBDC) [4] and the DatabaseCenter for Life Science (DBCLS) [5] as a part of the Inte-grated Database Project to integrate life science databasesin Japan. In order to take advantage of the latest technolo-gies for the integration of heterogeneous life science data,researchers and developers from around the world wereinvited to these hackathons.Figure 1 Overview of categories and topics raised during the BioHacksemantic relationships between categories.This paper contains an overview of the activities andoutcomes of two highly interrelated BioHackathon eventswhich took place in 2011 [6] and 2012 [7]. The themes ofthese two events focused on representation, publication,and exploration of bioinformatics data and tools usingstandards and guidelines set out by the Linked Data andSemantic Web initiatives.ReviewSemantic Web technologies are formalized as WorldWide Web consortium (W3C) standards aimed at creat-ing general-purpose, long-lived data representation, ex-change, and integration formats that replace current adhoc solutions. However, because they are general-purposestandards, many issues need to be addressed and agreed-upon by the community in order to apply them success-fully to the integration and interoperability problems ofthe life science domain. Therefore, participants of theBioHackathons fall into sub-groups of interest withinthe life sciences, representing the specific needs andstrengths of their individual communities within thebroader context of life science informatics. Thoughthere were multiple specific activity groups under eachof the following headings, and there was overlap andcross-talk between the activities of each group, we willorganize this review under the five general categories of:RDF data, Ontology, Metadata, Platforms and Applica-tions (Figure 1). Results and issues raised by each groupare briefly summarized in the Table 1. We also note thatmany groups have or will publish their respective out-comes in individual publications.RDF dataIn terms of RDF data generation, data were generatedfor genomic and glycomic databases (domain-specificathons of 2011 and 2012. Lines between the boxes representTable 1 Summary of investigated issues and results covered during BioHackathons 2011 and 2012RDF dataDomain specific modelsGenome and proteome dataIssue: No standard RDF data model and tools existed for major genomic dataResult: Created FALDO, INSDC, GFF, GVF ontologies and developed convertersSoftware: Converters are now packaged in the BioInterchange tool; improved PSICQUIC serviceGlycome dataIssue: Glycome and proteome databases are not effectively linkedResult: Developed a standard RDF representation for carbohydrate structures by BCSDB, GlycomeDB, GLYCOSCIENCES.de,JCGGDB, MonosaccharideDB, RINGS, UniCarbKB and UniProt developersSoftware: RDFized data from these databases, stored them in Virtuoso and tested SPARQL queries among the different dataresourcesText processingText extraction from PDF and metadata retrievalIssue: Text for mining is often buried in the PDF formatted literature and requires preprocessingResult: Incorporated a tool for text extraction combined with a metadata retrieval service for DOIs or PMIDsSoftware: Used PDFX for text extraction; retrieved metadata by the TogoDoc serviceNamed entity recognition and RDF generationIssue: No standard existed for combining the results of various NER toolsResult: Developed a system for combining, viewing, and editing the extracted gene names to provide RDF dataSoftware: Extended SIO ontology for NER and newly developed the BioInterchange tool for RDF generationNatural language query conversion to SPARQLIssue: Automatic conversion of natural language queries to SPARQL queries is necessary to develop a human friendly interfaceResult: Incorporated the SNOMED-CT dataset to answer biomedical questions and improved linguistic analysisSoftware: Improved the in-house LODQA system; used ontologies from BioPortalOntologyIRI mapping and normalizationIssue: IRIs for entities automatically generated by BioPortal do not always match with submitted RDF-based ontologiesResult: Normalized IRIs in the BioPortal SPARQL endpoint as either the provider IRI, the Identifiers.org IRI, or the Bio2RDF IRISoftware: Used services of BioPortal, the MIRIAM registry, Identifires.org and Bio2RDFEnvironmental ontologies for metagenomicsIssue: Semantically controlled description of a samples original environment is needed in the domain of metagenomicsResult: Developed the Metagenome Environment Ontology (MEO) for the MicrobeDB projectJOURNAL OFBIOMEDICAL SEMANTICSVan Slyke et al. Journal of Biomedical Semantics 2014, 5:12http://www.jbiomedsem.com/content/5/1/12RESEARCH Open AccessThe zebrafish anatomy and stage ontologies:representing the anatomy and development ofDanio rerioCeri E Van Slyke1*, Yvonne M Bradford1*, Monte Westerfield1,2 and Melissa A Haendel3AbstractBackground: The Zebrafish Anatomy Ontology (ZFA) is an OBO Foundry ontology that is used in conjunction withthe Zebrafish Stage Ontology (ZFS) to describe the gross and cellular anatomy and development of the zebrafish,Danio rerio, from single cell zygote to adult. The zebrafish model organism database (ZFIN) uses the ZFA and ZFS toannotate phenotype and gene expression data from the primary literature and from contributed data sets.Results: The ZFA models anatomy and development with a subclass hierarchy, a partonomy, and a developmentalhierarchy and with relationships to the ZFS that define the stages during which each anatomical entity exists. TheZFA and ZFS are developed utilizing OBO Foundry principles to ensure orthogonality, accessibility, andinteroperability. The ZFA has 2860 classes representing a diversity of anatomical structures from different anatomicalsystems and from different stages of development.Conclusions: The ZFA describes zebrafish anatomy and development semantically for the purposes of annotatinggene expression and anatomical phenotypes. The ontology and the data have been used by other resources toperform cross-species queries of gene expression and phenotype data, providing insights into genetic relationships,morphological evolution, and models of human disease.BackgroundZebrafish (Danio rerio) share many anatomical and physio-logical characteristics with other vertebrates, includinghumans, and have emerged as a premiere organism tostudy vertebrate development and genetics [1]. Zebrafishare amenable to genetic manipulation, and several tech-niques allow recovery of zebrafish mutations affectingdevelopmental patterning, organogenesis, physiology, be-haviour, and numerous other biological processes [2-4]. Inaddition to genetic screens, zebrafish are amenable to tar-geted gene knock-down utilizing morpholino antisense oli-gonucleotides (MOs) [5], TALENs [6], and CRISPRs [7].Use of transgenic constructs in zebrafish has further expe-dited the study of gene function [8,9]. These variousmethods for altering gene expression and regulation havegenerated a plethora of data that enable modelling of dis-ease states and that provide a greater understanding of gene* Correspondence: van_slyke@zfin.org; ybradford@zfin.orgEqual contributors1ZFIN, 5291 University of Oregon, Eugene, OR 97403-5291, USAFull list of author information is available at the end of the article© 2014 Van Slyke et al.; licensee BioMed CentCommons Attribution License (http://creativecreproduction in any medium, provided the orfunction, development, and evolution. ZFIN, the zebrafishmodel organism database [10] manually curates these dis-parate data obtained from the literature or by direct datasubmission.ZFIN serves as the central repository for zebrafishgenetic, genomic, phenotypic, expression, and develop-mental data and provides a community web based re-source to enable access to this highly integrated data[11,12]. To support annotation of gene expression pat-terns and phenotype information in wild types and fishwith altered gene function, ZFIN has developed theZebrafish Anatomy Ontology (ZFA) [13] and the Zebra-fish Stage Ontology (ZFS) [14]. By using the ZFA andZFS to annotate gene expression and phenotypic data,ZFIN is able to provide efficient querying and analysisacross ZFIN data as well as cross-species inference [15].ZFIN is actively involved in the zebrafish and ontologyresearch communities to improve the ZFA throughaddition of classes, definitions, relations, and commondesign patterns and efforts towards interoperability withother ontologies. We report here on the design of theral Ltd. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly credited.Van Slyke et al. Journal of Biomedical Semantics 2014, 5:12 Page 2 of 11http://www.jbiomedsem.com/content/5/1/12ZFA and ZFS, the current state of the ontologies, andongoing efforts to maintain these ontologies for repre-senting the knowledge of zebrafish and more broadly,vertebrate anatomy.ResultsDesign considerations ZFAThe ZFA ontology includes a representation of the anat-omy of Danio rerio at all stages of life - from a single-cell zygote to an adult. The main features of the ZFA, inaddition to its largely structure-based subclass hierarchy,are its partonomy (using the part_of relation) and de-velopmental hierarchy (using the develops_from rela-tion). Each anatomical class in ZFA is defined using theserelationships to other classes in ZFA as well as to stageclasses in ZFS. The relations used in the ZFA and ZFS on-tologies are listed in Table 1, and include start_stageand end_stage. The start_stage utilized is equiva-lent to Relation Ontology (RO) [16] starts_during andend_stage is equivalent to RO ends_during. In thisway, each anatomical entity can be defined in terms of whatit is a type of, what it is a part of, what it develops from, andduring which stages it exists. Figure 1 shows an example ofhow the ZFA describes the development of the heart and il-lustrates the relationships used to describe the partonomy,developmental series, and relationships between anatomicalentities and stages.The ZFA was developed based on the original zebrafishanatomical dictionary, which was a loosely structured part-onomy. The anatomical dictionary was developed to: 1)computationally disseminate gene expression and pheno-typic data; 2) define the anatomical structures of the zebra-fish to establish an ontological framework that could beused by all zebrafish researchers to describe and contributedata; 3) provide an interoperable anatomical description ofzebrafish to effectively map relationships between analo-gous structures across species [17]. The biologists involvedin conceptualizing the ZFA used the anatomical dictionaryas a framework and structured the ZFA according to theoriginal version of the Common Anatomy Reference Ontol-ogy (CARO) [18] at its upper levels of organization, makingTable 1 Examples of relationships used in the ZFA and ZFSProperty Explanationis_a (subclass in OWL) Subtypes a class by its intrinsic nature.part_of (BFO:0000050) Describes what structure or system the claa part of.develops_from (RO:0002202) Describes a class by its progenitors.start_stage (RO:0002091) Describes a class that is observed to beginparticular stage.end_stage (RO:0002093) Describes a class that is observed to endparticular stage.immediately_preceded_by(RO:0002087)Describes the order in which process clasoccur in time.the ZFA interoperable with other ontologies built usingCARO as a framework. This is in contrast to alternativeapproaches taken by the Mouse Gross Anatomy andDevelopment Ontology (EMAP) [19,20], or the Drosophilagross anatomy (FBbt) [21], where a partonomy is repre-sented for each developmental, or life, stage. In addition tothe standard CARO classes, the ZFA includes an additionalhigh level term embryonic structure (ZFA:0001105)a, toorganize embryonic tissues described by fate mapping orgene expression in the early embryo. This class is especiallyuseful for organizing presumptive anatomical structures orareas described as anlagen, primordia, or undifferentiatedbuds. These developmental classes are difficult to classifystructurally, without use of more complex class expressions,thus it makes more sense for the user to organize theseclasses based on ontogeny. Structural representation ofsuch entities was originally described by the CARO devel-opers [18], but as was noted, requires enhancement basedon ontogeny.ZFA classes have human-readable text definitions thatusually are structured in the genus-differentia format asrecommended in Smith et al., 2007 [22] and codified inthe 2008 OBO Foundry principles [23]. Class definitionsalso include further biological description to aid in theidentification and understanding of zebrafish anatomystructures by the user or annotator. The ZFA does nothave logical (computable) definitions at this time, thoughthese are targeted for future development. In the mean-time, many computable definitions for ZFA classes canbe found in the uberon-zfa file [24].In an effort to include cell terms in the ZFA needed tosupport partonomy-based queries, we incorporated theappropriate leaf nodes of the Cell Ontology (CL) [25].Reusing CL classes instead of making new zebrafish cellclasses allows the ZFA to be more interoperable with theother OBO foundry ontologies [26]. To accommodateproper reasoning using these species-independent classes,the file header includes the line treat-xrefs-as-genus-differ-entia: CL part_of NCBITaxon:7955 that informs users andJOURNAL OFBIOMEDICAL SEMANTICSThacker et al. Journal of Biomedical Semantics 2014, 5:39http://www.jbiomedsem.com/content/5/1/39RESEARCH Open AccessThe Porifera Ontology (PORO): enhancing spongesystematics with an anatomy ontologyRobert W Thacker1, Maria Cristina Díaz2, Adeline Kerner3, Régine Vignes-Lebbe3, Erik Segerdell4,Melissa A Haendel4 and Christopher J Mungall5*AbstractBackground: Porifera (sponges) are ancient basal metazoans that lack organs. They provide insight into keyevolutionary transitions, such as the emergence of multicellularity and the nervous system. In addition, their abilityto synthesize unusual compounds offers potential biotechnical applications. However, much of the knowledge ofthese organisms has not previously been codified in a machine-readable way using modern web standards.Results: The Porifera Ontology is intended as a standardized coding system for sponge anatomical featurescurrently used in systematics. The ontology is available from http://purl.obolibrary.org/obo/poro.owl, or from theproject homepage http://porifera-ontology.googlecode.com/. The version referred to in this manuscript ispermanently available from http://purl.obolibrary.org/obo/poro/releases/2014-03-06/.Conclusions: By standardizing character representations, we hope to facilitate more rapid description andidentification of sponge taxa, to allow integration with other evolutionary database systems, and to perform charactermapping across the major clades of sponges to better understand the evolution of morphological features. Futureapplications of the ontology will focus on creating (1) ontology-based species descriptions; (2) taxonomic keys that usethe nested terms of the ontology to more quickly facilitate species identifications; and (3) methods to map anatomicalcharacters onto molecular phylogenies of sponges. In addition to modern taxa, the ontology is being extended toinclude features of fossil taxa.Keywords: Morphology, Taxonomic identification, Phylogenetics, EvolutionBackgroundPorifera (sponges) are sessile, aquatic, multicellular ani-mals that lack true organs and a nervous system. In-stead, sponges contain loosely aggregated cells that candifferentiate into a variety of cell types and produce di-verse skeletal structures. These skeletal elements can becomprised of proteinaceous spongin, chitin, collagen,calcium carbonate and/or silica, depending on the spe-cies. Traditional sponge systematics defines sponge taxaby recognizing particular sets of morphological featuresdescribed in sources such as Systema Porifera [1]. Al-though these features have been well characterized inthe Thesaurus of Sponge Morphology [2,3], and used inpioneering Artificial Intelligence (AI) classification sys-tems [4-6], the terms that are used to describe sponge* Correspondence: cjmungall@lbl.gov5Genomics Division, Lawrence Berkeley National Laboratory, Berkeley, CA,USAFull list of author information is available at the end of the article© 2014 Thacker et al.; licensee BioMed CentraCommons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.morphology have not previously been organized into theframework of a modern ontology.Sponges are conspicuous components of most benthicmarine ecosystems such as shallow coral reefs, man-groves, mesophotic reefs, and deep water environments[7]. Sponges play critical roles in these ecosystems, con-tributing to global cycling of carbon and nitrogen, stabil-izing (but also eroding) coral reef frameworks, andhosting incredibly diverse communities of macroscopicand microscopic symbionts [8,9]. Furthermore, spongeshave therapeutic potential and other human applicationsdue to their ability (or that of their symbionts) tosynthesize various unusual compounds [10] and there-fore present a wealth of biotechnological applicationopportunities.Sponge life depends on the flow of water through anaquiferous system (Figure 1), with water flowing into thebody through incurrent openings (ostia), through a net-work of canals that are lined by internal epithelium-likel Ltd. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/2.0) which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Figure 1 Marine sponges like Agelas conifera (a, left) contain an aquiferous system that pumps water through the sponge body(b, right; from [2]).Thacker et al. Journal of Biomedical Semantics 2014, 5:39 Page 2 of 8http://www.jbiomedsem.com/content/5/1/39cells (pinacocytes), into chambers lined by collared, flag-ellated cells (choanocytes), and out of the body throughexcurrent openings (oscules). Choanocytes closely re-semble choanoflagellates, a group of unicellular eukary-otes that are among the closest relatives to multicellularanimals [11]. Sponges are of interest to evolutionary bi-ologists studying the origins of multicellularity in ani-mals and the origins of the nervous system [12]. Despitehaving no neurons or synapses, some sponges have anearly complete set of post-synaptic protein homologs[13]. Likewise, sponges possess the elements of the cad-herin and ?-catenin complex that are critical for cellularadhesion in bilaterian tissues [11]. Therefore, a more for-mal representation of poriferan anatomy would enablemore complex queries across a diversity of taxa in searchof protein, network, and biological processes that haveregulated the evolution of multicellularity and the ner-vous system.Ontologies for evolutionBiological and biomedical ontologies are structured vo-cabularies that provide consistent names and textual def-initions for anatomical structures, biochemical entities,processes and functions associated with gene products,and many other kinds of biological features. With thesuccess of the Gene Ontology (GO) [14], ontologies havebecome common in biology [15-17] and more recentlythe systematics and evolutionary phenotype communi-ties have begun to use them for character description[18-20]. Through the use of description logic formalismsunderpinning the Web Ontology Language (OWL) [21](a World Wide Web Consortium standard) they facili-tate semantic reasoning within and across domains thatcan be performed by computers. Many freely available,open-source ontologies have been developed that pro-vide terms suitable for annotating, describing, and inte-grating a wide array of biological data [22].Ontologies are not the same as databases or taxo-nomic keys  however, ontologies can be used as an en-hancement to database systems and keys, both as astandard terminology, allowing different database sys-tems to interoperate, and as a logical extension, allowingdomain knowledge to be encoded in a way that enhancesquery capabilities or data integrity. For example, the GOprovides a stable identifier denoting the biologicalprocess of apoptosis  different databases can use thissame identifier for describing genes involved in apoptosis,allowing integration of data from multiple databases cov-ering genes in a variety of species. Furthermore, the know-ledge that apoptosis is a kind of cell death is encoded inthe ontology, which means queries for genes involved incell death will return genes described as being involvedin apoptosis.One important type of biological ontology is the anat-omy ontology. Anatomy ontologies typically include re-lationships between structures, such as the relationshipof parthood between a hand and a limb. In sum, theserelationships form a graph structure that can be used toenhance database queries or bioinformatic analyses. Forexample, a database query for genes expressed in thelimbs can return genes expressed in different parts ofthe limb (such as the hands) or deeper in the part-hierarchy (e.g., in the distal part of the finger). Anatom-ical ontologies are also used to standardize character-state descriptions in evolutionary databases, such as, forexample with the Phenoscape knowledge base [18].The fundamental unit of these anatomical graph struc-tures are classes (also known as terms). Each class repre-sents a distinct anatomical feature and is typicallyassigned a unique identifier that provides a key withwhich it may be cross-referenced to other ontologiesor databases. The open nature of commonly usedbio-ontologies allows terminology and definitions tobe re-used from other ontologies with which theyThacker et al. Journal of Biomedical Semantics 2014, 5:39 Page 3 of 8http://www.jbiomedsem.com/content/5/1/39overlap, reducing duplication of effort and promotingorthogonality.Some anatomy ontologies cover a specific taxon  forexample, the Drosophila Anatomy Ontology (DAO) [23]Others are applicable to a wide range of taxa  for ex-ample, the Plant Ontology (PO) [24] or the Uberon anat-omy ontology [25], which covers metazoans. Until now,the major focus of anatomy ontologies has been plantsand bilaterians, with no representation of the uniquebiology of sponges  whilst Uberon includes structuresapplicable across animals, the focus of the ontology ischordates, with the intention of federating with othermetazoan ontologies.For a period in the 1990s, sponges were amongst thedomains modeled in pioneering Expert System research[4,6]. In particular, SPONGIA was a rule-based systemfor classifying the species of a sponge given as input aset of character descriptors and measurements [26]. Ex-pert systems have some similarities with ontologies both are concerned with knowledge representation andclassification of concepts and data. In fact, expert sys-tems research has largely fragmented into different datascience domains, including Bayesian networks (for rep-resenting and reasoning with probabilistic knowledge)and description-logic based ontologies (for representingand reasoning with boolean knowledge). One conse-quence of these advances in information science is thatfirst-generation expert systems do not interoperate withmodern information systems. Ontologies provide ameans of encoding domain knowledge in an application-independent way.The present study initiates an ontological approach tothe morphology of Porifera by interpreting and organiz-ing the major anatomical characters developed bysponge taxonomists as summarized by the Thesaurus ofSponge Morphology [2].Results and discussionOntology contentsWe constructed the Porifera ontology (PORO) as a WebOntology Language (OWL) ontology using the The-saurus of Sponge Morphology as a primary source. Theontology primarily focuses on anatomical structures, butincludes other kinds of entities of interest to Poriferanbiologists  for example, traits and chemical entities.Each anatomical entity is represented using an OWLclass which is uniquely identified by a URI (uniform re-source identifier) in the OBO Library PORO identifierspace. In this paper, we provide examples of classesusing short forms of these URIs - for example, PORO_0000017 identifies the class spicule. The full URI of thisclass is http://purl.obolibrary.org/obo/PORO_0000017,which resolves to an OWL document rendered as ahuman-readable web page using the OntoBee system. Inthe current release, the ontology contains 625 classesunique to PORO (i.e., not imported from other ontologies),with 27 classes imported from other ontologies. Of the 625unique classes, 519 have definitions that have been sourcedfrom the Thesaurus of Sponge Morphology [2].Upper level classificationThe ontology follows the Common Anatomy ReferenceOntology (CARO) [27] upper level, making use of stand-ard upper-level terms such as organism substance andmaterial anatomical entity to structure the ontology.Due to the fundamental biology of sponges, manyCARO classes such as organ were not used. In contrastto other anatomy ontologies, many (50%) of the anatom-ical classes in the ontology are subtypes of acellular ana-tomical entity (for example, spicules and fibers). At thistime, only a minimal subset of CARO is being used (9classes). CARO is currently being refactored and ex-tended, and the development of PORO will serve as ause case for this work. For example, CARO may includea generic class for representing anatomical chambers,which may serve as the parent class for choanocytechamber in PORO.Body planA sponge body consists of three distinct functionallayers around an aquiferous system that can consist of acombination of pores, incurrent and excurrent canals,choanocyte chambers, and exhalent atria (Figure 1). Themost interior layer (the choanoderm) contains choano-cytes, which are the collared, flagellated cells that formthe choanocyte chambers (PORO_0000025). The mostexterior layer (the pinacoderm) contains the epithelial-like pinacocytes (PORO_0000023), which are tightlyconnected to each other and line the internal canals andexternal surfaces. Sandwiched between these two layersis the mesohyl (PORO_0000002), an extracellular matrixcomposed primarily of galectin, collagen, fibronectin-likemolecules, dermatopontin, and other polypeptides; themesohyl contains cells (microbial and eukaryotic) andskeletal elements (collagen, spongin, chitin, and/or min-erals) [2,28]. The choanoderm, pinacoderm, and mesohylare represented as separate non-overlapping partitionsof the sponge body through the use of OWL GeneralClass Inclusion axioms (GCIs). The sponge aquiferoussystem can be very simple, as in the small, sac-shaped,asconoid (PORO_0000149) bodies of some Calcarea, orextremely complex, as in the leuconoid (PORO_0000028) structures found in most other sponges [29].Acellular structures comprising the architecture of spongesCharacteristic features of many sponges are spicules(PORO_0000017), which form the skeleton of the organ-ism in most cases. Spicules can be composed of calciumThacker et al. Journal of Biomedical Semantics 2014, 5:39 Page 4 of 8http://www.jbiomedsem.com/content/5/1/39carbonate, silica, or spongin. They may also be classifiedby size (megascleres or microscleres). The primarymeans of classifying them is by their morphological, andin particular, symmetric structure. For example, a triax-one (PORO_0000602) is a spicule with 3 axes and 6 rays.This can be modeled precisely in OWL using a constructcalled a cardinality constraint. In the Manchester Syntaxvariant of OWL, this is written as:triaxone EquivalentTospicule and(has_component exactly 3 ray axis) and(has_component exactly 6 ray)Figure 2 shows a subset of the spicule hierarchy,focused on acanthostyle.Fibers are another architecturally important class, withfiber skeletal arrangements usually being dendritic or re-ticulate. Spicules can be embedded within the fibers orechinate (protrude from) the exterior of the fibers. Theoverall pattern of spicule and fiber distribution within asponge is termed the skeletal arrangement, with manydefined categories (for example, a dendritic or a reticu-late arrangement, but there are several other possiblepatterns). These different skeleton types are representedin the ontology.Cell typesWe decided to keep cell types within PORO, rather thanadd them to the central OBO cell type ontology (CL)[30], as these are relatively few in number and are largelyspecific to sponges. Examples include bacteriocyte(PORO_00001062), actinocyte (PORO_0000107) andchoanocyte (PORO_0000003). The latter is of particularinterest to evolutionary biologists due to their proposedhomology to choanoflagellates. Many sponges lack trueepithelia with basement membranes, so we introduce ast ac sg oxstacFigure 2 Ontology visualization showing a portion of the spicule hiervisualization is drawn using the OBO-Edit Graph View plug-in for Protégé 4an acanthostyle (ac), a strongyle (sg), and an oxea (ox).class epithelioid cell (PORO_0000004) rather than re-using the Cell Ontology class for epithelial cell. How-ever, many of the gene products required for epitheliaare found in Porifera [31], and Homoscleromorpha havebasement membranes and true epithelia [29], so in thesecases use of the CL class may be justified. Figure 3shows some of the cell types in PORO, together with thetissue layers in which they are located.Chemical entities and proteinsWe reuse classes from the Chemical Entities ontologyCHEBI [32] for chemical structures of relevance  forexample, calcium carbonate (CHEBI_3311), biogenicsilica (CHEBI_64389) and aragonite (CHEBI_52239). Insome cases, these are connected from other parts of theontology via a composed from relation. For example,calcareous spicule and calcium carbonate exoskeletonare composed of calcium carbonate. Many sponge tax-onomists use biochemical markers and lipid profiles asdescriptors, so we anticipate extending this part of theontology in the near future.Qualities and traitsWe include qualities and traits used by sponge taxono-mists in the ontology. For example, under relationshipto substrate we have sessile (PORO_0000526), rooted(PORO_000050) and endolithic (PORO_0000284). Infuture versions of the ontology some of these terms willbe contributed back to the PATO phenotype and traitontology [33].Applications and future directionsStudies of poriferan systematics have increased consider-ably in recent times, with the number of researchersstudying sponges doubling in the past two decades [29].The application of molecular approaches to sponge tax-onomy has revolutionized and considerably improvedour understanding of the diversity and complexsg oxarchy (with many terms omitted for space reasons). The graph. Inset are examples of spicules including, from left to right, a style (st),Figure 3 A subset of PORO illustrating the three layers of structures comprising a whole organism and some of the cell types thatcomprise these layers. The mesohyl is a gelatinous layer sandwiched between the external pinacoderm and internal choanocyte-lined surface(choanoderm). We re-use the CARO class portion of tissue for these layers, although some debate exists about whether these features constitutetrue tissues.Thacker et al. Journal of Biomedical Semantics 2014, 5:39 Page 5 of 8http://www.jbiomedsem.com/content/5/1/39evolutionary history of this group [34,35]. However, thepace at which molecular systematics generates informa-tion about the phylogenetic diversity of sponges has notbeen matched by a corresponding acceleration of ourunderstanding of the morphological and functional di-mensions of this biological diversity. The incorporationof an ontological approach to organize and connectstructural, functional, genetic, and gene expression con-cepts will allow us to improve this situation.The phylum Porifera contains over 8,000 accepted spe-cies [1], but at least 6,000 additional species are thoughtto exist based on surveys of museum collections [36].When integrating morphological and molecular datasets,most studies of sponge systematics find high support formorphological classifications at the species level, indicat-ing the ability of morphological characters to distinguishbetween sponge species [29,34,37-39]. However, there isoften a lack of resolution at the genus and family levels,suggesting that morphological characters are oftenhomoplasic [29,37]. This low phylogenetic resolutionwithin orders is not surprising since there are relativelyfew morphological characters available for analyses andsince these characters can be phenotypically plastic. Re-cently, Morrow et al. [40,41] demonstrated that somehomoplasic characters may actually represent distinctmorphological traits that are described by a single term(e.g., acanthostyle, Figure 2).A major question in the development of multi-speciesanatomy ontologies is whether ontological terms shouldbe designed with an assumption of the homology of ana-tomical structures [20]. In constructing PORO, we tookan explicitly pragmatic approach and made noassumptions that these ontological terms refer to evolu-tionarily homologous characters. Although there aremany known instances of homoplasy throughout spongesystematics [37,40,41], our current goal is to reflect ana-tomical terms as they have been used in recent and his-torical literature. For example, the term actine refers tothe ray of a spicule [2]. For sponges bearing calcareousspicules, actines do not contain an axial filament, whilefor sponges bearing siliceous spicules, actines do containan axial filament [42,43]. Although it is clear that actineis referring to a feature of two evolutionarily distincttypes of spicules, the concept of actine, that of a ray [2],provides a practical term when describing this feature.In future studies, we plan to use the PORO ontologicalframework to describe homoplasic characters and hopeto provide a higher degree of resolution when describingparticular anatomical features. Greater precision in nam-ing morphological features might allow sponge biologiststo create less ambiguity in character states, yielding less ho-moplasy. While the question of how much homology tobuild into an ontology is debated [44], it is common formulti-species ontologies to include structures that are not ex-plicitly determined to be homologous, and we have previ-ously used this strategy for other anatomical ontologies [25].A practical concern when identifying sponges is poorspecimen preservation. In some cases, critically import-ant morphological features can be difficult to determine,yielding difficulty in assigning taxonomic names usingexisting, primarily bifurcating, identification keys. By in-tegrating the morphological ontology with taxonomy, wehope to enable the creation of polytomous identificationkeys that can function with incomplete data sets. ByThacker et al. Journal of Biomedical Semantics 2014, 5:39 Page 6 of 8http://www.jbiomedsem.com/content/5/1/39using the Porifera ontology to annotate images of spongemorphology, we will facilitate the proper identificationof anatomical features.In the future, we plan to extend the core ontology toinclude extinct taxa. For example, Archaeocyatha arefossil sponges from the Cambrian era that lack spicules.Their functional biology is only deduced from a theoreticalmodel [45], but they appear to share many characters withmodern Porifera. Kerner et al. [46] standardized descrip-tions of the morphological characters of Archaeocyatha,building a descriptive knowledge base of illustrated andclearly defined terms. We are currently adding these char-acters into the Porifera Ontology and explicitly connectinganatomical terms between fossil and modern taxa. By link-ing the skeletal elements of Archaeocyatha to those ofsponges, we hope to enhance our understanding of thefunctional biology of Archaeocyatha as an analog ofsponges.It is important to note that PORO is not a completedatabase, knowledge base, or application in its ownright  however, PORO can form the terminological anddeductive knowledge backbone of such a system. POROis complementary to classification aids such as SPON-GIA [26], which is a powerful expert system able to inferthe species of a sample based on the answers to ques-tions concerning descriptive characters. Indeed, in con-structing that system, the authors noted:The simple work of character definition in thedomain model turned out to be a non-trivial task. Athesaurus of terms for sponges that was to contain animportant consensus among the European experts insponge systematics was in the pipeline and itspreliminary versions were available to us. However,the current vocabulary of our expert was not alwaysstandardized according to the previous consensus [26].PORO could be used directly as the domain modelused in systems such as SPONGIA and its successors.One intriguing possibility is the encoding of the taxo-nomic classification rules of SPONGIA directly in OWL,allowing the use of modern Description Logic reasoners.Given that OWL is expressive enough to encode classifi-cation rules involving conjunction and disjunction of ei-ther symbolic or quantitative characters, it may seemthat this would be easily achieved. However, one challengeis that the MILORD II framework used by SPONGIAmakes use of many-valued logic, reasoning with uncer-tainty and non-monotonic reasoning, all of which are out-side the scope of OWL. One research possibility would beto combine Bayesian and ontological reasoning, as hasbeen done in disease classification [47].Finally, a major effort is underway by the Next-GenerationPhenomics research team [48] to automate the process ofderiving character matrices from published species de-scriptions. As part of our future work, we will use theontology to annotate text and to standardize the names ofmorphological characters across various research groupsover the past 200 years of sponge taxonomy. In addition,the Phenomics team is seeking to automate character rec-ognition using image processing software. It will be crucialto annotate reference images with terms from the ontol-ogy to calibrate this novel imaging system.ConclusionsThere are a number of ontologies covering taxa such asplants, fungi and bilaterians, but the Porifera ontology isthe first ontology dedicated to a non-bilaterian meta-zoan. Because many terms used in sponge taxonomyand systematics have Porifera-specific meanings, we cre-ated the structure of the Porifera ontology from existingresources, primarily the Thesaurus of Sponge Morphology[2]. We will revise and expand PORO to accommodatenew concepts and relationships as we use the ontologyto build character matrices for modern and fossil taxa.By accelerating our ability to describe and understandsponge morphology, we seek to reconcile differences be-tween morphological and molecular approaches to pori-feran systematics. We hope that this integrative approachto taxonomy and systematics will inspire investigatorsworking with invertebrate and microbial taxa to add valueto their morphological datasets by placing the charactersused to describe additional taxonomic groups into anontological framework.MethodsBottom-up ontology developmentWhen building an anatomy ontology, it is possible totake a top-down approach or a bottom-up approach(or some combination thereof ). With a top-down ap-proach, the creator starts with upper-level categoriesand gradually introduces more specific classes. With abottom-up approach, the creator starts with the terms ofinterest (which are typically more specific) and buildsthem into a hierarchy, gradually working-up to the root.With PORO we took a bottom-up approach. Westarted with the online version of the Thesaurus ofSponge Morphology [2] and used a Perl script to generatea skeleton ontology in OBO format. This was adjustedusing a text editor and OBO-Edit [49], and then trans-lated into OWL and edited using the Protégé 4 ontologyeditor [50] (http://protege.stanford.edu). The translationretained the textual definitions obtained from the the-saurus, as well as annotations on the definitions referen-cing the source material. These are represented in theOWL ontology as axiom annotations.For Protégé editing, we make use of a number of plug-ins, including one for annotating images (https://github.Thacker et al. Journal of Biomedical Semantics 2014, 5:39 Page 7 of 8http://www.jbiomedsem.com/content/5/1/39com/balhoff/image-depictions-view), OBO-Edit style pro-ductivity assistance tools (https://github.com/balhoff/obo-actions) and the OBO-Edit Graph View plugin for Protégé(http://code.google.com/p/obographview/).The editors of the ontology met through meetings or-ganized by the Phenotype Research Coordination Net-work (RCN), where the ontology was reviewed andbiologists were provided with training in ontology build-ing and reasoning.We make use of the HermiT reasoner as part of theontology development process [51]. Due to the use ofOWL features such as cardinality constraints, we cannotuse the faster reasoners that operate over restricted pro-files of OWL, but due to the current relative small size ofthe ontology, reasoning can be performed dynamically.As well as using reasoning within Protégé, we run rea-soner checks as part of an automated build process,using the OBO Ontology Release Tool (http://code.google.com/p/owltools/wiki/OortIntro) executed within a Con-tinuous Integration server [52]. This server also checks forcommon problems that can occur during Protégé editing,such as duplicate labels, equivalent classes, or classes hav-ing multiple text definitions.AvailabilityPORO is always available in OWL from the OBO Librarypermanent URL http://purl.obolibrary.org/obo/poro.owl.The content of the ontology is available under a CC-BYlicense (http://creativecommons.org/licenses/by/2.0). Fur-ther details can be obtained from the project websitehttps://code.google.com/p/porifera-ontology/.Competing interestsThe authors declare that they have no competing interests.Authors contributionsCJM and RWT created the initial version of the ontology from the Thesaurusof Sponge Morphology [2]. All authors contributed to the design and contentof the ontology. All authors contributed to the manuscript. All authors readand approved the manuscript.AcknowledgementsWe acknowledge the support of the Phenotype Ontology ResearchCoordination Network (NSF-DEB-0956049) for supporting ES and organizingthe meetings that brought the developers together. This work was alsosupported by grants from the U.S. National Science Foundation, Division ofEnvironmental Biology [grant numbers 0829986 and 1208310 awarded toRWT]. CJM was supported by the Director, Office of Science, Office of BasicEnergy Sciences, of the U.S. Department of Energy under Contract No.DE-AC02-05CH11231. We thank the three anonymous reviewers for theirdetailed comments on a previous version of this manuscript.Author details1Department of Biology, University of Alabama at Birmingham, Birmingham,USA. 2Museo Margarita, Boca de Rio 6304, Venezuela. 3CR2P, UMR 7207CNRS-MNHN-UPMC, Département Histoire de la Terre, Muséum NationaldHistoire Naturelle, Bâtiment de Géologie, CP48, 57 rue Cuvier, 75005 Paris,France. 4Department of Medical Informatics and Clinical Epidemiology,Oregon Health & Science University, Portland, USA. 5Genomics Division,Lawrence Berkeley National Laboratory, Berkeley, CA, USA.Received: 1 July 2013 Accepted: 22 July 2014JOURNAL OFBIOMEDICAL SEMANTICSDupuch et al. Journal of Biomedical Semantics 2014, 5:18http://www.jbiomedsem.com/content/5/1/18RESEARCH Open AccessExploitation of semantic methods to clusterpharmacovigilance termsMarie Dupuch1,2,3*, Laëtitia Dupuch4, Thierry Hamon5,6 and Natalia Grabar1AbstractPharmacovigilance is the activity related to the collection, analysis and prevention of adverse drug reactions (ADRs)induced by drugs. This activity is usually performed within dedicated databases (national, European, international...), inwhich the ADRs declared for patients are usually coded with a specific controlled terminology MedDRA (MedicalDictionary for Drug Regulatory Activities). Traditionally, the detection of adverse drug reactions is performed with datamining algorithms, while more recently the groupings of close ADR terms are also being exploited. The StandardizedMedDRA Queries (SMQs) have become a standard in pharmacovigilance. They are created manually by internationalboards of experts with the objective to group together the MedDRA terms related to a given safety topic. Within theMedDRA version 13, 84 SMQs exist, although several important safety topics are not yet covered. The objective of ourwork is to propose an automatic method for assisting the creation of SMQs using the clustering of semantically closeMedDRA terms. The experimented method relies on semantic approaches: semantic distance and similarityalgorithms, terminology structuring methods and term clustering. The obtained results indicate that the proposedunsupervised methods appear to be complementary for this task, they can generate subsets of the existing SMQs andmake this process systematic and less time consuming.IntroductionThe development of new drugs has allowed the treat-ment of many diseases that were previously consideredincurable and with potential fatal outcomes for patients.However, this major therapeutic advance is limited by thetoxicity of some drugs that may also be dangerous forpatients. Tominimize the risks associated with drug use, itis necessary to detect as early as possible the adverse drugreactions (ADRs) that may have been unnoticed duringclinical trials. This is the role of regulatory authorities andof pharmacovigilance units within pharmaceutical labo-ratories and hospitals. The main source of knowledge forpharmacovigilance is based on the reporting of the ADRsby health professionals and patients. These case reportsare recorded in pharmacovigilance databases. To facili-tate the analysis of those data, ADRs are coded using acontrolled vocabulary, usually MedDRA (Medical Dictio-nary for Drug Regulatory Activities) [1]. The detection of*Correspondence: dupuchm@hotmail.fr1CNRS UMR 8163 STL; Université Lille 1&3, F-59653 Villeneuve dAscq, France2Centre de Recherche des Cordeliers, Université Pierre et Marie Curie - Paris6,UMR_S 872, Paris F-75006, FranceFull list of author information is available at the end of the articlenew pharmacovigilance alerts, or signal detection, is typi-cally based on a manual review of case reports by experts,and more recently in some countries by data mining tech-niques [2,3]. MedDRA is a fine-grained vocabulary withover 80,000 terms and it has been shown that the groupingof similar MedDRA terms (i.e., Hepatitis infectious, Hep-atitis infectious mononucleosis, Hepatitis viral) is oftennecessary in the process of the signal detection [4,5]. Itmay allow indeed to detect the toxicity of a drug morequickly.TheMedDRA terms are structured into five hierarchicallevels (Table 1). From the highest to the lowest, these lev-els are: System organ class (SOC), High level group term(HLGT), High level term (HLT), Preferred term (PT), andLow level term (LLT). The hierarchical organization ofthe MedDRA terminology is clearly oriented on the divi-sion by organ system, i.e. among the SOCs we can find forinstance Musculoskeletal and connective tissue disorders,Hepatobiliary disorders, Psychiatric disorders andCardiacdisorders. In Table 1, we indicate also examples of termsbelonging to these five hierarchical levels. In the majorityof cases, hierarchical levels have the subsumption is-arelations between them. For instance, in Table 1, the PT© 2014 Dupuch et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited.Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 2 of 14http://www.jbiomedsem.com/content/5/1/18Table 1 Structure of MedDRA: five hierarchical levels of MedDRA, number of terms per level and some examples ofthe termsLevel Expanded form Nb terms ExamplesSOC System Organ Class 26 Musculoskeletal and connective tissue disordersHLGT High Level Group Terms 332 Joint disordersHLT High Level Terms 1,688 Arthropathies NECPT Prefered Terms 18,209 ArthritisLLT Lowest Level Terms 66,587 Arthritis, Arthritis aggravated, Atrophic arthritisJoint inflammation, Finger arthritisTotal 86,842Arthritis is-a HLT Arthropathies NEC, while the HLTArthropathies NEC is-aHLGT Joint disorders. The situ-ation is different when we consider the relations betweenPTs and their LLTs [6]: these are no more subsumptionrelations but identical or subsumption relations instead,as the LLTs may be synonym or subordinate to theirPTs. Thus, in Table 1, the LLT Arthritis is identical to itsPT term Arthritis, although other LLTs such as Arthritisaggravated, Atrophic arthritis, Joint inflammation, Fingerarthritis, Knee arthritis are subordinated to this PT.A first method to group the MedDRA terms is basedon the hierarchical levels in MedDRA: HLT (High LevelTerms), HLGT (High Level Group Terms) or SOC (SystemOrgan Class) [7,8]. However, it was observed that somesafety topics are orthogonal to these hierarchical levels(their terms may belong to different SOCs), which led tothe development of the Standardized MedDRA Queries(SMQs) containing the MedDRA terms in connectionwith a safety topic [9] and independently from their SOCs.For example, the Haemorrhage SMQ contains MedDRAterms related to bleeding in all parts of the body: it groupsterms from a large set of SOCs. The SMQs are devel-oped internationally by experts looking manually in allthe MedDRA terms relevant to each SMQ. There are cur-rently 84 SMQs that do not cover the entire drug-inducedset of safety topics (Haemorrhage, Hepatic disorders, Sys-temic lupus erythematosus, Convulsions...). This situationleads us to propose methods for automating the cluster-ing of the terms when MedDRA provides no groupingcategory appropriate for a given safety topic. The lists ofMedDRA terms may then be presented for the selectionto the experts.Other work on the automatic clustering of pharma-covigilance terms relies on a specific resource ontoEIM.ontoEIM stands for ontology and Événements IatrogènesMédicamenteux (Adverse Drug Effects in French) [10].This resource is created through the projection of Med-DRA on the terminology SNOMED CT (SystematizedNomenclature of Medicine - Clinical Terms) [11]. Theprojection is performed on the basis of the UMLS (Uni-fied Medical Language System) [12], in which severalterminologies are already merged and aligned, includ-ing MedDRA and SNOMED CT. The ontoEIM resourcehas been exploited to build groupings through the hier-archical subsumption [10,13]. Precision observed is highwhile the recall is extremely low, which is due to the factthat the SMQs contain terms from different SOCs. Inother experiments, the ontoEIM resource has been usedin combination with the semantic distance algorithms andapplied to a subset of the MedDRA terms [14]. The sameapproach has been applied to a subset of WHO-ART(WHO Adverse Reaction Terminology) terms [15]. In theWHO-ART related experiment, the obtained groupingsdemonstrated interesting results because several typesof semantic relations were detected between the terms(synonyms, antonyms, physiological functions or abnor-malities, associated symptoms, abnormal laboratory tests,pathologies and their causes, close anatomical localiza-tion, degrees of severity, and heterogeneous groupings),although these groupings were not compared with theSMQs.ObjectivesWe address the problem of grouping the MedDRA phar-macovigilance terms in a way that reflects coherent andmedically sound safety topics. Although the MedDRAvocabulary is structured according to specific organ-basedsemantic characteristics of the terms, this organizationdoes not fully capture important semantic relationshipsamong terms.We aim to explore how to group these termsin a way that directly reflects the intuitions captured inmanually created SMQs. More precisely, our objective isto work on semantic methods for the automatic creationof groupings of the MedDRA terms. We propose to adaptand to combine two strategies: semantic distance and sim-ilarity algorithms, and terminology structuring methods.Special attention is paid to the merging and comparisonbetween these two methods and evaluation of the gener-ated term clusters. In order to measure the ability of ourmethods to produce clusters similar to the existing SMQs,we evaluate the generated clusters by taking these existingSMQs as a gold standard. Our method relies on two mainDupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 3 of 14http://www.jbiomedsem.com/content/5/1/18assumptions: (1) the MedDRA terms can be used for theautomatic creation of groupings of terms; (2) the combi-nation of the semantic methods provide complementaryresults.MethodsThe method is organized in four main steps (Figure 1): (1)computing the semantic distance and similarity betweenthe MedDRA terms using ontoEIM, (2) computing thesemantic relations from a flat list of the MedDRAterms with the terminology structuring methods, (3)clustering the MedDRA terms, (4) and evaluating theobtained clusters against the SMQs. For the implementa-tion, we exploit Perl and R (http://www.r-project.org) lan-guages, and several Natural Language Processing (NLP)tools.DataWemake use of several types of material.MedDRA termsThe MedDRA PT terms (n = 18,209) are exploited eitheras a flat list of terms, in which case the semantic relationsbetween them are computed with terminology structuringmethods, or through the ontoEIM resource [10], in whichcase the semantic relatedness between them is computedwith semantic distance and similarity algorithms. Wework with the PT terms because they are used for build-ing the SMQs and for coding the pharmacovigilance casereports. The ontoEIM resource attempts to improve theMedDRA structuring in two ways: the structure of Med-DRA terms becomes similar to the structure in SNOMEDCT which makes it more fine-grained (the hierarchy ismodified and enriched, and contains up to 14 hierarchi-cal levels); and the MedDRA terms receive formal def-initions (decomposition into their semantic primitives).Thus, in Table 2, the MedDRA ADR terms Abdominalabscess and Pharyngeal abscess are defined on two axes(Disorders and Body structure). For instance, Pharyngealabscess is semantically decomposed into the Disorder ele-ment Abscess morphology and Body structure elementNeck structure. The names of the formal definition ele-ments correspond to the names of the hierarchies of theSNOMED CT. Within the ontoEIM, we have three hier-archical trees (Figure 2): one for the MedDRA terms andone for each axis of the formal definitions. The ontoEIMresource is used with the semantic similarity and distancealgorithms.Lexical resourcesThree kinds of lexical resources are involved in themethods: (1) synonyms extracted from the UMLS (n =228,542); (2) synonyms acquired from three biomedicalterminologies thanks to their compositionality [16] (n =28,691); (3) synonyms from WordNet [17] (n = 45,782).Lexical resources provide pairs of synonyms such as{accord, concordance}, {pain, ache}, {aceperone, acetabu-tone}, {adenazole tocladesine} or {bleeding, haemorrhage}.ADR terms Body structure DisordersAbscess morphologyAbdominal cavity structureAbdominal abscessPharyngeal abscessNeck structure(Rada et al, 1989) (Zhong et al, 2002)Chodorow, 1998)(Leacock & formal definitions +?Flat list ofthe MedDRAtermsRadiusHACPOS?taggingSyntactic analysisGenia taggerOgmios platformYaTeADetection ofhierarchical relationsLexical inclusionFasterDetection ofsynonymy relationsSynotermFasterLexical resources Strongly connected componentsConnected componentsclustersMerging theSMQsBaselineExpertsontoEIM resourceClustering the MedDRA termsComputing the semantic distance and similarityPre?processing Term structuringEvaluationTerminology structuring methodsClustering within directed graphsSemantic distance and similarity methodsFigure 1 General schema of the method. Figure 1 presents the general schema of the proposed methods. The methods consist into four mainsteps: application of the semantic similarity and distance methods, application of the terminology structuringmethods, clustering of thesemantically similar terms and evaluation of the obtained results.Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 4 of 14http://www.jbiomedsem.com/content/5/1/18Table 2 Example of a formal definition for the MedDRAterms Abdominal abscess and Pharyngeal abscessMedDRA terms Disorders Body structureType of abnormality Anatomical localizationAbdominal abscess Abscess morphology Abdominal cavity structurePharyngeal abscess Abscess morphology Neck structureThe terms are semantically decomposed into their elements from Disorders andBody structure axes.These resources are used with the terminology structur-ing methods.StandardizedMedDRAQueries (SMQs)We use the 84 SMQs (2010 version) as the gold stan-dard for the evaluation of the generated clusters of terms.The SMQs contain MedDRA terms relevant to a givensafety topic. These terms usually belong to different SOCs.For instance, the Angioedema SMQ contains terms fromthe Immune system disorders SOC (Systemic allergic reac-tion, Allergic oedema, Sulfonamide allergy, Type I hyper-sensitivity), Skin and subcutaneous tissue disorders SOC(Angioedema, Cholinergic urticaria, Urticaria idiopathic,Acute angio oedema), Eye disorders SOC (Chemosis, Con-junctival oedema, Edema eyelid), etc. The size of theSMQs goes from 47 terms (Scleral disorders SMQ) upto 8,036 terms (Malignancies SMQ). The SMQs can becomposed of a flat list of terms or can be hierarchicallystructured.Experimental approachComputing the semantic distance and similaritySeveral semantic distance and similarity algorithms canbe applied within structured terminological resources[18-21]. In our work, we also rely on this type of approach.In this case, the algorithms count the number of edges(links) between the two terms in order to compute therelatedness of these terms. The simplest algorithm [18]counts the edges between terms and aims to find theshortest path between them. Thus, on the Figure 2, weshow an excerpt from a terminological graph. When wecompute the shortest path between the nodes Pharyngealabscess and Abdominal abscess, we follow the path withinthe ADR hierarchy and obtain the shortest path equal tofour edges. In addition to the path length, other criteriamay be taken into account: hierarchical depth of terms[22,23], information content [24], the nearest commonparent [25], etc. Besides the computing of the semanticcloseness between two terms or words, these algorithmshave been used in different contexts such as word-sensedisambiguation [22], information retrieval [23,26], geneannotation [27], and terminology enrichment and adap-tation [28,29]. A review of the semantic measure andsimilarity algorithms common within the biomedical areahas appeared [30].In our work, we separately exploit three algorithms tocompute the semantic distance and similarity betweentwo terms t1 and t2: (1) the Rada semantic distance[18] relies on the computing of the shortest path sp; (2)the LCH Leacock and Chodorow semantic similarity [20]relies on the shortest path sp and on the maximal depthfound within the terminology; (3) the Zhong semanticdistance [23] relies on the absolute depth of terms andon their closest common parent. Semantic distance andsimilarity are computed between the MedDRA terms butalso between the elements of their formal definitions (Dand B) to make the semantic representation of the termsmore fine-grained. To illustrate, lets consider Abdominalabscess and Pharyngeal abscess terms from Figure 2. Theweight of edges is set to 1 because all relations are of thesame kind (hierarchical subsumption), and the value ofFigure 2 Computing the shortest paths between two terms. Figure 2 presents the principle for the computing of the shortest paths spbetween two MedDRA terms (Abdominal abscess and Pharyngeal abscess) and between the elements of their formal definitions (axis Disorders andBody structure). Blue nodes are inherited from MedDRA, red nodes from SNOMED CT.Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 5 of 14http://www.jbiomedsem.com/content/5/1/18each shortest path corresponds to the sum of weights ofall its edges. For this pair of terms we obtain the followingshortest paths sp: spADR = 4, spB = 10 and spD = 0. Theunique semantic distance between theMedDRA terms foreach semantic distance measure is computed as follows:?x ? {Rada, LCH ,Zhong},?i?{ADR,D,B}Wi ? sdx(t1i, t2i)?j?{ADR,D,B}Wj,where {ADR,D,B} respectively correspond to the Med-DRA ADR terms, and the axes Disorders D and Bodystructure B; t1 and t2 are two MedDRA ADR terms;Wi isthe coefficient associated with each of the three axes (thevalue is set to 1 for B and ADR and to 2 forD to reflect theimportance of the latter [31]); and sd(t1, t2) is the seman-tic distance between t1 and t2, computed on a given axiswith one of the three semantic distance measures {Rada,LCH, Zhong}. For the example above, the unique semanticdistance is 3.5. According to the tested parameters (threesemantic distance measures and MedDRA terms with orwithout their formal definitions), we build six symmetricmatrices with the MedDRA terms from ontoEIM.Term structuringmethodsThe terminology structuring provides methods for thedetection of semantic relations between terms. Twostrategies may be distinguished: those which rely on theinternal analysis of the terms and those which rely on thecontexts within which the terms occur. Because we areworking on the terms out of their context, we exploit theterminology structuring methods which rely on the inter-nal analysis of the terms. These methods are applied to aflat list of 18,209 MedDRA PTs. They lead to the detec-tion of hierarchical subsumption and synonymy relationsbetween these terms. The terms are pre-processed: thePOS-tagging is done with Genia tagger [32] and the syn-tactic analysis with the YATEA shallow parser [33]. Threemethods are then applied for the acquisition of seman-tic relations: lexical inclusions, morpho-syntactic variantsand compositionality.Lexical inclusion and hierarchy The basic statement onlexical inclusion hypothesis [34] states that when a giventerm is lexically included in another term there is a seman-tic subsumption between them. This hypothesis is wellverified in the biomedical area [35,36].We distinguish three steps within this approach: the terms are syntactically analyzed into head andexpansion components. For instance, on Figure 3, thesyntactic analysis of the term muscle pain results intwo components: head component pain andexpansion component muscle; the syntactic and semantic relation is thenestablished between a given term and its headheadcomponent componentexpansionpain muscleFigure 3 Syntactic analysis of terms for the induction ofhierarchical relations. Figure 3 presents the syntactic analysis of thetermmuscle pain, its decomposition into head and expansioncomponents, which is then used for the induction of hierarchicalrelations between this term and its head component.component. For instance, the term on Figure 3provides the relation between muscle pain (the wholeterm) and pain (the head component of the term).With these specifications, the identified relations arehierarchical: the long term muscle pain is thehierarchical child of the short term pain. Indeed,muscle pain conveys a more specific information; parent and child terms have to be MedDRA terms,otherwise the identified relations are removed.With the applied specifications of this approach, theidentified relations are induced from lexical and syntac-tic information conveyed by the analyzed terms. Besides,these specifications guarantee that the identified relationscorrespond to the hierarchical subsumption. In fact, wedo not allow the induction of other kinds of relations.For instance, if relations between the whole terms andtheir expansion components were allowed, the identi-fied relations would be associative, such as localizationfor example from Figure 3: muscle pain is localized inmuscle.Morpho-syntactic variants Weworkwith Faster [37] forthe identification of morpho-syntactic variants betweenthe PT terms. This tool uses several transformationrules, such as insertion (cardiac disease/cardiac valve dis-ease), morphological derivation (artery restenosis/arterialrestenosis) or permutation (aorta coarctation/coarctationof the aorta). Each transformation rule is associated withhierarchical or synonymy relations: the insertion intro-duces a hierarchical relation (cardiac valve disease ismore specific than cardiac disease), while the permuta-tion introduces a synonymy relation. When several trans-formations are involved, such as in gland abscess andabscess of salivary gland (combination of permutation(synonymy) and insertion (hierarchy) rules), the hierarchi-cal relation prevails.Compositionality and synonymy The synonymy rela-tions are acquired in two ways:Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 6 of 14http://www.jbiomedsem.com/content/5/1/181. The synonymy relation is established between twosimple MedDRA terms if this relation is provided bythe lexical resources.2. The identification of synonym relations betweencomplex terms relies on the semanticcompositionality [38]. Compositionality appears tobe a common characteristics of the biomedical terms[16,39,40].In our work, we consider that two complex terms aresynonyms if one of their components at the samesyntactic position are synonyms and the othercomponents are identical or also synonyms. Forinstance, given the synonymy relation between painand ache provided by the lexical resources, the termsmuscle pain and muscle ache are also identified assynonyms [41] (Figure 4).Three transformation rules are applied: on the headcomponent like in the given example, on theexpansion component, and on head and expansioncomponents.We perform several experiments: each medical syn-onymy resource is used individually and then combinedwithWordNet.Clustering of termsDuring the clustering step, it is important to distinguishbetween disjoint and non disjoint clusters: with disjointclusters a given term may belong to at most one cluster,while with non disjoint clusters there is at least one termthat belongs to more than one cluster. We give advan-tage to the non disjoint clusters because they suit betterthe specificity of our objectives: one MedDRA term maybelong to several SMQs.For clustering the terms on the basis of their semanticdistance and similarity, we use two clustering methods:hierarchical ascendant classification HAC and Radius Rmethod. With HAC, the method first chooses the bestcenters for clusters and then builds the hierarchy of termsby progressively merging the smaller clusters into biggerones to finally obtain one unique cluster. The obtaineddendrogram is then segmented into k disjoint clusters.With the R radius approach, every MedDRA term is con-sidered as a possible center of a cluster and its closestterms are clustered together with it. This method gener-ates non disjoint clusters.For clustering of terms with the computed hierarchicaland synonymy relations, the relations are considered asdirected graphs: the terms are the nodes of the graph whilethe hierarchical relations are the directed edges. We par-tition these directed graphs in a way that each directedsub-graph correspond to a set of vertices such as at leatone vertix can reach the others by a directed path. Hence,the generated components are non disjoint clusters. Toimprove the coverage of these clusters, we add the syn-onyms: if a term has a synonymy relation with the termfrom a cluster then this term is also included in this clus-ter. The initial graph is then augmented with two edgesgoing from and to the synonyms.Finally, we perform two more steps to deduplicate andmerge the clusters: Separately for each method (semantic similarity andterminology structuring), we compute whethersmaller clusters are included into larger clusters andwe merge those clusters which have at least 80%overlap between them. Between the clusters computed by the two methods(semantic similarity and terminology structuring),two clusters provided by these methods and whichhave at least 80% overlap between them are alsomerged together.Evaluation of the generated resultsWe first evaluate the correctness of the generated seman-tic relations, which is done manually by a computerscientist.We then perform quantitative and qualitative evalua-tion of the generated clusters. The quantitative evaluationis performed thanks to their comparison with the SMQs.A cluster is associated to the SMQ with which it hasthe maximal F-measure. For the setting of the thresholdsof the semantic distance and similarity algorithms andtheir evaluation, we perform a ten-fold cross-validation:the data are partitioned into ten subsets, one subset isused for the setting up the methods while the remain-ing nine subsets are used for the evaluation. This processis done ten times with a different training subset eachheadcomponent componentexpansion headcomponent componentexpansionpain muscle ache muscleFigure 4 Syntactic analysis of terms for the induction of synonymy relations. Figure 4 presents the syntactic analysis of the termsmuscle painandmuscle ache, their decomposition into head and expansion components, which is then used for the induction of synonymy relations betweenthese two terms.Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 7 of 14http://www.jbiomedsem.com/content/5/1/18time. Three classical measures are then computed: preci-sion P (percentage of the relevant terms clustered dividedby the total number of the clustered terms), recall R(percentage of the relevant terms clustered divided bythe number of terms in the corresponding SMQ) and F-measure F (the harmonic mean of P and R). The finalevaluation values are computed with the thresholds whichprovide the best results the most frequently during thecross-validation step. We evaluate the clusters from eachmethod separately and after their merging. A qualitativeevaluation is done by a medical expert: we perform afailure analysis of our methods. As for the baseline, wechose the most frequently used approach for the group-ing of theMedDRA terms, which relies on the hierarchicalstructure of MedDRA: the exploitation of the hierarchicalsubsumption of the PTs through the HLT MedDRA level[7,8,42].ResultsThe 7,629 MedDRA terms from ontoEIM have been pro-cessed through the three semantic distance and similarityalgorithms. An excerpt from the generated matrices ispresented in Table 3: for instance, the distance betweenGastric ulcer and Gastrointestinal ulcer is 1, while thedistance between Gastric ulcer and Biopsy tongue is 10,which reflects the semantics of the terms from these twopairs (the first pair of terms is semantically closer than thesecond pair). The flat list of 18,209 MedDRA terms hasbeen processedwith the terminology structuring methodsfor the detection of hierarchical and synonymy relations.The results for the terminology structuring methods arepresented in Table 4. We can observe that the num-ber of the acquired hierarchical relations reaches up to4,000. The number of the acquired synonyms is lower(nearly 2,000), while the impact of the WordNet resourceis very low (37 and 60 relations). The percentage of theMedDRA PT terms involved in the generated hierar-chical relations is 32%. It reaches up to 40% when thesynonymy is also considered. With semantic distance,all the terms from ontoEIM, 51% of the MedDRA PTs,are used.Table 5 indicates the number of clusters and theirsize according to the strategies and methods (semanticdistance, terminology structuring and merging of theresults provided by these two methods). This table showsthat semantic distance method provides the majority ofthe clusters, and that number of clusters and their sizeincrease with the merging of the methods (semanticdistance and terminology structuring). With the cross-validation, we tested several parameters and determinedthe best thresholds: with the Radius clustering 4 for Rada,4.10 for LCH and 0.02 for Zhong; with theHAC clustering300 classes. With these thresholds, the number of clustersand their size become larger. We apply these best thresh-olds to generate the final set of clusters to be evaluatedand analyzed by the expert. The impact of the best thresh-olds on the clusters varies across the SMQs, but the globalaverage results are improved. For the terminology struc-turing methods the best results are obtained with lexicalinclusions, morpho-syntactic variants and synonyms.The generated semantic relations and clusters have beenevaluated via a comparison with the existing SMQs, withthe baseline, and through an analysis provided by a medi-cal expert and a computer scientist. The key observationsare that the proposed methods outperform the baselineand that the merging of the methods improves the results.We have also observed several limitations of the meth-ods and results. We discuss the performed analysis andevaluations in the following section.DiscussionLimitations of the ontoEIM resourceThe ontoEIM resource is unique of its kind, but currentlyit suffers from incompleteness: only 51% of the Med-DRA PTs are aligned with the SNOMED CT terms. Themain reason for this is that ontoEIM integrates the align-ments between these two terminologies which are alreadyproposed by the UMLS. The integration of additionalalignments [43,44] is planned but requires an importantexpertise of pharmacovigilance experts. Moreover, therecent development of this resource [45] is oriented tothe maintenance of the MedDRA hierarchical structureand on some of the existing SMQs. These two points (useof the MedDRA hierarchical structure and description ofsome existing SMQs) are not suitable for the methods wedesigned.Table 3 Semantic distancematrixGastric ulcer Venooclusive liver Reflux gastritis Biopsy tongue Gastrointestinaldisease ulcerGastric ulcer 0 5 3 10 1Venooclusive liver disease 5 0 7 11 6Reflux gastritis 3 7 0 12 5Biopsy tongue 10 11 12 0 11Gastrointestinal ulcer 1 6 5 11 0Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 8 of 14http://www.jbiomedsem.com/content/5/1/18Table 4 Acquisition of semantic relations (hierarchical subsumption and synonymy) between theMedDRA termsRelationships Methods Number of relationsHierarchical relations Lexical inclusion 3,366Hierarchical relations Morpho-syntactic variation with Faster 743Synonymy relations Compositionality with 3 biomed terminologies 1,879Synonymy relations Compositionality with 3 biomed terminologies and WordNet 1,939Synonymy relations Compositionality with the simple UMLS synonyms 190Synonymy relations Compositionality with the simple UMLS synonyms and WordNet 227Synonymy relations Morpho-syntactic variation with Faster 100Correctness of the semantic relationsA manual analysis of the generated hierarchical relations,done by a computer scientist, indicates that these relationsare usually correct: the syntactic constraints guaranteecorrect propositions. Nevertheless, we observed a smallnumber of syntactic ambiguities. They appear within 144pairs (5%) with maximal syntactic heads and correspondto pairs like: {anticonvulsant drug level, drug level}, {bloodsmear test, smear test}, {eye movement disorder, move-ment disorder}. Thus, within the first pair, there is anambiguity on drug as two dependencies seem possible:{anticonvulsant drug level, drug level} as proposed withthe maximal syntactic head analysis or {anticonvulsantdrug level, level} (analysis provided with the minimal syn-tactic head). In our work, we give preference to the syntac-tic analysis with maximal syntactic heads. But whateverthe performed syntactic analysis, the semantic relationsremain correct. Nevertheless, we will see that, althoughthe generated semantic relations are deemed correct, therelevance of these relations and of the terms they link isnot always perfect to the building of the SMQs. Indeed,some of the terms are seen to be relevant to the SMQswhile others do not, which may be due to the differenceexisting between the linguistically observable semantics ofthe relations and their domain or medical validity.Quantitative evaluation of the generated clusters throughtheir comparison with the SMQs.In Table 6, we indicate the average values of Precision,Recall and F-measure obtained with eachmethod individ-ually (semantic distance and terminology structuring) andTable 5 Clustering of terms: number of clusters andtheir sizeStrategy Clusters of terms#clusters Interval MeanSemantic distance 7,564 [2; 1,354] 132.67Terminology structuring 748 [1; 119] 3.82(hierarchical+synonymy)Merging (semantic distance + 7,684 [1; 1,354] 130.75hierarchical + synonymy)whenmerged. The average precision is usually higher than45%, although the recall is lower especially with the ter-minology structuring methods. This is due to the fact thatthe clusters generated with our methods are smaller thanthe SMQs and show their different aspects. In this table,we can also see that the merging of the methods allows toimprove the average performance of the generated clus-ters (Recall and F-measure), although we lose one percentin Precision.In Figures 5, 6 and 7, we indicate the evaluation resultsobtained against all the 84 SMQs for the three evaluationmeasures: Precision (Figure 5), Recall (Figure 6) and F-measure (Figure 7). Each figure shows the performance ofthe tested methods (terminology structuring and seman-tic distance). The x axis represents the 84 reference SMQs,the y axis the evaluation results. On the whole, we canobserve that precision is usually higher than recall, andthat there is an important variation across the SMQs.In our previous experiments [46], we gave advantage toprecision, while in the current experiment F-measure isadvantaged, which improves the global results by eightpoints. When we look closer at these Figures, we canobserve for example that, in Figure 5, the terminologystructuring method has the highest precision (with over30% of the clusters showing 100% precision), while thesemantic distance method shows the lowest precision.The situation is different with recall in Figure 6: the ter-minology structuring method has the lowest values, whilethe semantic distance method has the highest values. InFigure 7, we can see that the merging of the methods veryoften outperforms the semantic distance methods. Thesefigures also point out that there is a great variability acrossthe SMQs, while currently we use the same setting of themethods independently from the SMQs.Table 6 Evaluation results against the gold standard(84 SMQs): average valuesMethods Precision Recall F-measureSemantic distance 45.2 32.4 36.9Terminology structuring 68.4 12.2 18.7Merging 44.2 36.5 40.0Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 9 of 14http://www.jbiomedsem.com/content/5/1/18 0 20 40 60 80 100 0  10  20  30  40  50  60  70  80PrecisionSMQssem distancemergingterm structuringFigure 5 Evaluation of the clusters generated with the proposedmethods (84 SMQs): Precision. Figure 5 presents the evaluationPrecision values obtained further to the evaluation of the clustersgenerated with the proposed methods (semantic distance,terminology structuring and merging) against the gold standard data(84 SMQs).We also performed an additional analysis of the clustersgenerated with the terminology structuring which showsthe following contribution of the generated semantic rela-tions:1. the hierarchical relations form the basis of theclusters: they correspond to 96% of the involvedterms and show 69% precision. Only three clustersdo not contain hierarchical relations;2. the Faster relations are involved in 50% of clustersand show a precision between 75 and 85%; 0 20 40 60 80 100 0  10  20  30  40  50  60  70  80RecallSMQssem distanceterm structuringmergingFigure 6 Evaluation of the clusters generated with the proposedmethods (84 SMQs): Recall. Figure 6 presents the evaluation Recallvalues obtained further to the evaluation of the clusters generatedwith the proposed methods (semantic distance, terminologystructuring and merging) against the gold standard data (84 SMQs). 0 20 40 60 80 100 0  10  20  30  40  50  60  70  80F?measureSMQssem distanceterm structuringmergingFigure 7 Evaluation of the clusters generated with the proposedmethods (84 SMQs): F-measure. Figure 7 presents the evaluationF-measure values obtained further to the evaluation of the clustersgenerated with the proposed methods (semantic distance,terminology structuring and merging) against the gold standard data(84 SMQs).3. one third of the clusters contains synonymy relations,and their precision varies between 55 and 69%;4. the relations acquired with the UMLS resources areinvolved in 14% of clusters while their precision isonly 38%;5. the WordNet-based relations involve only six terms(such as those involved in the relations {heartsyndrome, nerve degeneration} and {heart injury,nerve damage}). The whole impact of the WordNetsynonyms is almost null. Moreover, the involvedterms are either proposed by other morecontributory methods or do not correspond tocorrect propositions. The most interesting (andcorrect) relation is {intestinal gangrene,gastrointestinal necrosis}. It is unique to theWordNet resource output.On the whole, observations of the impact of the meth-ods and resources on their contribution correspond to theexpected results but provide also with surprises. Thus, thehighest precision is observed with the morpho-syntacticFaster relations: these are based upon the morphologicalvariations of the terms and usually conveyminor semanticmodifications ({abdomen, abdominal}, {infect, infection}).The synonymy relations may involve greater semanticvariations (such as in {sepsis, infection} or {abdominal,intestinal}) and this explains their less impressive but stillacceptable precision. Moreover, the synonym terms mayhave a contextual semantic value [47], i.e. be valid insome but not in all the contexts. As a matter of fact, thepairs {sepsis, infection} and {abdominal, intestinal} havebeen acquired from terms considered as synonyms in theexisting terminologies. However, these synonyms are notDupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 10 of 14http://www.jbiomedsem.com/content/5/1/18deemed to be correct for the creation of the SMQs. Finally,the hierarchical relations convey yet greater semantic vari-ation (the hierarchical child terms are semantically morespecific than the parent terms), although their precisionis higher than the precision of the synonym relations.Moreover, the generated hierarchical relations participatevery actively in the creation of the clusters of terms. Aswe previously observed, the generated hierarchical rela-tions bring the majority of terms in the clusters. Thisis a surprising observation: we did not expect to receivesuch a great contribution from the hierarchical relations.Another surprising observation is related to the poor con-tribution of the synonymy relations from WordNet andthose extracted directly from the UMLS: both their cover-age and precision are weak and they are weakly involvedin the creation of the clusters.Comparison with the baselineOur baseline is the most common method utilized for thegrouping of the PT terms within MedDRA: their hier-archical subsumption through the HLT terms. Amongthe 1,688 HLTs and 84 SMQs, 46 of them have eitherdirect (Thrombocytopenias (SMQ) and Thrombocytope-nia (HLT)) or non ambiguous correspondences (Renalfailure and impairment (SMQ) and Acute renal failure(HLT)). We use these 46 SMQs as gold standard with thebaseline hierarchical subsumption and with our methods.These 46 SMQs are a subset of the whole set of the 84SMQs. Similarly to the results presented in the previousparagraph, the average performance on the baseline setis indicated in Table 7, while the detailed performanceper evaluation measure is indicated on Figures 8 (Preci-sion), 9 (Recall) and 10 (F-measure). We can observe thaton average, the baseline approach can be compared withthe terminology structuring method, although the base-line performance is lesser. The comparison with otherexperiments points out that precision is higher with thebaseline (although very close with the one provided bythe semantic distance), while recall and F-measure arenotably improved with other methods. The figures alsoshow that the proposedmethods outperform the baseline.These are positive observations which clearly indicate thatthe proposed methods contribute to the state of the art.Table 7 Evaluation results for the baseline and theproposedmethods (46 SMQs): average valuesMethods Precision Recall F-measureBaseline 60.3 9.2 14.9Semantic distance 46.0 33.9 34.1Terminology structuring 71.1 11.8 18.9Merging 41.0 45.6 37.3Figure 8 Comparison of the generated clusters with the baseline(46 SMQs): Precision. Figure 8 presents the precision valuesobtained further to the comparison of the baseline with the proposedmethods: exploitation of the MedDRA hierarchical structure and ofthe hierarchical subsumption of the PT terms through their HLTterms. We consider the 46 SMQs which have equivalent HLT terms.Qualitative evaluation with an expertIn Table 8, we indicate examples of seven clusters:Angioedema, Embolic and thrombotic events, arterial,Haemodynamic oedema, effusions and fluid overload,Periorbital and eyelid disorders, Peripheral neuropathy,Haemolytic disorders andAgranulocytosis. This table indi-cates the number of terms in the SMQs and in thecorresponding clusters (clu), as well as the number ofcommon terms between them (com) and the performance(Precision P, Recall R and F-measure F) when computedagainst the reference data Reference and also after the 0 20 40 60 80 100 0  5  10  15  20  25  30  35  40  45RecallSMQssem distanceterm structuringmergingbaselineFigure 9 Comparison of the generated clusters with the baseline(46 SMQs): Recall. Figure 9 presents the recall values obtainedfurther to the comparison of the baseline with the proposedmethods: exploitation of the MedDRA hierarchical structure and ofthe hierarchical subsumption of the PT terms through their HLTterms. We consider the 46 SMQs which have equivalent HLT terms.Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 11 of 14http://www.jbiomedsem.com/content/5/1/18 0 20 40 60 80 100 0  5  10  15  20  25  30  35  40  45F?measureSMQssem distanceterm structuringmergingbaselineFigure 10 Comparison of the generated clusters with thebaseline (46 SMQs): F-measure. Figure 10 presents the F-measurevalues obtained further to the comparison of the baseline with theproposed methods: exploitation of the MedDRA hierarchical structureand of the hierarchical subsumption of the PT terms through their HLTterms. We consider the 46 SMQs which have equivalent HLT terms.manual analysis performed by the expert (Manual). Theresults obtained with different strategies are indicated: thesemantic distance sd, the terminology structuring struc,as well as the merging merg of semantic distance andterminology structuring. We can observe that the perfor-mance of the methods varies a lot across the presentedSMQs and clusters. Usually, the terminology structur-ing provides a higher precision and lower recall thanthe semantic distance measures. The semantic distanceand merging approaches generate bigger clusters: theylead to increased recall but they decrease the precision.Usually, the F-measure takes advantage and is improved.The manual evaluation by the expert accepts additionalterms, which allows to have a more complete picture ofthe performance of the proposed methods. This expertevaluation leads also to increased precision, recall andF-measure.We performed a detailed qualitative analysis of sevenSMQs and clusters with the medical expert.For instance, the SMQ Angioedema contains 52 termswhich mean signs and symptoms of angioedema. Thesemantic distance algorithm provides a cluster with 56terms, among which 36 do not belong to this SMQ. ThreeTable 8 Evaluation results against the gold standard and further to themanual analysis of the expertSMQs Number of terms Reference ManualSMQ clu com P R F P R FAngioedemasd 52 56 20 36 38 37 41 44 42Angioedemastruc 52 31 19 61 36 45 61 36 45Angioedemamerg 52 41 21 51 40 45 71 48 57Embolic and thrombotic events...sd 132 140 48 34 36 35 39 41 40Embolic and thrombotic events...struc 132 13 12 92 9 16 92 9 16Embolic and thrombotic events...merg 132 140 48 34 36 35 47 46 46Haemodynamic oedema, effusions...sd 36 56 13 23 36 28 38 58 50Haemodynamic oedema, effusions...struc 36 31 13 42 36 39 84 72 78Haemodynamic oedema, effusions...merg 36 41 15 37 42 39 81 92 86Periorbital and eyelid disorderssd 39 44 22 50 56 53 52 59 55Periorbital and eyelid disordersstruct 39 4 4 100 10 18 100 10 18Periorbital and eyelid disordersmerg 39 45 22 48 56 52 78 46 58Peripheral neuropathysd 31 58 16 27 51 36 45 84 59Peripheral neuropathystruct 31 2 2 100 6 12 100 6 12Peripheral neuropathymerg 31 58 16 28 52 36 60 80 69Haemolytic disorderssd 26 27 12 44 46 45 66 69 67Haemolytic disordersstruct 26 3 3 100 11 20 100 11 20Haemolytic disordersmerg 26 27 12 44 46 45 78 81 79Agranulocytosissd 29 25 7 28 24 26 32 27 29Agranulocytosisstruc 29 13 9 69 31 42 77 34 47Agranulocytosismerg 29 11 9 81 31 45 77 34 47Dupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 12 of 14http://www.jbiomedsem.com/content/5/1/18of them (Injection site urticaria, Cervix oedema and Injec-tion site swelling) could be included in the SMQ becausethey are caused by drugs and are indeed the symptomsof angioedema. Eight more terms (i.e., Solar urticaria,Urticaria thermal, Urticaria contact) are true false posi-tives because they are not related to angioedema. Finally,other terms, although they mean oedemas, are not causedby drugs. Thus, according to the expert, three of the 36false positives should be considered for the inclusion inthe SMQ. As for the terminology structuring method, itprovides a cluster with 31 terms, among which 12 do notbelong to the SMQ. According to the expert, this eval-uation is correct: the term Injection site oedema has atoo broad meaning (although this SMQ seems to con-tain other broad terms, such as Gingival injury and Skinlesion), while 11 other terms mean oedemas not causedby drugs. With the merging we improve the performance:we obtain two more true positives (Oedema peripheral,Generalized oedema), while the false positives remain thesame. The results are different because the merging is notsupervised (it is based upon the intersection between theclusters): the clusters may be different when consideredseparately for each method and when considered throughtheir merging. As a matter of fact, this is precisely whathappens with the Angioedema SMQ: during the mergingstep, the clusters selected are different from those selectedduring the evaluation of the individual methods, and wegain one new true positive term.For the SMQ Embolic and thrombotic events, our meth-ods provide 92 false positives with the semantic distanceand one with terminology structuring. The analysis ofthese terms is very similar to what we observe for otherSMQs: some of the proposed terms should be consideredfor inclusion in the SMQ (such as Iliac artery stenosis,Hepatic artery stenosis, Vertebral artery stenosis, Cere-bellar ataxia, Penile vascular disorder) because they arevery close to the already included terms, other termshave a too broad meaning to efficiently contribute to theSMQ (Peripheral ischaemia, Chest injury, Ischaemia orInfarction). Finally, some other terms (Mesenteric arterystenosis...) are true false positives. Among the false neg-atives of the Angioedema SMQ, we find terms such asWheezing, Drug hypersensitivity, Swollen tongue, Penileoedema, and among the false negatives of the Embolic andthrombotic events SMQ, we find terms such as Venousocclusion, Splenic infarction, Subclavian artery thrombo-sis. The main reasons of the false negatives are: (1) withthe semantic distance and similarity algorithms, in addi-tion to the fact that only 51% of the MedDRA terms areincluded in the ontoEIM resource, when the terms areincluded they may be too distant in this resource, (2)with the terminology structuring, themethods may be notexhaustive enough to detect all the lexical and syntacticregularities within the terms.Conclusions and perspectivesWe combined several strategies and methods for theclustering of the MedDRA terms with similar or closemeaning. We performed a comparison of the resultsobtained and analyzed their complementarity. A ten-foldcross-validation was carried out in order to test differentparameters and select those which positively influence theresults. Although the automatic creation of the SMQs is adifficult task, our results indicate that the automaticmeth-ods may provide a basis for the creation of the SMQs.The current evaluation has been done against the existingSMQs, but we expect we can apply the same method forthe creation of new SMQs with similar performance. Ourmethods generate clusters which are smaller than the cor-responding SMQs and which show their different aspects.For this reason, the precision of the clusters is often high,while their merging leads to the improvement of theircompleteness.Our future work will address the current limitations ofour methods and results. The material, and more partic-ularly the ontoEIM resource, is being improved thanks toa better alignment with the SNOMED CT [43,44]. More-over, the future studies will lead to the identification ofother parameters which influence the quality of clustersand also of other factors and more robust methods for themerging of clusters [48-50]. Also, we would like to addressthe points related to the complementarity of the clus-ters and their potential hierarchical dependencies. As weobserved, the performance varies according to the SMQsand it appears that different strategies should be used fordifferent SMQs, while currently we apply the same set-tings of the methods to all the SMQs. We plan to performan exhaustive analysis of the nature of semantic relationswhich can be observed within the SMQs, which will allowto propose other methods and to reduce the current falsenegatives within the clusters. An alternative method willconsist into the exploitation of corpora for the detectionof other semantic relations among the terms. In addition,we intend to carry out a more detailed evaluation of thegenerated clusters. This addresses particularly the impactof the generated clusters on the exploring of the pharma-covigilance databases (such as the FDA database) and onthe signal detection tasks. The very first results of thistype of evaluation (not presented in the article) are verypromising because they lead to an improvement of thesignal detection by comparison with the results obtainedwith the SMQs.Competing interestsThe authors declare that they have no competing interests.Authors contributionsMD implemented and performed the semantic distance and similaritycomputing, the clustering experiences and the quantitative evaluation; LDhad the responsibility to evaluate the obtained clusters; TH adapted theterminology structuring methods, generated the results with these methodsDupuch et al. Journal of Biomedical Semantics 2014, 5:18 Page 13 of 14http://www.jbiomedsem.com/content/5/1/18and evaluated their correctness; NG led the work, proposed the methodology,participated in the evaluation and designed the article. All the authorsparticipated in the writing of the article. All authors read and approved thefinal manuscript.AcknowledgmentsThe presented research was conducted as part of the PROTECT consortium. Ithas received support from the Innovative Medicine Initiative Joint Undertaking(www.imi.europa.eu). The authors also acknowledge the support of theFrench Agence Nationale de la Recherche (ANR) and the DGA, under theTecSan grant ANR-11-TECS-012. The authors are also thankful to the reviewersfor their patience and remarks which improved the quality of this work.Author details1CNRS UMR 8163 STL; Université Lille 1&3, F-59653 Villeneuve dAscq, France.2Centre de Recherche des Cordeliers, Université Pierre et Marie Curie - Paris6,UMR_S 872, Paris F-75006, France. 3INSERM, U872, Paris F-75006 France.4Université Toulouse III Paul Sabatier, F-31062 Toulouse, France. 5LIMSI-CNRS,BP133 Orsay, France. 6Université Paris 13, Sorbonne Paris Cité, France.Received: 10 April 2012 Accepted: 1 January 2014Published: 16 April 2014JOURNAL OFBIOMEDICAL SEMANTICSEgaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42http://www.jbiomedsem.com/content/5/1/42SOFTWARE Open AccessExecuting SADI services in GalaxyMikel Egaña Aranguren1,2*, Alejandro Rodríguez González1 and Mark D Wilkinson1AbstractBackground: In recent years Galaxy has become a popular workflow management system in bioinformatics, due toits ease of installation, use and extension. The availability of Semantic Web-oriented tools in Galaxy, however, islimited. This is also the case for Semantic Web Services such as those provided by the SADI project, i.e. services thatconsume and produce RDF. Here we present SADI-Galaxy, a tool generator that deploys selected SADI Services astypical Galaxy tools.Results: SADI-Galaxy is a Galaxy tool generator: through SADI-Galaxy, any SADI-compliant service becomes a Galaxytool that can participate in other out-standing features of Galaxy such as data storage, history, workflow creation, andpublication. Galaxy can also be used to execute and combine SADI services as it does with other Galaxy tools. Finally,we have semi-automated the packing and unpacking of data into RDF such that other Galaxy tools can easily becombined with SADI services, plugging the rich SADI Semantic Web Service environment into the popular Galaxyecosystem.Conclusions: SADI-Galaxy bridges the gap between Galaxy, an easy to use but static workflow system with a wideuser-base, and SADI, a sophisticated, semantic, discovery-based framework for Web Services, thus benefiting bothuser communities.Keywords: Galaxy, Web services, SADI, RDF, SPARQL, OWLBackgroundThere is a growing global movement towards representa-tion of bioinformatics data and knowledge using contem-porary syntaxes and semantic languages approved by theWorld Wide Web Consortium (W3C) [1], like ResourceDescription Framework (RDF) [2] and Web OntologyLanguage (OWL) [3]. Major bioinformatics resourcesmaking their data available using these formats includeUniProt [4], EBI [5], and soon, the DNA Databank ofJapan [6]. Beyond these core providers, there are alsolarge integrated warehouses of bioinformatics data inRDF format including, most significantly, Bio2RDF [7],which integrates critical bioinformatics resources such asdbSNP [8], OMIM [9], and KEGG [10], and NCBI eutils*Correspondence: mikel.egana@ehu.es1Biological Informatics, Centre for Plant Biotechnology and Genomics (CBGP),Technical University of Madrid (UPM), Campus of Montegancedo, 28223Pozuelo de Alarcón, Spain2Genomic Resources, Department of Genetics, Physical Anthropology andAnimal Physiology, Faculty of Science and Technology, University of BasqueCountry (UPV/EHU), Sarriena auzoa z/g, 48940 Leioa - Bilbo, Spain[11], which wraps NCBI databases as resolvable RDFresources.This wealth of resources brings the inevitable require-ment for tools that support the flow of native RDF datathrough a formal bioinformatics analysis pipeline. TheSemantic Automated Discovery and Integration (SADI)project has established design-patterns for bioinformaticsresources that wish to natively consume and produce RDFdata [12], and there are SADI plug-ins to several populardata workflow and exploration environments, includingTaverna [13] and the IO Informatics Sentient KnowledgeExplorer [14]. While Taverna is a rich and full-featuredenvironment for constructing and editing complex work-flows, Galaxy [15] is showing itself to be a favorite ofbench-biologists due to its relative simplicity comparedto Taverna, and the familiarity it brings biologists byexposing the tools they commonly use in a manner thatthey can quickly interpret and work with. As such, it wasdesirable to bring support for SADI-based, RDF-nativedata and analysis tools into the Galaxy environment. Herewe describe SADI-Galaxy - a set of tools that retrieve and© 2014 Egaña Aranguren et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,distribution, and reproduction in any medium, provided the original work is properly credited. The Creative Commons PublicDomain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in thisarticle, unless otherwise stated.Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 2 of 11http://www.jbiomedsem.com/content/5/1/42wrap SADI Semantic Web Services in a manner thatallows them to be included in Galaxy workflows.ImplementationGalaxy is a Web server, written in Python, that offers avery usable interface for the typical bioinformatics com-putational analyses (Figure 1). A user can store data,and analyse it using a variety of tools, sending the out-put of one tool as input to the next. This process isstored in a chronological history from which workflowscan be extracted, providing an easy-to-reproduce abstrac-tion of common steps. Data, histories and workflows areWeb-shareable and can be imported and exported. AGalaxy tool is, typically, a wrapper for a terminal exe-cutable program. Since such wrappers are defined byan XML file describing the inputs and outputs of thetool as well as its Web interface [16], creating Galaxytools from pre-existing executables is not technicallydemanding.SADI is a standards-compliant set of lightweight designpatterns for publishing bioinformatics data and analy-sis services on the Semantic Web. It uses Semantic Webtechnologies at every level of the Web services stack.In particular, a SADI service describes its interface inOWL, and then both consumes and produces RDF datathat match that OWL logical description. Finally, SADIrequires that the output data is semantically connectedto the input data by a meaningful relationship. As such,workflows of SADI services output unbroken chains ofRDF Linked Data [17]. SADI services are catalogued in apublicly-accessible database (registry), and queries againstthat database will be used in this study to find, and retrievethe interface definitions for, SADI services of interestto any given Galaxy user; the retrieved service defini-tions will become templates for the Galaxy wrapper,and thus the services can be accessed through thesewrappers.SADI-Galaxy consists of two parts (Figure 2): theCore, and the Tool-generator. SADI-Galaxy Coreincludes three Galaxy tools that are installed in the samemanner as any other Galaxy tool [18], and they can beused on their own. These tools are: SADI generic client. This tool is able toexecute any SADI service, given the services URI andan RDF input file that is compatible with the service.The RDF output of the service is stored as any Galaxyoutput. Automated reasoning is used to check if theRDF input is compliant with the SADI Services inputOWL class; i.e. whether the RDF instance is inferredto be a member of the input OWL Class, effectivelyproviding up-front, low-cost automated workflowvalidation. RDF Syntax Converter. This tool is able toconvert an RDF file into a variety of formats, mostimportantly including Tab Separated Values (TSV)format (three columns for subject, predicate andFigure 1 Galaxy interface. The Galaxy web interface is divided in different views, listed at the top: Analyze data, Workflow, Shared Data,Visualization, Admin, Help, and User. The frontpage is the Analyze data view, shown in this figure, with three columns: available tools (left),current tool (center), and history of loaded data and executed tools (right). In this example the logged in user is working in a history named MikelmPuma testing: different datasets have been loaded (Steps 1, 3, 5) and a tool has been executed (The execution of tool Create ace TOCfrom SAM, using the result of step 5 as input, has resulted in the dataset stored in step 9). The tool ClustalW is selected to be executed next,using the dataset of step 1 as input, and the result will appear in the history as step 10. (Some steps have been deleted from the history).Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 3 of 11http://www.jbiomedsem.com/content/5/1/42Figure 2 SADI-Galaxy Core tools and SADI services as Galaxy tools. Galaxy Analyze data view (only left column shown) resulting from theinstallation of the SADI-Galaxy Core (SADI generic client, RDF Syntax Converter, and Merge RDF Graphs, under SADI COMMON UTILITIES)and a number of specific SADI services, retrieved by a SPARQL query through the SADI-Galaxy Tool-generator (Under SADI SERVICES; only afew shown).object), so that non-RDF-based Galaxy tools canconsume SADIs output. Graph Merge. This tool is able to merge the outputof different SADI services into a single RDF graph fordownstream processing.SADI-Galaxy then offers an additional, more advancedfunctionality through the Tool-generator (Figure 3). TheTool-generator adds the ability to query a SADI Serviceregistry, using arbitrary query parameters, to retrieve a setof matching SADI services. These Service-specific toolswill then be deployed in Galaxy, alongside the Core toolsdescribed above. The Tool-generator, at the code-level, issimply a command line executable (A Shell script) thatreads one or more SPARQL [19] queries and executesthem against a SADI registry. The matching service URIsretrieved by the query are then used to generate, for eachservice, a Galaxy compliant wrapper and install it as a newTool.The SPARQL queries used by the Tool-generator canbe tuned in order to install a concrete set of servicesmatching, for example, a particular research objectivea.Query examples are provided in the SADI-Galaxy bundle,as well as a query generator that is able to produce param-eterised queries from a base query (e.g. for different SADIservice publishers). The default query can be seen inFigure 4.The Galaxy tools created by the Tool-generator utilizethe Cores generic SADI client, but pre-configure manyof the parameters of that generic tool. Therefore, ratherthan requiring users to know, for example, the address anddata-types of a service, each is provided as an indepen-dent Galaxy tool, enabling simple invoking and storing ofdesired SADI tools as bookmarks. These bookmarksEgaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 4 of 11http://www.jbiomedsem.com/content/5/1/42Figure 3 SADI-Galaxy Tool-generator. The Tool-generator is a Shell script that reads a SPARQL query (top, center) and generates XML files (anXML file for each SADI service Galaxy Tool). The XML files are copied to the appropriate location on the Galaxy server and appear as regular tools inthe Galaxy tools menu, under SADI SERVICES.also enable Galaxys tool-search function to explore SADIService descriptions. In this way, the same SADI Servicecan be invoked either through the generic client (if theURI is known by the user) or through the correspondingGalaxy tool (if the URI is unknown and/or the service isused frequently).ResultsbA simple exampleTo demonstrate how SADI-Galaxy can be used to trans-form data to RDF, in order for SADI services to consumeit, and how different SADI services can be combined asregular Galaxy tools, a simple use case was envisioned,and captured in the workflow depicted in Figure 5. Theaim of the workflow is to obtain the UniProt entry asso-ciated with a PDB entry [20], that is, to obtain informa-tion of the protein whose 3D structure is described inPDB. The workflow starts from a TSV file (as it is cus-tomary in Galaxy) and the TAB2RDF toolc, part of theSPARQL tools tool-set [21], is used to transform it tothe RDF/XML syntax that a SADI service can consume.The file is submitted to the SADI service pdb2uniprotto obtain the UniProt ID, which is returned as an RDFfile. The RDF file is submitted to another SADI service,uniprotInfo, to obtain all the information about thatUniProt entry, also as an RDF file. This final RDF file canbe converted to a TSV file with RDF2TAB, or queried withSPARQL-Galaxy to obtain concrete information aboutthe protein [22].Galaxy as a complex SADI clientThis second use case will reveal the more detailed func-tionality provided by SADI-Galaxy, including registrysearching, reasoning, and RDF format conversions thatallow linking between SADI tools and other Galaxy tools.The premise of this use-case is that a researcher is inter-ested in retrieving all of the information about a specificprotein, that can be obtained from any SADI service, andEgaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 5 of 11http://www.jbiomedsem.com/content/5/1/42Figure 4 Tool-generator default SPARQL query. In this query a set of filters for retrieving SADI service URIs is defined: the service (?s) must be amember of the classes serv:serviceDescription and sadi:Service (Lines 11 and 12); the service must be provided by an organisation(serv:providedBy ?org, line 13); the service must add an OWL property predicate to the output (sadi:decoratesWith ?out .?out owl:onProperty ?prop, lines 17 and 18); etc. The query returns around 250 active SADI services from the default registry. Otherqueries can be defined, as long as the ?s variable is used for service URIs. For example, in line 15, the "wilkinsonlab.info" value can beused, instead of the variable ?pub, to retrieve SADI services provided by wilkinsonlab.info.then to integrate and query it, as shown in Figures 6, 7,8, 9, 10. The first step is to generate Galaxy Tools forall services that can consume data that complies with(i.e. is logically inferred to be a member of) the OWLclass UniProt_Record [23]. This is accomplished byusing SADI-Galaxy to execute the SPARQL query shownin Figure 6. As a result, the researcher obtains a Galaxyinterface in which only the SADI services relevant totheir investigation are presented, all pre-configured andready to invoke their individual SADI services as shownin Figure 7. The researchers starting data is first retrievedfrom the Biomart central server [24] using the Galaxystandard facility Get data: Biomart central server (Topof Figure 8). The input - a list of UniProt IDs -, is manip-ulated with the Galaxy default text manipulation tools togenerate an RDF file the SADI services can consume (Left-hand of Galaxy workflow in Figure 8). An interface toexecute each SADI service is made available to the user bysimply clicking on the services name (No need to knowthe services URI, as shown in Figure 7). Since the out-puts from all SADI services (Center of Galaxy workflowin Figure 8) are all RDF, merging them is trivial (Right-hand of Galaxy workflow in Figure 8) and results in anintegrated dataset that can be queried (Figure 9) to obtainresults that, in turn, resolve to actual resources on theWeb(Figure 10).This process, in which the user integrates disparateinformation produced by many different SADI services,can be executed any time new UniProt IDs are obtained,or any time new SADI services that can consume UniProtentries are published in the registry (This can be accom-plished by simply running the Tool generator with thesame query periodically, without having to know whenthe registry is updated, i.e. which new services have beenadded).DiscussionSADI-Galaxy is inspired by the Galaxy Web ServicesExtensions (GWSE) tool [25,26], which is able to dynam-ically load SAWSDL/WSDL web services as Galaxy tools.SADI-Galaxy focuses on SADIs SemanticWeb-compliantservices, rather than SAWSDL/WSDL services, and there-fore for the first time brings Semantic Web resourcesinto the native Galaxy environment. In addition, the twoextensions have slightly different behaviors. Where theWS Extensions tool dynamically loads new Galaxy tools,the ability to execute such dynamic loading is not part ofthe standard Galaxy distribution. The SADI-Galaxy codeconforms strictly to the Galaxy specification, thus worksin any Galaxy off-the-shelf installation. After a restart,the SADI-Galaxy generic client becomes available in theleft-hand column of the Galaxy interface, and individ-ual SADI services may be dynamically loaded into thattool by providing the service URI. To simplify this taskeven further, SADI-Galaxy allows the option of query-ing the SADI service registry to discover the URIs ofdesired services, thus making it possible to automaticallyEgaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 6 of 11http://www.jbiomedsem.com/content/5/1/42Figure 5 Galaxy workflow for use case A simple example. Top: Galaxy Workflow view interface; bottom: simplified version, with detaileddepiction of files, including RDF triples (not all the triples shown). The workflow starts with a TSV file containing the information that will be sent asinput to the SADI service pdb2uniprot (Input dataset). The file is processed to convert it to a column format Galaxy can recognise(Convert) and then transformed to RDF with the Tab-to-RDF and RDF format tools from the SPARQL tools tool-set. The RDF file issubmitted to the pdb2uniprot SADI service using the SADI services generic caller and the output RDF is sent to theuniprotInfo SADI service, also with the SADI services generic caller. The output RDF from the uniprotInfo SADI service canbe converted to a TSV file with RDF Syntax Converter or queried with SPARQL (Execute an SPARQL query against an RDFfile) to obtain concrete information (Also in TSV format). Note that triples are added to the RDF input as a result of executing a SADI service, inpdb2uniprot and uniprotInfo.Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 7 of 11http://www.jbiomedsem.com/content/5/1/42Figure 6 SPARQL query to retrieve SADI services forUniProt_Record. This query, when executed by SADI-Galaxy,retrieves the URIs of all the services that have UniProt_Recordas input class (Lines 10 to 12): the services consume RDF datacontaining instances that are inferred to be members ofUniProt_Record when automated reasoning is applied, i.e.instances that satisfy the restrictions defined in the OWL ClassUniProt_Record.generate a large number of desirable Galaxy tools, a pow-erful feature that is not available within theWS Extensionstool.Another system that has features comparable to SADI-Galaxy is Tavaxy [27], which is a standalone server thatis a mediator between Galaxy and Taverna. WhereTavaxymakes it possible to mix Taverna and Galaxy work-flows, SADI-Galaxy concentrates on bringing SemanticWeb Services - already available within Taverna - intothe Galaxy environment. A similar result could beachieved by first building a workflow of SADI servicesin Taverna, then importing that workflow into Tavaxy inorder to add the Galaxy services; however, that processis far from seamless. It is more desirable to providenative access to the thousands of SADI resources fromwithin the Galaxy environment itself, than to requireGalaxy users to use foreign tools such as Taverna andTavaxy.We suggest SADI-Galaxy as the minimum infras-tructure that marks the point of intersection betweenSADI Services and Galaxy, which can now act as thecore codebase upon which more powerful functional-ity is constructed. In particular, we expect two majordevelopments in the near future: discovery, where givenan RDF input, Galaxy is able to infer and automati-cally select the appropriate SADI Tool; and adding toolsdynamically (as WS Extensions already does), once aconsensus has been reached by Galaxy developers onhow to implement a standard function for dynamic toolloading.ConclusionsThe simplicity and predictability of the SADI Servicedesign patterns - effectively, to simply consume and pro-duce raw RDF over HTTP POST - has allowed us tocreate a highly generic service invocation infrastructurethat would have been extremely difficult using other Webservice frameworks where, often, a client needs to knowsignificantly more about the data schema and serviceinvocation process. Building a framework that focuses onsemantics, rather than syntax - not only for the data itself,but also for the messaging infrastructure - means that theFigure 7 Galaxy interface for getKEGGIDFromUniProt SADI service. On the left column, available SADI services that comply withUniProt_Record are listed. On the right column, one of them, getKEGGIDFromUniProt, is selected in order to invoke it: only the RDF datamust be provided (In this case, the RDF is produced by RDF format in step 69 of current history). When the Execute button is clicked,automated reasoning will be used to validate the RDF data against the UniProt_Record OWL Class, as noted by the Input OWL Class menuitem in the What it does section of the tool interface.Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 8 of 11http://www.jbiomedsem.com/content/5/1/42Figure 8 Galaxy workflow for use case Galaxy as a complex SADI client. In order to obtain the input for the workflow (Bottom), data isimported directly from a BioMart server, without having to upload the data to the Galaxy server (Top). The data, a list of UniProt IDs, is thenconverted to suitable RDF (Compliant with UniProt_Record) using the standard Galaxy text manipulation tools (Bottom workflow, left-hand).The resulting RDF is then sent to different SADI services, generated by SADI-Galaxy, that can consume it (Bottom workflow, middle); each service isexecuted as shown in Figure 7. The output of the services is merged with the Merge Graphs tool (Bottom workflow, right-hand), and queried withSPARQL Galaxy (Executed as shown in Figure 9) to generate the results shown in Figure 10.client can be largely agnostic so as to how to invoke anyservice, making the necessary decisions on an ad hoc basisat invocation-time. For example, in SADI-Galaxy, whencombining different SADI services, the RDF of the inter-mediate steps can just as easily be consumed by any otherRDF-based tool.Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 9 of 11http://www.jbiomedsem.com/content/5/1/42Figure 9 SPARQL Galaxy. This query takes as input the merged output of all the SADI services and finds the proteins that are encoded by a KEGGgene and are related to SNPs, to produce the results shown in Figure 10.Galaxys easy-to-use platform for storing data, pro-grams for analysing data, and the resulting workflows isan ideal ecosystem within which to provide SADIs dataretrieval and analysis functionalities to our target end-users in a very familiar and straightforward manner. Wehope that, by providing SADI services within the widely-used Galaxy platform, we can encourage the more rapidadoption of these powerful new Semantic Web technolo-gies, with SADI-Galaxy acting as the de facto interfacebetween these two projects.Figure 10 SPARQL query results. Galaxy interface for HTML results of query from Figure 9 (Top). The links point to actual resources: this is shownby the resolved KEGG gene entry (Bottom), obtained as the result of clicking on a link.Egaña Aranguren et al. Journal of Biomedical Semantics 2014, 5:42 Page 10 of 11http://www.jbiomedsem.com/content/5/1/42Availability and requirements Project name: SADI-Galaxy. Project home page: http://github.com/mikel-egana-aranguren/SADI-Galaxyd. Operating system(s): UNIX-based (GNU/Linux,Mac OS X, *BSD, etc.). Programming language: Java, Python, Shell Script,and Sed. Other requirements: a working Galaxy server(http://galaxyproject.org/). License: General Public License (GPL), version 3.EndnotesaDifferent SADI tool-sets can be addedsimultaneously to the Galaxy tools menu, by executingthe Tool-generator sequentially (with different SPARQLqueries) and by editing the title in the Galaxy tools menueach time a new tool set is added (The default title isSADI SERVICES, as shown in Figure 2). This way aresearcher can organise SADI services in meaningfulgroups, having all the groups available in the sameGalaxy interface.bThe use cases can be explored and executed in thefollowing Galaxy page (Also available in the http://biordf.org:8983 Galaxy server through shared data and thenpublished pages): http://biordf.org:8983/u/mikel-egana-aranguren/p/sadi-galaxy-jbms-use-cases. In orderto reproduce a use case a user must be created in theGalaxy server and the history and the workflow of theuse case imported, so that the first item of the history canbe used as input for the workflow.cA locally modified version of SPARQL tools wasused, adding the possibility of rendering user-definednamespaces. A patch has been submitted to the originalauthor for inclusion on the Galaxy tool shed repository;the modified version can be obtained at http://github.com/mikel-egana-aranguren/SPARQL_tools_tab2rdf.dSee also TAB2RDF (http://github.com/mikel-egana-aranguren/SPARQL_tools_tab2rdf), SADI genericclient (http://toolshed.g2.bx.psu.edu/repos/mikel-egana-aranguren/sadi_generic/) and SPARQL Galaxy(http://toolshed.g2.bx.psu.edu/repos/mikel-egana-aranguren/sparql_galaxy/).AbbreviationsRDF: Resource description framework; OWL: Web ontology language; SADI:Semantic automated discovery and integration; SPARQL: SPARQL protocoland RDF query language; TSV: Tab separated values.Competing interestsThe authors declare that they have no competing interests.Authors contributionsMEA developed SADI-Galaxy and ARG developed the SPARQL query engine toretrieve the SADI services. MDW contributed with test-cases, wrote andrevised portions of the manuscript, and supervises the Biological Informaticslaboratory within which this work was executed. All authors read andapproved the final manuscript.AcknowledgementsMikel Egaña Aranguren is funded by the Marie Curie Cofund programme (FP7)of the European Union and the Genomic Resources Group of the University ofBasque Country. Alejandro Rodríguez González and Mark D. Wilkinson arefunded by the Isaac Peral Programme of the CBGP-UPM.Received: 12 December 2013 Accepted: 19 September 2014Published: 22 September 2014JOURNAL OFBIOMEDICAL SEMANTICSLingutla et al. Journal of Biomedical Semantics 2014, 5:50http://www.jbiomedsem.com/content/5/1/50SOFTWARE Open AccessAISO: Annotation of Image Segments withOntologiesNikhil Tej Lingutla1, Justin Preece2, Sinisa Todorovic1, Laurel Cooper2, Laura Moore2 and Pankaj Jaiswal2*AbstractBackground: Large quantities of digital images are now generated for biological collections, including those developedin projects premised on the high-throughput screening of genome-phenome experiments. These images often carryannotations on taxonomy and observable features, such as anatomical structures and phenotype variations oftenrecorded in response to the environmental factors under which the organisms were sampled. At present, most of theseannotations are described in free text, may involve limited use of non-standard vocabularies, and rarely specify precisecoordinates of features on the image plane such that a computer vision algorithm could identify, extract and annotatethem. Therefore, researchers and curators need a tool that can identify and demarcate features in an image plane andallow their annotation with semantically contextual ontology terms. Such a tool would generate data useful for inter andintra-specific comparison and encourage the integration of curation standards. In the future, quality annotated imagesegments may provide training data sets for developing machine learning applications for automated image annotation.Results: We developed a novel image segmentation and annotation software application, Annotation of ImageSegments with Ontologies (AISO). The tool enables researchers and curators to delineate portions of an image intomultiple highlighted segments and annotate them with an ontology-based controlled vocabulary. AISO is a freelyavailable Java-based desktop application and runs on multiple platforms. It can be downloaded at http://www.plantontology.org/software/AISO.Conclusions: AISO enables curators and researchers to annotate digital images with ontology terms in amanner which ensures the future computational value of the annotated images. We foresee uses for suchdata-encoded image annotations in biological data mining, machine learning, predictive annotation, semanticinference, and comparative analyses.Keywords: Image annotation, Semantic web, Plant ontology, Image segmentation, Plant anatomy, Webservices, Computer vision, Image curation, Machine learningBackgroundAnnotation of Image Segments with Ontologies (AISO)is an interactive tool which allows users to segment andannotate a digital image  such as those produced withdigital photography or from scanned prints  withontology terms. An ontology is a controlled and struc-tured vocabulary of agreed-upon labels (terms) thatrepresent the knowledge of the types of entities within agiven domain [1]. Labeling image data with ontologyterms imbues it with semantic meaning, which makes it* Correspondence: jaiswalp@science.oregonstate.eduEqual contributors2Department of Botany and Plant Pathology, 2082 Cordley Hall, Oregon StateUniversity, Corvallis, OR 97331-2902, USAFull list of author information is available at the end of the article© 2014 Lingutla et al.; licensee BioMed CentraCommons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.possible to computationally infer relationships amongstdifferent images and parts of images. The use of ontol-ogies has gained increasing importance as the number,complexity, and size of biological data sets have in-creased [2]. AISO was developed in response to a needwithin the biology community for a streamlined tool thatenables consistent and structured labeling of digital im-ages. A shift in research focus towards high throughputphenotyping [3,4] requires specialized tools that bringconsistency to the image annotation process. AISO an-notates images with ontology terms and taxonomy labelsvia lightweight web services, allowing users to select andannotate image segments.Many photo-editing and illustration software packagesenable the ad hoc editing of an image, but any highlightingl. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Lingutla et al. Journal of Biomedical Semantics 2014, 5:50 Page 2 of 6http://www.jbiomedsem.com/content/5/1/50and labeling utility requires thorough knowledge of thesoftwares illustration capabilities (i.e. layering, boundarydetection) and does not include the structured integrationof scientific data. For example, any labels applied to hand-illustrated segments superimposed onto an image wouldhave to be individually constructed and associated with aparticular portion of an image. AISO simplifies this func-tionality and requires only a few input gestures and clicksto identify and label segments. The resulting structuredimage and ontology annotation allows for consistentextraction techniques, enabling future database storage,active learning, and semantic inference functionalities. Re-searchers are thus empowered to construct meaningfulimage data sets drawn from their laboratories, onlineimage archives, and publications.ImplementationSoftware architectureAISO is a multi-platform, Java desktop application ex-tending the source code of the Interactive SegmentationTool (IST) [5], originally developed for comparing the per-formance of image segmentation algorithms. The userinterface was constructed using the Standard WidgetToolkit [6], an open-source Java package. The ontologyterms are provided through a light-weight Plant Ontologyweb service [1], and returns data in the JSON format [7].Species names are provided via the uBio web service [8] inXML. Annotation data  segments, labels, and curationdetails  are all saved into a compressed ZIP archive,which contains the original image, binary segment datafiles, segment mask images, and an XML file storingsegment coordinates and other curation metadata. Anexample of the contents and structure of an AISO XMLmetadata file is available in the Help document, whichmay be accessed from the application menu.Choice of segmentation algorithmSegmentation algorithms are created with different ap-plication domains in mind and the computer vision re-search community is generally focused on segmentingimages of the human body and the built environment.Segmenting anatomical images of biological specimens,such as plants, presents a number of challenges thathave received scant attention in the literature. Plantscontain curvilinear and asymmetric forms, textures, andspatial orientations that make identification and classifi-cation more difficult for computer vision algorithms. Wechose the Interactive Graph Cuts (IGC) segmentationalgorithm because markups have a local effect, therebyavoiding major global deformations in the segmentedarea. This has great value in plant images that containdensely grouped features, such as many similar, overlap-ping leaf structures. The IGC algorithm also is more ac-curate in extracting foreground objects, and includes aresponsiveness that allows the user to iteratively refinethe segments. The average time required for a user to at-tain optimal object and boundary accuracy for an image,and the average total time spent annotating each imageare much lower when compared to other algorithms [9].Results and discussionSegmenting and annotating imagesAfter opening an image file in AISO (Figure 1A; allowedformats: JPEG, PNG, GIF, and BMP), the user works intwo alternating modes: Segmentation and Labeling. Thedefault Segmentation mode operates in a foreground/background paradigm and allows the user to mark theforeground of an image by drawing red lines with themouse or trackpad (left-click and drag); the back-ground is denoted with blue lines (right-click and drag).Once both the foreground and the background are des-ignated with these mark-up lines (Figure 1B,C), AISOwill execute its IGC segmentation algorithm [10], identi-fying and extracting borders circumscribing the fore-ground mark-up and visually highlighting the area(Figure 1C). Edges in an image can be extracted wher-ever there is a detectable change in the lightness ordarkness of adjacent pixels. The user may further refinetheir segmented area by adding foreground mark-uplines (red) outside of the initial area to expand the dis-cernable boundary, or by applying background mark-uplines (blue) across the initial area to exclude nearbyimage content from the segment. Once satisfied with aparticular segment, the user must form that segment bypressing the Form segment button, thereby fixing it asa new layer overlaid on top of the original image. Fol-lowing segmentation, the user then enters into Labelingmode by pressing the Labeling Mode button to assignan ontology term to a segment. The user may search fora Plant Ontology term by typing its name in thedropdown box labeled Annotate. The PO terms areprovided through a web service [1] which requires anInternet connection. The dropdown box will displayauto-completed term suggestions when the user pressesthe down arrow on their keyboard. After selecting aterm in the dropdown, the user should press the As-sign button to associate their selected PO term with thecurrently selected segment. The assigned ontology termwill appear on a segment when the user selects that seg-ment and hovers over it with the pointer (Figure 1D).The user may also assign a taxonomic name to the en-tire annotated image via the uBio namebank searchweb service [8] and enter additional curation metadata(Figure 1E).The user can save annotated image files into a customZIP package, and may also re-open previously annotatedimages for continued editing. The original image is al-ways preserved and viewable. When saving annotations,(A) (B)(E)(C) (D)Figure 1 Screenshots of AISO demonstrating the segmentation process. (A) Open an image file in AISO (e.g. a Chamerion angustifoliumflower) (B) Mark the foreground (red line): the aspect of the image you would like to segment. (C) Mark the background (blue line): the areaof the image you want to ignore. (NOTE: Auto-segmentation occurs after the background is marked; further refinement of both foreground andbackground is possible.) (D) Label the new image segment with an ontology term selected from the integrated web service query interface.(E) Screenshot of the annotation panel of AISO showing a selected ontology term, its definition and synonyms, and designated species(provided via web services), as well as curator, collection, and comment information entered by the user.Lingutla et al. Journal of Biomedical Semantics 2014, 5:50 Page 3 of 6http://www.jbiomedsem.com/content/5/1/50the user may optionally export an HTML file containinga web-enabled version of the annotated image, which al-lows the user to easily share their work in other mediaplatforms. For example, manuscript authors could sub-mit annotated images along with other supplementarydata, to enhance the collection of ontology-based imagedata for comparative analyses and machine learning.Annotated images could thereafter be used in onlineresources and publications, or placed in a file archive ordatabase for future analysis.Case studiesAISO was used to segment and label small library ofbotanical images (provided by Dennis Wm. Stevenson;New York Botanical Gardens), both as a test of AISOscapabilities and as the beginnings of a training set ofsegmented image data for a potential future active-learning system. One such image of the floral structures ofGalanthus elwesii, commonly known as Giant Snowdrop,was segmented and labeled with the Plant Ontologyterms plant ovary (PO:0009072), style (PO:0009074),tepal (PO:0009033), anther (PO:0009066) and perianth(PO:0009058) (Figure 2). Each segment was generatedwith only a few user-directed mouse or trackpad strokes,followed by a quick auto-complete search for the appro-priate Plant Ontology term. Each segments overlay isassigned a different preset color to help distinguish it fromany other segments in the same image.Additionally, AISOs capabilities have been applied topaleo-botanical images of fossilized lauraceous flowersfrom the Eocene epoch (Figure 3). In this particularcase, Plant Ontology terms stamen (PO:0009029),carpel (PO:0009030), and tepal (PO:0009033) were ap-plied to user-identified segments in a cross-section image.(A)(B) (C) (D) (E)Figure 2 Screenshots and insets of AISO displaying annotated segments of Galanthus elwesii (giant snowdrop) flowers. The representedsegments are (A) perianth (PO:0009058), (B) multiple instances of anther (PO:0009066), (C) tepal (PO:0009033), (D) style (PO:0009074) and (E) plantovary (PO:0009072).Lingutla et al. Journal of Biomedical Semantics 2014, 5:50 Page 4 of 6http://www.jbiomedsem.com/content/5/1/50This particular example highlights the IGC algorithms lo-calized border-detection capabilities when constructing animage segment.Comparison to existing softwareAISO brings together disparate image segmentation andsemantic labeling functionalities found in existing softwareand merges them into a user-friendly, science-focusedpackage. Hollink et al. [11] developed an application inter-face for annotating whole images with ontology terms, butit lacked an image segmentation feature. Conversely, Shaoet al. [12] developed image segmentation capabilities with-out segment-specific semantic tagging features. SemanticImage Annotator [13], built as an extension to the web-focused Semantic MediaWiki platform, allows users to de-fine rectangular areas on an image file and tag those areaswith semantic labels, but does not provide dynamic imagesegmentation. Koletsis and Petrakis dissertation work [14]includes an algorithm named Semantic Image Annota-tion which automatically annotates images with ontologyterms based on a training data set of similar images, butthis approach also lacks a segmentation feature.Future developmentFuture enhancements to AISO include extending webservice support for multiple ontologies, such as thosedeveloped by OBO Foundry [15] members and modelorganism databases. Enhancements would also includeenabling automated segmentation based on activelearning, and adding support for high-resolution im-ages (10120 megabytes).ConclusionsAISO allows researchers and curators to interactivelysegment images and assign semantic annotations tothose segments. This annotation capability gives biolo-gists the opportunity to enhance the computationalvalue of their own image data. Data-enriched images canbe used to mine biological data sets, train machinelearning software, and generate conclusions via semantic(A)(B)Figure 3 Cross section of a fossilized lauraceous flower from the Eocene epoch (~55 MA) showing a single carpel, stamen, and tepals.The original image (A) has been segmented and annotated using Plant Ontology terms in AISO (B). Note the automated color differentiationbetween segments. The annotation labels in this figure have been enlarged for readability.Lingutla et al. Journal of Biomedical Semantics 2014, 5:50 Page 5 of 6http://www.jbiomedsem.com/content/5/1/50Lingutla et al. Journal of Biomedical Semantics 2014, 5:50 Page 6 of 6http://www.jbiomedsem.com/content/5/1/50inference. We believe that the existing functionality ofAISO, combined with our future efforts in active learning,will provide a powerful tool for the biology communityand for scientific journals interested adding annotated im-ages and associated metadata to their publication pipeline.Availability and requirementsProject name: Annotation of Image Segments withOntologies (AISO).Project home page: http://www.plantontology.org/software/AISO.Operating system(s): Platform-independent (Mac OS X,Linux, Windows).Programming language: Java.Other requirements: An Internet connection, the JavaRuntime Environment (JRE).License: Creative Commons (Attribution-NonCommercial-NoDerivs 3.0 Unported).Any restrictions to use by non-academics: No.AbbreviationsAISO: Annotation of image segments with ontologies; IGC: Interactive graphcuts; IST: Interactive segmentation tool; JSON: Javascript object notation;PO: Plant ontology; XML: Extensible markup language.Competing interestsThe authors declare that they have no competing interests.Authors contributionsPJ and ST conceived the project. PJ and ST co-supervised NL for his mastersthesis project. NL and JP designed and wrote the software enhancements(building on Kevin McGuiness work; see acknowledgements), co-authoredthe manuscript, wrote user manuals, coordinated testing, and provideduser support and training. JP coordinated the software development andtechnical architecture of the new features. LC curates and coordinates thePlant Ontology project; tested the AISO software and mentored students inannotation of images, and contributed to the manuscript. LM tested theAISO software and created the annotated images presented herein. NL,JP, LC, ST, and PJ edited the manuscript. All authors read and approved thefinal manuscript.AcknowledgementsWe thank Kevin McGuinness at Dublin City University for sharing the originalIST source code, which we modified extensively to build AISO, and foradvising our development efforts. We are also grateful to Dr. Dennis Wm.Stevenson at the New York Botanical Gardens for his permission to usedigital images of botanical species in the segmentation testing of AISO. Wealso thank Brian Atkinson and Dr. Ruth A. Stockey at Oregon State University,for their contribution of the paleo-botanical image and subsequent segmentationand annotation, as described in the Case Studies and Figure 3. This work wasfinancially supported by the National Science Foundation (NSF) of USA awardIOS:0822201. PJ and ST were also supported by Oregon State University (OSU).The funders had no role in the software design, analysis or preparation of themanuscript.Author details1School of Electrical Engineering and Computer Science, Kelley EngineeringCenter, Oregon State University, Corvallis, OR 97331-2902, USA. 2Departmentof Botany and Plant Pathology, 2082 Cordley Hall, Oregon State University,Corvallis, OR 97331-2902, USA.Received: 13 September 2014 Accepted: 26 November 2014JOURNAL OFBIOMEDICAL SEMANTICSMarciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24http://www.jbiomedsem.com/content/5/1/24RESEARCH Open AccessTerminology extraction frommedical texts inPolishMa?gorzata Marciniak* and Agnieszka MykowieckaAbstractBackground: Hospital documents contain free text describing the most important facts relating to patients and theirillnesses. These documents are written in specific language containing medical terminology related to hospitaltreatment. Their automatic processing can help in verifying the consistency of hospital documentation and obtainingstatistical data. To perform this task we need information on the phrases we are looking for. At the moment, clinicalPolish resources are sparse. The existing terminologies, such as Polish Medical Subject Headings (MeSH), do notprovide sufficient coverage for clinical tasks. It would be helpful therefore if it were possible to automatically prepare,on the basis of a data sample, an initial set of terms which, after manual verification, could be used for the purpose ofinformation extraction.Results: Using a combination of linguistic and statistical methods for processing over 1200 children hospitaldischarge records, we obtained a list of single and multiword terms used in hospital discharge documents written inPolish. The phrases are ordered according to their presumed importance in domain texts measured by the frequencyof use of a phrase and the variety of its contexts. The evaluation showed that the automatically identified phrasescover about 84% of terms in domain texts. At the top of the ranked list, only 4% out of 400 terms were incorrect whileout of the final 200, 20% of expressions were either not domain related or syntactically incorrect. We also observedthat 70% of the obtained terms are not included in the Polish MeSH.Conclusions: Automatic terminology extraction can give results which are of a quality high enough to be taken as astarting point for building domain related terminological dictionaries or ontologies. This approach can be useful forpreparing terminological resources for very specific subdomains for which no relevant terminologies already exist. Theevaluation performed showed that none of the tested ranking procedures were able to filter out all improperlyconstructed noun phrases from the top of the list. Careful choice of noun phrases is crucial to the usefulness of thecreated terminological resource in applications such as lexicon construction or acquisition of semantic relations fromtexts.BackgroundTerminology extraction is the process of identifyingdomain specific phrases (terms) based on the analysisof domain related texts. It is a crucial component ofmore advanced tasks like: building ontologies for specificdomains, document indexing, construction of dictionar-ies and glossaries. The subject has been undertakenquite often, particularly in the context of molecular biol-ogy terminology. In particular, the Medline abstractsdatabase was frequently used as a data source for pro-tein and gene names, [1,2]. The biomedical domain is*Correspondence: mm@ipipan.waw.plInstitute of Computer Science PAS, Jana Kazimierza 5, 01-248 Warsaw, Polandchanging so rapidly that manually prepared dictionar-ies are becoming outdated very quickly. In more sta-ble domains, like clinical medicine, a lot of terminologyalso exists which is used locally and which is not listedin any dictionaries. For many languages, medicine andbiomedicine terminology is covered by several sourceslike those available in UMLS [3], e.g. MeSH or SNOMED,but there are still a lot of domain related expressionsoccurring within clinical texts which are not includedthere. Moreover, there are a number of languages (likePolish), whose medical linguistic resources are underde-veloped. In particular, for the Polish language there areno computer dictionaries, except MeSH, with medical© 2014 Marciniak and Mykowiecka; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,distribution, and reproduction in any medium, provided the original work is properly credited.Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 2 of 14http://www.jbiomedsem.com/content/5/1/24vocabulary or terminology, nor is there a SNOMEDtranslation.This lack of resources and the need for keeping up todate resources describing rapidly changing subdomainshas lead to exploring the idea of automatic terminol-ogy extraction. Several different approaches to this taskare discussed in [4]. It may be observed in the researchreported there that, regardless of the detailed assump-tions undertaken in the particular solutions, terminologyextraction usually consists of two steps. The first one iden-tifies candidates for the terms, and is usually supportedby linguistic knowledge. The second step, based on statis-tics, involves ranking and filtering candidates accordingto some measure of their relative domain importance.Although the general scheme of term extraction is quitestable, the specificity of a particular natural language, thedomain of interest, the size of data available and the acces-sibility of language processing tools, can all influence theresults. Until now, there has been no single strategy whichcan precisely select terms from non terms and whichhas proved to be best for all the domains and languagestested.Automatic extraction of phrases from texts makes itpossible not only to prepare a list of domain relatedterms, but also to identify the exact ways in which theyare expressed in context. These results can be used lateron to help create a domain ontology and in specify-ing the information that may be extracted from docu-ments with rule based methods, see [5]. While writingextraction rules we just have to describe all the identi-fied phrases. Assigning one semantic concept or ontologyclass to all lexical paraphrases requires the normalisa-tion step on which all variants are grouped together.In [6] the normalisation procedures are described. Theauthors consider the conflation of orthography and inflec-tional variants, as well as lexical synonyms, structuralvariants of phrases, and recognition of acronyms andabbreviations.What is common to all domain vocabularies is thatthe vast majority of terms are noun phrases. Althoughin some approaches verbal phrases are also taken intoaccount [7], terminology vocabularies usually containnominalised versions of such terms. Extracting can-didates for domain terms can be based on simplen-grams, e.g. [8], but in most approaches, linguistic infor-mation is used. Usually only small shallow grammarsare defined [9], but sometimes more elaborate linguisticprocessing is performedin [7] the terminology extrac-tion was carried out on fully syntactically parsed texts.While extracting domain terminology we are interestedin compound terms which describe precise concepts,e.g. kos´c´ ramienia humerus, the concepts attributes,e.g. powie?kszone we?z?y ch?onne enlarged lymph nodes orrelationships between two concepts, e.g. z?amanie kos´ciprzedramienia humerus fracture. These phrases are notonly expressing certain domain important concepts orevents but can also be used later on to build up a domainmodel in which we can include the knowledge that lymphnodes can be enlarged and that the bone can be broken.Recognition of complex expressions can entail recogni-tion of shorter phrases which are part of these longerones.At the initial stage of candidate selection, the longestsequences matching the set of defined rules are identi-fied. If we are to order phrases using weights based onthe number of times they appear in text, we should alsoanalyse phrases which occur inside others. For exam-ple, an occurrence of lewa nerka prawid?owa left kidneynormal should also be counted as an occurrence of thephrases: nerka kidney, lewa nerka left kidney and nerkapra-wid-?owy kidney normal. Another decision to bemade is whether to count the occurrences of all nestedphrases or only those which occurred at least once asa separate phrase. It may happen that a term which isvery important does not occur even once in a given dataset.The preselected set of phrases constitute input datafor the term selection algorithm which usually assignseach phrase a numerical value approximating the rela-tive likelihood that the phrase will constitute a domainterm. One of the most popular ranking methods, designedspecially for recognising multiword terms, is the C/NCmethod proposed in [9]. This method takes into accountphrase occurrences both in isolation and nested insidelonger ones, and the different contexts of their appear-ances. In this method every phrase is assigned a C-value,which is computed on the basis of the number of times itoccurs within the text, its length, and the number of dif-ferent contexts it takes (within noun phrases in which itoccurs).The definition of the C-value coefficient is given below(p  is a phrase under consideration, LP is a set of phrasescontaining p), r(LP)  the number of different phrases inLP, l(p) = log2(length(p)).C-value(p) =???l(p) ? (freq(p) ? 1r(LP)?lp? LP freq(lp)), if r(LP) > 0,l(p) ? freq(p), if r(LP) = 0(1)Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 3 of 14http://www.jbiomedsem.com/content/5/1/24Long phrases tend to occur more rarely than shorterones so the multiplication by the logarithm of lengthmoves them towards the leading positions. If a nestedphrase occurs in one context only, its C-value is set to 0 asit is assumed to be incomplete. If a nested phrase occurs ina lot of different contexts, the chance that it may constitutea domain term increases.A popular modification of the method was aimed atextending the ranking procedure for phrases of the length1 which originally all get a 0 value. For this purpose, thelogarithm of the length for one word phrases (used in theoriginal solution) was replaced with a non-zero constant.In [10], where this method was applied to Spanish texts,the authors initially set this constant to 0.1, but finally setit to 1, arguing that otherwise one word terms would belocated too low on the ranking list.Comparisons to other term extraction methods, per-formed in [11] among others, showed that in thebiomedical domain termhood-basedmethods outperformunithood-based methods where termhood is defined as adegree that a linguistic unit is related to domain-specificconcepts, and unithood as a degree of strength or stabil-ity of syntagmatic combinations and collocations [12]. In[4] the C-value method, which is based on frequency mea-sure, was judged to be better suited for term identificationthan mutual information or the Dice Factor describing thedegree of association measures.The C-value obtained using the equation cited abovereflects only the relationships between the terms them-selves. The results can be improved on the basis of thecontexts in which the terms occur within texts. In [9]it was suggested that appearing in the same context ashighly ranked terms should increase the rank of the candi-date term. For example, the frequent statement niepraw-id?owy twór abnormal formation is ranked high, while therare one nieprawid?owy cien´ abnormal shadow has muchlower a C-value. Both phrases occurred in the same sin-gular context: stwierdzono found. On this basis, the lowmark of the second term can be increased. The idea isrealised by the NC coefficient which is counted accordingto the following equation in which t is a candidate term,Ct is a set of distinct contexts of t, ft(b) is the frequency ofb occurring as a context of t andweight(b) = t(b)/nwheret(b) is the number of terms the context word b occurs withand n is the total number of the terms considered.NC-value(t) = 0.8 ? C-value(t)+ 0.2 ??b?Ctft(b) ? weight(b) (2)In the original solution contexts were just strings ofwordforms surrounding the given phrase within the text.The authors of [10] proposed using lemmas of the sur-rounding words instead of their forms for processingSpanish, which has different forms of adjectives and nounsaccording to number and grammatical gender.Applying the C/NC scheme or another ranking proce-dure we get an ordered list of the potential terms. Weexpect that phrases which are not domain relevant or lin-guistically incorrect are located low on this list and weare not interested in the exact value of the C/NC coeffi-cient of a particular term. Finally, a cut-off value accordingto a coefficient value or a position on the list is chosenat the final processing stage. A set of phrases which arelocated above this cut-off constitute the final result ofthe terminology extraction task. The different extractionmethods can be compared on the basis of a percentage ofthe selected phrases judged as not being terms during theevaluation stage.Results and discussionThe term extraction procedure was conducted on twosets consisting of discharge reports from two wards of aPolish children hospital: the allergies and endocrine ward(further referred to as o1) and the surgical ward. Theyconsisted respectively of about 78,000 tokens, and over360,000 tokens. The analysed texts were very concise asphysicians reported only the most important facts there.Thus, it occurred that the great majority of the extractednominal phrases were domain related. But not all of themwere equally useful for the given domain, and a shal-low grammar also resulted in extracting some sequenceswhich were not correct phrases at all. Thus, the order-ing of the results was still an important task. The C/NCmethod proved able impose an ordering which locatedimportant phrases at the beginning of the ranked list,while incorrect phrases were moved towards its end.The defined grammar together with the procedure ofidentifying nested phrases identified more than 4100 dif-ferent nominal phrases (nested or independent) in the o1set, more than 7100 in the surgery set and more than14150 in the both sets combined together. This meansthat about 1350 of them occurred in both sets (about onethird of the smaller set). The number of phrases extractedusing the shallow grammar and the distribution of theirlength and frequencies are given in Tables 1 and 2. About20% of these phrases are singular words; the largest groupof phrases has two elements (38%) while only about 5%have 5 or more words. The average phrase length is equalto 2.5. More than half of the phrases occurred exactlyonce, while less than 10% of them occurred more than10 times.Table 3 shows the distribution of the C-value. About onethird of phrases got a 0 value because they always had thesame context (within a phrase as its nested subphrase).The remaining 70% of phrases contained correct clinicalterms located both at the top of the list as well as closeto the bottom of the list. Medical terms which occurredMarciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 4 of 14http://www.jbiomedsem.com/content/5/1/24Table 1 Distribution of phrase lengthsPhrase Data Commonlength o1 surgery o1+surgery nb % from o1 in surg.?4156 11354 14156 1354 32.581 1381 2219 2880 720 52.142 1644 4212 5403 453 27.553 801 2941 3605 137 17.104 242 1301 1511 32 13.225 68 476 534 10 14.71> 5 20 205 223 2 10.00Max 12(8) 5(7) 12(8) 0 -very few times in isolation got a very low positive C-value, e.g. anestezjolog anaesthetist, torbielka small cyst.They cannot be differentiated by the method from nounssuch as kwiat flower or chodnik pavement which alsooccurred within the data. The positive effect of countingoccurrences of nested phrases can be observed for ostrydyz?ur emergency service, for example, which occurred inisolation only once, but was used 82 times in 6 differentcontexts and classified in 148th place.The answer to the question whether to count occur-rences of nested phrases which never occur in isolation isnot clear. One of the examples of the successful recogni-tion of such a term is kos´c´ ramienna humerus. Anotherexample is miedniczka nerki renal pelvis which also didnot occur in isolation but had 15 occurrences in 6 dif-ferent contexts and was located in 705th place. However,the strategy of promoting nested phrases on the basis ofthe occurrences of the phrases they are part of, can some-times lead to undesirable results. The phrase infekcja drógtract infection never occurred alone but had 11 differ-ent contexts and was located very high (216) in spite ofbeing an incorrect (truncated) phrase. An extreme exam-ple of such a phrase which gained a very high C-value iskarta informacyjna leczenia treatment information cardbeing a subsequence of the phrase karta informacyjnaleczenia szptialnego hospital treatment information card.Table 2 Distribution of phrase frequenciesPhrase Datafreq o1 surgery o1+surgery?4156 11354 14156=1 2272 7120 8211210 1417 4076 45721150 325 922 96951100 71 115 1571011000 71 168 2171000- 0 28 30Table 3 Standard C-value distributionTerms Datafreq o1 surgery o1+surgery?4156 11354 14156C= 0 1110 3458 4163C> 0 3046 7896 99930<C< 1 893 1509 1936C= 1 565 1301 1708C> 1 1588 5086 63491<C<= 2.5 898 2842 3531C> 2.5 690 2244 2818In surgical data it occurred 1164 times in this phrase andonce in a longer phrase poprzednia karta informacyjnaleczenia szpitalnego previous hospital treatment informa-tion card. For the C-value counting algorithm this meantthere were two different contexts in which this phraseappeared, and resulted in the sixth top value for a phrasewhich did not occur in the data and is probably not usedat all.The equation for C-value promotes sequences whichhave different contexts but, in the case of nested phrases,it may be possible that all these contexts describe a superphrase. e.g. for klatkasubst (cage, case, frame) there areseveral context super phrases like: klatkasubst pier-siowaadj chest, USG klatki piersiowej chest ultrasound,RTG klatki piersiowej chest RTG, zdje?cie klatki pier-siowej chest picture, klatka piersiowa prawid?owa chestnormal, but all these are contexts for the term klatkapiersiowa chest and should not promote klatka as anindependent term. This word is ambiguous and is ratherrarely used alone with respect to klatka piersiowa chest.The accepted solution (named as C1) relies on countingsuper phrases which differ only in the words adjacent to agiven term.The distribution of the C1-value is given in Table 4.For the C1-value method the phrase: karta informacyjnaleczenia treatment information card, which occurredTable 4 C1-value distributionTerms Datafreq o1 surgery o1+surgery?4156 11354 14156C= 0 2843 4140 4933C> 0 2843 7214 92230<C< 1 775 1243 1625C= 1 581 1339 17571<C<= 2.5 843 1487 3227C> 2.5 644 2068 2614Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 5 of 14http://www.jbiomedsem.com/content/5/1/24only as the nested phrase and has only one context,obtained the proper 0 C1-value. The proposed strategy,however, did not eliminate all unfinished phrases andyielded only a slight lowering of their score, e.g. from 28thplace down to 45th forUSG jamy USG of cavity in the listfor surgical data. The high ranking of this phrase on theterminology list is a result of it being part of the follow-ing two phrases: USGbrev:nw jamysubst:gen brzusznejadj:gen(used 377 times alone and 51 as a nested phrase) and lesscommon USGbrev:nw jamysubst:gen brzuchasubst:gen (used 3times alone). Both phrases have the same English equiv-alent: USG of abdominal cavity. Moreover, the phraseUSG jamy was recognised once in isolation because of aspelling error in the word brzusznej abdominal.C1 coefficients are by definition usually lower than theoriginal C-values. However, the changes in the rankingorder are not very large. For o1 data, of the top 600 ele-ments 20 received a C1-value equal to 0. Only two ofthem were good medical terms, the rest were incompletephrases like the one described above and were correctlysuppressed. For surgical data, these extreme changes wereeven smaller4 in 600 top phrases got a 0 C1-values, oneof them is a correct medical term. In the entire surgi-cal data, 119 terms which had a non-zero C-value got a0 C1-value, 46 of them were incorrect phrases. For thepreviously given example, infekcja dróg, we got 4 contextsinstead of 11, the coefficient value was lowered by about20%, but the position changed only by 20. Similarly, forthe very frequent phrase USG jamy the change, equal toabout 40% of coefficient value, resulted in a small changein position (of 17 places).In order to identify terminology that may not be relatedto the medical domain, we compared the terminologyextracted from medical data with phrases extracted fromthe general corpus of the Polish language (National Cor-pus of Polish (NKJP) [13])processed and ranked usingthe same tools. Then we compared terminology identi-fied in NKJP and medical data: surgery and o1 separately.Table 5 shows how many terms are recognised in bothcorpora (NKJP and the medical one) and the number ofterms that have a higher C1-value in the NKJP data. Thiscomparison gives only a general overview as the sizes ofthe compared corpora are different. The longest commonTable 5 Comparison with general corpusTerms o1 SurgeryCommon with NKJP 791 11551-word 680 969Multi words 111 186C1-value greater in NKJP 431 5461-word 374 477Multi words 57 69phrase has four words and there is only one in both cor-pora infekcja górnych dróg oddechowych upper respiratorytract infection. Multi-word terms that have a C1-valuehigher in the NKJP data account for about 2% of multi-word terms for o1 data and less than 1% for surgerydata. Moreover, most multi-word terms with a higherC1-value in NKJP are related to the medical domain,e.g.: poradnia zdrowia psychicznego mental health clinic,przewód pokarmowy gastrointestinal tract, oddzia? inten-sywnej terapii intensive care unit. But, of course, thereare also terms that are common in everyday language like:numer telefonu telephone number, drugie danie secondcourse or wycieczka autokarowa bus trip. The compari-son shows that in hospital documents there are very fewphrases that are frequently used in the corpus of generalPolish. Moreover, the common phrases are usually relatedto medicine. So, this stage turned out not to substantiallyinfluence the results.Finally we ordered the terms according to the C1/NCmethod. Tables 6 and 7 shows the leading terms for bothdata sets.To check if the changes introduced by the NC correc-tion method were significant we used the top 300 as aset of terms whose contexts were taken into considera-tion while calculating the NC coefficient. Unfortunately,clinical notes mostly contain noun phrases and a lot ofterms just have punctuationmarks as their contexts. Thus,reordering phrases according to the NC values did notintroduce many changes. In fact, most corrections onlycaused a difference of no more than 20 places. The biggerdifferences were seen only at the bottom of the list wherethey are not very important, as usually, the end of the listis not taken into account as a source of domain terms. Thepossible explanation of this minor positive effect is the rel-atively small size of the available data, as some phrasesfrom this 300 element list occurred little more than 15times.Manual evaluationWe performed two tests to evaluate the results of theextraction procedure. The first test was aimed at checkingthe completeness of the initial list of all considered nom-inal phrases. It involved the manual identification of ter-minology in documents and checking how many of theseterms were present in the full list of terms before truncat-ing it. The o1 documents were approximately two timeslonger, so we randomly selected two (1667 tokens) andfour (2074 tokens) documents for the evaluation respec-tively. The test was performed by two annotators. Theresults are given in Tables 8 and 9. As is evident from theinformation in the tables, about 85% of phrases indicatedby the annotators are common for both of them. The listsof extracted terms contain above 80% of phrases indicatedby the annotators.Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 6 of 14http://www.jbiomedsem.com/content/5/1/24Table 6 Top 20 phrases in o1 dataPhrase C1/NC Full Nestedkarta informacyjna leczeniaszpitalnego hospital treatment185.60 116 0information cardmorfologia krwi full bloodcount124.00 155 4wynik badania examinationresult114.04 118 27masa cia?a body mass 107.82 122 17stan ogólny generalcondition102.66 75 62uk?ad kielichowo-miedniczkowyposzerzony widenedpyelocalyceal system102.17 55 0pediatria ogólna generalpaediatrics93.60 117 0oddzia? alergologii allergyward93.60 117 0kod pacjenta patient code 92.80 116 0USG jamy brzusznej ultrasoundof the abdominal cavity92.14 66 10lekarz prowadza?cy attendingphysician91.28 114 0ordynator oddzia?u head ofhospital department91.28 114 0badanie ogólne generalexamination79.51 93 9RTG klatki piersiowej chestX-ray78.14 52 12nerka prawid?owej wielkos´cikidney of normal size74.81 59 0pe?cherzyk z?ó?ciowy prawid?owynormal gall bladder73.54 58 0uk?ad kielichowo-miedniczkowypyelocalyceal system69.35 4 59pe?cherz moczowy wype?nionyfilled bladder62.56 42 11klatka piersiowa chest 58.80 1 87badanie examination 55.20 35 665The second test indicated how many medical phraseswere at the top, in the middle and at the bottom of the listsof terms ordered from the highest to the lowest score oftheir C1/NC-value. The phrases were judged by the sametwo annotators, as to whether they belong to the termi-nology or not. The results of the evaluation are given inTables 10 and 11. In the top part of the lists, the greatmajority of terms (about 88%) is judged to be domainrelated by both annotators. The percentage of badly struc-tured terms is below 10%. The proportion of badly struc-Table 7 Top 20 phrases in surgical dataPhrase C1/NC Full Nestedkarta informacyjna leczeniaszpitalnego hospital treatment1862.40 1164 1information cardoddzia? chirurgiczno-urazowysurgical and casualty ward1332.80 833 0badanie ogólne generalexamination1030.95 1170 112wynik badania examinationresult964.56 1167 43oddzia? chirurgii surgical ward 943.26 1179 3kod pacjent patient code 931.20 1164 0zalecenie lekarskie medicalrecommendation924.80 1156 0zastosowane leczenie appliedtreatment735.22 919 1odp?yw pe?cherzowo-moczowodowyvesicoureteral reflux678.09 124 317pe?cherz moczowy bladder 662.48 325 525wskaz´nik protrombinowyprothrombin ratio609.60 762 1stan ogólny dobry good generalcondition526.40 414 0grupa krwi blood group 520.80 649 4USG jamy brzusznej ultrasound ofthe abdominal cavity511.34 377 51uk?ad kielichowo-miedniczkowypyelocalyceal system508.30 67 267karta informacyjna informationcard470.00 1 1173wsteczny odp?ywpe?cherzowo-moczowodowyvesicoureteral reflux468.70 238 14leczenie szpitalne hospitaltreatment466.40 0 1166stan ogólny general condition 430.81 222 422nerka prawid?owej wielkos´ci kidneyof normal size410.84 324 1tured terms in the other two sets is evidently higher whichproves that the C/NC ranking method moves bad termstoward the end of the list. However, as can be seen, eventhe last section of the list contains 6082% of domainterms.Table 8 Phrases in o1 texts1st annot. 2nd annot. Commonnb of phrases 241 235 208nb of extr. phr. 199 190 175% of extr. phr. 82.5 80.0 84.1Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 7 of 14http://www.jbiomedsem.com/content/5/1/24Table 9 Phrases in surgery texts1st annot. 2nd annot. Commonnb of phrases 163 164 138nb of extr. phr. 134 136 116% of extr. phr. 82.2 82.9 84.0Comparison with MeSHMeSH is a controlled biomedical vocabulary that was cre-ated to index articles from biomedical journals and tomake literature searches easier. Thus, for example, thedata contains the following terms: kidney and gallblad-der but does not contain the phrases: left kidney ornormal gallbladder which are used in hospital documen-tation but do not function as keywords in journal papers.Experiments in applying MeSH to clinical data were donefor English [14] and Swedish [15], UMLS resources wereused for information extraction in French [16,17], German[18], and Dutch [19]. A better source of data that containsclinical terminology is SNOMED but it is not translatedinto Polish. As there are no other publicly available elec-tronic resources of Polish medical terminology we com-pared the results obtained in the task with the terminologyrepresented in the Polish MeSH thesaurus. We performedthe experiment on the version available from http://www.nlm.nih.gov/mesh/ updated in 2012 which contains26581 main headings and 17638 synonyms. The data isbeing created in the GBL (Central Medical Library) inWarsaw.The extracted terms have simplified base forms whichcannot be directly compared with the thesaurus that con-tains terms in their nominative base form. There are threepossible solutions to this problem. The first one is to con-vert the terminology from simplified base forms into cor-rect grammatical phrases and check them in MeSH. Thesecond approach consists in converting MeSH data intosimplified base forms. The third approach is to comparethe simplified formswith data inMeSHusing approximatestring matching.We tested the first and the last method describedabove to perform a comparison of the top ranked sur-gical ward terminology with the MeSH thesaurus. WeTable 10 Phrases considered as terms in o1 documentsC1/NC - o11st annot. 2nd annot.Domain General Bad Domain General Badnb % nb % nb % nb % nb % nb %top200 176 88 19 9.5 5 2.5 178 89 14 7 8 4middle100 88 88 5 5.0 7 7.0 83 83 8 8 9 9end100 75 75 18 18.0 7 7.0 82 82 10 10 8 8Table 11 Phrases considered as terms in surgerydocumentsC1/NC - surgery1st annot. 2nd annot.Domain General Bad Domain General Badnb % nb % nb % nb % nb % nb %top400 353 88.3 28 7.0 19 4.7 348 87.0 27 6.7 25 6.3middle200 136 68.0 11 5.5 43 21.5 145 72.5 14 7.0 41 20.5end200 127 63.5 33 16.5 40 20.0 121 60.5 35 17.5 44 22.0wanted to test only medical terminology so we selected353 terms that underwent positive manual verificationby the first annotator. 52 terms (15%) are present in theMeSH thesaurus in their exact form, while 90 (25.5%)exact forms are nested in other terms. The method forapproximate string matching performed on the simpli-fied forms increased the number of recognised terms to106 (30%). 9 terms recognised by the method using exactforms were not recognised by the last method. Almostall these phrases contain gerunds whose lemma formsdiffer significantly from the words, e.g: leczenieger szpi-talneadj hospital treatment has a simplified base formleczyc´ szpitalny. Finally, we tested the approximate stringmatching method on the set of terms consisting of gram-matical phrases. In this case 119 (34%) terms gave positiveresults.The results presented in this paper are worse than theresults discussed in the paper [20]. In that experimentfrom 1987, manually extracted terminology from hospi-tal documents was compared with the English MeSH. Theauthors concluded that about 40% of these phrases werepresent in MeSH. The results we obtained are even worseand they show that the Polish MeSH is not large enoughfor the evaluation of clinical terminology extracted fromhospital documentation, so in this task it cannot serve asa source of normalised terminology.Results for simplified grammarFinally, we tested whether the precision of the extrac-tion grammar influences the results. We performed anexperiment in which we changed the grammar used forphrase identification in such a way that it relied onlyon information about part of speech and did not takeinto account gender, number and case agreement. Pol-ish taggers are not very reliable in assessing detailedvalues of morphological tags, especially for domain spe-cific text, while preparation of correction rules is timeconsuming. However, neglecting this information resultsin the extraction of many phrases that are syntacticallyincorrect. The experiment performed on the surgicaldata resulted in obtaining 13591 candidates (compared to11354). Although the results (see Table 12) obtained forMarciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 8 of 14http://www.jbiomedsem.com/content/5/1/24Table 12 Comparison of the results for different grammarsfor surgery documentsC1 - surgeryOriginal grammar Simplified grammarDomain General Bad Domain General Badnb % nb % nb % nb % nb % nb %top400 353 88.3 28 7.0 19 4.7 350 87.5 19 4.75 31 7.75next400 331 82.8 19 12.5 50 12.5 310 77.5 15 3.75 75 18.75the first 400 terms were good  87.5% of terms were clas-sified as domain related (in comparison to 88.3% obtainedwith the original grammar), but in the next 400 places thechanges were more significant: only 77.5% of the termswere domain related while 18.75% were badly structured(82.8% and 12.5% for the original grammar). These resultsconfirm the hypothesis that better initial selection of can-didates has a positive impact on the final results of thechosen method of terminology ranking.ConclusionsThe analysis of the results obtained in the automaticterminology extraction showed that the top part of theterminology list contains phrases that refer almost unex-ceptionally to the most frequent domain related conceptsdescribed in the data. The extracted terms may help tocreate a domain ontology and, most importantly, theyreflect the variety of phrases that are used in everydayhospital practice. The method can be useful for preparingterminological resources for very specific subdomains forwhich no relevant databases already exist.Clinical texts contain practically only domain specificknowledge and almost all correct phrases extracted by thegrammar are domain related. Thus, the standard methodof filtering the results by comparing the occurrences ofphrases to their frequencies in the general corpora can-not improve the results. As multiword expressions are lesslikely to be ambiguous for some domains, general datacan be used as an additional source of information aboutpossible contexts.The C-value approach turned out to be useful for rec-ognizing terms being subsequences of other phrases. Theperformed evaluation showed that none of the testedranking procedures were able to filter out all improp-erly constructed noun phrases from the top of the list, sothe processing stage consisting in choosing noun phrasesturned out to be very important to the usefulness of thecreated terminological resource.In particular, the comparison of the obtained resultswith manually extracted terminology from selected docu-ments showed that proper morphological tagging is veryimportant to the selected approach. The application of theNC part of the C/NC method to the clinical data does notsignificantly change the order of terms, so the NC step isnot very useful if the aim is to collect all possible domainrelated phrases, but can help in selecting those that aremost important in a particular domain.MethodsText characteristicsWe analysed two sets of data containing hospital dischargedocuments. They were collected from two wards of achildrens hospital. The first set of data consisted of 116documents (about 78,000 tokens) relating to patients withallergies and endocrine diseases. The second data set con-tained 1165 documents from a surgical ward (more than360,000 tokens). The documents were originally writtenin MS Word. They were converted into plain text filesto facilitate their linguistic analysis. During conversion,information serving identification purposes was substi-tuted with symbolic codes. The vocabulary of the clinicaldocuments is very specific, and significantly differs fromgeneral Polish texts. Inmedical data there are many abbre-viations and acronyms, some of them are in commonuse: RTG X-ray or godz (godzina) hour, but many ofthem are domain dependent. For example, por. in everydaylanguage means porównaj compare, but in the medicaldomain it is more often the abbreviation for poradniaclinic. Some abbreviations are created ad hoc, e.g., inthe phrase babka lancetowata ribwort plantain the wordlancetowata ribwort is abbreviated to lan or lanc. Theseabbreviations cannot be properly recognised out of con-text. Moreover, many diagnoses or treatments are writtenin Latin, e.g., immobilisatio gypsea immobilisation withgypsum.Another problem in analysing clinical data is misspelledwords. As the notes are not meant to be published, thetexts are not very well edited. Despite the spelling cor-rection tool being turned on, some errors still occurred,mainly in words missed from the standard editor dic-tionary like echogenicznos´ci echogenicity misspelled asechiogenicznos´ci, echogenicznosci and echogenicznos´a?ci.Grammatical errors are infrequent but most utterancesare just noun phrases, not complete sentences. Thus, ourobservations concerning the overall linguistic character-istics of Polish clinical data are consistent with thosedescribed by Kokkinakis and Thurin for Swedish [15].The first level of the linguistic analysis of data is itssegmentation into tokens. At this level we distinguish:words, numbers and special characters. Words and num-bers cannot contain any special characters. Words maycontain digits, but they do not start with digits. So,the string 12mm is divided into 2 tokens: 12numberand mmword, while the string B12 is treated as oneword.In the next step of data processing we annotated thedata with morphological information. Each word wasMarciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 9 of 14http://www.jbiomedsem.com/content/5/1/24assigned its base form, part of speech, and complete mor-phological characteristics. The annotation is done by theTaKIPI tagger [21] that cooperates with the MorfeuszSIAT morphological analyser [22] and the Guesser mod-ule [23] that suggests tags for words that are not in thedictionary.To correct Guessers suggestions and some systematictagging errors, we manually prepared a set of global cor-rection rules that work without context, see [24], so theywere only able to eliminate some errors, e.g. replace veryunlikely interpretations of homonyms. We also prepareda list of the most common abbreviations, which wereassigned the appropriate full form as their lemma. Finally,we (automatically) removed improperly recognised sen-tence endings after abbreviations, and added the end ofsentence tags at the ends of paragraphs.Phrase selectionIn this work we decided only to analyse nominal phrasesand put verbal constructions aside. The internal syntac-tic structure of nominal phrases that constitute termscan vary, but not all types of nominal phrases structuresare likely to characterise terminological items. In Polish,domain terms most frequently have one of the followingsyntactic structures: a single noun or an acronym, e.g. angiografiaangiography, RTG X-ray; a noun followed (or, more rarely, preceded) by anadjective, e.g. granulocytysubst oboje?tnoch?onneadjneutrofils, ostryadj dyz?ursubst emergency service; a sequence of a noun and another noun in genitive,e.g. biopsjasubst:nom tarczycysubst:gen biopsy of thyroid; a combination of the last two structures, e.g.gazometriasubst:nom krwisubst:gen te?tniczejadj:genarterial blood gasometry.The syntactic rules become more complicated as onewants to take additional features of Polish nominalphrases into account: word order: as Polish is a relatively free orderlanguage, order of phrase elements can vary; genitive phrase nesting: the sequences of genitivemodifiers can have more than two elements, e.g.wodonerczesubst:nom niewielkiegoadj:genstopniasubst:gen dolnegoadj:gen uk?adusubst:genpodwójnegoadj:gen nerkisubst:gen prawejadj:gen mildhydronephrosis of the duplicated lower collectingsystem of the right kidney; coordination: some terms include coordination (ofnoun or adjectival phrases), eg. USG naczyn´ szyjnychi kre?gowych ultrasound of the carotid and vertebralvessels, zapalenie mózgu i rdzenia inflammation ofbrain and medulla; prepositional phrases: there are also terms likewitaminy z grupy B vitamins of the B group whichinclude prepositional phrases inside.In our work we account for all of the nominal phrasetypes described above, except those including preposi-tional phrases and nominal coordination. To recognisethem, we defined a shallow grammar consisting of a cas-cade of six sets of rules being regular expressions. Therules operate on the data annotated with a part of speechand the values of morphological features. The resultsobtained by applying a set of rules on one level were usedas the input for the subsequent set. The rules are cited inTable 13 in a format slightly modified for this presenta-tion; in particular, this format does not include the outputTable 13 The sets of rules for recognizing noun phrasesSet RulesI N subst | gerNC (foreign_subst | foreign) +foreign?+foreign?NC brevnpun,nw| brevnpun,nwNC brevpun,nw + .? | brevpun,nw + .?NC brevnpun,nphr| brevnpun,nphrNC brevpun,nphr + .? | brevpun,nphr + .?AJ 2 adv?+(adjC,G,N| ppasC,G,N)AC brevadjw,npun| brevadjw,pun + .?CN iII A AJ+adv?A3 AC + - + AJC,G,NA3 adja + - + AJC,G,NAC AC + -+ACN NC,G,N+AJC,G,N| AJC,G,N+NC,G,NNZ subst(lemma=to/co/obra?b/kierunek/cel/czas/moz?liwos´c´/podstawa/cia?g/cecha/...)AZ IR(lemma=aktualny/daleki/gdy/pewien/wzgla?d/ten/inny/sam/niektóry/wczesny/...)III ADJP AADJP AC,G,N?+AC,G,N?+AC,G,N?+AC,G,N?+CN+AC,G,NADJP AC,G,N?+AC,G,N?+AC,G,N?+AC,G,N+AC?IV NB2 NC+ADJPNB2 AC+NNB N+ACNB ADJPC,G,N+NCNB ADJPC,G,N?+NC,G,N+ADJPC,G,N?V NG NBgen?+NBgen?+NBgen?+NBgen?+CN?+NBgenNG NBVI X NG+NGgen?+ADJPC,G,N?X NG+NCMarciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 10 of 14http://www.jbiomedsem.com/content/5/1/24part of the rules. Indexes describe values of morphologicalfeatures. Names in lowercase correspond to the respectivefeature values, capitalized names correspond to variablesreferring to case (C, C2), gender (G, G2) or number(N, N2).The Polish tagset is quite detailed (over 1000 actuallyused tags) and contains around 30 word classes. This set,for our purposes, was extended by the foreign tag usedfor Latin or English words used in discharge summaries.Words which can build up a nominal phrase can be fromone of the following categories: subst (noun), ger (gerund),foreign_subst, foreign, and brev:pun:nw, brev:pun:nphr,brev:npun:nw, brev:npun:nphr (abbreviation/acronym of anoun or noun phrase requiring or not requiring a periodafterwards). The first two types of these core elementsinflect and they are assigned to the N class. Foreign wordsand abbreviations do not inflect but they can also be mod-ified by adjectives. These words cannot be a source ofgender, number or case values and are assigned the cate-gory NC. Foreign names frequently consist of more thanone element, so sequences of up to three foreign wordsare also accepted by the grammar (we do not analysethe internal structure of Latin or English sequences). Thefirst set of rules also includes rules for identifying basicadjectivesinflective (AJ) and non-inflective (AC) whichcan possibly be modified by adverbs. The X notation isused to mark cases in which the morphological descrip-tion of the resulting phrase should be copied from the Xthelement of the rule and not from the first one (e.g. case,gender and number of an adjective phrase consisting of anadverb and an adjective should be the same as those of theadjective).In the second set of rules, adverbs can be attached toadjectives which are in front of them (but only if there isno adjective after themthis more preferable attachmentis covered by the first set of rules). There are also rules forspecial types of Polish complex adjectivesconstructionslike pe?cherzowo-moczowodowy vesico-ureteric contain-ing a special form of an adjective ending with -o followedby a hyphen and an adjective. The last two rules of the sec-ond set are defined specially for the procedure of nestedphrases borders identification procedure (special rulesare responsible for not constructing nested phrases whichinclude adjectives but do not include the nouns theymodify).The third set of rules describes compound adjecti-val phrases, the fourth one combines adjectival phraseswith nouns, the fifth describes sequences of genitivemodifiers, and the last one combines genitive modifiersand optional adjectival modifiers which can occur aftergenitive ones. There is also a rule which allows for anon-inflective noun as a last phrase element. This ruleaccounts for acronyms used at the end of noun phrases,but it turned out that due to the lack of punctuationit was responsible for recognising improperly structuredphrases.Applying such a general set of rules to our data wouldresult in a subset of phrases which we considered non-domain terms. These were phrases beginning with mod-ifiers describing that a concept represented by a subse-quent nested phrase was occurring, desired or expected,e.g. (w) trakciesubst choroby during illness. To eliminatesuch phrases we defined a set of words which were to beignored during phrase construction. Rules for recognis-ing them (and assigning NZ or AZ category) were addedto the first set. These words belong to the following threeclasses: general time or duration specification, e.g. czas time,miesia?c month; names of months, weekdays; introductory/intension specific words, e.g. kierunekdirection, cel goal, podstawa base, cecha feature(22 words more).In the results presented in this paper, only some typesof normalisation of the extracted terms described in [6]are completed. We recognise morphological variants ofterms. Domain abbreviations and acronyms that have aunique interpretation were extended and thus matchedwith their full versions. This cannot always be done ina straightforward manner, as there are many abbrevi-ations/acronyms that can be correctly interpreted onlyin context. Moreover, discharge documents do not con-tain definitions of abbreviations or acronyms, and manyacronyms are created from English phrases (e.g. MCVMean Corpuscular Volume) so it is impossible to adaptthe method proposed in [25] for acronym recognition,which was based on analysing acronym definitions.Identification of nested phrases and term weightingIn order to apply the C-value method, the operationof identifying phrases nested within other phrases iscrucial. In our solution, borders of nested phrases areintroduced by the grammar. As a nested phrase we takeevery fragment of a nominal phrase which is recog-nised by any of the grammar rules as being a nounphrase itself. For example, pe?cherzyksubst z?ó?ciowyadj gallbladder usually occurs with an adjective describing itscondition e.g, pe?cherzyksubst z?ó?ciowyadj prawid?owyadjnormal gall bladder, or kos´c´subst ramiennaadj humerusoccurs with information indicating the left or rightside. Recognising the first exemplary phrase resultsin identifying two candidates: pe?cherzyksubst z?ó?ciowyadjprawid?owyadj and pe?cherzyksubst z?ó?ciowyadj but notz?ó?ciowyadj prawid?owyadj as this is not a noun phrase.The original work in which the C/NC method was pro-posed concerned Englisha language with little inflec-tion and a rather stable noun phrase structure. Thus,Marciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 11 of 14http://www.jbiomedsem.com/content/5/1/24the authors did not have to pay a lot of attention todefining how they compared phrases and counted thenumber of different contexts. They compared word forms.However, for highly inflectional languages, like Polish, dif-ferent forms of a word can vary significantly, making adecision on term equality harder. Because of this, find-ing repeated nested phrases also cannot be done by justmatching the strings. For example, the following nomi-nal phrase in the nominative (which is traditionally con-sidered a basic form): zakaz?eniesubst:gen wirusemsubst:datgrypysubst:gen influenza virus infection is written in thegenitive as: zakaz?eniasubst:gen wirusemsubst:dat grypysubst:geninfluenza virus infection. In this latter phrase we oughtto recognise the term zakaz?enie wirusem grypy and threenested phrases: wirus grypy, wirus and grypa. None ofthem directly matches the considered phrase. The firstone matches the basic (nominative) form, but the nomi-native form of the nested phrases does not match eitherthe genitive or nominative form of the entire phrase. Thisproves that lemmatisation of the entire phrase does notsolve the problem.To overcome this difficulty we decided to transformthe identified phrases into simplified base forms, beingsequences of lemmas of phrase elements. In the citedexample, such a simplified lemma is: zakaz?enie wirusgrypa infection virus influenza. In this sequence all theabove nested terms (converted into their simplified baseforms) can be found easily.Our approach is much simpler and more robust than aformally correct one. It allows not only for easier recog-nition of nested phrases but also helps in cases whereestablishing a correct basic form can be difficult for shal-low rules. For example, the correct lemma for the phraseokresowegogen badaniagen ogólnegogen moczugen should beokresowe badanie ogólne moczu periodic general exam-ination of urine (periodic urinalysis) but could possiblyalso (syntactically) be okresowe badanie ogólnego moczuperiodic examination of general urine. Introducing arti-ficial base forms we avoid this difficulty. Simplified baseforms allow us also to join phrases with various abbrevi-ations of the same word like babka lan and babka lancwith their full formbabka lancetowata ribwort plan-tain (from patch tests). As proper lemmatisation of allphrases is also prone to tagging errors, our approach ismuch easier and more robust than a formally correct one.The lemmatisation approach explained above meansthat sometimes semantically different phrases have thesame simplified base forms.This may happen due to: phrases with genitive modifiers occurring in differentnumbers e.g. zapalenie ucha ear inflammation andzapalenie uszu ears inflammation are bothconverted into the singular; the adjectives in different degrees (small, smaller)having the same base forms, e.g. miednica ma?a smallpelvis (more frequently written as ma?a miednicawhere ma?a small refers to its size) and miednicamniejsza (mniejsza smaller indicates anatomic part)lower pelvis; negated and positive forms of adjectival participles,e.g. powie?kszony/niepowie?kszony increased/notincreased, both have the lemma powie?kszyc´ infincrease. gerunds and participles having infinitives as theirbase forms, so e.g.: phrases usunie?ciegerkamieniasubst:gen removing stone (an operation) andusunie?typpas kamien´subst:nom removed stone(description of the stone) have the same simplifiedbase form usuna?c´inf kamien´subst .After normalisation of the recognised phrases consist-ing in their transformation into simplified forms we haveto decide on a way of differentiating contexts. The C-valuecoefficient greatly depends on the way for counting thenumber of different contexts in which a nested phraseoccurs. In comparison to [9], we introduced slight modifi-cations to the way of computing this number. In the orig-inal solution all different sequences consisting of differentinitial words and different final words were counted. Forexample, if we consider a set of four terms: powie?kszenie [we?z?ów ch?onnych] lymph nodesenlargement powie?kszenie [we?z?ów ch?onnych] krezkowychmesenteric lymph nodes enlargement znaczne powie?kszenie [we?z?ów ch?onnych]significant lymph nodes enlargement powie?kszenie [we?z?ów ch?onnych] szyji neck lymphnodes enlargementthe number of context types for we?z?ówsubst:pl:gench?onnychadj:pl:gen lymph nodes would be four. But thismethod of context counting obscures the fact that theclose context of we?z?ów ch?onnych does not change thatmuch. To account for this phenomenon, one may countonly the one word context of any nested phrase.While choosing this option one has still many pos-sibilities to combine right and left contexts. We testedthree approaches: the first one was to count pairs of leftand right full contexts combined together; in the secondapproach we counted different words in both left andright contexts grouped together. However, the best resultswere obtained for the third option in which we took themaximum from different left and right words contextscounted separately. So, in the above example, the left con-text is empty as the same word powie?kszenie enlargementappears in all phrases. This version is called C1. For ourMarciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 12 of 14http://www.jbiomedsem.com/content/5/1/24example the number of different contexts calculated usingthese methods would be accordingly:4: powie?kszenie, powie?kszenie-krezkowych,znaczne-powie?kszenie, powie?kszenie-szyji ;3: powie?kszenie, krezkowych, szyji ;2: krezkowych, szyji.We counted the C-value for all phrases including thoseof length 1. However, we set l(p) in the equation (1) to 0.1not to 1 like [10]. We observed that although one wordterms constituted only 19% of the first 1000 terms in theo1 data, while on the entire list there were 33% of them(14% and 19% respectively for surgical data), many of theone word terms occurred only once (34% and 37% respec-tively). Setting l(p) for one word phrases to 1 result in 46%of the first 1000 terms to be of length 1.For the results obtained using the C1 coefficient, weapplied the full C/NC method to take the external termscontext into account. For calculating the NC coefficientwe used one word contexts which were adjectives, nounsand verbs which occurred immediately before or imme-diately after any term which was in the top 300 positionsaccording to its C-value coefficient.Depending on the goal, requiring the imposition ofgreater stress on the recall or precision of the results, thesmaller or larger top part of the list ordered by the NCvalue can be taken as a resulting terminology resource.Manual evaluationThe manual evaluation was performed by two annotators:one was a paediatrician specialising in allergology and pul-munology, the secondwas involved in the experiment, hada computer background and had experience in linguisticand medical data processing.The two annotators were only given very generalinstructions to mark a phrase which they thought of asbeing important in clinical data and which did not includeprepositions. The basic problem of this task was to decidewhat kind of phrases constituted terminology. Sometimesonly the boundaries of the phrase indicated by the annota-tors were different, e.g: in the phrase na ca?ym ciele on thewhole body only cia?o body was recognised by the firstannotator, while the second annotator included the wordca?e whole. Moreover, both annotators had a tendencyto indicate phrases that contained coordinations of nounswhich were not covered by the grammar, e.g:Wyniki pod-stawowych badan´ morfotycznych i biochemicznych krwi imoczu The results of basic morphotic and biochemicalblood and urine examinations. The first annotator recog-nised 42 terms in the o1 data that were absent from theautomatically prepared list for the following reasons: lackof grammar rules recognising the coordination of nom-inal phrases  6 errors; lack of other grammar rules 8; tagging errors  11; problems with rules containingabbreviations and their tagging  10; phrases contain-ing time expressions and introductory/intension specificwords (e.g: week, goal, direction)  6.For the second evaluation experiment for the o1 datawe took the top 200 terms, and randomly selected 100terms from the middle of the list (C1/NC-value ? (1.0,2.5 ?) and 100 from the bottom part of the list (C1/NC-value ? ?0.0, 1.0?). For surgery data we evaluated the 400topmost terms and 200 terms from the middle and bot-tom part of the lists. Then, the phrases were judged bythe same two annotators, as to whether they belonged tothe terminology or not. Not all phrases from the top partof the lists were classified as terms. Despite attempts toeliminate semantically odd phrases like USG jamy USGof cavity and infekcja dróg infection of tract (only in theo1 data) they still appear in the top part of the lists asthey are often in the data and cavity and tract are partof several well established phrases. Another problem wascaused by abbreviations attached to correct phrases likeuraz g?owy S head injury S where S is a part of the ICD-10 code of the illness S00 written with a space betweenS and 00. Our grammar does not exclude such contrac-tions as it is possible that an abbreviation is at the end ofa phrase, e.g: kontrolne badanie USG control ultrasoundexamination.Comparison of simplified terms with MeSHBelow we describe three possible solutions for comparingour list of simplified base forms of terms with terminol-ogy in MeSH that contains correctly structured nominalphrases in the nominative case. We applied the first andthe last method of term forms matching as describedbelow.The first one is to convert the terminology from sim-plified base forms into correct grammatical phrases andcheck them in MeSH. We have to take into accountthat the general Polish morphological dictionary does notrecognise about 18.8% of word-tokens in clinical data,see [24]. In general, the automatic generation of correctbase forms from simplified ones is error prone, but theconstruction of medical phrases is more restricted thanfor literary language so the results are better. We per-formed this task with the help of phrases extracted fromclinical data, in which we identified fragments that are sta-ble like genitive complements. This solution significantlydecreases the role of unknown words. For example in thephrase wirussubst:sg:nom Epsteinasubst:sg:gen-Baarsubst:sg:genEpstein-Barr virus the part Epsteinasubst:gen-Baarsubst:genhas the same form in all inflected forms of the wholephrase. So it is possible to copy this part from the phraseextracted from the data. We have to take into accountthat some of the terminology in Polish MeSH is nom-inal phrases in the plural, e.g. the above phrase is inMarciniak and Mykowiecka Journal of Biomedical Semantics 2014, 5:24 Page 13 of 14http://www.jbiomedsem.com/content/5/1/24plural form in MeSH:Wirusysubst:pl:nom Epsteinasubst:sg:gen-Baarsubst:sg:gen Epstein-Barr viruses. This problem can beovercome by generating both singular and plural forms.This will account for medical plurale tantum phrases likedrogisubst:pl:nom moczoweadj:pl:nom urinary tract that noware improperly lemmatised to a phrase in the singulardrogasubst:sg:nom moczowaadj:sg:nom.We converted the selected 353 terms into their correctbase forms. For the following 11 terms, their base formswere corrected manually as they were unknown to themorphological dictionary and should be inflected: uro-dynamiczny urodynamic, przype?cherzowy paravesical,detromycynowy chloramphenicol and podpe?cherzowybladder outlet and compound words pe?cherzowo-moczowy vesicoureteral (4 terms) and miedniczkowo-moczowodowy pelvi-ureteric (3 terms).The second approach consists in converting MeSH datainto simplified base forms. This method also has disad-vantages as 42% of words contained in MeSH are notrepresented in the general Polish dictionary that we usedfor the annotation of our data and which was used toannotate the NKJP corpus [13]. Converting MeSH termi-nology into simplified base forms does not solve all prob-lems either. For example, Polish MeSH does not containthe phrase: chirurgiasubst naczyniowaadj vascular surgerybut it contains zabiegisubst chirurgiczneadj naczynioweadjvascular surgery operations. The English equivalent ofthe last phrase contains the first phrase but this is nottrue of the Polish version. The simplified form of thefirst phrase chirurgia naczyniowy is not contained in thesimplified version of the last phrase zabieg chirurgicznynaczyniowy as the strings chirurgia and chirurgiczny aredifferent.The third approach is to compare the simplified formswith data in MeSH using approximate string matching.To apply this method we perform a sort of stemming byremoving suffixes indicating cases of nouns and adjec-tives. Then we apply the Levenshtein distance measurewhich takes into account the position of a non-matchingletter in the analysed word. Words are more similar if dif-ferences are found nearer to the end of the word thanto the beginning. For each word from a phrase in ques-tion we find a set of similar words. Then we look forMeSH terms that contain one similar word for each phraseelement.Abbreviationsadj: Adjective; brev: Abbreviation; ICD: International Classification of Diseases;gen: Genitive; ger: Gerund; MeSH: Medical Subject Headings; NKJP: NationalCorpus of Polish; nom: Nominative; nphr: Noun phrase; npun: No punctuation;nw: Noun word; pl: Plural; pun: Punctuation; POS: Part of Speech; sg: Singular;SNOMED: Systematized Nomenclature of Medicine; subst: Substantive; UMLS:Unified Medical Language.Competing interestsThe authors declare that they have no competing interests.Authors contributionsMM performed data pre-processing, developed the evaluation scheme of theresults, and scrutinised the entire experiment. AM defined the shallowgrammar, implemented C/NC methods, and took part in the evaluation. Thewhole paper was written and corrected by both authors. Both authors readand approved the final manuscript.AcknowledgementsThe research was supported partially by the POIG.01.01.02-14-013/09 projectwhich is co-financed by the European Union under the European RegionalDevelopment Fund. The authors would like to thank Beata Gosk (pediatrician)for consultations and taking part in the evaluation of the terminology, andPiotr Rychlik for implementing the terminology comparison functions and theLevenshtein distance measure to the Polish MeSH. This article has beenpublished as part of the Semantic Mining of Languages in Biology andMedicine (SMLBM) thematic series of the Journal of Biomedical Semantics. Aninitial version of the article was presented at the 5th International Symposiumon Semantic Mining in Biomedicine.Received: 1 July 2013 Accepted: 13 May 2014Published: 31 May 2014JOURNAL OFBIOMEDICAL SEMANTICSRöhl and Jansen Journal of Biomedical Semantics 2014, 5:27http://www.jbiomedsem.com/content/5/1/27RESEARCH Open AccessWhy functions are not special dispositions:an improved classification of realizables fortop-level ontologiesJohannes Röhl1 and Ludger Jansen2*AbstractBackground: The concept of function is central to both biology and technology, but neither in philosophy nor informal ontology is there a generally accepted theory of functions. In particular, there is no consensus how toinclude functions into a top-level ontology or whether to include them at all.Methods: We first review current conceptions of functions in philosophy and formal ontology and evaluate themagainst a set of criteria. These evaluation criteria are derived from a synopsis of theoretical and practicalrequirements that have been suggested for formal accounts of functions. In a second step, we elucidate inparticular the relation between functions and dispositions.Results: We argue that functions should not be taken as a subtype of dispositions. The strongest reason for this isthat any view that identifies functions with certain dispositions cannot account for malfunctioning, which is havinga function but lacking the matching disposition. As a result, we suggest a cross-classification of realizables withdispositions supervening on the physical structure of their bearer, whereas both functions and roles also have someexternal grounding. While bearers can survive the gain, loss and change of roles, functions are rigid properties thatare essentially connected to their particular bearers. Therefore, Function should not be regarded as a subtype ofDisposition; rather, the classes of functions and dispositions are disjoint siblings of Realizable.Keywords: Function, Disposition, Role, Process, Realizable, Artefacts, Top-level ontology, BFOBackgroundThe ascription of functions is central to biology as wellas to psychology, technology and engineering. However,realizable entities like functions, dispositions and rolesare notoriously difficult to understand and there is noconsensus how to model them within a top-level ontol-ogy. The more general debates in the philosophy ofbiology and technology also offer several theories offunction with their respective advantages and shortcom-ings. Because of the diversity, plurality and ambiguity offunction concepts in, e.g., engineering, some authorshave claimed that there are many different function con-cepts which are only connected by family resemblances[1]. Even if this is true, it will nevertheless be useful for therepresentation of scientific statements about functions to* Correspondence: ludger.jansen@uni-muenster.deEqual contributors2Philosophisches Seminar, Universität Münster, 48143 Münster, GermanyFull list of author information is available at the end of the article© 2014 Röhl and Jansen; licensee BioMed CenCreative Commons Attribution License (http:/distribution, and reproduction in any mediumDomain Dedication waiver (http://creativecomarticle, unless otherwise stated.focus on some of the more important uses of the word andfix these within a formal ontological framework. One par-ticular challenge is whether there can be an overarchingmeaning of the term function both for biological and arte-factual functions.Another challenge we will address concerns the rela-tionships between functions and other kinds of entitieslike dispositions and roles in formal ontology. That theserelations are not at all clear is witnessed by BFO, theBasic Formal Ontology [2,3]: BFO versions up to 1.1.1contain the categories Disposition, Function and Role asjointly exhaustive and pairwise disjoint children of thecategory Realizable, but in the transition to the newversion BFO 2 it is planned to position Function as asubtype of Disposition (cf. Table 1 for more details).In this paper, we will try to meet these challenges. Forthis purpose, we will review different philosophical the-ories of functions and evaluate them with respect to aset of desiderata for function theories. In the remaindertral Ltd. This is an Open Access article distributed under the terms of the/creativecommons.org/licenses/by/4.0), which permits unrestricted use,, provided the original work is properly credited. The Creative Commons Publicmons.org/publicdomain/zero/1.0/) applies to the data made available in thisTable 1 Definitions of the children of realizables in BFO1.1.1 and 2 (Graz release)Definition in BFO 1.1.1 [2] Definition in BFO 2 [9]Disposition = A realizable entitythat essentially causes a specificprocess or transformation in theobject in which it inheres, underspecific circumstances and inconjunction with the laws ofnature. A general formula fordispositions is: X (object) has thedisposition D to (transform, initiatea process) R under conditions C.b is a disposition means: b is arealizable entity & bs bearer issome material entity & b is suchthat if it ceases to exist, then itsbearer is physically changed, &bs realization occurs when andbecause this bearer is in somespecial physical circumstances,& this realization occurs in virtueof the bearers physical make-up.Function = A realizable entity themanifestation of which is anessentially end-directed activity of acontinuant entity in virtue of thatcontinuant entity being a specifickind of entity in the kind or kindsof contexts that it is made for.A function is a disposition thatexists in virtue of the bearersphysical make-up and this physicalmake-up is something the bearerpossesses because it came intobeing, either through evolution(in the case of natural biologicalentities) or through intentionaldesign (in the case of artefacts),in order to realize processes of acertain sort.Role = A realizable entity themanifestation of which bringsabout some result or end that isnot essential to a continuant invirtue of the kind of thing that itis but that can be served orparticipated in by that kind ofcontinuant in some kinds ofnatural, social or institutionalcontexts.b is a role means: b is a realizableentity & b exists because thereis some single bearer that is insome special physical, social, orinstitutional set of circumstancesin which this bearer does nothave to be & b is not such that,if it ceases to exist, then thephysical make-up of the beareris thereby changed.Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 2 of 16http://www.jbiomedsem.com/content/5/1/27of this section, we will give some background informa-tion on the current state-of-the-art representation offunctions, reviewing work on this topic published by theresearch groups that developed the top-level ontologiesBFO, DOLCE and GFO [2,4,5].Functions as realizables in BFOIn the older versions of BFO, Function, Disposition and Roleare sibling subclasses of the class Realizable dependentcontinuant [6]. This common superclass implies that in-stances of any of these three classes share the followingcharacteristics: They are continuants, i.e. they are wholly present atevery time of their existence. Like qualities, they are (specifically) ontologicallydependent on an independent continuant (somematerial thing or system) that is their bearer. They are realizable, i.e. they are by definitionconnected to certain types of processes such thatinstances of such a process type can be realizationsof the realizable entity in question. When they are realized, their bearers are participantsof their realization processes, i.e. of the processes theyare roles, dispositions or functions for.Note that realizables do not need to be (always orever) realized [7], as, e.g. in the case of a safety mechan-ism, the function of which will only be realized if certainconditions obtain (and they may never obtain). Also notethat several other types of realizables are conceivablelike, e.g., propensities, tendencies, abilities, capacities,virtues and vices [8]. Arp and Smith [6] conceptualisethe specific differences between functions, roles and dis-positions as follows (cf. Table 1): The realizations offunctions and dispositions take place in virtue of thebearers physical makeup, whereas a role is optional:It does not reflect the intrinsic structure of their bearerbut a natural, social or institutional set of circum-stances. Functions are distinguished from dispositionsby the additional condition that the function bearer pos-sesses the physical structure that grounds the functionbecause of how it came to be there in the first place: Inthe case of artefactual functions by intentional designand production or in the case of biological functions bya history of evolutionary selection. In BFO 2 the relationof functions and dispositions was revised: functions arenow a subclass of dispositions [9], as detailed in Table 1.Although we sympathize with BFO as a top-level ontol-ogy and will later on use its other categories, especiallythe fundamental disjoint classes of continuants andoccurrents and their relations, we find that its treatmentof functions is in need of some improvement and clarifi-cation and we will later suggest a way to do so.Functions and flows in DOLCEWhile the top-level ontology DOLCE [4] neither in-cludes functions nor dispositions or roles in its core ver-sion, there are several suggestions for a formalisation ofengineering functions within the DOLCE framework[10-12]. However, their formalisation starts from a veryspecific technical approach that focuses on flows ofmaterials, energy or signals. Functions are then, basic-ally, what relates certain input and output flows. Whilebeing useful in engineering, this approach is mostly or-thogonal to the debates on the functions of biologicalentities. One main goal of these authors seems to be theintegration of the flows in the DOLCE ontology andthey accordingly classify different sorts of flow as varioustypes of process-like entities in DOLCE (states, pro-cesses etc.) and characterise their dependencies andrelationships with the continuants involved (which arecalled endurants in DOLCE). The relationships be-tween functions, dispositions and roles that concern usin the present paper are not discussed. We will not dis-tinguish between different types of processes as inputsor outputs for functions for two reasons: First, DOLCEssubtypes of processes are heavily description-relative and,hence, linguistic artefacts. Second, this model is not quiteas natural for biology as for applications in engineering. WeRöhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 3 of 16http://www.jbiomedsem.com/content/5/1/27can state that the function of the eye is to see without beingable to specify input and output flows. The point is not thatone could not conceptualise seeing in terms of flows ofvisual and neurological signals, but that it is not necessaryto understand the function of biological organs in theseterms. Consequently, analyses of functions in the philoso-phy of biology do not normally refer to input and outputflows (though it can be argued that such ideas are highlyrelevant to certain strands of psychology or functional the-ories of the mind). Let us go through an example: Thefunction of an animals legs is locomotion, so the actuallocomotive process of the animal is the realization of thefunction and this motion could also be called an output.But what is the input? Is it some neurological signal frominside the animal or some stimulus from the environment?Or is it a flow or change of energy in some cell or neuralpathway? In order to understand that the function of legs islocomotion, the question about the input seems to be ir-relevant. In any case, such a fine-grained approach with re-spect to input or output flows does not add anything to themain question of our paper. We take up this point again inthe Methods section.Work from the GFO-GroupAn elaborate and somewhat complicated model of func-tions has been developed by researchers from theOntoMed group that has produced the General FormalOntology (GFO) [13,14]. According to GFO, like inBFO, functions have realizations which are usually pro-cesses, but unlike in BFO, these realizations could alsobe continuant entities. Burek et al. characterise a func-tion by three function determinants, in particular:(1) the preconditions for the realization of thisfunction,(2) the goal affected or brought about by the function(or its realization, respectively),(3) the functional item which corresponds to the roleof the material bearer of the function as theparticipant in the realization of the function.In addition to functions and realizations, they intro-duce realizers. A realizer is the actual particular entitythat plays the role of the functional item. Furthermore,they distinguish the realization of a function (usually aprocess) from the goal which is a state of the worldthat is reached by means of the realization starting fromthe precondition requirements. The authors illustratethese components using the example of oxygen trans-port in the human body. The realization of this functionis an actual process of oxygen transportation and it hasas precondition the presence of some oxygen at locationA and as goal state the presence of some oxygen atlocation B. The functional item is denoted by thenominalised role term oxygen transporter and in thecase of oxygen transports in the human body this roleis played by instances of red blood cells, which are,thereby, realizers of this function. Thus, according tothis approach, roles are, to some extent, always in-volved when we deal with functions and roles are expli-citly dependent on role contexts. However, note thatthese roles are different from BFO roles and shouldnot be confused with them.Burek et al. also distinguish dispositional functionfrom actual function where the latter applies only tofunctions that are actually realized. This seems to be re-dundant as it ignores that this modal feature is capturedalready by the distinction between functions as realiz-ables and their realization processes. Despite its elabor-ate apparatus, this approach does not help much todistinguish functions from roles and dispositions be-cause all three terms are used to characterise functions.Elsewhere Burek seems to conceive of functions as asubclass of dispositions, stating that only the dispositionsfor effects of an item which are related to some (pre-)defined system of functions and goals are its functions[14]. This statement could be taken to mean that func-tions should be understood as a special kind of dispos-ition. In any case, it seems that intentions of agents (likedesigners and users of an artefact) choose the functionsof an item and functions are characterised as intentionalentities ([14] definition 67, p.157) and, therefore, agent-dependent (or community-dependent) and subjective.Altogether, this account seems to be better suited totechnical functions. Still, many of its features are per-fectly compatible with the BFO account, which we try toimprove upon. The central structure of functions ashaving bearers and being realized in processes with thebearers as participants is very similar. BFO refrains fromsome subtleties like the explicit consideration of thepreconditions for the realization of a function or the dis-tinction between a goal-state and the realization as aprocess leading to a goal state. A possible reason forabstracting from these distinctions is the actual talkabout functions in much of biology (and the philosophyof biology) where the process of blood-pumping as arealization of the hearts function is normally not con-trasted with a distinct goal state like, e.g., the distribu-tion of nutrients and oxygen in the body achieved by theblood-pumping. For many organic functions, the pre-conditions for realization are often implicitly presup-posed. For example, the function of the heart is to pumpblood and in order to realize this function it has to bepart of a living body, be connected to arteries and veins,and enough blood to be pumped has to be present. Allof these are, of course, features of a physiological organ-ism. Biologists do describe these from the point of viewof the whole organism and not for each organ separately.Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 4 of 16http://www.jbiomedsem.com/content/5/1/27To spell these conditions out would be a tedious andpossibly endless endeavour: In order to play a part inlocomotion from A to B, legs must be part of an organ-ism situated at A, but not at B, there must not be a hin-drance between A and B, it must be possible to belocated at B, etc. All this is true, but of no specific inter-est to the biologist. To conclude, the approach by Bureket al. seems to be compatible with the BFO approach inits main features, but does not help much with the prob-lems of distinguishing and relating functions, roles anddispositions.MethodsWe will proceed in two steps in this paper. First, we surveyphilosophical theories of functions and evaluate themagainst a set of requirements. The theories evaluated aremostly theories of biological functions, but we also look atsome theories for artefact functions. In the discussion sec-tion we take up the results of this survey and take a closerlook at the relation between functions and dispositions.The requirements used as evaluation criteria will bedetailed in this section. In the literature, several lists ofrequirements or criteria for a theory of functions havebeen suggested. As will become clear, we will not use allthese criteria, but need to pick out a coherent subset ofcriteria that we will use in this paper.We extract from Artiga [15] for biological and fromHoukes and Vermaas [16] for artefactual functions thefollowing list of adequacy criteria for function theories(somewhat adapting them to our own terminology):(1) Teleology: The function should have a central rolein the explanation of the existence of the functionbearer [15].(2) Restriction: Proper or essential functions of a thingcan be distinguished from its accidental functionsor transient effects [15,16].(3) a. Normativity: The performance of a function canbe evaluated as better or worse according to anorm given (at least implicitly) by the functionascription [15].b. Malfunctioning: A thing can have a function,although it fails to perform according to thisfunction occasionally or even permanently. Failure toproperly perform according to ones function can be acase of malfunctioning or of non-functioning [16].(4) a. Avoidance of epiphenomenalism: Functionsshould be determined by current performance of itsbearer, not mainly by causally inert historical factslike its (evolutionary or cultural) history or a mereascription by its producers, users, or observers [15].b. Support: The physical structure of the functionbearer supports its function, even in the cases ofaccidental functions or dysfunction [16].(5) Innovation: Novel functions can be ascribedcorrectly to innovative artefacts [16], but also tonewly evolved organisms.There are some tensions between these desiderata [17]which are also acknowledged by the authors who pro-posed them. Artiga argues that the normativity criterionand avoidance of epiphenomenalism cannot be satisfiedat the same time because (3) implies that function as-criptions should somehow be independent from theactual properties or activities of the function bearerwhile (4a) explicitly denies this [15]. Likewise, Houkesand Vermaas state a tension between the malfunctioncriterion (3b) and the support criterion (4b) for theirtheory of artefact functions [16]. They claim to havesolved this for artefactual functions. This solution isbased on, firstly, an optimistic view of the rationality ofdesigners, users and other ascribers of functions whowould not, or so they assume, assign unsupported func-tions and, secondly, on a history of maintenance and re-pair that makes a distinction between non-function andmalfunction easier. But it might well be that not all ofthe criteria can be satisfied simultaneously and thatsome may have to be relaxed in any theory of functions.Normativity is one of the most crucial, but also mostcontested desiderata for functions. To assert that some-thing has a function or that a function can be performedbetter or worse, we need some kind of normative dimen-sion. Franssen points out that, as a biological concept, itwould have to be naturalist and, thus, cannot be norma-tive in the full-blown moral sense [18], but for the nor-mative dimension of functions it should suffice that itaccounts for the fulfilling (or missing) of purpose in anaturalist way [19]. Franssen suggests that we couldunderstand the normativity of functions in a deflationaryway; that is, in terms of the rational expectations wehave with respect to the functions of organs or artefacts.However, this kind of normativity ascription is too weakbecause it lays no special claim to functions in particu-lar. We form these rational expectations also with re-spect to a dropped stone that is supposed to fall(thereby obeying a law of nature). Furthermore, Franssenargues that, in biology, functions are attributed morewidely than the associated normative evaluations [18].Many traits or behaviours of living beings can be de-scribed as having functions in the evolutionary sense, butare usually not evaluated in the way organs (as quasi-toolsof an organism) are. However, we can ignore these border-line cases and still acknowledge that we need the norma-tivity in the case of organs or more generally for allsubsystems of organisms to which functions are ascribedand which are evaluated with respect to their perform-ance, as it is clearly the case in medical contexts. Theseuse-cases show that we do need to deal with theRöhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 5 of 16http://www.jbiomedsem.com/content/5/1/27normative aspect and, therefore, we will retain this criter-ion at some expense of criterion (4).We supplement these criteria with some requirementsfor an ontology of functions proposed by Burek [14]:(6) Continuants: Functions should not be conflatedwith their realizations. While functions (andother realizables like dispositions and roles) arecontinuants, i.e. existing wholly at any pointof time during their existence, their realizationsare processes that stretch out in time.(7) Bridging of Domains: An ontology of functionsshould account for both artefacts (devices) andnon-artefacts.We regard it as an asset of a theory of function if itcan account for biological and engineered function asuniformly as is permissible given the differences betweenthe two domains. While (6) is fulfilled by all approachesin our survey, it is acted against by such a prominentsystem as the Gene Ontology [20], which does notclearly separate functions and activities because the termMolecular function is used both in the sense of an activ-ity and in the sense of a capability: Molecular functionis defined as the biochemical activity (including specificbinding to ligands or structures) of a gene product. Thisdefinition also applies to the capability that a gene prod-uct (or gene product complex) carries as a potential.[21] That functions themselves can sometimes beprocesses has also been suggested by Kitamura andMizoguchi [22], who talk about processes as actualfunctions. However, what they call an actual functioncorresponds rather to functioning than to function, i.e.to what we (with BFO) call a realization of a function.Only their capacity function corresponds to what wecall function proper. Thus, while they use the termfunction for processes, they do not claim processesare functions in the same sense as the functions as-cribed to devices.Hence, we strongly support (6) and (7), but we willnot consider the following desiderata (8) and (9), whichhave also been suggested by Burek [14]:(8) Process functions: Processes can be bearers offunctions.(9) Decomposition: An ontology of functions shouldsupport functional decomposition, i.e. the analysisin terms of sub-functions.Criterion (8) is contentious, as it is in direct contradictionto our preferred framework BFO where only independentcontinuants can be bearers of functions. We do acknow-ledge that natural language does, in fact, attribute functionsto processes like in the following examples:A. His knocking at the door had the function to causesomeone to open the door.B. The pumping has the function to keep the bloodcirculating.However, surface grammar might be deceiving. Ex-ample (A) is a description of an intentional action andthe agent knocking at the door performs this action be-cause of a certain purpose. This shows that it is not ne-cessary to ascribe functions to processes because thiscomes down to the ascription of intentions or plans ofpersons participating in these processes. In example (B),the function to keep the blood circulating can, as well,be ascribed to the heart, which is an independent con-tinuant, as BFO requires. The heart can also be said tohave the function to pump blood and given the heartscanonical location within a circulatory system, it fulfilsthe former function by realizing the latter: It keeps theblood circulating by pumping it. Hence, in these cases,we have an instrumental or causal relation between twoprocesses. While this is an important feature to be mod-elled, we hesitate to deal with it together with the func-tions of independent continuants.We do not consider (9), as decomposition is orthog-onal to the relationship between functions and disposi-tions and, thus, not relevant to the topic of this paper.Decomposition seems to be of interest mainly in thetechnical domain and is hardly discussed for bio-functions in applied ontology, but it can, of course, beextended to this domain. This is nicely shown by theheart example that we just discussed: The heart sup-plies oxygene to the body by circulating the blood, andit keeps the blood circulating by pumping it (cf. thehierarchies of biological functions in [23]). Of course,composition of functions cannot be analogous to themereological composition of the whole by its parts be-cause the function of the whole is not simply the sumof the functions of its parts. Nevertheless, a function ofa whole often depends on the functions of its parts, butthe composition relation for functions is not straight-forward and would be a different topic. While func-tional decomposition is compatible with our approach,it is not relevant for our present purposes. For thesereasons, we will use the criteria (1)(7) only.ResultsSurvey of philosophical theories of functionsThe concept of a function has provoked a lively debatein the philosophy of biology, of mind and of engineering.Several accounts have been proposed to illuminate thenature of biological functions; see [24] and [25] for re-cent contributions to this debate. We will now surveythe most important suggestions and evaluate them withrespect to the desiderata given above.Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 6 of 16http://www.jbiomedsem.com/content/5/1/27Among the many different conceptions and theories offunctions, three main classes of theories may be distinguished.Like Franssen [18] and Artiga [15], we start by looking at (a)causal contribution theories of functions and (b) etiological ac-counts. We add to this by looking at (c) intentional accountsand two further developments of the intentional account,namely (d) the ICE theory that combines intentional, causaland etiological elements, and the (e) fictionalist account thatextends the intentional account to biological functions.Causal contribution theories of functionsCausal contribution theories take criterion (4) as suffi-cient for the ascription of a function. They are also calleddispositional [15], causal role [26], goal-contribution[27], and systemic [28] accounts of functions. They allhave in common that the function of a thing is linked tothe present causal contribution of the function bearer in acertain context. The most straightforward is the simplecausal role analysis. According to this analysis, X has func-tion F simply means that X does causally contribute tosome output O of a complex system S [26,27]. A well-known problem of this account is that it is extremely broadand admits many unintuitive functions: It implies, e.g. thatclouds could be ascribed the function to produce rain be-cause they undoubtedly have a central causal role in theproduction of rain (more examples and further criticism in[27]). Thus, the simple causal role account fails the teleologycriterion and no distinction between essential and accidentalfunctions is possible. To ameliorate this, further conditionshave to be added in order to narrow down possible functionsof a thing. As functions are intuitively connected either withsome intention, as in artefactual functions, or a (not neces-sarily intended or conscious) goal in biological functions,Boorses general goal-contribution approach [27] could beconsidered the minimal core of the concept of a functionwith system S, system part X and goal G:X performs function Z in the G-ing of S at t if and onlyif at t, the Z-ing of X is a causal contribution to G.But this still does not satisfy all of the criteria. First,Boorses definition does not capture functions as such,but only their realizations. Moreover, Boorse-functionscould be performed only once or accidentally and fulfilthis definition which would usually not be what wemean when we ascribe a function. (A formal-ontologicalcharacterization of such a goal-contribution approachhas been sketched in [29]). Boorse still has to rely ondistinctions like the normal function of a type as opposedto accidental functions or deviations of single tokens toavoid those counterintuitive accidental functions. We willcome back to the type/token relation as a source of norma-tivity below. An even more challenging problem for this ac-count is posed by malfunctioning because if X does notperform Z (i.e. if function Z is not actually realized) and noactual causal contribution takes place, we cannot ascribe afunction at all. Therefore it is not clear how malfunctioningshould be handled within this framework.The concept of a systemic function, as suggested byMizoguchi et al. [28], is very similar. They introduce asystemic context that structures a system into a nestedhierarchy of subsystems and components and assigns aspecific behaviour to the system and its components inorder to define an objects systemic function:An object A performs a systemic function within asystemic context C, if and only if there is a system Ssuch that:(1) C is a systemic context for S,(2) according to C, A is a component of a subsystem of S,(3) the goal of this subsystem is to realize the goal of C, and(4) some behaviours of A play the (functional) roledetermined by C.This definition can be applied both to biological andto technical functions. To give a biological example, asystemic context C for the human liver (=A) would bethe human digestive system (goal: digestion of food andextraction of nutrients) and within the subsystem of fatdigestion the function of the liver is the production ofbile. The point is that the systemic function is context-dependent in a specific way, i.e. via a system its bearer ispart of. Hence, the function of a thing can change de-pending on the system it is a component of. In an earlierpaper [22], Kitamura and Mizoguchi discuss the exampleof a heat-exchanging device. This device shows as char-acteristic behaviour a process of heat transfer that leadsto a temperature change in some fluid. This behaviourcan serve different functions: It can either have thefunction of heating by giving heat or the function ofcooling by removing heat. Kitamura and Mizoguchi,therefore, call the role of the behaviour which the deviceplays in the respective teleological context (here: heatexchange) the actual function of the device. Dependingon context, this will be heating or cooling. Their expressionactual function corresponds to what we call realizationof a function and their capacity function corresponds toour function. If we follow the idea that the realization getsits role depending on context, we could say that the devicehas the capacity (=disposition) for the realization heat-exchanging, but can have the (capacity) function of heatingor cooling depending on the context. Thus, the context-dependence of the realization of a function can also beclaimed for the function as a realizable entity.In this way, the proper function is fixed by the respect-ive context and can be distinguished from accidentalfunctions. Therefore, the accidental/essential distinctionRöhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 7 of 16http://www.jbiomedsem.com/content/5/1/27can be maintained by making functions context-dependent:all systemic functions are essential with respect to thesystemic context [28] (italics in the original). However,it is not clear how the systemic context can be fixedwithout recourse to other factors. Intentional design issuch a factor which Kitamura and Mizoguchi [22] takeas a standard example for such a context.To summarize: The causal contribution approachescan account well for the support criterion because func-tions are closely tied to the dispositions of their bearersand their realizations. They have no problem with novelfunctions because nothing is said about the history ofthe function bearer; they avoid epiphenomenalism be-cause the actual performance is central. On the otherhand, the problem of accidental and essential functionscan only be dealt with by embedding the bearer in a sys-tem, which, in turn, may lead to the problem of deter-mining the proper context of a function and, thus, tocircularity, making the proper context depend on theproper function and vice versa. A central problem forthis account is the possibility of malfunctioning, for iffunction is taken to be identical with the actual causalcontribution of the function bearer, it is not clear howsomething can both have a function and not perform ad-equately and it seems very hard to distinguish malfunc-tion from the absence of function. We will come back tothis later, but we already note here that many authors donot employ the idea of realizables and, therefore, do notdistinguish as sharply between the having of a function(or a disposition) and the actual taking place of the re-spective realization process. In the approach that takesfunctions as realizable entities, it is clear from the outsetthat something may have a function without actually orever performing it. This does justice to the fact that afunction ascription is not tied to actual performanceand, thus, rejects the causal role criterion as sufficientfor a function ascription, but this alone is not sufficientto explain malfunction.Etiological theories of functionsAs an alternative to causal contribution theories, etio-logical accounts of biological functions have been sug-gested [30,31]. While the causal theories of functionswhich we discussed so far focus on the effects of func-tions, etiological theories focus on their causes: The coreidea of etiological theories of functions is that a functionof X plays a relevant role in an explanation of why Xexists in the first place. In the present section we willdiscuss only those etiological accounts that refer to non-intentional causes, while we will discuss intentionalaccounts of functions in the next section. In non-intentional etiological accounts, a function is taken to bedependent on the history of the biological kind whoseinstances are bearers of the function in question (or, assome prefer, on the history of matching reproductivelyestablished families [30]), i.e. on the series of evolu-tionary precursors of a present function bearer. It isevolutionary selection that causally explains the exist-ence of the functional parts in the first place. Thus wecan phrase the etiological account of biological func-tions as follows:Instances of a biological kind X have the function todo F if and only if the F-ing of the instances of X hasin the past been causally responsible for the positiveselection of X and, thus, indirectly for the presentexistence of instances of X.Such an etiological approach can deal with the mostsalient difficulties of the causal contribution accounts.Essential functions can be distinguished from accidentalones by referring to the selection history of the trait; fur-thermore, this history is determined by salient effects ofearlier versions of the trait, so no external goal or con-text seems necessary. Since performance of a function isclearly distinguished from having a function, somethingcan have a function (due to its history), but actually bemalfunctioning in the present [15]. However, recallFranssens criticism of the normativity of evolutionaryfunctions mentioned in the methods section. Many traitsof an organism could be considered to be functional be-cause of their evolutionary history, but would not qualifyas functions in the everyday sense and are usually notevaluated with respect to norms. Franssen also sees aproliferation problem if, e.g., foxes are ascribed the func-tion to hunt, eat and, thus, control the population ofrabbits because they evolved in this way [18].In any case, etiological accounts have problems withrespect to the avoidance of epiphenomenalism and withnovel functions: Functionality is exclusively determinedby evolutionary development, and such historical factsare causally inert. Moreover, due to the necessity of hav-ing a selection history, there cannot be any functions inthe first generation of a biological type, although theactual structure of the organs would be functional inthe everyday sense, which is very counterintuitive. Inaddition, a certain body part may acquire new uses andfunctions during the evolutionary history of a specieswhile the early history and, hence, the reasons for itsexistence remain the same [27]. Sea turtles, for ex-ample, use their flippers to bury their eggs in the sand;this seems to be one of the functions of their flippers.During the evolutionary history of the turtles, however,the flippers developed as a means for locomotion andonly later acquired the digging function [32]. PeterGodfrey-Smith has argued that functional explanationhas to focus on the recent history of species ratherthan on its distant evolutionary origins [33]. One of hisRöhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 8 of 16http://www.jbiomedsem.com/content/5/1/27arguments is that otherwise there would be no differ-ence between functional explanations and evolutionaryhistory, which are two distinct tasks according toTinbergens famous four questions that biology has toanswer [34]. Hence, if functions tell us anything aboutthe survival value of a certain trait, the recent historyof a species seems to be more important than its dis-tant evolutionary past.Furthermore, adaptionism is no longer seen as theonly possibility how biological differentiations and func-tions can arise [35]. More generally, according to etio-logical accounts, the actual performance of a body partshould not be relevant because the evolutionary historyis the only thing that matters. Thus, functions seem tobe mere epiphenomena of some causal history whereas,intuitively and methodologically, the essence of a func-tion lies in what a thing can and is supposed to do now,which can be understood and discovered independentlyof how the thing came to be there.Intentional accounts of functionsAs a third group of theories of functions we considerintentional accounts. They are tailored to accounting forthe design functions of artefacts: A screwdriver can besaid to have the design function to drive screws becauseit is produced with the plan to be used for this purpose.A design function, then, is not a property that inheres inthe functional artefact, but it is the content of an ascrip-tion by an agent or a group of agents involving a planabout the future use of this artefact (or of artefacts ofthis type). They are made for a certain purpose, theirfunction. Let us call this the planning account of designfunctions. On this account, the truth-maker of a func-tion ascription is a plan. Houkes et al. have given anaction-theoretic analysis of use and design claiming thatthe designers intentional plans are, in a sense, prior toboth the design of an artefact and its use by a prospect-ive user [36]. According to this approach, both use anddesign functions of artefacts are dependent on the de-signers (or the clients) ends and on his plan for achiev-ing these ends. On such an account, artefacts are to bedescribed as objects playing a role in the contexts ofboth use and design, contexts that are mediated by thecommunication of a user plan [36]. (The more elabor-ate ICE theory of technical artefacts by Houkes andVermaas [16] will be discussed in the next section as afurther development).Intentional accounts fare well with regard to the tele-ology criterion: Artefacts are produced in order to fulfilthe function ascribed to them by their designer. Theycan distinguish well between essential and accidentalfeatures and they can account for normativity andinnovation because all of these can be based on the in-tentions and expectations of the designer.On the downside, design functions are grounded indesigners function ascriptions and not in the physicalstructure of artefacts. Thus, an artefact could have anyfunction independently of its physical structure and dis-positions. (It will, though, not be able to realize its func-tion unless it possesses a corresponding disposition todo so). One option to ameliorate this is to demand ra-tionality of the designer: A rational designer would havejustified (although, not necessarily true) beliefs about thecomponents and their working together when he as-cribes a function to the system, and would not assignfunctions not supported by the structure and disposi-tions of the components. However, it is not easy to seehow this approach can be transferred to biology, for itseems to presuppose a rational and intentional Creatorand looks almost like intelligent design theory, outrightlyrejected by many (though not all) biologists, philoso-phers and theologians [37]. (We will later discuss a fic-tionalist approach that tries to overcome this problem.)The intentional account can easily be modified inorder to deal with the so-called use functions of artefacts(which would be classified as roles within the BFOframework). Use functions are directed at those activitiesthat users actually use things for. (Cf., e.g., [28] for moreon the distinction between design function and use func-tion and [17] for problems of this distinction.) If I usemy screw driver to open my paint cans, it has the usefunction to open paint cans. It has not been producedfor this purpose; hence, the use function can differ fromits design function, though it might be just the same.Moreover, one and the same thing can have many differ-ent use functions at different occasions. This accountcan also be extended to biomedical entities: If someoneuses digitalis to kill his wife, he has a certain action planthat involves the participation of both a probe of digitalisand his wife with a certain intended outcome.The ICE theoryHoukes and Vermaas have developed a function theoryfor artefacts they call ICE theory, reflecting the factthat they include Intentional, Causal and Evolutionaryelements in their account [16]. The main intentionalelement (I) is a use plan for the artefact that reflects thedesigners and prospective users intentions. The causalaspect (C) shows in their identification of function as-criptions with ascriptions of physico-chemical capaci-ties or, in our terminology, dispositions that ensure thesupport of the function ([15]; p. 100):A designer justifiably ascribes the disposition to V asa function to an artefact x relative to a use plan p forx, and relative to an account A, if and only if (I) thedesigner believes both that (I1) x has the capacity toV and that (I2) p leads to its goals due to, in part, xsRöhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 9 of 16http://www.jbiomedsem.com/content/5/1/27capacity to V; and (C) the designer can justify thesebeliefs on the basis of A.The evolutionary aspect of the ICE theory is very weakand only relevant for function ascriptions by passive,non-designing users (as opposed to designers) and theiruse of artefacts because they need to have warranted in-formation (by testimony) about the designers beliefs (I1)and (I2) and this is transmitted historically. The au-thors claim that this account does well according totheir criteria, i.e. to (2) Restriction, (3b) Malfunctioning,(4b) Support and (5) Innovation. Due to the specifica-tions of the originally intended use plan, accidental func-tions can be excluded and novel functions are noproblem; and on account of the identification of func-tions with dispositions, there is no threat of epiphenom-enalism: Since designers and users need to have thejustified belief that the thing in question actually has thisdisposition in order to ascribe the corresponding func-tion, the support criterion is satisfied and cases ofwishful thinking are excluded. Note that this is an epi-stemic understanding of the support that the physicalstructure lends to functions. Although for many stand-ard cases of function ascriptions the support criterioncan be satisfied in this way, such rationality constraintsdo not completely exclude mad scientists and strangefunction ascriptions. They cannot guarantee that func-tion ascriptions are always veridical, and it could hap-pen that function ascriptions are rationally justifiedwith respect to the available information but false.Malfunctioning is still considered a difficult case for theICE-theory by its authors. In many cases of malfunctioning,users ascribing an ICE-function may be wrong but justifiedbecause they are not aware that the artefact in question haslost the respective disposition. Users can also be fully awareof the lack of capacity because they know the artefact to bebroken, but they assume that repair is possible and planned.Intuitively, we would ascribe the function to the brokenartefact, but, according to the ICE definition, this wouldnot be justified. Houkes & Vermaas distinguish having adisposition from its performance and use this for othercases of non-performance because necessary conditions forthe realizations are not met, as in the case of a car with anempty fuel tank. Overall, they find it necessary to refer to abackground of maintenance and repair to distinguish mal-functioning from non-functioning. Some damage of arte-facts responsible for their malfunction can be repaired, sothese are cases of (temporary) malfunction. However, insome instances, repair is not technically feasible; in suchcases an artefact would definitely have lost its function.The fictionalist accountAnother modification of the intentionalist approach is thefictionalist approach [38-40]. In pre-Darwinian biology,organisms and their parts were described as if they, too,were something created  either, allegorically spoken, by apersonified Nature or by God as a creator. In the latter case,ascribing functions to biological entities could be conceivedof as reading the mind of God before the act of creationand as a reconstruction of the reasoning underlying Hiscreation; we mentioned this option in the context of thepure intentional account. This is no longer a viable accountfor modern science and much of the discussion aboutbiological functions can be understood as finding somesubstitution for this intentional model. In the former alle-gorical case, we have something like an as-if parlance,which can be found, e.g. in Aristotle: Although Aristotle re-jects the idea that the universe or life had a beginning intime, he often says that Nature has well organised her crea-tures [41]. We suggest to read this as a fictionalist or as if way to talk about biological function, meaning: Were thisplant or animal brought about by Mother Nature (a veryintelligent designer), she would have done so for goodreasons. Hence, the planning account can be upheld forbiological functions with a small modification: The truth-maker of the ascription of a biological function is no actualplan, but a plan within the fiction of Mother Nature design-ing her creatures. The fictionalist account simply invitestaking the intentional approach seriously as a model, i.e. acertain kind of fiction [42,43], without an ontological com-mitment to the existence of Mother Nature as a real personwith beliefs and desires. That is, Mother Nature is the func-tional equivalent to extensionless mass-points in mechanicsor rational utility maximizers in economics: They do notneed to exist to make the theory in question an explanatoryor predictive success. This way, the fictionalist accountallows extending the intentional account to biologicalfunctions, while retaining the plain intentional accountfor artefacts.The fictionalist account does well with respect to ourlist of criteria. Like in the original intentional account,functions feature in a teleological explanation (thoughsome transfer is needed from the fictive design model toevolutionary reality). The fictionalist account can distin-guish between accidental and essential functions; and itallows for normativity and malfunctioning. Novel func-tions can be accounted for if we situate the fictive designerat a later stage in evolutionary history faced with differentevolutionary challenges. We can also give slightly moreoptimistic answers regarding the threat of epiphenomenal-ism and the support for the actual performance of a cer-tain body part, as its physical structure is the mostimportant evidence for the function of a body part andany informed judgement about the function of a part mustaccount for how its structure supports this function. Fi-nally, there is now a viable bridge from biological func-tions to artefactual functions, as intentional accounts farewell with the latter anyway.Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 10 of 16http://www.jbiomedsem.com/content/5/1/27DiscussionFormal ontological and philosophical accounts offunctionsAs should be clear now, the philosophical theories faredifferently with respect to the criteria set out above(Table 2), and they do not always distinguish sharply be-tween functions, roles and dispositions. However, BFOdraws on the different function theories for its distinc-tions between the types of realizables. To begin with,BFO functions and BFO dispositions are determinedby their causally relevant internal structure; this fits tothe causal-role account, as it is said that the realizationof a (biological) function helps to realize the charac-teristic physiology and life pattern for an organism ofthe relevant type [6]. The difference between dispositionsand functions is founded on a historical (evolutionary) orintentional (design) component, respectively. Similarly,these intentional and historical criteria are used in BFO 2as the specific difference of functions as opposed to non-functional dispositions. Thus, BFO functions are essentialfeatures of their bearers because of either an evolution-ary or an intentional component, whereas functionsaccording to a mere causal-role account would prob-ably be classified as dispositions within BFO. What iscalled optional, accidental or use functions in thegeneral debate are roles in BFO because they are notessential to their bearers. In fact, it is essential to BFO-roles that their bearers are not essentially playingthese roles; this is particularly clear in the case of socialroles.Function and homologyWhen explicating the distinction between functions,roles and definitions in BFO, Arp and Smith define bio-logical functions as follows:A biological function is a function which inheresin an independent continuant that is (i) part ofan organism and (ii) exists and has the physicalstructure it has as a result of the coordinatedexpression of that organisms structural genes. [6]Table 2 Evaluation of philosophical function theoriesCausal role accounts Etiological a(1) Teleology ? +(2) Accidental/essential ? +(3) Normativity/Malfunctioning ? +(4) Support/No epiphenomalism + ?(5) Novel functions + ?(6) Continuants + +(7) Bridging of domains + +Philip Lord [44] points out that several prima facie bio-functions are not captured by this definition because of itsrestriction to organism parts on the one hand and to geneexpression as grounds for the structure on the other hand.His counterexamples are molecular functions and functionsof whole organisms. Lord gives an alternative definition:A biological function is a realizable entity thatinheres in a continuant which is realized in anactivity, and where the homologous structure(s) ofindividuals of closely related and the same speciesbear this same biological function. [44]Lord claims that his definition is recursive rather thancircular, despite the occurrence of the word functionin the definiens. While this can be considered to be aproblem on its own, his suggestion is also open to coun-terexamples when it comes to more recently acquiredfunctions like the sea turtles flippers, which are used tobury their eggs in the sand (see above), but have homo-logues in other species that have (only) the function oflocomotion. Lord realizes this when trying to distinguishfunctions from roles: A human can walk on his hands,but the function of the human hand is not walking asmost humans do not walk on their hands, whereas thehands homologous structures in other primates do havethis function. Therefore in humans, the hand may havethe role, but does not have the function walking. Hencehe concludes that among the instances of realizables thatare realizables for the same type of process can be bothroles and functions depending on the species the realiz-ables bearer belongs to. This presents a problem for thedistinction between functions and roles. We will comeback to this distinction in due course, but it should benoted that this problem exists for Lord mainly becauseof the reference to homologues in other species in hisfunction definition. To put it bluntly: Had evolutionstopped after the first species, according to Lords defin-ition, there would not have been any biological functionat all. If we look only at humans, we would not supposethat the function of the human hand is to walk with, butccounts Intentional accounts ICE account Fictionalist account+ + ++ + ++ + +? + ++ + ++ + +? ? +Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 11 of 16http://www.jbiomedsem.com/content/5/1/27that it can assume such a role under certain circum-stances for appropriately trained individuals. Therefore,it seems that connecting functions by definition with ho-mologues in other species (and their respective func-tions) does introduce more difficulties than it solves,even if homologues are of huge importance heuristically.Function vs. roleLet us in more detail review the differences betweenfunctions and roles. Lord [44] states that in actual bio-medical ontologies, this distinction between functionsand roles is hardly observed. Lord cites OBI, the Ontol-ogy for Biomedical Investigation [45], as an example thatignores this distinction. The context dependence doesnot seem sufficient to distinguish functions from roles.Dumontier has claimed that on the level of proteins andmolecules the distinction between functions and rolesbecomes redundant: The difference between functionsand roles is not particularly obvious in molecular sys-tems and may in fact be redundant. For instance, thefunction of an enzyme is to catalyze a reaction [] Everytime a protein executes such functionality, it necessarilyrealizes the enzyme role [46]. Consequently, Dumontieruses only roles in his analysis of molecular reaction. Thisseems appropriate because the very same type of proteincan have the enzyme role in one reaction type and a dif-ferent (e.g. substrate) role in a different reaction type. Inour approach, we would ascribe several dispositions tothe protein as prerequisites to the different realizationsthat would correspond to the roles of enzyme etc., sothe protein would have dispositions and roles, but nofunctions. Also, subrelations of the hasParticipant rela-tion could be employed when the different participantsof a reaction are classified according to their roles likehasSubstrate, hasProduct or hasCatalyst. Note thatthe inverse relation participatesIn and its subrelationscannot be used for this purpose because the same typeof molecule can have different roles in reactions and,thus, it will generally not be true that all molecules par-ticipate in some reaction type.Accordingly, many so-called functions in biomedicalontologies are, strictly speaking, roles. Also the so-calleduse functions of artefacts are roles. To come back to ourfunction criteria, roles clearly fail the first two, Teleology(1) and Restriction (2). They fail the Teleology criterionbecause a role-bearer is not created for the playing of amere role. This seems obvious in the case of artefact usecases not intended by the designer, i.e. using a chair tostand on to change a lightbulb, or social roles. Similarly,rabbits have not been selected for being food for foxes,though they may serve the role of fox food. Roles alsofail the essential-accidental distinction; as can be seen bythe examples given so far, roles are all accidental in asense. The other criteria are less clear. Malfunctioningcan, in some cases, be applied to roles, especially to so-cial roles. In other cases, there is no plausible norm forthe role to be evaluated against, as in cases of accidentaluse or misuse, because an artefact that is explicitly usedfor an unintended purpose will not be expected to per-form optimally. Roles also usually must have some phys-ical support in the dispositions and abilities of theirbearers to have successful realizations. For example, ascrewdriver can only serve the role as a makeshift chisel,because of its shape, its hardness etc.Functions vs. dispositionsNow, what does our review tell us about the relation be-tween functions and dispositions? A common philosoph-ical position thinks of dispositions as a certain type ofproperties [7,8,47]. According to this position, a dispos-ition is a causal property that is linked to a realization,i.e. to a specific behaviour or process which the individ-ual that bears the disposition will exhibit under certaincircumstances or as a response to a certain trigger. Some-thing is water-soluble if it can dissolve in water. In this fash-ion, dispositions establish a link between independentcontinuants (stable things) and occurrents (processes). Thefundamental connection is the following: Continuant typeS has disposition type D for a realization type P and, in casesome token p of P occurs as the realization of an instanceof D, then an instance s of type S is both the bearer of thedisposition d and a participant of this process instance p.The category of dispositions is often treated as a specialkind of dependent continuants that are linked to a processof realization by a respective formal relationship [6-8].Some, but not all of the positions surveyed above areexplicit about the relations between functions and dispo-sitions, and those that are do not agree on this matter.While Cummins writes that if something functions as apump in a system [] then it must be capable of pump-ing and that to attribute a function to something is, inpart, to attribute a disposition to it ([26]; p. 7578),Millikan states that a things having a function has todo not with its powers but with its history ([31]; p. 17).On a first superficial view, the idea that functions are, infact, dispositions seems to be at least compatible withmost of the positions reviewed: According to the causalrole accounts, functions can be identified with disposi-tions quite easily. Millikans resistance notwithstanding,other authors and BFO 2 combine an etiological accountwith the claim that functions are evolutionarily acquireddispositions [9,33]. Intentional accounts can argue thatfunctions are specially designed dispositions and the au-thors of the ICE account actually state that functionascriptions are special kinds of disposition ascriptions.More scrutiny, however, will show that the criteria (3a)and (3b), i.e. normativity and malfunctioning, require usnot to subsume functions under dispositions.Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 12 of 16http://www.jbiomedsem.com/content/5/1/27Are functions dispositions?We have seen that among others, both BFO 2 andHoukes and Vermaas conceive of functions as specialdispositions (or of function ascriptions as special ascrip-tions of dispositions). We will now apply some of thecriteria for functions above to try to capture also theprima facie differences between functions, roles and dis-positions. Dispositions can be blocked or incompletelyrealized, but their bearers are not evaluated in a norma-tive fashion. We usually do not evaluate the realizationof a disposition like inflammability; although, we cer-tainly have rational expectations about the perform-ance of the bearer of a disposition and rely on them inour intentions. Suppose that someone wants to start acampfire to get warmth and all available wood is wet. Insuch a case, the wood will only burn with difficulty andone will in fact evaluate the available wood negativelywith respect to its inflammability, but only because oneascribed a use function (i.e. a certain role) to the woodbefore. The having of a disposition need not explain theexistence of the disposition-bearer, so they generally failwith respect to the teleology criterion, although theremight be some cases where the having of a dispositiondoes explain the existence of its bearer. There are mate-rials (like alloys or textiles) that are specifically createdfor their dispositions because the artefacts made fromthe material will need those dispositions to fulfil thefunctions they are intended for. Thus, in these cases,some chunk of the alloy comes into existence because itwas designed to have a certain disposition. Could this betaken as an argument for functions as special disposi-tions? We do not think so. The teleological dimension,like the normative one, applies to dispositions only inthe case of artefacts or in connection with intentionaluse; that is, in contexts that have a teleological compo-nent (usually intentional) anyway.Peter Kroes has also argued against seeing functions asa type of dispositions because dispositions lack the nor-mative dimension we need for functions [48]. Althoughhis argument relies on Carnaps dated analysis of dispo-sitions, it does show the salient difference and couldprobably be adapted within a more sophisticated analysisof dispositions.Dispositions are usually ascribed because of theirrealization (their performance). They can be distin-guished from functions according to the criterion (4a)because there is no tension between current perform-ance and normativity for dispositions. Other than func-tions, dispositions do not face the threat of being mereepiphenomena.One further central difference between dispositions onthe one hand and functions and roles on the other handseems to lie in their context-dependence. Continuants maylose or acquire dispositions, but not without fundamentalchanges within the bearer. In contrast, many functions canbe performed by different types of bearers and an objectmay have different functions in different contexts withoutany change in itself. Chopsticks, for example, have the func-tion to support eating. Similar sticks found in the woods donot have any such function, though they may have the verysame physical structure and, hence, the same dispositions.Dispositions, that is, are purely internally grounded, whilethe function of the chopsticks is a historical property dueto the way this artefact has been produced. At the otherend of the spectrum, social functions and roles are exter-nally grounded; that is, they are dependent on the respect-ive context, relational and historical properties and mostlyindependent from the physical structure of their bearers.Biological functions like those of organs, enzymes etc. liesomewhat in between these extremes, as an entity can usu-ally perform several functions in a certain range of contexts.They are objective systemic functions in the sense men-tioned above and not merely ascribed by an agent; theircontext-dependence is fixed by the functional hierarchy ofthe respective physiological system. An organ like the liverhas many functions, like the production of bile, glycogenstorage, cholesterol synthesis etc., but all these are fixed bythe respective physiological systems in which the liver andits products are functionally involved. They are not as arbi-trary or flexible as the screwdriver that can serve the usefunctions (i.e. roles) of a can opener or a weapon.Do functions depend on dispositions?For all of these reasons, we should assume that functionsare not identical to dispositions. Nevertheless, even ifthey are distinct entities, functions could ontologicallydepend on dispositions. The support criterion suggestsindeed that, in some sense, functions could be basedon dispositions. On the intentional account, functions ofartefacts are clearly independent from the dispositions oftheir bearers; due to the fallibility of human designers,the one could easily occur without the other. From thepoint of the fictionalist extension of the planning accountto biological functions, however, the existence of a functionimplies the existence of the corresponding disposition intypical cases if we transfer the usual assumptions of Godsomniscience and benevolence to our fictional designer.Nonetheless, even if a biological function is typically ac-companied by a disposition, this concurrence is not univer-sal, as proven by malfunctioning.In our view, the dispositions are part of the internalstructure of a thing; they determine whether it can fulfilthe respective function in a given context. Johansson[49] calls this the substratum of a function. While thefunction itself is independent from its substratum, itsrealization depends on its existence. This dependencecan be a generic one because sometimes different dispo-sitions or structures can ground the same function: E.g.Table 3 A new cross-classification of realizablesInternally grounded(= non-optional giventhe physical structure)Externally grounded(= optional given thephysical structure)Essential(= non-optionalgiven the bearer)Essential disposition FunctionAccidental(= optional giventhe bearer)Accidental disposition RoleRöhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 13 of 16http://www.jbiomedsem.com/content/5/1/27the cooling function of a cooler can be implemented indifferent technical setups [49].As we know biological functions only through their ac-tual realizations, we would have no reason to ascribethem unless instances of a certain kind typically dis-played that behaviour and, a fortiori, possessed a corre-sponding substratum disposition. How would we knowthe biological function of, say, a heart, if hearts did nottypically have the disposition to pump blood and did nottypically realize this function? Thus, there should besome evidential connection between the function andthe disposition of the organ. This way, we can meet theSupport requirement (4) by giving it an epistemic inter-pretation: The discovery and ascription of a function isepistemically supported by the preliminary discovery andascription of dispositions in the same or other instancesof a certain species.On the other hand, many diseases like, e.g. heart insuf-ficiency, are characterized by the very contrast betweenfunctions and the lack of corresponding dispositions andso is malfunctioning in general. Malfunctioning artefactsor diseased organs are characterised by the loss of thedisposition to fulfil their function. E.g. a lung with a car-cinoma will still have the function to serve as an oxygenprovider for the body, but the function may no longerbe realized because the corresponding disposition (to beable to serve as an oxygen provider for the body) is nolonger present. Such an account of malfunctioning worksbecause (and only if) the function is ontologically inde-pendent of the disposition. From this point of view, the taskof medicine is to restore the disposition matching to thefunction, such that the organ would be (fully) functionalagain. In such a fashion one can also account for healingprocesses: A healing process, then, consists of restoring adisposition where there is a function without its corre-sponding disposition. We conclude that the correspondingdisposition is only necessary for the realization of a func-tion, not for the function itself. Since in biological (andmany artefactual) cases we can evaluate the perform-ance of token functions with respect to what is a nor-mal realization for the function type and because thenormal realization is dependent on the correspondingdisposition, we have a correspondence of function anddisposition at the type level or for prototypical tokens.However, this is to be distinguished from a token-leveldependence of the function on the corresponding dis-position. If we want to accommodate malfunctioning,we should reject the latter.As a source of the normativity of functions, we can iden-tify the type membership of an instance of a functionbearer. Being a token of a type involves an evaluative di-mension [32,50,51]. Therefore, in function ascription, theattribution of a function to a type of entity takes preferenceover the ascription to a token. A token is supposed to havea function and be able to perform it successfully because itbelongs to a certain type.A further reason not to treat functions as special dis-positions is the following difficulty: In formalised fash-ion, type-level relations like hasFunction are usuallydefined by universal quantification over their instancesusing the corresponding token-level-relation [52]. Assaid above in the normal or paradigmatic case, a func-tion of an entity comes along with the disposition toperform this very function, but as we want to allow forthe possibility of malfunctioning tokens that have lostthe corresponding disposition and, consequently, per-form the function insufficiently or not at all, we cannotassert a token-level dependence of disposition and func-tion for all instances of a type. One option here wouldbe to distinguish between canonical and non-canonicalentities within a type of function-bearers. Such a distinc-tion is used, e.g. with respect to anatomical structures inthe BioTop ontology [53]. If one follows this approach,all instances of the canonical type have the correspond-ing disposition and do (or would) function adequately,so for this subtype the standard definitional procedureworks. The instances of the non-canonical subtype willnot be ascribed the disposition, but only the function.RecommendationsWe can summarize the discussion by suggesting a newclassification schema for the three realizables function,disposition and role. It concurs with BFO 1.1.1 in treat-ing functions as siblings of dispositions rather than spe-cial dispositions as in BFO 2. It makes use of twoindependent criteria: Structure [6]: Does the realizable supervene on theinternal structure or is it externally grounded? Rigidity [54]: Is the realizable essential or accidentalto its bearer?Structure and Rigidity correspond to two flavours of(non-)optionality. While Structure deals with (non-)option-ality given the physical structure of the bearer, Rigidity dealswith (non-)optionality given the essence of the bearer (i.e.given the kind of thing it is). Thus, we end up with a crossclassification of realizables presented in Table 3.Röhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 14 of 16http://www.jbiomedsem.com/content/5/1/27We will start the elucidation with Structure. A realizablecan be optional given a certain physical structure of itsbearer. All realizables that are externally grounded, i.e.grounded in some context, are optional in this sense, e.g. allroles. In contrast, dispositions are internally grounded,based on the bearers physical structure and, therefore, notoptional given the bearers physical structure. A few clarifi-cations seem in order: Internal grounding does not requirethat there is a one-to-one-correspondence between a dis-position type and a structural type on which it is based.Various instances of one and the same type of dispositionscan be based on quite different physical structures. Fragility,for example, may be based on the molecular structure ofdry wood as well as on the structure of glass, respectively.Like many other dispositions, fragility can be structurallyconstituted in multiple ways, but in all of these ways thedispositions base is a structure literally internal to thebearer of the disposition, and it is independent of the con-text or the realization conditions.There is a debate whether dispositions need laws of na-ture to get connected to their realizations or whether lawsof nature are actually based on dispositions, so the latterare ontologically more basic than laws [55]. We will notenter this debate here, but we note that even if dispositionsdepend on laws of nature (whatever laws of nature are), thiswill not be an ontological dependence that would force usto reject the claim that dispositions are internal to theirbearers.Second, a realizable can be optional given the essenceof its bearer. Roles are optional in this sense, and since abearer can gain and lose dispositions, some dispositionsare also optional in this way. Note that in this case theyare not optional given the physical structure, but op-tional given the bearer: The same bearer can survive theacquisition of such a disposition (by learning, training,modification, etc.) and it may lose it without ceasing tobe (by forgetting, wear out, modification, etc.), althoughthe bearer has to change its physical structure in orderto gain or lose dispositions. However, not all dispositionsare optional for a given bearer. Some dispositions, likethe disposition of a proton to attract electrons, are es-sential: Losing this disposition would imply that the pro-ton ceases to be a proton, i.e. that it ceases to exist.There might be only a few essential dispositions andthey might be restricted to the domain of fundamentalphysics. For this reason, we acknowledge their existence,but do not encourage the introduction of separate onto-logical categories for accidental and essential disposi-tions, respectively. Functions, too, are essential for theirbearers: Given the essence of being a heart, it is not op-tional to have the function to pump blood; and given theessence of being a screwdriver, it is not optional to havethe function to manipulate screws. This is why screw-drivers are made; it is their origin, without which theywould not be the things they are. Without these functionsthey would not be hearts or screwdrivers, but rathersomething else. This notion of essentiality for functionsseems to be shared by Kitamura and Mizoguchi with re-spect to artefacts [22]. In a similar way, both the etio-logical and the system account can argue that the bearersof biological functions come into existence in order to ful-fil these functions; the fictionalist account will assert similarthings relative to the respective design fiction.Functions are externally grounded. We argued thatthere are good arguments not to treat functions as dis-positions, nor to make functions dependent on disposi-tions. This distinction is our central disagreement withthe BFO 2 suggestion discussed above. We also defineroles in a rather narrow way (following BFO, but diver-ging from [13,28,56] and others): On our account, rolesare never essential for its bearer. Hence, we think thatassigning an essential breather or eater role to a hu-man being is a loose way of speaking and not to be takenontologically serious. Breathing and eating are processes,not functions or roles. True, humans have to participatein breathing and eating processes on a regular basis, butthere is no need to add to it by postulating a breatherrole or an eater role for human beings. One could treatparticipation (or rather the property of being a possibleand viable participant) in processes as a role in a verygeneral sense. Starting from the original BFO suggestion,however, the category name Role is used in a more spe-cific way that is distinct from participation. In our view,participation is an ontological relation [57] (which wouldbe modelled as an object property in OWL), whereas arole is a realizable; that is, a (specifically) dependentcontinuant. The realization of a role, of course, in-volves the role-bearer standing in the participationrelation of a realization process of the type that is im-plied by that role. One of the main points of introdu-cing roles is to have the option to model these lessstrict relationships with a modal character that cannotbe modelled by stating the participant relation whichonly concerns the actual participation. Therefore, theso-called use functions (cf. above) are to be classifiedas roles in our classification scheme, in agreement withthe BFO conception of roles.ConclusionsWe surveyed several accounts for the analysis of func-tions, both for biological functions and for engineeredfunctions, and we evaluated them with respect to a listof criteria compiled from the literature. While some ofthese theories treat functions as a subclass of disposi-tions, this cannot account for the normativity connectedwith function ascriptions and for the possibility of mal-functioning. We suggest a new classification of realiz-ables by means of two independent criteria yielding fourRöhl and Jansen Journal of Biomedical Semantics 2014, 5:27 Page 15 of 16http://www.jbiomedsem.com/content/5/1/27subcategories: essential dispositions, accidental disposi-tions, functions and roles. Treating functions as a cat-egory in its own right solves the problems of normativityand malfunctioning.On this account, functions are not only disjoint fromdispositions, they are also ontologically independentfrom dispositions. Functions are, however, normallyand mostly accompanied by corresponding dispositions.Moreover, there cannot be a realization of a function with-out there being a realization of a matching disposition. It isfor these reasons, why it is so difficult to distinguish be-tween these categories. Malfunctioning, however, requiresthem to be distinct categories: It happens in case a functionis present but the corresponding disposition is lacking.Competing interestsThe authors declare that they have no competing interests.Authors contributionsBoth authors contributed equally to the present work. Both authors read andapproved the final manuscript.AcknowledgementsFirst research for this paper was supported by DFG grant JA 1904/2-1 withinthe project GoodOD. Many thanks to Andrew Spear who provided us with arecent draft of the follow-up version of [3] and to Amrei Bahr, Niels Grewe,Ulrich Krohs, Philip Lord and audiences at OBML 2012 and anonymousreferees for critical and helpful comments. For the publication of this paper,we acknowledge support by Deutsche Forschungsgemeinschaft and OpenAccess Publication Fund of the University of Münster.Author details1Institut für Philosophie, Universität Rostock, 18055 Rostock, Germany.2Philosophisches Seminar, Universität Münster, 48143 Münster, Germany.Received: 25 February 2014 Accepted: 23 April 2014Published: 2 June 2014JOURNAL OFBIOMEDICAL SEMANTICSHaendel et al. Journal of Biomedical Semantics 2014, 5:21http://www.jbiomedsem.com/content/5/1/21RESEARCH Open AccessUnification of multi-species vertebrate anatomyontologies for comparative biology in UberonMelissa A Haendel1*, James P Balhoff2,3, Frederic B Bastian4,12, David C Blackburn5, Judith A Blake6,Yvonne Bradford7, Aurelie Comte4,12, Wasila M Dahdul3,8, Thomas A Dececchi8, Robert E Druzinsky9,Terry F Hayamizu6, Nizar Ibrahim10, Suzanna E Lewis11, Paula M Mabee8, Anne Niknejad4,12,Marc Robinson-Rechavi4,12, Paul C Sereno10 and Christopher J Mungall11AbstractBackground: Elucidating disease and developmental dysfunction requires understanding variation in phenotype.Single-species model organism anatomy ontologies (ssAOs) have been established to represent this variation.Multi-species anatomy ontologies (msAOs; vertebrate skeletal, vertebrate homologous, teleost, amphibian AOs) havebeen developed to represent natural phenotypic variation across species. Our aim has been to integrate ssAOs andmsAOs for various purposes, including establishing links between phenotypic variation and candidate genes.Results: Previously, msAOs contained a mixture of unique and overlapping content. This hampered integration andJOURNAL OFBIOMEDICAL SEMANTICSBalhoff et al. Journal of Biomedical Semantics 2014, 5:45http://www.jbiomedsem.com/content/5/1/45SOFTWARE Open AccessAnnotation of phenotypic diversity: decouplingdata curation and ontology curation usingPhenexJames P Balhoff1,2*, Wasila M Dahdul3, T Alexander Dececchi3, Hilmar Lapp1, Paula M Mabee3 and Todd J Vision1,2AbstractBackground: Phenex (http://phenex.phenoscape.org/) is a desktop application for semantically annotating thephenotypic character matrix datasets common in evolutionary biology. Since its initial publication, we have addednew features that address several major bottlenecks in the efficiency of the phenotype curation process: allowingcurators during the data curation phase to provisionally request terms that are not yet available from a relevantontology; supporting quality control against annotation guidelines to reduce later manual review and revision; andenabling the sharing of files for collaboration among curators.Results: We decoupled data annotation from ontology development by creating an Ontology Request Broker(ORB) within Phenex. Curators can use the ORB to request a provisional term for use in data annotation; theprovisional term can be automatically replaced with a permanent identifier once the term is added to an ontology.We added a set of annotation consistency checks to prevent common curation errors, reducing the need for latercorrection. We facilitated collaborative editing by improving the reliability of Phenex when used with online foldersharing services, via file change monitoring and continual autosave.Conclusions: With the addition of these new features, and in particular the Ontology Request Broker, Phenex usershave been able to focus more effectively on data annotation. Phenoscape curators using Phenex have reported asmoother annotation workflow, with much reduced interruptions from ontology maintenance and filemanagement issues.Keywords: Annotation, Phenotypes, Ontology, Curation, Systematics, Character matrixBackgroundPhenex [1] is a desktop application for creating seman-tic annotations within the phenotypic character matrixdatasets common in evolutionary biology. Using termsfrom a user-configurable set of ontologies, free-textcharacter state descriptions can be annotated using theEntityQuality methodology for ontologically describ-ing phenotypes [2,3]. In addition, taxon entries can beannotated with identifiers from a taxonomy ontology[4]. Phenex exploits the ability of the NeXML file format[5] to attach arbitrary metadata, including ontologicalexpressions, to phylogenetic data elements, to embedthese ontology annotations within traditional character* Correspondence: balhoff@nescent.org1National Evolutionary Synthesis Center, Durham, NC, USA2Department of Biology, University of North Carolina, Chapel Hill, NC, USAFull list of author information is available at the end of the article© 2014 Balhoff et al.; licensee BioMed CentralCommons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.matrix data. Phenex was developed as part of the Phenos-cape project [6], and it has been used to connect the mor-phological diversity of vertebrates to model organismphenotypes via common ontological semantics [7,8].Since Phenex was initially described, we have addedseveral substantial new features that collectively aim toaddress major bottlenecks in the efficiency of the pheno-type curation workflow. Specifically, three main issuesled to the development of the following three features:(1) Ontology Request Broker (ORB). When using Phenexfor data annotation, curators would invariably encounterthe need to add new terms to the ontologies being usedto most accurately characterize the anatomy descrip-tions presented to them. Using earlier versions ofPhenex, data curators frequently had to switch tasksbetween data annotation and ontology development.That is, they paused data annotation, and changedLtd. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Balhoff et al. Journal of Biomedical Semantics 2014, 5:45 Page 2 of 5http://www.jbiomedsem.com/content/5/1/45software to a term tracker where they made a formalterm request, or to an ontology editor where they addedthe desired term, deferring the formal term request. Forthe rich, interoperability-promoting, and community-developed ontologies used in Phenoscape, formal termrequests often entail lengthy discussions. This processproved a major limitation to annotation efficiency, as asingle paper could require dozens of individual requests.To address this, we decoupled data annotation fromontology development by creating an Ontology RequestBroker (ORB) component in Phenex, building on therecently developed Provisional Term service providedby the National Center for Biomedical Ontology (NCBO)BioPortal as part of its application programming interface(API) [9]. The Phenex ORB component allows curatorsto submit requests for provisional terms directly fromthe curation interface and use them in data annotation.The requested terms can be reviewed asynchronouslyfor inclusion in the appropriate ontology as officialterms. Data files are later automatically updated toreference the appropriate permanent term ID.(2) Collaborative editing capability. Large-scale cur-ation projects like Phenoscape typically employ multiplepersonnel that participate in data curation, promptingthe need that curators be able to edit data files collab-oratively, even if not necessarily concurrently. Phenos-cape curators had previously been using, with mixedsuccess, a software version control system (Subversion[10]) in order to ensure that each curators copy of theproject files was up to date with respect to changesmade by others and to avoid conflicting edits. However,many biologists are not accustomed to installing orusing version control software, and its ease of use variesbetween operating systems. In contrast, file sharingservices such as Dropbox [11] can satisfy collaborativeediting requirements while being mostly transparent tothe user, and they are already widely used by biologists.To mitigate the risk of curators inadvertently overwritingchanges made to a file by another user, we added two basicfeatures to Phenex: file change monitoring and continualautosave. Phenex now immediately notifies the userwhen the file they are currently editing changes andoffers to reload it, and the continual autosave decreasesthe likelihood of a user having potentially conflicting yetunsaved edits.(3) Annotation consistency constraints. Phenotypedescriptions often use the full expressivity of naturallanguage, and as a consequence there are often severalor even many ways to compose ontological annotationsto represent the semantics of a description. If leftunchecked, the resulting variability can hinder inter-operability for automated integration and reasoning acrossannotations. To promote annotation consistency amongcurators, Phenoscape has, as have others, establishedcuration guidelines [8]. Aside from consistency, the qualityof annotations may also be affected by ontology termsapplied in incorrect contexts, such as inapplicable taxo-nomic group, or inconsistently with its definition, suchas using a quality term defined as inhering in a singleentity to annotate a relationship between two entitiesand vice versa. To better report and identify problematicor missing annotations, Phenex now features a ConsistencyReview panel, which allows curators to obtain a growingvariety of quality control reports, as well as an AnnotationChecker panel, which provides the same reports for thecurrently selected annotation.ImplementationThe version of Phenex described here has been archivedat http://dx.doi.org/10.5281/zenodo.12370. Phenex is de-veloped using the Java Swing graphical interface toolkit.It is built on the application framework developed forthe OBO-Edit ontology editor [12], which provides theontology object model, ontology reading capabilities,and configurable interface layout.Ontology Request Broker (ORB)The Ontology Request Broker relies on three main compo-nents: a term request interface and web service clientwithin Phenex, the Provisional Class web services providedby the NCBO BioPortal [13], and a standalone web userinterface for updating requested provisional terms withpermanent identifiers [14]. Before a user can issueprovisional term requests, they must enter their BioPortaluser ID and API key into a Phenex configuration panel.The Phenex term request client provides an entry panelthat allows the user to request a provisional term withthe given name and description, optionally with suggestedsuperclass and synonyms. Phenex sends the request(via HTTP POST) to the Representational State Transfer(REST) [15] based BioPortal web service API, and receivesin return a new unique URI to be used as a class identifier.Phenex adds this new term to the current ontology session,which makes it available for use in annotations. Annota-tions using provisional IDs are saved in output NeXML filesin the same manner as any other term reference.At application launch time, Phenex issues a HTTPGET request to the BioPortal API for all provisionalterms submitted by the configured user. This allowsPhenex to add all provisional terms previously requestedby the user to the users ontology session for use in an-notations. In addition, for any provisional terms in theBioPortal database that have since the initial requestbeen associated with permanent IDs, Phenex automatic-ally migrates any annotations using those terms to usethe permanent ID instead. This is accomplished bymarking those terms as obsolete within the Phenexontology session, and adding to their metadata aBalhoff et al. Journal of Biomedical Semantics 2014, 5:45 Page 3 of 5http://www.jbiomedsem.com/content/5/1/45replaced_by annotation with the permanent ID as itsvalue. Phenex updates any loaded data by followingreplaced_by relationships, whether these are due tonormal ontology changes that are part of their regularmaintenance, or due to provisional term resolutions. Toenable collaborative usage of provisional terms, and toavoid duplicate requests within the project, Phenoscapecurators use a shared BioPortal user ID.Phenoscape curators assign permanent IDs to provisionalterms using a separate ORB manager web interface[14], which is a simple client-side web application im-plemented using AngularJS [16] and the BioPortal webservices API.Collaborative editing supportPhenex monitors the file from which the currently opendocument was loaded for changes. The implementationmakes use of the jpathwatch Java library [17]. Anychanges to the file that are not due to Phenex saving thedocument are reported to the user, giving the user theopportunity to immediately reload the file, or to ignorethe changes. If the user has unsaved changes, reloadingthe file would result in these changes being discarded.To minimize the likelihood of this situation, Phenex alsoprovides a user-configurable option to autosave the fileafter any change is made. Phenex autosave is built on itspreviously existing undoredo support.Consistency reviewAnnotation consistency rules are implemented by theAnnotationConsistencyChecker class within the Phenexsource code. This class contains several rules focusedtightly on the EntityQuality model and the semanticimplications of the structure of the Phenotype and TraitOntology (PATO) [18]. For example, annotations mustinclude both an entity and a quality term; PATO rela-tional qualities, which are qualities that relate betweentwo entities rather than inhering in a single one, must beprovided with an additional related entity; and entitiesdescending from Gene Ontology biological process(GO:0008150) must be used only with descendants ofprocess quality (PATO:0001236). Phenex displays errorsor warnings in two locations. The Annotation Checkerpanel displays any issues found for the currently editedphenotype annotation; this panel continuously updates itsstatus as the user edits the annotation. The ConsistencyReview panel displays a list of errors for all annotations inthe dataset; additionally it notes if there are incompletelyannotated characters, which are characters for which somebut not all states have annotations.Results and discussionAs a result of these new features in Phenex, Phenoscapecurators using the tool report a smoother workflow, withfewer interruptions for tasks related to ontology main-tenance or file synchronization.The most impactful of these new features is theOntology Request Broker. Prior to its implementation,curators were required to manually keep track of unfin-ished annotations, while separately contributing miss-ing terms to the relevant ontology. Submitting a newterm to a community-developed ontology can be acomplex process, possibly involving lengthy discussionwith the expert community and ontology editors. TheOntology Request Broker in Phenex provides an anno-tation workflow that effectively decouples ontologyediting from annotation work (Figure 1). When a miss-ing term is encountered, the user can simply request it,without leaving Phenex, and receive a temporary iden-tifier that they can immediately use within annotations.The term request consists of metadata such as the sug-gested label, superclass, definition, and possible syno-nyms. Later, without having to interrupt a data curationsession, they can review their requested terms andmanage the community vetting process for eventuallyadding those terms to the relevant ontologies.Not every provisional term request necessarily resultsin an ontology term addition. In some cases, later reviewof the request reveals that a suitable term does existalready, but for some reason was not discovered. In thiscase the URI of the existing term can simply be enteredas the permanent identifier for the provisional term. Thefailure to discover the already existing term may indicatethat a naming variant is missing from the terms syno-nym annotations, and the ontology editor may thenchoose to add the respective synonym.JOURNAL OFBIOMEDICAL SEMANTICSClark et al. Journal of Biomedical Semantics 2014, 5:49http://www.jbiomedsem.com/content/5/1/49RESEARCH Open AccessA use case study on late stent thrombosis forontology-based temporal reasoning and analysisKim Clark1, Deepak Sharma2, Rui Qin2, Christopher G Chute2 and Cui Tao2,3*AbstractIn this paper, we show how we have applied the Clinical Narrative Temporal Relation Ontology (CNTRO) and itsassociated temporal reasoning system (the CNTRO Timeline Library) to trend temporal information within medicaldevice adverse event report narratives. 238 narratives documenting occurrences of late stent thrombosis adverseevents from the Food and Drug Administrations (FDA) Manufacturing and User Facility Device Experience (MAUDE)database were annotated and evaluated using the CNTRO Timeline Library to identify, order, and calculate theduration of temporal events. The CNTRO Timeline Library had a 95% accuracy in correctly ordering events withinthe 238 narratives. 41 narratives included an event in which the duration was documented, and the CNTROTimeline Library had an 80% accuracy in correctly determining these durations. 77 narratives included documentationof a duration between events, and the CNTRO Timeline Library had a 76% accuracy in determining these durations.This paper also includes an example of how this temporal output from the CNTRO ontology can be used to verifyrecommendations for length of drug administration, and proposes that these same tools could be applied to othermedical device adverse event narratives in order to identify currently unknown temporal trends.IntroductionThe Clinical Narrative Temporal Relation Ontology(CNTRO) [1] and its associated temporal reasoningframework (CNTRO Timeline Library) [2,3] can be usedto facilitate an efficient and semi-automated temporalanalysis of events documented within a narrative. Previ-ously it has been shown how CNTRO can be combinedwith LifeFlow [4] software developed by the Universityof Maryland, which is capable of visualizing event se-quences, such that it is possible to see patterns in theorder of events within several narratives [5]. CNTROsability to correctly answer temporal-related questions re-garding specific events that have occurred within a nar-rative has also been previously demonstrated [6]. Thegoal of this present paper is to illustrate how CNTRO(referring to both the ontology and its associated Time-line Library) can be used to analyze temporal propertiesof events documented across multiple narratives. In this* Correspondence: cui.tao@uth.tmc.edu2Division of Biomedical Statistics and Informatics, Mayo Clinic, Rochester, MN,USA3School of Biomedical Informatics, University of Texas Health Science Centerat Houston, Houston, TX, USAFull list of author information is available at the end of the article© 2014 Clark et al.; licensee BioMed Central LtCommons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.example, CNTRO is able to verify a recommendation forlength of drug administration.The Food and Drug Administration (FDA) requiresnotification of all medical device adverse events that areassociated with malfunction, serious injury, or death [7].Events leading up to the device failure are compiled andreported within a narrative text, which is made publi-cally available through the MAUDE (Manufacturer andUser Facility Device Experience) database [8,9]. Analystsat the Center for Devices and Radiological Health (CDRH)read the event histories of each narrative to identify poten-tial trends that may exist, which includes temporal pat-terns (similar sequences of events, similar durations of orbetween events, similar time/date stamps of event occur-rences, etc.) [10]. However with 80,000 to 120,000 device-related adverse events reported annually to the FDA [11],this approach to trend identification is time consuming,expensive, and the potential exists for a missed trend iden-tification. An automated temporal analysis of adverseevent narratives would lead to faster identification of pat-terns and/or earlier prediction of a future failure, whichcould be used to drive improvements into the next gener-ation of medical devices.Automating temporal analysis of events within a narra-tive is a complex problem. A computer program cannotd. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Clark et al. Journal of Biomedical Semantics 2014, 5:49 Page 2 of 9http://www.jbiomedsem.com/content/5/1/49create a timeline of events and answer time-related ques-tions by querying information directly from a narrativewithout semantic annotation and inference. Human ex-perts can understand temporal relationships through theuse of words such as before, after, during, follow-ing, etc. and appreciate that 1 year, 12 months, and 365days are approximately equivalent even though differencesin granularity are used. To allow for a machine-under-standable data representation and exchange of temporalinformation automatically, the CNTRO System uses aSemantic-Web [12] based framework to apply relation-ships between events within natural language narrativesthrough the use of the RDF (Resource Description Frame-work) triple representation [1]. An RDF triple consists of asubject, an object, and a predicate, which indicates the re-lationship between the subject and the object [10].Consider the following example. 60 days after stentimplantation, antiplatelet therapy was discontinued inpreparation for a splenectomy surgery. In this example,stent implantation is identified as the subject, antiplate-let therapy discontinuation is identified as the object,and after is identified as the predicate. A temporal re-lationship is created between stent implantation and dis-continuation of antiplatelet therapy using a temporaloffset of 60 days.The computer program now understands that stentimplantation occurred first, and discontinuation of anti-platelet therapy occurred second. It also understandsthat the time delay between these two events was 60days. Additionally, there is an inference that because an-tiplatelet therapy was stopped, it had to have started atsome point prior. The CNTRO framework then createsa timeline for events and provides a programmatic queryinterface to access the timeline information. This makesit possible for the time-related information to now bequeried in an automated manner. In our particular ex-ample, we could ask questions such as: Which event oc-curred first? How long after stent implantation wasantiplatelet therapy administration discontinued?Many previous efforts have been attempted to modeltemporal information within computer-based systems.Ontologies such as Time ontology [13] and the SWRLTemporal ontology [14] can formally model temporal in-formation in general and connect with semantic rea-soners for inferring new temporal relations based onsemantics defined within the ontologies. These ontol-ogies only focus on structured data with absolute timeinformation, however, and therefore cannot preciselycapture the temporal information expressed in humanlanguage [1]. In clinical narratives, many temporal fea-tures are expressed in relative (e.g. next Friday) or am-biguous (e.g. early last week) ways. Ignoring this datawill forgo valuable information that could be otherwiseleveraged in clinical research. Models such as the HL7time specification [15] and the TimeML model [16] offera way to represent temporal information form semi-structured or unstructured narratives. These approaches,however, do not provide the formal semantic definitioncapacities for domain knowledge as ontologies do. Inclinical narratives, temporal information is often not ex-plicitly expressed, but rather needs to be inferred beforethe data can be further analyzed. Without a reasoningcomponent, it is difficult to resolve a relatively completepatient history for profound clinical studies [17]. There-fore, we believe that the CNTRO system is necessary asit provides a formal ontology in OWL with well-definedsemantics for the time domain and enables semantic-web [12] based temporal reasoning.MethodsThe CNTRO systemCNTRO [1] is an OWL ontology designed to modeltemporal relations among clinical events. Figure 1 showsthe ontology overview. It models clinical events, tem-poral entities (including time instants, time intervals, re-peated time periods, and durations), time granularity(minute, hour, day, month, year), temporal relationships,and time uncertainties in the semantic web notation. Inorder for users to annotate events and time-related in-formation using CNTRO semantics, a Protégé plug-in,Semantator [18], was developed to interface with CNTROstemporal reasoning framework. Semantator provides userstwo modes: manual annotation mode and semi-automaticannotation mode. In the manual annotation mode, theusers can view the domain ontology, the document to beannotated, and the annotated result in the same environ-ment. For semi-automatic annotation, we have linkedSemantator with Natural Language Processing tools thatsupport automatic named entity recognition. Users canbrowse, revise, and save the annotation results at anytime.Semantator has been used to create gold standard annota-tions for evaluating the reasoning output in our project.The annotated information is stored as an OWL/RDF fileor an RDF triple. The CNTRO Timeline Library is thenused to infer temporal information not explicitly expressedin the original narrative [2]. The CNTRO Timeline Librarycontains a rule-based normalizer that automatically con-verts different temporal expressions into standard formatssuch as the XML dateTime format. It also leverages the se-mantic definitions in the ontology (e.g. OWL DL axioms,property transitivity and inversions) to support temporalrelation inference. These OWL features were handled bythe Pellet OWL reasoner. In addition, the Timeline Li-brary contains a set of Java functions for answering a listof time-related questions, such as when a particular eventhappened, chronological sequence of events, durations ofevents, durations between events, temporal relations be-tween events, and sorting a set of events on the timeline.Figure 1 CNTRO overview.Clark et al. Journal of Biomedical Semantics 2014, 5:49 Page 3 of 9http://www.jbiomedsem.com/content/5/1/49The Timeline Library first calls the reasoner to infer newtemporal relations. It then considers all the temporal rela-tions among the events, normalized timestamps of theevents, and durations of the events to compute new timestamps of events if possible. It can also calculate the dur-ation of an event given the start and end time of the eventand the duration between two events given the timestamps of them. After all the possible inferences and cal-culations are done, it tries to sort a given set of eventsbased on all the above information.Late stent thrombosis adverse event identificationLate Stent Thrombosis (LST) adverse event narrativeswere used to demonstrate how the CNTRO system andits automated temporal relation reasoning can be usedto verify drug therapy duration recommendations. Al-though the exact mechanism or mechanisms of LST arenot known, it has been observed to occur less frequentlywhen dual antiplatelet therapy has been administeredover a period of time [19,20]. Current guidelines recom-mend the administration of dual antiplatelet therapy for3 to 6 months following drug-eluting stent implantation,unless the patient is not at high risk for bleeding, inwhich case therapy is recommended for 12 months [21].The CNTRO System was used to evaluate the order ofevents within each narrative and query both the durationin which antiplatelet therapy was administered and theduration between initial stent implantation and the oc-currence of late stent thrombosis.Narratives used in this study were obtained from med-ical device adverse event reports documented within theMAUDE database. 238 adverse event reports were iden-tified in which late stent thrombosis occurred, definedeither as late within the report or by a duration of 6months between stent implantation and the occurrenceof thrombosis. These narratives were then manually an-notated using Semantator by an expert.Adverse event narrative annotationWe created a domain ontology which includes commonevents that occur after stent implantation was createdwith specific normalized event types. The domain ontol-ogy is relative to simple comparing to the CNTRO. Itonly defines the set of events we what to monitor forour use case. These events were then imported intoCNTRO for temporal relationship modeling. The follow-ing events were included: initial stent implantation, fol-low up stent implantation(s), start and stop time pointsof antiplatelet therapy administration, unrelated surger-ies occurring after stent implantation, late stent throm-bosis, myocardial infarction, admission to the emergencyroom, and patient death. Events such as guide wire in-sertion are required for all stenting procedures; thereforeannotation of these events would not be beneficial andwere therefore not performed. Life-saving events follow-ing the thrombosis detection were also not annotatedwithin the narratives as the focus of the application ofCNTRO was based on verifying the recommended dur-ation of drug administration and not the potential tosurvive following an occurrence of thrombosis.Start and stop point of the antiplatelet therapy were an-notated to determine the duration of therapy. Unrelatedsurgeries which occurred between stent implantation andidentification of thrombosis, myocardial infarction, ad-mission to the emergency room and patient death wereall annotated as events to verify the Event Order and In-ferred Relationship functions of the CNTRO TimelineLibrary.Annotations were performed using Semantator. Thefirst step in the annotation process involves identificationClark et al. Journal of Biomedical Semantics 2014, 5:49 Page 4 of 9http://www.jbiomedsem.com/content/5/1/49of the individual events. As shown in Figure 2, after eachevent is created, the text turns color specific to each eventtype.After the events are created, temporal relationships be-tween the events can be defined through annotation, seeFigure 3. A relationship connects two events and can indi-cate that the events occurred or began at the same time,or that one event occurred or began before another event.If the duration between the events is known, this is anno-tated after the relationship has been defined, see Figure 3.If a specific event has a duration, this information is anno-tated as well.CNTRO timeline evaluationFor each annotated narrative, the CNTRO Timeline Li-brary creates a matrix that visually shows the temporalrelationships between the events, which is a simple wayto track, view, document, and evaluate the accuracy ofCNTRO system timeline computations. Each annotatedFigure 2 Annotating an event.event is included within the matrix. The matrix indicateswhich events occur at the same time, and then ordersthe remaining events on a timeline as applicable. Figure 4shows a sample matrix. Figure 4(a) shows a partial com-plaint file with three events and their corresponding tem-poral constraints highlighted. Figure 4(b) shows the eventdescriptions. Figure 4(c) shows the annotated (asserted)temporal relations between events (e.g., EVENTID-3EQUAL EVENTID-1). Figure 4(d) shows both assertedand inferred temporal relations between events. Based onthe annotation result, the reasoner knows the timestampof EVENTID-1 and EVENTID-3 as well as the fact thatEVENTID-3 and EVENTID-1 happened at the sametime. It can therefore infer the timestamp of EVENTID-3which is the same as the one for EVENTID-1. Thenbased on the timestamps, it can infer the temporal rela-tions among the three events. Finally Figure 4(e) showsthe timeline bucket that includes a set of sorted timelineentries.Figure 3 Annotating a relationship.Clark et al. Journal of Biomedical Semantics 2014, 5:49 Page 5 of 9http://www.jbiomedsem.com/content/5/1/49The annotations of the Late Stent Thrombosis AdverseEvent Narratives were reviewed using these matrices andcompared against gold standard results, in which eventswere manually recorded in timeline order from two ex-ports reading each narrative. The timeline accuracy wasassessed by comparing the gold standard results to theCNTRO Timeline Library results. All conflicting resultsbetween CNTRO and the gold standard were reviewedamong the human experts to determine if the conflictresulted from an error in the gold standard result, anerror in manually annotations, or an error in the reason-ing component of CNTRO.CNTRO duration evaluationDurations can be computed for an individual event, be-tween two events, or between an event and a timestamp.CNTRO first determines if start and end time infor-mation exists for an event to calculate the duration. Ifone of these pieces of information is missing, the pro-gram then computes it by either using a duration anno-tation, Antiplatelet therapy was administered for twomonths (the antiplatelet therapy event is defined herewith a duration of 2 months) or uses a temporal relationto another event with a relative time stamp, Antiplatelettherapy was started in May 2006. In July 2006, the pa-tient underwent prostrate surgery. Antiplatelet therapywas stopped the day before surgery. In this secondexample the occurrence of antiplatelet therapy startingand stopping each have a time stamp, and CNTRO in-fers that antiplatelet therapy was administered for 2months based on the duration between the start and endtimes. In some cases, the duration of a pair of eventscannot be calculated directly (the two events are not dir-ectly connected through the RDF graph), but need to gothrough one or more intermediate events. In this case,the above two functions need to be called iterativelyuntil the duration of the two events are calculated.The adverse event narratives for late stent thrombosiscould describe durations in days, months, and/or years.Month was the most frequent granularity used in thecomplaint data, followed by years, and then days. To beable to compare data from different narratives, the dur-ation granularity was normalized to Month for this usecase as this was the most frequently used granularity,and estimating durations reported in years by number ofdays would likely increase the noise within the data. Thedurations calculated by CNTRO were compared to man-ual calculations to determine accuracy.Application of CNTRO temporal analysisTo provide an example of how the CNTRO system canpotentially be used to evaluate temporal propertieswithin narrative data, survival analysis was performedusing the narratives that specified both a duration ofFigure 4 Sample evaluation matrix (a) original document (partial); (b) event description; (c) asserted temporal relations; (d) assertedand inferred temporal relations; (e) timeline bucket.Clark et al. Journal of Biomedical Semantics 2014, 5:49 Page 6 of 9http://www.jbiomedsem.com/content/5/1/49antiplatelet therapy and time from stent implantation tolate stent thrombosis (or in which a duration could beinferred) to examine therapeutic guidelines for antiplate-let administration duration. Note that as this data comesfrom the FDA MAUDE Database, all records within theexample ended up with an event of late stent throm-bosis. Data of patients who have not had a late stentthrombosis occurrence are not easily accessible; there-fore this example is purely illustrative of the CNTROsystems capability. Similarly, because the data usedwithin this analysis comes from adverse event files indi-cating thrombosis occurred, no patient data requirescensoring.Late Stent Thrombosis adverse event files were dividedinto two different groups based on how long antiplatelettherapy was administered in patients following implantationof a drug-eluting stent. Using current antiplatelet therapyrecommendations, any adverse event narrative specifyingthat antiplatelet medication was administered for less than6 months was segregated into the Shorter Duration of An-tiplatelet Therapy group. Any adverse event narrative indi-cating that antiplatelet medication was administered for 6or more months was segregated into the Longer Durationof Antiplatelet Therapy group. Adverse event narrativesthat did not provide information specifying how long anti-platelet therapy was prescribed were excluded from theanalysis.ResultsCNTRO timeline and duration evaluation238 adverse event narratives included at least two events,such that a timeline could be created within CNTRO forClark et al. Journal of Biomedical Semantics 2014, 5:49 Page 7 of 9http://www.jbiomedsem.com/content/5/1/49system evaluation. For each narrative, the CNTRO system-inferred timeline was evaluated with a gold standard result.The CNTRO system was capable of correctly orderingeach event in all but 8 of the narratives. This resulted inan overall CNTRO timeline accuracy of 95%. There were41 adverse event narratives that included enough informa-tion such that the duration of antiplatelet therapy wasknown. The CNTRO automatic reasoning system had an80% accuracy in inferring and/or calculating this durationof an event. There were 77 adverse event narratives thatincluded enough information such that the duration be-tween stent implantation and identification of late stentthrombosis was known. The CNTRO Automatic reason-ing system had a 76% accuracy in inferring and/or calcu-lating this duration between events. An evaluation of theerrors and discussion of possible enhancements to theCNTRO system is included within the Discussion section.Late stent thrombosis adverse event temporal patternanalysisWithin this paper, the CNTRO system was used to con-firm what has been previously identified as a temporalpattern within the late stent thrombosis adverse event ina semi-automated manner, which is more efficient thanthrough manual observation. The common event patternwithin late stent thrombosis adverse events (stent im-plantation, administration of antiplatelet therapy, discon-tinuation of antiplatelet therapy, late stent thrombosis)was shown by CNTRO system through timeline identifi-cation of events. This result shows that the CNTRO sys-tem has the potential to be applied across multipleadverse event failure modes to identify new trends thathave previously not been observed.Figure 5 Survival analysis of shorter duration of antiplatelet therapylate stent thrombosis adverse events.There were 36 adverse events that specified both theduration between drug-eluting stent implantation andoccurrence of late stent thrombosis, and the duration ofantiplatelet therapy. These 36 reports were used to exe-cute a survival analysis. Although this represents only alimited subset of late stent thrombosis events and doesnot include patient information for those who have nothad late stent thrombosis, the data can still be used forillustration purposes of CNTROs temporal analysis cap-abilities. Late Stent Thrombosis adverse event files weredivided into two different groups based on how long an-tiplatelet therapy was administered in patients with animplanted drug-eluting stent. Adverse event narrativesthat did not provide information specifying how long an-tiplatelet therapy was prescribed were excluded from theanalysis. 14 adverse events reported that antiplatelettherapy was administered for 6 months or less followinginitial stent implantation. 22 adverse events reportedthat antiplatelet therapy was administered greater than 6months.Survival analysis with Kaplan-Meier curve and log-rank test was performed in Minitab. The median time toLST is 27.3 months for longer antiplatelet therapy groupand 14.6 months for shorter antiplatelet therapy group,respectively. The p-value of log-rank test is 0.029, whichindicates a significant association between duration ofantiplatelet therapy and time to LST. Figure 5 supportsthat on average, late stent thrombosis occurs later in pa-tients who continued to take antiplatelet therapy longerthan 6 months. Although this is a retrospective observa-tional study of a subset of LST cases only, the finding isconsistent and supports guidance for use of longer anti-platelet therapy [1]. This example validates the CNTRO(group 1) and longer duration of antiplatelet therapy (group 2) inClark et al. Journal of Biomedical Semantics 2014, 5:49 Page 8 of 9http://www.jbiomedsem.com/content/5/1/49Systems ability to confirm known temporal trends andverify drug administration duration recommendations.DiscussionError analysisAlthough the CNTRO system can provide relativelygood results for our use case, there are still limitationsin the system. First, the evaluation results work well withthe MAUDE reports because these reports are relativelyshort and simple compared to other clinical narrativessuch as clinical notes. Second, since the purpose of thisstudy is to evaluate CNTROs representation and reason-ing capacities, the reports were annotated manually.Many ambiguities and uncertainties were resolved dur-ing the annotation process. Nevertheless, this study pro-vides promising results and valuable analysis for us tocontinue develop the CNTRO system.The CNTRO system was able to order the event se-quences for 95% of the narratives. The reasoner failed dueto different interpretations of time intervals and back-ground assumptions in the manual annotation. Comput-ing the order of two events is difficult when using start orfinish temporal relations when both the start and endtimes cannot be annotated. For example, a narrative mightspecify that antiplatelet therapy began at the time of stentimplantation, and specify that it occurred for a period of 2months. The temporal relation of the event1 (antiplatelettherapy) and event2 (stent implantation) depends onwhether the start and end times of the events can be com-pared. When considering the start time, the two eventsstart at the same time (event1 starts event2). The systemcannot infer the relationship by the end time since theduration of stent implantation is not specified, given thatit occurs at a single point in time. Given the assumptionthat the stent implantation procedure cannot last for 2months, we can infer that event1 ends after event2. Thiskind of background knowledge needs to be further speci-fied in the domain ontology so that the CNTRO systemcan infer the correct order. Additionally, patient deathinherently is known to be the last event in a patient-caretimeline. This kind of inherited order needs to be incorpo-rated in the domain ontology so that the sequence ofevents can be correctly inferred.For duration inference, there are three major reasonsthe program failed to return the correct results. (1) An-notation ambiguities: some narratives contain durationinformation in an ambiguous way such as in range (e.g.,2-3 month), or in different levels of granularity (e.g.,two month and ten days) that the program cannotautomatically process. We are working on expanding theontology so that it can cover ranges. In addition, we areadding more functions to the reasoner so it can normalizedurations in different levels of granularity. (2) Long seriesof events: sometimes the duration calculation involves along series of events. The program sometimes fails whenthere are many intermediate events between the start andthe end events. This is usually due to one or more inter-mediate events were not annotated by the ontology andtherefore were not included during the reasoning process.3) Temporal relation granularity: an annotator can specifythe level of granularity over a temporal relation. For ex-ample, we can specify that the granularity of event1 be-fore event2 is day. This means that the temporalrelation was compared on the granularity of day, whichimplies that although event1 was before event2, but theyhappened on the same day. This assumption was not pro-grammed in the CNTRO reasoning system yet, andcaused errors when calculating the duration betweenevent1 and event3. For example, we know that Event3may have occurred 183 days after event2, but without theassumption that event1 and event2 happened on the sameday, the system cannot infer the duration between event1and event3. The CNTRO reasoner needs to be updated tohandle level of granularity on temporal relations.Areas for improvement of MAUDE database for temporalanalysisThere were some weaknesses identified regarding theuse of adverse event narratives from the MAUDE data-base. The MAUDE database does not have selectablefields for Device Manufacturer or Brand Name. Due tothe free text fields, there are a variety of spellings andmisspellings for both the Device Manufacturer andBrand Name which may have resulted in a missed latestent thrombotic adverse event based on how thesefields were used to sort complaints. The level of detail insome adverse event narratives was very limited and theduration between stent implantation and stent throm-bosis may not have been documented. Additionally, dueto patient privacy some time stamps were removed mak-ing the duration between stent implantation and stentthrombosis unknown. It is possible that late stent throm-bosis occurred in some patients but the complaint narra-tives were filtered out due to not being able to classify theevent as late. Late stent thrombosis adverse events mayalso have been missed while filtering from the files if adifferent term was used within the narrative as there isno searchable failure mode within MAUDE specific tothrombosis.Future CNTRO applicationsOf interest in recent literature is a current investigationinto understanding whether there is a link between in-complete stent apposition (ISA) (separation between thestent strut and the vessel wall) and late stent thrombosis.Stent which are not adequately apposed following im-plantation are referred to as acute ISA, and may be dueto incorrect stent sizing or inadequate expansion of theClark et al. Journal of Biomedical Semantics 2014, 5:49 Page 9 of 9http://www.jbiomedsem.com/content/5/1/49stent. Inadequate stent apposition identified at a laterpoint in time is referred to as late ISA. Late ISA can ei-ther be persistent, meaning that it was the result of inad-equate stent expansion, or acquired, meaning the vesselbecomes enlarged, or plaque or thrombosis in-betweenthe stent and wall dislodged creating space, or the stentrecoiled. There will likely be future studies attempting tolink late stent thrombosis with either persistent or ac-quired ISA. The CNTRO system could be of value inthis investigation to determine if there is a correlation ofpost-dilation frequency with late stent thrombosis or arelationship between the change in apposition and theduration between discontinuation of antiplatelet therapyand thrombus formation.ConclusionAlthough the CNTRO system was able to provide rela-tively good results for this use case, there are still limita-tions in the system. First, the evaluation results workwell with the MAUDE reports because these reports arerelatively short and simple compared to other narrativessuch as clinical notes. More CNTRO system evaluationneeds to be performed using complex electronic healthrecord data. Second, since the purpose of this study is toevaluate CNTROs representation and reasoning capaci-ties, the reports were annotated manually. The currentmanual annotation method is not practical for long-termuse, and an automatic annotation process is currentlyunder development. Third, many ambiguities were re-solved during the annotation process. Uncertainty rea-soning is currently being incorporated into the CNTROsystem to resolve these ambiguities. In spite of these lim-itations, this study provides promising results and valu-able analysis to support continuing the development ofthe CNTRO system.Competing interestsThe authors declare that they have no competing interests.Authors contributionsKC led the data annotation and analysis, and drafted the manuscript. DScontributed to the CNTRO reasoner implementation and result evaluation.RQ is the statistician of the project. CGC contributed on the ontology design.CT led the overall study design. All co-authors participated in writing, reviewing,discussion, and editing of the manuscript. All authors read and approved thefinal manuscript.AcknowledgmentsThis research is partially supported the National Library of Medicine of theNational Institutes of Health under Award Number R01LM011829. We thankMs. Donna Ihrke for her help on annotating the files.Author details1Boston Scientific Corporation, Maple Grove, MN, USA. 2Division ofBiomedical Statistics and Informatics, Mayo Clinic, Rochester, MN, USA.3School of Biomedical Informatics, University of Texas Health Science Centerat Houston, Houston, TX, USA.Received: 20 June 2014 Accepted: 13 November 2014JOURNAL OFBIOMEDICAL SEMANTICSYounesi et al. Journal of Biomedical Semantics 2014, 5:31http://www.jbiomedsem.com/content/5/1/31RESEARCH Open AccessCSEO  the Cigarette Smoke Exposure OntologyErfan Younesi1, Sam Ansari2*, Michaela Guendel1, Shiva Ahmadi1, Chris Coggins3, Julia Hoeng2,Martin Hofmann-Apitius1 and Manuel C Peitsch2AbstractBackground: In the past years, significant progress has been made to develop and use experimental settings forextensive data collection on tobacco smoke exposure and tobacco smoke exposure-associated diseases. Due to thegrowing number of such data, there is a need for domain-specific standard ontologies to facilitate the integrationof tobacco exposure data.Results: The CSEO (version 1.0) is composed of 20091 concepts. The ontology in its current form is able to capturea wide range of cigarette smoke exposure concepts within the knowledge domain of exposure science with areasonable sensitivity and specificity. Moreover, it showed a promising performance when used to answer domainexpert questions. The CSEO complies with standard upper-level ontologies and is freely accessible to the scientificcommunity through a dedicated wiki at https://publicwiki-01.fraunhofer.de/CSEO-Wiki/index.php/Main_Page.Conclusions: The CSEO has potential to become a widely used standard within the academic and industrialcommunity. Mainly because of the emerging need of systems toxicology to controlled vocabularies and also thelack of suitable ontologies for this domain, the CSEO prepares the ground for integrative systems-based research inthe exposure science.Keywords: Exposure, Cigarette smoke, Environmental risk, Ontology, Knowledge representationBackgroundRecently, there has been an increased focus in systemstoxicology on systems-oriented methodologies thatemphasize the understanding on the biological impact ofchemical exposures with increased mechanistic granularity[1,2]. In particular, a recent report by the US NationalResearch Council Committee on Toxicity Testing andAssessment of Environmental Agents advocates for a shiftaway from toxicological assessment at the level of apicalendpoints towards the understanding of the effects of anexposure on toxicity pathways [3]. Moreover, the Foodand Drug Administration (FDA) recently describes asystem-based omics-approach to discover pulmonarybiomarkers and to improve the evaluation of tobaccoproducts [4]. This indicates a growing recognition thatexposure science should be considered as an integratedpart of a systematic approach for risk assessment [5].* Correspondence: sam.ansari@pmi.comEqual contributors2Philip Morris International R&D, Philip Morris Products S.A., Quai Jeanrenaud5, 2000 Neuchâtel, SwitzerlandFull list of author information is available at the end of the article© 2014 Younesi et al.; licensee BioMed CentraCommons Attribution License (http://creativecreproduction in any medium, provided the orTo assess biological responses to environmentalexposure, a systems-based approach attempts to apply anintegrative strategy. A systems-based approach integratesa continuous model from the starting point of exposure todisease outcome [6]. A typical limitation in systemsapproaches is the lack of standards for harmonization ofheterogeneous data types that are experimentally obtainedfrom different resources. Such data types often havevarious structures, formats and annotations, whichadversely affect the degrees of their interoperability andflexibility for integrative methods. Standard terminologiesand proper contextual information are necessary for datasharing, reuse, and integration [7]. Recently, biomedicalontologies have emerged in support of systems approachesby facilitating the annotation of bio-simulation modelsand flexible access to knowledge [8]. The main purpose ofontologies is to organize data and information of aparticular knowledge domain in a structured, controlled,and standard manner. Thus the data can be shared amongscientists in different research areas or accessed andinterpreted using different computational tools. The coreof any ontology is a controlled vocabulary that attempts todescribe a unified definition for all terms and concepts inl Ltd. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly credited.Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 2 of 11http://www.jbiomedsem.com/content/5/1/31a particular subject area [9]. A good example is the GeneOntology (GO) that provides a controlled vocabularydescribing the roles of genes and their products in variousorganisms [10].At the heart of systems toxicology is the understandingof signaling pathways perturbed by biologically activesubstances and the identification of those that have thepotential to cause adverse health effects in humans. Thisrequires integrating OMICs data with in vitro and in vivotoxicological endpoints. The goal of systems toxicology istherefore to link disease susceptibility at the molecularlevel to environmental stress or toxicant effect at theclinical level. Despite advances in various aspects oftoxicogenomics, semantic representation of toxicologicaldata and endpoints is still in its infancy. A variety of tools,platforms, and workflows coexist but each uses its own setof terms and ontologies, a challenge for data exchange.Hardy et al. [11] in their review provide an overview ofexisting toxicology vocabularies and ontologies that arecurrently being used in predictive toxicology initiativesand applications [11].Recently, the toxicology OpenTox ontology has beendeveloped to support standard representation ofrelations between chemical and toxicological datasetsand experiments by unified terms. It is part of theOpenTox framework, which aims at unifying access totoxicity data, predictive networks, and validation procedures[12]. One of the advantages of the OpenTox ontology is thecombination of several related ontologies that covercommon information for chemical compounds, chemicaldatasets, algorithms, models, assays, in vivo studies, andtoxicological endpoints. Moreover, when integrated in asemantic environment, the OpenTox ontology servicefacilitates registering new resources, remote access, andsearching datasets using SPARQL. However, the OpenToxremains a high-level ontology and does not include conceptgranularity for the majority of its components in particularfor the domain of environmental exposure.Lately, the exposure ontology (ExO) has been proposedto provide the missing link between exposure science andvarious environmental health disciplines, includingtoxicology [13]. The main advantage of the ExO is that itprovides the first semantic template for representation ofexposure information around the following four rootconcepts: exposure stressor, exposure receptor, exposureevent, and exposure outcome. Although the currentversion of the ExO includes very general and high-levelconcepts to cover the breadth of the exposure knowledgedomain, it still lacks sufficient granularity that is requiredto capture detailed information. Besides, the ExO is notcompliant with the proposed upper-level ontologystandards such as the Basic Formal Ontology (BFO) [14]or the Descriptive Ontology for Linguistic and CognitiveEngineering (DOLCE) [15], which makes its integrationwith existing or new ontologies semantically moredifficult. Furthermore, Thomas et al. [16] describe theuse of a Smoking Behavior Risk Ontology (SBRO) torepresent risk models for phenotypes associated totobacco smoking behavior [16]. However, the scope oftheir ontology is limited to nicotine pharmacokinetics,pharmacodynamics, nicotine dependence, and clinicalsmoking cessation outcomes.Exposure to tobacco smoke is considered an environ-mental risk factor to human health and it is involvedin the initiation and progression of several respiratorydiseases including chronic obstructive pulmonary dis-eases (COPD) and lung cancer [17,18]. Elimination orminimization of exposure to cigarette smoke provides aclear opportunity to prevent related diseases. Althoughexperiments that measure exposure to environmentaltobacco smoke follow  to a large extent  the typicalprotocols used in toxicology experimental settings, nosemantic framework capturing information specific to thedomain of cigarette smoke exposure risk is available.In response to the need for semantic representation ofthe environmental exposure knowledge domain withparticular focus on the cigarette smoke exposure risk,the Cigarette Smoke Exposure Ontology (CSEO) wasdeveloped.ResultsPurpose of the cigarette smoke exposure ontologyThe development of an ontology starts by defining itsdomain and scope. The scope of the CSEO was definedbased on the potential application of the ontology in thedomain of environmental exposure and was focused onexposure to cigarette smoke. Since setting a properscope helps draw boundaries to the knowledge domainincluded in the ontology, the CSEO is intended toinclude all concepts and terms that represent processesand elements involved in conducting cigarette smokeexposure experiments, in association with cigarette-smokerelated diseases (Figure 1).The scope of the ontology revolves around the exposureexperiment concept and covers description of samplingand experimental factors, test items, test systems, exposurecondition, and link to diseases. These are the mainconcepts to be included in the CSEO by following the lifecycle of ontology building, as described in the Methodssection. Axiomatisation of concepts in the CSEO is basedon the axioms provided in the BFO and ExO. For example,the description of an exposure follows the lines of theexposure event class in the ExO. We have, furthermore,enriched the ExO classes with extra classes that make theontology more specific to cigarette smoke rather than justto exposures in general. The reason for choosing theseconcepts is that they represent the major players in systemstoxicology studies conducted in the domain of smokeFigure 1 High-level schematic representation of the CSEO scope. The scope of CSEO was designed around the key concept of exposureexperiment and its substantial elements.Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 3 of 11http://www.jbiomedsem.com/content/5/1/31exposure. Most exposure experiments follow a similarroutine summarized as follows: the design, factors, andprotocols of an experiment must be defined beforeconducting the experiment. This is often the case forexploratory systems-based approaches and lesser the casefor validated assays. The two main components of anexperiment are often a test system and test item, where thetest system describes the exposure receptor (e.g., a clinical,in vivo, or in vitro setup), and the test item describes theexposure stressor (e.g., chemical compounds, cigarettesmoke, and its characterization). Both of these componentsrequire terms that clearly specify the items. These twocomponents interact in an exposure experiment and theirinteraction is described by the exposure conditions, forexample, exposure transport path, frequency, and doses.The exposure condition, therefore, connects the testsystem and the test items under the experiment description.The exposed test system itself includes sampling proce-dures, which are bound to various endpoint measurements.In the case of systems-based approaches, the samplingprocedures cover a large number of procedures. Thesampling of the test items together with the endpointmeasurements leads to an outcome, which may beassociated with respiratory system diseases.The main purpose of the ontology is to support annota-tion of experimental data sets such as the details of theexperiment and its design, description of test item, testsystem, as well as the exposure path to outcomes.Additional file 1 shows an example on the use of CSEO toannotate experiments. GeneChip Microarray experi-ments generate high-throughput transcriptomic datathat can be reused for other research topics than theoriginally designed experiment. Therefore, the FGED(Functional Genomics Data) society created standardsto exchange these and other similar data types related tofunctional genomics. These standards not only include theformat of exchange but also the minimum requirementsfor experimental annotation so that experimental data canbe correctly reproduced and reused. The exchange fileformat is called MAGE-TAB [19], which includes anIDF file for the definition of the investigation, a SDRFfile for the specification of each sample, and an ADFfile for the specification of the microarray analyte layout.This file format is supported by the repository ArrayExpress[20] and gives open access to a large number of functionalgenomics datasets.While MAGE-TAB defines the exchange format, thereis another standard that describes the required annotationlevel, MIAME [21] the Minimum Information Abouta Microarray Experiment. Additional file 1 shows anexample of the SDRF file that is MAGE-TAB andMIAME compliant. Each row indicates the biologicalsamples with annotations and protocols for biologicalsample transformation. The data model starts with asubject, which is an animal model including additionalinformation about type, strain, and gender. When a protocolapplies, the biomaterial is changed, here from an untreatedanimal to a treated animal. The treatment is furtherdescribed with the exposure item, brand, smoking regimen,nicotine concentration, exposure path, and exposureduration. The next protocol defines a post-exposuretreatment and affects only part of the samples. Afterall exposures, the animal is dissected into organ partsthat are described by the next protocol. The organpart is now further defined as frozen alveolar tissue areafrom left lung of each animal. The next protocols defineYounesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 4 of 11http://www.jbiomedsem.com/content/5/1/31lysis in this tissue and the extraction of RNA that ishybridized on a GeneChip. The SDRF file ends with thereference to the raw data file names, processed data filename, and a summary of all experimental factor values.All protocols are defined in the IDF file (not shown).MAGE-TAB requires the use of ontology definedterms. The ontology resource is specified with locationand version in the IDF. Yellow marked columns inAdditional file 1 show the CSEO annotations thatcover a large fraction of the SDRF file and ensure richand proper annotation. The annotation level of this file ismuch richer than the MIAME requirement and supportsthe reproducibility and reusability of experimental data.Furthermore, conceptualizing and organizing thisknowledge domain in the form of an ontology allowsefficient augmentation of biological knowledge retrievaland extraction. Therefore, the sensitivity to whichbiological mechanisms are modulated in response todifferent risk factors posed by smoking toxicants inthe lungs can be captured.Framework and architecture of the CSEOThe CSEO was designed to be compliant with the BasicFormal Ontology (BFO). The BFO was adopted to definethe upper-level standard architecture. The BFO is designedto support development of domain ontologies for scientificresearch [22]. On the other hand, the ExO is the onlyexisting and intuitive semantic framework used by theexposure science community that provides a goodtemplate for plugging in subdomain ontologies relatedto the exposure domain. Therefore, the ExO superclasseswere used as root concepts for the CSEO. Accordingly,the CSEO populates the ExO for the concepts of theFigure 2 Schematic representation of the main ontology classes andblue: is-a relations; yellow: ExO: is_associated_with; orange: ExO: interacts_wviolet: ExO: interacts_with.cigarette smoke risk subdomain and also complieswith requirements of the OBO Foundry and RO(Relation Ontology). Figure 2 depicts the architectureof the CSEO in relation to BFO and ExO and its mainclasses. Such an architecture is expected to incorporateprovenance into the CSEO so that concepts can be tracedback to their corresponding upper-level classes in ExOand BFO.The CSEO comes in two different versions: the mainCSEO version is a BFO-compliant ontology, and thesecond version is a controlled vocabulary version,hereafter referred to as lexical version. The CSEO-BFOversion consists of the BFO top-level hierarchy intowhich the adjusted ExO hierarchy was plugged. TheCSEO classes were organized underneath these layers asa third layer of granularity. This is the so-calledcomputer-readable format of the CSEO, whichrepresents the formal ontology. The lexical version, onthe other hand, forms the so-called expert-readableformat and does not claim to be a standard-adheringontology in itself. Instead, it is an access point to theCSEO classes that is intuitive and easy to navigate formedical and biological experts. This lexical versionsupports the creation and review of the ontology byvarious experts within the field. It, furthermore, createsa categorization of ontology classes and terms intocontext categories inside the knowledge domain. This isusable also for context-sensitive text mining i.e., itcontains a branch that collects all terms related toexposure outcomes (including terms which are notnecessarily exposure types) compared to the CSEO-BFOversion where they have to be collected manually. Bothversions are available on the CSEO dedicated wiki website.class provenance between BFO, ExO, and CSEO. Arrow legend:ith_an_exposure_stressor_via; brown: MGED: has_experiment_ design;Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 5 of 11http://www.jbiomedsem.com/content/5/1/31Three-dimensional evaluation of the CSEOStructural measureMeasurement of the structural dimension of the ontologyreflects the organizational patterns of the concepts inthe ontology. The first draft of CSEO (version 1.0) iscomposed of 20091 concepts, including the BFO andExO classes. Additional file 2 provides several metricson structural properties of the ontology. These metricsinclude breadth, which relates to the cardinality of paths;depth, which relates to the cardinality of paths in agraph; tangledness, which relates to multi-hierarchicalnodes; and fanout factor, which relates to the dispersionof nodes.As shown in Additional file 2, the high number ofclasses and leaves together with high values for averagewidth and the fanout factor, point towards a broadcoverage of concepts by the ontology whereas the valuesfor depth show specificity of the concept types to thedomain of cigarette smoke exposure risk. The tanglednessfactor of 0.71 indicates the presence of multi-hierarchicalnodes in the ontology (i.e. categories having multipleparents). This is beneficial when greater crosslinking ofthe domain concepts is desired. Different relation typesfrom RO were used to relate concepts in the CSEOincluding part_of , precedes, has_participant, etc. Figure 2illustrates the relational view of the second-level conceptsin the CSEO.Functional measureMeasuring the functional dimension of the ontologyindicates how well the conceptualization of the ontologycaptures the semantic space of the knowledge domain. Thelexicalized ontology was used to calculate precision, recall,and F-score values (69.23, 77.81, 73.26, respectively).The result of this evaluation shows that the ontologyin its current form is able to capture a wide range ofconcepts related to cigarette smoke exposure in theknowledge domain of exposure with a reasonable sensitivityand specificity towards manual curation. The F-score ofabove 73% reflects the quality output of the ontologicalsearch in the published knowledge domain of cigarettesmoke exposure risk.Usability profileUsability profile of an ontology is defined by the extent ofuser-friendliness of the ontology in terms of easy navigation,knowledge accessibility, and meta-information availability.Navigation of the CSEO and its user interface has beenfacilitated using the WebProtégé software, which provides aweb-based access to the content of the ontology withoutthe need for software installation [23]. By following thehyperlink provided on the wiki website under CSEOaccess, the user is directed to the WebProtégé page inwhich clicking CSEO launches the formal BFO-compliantontology whereas clicking CSEO-Expert Readable hyperlinklaunches the hierarchy of controlled vocabulary underlyingCSEO. The search field makes it possible to search for anyCSEO-related concept and locate it in the tree (Figure 3).Feedbacks can be provided through the same portal and adedicated team will process them.To increase the level of efficiency in accessing differentviews (subdomains) of the ontology, the ExO root conceptswere used for further classification of the CSEO instants.By this means, tracking exposure-specific concepts forusers becomes easier and more efficient. Meta-information(i.e. annotations including synonyms, definition, andreference) is provided for each concept in the CSEOto enable users accessing relevant information.Since a proper documentation is needed to ensuredirect access and efficient usability of the ontology, awiki environment was created that contains instructions forusing the ontology, documentation on purpose and scope ofthe ontology, and information about interfacing to theontology. The wiki is accessible through the followinghyperlink in FireFox and Safari browsers: https://publicwiki-01.fraunhofer.de/CSEO-Wiki/index.php/Main_Page.Use-case scenario: answering competency questions byexpertsOntology-driven information retrieval and extractionsystems will guide analysis of literature in preciselyanswering complex scientific questions [24]. The lexicalizedform of the CSEO was used to automatically retrieveand extract domain specific knowledge related tocigarette smoke exposure risk from PubMed abstracts(see Methods). Experts in the knowledge domain ofcigarette smoke exposure risk were asked to designseveral complex questions to be posed to the ontology.The following questions were considered to test theperformance of the ontology: What are the potential effects of the toxicityinduced by tobacco smoke constituents on smokers? Which toxicological studies are available thatmeasure total particulate matter in electricallyheated cigarettes? Which documents report on the use of experimentalmouse models for investigating the effect ofcigarette smoke exposure on the risk of COPD?Queries were formulated in the SCAIView environmentusing the CSEO terminology. SCAIView displays namedentities by markup of the text (e.g. PubMed abstracts).The key feature of SCAIView is the possibility to performontological search in biomedical text using concepthierarchies and synonyms associated with each concept inthe ontology. While using the ontology in SCAIView, thehierarchical organization of the ontology was preserved byFigure 3 Illustration of term search and navigation through the CSEO.Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 6 of 11http://www.jbiomedsem.com/content/5/1/31transforming the ontology OWL file into an XML treestructure. Subsequently, retrieved documents weremanually checked for containing correct answers tothe posed competency questions. Table 1 summarizes thesequeries, their corresponding retrieval rate, and reference tothe relevant documents that contain correct answersto competency questions. Titles of both relevant andirrelevant abstracts are listed in Additional file 3.These results indicate that application of the CSEO-derived terminology to the semantic literature searchleads to retrieval of highly relevant publications containingthe correct answer to the posed competency question.Moreover, highlighted CSEO concepts (terms) bySCAIView allow users to detect and extract knowledgestatements, as illustrated in Figure 4. The CSEO termin-ology can be accessed through the SCAIView searchengine under: www.scaiview.com/scaiview-academia.html.DiscussionThe CSEO covers relevant concepts in the field ofsystems-based toxicology assessment and includes manyTable 1 Answering competency questions using CSEO-drivenQuery (22.03.2013) No. of retrieveddocs:(([CSEO: Smoke Constituent]) AND [CSEO: Toxicity])AND [CSEO: Tobacco]21([CSEO: Electrically heated cigarette]) AND [CSEO:Total Particulate Matter]7(([CSEO: Mouse model]) AND [CSEO: Cigarette SmokeExposure]) AND [MeSH Disease: Pulmonary DiseaseChronic Obstructive]9terms from the conventional toxicology assessment. Thus,the CSEO enables users to capture and integrate exposureinformation from the beginning of the experiment to thepoint of outcome measurement. Compared to otherrelevant ontologies, the CSEO covers a large numberof concept classes including the 44 external ontologies.Additionally, the CSEO uses semi-automated methods forthe term extraction and evaluation and therefore ensuresgood coverage of the knowledge domain.Another advantage of the CSEO over the existingrelated ontologies is the enrichment of high-resolutionconcepts that extends the higher-level exposure ontologyin areas where existing ontologies are particularly weak.For instance, the CSEO describes mouse and rat strainsthat are commonly used in exposure experiments,includes human anatomy with a dedicated subclass tomicroanatomy of the respiratory system, and articulatesstaging of progressive diseases. Moreover, the CSEO canbe used for text mining and knowledge discovery purposesbecause the CSEO is a lexicalized ontology that supportsontology-driven information retrieval and extraction assemantic search in PubMed abstractsNo. of relevantdocs:PMIDs of relevant documents:17 (80.95%) 14521141 [25], 1188959 [26], 18848577 [27], 21651432[28], 17661226 [29], 2002748 [30], 12857635 [31],19330121 [32], 14698566 [33], 11731039 [34],18383128 [35], 16859820 [36], 21651433 [37],21417965 [38], 2165143 1[39], 15072838 [40] ,18464053 [41]7 (100%) 12975773 [42], 12975774 [43], 14698566 [33], 12975771[44], 18590791 [45], 12975772 [46], 16963170 [47]9 (100%) 20133926 [48], 19017996 [49], 23044435 [50], 22279084[51], 18988919 [52], 21700603 [53], 20228194 [54],19491340 [55],16510458 [56]Figure 4 An example of highlighted CSEO terms in the PubMed abstracts as appears in the SCAIView environment. The highlightedterms guide users to informative statements and facilitates their detection, quality check and extraction.Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 7 of 11http://www.jbiomedsem.com/content/5/1/31described in the application scenario. Finally, the ability touse the CSEO in different systems may be facilitatedby the BFO upper-level ontology. Thus, various sub-ontologies relevant to exposure can be integrated withthe ExO-CSEO structure under the BFO framework.Similar to other ontologies, the CSEO suffers from thesparse granularity and misclassification of concepts insome parts of the ontology. Other shortcomings commonto all ontologies such as missing concepts, lack of standarddefinitions, and incompleteness of synonym lists should beaddressed by engagement of the research community andinclusion of their feedback in the process of ontologyenrichment. To facilitate the community contribution, awebsite has been prepared with the aim of collecting usersfeedback and providing access to the latest version of theontology. With the public release of the ontology, it ishoped to reach out to the broader community and collectfeedback and comments, which will be integrated in thefuture versions of the CSEO and be used to improve theontology. With the version 1.0 of the CSEO, the ontologyis sufficiently established to be useful for the scientificcommunity. Furthermore, the project team will continueto review articles, abstracts, and other resources relevantfor the domain and to extract novel terms and synonyms.New releases of the CSEO will be announced and madeavailable through the NCBOs bioportal.ConclusionsWith the creation of the CSEO including relevant termsfor describing exposure experiments, it can serve as apowerful glossary for definition finding and relationshipvisualization, facilitating the right use of terms. TheCSEO has the potential to grow in the future and be usedas a dictionary for various processes such as controllinginternal documents (e.g. Excel Workbooks) or efficient useof Laboratory Information Management Systems (LIMS).This functionality can be used for the identification ofrelevant information (internally or publicly) or for theextraction of relevant knowledge statements.MethodsDefining scope of the CSEOTo define the scope, a qualitative survey was performedinvolving various experts in the domain of environmentalexposure. Experts in toxicology, molecular biology, andclinical pathology fields in PMI were consulted and askedfor their input on the concept classes that they deemas necessary to describe the knowledge domain ofenvironmental exposure from their viewpoint. Based onthis input, boundaries of the knowledge domain to be pre-sented by CSEO was determined as depicted in Figure 1.Resources and toolsDifferent resources were used for construction of theontology (Additional file 4). General and commonconcepts, for which an established ontological definitionexists, were captured. 44 publicly available ontologieslisted in Additional file 4 were re-used and the relevantterms/classes/concepts were selectively integrated in theCSEO along with their annotations. Specialized terms werecollected from various contributors mainly used for internalprocess and workflow tracking in systems, such as Labora-tory Information Management Systems (LIMS). Literaturesources either were searched by keywords (e.g. smoke, tox-icity, cigarette, tobacco in PubMed) or were recommendedby experts (e.g. CORESTA publications or handbooks).Additionally, relevant publicly available abstracts, a numberof relevant full-text articles, as well as The Handbook ofYounesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 8 of 11http://www.jbiomedsem.com/content/5/1/31Cigarette Smoke Toxicity by David Bernhard werereviewed. Here, relevant text bodies were manually an-notated, relevant terms were extracted and enrichedwith synonyms and integrated into the ontology.The Protégé 4.2 (Build 276) [57], developed andmaintained by The National Center for BiomedicalOntology together with its inbuilt HermiT 1.3.3 reasoner[58] were used to construct the ontology. The Knowtatorplugin [59] was used for manual annotation of abstractsinside the Protégé environment. The text-mining toolProMiner [60] was utilized for named entity recognitionof ontology terms in PubMed abstracts and resultswere integrated with SCAIView [61] for context-sensitivevisualization of query results.Ontology development and evaluation processDuring the process of ontology building, a hybrid ap-proach combining both bottom-up and top-downmethods was adopted so that the ontology was popu-lated at the level of superclasses and subclasses simul-taneously. The development of the CSEO wasaccomplished in four phases according to the commonlife cycle of the ontology building [62].Phase I: Knowledge acquisition and conceptualizationConcepts were extracted from previously identifiedresources (see Additional file 4). Resources wereFigure 5 Mapping resources used for generating the ontology contenclassified into two groups based on their contents: struc-tured content and unstructured content. Concepts fromstructured contents such as tables, ontologies, and listswere integrated automatically whereas concepts fromunstructured contents such as free text of publicationswere manually inspected and extracted with the help ofannotation tools. Figure 5 describes the cardinal map-ping of resources to the ontology contents. All conceptsin the ontology were annotated by additional informa-tion including synonym(s), definition(s), and reference(s). In the BFO version of the CSEO, relationshipsamong concepts were defined based on the standard re-lation types in the Relation Ontology (RO) [63] and werechecked using the HermiT reasoner.Phase II: Terminology analysis and concept enrichmentTransformation of the ontology OWL format into adictionary file was achieved using a Java script. Thescript extracts concept names and the correspondingsynonyms from the ontology OWL structure and assignsunique identifiers to each concept. This dictionarywas incorporated into ProMiner for named entityrecognition. In a subsequent step, the major super-class concepts were used as keywords for queries inPubMed. Five hundred relevant abstracts were chosenfrom the result list of each concept search. Aftercompiling all abstracts, the corpus was randomlyts to their corresponding branches in the CSEO.Younesi et al. Journal of Biomedical Semantics 2014, 5:31 Page 9 of 11http://www.jbiomedsem.com/content/5/1/31divided into a training set (250 abstracts) and test set(250 abstracts) using the randomization command inLinux. To create the reference gold standard, suitableannotation guidelines were developed so that annota-tors are guided to keep the breadth and depth of theontology in mind. For enrichment purposes (hereoptimizing both the ontology concepts and the corre-sponding dictionary), the training set was analyzed forfalse-negative entities, which  after individual expertevaluation  was added to the ontology. Classes were an-notated both manually and automatically by mapping themto external ontologies. For this purpose, the National Cen-ter for Biomedical Ontology (NCBO) was used [64]. CSEOclasses were manually annotated with equivalent externalontology classes using an annotation property. These anno-tations were then used to automatically retrieve synonyminformation via the NCBO services. The evaluation processrequired the performance comparison between automatic-ally and manually annotated text from the same set.Phase III: EvaluationA metric-based approach evaluating the ontology wasused in three dimensions after the completion of theontology [65]. Structural evaluation was performed bycalculating features such as depth, breadth, and othertopological features. To evaluate the functional qualityof the ontology in terms of measuring the boundaries ofthe knowledge domain it captures, precision, recall,and F-score values were calculated. Precision is thenumber of true positives (TP) divided by the sum ofTP and false positives (FP). Recall is the number ofTP divided by the number of results that shouldhave been returned (true positives (TP) + false nega-tives (FN)). The F-score = 2 × (precision × recall)/(precision + recall). These values were derived fromthe longest string match found between automaticallyannotated words using ProMiner and the human-curatedgold standard annotation for each abstract in the selectedcorpus [66].Phase IV: Visualization of concepts through the textThe ontology was integrated into the SCAIView literaturemining and visualization environment.Additional filesAdditional file 1: MAGE-TAB SDRF file with CSEO classes.Additional file 2: CSEO ontology metrics.Additional file 3: Titles of retrieved PubMed abstracts foranswering competency questions in Table 1.Additional file 4: Resources used for construction of CSEO.Competing interestsAuthors declare no competing interests.Authors contributionsEY conceived of the study, carried out ontology construction studies,participated in anntation and evaluation, and drafted the manuscript. SAconceived of the study, carried out data collection, participated in ontologyconstruction and evaluation, and helped to draft the manuscript. MGperformed ontology formalization, dictionary generation and technicalevaluation. SA performed ontology construction and participated inontology annotation and evaluation. CC participated in stakeholderengagement. JH participated in the design of the study and coordination.MHA and MCP conceived of the study and participated in its design andcoordination. All authors read and approved the final manuscript.AcknowledgementsThe authors wish to thank PMI internals Walter Schlage, Sandra Wagner,Pavel Pospisil, Michel Rotach, Regina Stabbert, Rodolphe Gualandris, KishorLad, Eva Garcia, Jacques-Antoine Duret, and Carole Mathis for their terminologycontribution and review. Moreover, we would like to acknowledge externalcollaborators Prof. Gerhard Scherer from ABF GmbH, Mehran Sharifi fromLabstat, Jacqueline Miller from JT International SA, Mark Ballantyne fromCovance Laboratories Ltd, and Carolyn Mattingly from NC State University.Authors wish to thank Theo Mevissen, Juliane Fluck, and Bernd Müller for theirassistance in setting up text-mining version of the ontology, as well as AshutoshMalhotra and Stephan Springstubbe for further support on the ontologygeneration.Author details1Fraunhofer Institute for Algorithms and Scientific Computing SCAI, SchlossBirlinghoven, 53754 Sankt Augustin, Germany. 2Philip Morris InternationalR&D, Philip Morris Products S.A., Quai Jeanrenaud 5, 2000 Neuchâtel,Switzerland. 3Carson Watts Consulting, 1266 Carson Watts Rd, King, NC27021-7453, USA.Received: 14 July 2013 Accepted: 3 July 2014Published: 10 July 2014JOURNAL OFBIOMEDICAL SEMANTICSVaz et al. Journal of Biomedical Semantics 2014, 5:43http://www.jbiomedsem.com/content/5/1/43RESEARCH Open AccessTypOn: the microbial typing ontologyCátia Vaz1,2*, Alexandre P Francisco1,3, Mickael Silva4, Keith A Jolley5, James E Bray5, Hannes Pouseele6,Joerg Rothganger7, Mário Ramirez4 and João A Carriço4AbstractBacterial identification and characterization at subspecies level is commonly known as Microbial Typing. Currently,these methodologies are fundamental tools in Clinical Microbiology and bacterial population genetics studies to trackoutbreaks and to study the dissemination and evolution of virulence or pathogenicity factors and antimicrobialresistance. Due to advances in DNA sequencing technology, these methods have evolved to become focused onsequence-based methodologies. The need to have a common understanding of the concepts described and theability to share results within the community at a global level are increasingly important requisites for the continueddevelopment of portable and accurate sequence-based typing methods, especially with the recent introduction ofNext Generation Sequencing (NGS) technologies. In this paper, we present an ontology designed for thesequence-based microbial typing field, capable of describing any of the sequence-based typing methodologiescurrently in use and being developed, including novel NGS based methods. This is a fundamental step to accuratelydescribe, analyze, curate, and manage information for microbial typing based on sequence based typing methods.Keywords: Ontology, Knowledge representation, Microbial typing methodsIntroductionIt is widely known that different strains from a givenbacterial species may have distinct phenotypic behaviors,such as a higher capacity to cause invasive disease, toasymptomatically colonize the host or to present resis-tance to antimicrobials [1]. Such distinguishing charac-teristics can be usually attributed to lineages identified atthe level of the genotype. Microbial typing refers to themethodologies used to identify these lineages and definethem at sub-species level. Microbial typing has impor-tant implications in several health related fields such assurveillance of infectious diseases, outbreak investigationand control, identification of pathogen reservoirs, andstudies on pathogenesis [2]. Traditionally these method-ologies were based on characterizing a limited numberof markers. These markers can be phenotypic character-istics, such as the presence of certain structures on thebacterial surface [3], or genetic characteristics, such asthe presence on the bacterial genome of DNA sequences*Correspondence: cvaz@cc.isel.ipl.pt1INESC-ID, R. Alves Redol 9, 1000-029 Lisboa, Portugal2Instituto Superior de Engenharia de Lisboa, Instituto Politécnico de Lisboa, R.Cons. Emídio Navarro 1, 1959-007 Lisboa, PortugalFull list of author information is available at the end of the articlethat are recognized and cleaved by specific enzymes, gen-erating band patterns by gel electrophoresis [4]. Morerecently, due to the low cost and increasing availability ofDNA sequencing technologies, the development of typ-ing methods became focused on the use of DNA sequenceinformation.Although these methods revolutionized microbial typ-ing, through the creation of novel, unambiguous andeasily understandable nomenclatures for human use,the existing databases still lack interfaces for machine-readable formats, which can be used for automated datasubmission and querying. An ontology describing theconcepts and relationships for sequence-based typingmethods can thus provide a powerful tool in the field.Sharing data annotated in a common language, and ina semantically rich machine-readable format, will allowa better integration of the data existing in databases ofsequence-based typing methods, epidemiological infor-mation and the novel NGS data being produced [5]. Fur-thermore, it can facilitate comparison of different typingschemas, and allow users to mine, in an effective way, theever-growing public data.In this paper, we describe the design of TypOn, amicrobial typing ontology. TypOn was developed from a© 2014 Vaz et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproductionin any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 2 of 11http://www.jbiomedsem.com/content/5/1/43previous prototype ontology [6], and focuses on sequence-based typing methods, including novel NGS methodolo-gies. We discuss the connection of TypOn to existingontologies, how to use it to annotate data already publiclyavailable, and the methods to effectively query it.Domain descriptionSeveral typing methods have been used in outbreakdetection and epidemiological surveillance ranging fromphenotypic methods to fragment based methods andsequence based methods [5,7].Multilocus Sequence Typing (MLST) [8,9] is a widelyadopted methodology to type several diferent species ofmicroorganisms. This method is based on determiningthe sequence of internal fragments of multiple (typicallyseven) loci encoding proteins with housekeeping func-tions. By a locus we understand a specific location inthe chromosome, where different sequences occupying agiven locus define distinct alleles of that locus. Each set ofloci defining an MLST scheme is proposed by a group ofresearchers that usually also provide evidence supportingits discriminatory power and suitability for its intendedpurpose. When applying the methodology, the set of alle-les identified at the loci considered define a sequence type(ST), a key identifier with this methodology. The loci cho-sen are usually different for each species, although somespecies may share some or even all loci in their MLSTschemas. The number of loci can be also variable andcan be greater or smaller than the seven loci more com-monly adopted. MLSTs large appeal for the communitywas the reproducibility and portability of results, whichallowed the deployment of databases for several bacte-rial species [10-12]. The strain nomenclature developedby MLST facilitated the global tracking and immediatecomparison of microbial strains in clinical and researchsettings.Another sequence-based method that derived its suc-cess from a common nomenclature and the ease ofstrain classification, was spa typing for Staphylococcusaureus [13], an important pathogen that is one of themajor causes of nosocomial infections [14]. This methodis based on repeat sequences present at a single locus, thespa gene. These repeats are short sequences of DNA (inthe case of spa about thirty nucleotides) that, althoughsharing consensus characteristics, can be variable in theirsequence. Their number is also variable and, in the case ofspa typing, this is expressed by a string of numbers repre-senting the identity and number of repeats present at eachspa allele. An updated list of identified repeats and spatypes can be found at http://spaserver.ridom.de/.Multilocus Variable Number of Tandem Repeats Anal-ysis (MLVA) [15] is a method that is based on the num-ber of repeat patterns present on several defined locusthat are, similarly to MLST, defined in a schema. Severalschemas are also available in multiple websites suchas http://www.mlva.eu/, http://www.mlva.net/ or http://www.miru-vntrplus.org/.With the advent of Next Generation Sequencing (NGS)technologies, and the ability to produce a draft genomesequence of a microbial strain in a couple of days insteadof weeks or months, researchers can use this informa-tion to classify the strain according to new or previouslyavailable sequence-based typing methods. Furthermore,novel typing methods are being developed that are able toprobe tens, hundreds or even thousands of different lociacross the genome [16], effectively extending the MLSTconcepts. An example of such a method is ribosomalMLST [17]. Other whole genome methodologies probethe genome for Single Nucleotide Polymorphisms (SNPs)when compared to reference genomes [18,19] in order todiscriminate strains.The main goal of typing methods is the characteriza-tion of individuals existing in a given sample. The sampleunder study can be recovered from sick or healthy sub-jects or directly from the environment. The first processis the isolation of the microorganisms to be character-ized from the sample collected. Each individual, or in thecase of bacteria, each colony isolated from the micro-bial population then becomes an isolate, referring tothe process of isolating it from a sample that containsmany microorganisms potentially representing distinctspecies. In particular each isolate can then be identifiedat subspecies, species, or genus level, identifying it asbelonging to a given taxonomic unit, i.e., taxon. More-over, since most microorganisms reproduce asexually thesubsequent propagation of this isolate in the laboratory asan axenic culture, would be expected to generate a clonalpopulation.Each isolate can be associated with typing informationand ancillary details. The isolate can have the nucleotidesequence of its genetic material determined and multi-ple typing characteristics defined through different typingmethods, such as MLST, spa typing or MLVA [15]. Thereare several categories of typing methods. For instance,although MLST and spa typing are both genotypic meth-ods, MLST is a multi-locus typing method while spatyping is a single-locus typing method.For the specific case of multi-locus typing methods,several schemas have been defined as indicated above.Each schema is characterized by the (possibly ordered)set of loci selected for a given taxon, usually defined atthe species level. Each schema is then administrated byone or a group of microbiologist, or by an institution, andeach isolates ancillary data and respective typing informa-tion are deposited in public repositories and validated bya curator. Different schemas can be defined for the sametaxon and an ontology will facilitate understanding therelationships between different schemas. Ancillary dataVaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 3 of 11http://www.jbiomedsem.com/content/5/1/43include information about the place where the microor-ganism was isolated, information about environment orhost, and other possible contextual details. Later on, wewill discuss how this information can be added and anno-tated in the context of TypOn.We also note that data ownership is a particular delicateissue in the surveillance of communicable diseases and, aswe will discuss later, the approach proposed in this paperallows a straightforward implementation of the agreedupon policies. The sensitive nature of the informationand the ethical issues associated can be also safeguardedthrough the application of suitable access control lev-els based on the ownership structure embedded in theontology.Microbial typing ontologyTypOn is an OWL ontology for describing microbial typ-ing, focusing on sequence based methods. Such a descrip-tion will provide a universal framework for the exchangeof information on the microbial typing field, allowingthe integration of data coming from the numerous anddisparate online databases.In the next Section, we describe and discuss the TypOnontology and its suitability for describing knowledge inthe microbiology typing methods domain. Later, we illus-trate how to annotate and query microbial typing data andinformation using TypOn.Ontology modelingThe main concepts and properties defined in TypOnare depicted in Figures 1 and 2. The ontology, whichis an extended version of a previous prototype ontol-ogy [6], is available at http://purl.phyloviz.net/ontology/typon. In this current version, we added new con-cepts and refined existing ones, based on commentsmade by domain experts, including microbiologists andindustry partners. The aim of TypOn is to representknowledge about any of the currently used sequence-based microbial typing methods. TypOn can be reusedas well as expanded, whenever new requirements andnew technologies demand it. For backwards compatibil-ity, older versions can still be used by explicitly stat-ing the TypOn version. For instance, version 20140606may be accessed and referenced as http://purl.phyloviz.net/ontology/20140606/typon. This ontology has beendeveloped in the context of the Patho-NGen-Traceproject http://patho-ngen-trace.eu/project/ as a way tostandardize microbial data exchange between onlinedatabases and current software using those databases.Ongoing developments, new versions, as well as usecases and examples, can be found in the project repositoryhttps://bitbucket.org/phyloviz/typon, and documented inthe project wiki https://bitbucket.org/phyloviz/typon/wiki. The base URL for TypOn, http://purl.phyloviz.net/ontology/typon, redirects transparently to the last stableversion of the ontology, in RDF/XML format, located inthe master branch in the development repository.The current version of TypOn has 44 classes, includingthose imported from other ontologies, and 47 properties,as shown in Table 1.Although we will discuss examples of bacterial typing,we believe that TypOn is equally applicable to typingmethods used to characterize any microorganism. Natu-rally, this implies that other loci, in addition to those usedin the examples, will have to be entered into the ontologyand new schemas will have to be defined. The ontologyoffers a flexible framework with which the existing andfuture sequence typing methods can be described and theexamples are meant to illustrate its application as wellas its flexibility. As described above, the first process inFigure 1 The Isolate concept and its object properties. Dashed lines represent object properties and solid lines represent subclass relations, e.g.,STAllele is-a Allele. Properties hasLocus and isOfLocus have cardinality of exactly one. All other properties do not have any cardinality restriction.Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 4 of 11http://www.jbiomedsem.com/content/5/1/43Figure 2 Typing Information hierarchy and its object properties. Dashed lines represent object properties and solid lines represent subclassrelations, e.g., Genotypic is-a Sequence-based Typing Information. Property hasSchema has cardinality of exactly one. All other properties do not haveany cardinality restriction.microbial typing is the recovery of the microorganisms tobe characterized from the sample collected and, thus, Iso-late is a main concept in TypOn and it is characterized byseveral properties.Figure 1 shows an overview of the Isolate class and itsrelated concepts and properties.Each Isolate when identified at subspecies, species, orgenus level, belongs to a certain Taxon, a relation thatwe express through the property isFromTaxon. The TaxonTable 1 Statistics concerning concepts and propertieseither defined in TypOn or reused from otherontologies/vocabulariesOntology ConceptsObject Dataproperties propertiesMicrobial Typing 24 21 25Ontology (TypOn)Basic Formal 8 1 0Ontology (BFO) [20]Sequence Ontology 6 0 0(SO) [21]Environment Ontology 2 0 0(EnvO) [22]Ontology for Biomedical 1 0 0Investigations (OBI) [23]Uniprot Core 1 0 0Ontology (UNIPROT) [24]Friend of a 1 0 0Friend (FOAF) [25]The DBpedia 1 0 0Ontology [26]concept is reused from the Uniprot Core Ontology [27]for classifying life forms. Moreover, we define that eachIsolate is an Organism, a concept that is reused fromthe Ontology for Biomedical Investigations (OBI) [28].We also know the Place from where each Isolate wasrecovered, which we describe through the property iso-latedAt. As with the Taxon concept, the Place concept isreused from another ontology, in this case the DBpediaontology [29]. One can also define the environment mate-rial or system where a given Isolate was collected. Theseconcepts are already found in the environment ontol-ogy (EnvO) [22] and, hence, we reused them and we addthe properties isolatedOnMaterial and isolatedOnSystem,relating these concepts with the concept Isolate. Each Iso-late, can have Sequence information, i.e. the nucleotidesequence of its genetic material, and Typing Information.The property hasTypingInformation commonly has cardi-nality higher than one for each Isolate, since several typingmethods, such as MLST or MLVA, can be applied to thesame Isolate. Later we will present an example of an isolatewith both MLST and spa typing information.In this context, it is important to note that TypingIn-formation is the root of a class hierarchy (see Figure 2).This hierarchy can be extended by including new, andalready known, typing techniques, such as phenotypicinformation related to antibiotic susceptibility. In particu-lar, we are able to distinguish different categories of typingmethods. Let us considerMLST and spa Typing. Both areGenotypic methods, but the first is a Multilocus methodwhile the latter is a Single Locusmethod.Let us consider the concepts Locus, Allele, Schema andMLST, as depicted in Figure 3. In MLST we can haveVaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 5 of 11http://www.jbiomedsem.com/content/5/1/43Figure 3MLST typing information. Dashed lines represent object properties and solid lines represent subclass relations. Property isOfLocus hascardinality of exactly one. All other properties do not have any cardinality restriction.several typing schemas administrated by some Agent, aconcept reused from FOAF ontology [25], for instance thedatabase curator, and composed by a set of Schema Parts.Such schemas are represented through the class Schema,which has associated the property hasSchemaPart. TheSchema Part concept allows us to identify a particularLocus and provide the index order for that locus in theunderlying schema through object property hasLocus anddata property index, respectively. In the case of MLST,each locus identifies a region within the coding sequenceof an housekeeping gene. Thus, the object property hasLo-cus associated to the Schema Part concept has cardinalityof exactly one.As depicted in Figure 1 and indicated above, each Iso-latemay have been characterized by more than one typingmethod. In the case of MLST, this kind of typing infor-mation can be subject to different schemas, resulting indifferent sequence types, which are characterized by thealleles found at each locus. Therefore, in our ontology,we associate to each MLST both a schema and a set ofobserved alleles through properties hasSchema and hasI-dentifiedAllele, respectively. The property hasSchema hascardinality of exactly one and, hence, MLST and MLVAinstances must be related with one and only one givenSchema. Notice that STAllele is an Allele (see Figure 1)defined in a MLST Schema, disjoint from Spa Allele andVNTAllele. So, we only associate to MLST typing infor-mation the concept of STAllele. This is necessary given thedifferent ways in which the distinct alleles are defined inthese typing methods.The entity Spa Allele is used in the context of spa typ-ing. Each spa typing information has amatching spa Allele,which corresponds to a specific sequence of repeats foundas a result of the amplification of the locus of the spa geneof Staphylococcus aureus. In Figure 2 we can observe that agiven Spa Allele has repeat profile parts, i.e., an entity thatstores the index order of a given Spa Repeat in a given SpaAllele. Notice that each Spa Repeat may occur more thanonce in a given Spa Allele and that different Spa Allelescan have distinct number of repeats.Notice also the difference between properties hasI-dentifiedAllele and hasDefinedAllele relating respectivelyMLST and Locus concepts with the Allele concept. Onecould imagine that alleles associated to a given MLSTinstance could be obtained through the defined Schema,since property hasDefinedAllele allows to relate loci andalleles. But this is not the case. A Locus may have associ-ated many alleles, with each of them belonging to manyMLST instances, and hence we cannot identify the allelebelonging to a particular MLST instance. That can beaccomplished through property hasIdentifiedAllele whichrelates each MLST instance with its identified alleles.Although we did not add those kind of assertions in ourontology, we can still use this information to assert thatthe identified alleles for a given MLST instance are suf-ficient against a given Schema. We could even infer forwhich schemas a given MLST instance provides enoughinformation.It is also important to note that, by knowing only theLocus, it is possible to identify the Taxon that it belongsto, using the isOfTaxon property (see Figure 1).Additional information for each class, such as samplecollection date and other id, are described through dataproperties. For instance, the class Allele has data proper-ties such as id and date entered. The class Isolate has dataproperties such as sample collection date and date entered.Whenever possible, we reuse concepts from and estab-lish relations with other ontologies as indicated in Table 1.An Agent is a concept imported from the FOAF ontol-ogy [25] and it can be a person, a group of persons oreven an institution. In TypOn it allows the descriptionof a person or a group of persons who have curated theVaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 6 of 11http://www.jbiomedsem.com/content/5/1/43information about the isolate, who have submitted thatinformation to the database and who own the isolate.These relations are described by the object propertiescurated by, sent by and owned by, respectively. Reusingthe Agent concept is extremely useful because it will allow,for instance, the use of TypOn together with the WebAccess Control ontology [30] for defining access controllevels in microbial typing databases, an important issue asmentioned above. Several applications in defining accesscontrol have been widely discussed and are well known tothe research community [31,32].All TypOn concepts were derived from the Basic FormalOntology (BFO) [20], the Ontology for Biomedical Inves-tigations (OBI) [23] and the Sequence Ontology (SO) [21]to ensure upper-level interoperability with other ontolo-gies. In order to avoid huge imports in TypOn we haveused OntoFox [33] to query and import only relevant con-cepts in top level ontologies. These are then imported as,and are available at, https://bitbucket.org/phyloviz/typon/raw/master/imported.owl.TypOn was also submitted to the BioPortal (https://bioportal.bioontology.org/), hosted by the National Cen-ter of Biomedical Ontologies (NCBO), being also availableat http://bioportal.bioontology.org/ontologies/TYPON.Concepts such as Schema and Typing Information arequalities (BFO_0000019), i.e., a categorical property. Asdiscussed, we have classified an Isolate as an organism(OBI_0100026). More details in Figure 4.Figure 5 depicts the Typon concepts that are related tothe Sequence Ontology. Notice that we define Locus asa region (SO:0000001), since it is a named region on agenome.Another example is the Spa Repeat concept which isa repeat unit (SO:0000657). Both Locus and Spa Repeatare also generically dependent continuant (BFO_0000031)since region (SO:0000001) and repeat unit (SO:0000657)concepts are subclasses of BFO_0000031 as defined in thesequence ontology (SO). Thus these concepts are relatedto both SO and BFO (see Figures 4 and 5).Use caseIn this section, we illustrate how we can represent typ-ing information annotated with the TypOn ontology. Ourexample makes use of data regarding the characteriza-tion of a Staphylococcus aureus isolate for the purposeof this case. We will use the Turtle language [34] in thedescription of our isolate.We can represent the isolate named Sa66296 as follows:@prefix dbpedia:<http://dbpedia.org/resource/> .@prefix typon:<http://purl.phyloviz.net/ontology/typon#> .:Sa66296rdf:type typon:Isolate ;rdfs:label "Sa66296"@en ;typon:sampleCollectionDate"2013-05-06T00:00:00Z"^^xsd:dateTime;typon:typon#dateEntered"2014-02-19T00:00:00Z"^^xsd:dateTime;typon:isolatedAt dbpedia:Lisbon ;typon:hasTypingInformation:mlst105 ,:spa_t002 ;typon:isFromTaxon<http://www.uniprot.org/taxonomy/1280>.This is an instance of typon:Isolate labelled asSa66296. rdfs:label is an instance of rdf:Property thatmay be used to provide a human-readable versionfor the name of a resource. We further specify thatit has two pieces of typing information mlst105 andFigure 4 Relationships among TypOn, the Basic Formal Ontology (BFO), and the Ontology for Biomedical Investigations (OBI). The linesrepresent subclass relations.Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 7 of 11http://www.jbiomedsem.com/content/5/1/43Figure 5 Relationship between TypOn and the Sequence Ontology (SO). The lines represent subclass relations.spa_t002 (instances of typon:MLST and typon:spaTyping,respectively). Thus, these pieces of typing information areother individuals annotated with our ontology. We cankeep track of the dates when the isolate was collected andwhen the isolate was entered into the system using proper-ties sampleCollectionDate and dateEntered, respectively.We also describe the origin of the isolate, with the indi-vidual labelled Lisbon which has type dbpedia-owl:Place.Note that this isolate was recovered in Lisbon, Portugal,represented as a resource in DBpedia. Figure 6 depicts theisolate information.The individual labelled mlst 105 represents thesequence based typing method with a schema defined bythe sequence of seven housekeeping loci, represented asfollows:@prefix typon:<http://purl.phyloviz.net/ontology/typon#> .:mlst105rdf:type typon:MLST ;rdfs:label "mlst 105"@en ;typon:ST "105"^^xsd:string ;typon:hasIdentifiedAllele:arcc1 ,:aroe4 ,:glpf3 ,:gmk_4 ,:pta_12 ,:tpi_1 ,:yqil28 ;typon:hasSchema:schema1 .Note that the individual labelled schema1 identifiesthe seven housekeeping loci, using individuals of typetypon:SchemaPart for keeping track of the index of eachlocus:@prefix typon:<http://purl.phyloviz.net/ontology/typon#> .:schema1rdf:type typon:Schema ;rdfs:label "schema 1"@en ;typon:PubMedID "10698988"^^xsd:string ;typon:hasSchemaPart:schema_part_1 ,:schema_part_2 ,:schema_part_3 ,:schema_part_4 ,:schema_part_5 ,:schema_part_6 ,:schema_part_7 .Let us now describe the individual that represent thearcc locus, the first locus in the considered schema asdescribed by the individual labelled schema_part_1:@prefix typon:<http://purl.phyloviz.net/ontology/typon#>.:schema_part_1rdf:type typon:SchemaPart ;rdfs:label "schema part 1"@en ;typon:index "1"^^xsd:int ;typon:hasLocus :arcc .:arccrdf:type typon:Locus ;rdfs:label "arcc"@en ;typon:hasDefinedAllele :arcc1 ;typon:isOfTaxon<http://www.uniprot.org/taxonomy/1280> .:arcc1rdf:type typon:STAllele ;rdfs:label "arcc 1"@en ;typon:id "1"^^xsd:int ;typon:isOfLocus :arcc ;typon:hasSequence :sequence1 .Note that the identified alleles of the loci can bedirectly obtained from the individual labelled mlst105.Furthermore all possible defined alleles can be obtainedfrom the respective loci. It is also possible to obtain thelocus that is associated to each allele, namely by prop-erty typon:isOfLocus. Figure 7 summarizes the represen-tation of the MLST typing information concerning theisolate in our example. The complete example is availableVaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 8 of 11http://www.jbiomedsem.com/content/5/1/43Figure 6 The isolate Sa66296 and its object and data properties. Dashed lines represent object properties and solid lines represent subclassrelations. Described properties do not have any cardinality restriction.at https://bitbucket.org/phyloviz/typon/raw/master/test/Sa66296.ttl.Annotating dataIn this section we discuss how to annotate a large datasetwith the TypOn ontology and how to perform queries.We started by writing a D2RQ mapping for the dataavailable for Neisseria spp, one of the databases avail-able in our local BIGSdb [16] installation. D2RQ [35,36]is a mapping language and platform for treating non-RDF relational databases as virtual RDF graphs, aimingto expose RDBs on the semantic web. The mappingallows us to reuse existing vocabularies and ontologies,such as TypOn, to map relational schemas, such as theone underlying BIGSdb. We note that our mapping isnot exhaustive and that we just annotated part of thedata with TypOn ontology. The mapping is availableat https://bitbucket.org/phyloviz/typon/raw/master/test/BIGSdb_d2r_mapping.ttl.Even though the D2RQ web app provides a SPARQLendpoint, it turns out that queries may take a long time tocomplete, causing high loads in the underlying database,and the web app may also become irresponsive. To over-come this issue we used the tool dump-rdf available withD2RQ to dump all triples and we uploaded them to a localinstance of Virtuoso, which among other functionalitiesincludes a highly efficient triple store (http://virtuoso.openlinksw.com/). Hence, a more responsive SPARQLendpoint is available at http://data.phyloviz.net/sparql,where we should select http://rest.phyloviz.net/neisseria/as the default graph. All resources at http://rest.phyloviz.net/neisseria/ are also dereferenceable through rewriterules against the SPARQL endpoint.Let us consider a few SPARQL queries for illustra-tion purposes. Imagine that we wanted to define a newMLST schema that includes loci carB, glnA, and rpiA.How can we find all isolates for which we already havetyping information under this new schema? It turnsout that we can answer this question with a SPARQLquery:PREFIX typon:<http://purl.phyloviz.net/ontology/typon#> .PREFIX uniprot:<http://purl.uniprot.org/core/> .PREFIX neisseria:<http://rest.phyloviz.net/neisseria/> .SELECT (str(?isolate_id) as ?isolate)?species?carB?glnA?rpiAFROM <http://rest.phyloviz.net/neisseria/>WHERE {?isolate_restypon:isFromTaxon ?taxon;Vaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 9 of 11http://www.jbiomedsem.com/content/5/1/43Figure 7MLST typing information for isolate Sa66296. Dashed lines represent object properties. Instancemlst 105 of typon: MLST is related withinstance schema 1 of typon: Schema through object property typon: hasSchema with cardinality of exactly one. Instancemlst 105 is also related withseveral instances of typon: Allele, e.g., instance aroe 4, through typon: hasIdentifiedAllele. Instance schema 1 is related with several instances of typon:SchemaPart, e.g., instance schema part 2, through property typon: hasSchemaPart. Each instance of typon: SchemaPart is related with an instance oftypon: Locus, e.g., instance schema part 2 is related with instance aroe, through property typon: hasLocuswith cardinality of exactly one. Each instanceof typon: Allele is related with an instance of typon: Locus, e.g., instance aroe 4 is related with instance aroe, through property typon: isOfLocus withcardinality of exactly one. Each instance of typon: Locus is also related with an instance of typon: Allele, e.g., instance aroe is related with instance aroe4 through property typon: hasDefinedAllele.typon:hasAllele ?carB_res ;typon:hasAllele ?glnA_res ;typon:hasAllele ?rpiA_res ;typon:isolateId ?isolate_id .GRAPH ?taxon {?taxon uniprot:scientificName ?species;uniprot:rank uniprot:Species.}?carB_restypon:isOfLocusneisseria:resource/loci/carB ;typon:id ?carB .?glnA_restypon:isOfLocusneisseria:resource/loci/glnA ;typon:id ?glnA .?rpiA_restypon:isOfLocusneisseria:resource/loci/rpiA ;typon:id ?rpiA .}ORDER BY ?species ?carB ?glnA ?rpiAWe can then submit this query to our endpoint at http://data.phyloviz.net/sparql and our results include:isolate species carB glnA rpiA...058-24 Neisseria lactamica 22 18 2909002S1 Neisseria lactamica 22 18 42...92001 Neisseria meningitidis 1 1 1154 Neisseria meningitidis 1 1 1H1964 Neisseria meningitidis 1 1 1...As another example, we may be interested in exploringthe variability at the third locus in any MLST schema inour dataset, but only for isolates of Neisseria polysaccha-rea found in Canada. Taking into account the relation-ships defined in TypOn and Uniprot, we can retrieve thisvariability as follows through a federated query:PREFIX typon:<http://purl.phyloviz.net/ontology/typon#> .PREFIX uniprot:<http://purl.uniprot.org/core/> .PREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#> .PREFIX xsd:<http://www.w3.org/2001/XMLSchema#> .SELECT DISTINCT?isolate?schema?locus?allele_idWHERE {SERVICE<http://beta.sparql.uniprot.org/sparql> {?taxonuniprot:scientificName"Neisseria polysaccharea".}?isolatetypon:isFromTaxon ?taxon;typon:isolatedAt<http://dbpedia.org/resource/Canada>;typon:hasAllele ?allele .?schema typon:hasSchemaPart ?spart .?spartVaz et al. Journal of Biomedical Semantics 2014, 5:43 Page 10 of 11http://www.jbiomedsem.com/content/5/1/43typon:hasLocus ?locus_res ;typon:index "3"^^xsd:int .?locus_res rdfs:label ?locus .?alleletypon:isOfLocus ?locus_res ;typon:id ?allele_id .}By submitting this query to http://data.phyloviz.net/sparql, we obtainisolate schema locus allele_iddb:isolates/ db:schemes/1 aroE 2865194db:isolates/ db:schemes/1 aroE 2895195where the prefix db: stands for http://rest.phyloviz.net/neisseria/resource/.Final remarksTypOn provides the basic concepts needed to establishthe vocabulary and the semantic relationships for differ-ent sequence-based typing methods, and it is designed toallow further expansion. It was defined based on threedifferent approaches to sequence-based typing: using theDNA sequence information directly, using the sequenceof repeats in a DNA sequence, and for MLVA, using thenumber of repeats in a locus. Since these three approachescan be used to define many of the existing typing meth-ods, TypOn can be easily expanded to encompass thenewer multilocus typing techniques that are appearingbased on NGS technologies, defined by expansion ofthe MLST concepts to larger numbers of genes [37] orby Single Nucleotide Polymorphism approaches, whereeach position on the genome can be viewed as a locusand the nucleotide present as an allele. Other advan-tages of this ontology is that it can provide a consistentlink with legacy microbial typing techniques and pro-vide a way to describe and annotate the evolution ofspecific typing schemas. This will be of paramount impor-tance, if schemas that will be constructed by groupingloci from existing schemas or adding new loci, are tobe designed and represented in an accurate way. Thisontology is the first stepping stone on the implementa-tion of a semantic web approach for the data repositoriesin this field. It lays the foundation for a common lan-guage that can be used to integrate and link data fromdifferent typing databases and for a complete merging ofmicrobial typing withmicrobial genomics. Using the strat-egy discussed in the previous section (Annotating data),a SPARQL endpoint is already deployed for the PubmlstMLST databases at http://pubmlst.org/sparql. This end-point accesses data annotated using TypOn for MLSTdatabases for 75 distinct bacterial species that are hostedat Pubmlst.org and further 29 species hosted externallyto Pubmlst.org. A RESTful API is also being developed tofacilitate data access without requiring the SPARQL end-point. Future work will focus on expanding the ontologyand creating and deploying RESTful APIs to perform notonly custom querying but also automated submission andcuration of data for authenticated users, in order to speedup and distribute the curating process, and ensure betterquality and reproducibility of data in the field of microbialtyping.Competing interestsThe authors declare that they have no competing interests.Authors contributionsAll authors contributed to the development of the ontology. CV, APF, JAC, MRand MS created/edited textual definitions of ontology terms. CV, APF, MR andJAC have wrote and edited the manuscript. CV, MS and JAC have contributedwith the individuals that exemplify the ontology. APF has annotated data inour local BIGSdb and JAC has helped him in the SPARQL queries examples. Allauthors discussed, read and approved both the ontology and the manuscript.AcknowledgementsThe work presented in this paper made use of data available at MLST.net [10],PubMLST [11] and Institut Pasteur MLST Databases [12]. This study was partlysupported by the European Community grant FP7-278864-2(PathoNgenTrace, http://www.patho-ngen-trace.eu/), and by national fundsthrough FCT  Fundação para a Ciência e Tecnologia, under projectsPTDC/EIA-CCO/118533/2010, EXCL/EEI-ESS/0257/2012, andPEst-OE/EEI/LA0021/2013.Author details1INESC-ID, R. Alves Redol 9, 1000-029 Lisboa, Portugal. 2Instituto Superior deEngenharia de Lisboa, Instituto Politécnico de Lisboa, R. Cons. Emídio Navarro1, 1959-007 Lisboa, Portugal. 3Instituto Superior Técnico, Universidade deLisboa, Av. Rovisco Pais 1, 1049-001 Lisboa, Portugal. 4Instituto deMicrobiologia, Instituto de Medicina Molecular, Faculdade de Medicina,Universidade de Lisboa, Av. Prof. Egas Moniz, 1649-028 Lisboa, Portugal.5Department of Zoology, University of Oxford, Oxford, UK. 6Applied Maths NV,Keistraat 120, 98308 Sint-Martens-Latem, Belgium. 7Ridom GmbH, Mendelstr.11, D-48149 Münster, Germany.Received: 20 June 2014 Accepted: 6 October 2014Published: 18 October 2014JOURNAL OFBIOMEDICAL SEMANTICSKarystianis et al. Journal of Biomedical Semantics 2014, 5:22http://www.jbiomedsem.com/content/5/1/22RESEARCH Open AccessMining characteristics of epidemiological studiesfrom Medline: a case study in obesityGeorge Karystianis1,2, Iain Buchan2,3 and Goran Nenadic1,2*AbstractBackground: The health sciences literature incorporates a relatively large subset of epidemiological studies thatfocus on population-level findings, including various determinants, outcomes and correlations. Extracting structuredinformation about those characteristics would be useful for more complete understanding of diseases and formeta-analyses and systematic reviews.Results: We present an information extraction approach that enables users to identify key characteristics ofepidemiological studies from MEDLINE abstracts. It extracts six types of epidemiological characteristic: design ofthe study, population that has been studied, exposure, outcome, covariates and effect size. We have developed ageneric rule-based approach that has been designed according to semantic patterns observed in text, and tested itin the domain of obesity. Identified exposure, outcome and covariate concepts are clustered into health-relatedgroups of interest. On a manually annotated test corpus of 60 epidemiological abstracts, the system achievedprecision, recall and F-score between 79-100%, 80-100% and 82-96% respectively. We report the results of applying themethod to a large scale epidemiological corpus related to obesity.Conclusions: The experiments suggest that the proposed approach could identify key epidemiological characteristicsassociated with a complex clinical problem from related abstracts. When integrated over the literature, the extracteddata can be used to provide a more complete picture of epidemiological efforts, and thus support understanding viameta-analysis and systematic reviews.Keywords: Text mining, Epidemiology, Key characteristics, Rule-based methodologyBackgroundEpidemiological studies aim to discover the patterns anddeterminants of diseases, and other health related statesby studying the health of populations in standardisedways. They are valuable sources of evidence for publichealth measures and for shaping of research questions inthe clinical and biological aspects of complex diseases.Nevertheless, the increasing amount of published litera-ture leads to information overload, making the task ofreading and integrating relevant knowledge a challengingprocess [1-3]. For example, there are more than 23,000obesity-related articles reporting on different epidemio-logical findings, including almost 3,000 articles withobesity/epidemiology as a MeSH descriptor in 2012, with* Correspondence: g.nenadic@manchester.ac.uk1School of Computer Science, University of Manchester, Kilburn Building,Oxford Road, Manchester, UK2Health e-Research Centre, Manchester, UKFull list of author information is available at the end of the article© 2014 Karystianis et al.; licensee BioMed CenCreative Commons Attribution License (http:/distribution, and reproduction in any mediummore than 15,000 such articles in the last 10 years.Therefore, there is a need for systems that enable theextraction of salient epidemiological study features inorder to assist investigators to reduce the time requiredto detect, summarise and incorporate epidemiologicalinformation from the relevant literature [4].Epidemiology is a relatively structured field with itsown dictionary and reporting style, deliberately writtenin a typical semi-structured format in order to standardizeand improve study design, communication and collabor-ation. The standard characteristics in most epidemiologicalstudies include [5]: study design - a specific plan or protocol that hasbeen followed in the conduct of the study; population - demographic details of the individuals(e.g., gender, age, ethnicity, nationality) participatingin an epidemiological study;tral Ltd. This is an Open Access article distributed under the terms of the/creativecommons.org/licenses/by/4.0), which permits unrestricted use,, provided the original work is properly credited.Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 2 of 11http://www.jbiomedsem.com/content/5/1/22 exposure - a factor, event, characteristic or otherdefinable entity that brings about change in a healthcondition or in other defined characteristics; outcome - the consequence from the exposure in thepopulation of interest; covariate - a concept that is possibly predictive ofthe outcome under study; effect size - the measure of the strength of therelationship between variables, that relates outcomesto exposures in the population of interest.In this paper we present a system that enables theidentification and retrieval of the key characteristicsfrom the epidemiological studies. We have applied thesystem to the obesity epidemiological literature. Obesityis one of the most important health problems of the 21stcentury [6], presenting a great public health and eco-nomic challenge [7-9]. The rapid and worldwide spreadof obesity has affected people of all ages, genders, geog-raphies and ethnicities. It has been regarded as a multi-dimensional disorder [10], with major behavioural andenvironmental determinants, with genetics playing onlya minor role [7].Related workIn the last decade, a significant amount of research hasbeen performed on the extraction of information in thebiomedical field, especially on the identification of bio-logical [11,12] and clinical concepts [13,14] in the litera-ture. In clinical text mining, several attempts have beenmade to extract various kinds of information from casestudies and clinical trials in particular [1-4,15-23]. Forexample, De Bruijn et al. [22] applied text classificationwith a weak regular expression matcher on randomizedclinical trial (RCT) reports for the recognition of key trialinformation that included 23 characteristics (e.g. eligibilitycriteria, sample size, route of treatment, etc.) with overallprecision of 75%. The system was further expanded toidentify and extract specific characteristics such as primaryoutcome names and names of experimental treatmentfrom journal articles reporting RCTs [4], with precision of93%. However, they focused solely on RCTs and especiallyon randomized controlled drug treatment trials. Hara andMatsumoto [1] extracted information about the design ofphase III clinical trials. They extracted patient populationand compared associated treatments through noun phrasechunking and categorisation along with regular expressionpattern matching. They reported precision for popula-tion and compared treatments of 80% and 82% respect-ively. Hansen et al. [2] worked on RCTs identifying thenumbers of the trial participants through a support vec-tor machine algorithm with 97% precision, while Fizmanet al. [19] aimed to recognize metabolic syndrome risk fac-tors in MEDLINE citations through automatic semanticinterpretation with 67% precision. However, to the best ofour knowledge, there is no approach available for recognis-ing key information elements from various types of epi-demiological studies that are related to a particular healthproblem.MethodsOur approach involved the design and implementationof generic rule-based patterns, which identify mentionsof particular characteristics of epidemiological studies inPubMed abstracts (Figure 1). The rules are based on pat-terns that were engineered from a sample of 60 epi-demiological abstracts in the domain of obesity. Mentionsof six semantic types (study design, population, exposures,outcomes, covariates and effect size) have been manuallyidentified and reviewed. Additionally, a development setwith additional 30 abstracts was used to optimise the per-formance of the rules. These steps are explained here inmore details.1. Abstract selection and species filtering. In thefirst step, abstracts are retrieved from PubMed usingspecific MeSH terms (e.g. obesity/epidemiology[mesh]). They are checked by LINNAEUS, a speciesidentification system [24], to filter out studies basedon non-human species.2. Building of dictionaries of potential mentions. Inthe second step, a number of semantic classes areidentified using custom-made vocabularies that includeterms to detect key characteristics in epidemiologicalstudy abstracts (e.g. dictionaries of words that indicatetudy design, population totals, etc.  a total offourteen dictionaries). We also identify mentions ofUnified Medical Language System (UMLS) [25]terms and additionally apply the Specialist lexicon[26] in order to extract potential exposure, outcome,covariate and population concepts. Finally, epidemio-logical abstracts are processed with an automaticterm recognition (ATR) method for the extraction ofmulti-word candidate concepts and their variants[27,28]. Filtering against a common stop-word list(created by Fox [29]) is applied to remove anyconcepts of non-biomedical nature.3. Mention-level application of rules. In the thirdstep, rules are applied to the abstracts for each ofthe six epidemiological characteristic separately. Therules make use of two constituent types: frozenlexical expressions (used as anchors for specificcategories) and specific semantic classes identifiedthrough the vocabularies (identified in step 2), whichare combined using regular expressions. The frozenlexical expressions can contain particular verbs,prepositions or certain nouns. Table 1 shows thenumber of rules created for each of the sixFigure 1 The four steps of the approach applied to epidemiological abstracts in order to recognise key characteristics. Linnaeus is usedto filter out abstracts not related to humans; Dictionary look-up and automatic term recognition (ATR) are applied to identify major medicalconcepts in text; MinorThird is used as an environment for the rule application and mention identification of epidemiological characteristics.Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 3 of 11http://www.jbiomedsem.com/content/5/1/22characteristics with some typical examples. As aresult of the application of rules, candidate mentionsof epidemiological concepts are tagged in text. Weused MinorThird [30] for annotating andrecognizing entities of interest.4. Document-level unification. Finally, in caseswhere several candidate mentions for a singleepidemiological characteristic were recognised ina given document, we also unified them to getdocument-level annotations using the followingapproach: if a given mention is part of a longermention, then we select only the longer. Mentionsthat are not included in other mentions (of thesame type) are also returned. In addition, whereapplicable (i.e. for exposures, outcomes and covariates),these mentions are mapped to one of the 15 UMLSsemantic groups (Activities and Behaviors, Anatomy,Chemicals and Drugs, Concepts and Ideas, Devices,Disorders, Genes and Molecular, Geographic Areas,Living Beings, Objects, Occupations, Organizations,Phenomena, Physiology and Procedures). We decidedto perform the mapping to high-level UMLS semanticgroups to assist epidemiologists in the application ofan epidemiological sieve, which could help themdecide whether or not to include abstracts for moredetailed inspection. For example, highlighting differenttypes of determinant (e.g. demographic vs. lifestyle)would be useful for considering the completenessand relevance of factors in a particular study byemphasizing possible connections between thebackground of the exposure and/or the outcomes.ResultsEvaluationWe evaluated the systems performance at the documentlevel by considering whether selected spans were cor-rectly marked in text. We calculated precision, recalland F-score for each of the characteristic of interestTable 1 Examples of rules for recognition of study design, population, exposure, outcome, covariate and effect size inepidemiological abstractsCharacteristic(number of rules)Examples Identified span (in bold)Study design(16 rules)Rule [@st a(types)]Methods: This was a cross-sectionalstudy of 214 overweight/obese cross-sectional studyPopulation(119 rules)Rule a(totals) re((of|on|in)) [@stats a(clusters)]Sibling study in a prospective cohortof 208,866 men from cohort of 208,866 menRule @multiple re(with|in|on)? [a(clusters) re(with|without) @multiple]bone mineral density in patientswith type 2 diabetesbone mineraldensityin patients with type 2diabetesExposure(134 rules)Rule a(relations) eq(between) [@multiple] eq(and) @multiple and analyze the associationbetween body mass index andblood pressure in association Between body massindexand blood pressureRule [@multiple] a(be) a(related) a(with) eq(onset)? eq(of)?Short sleep duration is associatedwith onset of obesityShort sleepdurationis associated with onset ofOutcome(100 rules)Rule @factors eq(of) [@multiple]Cardiovascular and disease relatedpredictors of depressionpredictors of depressionRule @multiple a(be) a(adverbs) a(related) a(with) [@multiple]Conclusions coffee intake is inverselyassociated with t2dm in Chinese.coffee intake is inversely associated with t2dmCovariate(28 rules)Rule a(adj) eq(for) [@multiple] after adjusting for age, smokingstatus, and clinical history ofdiabetes mellitus.adjusting for age, smoking status, andclinical history of diabetesmellitus.Rule eq(including) [@multiple] eq(as) @synonyms including visceral adipose tissue(vat) and subcutaneous adiposetissue (sat) as covariates.including visceral adiposetissue (vat) andsubcutaneous adiposetissue (sat)as covariatesEffect size(15 rules)Rule @multiple [a(preva) a(be) @perce]Hernia prevalence was 32.4% Hernia prevalence was 32.4%Rule @multiple @or @ci more likely to have elevatedblood pressure (or = 9.05, 95%ci: 1.44, 56.83)elevated bloodpressure(or = 9.05, 95% ci: 1.44, 56.83)The rule components in square brackets are the extracted spans that denote the key characteristic; the rest of the rule (if any) specifies the context. The rulesuse explicit matching of spans (e.g. eq(onset)), regular expressions (re) for matching specific verbs or prepositions (e.g. re((of|on|in))), various vocabularies thatcontain single (e.g. a(types)  matching words that indicate the conduction of a study (e.g. study, analysis, review)) and multiword terms (e.g. @st, a vocabularyof epidemiological study designs (e.g. case control)). totals contains words that suggest the participant population; stats is a dictionary that contains numbers and wordsthat express numeric values (e.g., one hundred); clusters includes the variations that a population sample can be described (e.g., men, patients, individuals); multiple containssingle or multi-word biomedical concepts (e.g., depression, type 2 diabetes); relations is a dictionary with single words that describe an association between concepts(e.g., relationship, link, association); factors contains single or multi-word terms that describe risk factors (e.g., risk factors, predictors); or is a dictionary that contains nounphrases in which the effect size odds ratio can be expressed, including the ways in which its numeric value is presented (e.g., odds ratio = 1.34, or = 2.56); ci follows asimilar pattern for confidence interval with its assigned numeric value e.g., (95% ci = 0.91, 95% ci: 4.36, 5.48).Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 4 of 11http://www.jbiomedsem.com/content/5/1/22using the standard definitions [31]. In order to create anevaluation dataset, 60 abstracts were randomly selectedfrom the PubMed results obtained by query obesity/epidemiology[mesh] and manually double-annotated forall the six epidemiological characteristics by the first authorand an external curator with epidemiological expertise.The inter-annotator agreement of 80% was calculated onthe evaluation dataset by the absolute agreement rate [32],suggesting relatively reliable annotations.Table 2 shows the results on the evaluation set, withto the results obtained on the training and developmentsets for comparison (Tables 3 and 4). The precision andTable 2 Results, including true positives (TP), falsepositives (FP), false negative (FN), precision (P), recall (R)and F-score on the evaluation set. Evaluation set (60 abstracts)TP FP FN P R FStudy design 12 0 1 100.0 92.3 95.9Population 35 1 4 97.2 89.7 93.3Exposure 45 8 11 84.9 80.3 82.5Outcome 73 19 13 79.3 84.8 82.4Covariate 17 2 0 89.4 100.0 94.4Effect size 65 2 10 97.0 86.6 91.5All classes (micro) 247 32 39 88.5 86.3 87.4All classes (macro) 91.3 88.9 90.0Micro averages are calculated across all different document level mentions;macro averages are calculated across different characteristics.Table 4 Results, including true positives (TP), falsepositives (FP), false negative (FN), precision (P), recall (R)and F-score on the development set. Development set (30 abstracts)TP FP FN P R FStudy design 11 1 2 91.6 84.6 88.0Population 36 4 4 90.0 90.0 90.0Exposure 59 4 0 93.6 100.0 96.7Outcome 65 13 1 83.3 98.4 90.2Covariate 13 3 0 81.2 100.0 89.6Effect size 50 17 5 74.6 90.9 81.9All classes (micro) 234 42 12 84.7 95.1 89.6All classes (macro) 85.7 93.8 89.5Micro averages are calculated across all different document level mentions;macro averages are calculated across different characteristics.Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 5 of 11http://www.jbiomedsem.com/content/5/1/22recall values ranged from 79% to 100% and 80% to100%, with F-measures between 82% and 96%. The bestprecision was observed for study design (100%). How-ever, despite having a relatively large number of studydesign mentions in the training set (38 out of 60), thedevelopment and evaluation sets had notably fewer men-tions and therefore the precision value should be takenwith caution. Similarly, the system retrieved covariatecharacteristic with 100% recall, but again the number ofannotated covariate concepts was low. The lowest preci-sion was observed for outcomes (79%), while exposureshad the lowest recall (80%). With the exception of studydesign that saw a little increase (7.7%), recall decreasedfor the rest of the characteristics when compared to thevalues on the development set. On the other hand, effectsize had a notable increase in precision, from 75%(development) to 97% (evaluation). Overall, the microF-score, precision and recall for all the six epidemiologicalTable 3 Results, including true positives (TP), falsepositives (FP), false negative (FN), precision (P), recall (R)and F-score on the training set. Training set (60 abstracts)TP FP FN P R F-scoreStudy design 37 5 1 88.0 97.3 92.5Population 94 10 5 90.3 94.9 92.6Exposure 104 21 14 83.2 88.1 85.5Outcome 125 26 8 82.7 93.9 88.0Covariate 13 4 0 76.4 100.0 86.6Effect size 41 5 9 89.1 82.0 85.4All classes (micro) 414 71 37 85.3 91.7 88.4All classes (macro) 84.9 92.7 88.4Micro averages are calculated across all different document level mentions;macro averages are calculated across different characteristics.characteristics were 87%, 88% and 86% respectively, sug-gesting reliable performance in the identification of epi-demiological information from the literature.Application to the obesity corpusWe applied the system on a large scale corpus consistingof 23,690 epidemiological PubMed abstracts returnedby the obesity/epidemiology[mesh] query (restricted toEnglish). We note that a number of returned MEDLINE ci-tations did not contain any abstract, resulting in 19,188processed citations. In total, we extracted 6,060 mentionsof study designs; 13,537 populations; 23,518 exposures;40,333 outcomes; 5,500 covariates and 9,701 mentions ofeffect sizes.Table 5 shows most frequent study types in obesityepidemiological research. The most common epidemio-logical study designs are cohort cross-sectional (n = 1,940;32%) and cohort studies (n = 1876; 31% of all recognizedTable 5 The most frequent study designs extracted fromthe obesity epidemiological literatureStudy design Frequency %Cross-sectional 1,940 32.0Cohort 1,876 30.9Review 678 11.1Population/epidemiological 521 8.5Case control 341 5.6Observational 191 3.1Non randomized controlled 109 1.7Non randomized 109 1.7Qualitative descriptive 95 1.5Qualitative 49 0.8Frequency is the number of documents, and the last column presents theshare within the entire set.Table 6 The most frequent exposures extracted from theobesity epidemiological literatureExposures Frequency %Obesity 2,450 10.4Body mass index 1,351 5.7Overweight 531 2.2Age 394 1.6Waist circumference 291 1.2Physical activity 289 1.2Hypertension 256 1.0Metabolic syndrome 240 1.0Body weight 218 0.9Type 2 diabetes 206 0.8Gender 193 0.8Smoking 186 0.7Abdominal obesity 135 0.5Insulin resistance 128 0.5Mortality 117 0.4Adiposity 116 0.4Weight gain 108 0.4Diet 98 0.4Childhood obesity 92 0.3Weight loss 89 0.3Waist to hip ratio 82 0.3Education 79 0.3Childhood 79 0.3Socioeconomic status 75 0.3Ethnicity 75 0.3Depression 70 0.2Central obesity 69 0.2Pregnancy 67 0.2Race 66 0.2Blood pressure 66 0.2Overweight/obesity 59 0.2CVD risk factors 59 0.2Height 55 0.2Morbidity 54 0.2Leptin 52 0.2Birth weight 49 0.1Asthma 49 0.1Bariatric surgery 48 0.1Physical inactivity 47 0.1Family history 45 0.1Frequency is the number of documents, and the last column presents theshare within the entire set.Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 6 of 11http://www.jbiomedsem.com/content/5/1/22studies), whereas there were only 109 (1.7%) randomizedclinical trials. Tables 6, 7, 8, 9, 10 and 11 present the mostfrequent exposures, outcomes and covariates along withtheir UMLS semantic types.DiscussionCompared to other approaches that focused specificallyon randomized clinical trials, our approach addresses asignificantly more diverse literature space. We aimed atextracting key epidemiological characteristics, which aretypically more complex than those presented in clinicaltrials. This is not surprising because clinical trials aresubject to strict regulations and are reported in highlystandardised ways. Although this makes it difficult tocompare our results with those of others directly, we stillnote that our precision (79-100%) is comparable toother studies (67-93%). The overall F-score of 87%suggests that a rule-based approach can generate reli-able results in epidemiological text mining despite therestrained nature of the targeted concepts. Here wediscuss several challenges and issues related to epi-demiological text mining, and indicate the areas for fu-ture work.Complex and implicit expressionsDespite having relatively reliable annotations (recall theinter-annotator agreement of 80%), epidemiological ab-stracts feature a number of complex, varying detail andimplicit expressions that are challenging for text mining.For example, there are various ways in which populationcan be described: from reporting age, sex and geograph-ical region to mentioning the disease the individuals arecurrently affected with or that are excluded from thestudy (e.g. The study comprised of 52 subjects with his-tologically confirmed advanced colorectal polyps and 53healthy controls [PMID  21235114]). Even more com-plex are the ways in which exposures are expressed,given that these are not often explicitly stated in text asexposures but rather part of the context of the study.Similarly, identification of covariate concepts is challen-ging as only a small number of covariates are explicitlystated in text.Finally, out dictionary coverage and focus were quitelimited by design: we focused on biomedical concepts,but other types of concepts may be studied as determi-nants and outcomes, or being mentioned as covariates(e.g., high school environmental activity). While thesehave been addressed by application of ATR, more gen-eric vocabularies may need to be used (see below forsome examples).Error analysis on the evaluation datasetOur approach is based on intensive lexical and ter-minological pre-processing and rules to identify the keyTable 7 Distribution of UMLS semantic groups assignedto exposuresSemantic group Frequency %Disorders 8,700 36.9Concepts/ideas 4,635 19.7Physiology 3,969 16.8Procedures 1,611 6.8Activities/behaviors 1,285 5.4Living beings 1,030 4.3Chemicals/drugs 857 3.6Objects 368 1.5Genes/molecular 344 1.4Anatomy 252 1.0Phenomena 180 0.7Geographic areas 145 0.6Occupations 73 0.3Devices 30 0.01Organizations 21 0.0Other 16 0.0Table 8 The most frequent outcomes extracted from theobesity epidemiological literatureOutcomes Frequency %Obesity 5,220 12.9Overweight 2,058 5.1Type 2 diabetes 1,379 3.4Body mass index 1,084 2.6Hypertension 728 1.8Cardiovascular disease 712 1.7Metabolic syndrome 659 1.6Mortality 460 1.1Insulin resistance 297 0.7Childhood obesity 289 0.7Coronary heart disease 260 0.6Death 250 0.6Health 225 0.5Waist circumference 211 0.5Abdominal obesity 209 0.5Smoking 194 0.4Physical activity 193 0.4Weight gain 181 0.4Morbidity 180 0.4Cvd risk factors 175 0.4Weight 162 0.4Adiposity 161 0.3Overweight/obesity 155 0.3Asthma 127 0.3Blood pressure 122 0.3Dyslipidemia 116 0.2Body weight 110 0.2Stroke 101 0.2Central obesity 98 0.2Depression 95 0.2Weight loss 94 0.2Underweight 91 0.2Chronic diseases 91 0.2Hypercholesterolemia 88 0.2Cancer 86 0.2Survival 85 0.2Cardiovascular risk 85 0.2Atherosclerosis 81 0.2Coronary artery disease 78 0.1Inflammation 68 0.1Frequency is the number of documents, and the last column presents theshare within the entire set.Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 7 of 11http://www.jbiomedsem.com/content/5/1/22epidemiological characteristics. The number of rules de-signed for obesity can be considered relatively high (412),given that they were engineered from relatively small train-ing (and development) datasets. On one hand, the numberof rules for study design (16), covariate (28) and effect size(15) were rather small in comparison to others e.g., popula-tion (119), indicating the existence of generic expressionpatterns that can identify concept types from more genericepidemiological characteristics (such as study design or ef-fect size). However, disease-related concepts often includea variety of determinants along with a number of outcomesof various nature (e.g. anatomical, biological, disease-related, etc.). Therefore, on the other hand, the task of rec-ognizing these epidemiological elements (e.g., outcomes,exposures) through a rule based approach is not an easytask and requires a number of rules to accommodate dif-ferent types of expression. We briefly discuss the cases oferrors for each of the characteristic below.Study designDue to the limited number of study design mentions(only 13) in the evaluation set, the high values of preci-sion, recall and F-score should be taken with caution.There were no false positives in the evaluation data set.However, it is possible that in a larger dataset, false posi-tives could appear if certain citations report more thanone mention of different study types. In addition, studydesigns without specific information can be ambiguousand thus were ignored (e.g. Metabolic and bariatricsurgery for obesity: a review [False Negative]).Table 9 Distribution of UMLS semantic groups assignedto outcomesSemantic group Frequency %Disorders 21,809 54.0Concepts/ideas 7,277 18.0Physiology 3,810 9.4Procedures 1,697 4.2Living beings 1,616 4.0Activities/behaviors 1,413 3.5Chemicals/drugs 990 2.4Anatomy 577 1.4Objects 314 0.7Genes/molecular 265 0.6Phenomena 250 0.6Geographic areas 137 0.3Occupations 102 0.2Organizations 36 0.0Devices 28 0.0Other 16 0.0Table 10 The most frequent covariates extracted fromthe obesity epidemiological literatureCovariates Frequency %Age 1,066 19.3Gender 631 11.4Body mass index 346 6.2Smoking 260 4.7Education 160 2.9Race 117 2.1Physical activity 108 1.9Alcohol consumption 83 1.5Ethnicity 70 1.2Type 2 diabetes 67 1.2Race/ethnicity 60 1.0Obesity 58 1.0Waist circumference 53 0.9Income 43 0.7Hypertension 42 0.7Socioeconomic status 39 0.7Height 36 0.6Marital status 33 0.6Demographics 32 0.5Parity 27 0.5Smoking status 25 0.5Energy intake 25 0.5Lifestyle 22 0.4Educational level 20 0.3Birth weight 20 0.3Weight 17 0.3Maternal age 17 0.3Family history 17 0.3Exercise 16 0.2Depression 15 0.2Total energy intake 14 0.2Region 13 0.2Insulin resistance 13 0.2Occupation 12 0.2Family income 12 0.2Blood pressure 12 0.2Adiposity 11 0.2Social class 10 0.1Gestational age 10 0.1Area 10 0.1Frequency is the number of documents, and the last column presents theshare within the entire set.Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 8 of 11http://www.jbiomedsem.com/content/5/1/22PopulationAn analysis of false positives reveals that rules relying onthe identification of prepositional phrases associatedwith populations (e.g. among and in) need more specificpresence of patient-related concepts. False negatives in-cluded 3,715 deliveries or 895 veterans who had bar-iatric surgery, which are referring to births and aspecific demographic respectively, but our lexical re-sources did not contain those. Nevertheless, the F-scorefor the population type was the second best (93%),showing that a rule-based approach can be used to iden-tify the participants in epidemiological studies. An inter-esting issue arose in the identification of populationassociated to meta-analyses. For example, the mentionincluded 3 studies involving 127 children was identifiedby patterns but it is clear that a specific approach wouldbe needed for meta-analysis studies.Exposures and outcomesWhile outcomes are often explicitly mentioned in textas such, exposure concepts are not, which makes theidentification of exposures a particularly challengingtask. Still, the use of dictionaries containing biomed-ical concepts for identification of potential mentionsproved useful for capturing exposure concepts. How-ever, dictionary-based look-up also contributed to in-correct exposure candidates that were extracted fromnon-relevant contexts. On the other hand, two frequentcauses of errors could be linked to missing conceptsfrom our dictionaries (e.g. late bedtimes or costs)Table 11 Distribution of UMLS semantic groups assignedto covariatesSemantic group Frequency %Physiology 2,381 43.2Concepts/ideas 1,044 18.9Disorders 783 14.2Activities/behaviors 591 10.7Living beings 232 4.2Procedures 184 3.3Chemicals/drugs 112 2.0Geographic areas 41 0.7Occupations 34 0.6Objects 29 0.5Phenomena 26 0.4Genes/molecular 17 0.3Anatomy 17 0.3Other 4 0.0Organizations 4 0.0Devices 1 0.0Karystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 9 of 11http://www.jbiomedsem.com/content/5/1/22and relatively complex exposure expressions (e.g. levelof PA during leisure).An important source of errors was the confusion be-tween exposures and outcomes, given they both refer tosimilar (semantic) types whose instances can  in differentstudies  be either exposure or outcome, and thus theirrole can be easily misinterpreted as an outcome rather thana studied determinant (and vice versa). We noted that rulessuch as association between < exposure > and < outcome>or <exposure > associated with < outcome> generatedencouraging results i.e., a number of TPs. This wasnot surprising: when a clinical professional is studyingthe relationship between two concepts, he explores thelink between an exposure and an outcome, which theabove patterns capture. Still, sometimes these patternswould match links irrelevant to exposure/outcome re-lationships (e.g. relationship between race and gender).Cases like these result in the generation of both false pos-itives and false negatives. Overall, a sentence-focused rulebased method may struggle to understand a conceptsrole in a given case, and a wider context might need tobe considered.CovariatesCovariates had only a limited number of identifiedspans, hence any conclusion regarding the systems per-formance is at most indicative. Still, the results could pro-vide an initial indication that (at least explicit) covariatementions could be detected with good accuracy, despitesome false positives (e.g. a generic mention potentialconfounders was identified as a covariate in  after ad-justment for potential confounders).Effect sizeThe rules designed to recognize effect size spans werebased on the combination of numerical and specificlexical expressions (e.g. relative risk, confidenceinterval). A relatively high recall (87%) revealed thatthis approach returned promising results, with only asmall number of mentions being ignored by the system,but with high precision. False negatives included expres-sions that included multiple values (e.g.,  increasedrisks of overweight/obesity at the age of 4 years (odds ratio(95% confidence interval): 15.01 (9.63, 23.38)),  bmistatistically significantly increased by 2.8% (95% confi-dence interval: 1.5% to 4.1%; p < 0.001) ).Application to the obesity corpusAlthough we had relatively good recall in both the devel-opment and evaluation datasets, the experiments withthe entire obesity dataset have shown that the system ex-tracted epidemiological information only from a limitednumber of documents. We have therefore explored thereasons for that.Study designWe identified study type from only around 40% of proc-essed articles (each tagged as obesity/epidemiology). Toexplore whether those missed study design mentionsare due to our incomplete dictionaries and rules, weinspected 20 randomly selected articles from those thatcontained no identified study type, and we identified thefollowing possible reasons: No mention of study design: while the articlepresents an epidemiological context, no specificepidemiological study had been conducted (and thusthere was no need to specify study design)  thiswas the case in almost 2/3 of the abstracts with nostudy design; Summarised epidemiological studies: articlessummarizing epidemiological information butwithout reporting a specific conducted study and itsfindings (15% of the abstracts); Other study designs: studies including comparativestudies, surveys, pilot studies, follow-up studies,reports, reviews that were not targeted for identifi-cation (20% of the abstracts).We note that we can see a similar pattern in the evalu-ation dataset (which was randomly selected from theobesity corpus). Importantly, for the majority of ab-stracts in the evaluation dataset, if the system was ableto detect the study type, all other epidemiologicalKarystianis et al. Journal of Biomedical Semantics 2014, 5:22 Page 10 of 11http://www.jbiomedsem.com/content/5/1/22characteristics have been extracted with relative success,providing a complete profile of an epidemiological study(data not shown).CovariatesOnly 5,500 confounding factors were recognised. To ex-plore the reason for so many articles not having covari-ates extracted, a random sample of 20 abstracts in whichno covariate concept was identified was investigated.None of the studied abstracts contained any covariatementions. Most abstracts used only generic expressions(e.g., after adjustment for confounding factors, aftercontrolling for covariates) without specifying the re-spective concepts. We note that we only processed ab-stracts and it seems likely that covariates may be definedin full-text articles.Effect sizeSimilar observations to the ones made for the covariatecharacteristic were noted for the effect size mentions(only 9,701 mentions were extracted). We explored asample of 20 abstracts in which no effect size was recog-nised. As many as 60% of the abstracts did not reportany observed effect size between the studied exposuresand outcomes due to the nature of the conducted study(e.g. pilot study, systematic review, article). We failed,however, to get effect size mentions in 40% of cases,mainly because of mentions that contained coordinatedexpressions (e.g. The prevalence of hypertension wasconsiderably higher among men than among women(60.3% and 44.6%, respectively; PMID 18791341) orstatistical significance data, which are not covered byour rules.OutcomesAs opposed to other characteristics, the number ofrecognised outcome concepts was more than double thenumber of abstracts. This is not a surprise, as mostof the epidemiological studies include more than oneoutcome of interest. In addition, with the current system,we have not attempted to unify synonymous terms (unlessthey are simple orthographic variants).ConclusionsWe presented a generic rule based approach for theextraction of the six key characteristics (study design,population, exposure(s), outcome(s), covariate(s) and ef-fect size) from epidemiological abstracts. The evaluationprocess revealed promising results with the F-score ran-ging between 82% and 96%, suggesting that automaticextraction of epidemiological elements from abstractscould be useful for mining key study characteristics andpossible meta-analysis or systematic reviews. Also, ex-tracted profiles can be used for identification of gaps andknowledge modelling of complex health problems. Al-though our experiments focused on obesity mainly for thepurpose of evaluation, the suggested approach of identify-ing key epidemiological characteristics related to a particu-lar clinical health problem is generic.Our current work does not include identification ofsynonymous expressions or more detailed mapping ofidentified terms to existing knowledge repositories, whichwould allow direct integration of the literature with otherclinical resources. This will be the topic for our futurework. Another potential limitation of the current work isthat we focused only on abstracts, rather than full-text arti-cles. It would be interesting to explore if full-text wouldimprove the identification (in particular recall) or it wouldintroduce more noise (reducing precision).Availability and requirementsProject name: EpiTeM (Epidemiological Text Mining)Project home page: http://gnode1.mib.man.ac.uk/epidemiology/Operating system(s): Platform independentProgramming language: PythonOther requirements: MinorThirdLicense: FreeBSDAny restrictions to use by non-academics: NoneAbbreviationsATR: Automatic term recognition; FN: False negatives; FP: False positives;P: Precision; R: Recall; RCT: Randomized clinical trial; TP: True positives;UMLS: Unified Medical Language System.Competing interestsThe authors declare that they have no competing interests.Authors contributionsThe study was conceived and designed by IB and GN. GK implemented thesystem, provided the data and performed the experiments. All authors readand approved the final manuscript.AcknowledgementsWe would like to thank Katherine McAllister (Institute of Population Health,University of Manchester) for the annotation of the datasets. This work waspartially supported by the UK Medical Research Council via a PhD grant toGK. GN and IB are partially-supported by the Health e-Research Centre (HeRC)grant. GN acknowledges support from the Serbian Ministry of Education andScience (projects III44006; III47003).This article has been published as part of the Semantic Mining of Languagesin Biology and Medicine (SMLBM) thematic series of the Journal of BiomedicalSemantics. An initial version of the article was presented at the 4th InternationalSymposium on Languages in Biology and Medicine (LBM) in 2011.Author details1School of Computer Science, University of Manchester, Kilburn Building,Oxford Road, Manchester, UK. 2Health e-Research Centre, Manchester, UK.3Centre for Health Informatics, Institute of Population Health, University ofManchester, Manchester, UK.Received: 9 April 2014 Accepted: 15 April 2014Published: 19 May 2014JOURNAL OFBIOMEDICAL SEMANTICSHoehndorf et al. Journal of Biomedical Semantics 2014, 5:15http://www.jbiomedsem.com/content/5/1/15REVIEW Open AccessThematic series on biomedical ontologies inJBMS: challenges and new directionsRobert Hoehndorf1*, Melissa Haendel2,3, Robert Stevens4 and Dietrich Rebholz-Schuhmann5,6AbstractOver the past 15 years, the biomedical research community has increased its efforts to produce ontologies encodingbiomedical knowledge, and to provide the corresponding infrastructure to maintain them. As ontologies arebecoming a central part of biological and biomedical research, a communication channel to publish frequent updatesand latest developments on them would be an advantage.Here, we introduce the JBMS thematic series on Biomedical Ontologies. The aim of the series is to disseminate thelatest developments in research on biomedical ontologies and provide a venue for publishing newly developedontologies, updates to existing ontologies as well as methodological advances, and selected contributions fromconferences and workshops. We aim to give this thematic series a central role in the exploration of ongoing researchin biomedical ontologies and intend to work closely together with the research community towards this aim.Researchers and working groups are encouraged to provide feedback on novel developments and special topics tobe integrated into the existing publication cycles.IntroductionThis editorial will explore the expectations linked toa growing infrastructure around biomedical ontolo-gies. Since they become an integral part of bio-logical and biomedical research for the annotationof data  its integration, analysis, and visualization[1]  the demand for a place arises in which the scientificcommunity can be made aware of new ontologies, majorupdates to existing ontologies, development and updatesto ontology-based tools, and the discussion of ontology-based methods. The JBMS thematic series on BiomedicalOntologies, and the annual JBMS Ontology Issue, willfill these gaps and establish a hub of information aboutbiomedical ontologies and their scientific applications.The role of ontologies in biological and biomedicalresearch has steadily increased in conjunction with theincrease in quality and quantity of data that is being col-lected in all areas of biology. Not only is the numberof ontologies increasing, their size growing, their rele-vance in biomedical research rising and they penetrate*Correspondence: leechuck@leechuck.deContributed equally1Department of Computer Science, Aberystwyth University, LlandinamBuilding, SY23 3DB Aberystwyth, UKFull list of author information is available at the end of the articlemore areas of biology and biomedicine; ontologies havealso begun to play a key part in the interpretation ofthe biomedical data as well as inspire the developmentof new tools for end users and new analysis methods forbiomedical scientists. As a result, data integration andinteroperability has become a relevant cost factor in theexecution of big data projects and has been acknowledgedby national and international projects, for example by theElixir initiative, which aims to establish a biomedical ITinfrastructure across Europe [2]. The development andapplication of ontologies will be an integral part of suchan infrastructure for the main reason that data interoper-ability requires tools to explicitly describe the semanticsof terms used to characterize the features of data, andontologies are widely used to fill this role.Which ontology did youmean?There has been considerable debate in the ontologyresearch community as to what constitutes an ontologyin biology [3-5] and what properties an ontology shouldhave. Traditional axes of classification for ontologiesinclude the expressivity of the language used to developand distribute the ontologies, the applications for whichthe ontologies are intended (i.e. who uses the ontology andhow) and the domain covered by the ontology. Argumentspertain© 2014 Hoehndorf et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedicationwaiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwisestated.Hoehndorf et al. Journal of Biomedical Semantics 2014, 5:15 Page 2 of 6http://www.jbiomedsem.com/content/5/1/15(a) To the degree of formality of the language used toexpress the information in an ontology, i.e., whethera formal language such as the Web OntologyLanguage (OWL) [6] is used or a graph-basedrepresentation without explicit formal semantics,(b) To the complexity of the ontology description, i.e.,whether rich axioms and relations are used orwhether a taxonomy, accompanied with textualdefinitions of classes in an ontology, is sufficient,(c) To the interpretation of what constitutes a class orrelation in an ontology, i.e., whether a class in anontology refers to something in the world or to amental construct, and(d) To the orthogonality of the content, i.e., whatcontent has been incorporated from other ontologiesand for which purposes.Depending on the intended applications, artifacts calledontologies are developed with any combination of theseproperties.In the JBMS thematic series on Biomedical Ontolo-gies, we employ a broad interpretation of ontology andinclude artifacts that primarily provide vocabularies forthe purpose of data annotation as well as formal theo-ries that provide a rich representation of certain aspects ofbiomedicine. To annotate data within a database, a taxon-omy of classes with labels and textual definitions is oftensufficient, while more expressive formal constructs wouldbe required if the ontology is developed to verify dataintegrity.Representing ontologiesThe annotation of research data using an ontology enablesintegration of data both within a database and acrossmultiple databases [1]. Ontologies provide a controlledset of classes together with an explicit (formal or infor-mal) representation of their meaning, a hierarchy betweenthese classes and complex axiom patterns (relations)[7] between the classes, and ontologies facilitate dataintegration when shared across multiple databases. Thetaxonomic relations allow integration through general orspecific aspects even if exact matches between data itemscan not be identified; and axioms between classes serve ascomplex relations that facilitate further data integration.Today, most biomedical ontologies are developed inshared formal languages, either the OBO Flatfile Format[8] or the Web Ontology Language (OWL) [6]. Both lan-guages are tightly coupled and thus allow translationsbetween them [9,10] so that the OBO Flatfile Format cannow be considered to be a fragment of OWL [8].The expressivity of a biomedical ontology is determinedby the particular subset of OWL that is being used toformulate the ontologies, and serves as a major distin-guishing factor. It characterizes the knowledge that canbe expressed (such as whether the ontology may containcontradictions) and determines the complexity of gen-eral tasks such as querying the ontology and categorizingdata with the ontology. OWL 2 comprises three emerg-ing profiles (OWL EL, OWL QL and OWL RL) apartfrom OWL DL [11]. The OWL EL profile forms a subsetwhich (a) allows to specify a taxonomy between classes(i.e., to state that one class is the subclass of another), (b)existential restrictions (i.e., to state that instances of oneclass must stand in a relation to some instance of anotherclass), and (c) disjointness of classes (i.e., to state that twoclasses cannot share any instances), and has been founduseful for a significant number of biomedical ontologies[12-15].Domains of ontologies and their applicationsOntologies are of particular importance in domains inwhich large volumes of data are being generated, and theemergence of high-throughput technologies has increasedthe importance of ontologies in some domains. In the1990s, research on discovering gene functions in diverseorganisms required ameans to standardize gene functionsfor comparison within and across multiple organisms:this need induced the development of the Gene Ontol-ogy (GO), which turned into one of the most importantresources in genomics research [16]. In a similar way, theSequence Ontology (SO) [17] emerged as a response tothe availability of more and more sequencing data, and toprovide compatibility between different data formats forbiological sequences and their features.Different anatomy ontologies specify the organismalcomponents for multiple species, and  on a smaller scaleof granularity  the developmental relations and featuresof cell types are characterized by the Celltype Ontology[18]. Phenotype ontologies are also available for multi-ple species and are widely used for the annotation ofthe abnormalities observed in mutagenesis experiments[19-21] as well as for the characterization of diseases anddrug effects [22].Further domains covered comprise chemical entities toannotate drugs and theirs biological activities [23], struc-tures, and pharmaceutical applications [23,24] for datainteroperability [25], and ontologies for experimental set-tings, e.g., the BioAssay Ontology [26], the ExperimentalFactor Ontology [27], the eagle-i ontology [28] and theOntology of Biomedical Investigations [29], capture thebiomedical metadata to characterize experiments. Simi-larly, ontologies for environmental conditions denote datasamples and their surroundings upon their encounter[27,30]. Ontologies are also being used to annotate andclassify journal articles [31,32], pathways [33], and specificbiological entities [34].Ontologies, together with their annotations, are exten-sively used in the analysis of biomedical data, for exampleHoehndorf et al. Journal of Biomedical Semantics 2014, 5:15 Page 3 of 6http://www.jbiomedsem.com/content/5/1/15in the form of Gene Set Enrichment Analysis (GSEA) [35]for the interpretation of gene expression datasets. GSEAmakes use of the structure of the Gene Ontology to iden-tify statistically over- or under-represented classes basedon gene expression observed in two biological states. Sim-ilar methods are also applied to other ontologies such asthe Human Disease Ontology [36], the Neuro BehaviorOntology [37], or even the full set of ontologies containedin BioPortal [38].Another analysis method relying on ontologies is tocompare data items and identify meaningful biologi-cal relations between them based on semantic similarity[39]. This approach has been applied to identify protein-protein interactions [40], classify chemicals [41], suggestcandidate genes involved in diseases [42,43] and repur-pose drugs [44,45]. When applying semantic similarity tocompare two data items, the choice of ontology deter-mines the kind of similarity that is revealed: using GOwill provide functional similarity, chemical entities fromChEBI will provide chemical structure similarity, andusing phenotype ontologies will result in phenotypic simi-larity.The integration of multiple ontologies  in particularfrom different domains  can reveal relations betweenannotated data items. For example, anatomy ontologiesfor cross-species comparisons  linking homologous oranalogous anatomical structures  can be used to trans-fer and compare annotations for multiple species [15,46].For this purpose, the UBERON anatomy ontology [15] wasdeveloped. It enables cross-species phenotype representa-tions that have been applied to deciphering humanGWASdata based on comparisons with mouse model pheno-types [47] as well as the prioritization of candidate genesand drug targets based on data from model organisms[42,44,48,49].Additionally, the rich axiom systems of some ontolo-gies help to verify and classify data according to con-straints on biological entities expressed in the ontologies.One example of such an application has been the clas-sification of proteins using ontologies [50], in which anontology provides rules according to which decisionsabout the protein family are made. The same, or sim-ilar, constraints expressed in ontologies can be used toverify data, i.e., determine whether a data item com-plies with the constraints expressed in the ontology ornot [51].Themain challenges for research in biomedicalontologiesEvaluation of ontologies and the development of a robustresearch methodologyEstablishing effective methods to evaluate ontologies both qualitatively and quantitatively, if possible  towardsfitness for a purpose is a major challenge in ontologyresearch [52]. Determining the best ontology for a givenpurpose becomes important, and criteria such as theontology structure, formality, its complexity, its coverage,as well as the amount of data annotated with it con-tribute to this decision. Effective methods for evaluationare particularly required for domains in which multipleontologies overlap in their content and intended appli-cations, such as for human diseases where ICD, MeSH,SNOMED CT, the Human Disease Ontology [53], theHuman Phenotype Ontology [22], the Unified MedicalLanguage System (UMLS) [54], and more specific ontolo-gies such as the Infectious Disease Ontology [55], Malariaontology [56], etc. are being used.The research methodology underlying the developmentof biomedical ontologies will also improve when effectiveevaluation criteria are being applied. The Ontology Sum-mit [57] has addressed this need with the topic OntologyEvaluation Across the Ontology Lifecycle in 2013, andontology evaluation featured prominently in panel dis-cussions at the International Conference on BiomedicalOntologies 2013 (ICBO) and will play a prominent roleat ICBO2014. The JBMS thematic series on Biomedi-cal Ontologies will follow the community discussions toaddress ontology evaluation principles and methods, andtheir instantiation in community-agreed guidelines andstandards.Standards and Interoperability: Linked Data and beyondEfficient reuse of ontologies, and the knowledge they con-tain, in the organization of open, linked data possiblyaccessible through multiple public interfaces (SPARQLendpoints) from different data providers is another chal-lenge [58]. The main task is to balance the complexityof processing and querying ontologies, which commonlyrequire the use of an automated reasoner, with the needto efficiently query large, linked datasets. In particularwhen multiple ontologies are used to annotate datasetsand automated reasoning over these ontologies providesthe means for finding relations between the classes inthese ontologies, the need for an infrastructure to supportcombined queries over ontologies with queries over linkeddata using SPARQL arises.Recently, some applications have come forward in whichautomated reasoning is used to answer complex queriesover ontologies and subsequently retrieve data [59-61].At the same time, major providers of biological andbiomedical data such as the European BioinformaticsInstitute (https://www.ebi.ac.uk/rdf/) and UniProt (http://beta.sparql.uniprot.org/) provide access to their contentthrough public SPARQL endpoints. In the future, weexpect exciting applications that combine reasoning overontologies in ontology repositories, such as the Ontol-ogy Lookup Service [62], BioPortal [63] or OntoBee [64],with (federated) SPARQL queries and provide a genuinelyHoehndorf et al. Journal of Biomedical Semantics 2014, 5:15 Page 4 of 6http://www.jbiomedsem.com/content/5/1/15knowledge-driven way for exploring linked biomedicaldata.Knowledge-based analysis of biomedical dataIntegration of ontologies  and the knowledge theycontain  in the analysis of biological and biomedicaldata is yet another challenge. Ontologies have been suc-cessfully integrated with biomedical analysis pipelines[35,39,65,66]. However, these analysis methods mainlyexploit the ontologies taxonomy and often make use ofthe axioms and constraints only implicitly.Many ontologies contain a lot more information thantaxonomic relationships, and some recent work has begunto exploit some additional information  disjointnessbetween classes in an ontology  to improve computa-tion of semantic similarity [67]. How the rich informationthat is further contained in formalized ontologies can beincorporated in the analysis of biomedical data remainsa research question, and novel methods will likely appearas the infrastructure and tool support around ontologiesevolves.The JBMS thematic series on BiomedicalontologiesThe JBMS thematic series on Biomedical Ontologies willprovide the venue for publishing research about biomedi-cal ontologies, their development, integration and qualityassurance. On a regular basis, we will have open calls forpapers on specific topics, and we welcome communityinput for important challenges to address.The annual JBMS Ontology Issue will become a centralpart of the thematic series where we focus on ontologiesthat have already been demonstrated to be useful for sci-entific applications. The Ontology Issue is intended for awide audience of readers; it does not specifically targetresearchers in ontology, but rather biological and biomed-ical researchers who may want to apply ontologies in theirdomain and require an overview over the currently avail-able artifacts they can already use. In the Ontology Issue,new ontologies can be described as well as updates toexisting ontologies. Updates in regular intervals producea better understanding of the progress in developing anontology and the major changes to its content, structureand applications.In the future, we aim to establish another regular callin which ontology-based tools and applications will bedescribed and updates to these tools published. Addition-ally, the thematic series will provide a venue to publishconference and workshop papers, and interested work-ing groups are encouraged to suggest special topics or tocontribute to existing publication cycles.We aim to make the JBMS thematic series on Biomed-ical Ontologies take a central role in the exploration ofcurrent research in biomedical ontologies, and we intendto work closely with the research community to achievethis aim. All researchers are invited to express ideas anddemands, ask for feedback on topics, and provide sugges-tions for novel developments.Competing interestsThe authors declare that they have no competing interests.Authors contributionsRH and DRS have drafted the first manuscript version. MH and RS havecontributed valuable improvements to the manuscript. All authors read andapproved the final manuscript.AcknowledgementsThe work of DRS is funded by the EU STREP project grant 296410 (MANTRA)under the 7th EU Framework Programme within Theme Information ContentTechnologies, Technologies for Digital Content and Languages[FP7-ICT-2011-4.1].Author details1Department of Computer Science, Aberystwyth University, LlandinamBuilding, SY23 3DB Aberystwyth, UK. 2OHSU Library and Department ofMedical Informatics, Oregon Health & Science University, Portland, Oregon,USA. 3Department of Medical Informatics and Epidemiology, Oregon Health &Science University, Portland, Oregon, USA. 4School of Computer Science, TheUniversity of Manchester, Oxford Road, M13 9PL Manchester, UK. 5Departmentof Computational Linguistics, University of Zürich, Binzmühlestrasse 14, 8050Zürich, Switzerland. 6European Bioinformatics Institute (EMBL-EBI), WellcomeTrust Genome Campus, Hinxton, Cambridge CB10 1SD, UK.Received: 8 January 2014 Accepted: 9 February 2014Published: 6 March 2014JOURNAL OFBIOMEDICAL SEMANTICSKlein et al. Journal of Biomedical Semantics 2014, 5:11http://www.jbiomedsem.com/content/5/1/11RESEARCH Open AccessBenchmarking infrastructure for mutation textminingArtjom Klein1*, Alexandre Riazanov2, Matthew M Hindle3 and Christopher JO Baker1*AbstractBackground: Experimental research on the automatic extraction of information about mutations from texts isgreatly hindered by the lack of consensus evaluation infrastructure for the testing and benchmarking of mutation textmining systems.Results: We propose a community-oriented annotation and benchmarking infrastructure to support development,testing, benchmarking, and comparison of mutation text mining systems. The design is based on semantic standards,where RDF is used to represent annotations, an OWL ontology provides an extensible schema for the data andSPARQL is used to compute various performance metrics, so that in many cases no programming is needed toanalyze results from a text mining system. While large benchmark corpora for biological entity and relation extractionare focused mostly on genes, proteins, diseases, and species, our benchmarking infrastructure fills the gap formutation information. The core infrastructure comprises (1) an ontology for modelling annotations, (2) SPARQLqueries for computing performance metrics, and (3) a sizeable collection of manually curated documents, that cansupport mutation grounding and mutation impact extraction experiments.Conclusion: We have developed the principal infrastructure for the benchmarking of mutation text mining tasks. Theuse of RDF and OWL as the representation for corpora ensures extensibility. The infrastructure is suitable forout-of-the-box use in several important scenarios and is ready, in its current state, for initial community adoption.IntroductionMutation text miningThe use of knowledge derived from text mining for men-tions of mutations and their consequences is increasinglyimportant for systems biology, genomics and genotype-phenotype studies. Mutation text mining facilitates a widerange of activities in multiple scenarios including theexpansion of disease-mutation database annotations [1],the development of tools predicting the impacts of muta-tions [2,3], the modelling of cell signalling pathways [4]and protein structure annotation [5,6]. The types of use-ful text mining tasks specific to mutations range from therelatively simple identification of mutation mentions [7,8],to very complex tasks such as linking (grounding) iden-tified mutations to the corresponding genes and proteins[9-11], interpretation of the consequences of mutations in*Correspondence: artjom.unb@gmail.com; bakerc@unb.ca1Computer Science And Applied Statistics Department, University of NewBrunswick, Saint John, CanadaFull list of author information is available at the end of the articleproteins [12], or identifying mutation impacts [13,14] andrelated phenotypes [15].Although the demand for mutation text mining soft-ware has lead to a significant growth of the experimentalresearch in this area, the development of such systemsand the publication of results is greatly hindered by thelack of adequate benchmarking facilities. For example,in developing a mutation grounding system [11] show-ing an encouraging level of performance accuracy, 0.73,on a homogeneous corpus of 76 documents, the authorsachieved only 0.13 on a heterogeneous corpus of largersize. When the system was reimplemented (see [16]), theauthors encountered another challenge  the evaluationof the new system by comparing it to the state-of-the-artwas practically unaffordable, despite the existence of sim-ilar systems, due to the lack of consensus benchmarkinginfrastructure.Such challenges and evaluation issues are not unique orspecific for mutation text mining, and are also present inother domains of biomedical text mining. In the follow-ing subsection, we discuss benchmarking and evaluation© 2014 Klein et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly cited.Klein et al. Journal of Biomedical Semantics 2014, 5:11 Page 2 of 13http://www.jbiomedsem.com/content/5/1/11difficulties in biological text mining in general, which arealso relevant to mutation text mining.Benchmarking and evaluation challenges in biomedicaltext miningBenchmarks, in the form of annotated corpora and relatedsoftware utilities, are usually designed and created forspecific text mining tasks and support fixed, usually hard-coded, evaluation metrics. Besides quantitative and qual-itative characteristics  number of entities annotated,distribution of annotation types, etc.  a corpus is char-acterized by the format, annotation schema (semanticsof annotations, annotation types), and evaluation metricsto calculate the performance of the text mining systems.For example, the benchmark for MutationFinder [7] (oneof the most popular single point mutation extractors) isin a custom tabular format, stores annotations and rawtext separately, has annotations of single point mutationJOURNAL OFBIOMEDICAL SEMANTICSTao et al. Journal of Biomedical Semantics 2014, 5:16http://www.jbiomedsem.com/content/5/1/16EDITORIAL Open AccessA 2013 workshop: vaccine and drug ontologystudies (VDOS 2013)Cui Tao1*, Yongqun He2 and Sivaram Arabandi3AbstractThe 2013 Vaccine and Drug Ontology Studies (VDOS 2013) international workshop series focuses on vaccine- anddrug-related ontology modeling and applications. Drugs and vaccines have contributed to dramatic improvementsin public health worldwide. Over the last decade, tremendous efforts have been made in the biomedical ontologycommunity to ontologically represent various areas associated with vaccines and drugs  extending existing clinicalterminology systems such as SNOMED, RxNorm, NDF-RT, and MedDRA, as well as developing new models such asVaccine Ontology. The VDOS workshop series provides a platform for discussing innovative solutions as well as thechallenges in the development and applications of biomedical ontologies for representing and analyzing drugs andvaccines, their administration, host immune responses, adverse events, and other related topics. The six full-lengthpapers included in this thematic issue focuses on three main areas: (i) ontology development and representation,(ii) ontology mapping, maintaining and auditing, and (iii) ontology applications.Introduction and backgroundDrugs and vaccines have been critical to prevent andtreat human and animal diseases. Work in both (drugsand vaccines) areas is closely related - from preclinicalresearch and development to manufacturing, clinical trials,government approval and regulation, and post-licensureusage surveillance and monitoring. Many drug and vaccinerelated ontologies have already been or are being developedfor different use cases and applications. The 2013 Vaccineand Drug Ontology Studies workshop (VDOS 2013) work-shop series aims to become an international forum forresearchers to identify, propose, and discuss solutions forimportant research problems in ontology representationand analysis of vaccine and drug formation and prepar-ation, administration, function mechanisms, and inducedhost immune responses. The immune responses can bepositive responses for prevention and/or treatment of adisease, or can be negative responses, i.e., adverse events.This workshop aimed to support the deeper understand-ing of vaccine and drug mechanisms and effects.VDOS 2013 was held on July 7, 2013, at Montreal, Qc,Canada. This workshop was part of the fourth Inter-national Conference on Biomedical Ontology (ICBO 2013).* Correspondence: cui.tao@uth.tmc.edu1School of Biomedical Informatics, The University of Texas Health ScienceCenter, Houston, TX, USAFull list of author information is available at the end of the article© 2014 Tao et al.; licensee BioMed Central LtdCommons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.The workshop attracted interest from many internationalattendees, including paper presenters, senior academic andgovernment scientists, postdoctoral fellows, and graduatestudents. After a rigorous peer review process (all submis-sions have been reviewed by at least three independentreviewers), six full-length papers and three short-lengthpapers were accepted for proceeding paper publicationsand oral presentations in the workshop. After one add-itional round of independent peer reviewing by theworkshop co-organizers and the journal editors, theselected six full-length papers were extended and acceptedfor publication in the current issue of the Journal ofBiomedical Semantics (JBMS).The VDOS-2013 workshop is the 2nd in this series.The first workshop of the series was organized as theVaccine and Drug Ontology in the Study of Mechanismand Effect workshop (VDOSME 2012) [1] on July 21,2012, at Graz, Germany, as part of the third InternationalConference on Biomedical Ontology (ICBO 2012). Forthis year, the name has been changed to Vaccine andDrug Ontology Studies (VDOS) to reflect the expansionin the scope to more than just mechanism and effect.The workshop series also covers vaccine and drug-relatedclinical data representation and analysis, including clinic-ally reported vaccine and drug adverse events.. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Tao et al. Journal of Biomedical Semantics 2014, 5:16 Page 2 of 3http://www.jbiomedsem.com/content/5/1/16Summary of selected papers in the thematic issueThe six papers selected for this thematic issue are extendedversions of the original full-length papers presented atthe VDOS 2013 [2-7]. These papers cover a wide rangeof topics including ontology development and repre-sentation, ontology mapping, maintaining and auditing,and ontology applications.In the area of ontology development and representation,Lin and He introduced their Ontology of Genetic Suscep-tibility Factors (OGSF) for representing the susceptibilityfactors for post vaccination events using a formal onto-logical mechanism [2]. OGSF is aligned with the BasicFormal Ontology (BFO). This paper ontologically definestwo core OGSF terms genetic susceptibility and geneticsusceptibility factor and the design pattern for represent-ing genetic susceptibility to a vaccine adverse event. Twouse cases related to the genetic susceptibility to adverseevents following vaccination of an influenza vaccine orsmallpox vaccine were studied using the OGSF ontology.Hanna et al built the Drug Ontology (DrOn) to modeldrug information for comparative-effectiveness research[3]. DrOn is represented in OWL2 and covers drug infor-mation derived from three different sources (RxNorm,ChEBI, and PRO). Although DrOn was originally designedfor comparative-effectiveness research studies, it can alsoservice many other use cases in the biomedical domain.Marcos and He [4] developed the Ontology of VaccineAdverse Events (OVAE). OVAE was built as an extensionof the Vaccine Ontology (VO) and the Ontology of AdverseEvents (OAE). It represents and classifies the adverse eventsrecorded in package insert documents of commercial vac-cines licensed by the USA Food and Drug Administration(FDA). OVAE will be very useful in supporting rationalVAE prevention and treatment and benefits public health.In the area of ontology mapping, Winnenburg, et al.tried to map the terms in (Anatomical TherapeuticChemical (ATC) and Medical Subject Headings (MeSH)through the Unified Medical Language System (UMLS)[5]. Both lexical-based and instance-based mapping wereperformed, which yielded hundreds of new mappingsbetween the two terminology systems. The alignmentbetween ATC and MeSH can be critical for drug evalu-ation and safety studies as well as for pharmacogeneticresearch. On one hand, MEDLINE literature is indexedusing MeSH. On the other hand, adverse drug events areusually analyzed in reference to ATC. In order to integratedrug information from these different sources, a reliablealignment between ATC and MeSH is very much desired.In the area of ontology applications, Doulaverakis et al.present a semantic framework to discover drug-drug anddrug-disease interactions [6]. They use SKOS to representthe semantics of the medical classification for drug rele-vant information derived from ICD-10, Unique Ingredi-ent Identifier (UNII), ATC, and the International VirusTaxonomy (IVT). Rule-based reasoning approaches werethen used to identify drug recommendations. Zhang et al.introduce a novel approach that combines ontologies andnetwork analysis technologies for identifying new associa-tions among vaccines, genes, and diseases [7]. The authorsleverage data extracted from MEDLINE and representedthis information using Resource Description Framework(RDF). This RDF graph can then be viewed as a networkto perform network analysis.Workshop presentations and discussionsIn the workshop, the six full-length papers described abovewere orally presented. In addition, three short papers wereaccepted for short oral presentations. Zhu et al. introducestheir work on building a drug and drug class networkderived from multiple drug terminological resources, suchas ATC, National Drug File Reference Terminology (NDF-RT), RxNorm, and Structured Product Label (SPL) [8]. Heet al. investigated how to audit the redundancies causedby importing top-level ontologies [9]. More specifically,they studied the redundancies in Drug Discovery Investi-gations ontology (DDI) when importing BFO. Hall et al.introduces their software that supports extracting newdrug information from drug structured product labels(SPL) to update the DrOn [10].During the discussion session, we discussed two mainareas  (a) mapping between different drug models (6papers), and (b) adverse events detection and analysis(3 papers). The major focuses on drug ontologies them-selves (including models that contribute greatly in thisarea), their goals, and the challenges in aligning them,show that some of the preparatory work still needs tobe done to continue the research into adverse events. Anumber of different approaches to drug mapping werediscussed  lexical, ingredient based, using chemicalstructures and by exploiting networks (e.g. via the UMLS).While each provided their unique benefits, there wasconcurrence on the need for using more than onemethod to improve the quality of mapping. Furthermore,these papers also demonstrated the need for enhancingthe definitions (logical and textual) of the terms and therelations both for improved human understanding as wellas for better integration between them. The two papersfocusing on adverse event detection and analysis alsohighlighted some of the gaps and shed light on areasfor further ontology development efforts. These studiesalso demonstrated promising usages and advantages ofontology in standardizing, integrating, and analyzingadverse event data.Overall, the VDOS 2013 workshop provided an idealplatform for ontology researchers and users to presentand discuss the progresses and issues in the developmentand applications of ontologies related to vaccines anddrugs. Positive feedbacks were obtained.Tao et al. Journal of Biomedical Semantics 2014, 5:16 Page 3 of 3http://www.jbiomedsem.com/content/5/1/16Competing interestsThe authors declare that they have no competing interests.AcknowledgementsAs editors of this thematic issue, we thank all the authors who submittedpapers, the Program Committee members and the reviewers for theirexcellent work. We appreciate the support and help from the ICBO 2013meeting organizers. We are grateful for editorial reviews from Dr. DietrichRebholz-Schuhmann from JBMS.Author details1School of Biomedical Informatics, The University of Texas Health ScienceCenter, Houston, TX, USA. 2Unit for Laboratory Animal Medicine, Departmentof Microbiology and Immunology, and Center for Computational Medicineand Bioinformatics, University of Michigan Medical School, Ann Arbor, MI,USA. 3Ontopro LLC, Houston, TX, USA.Received: 26 February 2014 Accepted: 17 March 2014Published: 20 March 2014JOURNAL OFBIOMEDICAL SEMANTICSClark et al. Journal of Biomedical Semantics 2014, 5:28http://www.jbiomedsem.com/content/5/1/28RESEARCH Open AccessMicropublications: a semantic model for claims,evidence, arguments and annotations inbiomedical communicationsTim Clark1,2,3*, Paolo N Ciccarese1,2 and Carole A Goble3AbstractBackground: Scientific publications are documentary representations of defeasible arguments, supported by dataand repeatable methods. They are the essential mediating artifacts in the ecosystem of scientific communications.The institutional goal of science is publishing results. The linear document publication format, dating from 1665,has survived transition to the Web.Intractable publication volumes; the difficulty of verifying evidence; and observed problems in evidence andcitation chains suggest a need for a web-friendly and machine-tractable model of scientific publications. This modelshould support: digital summarization, evidence examination, challenge, verification and remix, and incrementaladoption. Such a model must be capable of expressing a broad spectrum of representational complexity, rangingfrom minimal to maximal forms.Results: The micropublications semantic model of scientific argument and evidence provides these features.Micropublications support natural language statements; data; methods and materials specifications; discussion andcommentary; challenge and disagreement; as well as allowing many kinds of statement formalization.The minimal form of a micropublication is a statement with its attribution. The maximal form is a statement with itscomplete supporting argument, consisting of all relevant evidence, interpretations, discussion and challengesbrought forward in support of or opposition to it. Micropublications may be formalized and serialized in multipleways, including in RDF. They may be added to publications as stand-off metadata.An OWL 2 vocabulary for micropublications is available at http://purl.org/mp. A discussion of this vocabulary alongwith RDF examples from the case studies, appears as OWL Vocabulary and RDF Examples in Additional file 1.Conclusion: Micropublications, because they model evidence and allow qualified, nuanced assertions, can playessential roles in the scientific communications ecosystem in places where simpler, formalized and purelystatement-based models, such as the nanopublications model, will not be sufficient. At the same time they will addsignificant value to, and are intentionally compatible with, statement-based formalizations.We suggest that micropublications, generated by useful software tools supporting such activities as writing, editing,reviewing, and discussion, will be of great value in improving the quality and tractability of biomedicalcommunications.Keywords: Argumentation, Annotation, Data citation, Digital abstract, Scientific discourse, Scientific evidence,Methods citation, Research reproducibility, Nanopublications* Correspondence: tim_clark@harvard.edu1Department of Neurology, Massachusetts General Hospital, 55 Fruit Street,Boston, MA 02114, USA2Harvard Medical School, 25 Shattuck Street, Boston, MA 02115, USAFull list of author information is available at the end of the article© 2014 Clark et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited.Clark et al. Journal of Biomedical Semantics 2014, 5:28 Page 2 of 33http://www.jbiomedsem.com/content/5/1/28IntroductionDuring the past two decades the ecosystem of biomedicalpublications has moved from a print-based to a mainlyWeb-based model. However, this transition brings with itmany new problems, in the context of an exponentiallyincreasing, intractable volume of publications [1,2]; of sys-temic problems relating to valid (or invalid) citation of sci-entific evidence [3,4]; rising levels of article retractions[5,6] and scientific misconduct [7]; of uncertain reproduci-bility and re-usability of results in therapeutic develop-ment [8], and lack of transparency in research publication[9]. While we now have rapid access to much of theworlds biomedical literature, our methods to organize,verify, assess, combine and absorb this information in acomprehensive way, and to move discussion and anno-tation activities through the ecosystem efficiently, re-main disappointing.Computational methods previously proposed as solutionsinclude ontologies [10]; text mining [2,11,12]; databases[13]; knowledgebases [14]; visualization [15]; new forms ofpublishing [16]; digitial abstracting [1]; semantic annotating[17]; and combinations of these approaches. However, welack a comprehensive means to orchestrate these methods.We propose to accomplish this with a layered metadatamodel of scientific argumentation and evidence.Such a common metadata representation of scientificclaims, argument, evidence and annotation in biomedi-cine should serve as an integrating point for the originalpublication, subsequent annotations, and all other com-putational methods, supporting a single framework foractivities in the nine point cycle of authoring-publishing-consumption-reuse we discuss in the section on UseCases. This cycle can be thought of as an informationvalue chain in science. This means that each set of dis-parately motivated and rewarded activities, carried outby various actors, creates and passes along value to thenext, which consumes this value-added product as an in-put. A metadata representation to support this value chainwould need to: serve as a common Web-friendly nucleus forvalue-addition and extraction across the biomedicalcommunications ecosystem: understood, operatedupon and exchanged by humans and by computers,as supplements to the linear documents theycharacterize; enable more powerful use and sharing ofinformation in biomedicine, particularly throughintegration and mashup to provide the mostrelevant views for any social unit of researchers; enable the addition of value to the content whileproviding a detailed provenance of what was done; support computational processing in a way thatcomplete papers in un-augmented linear naturallanguage cannot yet integrate well with existinglinear textual representations.This paper introduces the micropublications semanticmetadata model. The micropublications model is adaptedto the Web, and designed for (a) representing the key ar-guments and evidence in scientific articles, and (b) sup-porting the layering of annotations and various usefulformalizations upon the full text paper.This model responds to the nine use cases we present,in which digital summarization of scientific argumentationwith its evidence and methodological support is required.These use cases, for the most part, deal directly with thescientific literature, rather than its processed reflection incurated topical databases. They illustrate how and whycurrently proposed statement-based approaches needricher representation and how this model can play sucha role.In this paper we present a Use Case analysis mapped to sets of commonactivities in the biomedical communicationsecosystem, showing the potential value addition andpath to implementation of the proposed model foreach Use Case; a formal model of micropublications; illustrative examples instantiating the model for eachUse Case; notes on an interface to nanopublications and otherstatement-based formalizations; discussion on how the model can supportreproducibility and verifiability in research; onimplementation in software; and relationship toother work; and our Conclusions about the role of this model innext-generation scientific publishing.We also provide, in three separate files of AdditionalMaterial:1. detailed class, predicate and rule definitions;2. a proposed Web-friendly representation, usingcommunity ontologies, serialized in the W3CWeb Ontology Language, with a set of examplesin RDF; and3. a comparison of micropublications to the SWANmodel.BackgroundBeyond statement-based modelsStatement-based models have been proposed as mecha-nisms for publishing key facts asserted in the scientific lit-erature or in curated databases in a machine processableform. Examples include: Biological Expression LanguageClark et al. Journal of Biomedical Semantics 2014, 5:28 Page 3 of 33http://www.jbiomedsem.com/content/5/1/28(BEL) statements [18]; SWAN, a model for claims andhypotheses in natural language developed for the annota-tion of scientific hypotheses in Alzheimers Disease (AD)research [14,19-21]; and nanopublications [22-26], whichcontribute to the Open PHACTS linked data warehouseof pharmacological data [26].What we mean by statement-based is that they con-fine themselves to modeling statements found in scien-tific papers or databases, with limited or no presentationof the backing evidence for these statements. Some offerstatement backing in the form of other statements in thescientific literature, but none actually has a complete rep-resentation of scientific argument including empirical evi-dence and methods. Of the three examples we mention, Nanopublications model only the indicatedstatement; SWAN models a principal statement, orhypothesis, with supporting statements, orclaims, from the same publication only, andJOURNAL OFBIOMEDICAL SEMANTICSHenriksson et al. Journal of Biomedical Semantics 2014, 5:6http://www.jbiomedsem.com/content/5/1/6RESEARCH Open AccessSynonym extraction and abbreviationexpansion with ensembles of semantic spacesAron Henriksson1*, Hans Moen2, Maria Skeppstedt1, Vidas Daudaravic?ius3 and Martin Duneld1AbstractBackground: Terminologies that account for variation in language use by linking synonyms and abbreviations totheir corresponding concept are important enablers of high-quality information extraction from medical texts. Due tothe use of specialized sub-languages in the medical domain, manual construction of semantic resources thataccurately reflect language use is both costly and challenging, often resulting in low coverage. Although models ofdistributional semantics applied to large corpora provide a potential means of supporting development of suchresources, their ability to isolate synonymy from other semantic relations is limited. Their application in the clinicaldomain has also only recently begun to be explored. Combining distributional models and applying them to differenttypes of corpora may lead to enhanced performance on the tasks of automatically extracting synonyms andabbreviation-expansion pairs.Results: A combination of two distributional models  Random Indexing and Random Permutation  employed inconjunction with a single corpus outperforms using either of the models in isolation. Furthermore, combiningsemantic spaces induced from different types of corpora  a corpus of clinical text and a corpus of medical journalarticles  further improves results, outperforming a combination of semantic spaces induced from a single source, aswell as a single semantic space induced from the conjoint corpus. A combination strategy that simply sums the cosinesimilarity scores of candidate terms is generally the most profitable out of the ones explored. Finally, applying simplepost-processing filtering rules yields substantial performance gains on the tasks of extracting abbreviation-expansionpairs, but not synonyms. The best results, measured as recall in a list of ten candidate terms, for the three tasks are:0.39 for abbreviations to long forms, 0.33 for long forms to abbreviations, and 0.47 for synonyms.Conclusions: This study demonstrates that ensembles of semantic spaces can yield improved performance on thetasks of automatically extracting synonyms and abbreviation-expansion pairs. This notion, which merits furtherexploration, allows different distributional models  with different model parameters  and different types of corporato be combined, potentially allowing enhanced performance to be obtained on a wide range of natural languageprocessing tasks.BackgroundIn order to create high-quality information extraction sys-tems, it is important to incorporate some knowledge ofsemantics, such as the fact that a concept can be signifiedby multiple signifiersa. Morphological variants, abbrevia-tions, acronyms, misspellings and synonyms  althoughdifferent in form  may share semantic content to differ-ent degrees. The various lexical instantiations of a concept*Correspondence: aronhen@dsv.su.seEqual contributors1Department of Computer and Systems Sciences (DSV), Stockholm University,Forum 100, SE-164 40 Kista, SwedenFull list of author information is available at the end of the articlethus need to be mapped to some standard representa-tion of the concept, either by converting the differentexpressions to a canonical form or by generating lexicalvariants of a concepts preferred term. These mappingsare typically encoded in semantic resources, such as the-sauri or ontologiesb, which enable the recall (sensitivity) ofinformation extraction systems to be improved. Althoughtheir value is undisputed, manual construction of suchresources is often prohibitively expensive and may alsoresult in limited coverage, particularly in the biomedi-cal and clinical domains where language use variability isexceptionally high [1].© 2014 Henriksson et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly cited.Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 2 of 25http://www.jbiomedsem.com/content/5/1/6There is thus a need for (semi-)automatic methods thatcan aid and accelerate the process of lexical resourcedevelopment, especially ones that are able to reflect reallanguage use in a particular domain and adapt to differ-ent genres of text, as well as to changes over time. Inthe clinical domain, for instance, language use in gen-eral, and (ad-hoc) abbreviations in particular, can varysignificantly across specialities. Statistical, corpus-drivenand language-agnostic methods are attractive due to theirinherent portability: given a corpus of sufficient size inthe target domain, the methods can be applied with no orlittle adaptation needed. Models of distributional seman-tics, building on the assumption that linguistic items withsimilar distributions in large bodies of linguistic datahave similar meanings, fulfill these requirements and havebeen used to extract semantically similar terms from largecorpora; with increasing access to data from electronichealth records, their application in the clinical domainhas lately begun to be explored. In this paper, we presenta method that employs distributional semantics for theextraction of synonyms and abbreviation-expansion pairsfrom two corpora: a clinical corpus (comprising healthrecord narratives) and a medical corpus (comprising jour-nal articles). We also demonstrate that performance canbe enhanced by creating ensembles of (distributional)semantic spaces  both with different model param-eter configurations and induced from different genresof text.The structure of this paper is as follows. First, wepresent some relevant background literature on syn-onyms, abbreviations and their extraction/expansion. Wealso introduce the ideas underlying distributional seman-tics in general and, in particular, the models employed inthis study: Random Indexing and Random Permutation.Then, we describe our method of combining semanticspaces induced from single and multiple corpora, includ-ing the details of the experimental setup and the mate-rials used. A presentation of the experimental results isfollowed by an analysis and discussion of their implica-tions. Finally, we conclude the paper with a summary andconclusions.Language use variability: synonyms and abbreviationsSynonymy is a semantic relation between two phono-logically distinct words with very similar meaning. It is,however, extremely rare that two words have the exactsame meaning  perfect synonyms  as there is often atleast one parameter that distinguishes the use of one wordfrom another [2]. For this reason, we typically speak ofnear-synonyms instead; that is, two words that are inter-changeable in some, but not all, contextsc [2]. Two near-synonyms may also have different connotations, such asconveying a positive or a negative attitude. To compli-cate matters further, the same concept can sometimes bereferred to with different words in different dialects; fora speaker who is familiar with both dialects, these canbe viewed as synonyms. A similar phenomenon concernsdifferent formality levels, where one word in a synonympair is used only as slang and the other only in a moreformal context [2]. In the medical domain, there is onevocabulary that is more frequently used by medical pro-fessionals, whereas patients often use alternative, laymanterms [3]. When developing many natural language pro-cessing (NLP) applications, it is important to have readyaccess to terminological resources that cover this variationin the use of vocabulary by storing synonyms. Examples ofsuch applications are query expansion [3], text simplifica-tion [4] and, as already mentioned previously, informationextraction [5].The use of abbreviations and acronyms is prevalent inboth medical journal text [6] and clinical text [1]. Thisleads to decreased readability [7] and poses challengesfor information extraction [8]. Semantic resources thatalso link abbreviations to their corresponding concept, or,alternatively, simple term lists that store abbreviations andtheir corresponding long form, are therefore as importantas synonym resources for many biomedical NLP appli-cations. Like synonyms, abbreviations are often inter-changeable with their corresponding long form in some, ifnot all, contexts. An important difference between abbre-viations and synonyms is, however, that abbreviations aresemantically overloaded to a much larger extent; that is,one abbreviation often has several possible long forms,with distinct meanings. In fact, 81% of UMLSd abbrevia-tions in biomedical text were found to be ambiguous [6].Identifying synonymous relations between termsThe importance of synonym learning is well recognized inthe NLP research community, especially in the biomedical[9] and clinical [1] domains. A wide range of techniqueshas been proposed for the identification of synonymsand other semantic relations, including the use of lexico-syntactic patterns, graph-based models and, indeed, dis-tributional semantics [10]  the approach investigated inthis study.For instance, Hearst [11] proposes the use of lexico-syntactic patterns for the automatic acquisition ofhyponymse from unstructured text. These patterns arehand-crafted according to observations in a corpus. Pat-terns can similarly be constructed for other types of lexicalrelations. However, a requirement is that these syntacticpatterns are common enough to match a wide array ofhyponym pairs. Blondel et al. [12] present a graph-basedmethod that takes its inspiration from the calculation ofhub, authority and centrality scores when ranking hyper-linked web pages. They illustrate that the central similarityscore can be applied to the task of automatically extract-ing synonyms from a monolingual dictionary, in this caseHenriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 3 of 25http://www.jbiomedsem.com/content/5/1/6the Webster dictionary, where the assumption is thatsynonyms have a large overlap in the words used in theirdefinitions; they also co-occur in the definition of manywords. Another possible source for extracting synonymsis the use of linked data, such as Wikipedia. Nakayamaet al. [13] also utilize a graph-basedmethod, but instead ofrelying on word co-occurrence information, they exploitthe links betweenWikipedia articles (treated as concepts).This way they can measure both the strength (the numberof paths from one article to another) and the distance (thelength of each path) between concepts: concepts close toeach other in the graph and with common hyperlinks aredeemed to bemore closely related than those farther away.There have also been some previous efforts to obtainbetter performance on the synonym extraction task by notonly using a single source and a single method. Inspirationfor some of these approaches has been drawn from ensem-ble learning, a machine learning technique that combinesthe output of several different classifiers with the aimof improving classification performance (see [14] for anoverview). Curran [15] exploits this notion for synonymextraction and demonstrates that ensemble methods out-perform individual classifiers even for very large corpora.Wu and Zhou [16] use multiple resources  a monolin-gual dictionary, a bilingual corpus and a largemonolingualcorpus  in a weighted ensemble method that combinesthe individual extractors, thereby improving both preci-sion and recall on the synonym acquisition task. Alongsomewhat similar lines, van der Plas and Tiedemann [17]use parallel corpora to calculate distributional similaritybased on (automatic) word alignment, where a trans-lational context definition is employed; synonyms areextracted with both greater precision and recall com-pared to a monolingual approach. This approach is, how-ever, hardly applicable in the medical domain due to theunavailability of parallel corpora. Peirsman and Geeraerts[18] combine predictors based on collocation measuresand distributional semantics with a so-called compound-ing approach, wherein cues are combined with stronglyassociated words into compounds and verified against acorpus. This ensemble approach is shown substantiallyto outperform the individual predictors of strong termassociations in a Dutch newspaper corpus. In informa-tion retrieval, Diaz and Metzler [19] report increasedperformance gains when utilizing language models thatderive evidence from both a target corpus and an externalcorpus, compared to using the target corpus alone.In the biomedical domain, most efforts have focusedon extracting synonyms of gene and protein namesfrom the biomedical literature [20-22]. In the clinicaldomain, Conway and Chapman [23] propose a rule-basedapproach to generate potential synonyms from the Bio-Portal ontology  using permutations, abbreviation gener-ation, etc.  after which candidate synonyms are verifiedagainst a large clinical corpus. Henriksson et al. [24,25]use models of distributional semantics to induce unigramword spaces and multiword term spaces from a large cor-pus of clinical text in an attempt to extract synonyms ofvarying length for SNOMED CT preferred terms. Zenget al. [26] evaluate three query expansion methods forretrieval of clinical documents and conclude that an LDA-based topic model generates the best synonyms. Pedersenet al. [27] explore a set of measures for automaticallyjudging semantic similarity and relatedness among med-ical term pairs that have been pre-assessed by humanexperts. The measures range from ones based on thesaurior ontologies (WordNet, SNOMED-CT, UMLS, MayoClinic Thesaurus) to those based on distributional seman-tics. They find that the measure based on distributionalsemantics performs at least as good as any of the ontology-dependentmeasures. In a similar task, Koopman et al. [28]evaluate eight different data-driven measures of seman-tic similarity. Using two separate training corpora, onecontaining clinical notes and the other medical literaturearticles, they conclude that the choice of training cor-pus has a significant impact on the performance of thesemeasures.Creating abbreviation dictionaries automaticallyThere are a number of studies on the automatic creation ofbiomedical abbreviation dictionaries that exploit the factthat abbreviations are sometimes defined in the text ontheir first mention. These studies extract candidates forabbreviation-expansion pairs by assuming that either thelong form or the abbreviation is written in parentheses[29]; other methods that use rule-based pattern matchinghave also been proposed [30]. The process of determin-ing which of the extracted candidates that are likely tobe correct abbreviation-expansion pairs is then performedeither by rule-based [30] or machine learning [31,32]methods. Most of these studies have been conducted forEnglish; however, there is also a study on Swedish medicaltext [33], for instance.Yu et al. [34] have, however, found that around 75% ofall abbreviations found in biomedical articles are neverdefined in the text. The application of these methods toclinical text is most likely inappropriate, as clinical text isoften written in a telegraphic style, mainly for documen-tation purposes [1]; that effort would be spent on definingused abbreviations in this type of text seems unlikely.There has, however, been some work on identifying suchundefined abbreviations [35], as well as on finding theintended abbreviation expansion among several possibleexpansions available in an abbreviation dictionary [36].In summary, automatic creation of biomedical abbre-viation dictionaries from texts where abbreviations aredefined is well studied. This is also the case for abbrevia-tion disambiguation given several possible long forms inHenriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 4 of 25http://www.jbiomedsem.com/content/5/1/6an abbreviation dictionary. The abbreviation part of thisstudy, however, focuses on a task that has not as yet beenadequately explored: to find abbreviation-expansion pairswithout requiring the abbreviations to be defined in thetext.Distributional semantics: inducing semantic spaces fromcorporaDistributional semantics (see [37] for an overview ofmethods and their application in the biomedical domain)were initially motivated by the inability of the vectorspace model [38]  as it was originally conceived  toaccount for the variability of language use andword choicestemming from natural language phenomena such as syn-onymy. To overcome the negative impact this had onrecall in information retrieval systems, models of dis-tributional semantics were proposed [39-41]. The the-oretical foundation underpinning such semantic modelsis the distributional hypothesis [42], which states thatwords with similar distributions in language  in thesense that they co-occur with overlapping sets of words tend to have similar meanings. Distributional methodshave become popular with the increasing availabilityof large corpora and are attractive due to their com-putational approach to semantics, allowing an estimateof the semantic relatedness between two terms to bequantified.An obvious application of distributional semantics isthe extraction of semantically related terms. As near-synonyms are interchangeable in at least some contexts,their distributional profiles are likely to be similar, whichin turn means that synonymy is a semantic relation thatshould, to a certain degree, be captured by these meth-ods. This seems intuitive, as, next to identity, the highestdegree of semantic relatedness between terms is real-ized by synonymy. It is, however, well recognized thatother semantic relations between terms that share similarcontexts will likewise be captured by these models [43];synonymy cannot readily be isolated from such relations.Spatial modelsf of distributional semantics generallydiffer in how vectors representing term meaning are con-structed. These vectors, often referred to as context vec-tors, are typically derived from a term-context matrix thatcontains the (weighted, normalized) frequency with whichterms occur in different contexts. Working directly withsuch high-dimensional (and inherently sparse) data where the dimensionality is equal to the number of con-texts (e.g. the number of documents or the size of thevocabulary, depending on which context definition isemployed)  would entail unnecessary computationalcomplexity, in particular since most terms only occur ina limited number of contexts, which means that mostcells in the matrix will be zero. The solution is to projectthe high-dimensional data into a lower-dimensionalspace, while approximately preserving the relative dis-tances between data points. The benefit of dimensionalityreduction is two-fold: on the one hand, it reduces com-plexity and data sparseness; on the other hand, it hasalso been shown to improve the coverage and accuracyof term-term associations, as, in this reduced (semantic)space, terms that do not necessarily co-occur directly inthe same contexts  this is indeed the typical case for syn-onyms and abbreviation-expansion pairs  will neverthe-less be clustered about the same subspace, as long as theyappear in similar contexts, i.e. have neighbors in common(co-occur with the same terms). In this way, the reducedspace can be said to capture higher order co-occurrencerelations.In latent semantic analysis (LSA) [39], dimensionalityreduction is performed with a computationally expensivematrix factorization technique known as singular valuedecomposition. Despite its popularity, LSA has conse-quently received some criticism for its poor scalabilityproperties. More recently, alternative methods for con-structing semantic spaces based on term co-occurrenceinformation have been proposed.Random indexingRandom indexing (RI) [44] is an incremental, scalableand computationally efficient alternative to LSA in whichexplicit dimensionality reduction is avoidedg: a lowerdimensionality d is instead chosen a priori as a modelparameter and the d-dimensional context vectors are thenconstructed incrementally. This approach allows new datato be added at any given time without having to rebuild thesemantic space. RI can be viewed as a two-step operation:1. Each context (e.g. each document or unique term) isfirst given a static, unique representation in thevector space that is approximately uncorrelated to allother contexts. This is achieved by assigning a sparse,ternaryh and randomly generated d-dimensionalindex vector: a small number (usually around 12%)of +1s and ?1s are randomly distributed, with therest of the elements set to zero. By generating sparsevectors of a sufficiently high dimensionality in thisway, the index vectors will be nearly orthogonali.2. Each unique term is assigned an initially emptycontext vector of the same dimensionality d. Thecontext vectors are then incrementally populatedwith context information by adding the (weighted)index vectors of the contexts in which the targetterm appears. With a sliding window contextdefinition, this means that the index vectors of thesurrounding terms are added to the target termscontext vector. The meaning of a term, representedby its context vector, is effectively the (weighted)sum of all the contexts in which it occurs.Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 5 of 25http://www.jbiomedsem.com/content/5/1/6RandompermutationModels of distributional semantics, including RI, generallytreat each context as a bag of wordsj. Suchmodels are oftencriticized for failing to account for term order. Recently,methods have been developed for building distributionalsemantic models that store and emphasize word orderinformation [45-47]. Random permutation (RP) [46] is amodification of RI that encodes term order information bysimply permuting (i.e., shifting) the elements in the indexvectors according to their direction and distancek fromthe target term before they are added to the context vector.For instance, before adding the index vector of a term twopositions to the left of the target term, the elements areshifted two positions to the left; similarly, before addingthe index vector of a term one position to the right of thetarget term, the elements are shifted one position to theright. In effect, each term has multiple unique representa-tions: one index vector for each possible position relativeto the target term in the context window. Incorporat-ing term order information not only enables order-basedretrieval; it also constrains the types of semantic relationsthat are captured.Model parametersThere are a number of model parameters that need to beconfigured according to the task that the induced seman-tic spaces will be used for. For instance, the types ofsemantic relations captured depends on the context def-inition [43,48]. By employing a document-level contextdefinition, relying on direct co-occurrences, one modelssyntagmatic relations. That is, two terms that frequentlyco-occur in the same documents are likely to be about thesame general topic. By employing a sliding window con-text definition, one models paradigmatic relations. Thatis, two terms that frequently co-occur with similar sets ofwords  i.e., share neighbors  but do not necessarily co-occur themselves, are semantically similar. Synonymy isa prime example of a paradigmatic relation. The size ofthe context window also affects the types of relations thatare modeled and needs to be tuned for the task at hand.This is also true for semantic spaces produced by RP; how-ever, the precise impact of window size on RP spaces andthe internal relations of their context vectors is yet to bestudied in depth.MethodThe main idea behind this study is to enhance theperformance on the task of extracting synonyms andabbreviation-expansion pairs by combining multiple anddifferent semantic spaces  different in terms of (1) typeof model and model parameters used, and (2) type ofcorpus from which the semantic space is induced. In addi-tion to combining semantic spaces induced from a singlecorpus, we also combine semantic spaces induced fromtwo different types of corpora: in this case, a clinicalcorpus (comprising health record notes) and a medi-cal corpus (comprising journal articles). The notion ofcombining multiple semantic spaces to improve perfor-mance on some task is generalizable and can loosely bedescribed as creating ensembles of semantic spaces. Bycombining semantic spaces, it becomes possible to benefitfrom model types that capture slightly different aspects ofsemantics, to exploit various model parameter configura-tions (which influence the types of semantic relations thatare modeled), as well as to observe language use in poten-tially very different contexts (by employing more than onecorpus type).We set out exploring this approach by query-ing each semantic space separately and then combiningtheir output using a number of combination strategies(Figure 1).The experimental setup can be divided into the fol-lowing steps: (1) corpora preprocessing, (2) constructionof semantic spaces from the two corpora (and from theconjoint corpus), (3) identification of the most profitablesingle-corpus (and conjoint corpus) combinations, (4)identification of the most profitable (disjoint) multiple-corpora combinations, (5) evaluations of the single-corpus(including the conjoint corpus) and multiple-corporacombinations, (6) post-processing of candidate terms, and(7) frequency threshold experiments. Once the corporahave been preprocessed, ten semantic spaces from eachcorpus, as well as the conjoint corpus, are induced withdifferent context window sizes (RP spaces are inducedwith and without stop words). Ten pairs of semanticspaces are then combined using three different combina-tion strategies. These are evaluated on the three tasks  (1)abbreviations ? expansions, (2) expansions ? abbrevia-tions and (3) synonyms  using the development subsetsof the reference standards (a list of medical abbreviation-expansion pairs for 1 and 2 and MeSH synonyms for 3).Performance is mainly measured as recall top 10, i.e. theproportion of expected candidate terms that are amonga list of ten suggestions. The pair of semantic spacesinvolved in the most profitable combination for each cor-pus is then used to identify the most profitable multiple-corpora combinations, where eight different combinationstrategies are evaluated. The best single-corpus combi-nations are evaluated on the evaluation subsets of thereference standards, where using RI and RP in isolationconstitute the two baselines. The best multiple-corporacombination is likewise evaluated on the evaluation sub-sets of the reference standards; here, the results arecompared both to (1) semantic spaces induced from asingle corpus and the conjoint corpus, and (2) ensem-bles of semantic spaces induced from a single corpus (andthe conjoint corpus). Post-processing rules are then con-structed using the development subsets of the referencestandards and the outputs of the various semantic spaceHenriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 6 of 25http://www.jbiomedsem.com/content/5/1/6Figure 1 Ensembles of semantic spaces for synonym extraction and abbreviation expansion. Semantic spaces built with different modelparameters are induced from different corpora. The output of the semantic spaces are combined in order to obtain better results compared tousing a single semantic space in isolation.combinations. These are evaluated on the evaluation sub-sets of the reference standards using the most profitablesingle-corpus and multiple-corpora ensembles. All eval-uations on the evaluation subsets of the reference stan-dards also include an evaluation of weighted precision,see Eq. 1:Weighted Precision :Pw =?j?1i=0 (j ? i) · f (i)?j?1i=0 j ? iwheref (i) ={1 if i ? {tp}0 otherwise(1)and j is the pre-specified number of labels  here, ten,except in the case of a dynamic cut-off  and {tp} is the setof true positives. In words, this assigns a score to true pos-itives according to their (reverse) ranking in the list, sumstheir scores and divides the total score by the maximumpossible score (where all j labels are true positives).Finally, we explore the impact of frequency thresholds(i.e., how many times each pair of terms in the referencestandards needs to occur to be included) on performance.Inducing semantic spaces from clinical andmedical corporaEach individual semantic space is constructed with onemodel type, using a predefined context window size andinduced from a single corpus type. The semantic spacesare constructed with random indexing (RI) and randompermutation (RP) using JavaSDM [49]. For all semanticspaces, a dimensionality of 1,000 is used (with 8 non-zero,randomly distributed elements in the index vectors: four1s and four -1s). When the RI model is employed, theindex vectors are weighted according to their distancefrom the target term, see Eq. 2, where distit is the distanceto the target term. When the RP model is employed, theelements of the index vectors are instead shifted accord-ing to their direction and distance from the target term;no weighting is performed.weighti = 21?distit (2)For all models, window sizes of two (1 + 1), four (2 + 2)and eight (4 + 4) surrounding terms are used. In addition,RI spaces with a window size of twenty (10 + 10) areinduced in order to investigate whether a significantlywider context definition may be profitable. Incorporatingorder information (RP) with such a large context windowmakes little sense; such an approach would also sufferfrom data sparseness. Different context definitions areexperimented with in order to find one that is best suitedto each task. The RI spaces are induced only from corporathat have been stop-word filtered, as co-occurrence infor-mation involving high-frequent and widely distributedwords contribute very little to the meaning of terms. TheRP spaces are, however, also induced from corpora inwhich stop words have been retained. The motivationbehind this is that all words, including function words these make up the majority of the items in the stop-wordlists  are important to the syntactic structure of languageand may thus be of value when modeling order infor-mation [45]. A stop-word list is created for each corpusby manually inspecting the most frequent word typesand removing from the list those words that may be ofHenriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 7 of 25http://www.jbiomedsem.com/content/5/1/6interest, e.g. domain-specific terms. Each list consists ofapproximately 150 terms.The semantic spaces are induced from two types of cor-pora  essentially belonging to different genres, but bothwithin the wider domain of medicine: (1) a clinical corpus,comprising notes from health records, and (2) a medicalcorpus, comprising medical journal articles.The clinical corpus contains a subset of the StockholmEPR Corpus [50], which encompasses health records fromthe Karolinska University Hospital in Stockholm, Swedenover a five-year periodl. The clinical corpus used in thisstudy is created by extracting the free-text, narrativeparts of the health records from a wide range of clini-cal practices. The clinical notes are written in Swedishby physicians, nurses and other health care professionalsover a six-month period in 2008. In summary, the cor-pus comprises documents that each contain clinical notesdocumenting a single patient visit at a particular clinicalunit.The medical corpus contains the freely available subsetof Läkartidningen (19962005), which is the Journal of theSwedish Medical Association [51]. It is a weekly journalwritten in Swedish and contains articles discussing newscientific findings in medicine, pharmaceutical studies,health economic evaluations, etc. Although these issueshave been made available for research, the original orderof the sentences has not been retained due to copy-right reasons. The sentences thus appear in a randomizedorder, which means that the original texts cannot berecreated.Both corpora are lemmatized using the Granska Tagger[52] and thereafter further preprocessed by removingpunctuation marks and digits. Two versions of each cor-pus are created: one version in which the stop words areretained and one version in which they are removedm. Asthe sentences in Läkartidningen are given in a randomorder, a document break is indicated between each sen-tence for this corpus. It is thereby ensured that contextinformation from surrounding sentences will not be incor-porated in the induced semantic space. Statistics for thetwo corpora are shown in Table 1.In summary, a total of thirty semantic spaces areinduced  ten from each corpus type, and ten from theconjoint corpus. Four RI spaces are induced from eachTable 1 Corpora statisticsCorpus With stop words Without stop words SegmentsClinical ?42.5M tokens ?22.5M tokens 268,727 documents(?0.4M types) (?0.4M types)Medical ?20.3M tokens ?12.1M tokens 1,153,824 sentences(?0.3M types) (?0.3M types)The number of tokens and unique terms (word types) in the medical and clinicalcorpus, with and without stop words.corpus type (12 in total), the difference being the contextdefinition employed (1 + 1, 2 + 2, 4 + 4, 10 + 10). Six RPspaces are induced from each corpus type (18 in total), thedifference being the context definition employed (1 + 1,2 + 2, 4 + 4) and whether stop words have been removedor retained (sw).Combinations of semantic spaces from a single corpusSince RI and RP model semantic relations between termsin slightly different ways, it may prove profitable to com-bine them in order to increase the likelihood of capturingsynonymy and identifying abbreviation-expansion pairs.In one study it was estimated that the overlap in the out-put produced by RI and RP spaces is, on average, onlyaround 33% [46]: by combining them, we hope to cap-ture different semantic properties of terms and, ultimately,boost results. The combinations from a single corpus typeinvolve only two semantic spaces: one constructed with RIand one constructed with RP. In this study, the combina-tions involve semantic spaces with identical window sizes,with the following exception: RI spaces with a wide con-text definition (10 + 10) are combined with RP spaces witha narrow context definition (1 + 1, 2 + 2). The RI spacesare combined with RP spaces both with and without stopwords.Three different strategies of combing an RI-basedsemantic space with an RP space are designed and evalu-ated. Thirty combinations are evaluated for each corpus,i.e. sixty in total (Table 2). The three combination strate-gies are: RI ? RP30Finds the top ten terms in the RI space that areamong the top thirty terms in the RP space. RP ? RI30Finds the top ten terms in the RP space that areamong the top thirty terms in the RI space. RI + RPSums the cosine similarity scores from the two spacesfor each candidate term.For the first two strategies (RI ? RP30 and RP ? RI30)a two-stage approach is applied. First one type of modelis used (RI or RP) to produce an initial ranking of wordsaccording to a given query. The other model type, trainedon the same corpus, is then used to re-rank the top30 words produced by the first model according to itsinternal ranking. The intuition behind this approach isto see if synonyms and abbreviation-expansion pairs canbe detected by trying to ensure that the set of contex-tually related words also have similar grammatical prop-erties, and vice versa. In the third strategy (RI + RP),we apply a straightforward summing of the generatedsimilarity scores.Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 8 of 25http://www.jbiomedsem.com/content/5/1/6Table 2 Overview of experiments conducted with a single semantic spaceFor each of the 2 corpora, 10 semantic spaces were induced.RI spaces RI_20 RI_2 RI_4 RI_8RP spaces RP_2 RP_2_sw RP_4 RP_4_sw RP_8 RP_8_swThe induced semantic spaces were combined in 10 different combinations.CombinationsIdentical window size RI_2, RP_2 RI_4, RP_4 RI_8, RP_8Identical window size, stop words RI_2, RP_2_sw RI_4, RP_4_sw RI_8, RP_8_swLarge window size RI_20, RP_2 RI_20, RP_4Large window size, stop words RI_20, RP_2_sw RI_20, RP_4_swFor each combination, 3 combination strategies were evaluated.Combination strategies RI ? RP30 RP ? RI30 RI + RPFor each of the two corpora and the conjoint corpus, 30 different combinations were evaluated. The configurations are described according to the following pattern:model_windowSize. For RP, swmeans that stop words are retained in the semantic space. For instance,model_20means a window size of 10+10 was used.Combinations of semantic spaces frommultiple corporaIn addition to combining semantic spaces induced fromone and the same corpus, a combination of semanticspaces induced from multiple corpora could potentiallyyield even better performance on the task of extractingsynonyms and abbreviation-expansion pairs, especially ifthe terms of interest occur with someminimum frequencyin both corpora. Such ensembles of semantic spaces in this study consisting of four semantic spaces  allownot only different model types and model parameter con-figurations to be employed, but also allow us to capturelanguage use in different genres or domains, in whichterms may be used in slightly different contexts. The pairof semantic spaces from each corpus that is best able toperform each of the aforementioned tasks  consisting oftwo semantic spaces  is subsequently combined usingvarious combination strategies.The combination strategies can usefully be divided intotwo sets of approaches: in the first, the four seman-tic spaces are treated equally  irrespective of source and combined in a single step; in the other, a two-step approach is assumed, wherein each pair of semanticspaces  induced from the same source  is combinedseparately before the combination of combinations is per-formed. In both sets of approaches, the outputs of thesemantic spaces are combined in one of two ways: SUM,where the cosine similarity scores are merely summed,and AVG, where the average cosine similarity score iscalculated based on the number of semantic spaces inwhich the term under consideration exists. The latter isan attempt to mitigate the effect of differences in vocabu-lary between the two corpora. In the two-step approaches,the SUM/AVG option is configurable for each step. Inthe single-step approaches, the combinations can be per-formed either with or without normalization, which inthis case means replacing the exact cosine similarityscores of the candidate terms in the output of each queriedsemantic space with their ranking in the list of candi-date terms. This means that the candidate terms are nowsorted in ascending order, with zero being the highestscore. When combining two or more lists of candidateterms, the combined list is also sorted in ascending order.The rationale behind this option is that the cosine sim-ilarity scores are relative and thus only valid within agiven semantic space: combining similarity scores fromsemantic spaces constructed with different model typesand parameter configurations, and induced from differ-ent corpora, might have adverse effects. In the two-stepapproach, normalization is always performed after com-bining each pair of semantic spaces. In total, eight combi-nation strategies are evaluated:Single-step approaches SUM: RIclinical + RPclinical + RImedical + RPmedicalEach candidate terms cosine similarity score in eachsemantic space is summed. The top ten terms fromthis list are returned. SUM, normalized: norm(RIclinical) + norm(RPclinical) + norm(RImedical) + norm(RPmedical)The output of each semantic space is first normalizedby using the ranking instead of cosine similarity; eachcandidate terms (reverse) ranking in each semanticspace is then summed. The top ten terms from thislist are returned. AVG: RIclinical + RPclinical + RImedical + RPmedicalcounttermEach candidate terms cosine similarity score in eachsemantic space is summed; this value is then averagedover the number of semantic spaces in which the termexists. The top ten terms from this list are returned. AVG, normalized:norm(RIclinical)+ norm(RPclinical)+ norm(RImedical)+ norm(RPmedical)counttermThe output of each semantic space is first normalizedby using the ranking instead of cosine similarity; eachHenriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 9 of 25http://www.jbiomedsem.com/content/5/1/6candidate terms normalized score in each semanticspace is then summed; this value is finally averagedover the number of semantic spaces in which the termexists. The top ten terms from this list are returned.Two-step approaches SUM?SUM: norm(RIclinical + RPclinical) +norm(RImedical + RPmedical)Each candidate terms cosine similarity score in eachpair of semantic spaces is first summed; these arethen normalized by using the ranking instead of thecosine similarity; finally, each candidate termsnormalized score is summed. The top ten terms fromthis list are returned. AVG?AVG:norm(RIclinical + RPclinicalcountterm?source?a)+ norm(RImedical + RPmedicalcountterm?source?b)countterm?source?a + countterm?source?bEach candidate terms cosine similarity score for eachpair of semantic spaces is first summed; for each pairof semantic spaces, this value is then averaged overthe number of semantic spaces in that pair in whichthe term exists; these are subsequently normalized byusing the ranking instead of the cosine similarity;each candidate terms normalized score in eachcombined list is then summed and averaged over thenumber of semantic spaces in which the term exists(in both pairs of semantic spaces). The top ten termsfrom this list are returned. SUM?AVG:norm(RIclinical +RPclinical)+ norm(RImedical +RPmedical)counttermEach candidate terms cosine similarity score for eachpair of semantic spaces is first summed; these are thennormalized by using the ranking instead of the cosinesimilarity; each candidate terms normalized score ineach combined list is then summed and averaged overthe number of semantic spaces in which the termexists. The top ten terms from this list are returned. AVG?SUM: norm(RIclinical + RPclinicalcountterm)+norm(RImedical + RPmedicalcountterm)Each candidate terms cosine similarity score for eachpair of semantic spaces is first summed and averagedover the number of semantic spaces in that pair inwhich the term exists; these are then normalized byusing the ranking instead of the cosine similarity;each candidate terms normalized score in eachcombined list is finally summed. The top ten termsfrom this list are returned.Post-processing of candidate termsIn addition to creating ensembles of semantic spaces, sim-ple filtering rules are designed and evaluated for theirability to enhance performance further on the task ofextracting synonyms and abbreviation-expansion pairs.For obvious reasons, this is easier for abbreviation-expansion pairs than for synonyms.With regards to abbreviation-expansion pairs, the focusis on increasing precision by discarding poor suggestionsin favor of potentially better ones. This is attempted byexploiting properties of the abbreviations and their cor-responding expansions. The development subset of thereference standard (see Evaluation framework) is used toconstruct rules that determine the validity of candidateterms. For an abbreviation-expansion pair to be consid-ered valid, each letter in the abbreviation has to be presentin the expansion and the letters also have to appear in thesame order. Additionally, the length of abbreviations andexpansions is restricted, requiring an expansion to con-tain more than four letters, whereas an abbreviation isallowed to contain a maximum of four letters. These rulesare shown in Eq. 3 and Eq. 4.For synonym extraction, cut-off values for rank andcosine similarity are instead employed. These cut-off val-ues are tuned to maximize precision for the best semanticspace combinations in the development subset of the ref-erence standard, without negatively affecting recall (seeFigures 2, 3 and 4). Used cut-off values are shown in Eq. 5for the clinical corpus, in Eq. 6 for the medical corpus, andin Eq. 7 for the combination of the two corpora. In Eq. 7,Cos denotes the combination of the cosine values, whichmeans that it has a maximum value of four rather thanone.Exp ? Abbr ={ True, if (Len < 5) ? (Subout = True)False, Otherwise(3)Abbr ? Exp ={True, if (Len > 4) ? (Subin = True)False, Otherwise(4)Synclinical={True, if (Cos?0.60)?(Cos?0.40?Rank<9)False, Otherwise(5)Synmedical ={True, if (Cos ? 0.50)False, Otherwise (6)Synclinical+medical ={True, if (Cos ? 1.9) ? (Cos ? 1.8 ? Rank < 6) ? (Cos ? 1.75 ? Rank < 3)False, Otherwise (7)Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 10 of 25http://www.jbiomedsem.com/content/5/1/6Figure 2 Distribution of candidate terms for the clinical corpus. The distribution (cosine similarity and rank) of candidates for synonyms for thebest combination of semantic spaces induced from the clinical corpus. The results show the distribution for query terms in the developmentreference standard.Figure 3 Distribution of candidate terms for the medical corpus. The distribution (cosine similarity and rank) of candidates for synonyms forthe best combination of semantic spaces induced from the medical corpus. The results show the distribution for query terms in the developmentreference standard.Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 11 of 25http://www.jbiomedsem.com/content/5/1/6Figure 4 Distribution of candidate terms for clinical + medical corpora. The distribution (combined cosine similarity and rank) of candidatesfor synonyms for the ensemble of semantic spaces induced from medical and clinical corpora. The results show the distribution for query terms inthe development reference standard.Cos: Cosine similarity between candidate term andquery term.Rank: The ranking of the candidate term, ordered bycosine similarity.Subout: Whether each letter in the candidate term ispresent in the query term, in the same orderand with identical initial letters.Subin: Whether each letter in the query term ispresent in the candidate term, in the sameorder and with identical initial letters.Len: The length of the candidate term.The post-processing filtering rules are employed in twodifferent ways. In the first approach, the semantic spacesare forced to suggest a predefined number of candidateterms (ten), irrespective of how good they are deemed tobe by the semantic space. Candidate terms are retrieved bythe semantic space until ten have been classified as correctaccording to the post-processing rules, or until one hun-dred candidate terms have been classified. If less than tenare classified as incorrect, the highest ranked discardedterms are used to populate the remaining slots in thefinal list of candidate terms. In the second approach, thesemantic spaces are allowed to suggest a dynamic num-ber of candidate terms, with a minimum of one and amaximum of ten. If none of the highest ranked terms areclassified as correct, the highest ranked term is suggested.Evaluation frameworkEvaluation of the numerous experiments is carried outwith the use of reference standards: one contains knownabbreviation-expansion pairs and the other containsknown synonyms. The semantic spaces and their var-ious combinations are evaluated for their ability toextract known abbreviations/expansions (abbr?exp andexp?abbr) and synonyms (syn)  according to theemployed reference standard  for a given query termin a list of ten candidate terms (recall top 10). Recallis prioritized in this study and any decisions, such asdeciding which model parameters or which combina-tion strategies are the most profitable, are solely basedon this measure. When precision is reported, it is cal-culated as weighted precision, where the weights areassigned according to the ranking of a correctly identifiedterm.The reference standard for abbreviations is taken fromCederblom [53], which is a book that contains lists ofmedical abbreviations and their corresponding expan-sions. These abbreviations have been manually collectedfrom Swedish health records, newspapers, scientific arti-cles, etc. For the synonym extraction task, the referencestandard is derived from the freely available part of theSwedish version of MeSH [54]  a part of UMLS  aswell as a Swedish extension that is not included in UMLS[55]. As the semantic spaces are constructed only tomodelunigrams, all multiword expressions are removed fromthe reference standards. Moreover, hypernym/hyponymand other non-synonym pairs found in the UMLS ver-sion of MeSH are manually removed from the referencestandard for the synonym extraction task. Models of dis-tributional semantics sometimes struggle to model theHenriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 12 of 25http://www.jbiomedsem.com/content/5/1/6meaning of rare terms accurately, as the statistical basisfor their representation is insufficiently solid. As a result,we only include term pairs that occur at least fifty timesin each respective corpus. This, together with the factthat term frequencies differ from corpus to corpus, meansthat one separate reference standard is used for the eval-uation of the clinical corpus and another is used for theevaluation of the medical corpus. For evaluating combina-tions of semantic spaces induced from different corpora,a third  common  reference standard is therefore cre-ated, in which only term pairs that occur at least fiftytimes in both corpora are included. Included terms arenot restricted to form pairs; in the reference standard forthe synonym extraction task, some form larger groups ofterms with synonymous relations. There are also abbrevi-ations with several possible expansions, as well as expan-sions with several possible abbreviations. The term pairs(or n-tuples) in each reference standard are randomly splitinto a development set and an evaluation set of roughlyequal size. The development sets are used for identi-fying the most profitable ensembles of semantic spaces(with optimized parameter settings, such as window sizeand whether to include stop words in the RP spaces) foreach of the three tasks, as well as for creating the post-processing filtering rules. The evaluation sets are used forthe final evaluation to assess the expected performance ofthe ensembles in a deployment setting. Baselines for thesingle-corpus ensembles are created by employing RI andRP in isolation; baselines for the multiple-corpora ensem-bles are created by using the most profitable clinical andmedical ensembles from the single-corpus experiments,as well a single space induced from the conjoint cor-pus and an ensemble of semantic spaces induced fromthe conjoint corpus. Statistics for the reference standardsare shown in Table 3. The differences in recall betweenthe different semantic spaces/ensembles, when evaluatedon the evaluation subset of the reference standards, aretested for statistical significance. The exact binomial signtest is used ([56], pp. 532535), assuming independencebetween all query terms.In addition to the automatic evaluation using the ref-erence standards, a small manual evaluation is also car-ried out on the synonym task. A random sample of 30query terms (out of 135 terms in the Clinical + Medi-cal reference standard) and their respective ten candidateterms as suggested by the best combination of seman-tic spaces is investigated and a manual classification ofthe semantic relation between each of the candidateterms and the target term is carried out. The candi-date terms are manually classified as either a synonym,an antonymn, a hypernymo, a hyponym or an alterna-tive spelling (for instance rinitis/rhinitis) of the targetterm.ResultsThe experimental setup was designed in such a mannerthat the semantic spaces that performed best in com-bination for a single corpus would also be used in thesubsequent combinations from multiple corpora. Identi-fying the most profitable combination strategy for eachof the three tasks was achieved using the developmentsubsets of the reference standards. These combinationswere then evaluated on separate evaluation sets con-taining unseen data. All further experiments, includingthe post-processing of candidate terms, were carried outwith these combinations on the evaluation sets. Thisis therefore also the order in which the results will bepresented.Combination strategies: a single corpusThe first step involved identifying the most appropriatewindow sizes for each task, in conjunction with evalu-ating the combination strategies. The reason for this isthat the optimal window sizes for RI and RP in isolationare not necessarily identical to the optimal window sizeswhen RI and RP are combined. In fact, when RI is usedin isolation, a window size of 2 + 2 performs best on thetwo abbreviation-expansion tasks, and a window size of10 + 10 performs best on the synonym task. For RP, asemantic space with a window size of 2 + 2 yields theTable 3 Reference standards statisticsReference standardClinical corpus Medical corpus Clinical + MedicalSize 2 Cor 3 Cor Size 2 Cor 3 Cor Size 2 Cor 3 CorAbbr?Exp (Devel) 117 9.4% 0.0% 55 13% 1.8% 42 14% 0%Abbr?Exp (Eval) 98 3.1% 0.0% 55 11% 0% 35 2.9% 0%Exp?Abbr (Devel) 110 8.2% 1.8% 63 4.7% 0% 45 6.7% 0%Exp?Abbr (Eval) 98 7.1% 0.0% 61 0% 0% 36 0% 0%Syn (Devel) 334 9.0% 1.2% 266 11% 3.0% 122 4.9% 0%Syn (Eval) 340 14% 2.4% 263 13% 3.8% 135 11% 0%Size shows the number of queries, 2 cor shows the proportion of queries with two correct answers and 3 cor the proportion of queries with three (or more) correctanswers. The remaining queries have one correct answer.Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 13 of 25http://www.jbiomedsem.com/content/5/1/6best results on two of the tasks  abbr?exp and syn while a window size of 4 + 4 is more successful onthe exp?abbr task. These are the model configurationsused in the RI and RP baselines, to which the single-corpus combination strategies are compared in the finalevaluation.Using the semantic spaces induced from the clinical cor-pus, the RI+RP combination strategy, wherein the cosinesimilarity scores are merely summed, is the most success-ful on all three tasks: 0.42 recall on the abbr?exp task,0.32 recall on the exp?abbr task, and 0.40 recall on thesyn task (Table 4). For the abbreviation expansion task, awindow size of 2 + 2 appears to work well for both mod-els, with the RP space retaining stop words. On the task ofidentifying the abbreviated form of an expansion, seman-tic spaces with window sizes of 2 + 2 and 4 + 4 performequally well; the RP spaces should include stop words.Finally, on the synonym extraction task, an RI space witha large context window (10 + 10) in conjunction with anRP space with stop words and a window size of 2 + 2 is themost profitable.Using the semantic spaces induced from the med-ical corpus, again, the RI + RP combination strategyoutperforms the RI ? RP30 and RP ? RI30 strategies:0.10 recall on the abbr?exp task, 0.08 recall on theexp?abbr task, and 0.30 recall on the syn task (Table 5)are obtained. This combination outperforms the othertwo by a large margin on the exp?abbr task: 0.08 recallcompared to 0.03 recall. The most appropriate windowsizes for capturing these phenomena in the medicalcorpus are fairly similar to those that worked best withthe clinical corpus. On the abbr?exp task, the opti-mal window sizes are indeed identical across the twocorpora: a 2 + 2 context window with an RP space thatincorporates stop words yields the highest performance.For the exp?abbr task, a slightly larger context windowof 4 + 4 seems to work well  again, with stop wordsretained in the RP space. Alternatively, combining a largeRI space (10 + 10) with a smaller RP space (2 + 2, withstop words) performs comparably on this task and withthis test data. Finally, for synonyms, a large RI space(10 + 10) with a very small RP space (1 + 1) that retainsall words best captures this phenomenon with this type ofcorpus.Using the semantic spaces induced from the conjointcorpus, the RI ? RP30 combination strategy outperformsthe other two strategies on the abbr?exp task: 0.30 recallcompared to 0.25 and 0.23 (Table 6). On the exp?abbrtask, this and the RI + RP combination strategy performequally well, with 0.18 recall. Finally, on the synonym task,the RI +RP performs best with a recall of 0.46. In general,somewhat larger window sizes seem to work better whencombining semantic spaces induced from the conjointcorpus.The best-performing combinations from each corpusand for each task were then treated as (ensemble) base-lines in the final evaluation, where combinations ofsemantic spaces from multiple corpora are evaluated.Combination strategies: multiple corporaThe pair of semantic spaces from each corpus thatperformed best on the three tasks were subsequentlyemployed in combinations that involved four semanticspaces  two from each corpus: one RI space and oneRP space. The single-step approaches generally performedbetter than the two-step approaches, with some excep-tions (Table 7). The most successful ensemble was asimple single-step approach, where the cosine similar-ity scores produced by each semantic space were simplysummed (SUM), yielding 0.32 recall for abbr?exp, 0.17recall for exp?abbr, and 0.52 recall for syn. The AVGoption, although the second-highest performer on theabbreviation-expansion tasks, yielded significantly poorerresults. Normalization, whereby ranking was used insteadof cosine similarity, invariably affected performance neg-atively, especially when employed in conjunction withSUM. The two-step approaches performed significantlyworse than all non-normalized single-step approaches,with the sole exception taking place on the synonymextraction task. It should be noted that normalization wasTable 4 Results on clinical development setStrategyAbbr?Exp Exp?Abbr SynRI RP Result RI RP Result RI RP ResultRI ? RP30 RI_8 RP_8_sw 0.38 RI_8 RP_8 0.30 RI_8 RP_8 0.39RP ? RI30 RI_20 RP_4_sw 0.35RI_4 RP_4_sw0.30RI_8 RP_80.38RI_20 RP_4_sw RI_8 RP_8_swRI_20 RP_2_swRI + RP RI_4 RP_4_sw 0.42 RI_4 RP_4_sw 0.32 RI_20 RP_4_sw 0.40RI_8 RP_8_swResults (recall, top ten) of the best configurations for each model and model combination on the three tasks. The configurations are described according to thefollowing pattern:model_windowSize. For RP, swmeans that stop words are retained in the model.Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 14 of 25http://www.jbiomedsem.com/content/5/1/6Table 5 Results onmedical development setStrategyAbbr?Exp Exp?Abbr SynRI RP Result RI RP Result RI RP ResultRI ? RP30RI_4 RP_4_sw0.08RI_2 RP_20.03 RI_20 RP_4_sw 0.26RI_20 RP_2 RI_4 RP_4RI_20 RP_4_sw RI_4 RP_4_swRI_8 RP_8RI_20 RP_2RI_20 RP_2_swRI_20 RP_4RI_20 RP_4_swRP ? RI30RI_2 RP_2_sw0.08RI_2 RP_20.03 RI_8 RP_8_sw 0.24RI_4 RP_4 RI_2 RP_2_swRI_4 RP_4_sw RI_4 RP_4RI_8 RP_8 RI_4 RP_4_swRI_8 RP_8_sw RI_8 RP_8RI_20 RP_2_sw RI_8 RP_8_swRI_20 RP_4 RI_20 RP_2RI_20 RP_4_sw RI_20 RP_2_swRI_20 RP_4RI_20 RP_4_swRI + RP RI_4 RP_4_sw 0.10 RI_8 RP_8_sw 0.08 RI_20 RP_2_sw 0.30RI_20 RP_4_swResults (recall, top ten) of the best configurations for each model and model combination on the three tasks. The configurations are described according to thefollowing pattern:model_windowSize. For RP, swmeans that stop words are retained in the model.always performed in the two-step approaches  this wasdone after each pair of semantic spaces from a single cor-pus had been combined. Of the four two-step combina-tion strategies, AVG?AVG and AVG?SUM performedbest, with identical recall scores on the three tasks.Final evaluationsThe combination strategies that performed best on thedevelopment sets were finally evaluated on completelyunseen data in order to assess their generalizability tonew data and to assess their expected performance in aTable 6 Conjoined corpus space results on clinical + medical development setStrategyAbbr?Exp Exp?Abbr SynRI RP Result RI RP Result RI RP ResultRI ? RP30 RI_4 RP_4_sw 0.30 RI_4 RP_4_sw 0.18 RI_8 RP_8_sw 0.41RI_20 RP_4_swRP ? RI30RI_4 RP_40.23RI_4 RP_4_sw0.13RI_8 RP_80.36RI_4 RP_4_sw RI_8 RP_8_sw RI_8 RP_8_swRI_8 RP_8 RI_20 RP_2_sw RI_20 RP_2_swRI_20 RP_2 RI_20 RP_4_sw RI_20 RP_4_swRI_20 RP_4RI + RP RI_2 RP_2_sw 0.25RI_4 RP_4_sw0.18 RI_8 RP_8_sw 0.46RI_8 RP_8_swRI_20 RP_4_swResults (recall, top ten) of the best configurations for each model and model combination on the three tasks. The configurations are described according to thefollowing pattern:model_windowSize. For RP, swmeans that stop words are retained in the model.Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 15 of 25http://www.jbiomedsem.com/content/5/1/6Table 7 Disjoint corpus ensemble results on clinical + medical development setStrategy NormalizeAbbr?Exp Exp?Abbr SynClinical Medical Clinical Medical Clinical MedicalRI_4 RI_4 RI_4 RI_8 RI_20 RI_20RP_4_sw RP_4_sw RP_4_sw RP_8_sw RP_4_sw RP_2_swAVG True 0.13 0.09 0.39AVG False 0.24 0.11 0.39SUM True 0.13 0.09 0.34SUM False 0.32 0.17 0.52AVG?AVG 0.15 0.09 0.41SUM?SUM 0.13 0.07 0.40AVG?SUM 0.15 0.09 0.41SUM?AVG 0.13 0.07 0.40Results (P = weighted precision, R = recall, top ten) of the best models with and without post-processing on the three tasks. Dynamic # of suggestions allows themodel to suggest less than ten terms in order to improve precision. The results are based on the application of the model combinations to the development data.deployment setting. Each evaluation phase involves com-paring the results to one or more baselines: in the caseof single-corpus combinations, the comparisons are madeto RI and RP in isolation; in the case of multiple-corporacombinations, the comparisons are made to semanticspaces induced from a single corpus (as well as the con-joint corpus) and ensembles of semantic spaces inducedfrom a single corpus (and, again, the conjoint corpus).When applying the single-corpus combinations fromthe clinical corpus, the following results were obtained:0.31 recall on abbr?exp, 0.20 recall on exp?abbr, and0.44 recall on syn (Table 8). Compared to the results onthe development sets, the results on the two abbreviation-expansion tasks decreased by approximately ten per-centage points; on the synonym extraction task, theperformance increased by a couple of percentage points.The RI baseline was outperformed on all three tasks;the RP baseline was outperformed on two out of threetasks, with the exception of the exp?abbr task. Finally,it might be interesting to point out that the RP base-line performed better than the RI baseline on the twoabbreviation-expansion tasks, but that the RI baseline didsomewhat better on the synonym extraction task.With the medical corpus, the following results wereobtained: 0.17 recall on abbr?exp, 0.11 recall onexp?abbr, and 0.34 recall on syn (Table 9). Comparedto the results on the development sets, the results werehigher for all three tasks. Both the RI and RP baselineswere outperformed, with a considerable margin, by theircombination. However, the improvement in recall for thecombination method compared to the best baseline wasonly statistically significant for the synonym task. In com-plete contrast to the clinical corpus, the RI baseline hereoutperformed the RP baseline on the two abbreviation-expansion tasks, but was outperformed by the RP baselineon the synonym extraction task.When applying the disjoint corpora ensembles, the fol-lowing results were obtained on the evaluation sets: 0.30Table 8 Results on clinical evaluation setEvaluation configurationAbbr?Exp Exp?Abbr SynRI_4+RP_4_sw RI_4+RP_4_sw RI_20+RP_4_swP R P R P RRI Baseline 0.04 0.22 0.03 0.19 0.07 0.39RP Baseline 0.04 0.23 0.04 0.24 0.06 0.36Clinical Ensemble 0.05 0.31 0.03 0.20 0.07 0.44+Post-Processing (Top 10) 0.08 0.42 0.05 0.33 0.08 0.43+Dynamic Cut-Off (Top ? 10) 0.11 0.41 0.12 0.33 0.08 0.42Results (P = weighted precision, R = recall, top ten) of the best models with and without post-processing on the three tasks. Dynamic # of suggestions allows themodel to suggest less than ten terms in order to improve precision. The results are based on the application of the model combinations to the evaluation data. Theimprovements in recall between the best baseline and the ensemble method for the synonym task and for the abbr?exp task are both statistically significant for ap-value < 0.05. (abbr?exp task: p-value = 0.022 and synonym task: p-value = 0.002.) The improvement in recall that was achieved by post-processing is statisticallysignificant for both abbreviation tasks (p-value = 0.001 for abbr?exp and p-value = 0.000 for exp?abbr).Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 16 of 25http://www.jbiomedsem.com/content/5/1/6Table 9 Results onmedical evaluation setEvaluation configurationAbbr?Exp Exp?Abbr SynRI_4+RP_4_sw RI_8+RP_8_sw RI_20+RP_2_swP R P R P RRI baseline 0.02 0.09 0.01 0.08 0.03 0.18RP baseline 0.01 0.06 0.01 0.05 0.05 0.26Medical ensemble 0.03 0.17 0.01 0.11 0.06 0.34+Post-processing (top 10) 0.03 0.17 0.02 0.11 0.06 0.34+Dynamic cut-off (top ? 10) 0.17 0.17 0.10 0.11 0.06 0.34Results (P = weighted precision, R = recall, top ten) of the best semantic spaces with and without post-processing on the three tasks. Dynamic # of suggestions allowsthe model to suggest less than ten terms in order to improve precision. The results are based on the application of the model combinations to the evaluation data.The difference in recall when using the ensemble method compared to the best baseline is only statistically significant (p-value < 0.05) for the synonym task (p-value =0.000).recall on abbr?exp, 0.19 recall on exp?abbr, and 0.47recall on syn (Table 10). Compared to the results onthe development sets, the results decreased somewhat ontwo of the tasks, with exp?abbr the exception. The p-values for the significance tests of the recall differences inTable 10 are shown in Table 11. The two ensemble base-lines were clearly outperformed by the larger ensemble ofsemantic spaces from two types of corpora on two of thetasks; the clinical ensemble baseline performed equallywell on the exp?abbr task.Post-processingIn an attempt to further improve results, simple post-processing of the candidate terms was performed. In onesetting, the system was forced to suggest ten candidateterms regardless of their cosine similarity score or otherproperties of the terms, such as their length. In anothersetting, the system had the option of suggesting a dynamicnumber  ten or less  of candidate terms.This was unsurprisingly more effective on the twoabbreviation-expansion tasks. With the clinical corpus,recall improved substantially with the post-processing fil-tering: from 0.31 to 0.42 on abbr?exp and from 0.20to 0.33 on exp?abbr (Table 8). With the medical cor-pus, however, almost no improvements were observed forthese tasks (Table 9). For the combination of semanticspaces from the two corpora, the improvements in recallafter applying post-processing on the two abbreviationtasks are not statistically significant (Table 10).With a dynamic cut-off, only precision could beimproved, although at the risk of negatively affect-ing recall. With the clinical corpus, recall was largelyTable 10 Results on clinical + medical evaluation setEvaluation configurationAbbr?Exp Exp?Abbr SynClinical Medical Clinical Medical Clinical MedicalRI_4 RI_4 RI_4 RI_8 RI_20 RI_20RP_4_sw RP_4_sw RP_4_sw RP_8_sw RP_4_sw RP_2_swSUM, False SUM, False SUM, FalseP R P R P RClinical space 0.03 0.17 0.03 0.19 0.05 0.29Medical space 0.01 0.06 0.01 0.08 0.03 0.18Conjoint corpus space 0.03 0.19 0.01 0.08 0.05 0.30Clinical ensemble 0.04 0.24 0.03 0.19 0.06 0.34Medical ensemble 0.02 0.11 0.01 0.11 0.05 0.33Conjoint corpus ensemble 0.03 0.19 0.02 0.14 0.07 0.40Disjoint corpora ensemble 0.05 0.30 0.03 0.19 0.08 0.47+Post-processing (top 10) 0.07 0.39 0.06 0.33 0.08 0.47+Dynamic cut-off (top ? 10) 0.28 0.39 0.31 0.33 0.08 0.45Results (P = weighted precision, R = recall, top ten) of the best semantic spaces and ensembles on the three tasks. The results are based on the clinical + medicalevaluation set and are grouped according to the number of semantic spaces employed: one, two or four. The disjoint corpus ensemble is performed with and withoutpost-processing. A dynamic cut-off allows less than ten terms to be suggested in an attempt to improve precision. Results for tests of statistical significance are shownin Table 11.Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 17 of 25http://www.jbiomedsem.com/content/5/1/6Table 11 P-values for recall results presented in Table 10P-values, recall Medical Conjoint Clinical Medical Conjoint Disjoint(synonym task) space corpus ensemble ensemble corp. ens. corp. ens.Clinical space 0.011 1.000 0.057 0.885 0.003 0.000Medical space - 0.004 0.000 0.000 0.000 0.000Conjoint corpus - - 0.210 1.000 0.001 0.000Clinical ensemble - - - 0.480 0.189 0.001Medical ensemble - - - - 0.047 0.000Conjoint corp. ens. - - - - - 0.041P-values for the differences between the recall results on the synonym task for the semantic spaces/ensembles presented in Table 10. P-values showing a statisticallysignificant difference (p-value < 0.05) are presented in bold-face.P-values for the post-processing and for the abbr?exp and exp?abbr are not shown in the table. However, for the significance level p-value < 0.05, there were nostatistically significant recall difference between the standard Disjoint Corpora Ensemble and the post-processing version for any of the three tasks (p-value = 0.25 forabbr?exp and p-value = 0.062 for exp?abbr). When testing the recall difference between the pairs of semantic spaces/ensembles shown in Table 10 for theabbr?exp task, there was only a significant difference for the pairs Medical Space vs. Clinical Ensemble (p-value = 0.039), Medical Space vs. Disjoint Corpora Ensemble(p-value = 0.004) and Medical Ensemble vs. Disjoint Corpora Ensemble (p-value = 0.039). For the exp?abbr task, there were no statistically significant differences.unaffected for the two abbreviation-expansion task, whileprecision improved by 37 percentage points (Table 8).With the medical corpus, the gains were even more sub-stantial: from 0.03 to 0.17 precision on abbr?exp andfrom 0.02 to 0.10 precision on exp?abbr  without hav-ing any impact on recall (Table 9). The greatest improve-ments on these tasks were, however, observed with thecombination of semantic spaces from multiple corpora:precision increased from 0.07 to 0.28 on abbr?exp andfrom 0.06 to 0.31 on exp?abbr  again, without affectingrecall (Table 10).In the case of synonyms, this form of post-processingis more challenging, as there are no simple properties ofthe terms, such as their length, that can serve as indica-tions of their quality as candidate synonyms. Instead, onehas to rely on their use in different contexts and grammat-ical properties; as a result, cosine similarity and rankingof the candidate terms were exploited in an attempt toimprove the candidate synonyms. This approach was,however, clearly unsuccessful for both corpora and theircombination, with almost no impact on either precisionor recall. In a single instance  with the clinical corpus precision increased by one percentage point, albeit at theexpense of recall, which suffered a comparable decrease(Table 8). With the combination of semantic spaces fromtwo corpora, the dynamic cut-off option resulted in alower recall score, without improving precision (Table 10).Frequency thresholdsIn order to study the impact of different frequency thresh-olds  i.e., how often each pair of terms had to occur inthe corpora to be included in the reference standard  onthe task of extracting synonyms, the best ensemble sys-temwas applied to a range of evaluation sets with differentthresholds from 1 to 100 (Figure 5). With a low frequencythreshold, it is clear that a lower performance is obtained.For instance, if each synonym pair only needs to occurat least once in both corpora, a recall of 0.17 is obtained.As the threshold is increased, recall increases too - up toa frequency threshold of around 50, after which no per-formance boosts are observed. Already with a frequencythreshold of around 30, the results seem to level off. WithFigure 5 Frequency thresholds. The relation between recall and the required minimum frequency of occurrence for the reference standard termsin both corpora. The number of query terms for each threshold value is also shown.Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 18 of 25http://www.jbiomedsem.com/content/5/1/6frequency thresholds over 100, there is not enough data inthis case to produce any reliable results.DiscussionThe results clearly demonstrate that combinations ofsemantic spaces lead to improved results on the synonymextraction task. For the two abbreviation tasks, most of theobserved performance gains were not statistically signifi-cant. Combining random indexing and random permuta-tion allows slightly different aspects of lexical semantics tobe captured; by combining them, stronger semantic rela-tions between terms are extracted, thereby increasing theperformance on these tasks. Combining semantic spacesinduced from different corpora further improves perfor-mance. This demonstrates the potential of distributionalensemble methods, of which this  to the extent of ourknowledge  is the primary implementation of its kind,and it only scratches the surface. In this initial study, onlyfour semantic spaces were used; however, with increas-ing computational capabilities, there is nothing stoppinga much larger number of semantic spaces from beingcombined. These can capture various aspects of seman-tics  aspects which may be difficult, if not impossible,to incorporate into a single model  from a large varietyof observational data on language use, where the contextsmay be very different.Clinical vs. medical corporaWhen employing corpus-driven methods to support lex-ical resource development, one naturally needs to haveaccess to a corpus in the target domain that reflectsthe language use one wishes to model. Hence, one can-not, without due qualification, state that one corpus typeis better than another for the extraction of synonymsor abbreviation-expansion pairs. This is something thatneeds to be duly considered when comparing the resultsfor the semantic spaces on the clinical and medical cor-pora, respectively. Another issue concerns the size of eachcorpus: in fact, the size of themedical corpus is only half aslarge as the clinical corpus (Table 1). The reference stan-dards used in the respective experiments are, however, notidentical: each term pair had to occur at least fifty timesto be included  this will differ across corpora. To someextent this mitigates the effect of the total corpus size andmakes the comparison between the two corpora fairer;however, differences in reference standards also entail thatthe results presented in Tables 8 and 9 are not directlycomparable. Another difference between the two corporais that the clinical corpus contains more unique terms(word types) than the medical corpus, which might indi-cate that it consists of a larger number of concepts. It haspreviously been shown that it can be beneficial, indeedimportant, to employ a larger dimensionality when usingcorpora with a large vocabulary, as is typically the casein the clinical domain [57]; in this study a dimensional-ity of 1,000 was used to induce all semantic spaces. Theresults, on the contrary, seem to indicate that better per-formance is generally obtained with the semantic spacesinduced from the clinical corpus.An advantage of using non-sensitive corpora like themedical corpus employed in this study is that they are gen-erally more readily obtainable than sensitive clinical data.Perhaps such and similar sources can complement smallerclinical corpora and yet obtain similar or potentially evenbetter results.Combining semantic spacesCreating ensembles of semantic spaces has been shown tobe profitable, at least on the task of extracting synonymsand abbreviation-expansion pairs. In this study, the focushas been on combining the output of the semantic spaces.This is probably the most straightforward approach andit has several advantages. For one, the manner in whichthe semantic representations are created can largely beignored, which would potentially allow one to combinemodels that are very different in nature, as long as onecan retrieve a ranked list of semantically related termswith a measure of the strength of the relation. It alsomeans that one can readily combine semantic spaces thathave been induced with different parameter settings, forinstance with different context definitions and of differ-ent dimensionality. An alternative approach would per-haps be to combine semantic spaces on a vector level.Such an approach would be interesting to explore; how-ever, it would pose numerous challenges, not least incombining context vectors that have been constructed dif-ferently and potentially represent meaning in disparateways.Several combination strategies were designed and eval-uated. In both the single-corpus and multiple-corporaensembles, the most simple strategy performed best: theone whereby the cosine similarity scores are summed.There are potential problems with such a strategy, sincethe similarity scores are not absolute measures of seman-tic relatedness, but merely relative and only valid withina single semantic space. The cosine similarity scores will,for instance, differ depending on the distributional modelused and the size of the context window. An attemptwas made to deal with this by replacing the cosine sim-ilarity scores with ranking information, as a means tonormalize the output of each semantic space before comb-ing them. This approach, however, yielded much poorerresults. A possible explanation for this is that a measureof the semantic relatedness between terms is of muchmore importance than their ranking. After all, a list ofthe highest ranked terms does not necessarily imply thatthey are semantically similar to the query term; only thatthey are the most semantically similar in this space. ForHenriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 19 of 25http://www.jbiomedsem.com/content/5/1/6the multiple-corpora ensembles, the AVG strategy wasapplied with the aim of not penalizing candidate syn-onyms that only appear in one of the two corpora. It isnot surprising that this strategy was not successful giventhe form of the evaluation, which consisted of suggestingcandidate synonyms that were known to occur at least 50times in both corpora. The two-step approaches for themultiple-corpora ensembles all included a normalizingand/or averaging component, resulting in a lower recallcompared to the SUM strategy, probably for the same rea-sons as when these strategies were applied in the one-stepapproach.To gain deeper insights into the process of combiningthe output of multiple semantic spaces, an error analy-sis was conducted on the synonym extraction task. Thiswas achieved by comparing the outputs of the most prof-itable combination of semantic spaces from each corpus,as well as with the combination of semantic spaces fromthe two corpora. The error analysis was conducted on thedevelopment sets. Of the 68 synonyms that were correctlyidentified as such by the corpora combination, five werenot extracted by either of the single-corpus combinations;nine were extracted by the medical ensemble but not bythe clinical ensemble; as many as 51 were extracted by theclinical ensemble but not by its medical counterpart; inthe end, this means that only three terms were extractedby both the clinical and medical ensembles. These resultsaugment the case for multiple-corpora ensembles. Thereappears to be little overlap in the top-10 outputs ofthe corpora-specific ensembles; by combining them, 17additional true synonyms are extracted compared to theclinical ensemble alone. Moreover, the fact that so manysynonyms are extracted by the clinical ensemble demon-strates the importance of exploiting clinical corpora andthe applicability of distributional semantics to this genreof text. In Table 12, the first two examples, sjukhem(nursing-home) and depression show cases for which themultiple-corpora ensemble was successful but the single-corpus ensembles were not. In the third example, boththe multiple-corpora ensemble and the clinical ensembleextract the expected synonym candidate.There was one query term  the drug name omepra-zol  for which both single-corpus ensembles were ableto identify the synonym, but where the multiple-corporaensemble failed. There were also three query terms forwhich synonyms were identified by the clinical ensemble,but not by the multiple-corpora ensemble; there were fivequery terms that were identified by the medical ensem-ble, but not by the multiple-corpora ensemble. This showsthat combining semantic spaces can also, in some cases,introduce noise.Since synonym pairs were queried both ways, i.e. eachterm in the pair would be queried to see if the othercould be identified, we wanted to see if there werecases where the choice of query term would be impor-tant. Indeed, among the sixty query terms for which theexpected synonym was not extracted, this was the casein fourteen instances. For example, given the query termblindtarmsinflammation (appendix-inflammation), theexpected synonym appendicit (appendicitis) was given asa candidate, whereas with the query term appendicit, theexpected synonym was not successfully identified.Models of distributional semantics face the problem ofmodeling terms with several ambiguous meanings. Thisis, for instance, the case with the polysemous term arv(referring to inheritance as well as to heredity). Distantsynonyms also seem to be problematic, e.g. the pair reha-bilitation/habilitation. For approximately a third of thesynonym pairs that are not correctly identified, however,it is not evident that they belong to either of these twocategories.Post-processingIn an attempt to improve results further, an additionalstep in the proposed method was introduced: filteringof the candidate terms, with the possibility of extract-ing new, potentially better ones. For the extraction ofabbreviation-expansion pairs, this was fairly straightfor-ward, as there are certain patterns that generally applyto this phenomenon, such as the fact that the letters inan abbreviation are contained  in the same order in its expansion. Moreover, expansions are longer thanabbreviations. This allowed us to construct simple yeteffective rules for filtering out unlikely candidate termsfor these two tasks. As a result, both precision and recallincreased; with a dynamic cut-off, precision improvedsignificantly. Although our focus in this study was pri-marily on maximizing recall, there is a clear incentiveto improve precision as well. If this method were tobe used for terminological development support, withhumans inspecting the candidate terms, minimizing thenumber of poor candidate terms has a clear value. How-ever, given the seemingly easy task of filter out unlikelycandidates, it is perhaps more surprising that the resultswere not even better. A part of the reason for this maystem from the problem of semantically overloaded wordtypes, which affects abbreviations to a large degree, par-ticularly in the clinical domain with its telegraphic styleand where ad-hoc abbreviations abound. This was alsoreflected in the reference standard, as in some casesthe most common expansion of an abbreviation was notincluded.The post-processing filtering of synonyms clearly failed.Although ranking information and, especially, cosine sim-ilarity provide some indication of the quality of synonymcandidates, employing cut-off values with these featurescan impossibly improve recall: new candidates will alwayshave a lower ranking and a lower cosine similarity scoreHenriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 20 of 25http://www.jbiomedsem.com/content/5/1/6Table 12 Examples of extracted candidate synonymsQuery term: sjukhem (nursing-home)Clinical Medical Clinical + MedicalHeartcenter (heart-center) Vårdcentral (health-center) Vårdcentral (health-center)Bröstklinik (breast-clinic) Akutmottagning (emergency room) Mottagning (reception)Hälsomottagningen (health-clinic) Akuten (ER) Vårdhem (nursing-home)Hjärtcenter (heart-center) Mottagning (reception) Gotland (a Swedish county)Län (county) Intensivvårdsavdelning (ICU) Sjukhus (hospital)Eyecenter (eye-center) Arbetsplats (work-place) Gård (yard)Bröstklin (breast-clin.) Vårdavdelning (ward) Vårdavdelning (ward)Sjukhems (nursing-homes) Gotland (a Swedish county) Arbetsplats (work-place)Hartcenter (hart-center) Kväll (evening) Akutmottagning (emergency room)Biobankscentrum (biobank-center) Ks (Karolinska hospital) Akuten (ER)Query term: depression (depression)Clinical Medical Clinical + MedicalSömnstörning (insomnia) Depressioner (depressions) Sömnstörning ( insomnia)Sömnsvårigheter (insomnia) Osteoporos (osteoporosis) Osteoporos (osteoporosis)Panikångest (panic disorder) Astma (asthma) Tvångssyndrom (OCD)Tvångssyndrom (OCD) Fetma (obesity) Epilepsi (epilepsy)Fibromyalgi (fibromyalgia) Smärta (pain) Hjärtsvikt (heart failure)Ryggvärk (back-pain) Depressionssjukdom (depressive-illness) Nedstämdhet (sadness)Självskadebeteende (self-harm) Bensodiazepiner (benzodiazepines) Fibromyalgi (fibromyalgia)Osteoporos (osteoporosis) Hjärtsvikt (heart-failure) Astma (asthma)Depressivitet (depressitivity) Hypertoni (hypertension) Alkoholberoende ( alcoholism)Pneumoni (pneumonia) Utbrändhet (burnout) Migrän (migraine)Query term: allergi (allergy)Clinical Medical Clinical + MedicalPollenallergi (pollen-allergy) Allergier (allergies) Allergier (allergies)Födoämnesallergi (food-allergy) Sensibilisering (sensitization) Hösnuva (hay-fever)Hösnuva (hay-fever) Hösnuva (hay-fever) Födoämnesallergi ( food-allergy)Överkänslighet (hypersensitivity) Rehabilitering (rehabilitation) Pollenallergi (pollen-allergy)Kattallergi (cat-allergy) Fetma (obesity) Överkänslighet (hypersensitivity)Jordnötsallergi (peanut-allergy) Kol (COPD) Astma (asthma)Pälsdjursallergi (animal-allergy) Osteoporos (osteoporosis) Kol (COPD)Negeras (negated) Födoämnesallergi (food-allergy) Osteoporos ( osteoporosis)Pollen (pollen) Astma (asthma) Jordnötsallergi (peanut-allergy)Pollenallergiker (pollen-allergic) Utbrändhet (burnout) Pälsdjursallergi (animal-allergy)The top ten candidate synonyms for three different query terms with the clinical ensemble, the medical ensemble and the disjoint corpus ensemble. The synonym inthe reference standard is in boldface.than discarded candidate terms. It can, however  atleast in theory  potentially improve precision whenusing these rules in conjunction with a dynamic cut-off,i.e. allowing less than ten candidates terms to be sug-gested. In this case, however, the rules did not have thiseffect.ThresholdsIncreasing the frequency threshold further did notimprove results. In fact, a threshold of 30 occurrencesin both corpora seems to be sufficient. A high frequencythreshold is a limitation of distributional methods; thus,the ability to use a lower threshold is important, especiallyHenriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 21 of 25http://www.jbiomedsem.com/content/5/1/6in the clinical domain where access to data is difficult toobtain.The choice of evaluating recall among ten candidateswas based on an estimation of the number of candidateterms that would be reasonable to present to a lexicog-rapher for manual inspection. Recall might improve ifmore candidates were presented, but it would likely comeat the expense of decreased usability. It might insteadbe more relevant to limit further the number of candi-dates to present. As is shown in Figure 4, there are onlya few correct synonyms among the candidates ranked 610. By using more advanced post-processing techniquesand/or being prepared to sacrifice recall slightly, it ispossible to present fewer candidates for manual inspec-tion, thereby potentially increasing usability. On the otherhand, a higher cut-off value could be used for evaluating asystem aimed at a user who is willing to review a longer listof suggestions. An option for incorporating this differencein user behavior would be to use an evaluation metrics,such as rank-biased precision [58], that models the per-sistence of the user in examining additional lower-rankedcandidates.Reflections on evaluationTo make it feasible to compare a large number of seman-tic spaces and their various combinations, fixed referencestandards derived from terminological resources wereused for evaluation, instead of manual classification ofcandidate terms. One of the motivations for the currentstudy, however, is that terminological resources are sel-dom complete; they may also reflect a desired use of lan-guage rather than actual use. A manual classification on asample of one of the reference standards,Medical + Clini-cal, was carried out on the synonym task in order to verifythis claim. The results in this study thus mainly reflect towhat extent different semantic spaces  and their com-binations  are able to extract synonymous relations thathave been considered relevant according to specific termi-nologies, rather than to what extent the semantic spaces and their combinations  capture the phenomenon ofsynonymy. This is, for instance, illustrated by the queryterm depression in Table 12, in which one potential syn-onym is extracted by the clinical ensemble  depressivitet(depressitivity)  and another potential synonym by themedical ensemble: depressionsjukdom (depressive illness).Although these terms might not be formal or frequentenough to include in all types of terminologies, they arehighly relevant candidates for inclusion in terminologiesintended for text mining. Neither of these two termsare, however, counted as correct synonyms, and only themultiple-corpora ensemble is able to find the synonymincluded in the terminology.Furthermore, a random sample of 30 words (out of135) was manually classified for the semantic relationbetween each of the candidate terms in the sample, assuggested by the best combination of semantic spaces(the Disjoint Corpus Ensemble, see Table 10), and thetarget term. In the reference standard for this sample,33 synonyms are to be found (only three target wordshave two synonyms; none have three or more). The bestcombination finds only 10 of these reference synonyms(exact match), which accounts for the low recall figuresin Table 10. However, a manual classification shows thatthe same combination finds another 29 synonyms thatdo not occur in the reference standard. Furthermore,the Disjoint Corpus Ensemble also suggests a total of 15hyponyms, 14 hypernyms and 3 spelling variants as can-didate terms, which, depending on the context, can beviewed as synonyms. Among the candidate terms, we alsofind 3 antonyms, which shows the inability of the modelsreadily to distinguish between different types of semanticrelations.In one instance, we also capture a non-medical sense ofa term while completely missing the medical sense. Forthe target term sänka (erythrocyte sedimentation rate), 9out of 10 candidate terms relate to the more general senseof lowering something (also sänka in Swedish), with can-didate terms such as rising, reducing, increasing, halvingand decreasing. None of these are included in the ref-erence standard, which for this word only contains theabbreviation SR (ESR) as a synonym.In the case of the target term variecella, the referencestandard contains only the synonym vattkoppor (chick-enpox), while the Disjoint Corpus Ensemble correctlysuggests the abbreviation VZV, as well as herpes and theplural form varicellae (which is apparently missed by thelemmatizer).It is important to recognize that this type of manualpost-evaluation always bears the risk that you are toogenerous, believing in your method, and thus (manually)assign too many correct classifications  or, alternativelythat you are too strict in your classification in fear of beingtoo generous. Future studies would thus benefit froman extensive manual classification of candidates derivedfrom data generated in clinical practice, beforehand, withthe aim of also finding synonyms that are not alreadyincluded in current terminologies but are in frequentuse. These could then be used as reference standards infuture evaluations.The choice of terminological resources to use as ref-erence standards was originally based on their appropri-ateness for evaluating semantic spaces induced from theclinical corpus. However, for evaluating the extractionof abbreviation-expansion pairs with semantic spacesinduced from the medical corpus, the chosen resources in conjunction with the requirement that terms shouldoccur at least fifty times in the corpus  were less appro-priate, as it resulted in a very small reference standard.Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 22 of 25http://www.jbiomedsem.com/content/5/1/6This, in turn, resulted in no significant differences foreither of the two the abbreviation tasks between the bestsingle space and the combination of medical spaces, orbetween the conjoint corpus ensemble and the disjointcorpus ensemble. When assessing the potential of usingsemantic spaces for abbreviation-expansion tasks, morefocus should therefore be put on the results from the eval-uation on the spaces created from the clinical corpus, asthe improvement in recall gained by post-processing wasstatistically significant for both the abbr?exp task and theexp?abbr task, as was also the improvement gained fromusing an ensemble of spaces compared to a single corpusspace for the abbr?exp task.For synonyms, the number of instances in the referencestandard is, of course, smaller for the experiments withmultiple-corpora ensembles than for the single-corpusexperiments. However, the differences between the singlespace and the ensemble of spaces are statistically signifi-cant. Moreover, when evaluating the final results with dif-ferent frequency thresholds, similar results are obtainedwhen lowering the threshold and, as a result, includingmore evaluation instances. With a threshold of twentyoccurrences, 306 input terms are evaluated, which resultsin a recall of 0.42; with a threshold of thirty occurrencesand 222 query terms, a recall of 0.46 is obtained.Future workNow that this first step has been taken towards creat-ing ensembles of semantic spaces, this notion should beexplored in greater depth and taken further. It would, forinstance, be interesting to combine a larger number ofsemantic spaces, possibly including those that have beenmore explicitly modeled with syntactic information. Toverify the superiority of this approach, it should be com-pared to the performance of a single semantic space thathas been induced from multiple corpora.Further experiments should likewise be conductedwith combinations involving a larger number of corpora(types). One could, for instance, combine a professionalcorpus with a layman corpus  e.g. a corpus of extractsfrom health-related fora  in order to identify laymanexpressions for medical terms. This could provide a usefulresource for automatic text simplification.Another technique that could potentially be used toidentify term pairs with a higher degree of semantic simi-larity is to ensure that both terms have each other as theirclosest neighbors in the semantic subspace. This is notalways the case, as we pointed out in our error analysis.This could perhaps improve performance on the task ofextracting synonyms and abbreviation-expansion pairs.A limitation of the current study  in the endeavor tocreate a method that accounts for the problem of languageuse variability  is that the semantic spaces were con-structed to model only unigrams. Textual instantiationsof the same concept can, however, vary in term length.This needs to be accounted for in a distributional frame-work and concerns paraphrasing more generally thansynonymy in particular. Combining unigram spaces withmultiword spaces is a possibility that could be explored.This would also make the method applicable for acronymexpansion.ConclusionsThis study demonstrates that combinations of semanticspaces can yield improved performance on the task ofautomatically extracting synonyms. First, combining twodistributional models  random indexing and randompermutation  on a single corpus enables the capturingof different aspects of lexical semantics and effectivelyincreases the quality of the extracted candidate terms, out-performing the use of one model in isolation. Second,combining distributional models and types of corpora a clinical corpus, comprising health record narratives, anda medical corpus, comprising medical journal articles improves results further, outperforming ensembles ofsemantic spaces induced from a single source, as well assingle semantic space induced from the conjoint corpus.We hope that this study opens up avenues of explorationfor applying the ensemble methodology to distributionalsemantics.Semantic spaces can be combined in numerous ways. Inthis study, the approach was to combine the outputs, i.e.ranked lists of semantically related terms to a given queryterm, of the semantic spaces. How this should be done isnot wholly intuitive. By exploring a variety of combinationstrategies, we found that the best results were achieved bysimply summing the cosine similarity scores provided bythe distributional models.On the task of extracting abbreviation-expansion pairs,substantial performance gains were obtained by applyinga number of simple post-processing rules to the list of can-didate terms. By filtering out unlikely candidates based onsimple patterns and retrieving new ones, both recall andprecision were improved by a large margin.Lastly, analysis of a manually classified sample from thesynonym task shows that the semantic spaces not onlyextract synonyms that are present in the reference stan-dard. Equally valid synonyms not present in the referencestandard are also found. This serves to show that the refer-ence standards, as most often is the case, lack in coverage,as well as supports the fact that the semantic spaces canbe used to enrich and expand such resources.EndnotesaSignifiers are here simply different linguistic itemsreferring to the same concept.bOntologies are formal descriptions of concepts andtheir relationships.Henriksson et al. Journal of Biomedical Semantics 2014, 5:6 Page 23 of 25http://www.jbiomedsem.com/content/5/1/6cThe words big and large are, for instance, synonymouswhen describing a house, but certainly not whendescribing a sibling.dUnified Medical Language System: http://www.nlm.nih.gov/research/umls/eHyponyms are words that are subordinate to anotherword, its hypernym. For instance, dog is a hyponym ofmammal, which in turn is a hyponym of animal.fThere are also probabilistic models, which viewdocuments as a mixture of topics and represent termsaccording to the probability of their occurrence duringthe discussion of each topic: two terms that share similartopic distributions are assumed to be semantically related.gExplicit dimensionality reduction is avoided in thesense that an initial term-context matrix is notconstructed, the dimensionality of which is then reduced.The high-dimensional data is prereduced, if you will, byselecting a much lower dimensionality from the outset(effectively making this a parameter of the model).hTernary vectors allow three possible values: +1s, 0sand ?1s. Allowing negative vector elements ensures thatthe entire vector space is utilized.iOrthogonal index vectors would yield completelyuncorrelated context representations; in the RIapproximation, near-orthogonal index vectors result inalmost uncorrelated context representations.jThe bag-of-words model is a simplified representationof a text as an unordered collection of words, wheregrammar and word order are ignored.kAn alternative is to shift the index vectors according todirection only, effectively producing direction vectors [46].lThis research has been approved by the RegionalEthical Review Board in Stockholm(Etikprövningsnämnden i Stockholm), permissionnumber 2012/834-31/5.mThe used stop word lists are available at http://people.dsv.su.se/~mariask/resources/stoppord.txt (clinicalcorpus) and http://people.dsv.su.se/~mariask/resources/lt_stoppord.txt. (medical corpus)nAntonyms are words that differ in one dimension ofmeaning, and thus are mutually exclusive in this sense.For instance, something cannot be both large and smallin size at the same time.oHypernyms are words that are superordinate toanother word, its hyponym. For instance, animal is ahypernym ofmammal, which in turn is a hypernym ofdog.Competing interestsThe authors declare that they have no competing interests.Authors contributionsAH was responsible for coordinating the study and was thus involved in allparts of it. AH was responsible for the overall design of the study and forcarrying out the experiments. AH initiated the idea of combining semanticspaces induced from different corpora and implemented the evaluation andpost-processing modules. AH also had the main responsibility for themanuscript and drafted parts of the background and results description. HMand MS contributed equally to the study. HM initiated the idea of combiningsemantic models trained differently (Random Indexing and RandomPermutation) and was responsible for designing and implementing strategiesfor combining the output of multiple semantic models. HM also drafted partsof the method description in the manuscript and surveyed relevant literature.MS initiated the idea of applying the method to abbreviation-expansionextraction and to different types of corpora. MS was responsible for designingthe evaluation part of the study, as well as for preparing the referencestandards. MS also drafted parts of the background and method description inthe manuscript. VD, together with MS, was responsible for designing thepost-processing filtering of candidate terms. MD provided feedback on thedesign of the study and drafted parts of the background and methoddescription in the manuscript. MD also carried out the manual evaluation, andthe analysis thereof. AH, HM, MS and MD analyzed the results and drafted thediscussion and conclusions in the manuscript. All authors read and approvedthe final manuscript.AcknowledgementsThis work was partly (AH) supported by the Swedish Foundation for StrategicResearch through the project High-Performance Data Mining for Drug EffectDetection (ref. no. IIS11-0053) at Stockholm University, Sweden. It was alsopartly (HM) supported by the Research Council of Norway through the projectEviCare - Evidence-based care processes: Integrating knowledge in clinicalinformation systems (NFR project no. 193022). We would like to thank themembers of our former research network HEXAnord, within which this studywas initiated. We would especially like to thank Ann-Marie Eklund for hercontributions to the initial stages of this work. We are also grateful to StaffanCederblom and Studentlitteratur for giving us access to their database ofmedical abbreviations. Finally, we would like to thank the three reviewers fortheir insightful comments.Author details1Department of Computer and Systems Sciences (DSV), Stockholm University,Forum 100, SE-164 40 Kista, Sweden. 2Department of Computer andInformation Science, Norwegian University of Science and Technology,NO-7491 Trondheim, Norway. 3Faculty of Informatics, Vytautas MagnusUniversity, Vileikos g. 8 - 409, Kaunas, LT-44404, Lithuania.Received: 4 June 2013 Accepted: 17 January 2014Published: 5 February 2014JOURNAL OFBIOMEDICAL SEMANTICSSarntivijai et al. Journal of Biomedical Semantics 2014, 5:37http://www.jbiomedsem.com/content/5/1/37DATABASE Open AccessCLO: The cell line ontologySirarat Sarntivijai1,2*, Yu Lin2, Zuoshuang Xiang2, Terrence F Meehan3, Alexander D Diehl4, Uma D Vempati5,Stephan C Schürer5, Chao Pang6, James Malone3, Helen Parkinson3, Yue Liu2, Terue Takatsuki7, Kaoru Saijo7,Hiroshi Masuya7, Yukio Nakamura7, Matthew H Brush8, Melissa A Haendel8, Jie Zheng9, Christian J Stoeckert9,Bjoern Peters10, Christopher J Mungall11, Thomas E Carey2, David J States12, Brian D Athey2 and Yongqun He2*AbstractBackground: Cell lines have been widely used in biomedical research. The community-based Cell Line Ontology (CLO)is a member of the OBO Foundry library that covers the domain of cell lines. Since its publication two years ago,significant updates have been made, including new groups joining the CLO consortium, new cell line cells, upperlevel alignment with the Cell Ontology (CL) and the Ontology for Biomedical Investigation, and logical extensions.Construction and content: Collaboration among the CLO, CL, and OBI has established consensus definitions of cellline-specific terms such as cell line, cell line cell, cell line culturing, and mortal vs. immortal cell line cell. A cellline is a genetically stable cultured cell population that contains individual cell line cells. The hierarchical structure ofthe CLO is built based on the hierarchy of the in vivo cell types defined in CL and tissue types (from which cell linecells are derived) defined in the UBERON cross-species anatomy ontology. The new hierarchical structure makes iteasier to browse, query, and perform automated classification. We have recently added classes representing morethan 2,000 cell line cells from the RIKEN BRC Cell Bank to CLO. Overall, the CLO now contains ~38,000 classes ofspecific cell line cells derived from over 200 in vivo cell types from various organisms.Utility and discussion: The CLO has been applied to different biomedical research studies. Example case studiesinclude annotation and analysis of EBI ArrayExpress data, bioassays, and host-vaccine/pathogen interaction. CLOs utilitygoes beyond a catalogue of cell line types. The alignment of the CLO with related ontologies combined with the useof ontological reasoners will support sophisticated inferencing to advance translational informatics development.Keywords: Cell line, Cell line cell, Immortal cell line cell, Mortal cell line cell, Cell line cell culturing, AnatomyBackgroundCell culturing dates back to as early as 1911 when AlexisCarrel attempted to grow living cells outside an organ-ism. The establishment of the first human cell line,HeLa, in 1951 has since brought the fruitful develop-ment of other cell lines from different organisms. Celllines have been commonly used in many aspects of bio-medical research and experimentation. Mass productionof cell line culture of animal cells is fundamental to themanufacture of viral vaccines and other products in bio-technology such as enzymes, synthetic hormones, anti-cancer agents, and immunobiologicals (e.g., monoclonalantibodies, interleukins, and lymphokines). However, it* Correspondence: siiraa@umich.edu; yongqunh@med.umich.edu1US Food and Drug Administration, Silver Spring, MD, USA2University of Michigan, Ann Arbor, MI, USAFull list of author information is available at the end of the article© 2014 Sarntivijai et al.; licensee BioMed CentCommons Attribution License (http://creativecreproduction in any medium, provided the orhas been realized that cell lines are often contaminated byother lines  for example, the robust HeLa line has beenshown to have widely contaminated many cell lines [1-3].In addition to the cross-contamination, other issuesexist in the domain of cell line representation. Due inlarge part to a history of bottom-up naming practices,cell line nomenclature has not been standardized or con-trolled by any centralized authority. This has made man-agement and tracking of cell line information a difficulttask, despite the existence of various public repositoriesand indexed catalogues available for open access. More-over, cell line related terms are loosely interchangeableand inconsistently used across communities, such thatterms like primary cells, primary cell culture, and cellline have become loaded with conflated and ambiguousmeaning. Confusion can also come from variability inhow cell lines are categorized. This results in part fromral Ltd. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly credited.Sarntivijai et al. Journal of Biomedical Semantics 2014, 5:37 Page 2 of 10http://www.jbiomedsem.com/content/5/1/37the wide range of methods for generating and modifyingcell lines confer diverse attributes used in their clas-sification. As we move toward the establishment of acentralized resource for cell lines, the ambiguity of cellline-associated terms needs to be clarified.Many of the challenges can be addressed by the devel-opment of an ontology for cell lines, wherein the variouscell line attributes can be normalized and based onagreement between users in the community. The dif-ferent aspects of describing a cell line can be modu-larized by their corresponding source organism andanatomical part, modifications, and culturing methods,or related diseases.The Cell Line Ontology (CLO) is a community-basedontology that covers the biological cell line domain. TheCLO was originally presented in the International Con-ference on Biomedical Ontology (ICBO) in 2011 [4].The original CLO was developed cooperatively by ontol-ogy editors from the University of Michigan Cell LineKnowledge base (CLKB) team, the EBI Functional Gen-omics Production Team, Cell Ontology (CL) [5] team,and the Bioassay Ontology (BAO) development team atthe University of Miami. Subsequently, the Cell Bank ofRIKEN BioResource Center (BRC) in Japan, the Ontologyfor Biomedical Investigation [6], and Reagent Ontology [7]joined the CLO development consortium. The CLO Con-sortium aims to unify publicly available cell line data frommultiple sources to a standardized format based on a con-sensus design pattern derived from the establishment ofCLO. This manuscript focuses on introducing recent up-dates on the CLO development.Construction and contentCLO statisticsThe development of CLO follows the OBO Foundryprinciples, including openness, collaboration, and use ofa common shared syntax [8]. As a result, the CLO devel-opers have been working to establish a common under-standing and agreement of key CLO-specific terms. TheCLO terms and definitions have been created and re-fined with the input from participants who utilize theCLO in their studies. In addition, the CLO also makesuse of existing ontologies via an OntoFox import strategy[9]. The CLO is aligned with the Basic Formal Ontology(BFO; http://www.ifomis.org/bfo) version 2 (Graz release)by importing of all the class terms of BFO as its upper levelontology. Note that the preparation for BFO version 2.0alignment has also been implemented in the event that itsuse becomes an OBO recommendation. Object propertiesimported from the Relation Ontology (RO) are used torepresent relations in the CLO. Some terms from SpatialOntology (BSPO), SemanticScience Integrated Ontology[10] and Information Artifact Ontology (IAO) [11] are alsoimported as part of the top-level ontology manager. Table 1summarizes classes imported for the upper-level ontologyoperations, along with the classes utilized from other exter-nal ontologies to establish data integration and to supportautomated reasoning.In summary, CLO consists of over 38,000 terms forvarious cell line cells. These are mostly cell line informa-tion obtained from cell line records deposited at four cellline resources: the ATCC and HyperCLDB cell linesstored in the CLKB from the University of Michigan,Corriell Cell Lines processed by EBI, and the cell linesfrom the Cell Bank of RIKEN BioResource Center (BRC).Almost 300 cell line entry descriptors of cell types wereimported from CL and over 1,300 anatomical entities wereimported from UBERON [12]. CLO currently contains in-formation of cell lines derived from more than 350 species(NCBITaxon entities). Biomedical experiment relatedterms were imported from OBI and EFO [13]. Whenapplicable, components from the following resources: GeneOntology (GO), Phenotypic Quality Ontology (PATO),Protein Ontology (PRO) [14], Chemical Entity of BiologicalInterest (ChEBI), and Human Disease Ontology (DOID)are imported into CLO based on available information inthe cell line cell records.The CLO was developed using the format of W3Cstandard Web Ontology Language (OWL2) (http://www.w3.org/TR/owl-guide/). The latest CLO is available forpublic view and download at http://code.google.com/p/clo-ontology/. The latest version of the CLO is also avail-able for visualization and downloading from Ontobee(http://www.ontobee.org/browser/index.php?o=CLO) orNCBOs BioPortal: (http://purl.bioontology.org/ontology/CLO). The source code of CLO is open and freely availableunder the Apache License 2.0.Alignment of core domain concepts between CLO, OBI,and CLAs part of the CLO refactoring process, a working groupwas established between members of several key openbiomedical ontologies where cell line-related entities arerepresented, including the Cell Ontology (CL) and theOntology for Biomedical Investigation [6]. The goal ofthis group was to align modelling related to culturedcells in accordance with OBO Foundry principles of or-thogonality and re-use. One key outcome of this workwas the integration of inconsistent representations into asingle shared model. Classes representing key conceptswere implemented in the CL, CLO, and OBI - with theCL as a home for high-level in vitro cell modelling (e.g.cultured cell), the CLO as a home for more specific cellline cell and cell line classes, and the OBI as a home forexperimental entities related to these cell lines (e.g. cellline culture and establishing cell line classes). As a result,each term has a single representation that is re-used be-tween ontologies through established import mechanisms.Table 1 Summary of ontology terms in CLO and major source ontologies used in CLO as of November 21st, 2013Ontology Classes Object properties Annotation properties TotalCLO (Cell Line Ontology) specific 38453 2 0 38455Imported upper-level ontologiesBFO (Basic Formal Ontology) 22 38 0 60RO (Relation Ontology) 0 85 1 86BSPO (Spatial Ontology) 0 18 0 18SIO (SemanticScience Integrated Ontology) 0 3 0 3IAO (Information Artifact Ontology) 17 2 17 36Imported entities from other external ontologiesOBI (Ontology for Biomedical Investigation) 20 6 2 28EFO (Experimental Factor Ontology) 149 1 1 151CL (Cell Ontology) 269 0 0 269UBERON 1315 0 0 1315NCBITaxon (NCBI Taxonomy) 354 0 0 354PATO (Phenotypic Quality Ontology) 22 0 0 22GO (Gene Ontology) 299 0 0 299PR (Protein Ontology) 5 0 0 5DOID (Human Disease Ontology) 741 0 0 741ChEBI (Chemical Entities of Biological Interest) 32 0 0 32Total 41698 155 21 41874The detail of total number of terms and the most recent update can be viewed at http://www.ontobee.org/ontostat.php?ontology=CLO.Sarntivijai et al. Journal of Biomedical Semantics 2014, 5:37 Page 3 of 10http://www.jbiomedsem.com/content/5/1/37A second key outcome of this alignment work was thecrafting of clear consensus definitions for common butambiguous domain terminology, including a carefulcharacterization of the term 'cell line' itself. Updateddefinitions for a selection of key CLO classes resultingfrom this work are detailed below.A cell line is defined as a genetically stable andhomogenous population of cultured cells that shares acommon propagation history (i.e. has been successivelypassaged together in culture). This view clarifies two keyconfusions surrounding the term cell line. The first re-lates to the scale at which the term applies, here refer-ring to discrete experimental populations rather thanmaximal collections representing an entire lineage (e.g.the collection of all HeLa cells that exist). The secondconcerns the criteria that establish when a cultured cellpopulation qualifies as a line. By applying cell line toexperimental populations with a shared culture history,we define the term consistently with its most prevalentusage in domain discourse, and in a way that is most fitfor data annotation needs, as it represents populationsthat are actually cultured, experimented upon, and sharedin the practice of science.A cell line cell is defined simply as a cultured cell thatis part of a cell line. This class is a child of CL:culturedcell, defined as a cell in vitro that is or has been culturedin vitro (Figure 1). Cell line cells, like cell lines, can existin active culture or stored in quiescence. The represen-tation of each specific cell line (e.g. HeLa, HEK293) isimplemented at the scale of individual cells, such thatcell line cell is the root of the core CLO hierarchy ofcell line types.A clonal cell line is defined as a cell line that derivesfrom a single cell that is expanded in culture. Feedbackfrom community experts and stakeholders initiated therepresentation of this specific type of cell line as a key ex-perimental resource with unique and valuable attributes.Finally, a cell line culture represents an actual physicalculture of cell line cells that is an input to experimentalprocesses, and is comprised cell line cells and the mediaalong with any added components. This term is intendedto cover actively propagated cultures as well as thosekept frozen.Through the alignment efforts summarized above, wehave increased the utility of the CLO as a communityresource for standardizing reference to domain conceptsand facilitating the exchange and discovery of cell linerelated information.Basic CLO cell line design patternThe basic CLO design pattern represents organ anat-omy, cell types, disease and pathology, source informa-tion in the form of ownership and derivation where celllines are related to each other, and technical informationFigure 1 The top level CLO hierarchical structure and key ontology terms. Terms imported from other ontologies are indicated by theontology abbreviations inside parentheses. Terms without an identified source are CLO terms. All the arrows indicate the is_a relation except theexplicitly labelled has grain relation.Sarntivijai et al. Journal of Biomedical Semantics 2014, 5:37 Page 4 of 10http://www.jbiomedsem.com/content/5/1/37such as culture conditions (Figure 2A). A core designpattern implemented for, cell line cell, describes deriv-ation from an in vivo cell, which is in turn, part of someanatomical structure, and in turn, anatomical structureis part of a species of organism. In addition, diseasesborne by the source organism can be captured in thispattern when known. An object property is model for isdefined to represent a relation between a cell line celland a disease where the cell line cell is a model systemfor studying the disease. An example of applying thisgeneral design pattern to a concrete example is shownin Figure 2B where the design pattern is used to repre-sent a HeLa cell transfected with luciferase reporter inBAO. In this example, a HeLa cell transfected with a lu-ciferase reporter derives from HeLa cell through astable transfection that is a cell line cell modificationprocess. The transfected HeLa cells are part of or speci-fied input of cell line culturing process, specifically, ad-herent cell line culturing process. This cell line cell alsoinherits the pathological cell properties from HeLa cell(as a cell line cell), which derives from some epithelialcell that is part of  some uterine cervix, which is part of an organism Homo sapiens. This specific instance ofHomo sapiens also has disease of some cervical carcin-oma, which is a carcinoma and HeLa cell is a cell linemodel for studying this cancer. Relations of knowledgein multiple domains are shown in this example of a cellline cell annotation in BAO with the semantic infra-structure provided by CLO. Modification of a cell linecell can give rise to another derived cell line cell, whichis also described by derives from relation. A cell linecell is specified input of some special culturing process(CLO:cell line cell culturing, a subclasss of OBI:maintaininga cell line culture), which can differ from culturing one cellline cell to another (e.g., suspension cell line culturingor adherent cell line culturing). The relation derivesfrom is not a transitive relation. The derives from re-lation could be transitive if, and only if, there are noconfounding environmental and/or experimental con-ditions that affect the cell lines characteristics (suchas cross contamination). The cell line cell culturing re-flects a particular culturing condition or growth mode(e.g. suspension or cell surface adhesion). A cell line issupplied, maintained, or catalogued by a specific organi-zation such as American Type Cell Culture (ATCC) thathas cell line repository role. Since relation terms such assupply, own, or manage have not been fully developedin any ontology, we have created a CLO-specific relationreflecting this activity with the label is in cell line reposi-tory. This object property designates the representationof a particular cell lines information in such repository.This is an update from the previous version that utilizedrelation mentions to describe this activity. The relationof class label mentions was used in CLOs previous re-JOURNAL OFBIOMEDICAL SEMANTICSLaurenne et al. Journal of Biomedical Semantics 2014, 5:40http://www.jbiomedsem.com/content/5/1/40RESEARCH Open AccessMaking species checklists understandable tomachines  a shift from relational databases toontologiesNina Laurenne1*, Jouni Tuominen1, Hannu Saarenmaa2 and Eero Hyvönen1AbstractBackground: The scientific names of plants and animals play a major role in Life Sciences as information is indexed,integrated, and searched using scientific names. The main problem with names is their ambiguous nature, becausemore than one name may point to the same taxon and multiple taxa may share the same name. In addition, scientificnames change over time, which makes them open to various interpretations. Applying machine-understandablesemantics to these names enables efficient processing of biological content in information systems. The first step is touse unique persistent identifiers instead of name strings when referring to taxa. The most commonly used identifiersare Life Science Identifiers (LSID), which are traditionally used in relational databases, and more recently HTTP URIs,which are applied on the Semantic Web by Linked Data applications.Results: We introduce two models for expressing taxonomic information in the form of species checklists. First, weshow how species checklists are presented in a relational database system using LSIDs. Then, in order to gain a moredetailed representation of taxonomic information, we introduce meta-ontology TaxMeOn to model the same contentas Semantic Web ontologies where taxa are identified using HTTP URIs. We also explore how changes in scientificnames can be managed over time.Conclusions: The use of HTTP URIs is preferable for presenting the taxonomic information of species checklists. AnHTTP URI identifies a taxon and operates as a web address from which additional information about the taxon can belocated, unlike LSID. This enables the integration of biological data from different sources on the web using LinkedData principles and prevents the formation of information silos. The Linked Data approach allows a user to assembleinformation and evaluate the complexity of taxonomical data based on conflicting views of taxonomic classifications.Using HTTP URIs and Semantic Web technologies also facilitate the representation of the semantics of biological data,and in this way, the creation of more intelligent biological applications and services.Keywords: Scientific name, Taxonomic concept, LSID, HTTP URI, Ontology, Semantic web, Linked data,Species checklistBackgroundResearch on biodiversity requires integrating data fromdistributed heterogeneous sources, such as scientific lit-erature, observations, and biomedical resources. Data isoften presented using a variety of terms, vocabularies, andlanguages, which presents a barrier to interoperability and*Correspondence: nina.laurenne@helsinki.fiEqual contributors1Semantic Computing Research Group (SeCo), Department of MediaTechnology, Aalto University, P.O. Box 15500, 00076 Aalto, Espoo, FinlandFull list of author information is available at the end of the articlemakes data reuse and integration a challenge for bothhuman users and machines.Scientific names are important for interlinking infor-mation about taxa in all fields of the Life Sciences. Ataxon is a group of one or more organisms whose mem-bers are considered evolutionarily related to one another;a taxon typically has a name and rank, i.e., a species, genus,etc. Taxon names are especially necessary when indexingbiological information and cataloguing biodiversity. Thenature of names, whether important or problematic, hasrecently been re-examined by several researchers [1-6].© 2014 Laurenne et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited.Laurenne et al. Journal of Biomedical Semantics 2014, 5:40 Page 2 of 13http://www.jbiomedsem.com/content/5/1/40Difficulties arise when a particular taxon can be referredto using multiple names, since scientists opinions differon how evolutionary units should be organised into classi-fications. Also, researchers may use the same name with adifferent meaning when referring to taxa. Well-conductedtaxonomic studies may be 250 years old and still use-ful but in most cases, the perceived boundaries of taxahave been revised several times after the original publica-tion. Contrary to popular belief, a generally agreed-upon,single taxonomy of organisms does not exist, and thisfact is directly reflected in the scientific naming systemthrough the various usages of names. For a taxonomist,a scientific name is a label that mirrors an evolution-ary hypothesis that is under continuous testing. Therewill never be a commonly agreed upon single taxonomyand there will always be multiple competing current tax-onomic views. Nevertheless, efforts are made to provideusable taxonomies for non-taxonomists.Checklists are species catalogues where taxa are organ-ised hierarchically according to an authors current view ofa classification. The coverage of a species checklist variesfrom a geographically limited area to a worldwide list,and it typically focuses on a particular organismal group.An authors view of research results is thus inevitablyemphasised, which opens the lists to interpretation ifthey lack sufficient taxonomic details. A regional specieslist indexes taxa of a given area, but it can also containadditional information. For example, Fauna Europaea [7]and the Atlas of Living Australia [8] provide distributionmaps and visualisation tools. The database Encyclope-dia of Life (EoL) [9] covers the whole world and has aconsiderable amount of species information. Also, unlikemost resources, it supports multiple classifications sincedata providers can upload differing taxonomies into thesystem.Checklists were previously only published in journals(static lists), but up-to-date checklists (dynamic lists) areincreasingly available on the web. For example, the mostnotable database, Catalogue of Life (CoL) [10], aims toinclude all known species and currently contains nearly1,352,112 species from 132 taxonomic datasets (2013Annual Checklist). The database of zoological namesZooBank [11] currently has 101,777 nomenclatural acts.The Global Biodiversity Information Facility (GBIF) [12]has made an effort to stabilise name usage by setting upa Checklist Bank [13] for storing names and informationabout them. The widely used Taxonomic Concept Trans-fer Schema (TCS) [14] specifies the format (XML), inwhich taxonomic information is presented when exchang-ing data. Darwin Core (DwC) [15,16], created by Biodi-versity Information Standards (TDWG) [17], is a stan-dardised form of presenting biological information. Themetadata elements in DwC are not strictly defined asthe format and the element values are not fully specified.This means that the interoperability of DwC records isnot achieved if the elements are not used in a consis-tent way. For example, a taxon name may be a literalvalue or referred to using a URI. Darwin Core Archive(DwC-A) [18] is a data standard for producing a self-contained dataset for sharing species-related data, suchas occurrence records and checklists. The CSV (Comma-Separated Values) data files of an archive are organisedin a star-like manner, with one core data file and possibleextensions, e.g., for vernacular names or distribution data.The scope of biomedical resources differs from check-lists because the focus is on a gene or a cell level. Nev-ertheless, the name question remains relevant becausescientific names are used for linking information. Cur-rently, the National Center for Biotechnology Information(NCBI) [19] provides a single robust consensus hierar-chy of taxa constructed by experts, but NCBI ambi-tiously seeks to build a topology based on monophyleticgroups, i.e., taxa derived from a common ancestor. NCBIallows flexibility in the acceptance of informal namesand surrogate names can be used when contributingdata and searching for taxa [5]. The majority of thesubmitted DNA sequences do not have a binominal sci-entific name because specimens are not identified intoa species level at the time of submission or only sur-rogate names are used [5]. The significance of DNAsequence data is increasing due to the rapid developmentof molecular methods that are applied in constructingevolutionary hypotheses and barcoding biodiversity. Con-sequently, descriptions of new species based on molecularevidence result in an increased number of species inchecklists.A major source for ambiguity in scientific names is thatthey change over time. One of the most common types ofchange concerns a Linnean binominal name combination.The genus of a binominal name changes when a species ismoved to another genus. For example, the parasitic waspspecies moscaryi once belonged to the genus Tetraconus,but as a result of a taxonomic revision that synonymisedtwo genera, its new name combination is Monomachusmoscaryi [20]. Synonymisation happens due to assess-ments of the identity of types (i.e., typically a physicalspecimen to which a scientific name is attached). If twoor more taxa are lumped, the older name remains validbut with a changed taxonomic circumscription, and themore recent names become its synonyms. Consequently,there is more than one name pointing to the taxon, andthe taxonomic concept associated with the older namechanges. The opposite situation is the split of taxa, whereone taxon is divided into two or more taxa. The diver-gence between a name and its meaning is characteristicof taxonomy, because a scientific name does not neces-sarily change despite the fact that taxon boundaries areredefined. Researchers can also classify the same speciesLaurenne et al. Journal of Biomedical Semantics 2014, 5:40 Page 3 of 13http://www.jbiomedsem.com/content/5/1/40in various ways, thus leading to the existence of multiplename combinations.Berendsohn [21] introduced the concept of a poten-tial taxon, which is a scientific name with informationon a circumscription. He proposed the term secundum(abbr. sec) be attached to a scientific name when refer-ring to a particular taxonomic circumscription. This wasa concrete suggestion on how to interlink differing tax-onomic views while continuing to retain the adequatetaxonomic information in databases [22]. Having infor-mation on circumscriptions in databases is an improve-ment, but machine-readable semantics need to be used inorder to enhance themachine-processability of taxonomicinformation.In this paper we present two models for describing tax-onomic information in a machine-processable way. Thefirst model describes species checklists as a relationaldatabase and the second one is further developed repre-sentation of taxonomic information using Semantic Webtechnologies. We explore the reasons for moving awayfrom relational databases towards semantic technology,and we also discuss options for managing scientific namesas they change over time.Towards semantic handling of biological namesA biologist understands the semantics of scientific namesby reading scientific literature, but computers requireexplicit identifier systems and data models to processsemantics. It is obvious that persistent identifiers for taxashould be used instead of ambiguous name strings toincrease the processability of scientific names. Using iden-tifiers allows information to be connected unambiguously,which enables interoperability between systems. Further-more, there is a need to interlink taxa between the differ-ent versions of checklists as they are updated. Otherwise,data indexed using an earlier version of a checklist can-not categorically be found using a later version of thechecklist.Recognising taxa using identifiersThe most commonly used identifiers in biology are LifeScience Identifiers (LSID) [23]. An LSID consists of sixparts (Figure 1): the first two indicate that the type ofURN (Uniform Resource Name) is an LSID, the thirdpart expresses the authority, and the fourth specifies thenamespace (which specifies the type of an LSID, e.g., sci-entific name, living thing, picture, or museum specimen),the fifth points to the object ID, and the optional sixth partis for versioning information. An LSID can be accommo-dated to a single name or to a set of taxonomic details,depending on its purpose [2,24]. For example, identi-fiers are given to scientific names in the World Registerof Marine Species [25], but in the Catalogue of Life [10]they are given to taxonomic concepts. The Universal Bio-logical Indexer and Organizer (uBio) [26] has 11,106,374namebank records where LSIDs are used for referring totaxonomic concepts [6]. Also, an RDF (Resource Descrip-tion Framework) representation [27] is provided but someof the essential information is expressed as literals (aclassification, taxonomic rank and a typing of resources)instead of URIs, which hampers machine-processability.The data carried by an LSID is obtained using a specificresolver. Locating the resolver via the Domain Name Sys-tem (DNS) of the Internet requires that the resolver beconfigured in a DNS SRV record (DNS service record) ofthe domain used as the authority part of an LSID. LSIDscan also be used without a resolver if they are presented asHTTP URIs using an LSID HTTP proxy. According to theTDWG guidelines for using identifiers, an LSID resolvershould return metadata about the requested resource inRDF form [27]. The application of LSIDs in the Catalogueof Life is thoroughly discussed by Jones et al. [2]. GBIF haspublished recommendations for the adoption of LSIDsand HTTP URIs [28,29].The URN scheme applied to LSIDs is a URI schemestandardised by the Internet Assigned Numbers Author-ity (IANA) [30]. HTTP is also a URI scheme, but thereis a fundamental difference between URNs and HTTPURIs. HTTP URIs are based on the DNS, where the globaluniqueness of identifiers is guaranteed by the DNS infras-tructure, which also facilitates addressing and retrievinginformation about HTTP URIs. In contrast to URNs, sep-arate web services are not necessary to manage identifiercreation or resolve them for data retrieval because thesefunctions are already available in the infrastructure of theweb. As a result, HTTP URIs are used as the identifiermechanism for the Semantic Web and Linked Data [31].In addition, the form of an HTTP URI is flexible becauseFigure 1 The structure of an LSID. An LSID of a cerambycid beetle species obtained from the Catalogue of Life database.Laurenne et al. Journal of Biomedical Semantics 2014, 5:40 Page 4 of 13http://www.jbiomedsem.com/content/5/1/40it does not have strictly defined parts like LSIDs. HTTPURIs allow linking data across the web on the basis of themeaning of concepts that are identified with HTTP URIs,which enables the creation of the Web of Data.LSIDs were the first attempt to solve the name problem,but due to the rapid development of Semantic Web tech-nologies, the trend now favours standardised web technol-ogy. Themain differences between LSIDs and HTTPURIsare presented in Table 1. The technology applied does notsolve the problem of the divergence between a name andits meaning, but it does provide an appropriate solutionfor publishing and interlinking data in an interoperableway on the web.Both LSID- and HTTP URI-based checklists can bepublished for humans via a user interface and formachines as APIs (Application Programming Interface) toprovide access to the data in multiple ways. For example,the user interface can be used to check a valid name for ataxon and browse a classification. The same informationcan be obtained using a specialised API, but more generalquery interfaces can also be provided. In Linked Data, anAPI for reading the RDF description or a human-readableHTML page for a resource is typically provided, as wellas a general purpose endpoint service that can be queriedusing the SemanticWeb query language SPARQL. In addi-tion, checklists can be made available as downloadablefiles [31].Semantic modelling of taxonomiesOn the Semantic Web, taxonomies are represented usingRDF resources, i.e., entities with URI identifiers, andexplicit relations between them. A relatively new approachis to express taxonomic information as an ontology. Thefirst ontology model for a taxonomic classification waspresented by Schulz et al. [34], with taxa organised intoa single hierarchy. Franz and Peet [35] and Franz andThau [36] have offered further insight into the issues oftaxonomic ontology modelling. So far, a few taxonomicontologies have been published in the NCBO BioPortalTable 1 Themain differences between LSIDs and HTTPURIsLife science HTTP URIsidentifiersStandardised by Object Management Internet EngineeringGroup [32] Task Force [33]Reuse existing Defines a new Uses anURI schemes URN subscheme existing schemeData retrieval/ Specific resolving Uses existingdereferenceability service needed web technology(DNS, web servers)Structure of identifier Strict FlexibleLinked Data compatibility No Yes[37-41] and the ONKI ontology service [42]. The mostcomprehensive of them is the NCBI Organismal clas-sification [41], which contains more than 352,000 taxain a single hierarchy. Common to the classifications inthe NCBO BioPortal is that the hierarchy is constructedusing subclassOf (isA) relations and presented in theOBOontology language [43]. TaxonConcept.org [44] tacklesthe name problem of taxonomic information in prac-tice and shows how to publish the information as LinkedOpen Data. It also demonstrates how data from externalsources are integrated and investigates how to combinetaxonomic concepts with specimen data. However, someof the important information about names are describedas literals, e.g., the classification of taxa. Also, the taxo-nomic change types are not described (split or lump oftaxa). The Taxonomic Meta-Ontology TaxMeOn [45,46]aims to respond to the practical needs of managing bio-logical names over time, and it links taxonomic infor-mation to names. This meta-ontology differs in that itoffers a greater level of detail and supports differingclassifications.An increasing number of ontologies are available andtherefore ontology evolution has become an importantissue. The world  and our conceptualisation of it  iscontinually changing, which makes ontology versioningessential [45,47,48]. Existing data that refer to a conceptshould be kept consistent when its meaning changes orwhen it is removed from an ontology. Data describedusing different versions of an ontology then can be inte-grated by utilising mappings (alignments) between theontology versions [49]. Khattak et al. [50] document ontol-ogy evolution by keeping a log of changes in concepts.Small changes in an ontology are grouped into sets, whichcan later be used to revert to previous stages. An alter-native solution is to recognise concept changes instead ofversioning an ontology. Wang et al. [51] show how thechanges in concepts and their impacts can be identifiedautomatically by comparing the concepts both extension-ally and intensionally in cases where they do not have fixedidentifiers.MethodsIn order to develop two models for presenting taxonomicinformation in a machine-processable way, four designprinciples were applied to satisfy the following conditions:1. use as few terms as possible to express as muchinformation as possible in the schema of the model.The taxonomic terminology and its usage isestablished in biology, and the terms are used inconsistent way. As few new terms as possible areintroduced.2. focus on a restricted domain, that is, scientificspecies checklists including all taxonomicLaurenne et al. Journal of Biomedical Semantics 2014, 5:40 Page 5 of 13http://www.jbiomedsem.com/content/5/1/40information and excluding any other taxon-relatedinformation (e.g., distribution).3. support information on various levels of granularity,as the source material is heterogeneous in its level ofdetail.4. accept all views of taxonomy equally legitimateregardless of the time they were disseminated.The focus of the models is in representing the taxo-nomic relations between taxa in a single checklist (clas-sification, synonymy), in different checklists (mappingtaxonomic concepts) and in individual versions of a check-list (managing taxonomic changes).The datasets utilised in the study consist of 20 publishedspecies checklists that cover mainly northern Europeanmammals, birds and several groups of insects and assem-ble ca. 78,000 taxon names (Additional file 1). Twomodelsare applied to the same datasets. Namemappings betweenthe checklists are provided for eight families of papilionoidand hesperioid butterflies.ResultsTaxonomic databaseThe main elements of the Taxonomic Database (Figure 2)[52] are a binominal scientific name and a taxonomicconcept that connects the names that refer to the sametaxon. Each concept is identified with a concept LSID. Inaddition, three other attributes are assigned to the sci-entific name: 1) a reference to the original publication(author name and year of publication) in which the taxondescription was first published, 2) a status of a name indi-cating its validity in the checklist, and 3) a taxonomic rankFigure 2 A simplified structure of the relational taxonomicdatabase. The boxes illustrate the tables of the database, and thelines present the relations between them. LSIDs are given totaxonomic concepts and scientific names (illustrated with a darkercolour). Taxonomic concepts are linked to each other using therelations described in Table 3 and each concept is linked to ascientific name. External LSIDs and common names are connected tothe concepts. An author reference, validity, and a taxonomic rank areassigned to the scientific names.expressing level in a hierarchical classification (species,genus, etc.). A taxonomic hierarchy between scientificnames is constructed using a hierarchical part-of relation.An LSID that is obtained from an external source canbe assigned to a taxon concept as an attribute. Commonnames in multiple languages can be connected to the con-cept, but no taxonomic rank can be specified for them.In order to recognise the orthographic variants of scien-tific names, LSIDs are accommodated to the names aswell.A new LSID is given to a concept if it changes, such as ataxonomic change, an addition or removal of a synonym,or a change in relations between taxa. An LSID is assignedto a new taxon when added to a dynamic checklist. LSIDsare versioned in the case of minor changes using theoptional part of the identifier. The decision whether tocreate a new object identifier of an LSID or a new versionis made by a maintainer.Taxa can be searched using a complete or partial sci-entific name via a user interface, and the system returnsa currently valid name and its synonyms. If the taxonis found in other checklists, their interrelations are alsodescribed. The information is also provided as an RDFrepresentation for machine consumption. Only the latestversions of dynamic checklists can be seen in the system.However, older ones are stored internally in the database.Taxonomic concepts are linked on the basis of theirequivalence at a species level, but at higher levels thealignment of taxa is based on the species content. Forinstance, two species that have the same name and thesame authorship citation are linked as congruent bydefault, but two genera are linked as congruent only if thespecies belonging to the genera are the same. The rea-sons for treating species and taxa above the species leveldifferently are debated in the Discussion.Taxonomic meta-ontologyTaxMeOn is an ontology schema for biological names, andhere we present the part that describes species checklists.Themodel is based on RDFS (RDF Schema) and some fea-tures of OWL (Web Ontology Language); it contains 12classes with 49 subclasses (excluding 61 subclasses of theclass TaxonomicRank) and 28 properties. The core classesand their relations are illustrated in Figure 3.The class TaxonInChecklist represents both a scientificname and its concept. The relation rdfs:label expresses theunominal name of a taxon which is 1) the last epithet of aname combination, or 2) a name of a taxon at higher lev-els, e.g., a family. The taxonomic hierarchy is constructedusing the relation isPartOfHigherTaxon.JOURNAL OFBIOMEDICAL SEMANTICSWang et al. Journal of Biomedical Semantics 2014, 5:36http://www.jbiomedsem.com/content/5/1/36RESEARCH Open AccessStandardizing adverse drug event reporting dataLiwei Wang1,2*, Guoqian Jiang2, Dingcheng Li2 and Hongfang Liu2AbstractBackground: The Adverse Event Reporting System (AERS) is an FDA database providing rich information onvoluntary reports of adverse drug events (ADEs). Normalizing data in the AERS would improve the mining capacityof the AERS for drug safety signal detection and promote semantic interoperability between the AERS and otherdata sources. In this study, we normalize the AERS and build a publicly available normalized ADE data source. Thedrug information in the AERS is normalized to RxNorm, a standard terminology source for medication, using anatural language processing medication extraction tool, MedEx. Drug class information is then obtained from theNational Drug File-Reference Terminology (NDF-RT) using a greedy algorithm. Adverse events are aggregatedthrough mapping with the Preferred Term (PT) and System Organ Class (SOC) codes of Medical Dictionary forRegulatory Activities (MedDRA). The performance of MedEx-based annotation was evaluated and case studies wereperformed to demonstrate the usefulness of our approaches.Results: Our study yields an aggregated knowledge-enhanced AERS data mining set (AERS-DM). In total, theAERS-DM contains 37,029,228 Drug-ADE records. Seventy-one percent (10,221/14,490) of normalized drug conceptsin the AERS were classified to 9 classes in NDF-RT. The number of unique pairs is 4,639,613 between RxNormconcepts and MedDRA Preferred Term (PT) codes and 205,725 between RxNorm concepts and SOC codes afterADE aggregation.Conclusions: We have built an open-source Drug-ADE knowledge resource with data being normalized andaggregated using standard biomedical ontologies. The data resource has the potential to assist the mining of ADEfrom AERS for the data mining research community.IntroductionSince the early 1990s, adverse drug events (ADEs) havereceived considerable attention from researchers in qual-ity and patient safety [1]. Although randomized clinicaltrials (RCTs) are considered as a gold standard for deter-mining the safety issues of drugs, it is generally recog-nized that premarketing RCTs may not detect all safetyissues related to a particular drug in clinical practice [2].The US Food and Drug Administration (FDA) AdverseEvent Reporting System (AERS) is one of the main re-sources in post-marketed ADE detection based on datamining techniques [3,4]. The main data mining metricsused for ADE detection include the proportional report-ing ratio (PRR), the reporting odds ratio (ROR), the in-formation component (IC), and the empirical Bayesgeometric mean (EBGM) [5]. For example, Kadoyama* Correspondence: wlw@jlu.edu.cn1Department of Medical Informatics, School of Public Health, Jilin University,Jilin, China2Department of Health Sciences Research, Mayo Clinic, Rochester, MN, USA© 2014 Wang et al.; licensee BioMed Central LCommons Attribution License (http://creativecreproduction in any medium, provided the oret al. [6] used the above metrics and detected signals forpaclitaxel-associated mild, severe, and lethal hypersensi-tivity reactions and docetaxel-associated lethal reactions.Poluzzi et al. [7] detected drug-induced torsades depointes (TdP) signals of linezolid, caspofungin, posaco-nazole, indinavir, and nelfinavir using ROR. However,most of existing studies on the AERS were carried outfor a small number of drugs [6,8-10], and few studieswere focused on large-scale mining or on detecting theetiology of ADE signals in terms of mechanism of action,physiologic effect, or molecular structure of drugs [11].We realize that potential of the AERS has not been fullyutilized, and one of main reasons for this is becausethere is a lack of standardization among drug names.In the AERS, drugs can be registered by arbitrary names,including trade names, abbreviations, and even typograph-ical errors since they are directly entered by health careprofessionals (e.g., physicians, pharmacists, nurses, etc.) andconsumers (e.g., patients, family members, lawyers, etc.)[5]. There is limited normalization effort for the AERS data.td. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly credited.Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 2 of 13http://www.jbiomedsem.com/content/5/1/36For example, for drug names AERS uses valid tradenames, if available, based on sources such as the OrangeBook [12] and other internal databases [13]. Otherwise,the verbatim names are used, thus forming substantialbarriers for data integration for the purpose of ADE signaldetection. There have been some attempts in drug namenormalization when mining AERS, but typically it is eitherunclear how the normalization was conducted or thenormalization was attempted only for a small number ofdrugs [3,6,9-11,14-16].In terms of ADE names, AERS does provide normal-ized terms based on Medical Dictionary for RegulatoryActivities (MedDRA) preferred terms (PTs), though theuse of MedDRA requires a license. In this study, we havedemonstrated that MedDRA PT-based normalization ac-tually enables the powerful data aggregation capabilitywhen we link the PT terms to their correspondingSystem Organ Class (SOC) categories.In this study, we aim to produce an open-source AERSdata mining set (AERS-DM), which is normalized andaggregated with two standard drug ontologies, includingRxNorm and the National Drug File-Reference Termin-ology (NDF-RT), and one ADE terminology, MedDRA.MethodsResourcesThe FDA AERS database is a public database that in-cludes 7 tables. Its structure is in compliance with theinternational safety reporting guidance (ICH E2B) [17].Information related to a single AERS report can be re-trieved from those tables using a unique identifier (i.e.,Individual Safety Report (ISR) number). Among them,the DRUG table includes drug-related information suchas DRUG_SEQ (a unique number for identifying adrug in a report), DRUGNAME, ROUTE (the routeof drug administration), and DOSE_VBM (verbatimtext for dose, frequency, and route, exactly as entered onthe report). The REAC table includes adverse event in-formation using PTs in MedDRA, a medical terminologyadopted to describe adverse drug event [18]. The DEMOtable includes patient demographics and administrativeinformation of the events, including CASE (case num-ber for identifying an AERS case) and FDA_DT (dateFDA received report).RxNorm, released initially in 2004, is a standardized no-menclature for clinical drugs and drug delivery devices[19]. Since its creation, RxNorm has been increasingly rec-ognized by the biomedical informatics community as anemerging standard for clinical information exchange [20].RxNorm is organized by concepts, in which each conceptconsists of drug names sharing the same meaning at a spe-cific level of abstraction and is assigned a concept uniqueidentifier (RxCUI). RxNorm also provides relationshipsbetween concepts, as indicated in Figure 1, adapted fromPeters and Bodenreider [21]. For example, 'Diphenhydra-mine Hydrochloride is the precise_ingredient_of TylenolPM. The description of all such relationships can be re-trieved, see the Availability and requirements section forthe webpage. Data in RxNorm is distributed in Rich Re-lease Format (RRF) tables, which is the default relationalformat used by the National Library of Medicine (NLM).NDF-RT was developed by the Veterans Health Admin-istration, providing clinical information about medications,and has been included in RxNorm. NDF-RT uses a de-scription logic-based, formal reference model that groupsdrugs and ingredients into the high-level classes for Chem-ical Structure (e.g., Acetanilides), Mechanism of Action(e.g., Prostaglandin Receptor Antagonists), PhysiologicalEffect (e.g., Decreased Prostaglandin Production), drug-disease relationship describing the Therapeutic Intent (e.g.,Pain), Pharmacokinetics describing the mechanisms of ab-sorption and distribution of an administered drug within abody (e.g., Hepatic Metabolism), and legacy VA-NDF clas-ses for Pharmaceutical Preparations (VA Drug Class; e.g.,Non-Opioid Analgesic) [22]. Figure 2 shows the NDF-RTcontent model [23] together with an example showing drugclass information for BUTABARBTIAL NA 100MG CAP.We utilized RxNorm and NDF-RT for normalizingand aggregating drug information in AERS in consider-ation of three reasons. First, these two ontologies arepublicly available medication ontologies that have beenintensively developed and used for drug data integration[22,24,25]. Second, RxNorm aims to enable various sys-tems using different standardized drug nomenclatures toshare and exchange data efficiently, which we believemeets the requirements for meaningful use of the ADEreporting data. In addition, since RxNorm only repre-sents a nomenclature of drugs and does not containdrug categorical information, we leveraged the categor-ical information extracted from NDF-RT for medicationdata aggregation. Third, as a part of the Unified MedicalLanguage System (UMLS), RxNorm and NDF-RT canfunction as interoperable drug standards that can inte-grate with other health data, such as electronic healthrecords (EHRs), so as to facilitate the semantic integra-tion of the data in the health domain.Finally MedDRA is a controlled terminology devel-oped for reporting adverse events, related to drugs, toregulatory agencies [26]. MedDRA has a hierarchicalstructure with five levels: SOC, High-Level Group Term(HGLT), High-Level Term (HLT), PT and Lowest-LevelTerm (LLT). There are 26 classes (SOCs). PTs are themain descriptors in MedDRA and are used in AERS. AllMedDRA terms are integrated UMLS.ToolIn our study, we used a natural language processing(NLP) tool, MedEx, to normalize AERS drugs. MedExFigure 1 Relations among RxNorm concepts, adapted from Peters and Bodenreider [21].Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 3 of 13http://www.jbiomedsem.com/content/5/1/36extracts medication information from clinical notes [27].Besides MedEx, there are a number of other existingNLP-based tools available that could be used for drugnormalization, including MedLEE [28], National Centerfor Biomedical Ontology (NCBO) Annotator Web Service[29], and Mayo cTAKES [30]. We chose to use the MedExbecause we consider it an optimized system for drugnormalization with a relative good performance, rankedsecond in the 2009 i2b2 Medication Extraction challenge,where the first-place system in that challenge is notavailable for public use [31]. The evaluation showed thatMedEx performed well on identifying drug names, withprecision (97%), recall (88%) and F-measure (92%) for 50discharge summaries and precision (95%), recall (92%)and F-measure (93%) for 25 clinic notes, respectively [27].In this study, we used MedEx version 2.0. Input files forMedEx included concrete information on drugs, and out-put normalized data included RxNorm codes.Data processingFigure 3 presents an overview of the data processing flowthat contains three steps: de-duplication, drug normalization,and data aggregation. In the de-duplication step, redundantreports are removed. In the normalization step, MedEx isapplied to normalize AERS drugs to RxNorm codes. Duringaggregation, adverse events are aggregated according toMedDRA SOC and PT codes, and NDF-RTbased classi-fication information for those drugs is obtained fromRxNorm. Two tables are formed; one stores the normalizedDrug-ADE information and the other stores the aggregatedFigure 2 NDF-RT content model and the example. Triangles denote hierarchies of related concepts, categorized in the rectangles within thetriangles. Taxonomic or ISA relationships (upward-pointing blue solid arrows) unify NDF-RT clinical drug concepts into a polyhierarchy, classifiedboth by their VA drug class and their generic ingredient(s). Various named role relationships (sideways-pointing amber dash arrows) define thecentral drug concepts (green) from which they originate in terms of the reference hierarchy concepts (blue) pointed to. Role relationships are alsoinherited into subsumed clinical drug concepts. Adapted from NDF-RT document [April 2012 Version] [23].Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 4 of 13http://www.jbiomedsem.com/content/5/1/36information of Drug-ADE. The data in the two tables canbe connected through the RxNorm codes. Those steps arefurther detailed below.Removing redundant AERS dataAccording to the FDAs recommended method for de-duplication, for reports with the same CASE number,we select the latest (most recent) report date (i.e.,FDA_DT) in the AERS DEMO table. For reports withthe same CASE and FDA_DT values, we select the onewith the highest ISR number. Table 1 shows examplesof how to delete duplicate reports. We select the ISR4275741 for CASE number 4047837, and ISR7637797 for CASE number 8468457. Consequently,Figure 3 An overview of the data processing flow.the selected ISRs in the DEMO table are kept in DRUGand REAC tables.Normalizing AERS drug and ADE dataAfter de-duplication, we concatenate the following fieldsin the DRUG table: DRUGNAME, ROUTE, andDOSE_VBM, and the resulting strings are normalizedwith MedEx. Compared to DRUGNAME alone, theconcatenated string gives more comprehensive informa-tion about the corresponding drug, since the ROUTEfield can provide information such as Oral and theDOSE_VBM field can provide information such asTablet. The results are mapped to the RxNorm codeRxCUI. For example, the concatenated string POTASSIUMTable 1 Examples of duplicate reportsISR CASE FDA_DT De-duplication4269368 4047837 20040113 ×4275741 4047837 20040121 ?7637789 8468457 20110720 ×7637797 8468457 20110720 ?ISR is the unique number for identifying an AERS report. CASE is thenumber for identifying an AERS case. "FDA_DT is the date FDA receivedreport. De-duplication indicates the positive or negative action.Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 5 of 13http://www.jbiomedsem.com/content/5/1/36CHLORIDE EXTENDED RELEASE TABLET EXTENDEDRELEASE TABLET ORAL 20 MEQ BID ORAL is nor-malized to RxCUI 198116 (i.e., Potassium Chloride 20MEQ Extended Release Tablet). Meanwhile, we mapPT entries of the REAC table to PT and SOC codes ofMedDRA.Aggregating normalized drug and ADE dataThe algorithm for classifying AERS drugs based on NDF-RT includes two parts; the first part is to identify the cor-responding NDF-RT concepts for those normalized AERSFigure 4 Traversal pathways for classifying AERS drugs based on NDFdrugs and the second is to extract the associated NDF-RTclassification information.Specifically, RxNorm contains NDF-RT ingredients andclinical drugs. Meanwhile, NDF-RT ingredients are con-nected to their mechanisms of action, physiologic effects,and therapeutics (indications and contraindications) andthe corresponding clinical drugs inherit those relations.NDF-RT clinical drugs are also connected to their corre-sponding VA drug classes. Therefore, if a given RxCUIitself is an NDF-RT concept (i.e., one of its sources isNDF-RT), we use it to find NDF-RT classification infor-mation. Otherwise, we traverse the relations provided byRxNorm to greedily identify the related NDF-RT ingredi-ents and clinical drugs, and then extract the associatedclassification information (see Figure 4).We obtain PT and SOC codes in MedDRA for PT en-tries in the AERS. For example, PT entries Anaemia ofchronic disease, Anaemia of malignant disease, andNephrogenic anaemia are mapped to corresponding PTcodes 10002073, 10049105, and 10058116. And thesecodes are aggregated under the SOC code 10005329(i.e., blood and lymphatic system disorders).-RT.Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 6 of 13http://www.jbiomedsem.com/content/5/1/36ExperimentsFor the experiments, we first gathered the AERS data thatare publicly available from 2004 to 2011 [32]. We used theMay 2012 release of RxNorm (which contains the map-pings to NDF-RT) and the 14.1 version of MedDRA. Weultimately produced an AERS-DM that is composed oftwo tables, one containing the normalized Drug-ADE in-formation and the other containing the aggregation infor-mation of the Drug-ADEs. We then analyzed the statisticsof the normalization and aggregation for AERS data in theAERS-DM. We also performed case studies to demon-strate the usefulness of the AERS-DM.We evaluated the normalization performance of MedExin two steps. For the first step, we randomly selected 200unique input drug names before MedEx-based annota-tions. We recruited three reviewers with medical back-ground who manually reviewed the 200 drug names andannotated them using the RxNorm codes. The version ofthe RxNorm used in the evaluation was the same as thatincluded in MedEx. A gold standard was generated afterthe reviewers achieved inter-agreements. Precision (P), Re-call (R), and F-measure (F) were calculated for these se-lected drug names, using P = TP/(TP + FP), R = TP/(TP +FN), and F = 2PR/(P + R), in which TP stands for TruePositive, FP stands for False Positive, and FN stands forFalse Negative. For the second step, we randomly selected100 drug names that failed to be normalized. Then weconfirmed whether or not the drug names are included inRxNorm. In addition, we checked to see if an unmatcheddrug name is a valid domestic or foreign drug by usingtwo drug resources. The first resource is Drugs@FDA, adrug dictionary providing FDA-approved brand and gen-eric drug information [13], and the second resource isDrugs.com, the largest independent drug informationwebsite available on the Internet [33].Additionally, we evaluated the validity of the algorithmused for identifying the mappings between RxNorm codes0%20%40%60%80%100%120%0 5Percentage of  unique AERS drug names  covered by RxNorm Frequency (lFigure 5 Distribution of unique drug names in AERS.and NDF-RT concepts. We randomly selected 20 AERSdrug names and manually checked the accuracy of themappings produced by our algorithms.ResultsGeneral statistics of normalization and aggregationAfter de-duplicating reports, according to the recom-mended method in the download files provided by FDA[34], the number of AERS records is reduced to 2,643,979from the original 3,874,965. The number of unique verba-tim drug names is reduced to 1,517,811 from the original1,700,925.For drug name normalization, 1,125,045 of 1,517,811(74%) AERS unique drug names were normalized to14,489 unique RxNorm concepts, of which 10,221 (71%)were classified in NDF-RT.For the ADE normalization, we mapped 14,740 existingMedDRA PT terms in the AERS to MedDRA codes, ac-counting for 76% of 19,294 total MedDRA PT terms.These MedDRA PT codes were then mapped to 26 Med-DRA SOCs.Figure 5 shows the distribution of unique drug names inthe AERS (normalized with the single field DRUGNAME),which follows Zifps Law. In other words, the more popu-lar the drug names are, the higher the chance those drugnames are to be normalized by the RxNorm codes.We classified the RxNorm concepts using the NDF-RT.Table 2 shows the coverage of AERS drugs by NDF-RTclasses. For example, 5,823 RxNorm concepts were mappedto 29 corresponding VA classes, accounting for 57% of allclassified RxNorm concepts and corresponding to 77% oftotal AERS reports. Table 3 shows the results of MedDRAPTcode aggregation by MedDRA SOCs.Statistics of AERS-DMThe AERS-DM includes two tables, as discussed above.There are 37,029,228 Drug-ADE records after de-10 15 20og2) of unique drug name in AERSTable 2 Coverage of AERS drugs by NDF-RT classesDrug_class_type1 Drug_class, No.2 RxNormConcept, No. in the AERS3 a%4 b%5Generic Ingredient Combinations 26 9,813 96 88Chemical Ingredients Class 16 9,331 91 81Therapeutic Intent 24 8,069 79 78Mechanism of Action 7 8,061 79 81Physiologic Effect 16 7,989 78 82VA Class 29 5,823 57 77FDA Established Pharmacologic Class (EPC) 66 3,049 30 46Therapeutic Category 7 2,880 28 50Pharmacokinetics 1 730 7 28Total 192 10,221 100 1001Drug_class_type represents various classes provided by NDF-RT as shown in Figure 2.2Drug_Class, No. is the number of direct subordinate classes of each Drug_class_type.3RxNorm concept, No. in the AERS shows the total number of RxNorm concepts in the AERS for each drug class type.4a% indicates the percentage of classified RxNorm concepts for each Drug_class_type, calculated as the division of RxNorm concept, No. in the AERS by10,221; a total of RxNorm concepts mapped to at least one NDF-RT class.5b% indicates the percentage of classified AERS reports, calculated as the division of the number of AERS reports assigned with the Drug_class_type by the totalAERS report number (2,643,979).Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 7 of 13http://www.jbiomedsem.com/content/5/1/36duplication. The number of unique pairs between RxNormconcepts and MedDRA codes is 4,639,613, and betweenRxNorm concepts and SOC pairs after ADE aggregation,205,725. Tables 4 and 5 show the top 10 most frequentpairs (Therapeutic Category, PT and Therapeutic Category,SOC), respectively.Evaluation resultsThe initial inter-annotator agreement on the 200 anno-tated drug names was 62.8%, a low agreement rate, maybedue to the various understandings in the rules for drugname normalization from the annotation training. It in-deed indicated the difficulty of the drug normalization inAERS. Comparing MedEx-based annotations with thegold standard generated from majority votes by three hu-man reviewers, we calculated the performance measures.TP, FP and FN were calculated as 138, 7 and 6 respect-ively. Recall, Precision and F-measure were then calculatedas 95.8%, 95.2% and 95.5% respectively, which is compar-able with performance measures in the original evaluationof MedEx [27]. The slight difference may be caused by dif-ferent evaluation contexts, and different drug name num-bers in gold standards, with discharge summaries (377),clinic notes (200) and AERS (200). The false positive waslow in the 200 annotated drug names, which was mainlydue to the false recognition of partial drug names, for ex-ample, both drug names ADONA (CARBAZOCROMESODIUM SULFONATE) and CLEXANE (HEPARNI-FRACTION SODIUM SALT) were normalized to so-dium (RxCUI 9853). The first, a foreign drug from Italyand Japan, is out of the scope of RxNorm, the second iscovered by RxNorm but not identified. As a result bothare falsely mapped by MedEx.Table 6 shows the evaluation results of 100 drugnames failed to be normalized by MedEx. A large por-tion of them (75) are due to problems associated withAERS records, including names not covered by RxNormsuch as foreign drug names, typographical errors, un-specified names (e.g., BLINDED PLACEBO), herbs (e.g.,ALOEELITE), domestic drugs (e.g., PANHEPRIN), newdrugs (e.g., HIZENTRA, approved by the FDA in 2010),and non-drugs (e.g., RADIATION THERAPY). A smallportion of them (25) that failed to be mapped are due toMedEx. The evaluation results revealed several issuesrelated to drug name normalization. First, the public re-lease of MedEx (version 2.0) is in an executable format,which prevents the use of the latest version of RxNormfor drug name normalization. For example, the drugname SAPHRIS did not have a match using MedExbecause it is included in RxNorm in the 2012 versionbut not in the 2008 version (used in MedEx). Second,RxNorm does not contain foreign brand names since it isintended to cover drugs prescribed in the United States.Third, we found that some of the records in AERS containunspecified names. An example is the name BLINDEDPLACEBO, which is used as a drug name and makes thenormalization infeasible. We would suggest that incorpor-ating thorough normalization at the point of data entry isdesired, so as to improve the data quality for data mining.Manual evaluation shows the greedy algorithm used tofind mapping between RxNorm and NDF-RT is 100% valid.Case studies for AERS-DMAs described in the section above, the study produced anAERS-DM containing normalized and aggregated AERSreporting data. To demonstrate the usefulness of theAERS-DM, we used the AERS-DM to analyze the NDF-Table 3 Aggregation of MedDRA PT codes by MedDRA SOCsMedDRA SOC Code MedDRA SOC Terms PT Codes, No.1 AERS Reports, No. (%)210018065 General disorders and administration site conditions 595 875,070 (15.6)10029205 Nervous system disorders 729 570,448 (10.2)10017947 Gastrointestinal disorders 699 447,243 (8.0)10022891 Investigations 2,748 407,270 (7.3)10037175 Psychiatric disorders 453 329,198 (5.9)10022117 Injury, poisoning, and procedural complications 697 311,952 (5.5)10038738 Respiratory, thoracic, and mediastinal disorders 437 287,425 (5.1)10028395 Musculoskeletal and connective tissue disorders 365 276,784 (4.9)10040785 Skin and subcutaneous tissue disorders 390 273,674 (4.9)10021881 Infections and infestations 1,361 267,741 (4.8)Infections and infestations10007541 Cardiac disorders 283 237,037 (4.2)10047065 Vascular disorders 595 195,591 (3.5)10027433 Metabolism and nutrition disorders 236 160,134 (2.9)10038359 Renal and urinary disorders 288 132,055 (2.4)10029104 Neoplasms benign, malignant and unspecified (incl. cysts and polyps) 1,293 129,227 (2.3)10005329 Blood and lymphatic system disorders 218 122,276 (2.2)10015919 Eye disorders 462 114,312 (2.0)10042613 Surgical and medical procedures 1,176 89,005 (1.6)10019805 Hepatobiliary disorders 162 79,067 (1.4)10038604 Reproductive system and breast disorders 381 73,431 (1.3)10021428 Immune system disorders 111 64,127 (1.1)10041244 Social circumstances 189 48,077 (0.9)10036585 Pregnancy, puerperium and perinatal conditions 182 36,959 (0.7)10013993 Ear and labyrinth disorders 75 30,174 (0.5)10010331 Congenital, familial, and genetic disorders 833 21,244 (0.4)10014698 Endocrine disorders 129 18,053 (0.3)1PT Codes, No. shows the number of PT codes for each SOC.2AERS Reports, No. (%) shows the number of AERS reports and the percentage among the overall AERS report number for each SOC, where one report maycontain several SOCs associated with several drugs.Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 8 of 13http://www.jbiomedsem.com/content/5/1/36RT drug class Pharmacokinetics and their correspond-ing ADE categories represented by the MedDRA SOCs.Specifically, we joined the two tables in the AERS-DMand retrieved the AERS reports under seven existingpharmacokinetics classes. We then analyzed the data,including their corresponding MedDRA SOC-basedADE categories.Figure 6 shows a profile of seven pharmacokineticsclasses in the AERS-DM. The bars represent the totalnumber of AERS reports for each individual pharmaco-kinetics class and the line represents the total drug num-bers under each pharmacokinetics class. This figureillustrates that most drugs with pharmacokinetics classinformation reported in the AERS are relevant to RenalExcretion and Hepatic Metabolism. We also foundthat the number of AERS reports is disproportional tothe number of drugs for some pharmacokinetics classes.For example, the class Hepatic excretion containsfewer drugs than the class Fecal excretion but hasmore AERS reports. The result indicates that the drugsin the Hepatic excretion class may be associated withmore AERS reports than the Fecal excretion class, andinteresting etiology knowledge may be found throughfurther mining with disproportionality metrics and otherdata sources, including EHRs.Figure 7 shows a profile of 26 MedDRA SOC-basedADE categories that corresponds to the seven pharma-cokinetics classes in the AERS. The bars represent thetotal number of AERS reports for each individual SOCand the line represents the total number of drugs associ-ated with each SOC category. The figure illustrates thatthe top five most frequent ADE categories are GeneralTable 4 Therapeutic category and PT pairsCo-occurrence, No. Therapeutic Category, Codes (Terms) PT, Codes (Terms)31,858 2225 (Central Nervous System Agent) 10028813 (Nausea)30,824 2225 (Central Nervous System Agent) 10013709 (Drug ineffective)24,922 882 (Antirheumatic Agent) 10022086 (Injection site pain)23,633 882 (Antirheumatic Agent) 10028596(Myocardial infarction)20,240 2095 (Cardiovascular Agent) 10028813 (Nausea)18,435 2095 (Cardiovascular Agent) 10013968 (Dyspnoea)15,695 4703 (Gastrointestinal Agent) 10028813 (Nausea)11,709 4703 (Gastrointestinal Agent) 10012735 (Diarrhoea)10,509 884 (Anti-infective Agent) 10028813(Nausea)10,505 884 (Anti-infective Agent) 10037660 (Pyrexia)10,022 988 (Antineoplastic Agent) 10012735 (Diarrhoea)8,865 988 (Antineoplastic Agent) 10011906 (Death)100 106571 (Diagnostic Agent) 10022061(Injection site erythema )69 106571 (Diagnostic Agent) 10028813(Nausea)Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 9 of 13http://www.jbiomedsem.com/content/5/1/36disorders and administration site conditions, Nervoussystem disorders, Gastrointestinal disorders, Investi-gations, and Respiratory, thoracic, and mediastinal dis-orders. Similarly, we found that the number of AERSreports is disproportional to the number of drugs forsome MedDRA SOC categories. The result may revealsome important areas of ADE surveillance for post-marketing drugs if combined with prescription informa-tion as the denominator.DiscussionIn addition to the significance described in the Introductionsection above, the present study was also partially moti-vated by our previous work on building a standardizedTable 5 Therapeutic category and SOC pairsCo-occurrence, No. Therapeutic Category, Codes (Terms)133,936 2095 (Cardiovascular Agent)90,959 2095 (Cardiovascular Agent)194,311 2225 (Central Nervous System Agent)175,172 2225 (Central Nervous System Agent)129,953 882 (Antirheumatic Agent)61,386 882 (Antirheumatic Agent)76,007 4703 (Gastrointestinal Agent)69,825 4703 (Gastrointestinal Agent)65,105 884 (Anti-infective agent)41,900 884 (Anti-infective agent)53,478 988 (Antineoplastic Agent)35,625 988 (Antineoplastic Agent)403 106571 (Diagnostic Agent)251 106571 (Diagnostic Agent)knowledge base of ADEs known as ADEpedia, in whichwe intended to integrate and normalize known ADEknowledge from disparate ADE datasets (e.g., the FDAstructured product labels, and the UMLS) [35,36]. Weconsider that the FDA AERS reporting database isanother important data source for ADE knowledgediscovery and the normalization of both the drug andADE names is the first important task we need to tacklewith.Drug and ADE aggregationNDF-RT is a major national source of drug classificationinformation, providing multi-axial classifications such asphysiological effect, mechanism of action, etc. It has provedSOC, Codes (Terms)10018065 (General disorders and administration site conditions)10029205 (Nervous system disorders)10018065 (General disorders and administration site conditions)10029205 (Nervous system disorders)10018065 (General disorders and administration site conditions)10029205 (Nervous system disorders)10018065 (General disorders and administration site conditions)10017947 (Gastrointestinal disorders)10018065 (General disorders and administration site conditions)10029205 (Nervous system disorders)10018065 (General disorders and administration site conditions)10017947 (Gastrointestinal disorders)10018065 (General disorders and administration site conditions)10040785 (Skin and subcutaneous tissue disorders))Table 6 Reasons for non-normalization of drug names byMedExReasons No.MedEx reasons 25AERS entry reasons 75Foreign brand names 41Typo 19Unspecified names 8Herbs 4Uncovered domestic drug 1New drug 1Non-drug 1Total 100Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 10 of 13http://www.jbiomedsem.com/content/5/1/36to be capable of representing medications in clinical set-tings. For example, Rosenbloom et al. [37] investigated thecoverage of the Physiologic Effects hierarchy in NDF-RTand found this category to be sufficient for classifying med-ications. Zhu et al. [38] used the FDA Established Pharma-cologic Class in NDF-RT to profile Structured ProductLabelling for clinical applications.On the other hand, the applicability of NDF-RT andRxNorm for clinical drug classification was explored, andthe imperfect mappings between RxNorm and NDF-RTand incomplete drug classification were evidenced by sev-eral studies [22,24,25]. Palchuk et al. [25] used the NDF-RTs drug class tree to organize RxNorm into a hierarchyand evaluated this mapping using data from EHRs. Pathaket al. [22] investigated the applicability of RxNorm andNDF-RT for representation and classification of medica-tion data from EHRs using the NLMs NDF-RT web ser-vices API for NDF-RT drug class assignment. Both of theabove studies were limited to the Drug Products by VAClass hierarchy under Pharmaceutical Preparations,0100000200000300000400000500000600000AERS report number (bar)Figure 6 A profile of pharmacokinetics classes in AERS.with no consideration of the multi-axial hierarchies. Inaddition, issues in mapping and classifying drugs fromRxNorm using the NDF-RTs multi-axial classificationwere investigated by Pathak and Chute [24]. In the study,they identified the issues in NDF-RT, including the lackof coverage of drug classes (chemical structure, mech-anism of action, physiologic effect, therapeutic intent,and pharmacokinetics) for clinical drugs, and suggestedthat the resolution would rely on the targeted improve-ment of NDF-RT. Thus, the existing studies on the clas-sification of RxNorm using NDF-RT are either aboutmulti-axial classification, based on the mapping be-tween the two ontologies, or limited to VA class basedon EHR data.In the context of standardizing the AERS data in thisstudy, we developed a systematic algorithm in which therich semantic connections within RxNorm were fullyutilized to build the mappings with the related conceptsin NDF-RT. The mappings were then used to aggregatethe AERS data under multi-axial classifications in NDF-RT. Different from those related studies, the presentstudy focuses on real-world data in the AERS and pre-sents a normalized AERS-DM data set together with allcorresponding drug classification information providedby NDF-RT for large-scale data mining purposes. Theevaluation results show that the greedy algorithm formaximum mapping to corresponding NDF-RT conceptsfrom RxNorm codes was valid. We believe that the map-ping method developed in our study could be useful inother similar context.We found that the MedDRA PT terms coding ADEsin the AERS changed over time. The used AERS dataranged from 2004 to 2011, and during that time periodthe MedDRA versions had been updated twice annually[39]. Some ADE codes in the AERS from the older Med-DRA edition had become obsolete and may hinder the0100200300400500600Drug number(line)Figure 7 A profile of MedDRA SOC for pharmacokinetics classes in AERS.Wang et al. Journal of Biomedical Semantics 2014, 5:36 Page 11 of 13http://www.jbiomedsem.com/content/5/1/36identification of Drug-ADE pairs. In the future, we willconsider whether making the PT terms consistent overthe years before conducting ADE detection (i.e., buildinga version control mechanism) may be useful for improv-ing data quality.In addition, other studies have identified a number ofissues related to the use of MedDRA. For example, thehierarchy problems and semantic reasoning incapabilityof MedDRA mitigate its usefulness for querying andanalyzing AERS data. SNOMED-CT, as the largest clin-ical terminology, can complement these disadvantages,with as many levels of hierarchy as are considered ap-propriate, and the semantic consistency in relationships[40]. In addition, using SNOMED-CT for ADE codingcan also achieve the integration of ADE data in theAERS with other health data sources, including EHRs. Afew studies have demonstrated the mappings betweenMedDRA and SNOMED-CT. For example, Bodenreider[41] proposed the mappings by leveraging the structureof SNOMED-CT for aggregation purposes. Mougin et al.[42] proposed to improve the mapping through an auto-matic lexical-based approach. A recent study [43] com-pared three methods using the Ontology of AdverseEvents (OAE), MedDRA, and SNOMED-CT in classify-ing the ADE terms associated with two vaccines. Amongthe three methods,, the OAE method provided betterclassification results. This initiative is inspiring in thefield of ADE detection for vaccines. However, given thatit is a newly emerging ontology with only 2723 terms,the coverage of the OAE is very limited. For the AERSdata normalization and aggregation, we consider that thewidely used SNOMED-CT would be a better candidate asan ADE terminology, and this will be one of our futureworks.Case studiesWe demonstrated the usefulness of the AERS-DM pro-duced by this study by analyzing the data set using thepharmacokinetics class. There are other class dimen-sions available for analysis, including physiologic effect,mechanism of action, and VA class, all of which comefrom the knowledge structure asserted in the NDF-RT.We believe that the knowledge asserted in the standardontologies will enrich the AERS-DM and enable themeaningful use of AERS data for ADE signal detectionand data mining.Implication of studyBased on AERS-DM, more efficient data mining and ADEdetection in individual drugs would be achieved and facili-tated. The reasons are three-fold as follows. First, havingenriched features of drugs and enlarged cohort informa-tion, AERS-DM could provide potential explanations forindividual differences in ADEs. Second, with the capabilityof large-scale ADE mining, comparative analysis of differ-ent drug classes could be explored, thus accumulatingADE evidence in the field of individual drugs. Third, pre-senting more meaningful organization of drug and ADEdata with standard terminologies, AERS-DM could be aplatform for deeper mining by further connecting withWang et al. Journal of Biomedical Semantics 2014, 5:36 Page 12 of 13http://www.jbiomedsem.com/content/5/1/36clinical notes, scientific literature, gene expression, pro-teomics and pharmacogenomics data, and various otherontologies. We believe that AERS-DM could be used toexplore the complex network among drugs and ADEs,and such research would bear far-reaching significance interms of the study paradigm of ADEs.LimitationsWe used only two drug ontologies (RxNorm and NDF-RT)and one ADE terminology (MedDRA) to normalize and ag-gregate AERS data. We believe additional investigations ofother standard terminologies, such as SNOMED-CT wouldbe beneficial in exploring the potential of standardizedAERS reporting data in data mining.ConclusionIn this study, we leveraged three biomedical ontologies?Rx-Norm, NDF-RT, and MedDRA?for normalizing and aggre-gating the AERS data and produced a standardized ADEdataset referred to as AERS-DM. With the normalizedcodes and aggregated features, the AERS-DM would be use-ful for the research community in the data mining field. Wewill continue to refine and optimize the AERS-DM and up-date it periodically in the future. In addition, we will investi-gate the integration of the AERS-DM data set with otherhealth data sources, such as EHR data, literature databases(e.g., Semantic Medline [44]) and other ontologies (e.g.,Drug Ontology [45]), for the purpose of promoting ADEdetection in individual drugs. Finally, we will leverageSNOMED-CT for standardizing the AERS data.Availability and requirementsDataset name: AERS-DMDataset home page: http://informatics.mayo.edu/adepedia/index.php/DownloadOperating system(s): Platform independentOther requirements: NoneLicense: GPLAny restrictions to use by non-academics: NoneRxNorm Files: http://www.nlm.nih.gov/research/umls/rxnorm/docs/rxnormfiles.htmlRxNorm concept relationships: http://www.nlm.nih.gov/research/umls/rxnorm/docs/2013/appendix1.htmlAbbreviationsAERS: The Adverse Event Reporting System; NDF-RT: National Drug File-ReferenceTerminology; MedDRA: Medical Dictionary for Regulatory Activities; PT: PreferredTerm; SOC: System organ class; HGLT: High-Level Group Term; HLT: High-levelterm; LLT: Lowest-level term; ADEs: Adverse drug events; AERS-DM: AERS datamining set; EHRs: Electronic health records; PRR: Proportional reporting ratio;ROR: Reporting odds ratio; IC: Information component; EBGM: Empirical Bayesgeometric mean; RRF: Rich Release Format; NLM: National Library of Medicine;RxCUI: RxNorm concept unique identifier; UMLS: Unified Medical LanguageSystem.Competing interestsThe authors declare that they have no competing interest.Authors contributionsAll co-authors are justifiably credited with authorship, according to the authorshipcriteria. Final approval is given by each co-author. In detail: LW design,development, analysis of data, interpretation of results, and drafting of themanuscript; GJ  conception, interpretation of results, and critical revision of themanuscript; DL  analysis of data; HL  conception, design, development,interpretation of results, and critical revision of manuscript.AcknowledgementsLiwei Wang is the recipient of a scholarship granted by the State ScholarshipFund from the China Scholarship Council. The study was also supported byNSF grant ABI:0845523 and NIH grant R01LM009959A1.Received: 13 June 2013 Accepted: 23 July 2014Published: 12 August 2014JOURNAL OFBIOMEDICAL SEMANTICSNichols et al. Journal of Biomedical Semantics 2014, 5:1http://www.jbiomedsem.com/content/5/1/1DATABASE Open AccessNeuroanatomical domain of the foundationalmodel of anatomy ontologyB Nolan Nichols1*, Jose LV Mejino1, Landon T Detwiler1, Trond T Nilsen1, Maryann E Martone2, Jessica A Turner3,Daniel L Rubin4 and James F Brinkley1AbstractBackground: The diverse set of human brain structure and function analysis methods represents a difficultchallenge for reconciling multiple views of neuroanatomical organization. While different views of organization areexpected and valid, no widely adopted approach exists to harmonize different brain labeling protocols andterminologies. Our approach uses the natural organizing framework provided by anatomical structure to correlateterminologies commonly used in neuroimaging.Description: The Foundational Model of Anatomy (FMA) Ontology provides a semantic framework for representingthe anatomical entities and relationships that constitute the phenotypic organization of the human body. In thispaper we describe recent enhancements to the neuroanatomical content of the FMA that models cytoarchitecturaland morphological regions of the cerebral cortex, as well as white matter structure and connectivity. This modelingeffort is driven by the need to correlate and reconcile the terms used in neuroanatomical labeling protocols. Byproviding an ontological framework that harmonizes multiple views of neuroanatomical organization, the FMAprovides developers with reusable and computable knowledge for a range of biomedical applications.Conclusions: A requirement for facilitating the integration of basic and clinical neuroscience data from diversesources is a well-structured ontology that can incorporate, organize, and associate neuroanatomical data. Weapplied the ontological framework of the FMA to align the vocabularies used by several human brain atlases, andto encode emerging knowledge about structural connectivity in the brain. We highlighted several use cases ofthese extensions, including ontology reuse, neuroimaging data annotation, and organizing 3D brain models.Keywords: Data integration, Neuroanatomy, Neuroscience, Ontology, Brain atlas, Neuroinformatics, Informationretrieval, mriBackgroundLarge-scale human brain imaging initiatives are generat-ing Big Data to characterize normal and neuropsychi-atric brain structure and function. The AlzheimersDisease Neuroimaging Initiative (ADNI, [1]), HumanConnectome Project (HCP, [2]), NKI-Rockland Sample[3], and others provide researchers with unprecedentedaccess to massive amounts of shared neuroimaging data.While tools and methods are available to support thevisualization [4-8], data management [9-13] and analysis[14-17] of shared or privately collected neuroimaging* Correspondence: nolan.nichols@gmail.com1University of Washington, Seattle, WA, USAFull list of author information is available at the end of the article© 2014 Nichols et al.; licensee BioMed CentralCommons Attribution License (http://creativecreproduction in any medium, provided the ordata, these tools use different approaches to define, seg-ment, and label neuroanatomical structures.One important component of research in this domaininvolves the development of digital brain atlases, whichprovide both a template brain and neuroanatomical la-bels in a standard coordinate system. Imaging data fromindividual participants are aligned to the template andbrain region labels are propagated over to provide con-text to observed features in the data (e.g., location of ac-tivation foci). Atlases (i.e., the brain template andanatomical label pair) can be based on manual or auto-matically labeled brain regions that are derived usingvolume-based or surface-based methods. Each brainatlas develops, adopts, or refines an anatomical labelingprotocol [18-24]. As a result, the labeling protocols usedto define the boundaries of neuroanatomical regions canLtd. This is an open access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly cited.Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 2 of 13http://www.jbiomedsem.com/content/5/1/1vary widely across brain atlases, so collections of ana-tomical entities from different labeling schemes do notalways stand in a relation that allows a one-to-one map-ping. Thus, data annotated with labels from different at-lases are difficult to compare based on labels alone.Previous efforts to reconcile terminologies in neuroim-aging, referred to as the brain atlas concordance problem,have taken both quantitative and qualitative approaches.Taking a quantitative, bipartite graph approach, Bohlandet al. demonstrated that different brain atlas labelingschemes lack a high degree of spatial concordance whencomparing labels that seemingly refer to the same anato-mical structure [25]. Qualitative approaches, such as ourown, organize anatomical labels through synonymy, rela-tions, and class hierarchies that provide practical utility(e.g., information retrieval and data integration) withoutresolving fine-grained spatial discrepancies. These ap-proaches are complementary and both will be necessary inidentifying a satisfactory solution to the brain atlas con-cordance problem.To improve our symbolic model of the brain atlas con-cordance problem the labeling protocols from each brainatlas need to be made explicit. However, the labelingprotocols that define anatomical structure boundaries inbrain atlases are generally published in natural language(i.e., as a manuscript) and lack the term definitions andrelationships provided by a machine-readable ontologyframework. Neuroimaging data and information encodedby these terms cannot be accurately interpreted, com-pared, correlated and applied across different studies. Asimilar standardization issue faces the development ofwhite matter connectivity atlases [26,27], in which ourunderstanding of human brain connectivity is rapidlyevolving. As new white matter analysis methods and la-beling protocols are developed, a proliferation of termsto label newly identified structures in white matter at-lases will likely occur.A robust semantic framework is needed to explicitlyrepresent the anatomical labels from different atlasesusing relationships that describe anatomical structure.Our goal is to provide such a framework for humanneuroimaging that will facilitate the integration andharmonization of data registered to standard coordinatesystems with labels for structures in human brainatlases.Approaches to labeling brain structuresWe selected brain atlases that are widely used in the hu-man brain mapping community and harmonized theterms used in each atlas labeling scheme with the ana-tomical structures modeled in the Foundational Modelof Anatomy (FMA) Ontology. In this section, we providea summary of brain labeling protocols that describe theanatomical knowledge and spatial relationships encodedin the Talairach Daemon, Desikan-Killiany (i.e., FreeSur-fer), and Anatomical Automatic Labeling (AAL) atlases,as well as NeuroLex. We conclude with a proposal forharmonizing all atlas and NeuroLex terms with classesin the FMA ontology.Talairach daemon labelsThe Talairach Daemon (TD) is an information systemthat provides a mapping between 3D coordinates (i.e.,Talairach coordinates) and specific brain structure labels[28]. It is a digital representation of the original Talair-ach atlas [19] that is hierarchically organized into fivelevels:1. Hemisphere2. Lobe3. Gyrus4. Tissue type5. Cell typeFor example, the label Right Cerebrum.Temporal Lobe.Inferior Temporal Gyrus.Gray Matter.Brodmann area 20represents a number of 3D coordinates in the Brodmannarea 20 cell-type level, the gray matter tissue-type leveland so on. While this approach has been broadly appliedin human brain mapping, there are limitations when nor-malizing patient MRI scans due to natural morphologicaldifferences between individuals.Desikan-Killiany atlasThe Desikan-Killiany (DK, [29]) atlas is a gyral, surface-based parcellation scheme for labeling anatomical MRIscans. The anatomical labeling protocol was manuallyapplied to 40 MRI scans to build a template brain withlabels for 34 cortical regions of interest (ROI) per hemi-sphere. This atlas is packaged with the FreeSurfer MRIdata analysis package [30] that provides researchers withaccess to a variety of image processing tools that in-cludes labeling anatomical ROIs with a predefined set ofterms.Automated anatomical labelingThe Automated Anatomical Labeling (AAL) brainatlas provides labels for 90 anatomical regions ofinterest (45 per hemisphere) from a single participantusing magnetic resonance imaging (MRI) [20]. Ana-tomical structures (45 per hemisphere) were identifiedin a high-resolution MRI by manually tracing struc-tures in each slice of a 3D volume. The AAL Toolboxfor the Statistical Parametric Mapping Matlab package[17,31] provides researchers with a method for label-ing brain regions in their data using the AAL proto-col and corresponding vocabulary.Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 3 of 13http://www.jbiomedsem.com/content/5/1/1NIFSTD and NeuroLexThe Neuroscience Information Framework (NIF) stand-ard ontologies (NIFSTD) are developed to provide aconsistent source of terminology for neuroscience con-cepts [32]. NIFSTD is not a brain labeling protocol noris it tied to a particular spatial arrangement of brain re-gions, but is a collection of brain region labels andinter-relationships. Neurolex also represents a generalmammalian hierarchy of brain parts, with each struc-ture assigned a taxon rank at which it is generally con-sidered to hold, whereas other ontologies, like theFMA, are more species specific. NIFSTD is a formalontology constructed through the import of commu-nity ontologies with specific extensions for neuro-science, covering the major domains of neuroscience[32,33]. For community contributions, NIF maintainsthe Neurolex lexicon, where each entity within theontology is exposed as a wiki page (http://neurolex.org),built using the Semantic Media Wiki Platform. Entitiesmigrate from Neurolex into the more formal NIFSTDontologies [33].An important feature of the project is to clearly and ex-plicitly define all of the terms that are used to describedata (e.g., anatomical terms, techniques, organism names).The NIF gross anatomy module was largely based on theNeuroNames hierarchy [34-36], re-coded in the WebOntology Language (OWL), but has been extensivelymodified through contributions to Neurolex. Neurolexserves as a community platform where those with minimalknowledge of building ontologies can still contribute theirexpertise. Through programs such as the Neuron Registryproject of the International Neuroinformatics Coordinat-ing Facility (http://incf.org), Neurolex is growing into asignificant knowledge base for neuroscience. However,NeuroLex does not currently provide the framework ne-cessary to correlate the terms from different brain labelingschemes.Our approach: the Foundational Model ofAnatomy ontologyThe Foundational Model of Anatomy Ontology (FMA) [37]is an open source reference ontology for the domain of ana-tomy that takes into account, at all biologically salient levelsof organization, the entities and their spatio-structural rela-tions which constitute and form the structural phenotype ofvertebrates with a special emphasis on the human organism.It is based on a unifying theory that explicitly defines ana-tomy and its content from the structural point of view. Inparticular, it provides a framework that can incorporate andaccommodate all entities under the purview of the anatomydomain.The FMA is implemented as a computable informa-tion artifact and is primarily intended for developers ofterminologies and application ontologies [38] in clinicalmedicine and biomedical research that require anatom-ical knowledge. Ontologists primarily value its merits be-cause it is both broader and more fine-grained thanextant anatomy texts or terminologies. For example, theFMA models both abstract, high-level concepts and leaf-level, fine grained concepts such as Material anatomicalentity and Brodmann area 1 of left postcentral gyrus,respectively. This approach is not entirely consistentwith the tradition-based representation of anatomy thatclinical practitioners and biomedical researchers aretaught in their training. Therefore, the benefits the FMAoffers to end users are best realized through derived ap-plication ontologies [38,39] and biomedical software thatutilize anatomical knowledge.The principled framework provided by the FMA isflexible enough to capture the intended semantics of ter-minologies developed for more specific purposes. Forexample, application ontologies derived from the FMA(e.g., RadLex [40,41]) can be used to reconcile prevalentviews of anatomy (e.g., radiologists or anatomy teachers)with an ontological representation of biological struc-ture. Thus, knowledge extracted from the FMA can beabstracted to a level that is familiar to individuals in agiven domain. The FMA can also incorporate annota-tions on anatomical entities that provide a mapping toexternal knowledge sources (e.g., ontologies or brainatlas terms), as well as a means to correlate betweenmapped terms. A central goal of this paper is to demon-strate how the FMA can be used to harmonize the grow-ing number of neuroscience terminologies and provide aframework and use cases for developing useful biomed-ical applications.Construction and contentAuthoring environmentThe FMA information artifact is implemented inFrames using Protégé, an authoring and editing envir-onment created by members of the Stanford BiomedicalInformatics Group [42]. Currently, the master copy ofthe FMA is stored in a relational MySQL database;however, many major biomedical ontologies (e.g., thosein the OBO Foundry [43]) are now developed usingOWL. OWL is now the standard language for describ-ing ontologies on the Web, and there are ongoingefforts to translate the FMA into OWL. Previous at-tempts succeeded in creating a version of the FMA inOWL Full [44], and more recently a subset of the FMAwas converted into OWL 2 [45]. The migration of theentire FMA into OWL 2 would greatly facilitate inte-gration and interoperability with external ontologiesand Semantic Web-related technologies. A strategy forthis conversion is in early development, thus outsidethe scope of this paper; however, even without thisNichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 4 of 13http://www.jbiomedsem.com/content/5/1/1migration, the current Frames version allows us to rec-oncile existing neuroanatomy terminologies.Enhancing neuroanatomy content in the FMAThe neuroanatomical content of the FMA was enhancedwith detailed modeling for cerebral hemisphere brain la-beling schemes, cerebral sulci, white matter structures,and neural connectivity relationships. These enhance-ments were designed to support use cases in humanbrain imaging by incorporating four major terminolo-gies, described above, that are widely used for annotatingneuro-related data (i.e., Talairach, Desikan-Killiany, AAL,NeuroLex). Our goal was to augment the FMA with thespatio-structural properties needed to represent differentbrain labeling schemes, while maintaining a single co-herent framework. By accommodating different viewswithin the same framework, we can use the enhancedFMA properties to correlate disparate brain labelingschemes. In the next section we describe the extensionin more detail. Note that in this paper we representFMA classes in Courier New font and relationships inbold italic.Cerebral hemisphere labelingFor a given anatomical labeling protocol, the terms usedto label or annotate brain structures may refer to neuro-anatomical entities at different levels of granularity orusing disparate features (e.g., morphological vs. cyto-architectural) to define the boundaries of specific struc-tures and their corresponding labels. This means thatthere may not be a direct or one-to-one correspondencebetween the terms from different atlases or terminologies.Figure 1 Protégé screen capture showing the slots for the different tsuperior frontal gyrus maps to Talairach and DK (Freesurfer).However, by mapping these terms to the FMA, the onto-logical structure of the FMA explicitly defines what en-tities are represented by the terms and how they correlatewith one another according to the properties and spatio-structural relationships established for them in the FMA.In this section we provide a technical overview of how theFMA was enhanced to accommodate and correlate differ-ent brain labeling protocols.To provide a mapping between different terminologieswe used Protégé to introduce property slots (e.g., sourcenames and unique identifiers) that link FMA classes to cor-responding annotation terms (Figure 1). The labels used ineach labeling scheme were manually correlated with a cor-responding FMA term. A list of potential mappings wassemi-automatically generated using direct string matching,synonymy mapping, lexical mapping, or by interpreting thesymbols and abbreviations used in a given labeling protocol(e.g. R for Right, ctx for cortex, etc.).Mapping NeuroLexUsing the term matching approach described above, theterms from NeuroLex were mapped to a subset of clas-ses from the FMA that model neuroanatomical know-ledge. For example, Right frontal lobe is a directstring match between FMA and NeuroLex, Inferiorhorn of the lateral ventricle in NeuroLex issynonymous with Temporal horn of lateral ventricle inthe FMA, and Lateral occipital cortex in Neu-roLex is a lexical match to Cortex of lateral occipitalgyrus in the FMA. The mappings were then manually in-corporated into the FMA by adding specific NeuroLexidentifiers to the newly defined NeuroLex_ID property.erminologies. In the example, the FMA class Gray matter of rightNichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 5 of 13http://www.jbiomedsem.com/content/5/1/1Mapping TalairachTalairach Daemon annotations explicitly represent fivelevels of partonomy where the level of granularity foreach neuroanatomical entity is denoted by a period. Forexample, the Talairach label Right Cerebrum.Frontal Lobe.Superior Frontal Gyrus.GrayMatter.Brodmann area 6 indicates a set of coordi-nates located on Right Brodmann area 6, on theRight superior frontal gyrus of the Rightfrontal lobe in the Right cerebral hemisphere.The actual neuroanatomical entity being represented here isBrodmann area 6 of right superior frontal gyrus, which ex-ists in the FMA and is therefore directly mapped to the cor-responding Talairach term.Where appropriate and necessary in the ontology, weadded new classes, properties and relations to completethe mappings between FMA classes and the different an-notation terms [46,47]. This is particularly true for ac-commodating and reconciling different labeling schemesfor the cerebral cortex. For example, the Talairachterm Right Cerebrum.Frontal Lobe.SuperiorFrontal Gyrus.Gray Matter.Brodmann area 6refers to an area in the gray matter of the right su-perior frontal gyrus that overlaps with Brodmann area6. Whereas the gyrus is subdivided into regions basedon topographical surface landmarks, Brodmann areasare regions defined on the basis of the underlyingcytoarchitecture or cellular and laminar organization.Although both types of regional partitions are in theFMA, neither Brodmann area 6 nor the gray matter ofthe right superior frontal gyrus had been partitioned toaccount for the overlap. We therefore reconciled bothmorphological and cytoarchitectural schemes into theFMA ontology with the following modeling pattern.First, we created a class for the Gray matter of thesuperior frontal gyrus, which overlaps with (i.e.,has_regional_part) Brodmann areas 6, 8, 9, 10 and 11(Figure 2). Second, we created a class for Brodmann area6 and model overlaps with the gray matter of the precentral,the superior frontal, the middle frontal, the inferior frontaland the medial frontal using has_regional_part relations(Figure 3).Going back to our Talairach example above, wemapped it to the new FMA class called BrodmannFigure 2 A listing of regional parts (i.e., Brodmann Areas) for the grayarea 6 of right superior frontal gyrus whichis_a Segment of Brodmann area 6 and a regional_-part_of both Right Brodmann area 6 and Graymatter of right superior frontal gyrus(Figure 4). And following the transitive part_of relationof Brodmann area 6 of right superior frontalgyrus up the FMA part hierarchy reveals that all thegranularity levels implicitly encoded in the Talairachlabel are explicitly represented in the part hierarchy ofthe FMA (Figure 5). The latter is the kind of informationthat the ontology can provide to facilitate automatedreasoning by any system.Mapping Desikan-Killiany and AALThe labels used in FreeSurfer with the Desikan-Killiany(DK) atlas contain abbreviations and acronyms such asCtx, lh and wm which mean Cortex, Left hemi-sphere and White matter, respectively. For examplethe term ctx-lh-postcentral maps to Gray matter ofleft postcentral gyrus. Many of the terms usedin DK (e.g. ctx-rh-inferiortemporal) and AAL (e.g. Tem-poral_Inferior_Right) are customized and specific only totheir respective projects. Therefore, some semantic inter-pretation is required to parse the meaning and interoper-ate with other atlas terminologies. In the FMA we providea semantic framework that explicitly declares the intendedmeanings of the terms used.We identified anatomical entities (i.e., classes) in theFMA that most closely correspond to a given brain atlaslabel. We then elaborated on the properties associatedwith each FMA class to provide additional relationshipsthat capture information necessary to correlate with thelabels from other brain atlases and NeuroLex (e.g., iden-tifiers, preferred names, etc.). From the above examples,ctx-rh-superiorfrontal from DK is mapped to the FMAclass Gray matter of right superior frontalgyrus and Frontal_superior_right from AAL to FMAclass Right superior frontal gyrus.Note that the structural entities represented by the dif-ferent terms are at various levels of granularity, withTalairach, FreeSurfer and AAL at the levels of Brodmannarea, gray matter of cortex, and gyrus, respect-ively. Furthermore a laterality attribute is specified for allthree representations as opposed to NeuroLex, which doesmatter of the superior frontal gyrus.Figure 3 A listing of regional parts (i.e., cortical gyri) that intersect Brodmann Area 6.Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 6 of 13http://www.jbiomedsem.com/content/5/1/1not require left/right attributes. However the FMA hasthe framework to correlate all the entities based on theirontological definitions and relationships as shown inFigure 6. In this example, the Talairach term is mapped tothe FMA class Brodmann area 6 of right superiorfrontal gyrus, a part_of Gray matter of rightsuperior frontal gyrus, the FMA class referencedby DK, which in turn is a part_of the AAL mapped entityRight superior frontal gyrus. The non-lateralized NeuroLex classes are then mapped via is_a re-lation to the lateralized entities represented in the otherterminologies (e.g., Right superior frontal gyrus(AAL) is_a Superior frontal gyrus (NeuroLex)).White matter and connectivity relationshipsThere are a growing number of human neuroimagingtechniques from the emerging field of connectomics[2,48,49] that can describe white matter connectivity at anincreasing level of detail. Parallel ontological representa-tions are required to capture and accommodate the newlyderived or updated knowledge models these methods pro-vide. This is necessary to establish precise and reliablestructural-functional correspondence between disparateFigure 4 Part relationships of Brodmann area 6 of right superior fronrepresentations of white matter structures. Using new andclassic neuroanatomical knowledge we have enhanced theFMA representation of white matter tracts and connectiv-ity relationships between gray and white matter structures.Partonomy of white matter structuresWe pursued a comprehensive spatio-structural representa-tion of white matter tracts, particularly relating to parton-omy and connectivity relationships. A good exampleaddressed by this approach relates to the common practiceof using the same term to represent both the entire tractand its segments, as in the case of the Corticospinaltract. In cases where only a very specific segment of thetract is to be identified, the indiscriminate use of a non-exclusive term for its annotation can lead to errors andinconsistencies, especially when machine-processing isinvolved. This can be avoided by properly declaring theparts of a structure with unique terms assigned to eachpart and only using the structure specific term for annota-tion. Figure 7 illustrates this approach using the Corti-cospinal tract (i.e., the complete structure) and all ofits named segments/parts.tal gyrus.Figure 5 Correlation of Talairach label to part relationships of Brodmann area 6 of right superior frontal gyrus in the FMA.Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 7 of 13http://www.jbiomedsem.com/content/5/1/1Granularity of connectivity relationshipsConnectivity between neuroanatomical entities entailsrelationships at different levels of granularity. Numerousterms have been used inconsistently to establish con-nectivity relationships between neurons, between nervefibers or tracts and between gray matter structures.Figure 6 An example of how terms from brain atlases and vocabularicorresponding class in the FMA hierarchy.In this paper we proposed and gave definitions tospecific connectivity types for different granular entities(Table 1).At the neuronal level, a neuron can synapse_with an-other neuron or a muscle fiber or a gland cell. Connect-ivity is at the subcellular level between the pre-synaptices (right, yellow) can be correlated by mapping to theFigure 7 Regional partition of the corticospinal tract from thebrain to the spinal cord.Table 1 White matter connectivity terms and definitionsInnervate A connectivity relation where a neurite of one neuroa region of a muscle cell or a gland cell.Synapse_with A connectivity relation where there is apposition bepostsynaptic membrane of one or more neurites ofform of neurotransmission is evident between themProjects_to A connectivity relation where individual axons compsynapse_with neurites or somas of a collection of nesynonymous with terminate_in.Projects_from A connectivity relation where individual axons compmore brain regions. This relation may be synonymouSends_output_to A subproperty of project_to relation where neurotrabrain regions.Receives_input_from A subproperty of project_from relation where neurobrain regions.Has_pathway A connectivity relation where a collection of neuronlocated in B via axons comprising the fiber tract fromNichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 8 of 13http://www.jbiomedsem.com/content/5/1/1membrane of a neurite of a neuron with a post-synapticmembrane of a neurite or soma of another neuron orwith a region of a muscle fiber or a gland cell. Whitematter structures at the nerve or tract-level (i.e., collec-tion of axons) projects_to and projects_from any regionof the neuraxis (i.e., a term referring to both brain andspinal cord). For example, the Dorsal segment ofsuperior longitudinal fasciculus (i.e., SLF I)projects_from Brodmann area 6 of superiorfrontal gyrus and projects_to Brodmann area 5 ofsuperior parietal lobule. Finally, we created tern-ary relationship types that model neural connectivitybetween any two regions of the neuroaxis connected bywhite matter. Gray matter structures receives_input_fromand sends_output_to other gray matter structures, asshown in Figure 8 for Putamen.The connectivity relationships we propose capture the es-sential levels needed to express how information is commu-nicated throughout the brain. However, several other effortsare working on the issue of neural connectivity relations.For example, the OBO Relation Ontology [50] proposes re-lations such as has_fasciculating_neuron_projection andaxon_synapses_in. As additional relationships are defined,the FMA will provide a framework to accommodate theseterms for further refinement of connectivity representation.Cerebral sulciAmong immaterial anatomical entities, particular atten-tion was directed to anatomical spaces such as the cere-bral sulci. Sulci are defined in different contexts,depending on the operational needs of the users. Insome labeling protocols, sulci are treated as 1-D linesthat serve as boundaries of gyri, whereas in surface-based parcellation models they are spaces or groovesthat surround the gyri. The PALS-B12 atlas from Caret[51], a significant labeling scheme of widespread utility,involves the use of sulci to identify and contour buriedn synapses with a neurite or a region of the soma of another neuron ortween the presynaptic membrane of a neurite of one neuron and theanother neuron or a region of a muscle cell or a gland cell and some.rising a fiber tract originating from one or more brain regionsurons located in one or more other brain regions. This relation may berising a fiber tract are parts of a collection of neurons located in one ors with originate_in.nsmission is sent from one brain region to one or more othertransmission is received by one brain region from one or more others located in brain region A sends_output_to a collection of neuronsbrain region.Figure 8 Connectivity relationships for the Putamen.Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 9 of 13http://www.jbiomedsem.com/content/5/1/1cortex among gyri. Here the term sulcus denotes a 3Dvolume, the segments of gyri located in the furrows. Wedisambiguated the representation of sulcus by treating itas an anatomical space and for the area of the gyrus inthe sulcus, we regarded it as an anatomical structurethat is part of the gyrus and classified it as sulcal seg-ment of a gyrus under Segment of gyrus of brain.As shown in Figure 9, the middle frontal gyrus consistsof several regional parts or segments, one of which isSulcal segment of middle frontal gyrus andthe rest are parts of the gyrus that are externally visible.With this approach, the entire middle frontal sulcus canFigure 9 Regional parts of the middle frontal gyrus. Sulcalsegment of middle frontal gyrus is the part located in the sulci.be modeled as belonging to two gyri  both the Sulcalsegment of middle frontal gyrus and Sulcal segment ofinferior frontal gyrus.Utility and discussionExtensions to the neuroanatomical axis of the FMA weremotivated by use cases in ontology development, knowledgeretrieval, and data integration. In this section we describeseveral use cases for our work and discuss how reusable andcomputable anatomical knowledge captured in the ontologycan be utilized to solve real-world problems.Neuroanatomical knowledge reuseThe FMA is a reference ontology that can be imported intoother ontologies as a way to reuse curated knowledge aboutanatomy [37]. RadLex [40] is an ontology composed ofstandard terms for the domain of radiology, including im-aging observations, characteristics, and techniques, as wellas diseases, radiology reporting terminology, and anatomy.For anatomy, RadLex incorporates a subset of the FMAthat is relevant to the radiological scale of analysis [41].RadLex also contains knowledge beyond anatomy thatenables additional radiology oriented use cases such as hu-man brain imaging. For example, a digital brain label canindicate the anatomical structure that a set of image coor-dinates pertains to in a brain template, whereas RadLexcan be used to describe key aspects of a neurological im-aging examination including modality, technique, visualfeatures, anatomy, findings, and pathology. By incorporat-ing neuroanatomical content from the FMA, RadLex en-ables rich dataset annotations and provides a means tocorrelate and integrate the findings with other externaldata and studies as discussed in the following section.Ontological knowledge retrievalTo leverage the knowledge we encoded into the FMAthe ontology can be accessed using a query engine. Forthis purpose we used the Query Integrator (QI) as anNichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 10 of 13http://www.jbiomedsem.com/content/5/1/1underlying technology [52] to query the neuroanatom-ical content of the FMA as represented in OWL-Full.The QI is a Web-based query management and executionsystem that enables queries over any Web-accessible dataor knowledge source (e.g. ontology). The QI supportsmultiple query languages, including SPARQL [53] for RDF[54] data sources. QI queries may be stored for reuse, exe-cuted via RESTful Web services, and chained together toform query pipelines. This latter capability allows the re-sults of ontology queries to be joined with data queries toanswer more interesting questions than are possible basedon the data alone.Dataset annotation and intelligent queryAs reported in Turner et al. [47], the FMA was used toannotate a large dataset of task-based functional MRI(fMRI) signal activations in subjects with schizophreniaand healthy subjects. The activation locations were an-notated with neuroanatomical labels from the TalairachDaemon [28,55]. These labels combined cytoarchitec-tural labels from one method for labeling brain regions,with morphological terms based on sulci and gyri. TheFMA was extended to include intersections betweenlabel pairs when regions overlap. For example, withinareas covered by the label Inferior temporalFigure 10 Screenshot of the 3D model asset manager component ofand the selected asset set on the left (AAL - Brain). Scenes are generatgyrus exist areas covered by the label Brodmannarea 20. Therefore, part_of the Inferior temporalgyrus is part_of Brodmann area 20, and part_of theInferior temporal gyrus is not in Brodmannarea 20. Conversely, Brodmann area 20 has partsthat are in the Inferior temporal gyrus and partsof Brodmann area 20 which are in other gyri. This ex-tension of the ontology in conjunction with a reasoningengine allowed novel questions to be asked about thedata.3D anatomical model managementIn biomedical education and research, 3D surface modelsare useful and commonplace. For example, a researcherusing the brain atlases described above may generateneuroanatomical surface models from a patients MRI,where each model is a different brain region. While thesemodels can be organized using naming conventions or dir-ectory structures, it may be more meaningful to annotatemodels using terms from the FMA. Similar to our workon annotating tabular datasets, the knowledge in the FMAcan be queried and used to reason about which 3D modelsto select and display in a 3D scene. We have developed aprototype scene generation system that implements thisidea and will allow users to create Web-based 3D scenesthe scene generator displaying a list of asset sets on the righted from the selected asset set based on queries to the FMA.Nichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 11 of 13http://www.jbiomedsem.com/content/5/1/1using the results of queries over the FMA or data sets an-notated with FMA identifiers (Figure 10).The system provides access to several biomedical 3Dmodel sets, including models generated from the brainatlases described above (e.g., AAL and DK) and allowsusers to upload their own models. Scenes can be con-structed by hand or using queries, and queries can beshared between users and customized using parameters.Since all scenes are rendered using WebGL, they caneasily be embedded within any website or Web-basedpublication. For example, users of this system can accessknowledge in the FMA to generate a scene showing allportions of the brain with blood supply from the middlecerebral artery or all structures connected by a givenwhite matter tract. Similarly, scenes can be generated todisplay different model sets of the same structures suchas 3D models of the left hemisphere in the DK(Figure 11) and AAL anatomical labeling schemes(Figure 12).This tool currently provides users with the ability tomanage 3D model sets, query annotated models, andvisualize query results as 3D scenes. As this tool maturesit will also incorporate support for building scenes fromconnectivity relationships, visualizing the correlation be-tween model sets, and displaying volumetric anatomicalimages. In addition, our tool provides model managementfeatures that facilitate sharing and reuse of model sets.These tools are also useful in validating and exploring theFMA and other terminologies, as well as model sets basedFigure 11 A view of the DK left hemisphere parcellation withthe left precuneus selected. A link to this scene can be foundat: http://purl.org/sig/docs/neurofma-jbs.Figure 12 A view of the AAL left hemisphere labeling with theleft superior temporal gyrus selected. A link to this scene can befound at: http://purl.org/sig/docs/neurofma-jbs.on these. Misalignments, missing structures, and issueswith coordinate sets readily become apparent in query-driven scenes and, in many cases, can be remedied fromwithin the tool and re-exported for use elsewhere. Simi-larly, visualizations can be composed that relate the struc-tures defined in different terminologies to one another. Asmentioned, this system is currently under development;we anticipate publishing and releasing it to the commu-nity in the near future. Links to any publications associ-ated with this tool will be available on the supplementarymaterials page (http://purl.org/sig/docs/neurofma-jbs).ConclusionsWe demonstrated that the framework provided by theFMA ontology can be extended to accommodate and cor-relate the terms used in three human brain labelingschemes and NeuroLex. We then used the enhanced FMAto highlight use-cases for neuroanatomical knowledge re-use and retrieval. The FMA was found to sufficiently cap-ture and clarify the relationships between differentNichols et al. Journal of Biomedical Semantics 2014, 5:1 Page 12 of 13http://www.jbiomedsem.com/content/5/1/1anatomical labeling schemes necessary to fulfill the usecases.As a result, the disciplined and principled approach inthe FMA lays the foundation for:1. An ontology-based standard for anatomical dataannotation2. Queries that can use the ontology to inferanatomical relationships in data3. Data visualization systems that incorporateanatomical knowledge4. A meta-atlas that harmonizes different brainlabeling protocols5. A unifying anatomical framework for integrating avariety of biomedical dataWhile this effort advances state-of-the-art knowledgerepresentations of human neuroanatomy, further work isneeded to address the brain atlas concordance problem.Symbolic representations of anatomical labeling schemesalone are not sufficient to model the spatial informationin brain atlases. Computational frameworks, such asproposed by Bohland, et al. [25], provide quantitativemeasures of spatial concordance but do not address theissue of lexical mappings. A hybrid framework that inte-grates quantitative information with ontologies wouldoffer a more comprehensive solution to reconcilingneuroanatomical labeling schemes. Additionally, thepurely structural approach taken by the FMA only ac-commodates anatomical descriptors, and the need forfunctional divisions of the brain calls for future develop-ment of a functional brain labeling ontology.Availability and requirementsLatest Release:Foundational Model Explorer (Online):http://sig.biostr.washington.edu/projects/fm/FMEFrames:http://sig.biostr.washington.edu/projects/fma/release/index.htmlOWL-Full:http://sig.biostr.washington.edu/share/downloads/fma/FMA_Release/alt/v3.2.1/owl_file/fma_3.2.1_owl_file.zipLicensing:Creative Commons Attribution 3.0: http://creative-commons.org/licenses/by/3.0AbbreviationsAAL: Automated anatomical labeling; FMA: Foundational model of anatomy;fMRI: Functional magnetic resonance imaging; MRI: Magnetic resonanceimaging; NIF: Neuroscience information framework; NIFSTD: Neuroscienceinformation framework standard ontology; OBO: Open biomedicalontologies; QI: Query integrator; RO: Relations ontology; ROI: Region ofinterest; TD: Talairach daemon.Competing interestsThe authors report no competing interests with the work described inthis manuscript.Authors contributionsBNN provided expertise on brain atlases, extracted connectivity informationfrom the literature, and coordinated use case efforts. JLVM is a primaryauthor of the FMA and manually incorporated terms into the FMA usingProtégé. TTN is the primary developer of the model management andvisualization application. MEM provided expertise on NIF, NeuroLex, andneuroanatomy. JAT provided expertise on neuroanatomical datasetannotation and brain imaging. DLR provided expertise on RadLex. JFBoversaw all research activities and provided expertise on the FMA and QIapplication. All authors reviewed, edited, and approved of the manuscript.AcknowledgementsThis work was supported in part by RC4 NS073008-01 (BNN , JFB), theNational Academies Keck Futures Initiative (BNN), 1R01MH084812-01A1 (JAT),RSNA-NIBIB HHSN268200800020C (DLR, JAT, JFB, LTD, JLVM) and DoD/USAMRMC GRANT10362280 (JFB, TTN). This work was conducted using theProtégé resource, which is supported by grant GM10331601 from theNational Institute of General Medical Sciences of the United States NationalInstitutes of Health.Author details1University of Washington, Seattle, WA, USA. 2University of California SanDiego, San Diego, CA, USA. 3Mind Research Network, Albuquerque, NM, USA.4Stanford University, Stanford, CA, USA.Received: 4 July 2013 Accepted: 24 December 2013Published: 8 January 2014JOURNAL OFBIOMEDICAL SEMANTICSDoulaverakis et al. Journal of Biomedical Semantics 2014, 5:13http://www.jbiomedsem.com/content/5/1/13RESEARCH Open AccessPanacea, a semantic-enabled drugrecommendations discovery frameworkCharalampos Doulaverakis1*, George Nikolaidis2, Athanasios Kleontas2,3 and Ioannis Kompatsiaris1AbstractBackground: Personalized drug prescription can be benefited from the use of intelligent information managementand sharing. International standard classifications and terminologies have been developed in order to provide uniqueand unambiguous information representation. Such standards can be used as the basis of automated decisionsupport systems for providing drug-drug and drug-disease interaction discovery. Additionally, Semantic Webtechnologies have been proposed in earlier works, in order to support such systems.Results: The paper presents Panacea, a semantic framework capable of offering drug-drug and drug-diseasesinteraction discovery. For enabling this kind of service, medical information and terminology had to be translated toontological terms and be appropriately coupled with medical knowledge of the field. International standardclassifications and terminologies, provide the backbone of the common representation of medical data while themedical knowledge of drug interactions is represented by a rule base which makes use of the aforementionedstandards. Representation is based on a lightweight ontology. A layered reasoning approach is implemented where atthe first layer ontological inference is used in order to discover underlying knowledge, while at the second layer atwo-step rule selection strategy is followed resulting in a computationally efficient reasoning approach. Details of thesystem architecture are presented while also giving an outline of the difficulties that had to be overcome.Conclusions: Panacea is evaluated both in terms of quality of recommendations against real clinical data andperformance. The quality recommendation gave useful insights regarding requirements for real world deploymentand revealed several parameters that affected the recommendation results. Performance-wise, Panacea is comparedto a previous published work by the authors, a service for drug recommendations named GalenOWL, and presentstheir differences in modeling and approach to the problem, while also pinpointing the advantages of Panacea.Overall, the paper presents a framework for providing an efficient drug recommendations service where SemanticWeb technologies are coupled with traditional business rule engines.Keywords: Ontologies, Decision support, Rule-based reasoning, Drug recommendationsBackgroundOne of the health sectors where intelligent informationmanagement and information sharing compose valuablepreconditions for the delivery of top quality services ispersonalized drug prescription. This is more evident incases where more than one drug is required to be pre-scribed, a situation which is not uncommon, as drug inter-actions may appear. The problem is magnified by the wide*Correspondence: doulaver@iti.gr1Centre for Research and Technology Hellas, Information TechnologiesInstitute, Thessaloniki, GreeceFull list of author information is available at the end of the articlerange of available drug substances in combinationwith thevarious excipients in which the former are present.If one takes into account that there exist more than18,000 pharmaceutical substances, including their excip-ients, then it is clear that the continuous update ofhealth care professionals is remarkably hard. Over this,the extensive literature makes discovery of relevant infor-mation a time consuming and difficult process, while thedifferent terminologies that appear between sources addmore burden on the efforts of medical professionals tostudy available information.Semantic Web technologies can play an important rolein the structural organization of the available medicalinformation in a manner which will enable efficient© 2014 Doulaverakis et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedicationwaiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwisestated.Doulaverakis et al. Journal of Biomedical Semantics 2014, 5:13 Page 2 of 10http://www.jbiomedsem.com/content/5/1/13discovery and access. Research projects funded forenabling Semantic Web technologies in the diagnosis andtherapeutic procedures exist such as REMINE [1], PSIP[2], NeOn [3] and Active Semantic Documents [4] orworks such as [5], but they dont fully address the prob-lem of automated drug prescription using drug-drug anddrug-disease interactions.Rule-based approaches have been proposed for address-ing issues relating to biomedical ontologies research. Itis common for ontologies written in expressive Seman-tic Web languages such as OWLa, to not be able tohandle all requirements for capturing the knowledge inseveral biomedical and medicine domains. As a methodfor enriching the expressiveness of ontology languages,researchers have proposed the use of rules which act uponthe defined ontological knowledge. According to [6], rulesare helpful in the following situations relating to biomed-ical ontologies: defining standard rules for chainingontology properties, bridging rules for reasoning acrossdifferent domains, mapping rules for defining map-pings between ontologies entities and querying rules forexpressing complex queries upon ontologies. The authorgives a thorough review of RuleMLb and SWRLc, the twomajor ontology rule languages, the available rule forma-tion tools and the reasoners. Golbreich et al. [7] makesuse of the outcomes of the previous paper to showcasethe need for rules in biomedical applications with a usecase of a brain anatomy definition, where a brain struc-ture ontology is defined in OWL but rules describing therelationships between the properties and entities that areneeded for correct annotation of MRI images. Anotherwork citing the need for semantically enriched rules,where an ontology is coupled with SWRL rules for anno-tating pseudogenes and answering research questions, hasbeen proposed in [8]. All the above papers present theneed for extending ontologies with rules in order capturethe knowledge of complex biomedical domains.The paper presents Panacea, a semantic-enabled sys-tem for discovering drug recommendations and inter-actions. Panacea is based on experiences and lessonsdrawn from the development of GalenOWL [9], a sim-ilar system which had Semantic Web technologies inits core. As such, Panacea can be considered the evo-lution of GalenOWL in terms of design and scalability.Panacea makes use of established and standardized med-ical terminologies together with a rich knowledge baseof drug-drug and drug-diseases interactions expressed asrules. Panacea is implemented having in mind scalabil-ity, completeness of results and responsiveness in queryanswering.Standard terminologies and semantic webStandard terminologies and classifications in the med-ical domain have been developed in order to supportinformation sharing and exchange and to enable a com-mon expression of key concepts. Such is the case forexample for the ICD-10d (International Classification ofDiseases) index of theWorld Health Organization (WHO)where it is used to classify diseases and other health prob-lems recorded on many types of health and vital recordsacross many countries. The classification is also used forstoring and retrieval of diagnostic information and forthe compilation of national statistics reports by the WHOmembers.On the other hand, ontologies and the Semantic Webenable a common representation and understanding ofknowledge. Ontologies can effectively capture a domainsknowledge by specifying the definitions of terms bydescribing their relationships with other terms. A rea-soner can be employed upon an ontology in order touncover implicitly defined information while the expres-siveness of ontologies can be further enriched by formu-lating rules in standard rule languages, such as RuleMLor SWRL that are mentioned above, thus not sacrificinginteroperability.Panacea aims to combine and make use of the benefitsof standard terminologies and Semantic Web technolo-gies by enabling inference and rule-based reasoning onontologies that have been expressed using the medicalstandards.MethodsArchitecture and functional designThe purpose of Panacea is to provide drug prescriptionrecommendations based on a patients medical record, i.e.advise physicians to prescribe medications according tothe drugs active substance indications and contraindica-tions. For details regarding the initiative that triggereddevelopment of Panacea and the initial medical and phar-maceutical data that were available, the reader is encour-aged to read [9].Panacea follows a layered reasoning process which isdepicted in Figure 1. During the start-up of the system,the medical terminologies, namely ATCe (AnatomicalTherapeutic Chemical), UNIIf (Unique Ingredient Identi-fier), ICD-10, ICTVg (International Virus Taxonomy) andcustom encodings, are transformed to semantic entities,using an appropriate vocabulary, and the initial ontol-ogy is constructed. The ontology binds to a reasoner toinfer relations such as inheritance and unions. This pro-cess is performed once offline during initialization and theknowledge base is available to the system for further uti-lization. In order to get recommendations in Panacea, apatient instance with the appropriate medical record datais created and fed to the knowledge base. The reasoningprocess enriches the patient instance with inferred knowl-edge, thus making it explicit. On this enriched instance,and by utilizing a different reasoning process, the set ofDoulaverakis et al. Journal of Biomedical Semantics 2014, 5:13 Page 3 of 10http://www.jbiomedsem.com/content/5/1/13Figure 1 Panacea framework architecture and data flow.medical rules is applied upon. The result of this final stageof rule-based reasoning is the recommendations list whichcan be retrieved through SPARQLh querying.A key characteristic of the suggested architecture isthat, regarding second level reasoning, the framework canutilize any rule-based reasoner or rule engine. Since allthe inferred knowledge of the medical definitions andpatient data is materialized in the knowledge base, themedical rules can be expressed and loaded in an appro-priate rule engine. The rule engine could be an ontologyreasoner or a business rule manager with appropriate cus-tomizations in the data structures. This approach helps inbringing together the best of both worlds: semantic andmeaningful representation of data using Semantic Webtechnologies and the maturity of traditional rule enginesin efficiently handling complex and large amounts ofrules.Use case scenarioIn order to demonstrate the benefits of the proposedsemantic recommendation system, a use case regardinga possible scenario is described below. The codings inthe parentheses represent the corresponding ICD-10 andATC codes of diseases and drugs, respectively.An elder man visits his family doctor complainingfor pain in his right lower back and abdominal regionwhich is accompanied with fever. After appropriate clin-ical examination, he is diagnosed with right pyelonephri-tis (ICD-10: N11.0). According to the patients medicalhistory, he is suffering from chronic atrial fibrillation(ICD-10: I48.2) for which he receives clopidogrel (ATC:B01AC04), vertigo (ICD-10: H81.49) for which he receivescinnarizine (ATC: N07CA02), high arterial blood pressure(ICD-10: I10) for which he receives candesartan (ATC:C09CA06) and amlodipine (ATC: C08CA01), and dia-betes mellitus (ICD-10: E11.9) for which he receives met-formin (ATC: A10BA02) and sitagliptin (ATC: A10BH01).For the new condition of pyelonephritis that was diag-nosed, the treating doctormust decide a number of things.Regarding the prescription for treating this new disease,the doctor has to decide which active substances to pre-scribe in order to treat the resulting inflammation, thecause of the inflammation, the back and abdominal painand the resulting fever.However, before a decision is made the following fac-tors regarding the patients medical history should also beconsidered: There should be a check for drug-drug interactionthat the patient is taking, before the onset of the newcondition (the pyelonephritis). There should be a check for drug-disease interactionof the drugs that the patient is already prescribedwith the new condition. The new prescription has to be verified that it willnot have adverse effects or interactions with thepreviously prescribed medication and with thepatients medical history.Doulaverakis et al. Journal of Biomedical Semantics 2014, 5:13 Page 4 of 10http://www.jbiomedsem.com/content/5/1/13It is clear that the task for the doctor can be hard and amisjudgment could lead to wrong prescriptions. Using anautomated drug recommendation system can minimizethis risk. The recommendation system will use the inputdata and the pharmaceutical rules in order to propose atreatment that will be safe for the patient.Semantic transformationsPanacea is built on top of international standards ofmedical terminology in order to represent medical andpharmaceutical information. The following standard ter-minologies are used:ICD-10: International Classification of Diseases. It isused in Panacea for unique identification of diseasesthus uniquely identifying drug indications andcontraindications related to diseases. The latest 2010version was used in this work.UNII: Unique Ingredient Identifier. Used for theidentification of active ingredients found in drugs. InPanacea it is used for uniquely identifying drugindications and contraindications related toingredients. The 2013 index was used.ATC: The Anatomical Therapeutic ChemicalClassification is used for the classification of drugs. InPanacea it is used in similar fashion to UNII. Thelatest 2013 index was used.ICTV: The International Committee on Taxonomyof Viruses indexing is used for the classification ofviruses. In Panacea it is used in order to uniquelydrug indications and contraindications related toviruses. The latest 2012 release was used.Besides these international standards, a number ofdomain classifications have been declared and used inorder to enhance the usability of the system or to repre-sent data that are not included in the standards. Theseclassifications act as supplementary to the standards.Substance: As the use of encodings for drug ingre-dients is not convenient for humans, the identificationof active substances is done using its common nameJOURNAL OFBIOMEDICAL SEMANTICSPesquita et al. Journal of Biomedical Semantics 2014, 5:4http://www.jbiomedsem.com/content/5/1/4RESEARCH Open AccessThe epidemiology ontology: an ontology for thesemantic annotation of epidemiological resourcesCatia Pesquita1,3*, João D Ferreira1,3, Francisco M Couto1,3 and Mário J Silva2,3AbstractBackground: Epidemiology is a data-intensive and multi-disciplinary subject, where data integration, curation andsharing are becoming increasingly relevant, given its global context and time constraints. The semantic annotationof epidemiology resources is a cornerstone to effectively support such activities. Although several ontologies coversome of the subdomains of epidemiology, we identified a lack of semantic resources for epidemiology-specificterms. This paper addresses this need by proposing the Epidemiology Ontology (EPO) and by describing itsintegration with other related ontologies into a semantic enabled platform for sharing epidemiology resources.Results: The EPO follows the OBO Foundry guidelines and uses the Basic Formal Ontology (BFO) as an upperontology. The first version of EPO models several epidemiology and demography parameters as well astransmission of infection processes, participants and related procedures. It currently has nearly 200 classes and isdesigned to support the semantic annotation of epidemiology resources and data integration, as well asinformation retrieval and knowledge discovery activities.Conclusions: EPO is under active development and is freely available at https://code.google.com/p/epidemiology-ontology/. We believe that the annotation of epidemiology resources with EPO will help researchers to gain abetter understanding of global epidemiological events by enhancing data integration and sharing.BackgroundEpidemiology is the study of the factors influencing theoccurrence and distribution of health-related states orevents in specified populations, and the application ofthis knowledge to control health problems [1]. It is amulti-disciplinary subject that integrates diverse areas ofknowledge, such as medicine, biology, statistics, socialsciences and geography.Epidemiology is becoming increasingly data-intensive,considering the large volumes of data generated by bio-medical research and by the recent explosion of mobilephone and Internet usage - which contains epidemiologi-cally relevant behaviors, such as disease symptoms reports[2], and also the data created by large-scale computationalsimulations and models of disease transmission andspread [3,4]. To handle these challenges, epidemiologyneeds to embrace the new scientific methodology desig-nated as the fourth paradigm, whereby vast troves of data* Correspondence: cpesquita@di.fc.ul.pt1LASIGE, Campo Grande, Lisboa, Portugal3Universidade de Lisboa, Lisboa, PortugalFull list of author information is available at the end of the article© 2014 Pesquita et al.; licensee BioMed CentraCommons Attribution License (http://creativecreproduction in any medium, provided the orare collected, analyzed, validated and visualized [5]. Ontol-ogies are crucial to support this new paradigm, sincethey provide the means to semantically describe epi-demiological resources, supporting their categorizationand sharing.Consider the following example: a research team isbuilding a model for herd immunity in populations wherea measles vaccine can be administered. To achieve this,they need data on measles incidence rates and vaccinationrates in different populations/locations over time, as wellas other parameters, such as birth rate, factors influencingvaccination (e.g. legal frame, income and education levelof parents), transmission mode and secondary attack rate(i.e. the number of cases of an infection that occur amongcontacts within the incubation period following exposureto a primary case in relation to the total number of ex-posed contacts). These data can then be used to fit theparameters of their model. Traditionally, to collect the data,researchers would conduct extensive literature searchesto find a set of relevant scientific articles, read them toextract the relevant information and/or contact the authorsto request access to the datasets directly. The epidemiologyl Ltd. This is an open access article distributed under the terms of the Creativeommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andiginal work is properly cited.Pesquita et al. Journal of Biomedical Semantics 2014, 5:4 Page 2 of 7http://www.jbiomedsem.com/content/5/1/4community has not yet adopted the practice of publiclysharing datasets in open databases [6], which further hin-ders the collection of pertinent data. However, epidemi-ology is a domain where timeliness is crucial. For instance,when facing a new pandemic, laboratories need to be ableto produce new vaccines very quickly, and public healthofficials need to understand the disease and its spread sothey can issue recommendations to the population to ef-fectively contain the pandemic and diminish its impact.To make data collection more efficient and effective,epidemiological resources need to be easily searchableand retrievable, which can be achieved by semantic-enabled platforms for sharing epidemiological resources.An approach is supporting the annotation of datasets withontological concepts, so that the semantics encoded inontologies can be used to find relevant resources. For in-stance, resources that do not refer to measles, but to othertypical childhood diseases with the same transmissionmode can very well be of interest to extract parametersfor the measles herd immunity model.The only currently available ontology specifically intendedfor epidemiology is integrated into the BioCaster GlobalHealth Monitor [7], a news filter created with the aim ofproviding an early warning monitoring station for epi-demic and environmental diseases. However, the 2,000classes of the BioCaster ontology are insufficient toprovide enough coverage and granularity for a full se-mantic annotation of epidemiological resources. Forinstance, there is no class for vaccine, and diseases aredirect instances of Human Disease or Avian Disease,which are direct subclasses of Disease, highlighting thecomplexity of modeling these domains [8]. However, insuch a multidisciplinary domain as epidemiology, severalkey areas have already been described in existing ontol-ogies, including, among others, the Disease Ontology[9], Infectious Disease Ontology (IDO) [10], SymptomOntology [11], Vaccine Ontology [12] and the PathogenTransmission Ontology (TRANS) [11]. In previous work,we have outlined a Network of Relevant Ontologies forEpidemiology (NERO) [13]. We found that while someconcepts are fully covered by these ontologies, others arenot, in particular the specific epidemiological conceptsthat are seldom used outside this domain, such as, for in-stance, parameters like exposure ratio or attack rate.Consequently, a new ontology that covers these specificepidemiology concepts, while reusing and complementingrelevant existing ontologies in related domains is needed.Bearing this in mind, we have created the EpidemiologyOntology (EPO), which aims at covering the areas of epi-demiology not well described by other quality ontologies,particularly those related with metrics, parameters andmodels. EPO currently covers epidemiological and demo-graphical parameters, for which there was very little cover-age in surveyed ontologies, as well as transmission ofinfection, complementing classes from the TRANS ontol-ogy. In future versions, the scope of EPO will be expandedto include all parameters that influence epidemic pro-cesses, in articulation with existing and in developmentontologies for public health and medical surveillance.In this paper, we describe the current state of EPO andhow it is related to other ontologies relevant for the epi-demiological domain. We also explain how EPO is beingused to annotate epidemiological resources in a platformfor epidemiological resource sharing, where it supportsdata querying and integration, and provide examples ofhow it could also be used for annotation of other databasesand literature. The current version of EPO has 190 classes,of which 118 are newly created and 33 are imported fromtwo relevant OBO foundry candidate ontologies, IDO andTRANS. EPO uses the Basic Formal Ontology (BFO) [14]as an upper ontology, and IAO [15] as a source of annota-tion properties, further supporting its interoperability withother OBO foundry ontologies and candidate ontologies.We have submitted EPO to the OBO Foundry [16], aswell as to the BioPortal site of the National Center forBiomedical Ontologies (NCBO) [17]. EPO is freely avail-able at https://code.google.com/p/epidemiology-ontology/.ResultsModellingWe used the Dictionary of Epidemiology (DoE) [1] inthe creation the EPO. The Dictionary of Epidemiology isa well-established reference that captures the nomencla-ture commonly used in epidemiology. Most class labels,synonyms and definitions in EPO correspond to diction-ary entries or sub-entries.In the current version of EPO, we have focused ourmodeling activity in three major areas: demographic pa-rameters, epidemiological parameters and transmissionof infection.Although some resources contain a few demographicparameters, such as MeSH [18] and NCI Thesaurus [19],we have found that the majority of such parameters arenot represented in hierarchical vocabularies or ontol-ogies. Likewise, the coverage for epidemiological param-eters was also quite sparse. However, there are severalresources that model transmission of infection, in-cluding the Pathogen Transmission Ontology (TRANS)with 25 classes fully dedicated to transmission of infection,the Host Pathogen Interaction Ontology [20], InfluenzaOntology [21] and NCI Thesaurus. Nevertheless, TRANSmodels transmission of infection types only, and it does soin a different fashion from the DoE, with a different hier-archical organization and definitions. Consequently, wechose to include classes for transmission of infection inEPO in accordance with the entries in the DoE. Wheneveran equivalent class was present in TRANS we imported it,but used the label and definition from the DoE as editorFigure 1 A representative portion of EPO. This diagram represents a portion of EPO and how EPO classes are related to each other and toother ontologies classes. Unlabeled arrows represent subclass relationships, and labeled arrows represent relations imported from RO. Theontology for each class is identified by its prefix.Table 1 Statistics of EPO specific and imported classesand propertiesOntology Number of classesor propertiesEpidemiology Ontology (EPO) 118Infectious Disease Ontology (IDO) 19Pathogen Transmission Ontology (TRANS) 14Basic Formal Ontology (BFO) 38Relation Ontology (RO) 4Information Artifact Ontology (IAO) 7OBOInOWL 1Phenotypic Quality Ontology (PATO) 1Total 202Pesquita et al. Journal of Biomedical Semantics 2014, 5:4 Page 3 of 7http://www.jbiomedsem.com/content/5/1/4preferred label and definition, which resulted in reusing14 TRANS classes, for a total of 21 transmission of infec-tion types modeled in EPO. These classes are organized insingle inheritance, in up to five levels, increasing thegranularity level given by TRANS by two levels, but alsowidening its scope by including classes for the participantsin the transmission of infection process. These includeclasses imported from IDO as well as EPO-specific classes,which are linked to their respective transmission type viaparticipates_in relations (see Figure 1. for a relevant por-tion of EPO).Furthermore, EPO also contains 17 classes dedicatedto transmission of infection-related processes, such asisolation, containment and eradication, to name a few.These classes are particularly relevant for the descriptionof public health procedures and their impact on epi-demic events. Their articulation with transmission of in-fection types in describing epidemiological resources willallow the elucidation of the relations between these pro-cedures and the mode of transmission.In the demographic and epidemiological parametersbranches we currently have 36 and 21 classes, respectively.These are organized in a multiple inheritance structure,with classes being both subclasses of either demographyparameter or epidemiology parameter, as well as of theirspecific parameter type, like rate. To the best of ourknowledge, there were no suitable ontologies from whichto import classes in these areas, since the very fewterms that exist are poorly defined and structured. How-JOURNAL OFBIOMEDICAL SEMANTICSGonzález et al. Journal of Biomedical Semantics 2014, 5:46http://www.jbiomedsem.com/content/5/1/46SOFTWARE Open AccessAutomatically exposing OpenLifeData via SADIsemantic Web ServicesAlejandro Rodríguez González1, Alison Callahan2, José Cruz-Toledo3, Adrian Garcia1, Mikel Egaña Aranguren4,Michel Dumontier2 and Mark D Wilkinson1*AbstractBackground: Two distinct trends are emerging with respect to how data is shared, collected, and analyzed withinthe bioinformatics community. First, Linked Data, exposed as SPARQL endpoints, promises to make data easier tocollect and integrate by moving towards the harmonization of data syntax, descriptive vocabularies, and identifiers,as well as providing a standardized mechanism for data access. Second, Web Services, often linked together intoworkflows, normalize data access and create transparent, reproducible scientific methodologies that can, inprinciple, be re-used and customized to suit new scientific questions. Constructing queries that traversesemantically-rich Linked Data requires substantial expertise, yet traditional RESTful or SOAP Web Services cannotadequately describe the content of a SPARQL endpoint. We propose that content-driven Semantic Web Servicescan enable facile discovery of Linked Data, independent of their location.Results: We use a well-curated Linked Dataset - OpenLifeData - and utilize its descriptive metadata to automaticallyconfigure a series of more than 22,000 Semantic Web Services that expose all of its content via the SADI set ofdesign principles. The OpenLifeData SADI services are discoverable via queries to the SHARE registry and easy to integrateinto new or existing bioinformatics workflows and analytical pipelines. We demonstrate the utility of this system throughcomparison of Web Service-mediated data access with traditional SPARQL, and note that this approach not only simplifiesdata retrieval, but simultaneously provides protection against resource-intensive queries.Conclusions: We show, through a variety of different clients and examples of varying complexity, that data from themyriad OpenLifeData can be recovered without any need for prior-knowledge of the content or structure of theSPARQL endpoints. We also demonstrate that, via clients such as SHARE, the complexity of federated SPARQL queriesis dramatically reduced.Keywords: OpenLifeData, Bio2RDF, SADI, Semantic web services, SPARQL, SHARE, Sentient knowledge explorer, GalaxyBackgroundData integration is an ongoing challenge for biologicalinformaticians, and is often a study unto itself, withnumerous research groups worldwide approaching theproblem from a variety of perspectives [1]. Integration isdifficult for a variety of reasons, generally broken intothe three core issues of syntax, structure, and semantics[2]. In addition, assigning and using unique identifiersfor data items and concepts is an essential requirement inbiology and elsewhere, and forms an equally disruptivebarrier to successful integration [3]. Syntactic barriers* Correspondence: markw@illuminae.com1Centro de Biotecnología y Genómica de Plantas, Universidad Politécnica deMadrid, Madrid, SpainFull list of author information is available at the end of the article© 2014 González et al.; licensee BioMed CentrCommons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.include issues such as binary or textual format, andfree-text or structured text; structural barriers involve suchthings as flat-file formats, and XML Schema; semanticbarriers include inconsistent naming, naming conflicts(multiple things with the same name, or multiple names forthe same thing) or insufficiently defined names; andfinally identification issues involve non-unique identifiers,identifiers that can only be interpreted within a particularscope (e.g. in the context of a given database), non-opaqueidentifiers, and unstable or unpredictable identifiers.The Semantic Web Initiative [4] has recently emergedwith technologies and frameworks aimed at solving atleast some of these problems. In particular, the ResourceDescription Framework (RDF [5]) is an entity-relationshipdata model that is, in principle, machine-readable andal Ltd. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,González et al. Journal of Biomedical Semantics 2014, 5:46 Page 2 of 12http://www.jbiomedsem.com/content/5/1/46capable of representing any concept or data entity. RDFalso proposes several approved syntaxes, aimed atmaximizing machine-readability. Importantly, a querylanguage has been developed for RDF - the SPARQLProtocol and RDF Query Language (SPARQL [6]) - and aprotocol for exploring and retrieving the RDF stored inSPARQL endpoints on the Web is now well-establishedand, in our experience, highly consistent from implemen-tation to implementation.With RDF as its core, the Linked Data initiative [7]proposes several best practices that dramatically improvethe discoverability and integration of data on the Web.First, all data entities and relationships must be identifiedby a Uniform Resource Identifier (URI), which guaranteesuniqueness on a global scale. Second, URIs shouldresolve to data and metadata using the most commonWeb protocol, HTTP. Third, URIs should resolve touseful/informative information, and resource providersshould offer this information in a variety of syntaxesthat can be selected by HTTP content-negotiation; inparticular, Linked Data resources should provide a meansof retrieving the data in the form of RDF. Finally, this RDFshould contain labeled links between that piece of data,and other pieces of data also identified by resolvable URIs,where the links indicate the relationship between the twodata elements and are, themselves, resolvable URIs. Withthe ability to retrieve, share, and re-use these relationshipdefinitions, we begin to move towards the semanticaspect of the Semantic Web.Attempts to unify semantics have long been a focus ofbiomedicine. The medical world has engaged for centuriesin the development of nosologies for naming andclassifying diseases. Within the bioinformatics community,ontologies have become widely adopted in the pastdecade, with the most prominent of these being the GeneOntology [8]. While such ontologies generally focus onconsistent and sensible human-readable names, they havededicated less attention to the unique identification of theconcept - names in ontologies are not guaranteed to beglobally unique, nor are concepts guaranteed to beuniquely named, and can appear in multiple ontologies.However, as these ontologies became encoded using therules of Linked Data, aspects of these problems were alsosolved. Concepts became globally and uniquely identifiedby resolvable URIs, and shared concepts could be referredto by URI from one ontology to another. Moreover, mod-ern ontologies use of the Web Ontology Language(OWL) [9] description logic to define the meaning ofthe links in Linked Data - effectively, the precisenature of the relationship between one data entity andanother - enabled machines to automatically traverse theselinkages in a meaningful way.Two key issues remained problematic, however, even withLinked Data. First there was no widely-used mechanism toensure the stability and predictability of URIs representingdata and concepts - for example there was no way topredict the URI for the Protein Data Bank record ofthe Arabidopsis UFO protein, and even if this URIwere determined, it might not be the same from oneday to the next. As a result, individual Linked Dataresources could not reliably link out to other LinkedData resources, because the URIs were unpredictableand unstable. Data tended to remain siloed even inthe Linked Data world because links generally pointedinward, rather than outward, as a result of this instabilityand unpredictability. Second, the structure of what wasreturned when the URI to a piece of data was resolved wasalso not sufficiently predictable, and not consistent fromsite to site, even for the same type of data. While LinkedData is a significant improvement over XML Schema withrespect to the predictability of its data structures, therewere still no guidelines for how to arrange the relationshipsbetween pieces of data, or even what those relationshipscould/should be. It was these remaining problemsthat became the focus of the Bio2RDF project.Bio2RDF is an open source project that uses SemanticWeb technologies to create a sustainable infrastructurefor publishing biological data in a manner that eases thetask of data integration [10-12]. Bio2RDF scripts convertheterogeneously formatted data (e.g. flat-files, tab-delimitedfiles, dataset specific formats, SQL, XML etc.) into RDF.Bio2RDF follows a set of basic conventions to generateand provide Linked Data which are guided by TimBerners-Lees design principles and a set of community-established guidelines and practices. Specifically, entities,their attributes and relationships are named using asimple convention to produce Internationalized ResourceIdentifiers (IRIs) that are highly predictable in theirstructure, while statements are articulated using thelightweight semantics of RDF Schema (RDFS) and DublinCore. Bio2RDF, however, did not reliably implement all ofthe requirements of well behaved linked data, such asHTTP content-negotiation, and had somewhat limitedexpressivity in its relationships as a result of using thesemantics of RDF Schema. OpenLifeData providescustomized services over Bio2RDF SPARQL endpoints. Itsgoal is to provide alternative user interfaces and applicationprogramming interfaces to Linked Open Data beyond whatBio2RDF currently does. OpenLifeData enriches Bio2RDFsRDFS semantics to OWL expressivity, implements richHTTP content-negotiation, and utilizes query-rewriting toresolve OpenLifeData IRIs and SPARQL queries againstthe Bio2RDF SPARQL endpoints.OpenLifeData data is accessed by users either via the Web,through resolution of a URI to an HTML-representation ofits data content in their browser, or by the submission of aSPARQL query to one of the OpenLifeData endpoints.While the data-types and relationships within each endpointGonzález et al. Journal of Biomedical Semantics 2014, 5:46 Page 3 of 12http://www.jbiomedsem.com/content/5/1/46can be determined by manual exploration of the endpoint,SPARQL queries must nevertheless be constructedmanually, and then posed against the appropriate endpoint(s). Extracting OpenLifeData Linked Data, therefore, remainsa non-trivial task for even experienced bioinformaticians.The 2014 release of OpenLifeData (based on Release 3 ofBio2RDF) developed a scheme to provide a pre-computedsummary, or index, of the contents of each OpenLifeDataSPARQL endpoint in order to reduce the computationalload required for exploratory queries and enable newapplications. Summary metrics were pre-computed,including number of triples, number of unique subjects,number of unique predicates, number of unique objects,list and frequency of unique types, list and frequency ofunique predicates, list and frequency of unique subject,predicate-unique object tuples, list and frequency ofinstances of subject type, predicate, and instances ofunique object type, and finally number of links to otherdatasets. These indexes make it easier to determine thestructure and content of each OpenLifeData endpoint,and moreover, the structures are highly consistent fromendpoint to endpoint.Semantic Automated Discovery and Integration (SADI)[13] is a set of design principles for exposing Web Servicesin a manner that simplifies their integration with otherSemantic Web resources. Described simply, SADI Servicesare Web-based tools that consume a particular type ofdata, and return another type of data that is explicitlyrelated to that input. For example, you could send DNAsequences to a SADI tblastx service, and it wouldgive you back Protein sequences that are connected to theoriginal DNA sequence by the hasProteinHomologyTorelationship. Expressed more concretely, SADI servicesconsume and produce RDF data, where instances of aninput OWL class, represented in RDF, are submitted to theservice by HTTP POST, and RDF instances of an outputOWL class are returned in response. The constraint SADIplaces on these data is that the output class must be aspecialization of the input class such that the inputinstances are related to the new service-generateddata nodes through ontologically-defined relations.The result of chaining SADI services together, therefore, isan unbroken network of well-formed and ontologically-grounded Linked Data, which can be explored andtraversed using standard tools such as SPARQL.SHARE (Semantic Health And Research Environment)[14,15] is a SADI client that combines: a registry of theinput and output OWL classes for all known SADIservices, a service discovery and invocation API, anautomated workflow design and enactment engine, and alogical reasoner. While other components of SHARE arediscussed in detail in the previously-referenced papers, itis relevant to this manuscript that service discovery isachieved by indexing all known SADI services in theSHARE registry, such that input types, output types, andthe properties that link them, are all rapidly searchable.This registry is made publicly available as a SPARQLendpoint, where the data model of the registry follows thatof the myGrid serviceDescription [16] ontological class.The similarity between the input-type, property,output-type signature of a SADI Web Service, and thesubject-type, predicate, object-type indexes of theOpenLifeData endpoints provides a natural mechanismthrough which these two initiatives could be combined,such that OpenLifeData becomes discoverable and access-ible via SADI. At the NBDC/DBCLS BioHackathon 2013we proposed that it should be possible to automaticallygenerate (a) formalized definitions of SADI services, (b)SPARQL queries to retrieve the service-appropriatedata from the OpenLifeData endpoints, and (c) theSADI service code to serve that data, all by simply parsingthe OpenLifeData indexes. This manuscript describesthe realization of that vision. For the remainder of themanuscript will use the short name OpenLifeData2SADIas a convenient way of referring to the project as a whole.ImplementationOpenLifeDatas content summaries are provided as RDF[17]. We utilize the Jena [18] Java libraries to parse the[Subject type - predicate - Object type] (SPO) triplepatterns in these indexes, and additional indexes createdspecifically for this project, to generate sets of threeconfiguration files used by OpenLifeData2SADI toserve each data-type within OpenLifeData. The firstfile contains two OWL ontological classes, describing theinput and output data for the service. These ontologiesare published on the Web such that the input and outputclass URIs are resolvable through HTTP GET. The secondfile is a summary containing the URIs of the input andoutput OWL Classes for that service, the human-readableclass names, and the URI and name of the RDF predicatethat links the two classes. Finally, a third file is generatedthat contains a SPARQL query template that, whenfilled-in with data and executed against the appropriateOpenLifeData endpoint, retrieves the output dataappropriate for that service. We now describe in additionaldetail how each of these steps is undertaken.Parsing the indexesEach OpenLifeData dataset is served from its ownSPARQL endpoint, and contains data within a specificnamespace (e.g. sgd for Sacharromyces Genome Database,or ncbigene for the NCBI Gene). The content of eachendpoint has been pre-indexed, using VoID (Vocabulary ofInterlinked Datasets), where the index captures all uniquedata-type/predicate/data-type triples for that endpoint. Forexample, one of the index triples for the HGNC endpointis Gene Symbol/x-omim/Gene. The Java collector firstGonzález et al. Journal of Biomedical Semantics 2014, 5:46 Page 4 of 12http://www.jbiomedsem.com/content/5/1/46parses the information provided in the OpenLifeData in-dexes to obtain two parameters: SPARQL Endpoint URLand Namespace - effectively, the location of each dataset,and the domain/scope of that dataset. In principle, eachendpoint could be interrogated to retrieve all SPO patternsby executing the following SPARQL query:This would be sufficient to gather all informationnecessary to create SADI services that output resourcenodes (URIs); however, at this time, OpenLifeData doesnot index the large component of data that exists asliteral values (numbers and strings). As such, to be fullycomprehensive, we execute an iterative set of queriesover each endpoint which gathers all subject-types, thenthe predicates associated with each subject-type, andfinally the object type that is connected by eachpredicate, including the cases where the object is aliteral value. To further enrich the semantics, we thendo federated queries over multiple end-points in anattempt to determine more specific details about theobject types. For example, the omim dataset includes linksto entities in the hgnc dataset, but considers all of these tobe Resources - a generic term for something that existsin another dataset. Through our federated queries, we candetermine that these hgnc Resources represent, forexample, Genes, or SNPs, and thereby we are able toconstruct semantically richer descriptions of what theSADI services will consume/produce.The queries we execute (in template form) are asfollows:Get Subject-typesGet Predicate-typesGet Object-typesGet Data-typesFederated Query for object typesConfiguration file creationAfter retrieving all SPO patterns for each endpoint,OpenLifeData2SADI then builds the files needed toautomatically configure the SADI Service; each SPOtriple pattern becomes its own Service, where the serviceconsumes data of the Subject type, and returns alltriples from that endpoint matching the SPO patternfor that Subject. For each Service, three configurationfiles must be created:Input and output ontology classesUsing the Java OWL API [19] we create ontology classesbased on the pattern of each SPO in each endpoint;these classes describe the OWL properties requiredfor/provided by the Input and Output of the servicerespectively.In OpenLifeData URIs the class/predicate identifier, andnamespace are separated either by the hash (#) or colon (:)characters. Since we intend that OpenLifeData2SADIservices should make sense to both machines andhumans, an attempt is made to construct a human-readablename for each class and property. The code first attemptsto resolve the URI to retrieve its rdf:label, and this label, ifavailable, is used as the human readable class/propertyname in the final configuration file for that service. If nolabel can be retrieved, the hash or colon separator is used tosplit a name from the rest of the URI and this is used as thehuman-readable name. While not entirely successful, this isGonzález et al. Journal of Biomedical Semantics 2014, 5:46 Page 5 of 12http://www.jbiomedsem.com/content/5/1/46our best attempt at automatically building services that haveaccurate human-readable descriptions.The input class (generically called Subject_Class inthis discussion) is defined in OWL, simply, as the rdf:type of the Subject of the SPO triple, as defined byOpenLifeData. It contains no other axioms or restrictions.The class representing the output of the service is thendefined as the Subject_Class with an additional propertydefined by the Predicate of the SPO triple, where therange of that predicate is defined by the Object data-typecomponent of the SPO triple. This is represented inManchester Syntax as follows:Logically, therefore, the Service output is a subclass ofthe Service input (Subject_Class), as is typical for all SADIservices. A similar approach is taken for OpenLifeDatapredicates with Literal value ranges. The resulting ontologyis then saved to the local filestore with the namingconvention ./<namespace>/<subject_predicate_object > .owland this is published on the Web such that the URIs in thatontology resolve correctly.In a second phase, the process above is duplicated, butin this second iteration, the owl:Inverse of the Predicateis used, and Subject and Object are reversed. This allowsus to automatically create SADI services that traversethe OpenLifeData in either orientation, and thus behavein a manner akin to conventional SPARQL, where eitherSubject or Object may be bound in a constraint clauseof the query.Configuration fileThis file contains parameters required to properly con-figure the SADI service such that it (a) serves theappropriate data using the appropriate descriptors,and (b) provides its own metadata in a form that iscomprehensible to humans. The Java code that createsthese configuration files requires a single argument - theroot URL to the final location of the ontologies (createdabove) on the Web. The configuration file contains thefollowing parameters: INPUTCLASS_NAME: The name of the inputclass after removing the namespace. In caseswhere the class name is opaque or numerical,an attempt is made to resolve the class URI toits full OWL-RDF definition, and retrieve thelabel property, such that the class name ishuman-readable. INPUTCLASS_URI: The URI of the input class OUTPUTCLASS_NAME: The name of the outputclass. This is the same for all services, but conflictsare avoided since each output class name exists in aunique namespace (ontology); every output class isnamed #ServiceOutput. OUTPUTCLASS_URI: The Web-resolvable URI ofthe output class. This URI is generated by the con-catenation of the root URL, the namespace of theOpenLifeData dataset, the path and name of theontology file, and the generic class-name#ServiceOutput. PREDICATE_NAME: The name of the predicate. Aswith the input class name, an attempt is made toretrieve the human-readable label of the predicate ifit appears that the predicate is somehow opaque ornumerical. PREDICATE_URI: The URI of the predicate. ORIGINAL_ENDPOINT: The URL of the originalendpoint indexed by OpenLifeData. GENERIC_ENDPOINT: The endpoint that shouldbe queried by the SADI service using SPARQL.OpenLifeData is duplicated in several locations; thepreferred location to query would be the value ofthis field. OUTPUT_CLASS: The rdf:type of the data that willbe added during the service execution.The resulting file is written to the local filestore inthe same folder as the ontology file, with the namingconvention./<namespace>/<subject_predicate_object > .cfg.SPARQL query fileThe third file generated by OpenLifeData2SADI containsthe SPARQL query that should be executed within thebusiness logic of the SADI Web service. The content ofthis query is service specific, but follows the pattern:where < PREDICATE_NAMESPACE > is replaced withthe namespace of the predicate provided by the SPO,and < predicate > element is replaced with the localname of the predicate (the component after the # or :character). %VAR is left in the query template, and will besubstituted by the SADI service at run-time, based on theinput data.In the case of the SPARQL query, there is no differencebetween the forward Predicate and the inverse predicate.Inverse predicates do not exist in the OpenLifeDataSPARQL endpoints, but rather are simply defined in theOWL logic that defines the entities and relationshipsin those endpoints. As such, we rely on logical rea-soning to determine that an inverse invocation can besolved equally well by a forward query; thus theGonzález et al. Journal of Biomedical Semantics 2014, 5:46 Page 6 of 12http://www.jbiomedsem.com/content/5/1/46query that serves both forward and inverse services isidentical.SADI service implementationTo serve the OpenLifeData data, a single Perl scriptusing the standard SADI::Simple code libraries act as theSADI Service Daemon for all services. The script listensfor HTTP calls to URLs of the form:In this URL, SADI is the name of the OpenLifeData2SADIService script, while the additional path information(namespace and service name) are used as keys to accessthe configuration file and SPARQL query file appropriatefor that service, as described above. The SADI Perl scriptparses these files, and configures itself to be capable of:HTTP GET: Returning the complete service interface definition,represented as an owl:Individual of the mygridontology ServiceDescription Class, as per the SADIdesign patterns.HTTP POST: Parsing the input data, which arrives in RDF syntaxas owl:Individuals of that services Input OWL Class. Executing the SPARQL query, extracted from theconfiguration files, against the correct OpenLifeDataendpoint for that service, using each of theincoming owl:Individuals to fill the query variablesfor that particular invocation. Constructing owl:Individuals compliant with theclass definition of that services Output OWL Class,and passing this data back to the caller.This is all accomplished using the normal SADI servicetemplate [13]. The key difference is that the Servicesinterface template retrieves its values from a dynamiclook-up of data from the configuration files, rather thanbeing hard-coded into the service.Service registrationTwo scripts were written to automate the registration andderegistration of the full suite of OpenLifeData2SADI.The registration code and deregistration code are availablein the Perl folder of the GitHub project (see Availabilityand Requirements section). They operate by querying allof the configuration files (for registration) or all of theexisting SHARE registry entries (for deregistration) andtriggering the registry to call GET on each service end-point. The registry functions by creating a service if it findsa valid service description document at that endpoint, orderegistering a service if it does not. Therefore, in the caseof registration, the SADI script should be installed on thedesignated service endpoint first, in order to respond tothe registry calls. In the case of deregistration, the SADIService code should be removed prior to running thederegistration script.Workflows of Bio2RDF servicesThe establishment of the OpenLifeData2SADI suite ofservices made it possible to more easily explore theinterconnections between OpenLifeData endpoints. Inorder to generate an exhaustive list of these connections,to assist third-parties in building novel exploration tools,the following query was issued which creates a list of allvalid service-output to service-input pairs within the set ofOpenLifeData services (note that the PREFIX directives inthis example are shared for all queries in this manuscript,and will not be repeated in later examples):Since this, in principle, represents the complete set ofpotential workflow connections that could be constructedwithin these services, we chose to formally represent the out-put of this query as an abstract workflow template, using theOpen Provenance Model for Workflows (OPMW) AbstractTemplate ontology [20,21]. Those interested in generating acopy of this abstract template for their own exploration cansimply execute the OpenLifeData2SADI2OPMW.pl script inthe GitHub project, which will generate a copy based on thecontents of the public SHARE registry. A copy generatedat the time of writing is also available in the projects Gitrepository (see Availability section).ProvenanceProvenance of data is becoming increasingly import-ant as datasets get larger, more dispersed over theWeb, and as data gathering and analyses become moreautomated. The OpenLifeData2SADI project has selectedthe NanoPublication [22] conventions and model forGonzález et al. Journal of Biomedical Semantics 2014, 5:46 Page 7 of 12http://www.jbiomedsem.com/content/5/1/46passing provenance information to the client, alongwith the results of their service invocation. As withall SADI services, this is achieved through normalHTTP content negotiation. If the client passes an Accept:application/n-quads HTTP header, the OpenLifeData2-SADI service will respond by returning three namedgraphs, constructed according to the NanoPublicationspecifications. One graph contains the service output,the second contains the metadata describing the serviceand, for example, its name, description, and URL, and thethird describing the date and time the NanoPublicationwas generated.Results and discussionAt this time there are more than 22,000 OpenLifeData2-SADI services from 26 independent endpoints, and morewill be generated as OpenLifeData expands into new data-types. These services are discoverable through simplequeries against the SHARE registry, or through a varietyof client applications. We now demonstrate the utilityof the OpenLifeData2SADI application by a series ofwalkthroughs, where the process of discovery, execu-tion, and chaining-together of SADI-wrapped OpenLi-feData services is described in more detail andcompared to the interrogation of OpenLifeData dir-ectly via SPARQL.We will start with a small fragment of RDF data repre-senting a Human Gene Naming Committee (HGNC) GeneSymbol:Discovery of OpenLifeData2SADI servicesDiscovery of services is generally accomplished byexecuting a SPARQL query against the SHARE registry[23]. Discovery of the OpenLifeData2SADI servicescan be accomplished by a wide variety of query structures,but in this example we will query for services thatconsume OpenLifeData HGNC Gene Symbols and haveapproved-name somewhere in the services descriptivetext. The query is:This returns a single result, which is the URL for ser-vice hgnc_vocabulary_Gene-Symbol_hgnc_vocabular-y_approved-name_string.Invocation of OpenLifeData2SADI servicesInvocation of a discovered OpenLifeData data retrievalservice simply consists of sending the data to the serviceendpoint using HTTP POST. This can be accomplishedwith widely available tools such as Unix curl. Below, thesample HGNC Gene Symbol record described earlier, is inthe file sampledata_hgnc.rdf. Curl is then used to invokethe service, as follows:the result of this service invocation is the output data,containing the approved name from the OpenLifeDataHGNC endpoint:Client applicationsWe do not expect that our users will typically discoveror access OpenLifeData2SADI services via SPARQL queriesor the command-line. More commonly, the same discoveryand invocation interactions presented in their raw formabove are presented to the user graphically via one of theSADI plug-ins or client applications; nevertheless, discoveryand invocation happens the same way as described above,regardless of the client. We believe that this simplestandardization provides a very low barrier-to-adoption fornew users and tool-developers who wish to gain access tothe myriad OpenLifeData resources.There are a wide range of graphical clients capable ofexecuting SHARE registry queries in response to the userscontextual needs, or in some cases, fully automatically.We will now present several of these applications,showing how OpenLifeData services can be accessed andchained-together within these diverse clients.The list of services we will use for this demonstrationare:(1) Gene-Symbol_approved-name(2) Gene-Symbol_x-omim(3) Gene_gene-function(4) Gene_article(5) Gene_x-mgi(6) Gene_x-uniprotServices (1) and (2) link an HGNC resource to itsapproved name and a linked OMIM entry, services(3), (4), and (5) link an OMIM resource to a genefunction description, its associated PubMed entries,González et al. Journal of Biomedical Semantics 2014, 5:46 Page 8 of 12http://www.jbiomedsem.com/content/5/1/46and its associated Mouse Genome Informatics (MGI)Gene, while service (6) links an MGI gene identifier to itsassociated UniProt identifier. The template workflowconnecting these services in a biologically-meaningful wayis shown in Figure 1.IO informatics knowledge explorerThe SADI plug-in to the IO Informatics KnowledgeExplorer [24,25] (KE) provides menu-driven access tothe SHARE registry through a context menu thatappears when right-clicking a piece of biological data onthe KE canvas. In Figures 2A and B we show the samesample data from the examples above, loaded into theKnowledge Explorer. A right click reveals the FindSADI Services menu option, which then initiates asearch based on the data-type that was selected. Here wehave chosen the approved-name service from the resultingservices menu by clicking the selection box. In Figure 2Cthe approved name for HGNC:7 has been added as newinformation to the canvas. Figure 2D shows the final resultafter a series of OpenLifeData2SADI services have beenexecuted, following the workflow path in Figure 1.SHAREThe SHARE client [14] is one of several SADI clientscapable of chaining multiple services together. We willutilize this client to emphasize the fact that, as a resultof exposing OpenLifeData data as SADI services, it is nolonger necessary to know which data exists in which ofthe 26+ OpenLifeData SPARQL endpoints. In this usecase we imagine that a researcher has studied somehuman condition, has narrowed-down to a specific genelist of interest, and now wants to know more about thosegenes, their functions, and whether or not the proteinsmight be suitable drug targets based on known proteinFigure 1 A workflow of OpenLifeData2SADI services, numberedas in the list of services above, and the output data thatwill result.information from their respective Mouse homologues.Diagrammatically, the workflow is as shown in Figure 2(using the service numbers and starting-data from above).The SHARE interface is at http://dev.biordf.org/cardioSHARE. SHARE exposes SADI Web Services as ifthey were combined into a single, global, SPARQLendpoint. The SHARE SPARQL query that will invokethe workflow from Figure 2 is:Note that it was not necessary to know which endpointcontained which data elements, nor to use servicequeries to federate over these endpoints. This is importantwhen considering the complex structure of federatedSPARQL queries, where it is necessary to know the locationof the endpoint, and in some cases, the named-graphthat must be queried. For example, the equivalentSPARQL query over the OpenLifeData endpoints, wouldbe as follows:As such, we believe that OpenLifeData2SADI makesthe exploration across the more than 20 OpenLifeDatadata endpoints considerably more straightforward.GalaxyThe Galaxy [26] workflow environment is very popularamong life scientists, yet to date, we know of no Galaxyworkflow that accesses OpenLifeData or Bio2RDF data.This is likely due to the lack of life science tools andservices that deal with RDF-formatted data at all, andthe lack of a straightforward template for mapping databetween a workflow and a SPARQL query (and backagain). The SADI Galaxy plugin [27,28] provides SADIFigure 2 Discovery and invocation of OpenLifeData2SADI services using the SADI plugin to the Sentient Knowledge Explorer. A. Datanodes respond to a right-click with a context menu item Find SADI Services. B. a set of services capable of consuming nodes of that type arediscovered and presented in a menu-like manner. C. the result of selecting the approved-name service from the menu. D. the output afteriteratively invoking all 6 of the services from the example service list (effectively, manually executing the workflow in Figure 1).González et al. Journal of Biomedical Semantics 2014, 5:46 Page 9 of 12http://www.jbiomedsem.com/content/5/1/46services as normal Galaxy tools [29], thus making itstraightforward to chain OpenLifeData services togetherin the Galaxy environment. Figure 3 shows the sameworkflow as above, created within the Galaxy workbench.In order to reproduce the workflow, it is necessary tocreate, for yourself, a user on our Galaxy server [30] andimport the history and workflow [31,32]. The first item ofthe history can be used as the input to the workflow toreproduce the results reported here.Limitations and scalabilityThe use of SADI to expose data in SPARQL endpointsclearly adds a certain amount of overhead with respect toboth execution-time and computational load; however, itis difficult to directly compare the two scenarios because(a) speed and load depend on the client, and web serviceclients are significantly different from one another, andfrom SPARQL clients; (b) the time (and knowledge)required to manually construct each desired SPARQLquery, compared to the automated dynamic discoveryof appropriate SADI services, is not considered in ahead-to-head comparison of their respective executiontimes, and (c) It is considerably easier to optimize theexecution plan for a SPARQL query, versus a serviceworkflow. Nevertheless, a direct comparison of thefederated query entered into the SHARE client (above)versus the equivalent federated SPARQL query enteredinto the Virtuoso web-based query interface, showedexecution times of 34-39 seconds for SHARE comparedto 2-3 seconds for Virtuoso. Thus, while the overhead ofFigure 3 A workflow of OpenLifeData2SADI services in the Galaxy workbench environment. This workflow is an instantiation of thetemplate workflow in Figure 1.González et al. Journal of Biomedical Semantics 2014, 5:46 Page 10 of 12http://www.jbiomedsem.com/content/5/1/46the Web Service solution is, in this case, significant, wefeel it is still within reason for a user-interface, given thedifficulty a user would face in creating and debuggingSPARQL queries. Similarly, we would argue that thedynamically-generated, menu-driven interface provided bythe KE plugin is orders of magnitude faster than the userhaving to manually type each SPARQL query into the KESPARQL interface.ConclusionsIn this work we attempted to address four distinctproblems:The first is that, since most bioinformatics workflowscombine a variety of different kinds of Web Servicestogether with local processors to execute the data retrievaland analysis, it is highly desirable to expose SPARQLendpoints in a discoverable manner akin to Web Services.Current approaches to exposing SPARQL endpoints as ser-vices result in services with low discoverability and incom-plete (or even absent) descriptions of what will be returnedfrom a service invocation. By exposing the contents ofOpenLifeData as SADI Web Services, it becomes straight-forward to integrate these endpoints into popular workflowenvironments such as Taverna [33] or Galaxy, and moreimportantly, the service interface, and the data that passesthrough the service, is explicitly semantically defined.Second, the discovery of data in Bio2RDF (or anySPARQL endpoint) has, historically, required considerableprior knowledge and often trial-and-error explorationuntil an appropriate SPARQL query has been constructed.By enhancing the semantics of Bio2RDF, and indexing allof its semantically rich entity-relationships in the recentrelease of OpenLifeData, it became possible to expose allof this data as SADI services that are registered in theSHARE registry. It is now straightforward to discover,through highly predictable SPARQL queries, whichBio2RDF endpoints contain data of interest, and what thenature of that data is. Moreover, because the registryquery is predictable, it is trivial to make a comprehensivemap of all entity-to-entity connections within the entireOpenLifeData universe, even spanning between separateOpenLifeData endpoints, and this was made publiclyavailable as an abstract workflow template followingthe Open Provenance Model for Workflows (OPMW)ontology [20,21].Third, SPARQL endpoints are a highly granular approachto bioinformatics data publishing akin to publicly exposingthe SQL interface to a relational database. Historically,there have been very few core bioinformatics datahosts who allowed such fine-grained access to theirdatabases, primarily because of the potential for usersto submit resource-hungry queries (either accidentally,González et al. Journal of Biomedical Semantics 2014, 5:46 Page 11 of 12http://www.jbiomedsem.com/content/5/1/46or on purpose). Indeed, this has already been identified asa problem with respect to queries against the UniProtSPARQL endpoint [34]. While losing the potential forquery optimization, exposing RDF data via SADI Serviceshas several advantages over exposing RDF triple-stores asSPARQL. First, because SADI services can be executed ina multiplexed manner, and asynchronously, bulk datarequests could be easily managed over the availablecompute-resources at the host site. This is difficult toachieve with existing SPARQL endpoints. Second, becausethe Service exposes the RDF data via a wrapper around asimple SPARQL query that is guaranteed to be correct, itbecomes impossible for the data request to be malformedor resource-consuming (beyond what the provider allows).Thus, the data provider is better-shielded from misuseor abuse.Finally, in these early days of Linked Data publishing inthe life sciences, Linked Data resources can, justifiably,change as the data providers adapt their models, publishingprocedures, publisher metadata and even endpointlocations and access restrictions to accommodate newbehaviors or concerns. As such, SPARQL queries that aresuccessful one day, may not be successful the next. Byserving OpenLifeData through SADI Services, which aredynamically discovered by clients that automaticallyconfigure themselves for correct access, we provide anAPI that is much more resilient to underlying changethan a pure SPARQL interface would be; updating theOpenLifeData2SADI behavior is simply a matter ofchanging the configuration file, or (in the worst case)updating one simple piece of code, compared to all usersof the data being required to update their own software.For all of these reasons, we feel that the availabilityof OpenLifeData2SADI will dramatically enhance theaccess to, and utility of, OpenLifeData for all biologists.This, in turn, will hopefully spur a more rapid adoption ofboth Linked Data and Semantic Web Services throughoutthe life science data provider community.Availability and requirementsThe Java code and Perl scripts for this project are availableon the Wilkinson laboratory Github, at https://github.com/wilkinsonlab/OpenLifeData2SADI under the Apacheversion 2 license. Indexing the OpenLifeData endpointscan be executed in either Java or Perl. Ontology andconfiguration file creation requires Java 6, Jena, andthe OWL API. Serving the data as Web Services isaccomplished by running the indexer (in Java or Perl),building the configuration files, and deploying the SADIPerl script, with core dependencies on the SADI::Simpleand RDF::Query::Client libraries from CPAN. The repositoryalso contains the output files from the most recentexecution of the OpenLifeData2SADI2OPMW.pl script, andthese are free to use in any way.AbbreviationsDBCLS: Database center for life science; HTTP: Hypertext transport protocol;NBDC: National bioscience database center; OPMW: Open provenance modelfor workflows; OWL: Web ontology language; RDF: Resource descriptionframework; SPARQL: SPARQL Protocol and RDF Query Language;SADI: Semantic automated discovery and integration; SPO:Subject-predicate-object; VoID: Vocabulary of interlinked datasets.Competing interestsThe authors declare they have no competing interests.Authors contributionsMDW and MD conceived of, and planned, the project. MD, AC, and JCTcreated the indexes of the OpenLifeData dataset, and enriched thesemantics of the underlying Bio2RDF data such that it could be optimallyrepresented using SADI Semantic Web Services. ARG wrote the Javacodebase and created the configuration files. MDW wrote the Perl SADIservice script and the Perl version of the OpenLifeData indexer. MEA createdthe SADI Galaxy plugin, tools, and workflow. AG updated the KnowledgeExplorer plugin as depicted in Figure 2. All authors read, corrected, andapproved the manuscript.Authors informationMD is the lead investigator of the OpenLifeData project and is AssociateProfessor at Stanford University. MDW is the founder and leader of the SADIproject, Issac Peral Distinguished Researcher at the Center for PlantBiotechnology and Genomics, and Director of the FBBVA-UPM Chair inBiological Informatics at the Universidad Politécnica de Madrid. JCT, AC,AG, MEA and ARG are, or have been, researchers in these collaboratinglaboratories during the execution of this project.AcknowledgementsMDW is supported by the Isaac Peral and Marie Curie COFUND Programmesof the Universidad Politécnica de Madrid, Centre for Plant Biotechnology andGenomics UPM-INIA. ARG is supported by the Isaac Peral programme, UPM.Portions of this work have been funded by the Fundación BBVA. We wish toexpress deep gratitude and appreciation to the National Bioscience DatabaseCenter (NBDC) and Database Center for Life Science (DBCLS) in Tokyo, Japan,and Dr. Toshiaki Katayama who has been organizing annual BioHackathon inJapan since 2008; The idea for OpenLifeData2SADI emerged as a result ofthe face-to-face contact facilitated by these extraordinary events.Author details1Centro de Biotecnología y Genómica de Plantas, Universidad Politécnica deMadrid, Madrid, Spain. 2Center for Biomedical Informatics Research, StanfordUniversity, Stanford, CA, USA. 3Department of Biology, Carleton University,Ottawa, ON, Canada. 4Genomic Resources Group, University of the BasqueCountry (UPV-EHU), Bilbao, Spain.Received: 17 July 2014 Accepted: 7 November 2014Published: 19 November 2014JOURNAL OFBIOMEDICAL SEMANTICSMalone et al. Journal of Biomedical Semantics 2014, 5:25http://www.jbiomedsem.com/content/5/1/25DATABASE Open AccessThe Software Ontology (SWO): a resource forreproducibility in biomedical data analysis,curation and digital preservationJames Malone1, Andy Brown2, Allyson L Lister2, Jon Ison1, Duncan Hull2, Helen Parkinson1 andRobert Stevens2*AbstractMotivation: Biomedical ontologists to date have concentrated on ontological descriptions of biomedical entitiessuch as gene products and their attributes, phenotypes and so on. Recently, effort has diversified to descriptions ofthe laboratory investigations by which these entities were produced. However, much biological insight is gained fromthe analysis of the data produced from these investigations, and there is a lack of adequate descriptions of the widerange of software that are central to bioinformatics. We need to describe how data are analyzed for discovery, audittrails, provenance and reproducibility.Results: The Software Ontology (SWO) is a description of software used to store, manage and analyze data. Input tothe SWO has come from beyond the life sciences, but its main focus is the life sciences. We used agile techniques togather input for the SWO and keep engagement with our users. The result is an ontology that meets the needs of abroad range of users by describing software, its information processing tasks, data inputs and outputs, data formatsversions and so on. Recently, the SWO has incorporated EDAM, a vocabulary for describing data and related conceptsin bioinformatics. The SWO is currently being used to describe software used in multiple biomedical applications.Conclusion: The SWO is another element of the biomedical ontology landscape that is necessary for the descriptionof biomedical entities and how they were discovered. An ontology of software used to analyze data produced byinvestigations in the life sciences can be made in such a way that it covers the important features requested andprioritized by its users. The SWO thus fits into the landscape of biomedical ontologies and is produced usingtechniques designed to keep it in line with users needs.Availability: The Software Ontology is available under an Apache 2.0 license at http://theswo.sourceforge.net/; theSoftware Ontology blog can be read at http://softwareontology.wordpress.com.BackgroundWe report on the Software Ontology (SWO) [1,2], anontology for describing the software used within compu-tational biology, which includes bioinformatics resourcesand any software tools used in the preparation and main-tenance of data. Development of the SWO is motivatedby the growing interest in the recording and reproducibil-ity of biomedical investigations [3,4]. Reproducibility is asimportant for computational investigations of data as it*Correspondence: robert.stevens@manchester.ac.uk2School of Computer Science, University of Manchester, Oxford Road,Manchester, M13 9PL, UKFull list of author information is available at the end of the articleis for investigations in the wet laboratory [5,6]. In orderto understand research results presented from data analy-sis investigations or perform new analyses based on theseresults, it is important to know whence the data came,how they were analysed and with what tools. In a recentScience paper, Peng [7] suggested that making researchthat uses computational methods reproducible requiresmuch greater attention to detailing the software as partof the experimental process. Gentleman et al [5] state theneed for reproducibility by combining analysis code withthe data; e.g., using BioConductor packages to analyzeMicroArray data. However, for reproducibility, the versionof the BioConductor packages, R and any associated soft-ware that may have an influence on the outputs would© 2014 Malone et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited.Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 2 of 13http://www.jbiomedsem.com/content/5/1/25need to be known - and even the hardware upon which itwas run, as all of these can have an influence on the resultsobtained.The growing use of workflows as a means of analyz-ing biological data [8-10] and as a means of recordingand exchanging method [11] has provided one avenue forthe recording of method. There has also been a move toautomatically describe the provenance of computations(including the actual run of a workflow), and ontologieshave been provided to support this recording [12]. Anontology such as the SWO provides the vocabulary andidentifiers for the software aspects of such automaticallyrecorded provenance.As well as the reproducibility angle, describing softwareand the data it consumes and produces is importantfor search for software and construction of applica-tions and workflows. Registries such as BioCatalogue [13]describe Web services used in bioinformatics accord-ing to data consumed and produced, and the func-tional units of the services involved and so on. Thesesemantic descriptions can be then used for search andretrieval. Similarly, automated or semi-automated work-flow construction depends on descriptions of the services[14].An ontology of software can have impact in all ofthese areas by providing the means to describe soft-ware used, the data consumed and produced, its versionsand so on. The scope of SWO is thus broad; it needsto cover not only bioinformatics, but any tools used inthe management, analysis and presentation of biologi-cal data. Prima facie, the SWO needs to cover, but isnot limited to, this range of software, and descriptionsof its objectives (for what it is used), the data it con-sumes and produces, the algorithms it implements toachieve these objectives, its version, and some aspectsof its project details. This software would include, forexample, spreadsheets, word-processors, databases, aswell as the bespoke desktop and services on the Webused by bioinformaticians within computational biology.A rich description of such a broad range of softwareused in life science investigations implies a similarly broadscope for an ontology of software. To date, however,attempts to produce such an ontology have not beenconvincing, although some promising efforts have beenmade: DOAP (Description Of A Project) [15] describes asoftware project (home page, developers, language,etc.), rather than the software itself. There is anoverlap in scopethe home page and developers arefeatures that the SWO will need, but the SWO aimsto model the software artifact itself, not the project. OWL-S [16] is a domain neutral and generalmechanism for describing Web services, such thatthey can be discovered, composed and invoked. Of itsthree aspects, OWL-Ss profiling mechanism comeswithin the scope of the SWO, but the grounding forthe automatic invocation of a Web service is outsidethe SWOs scope. In addition, its focus on only Webservices precludes its model from our use.WSMO [17] (Web Services Modeling Ontology) hasa similar remit and again is not suitable for theSWOs aims. Ontologies of data mining tools cover much of thesame ground as the SWO, but are obviously restrictedto data mining. As data mining tools are used withinbioinformatics and computational biology, we aim toinclude such descriptions into the SWO, and weincorporate portions of DMOP [18] into the SWO. The Ontology of Biomedical Investigations (OBI) [19]has a broad remit of enabling scientists to accuratelyand precisely describe how a biomedical investigationwas planned and performed, including devices,samples, sample preparation, etc. As its remit isbroad, it has also spun out ontologies such as theInformation Artifact Ontology (IAO) [20] to allowdescriptions of information such as digital documentsassociated with an investigation. Descriptions ofsoftware used in an investigation could fit withinOBIs remit, but the SWO has been deliberately keptseparate as its scope is greater than biomedicalinvestigations themselves. However, the SWO is builtto be broadly inline with OBI and IAO. EDAM [21] is a vocabulary for describing data relatedconcepts in bioinformatics including types of data,identifiers, formats, operations and topics of broadbiological areas. EDAM is limited to these conceptsand as such does not cover software and relatedsoftware items like algorithms and licenses.The SWO aims at a wider scope of describing anysoftware used in the pursuit of computationalbiology; this includes spreadsheets, database systems,XML parsers, ontology development environmentsand the like. EDAM is now a subset of the SWO andhas been integrated into the SWO. The Bioinformatics Resource Ontology (BRO) [22]has a similar remit to EDAMresource descriptionand discovery. However, it models resources verybroadly, capturing concepts concerning researchinfrastructures, people, funding etc. It also conflatesvarious aspects of software, such as algorithm andimplementation. The myGrid Ontology [23], though focusing onbioinformatics software resources, makes many ofthe same distinctions as the SWO, but has not beenactively maintained for a long-time. EDAM coversthe concepts included in myGrid and, with the SWO,supersedes and replaces the myGrid ontology.Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 3 of 13http://www.jbiomedsem.com/content/5/1/25Bio-ontologies now cover a broad range of life scienceentities from biological sequences (SO) [24] to the func-tional attributes of gene products (GO) [25], and fromcells (CTO) [26] to gross anatomy (Uberon) [27] andphenotype (PATO) [28]. We now also have ontologiesdescribing small molecules and their roles that participatein many biological processes (ChEBI) [29]. Added to thisare descriptions of biomedical investigations such as theOBI [19], the Experimental Factor Ontology (EFO) [30]and the BioAssay Ontology [31]. The SWO fits neatly andindependently into this ontology landscape in its role asan ontology concerned with the description of resourcesused in the investigation of biomedical phenomena, ratherthan the biomedical phenomena themselves.User storiesThe principal use for the SWO is in the description ofresources used in storing, managing and analyzing data.Our SWO workshops produced a broad range of whatin agile development are termed user stories for theSWO [32] and we highlight a few here: Describing the software used in the analysis of geneexpression data. With the development ofnext-generation high throughput sequencingtechniques, a slew of new software packages foranalyzing this data has emerged. Understanding howresults have been produced requires knowledge of thesoftware used. This is an important consideration forthe European Bioinformatics Institute (EBI), whichhosts such data resources as ArrayExpress [33] wheredescribing how data were analysed is important. The eagle-I project [34] concerns the collection ofdescriptions of biomedical resources and servicesavailable at various sites in North America such thatconsumers can find those relevant to their work. Theeagle-I consortium require descriptions of data typesand formats, software names, programming languagesand licensing information to describe these resources. The European consortium under the BioMedBridgesproject [35] wish to collect descriptions of Webservices and software tools used both within theconsortium and more widely in Europe in a Tool andService Registry [36]. They wish to answer userquestions about software attributes such as data typesand formats, licenses, and developer and source codeinformation and function. Queries require structurefor faceting such that higher-level categories in theontology can be used to drill down to subsets ofinterest. The British Library and UK National Archivesrequire software descriptions to assist in the curationof digital artifacts for preservation purposes. Datacurated using specific versions of a piece of softwarecan produce varied results, so being able to describeattributes such as version and data formats are ofgreat importance.Materials andmethodThe Software Ontology has adapted agile software engi-neering methods into the ontology engineering pro-cess [1]. Agile methods offer a number of principles thataim to keep users involved in the process of developingsoftware and enable rapid response to changing require-ments whilst also building in consistent quality controlchecks [32,37]. Specifically, the SWO project focused onthe following agile principles [38] and adapted them toontology development: The SWOs users, domain experts, and ontologyengineers are all active contributors throughout theprocess; Close engagement with users meant the introductionof requirements gathering and ontology modellingsessions as iterative activities, each iteration (sprint)resulting in a new increment; Acknowledgment that requirements can evolve andpriorities can change throughout the engineeringlifecycle; Encouragement of self-organised andcross-functional teams of developers; Testing is an integral part of development andhappens all the time; Provision of regular and frequent builds to theparticipants for discussion, testing and refinement(and ultimately agreement).Applying these principles requires a number of eventsto take place in order to deliver information to otherevents in a cyclic manner, though events can be run inparallel. The agile ontology engineering method can besummarised as follows: Requirements gathering. Requirements arecaptured from stakeholders by identifying key areasof interest, eliciting competency questions [39] anddesirable features of the ontology. Activities aredriven by user stories, in the form of competencyquestions, and card sorting exercises. Requirements prioritisation. Prioritisation ofrequirements has two components, both of which areadapted from agile engineering techniques. The firstis the estimation of the complexity of implementing aparticular requirement using planning poker [32].The second component allows participants tocollectively rank requirements by bidding onindividual requirements of most interest to theirneeds based on the Buy a Feature method [40].Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 4 of 13http://www.jbiomedsem.com/content/5/1/25 Implementation of the Top requirements. Theimplementation of the ontology focuses on theprioritised requirements that were bought from theprevious event. Separating the ontology componentsinto modules allows concurrent development tooccur from co-located or distributed developers.Additional content is gathered from stakeholdersusing methods such as template completion via tools,such as Populous [41] - a tool for creating ontologiesusing a spreadsheet like interface, which are wellsuited to large-scale concept collection. Evaluation of product. At the end of each iteration,the ontology is evaluated against competencyquestions [39]. Defined classes based on competencyquestions act as queries within the ontology and areused to demonstrate delivery of a particular feature tostakeholders.The SWO project conducted three face-to-face work-shops between 2011-2012 (see [42] for details), dur-ing which the method outlined above was applied [1].The first workshop (WS1) was used primarily to gatherrequirements and potential content, since there was noontology to evaluate at that point. The second (WS2) andthird (WS3) workshops took place four months and 12months later and were used to both evaluate form andcontent, as well as to generate new content for the SWO.There were 18 participants in WS1, 14 in WS2 and 17in WS3. Seven of these participants attended all threeworkshops. Participants represented a user base underthe broad heading of digital curation and preservation,with more specific areas including archiving organiza-tions, software sustainability, library services, astronomy,life science and pharmaceutical research.Spreadsheets and populousThroughout the project, spreadsheets created using thePopulous tool [41] were used to collect specific softwaredescriptions from the community. Populous is a tool thatallows cell values to be connected to ontology parts suchthat each row becomes a description for an ontology classfollowing a specified template. In this way, members ofthe community did not have to learn new technology orontology languages to contribute directly to the ontol-ogy; instead they simply worked in a familiar spreadsheetenvironment.Testing competency questions via DL queriesThe testing component of the method concerns the useof competency questions phrased as description logicaxioms executed as queries (DL queries). An ontology inOWL should be able to satisfy competency questions pre-cisely and this can be tested using the description logicaspects of the language.In the testing phase a DL query is formulated whichrepresents a question of interest, e.g. which software cantake as input image data in the JPG format images. If theDL query is not producing the desired results then theontology needs further refinement and a further itera-tion occurs. Testing using DL queries in this way is a testafter [43] approach since test driven approaches require atest to be written before the encoding. This is not suitablefor ontology development in most current environmentssince writing a DL query as a test to be executed beforedevelopment requires testing infrastructure that, as yet,does not exist in most environments.ResultsWhat should be modelled in the SWO?WS1 resulted in a set of requirements that the ontologywas required tomatch; these were sorted into 15 groups offeatures, each groups label became a feature for modellingin the ontology. In addition, there were 91 competencyquestions aligned to these features (see Table 1 for the fea-ture groups and [44] for the competency question groups).For instance, the group Function contained sticky notescontaining can the software perform XML editing? and canthe software be used for word processing? It is worth not-ing that a question could also fall into multiple groups,for instance can the software perform XML editing? fallsinto both Function and, by implication, Data/format fea-ture groups since the software would need to be able toparse XML. That all of the competency questions couldbe aligned to a feature group, and conversely that eachfeature group contained competency questions, provideda validation of the process, since an orphaned questionmight suggest a missing category or an empty featuregroup.The list of features for the SWO gained from the work-shops are shown in Table 1 along with whether or notthey were bought, i.e. were prioritised in the user priori-tisation sessions. From a modelling perspective, boughtfeatures were a combination of both simple concepts andmore complex components; some features were deemedimportant but too costly to model in a way suited tocustomers needs, such as modelling the hardware uponwhich software is run. One interesting result of the pri-oritisation event is that the users initially suggested thatsome features, such as algorithm, were ranked highly, butfollowing effort estimation suggesting this was very costlyto represent, the feature was not bought. Some featureswhich were discussed as important remained so afterprioritisation and were duly bought, such as data andfunction.In a second prioritisation event, the exercise wasrepeated. The algorithm component of software (origi-nally not prioritised) was consideredmore important thanhad previously been determined and was added to the listMalone et al. Journal of Biomedical Semantics 2014, 5:25 Page 5 of 13http://www.jbiomedsem.com/content/5/1/25Table 1 The feature groups identified by the workshop participantsFeature Definition Bought? Example competency questionSoftware The software itself Yes What is the name of the software?Data Data that the software consumes and pro-ducesYes Will it render a gif format image?Function The task the software is used to do, some-times called objectiveYes Does this software provide XML editing?Algorithm The specific instructions as part of softwareto perform a given taskNo What is the normalization algorithm used inthis software?Configure parameters Parameters required to run the software; set-tingsNo What are setting needed to run this analysis?Life cycle Stage of maturity of a piece of software No Does the software meet the ISO-4 standard?Version The version information Yes What is the latest version of this software?Supplier Developer and/or maintainer of software Yes Who developed this software?Dependencies Other pieces of software or libraries requiredto run itNo What are the dependencies for using OWL-API?Interface Modes of interaction with the software Yes Is there a Web API for Blast?Source code location URL or otherwise of source code Yes Where can I get the code?Cost of ownership Cost to purchase but also to run No Is it free?Platform Which platform is required to run software No Will the software run on Ubuntu?License What license and usage restrictions exist fora given softwareYes What software can I use for my task which isunder the Apache 2 license?Architecture Architectural structure of the software, suchas peer-to-peerNo Is the software client-server?Those features that were bought were then prioritised for inclusion in the ontology.of features. This became apparent after the initial exam-ples failed to answer some of the competency questionsregarding software that implements a given algorithm.Since there was a small amount of additional extra effortavailable, algorithm was included in some descriptions ofsoftware added more recently to the SWO.The ontologyThe ontology was authored in the Web Ontology Lan-guage (OWL) [45] using the schema shown in Figure 1as a guide for the top-level distinctions made in describ-ing software. As of Release 1.1 in December 2013, theSWO contained 3 777 classes, 50 object properties, 5 dataproperties and 114 individuals. Table 2 shows the numberof classes under each major division in the SWO. Ini-tially, addition of software used in bioinformatics to theSWO was driven by the needs of the ontologys authorsand client projects. Latterly, however, a more systematicapproach has been adopted; we are using results of a sur-vey of Genome Biology and BMC Bioinformatics withBioNERDS [46], a named entity recogniser for bioinfor-matics software and databases. This survey provided a listof software and databases ranked by the number of docu-ments in which those resources were mentioned. We tookthe top 50 resources and removed the databases and anyobviously spurious entries to leave only software (databasemanagement systems such as mySQL are software, but forthe SWO a reference to database content, such as SWISS-PROT, does not count as software). Genome Biology gave27 software names, and BMC Bioinformatics 25 namesout of the top 50 resources in each case. These correlateto 47.5% of the total document level mentions within thetop 50 in Genome Biology, and 53.7% in BMC Bioinfor-matics. In this way we expect to be able to make the SWOcover the main software used in bioinformatics and com-putational biology (the list of software is available in thesupplementary data).The SWO is separated into discrete ontology modulesthat are combined to produce software descriptions. Sep-arating the different aspects of software in this way allowsfor both concurrent development and reuse of those com-ponents useful for other projects, for instance the orga-nizations module for an ontology describing biomedicalinstruments and license module for an ontology of liter-ature. Figure 2 illustrates the different OWL module filesand which components of the SWO they contain.For describing the ontology we use the following fontconventions: classes and object properties. The SWOis axiomatised as follows; the class Software is natu-rally the focus of attention. A class of software may beMalone et al. Journal of Biomedical Semantics 2014, 5:25 Page 6 of 13http://www.jbiomedsem.com/content/5/1/25license clausesoftware interfacehas_clausedata data format specificationhas_format_specificationhas_specified_data_inputhas_specified_data_outputalgorithmsoftwareimplementis_encoded_inorganizationsoftware publishing processsoftware development processhas_participanthas_participantlicense interfaceprogramming languageversionhas_versionhas_license has_interfaceinformation processingis_    executed_Figure 1 The SWOs schema.described in terms of the data it takes as input, the datait produces as output, the objective or processing task itis designed to meet, licensing restrictions that apply tousing the software (and so on). Few of these propertiesare universally true of software (there is software that, atthe granularity at which the SWO is represented) takesno data as an input), so using restrictions to representthese notions is not desirable. The only restriction onSoftware is that it is executed in some process. A typicalpiece of software would be described as follows:Table 2 The number of classes or individuals under eachmajor division in the SWO; these are things that describedomain content, rather than ontology infra-structureSWO division Number Example classes or individualsSoftware 512 Blast, Excel, Endnote, ClustalData 1168 heatmap, sequence alignment(protein), 2D PAGE imageData Format 434 XLS, RDF-XML, BAM, JPEGInformation processing 608 Phylogenetic tree construction,spreadsheet editing, ontologyengineering,protein structure analysisAlgorithm 159 ANOVA, Chi-square, t-testOrganization 78 Agilent Technologies, AdobeSystems, Bioconductor, SASInstitute Inc.Programming language 46 C++, Java, MATLAB language, RubySoftware license 30 Apache license v2, GNU GPL, MITLicense A property has specified data input links a software toits input data while has specified data outputsimilarly links software to its output data. thespecified part of these property names seeks tocapture the choice inherent in, for instance, datainputs by enabling statements such as software x isspecified to be able to take data types p, q and r asinputs, without saying each and every instance ofsoftware actually does so. The tree of data is presentlyfairly flat, with some structure separating image datafrom much of the other data. Data format is separated from the data itself and isrelated via a property has format specification whichcan be used to specify that the data has a certainsyntax, such as XML or SVG. In the SWO we make adistinction between data and the format of the data.These are easily conflated, but useful to pull apart asone type of data can be presented in many formats.Perhaps the easiest example is that of image data;here the data is the symbols or values that representthe meaning of the data; the format, however, is thesyntax that governs the encoding and decoding ofthat data. Thus data are the symbols upon which acomputer (typically via software) performsoperations. the format specifies how the data are tobe encoded. So, for images, image data is image data(some symbols), but an image file could be encodedin PNG, JPEG, PDF, etc, and, in some cases,inter-converted, preserving the image data itself. The algorithm section of the SWO capturesalgorithms which a piece of software implements.Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 7 of 13http://www.jbiomedsem.com/content/5/1/25versiondatadata format specificationalgorithmorganizationsoftware publishing processsoftware development processlicense clauselicense interfacesoftwareprogramming languageinformation processingFigure 2 The SWO s ontology consists of several modules which are used to compose software descriptions. The property is executed in links a software to theinformation processing class in which it isexecuted. Information processing can be seen here asthe task for which the software is being used to helpaccomplish. For instance, differentialexpression analysis and ontologyengineering. Software can have a version using the has versionproperty. Versioning is complex and is discussed inmore detail below. Licenses are also described in the SWO as types ofsoftware license. Software licenses aredescribed in terms of license clause classes viathe has clause property which capture specificlicensing aspects such as how software can be used,redistributed, extended or modified. Interfaces to software, such as APIs and graphical userinterfaces, are described in software interfaceand related to software by the property has interface. Software is encoded within a programming languageand this is represented via the property is encoded inthe programming language part of the ontology. Organizations involved in developing or publishingsoftware are captured as individuals under theorganization class and related to software as theoutput of either via the software developmentprocess or the software publishingprocess. The SWO also contains datatype properties forconnecting, for instance documentation locations tosoftware with has documentation, homepage forsoftware has website homepage and a download URLwith has download location.If we consider the example ofMicrosoft Excel 2007 thisis described in the SWO as follows: Excel is specified to be able to take as input any datain various data formats such as the XLS spreadsheetformat and the tab delimited format.has specified data input some(data and (has format specification some XLS spread-sheet format))has specified data input some(data and (has format specification some tab delim-ited file format)) Similarly, Excel is also specified as inputting andoutputting data in various data formats such as theXLS spreadsheet format or as a tab delimited format.has specified data input some(data and (has format specification some XLS spread-sheet format))has specified data input some(data and (has format specification some tab delim-ited file format)) Excel 2007 is versioned as Excel Microsoft 2007.has version value Microsoft 2007 version It has a proprietary commercial license.has license some Proprietary commercial soft-ware license It has a graphical user interface.has interface some Graphical user interface Excel 2007 is both developed and published byMicrosoft.output of some(software development process and (has partici-pant value Microsoft))output of some(software publishing process and (has partici-pant value Microsoft)) Excel can be used to edit spreadsheets.is executed in some spreadsheet editingFor Excel, the specified inputs and outputs are Data, asspreadsheets can have content of a more or less arbitrarytype. The SWOhas not attempted to represent all possibleMalone et al. Journal of Biomedical Semantics 2014, 5:25 Page 8 of 13http://www.jbiomedsem.com/content/5/1/25formats for software such as Excel. Instead those data for-mats that are necessary for the annotations and searchesfor the SWOs use cases are prioritised. In line with manyontology projects, the SWO is largely driven by the needsof its users.BLAST 2.2.26 is described as follows: BLAST 2.2.26 can take as input, for example, DNAsequence data in FASTA format or in GenBankformat.has specified data input some(DNA nucleotide sequence and (has format specifica-tion some FASTA format))has specified data input some(DNA nucleotide sequence and (has format specifica-tion some GenBank format)) It has several interfaces including a command lineinterface and web based.has interface some command-line interfacehas interface some web user interface This version is developed by the NIH.output of some(software development process and (has partici-pant value NIH)) Blast can be used to perform multiple sequencealignment. It can also be used to perform pairwisesequence alignment.is executed in some multiple sequence alignmentis executed in some pairwise sequence alignment It can be downloaded from ftp://ftp.ncbi.nih.gov/blast/executables/has download location value ftp://ftp.ncbi.nih.gov/blast/executables/The SWO makes a distinction between an item of soft-ware and a software suite; this is MS Word 2010 asopposed to MS Office 2010 that is a bundle of severalMS products including MS Word. A software suite is apiece of software in its own right, as it provides a thinwrapper around the bundled softwareeven if this isjust for presentational reasons. The SWO describes MSOffice by using the property has part to relate the softwarecomponents. For example, MS Office 2001:has part some Microsoft Excel 2002has part some Microsoft Word 2001Software licencesSeveral competency questions focused on licensing issuessuch as is the software open source or available with-out restrictions on derivatives. To capture this, softwarelicenses were given parts, as mentioned above, which werelicense clauses. This way, a license can be described byattaching the relevant clause components which enablesquestions to be asked over these components. Figure 3illustrates an example of a defined class that uses thesame logic to infer types of software licenses that haveclauses that indicate the software is open source. Thehighlighted class, GNU project Free Software LicenseType is described as follows:software licenseand (has clause some Source code available)and (has clause some(Distribution unrestrictedor Distribution with notices))Figure 3 Inferring open source software licenses from the ontology.Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 9 of 13http://www.jbiomedsem.com/content/5/1/25Software versionsThe version name class is used to describe individu-als which are a specific version name for a given piece ofsoftware. These versions are then related to the class ofsoftware with which they are associated using has_version.The versions name is captured in the RDFS:label anno-tation of the given individual.Competency questions for software versions requirednot only a record of what version name was attributedto a given software instance but also which versions pre-ceded and proceeded a given piece of software. There aretwo forms of the question: Find all previous versions andfind the version prior to the one in hand (and similarlyfor subsequent versions). This is a list of versions and weuse the pattern described in [47]. The directly followingand preceding version individuals are asserted via theproperties directly followed by and directly preceded by.These properties have the super-properties followed byand preceded by, which are transitive (if A is precededby B and b is preceded by C, then A is preceded by C).In OWL the sub-property implies the super-property, sothe chain of transitive links is maintained automatically.This means that both forms of the competency questionfor versions can be answered. The variant of asking forthe version n back in the chain would be answered withan expression like directly preceded by somedirectly preceded by version x for theversion two versions back in the list. In addition softwarewhich has a dual licensing form (often for branding) canalso be captured. In Manchester OWL this appears asfollows:Microsoft Excel 2007has_version value Microsoft 2007 versionMicrosoft 2007 versiondirectly_preceded_by Microsoft 2003 versiondirectly_followed_by Microsoft 2010 versionWe can now perform the query by using the two transi-tive parent properties which will allow us to get, for exam-ple, all predecessors. Continuing the example, for versionsofMicrosoft Excel which came before this current version,in Manchester OWL:Microsoft Excel and (has_version some (version nameor num-ber and (followed_by value Microsoft 2007 version)))which when asked of the SWO returns the classesMicrosoft Excel 2002 and Microsoft Excel2003.Merging SWO and EDAMThe SWO has a broader scope of software than EDAM,but both broadly model software in the same way. As such,EDAM is a subset of the SWO, we have been mergingEDAM into the SWO. Although much of EDAM is nowmerged into the SWO, there is still an ongoing process ofrefactoring to align these fully. Full details of the mergeprocedure can be found on the SWO blog [42]. Theprocess to date can be summarised as follows:1. Modifications to the underlying annotations withinEDAM were performed to align the structure of theontologies more closely.2. The native OBO format was converted to OWL.3. High-level EDAM hierarchies were merged into theSWO structure.Annotations and Conversion to OWLA number of annotations were added to the EDAMontology in preparation for its conversion to OWL and,ultimately, merging with the SWO. These included: The addition of the definition_editor annotationproperty from EFO to all classes with definitions,providing authorship in a manner in line with themethod already employed within the SWO. The addition of the EDAM idspace to all properties(and usages of those properties) within the OBO fileas the automated conversion to OWL creates anincorrect OWL-based namespace, The addition of appropriate alternative annotationsto the converted OWL files, as the OWLAPI 4.1 doesnot convert some annotations to OWL correctly, The automatic conversion of EDAM to OWL usingProtege 4.1s conversion feature, which makes use ofthe underlying OWLAPI. While the Protege 4.1conversion process between OBO and OWL isstraightforward, some manual changes were required[48]. The merge of EDAM into SWO and thereforeinto OWL will render this process unnecessary in thefuture.MergingThere are four high-level EDAM terms: Data, Format,Operation and Topic. These terms and their hierar-chies are in the process of being manually merged withthe SWO. The initial stages of this have been previouslydescribed in [49]. In this process, each high-level EDAMterm is compared against the SWO and either added asa subclass to an appropriate point (where no equivalentclass exists) or formally axiomatised as equivalent to apre-existing SWO class.EDAMs Format and Data have been fully merged,and can be found within the SWO as equivalent classesto data format specification and data, respec-tively. EDAMs Topic class describes broad domains orfields of interest and has no equivalent class within theSWO, and has been added without any modifications as achild of the SWOs information class.Initially it appeared that the EDAM Operation classwould be a good match for the SWO ObjectiveMalone et al. Journal of Biomedical Semantics 2014, 5:25 Page 10 of 13http://www.jbiomedsem.com/content/5/1/25hierarchy. EDAM Operation describes tasks, suchas data annotation or classification in much thesame way as SWO objective. However EDAMsObjective, defined as information describing theintended outcome of running a process, does not matchthe SWOs Operations modelling of the whole pro-cess (inputs, outputs, process and outcome). As the def-inition of EDAM Operation class fitted better underprocess in the SWO, Operation has been mergedwith information processing (a child of processin the SWO) and the two classes have been axiomatised asbeing equivalent.If EDAMs Operation had been simply placed underprocess in the SWO, then the SWO Objectiveand newly-enhanced process hierarchies would havecontained many similarities. For example, the EDAMsequence analysis class within Operationhas many similarities with the SWO classes withinObjective such as molecular sequenceanalysis. As such, Operation was first merged withthe SWO information processing, then the SWOObjective hierarchy was refactored as part of theprocess hierarchy, and finally the Objective classitself was deprecated (for further details see [50]).An additional issue arose with the EDAM classParameter. Parameter was considered a class of datain EDAMwhereas the contextual nature of whether or notsomething is a parameter would suggest it is a role in theSWO. The class metadata is a type of data in the SWObut in EDAM this is a type of report.There is also a use of asserted multiple hierarchies inEDAM, for example BioXSD (format) class is an assertedsubclass of five other classes; Alignment format(XML), Raw sequence format, Sequencefeature annotation format, Sequencerecord format and XML. The SWOhierarchy enforcesa single axis of asserted classification and multiple classi-fications are built by inference following a normalisationstyle approach [51]. EDAM did not have this strict con-straint during its development, so in the merged SWOand EDAM asserted polyhierarchy exists, however, refac-toring is ongoing to remove any remaining assertedpolyhierarchy.Some of this integration can be seen in Figure 3.The shared EDAM upper level classes with the SWO,such as data (Data in EDAM) and data formatspecification (Format in EDAM) can be seen here.Equivalence axioms were placed between classes whereintegration was clear (i.e. the ontologies referred to thesame concept but with different URIs).The SWOs polyhierarchyThe polyhierarchy produced and maintained in the SWOby this approach produces an ontology in which softwareis described along many dimensions. These dimensionsare those captured in the properties and divisions withinthe SWO. As well as license and version above, soft-ware can also be classified along the other dimensionspreviously described, such as: The data it takes as input:software and has specified data input some data The data it takes as output:software and has specified data output some data The process the software supports:software and is executed in some information processing The data format supported by the softwaresoftware and has specified data input some (data and (hasformat some data format specification))and similarly for specified data outputs Algorithms implementedsoftware and implement some algorithm Programming languagesoftware and is encoded in some programming language Developer of softwareoutput of some (software development process and(has participant value Microsoft))These dimensions can be combined in arbitrary forms,e.g., Information processing task, inputs and outputs.Defined classes instantiating these classifications are notnumerous within the SWO; instead these queries wouldbe deployed at time of use from within software applica-tions using the SWO.The SWO appliedBioMedBridges software registryThe Tools and Data Services Registry [36] is a catalogueof the prevalent bioinformatics tool and data resources,including the Web services, portals and applications usedby scientists within the BioMedBridges research infras-tructures. The registry, which is developed in a sustainableway by ELIXIR [52], requires a detailed description ofsoftware and resources. The vocabulary for this descrip-tion is provided by the SWO and EDAM, and includesthe type of software and software interface, topic (gen-eral scientific domain), function, types of input and outputdata, data formats, software maturity, supported platform,language, license and cost. The registry is built using afederated curation model in which software descriptionsare harvested from key providers and other registries,Malone et al. Journal of Biomedical Semantics 2014, 5:25 Page 11 of 13http://www.jbiomedsem.com/content/5/1/25working with these partners to ensure annotations aremade at source. For example, the registry will includecontent from BioCatalogue [53], which will also be anno-tated using the SWO and EDAM.eagle-IThe eagle-I network is a US$15 million NIH-fundedproject with the aim of facilitating biomedical research bycreating a network of research resources repositories [34].More than 50,000 resources which include biomedicaldata, software, databases and services - are listed andmore are added every week. The Software Ontologyplays an important role within the eagle-Is applica-tion ontology which is used for indexing and searchingthese resources. This includes the discovery of resourcesbased on data sets and formats, licenses and softwarefunction.Gene Expression Atlas DataThe Gene Expression Atlas has produced an RDF repre-sentation [54] which describes summaries of whether ornot a gene is differentially expressed given a particularcondition, e.g. human liver. As part of these descriptions,SWO and EDAM classes are used to capture which soft-ware analysis packages were used to produce the summaryinformation and to type data resources which link to thisgene expression data, such as an Entrez Gene DatabaseReference. SWO was also applied to the RDF export ofthis data into the new EBI RDF Platform [55] whereinthe statistical packages used to generate the results weretypedwith SWOclasses enbling querying over the specificsoftware.Evaluating the SWOOur evaluation of the SWO took two forms:1. Testing by competency questionsDo we meet thetests as supplied by the competency questions set byour customers?2. CoverageDoes it it contain terms required toannotate with?As described above, the SWO has been used to describesoftware in several settings. The informal feedback fromusers browsing the ontology is that the SWO has theappropriate shape and talks about the right features forcustomers tasks. The BMB project has, however, raisedthe issue of describing the platform upon which the soft-ware is capable of running or was run in a particularsetting. This was raised as an important issue in the SWOworkshops, but a complete description was deemed toocostly to be bought. A similar missing feature is the costof software; again, this was raised in the workshops, butwas not a high enough priority to be bought. Cost coversmany facetstheres the monetary cost, but there is alsothe cost of use and maintenance. Monetary cost is rela-tively straight-forward to model, but the other costs arehighly subjective. Our current thoughts are to use a richdescription of licences to imply whether or not a softwareis free to use and form slightly more complex axioms tocover the case when the software is free to a subset ofusers, for example free to academics and such like. Thislatter modeling of cost is now being built into the latestversions of the SWO.As well as the features described, the in-use evalua-tion naturally reveals a lack of content; the software thatneeds to be described is not present. As previously men-tioned, as well as direct submissions from the community,the SWO has more recently been evaluating against theBioNERDS list of software mentions in biomedical liter-ature and is looking to improve to 100% coverage of thetop 200 within the next 6 months. The dynamic and fluidnature of software availability and development within thebioinformatics community is an ongoing issue and is notunique to the SWO. The SWO has reused, where pos-sible, reference bio-ontologies such as the OBO RelationOntology and Information Artifact Ontology and has con-sulted with various other ontology consortia on the modelused to describe software. This has helped to populatesome small areas of the ontology more quickly than oth-ers, though generally much of what is in the SWO doesnot exist within these reference ontologies, reinforcing theneed for an ontology like the SWO.Our on-going testing and ontologising to pass failedtests works as a tactic in ontology development. However,frameworks for doing this are only nascent. The processesused in the SWO aspire to follow similar methods to thoseused in the development of production ontologies suchas the EFO at the EBI [30]. Here, continuous integrationsystems are used to test each commit of a version of anontology such that potential bugs are caught early.Discussion and conclusionsAn ontology of software is necessary for the descriptionof the data that are now central to the pursuit of life sci-ence research. Just as we need ontologies to specify thebiomedical entities that are discovered through our sci-ence, we also need a description of how those entities werediscoveredboth in the wet lab and the dry computa-tional analysis of the data produced by those biomedicalinvestigations. The SWO fits into this ontological land-scape.Descriptions of a softwares information processingtasks, the data it consumes and produces, together withthe format of those data, are central to the SWO. In addi-tion to the core areas, the SWOdescribes many peripheralbut useful concepts including software developers andtheir organizational background as well as software ver-sions, locations, and licensing. To create an ontology thatMalone et al. Journal of Biomedical Semantics 2014, 5:25 Page 12 of 13http://www.jbiomedsem.com/content/5/1/25is complete in any of these areas is ambitious. For instance,it is not feasible to describe the universe of softwaresinformation processing tasks. Instead, the SWO takes thestance of doing what is necessary for the job in hand; ourAgile approach should help in keeping the SWO fit forpurpose. Nevertheless, the SWOs conceptual frameworkseeks to be able to accommodate the changes necessary tokeep it fit for purpose.The work to integrate with EDAM has enriched theSWO with additional concepts in the areas of bioinfor-matics resources and Web services. In the context ofwider biomedical investigations, the SWO with EDAMshould play a significant role in annotating experimen-tal protocols, alongside complementary ontologies such asOBI.Biomedical ontologies typically focus on biological andmedical entities which introduces its own levels of com-plexity, particularly placing knowledge into the contextof evolution. Biomedical software faces different com-plexities; evolution is replaced with the diversities ofhuman design and practice. It is clear that this varia-tion introduces difficulties in making biomedical analysesboth describable and reproducible but this requires morethan just the appropriate ontologies to be available. Thereneeds to be a paradigm shift towards both releasing alldata associated with investigations and in describing thecomponents in sufficient detail that they are understand-able and reproducible. This issue only becomes moresalient in the age of so called Big Data, lest we face theproblems we already encountered when interpreting thecurrent archive ofMediumData [56]. This requires a com-bination of elements including tooling, funding and thetreatment of metadata as a first class citizen. An ontologyof software will play an important role in achieving thisaim.The SWO has been developed under the Apache 2.0open source license and is open to collaboration fromexternal bodies. Already, several groups are making editsto the ontology and we hope to increase this number withadditional members of the community. New user groupshave recently emerged such as the new CLI-mate [57] tooland we intend to support these activities.Competing interestsThe authors declare that they have no competing interests.Authors contributionsJM, AB, ALL, JI, DL, HP and RS contributed content to the SWO. JI is leaddeveloper of EDAM. JM and RS managed the SWO project and organised userworkshops. All authors read and approved the final manuscript.AcknowledgementsWork on the Software Ontology has been funded by the JISC SWORD projectand EPSRC grant EP/C536444/1. We acknowledge funding from EuropeanMolecular Biology Laboratory, European Bioinformatics Institute (EMBL-EBI).We would like to thank everyone that attended the SWO workshops for theirinvaluable contributions.Author details1EMBL-EBI, Wellcome Trust Genome Campus, Cambridge, CB10 1SD, UK.2School of Computer Science, University of Manchester, Oxford Road,Manchester, M13 9PL, UK.Received: 20 June 2013 Accepted: 19 April 2014Published: 2 June 2014INTRODUCTION Open AccessSelected papers from the 16th AnnualBio-Ontologies Special Interest Group MeetingLarisa N Soldatova1*, Philippe Rocca-Serra2, Michel Dumontier3, Nigam H Shah3From Bio-Ontologies Special Interest Group 2013Berlin, Germany. 20 July 2013* Correspondence: larisa.soldatova@brunel.ac.uk1Brunel University, London, UKAbstractOver the 16 years, the Bio-Ontologies SIG at ISMB has provided a forum for vibrantdiscussions of the latest and most innovative advances in the research area ofbio-ontologies, its applications to biomedicine and more generally in the organisation,sharing and re-use of knowledge in biomedicine and the life sciences. The six papersselected for this supplement span a wide range of topics including: ontology-based dataintegration, ontology-based annotation of scientific literature, ontology and data modeldevelopment, representation of scientific results and gene candidate prediction.Summary of selected papersIn 2013, the SIG received 26 submissions, including 15 papers, 5 flash updates and 6 posterabstracts. 7 papers and 6 flash updates (some papers were converted to flash updates) wereselected for presentation at the meeting, out of which 6 appear in this supplement. The sixpapers selected for this supplement are extended versions of the original papers and flashupdates presented at the 2013 SIG. The papers include research on such classic but never-theless crucially important problems as ontology-based data integration [1-3], ontology-based annotation of scientific literature [1-4], ontology and data model development[2,3,5], representation of scientific results [5] and gene candidate prediction [6].Bölling et al in the paper titled SEE: structured representation of scientific evidence inthe biomedical domain using Semantic Web techniques present an RDF/OWL basedapproach for detailed representation of scientific evidences [1]. Knowledge in biomedicineis context-dependent and based on a variety of evidences obtained by experimental obser-vations, inferences from other results, different interpretations, and modeling approaches.Bölling et al suggest RDO (the Reasoning and Discourse Ontology) - a lightweight OWLvocabulary for the representation and recording of how scientific claims are made andhow they are related to each other. It provides computationally accessible representationsof evidence-related information such as the materials, methods, assumptions and informa-tion sources used to establish a scientific finding. The proposed approach is demonstratedon the case study of evidence gathered in the literature regarding a claimed source of theenzyme glutamine synthetase. SEE resources, including the RDO ontology, are availablefrom http://purl.org/see.Soldatova et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):I1http://www.jbiomedsem.com/content/5/S1/I1 JOURNAL OFBIOMEDICAL SEMANTICS© 2014 Soldatova et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.The paper titled Statistical algorithms for ontology-based annotation of scientific lit-erature by Chakrabarti et al. reports on a probabilistic framework for annotatingBrainMap literature using the Cognitive Paradigm Ontology (CogPO) [2]. This frame-work exploits hierarchical information, dependences and restrictions available in theontology. At present, articles in the BrainMap repository are annotated manuallyaccording to CogPO definitions and it is a time and efforts intensive process that pre-sents the major bottleneck for the whole repository. The proposed annotation frame-work would enable (semi-) automated solutions for the annotation of BrainMapliterature. The proposed stochastic approaches for literature annotation were testedagainst the gold standard - the annotation by human subject matter experts, andyielded encouraging results.Merrill et al in their paper Semantic Web repositories for genomics data using theeXframe platform addresses the critical task of the integration of genomic databasesand data re-use [3]. They developed the second generation of the eXframe platformthat supports the creation of online repositories to deposit genomics data as LinkedData. The eXframe platform provides a built-in SPARQL (Sparql Protocol and RDFQuery Language) endpoint to query the data. The platform uses biomedical ontologies,e.g. OBI (the Ontology for Biomedical Investigations), DO (Disease Ontology), ChEBI(Chemical Entities of Biological Interests) ontology, to enable interoperability of theproduced repositories. The platform also provides support for accessing data usingpopular statistical programming language R. The platform has been successfully testedthrough the case study of the Stem Cell Commons project of the Harvard Stem CellInstitute. eXframe is freely available at: https://github.com/mindinformatics/exframe.Oellrich et al. in the paper titled The influence of disease categories on gene candidatepredictions from model organism phenotypes analyse Exomisers performance withrespect to disease categories provided by Orphanet [4]. Exomiser is a tool previouslydeveloped by the authors to narrow down gene candidate lists that have been identified inexome analyses using cross-species phenotype comparisons amongst other sources of evi-dence. Oellrich et al. show that the prediction results depend on the organism and whenautomatically predicting disease gene candidates careful consideration is required as towhich organism to apply for the predictions. For each disease category, they investigatedthe ten most common clinical phenotypes. Oellrich et al. found, for example, that the per-formance for zebrafish for nearly all disease categories is much more dependent on thedisease category than it is for the mouse. The authors conclude that smarter tools capableof taking into account the differences between species and accumulate predictions arerequired.The paper Evolving BioAssay Ontology (BAO): modularization, integration andapplications by Abeyruwan et al. outline the work on the development of commonreference metadata terms and definitions required for the reporting of informationabout low- and high- throughput drug and probe screening assays and results [5]. Theauthors have created BAO to support effective integration, aggregation, retrieval, andanalyses of drug screening data. Abeyruwan et al. employed a modular approach forthe development of BAO with domain-level components separated from structuralcomponents. The main components include bioassay, assay biology, assay method, assayformat, assay endpoint and assay screened entity. BAO is sufficient to enable modelingof result profiles (signatures) generated in panel and profiling assays, for example thoseSoldatova et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):I1http://www.jbiomedsem.com/content/5/S1/I1Page 2 of 3in the LINCS (the Library of Integrated Network-based Cellular Signatures) project.The authors have leveraged BAO in software tools, such as the Semantic Web softwareapplications BAOSearch, LIFE, and the BioAssay Research Database (BARD). BAO isavailable at http://bioassayontology.org.Tatum et al. in their paper titled Preserving sequence annotations across referencesequences present an RDF data model for describing sequence annotation instanceswithin an established ontological framework that fits common practice of working withreference sequences and different versions of genome assemblies [6]. Tatum et al. createdthe Reference Sequence Ontology to provide a mechanism for linking annotationinstances to different reference sequences. They also investigated how sequence annota-tions using different reference sequences can be semantically linked and identified threetypes of reference sequence relationships that are crucial for data integration. Tatum et al.present a working data model of sequence annotations that can be preserved across differ-ent reference sequence assemblies. The ontology of Reference Sequence Annotation isavailable at http://purl.bioontology.org/ontology/RSA.Competing interestsThe authors declare that they have no competing interests.AcknowledgementsAs editors of this supplement, we thank all the authors who submitted papers, the Program Committee members andthe reviewers for their excellent work. We are grateful for help from Sarah Headley from BioMed Central in puttingthis supplement together.This article has been published as part of Journal of Biomedical Semantics Volume 5 Supplement 1, 2014: Proceedingsof the Bio-Ontologies Special Interest Group 2013. The full contents of the supplement are available online at http://www.jbiomedsem.com/supplements/5/S1.Authors details1Brunel University, London, UK. 2University of Oxford, Oxford e-Research Centre, UK. 3Stanford University, CA, USA.Published: 3 June 2014