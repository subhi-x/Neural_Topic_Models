Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 
https://doi.org/10.1186/s13326-020-00223-z
RESEARCH Open Access
Enabling ad-hoc reuse of private data
repositories through schema extraction
Lars Christoph Gleim1* , Md Rezaul Karim1,2, Lukas Zimmermann3, Oliver Kohlbacher3,4,5,6,7,
Holger Stenzhorn3,8, Stefan Decker1,2 and Oya Beyan1,2
Abstract
Background: Sharing sensitive data across organizational boundaries is often significantly limited by legal and ethical
restrictions. Regulations such as the EU General Data Protection Rules (GDPR) impose strict requirements concerning
the protection of personal and privacy sensitive data. Therefore new approaches, such as the Personal Health Train
initiative, are emerging to utilize data right in their original repositories, circumventing the need to transfer data.
Results: Circumventing limitations of previous systems, this paper proposes a configurable and automated schema
extraction and publishing approach, which enables ad-hoc SPARQL query formulation against RDF triple stores
without requiring direct access to the private data. The approach is compatible with existing Semantic Web-based
technologies and allows for the subsequent execution of such queries in a safe setting under the data providers
control. Evaluation with four distinct datasets shows that a configurable amount of concise and task-relevant schema,
closely describing the structure of the underlying data, was derived, enabling the schema introspection-assisted
authoring of SPARQL queries.
Conclusions: Automatically extracting and publishing data schema can enable the introspection-assisted creation of
data selection and integration queries. In conjunction with the presented system architecture, this approach can
enable reuse of data from private repositories and in settings where agreeing upon a shared schema and encoding a
priori is infeasible. As such, it could provide an important step towards reuse of data from previously inaccessible
sources and thus towards the proliferation of data-driven methods in the biomedical domain.
Keywords: Semantic web, Linked data, RDF, SPARQL, Schema extraction, Privacy, Data access, Distributed systems,
Query design, Personal health train, FAIR data
Background
Data-driven methods play an increasingly important role
for cost-efficient and timely research results and effec-
tive decision support [2] throughout numerous domain
such as economics [3], education [4], manufacturing [5],
healthcare and life sciences [68].
At the same time, the data that build the founda-
tion of these models oftentimes underlies strict sharing
*Correspondence: gleim@cs.rwth-aachen.de
This work is an extended version of a paper previously published at the
SeWeBMeDA-2018 workshop [1].
1Informatik 5, RWTH Aachen University, Ahornstr. 55, 52062 Aachen, Germany
Full list of author information is available at the end of the article
requirements. For example, in the sensitive healthcare
domain, although first responders, hospitals, and many
other stakeholders already collect valuable data for data-
driven research and treatment today, large portions of this
data remain inaccessible to the majority of stakeholders
 largely due to ethical, administrative, legal and political
hurdles that render data sharing infeasible [9]. In prac-
tice, this leads to an inability to access large amounts of
data crucial for a variety of tasks such as the optimiza-
tion of decision support systems, first response systems
and data-driven research. At the core of this issue lies the
lack of an effective mechanism to allow for data access
in a legally certain, sustainable and cost-efficient manner
without extensive delays.
© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate
credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were
made. The images or other third party material in this article are included in the articles Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not included in the articles Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative
Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made
available in this article, unless otherwise stated in a credit line to the data.
Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 2 of 15
For example, learning health systems, allowing for data-
driven research on sensitive data such as electronic health
records (EHRs), have long been said to bear the poten-
tial to fill major knowledge gaps about health care costs,
the benefits and risks of drugs and procedures, geo-
graphic variations, environmental health influences, the
health of special populations, and personalized medicine.
[10]. While a variety of such systems have been proposed
[1013], practical implementation has so far not become
a reality, likely due to the aforementioned hurdles.
In order to enable data economy in privacy-sensitive
domains and effective reuse of existing data and research,
novel approaches are emerging to overcome these limi-
tations. One of those approaches is the Personal Health
Train (PHT) framework [14], which aims to bring algo-
rithms and statistical models to data sources, rather than
sharing data with the third parties such as researchers.
The main benefit of this approach is its ability of utilizing
all the data, including the sensitive and private informa-
tion, without data having to leave the original data source.
A key challenge of this approach is that data users (such as
researchers) are required to develop their models without
having a grasp of the actual data. Unless there are univer-
sally agreed information models and data set descriptions,
there is a need to create and communicate a schema  that
is information about the structure of the data  to enable
writing queries for heterogeneous data resources.
This work is embedded in our ongoing efforts support-
ing data reuse in healthcare environments and conducted
as part of the SMITH [15] and DIFUTURE [16] projects.
The key contributions of this paper consist of an auto-
mated approach for extracting task-relevant schema from
RDF data sources for the efficient formulation of data
selection and integration queries without direct access to
the data and a corresponding integration with an infor-
mation system architecture that allows for the subsequent
evaluation of that query in a secure enclave.
In the following, we describes some related work and
the basic foundations of our approach. Subsequently, we
outline the motivation of our research, as well as the key
challenges of schema extraction from sensitive data with-
out sacrificing privacy, followed by the description of our
proposed schema extraction approach from existing data
in the methods section. We then present a number of
evaluation results of the proposed data selection and inte-
grationmethodology, based on the schema extracted from
a sample use case. After a discussion of our results, we fin-
ish with a conclusion of our results and a short outlook of
directions for future work.
Related work
In order to facilitate knowledge discovery for both
humans and machines, the FAIR data principles [17]
have been proposed: A set of guiding principles to make
research and scientific data Findable, Accessible, Interop-
erable, and Re-usable. These guidance principles promise
to help in the discovery, access, integration and analysis of
task-appropriate scientific data and associated algorithms
andworkflows. Thus, FAIR is gaining a lot of attention and
increasing adoption.
Core to realizing these principles are Semantic Web
Technologies [18], which provide a framework for data
sharing and reuse by making the semantics of data
machine interpretable. Particularly the directed, graph-
based data model RDF [1921] (built entirely upon the
notion of statements, i.e. data in the form of subject
predicate object triples) in conjunction with formal
conceptualizations of information models, semantics and
encoding conventions in RDF vocabularies and ontologies
takes an important role.
As such, RDF Schema (RDFS) [22] and the Web Ontol-
ogy Language (OWL) [23] provide a proven framework in
order to describe (but not necessarily enforce) the struc-
ture and semantics of data. Substantially, RDFS introduces
the concepts of classes and properties as well as basic rela-
tions between them. OWL  a computational logic-based
language  extends upon these concepts in order to repre-
sent rich and complex knowledge about things, groups of
things, and relations between them.
In the context of this work, we use the term schema
to refer to the semantic and structural annotation of data
using especially these two vocabularies.
On the other hand, the classical notion of schema as
the formal definition of the shape that data needs to com-
ply with in order to be valid (i.e. schema validation and
enforcement) also exists in the Semantic Web with the
Shape Expression Language (ShEx) [24] and the Shapes
Constraint Language (SHACL) [25]. At this time, there
are however no established ways of sharing data shapes
through public repositories and as such, in practice, they
are only adapted in isolated deployments.
Nevertheless, using RDFS and OWL, it is possible to
create domain-specific, optionally interoperable vocabu-
laries and ontologies, which may declare e.g. term or con-
cept equivalences and dependencies between each other
and subsequently enable interoperability across individual
encodings.
Key to realizing the semantics described in RDFS
and OWL vocabularies is the inference or entailment
of implicit knowledge (inferred triples) that follow from
explicit knowledge (dataset triples) via the semantics
described in the corresponding vocabularies. Figure 1
illustrates some of the inferred triples that follow from
the formal RDFS and OWL entailment semantics [26].
Here we assume the namespaces ex and snomed to be
defined1.
1Likely the definitions would be http://example.org/ and http://purl.
bioontology.org/ontology/SNOMEDCT/
Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 3 of 15
Fig. 1 Illustration of several effects of entailment support of a SPARQL endpoint
In this example, only a small number of triples is con-
tained in the actual dataset, while the majority of knowl-
edge is inferred using RDFS and OWL semantics. Notably
each resource is either a class, a property or an individual,
i.e. either schema or data.
Popular examples of RDFS and OWL vocabularies
include the Ontology for Biomedical Investigation (OBI)
[27] in the biology and healthcare domain, the GoodRela-
tions ontology [28] in eBusiness and theDCAT vocabulary
[29], which is used for the general purpose metadata
annotation of datasets and data catalogs.
In the context of eHealth systems, support for the
Semantic Web is becoming more and more promi-
nent with candidates such as the multilingual thesaurus
SNOMEDCT [30], ongoing research efforts into an RDF
specification of HL7 FHIR [31], as well as the establish-
ment of clear guidelines for dataset descriptions such as
the HCLS Community Profile [32].
Various high-quality catalogs of freely reusable vocabu-
laries exist, allowing for the easy discovery of suitable ter-
minology to semantically annotate data. Examples include
the Linked Open Vocabulary (LOV) [3335] and the Bio-
Portal [3638] project.
The related idea of using schema export and import for
federated data access date back to as early as 1985 [39] but
it is only recently that the idea has receivedmore attention
in the context of the Semantic Web.
Kellou-Menouer et al. [40] propose a schema discov-
ery approach based on hierarchical clustering instead of
data annotations thus leading to an approximate schema.
Florenzano et al. [41], Lohmann et al. [42, 43] and
Dudá et al. [44] introduce approaches focused on schema
extraction for visualization of the data structure but do
not consider publishing or reuse of the extracted schema.
Benedetti et al. [45, 46] propose an interesting related
approach for schema extraction, visualization and query
generation but do not consider interoperability issues and
rely on custom mechanisms for schema storage.
Motivation
Recently, Jochems et al. [47] and Deist et al. [48]
introduced two related promising Semantic Web-based
approaches in the context of the PHT initiative, founded
on the key concept of bringing research to the data rather
than bringing data to the research. As such the underly-
ing information system architecture enables learning from
privacy sensitive data without the data ever crossing orga-
nizational boundaries, maintaining control over the data,
preserving data privacy and thereby overcoming legal and
ethical issues common to other forms of data exchanges.
The general approach of this underlying system can be
outlined as follows:
1 Initially, both the client and data provider agree upon
a set of attributes or features, such that all
participating data providers have corresponding
sources of (privacy sensitive) data.
2 Then each data provider encodes their data using an
(also agreed upon) ontology or vocabulary,
converting it into RDF representation. This process
yields proper Linked Data [49] and thus enables
semantic interoperability [50].
3 The resulting RDF data is deployed to a private triple
store at each location, providing a private SPARQL
[51] query endpoint, which is not directly accessible
by the client.
4 A SPARQL data query is then formulated based on
the previously agreed upon encoding and a
corresponding distributable processing algorithm
defined.
5 The shared query is then executed locally at each
data provider against their respective triple stores
Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 4 of 15
and the returned data processed using the
corresponding algorithm.
6 The local results are then combined into a global one.
7 Depending on the approach, steps 5 and 6 may be
further iterated.
While these approaches  introduced in the context
of the PHT initiative  work well when multiple parties
agree on jointly collecting, encoding and evaluating data
in advance  such as is the case for conducting individual
coordinated studies  they solve the issue of interoperabil-
ity by agreeing on a single shared knowledge representa-
tion and encoding methodology a priori (steps 1-3 in the
above process). In an optimal setting where agreeing on a
single shared and global information model and encoding,
reuse of diverse and existing data could always be directly
accomplished with this approach.
However, to our knowledge, so far all corresponding
efforts have been unsuccessful. At the time of writing
the popular https://fairsharing.org/ portal indexes 1084
databases using 1183 standards, suggesting that in prac-
tice, each collected dataset and domain much rather tends
to introduce its own encoding methodology.
Additionally, RDF datasets de facto often combine terms
from multiple vocabularies and ontologies, sometimes
deviating from the originally intended information mod-
els and encodings.
Thus when trying to reuse diverse existing data, a proper
understanding of the real structure of the available data 
i.e. the schema of the data  is indispensable. For a client
without direct access to the data, this information is how-
ever typically not available, since its acquisition inherently
relies upon inspection of the structure of the data.
Approaches, such as the PHT, depend upon ad-hoc
data selection and integration facilities (step 4 of the
PHT approach, corresponding to the first two steps of
the classical Knowledge Discovery in Databases (KDD)
process [52]) for the efficient and effective extraction of
knowledge from private data sources. In order to enable
the usage of such an approach with diverse existing
data, suitable methods for the extraction and distribution
task-specific schema, tailored specifically for the purpose
of enabling ad-hoc data selection and integration, are
needed.
Methods
In this section, we propose an automated approach for
extracting task-specific schema from RDF data sources in
order to enable the efficient formulation of SPARQL data
selection and integration queries without direct access to
the data. First, we describe basic requirements for the
extracted schema, as well as the fundamental idea of the
schema extraction technique before subsequently intro-
ducing a number of extensions, in order to support for
more generally applicable schema extraction methodol-
ogy. We discuss the trade-offs to be made between differ-
ent versions of the schema extraction approach and finally
show how the extracted schema can be used further for
the data selection and integration.
In the context of RDF data, the fundamental knowl-
edge required for the creation of SPARQL queries for
data selection and integration consists of the various
rdf:type objects, the rdf:Property predicates and the struc-
tural relations between them. This information can itself
be represented using Semantic Web Standards, such as
RDFS, OWL, ShEx or SHACL.
While shape languages such as ShEx and SHACL
are natural candidates for representing prescriptive data
schema, they are designed specifically for the validation of
clearly structured individual data shapes and to commu-
nicate explicit graph patterns. As such they are however
not equally well suited for the formalization of the flexible
schema of entire semi-structured datasets.
RDFS on the other hand provides a simple and descrip-
tive structural annotation of the relationships between
properties and classes and as such serves as a promising
candidate for the task at hand.
While OWL further extends RDFS with a pow-
erful set of description logic-based modeling prim-
itives, the corresponding semantic complexity adds
significant overhead to the schema extraction pro-
cess. Especially since the extracted schema is only
meant to be used for query authoring and explic-
itly not for reasoning, in the context of this work
we generally restrict our effort to extracting schema
using RDFS and the OWL owl:equivalentClass,
owl:equivalentProperty and owl:sameAs pred-
icates, which we deem most relevant in order to enable
interoperability and the effective formulation of selection
and integration queries.
Especially in order to ensure interoperability with
existing Semantic Web technologies and compatibility
with standard Semantic Web tools, such as schema-
introspection-assisted SPARQL query builders, the
extracted schema should thus be available as a simple
RDFS and OWL vocabulary via a SPARQL endpoint.
Schema-introspection refers to the process of examin-
ing the schema definition to determine which types of
entities exist, which properties are defined upon them and
subsequently, what can be queried for. Since the schema
needed to create data queries (e.g. using SPARQL) only
contains basic structural information about the original
data, it also conveys far less privacy critical information
than exposing the actual data. As such it can be published
publicly without privacy concerns in many scenarios.
In the following, we describe an automated approach for
schema extraction fromRDF data which allows for the for-
mulation of data selection and integration queries without
Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 5 of 15
direct access to the data and the subsequent evaluation of
that query in a secure enclave.
Schema extraction
We propose an approach for schema extraction based
on exploiting key characteristics of RDF, RDFS, and
OWL. RDF data encoded in compliance with correspond-
ing vocabularies inherently include metadata about their
semantics and structural relationships.
For the schema extraction, the rdf:type relation plays
the key role, as it declares data points to be instances of
specific data types or, according to RDFS terminology and
semantics [53], classes. Anything that is a type in the sense
of occurring as the target of this relation should thus auto-
matically becomes part of the schema as an entity of type
rdfs:Class. Additionally, any property relation (that
is any identifier occurring in the predicate position of a
subject-predicate-object triple) which occurs in the data
should be included as an entity of type rdf:Property.
Finally all directly describing properties of these classes
and properties should be included as well. For the scope
of this work, we assume that all data in the private data
repository is sensitive and should remain private.
Entailment supported schema extraction Assuming
perfect conditions, namely proper inclusion of all used
vocabularies into the triple store, correct usage of those
vocabularies, as well as OWL entailment [26] support of
the SPARQL endpoint providing access to the data, the
entire schema of a given RDF data set can be extracted
using a single simple SPARQL CONSTRUCT query as
depicted in Listing 1.

CONSTRUCT {?s ?p ?o}
WHERE {
{[] ?s []}
UNION {[] a ?s} .
?s ?p ?o .
}
 
Listing 1 SPARQL schema extraction query relying on proper
entailment support of the endpoint.
Note that we explicitly define the relevant subset of all
available schema information to be that which is actually
used in the data, i.e. the instantiated schema, and thus only
extract that.
The preceding query constructs an RDF graph (line 1)
containing all the directly describing triples ?s ?p ?o
that occur in the tripe store but having only the following
subjects:
1 Instantiated RDF properties ?s (line 3) which
according to RDF 1.1 Semantics [53] are any IRI used
in predicate position (c.f. rdfD2).
2 Instantiated RDFS classes ?s (line 4) via their
occurrence as the object of a triple with rdf:type
as the predicate. The fact that these are RDFS classes
follows directly from the RDFS axiomatic triple
rdf:type rdfs:range rdfs:Class . in
conjunction with RDFS entailment pattern rdfs3
[53].
According to the SPARQL entailment regime, all the
subclass relationships, transitive properties, equivalences
etc. used in the data are automatically materialized (i.e.
included in the dataset as inferred knowledge as illus-
trated in Fig. 1) and thus resolved and included too (c.f.
[53, 54]).
It should be noted that the query only extracts direct
properties (i.e. triples ?s ?p ?o directly related to the
subject ?s) and as such, some complex constraints such
as OWL disjointness axioms are not included in the
extracted schema. However, as stated before, for the task
of query formulation we consider this to be sufficient.
Directly instantiated schema Since in practice few
SPARQL endpoints actually support any kind of entail-
ment and usually do not materialize implicit triples, the
applicability of this basic approach is limited. While the
original query can theoretically also be executed with-
out entailment support, it does not guarantee that all
used properties and classes are annotated accordingly
as rdf:Property and rdf:Class and completely
ignores any resource ?s that lacks further describing
triples ?s ?p ?o.
Thus, in the following we introduce several revisions
of the initial extraction query 1 that allow us to reintro-
duce the missing triples without relying upon entailment
support. Additionally, many datasets de facto employ
terms from a number of different vocabularies and ontolo-
gies and deviate from the originally intended informa-
tion model. Since the availability of information about
domain and range of the different properties employed
in the dataset is especially relevant in order to assist the
query creation process, we further explicitly construct
rdfs:domain and rdfs:range statements according
to the propertys respective usage in the dataset.
In scenarios where it is sufficient to consider only those
types and properties that are directly used in the dataset
or where no information whatsoever about the employed
vocabularies is available, it can be reasonable to disregard
the inference generalizations and equivalences entirely.
Listing 2 proposes a SPARQL query for the extraction
of a corresponding schema, which closely reflects the
structure of the underlying data and works even if the
definitions of the employed ontologies are unavailable.
For this and all further queries, we assume standard
SPARQL namespace and prefix definitions as specified by
the World Wide Web Consortiums OWL and SPARQL
specifications [53, 55].
Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 6 of 15

CONSTRUCT {
?predicate ?a ?b; a rdf:Property;
rdfs:domain ?pDomain; rdfs:range ?
pRange.
?concept ?c ?d; a rdfs:Class.
} WHERE {
?s ?predicate ?o.
OPTIONAL {?s a ?pDomain}
OPTIONAL {?o a ?pRange}
OPTIONAL {?predicate ?a ?b}
[] a ?concept
FILTER(!isBlank(?concept))
OPTIONAL {?concept ?c ?d}
}
 
Listing 2 Basic SPARQL schema extraction query which only
discovers RDFS classes and properties directly instantiated in the
queried dataset.
Analogously to query 1, we detect predicates as any
Internationalized Resource Identifier (IRI) used in predi-
cate position (line 5) and classes as IRIs used as objects of
RDF type triples (line 9). We also include any additional
information directly relating to those subjects that might
be available in the dataset (lines 8 and 11). To explicitly
construct rdfs:domain and rdfs:range information
of the predicates, we further determine the rdf:type
of each subject (line 6) and object (line 7), if available.
Additionally we filter out any class declarations without
an own identifier (line 10) to avoid potential referenc-
ing issues with the extracted schema. Lastly we construct
the schema graph as all discovered predicates (explicitly
typed as rdf:Property) and their related informa-
tion (line 2) and all discovered classes (explicitly typed as
rdfs:Class) and their related information (line 3).
When applying this extraction approach to the dataset
depicted in Fig. 1, we end up with the schema depicted in
Fig. 2 where classes are highlighted in blue and properties
in green (i.e. with implicit rdf:type triples).
Subsequently, in this exemplary use case, following the
extracted schema closely one could query for instances
of the ex:Patient class and their corresponding prop-
erty ex:treatedAt, which however perfectly reflects
the available dataset without inferred knowledge.
It should be noted, that this extracted schema is explic-
itly not suited for triple entailment according to RDFS
semantics, due to the conjunctive nature of multiple
rdfs:domain and rdfs:range definitions on prop-
erties (c.f. RDFS entailment patterns rdfs2 and rdfs3
[53]). A semantically correct alternative would be the
usage of Schema.orgs schema:domainIncludes and
schema:rangeIncludes properties in line 2, instead
of their RDFS equivalents. However, since RDFS domain
and range semantics are implemented in a variety of tools
for schema exploration, visualization and assisted query
authoring [5658], while schema.org semantics are not
equally well supported, we deliberately defer semantic
correctness to a closer representation of the underlying
datas structure.
Locally inferred schema In order to re-include previ-
ously inferred information such as additional types and
classes due to sub-property, subclass, domain, range or
equivalence relationships, we can extract the relevant
schema directly from the data and the full definitions of
the employed ontologies using the SPARQL 1.1 Property
Paths [59] feature, independent of entailment support or
statement materialization on the endpoint.
A corresponding SPARQL query is depicted in Listing 3.

WHERE {
?s ?x ?o. OPTIONAL {?s a ?pDomain}
OPTIONAL {?o a ?pRange}
?x (rdfs:subPropertyOf|owl:
equivalentProperty|^owl:
equivalentProperty
|owl:sameAs|^owl:sameAs)* ?predicate
OPTIONAL {?predicate ?a ?b}
{?predicate (rdfs:range|rdfs:domain) ?y}
UNION {[] a ?y}
?y (rdfs:subClassOf|owl:equivalentClass|^
owl:equivalentClass|owl:
sameAs|^owl:sameAs)* ?concept
FILTER(!isBlank(?concept))
OPTIONAL {?concept ?c ?d}}
 
Listing 3 Extended WHERE clause of schema extraction query 2
employing SPARQL 1.1 Property Paths to emulate RDFS
specialization, domain and range semantics, as well as OWL
equivalence entailment.
The query constructs a graph, which in addition to all
instantiated RDFS classes and RDF properties (and their
direct properties) includes generalizations and equivalent
resources of those via RDFS and OWL semantics.
For both properties and classes, we resolve corre-
sponding generalizations directly using the relevant
RDFS entailment patterns (rdfs5, rdfs7, rdfs9, rdfs11)
[53] and concept equivalences using OWLs owl:
equivalentClass, owl:equivalentProperty
and owl:sameAs predicates [54] in lines 5 and 9. While
owl:sameAs is only supposed to be used for the decla-
ration of equivalence between individuals, it is commonly
misused in practice and as such deliberately included in
this query.
rdfs:Class annotations are further inferred follow-
ing RDFS entailment rules rdfs2 and rdfs3 [53] from
rdfs:domain and rdfs:range properties declared
on instantiated rdf:Property resources (line 7).
When applying this extraction approach to the dataset
depicted in Fig. 1, we end up with the relevant schema
depicted in Fig. 3. As before, classes are highlighted in blue
and properties in green.
Following the extracted schema, it is now also possible
to query for instances of the hospital and person classes, as
Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 7 of 15
Fig. 2 Directly instantiated schema extracted from Example 1
well as a number of equivalent SNOMEDCT vocabulary
terms.
Employing terminology services
In practice, individual SPARQL endpoints providing
access to individual datasets cannot be (and are not) bur-
dened with serving all vocabularies and terminologies
used in the dataset and related to those. That is the pur-
pose of specialized terminology services and vocabulary
catalogs, such as the aforementioned LOV and BioPortal
projects.
In order to resolve equivalences and generalizations
across vocabularies, it is thus possible to make use of the
SPARQL 1.1 Federated Query protocol [60, 61] in order
to entail additional schema triples using external termi-
nology services. The query depicted in Listing 4 employs
federated queries to the SPARQL endpoint http://
example.org/terminology in order to accomplish
this. The query further explicitly filters out all subject that
are blank nodes in order to avoid renaming and resolu-
tion issues between blank nodes from different sources
(c.f. [60]).
While the approach follows the same principles as the
previously introduced local inference (c.f. Listing 3), here
each inference step also includes results from the exter-
nal terminology service. As such, following the exam-
ple from before, the extracted schema would now also
include all inferred knowledge from the SNOMEDCT
vocabulary as well as any vocabulary known to the
terminology service that declares equivalences with
SNOMEDCT.
In some cases, such as with rare diseases, even the
limited communication with remote terminology services
might affect data privacy, since the instantiation of cer-
tain very rare classes or predicates might in itself reveal
private data. In such cases a local terminology service can
be employed, i.e. by creating a local deployment of the
LOV service or by providing local copies of the relevant
full vocabularies. Nevertheless, sharing of the extracted
schema in such cases may still require additional consid-
erations.
Unfortunately, current implementations of federated
SPARQL queries still typically incur large performance
penalties by using suboptimal resolution strategies. As
such, in practice, it is often helpful tomanually decompose
the single query into multiple query steps. An exem-
plary four-step approach using the SPARQL 1.1 UPDATE
construct [62, 63] can be found in the supplementary
Fig. 3 Locally inferred relevant schema extracted from example 1
Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 8 of 15
materials2, which also includes performance optimized
reformulations of the other queries.

WHERE {
?s ?x ?o.
OPTIONAL {?s a ?pDomain}
OPTIONAL {?o a ?pRange}
{?x (rdfs:subPropertyOf|owl:
equivalentProperty|^owl:
equivalentProperty|owl:sameAs|^owl:sameAs)
* ?predicate}
UNION {SERVICE < http://example.org/
terminology> {
?x (rdfs:subPropertyOf|owl:
equivalentProperty|^owl:
equivalentProperty|owl:sameAs|^owl:sameAs)
* ?predicate
}}
{[] a ?y}
UNION {
{?predicate (rdfs:range|rdfs:domain) ?y}
UNION { SERVICE < http://example.org/
terminology> {
?predicate (rdfs:range|rdfs:domain) ?y
}}
} FILTER(!isBlank(?y))
{?y (rdfs:subClassOf|owl:sameAs|^owl:
sameAs|owl:equivalentClass|^owl:
equivalentClass)* ?concept}
UNION {SERVICE < http://example.org/
terminology> {
?y (rdfs:subClassOf|owl:sameAs|^owl:sameAs
|owl:equivalentClass|^owl:
equivalentClass)* ?concept
}} FILTER(!isBlank(?concept))
OPTIONAL {
{?predicate ?a ?b}
UNION {SERVICE < http://example.org/
terminology> {?predicate ?a ?b}}
}
OPTIONAL {
{?concept ?c ?d}
UNION {SERVICE < http://example.org/
terminology> {?concept ?c ?d}}
}
}
 
Listing 4 Extended WHERE clause of query 3 for entailing class
and property equivalences and generalizations using an external
terminology service.
Schema-aided data selection and integration
Once the schema is extracted, the resulting schema can
be publicly or semi-publicly (e.g. with prior authentica-
tion) exposed using a dedicated SPARQL endpoint. It is
then possible to use existing SPARQL query writing assis-
tance tools (i.e. query builders) such as OWLPath [64],
QueryVOWL [58] or VSB [57] together with the extracted
schema for schema introspection aided design of data
selection and integration queries without direct access to
2https://github.com/PersonalHealthTrainGermany/schemaExtraction
the private dataset. An overview of available tools can be
found in [65].
Figure 4 depicts a screenshot of the visual query
builder VSB [57], configured to employ introspection of
a schema extracted using the locally inferred approach,
as conducted in the following evaluation. Correspond-
ing instructions for schema extraction and deployment
can be found in the supplementary materials. This exam-
ple illustrates how introspection of the public schema
allows for the automated suggestion and autocompletion-
assisted search for available properties and classes, as
well as the relations between them, enabling easy query
writing through interactive schema exploration. In the
depicted case, the user is interested in instances of the
schema:Person class and provided with a list of prop-
erty suggestions for the search string fa, as available in
the original private data.
Such tools may optionally also employ the provided
schema in order to construct SPARQL 1.1 queries that
can resolve term generalizations and equivalences follow-
ing the semantics of the extracted schema. As such, the
user does not have to rely upon proper entailment sup-
port of the dataset SPARQL endpoint but can construct
explicit queries that specify the relevant equivalences, fur-
ther enabling ad-hoc data integration queries through the
provided resource equivalences.
As such, e.g. in the example depicted in Fig. 3, it is likely
that the private data endpoint does not support entail-
ment. Thus, the query must be constructed in a way to
account for the semantic implications of the schema. For
example, in order to find all persons, one would have to
query not only for all instances of the person class, but also
for all instances of its equivalent classes, subclasses, their
equivalent classes, as well as those that occur as subject or
object of a property with corresponding domain or range,
in this case subject of a triple with ex:treatedAt pred-
icate. Query builders and query writing assistance tools
can however automatically construct queries accounting
for this without burdening the user. Such queries thus
allow for the ad-hoc integration of data encoded with
different ontologies and standards, based only on the
previously extracted schema.
System architecture The workflow of the proposed
architecture is illustrated in Fig. 5, which depicts the com-
munication between client and data provider over a public
network. In this scenario, the data providers internal
communication within its private network is highlighted
by the bounding box.
In preparation for client usage, the schema of the
sensitive data stored in the private triple store
is extracted in step 0 using the approach presented
above and deployed to a publicly accessible schema
endpoint.
Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 9 of 15
Fig. 4 An example of a visual SPARQL query builder tool interacting with the schema introspection endpoint to enable assisted query design
Since the private data store remains inaccessible from
outside its private network at all times, the schema
extraction has to be conducted by the data provider
herself. This could either be done by manually extract-
ing the schema on-demand, e.g. using the four-step
LOV inferred schema extraction approach employing
the SPARQL Update construct, by automatically running
a corresponding extraction script in regular time intervals
or by creating a schema view for the data store, which
can then directly be queried by data consumers.
Once the schema endpoint is available, the client can
start to create a SPARQL query in step 1, using a query
builder of their choice in conjunction with the schema
endpoint for introspection. The query is then sent to a
submission endpoint acting as the gateway between
the data provider and the client in step 2. For the scope
Fig. 5Workflow of the proposed architecture
Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 10 of 15
of this work, we assume that this requests includes algo-
rithmic means of data anonymization, ensuring its results
are no longer privacy sensitive and that validation is done
manually.
Once validated, the request is scheduled in step 4 for
processing within a secure enclave (processing), where
the query and algorithm are evaluated (step 5). This is
analogous to the approach proposed by Jochems et al. [47]
and Deist et al. [48] as detailed in the related work section.
Finally, only the processing result is returned to the client
in step 6 without ever directly granting access to the
data.
Evaluation
In order to evaluate the proposed approach, we extract
schema information from a synthetic dataset of patient
records (PRs), specifically generated in order to illus-
trate the intended use case, as well as the three corpora
GenDR, Orphanet and NCBI Homologene, as distributed
through the third release of the interlinked life science
data repository Bio2RDF [66].
The PRs dataset contains personal information of
10,000 individuals such as name, birthday and phone
number and is published in conjunction with this paper.
The dataset was generated using the open source gener-
atedata tool3 and converted to a corresponding dataset of
15,0000 RDF triples using the SPARQL Generate exten-
sion [67, 68]. Half of the records are encoded using
the FoaF vocabulary [69] and half using the Schema.org
vocabulary [70].
GenDR [71] is a database of genes associated with
dietary restriction (DR), intended to facilitate research
on the genetic and molecular mechanisms of DR-induced
life-extension.
Orphanet[72] is a database of information on rare dis-
eases and orphan drugs for all publics, intended for the
improvement of the diagnosis, care and treatment of
patients with rare diseases.
HomoloGene [73] is a database of homolog sequence
relationships between 20 completely sequenced and anno-
tated eukaryotic genomes.
GenDR, Orphanet and Homologene respectively pro-
vide custom vocabulary definitions describing their data
encoding and semantics.
All four datasets were separately deployed to a pri-
vate triple store and relevant schema extracted using the
three presented direct extraction methods, i.e. extract-
ing only directly instantiated properties and classes
using query 2, using local inference together with the
respectively employed vocabularies (such as FoaF and
Schema.org definitions for the PRs dataset) via query 3
and finally using the LOV terminology server via query 4.
3https://github.com/benkeen/generatedata
The employed data and scripts may be found in the
supplemental materials.
Results
In order to evaluate the effectiveness of the schema
extraction process, we employ the HCLS core statistical
measures [32] to compare the characteristics of the vocab-
ularies, datasets and the extracted schema. Table 1 lists
results for the entire Linked Open Vocabularies dataset
(employed in the terminology service), the full datasets
PRs, GenDR, Orphanet and Homologene, the respective
complete vocabularies employed for coding the datasets
as well as the three extracted schemata per dataset.
On first sight, the results clearly show that for all three
approaches, the number of extracted triples is signifi-
cantly lower compared to the respective combined source
data, thus reducing the cognitive and computational effort
required for schema introspection.
For the directly instantiated properties and classes, we
have to compare the extracted schema directly with the
full dataset, since there is no other available vocabulary
definition in the data source to compare it to. For the
PRs dataset, only 47 instead of 15,0000 (i.e. about 0.03%
of the number of triples contained in the full dataset) are
included in the schema. Nevertheless, manual validation
shows that the 23 subjects are exactly the 20 properties
and three classes found in the full dataset.
Similarly, with 361, 799 and 184 triples, the size of the
extracted schema for GenDR, Orphanet and Homolo-
gene datasets is about 3.11%, 0.21% and 0.03% of the
respective full dataset. However, compared to the triple
counts of the corresponding authoritative vocabularies,
there is a significant amount of additional information
in the extracted schema with 361:192 (?88% overhead),
799:402 (?99% overhead) and 184:62 (?197% overhead)
triples. This characteristic is proportionally reflected in
the number of typed entities and subjects extracted with
37:20 (+85%), 67:40 (+67.5%) and 24:7 (+200%). Closer
inspection reveals that the additional subjects in the
extracted schema are in fact additional properties and
classes in the dataset, which are not included in the
respective authoritative dataset vocabulary but stem from
usage of terms from additional vocabularies within the
dataset. As such, reliance onto the data model specified in
the authoritative vocabulary when creating data queries,
could actually hinder making proper usage of the full
available data, while the extracted schema more closely
reflects the actual data structure at hand.
Manual validation (c.f. supplementary materials) shows
that for the GenDR and Homologene datasets all sub-
jects of the authoritative vocabulary are also included
in the extracted schema. For Orphanet all but one
are included, the http://bio2rdf.org/orphanet_vocabulary:
Disorder-Gene-Association class, which itself does not
Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 11 of 15
Table 1 HCLS core statistics [32] of evaluated datasets, vocabularies and extracted schemata
HCLSMetric 6.6.1.1 6.6.1.2 6.6.1.3 6.6.1.4 6.6.1.5 6.6.1.6 6.6.1.7
number of unique triples typed entities subjects properties objects classes literals
LOV Vocabulary Corpus 833834 129827 171168 1209 145498 1469 180680
PRs Dataset 150000 20000 20000 20 10003 3 70717
Vocabulary schema.org (1) 8427 1617 1619 15 476 31 3193
foaf (2) 631 84 86 15 38 9 154
merge of (1), (2) 9058 1701 1705 23 508 38 3335
Schema directly instantiated 47 23 23 3 5 2 0
locally inferred 576 95 95 13 71 9 118
LOV inferred 2345 208 208 87 379 16 850
GenDR Dataset 11609 1123 1123 24 1232 13 5158
Vocabulary GenDR Vocabulary 192 20 20 8 6 5 116
Schema directly instantiated 361 37 37 10 16 7 105
locally inferred 380 37 37 10 16 7 124
LOV inferred 911 71 71 58 127 12 370
Orphanet Dataset 377947 28871 28871 38 42891 29 144773
Vocabulary Orphanet Vocabulary 402 40 40 9 7 5 239
Schema directly instantiated 799 67 67 12 41 7 217
locally inferred 840 68 68 12 41 7 256
LOV inferred 1380 102 102 59 153 12 506
Homologene Dataset 7189742 869981 869981 14 1420471 10 2865019
Vocabulary Homologene Vocabulary 62 7 7 8 6 5 38
Schema directly instantiated 184 24 24 10 13 7 40
locally inferred 190 24 24 10 13 7 46
LOV inferred 721 58 58 58 124 12 292
occur in the dataset but is a superclass of 8 instantiated
and correctly included classes. This superclass is however
included in the locally inferred version of the extracted
schema and thus illustrates the proper functioning of the
rdfs:subClassOf inference. Since the validation fur-
ther shows, that for all extracted schemata, the triples
contained in the directly instantiated schema are a sub-
set of those in the locally inferred one, which are in turn
a subset of the LOV inferred schema, all subjects ocur-
ring in the authoritative vocabulary are also contained in
all other extracted schemata.
With 576 triples, the locally inferred schema of the PRs
dataset is about 6.36% of the size of the union of the full
employed vocabularies and a superset of the previously
extracted schema of directly instantiated properties and
classes. As intended, only the subset of the full vocabular-
ies that actually describes the private dataset (and as such
is actually relevant to the data) is extracted, allowing for
focused query design based on only the relevant schema,
thus saving cognitive as well as computational effort dur-
ing schema introspection. Manual validation supports the
correctness and completeness of the extracted schema.
Similarly, the locally inferred schemata of GenDR,
Orphanet and Homologene are extended versions of their
respective locally instantiated variants, enriched by rele-
vant semantically inferred knowledge from the respective
full authoratative vocabularies, such as the entailment of
generalizations and equivalent classes and properties.
Since these extracted schemata also contain explicit
equivalence information (for example between the
foaf:Person and schema:Person, which is in this
case only declared in the schema.org vocabulary) it is
possible to explicitly design queries considering the cor-
responding implications at query design time without
relying upon inference support of the SPARQL endpoints.
As such it may provide an additional building block for
enabling efficient interoperability across different data
codings.
Finally the schema inferred using the central LOV ter-
minology service extends the locally inferred schema
further by entailing additional schema equivalences, gen-
eralizations and knowledge. The extracted schema for the
PRs dataset consists of 2345 triples, which roughly equals
0.28 percent of the entire LOV corpus and is yet again
Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 12 of 15
a superset of the locally inferred schema. Overall it con-
tains triples occurring in 265 of the 648 vocabularies that
make up the entire LOV dataset. As such it provides a
valuable source for semantic integration of data across
various vocabularies used for coding data. Similarly, with
911, 1380 and 721 triples, the LOV inferred schemata for
the GenDR, Orphanet and Homologene datasets weigh in
at about 0.11%, 0.21% and 0.09% compared to the full LOV
corpus.
While no in-depth evaluation of the runtime has been
conducted, it might be of interest that the extraction of
the LOV inferred schema takes about one minute for
the largest evaluated dataset (Homologene) and under ten
seconds for all other datasets, using two Fuseki4 SPARQL
server processes serving as triple store and terminology
server on a single Intel i7-8700k desktop CPU. Hereby
the runtime is largely dominated by the explicit construc-
tion of rdfs:domain and rdfs:range properties, not
by the SPARQL federation to the terminology service, as
illustrated by the fact that the extraction of the locally
instantiated schema took only between two to four sec-
onds less extraction time in all evaluated datasets.
Discussion
The presented schema extraction and architecture strive
to close a gap between owners and consumers of sensi-
tive data. While related work has already provided us with
basic infrastructure in order to allow for the processing of
data under the owners control, to our best knowledge, all
existing approaches relied upon a-priori agreement upon
a shared schema and data encoding.
In contrast, the approaches presented in this work are
capable of extracting relevant (i.e. instantiated) schema
from a given RDF dataset with a configurable amount of
inferred information based on RDFS and OWL semantics,
which can subsequently be used for SPARQL query design
without requiring access to the original data.
As such, ongoing research efforts such as the Personal
Health Train initiative could benefit significantly from
implementing this or a similar approach in order to enable
optional interoperability and ad-hoc data integration and
re-use. Especially given the challenges of trans-national
standardization efforts of vocabularies and data models,
the evolution of such standards over time and the need of
individual research to diverge from standards, we believe
a system such as the one presented in this work to be
essential for the realization of effective data reuse.
As illustrated in Fig. 4, the schema extraced using
our approach is suitable for usage with existing schema-
introspection-assisted SPARQL query writing tools. As
such it does enable the formulation of ad-hoc SPARQL
data-integration queries against RDF triple stores without
4https://jena.apache.org/documentation/fuseki2/index.html
requiring direct access to the private data. The approach
is compatible with existing Semantic Web-based tech-
nologies and in can be employed in conjunction with
the presented system architecture for the subsequent exe-
cution of such queries in a safe setting under the data
providers control, i.e. in the context of the PHT initiative.
Thus the presented approach can enable the ad-hoc reuse
of private data repositories through schema extraction.
While we believe the current approach to be universally
applicable to any data domain (just as the underlying RDF
data and RDFS/OWL semantics model), many areas are
currently lacking authoritative terminology servers with
SPARQL endpoints and support for inference or SPARQL
1.1 features required for any manual inference using our
methodology. While the presented approaches work well
in conjunction with e.g the Linked Open Vocabularies
project, compatibility with e.g. the important Bioportal
project is hindered by its SPARQL endpoint, which lacks
entailment and SPARQL 1.1 support.
Additionally, many vocabularies introduce custom
schema semantics that go beyond RDFS and the subset
of OWL that we consider in this work. Examples include
schema.orgs domainIncludes and rangeIncludes
for properties or Wikidatas own terms for class and prop-
erty equivalences and concept generalization. While it is
easily possible to extend the presented schema extrac-
tion mechanism to account for these additional terms,
one may nevertheless wonder about the reasonableness of
the redefinition of these basic schema concepts in vari-
ous vocabularies. Nevertheless, as schema definition lan-
guages, vocabularies with their own semantics and related
requirements evolve, we believe that a flexible solution
such as the one presented in this work will only growmore
relevant in practical applications in order to bridge the gap
between competing systems and standards.
Conclusion and outlook
In this paper, we proposed an automated way of schema
extraction from Linked Data in RDF format which enables
the introspection supported development of SPARQL
queries without direct access to the actual data. The
approach further allows for the extraction of a config-
urable amount of semantically inferred schema and the
resolution of equivalences across multiple vocabularies
and standards. As such, it could provide an important
building block in order to enable optional interoperabil-
ity across competing standards and data encodings. Based
on existing Semantic Web Technologies and inspired by
recently published work in the context of the Personal
Health Train initiative, we further presented a system
architecture to realize reuse of data locked in private
repositories without having to share the actual data. From
the users perspective, our approach enables straight for-
ward query formulation against privacy sensitive data
Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 13 of 15
sources and successive evaluation of that request in a
secure enclave at the data providers end.
With this architecture, we can overcome the reliance
of previous approaches on agreeing upon shared schema
and encoding a priori in favor of more flexible schema
extraction and introspection. While the methodology is
designed specifically with the context of the PHT in mind,
the approach is likely equally applicable to the broader
context of semantic data exploration as well. As such,
the presented method promises to provide a key building
block in enabling efficient reuse of data across a variety of
domains. In conjunction with advanced distributed learn-
ing and processing systems, the approach could be used
in order to overcome existing data sharing hurdles and
unlock hidden value in existing data silos.
Availability of data and supplementary materials
All source code and all data generated or analyzed as part of this work are
documented and publicly accessible on GitHub at https://github.com/
PersonalHealthTrainGermany/schemaExtraction.
Abbreviations
DR: Dietary restriction; EHR: Electronic health record; GDPR: EU general data
protection Rules; IRI: Internationalized resource identifier; KDD: Knowledge
discovery in databases; LOV: Linked open vocabulary; OBI: Ontology for
biomedical Investigations; OWL: Web ontology language; PHT: Personal health
train; RDF: Resource description framework; RDFS: RDF schema; SHACL: Shapes
constraint language; ShEx: Shape expression language; SPARQL: SPARQL
protocol and RDF query language
Acknowledgements
This paper is an extended version of our previous workshop paper [1]
presented at the Extended Semantic Web Conference (ESWC2018) Workshop
on Semantic Web Solutions for Large-scale Biomedical Data Analytics
(SeWeBMeDA), in Crete, Greece, 3-4 June 2018. The authors would like to
thank the anonymous reviewers for their helpful comments and improvement
suggestions.
This work was conducted jointly by RWTH Aachen University, Tubingen
University and Fraunhofer FIT as part of the PHT and GoFAIR implementation
network, which aims to develop a proof of concept information system to
address current data reusability challenges of occurring in the context of
so-called data integration centers that are being established as part of
ongoing German Medical Informatics BMBF projects (https://www.bmbf.de/
de/medizininformatik-3342.html).
Authors contributions
LG, MRK, LZ and OB participated in the project design and goal formulation.
LG and LZ gathered requirements for the schema extraction. MRK contributed
data and ideas. LG drafted the paper, designed the schema extraction and
conducted the evaluation. MRK, OB, OK, HS and SD contributed feedback and
reviewed the manuscript. All authors read and approved the final manuscript.
Funding
This work was supported by the German Ministry for Research and Education
(BMBF) as part of the SMITH consortium (LG, MRK, OB and SD, grant no.
01ZZ1803K) and the DIFUTURE consortium (OK, HS, and LZ, grant no.
01ZZ1804D).
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Author details
1Informatik 5, RWTH Aachen University, Ahornstr. 55, 52062 Aachen, Germany.
2Fraunhofer FIT, Schloss Birlinghoven, 53754 Sankt Augustin, Germany.
3Institute for Translational Bioinformatics, University Hospital Tübingen, Sand
14, 72076 Tübingen, Germany. 4Applied Bioinformatics, Department of
Computer Science, University of Tübingen, Sand 14, 72076 Tübingen,
Germany. 5Institute for Bioinformatics and Medical Informatics, University of
Tübingen, Sand 14, 72076 Tübingen, Germany. 6Quantitative Biology Center,
University of Tübingen, Auf der Morgenstelle 10, 72076 Tübingen, Germany.
7Biomolecular Interactions, Max Planck Institute for Developmental Biology,
Max-Planck-Ring 5, 72076 Tübingen, Germany. 8Institute for Medical Biometry,
Epidemiology und Medical Informatics, Saarland University Medical Center,
Kirrberger Str., Building 86, 66421 Homburg, Germany.
Received: 25 October 2018 Accepted: 23 July 2019
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 
https://doi.org/10.1186/s13326-020-00221-1
RESEARCH Open Access
Neural side effect discovery from user
credibility and experience-assessed online
health discussions
Van-Hoang Nguyen* , Kazunari Sugiyama, Min-Yen Kan and Kishaloy Halder
Abstract
Background: Health 2.0 allows patients and caregivers to conveniently seek medical information and advice via
e-portals and online discussion forums, especially regarding potential drug side effects. Although online health
communities are helpful platforms for obtaining non-professional opinions, they pose risks in communicating
unreliable and insufficient information in terms of quality and quantity. Existing methods in extracting user-reported
adverse drug reactions (ADRs) in online health forums are not only insufficiently accurate as they disregard user
credibility and drug experience, but are also expensive as they rely on supervised ground truth annotation of
individual statement. We propose a NEural ArchiTecture for Drug side effect prediction (NEAT), which is optimized on
the task of drug side effect discovery based on a complete discussion while being attentive to user credibility and
experience, thus, addressing the mentioned shortcomings. We train our neural model in a self-supervised fashion
using ground truth drug side effects from mayoclinic.org. NEAT learns to assign each user a score that is
descriptive of their credibility and highlights the critical textual segments of their post.
Results: Experiments show that NEAT improves drug side effect discovery from online health discussion by 3.04%
from user-credibility agnostic baselines, and by 9.94% from non-neural baselines in term of F1. Additionally, the latent
credibility scores learned by the model correlate well with trustworthiness signals, such as the number of thanks
received by other forum members, and improve credibility heuristics such as number of posts by 0.113 in term of
Spearmans rank correlation coefficient. Experience-based self-supervised attention highlights critical phrases such as
mentioned side effects, and enhances fully supervised ADR extraction models based on sequence labelling by 5.502%
in terms of precision.
Conclusions: NEAT considers both user credibility and experience in online health forums, making feasible a
self-supervised approach to side effect prediction for mentioned drugs. The derived user credibility and attention
mechanism are transferable and improve downstream ADR extraction models. Our approach enhances automatic
drug side effect discovery and fosters research in several domains including pharmacovigilance and clinical studies.
Keywords: Online health communities, Drug side effect discovery, Credibility analysis, Deep learning, Natural
language processing
*Correspondence: vhnguyen@u.nus.edu
School of Computing, National University of Singapore, 13 Computing Drive,
117417 Singapore, Singapore
© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate
credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were
made. The images or other third party material in this article are included in the articles Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not included in the articles Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative
Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made
available in this article, unless otherwise stated in a credit line to the data.
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 2 of 16
Background
Seeking medical opinions from online health commu-
nities has become popular: 71% of adults aged 1829
(equivalent to 59% of all U.S. adults) reported consulting
online health websites for opinions [1]. These opinions
come from an estimated twenty to one hundred thousand
health-related websites [2], inclusive of online health com-
munities that network patients with each other to pro-
vide information and social support [3]. Platforms such
as HealthBoards1 and MedHelp2 feature users report-
ing their own health experiences, inclusive of their self-
reviewed drugs and medical treatments. Hence, they are
valuable sources for researchers [4, 5].
Although patients use these platforms to access valuable
information about drug reactions, there are challenges
to their effective, large-scale use. There is lexical varia-
tion where users describe the same side effect differently.
For example, dizziness can be expressed as giddiness or
my head is spinning, posing difficulty to most feature-
based or keyword matching approaches. Separately, there
are valid concerns regarding credibility of user-generated
contents to be harvested at large in which research has
shown to be of variable quality and should be approached
with caution [69]. One proxy indicator for information
quality is the authors trustworthiness [10]. In the con-
text of social media or online forums, user trustworthiness
is often approximated via ratings from other users, i.e.,
number of thanks or upvotes [11], or via their consistency
of reporting credible information [12, 13]. In addition to
credibility, forum members also offer expertise thanks to
their own experience  with prescriptions in particular 
and facilitate responses to drug queries [14]. For instance,
while reporting expected side effects for a specific treat-
ment, patients with long-term use of certain drugs can be
a complementary source of information:
While my experience of 10 years is with Paxil, I expect that Zoloft will be
the same. You should definitely feel better within 2 weeks. One way I found to
make it easier to sleep was to get lots of exercize [sic]. Walk or run or whatever
to burn off that anxiety.  User 3690.
The above is an answer to a thread asking for expected
side effects for depression treatment with Zoloft.
User 3690s history of active discussion on other anti-
depressants such as Lexapro and Xanax lends credibil-
ity to them being an authority on depression treatments.
We noticed that Zoloft (mentioned in the thread)
shares many common side effects with the other two
anti-depressants: changed behavior, dry mouth, and
sleepiness or unusual drowsiness. as illustrated in Table 1.
Many such examples suggest that drugs which are often
prescribed together for the same treatment, such as anti-
depressants, are likely to be discussed within a same
1https://www.healthboards.com/
2https://medhelp.org/
Table 1 Side effects of anti-depressants
Drugs Side effects
Lexapro chills, constipation, cough, decreased appetite, decreased
sexual desire, diarrhea, drymouth, joint pain, muscle
ache, tingling feeling, sleepiness or unusual
drowsiness, unusual dream, sweating, ...
Xanax abdominal or stomach pain, muscle weakness , changed
behavior, chills, cough, decreased appetite, decreased
urine, diarrhea, difficult bowel movement, cough, dry
mouth, tingling feeling, sleepiness or unusual
drowsiness, slurred speech, sweating, yellow eye,..
Zoloft changed behavior, decreased sexual desire, diarrhea,
drymouth, heartburn, sleepiness or unusual
drowsiness, sweating,..
The Drugs and Side effects columns respectively list the anti-depressants and their
side effects extracted from a drugside effect database. Side effects in common
among those listed are bold
thread and share common side effects. In addition, users
who have experienced certain drug reactions are more
outspoken and active on those discussions involving drugs
of similar side effects. These signals arise from the rich
context of online health information; hence, we expect
systems to explore beyond individual statements. Specif-
ically, they should consider the complete discussion con-
tent as well as the global experience of each involved users,
in order to discover drug side effects or extract adverse
drug reactions (ADRs).
We argue that modeling user expertise from experi-
enced side effects is more robust compared against gen-
eral user profile and engagement features [13, 14], as user
expertise provides more meaningful signals for side effect
discovery. To the best of our knowledge, there is no pre-
vious work that incorporates user expertise in side effect
discovery in discussion forums at either the thread or
post level. In this work, given online health discussions,
we propose a novel end-to-end neural architecture that
jointly models each authors credibility, their global expe-
rience and their posts textual content to discover the
side effect of unseen drugs. We optimize the model on
a self-supervised task of predicting side effect of men-
tioned drugs for complete threads, where ground truth
is accessible. Our key observation is that users can be
grouped into clusters that share the same expertise or
interest in certain drugs, possibly due to their common
treatment or medical history. We incorporate this crit-
ical observation into our user model in representing a
posts content via a cluster-sensitive attention mechanism
[15]. We also follow general definition of truth discov-
ery and let the model learn a credibility score that is
unique to every user and descriptive of their trustworthi-
ness. Our experiments include an overall ablation study
to validate the significance of each model component.
This paper extends our former work [16] by conducting
a correlation study that analyzes the representativeness of
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 3 of 16
learned credibility scores and a comparison between our
self-supervised attention-based approach and traditional
supervised sequence labeling approaches on side effect
mention extraction.
We summarize our contributions as follows:
 We propose a NEural ArchiTecture, NEAT, that
captures 1) user expertise and 2) credibility, 3) the
semantic content of individual posts and 4) the
complete discussion thread, to improve side effect
discovery from online health discussions. NEATs
main means of user credibility and experience
assessment can be easily adopted by various neural
attentional encoders [17, 18].
 We formulate a self-supervised task of side effect
prediction of mentioned drugs for the proposed
network to jointly optimize its components.
 We conduct experiments to verify the validity of our
learned credibility and the robustness of
self-supervised attention-based extraction,
comparing against traditional supervised sequence
labeling baselines.
Related work
We first review existing approaches to drug side effect
discovery from health forums and social media. Next, we
examine how these works incorporate user credibility and
expertise in their learning objective. Finally, we justify
our choice of neural architecture by discussing its mod-
eling capability of context-rich structures such as online
discussion.
Drug Side Effect Discovery. Existing methods for drug
discovery from online content extract drugs at post and
statement level. ADR mining systems typically include a
named entity recognition (NER) model and a relationship
or semantic role labeling model [19, 20]. Recent neu-
ral approaches address lexical variation in user-generated
content  the difficulty faced by traditional keyword
matching and rule-based approaches  to improve recog-
nition and labeling components [21, 22]. Distributed word
representations [23, 24] constructed from context can
capture semantics based on the hypothesis that syn-
onyms often share similar contextual words. For example,
headache and cephalea will have close representations
if they share contextual words such as head or pain.
Approaches to sub-word embedding [25, 26] model the
morphology of words by leveraging sub-word or charac-
ter information. These representations are naturally inte-
grated into neural sequential models [17, 18, 27] that are
sensitive to syntactic order. However, supervised sequence
labeling or mention extraction approaches require labo-
rious annotations at the word (token) level, and are
only capable of discovering side effects that are explic-
itly present in the text. Expert supervision or additional
semantic matching models are also required to map such
recognized text segments to standardized vocabularies or
thesaurii [28]. In contrast, our proposed self-supervised
task formulation discovers the aggregated side effects of
mentioned drugs for each community discussion by con-
sidering the whole threads content. The list of discussed
drugs are tagged by forummoderators or obtained by pat-
tern matching. This learning design not only effectively
alleviates the need for expensive, finer-grained annota-
tions but also allows for the prediction of side effects not
explicitly mentioned in the discussion.
User Credibility and Expertise Integration. Credi-
bility is of the utmost concern in large-scale knowl-
edge harvesting [8, 29, 30]. Previous work on side effect
discovery from individual statements or posts derive
information credibility by verifying a statements men-
tioned side effects against ground truth drug side effect
databases, and assess associated user credibility by mea-
suring the percentage of a users credible statements
[13, 31]. In contrast, our approach to side effect discov-
ery from discussions by jointly modeling multiple posts
and authors eschews the assessment of statement cred-
ibility and derives user credibility differently. We assign
each user a positive score that is used to weight their
post content in representing the discussions holistic con-
tent. Suchweighted summation is detailedmathematically
in Appendix 1 to conform to the general principle of
truth discovery, where sources providing credible infor-
mation should be assigned higher credibility scores, and
the information that is supported by credible sources will
be regarded as true [10]. Although our dataset does not
provide any ground truth for user trustworthiness, we fol-
lowed the previous usage of ratings or upvotes in online
forums and adopted the number of thanks received from
other forum members [11] as our proxy for user trust-
worthiness. Previous works have modeled user expertise
based on user profiles such as demographics; activity
features such as posting frequency and posting pattern
through time series and network analysis [13, 14]. As
shown in an earlier example in Section 1, modeling user
expertise from previously experienced side effects better
captures author authoritativeness for certain side effects.
It is also universally applicable to any online platform.
Modeling Online Discussion Content and Structure
As our work makes use of the rich topographical proper-
ties of online communities, we briefly review approaches
for modeling textual content and post-thread discus-
sion structure. Previous works use probabilistic graphical
models implicitly to represent textual content (especially,
topicmodeling) as bags-of-words [28, 32] or inventories of
stylistic and linguistic features [13]. Such lightweight rep-
resentation are well-suited in moderately short contexts,
i.e., sentences or posts. However, in terms of modeling
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 4 of 16
long discussions consisting of multiple posts, state-of-the-
art models for Community Question Answering (CQA)
feature hierarchical neural architectures [3335]. In term
of encoding text, sequential encoders such as Long Short-
TermMemory (LSTM) [36] or Convolutional Neural Net-
works (CNN) [18] are capable of encoding long-term
dependencies and semantic expressiveness by leverag-
ing word embeddings. In terms of encoding hierarchical
structures such as community discussions consisting of
post- and thread-level features, neural architectures allow
for straightforward and efficient integration of multiple
learning objectives. In addition, our neural architecture,
NEAT, incorporates attention mechanism that focuses on
essential phrases while encoding post content, and joint
user credibility learning while optimizing for the side
effect discovery objective.
Methods
Basic Terminology. To ensure a consistent representa-
tion, we define some terms and formalize them as follows:
 A drug d has a set of side effects,
Sd = {s1, s2, . . . , s|Sd|} A post p is a message in online forums and contains a
sequence of words. Each post p belongs to the set of
all online forum posts P and is written by a user u
and belongs to a thread t.
 A user u is a member of an online forum and
participates in a list of threads, i.e.,
Tu = {t1, t2, . . . , t|Tu|} by writing at least one post in
each thread. We use the terms user and author, as
well as user experience and user expertise
interchangeably. Each user belongs to the set of all
online forum users U and is characterized by their
credibility and expertise. Credibility wu of user u
reflects the probability of user u provide trustworthy
or helpful information, and is approximated from the
number of thanks given from other forummembers.
 A thread t (see Table 2) is an ordered collection of
postuser pairs,
Qt = {(p1,u1) , (p2,u2) , . . . ,
(
p|Qt |,u|Qt |
)}.
Each thread discusses the treatment for a particular
condition and entails a list of prescribed drugs
Dt = {d1, d2, . . . , d|Dt |}. Hence, every thread has a list
of aggregated potential side effects defined as
St = Sd1 ? Sd2 · · · ? Sd|Dt | .
Task Definition. Drug side effect discovery from discus-
sions is the task of assigning the most relevant subset of
potential side effects to threads discussing certain drugs,
from a large collection of side effects. We view the drug
side effect discovery problem as a multi-label classifica-
tion task. In our setting, an instance of itemlabel is a
tuple
(xt , y
)
where xt is the feature vector of thread t
derived from its list of postuser pairs Qt and y is the side
effect label vector i.e., y ? {0, 1}|S|, where |S| is the number
of possible side effect labels. Given training instances, we
train our classifier to predict the list of drug side effects in
unseen threads discussing unseen drugs.
Formal Hypothesis. Given a thread t with Qt , we
hypothesize that considering the credibility and experi-
ence of user u ? (p,u) ? Qt improves the quality of feature
representation in thread t, resulting in better drug side
effect discovery performance.
Self-supervised Drug Side Effect Discovery. We pro-
pose a self-supervised learning objective. Instead of
relying on the identical and independently distributed
assumption of fully supervised learning, we construct
the dataset from threads that can discuss a set of com-
mon drugs. We look up the side effects of these men-
tioned drugs via a drugside effect medical database
obtained from Mayo Clinic portal. Our self-supervised
task explores discussion-based side effect discovery which
alleviates the need for finer-grain annotation compared
against existing approach of statement-based side effect
discovery. We also propose our neural architecture,
Table 2 A sample discussion thread from an online health community
User IDs Posts Mentioned drugs Aggregated side effects
3690 While my experience of 10 years is with Paxil,
I expect that Zoloft will be the same. You
should definitely feel better within 2 weeks.
One way I found to make it easier to sleep
was to get lots of exercize. Walk or run or
whatever to burn off that anxiety.
Zoloft, Paxil changed behavior, decreased sexual desire,
diarrhea, dry mouth, heart-burn, sleepiness
or unusual drowsiness,...
26521 Ive heard of people going cold turkey and
having withdrawal at 6 months! Please, get
in contact with a doctor ASAP! common
symptoms include dizziness, electric shock-
like sensations, sweating, nausea, insomnia,
tremor, confusion, nightmares and vertigo
The User IDs and Posts columns respectively list the IDs of users involved in the discussions and their messages. The Mentioned drugs and Aggregated side effects columns
respectively list the explicitly discussed drugs and their combined side effects
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 5 of 16
NEAT, that jointly models user credibility, expertise and
text content with attention while optimizing for the self-
supervised objective. The network has three major com-
ponents: 1) user expertise representation with rich multi-
dimensional vectors; 2) cluster-sensitive attention being
capable of focusing on relevant phases for post con-
tent encoding improvement; and 3) credibility weighting
mechanism which effectively learns to assign credibility
score to each user, based on their content. We discuss its
implementation in the following sections. Figure 1 shows
the detailed network architecture of our model.
User Expertise Representation (UE). We embed each
user u ? U as a vector vu so that the vector captures user
us experience with certain side effects. As each user u par-
ticipates in the threads Tu, entailing a list of experienced
side effects, we derive user side effect experience vector
v?u ? R|S| where S is the set of all possible side effects
and v?ui = nui where user u has discussed ith side effect
in nui threads. We obtain a user drug experience matrix
M? ? R|U|×|S| where jth row of M? denotes user side
effect experience vector of jth user. To avoid learning from
sparse multi-hot encoded representations and to improve
Fig. 1 The neural architecture of our proposed NEAT. The wu and vu boxes denote Credibility Weight (CW) component and User Expertise (UE)
component. The yellow boxes and blue boxes denote Cluster Attention (CA) component and neural text encoders with attention. The highlighted
words in red denoted the text segments that are being attended by the encoder. The ×, ?, and ? symbols denote the multiplication, summation,
and sigmoid, respectively
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 6 of 16
the models scalability with the number of side effects, we
perform dimensionality reduction, specifically principal
component analysis (PCA) [37], to our experience matrix
M? obtained from training set. Figure 2 shows percentage
of variance explained versus number of included principal
components. Since our PCA plots do not show signifi-
cant improved percentage of variance explained beyond
100 components, we use g = 100 components, reduc-
ing our original M? ? R|U|×|S| to user expertise matrix
M ? R|U|×g .
User Cluster Attention (CA).We make an assumption
via observations that users in online health communities
can be effectively grouped into clusters based on their
previous side effect experience. The advantages of clus-
tering users is twofold: First, since users in the same
clusters share certain parameters, they are jointly mod-
eled and more active forum members leverage less active
ones. Second, clustering efficiently reduces the number
of parameters to learn and improves optimization and
generalization. We apply K-means  a distance-based
unsupervised clustering algorithm [38]  to binary-valued
user experience vectors v?u after normalization. By using
cosine similarity, the algorithm effectively groups users
with a high number of co-occurred side effects in the same
cluster. To determine the number of clusters c, we plot
the silhouette scores against the number of clusters and
observe the sharp drop after c = 7 (Fig. 3). The average
silhouette score is 0.57 for our choice of c = 7, indi-
cating that users are moderately matched to their own
groups and separated from other groups. The top 5 most
common side effects in each clusters are shown in Table 3.
In the larger domain of natural language processing,
attention has become an integral part for modeling text
sequences [39, 40]. By learning to focus on essential text
segments, attention allows text encoders to capture long
term semantic dependencies with regard to auxiliary con-
textual information [41, 42]. In our related task of ADR
mentions extraction, attention has been adopted recently
in neural sequence labelling models [21, 43], resulting
in promising improvement. Inspired by the concept, we
enhance text encoding with user expertise attention. Even
though the attention is adjusted to the non-extractive self-
supervised task of thread-level drug side effect discovery,
we hypothesize that our model learns to highlight the
mentioned accurate side effects, and can be used as a
self-supervised baseline for side effect extraction. Based
on the previously obtained clustering results, we assign a
learnable cluster attention vector for each user group and
incorporate their expertise into the text encoding process.
Post Content Encoding. NEAT takes the content of a
thread t as input, which is a list of postuser pairs Qt .
Post pi of pair (pi,ui) ? Qt consists of a sequence of
words xpi = {w1, . . . ,wn} with length n. We seek to rep-
resent a post pi as a vector vp that effectively captures
its semantics through an encoding function f (xpi) mod-
eled by a neural text encoding module (the blue boxes
in Fig. 1). We embed each word into a low dimensional
vector and transform the post into a sequence of word
vectors {vw1 , vw2 , . . . , vwn}. Each word vector is initialized
using pre-trained GloVe [24] embeddings, and each out-
of-vocabulary word vector is initialized randomly. We
make use of modularity  a major advantage of neural
Fig. 2 Principal component analysis on user experience vectors. The horizontal axis denotes the number of principal components chosen for PCA,
while the vertical axis denotes their percentage of variance explained. We notice that the percentage of variance explained does not increase
significantly after 100 principal components
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 7 of 16
Fig. 3 Silhouette scores for User Clustering. The horizontal axis denotes the number of clusters chosen for K-means clustering, while the vertical axis
denotes their the silhouette scores. We notice that the silhouette scores drop sharply after 7 clusters.
architectures  and design the post content encoder as
a standalone component that can be easily updated with
any state-of-the-art text encoder. In this work, we pro-
vide two neural text encoders: long-short term memory
(LSTM, see Fig. 4) [36] and convolutional neural net-
works (CNN, see Fig. 5) [18], both of which incorporates
attention mechanism.
A bi-directional LSTM encodes the word vector
sequence and outputs two sequences of hidden states: a
forward sequence, Hf = hf1,hf2, . . . ,hfn that starts from
the beginning of the text; and a backward sequence,Hb =
hb1,hb2, . . . ,hbn that starts from the end of the text. Formany
sequence encoding tasks, knowing both past (left) and
future (right) contexts has proven to be effective [44]. The
states hfi ,hbj ? Re of the forward and backward sequences
are computed as follows:
hfi = LSTM(hfi?1, vwi), hbj = LSTM(hbj+1, vwj),
where e is the number of encoder units, and hfi ,hbj are the
ith and jth hidden state vector of the forward (f ) and back-
ward (b) sequence. We derive the cluster attention vector
as vai ? Re for each user ci, from which the weights of
each hidden state hfj and hbj based on their similarity with
the attention vector are:
waj =
exp(vaihj)?n
l=1 exp(vaihl)
. (1)
The intuition behind Eq. (1), inspired by Luong et al.
[39], is that hidden states which are similar to the atten-
tion vector vai should be paid more attention to; hence
are weighted higher during document encoding. vai is
adjusted during training to capture hidden states that are
significant in forming the final post representation. waj
is then used to compute forward and backward weighted
feature vectors:
hf =
n?
j
wajh
f
j , hb =
n?
j
wajhbj . (2)
We concatenate the forward and backward vectors to
obtain a single vector, following previous bi-directional
LSTM practice [45].
Table 3 Most common experienced side effects for each user
cluster ci (i = 1 to 7)
Cluster Most common experienced side effects
c1 vision blurred, yellow skin, vision double, yellow eye,
nose stuffy
c2 headache, itch, stomach pain, weak, nausea
c3 itch, irritate, headache, pain abdominal, stomach
cramp
c4 bad taste, nausea, tiredness, irritate, mouth ulcer
c5 skin red, itch, rash skin, skin peeling, burning skin
c6 sneezing, nose runny, nose stuffy, decrease sexual
desire, pain breast
c7 nausea, stomach pain, vomit, diarrhea, pain
abdominal
The left column lists the names of 7 clusters, and the right column describes the
most common experienced side effects of users in each cluster
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 8 of 16
Fig. 4 LSTM-based encoder with cluster attention. The × and + cells denote the attention-weighted summation described in Eq. (2). The C cell
denotes the concatenation of the forward, hf , and backward, hb , hidden states
Our choice of CNN-based encoder is based on prior
work [18, 46]. A convolution block k consists of two sub-
components: a convolution layer and a cluster attention
layer. In the convolution layer, a kernel of window s
(0 < s < n) of weight W is used to generate
the hidden representation hkj for the word embeddings
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 9 of 16
Fig. 5 CNN-based Encoder with Cluster Attention. The × and + cells denote the attention-weighted summation described in Eq. 2. The C cell
denotes the concatenation of the final hidden states of K convolution blocks
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 10 of 16
{vwi?s+1 , · · · , vwi} as:
hkj = CONV (W , {vwi?s+1 , · · · , vwi}) (3)
where CONV (·) is the convolution operation described
in [18]. In the cluster attention layer, we first derive the
attention weight waj for each hidden representation hkj
similarly to the LSTM-based encoder. Attention weighted
pooling is used to obtain the convolution block output as
follows:
hk =
n?
j
wajhkj (4)
Since we use multiple convolution blocks of different
kernel sizes, the final post representation is the concate-
nation of K block outputs hk .
Thread Content Encoding with Credibility Weights
(CW). For every postuser pair (pi,ui) at thread t, we
first compute feature vector vpi for post pi. NEAT then
concatenates this postuser representation with user uis
expertise vector vui to form postuser complex vector vpui .
This postuser complex is weighted by a user credibil-
ity ewui , where wui initially set to 0 per user and updated
while training for the self-supervised side effect discovery
objective. We implement credibility learning according to
the general intuition from the truth discovery literature:
users who give quality posts, on which the model can
solely base to make correct predictions, are given a higher
credibility. We also exploit this credibility score to encode
the thread representation by placing emphasis on the con-
tent of credible users. A representation of a thread that
meets the above description is the weighted sum of each
postuser complex vector:
vt =
n?
i=1
vp?ui =
n?
i=1
ewui vpui (5)
Multi-label Prediction:NEAT feeds the thread content
representation vt through a fully connected layer whose
outputs can be computed as follows:
st = W tanh(vt) + b, (6)
where W and b are weights and biases of the layer. The
output vector st ? R|S| is finally passed through a sigmoid
activation function ?(·), and trained using cross-entropy
loss L defined as follows:
L = 1|T |
|T |?
t=1
{yt · log(? (st)) + (1 ? yt) · log(1 ? ?(st))}
+ ?1
??
u
v2u + ?2
?
i
|wui |
(7)
We adopt regularization that penalizes the training loss
with the user experience matrixs L2 norm by a fac-
tor of ?1 and the user credibility vector wus L1 norm
by a factor of ?2. The loss function is differentiable,
thus trainable with the Adam optimizer [47]. During
our gradient-based learning, user uis credibility score
wui is updated by calculating ?L?wui by back-propagation
(see Appendix 1).
Results
We conduct experiments to validate the effectiveness of
our proposed model. We design an ablation study to high-
light the effectiveness of each component of NEAT in
our self-supervised side effect prediction. In addition, we
expand our previous work [16]. More specifically,
1. We verify the representativeness of the learned
credibility scores via correlation analysis and ranking
metrics using number of thanks received by other
forum members as the trustworthiness proxy.
2. We compare the models performance in unseen drug
side effect discovery against non-neural baselines.
3. We examine the applicability of cluster attention in
side effect mention extraction from user posts both
at the macroscopic and microscopic levels.
Dataset and Experiment Settings. We conduct our
experiments on the same dataset as [13] including 15,000
users and 2.8 million posts extracted from 620,510
HealthBoards[1] threads. The ground truths for self-
supervised learning are defined as side effects of men-
tioned drugs in the discussion. As annotating such
amount of posts is expensive, drug side effects are
extracted from Mayo Clinics Drugs and Supplements
portal3 and are used as surrogates for potential drug reac-
tions. From the original dataset, we only extract threads
that are annotated with drugs and their side effects, along
with the lists of contained posts and corresponding users.
Table 4 shows some statistics of our dataset.
For CNN encoder, we adopt the work by Kim (2014) [18]
and use three kernels of sizes 3, 4, 5 with output chan-
nel size = 100. For Bi-LSTM we use a single layer with a
hidden state size = 32.
We used Natural Language Toolkit 4 for tokenization
and stop-word elimination before representation mod-
eling. We perform 10-fold cross-validation (with 8:1:1
folds for training, validation, and testing, respectively).
We perform PCA and K-means clustering on training set,
using scikit-learns built-in modules [48], 100 prin-
cipal components (g = 100). All models are trained using
PyTorch5 library. We have released our codes at 6.
Ablation Study. We include each component in
Section 1 to the architecture at a time and ver-
ify the incremental enhancement. We implement both
3https://www.mayoclinic.org/drugs-supplements
4https://www.nltk.org/index.html
5https://pytorch.org/
6https://github.com/nguyenvanhoang7398/NEAT
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 11 of 16
Table 4 Some statistics on our dataset
# Users 14,966
# Threads 78,213
Avg. words per post 67.45
Avg. posts per thread 3.97
Avg. participated threads per user 54.7
# Side effects (SE) 315
Avg. SEs per thread 74.25
# Drugs 1869
Avg. experienced side effects per user 128.12
The left column contains the statistics descriptions while the right column contains
the statistics values
CNN and LSTM-based text encoders to confirm the
consistent improvement across different neural encoders.
The ablated baselines and the full models are as follows:
 Vanilla: We implement a neural text encoder
baseline without any proposed component.
 Weighted Post Encoder (WPE): We construct
thread representation by summing each of its
postuser complex vector weighted by user
credibility.
 Weighted Post Encoder with User Expertise
(WPEU): We concatenate user expertise and post
vector to create postuser complex vector.
 NEAT: We incorporate all three components  UE,
CW and CA  as described.
Table 5 shows the precision, recall (sensitivity), and F1
(the harmonic mean of precision and sensitivity) obtained
by our method and the four baselines. We also report
the performance of baselines implementing UE and CA
individually in Table 11.
User Credibility Analysis. We discuss how descrip-
tive the credible users assigned by the model are to our
Table 5 Performance of CNN-based models and LSTM-based
models in Ablation Study
Systems Components Evaluation Metrics
CW UE CA Pre. Rec. F1
LSTM-Vanilla 0.6173 0.407 0.4335
LSTM-WPE  0.6376 0.4344 0.4503
LSTM-WPEU   0.6064 0.5001 0.4896
LSTM-NEAT    0.6197 0.5134 0.5064
CNN-Vanilla 0.7214 0.5503 0.5637
CNN-WPE  0.7423 0.5799 0.5804
CNN-WPEU   0.6923 0.6350 0.5910
CNN-NEAT    0.7066 0.6431 0.6139
In the Components column, CW, UE, CA denote Credibility Weights, User Expertise
and Cluster Attention module components, respectively. In the Evaluation Metrics
column, Pre., Rec. and F1 denote Precision, Recall, and F1 score
common notion of trustworthy users in online communi-
ties. We employ the number of thanks received by other
community members as the proxy for a users credibility,
at both global, forum-wise scope and local, thread-wise
scopes. Specifically, at forum-wise scope, we measure
Spearmans rank correlation coefficient to examine how
our output user scores approximate the ordering of user
trustworthiness. Thread-wise, we examine how accurate
our output user scores are in ranking trustworthy respon-
dents within a single discussion by measuring both Spear-
mans coefficient and nDCG@2 with regard to the order-
ing provided by our credibility proxy. We find the forum-
wise rankingmetrics meaningful as answer ranking, based
on user credibility in our case, is a well-formulated task in
CQA [4951]. The measurement results are presented in
Table 6.
Drug Side Effect Discovery. We test the performance
of NEAT in the task of Drug Side Effect Discovery. Specif-
ically, the model has to predict the side effects of one of
five unseen drugs based on their discussions as a whole.
Such task is necessary to verify that our self-supervised
objective of predicting for side effects of discussed drugs
generalizes well to drugs that have not been discussed in
training data. We highlight the performance of an end-
to-end neural architecture against Random Forest (RF)
 a competitive, non-neural baseline trained on bag-of-
word text representations of thread content. Additionally,
we examine a baseline, uNEAT, where the user identities
are randomized in order to verify that NEAT effectively
considers both user credibility and expertise. The results
of drug side effect discovery on Ibuprofen, Levothyrox-
ine, Metoformin (Table 7), and Omeprazole, Alprazolam
(Table 8) are reported.
Side Effect Extraction with Cluster Attention. As dis-
cussed in Section 1, our employed attention mechanism
not only offers a better textual encoding capacity but
also locates the informative segments of user-generated
content. This concept is well-aligned with ADR men-
tion extraction, which is mainly modeled as a sequence
labeling problem. We conduct experiments to examine
Table 6 Analysis of NEATs Credibility versus baselines in
approximating credibility proxy
Methods Thread
nDCG@2
Thread
Spearman
Forum
Spearman
Random 0.7968 -0.0271 0.0
Post frequency 0.8812 0.4223 0.1924
Question frequency 0.8341 0.1773 0.0279
NEATs Credibility 0.8856 0.4403 0.3055
The Thread nDCG@2, Thread Spearman, and Forum Spearman columns
respectively denote the values of Normalized Discounted Cumulative Gain at 2 at
thread level, Spearmans rank correlation coefficient at thread level and forum level
of each method when using rankings by number of thanks as ground truths
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 12 of 16
Table 7 Performance of NEAT versus baselines in Side Effect Discovery of Ibuprofen, Levothyroxine, and Metoformin
Methods Ibuprofen Levothyroxine Metoformin
Pre. Rec. F1 Pre. Rec. F1 Pre. Rec. F1
RF 0.583 0.414 0.474 0.319 0.401 0.347 0.48 0.647 0.491
uNEAT 0.859 0.371 0.487 0.505 0.349 0.404 0.798 0.361 0.497
NEAT 0.845 0.427 0.536 0.549 0.385 0.443 0.814 0.365 0.504
In the Methods column, RF denotes Random Forest baseline from Bag-of-word, and uNEAT denotes User permutation baseline from NEAT. Pre., Rec. and F1 denote Precision,
Recall, and F1 score, respectively.
the effectiveness of CNN-NEATs Attention in locating
the text segments containing the correct side effects.
We benchmark our results against a lexicon-based tag-
ging using UMLS thesaurus for medical terms [52] and
a state-of-the-art neural side effect extractor [21] which
was supervisedly trained to identify side effect mentions
in social media contents. In this task, correctly predict-
ing the positive side effects is of the utmost importance,
hence, we benchmark the text segments extracted from
CNN-NEATs Attention against two mentioned baselines
on precisionmetric. The experiment results on Ibuprofen,
Levothyroxine,Metoformin, Omeprazole and Alprazolam
are reported in Table 9.
Discussion
Ablation Study. Firstly, all of the three models that apply
credibility weighting (CW)  WPE, WPEU, and NEAT 
outperform both LSTM-Vanilla and CNN-Vanilla base-
lines. Specifically, in LSTM-Vanilla, solely weighting each
post by its author credibility improves the performance of
the naive post encoder by 2.03%, 2.74% and 1.68% on pre-
cision, recall, and F1, respectively. We observe a similar
margin of improvement from CNN-Vanilla. These results
demonstrate the effectiveness of accounting for author
credibility when encoding thread content, improving side
effect prediction.
Improvements by incorporating user experience (UE)
are also testified in both neural encoders. In LSTM-
based models, adding UE (LSTM-WPEU vs. LSTM-
WPE) improves recall by 6.57% and 3.93% in F1. Again,
the CNN-based counterpart, CNN-WPEU, shows simi-
lar performance trends. On a macro scale, these statistics
indicate that our model successfully learns to include
Table 8 Performance of NEAT versus baselines in Side Effect
Discovery of Omeprazole and Alprazolam
Methods Omeprazole Alprazolam
Pre. Rec. F1 Pre. Rec. F1
RF 0.229 0.458 0.271 0.639 0.432 0.511
uNEAT 0.534 0.393 0.394 0.981 0.551 0.663
NEAT 0.522 0.421 0.41 0.977 0.596 0.704
In the Methods column, RF denotes Random Forest baseline from Bag-of-word, and
uNEAT denotes User permutation baseline from NEAT. Pre., Rec. and F1 denote
Precision, Recall, and F1 score, respectively
more side effects in its prediction, where many are
relevant to the ground truth. This is consistent with our
hypothesis that considering author experience of each
post is effective in predicting out-of-context side effects.
Applying cluster-sensitive attention (CA) in combining
both the LSTMs and CNNs hidden states also improves
the performance. In LSTM-based systems, we observe
that adding CA (LSTM-NEAT vs. LSTM-WPEU) uni-
formly improves all retrieval metrics; our CNN-based
counterpart, CNN-NEAT, also demonstrates similar per-
formance improvements. Although CNN-NEAT and its
ablated baselines obtain higher performance than the
LSTM counterparts, when measuring relative improve-
ment, the gains are comparable. This confirms the con-
sistent improvement of our proposed components across
different neural encoders. According to the macroscopic
analysis of results in Table 5, we generally conclude that
all of the three components in our proposed architec-
ture  namely, CW, UE, and CA  yield a positive impact
on the overall model performance. We observe consis-
tent improvements in F1 after adding each component,
and this lends support our stated hypotheses. The signif-
icance of these findings were verified by one-tail t-test of
p < 0.05.
User Credibility Analysis. Results at both the thread
and forum level show that user scores assigned by NEAT
reasonably approximate our credibility proxy, i.e. the
number of thanks given by other forum users. Regarding
ranking users by their helpfulness within a thread, NEATs
credibility improves heuristics such as post or question
frequency marginally by 0.0044 in term of nDCG@2 and
moderately by 0.0180 in term of Spearmans coefficient.
Regarding ranking users by their helpfulness in the whole
forum, we report a more significant improvement of
0.1131 in term of Spearmans coefficient from the clos-
est performing baseline of post frequency. These results
verify the representativeness of the credibility scores
obtained in the self-supervised manner by our system.
Drug Side Effect Discovery. Both neural meth-
ods, namely uNEAT and NEAT, outperform non-neural
approach based on bag-of-word vectors on all metrics and
across all drugs. Specifically, NEAT improves RF by 9.94%
in F1 on average across five different drugs. This confirms
themodeling capability of our neural network and justifies
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 13 of 16
Table 9 Performance of CNN-NEATs Attention versus baselines in Side Effect Extraction in term of Precision
Methods Ibuprofen Levothyroxine Metoformin Omeprazole Alprazolam
UMLS Tagging 0.6801 0.6145 0.8378 0.5218 0.614
Neural Extractor [21] 0.6741 0.6259 0.8092 0.4665 0.6161
CNN-NEATs Attention 0.7073 0.7119 0.8557 0.504 0.688
The Methods column includes the two baselines, UMLS Tagging and Neural Extractor, and the extracted attention of our proposed NEAT  CNN-NEATs Attention. We
present the five evaluated drugs, Ibuprofen, Levothyroxine, Metoformin, Omeprazole, Alprazolam and the Precision of extracting their side effects for all three methods.
its application to our task. We also observe a decrease in
performance while optimizing NEAT being user-unaware.
This shortcoming is prevalent in all side effect discov-
ery settings, ranging from a 0.7% F1 score decrease for
Metformin to 4.9% decrease for Ibuprofen. Overall, being
aware of user experience and expertise improves drug side
effect discovery by 3.04% in F1 on average across five dif-
ferent drugs. This gives substantial indicative evidence
congruent with our hypothesis that considering the cred-
ibility and experience of users improves drug side effect
discovery performance in online health communities.
Side Effect Extraction with Cluster Attention. We
notice improvement of CNN-NEATs Attention from
both baselines across most drugs with the exception of
Omeprazole. At a macro level, positive precision scores
confirm CNN-NEATs emphasis on critical text segments,
i.e. those containing correct drug side effects. The sig-
nificant improvement in most cases also suggests the
selectiveness of CNN-NEATs Attention. Unlike the two
proposed baselines which extract any side effect men-
tions, CNN-NEATs Attention selectively emphasizes cor-
rect ADRs. We also examine this hypothesis at the micro
level in Table 10. UMLS tagging identifies any phrase
having a medical nuance, i.e. pain and back, and poten-
tially forms side effects that are not actually mentioned,
i.e. back pain, whereas both neural methods, Neural
Extractor and NEAT, that model textual semantics are
able to dismiss such trivial mentions.Discomfort, although
correct, is questionably an intentionally reported side
effects and is also dismissed by both Neural Extractor
and CNN-NEAT. Post-processing rules can arguably alle-
viate the shortcomings of UMLSs matching strategy due
to missing context awareness. However, such rules are not
efficient to engineer. On the other hand, unsupervised
context encoding for keyword-based extraction is chal-
lenging, and the task is left open for future works. We
attempt to explain CNN-NEATs decision to dismiss rest-
lessness based on its contextual awareness. The attention
was derived from each clusters experienced side effects,
in which there is the weak side effects being semanti-
cally contradicting to restlessness. Although having not
fully covered all side effects, all mentions extracted by
CNN-NEAT are accurate, giving it the highest precision
amongst the three considered models. Specifically, CNN-
NEATs Attention improves over neural ADR extraction
model [21] by 5.502% and UMLS tagging by 3.974%
on average in terms of precision across five different
drugs. Despite being derived from a self-supervised objec-
tive, CNN-NEATs Attention offers helpful indications for
attention-based models and a strong baseline for ADR
extraction.
Limitations. We call attention to limitations from our
design choice of defining a users credibility by Eq. (5),
as well as our choice of credibility proxy as defined by
the number of thanks. A users credibility can be dam-
aged if their posts do not directly help with predicting
the correct side effects. This assumption is question-
able when users are asking for some information instead
of giving answers without any intent to give mislead-
ing information. In contrast, we also observe the cases
where users receive thanks for giving helpful information
such as suggesting nutritious diet or healthy lifestyle with-
out mentioning any relevant side effects. We recognize
the limitation of our model where users without mali-
cious intent are possibly assigned a low credibility score.
This case of failure can explain why some users are
assigned low to moderate credibility despite their high
number of thanks. However, our definition makes sure
Table 10 A test example highlighting the extracted side effects obtained by CNN-NEATs Attention versus baselines
User IDs Clusters side effects Post content
8420 stomach pain, headache, itch, weak, nausea I wont´ commit suicide but the discomfort < U > is enough
to make me want to die right now [...] I feel like I have
sever Akathisia (inner restlessness<U,X> that makes you feel
like your body is electrified [...] I also have nausea<U,X,N>,
but I can eat a little, sweating<U,X,N>/cold, and extreme
fatigue<U,X>, although I already have chronic fatigue [...] I also
get anxious<U,X,N> if I take more oxycodone for breakout
pain< U > and then go right back< U > down
In the Post content column, the correct and incorrect side effects are highlighted in blue and red, respectively. The extracted side effects of UMLS Tagging, Neural Extractor and
CNN-NEATs Attention are followed by < U >, < X >, and < N >, respectively. The Clusters side effects column shows the list of common side effects in user 8420s cluster
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 14 of 16
that the credibility learning mechanism does not express
the opposite adverse behavior of assigning high credibility
to untrustworthy users.
Annotating a posts side effects solely from looking up
the mentioned drugs in theMayo Clinic database presents
another limitation. To generalize the usability and ensure
the effectiveness of the learning framework to a broader
community such as medical informatics, we suggest to
have these annotations cross-checked with healthcare
professionals or pharmacovigilance experts, in order to
ensure the correlation between Mayo Clinics annotations
and the posts actually described side effects.
Overall, our analysis suggests that user credibility
scores, although learned in a self-supervised manner, can
capture the expected notion of credibility and are descrip-
tive of trustworthiness. Every component of our archi-
tecture is also shown to be vital in achieving the highest
performance.
Conclusion
We have addressed the importance of user experience and
credibility in modeling thread contents of online commu-
nities, specifically through the task of drug side effect dis-
covery. Our proposed neural architecture, NEAT, suggests
a subset of side effects relevant to the mentioned treat-
ment in the given discussion, while taking into account
the each post content and its author side effect experience
via attention mechanism to represent forum discussions
better. Mainstream models for drug discovery in online
communities have not captured thread content and user
experience holistically inanend-to-end optimizable system.
We modeled users expertise by examining their expe-
rience with different side effects, and then grouped the
users with similar experience into clusters that share a
common attention representation. We also proposed an
self-supervised method which assigns credibility scores
to users based on the correctness of their contents and
overall improves thread representations. Correlation anal-
ysis testifies the representativeness of learned credibility
scores of trustworthiness approximated by number of
thanks received by other online community members.
In addition, our integrated attention mechanism not only
enhances textual encoding but also highlights essential
text segments and benefits ADR extraction approaches.
We believe that our model is applicable to other
domains. We plan to generalize its application to main-
stream CQA or expertise-based thread recommendation
for health forum members.
Appendix
A User credibility weighting and the general principle of
truth discovery
In order to demonstrate the correlation between learned
user credibility scores and the general notion of
trustworthiness, we derive how user credibility scores are
updated after each turn of back-propagation via stochas-
tic gradient descent. The overall loss function in Eq. (7)
can be rewritten as logistic loss without regularization on
a single training example and a single label s as follows:
L = log(1 + exp(?ys(ws tanh(vt) + bs))), (8)
where ys is the binary truth for label s, bs is a classification
bias, and ws ? Rg×1 is a row of W in Eq. (6). ws is the
classification weight vector of a single label s.
In back-propagation, we update the score wui of user ui
based on the gradient calculated by taking the derivative
of the loss L with regard to wui :
?L
?wui
= (1 ? tanh(vt)
2)ysws vpuiewui
1 + exp(ys(ws tanh(vt) + bs))
= ?wui L. (9)
The user score wui is updated as follows:
wt+1ui = wtui ? ??wtui L, (10)
where ? is the learning rate.
When the prediction is correct, ys and (ws tanh(vt)+bs)
share the same sign and ys(ws tanh(vt) + bs)) is highly
positive, making the denominator highly positive and the
overall gradient small. The user score wui is minimally
updated.
When the prediction is incorrect, ys and (ws tanh(vt) +
bs) have different signs and the denominator approaches
1. In the nominator, ws vpui is the prediction if we solely
consider the post vector vpui of user ui.
 If this prediction is correct, which fits our definition
of credible user, ysws vpui is positive, making the
overall gradient positive. Then, the user score wui is
updated in the positive direction and the credibility
score ewui used to weight user uis content increases.
 On the other hand, when the prediction from solely
considering the post vector vpui of user ui is incorrect,
indicating a not credible user, ysws vpui is negative,
and the overall gradient is negative. wui is updated in
the negative direction and the credibility score ewui
used to weight user uis content decreases.
 The magnitude of the gradient is proportional to ewui .
This indicates that users who are currently learned as
credible are most affected by back-propagation when
the models prediction is incorrect.
Algorithm performance for individual integration of UE
and CA.
Table 11 reports the performance of baselines imple-
mented UE, and CA individually.
Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 15 of 16
Table 11 Performance for individual integration of UE and CA in
Ablation Study
Systems Components Evaluation Metrics
CW UE CA Pre. Rec. F1
LSTM-UE  0.6513 0.4204 0.4531
LSTM-CA  0.6416 0.4293 0.4611
CNN-UE  0.6738 0.6185 0.5743
CNN-CA  0.7441 0.5616 0.5883
In the Components column, CW, UE, CA denote Credibility Weights, User Expertise
and Cluster Attention module components, respectively. In the Evaluation Metrics
column, Pre., Rec. and F1 denote Precision, Recall, and F1 score
Abbreviations
Avg: Average; UE: User Expertise Representation; CA: Cluster-sensitive
Attention; CW: Thread Content Encoding with Credibility Weights; CNN:
Convolutional Neural Network; LSTM: Long-short Term Memory; F1 score: The
harmonic average of the precision and recall; PCA: Principal Component
Analysis; #: Number of; NEAT: The neural architecture proposed in this work;
uNEAT: The ablated version of NEAT where the user identities are randomized;
ADR: Adverse Drug Reaction; RF: Random Forest; nDCG@2: normalized
Discounted Cumulative Gain at 2.
Acknowledgements
None declared.
Authors contributions
All authors conceived of and planned the reported work. KS proposed the
problem statement and suggested literature for review. VHN mainly designed
the model architecture with the help from MYK and KH. VHN implemented
the experiments, and interpreted the results. VHN, KS, MYK took the lead in
writing the manuscript with support from KH. All authors discussed the results
and commented on the manuscript.
Funding
This research is supported by the National Research Foundation, Singapore
under its International Research Centres in Singapore Funding Initiative. Any
opinions, findings and conclusions or recommendations expressed in this
material are those of the author(s) and do not reflect the views of National
Research Foundation, Singapore.
Availability of data andmaterials
The dataset analysed during the current study was first published in [13] and is
publicly available at https://www.mpi-inf.mpg.de/departments/databases-
and-information-systems/research/impact/peopleondrugs/
The source codes of the implementation of this study is publicly available at
https://github.com/nguyenvanhoang7398/NEAT
The source codes used to generate the results of Side Effect Extraction in
Section 1 is available at https://github.com/nguyenvanhoang7398/NEAT/
blob/master/adr_extraction.py.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
All authors declare that they have no competing interests.
Received: 2 May 2019 Accepted: 7 June 2020
RESEARCH Open Access
De-identifying free text of Japanese
electronic health records
Kohei Kajiyama1, Hiromasa Horiguchi2, Takashi Okumura3, Mizuki Morita4 and Yoshinobu Kano1*
Abstract
Background: Recently, more electronic data sources are becoming available in the healthcare domain. Electronic
health records (EHRs), with their vast amounts of potentially available data, can greatly improve healthcare.
Although EHR de-identification is necessary to protect personal information, automatic de-identification of Japanese
language EHRs has not been studied sufficiently. This study was conducted to raise de-identification performance
for Japanese EHRs through classic machine learning, deep learning, and rule-based methods, depending on the
dataset.
Results: Using three datasets, we implemented de-identification systems for Japanese EHRs and compared the de-
identification performances found for rule-based, Conditional Random Fields (CRF), and Long-Short Term Memory
(LSTM)-based methods. Gold standard tags for de-identification are annotated manually for age, hospital, person, sex,
and time. We used different combinations of our datasets to train and evaluate our three methods. Our best F1-
scores were 84.23, 68.19, and 81.67 points, respectively, for evaluations of the MedNLP dataset, a dummy EHR
dataset that was virtually written by a medical doctor, and a Pathology Report dataset. Our LSTM-based method
was the best performing, except for the MedNLP dataset. The rule-based method was best for the MedNLP dataset.
The LSTM-based method achieved a good score of 83.07 points for this MedNLP dataset, which differs by 1.16
points from the best score obtained using the rule-based method. Results suggest that LSTM adapted well to
different characteristics of our datasets. Our LSTM-based method performed better than our CRF-based method,
yielding a 7.41 point F1-score, when applied to our Pathology Report dataset. This report is the first of study
applying this LSTM-based method to any de-identification task of a Japanese EHR.
Conclusions: Our LSTM-based machine learning method was able to extract named entities to be de-identified
with better performance, in general, than that of our rule-based methods. However, machine learning methods are
inadequate for processing expressions with low occurrence. Our future work will specifically examine the
combination of LSTM and rule-based methods to achieve better performance.
Our currently achieved level of performance is sufficiently higher than that of publicly available Japanese de-
identification tools. Therefore, our system will be applied to actual de-identification tasks in hospitals.
Keywords: De-identification, Electronic health records, Japanese language
Background
Recently, more electronic data sources are becoming
available in the healthcare domain. Utilization of
electronic health records (EHRs), with their vast
amounts of potentially useful data, is an important task
in the healthcare domain. New legislation in Japan has
addressed the treatment of medical data. The Act on
the Protection of Personal Information [1] was revised
in 2017 to stipulate that developers de-identify special
care-required personal information. This legislation
© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if
changes were made. The images or other third party material in this article are included in the article's Creative Commons
licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons
licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain
permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the
data made available in this article, unless otherwise stated in a credit line to the data.
* Correspondence: kano@inf.shizuoka.ac.jp
1Faculty of Informatics, Shizuoka University, Johoku 3-5-1, Naka-ku,
Hamamatsu, Shizuoka 432-8011, Japan
Full list of author information is available at the end of the article
Kajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 
https://doi.org/10.1186/s13326-020-00227-9
further restricts the use of personal identification codes
including individual numbers (e.g. health insurance card
numbers, drivers license card numbers, and governmen-
tal personnel numbers), biometric information (e.g. fin-
gerprints, DNA, voice, and appearances), and
information related to disability. This legislation can be
compared with the Health Insurance Portability and
Accountability Act (HIPAA) [2] of the United States, in
that the Japanese Act in 2017 includes additional codes,
with abstract specifications such as you should strive
not to discriminate or impose improper burdens, and
with exclusion of birth dates and criminal histories, as
stipulated by HIPAA. Another related act of Japanese le-
gislation, the Act on Anonymously Processed Medical
Information to Contribute to Medical Research and De-
velopment [3] was established in 2018. This legislation
allows specific third-party institutes to handle EHRs,
thereby promoting wider utilization of medical data.
De-identification of structured data in EHRs is easier
than that of unstructured data because it is straightfor-
ward to apply de-identification methods to structured
data such as numerical tables. Although de-identification
of unstructured data in EHRs is necessary, it is virtually
impossible to de-identify the huge number of documents
manually.
Several earlier works have examined EHR de-
identification. The Informatics for Integrating Biology &
the Bedside (i2b2) task [4] in 2006 was intended for
automatic de-identification of clinical records to satisfy
HIPAA requirements [2]. An earlier study prepared 889
EHRs, comprising 669 EHRs for training and 220 EHRs
for testing. Their annotations included 929 patient tags,
3751 doctor tags, 263 location tags, 2400 hospital tags,
7098 date tags, 4809 id tags, 232 phone_number tags,
and 16 age tags. The best performing method of i2b2 in-
corporated diverse features such as a lexicon, part-of-
speech identification, word frequencies, and dictionaries
for learning using an ID3 tree learning algorithm.
Grouin and Zweigenbaum [5] prepared 312 cardiovas-
cular EHRs in French, with 3142 tags annotated by two
annotators (kappa = 0.87). Their tags include 238 date
tags, 205 last_name tags, 109 first_name tags, 43 hospital
tags, 22 town tags, 8 zip_code tags, 8 address tags, 8
phone tags, 8 med_device tags, 3 serial_number tags. Of
the person tags, 75% were replaced with other French
person names. The other 25% were replaced with inter-
national names. They also collected 10 photopathology
documents, for which a single annotator assigned 29
date tags, 68 last_name tags, 53 first_name tags, 17 hos-
pital tags, 17 town tags, 13 zip_code tags, 14 address
tags, 1 phone tag, 1 med_device tag, and 7 serial_number
tags. They performed de-identification experiments
using 250 documents as their training data and 62 docu-
ments as their test data for the cardiology corpus. They
obtained better F1-scores (exact match, 0.883; overlap
match, 0.887) using conditional random fields (CRF)
than they obtained using their rule-based method (exact
match, 0.843; overlap match, 0.847). However, their
rule-based method was better for the photopathology
corpus (exact match, 0.681; overlap match, 0.693) than
their CRF-based method (exact match, 0.638; overlap
match, 0.638) because the data were fewer than those of
the cardiology corpus.
Grouin and Névéol [6] discussed annotation guidelines
for French clinical records. After collecting 170,000 doc-
uments of 1000 patient records from five hospitals, they
first prepared a rule-based system and their CRF-based
system from their earlier study [5], which we described
earlier. Their rule-based system relies on 80 patterns
specifically designed to process the training corpus, and
lists which they gathered from existing resources from
the internet. They randomly selected 100 documents
(Set 1) from their dataset and applied both systems. For
each document, they randomly showed one output of
the two systems to the annotators for revision. They ap-
plied their rule-based system to another set of 100 docu-
ments (Set 2), which were further reviewed and revised
by a human annotator. They re-trained their CRF-based
system using the revised Set 2 annotations, which is fur-
ther applied to the other set of 100 documents (Set 3).
Annotators reviewed these annotations in subsets for
different agreement analyses. The study also compared
human revision times among different annotation sets,
which was a main objective of their study. They anno-
tated 99 address tags, 101 zip_code tags, 462 date tags,
47 e-mail tags, 224 hospital tags, 59 identifier tags, 871
last_name tags, 750 first_name tags, 383 telephone tags,
218 city tags, in Set 1. They reported their rule-based
method as better (0.813) in terms of the F1-score than
their CRF-based method (0.519) when evaluated with 50
documents in Set 1. When trained with Set 2, the corpus
of the same domain, their CRF-based system performed
better, yielding 0.953 for Set 3 and 0.888 for Set 1 in
their F1-scores.
From the Stockholm EPR [7], a Swedish database of
more than one million patient records from two thou-
sand clinics, Dalianis and Velupillai [8] extracted 100 pa-
tient records to create gold standard for automatic de-
identifications based on HIPAA. They annotated 4423
tags, including 56 age tags, 710 date_part tags, 500 full_
date tags, 923 last_name tags, 1021 health_care_unit
tags, 148 location tags, and 136 phone_number tags.
They pointed out that Swedish morphology is more
complex than that of English. It includes more inflec-
tions, making the de-identification task in Swedish more
difficult.
Jian et al. [9] compiled a dataset of 3000 documents in
Chinese. It comprises 1500 hospitalization records, 1000
Kajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 2 of 12
summaries, 250 consulting records, and 250 death re-
cords. They extracted 300 documents from this dataset
randomly, discussed a mode of de-identification with
lower annotation cost. They annotated their tags to
these 300 documents (kappa = 0.76 between two annota-
tors for their 100 document subset). Then they applied
their pattern-matching module to these 300 documents,
yielding a dense set of 201 sentences that include PHI
(Protected Health Information). These 201 sentences in-
cluded 141 name tags, 51 address tags, and 22 hospital
tags.
Du et al. [10] conducted de-identification experiments
using 14,719 discharge summaries in Chinese: two stu-
dents annotated 25,403 tags. This dataset includes 6403
institution tags, 11,301 date tags, 33 age tags, 2078 pa-
tient_name tags, 3912 doctor_name tags, 326 province
tags, 310 city tags, 774 country tags, 917 street tags, 277
admission_num tags, 21 pathological_num tags, 23 x-
ray_num tags, 263 phone tags, 420 doctor_num tags, and
13 ultrasonic_num tags (inter-annotator agreement was
96%, kappa = 0.826). Their experiments demonstrated
that their method of combining rules and CRF per-
formed best, yielding a 98.78 F1-score. The Chinese lan-
guage shares some issues with the Japanese language:
they both require tokenization because no spaces exist
between words. This issue makes de-identification tasks
more difficult than they are in other languages.
The reports described above present a range of differ-
ent evaluation scores. However they adopted different
annotation criteria, which make direct comparison diffi-
cult. For instance, Grouin and Névéol used more de-
tailed annotations than those used by Jian et al., as
follows. Jian et al. introduced Doctor and Patient tags,
but evaluated both simply as Name. Grouin and Névéol
introduced ZipCode, Identifier, Telephone, and City tags,
none of which is annotated in the work of Jian et al.
Additionally, they assigned Last Name and First Name
tags, where performance of First Name was better than
Last Name by around 10 points. However, both are
worse than the results reported by Jian et al., probably
because Jian et al. applied their pattern-matching algo-
rithm to filter their training data. Regarding Address
tags, Jian et al. obtained a 94.2 point F-score, whereas
the Grouin and Névéol CRF method obtained scores of
fewer than 10 points. As Grouin and Névéol suggested,
eliminating City tags in street names can greatly improve
their results: their rule-based method yielded an 86 point
F-score.
Unfortunately, automatic de-identification of EHRs
has not been studied sufficiently for Japanese language.
De-identification shared tasks for Japanese EHRs were
held as tasks in MedNLP-1 [11]. Then named entity ex-
traction was attempted in MedNLP-2 [12] tasks using
datasets similar to MedNLP-1. We designate MedNLP-1
simply as MedNLP hereinafter because we specifically
examine de-identification tasks but not other tasks held
in the MedNLP shared task series.
Regarding machine learning methods, Support Vector
Machine (SVM) [13] and CRF [14] were used often in
earlier Named Entity Recognition (NER) tasks in
addition to rule-based methods. Recent deep learning
methods include Long-Short Term Memory (LSTM)
[15] with character-embedding and word-embedding
[16], which performed best for the CoNLL 2002 [17]
(Spanish and Dutch) and CoNLL 2003 [18] (English and
German) NER shared task data: these tasks require de-
tection of personal, location, organization, and
other tag types. Another LSTM model, which is similar
to earlier work [16], was also applied to a task of NER
from Japanese newspapers [19]. Although deep neural
network models have been showing better results re-
cently, rule-based methods are still often better than ma-
chine learning methods, especially when insufficient
annotated data are available.
To evaluate the effectiveness of such different methods
for the Japanese language, we implemented two EHR de-
identification systems for the Japanese language in our
earlier work [20]. We used the MedNLP shared task
dataset and our own dummy EHR dataset, which was
written as a virtual database by medical professionals
who hold medical doctor certification. Based on this
earlier work, we added a new dataset of pathology re-
ports to this study, for which we annotated the following
tags. De-identification tags of age, hospital, sex, time,
and person are annotated manually in all these datasets,
following the annotation standard of the MedNLP
shared task to facilitate comparison with earlier studies.
We assume these annotations as our gold standard for
our de-identification task. To these three datasets, we
applied a rule-based method, a CRF-based method, and
an LSTM-based method. Additionally, we have anno-
tated our own tags to these three datasets by three anno-
tators to calculate inter-annotator agreement. We have
observed the coherency of the original annotations of
the datasets. Overall, this study differs from our earlier
work [20] in that we added a new pathology dataset and
its annotations, trained and evaluated our machine
learning models using the new dataset, and evaluated
the results using newly created annotations by three an-
notators to observe characteristics of the original and
our own annotations.
Datasets
Our datasets were derived from three sources: MedNLP,
dummy EHRs, and pathology reports. Irrespective of the
dataset source, de-identification tags of five types are an-
notated manually: age (numerical expressions of sub-
jects ages including its numerical classifiers), hospital
Kajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 3 of 12
(hospital names), sex (male or female), time (subject re-
lated time expressions with its numerical classifiers), and
person (person names). Characteristics of these datasets
are presented in Table 1. It is noteworthy that texts of
the MedNLP and dummy EHRs are not actual texts, but
they were written by medical professionals, each of
whom holds medical doctor certification. However, char-
acteristics of the descriptions differ between these two
sources, probably because of differences of the writers.
The number of annotators is not described for the
MedNLP dataset, but a single annotator created the an-
notations of the dummy EHR dataset and the Pathology
Report dataset, individually.
MedNLP shared task dataset
We used the MedNLP de-identification task dataset for
comparison with earlier studies that have used the same
dataset. This dataset includes the dummy EHRs
(discharge summaries) of 50 patients. Although the
training dataset and test dataset were provided from the
shared task organizers, the test dataset of the formal run
is not publicly available now. It is not possible to
compare results directly with earlier works in the
MedNLP shared task formal run (Tables 2 and 3 show
the formal run results). However, both training and test
datasets were originally parts of a single dataset. There-
fore, we can discuss their characteristics in comparison
with those found in earlier works conducted using the
training dataset only. We calculated inter-annotator
agreement by three annotators for the training dataset.
The average F1-score of three pairs among these three
annotators was 86.1, in 500 sentences of this dataset.
Dummy EHRs
Another source is our original dummy EHRs. We
built our own dummy EHRs of 32 patients, assuming
that the patients were hospitalized. Documents of our
dummy EHRs were written by medical professionals
(doctors). We added manual annotations for de-
identification following the guidelines of the MedNLP
shared task. These annotations were originally
assigned by a single annotator. Additionally, we added
Table 1 Dataset characteristics
Dataset name MedNLP Dummy-EHRs Pathology Reports
# of documents 50 reports 32 pairs of records and summaries 1000 reports
# of sentences 2244 8183 3012
# of tokens 42,621 154,132 194,449
# of all tags 490 3017 295
# of age tags 56 39 0
# of hospital tags 75 170 31
# of person tags 0 135 224
# of sex tags 4 16 0
# of time tags 355 2657 40
Example in
original Japanese
text
????????<a > 64
?</a >? < x >??</
x >?
???????????<a > 86?</a > <x >?
?</x >????
<<???? <h >?????????
?</h >? < p >???</p>
Example
translated into
English
A < a > 64-year-old</a > <x >
man</x > works in a factory
An <a > 86-year-old</a > <x > woman</x >
bedridden in a nursing home. Total assistance
required
<<Ex-hospital sample < h > Shizudai
Dermatology Clinic</h > , < p > Satoshi
Kuwata</p>
Table 2 Overall results
P R F A
C3 89.59 91.67 90.62 99.58
B3 91.67 86.57 89.05 99.54
B1 90.05 87.96 88.99 99.49
B2 90.82 87.04 88.89 99.52
C1 92.42 84.72 88.41 99.49
A1 91.50 84.72 87.98 99.47
C2 91.50 84.72 87.98 99.46
A2 90.15 84.72 87.35 99.41
D1 86.10 74.54 79.90 99.36
G1 82.09 76.39 79.14 99.38
D3 85.87 73.15 79.00 99.35
D2 80.81 74.07 77.29 99.24
H2 76.17 75.46 75.81 99.28
H1 75.81 75.46 75.64 99.27
H3 74.88 74.54 74.71 99.26
P, R and F were calculated at the phrase level: P, precision; R, recall; F, F1-measure;
and A, accuracy. A was calculated in the word level (the agreement ratio of B-*, I-*
and O).
The first column stands for participants team names, where the first letter stands
for a team ID and the second numerical value stands for a submission run ID
Kajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 4 of 12
new annotations by three annotators to a part of this
dataset and calculated inter-annotator agreement. The
average F1-score of three pairs among these three
annotators was 76.1 for 730 sentences of the Dummy
EHR dataset.
Pathology reports
The other source is a dataset of 1000 short pathology
reports, that differ greatly from the EHRs above. Pathology
reports describe pathological findings by which personal
information (names of patients, doctors, hospitals, and
time expressions) frequently appears, but for which tags of
sex and age rarely appear. Personal names, hospital names,
and dates were manually de-identified beforehand by the
dataset provider, and replaced with special characters. For
machine learning methods to support realistic training
and evaluation, we replaced these special characters with
randomly assigned real entity names as follows. For the
hospital names, we collected 96,167 hospital names which
cover most of the Japanese hospital names, published by
the Japanese government. For the person names, we
manually created 20 dummy-family names and 20
dummy-first names using one of the last names only, or
combining one of the last names and one of the first
names. Additionally, we calculated the inter-annotator
agreement by three annotators. The average F1-score of
three pairs among these three annotators was 80.2 for 500
sentences of this dataset. This Pathology Report dataset is
the only real (not dummy) dataset among our three
datasets. Because we received manually de-identified
version of the original real pathology reports, no ethical
review was necessary.
Methods
We used a Japanese morphological analyzer, Kuromoji,1
for tokenization and part-of-speech (POS) tagging. We
registered our customized dictionary, derived from
Wikipedia entry names and entries of the Japanese
Standard Disease-code master [21], to this morphological
analyzer in addition to the analyzers default dictionary.
We implemented rule-based, CRF-based, and LSTM-
based methods.
Rule-based method
Unfortunately, the implementation of the best system for
the MedNLP-1 de-identification task [22] is not publicly
available. We implemented our own rule-based program
based on the descriptions in their paper, to replicate the
same system to the greatest extent possible. We present
their rules below for a target word x for each tag type.
Age
If xs detailed POS is numeral, then apply the rules in
Table 4.
Hospital
If one of following keywords appeared in x, then mark it
as hospital: ?? (a near clinic or hospital), ?? (this
clinic or hospital), or ?? (same clinic or hospital).
If xs POS is noun and if detailed POS is not non-au-
tonomous word, or if x is either ?, ?, ? or ? (these
symbols are used for manual de-identification because the
datasets are dummy EHRs), and if suffix of x is one of the
Table 3 Detailed results for each privacy type in MedNLP-1 (De-identification task)
<a > age <x > sex <t > time <h > hospital name
P R F P R F P R F P R F
C3 90.32 87.5 88.89 100 100 100 87.16 91.49 89.27 97.30 94.74 96.00
B3 90.00 84.38 87.10 100 50.00 66.67 91.30 89.36 90.32 97.06 86.84 91.67
B1 93.33 87.5 90.32 100 100 100 90.65 89.36 90.00 89.47 89.47 89.47
B2 90.00 84.38 87.10 100 100 100 91.24 88.65 89.93 91.89 89.47 90.67
C1 96.67 90.62 93.55 100 50.00 66.67 91.18 87.94 89.53 93.55 76.32 84.06
A1 92.86 81.25 86.67 100 50.00 66.67 91.04 86.52 88.73 91.89 89.47 90.67
C2 96.67 90.62 93.55 100 50.00 66.67 89.13 87.23 88.17 96.77 78.95 86.96
A2 92.86 81.25 86.67 100 50.00 66.67 89.05 86.52 87.77 91.89 89.47 90.67
D1 92.31 75.00 82.76 100 50.00 66.67 82.84 78.72 80.73 96.15 65.79 78.12
G1 80.65 78.12 79.37 100 50.00 66.67 84.56 81.56 83.03 72.73 63.16 67.61
D3 88.89 75.00 81.36 100 50.00 66.67 83.08 76.60 79.70 96.15 65.79 78.12
D2 92.31 75.00 82.76 100 50.00 66.67 75.86 78.01 76.92 96.15 65.79 78.12
H2 83.87 81.25 82.54 100 100 100 73.79 75.89 74.83 77.78 73.68 75.68
H1 80.65 78.12 79.37 100 100 100 75.86 78.01 76.92 70.27 68.42 69.33
H3 83.87 81.25 82.54 100 100 100 73.79 75.89 74.83 70.27 68.42 69.33
P, R and F were calculated at the phrase level: P, precision; R, recall; F, F1-measure; and A, accuracy. A was calculated in the word level (the agreement ratio of B-*, I-* and O).
The first column stands for participants team names, where the first letter stands for a team ID and the second numerical value stands for a submission run ID
1https://www.atilika.com/ja/kuromoji/
Kajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 5 of 12
following keywords, then mark it as hospital:?? (hospital
or clinic),????? (clinic), or?? (clinic).
Sex
If x is either ?? (man), ?? (woman), men, women,
man, woman (in English), then mark it as sex.
Time
If xs detailed POS is numeral and if x consists
of four-digit-numbers+slash+two-or-one-digit-numbers
(corresponds to yyyy/mm) or two-or-one-digit-
numbers+slash+two-or-one-digit-numbers (corresponds to
mm/dd), then mark it as time.
Table 4 Rules used for our rule-based method, original Japanese with English translations
Option 1 main rule Option 2
?
(next)
??? two years ago ?? (from)
?
(before)
?? last year ?? (until)
???
(before hospitalization)
?? last month ? (s)
???
(after hospitalization)
?? last week ?? (early)
????
(after visit)
?? yesterday ?? (last)
??
(a.m.)
?? this year -- (from)
??
(p.m.)
?? this month -- (from)
????
(after onset)
?? this week ?? (over)
??????
(after onset)
?? today ?? (under)
??????
(after care)
?? today ?? (from)
?? next year ? (when)
?? next month ? (about)
?? next week ?? (about)
?? tomorrow ?? (about)
??? the week after next ?? (early)
??? day after tomorrow ?? (mid)
?? same year ?? (late)
?? same month ? (spring)
?? same day ? (summer)
?? following year ? (fall)
?? the next day ? (winter)
?? the next morning ? (morning)
?? the previous day ? (noon)
?? early morning ? (evening)
??? after that ? (night)
xx? xx (year) ?? (early morning)
xx? xx (month) ?? (early morning)
xx?? xx (week) ?? (before)
xx? xx (day) ?? (after)
xx? xx (oclock) ?? (evening)
xx? xx (minutes) ?? (about)
Kajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 6 of 12
If xs detailed POS is numeral and followed by either
of ? (old), ? (old), or? (s), then mark it as time.
If x is followed further by either of ??, ??, ?
?, ??, ??, ??, ?, ?, ??, ??,
??, ????, ????, ???, ????,
or ????, then include these words in the span of
the marked time tag.
CRF-based method
We implemented a CRF-based system because many
participants used CRFs in the MedNLP-1 de-identification
task, including the second-best team and the baseline
system. The best participant used a rule-based system, as
described previously. We used the MALLET2 library for
CRF implementation. We defined five training features for
each token3: part-of-speech (POS), detailed POS, character
type (Hiragana, Katakana, Kanji, or Number), a binary
feature whether a token is included in our user dictionary
or not, and another binary feature whether a token is
beginning of its sentence or not.
LSTM-based method
Our LSTM-based method combines bidirectional LSTM
(bi-LSTM) and CRF, using character-based and word-
based embeddings (Fig. 1) following earlier work that
had been reported as successful for other languages [16].
For word-based embedding, we used the existing
Word2Vec [23] model, which was trained using Japanese
Wikipedia.4 We used bi-LSTM to embed characters;
then we concatenated these two embeddings. This
concatenated output was fed to another bi-LSTM and
then sent to a CRF to output IOB tags.
Our implementation has been made publicly available
in GitHub.5 Table 5 presents the parameter settings.
Results
Experiment settings and evaluation metrics
We followed the evaluation metrics of the MedNLP-1
shared task using IOB2 tagging [24]. We used four-fold
cross validation, whereas the rule-based method requires
no training data. We prepared five datasets: MedNLP
(MedNLP), dummy EHRs (dummy), pathology reports
(pathology), and MedNLP + dummy EHRs (MedNLP +
dummy). We also prepared a dataset that comprises
these three datasets (all). For each dataset, we applied
cross validation. The CRF and LSTM are trained with
three patterns of training data: the target dataset only,
one of other datasets only, MedNLP + dummy, and all.
Our evaluation uses a strict match of named entity
spans, calculating F1-scores, precisions, and recalls.
Table 6 presents the evaluation results.
Results obtained using the MedNLP dataset
In this MedNLP dataset, the total number of sex is very
small; that of person is zero. The rule-based system per-
formed best in terms of the F1-score because its rules
were tuned originally to the very MedNLP dataset.
LSTM performed best for age and time, probably be-
cause these tags exhibit typical patterns of less variation.
LSTM is superior to Rule, except for sex and hospital.
Regarding sex, we observe better performance when
LSTM uses more training data. Therefore, the data size
is expected to have been the reason why LSTM was not
good in sex.
Results obtained using the dummy EHR dataset
LSTM (M + d) performed best in terms of the F1-score.
CRF performed better when trained by M+ d dataset
than with the target dataset only. This performance in-
crease consists of decrease of age and increase of all
other tags, suggesting that these two datasets differ in
their age tag annotation scheme.
The overall performance of this dummy EHR dataset
is worse than the MedNLP dataset, suggesting that the
dummy EHR dataset is more difficult to de-identify.
Results obtained using the pathology report dataset
The LSTM-based method was better (81.67) than the
CRF-based method (74.26), as shown by the 7.41 point
F1-score when applied to our Pathology Report dataset.
Our rule-based system achieved very high recall, but
very low precision scores for time, exhibiting a difference
by 38 points. The pathology reports include many clin-
ical inspection values written in an xx/yy format, which
might engender confusion with dates expressed in an
mm/dd format. We applied a workaround to limit
[1 < = mm < = 12] and [1 < = dd < = 31], but it was insuf-
ficient: we need contextual information, not just rules.
In addition, hospital is better than time, with less differ-
ence (15 points) of precision and recall.
When trained with the Pathology Report dataset only,
its performance is better than our rule-based system.
When trained with the M+ d dataset, which does not
contain the pathology dataset, neither CRF nor LSTM
works fine because the pathology reports differ greatly in
terms of their styles of description and named entities.
Discussion
These results suggest that our datasets have quite differ-
ent characteristics in what context and in what form
their named entities appear, but LSTM adapted to these
differences well. Adding the Pathological Report dataset
2http://mallet.cs.umass.edu/
3Hereinafter, token means a morpheme of the Japanese language,
which does not have any space between tokens. A morpheme is the
smallest meaningful unit in a language.
4http://www.cl.ecei.tohoku.ac.jp/~m~suzuki/jawiki_vector/
5https://github.com/johokugsk
Kajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 7 of 12
to the training data seems to degrade the system per-
formance for other target test datasets because of the
different dataset characteristics (examples presented in
Table 1). For example, when trained with the Patho-
logical Report dataset, the hospital tags of the MedNLP
dataset show lower performance because of the different
descriptions of hospital names among these two data-
sets. The Pathological Report dataset has full hospital
names such as Shizudai Dermatology Clinic, but the
other two datasets have more casual descriptions such as
Fig. 1 Conceptual figure of our LSTM-based model, showing embedding and NER in separate figures. + means concatenation. The first figure
shows the embedding part, where Wx is an x
th input word, Lx,i is an i
th letter of the word Wx, r denotes right to left (forward) LSTM, l denotes left
to right (backward) LSTM, Vx is an intermediate node which corresponds to Wx. The second figure shows the NER part, where fl denotes forward
LSTM, bl denotes backward LSTM, c denotes concatenated vector, finally a CRF layer is shown with an example predicted named entities in the
BIO annotation style
Table 5 LSTM parameter settings
Word embedding size 200
Character embedding size 100
Hidden layer of character 100
Hidden layer of LSTM 300
Learning rate 0.001
Kajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 8 of 12
Table 6 Evaluation results for each tag and in total, for different methods (rule, CRF, LSTM) and different evaluation datasets
(MedNLP, dummy EHR, and pathology reports). M, d, and P respectively denote training data of MedNLP, dummy EHR, and
Pathology reports; M + d denotes that training data consist of MedNLP+dummy EHR, all stands for all of these three datasets; other
machine learning methods use the target evaluation dataset as its training data. In each cell, F1-score, precision, and recall are
shown (in values multiplied by 100). The best scores for each tag type for each evaluation metric are presented in bold typeface. All
evaluations were done by four-fold cross validations
Evaluation Results on MedNLP dataset
tag type #of tags scores Rule CRF CRF
d
CRF
P
CRF
M+ d
CRF
all
LSTM LSTM
d
LSTM
P
LSTM
M+ d
LSTM
all
total 490 F1 84.23 82.62 43.85 0.71 26.40 67.34 83.07 41.26 0.43 67.35 57.03
prec 78.90 85.63 46.20 2.50 21.51 66.54 81.33 41.07 0.48 66.98 57.94
recall 90.42 79.95 42.33 0.41 59.76 68.38 86.12 41.57 0.38 68.17 56.34
age 56 F1 93.43 71.12 30.00 0.00 32.55 53.04 95.83 71.11 0.00 84.72 87.50
prec 96.00 78.24 37.50 0.00 26.93 56.85 95.83 71.11 0.00 84.72 87.50
recall 91.16 65.47 28.13 0.00 46.05 50.00 95.83 71.11 0.00 84.72 87.50
hospital 75 F1 84.73 87.09 43.25 0.00 26.02 70.04 66.67 13.33 13.89 66.67 41.67
prec 80.75 93.52 66.67 0.00 20.55 91.67 75.00 11.11 10.67 70.83 45.83
recall 89.90 81.71 27.50 0.00 53.06 60.42 62.50 16.67 20.00 63.89 38.89
person 0 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A
sex 4 F1 50.00 16.67 16.67 0.00 14.65 25.00 0.00 20.00 0.00 25.00 25.00
prec 50.00 25.00 12.50 0.00 8.68 25.00 0.00 20.00 0.00 25.00 25.00
recall 50.00 12.50 25.00 0.00 50.00 25.00 0.00 20.00 0.00 25.00 25.00
time 355 F1 50.00 16.67 47.43 0.98 14.65 70.57 96.14 67.22 42.98 89.78 82.67
prec 50.00 25.00 45.16 2.50 8.68 65.46 95.00 66.26 39.46 88.68 81.53
recall 50.00 12.50 50.19 0.61 50.00 76.50 97.41 68.30 47.94 91.00 82.67
Evaluation Results on Pathology Report dataset
tag type #of tags scores Rule CRF CRF
M
CRF
d
CRF
M+ d
CRF
all
LSTM LSTM
M
LSTM
d
LSTM
M+ d
LSTM
all
all 71 F1 13.97 74.26 0.00 0.62 1.45 57.63 81.67 0.00 0.00 1.45 81.25
prec 8.65 86.72 0.00 1.47 10.00 64.98 86.88 0.00 0.00 10.00 82.48
recall 43.33 65.16 0.00 0.39 0.78 54.06 78.84 0.00 0.00 0.78 80.15
age 0 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A
hospital 31 F1 31.19 0.00 0.00 0.00 0.00 0.00 25.00 0.00 13.33 0.00 58.33
prec 26.47 0.00 0.00 0.00 0.00 0.00 25.00 0.00 13.33 0.00 58.33
recall 41.28 0.00 0.00 0.00 0.000 0.00 25.00 0.00 13.33 0.00 58.33
person 224 F1 0.00 91.08 0.00 0.00 6.25 71.31 95.19 0.00 0.00 0.00 95.83
prec 0.00 95.83 0.00 0.00 10.00 74.79 95.19 0.00 0.00 0.00 95.83
recall 0.00 87.21 0.00 0.00 4.55 69.63 95.19 0.00 0.00 0.00 95.83
sex 0 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A
time 40 F1 9.25 10.57 0.00 2.00 0.00 18.82 25.00 3.81 0.00 6.25 19.44
prec 5.25 16.67 0.00 1.79 0.00 20.83 25.00 6.67 0.00 10.00 19.44
recall 43.09 9.09 0.00 2.27 0.00 19.32 25.00 2.67 0.00 4.55 19.44
Kajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 9 of 12
?? (hospital nearby) and ?? (our hospital). The
Pathology Report dataset has different contextual pat-
terns that could have learned by machine learning
methods such as ???? (ex-hospital sample) imme-
diately before hospital tags, and a suffix/prefix such as
xx hospital or xx clinic. These words, hospital and
clinic, might have been learned as semantically similar
by Word2Vec.
Another difference of datasets is the coherence of an-
notations. We compared the original annotations of the
datasets with our own new annotations created for this
study by three annotators. These new annotations were
created to calculate inter-annotator agreement as de-
scribed in the Dataset section. The original versus new
inter-annotator agreement (and inter-annotator agree-
ment of the three annotators) in average F1-scores were
0.566 (0.861), 0.342 (0.761), and 0.772 (0.802), respect-
ively, for the MedNLP, Dummy, and Pathology Report
datasets. As these scores strongly suggest, the original
annotations were insufficiently coherent. By contrast,
our new annotations are much more coherent because
we have included more detailed annotation guidelines.
For example, our guidelines include specifications of
prefixes, suffixes and classifiers.. Annotating larger data-
sets with this coherent guideline is anticipated as a sub-
ject for future work. It is particularly interesting that our
system performance was better than the inter-annotator
agreement in the Pathology Report dataset. One reason
is expected to be the remaining vague part of the guide-
line, such as inclusion of particles when assigning named
entities. We applied the automatic tagger for pre-
annotation; then human annotators reviewed the results.
However, annotators sometimes overly depend on auto-
matically annotated parts-of-speech without considering
the context and semantics; alternatively, the part-of-
speech tagger can simply fail. Therefore, an annotation
guideline including precise part-of-speech specifications
will be required.
An earlier study that applied a similar LSTM-based
method to de-identify English medical data [25] found
lower F1-scores for LOCATION and NAME tags on the
i2b2 2014 dataset and MIMIC-III dataset [26], which
includes records of 61,532 patients in an intensive care
unit (ICU); performance of naïve CRF was very low. This
LOCATION tag corresponds to our hospital tag, exhi-
biting similar characteristics among different languages.
Table 6 Evaluation results for each tag and in total, for different methods (rule, CRF, LSTM) and different evaluation datasets
(MedNLP, dummy EHR, and pathology reports). M, d, and P respectively denote training data of MedNLP, dummy EHR, and
Pathology reports; M + d denotes that training data consist of MedNLP+dummy EHR, all stands for all of these three datasets; other
machine learning methods use the target evaluation dataset as its training data. In each cell, F1-score, precision, and recall are
shown (in values multiplied by 100). The best scores for each tag type for each evaluation metric are presented in bold typeface. All
evaluations were done by four-fold cross validations (Continued)
Evaluation Results on Dummy EHR dataset
tag type #of tags scores Rule CRF CRF
M
CRF
P
CRF
M+ d
CRF
all
LSTM LSTM
M
LSTM
P
LSTM
M+ d
LSTM
all
total 3017 F1 43.74 66.97 44.01 19.67 67.13 65.79 63.99 20.33 1.60 69.82 68.19
prec 42.89 66.77 67.35 56.72 67.60 68.27 68.76 26.68 2.22 72.79 80.26
recall 44.75 67.34 33.28 12.34 66.69 63.63 60.20 17.03 1.25 67.24 60.04
age 39 F1 51.13 48.46 29.35 0.00 38.87 33.82 50.00 22.38 0.00 50.00 41.67
prec 51.97 65.25 28.85 0.00 41.56 35.72 50.00 19.05 0.00 50.00 45.83
recall 50.46 53.74 30.00 0.00 36.71 32.50 50.00 32.38 0.00 50.00 41.67
hospital 170 F1 15.98 47.85 33.19 0.00 48.62 35.73 22.22 35.79 0.00 40.00 43.33
prec 10.07 53.18 38.75 0.00 44.91 35.90 28.33 34.48 0.00 37.50 45.83
recall 39.06 43.73 29.42 0.00 53.60 37.81 29.17 37.33 0.00 43.75 41.67
person 135 F1 0.00 26.96 0.00 0.00 28.36 15.48 50.00 0.00 0.00 45.83 37.50
prec 0.00 26.79 0.00 0.00 29.91 19.64 50.00 0.00 0.00 45.83 37.50
recall 0.00 30.71 0.00 0.00 27.99 13.39 50.00 0.00 0.00 45.83 37.50
sex 16 F1 93.75 35.92 29.17 0.00 90.08 33.93 0.00 40.00 0.00 50.00 50.00
prec 100.0 44.27 50.00 0.00 95.83 50.00 0.00 40.00 0.00 50.00 50.00
recall 90.00 43.13 20.83 0.00 85.63 27.08 0.00 40.00 0.00 50.00 50.00
time 2657 F1 49.48 71.28 42.14 21.20 70.60 68.33 83.93 51.97 48.89 85.70 88.20
prec 51.81 71.44 64.94 59.35 71.24 70.94 84.82 52.59 48.89 86.51 89.24
recall 47.38 71.15 32.08 13.58 70.00 66.08 83.29 51.46 48.89 84.93 87.23
Kajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 10 of 12
The LSTM-based method can be regarded as effective in
Japanese medical de-identification tasks as well. If a
larger dataset were available, then it would yield better
performance.
Japanese-specific issues include the following difficul-
ties: Japanese (and Chinese) have no spaces between to-
kens, which makes tokenization much more difficult and
ambiguous. The number of letter types is much greater
than in other languages, including tens of thousands of
kanji letters, 50 hiragana letters, 50 katakana letters, nu-
merals, and alphabets. The languages also have more
synonyms than in other languages.
Our system performance almost reaches to the inter-
annotator agreement, which can be regarded as upper
bound of system performance. The current performances
are sufficiently high compared to other publicly available
Japanese de-identification tools. Therefore, we plan to apply
our system to actual de-identification tasks in hospitals.
Conclusions
We implemented three de-identification methods for
Japanese EHRs and applied these methods to three data-
sets, which are derived from two dummy EHR sources
and one real Pathology Report dataset. These datasets
have manually annotated de-identification tags, following
the MedNLP shared task annotation guideline.
Our best F1-scores over all the tag types are 84.23 (rule-
based), 68.19 (LSTM), and 81.67 (LSTM) points, respect-
ively, for the MedNLP dataset, the dummy EHR dataset,
and the Pathology Report dataset. Our LSTM-based
method performed best in two datasets, whereas our rule-
based method performed best in the MedNLP dataset.
However, our LSTM-based method also achieved a good
score of 83.07 points in the MedNLP dataset, which only
differs 1.16 points from the best score of the rule-based
method. Our results demonstrate that the bi-LSTM based
method with character-embedding and word-embedding
tends to work better than other methods, exhibiting more
robustness than CRF over different data sources. The
LSTM-based method was better than the CRF-based
method, exhibiting a 7.41 point F1-score difference when
applied to our Pathology Report dataset. This report is the
first describing a study applying this LSTM-based method
to any de-identification task of Japanese EHRs.
Machine learning methods can extract named entities
of de-identification comparable to a rule-based method
that is tuned manually to specific target data. However,
machine learning methods are still less adequate for ap-
plication to expressions with low occurrence. Probably
because of the insufficient data size, our methods yielded
worse evaluation scores than were obtained with the
other languages when applied to the i2b2 task and
MIMIC-III. Combinations of LSTM and rule-based
methods are left as a subject for future work.
The current performance is sufficiently high among
publicly available Japanese de-identification tools. There-
fore, we plan to apply our system to actual de-
identification tasks in hospitals. Although it is still diffi-
cult to make real EHRs publicly available, we could use
our large amount of EHRs inside our hospitals. Increas-
ing the size of annotated datasets for such internal usage
is left as another subject for future work.
Abbreviations
NLP: natural language processing; LSTM: Long Term Short Memory: a kind of
recurrent neural network; CRF: Conditional Random Field: a kind of machine
learning method; POS: part-of-speech; EHR: electronic health record
Acknowledgments
We wish to thank the Research Center for Medical Bigdata at National
Institute of Informatics, Japan, for providing the anonymized pathology
reports. We thank the JP-AID/NII Research Group for their cooperation and
for providing clinical data.
Authors contributions
KK designed and implemented all the systems. YK directed the research,
especially that related to training and evaluation. TO and HH created the
dummy EHRs. MM and YK created the MedNLP task series data. The authors
read and approved the final manuscript.
Funding
This work was partially supported by Japanese Health Labour Sciences
Research Grant and JST CREST.
Availability of data and materials
The source code will be made available on the web; datasets will be made
partially available.
Ethics approval and consent to participate
The Pathology Reports dataset was used under approval by the ethics
committee and the research committee of the Japanese Society of
Pathology under a research grant from the Japan Agency for Medical
Research and Development (AMED), Japan Pathology AI Diagnostics Project
(JP-AID).
Consent for publication
All the authors have agreed to publication of this manuscript.
Competing interests
N/A
Author details
1Faculty of Informatics, Shizuoka University, Johoku 3-5-1, Naka-ku,
Hamamatsu, Shizuoka 432-8011, Japan. 2National Hospital Organization
Headquaters, 2-5-21 Higashigaoka, Meguro-ku, Tokyo 152-8621, Japan.
3National University Corporation Kitami Institute of Technology, 165,
Koencho, Kitami, Hokkaido 090-8507, Japan. 4Graduate School of
Interdisciplinary Science and Engineering in Health Systems, Okayama
University, 2-5-1, Kita-ku, Okayama, Okayama 700-8558, Japan.
Received: 13 May 2019 Accepted: 7 August 2020
RESEARCH Open Access
Structuring, reuse and analysis of electronic
dental data using the Oral Health and
Disease Ontology
William D. Duncan1,2* , Thankam Thyvalikakath2,3, Melissa Haendel4, Carlo Torniai5, Pedro Hernandez6, Mei Song7,
Amit Acharya8, Daniel J. Caplan9, Titus Schleyer2,10 and Alan Ruttenberg11
Abstract
Background: A key challenge for improving the quality of health care is to be able to use a common framework to
work with patient information acquired in any of the health and life science disciplines. Patient information
collected during dental care exposes many of the challenges that confront a wider scale approach. For example, to
improve the quality of dental care, we must be able to collect and analyze data about dental procedures from
multiple practices. However, a number of challenges make doing so difficult. First, dental electronic health record
(EHR) information is often stored in complex relational databases that are poorly documented. Second, there is not
a commonly accepted and implemented database schema for dental EHR systems. Third, integrative work that
attempts to bridge dentistry and other settings in healthcare is made difficult by the disconnect between
representations of medical information within dental and other disciplines EHR systems. As dentistry increasingly
concerns itself with the general health of a patient, for example in increased efforts to monitor heart health and
systemic disease, the impact of this disconnect becomes more and more severe.
To demonstrate how to address these problems, we have developed the open-source Oral Health and Disease
Ontology (OHD) and our instance-based representation as a framework for dental and medical health care
information. We envision a time when medical record systems use a common data back end that would make
interoperating trivial and obviate the need for a dedicated messaging framework to move data between systems.
The OHD is not yet complete. It includes enough to be useful and to demonstrate how it is constructed. We
demonstrate its utility in an analysis of longevity of dental restorations. Our first narrow use case provides a
prototype, and is intended demonstrate a prospective design for a principled data backend that can be used
consistently and encompass both dental and medical information in a single framework.
(Continued on next page)
© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if
changes were made. The images or other third party material in this article are included in the article's Creative Commons
licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons
licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain
permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the
data made available in this article, unless otherwise stated in a credit line to the data.
* Correspondence: wdduncan@gmail.com
Titus Schleyer and Alan Ruttenberg are joint senior authors
1National Center for Ontological Research, Buffalo, NY, USA
2Center for Biomedical Informatics, Regenstrief institute, Inc., Indianapolis, IN,
USA
Full list of author information is available at the end of the article
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 
https://doi.org/10.1186/s13326-020-00222-0
(Continued from previous page)
Results: The OHD contains over 1900 classes and 59 relationships. Most of the classes and relationships were
imported from existing OBO Foundry ontologies. Using the LSW2 (LISP Semantic Web) software library, we translated
data from a dental practices EHR system into a corresponding Web Ontology Language (OWL) representation based
on the OHD framework. The OWL representation was then loaded into a triple store, and as a proof of concept, we
addressed a question of clinical relevance  a survival analysis of the longevity of resin filling restorations. We
provide queries using SPARQL and statistical analysis code in R to demonstrate how to perform clinical research
using a framework such as the OHD, and we compare our results with previous studies.
Conclusions: This proof-of-concept project translated data from a single practice. By using dental practice data, we
demonstrate that the OHD and the instance-based approach are sufficient to represent data generated in real-
world, routine clinical settings. While the OHD is applicable to integration of data from multiple practices with
different dental EHR systems, we intend our work to be understood as a prospective design for EHR data storage
that would simplify medical informatics. The system has well-understood semantics because of our use of BFO-
based realist ontology and its representation in OWL. The data model is a well-defined web standard.
Keywords: Ontology, Dental health, Informatics, Electronic heath record, OWL, SPARQL
Background
A key challenge for improving the quality of healthcare
is to be able to use a common framework to work with
patient information acquired in any of the health and life
science disciplines. The patient information collected
during dental care exposes many of the challenges that
confront a wider scale approach. Within dentistry, a key
aspect for improving the quality of care is the ability to
collect and analyze data about oral health conditions
and procedures, such as the longevity of fillings, the fre-
quency of patient checkups, and incidence of tooth loss.
Recent reports estimate that 73.8% of solo practitioners
and 78.7% of group practitioners in the U.S. use a com-
puter to manage some, and 14.3 and 15.9%, respectively,
all patient information on a computer [1] . In conse-
quence, we now have the opportunity to study dental
health services and perform outcomes research using
large amounts of secondary data obtained from geo-
graphically dispersed dental practices [2].
Large secondary datasets could help us more easily
study diseases in a sizable samples with increased statis-
tical power, track patients for an extended period of
time, provide valid and representative samples, supply
correlates not commonly collected in an oral health set-
ting, collect data in real time and ascertain potential
confounders [2].
Analyzing data from electronic health records (EHR),
however, presents a number of challenges. First, dental
EHR information is often stored in relational databases
that are poorly documented and have complex relations
between tables. This makes extracting and analyzing
data from even a single practices system difficult. Sec-
ond, dental EHR database schemas vary depending on
the vendor who developed the system. This adds diffi-
culty when integrating data from multiple practices.
Third, information is not always encoded in the same
way. For example, a tooth encoded as number (e.g.,
tooth 6) or as a character array in which the index pos-
ition of a character represents the tooth (e.g., the Y in
the character array NNNNNYNNNNNNNNNNNN
NNNNNNNNNNNNNN represents a right upper sec-
ondary canine tooth, i.e., tooth 6). Last, dental EHR sys-
tems are typically only loosely specified. So, outside of a
common core of structures for the oral cavity and its
parts, there is wide variation in how information such as
specific types of materials, details of methods, instru-
ments, and general patient health is represented. Much
of this information is either semi-or unstructured text.
While we focus here on dental EHRs, these same prob-
lems are endemic in other EHR systems.
To demonstrate how to address some of these problems,
we have developed the Oral Health and Disease Ontology
(OHD) as a common framework for representing dental
health information embedded in a larger framework ad-
equate to accommodate structured representation that
goes beyond that in current dental EHR systems and ex-
tends into general medicine. The OHD contains terms for
representing anatomical structures (e.g., distal surface of
tooth), dental procedures (e.g., tooth extraction), and oral
conditions (e.g., caries), as well as relations between terms
(e.g., distal surface is part of tooth). The OHDs structure
provides a common representation of the entities that
EHR data is about, without being designed in a way that
unintentionally limits it to only dental health data. This
makes it possible to use the OHD as framework for inte-
grating inhomogeneous data from disparate database sys-
tems and support representations for future systems.
Using the OHDs terms and relations, information from
multiple dental EHRs can now be translated into OWL 2
[3] statements, stored in a semantic database or triple
store, and queried using SPARQL [4] to extract informa-
tion for analysis.
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 2 of 19
As a proof of concept, we have translated dental EHR
data from a single dental practice, and performed a sur-
vival analysis of the longevity of resin filling restorations.
The proof of concept demonstrates both aspects of the
OHD that are specific to dentistry (e.g. teeth, restora-
tions and other procedures) as well as aspects that would
remain unchanged in a general medical context, such as
demographic correlates. The output of this analysis is
discussed in the Results section. In the Discussion sec-
tion we describe the potential for wider application.
Related work
The work in this paper expands upon previous work de-
veloping the OHD [5, 6], and provides a more detailed
explication of the OHDs structure. It differs from previ-
ous ontology work, such as PeriO [7] and BigMouth [8],
in two respects.
First, it focuses on the domain of dental anatomy and
procedures rather than genomic information. Second,
the OHDs use of the Basic Formal Ontology and Ontol-
ogy of Biomedical Investigations as an upper-level
framework sets the stage for seamlessly extending it to
general medical information. Moreover, the OHD is not
a data repository, such as BigMouth [8], but a semantic
framework for representing data that may be used in the
design of repositories  such as our semantic
technology-based repository of information translated
from (for now) a single dental practice.
We considered using SNOMED and its dental subset
SNODENT, but there are problems that make these
standards, at the moment, unusable for our purposes.
First, their licenses restrict modification of substantial
parts of the standard. This prevents us from reorganiz-
ing content according to realist principles, adding defini-
tions, or adding or correcting axioms. Not all countries
license to use SNOMED, and this would prevent our
work from being replicable worldwide.
Second, there are serious quality issues with
SNOMED, and SNODENT in particular [9]. A major
issue is the question of ontological commitment  what
terms mean. The vast majority of terms in SNODENT
and SNOMED come without textual definitions [10],
and the question of what SNOMED terms actually rep-
resent is still up for debate.
Third, use of these terminologies typically is within a
layered framework that brings unnecessarily complica-
tion [11]. In common usage these resources are bound
to data models of medical records [12]. That means that
one needs to separately understand the data models and
the ontology. By contrast, in our approach the ingredi-
ents for a representation are simple  an OWL ontology
and high-quality SPARQL, OWL and RDF W3C specifi-
cations. Those logic-based specifications are substan-
tially clearer than HL7 specifications.
The Open Biological and Biomedical Ontology (OBO)
Foundry approach is to have, for any given class, a single
identifier, if necessary coordinating with developers of
other ontologies. The realism-based approach empha-
sizes that classes are collections of instances, that the in-
stances are things in the world, and that documentation
should make clear what those instances are. The OHD
and the semantic technologies used to implement the
ontology make it relatively easy to merge data. The data
is just added together, untransformed. It is possible to
do this, in theory, because all parts of the representation
are clearly understood, the types of entities are shared,
and the choice to represent particulars using the stand-
ard methods provided by semantic web standards allow
for little creativity in how concrete representations are
constructed. Because our focus is on showing how a uni-
fied representation system works, we consider out of
scope general methods for harmonizing or interchanging
data with different representations, as is the focus of
HL7.
Recently, authors AR and WD have started participat-
ing in the review and development of SNODENT. It is
entirely possible that in the future that SNOMED and
SNODENT might be used in the same manner that we
use OHD here. The OHD and the source code used for
translation and analysis are available in full at https://
github.com/oral-health-and-disease-ontologies/ohd-
ontology and in part in the Additional file 1.
Methods
Ontology development
The OHD was developed in a collaborative effort be-
tween dental researchers, practicing dentists, statisti-
cians, informatics experts, and ontologists. Our first task
was to identify which dental entities would be repre-
sented. To guide this process, we developed a set of re-
search questions. For example, for the research question,
What is the time from one restoration to its replace-
ment on the same tooth?, we determined that we would
need to represent restoration procedures, the dates of
the procedures, patients, patients teeth, surfaces of
teeth, and the restoration materials used to restore teeth.
We provide the list of driving research questions in
Additional file 1.
Once our domain of focus was identified, our next
step was to catalog the terms1 we would need in the
ontology. We imported the Basic Formal Ontology
(BFO) and the Ontology for General Medical Science
(OGMS) as a whole and otherwise extracted terms from
existing OBO Foundry ontologies that represented en-
tities relevant to our dental health domain using custom
1In this paper, we use the word term as a unique natural language
expression for a class, instance, or relation in our ontology.
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 3 of 19
programs as well as the OntoFox2 web tool [13]. The
OntoFox tool implements the Minimum Information to
Reference an External Ontology Term (MIREOT)
principle [14]. MIREOT is a practice by which one im-
ports a selected set of terms from another ontology ra-
ther than including the whole ontology, as importing in
OWL would do. Where relevant terms were not present
in an existing ontology, we created new terms. Each new
term was assigned an Internationalized Resource Identi-
fier (IRI) [15], a human-readable label, a definition or
documentation, and the name of the terms editor(s).
When appropriate, other metadata was included, such as
the reference source for a definition and comments
about a terms definition such as its rationale, scope, and
usage. Throughout the ontology development process,
the definitions were reviewed multiple times by team
members. In the following sections, we discuss the
methods for acquiring the necessary terms.
Ontology architecture
The OHD is constructed in line with a number of OBO
Foundry principles. The OBO Foundry [16] is a collect-
ive of ontology developers who are committed to collab-
oration and adherence to shared principles. The mission
of the OBO Foundry is to develop a family of interoper-
able ontologies that are both logically well-formed and
scientifically accurate. OBO Foundry principles include
use of the Basic Formal Ontology (BFO) [17], an upper-
level ontology, use of a standard IRI identifier space, re-
use, where possible, of other Foundry ontologies, and
the inclusion of a textual and, where feasible, logical def-
inition for each class and relation.
Ontology reuse
The OHD uses BFO as its upper-level ontology. BFO is de-
signed as a domain-independent ontology based on princi-
ples of ontological realism [18] As an upper-level ontology,
BFO establishes categories such as material entities, pro-
cesses, time, space, and realizable entities (properties), as well
as relations among them, such as the relation between a par-
ticipant and a process they participate in.
We reuse a number of classes and relations from exist-
ing OBO Foundry ontologies, such as the Foundational
Model of Anatomy (FMA) [19] and the Ontology for
Biomedical Investigations (OBI) [20].
This construction methodology serves two purposes.
First, it allows us to leverage the experience of the devel-
opers of OBO ontologies. Second, adhering to OBO
standards and precedents makes the OHD more easily
interoperable with other OBO ontologies [16], and this
allows developers to reuse our classes and provide
feedback on how to improve the OHD. A summary of
the reused ontologies is provided in Table 1.
Classes from the ontologies listed in Table 1 are then
extended to encompass entities in the oral health do-
main. At present, this includes classes for representing
teeth and tooth surfaces, dental procedures, patients,
providers, restoration materials, dental findings, and bill-
ing codes. Each of these classes is discussed in the fol-
lowing sections.
Anatomical structures
We use the FMAs classes to represent anatomical struc-
tures, such as jaws, teeth, and tooth roots. However, in
our initial construction of the OHD, we found that the
FMA was not adequate for representing surfaces of
teeth. The FMAs class surface of tooth is used to repre-
sent the two-dimensional curved plane that forms the
outer boundary of a tooth. This is not suitable for repre-
senting the portions of enamel into which restoration
material is placed. Thus, we added the class surface en-
amel of tooth to represent the portions of enamel that
constitute a tooths anatomical crown. The need for this
class was reported to the FMAs curators, and the FMA
now includes the class surface layer of tooth3 to address
this.
Until recently, the FMA was authored in a representa-
tion system called Protégé Frames. In order to use it
within the OBO framework we needed to translate from
the native frames version to a version that integrates
with OBO ontologies. As part of that translation, classes
in FMA were placed as children of the appropriate BFO
or OBO classes. Second, we needed to translate the
frames expressions [25] to OWL before we could use it
with the other classes OBO classes.
2http://ontofox.hegroup.org 3http://purl.org/sig/ont/fma/fma290055 (accessed August 2018)
Table 1 Summary of ontology reuse in OHD
Ontology Classes/relations reused or specialized
Basic Formal Ontology (BFO) upper-level ontology used to
coordinate other OBO ontologies
Ontology for General Medical
Science (OGMS) [21]
health care entities; e.g., patient role,
visit, disorder
Foundational Model of
Anatomy (FMA)
anatomical entities; e.g., jaw, tooth,
tooth surface
Ontology for Biomedical
Investigations (OBI)
relations between processes to
entities; e.g., restoration procedure has
specified input some tooth
Information Artifact Ontology
(IAO) [22]
information entities in the dental
health care domain; e.g., billing codes,
goals of dental procedures
Ontology of Medically Related
Social Entities (OMRSE) [23]
gender of patient
Common Anatomy Reference
Ontology (CARO) [24]
male and female organism
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 4 of 19
Patients and health care providers
A given dental procedure (such as an oral evaluation or
restoration procedure) will minimally involve a patient
and the dental health care provider. We define for the
patient and health care provider roles that characterize
the way in which patients and providers participate in
dental procedures.
In BFO, roles are realizable entities which are in
turn dependent entities. A dependent entity is one
that cannot exist unless the entity bearing the role
exists. For example, a particular patient role cannot
exist unless the organism that bears the role (i.e., the
patient) exists. A role is optional in the sense that an
entity may gain or lose a role without its physical
makeup being changed. For instance, a person may
cease to be patient at some practice due to the prac-
tice going out of business. The practices going out of
business is an event that is external to the person,
and, thus, does not necessitate that the person is
somehow physically changed. Roles are realizable in
the sense that their existence can be manifested in a
correlated process. For instance a dental hygienist role
is realized when the hygienist engages in processes
related to their profession, such as plaque removal
and application of fluoride treatment. Roles and other
dependent continuants inhere, or are borne by, mater-
ial entities.
Employing this distinction between roles and their
bearers, we define the types dental health care provider
and human dental patient by first defining the appropri-
ate roles for each kind of entity, and then defining pro-
viders and patients as being bearers of the roles4:
A dental health care provider role is a role that
inheres in a person who is licensed to provide den-
tal health care and is realized in a health care
process.
A dental health care provider is a human being who
bears a dental health care provider role.
A patient role is a role that inheres in a person and
is realized by the process of being under the care of
a physician or health care provider. (OGMS)
A dental patient role is a patient role that is realized
by the process of being under the care of a dental
health care provider.
A human dental patient is a human being who
bears a dental patient role.
In order to define the patients gender, we use the gen-
der role types from the Ontology of Medically Related
Social Entities (OMRSE). The OMRSE is a realist repre-
sentation of medically related social entities developed
to cover demographics data and common roles of people
in healthcare encounters for reuse in the context of the
OBO Foundry. The gender role types are defined as
follows:
A gender role is a human social role borne by a hu-
man being that is realized in behavior which is con-
sidered socially appropriate for individuals of a
specific sex in the context of a specific culture.
(OMRSE)
A female gender role is a gender role borne by a hu-
man being that is realized in behavior which is con-
sidered socially appropriate for individuals of the
female sex in the context of the culture in question.
(OMRSE)
A male gender role is a gender role borne by a hu-
man being that is realized in behavior which is con-
sidered socially appropriate for individuals of the
male sex in the context of the culture in question.
(OMRSE)
Female and male dental patients are then simply de-
fined by relating the patient to the female and male gen-
der roles:
A female dental patient is a human dental patient
who bears a female gender role.
A male dental patient is a human dental patient
who bears a male gender role.
Using roles to define patients and dental health care
providers has two advantages. First, because roles are
formally defined, they represent the semantics for how
an entity participates in a procedure. That is, for a given
dental procedure, the patient participant is the entity
whose participation realizes the dental patient role, and
the provider participant is the entity whose participation
realizes the dental health care provider role. In contrast,
field names and values in relational databases are purely
syntactic.
Second, by using gender roles instead of anatomical
sex to represent male and female dental patients, we
allow for the possibility that the gender a patient assigns
to himself or herself may differ from the patients ana-
tomical sex (at birth), matching the common practice of
recording patient-reported gender in clinical systems. In
those cases in which biological sex needs to be4Classes/relations are defined in the OHD unless indicated otherwise.
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 5 of 19
represented, the OHD includes CAROs types female or-
ganism and male organism:
A female organism is a gonochoristic organism that
can produce female gametes. (CARO)
A male organism is a gonochoristic organism that
can produce male gametes. (CARO)
Using these classes, a patients biological sex can then
be defined according to biological criteria rather than
gender selection.
Dental procedures
We define the class dental procedure as a subclass of
OGMS health care encounter class:
A health care encounter is a temporally-connected
health care process that has as participants an
organization or person realizing the health care pro-
vider role and a person realizing the patient role.
The health care provider role and patient role are
realized during the health care encounter. (OGMS)
A dental procedure is a health care encounter that
realizes a dental patient role in which the patient
undergoes a diagnostic or therapeutic process.
As illustrated in Fig. 1, specific dental procedures are
then defined by specializing the dental procedure class.
For instance, endodontic procedure, surgical dental pro-
cedure, and tooth restoration procedure are defined as
follows:
An endodontic procedure is a dental procedure that
is performed on the pulp chamber and/or root canal
of a tooth, or a part thereof.
A surgical dental procedure is a dental procedure in
which there is structural alteration of soft tissue or
bone in or around the oral cavity by incision or
destruction of tissues or by manipulation with in-
struments causing localized alteration or transporta-
tion of tissue, including lasers, ultrasound, ionizing
radiation, scalpels, probes, and needles.
A tooth restoration procedure is dental procedure in
which either a whole tooth or a part of a tooth is re-
placed by dental restoration material in order to re-
establish the tooth's anatomical and functional form
and function.
More specific surgical and restoration procedures are
then defined as subclasses of these terms. For example, a
non-exhaustive set of surgical and restorative procedures
defined in the OHD include:
A tooth extraction procedure is a surgical dental
procedure that removes a tooth from the oral cavity.
A crown restoration procedure is a tooth restoration
procedure whereby an artificial crown replaces all or
part of the natural dental crown.
A direct restoration procedure is a tooth restoration
procedure in which the dental restoration material is
placed in the tooth via some direct dental material
insertion process.
An indirect restoration procedure is a tooth restor-
ation procedure in which the dental restoration ma-
terial is placed in the tooth via some dental material
tooth attachment process.
An intracoronal restoration procedure is a tooth res-
toration procedure in which a dental restoration ma-
terial is placed into a site that is located in the
crown of the tooth.
A veneer restoration procedure is a tooth restoration
procedure in which a thin layer of material (i.e., a
veneer) is placed over one or more surfaces of the
Fig. 1 A portion of the hierarchy of health care encounters in OHD. Numbers represent the number of direct subclasses for a class, some not
shown for reasons of space
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 6 of 19
tooth for purposes such as improving the aesthetics
of the tooth or protecting the tooth's surface from
damage.
Patients and providers are related to dental procedures
using BFOs has participant and realizes relations. The
has participant relation is a general way of relating pro-
cesses to the entities involved in them. For example, an
oral evaluation (minimally) has as participants the pa-
tient undergoing the evaluation and the provider doing
the evaluation. The realizes relation holds between a
process and a realizable entity such as a role. A role is
defined in terms of what bears it, what process realizes
it, and the manner in which the bearer participates in
the process. As an example, consider the aforementioned
dental patient role and dental health care provider role.
When a dental procedure is performed, the procedure
realizes the roles of the patient and provider. The person
upon whom the procedure is performed acts (or be-
haves) as the dental patient and the person doing the
procedure acts (or behaves) as the provider. In this way,
the dental procedure realizes the dental patient role of
the patient and the dental health care provider role of
the provider.
BFO defines temporal regions and a relation occupies
temporal region that defines the temporal span of a
process. However, we dont use this representation for
two reasons. First, there isnt yet an established OBO
practice for specifying concrete dates. Second, the time
of a procedure is recorded only to the granularity of a
day. Pending development of representations that ac-
commodate these issues, we defined a date property, oc-
currence date, that relates a process to an xsd:dateTime
some time during which the process occurred. Another
data property birth date was defined to relate a patient
to their date of birth.
To characterize the way in which a tooth participates
in a specific dental procedure, we define roles that are
borne by the tooth and realized in the appropriate corre-
sponding procedure. For example, in order to represent
that a tooth undergoes a root canal treatment, we specify
that a tooth bears a particular tooth to undergo endodon-
tic procedure role and this role is then realized in a par-
ticular endodontic procedure.
For procedures that involve restorative materials, we de-
fine a dental restoration material role that is borne by (i.e.
possessed by) the restoration material. The role helps de-
fine the material in a domain-neutral way. All gold is
metal, but not all gold is used in dental restorations, just
those that bear the dental restoration material role.
This role is then realized by the corresponding restor-
ation procedure. For instance, an intracoronal restor-
ation procedure (see above) realizes the dental
restoration material role of the material that is placed
inside the crown of the tooth. In procedures that involve
a specific kind of material, we use OBIs has_specified_
input relation to express that a procedure uses that ma-
terial. For example, an amalgam filling restoration is de-
fined as follows:
An amalgam filling restoration is an intracoronal
restoration procedure that uses amalgam to restore
the tooth.
As part of the logical framework of the OHD, we then
include the axiom that an amalgam filling restoration
has_specified_input some portion of amalgam restor-
ation material.
Restoration materials, restored teeth, and prosthetics
For dental procedures that involve the use of restoration
materials (e.g., amalgam), we define the restoration ma-
terials in terms of the role the material has in replacing
portions of the tooth. In general, dental restoration ma-
terial has the role of serving as a prosthetic, that is, the
material has the role of replacing a missing body part.
However, not all prosthetics replace the function of the
missing body part, for example, a prosthetic eye cannot
see, although it still functions to maintain the shape of
the skull near the eyes. To address this, we define the
term functional prosthetic role to represent a prosthetic
that performs the function of the replaced body part.
Since dental restoration materials perform the function
of parts of the tooth they replace, we define dental res-
toration material role as a subtype of functional pros-
thetic role:
A functional prosthetic role is a prosthetic role that
is realized by activities in which the material entity
(bearing the role) is used a manner that is similar to
how the body part that the prosthesis replaces
would be used.
A dental restoration material role is a functional
prosthetic role that is borne by a portion of dental
restoration material and is realized in a tooth restor-
ation procedure in which the restoration material
becomes part of a restored tooth.
Functional prosthetic role is not a term that is specific
to oral health and so should lie outside the scope of the
OHD. Our inclusion of it demonstrates a necessarily
pragmatic approach in which we sometimes define gen-
eral terms necessary to capture a term within our do-
main when they are not yet present in a more
appropriate ontology. In this case, the term would more
properly belong to OGMS.
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 7 of 19
Outright replacements of teeth such as pontics or im-
plants also have a functional prosthetic role. We do not
consider, however, restored teeth to be, as a whole, pros-
thetic. We introduce the term functional tooth, as the
union of original teeth, restored teeth, and prosthetic
teeth.
The restoration materials themselves are defined as a
kind of processed material that bears a dental restoration
material role:
A processed material is a material entity that is cre-
ated or changed during material processing. (OBI)
A dental restoration material is a processed material
that bears a dental restoration material role.
Specific restoration materials are defined according to
the substance that constitutes them or how they are
used. For instance, amalgam dental restoration material
and resin dental restoration material are defined as
follows:
A metal dental restoration material is a dental res-
toration material that consists mostly of metal
atoms.
An amalgam dental restoration material is a metal
restoration material that consists of a silver-colored,
metallic alloy which is composed of a mixture of
mercury and other metals.
A resin dental restoration material is a tooth-
colored dental restoration material made from a
mixture of resin, silica and other materials used in
direct restorations.
By using the term dental restoration material role to
define dental restoration material, we are then able to
relate the restoration materials to the tooth restoration
process that use these materials by specifying that the
procedure realizes the dental restoration material role
borne by the dental restoration material.
Finally, we relate a particular instance of a restor-
ation material to the anatomical part it restores using
the is dental restoration of relation. For example, an
amalgam filling procedure that restores the occlusal
surface of the right upper first secondary molar (i.e.,
ADA tooth number 3), is represented as an instance
of amalgam dental restoration material that is dental
restoration of the particular occlusal surface which is
part of the patients tooth 3. We use a relation here
because of an unusual temporal arrangement: the res-
toration material and the tooth surface dont neces-
sarily exist at the same time.
Dental findings
In the OHD, we make a clear distinction between infor-
mation that is part of the patients medical record, and
the oral condition or treatment referred to by the infor-
mation. To represent information, we define the class
dental finding as a specialization of OGMSs clinical
finding class:
A clinical finding is a representation that is either
the output of a clinical history taking or a physical
examination or an image finding, or some combin-
ation thereof. (OGMS)
A dental finding is a clinical finding that is a speci-
fied output of a dental exam and is about the oral
cavity, maxillofacial area, and/or the adjacent and
associated structures, or their parts, or pathological
anatomical entities derived from them.
We are then able to further specialize dental findings
as needed. For instance, a caries finding that a patient
has caries on some tooth is defined:
A caries finding is a dental finding that indicates a
carious lesion of a tooth.
Findings are related to their targets using IAOs is
about relation. For example, a particular caries finding
has as its target a particular tooth in a patients mouth.
This is represented as instance of a caries finding that is
about the patients particular tooth.
For representing findings about missing teeth, we were
faced with the issue that, following the principles of
ontological realism, one cannot have a finding about a
non-existent entity. For instance, if a patient is missing
his right upper first secondary molar (i.e., tooth 5), you
cannot have finding about this tooth because there is no
tooth that is the target of the finding. To address this,
we defined a missing tooth finding to be about a patients
mouth, and we made use of the strategy put forth in
Hoehndorf et al. (2010) to represent anatomical entities
that lack a particular part [26]. In brief, Hoehndorf et al.
represent such entities by negating the part of relation
that holds between an entity and one of its parts. A ma-
ture red blood cell, for example, does not have a nucle-
olus. Thus, a red blood cell is formally defined (i.e., =df)
as follows:
red blood cell =df blood cell and (not has part some
nucleus)
Extending this strategy to missing teeth findings, we
formally define a missing tooth finding as being about a
mouth (i.e., is about some mouth), and a missing tooth
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 8 of 19
finding for a particular tooth as being about a dentition
that lacks a particular tooth. For instance, a missing
tooth 5 finding is formally defined as follows:
missing tooth 5 finding =df missing tooth finding and
is about some (secondary dentition and (not has
part some tooth 5))
In natural language, this formal definition is rendered as:
A missing tooth 5 finding is a missing tooth finding
in which tooth 5 is found to be missing.
The missing tooth findings for the other teeth are then
defined using the same approach as a missing tooth 5 find-
ing, but with tooth 5 being replaced by the relevant tooth
(e.g., missing tooth 6 finding, missing tooth 7 finding, etc.).
Billing codes
Given the ubiquity of current dental terminology (CDT)
codes in electronic dental record systems, we need to
represent these codes in the OHD. Since CDT codes are
a kind of information, we defined them as a subclass of
IAOs centrally registered identifier (CRID). A CRID is a
symbol that is registered as part of some organization.
For example, a social security number is a CRID this is
registered with the Social Security Administration. Simi-
larly, CDT codes are registered and developed by the
American Dental Association [27], and thus, the current
dental terminology code class is defined as:
A current dental terminology code is a centrally regis-
tered identifier that is maintained by the American Den-
tal Association and used for recording dental services
provided. It is typically used on the patient record, and
when reporting procedures on a paper or electronic
submission.
To date, the OHD contains only a subset of total num-
ber of CDT codes, and most of the represented codes
lack definitions. This is because current dental termin-
ology code subclasses were created by using computer
programs to extract information about CDT codes from
the relevant ADA documentation [28].
Translating dental EHR data into OHD-based
representations
In parallel with developing the OHDs class hierarchy,
we developed methods to programmatically translate
dental EHR data into OWL statements. For this, we
wrote custom Common Lisp5 programs to extract and
translate dental EHR data (data source described below)
stored using Eaglesoft Dental Practice Management and
Imaging Software.6 This process consisted of three steps.
First, we extracted data from the dental EHR system
using SQL queries issued by our Common Lisp program.
In order to facilitate access to the EHR data, we com-
bined several of the Eaglesoft tables into a single table
named patient_history. This made queries simpler to
write and understand. That is, instead of having to do a
number of joins across multiple database tables, we were
now able to query just one table. For example, the fol-
lowing query extracts data about patients who had
undergone single surface amalgam restorations:
SELECT * FROM patient_history
WHERE ada_code LIKE 'D2140'
Second, once extracted, the dental EHR data was
translated into OWL statements about instances of the
entities involved using the Lisp Semantic Web Library
(LSW) [29], a Common Lisp library whose syntax is
similar to OWLs functional syntax [3]. For example, the
following OWL functional syntax statements declare
that a particular individual is an instance of a tooth:
Declaration
(NamedIndividual(obo:tooth_instance))
ClassAssertion
(obo:FMA_12516 obo:tooth_instance)
In LSW, these statements are written as follows:
(declaration (named-individual !obo:
tooth_instance))
(class-assertion !Tooth@ohd !obo:
tooth_instance)7
The close affinity of the LSW syntax to the OWL
functional syntax, as well as its ability to reference clas-
ses by name instead of IRI, allows us to write OWL
statements that we can easily understand and evaluate.
This is in stark contrast to OWL statements represented
as RDF/XML, which are not easily understood by
humans. The output of the Common Lisp program is a
number of OWL files that together contain an OWL
representation of the dental EHR data.
Third, the OWL files are loaded into a semantic data-
base, or triple store, that uses the OHD as the schema
for representing the data. This allows for data to be eas-
ily queried and analyzed (discussed in the Results sec-
tion). For this project, we used the GraphDB SE version
7.2 triple store,8 a semantic database with integrated rea-
soning that is built on Semantic Web standards. To ver-
ify that data was translated correctly, we ascertained that
5We use Armed Bear Common Lisp (https://abcl.org) for
implementation of Common Lisp.
6https://www.pattersondental.com/eaglesoft (accessed January 2018)
7LSW provides the means to retrieve the IRI for a term using the
syntactic form !<term label>@<ontology label>. In this
example,!Tooth@ohd retrieves the IRI for the term Tooth (http://
purl.obolibrary.org/obo/FMA_12516).
8https://ontotext.com/products/graphdb/
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 9 of 19
the number of entities in the triple store matched the
number in the dental EHR relational database. For ex-
ample, we queried the number of patients and dental
procedures in the triple store and compared those re-
sults to SQL queries used to extract the same informa-
tion from the dental EHR.
Results
Summary of the OHD
In its present state, the OHD contains 1947 classes, and
59 relationships. Tables 2 and 3 summarize the source
of these classes and relations.
In the above tables, it is important to point out the
relatively low number of new classes and relations cre-
ated specifically for the OHD. That is, 192 out of 1947
classes and 3 out of 59 relations were specifically devel-
oped for the OHD. This high amount of reuse demon-
strates the effectiveness of the OBO Foundry principle
of having distinct scientific communities develop rigor-
ous interoperable ontologies. Without OBO Foundry on-
tologies that follow these principles, we would have to
draw upon other terminologies, such as SNOMED CT,9
that dont provide the same sound basis, clarity, level of
documentation, or instance-orientation that character-
izes the OHD.
Translation of practice data
The primary data source for this analysis was the rela-
tional database from a dental EHR containing de-
identified dental records for 7337 patients from a single
dental practice spanning the years 19992011. Of these
patients, approximately 4500 had treatment records. The
practice used Eaglesoft (Patterson Dental, Effingham,
IL), a leading dental EHR system in the U.S. In total, the
database contained 232,270 records that pertained to pa-
tients dental health history.
After receiving IRB approval, we used the methods de-
scribed in Section 3.3 to translate the dental EHR data
into OWL. Where there were questions regarding what
certain data meant, we consulted the vendor of the sys-
tem and the practice clinician.
The following kinds of procedures were translated: fill-
ings, crowns, onlays, inlays, veneers, endodontic proce-
dures, surgical extractions, and oral evaluations. Table 4
summarizes the resulting number of translated
procedures:
The SPARQL query used to retrieve the number of
translated procedures is provided in Additional file 1.
The translated data were then loaded into a GraphDB
SE (version 7.2) triple store using GraphDBs OWL2-RL
automated reasoner. Result sets for analysis were ob-
tained by querying the triple store using SPARQL 1.1.
A clinical use case: longevity of restorations
As a proof of concept for using the OHD structured data
to analyze dental EHR data we performed an analysis of
the longevity of tooth restorations. For our study the
steps involved were
1. define what would be considered a restoration
failure;
Table 2 Summary of number of classes used in the OHD
Ontology Number of Classes
Foundational Model of Anatomy 1515
Oral Health and Disease Ontology 192
Current Dental Terminology codes 174
Ontology for General Medical Science 74
Basic Formal Ontology 32
Information Artifact Ontology 14
Ontology for Biomedical Investigations 13
Common Anatomy Reference Ontology 3
Ontology of Medically Related Social Entities 3
NCBI Taxon 1
Total 1947
Table 3 Summary of number of relations used in the OHD
Ontology Number of Relations
Basic Formal Ontology 38
Foundational Model of Anatomy 12
Ontology for Biomedical Investigations 5
Oral Health and Disease Ontology 3
Information Artifact Ontology 1
Total 59
Table 4 Summary of the number of procedures translated in
the OHD
Procedure Total
Fillings 22,252
Crowns 12,636
Onlays 1269
Inlays 365
Veneers 877
Endodontic procedures 1441
Tooth extractions 999
Oral evaluations 28,566
Total 68,405
9Systemized Nomenclature of Medicine  Clinical Terms, http://www.
snomed.org/ (accessed January 2018).
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 10 of 19
2. define a set of correlates for the analysis: We look
at patient, gender, age, tooth position and number
of surfaces restored;
3. design the queries to extract the relevant
information;
4. prepare the database for the sorts of queries we
would be making; and
5. using the R statistics program to run the statistics
and plot curves.
Defining restoration failure
There appears to not be a standard approach to studying
restoration longevity. Prior studies have used a variety of
criteria to define or categorize failure, and different cor-
relates. We describe next a representative set.
Bogacki et al. [30] defined failure as replacement of a
restoration on the same surface. They censored cases
where a larger restoration replaced the initial one (e.g., a
one-surface restoration replaced by a two-surface restor-
ation) or at the last known patient visit. Correlates in-
cluded restoration type, prior history, provider and
patient age, tooth location, year of treatment, and
whether the provider changed.
Gulambi et al. and Redman et al. [31, 32] categorized
failures into major and minor based on United States
Public Health Services USPHS score, and analyzed major
failures and total failures separately. Major failures were
defined as restorations that required complete replace-
ment. Restorations were censored if there was no other
event at last follow-up. Correlates included patient gen-
der, provider, etiology of tooth wear, material, opposing
dentition and incisal relationship.
In Janus et al. [33] failure was defined as the tooth be-
ing lost due to extraction, or the tooth requiring an add-
itional restoration, crown, or other treatment such as an
endodontic treatment. Only a single tooth from any pa-
tient was chosen to minimize dependence. Restorations
without further activity were censored as of last date the
patient was seen. Correlates were gender, race, age, site,
type of restoration, and whether the initial restoration
was supervised by a specialist. We defined failure similar
to Janus et al. A restoration was considered a failure if
there was a subsequent restoration on any of the sur-
faces that were initially restored. Cases were censored if
there was no failure the last time the patient was seen. If
there was no encounter after the restoration, we did in-
clude the restoration in our analysis.
Correlates
Correlates were chosen for relevance and based on their
availability in our data set. We considered the patients
age at restoration, gender, whether the tooth was anter-
ior or posterior, and how many surfaces were initially re-
stored. Age was broken into two groups: below and
above 40. When an initial analysis did not find signifi-
cance using the exact number of surfaces restored, we
chose to group the data into two categories  those
where a single surface was restored and those where
more than one surface was restored.
Etiology and condition at time of failure are likely to
be recorded in progress notes, but could not be easily
extracted from our source data. While provider informa-
tion was ostensibly recorded, we were told it was not
reliable.
Database preparation
After translation, we noted an issue. We had given den-
tal visit an occurrence date, but not the processes that
occurred during visit, such as the exams or restorations.
Ideally we would add an OWL property chain has_part
o occurrence_date - > occurrence_date, however OWL2
only has object property chains. Instead we used a
SPARQL Insert command to update this as shown in
Additional file 1.
To make queries both efficient and clear we created a
relation that linked, in chronological order, each patient
encounter. For each patient, next_encounter relates each
encounter to the following encounter with the patient.
Note that since encounters may be part of other encoun-
ters, and that our time granularity is a day, there may be
several next encounters after a single one. To create the
chain of encounters, we used SPARQLs ability to test
whether a variable is bound in a solution, using the
strategy shown in Fig. 2. We did a query for triples of
events in order, but we filtered any solutions in which
the middle encounter was bound.
subsequent_encounters is asserted as a transitive super-
property of next_encounter, making relatively easy to
construct queries looking for ordered pairs of type of
processes.
Queries
The general schema for the SPARQL queries for pairs of
events for survival analysis was:
?first_event rdf:type <type of restoration>:.
<bind tooth and surface>
?first_event subsequent_encounter: ?later_event.
<constrain second event>
?later_event occurrence_date ?date.
<bind survival analysis correlates>
Filter for minimal ?date of ?later_event
Table 5 lists the kinds of failures and conditions that
were used as second events. A query retrieving
restorations whose second event is one of the four
conditions is captured in the query in Additional file 1.
All queries returned information solely about
secondary teeth. Out first query retrieved the date of the
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 11 of 19
restoration as well as the date of the last recorded
encounter with the patient, the latter used as right-
censored events in the survival analysis when there was
no recorded failure. In that case the constraint on the
later event is that there is no subsequent event, i.e. there
is no subsequent_encounter relation. When a restoration
was placed during the last patient visit on record, it was
not included in the analysis.
Table 5 Failure event type and constraint
Type of second event Second event constraint
Restoration or inlay Same tooth and surface
Endodontic procedure Same tooth
Crown replacement Same tooth
Extraction Same tooth
Last recorded encounter No later encounter
Fig. 2 Illustration showing instances used in representation of a two-surface resin restoration. Each box is an OWL individual. Arrows indicate the
relations among the individuals, and box shape indicates the upper level BFO universal which it instantiates. In some cases, a proximate
superclass is listed after a dash. In other cases, the label until the instance number or of names the proximate class. Where of is used it indicates
a functional relation. Underlined dates are data values
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 12 of 19
The second query looked for pairs of restorations and
failures. The target restoration type here was resin filling
restoration, and we considered as failure either a
restoration or inlay on any surface of the first
restoration, or replacement by a crown, or an extraction
of the tooth, or an endodontic procedure on the tooth.
It should be noted that we did not need to use an
explicit list of dental codes for our query, as is
commonly the case. Instead the dental procedure
hierarchy provides us the ability to query all instances
of, e.g., endodontic procedures and its subclasses.
Should they be necessary, queries can still be made in
terms of CDT codes since axioms relate CDT codes to
what they are about.
Statistical analysis
Our dataset included 13,922 resin restorations, of which
12,704 had follow up. Table 6 provides a breakdown of
the events by correlate, giving, for each group the
number of patients, the number of events (failures +
censored) and the number of failures.
For analysis we used the R packages survival [34],
simPH [35], and ggsurv from GGally [36] for the
Kaplan-Meier plots. The muhaz [37] package was used
to compute the hazard function. With a cutoff of .05 for
p-value all correlates, all were statistically significant.
Table 7 shows p value and relative hazard for significant
correlates.
Figure 3 shows Kaplan-Meier curves for overall
longevity and comparison curves for each correlate.
Grayed area are 95% confidence intervals. Median
survival, overall, is somewhere in the range of 12
years, falling to 10.5 years median survival for those
older than 40. Our results on longevity most
resemble Bogackiet et al. [30]. They dont seem to
concord with Redman et al. who found median
survival to be substantially less at under 5 years [31].
The hazard rate was plotted once it was determined
that there was a surfeit of early failures. The plot clearly
shows this with elevated hazard in the first 2 years. Code
to perform the analysis is available at https://github.
com/oral-health-and-disease-ontologies/ohd-ontology/
blob/master/src/analysis/survival.r.
Discussion
We have outlined the principles used to create the OHD
classes and relations, and discussed how data translated
into OWL can be queried and analyzed. While there
have been recent efforts to use instance data in a clinical
setting [38], this work represents the first example of
end-to-end use of BFO and OBO-ontology structured
instance data for clinical research in healthcare. By using
practice data and demonstrating how to answer a clinical
research question, we have demonstrated that the ap-
proach is practical with current technology.
Along the way to implementation we were faced with
a number of situations where existing ontologies alone
were inadequate or where choices of how to use them
were not obvious. We chose to represent CDT codes as
information artifacts beside, so to speak, representations
of the processes those codes refer to, to make room for,
but not tie us to, a single coding system.
The treatment of time in current OBO OWL
ontologies is inadequate [39]. In different cases we
variously ignored it, addressed it with an alternate
ontology pattern, or used a workaround. We use the is
part of relation without a time index despite BFOs part
relation being time-dependent. A common error in
representing processes is to define them in terms of par-
ticipants having certain roles. The has role relation
should also be time-indexed and, importantly, entities
with roles can and do participate in processes where
those roles are not relevant. We use the role-realization
pattern to clearly relate roles, when they are relevant, to
a process and this has an added benefit of having correct
temporal scope without specifying a time index. Our use
of occurrence date is a workaround for the lack of a way
of specifying concrete times and also makes a choice
about temporal granularity, something else not clearly
addressed in current OBO.
We needed to work with natural, restored and prosthetic
teeth. Anatomy ontologies are typically canonical, and so do
not address these cases. There have been various efforts to
address this, for example in understanding how non-
canonical phenotypes relate to anatomy [40]. Here, we intro-
duced a representation of prosthetics and introduced the
Table 6 Breakdown by correlate of events for survival analysis
Correlate No. Patients No. Events No. Failures
Gender female 1441 7114 1358
Gender male 1117 5590 1251
Anterior tooth 1195 3692 989
Posterior Tooth 2234 9012 1620
Age < 40 1082 5159 824
Age over 40 1545 7545 1785
Single surface 2016 6216 1346
Multiple surface 2021 6488 1263
Table 7 Significant correlates significance and effect size
Correlate p Hazard difference
posterior vs anterior <.001 23% decrease
Male vs female <.001 16% increase
Age 40+ vs < 40 <.001 40% increase
multiple vs single surface 0.0019 12% decrease
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 13 of 19
term functional tooth, to capture the necessary distinctions.
Some of the terms are not specific to our domain and we
have proposed they be added to OGMS.
It was not clear at the start how to have performant
queries that retrieved pairs of related encounters such as
one in which a restoration is done and a later one
requiring a root canal because the first failed. Creating
the next_encounter and subsequent_encounter relations
and using them for such queries proved to be effective,
both computationally and cognitively.
Ontologies sometimes did not have terms in their
domain that we needed. While the FMA is incredibly
detailed, we discovered it did not give us a
representation of the substantial (as opposed to two-
dimensional) surface of teeth. We created the necessary
terms and worked with the developers of the FMA to
have them added, a success for the collaborative ap-
proach espoused by the OBO Foundry.
We see five benefits of the approach we used. The
first, which can be easily overlooked, is that building the
OHD aided in understanding the domain and data by
forcing us to use unambiguous terms. As an example,
consider the term restoration. Depending on the
context, a practitioner may mean the process of
restoring a tooth or the material used to restore a tooth.
These two uses of restoration are interrelated: The
process of restoring a tooth requires the use of
restoration material. However, the process and the
material are distinct types of entities.
The second benefit is that we are not reliant upon a
particular coding system, but can still use one or more
effectively. For instance, in the United States, teeth are
denoted according to the Universal Tooth Numbering
System [41], which uses the numbers 132 for
permanent teeth and the letters A-T for primary teeth.
But, other countries use the World Health Organization
(WHO) notation system, which denotes teeth using
combination of numbers to represent the quadrant of
the mouth and the tooths position in the quadrant. The
OHD, in contrast, represents teeth using anatomical
classes from the FMA. For example, the OHD represents
a patients upper right wisdom tooth using the FMA
class Right upper third secondary molar tooth, which is
also denoted as 1 in the Universal Tooth Numbering
System and 18 in the WHO system. Thus, different
tooth coding systems can be mapped to the same FMA
class. Similarly, whereas dental representations often use
CDT codes, which also are used for billing, the OHD
represents types of procedures and codes separately.
When multiple codes are meant to refer to the same
thing, their meeting point can be, for example, a single
process type.
The third benefit of our approach is that using the
OHD allows for queries that leverage the logical
structure of the ontology. Two examples of this are
hierarchical queries and relational queries.
By design, an ontology behaves such that if you query
for instances of a class, instances which are only asserted
to be of a subclass (or child class) are queried for as
well. That is because they are inferred to also be the
parent type. For example, consider a query to retrieve all
crown procedures. Using the OHD, this query is
straightforward10:
select (count(distinct ?procedure) as ?total_crowns)
where { ?procedure rdf:type crown_procedure: }
However, to do this in a relational database using
ADA billing codes alone, you have to account for at
least 40 billing codes.11
In addition to hierarchical queries, the OHD permits
queries that make effective use of the relations between
entities. For example, in the OHD, we specify that a
tooth restoration procedure must include restoration
material during the process. This permits us to query for
all procedures that use a specific restoration material
instead of having to recall which materials are denoted
Fig. 3 Construction of the next encounter relation
10The prefix definitions have been excluded. The full query is given in
Additional file 1.
11The exact number of billing codes needed depends on the version of
ADA codes used.
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 14 of 19
by ADA codes. The query below leverages the OHD
specification to find the number of restoration
procedures using resin broken down by the type of
procedure.12 The result of this query is summarized in
Table 8.
select ?procedure_name (count(?procedure_type) as
?total) where {
?material_instance rdf:type resin: .
?procedure_type rdfs:subClassOf dental_procedure: .
?procedure asserted_type: ?procedure_type .
?procedure has_participant: ?material_instance .
?procedure_type rdfs:label ?procedure_name .
} group by ?procedure_name
order by desc(?total)
The fourth benefit is easy generalization to wider domains
of medicine and clinical research, and is conferred by our
extensive use of instances and types. Consider the
representation in Fig. 4. This representation contrasts with
typical representations from relational databases in that it
more explicitly tracks what happens and include type
information making it easier to discover and understand
contents. For example, the situation represented in Fig. 4
might be represented in a single row in a relational database.
Primary
key
Provider
id
Patient
id
Tooth
no.
Surfaces CDT
Code
XXXXX 21 17 19 MO D2392
The relational representation provides economy of
space and may be suitable for a self-contained applica-
tion. However, this representation makes it difficult
when it comes to integration in either a data warehouse
or a system which records information that goes beyond
dentistry. Information needed to interpret it, if even
available, typically lies outside the representation. Profes-
sionals charged with integrating or querying such data
are typically not domain experts. For example, a person
who needs to understand the representation needs to
understand that each letter in the surface column repre-
sents one surface and find a reference for the lettering
system. In addition, the representation provides little
guidance about how one might go about representing
the information in a way compatible with more general
medical information about a patient. For example, the
provider id implicitly identifies a dentist. In a more gen-
eral setting, there are a wide variety of types of providers
and this information must be made explicit.
By comparison, the representation in Fig. 4 might appear
at first glance to be overkill. However, it has two important
qualities. First, it is easier to generalize to general medicine
 the same schema can easily accommodate procedures of
any sort, by providers of any sorts, on parts of the anatomy
of any sort. Second, because it creates instances for the
entities it removes ambiguities that might hinder careful
clinical analyses, a problem identified and addressed in the
strategy of referent tracking [42]. With this representation
one can easily track the particular material used in a filling
even when the filling is redone after failure or refer to the
specific usage of this CDT code in context, for example in
an audit.
The final benefit of our approach is that it allows us to
use automated reasoning to infer information. That is,
automated reasoners can use OHDs axiomatic definitions
to infer relations between entities that were not explicitly
stated as being related. Two examples of this are the
OHDs use of transitive relations and the functional tooth
class. A transitive relation is a relation where if two
relationships have a common element that is object of the
first relation and subject of the other, then the respective
subject and object of those relations are also related. For
instance, in mathematics the < (less than) operator is
transitive. Given that 5 < 6 and 6 < 10, you are licensed to
infer that 5 < 10. In the OHD, we define the is part of as
transitive, and use it relate a tooths surface to its tooth
and a tooth to a patient. Using these relations between
surface and tooth, and between tooth and patient, we can
now easily query for patients tooth surfaces as follows13:
select ?surface ?patient where {
?patient rdf:type patient: .
?surface rdf:type tooth_surface:; is_part_of:
?patient . }
In this query, the parthood relation between the tooth
surface and patient is inferred, and not explicitly stated
in the translated data.
In the OHD, a functional tooth is defined to be either a
natural tooth, a restored tooth, or a prosthetic tooth. This
Table 8 Summary of the number of restoration procedures
using resin
Procedure Name Total
resin filling restoration 13,
860
resin laminate veneer restoration 135
resin inlay restoration 13
resin onlay restoration 11
resin with predominantly base metal crown restoration
procedure
2
resin crown restoration procedure 1
stainless steel with resin window crown restoration procedure 1
12The full query is provided in Additional file 1. 13The full query and results are provided in Additional file 1.
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 15 of 19
Fig. 4 a-e: Survival plots f: Hazard rate
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 16 of 19
allows us to easily query for all restoration procedures
performed on a tooth regardless of whether the tooth is
natural or an implant. However, when translating the data,
we do not explicitly state that a tooth or implant is a
functional tooth. Rather, we define the axioms that specify
the necessary and sufficient conditions for being a
functional tooth and let the automated reasoner do the
work of classifying the instances. Our reason for classifying
an instance of functional tooth in this manner is that as the
number of classes to which an instance can belong grows,
the complexity of the ontology increases, which likewise
increases the chance for misclassifying an instance. By
offloading this task to the automated reasoner, we reduce
the chance of introducing human error into the process of
classifying instances of functional tooth.
Conclusions & future work
Our goal of developing the OHD was to leverage semantic
web standards to provide a common representation of the
dental health care domain that exercises good ontology
practices and can be generalized to general medicine. In
doing this, we reused classes from OBO Foundry
ontologies when possible. The advantage benefit from this
was twofold. First, we were able to effectively leverage work
done by others within the OBO community. Second, we
were then able to improve on the OBO ontologies when
we found cases that were not yet represented, for instance
the surface enamel of tooth class that was added to the
FMA as a result of our work. This has the added benefit of
enriching the OBO Foundry community in general.
We only translated data from one practice, and while
our purpose was not to design a data integration
pipeline, building such a pipeline was necessary in order
to have data for analysis. Our translation method can be
applied to other practices with different dental EHR
systems, and efforts are currently under way to integrate
data from 99 geographically dispersed dental practices
using the OHD. One notable consideration to our
approach is that in order to translate dental EHR
records, one must learn the intricacies of the dental
EHR database schema. We had the advantage of having
an open line of communication with Eaglesoft, the
practices dental EHR vendor, but researchers wishing to
adopt the OHD might not have this advantage. Thus,
further work needs to be done on how to communicate
the necessary steps needed to query a vendors database
to those who do not have contact with the vendor and
to advocate that new systems use an ontology-based
representation.
Our use cases for this project primarily focused the
representation of tooth restorations (e.g., fillings and
crowns), and dental procedures or conditions that would
indicate a tooth restoration had failed. We recognize
that there are number of other procedures and
conditions that need to be represented. The
representation of these other entities is part of our
ongoing work, as is the application of the methodology
in the wider medical arena. The OHD and examples of
translation code are available at https://github.com/oral-
health-and-disease-ontologies/ohd-ontology.
Finally, the OHD only contains a subset of CDT
codes. A more complete and formal representation of
them and other coding systems relevant to oral health,
such as SNO-DDS [43], remains future work.
Supplementary information
Supplementary information accompanies this paper at https://doi.org/10.
1186/s13326-020-00222-0.
Additional file 1: Appendix A. Research questions used in developing
the OHD. Appendix B. Relate all questions to patient gender and age.
SPARQL count by procedure query. Appendix C. SPARQL Construct
propagating occurrence date. Appendix D. SPARQL Query for patients
age at first dental encounter. Appendix E. Query to retrieve the total
number of crown restoration procedures. Appendix F. Query to retrieve
restoration procedures that use resin. Appendix G. Query to retrieve
tooth surfaces and patients. Appendix H. Query to retrieve resin
restorations and failures.
Abbreviations
BFO: Basic Formal Ontology; CARO: Common Anatomy Reference Ontology;
CDT: Current Dental Terminology; EHR: Electronic Health Record;
FMA: Foundational Model of Anatomy; IAO: Information Artifact Ontology;
LSW2: LISP Semantic Web version 2; OBI: Ontology for Biomedical
Investigations; OBO Foundry: Open Biological and Biomedical Ontology
Foundry; OHD: Oral Health and Disease Ontology; OMRSE: Ontology of
Medically Related Social Entities; OWL: Web Ontology Language;
RDF: Resource Description Framework; RDF/XML: RDF statements
represented using XML syntax; SPARQL: SPARQL Protocol and RDF Query
Language; SQL: Structured Query Language; XML: Extensible Markup
Language
Acknowledgements
We wish to thank Mantilla Rodriguez, Andres Alfredo and Werner Ceusters
for their assistance in editing this manuscript, Ted Fruchtl and Jim Garrett of
Eaglesoft for their help in understanding the database structure, and the
dental health provider who donated the data for this project.
Availability of data and material
The OHD, SPARQL queries, LSW2 source code for generating the OWL
files, and R code executing the statistical analysis are available on
GitHub: https://github.com/oral-health-and-disease-ontologies/ohd-
ontology. The OHD is located in the src/ontology directory. The LSW2
code is in the r21-study directory. The SPARQL queries and R code is in
the analysis directory.
The data used to generate the OWL files is not available for download.
Making such data publicly available would violate the study protocol. We
are, however, considering construction of a synthetic data set to enable
experimentation.
Authors contributions
WDD composed, edited, and revised the manuscript, and served as the lead
developer for the OHD. AR edited the manuscript, performed the statistical
analysis for, and wrote the section describing, the restoration longevity use
case. TPT, PH, MH, CT, MS, AA, and DC were instrumental in the
development of the OHD and understanding dental health data. TS and AR
directed the work. The author(s) read and approved the final manuscript.
Duncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 17 of 19
Funding
We gratefully acknowledge support from National Institute of Dental and
Craniofacial Research (NIDCR) grants R21-DE-19683, R21-DE-21178, and R03-
DE-023358; the Lilly Endowment, Inc. Physician Scientist Initiative; the Clem
MacDonald chair account; and start-up funds provided by the School of
Dental Medicine, University at Buffalo.
Ethics approval and consent to participate
Research for this study was conducted in accordance with Indiana University
IRB Protocol 1506009585 and University of Pittsburg IRB Protocol
PRO11040275. All data used was completely deidentified before delivery to
investigators.
Consent for publication
Not applicable
Competing interests
The authors declare that they have no competing interests.
Author details
1National Center for Ontological Research, Buffalo, NY, USA. 2Center for
Biomedical Informatics, Regenstrief institute, Inc., Indianapolis, IN, USA.
3Dental Informatics Core, Indiana University School of Dentistry, Indianapolis,
IN, USA. 4Translational and Integrative Sciences Lab, Oregon State University,
Corvallis, OR, USA. 5Domino Data Lab, San Francisco, CA, USA. 6Reparto
Universitario, San Juan, PR, USA. 7Magee-Womens Research Institute,
Pittsburgh, PA, USA. 8Marshfield Clinic Research Institute, Marshfield, WI, USA.
9University of Iowa College of Dentistry, Iowa City, IA, USA. 10Indiana
University School of Medicine, Indianapolis, IN, USA. 11School of Dental
Medicine, State University of New York at Buffalo, Buffalo, NY, USA.
Received: 23 September 2018 Accepted: 9 June 2020
RESEARCH Open Access
An integrative knowledge graph for rare
diseases, derived from the Genetic and
Rare Diseases Information Center (GARD)
Qian Zhu1* , Dac-Trung Nguyen1, Ivan Grishagin1, Noel Southall1, Eric Sid2 and Anne Pariser2
Abstract
Background: The Genetic and Rare Diseases (GARD) Information Center was established by the National Institutes
of Health (NIH) to provide freely accessible consumer health information on over 6500 genetic and rare diseases. As
the cumulative scientific understanding and underlying evidence for these diseases have expanded over time,
existing practices to generate knowledge from these publications and resources have not been able to keep pace.
Through determining the applicability of computational approaches to enhance or replace manual curation tasks,
we aim to both improve the sustainability and relevance of consumer health information, but also to develop a
foundational database, from which translational science researchers may start to unravel disease characteristics that
are vital to the research process.
Results: We developed a meta-ontology based integrative knowledge graph for rare diseases in Neo4j. This
integrative knowledge graph includes a total of 3,819,623 nodes and 84,223,681 relations from 34 different
biomedical data resources, including curated drug and rare disease associations. Semi-automatic mappings were
generated for 2154 unique FDA orphan designations to 776 unique GARD diseases, and 3322 unique FDA
designated drugs to UNII, as well as 180,363 associations between drug and indication from Inxight Drugs, which
were integrated into the knowledge graph. We conducted four case studies to demonstrate the capabilities of this
integrative knowledge graph in accelerating the curation of scientific understanding on rare diseases through the
generation of disease mappings/profiles and pathogenesis associations.
Conclusions: By integrating well-established database resources, we developed an integrative knowledge graph
containing a large volume of biomedical and research data. Demonstration of several immediate use cases and
limitations of this process reveal both the potential feasibility and barriers of utilizing graph-based resources and
approaches to support their use by providers of consumer health information, such as GARD, that may struggle
with the needs of maintaining knowledge reliant on an evolving and growing evidence-base. Finally, the successful
integration of these datasets into a freely accessible knowledge graph highlights an opportunity to take a
translational science view on the field of rare diseases by enabling researchers to identify disease characteristics,
which may play a role in the translation of discover across different research domains.
Keywords: GARD, Rare diseases, Ontology, Data integration, Knowledge graph
© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if
changes were made. The images or other third party material in this article are included in the article's Creative Commons
licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons
licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain
permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the
data made available in this article, unless otherwise stated in a credit line to the data.
* Correspondence: qian.zhu@nih.gov
Qian Zhu and Dac-Trung Nguyen contributed equally to this work.
1Division of Pre-Clinical Innovation, National Center for Advancing
Translational Sciences (NCATS), National Institutes of Health (NIH), Rockville,
MD 20850, USA
Full list of author information is available at the end of the article
Zhu et al. Journal of Biomedical Semantics           (2020) 11:13 
https://doi.org/10.1186/s13326-020-00232-y
Introduction
An estimated 30 million people in the United States are
affected by a rare disease, which is defined as a disease
that affects fewer than 200,000 individuals in the United
States [1]. The majority of rare disease are thought to
have a genetic etiology [2] with studies reporting them
responsible for almost 10% of adult and 30% of pediatric
hospitalizations [3]. Despite the great heterogeneity of
diseases included in this definition, many patients and
their families share in common struggles, such as with
diagnostic delay leading to an average of 7.6 years from
initial onset of symptoms to receiving a diagnosis and
requiring the involvement of 7.3 physicians on average
[4]. These shared challenges faced in the broader rare
disease patient community are often due to a lack of
either up-to-date information or awareness amongst
providers and the public at large. Efforts to tackle these
issues led to the passage of the Rare Disease Act of 2002
and the establishment of several programs by the
National Institutes of Health (NIH) to improve research
activities and public access to information on rare dis-
eases. In particular, the Genetic and Rare Diseases
(GARD) information center was charged with providing
freely accessible consumer health information in plain
language, and it has been investigating the challenge of
shifting from an entirely manual process to leveraging
computational approaches to curate the accumulated
biomedical and clinical research knowledge of over 6500
rare diseases, and more rapidly make information ac-
cessible 1) to educate patients, families, and health care
providers with more accurate and real-time knowledge
about a rare disease, and 2) to support novel scientific
research efforts and apply disease-agnostic translational
science approaches to the field of rare diseases as a
whole [5].
Given the pace of ongoing scientific discovery, parsing
through the accumulated research publications and
conveying this knowledge in a plain language format ac-
cessible to low-health literacy audiences presents a sig-
nificant task for a single disease, let alone for over 6500
rare diseases. Thus, a huge amount of effort to accumu-
late and curate data for rare diseases has been made glo-
bally. For instance, the GARD Information Center
provides interpretable profiles in plain language for each
rare disease [5]; Orphanet focuses on expert manual cur-
ation of a diseases clinical presentation [6]; and Online
Mendelian Inheritance in Man® (OMIM®) conducts a
similar expert-driven focus on defining genotype and
phenotype relationships [7]. The discreteness of such
heterogeneous data, however, impedes their direct use
for consumer audiences. To overcome this barrier, in
this study, we integrated these well-known resources in
one knowledge graph to semantically interconnect all
data together by means of the data points as nodes and
their relationships as edges, as a first step in bridging the
use of these resources in consumer-facing health
information.
Biomedical data integration is an important and tech-
nical approach to tackling biomedical problems. Current
progress in computational technology allows vast data
storages and powerful computational processes to be
more affordable and accessible. As a result, biomedical
scientists have gradually gained an awareness of the im-
portance of pooling diverse types of data pertaining to a
specific medical entity to enhance their research under-
standing [8]. Representing integrative data in the form of
a graph has attracted many interests, particularly in the
biomedical domain. Karczewski K, et al. have reviewed
and discussed the potential and usage and challenges of
integrating diverse types of omics data for human health
and disease [9]. Biomedical Informatics Research Net-
work (BIRN) is an integrative resource by semantically
integrating data produced by multiple institutions for
data analysis on Neurosciences [10]. Similar efforts have
also begun to emerge with applications directed at the
field in rare disease, such as the semantic Diseasecard,
which integrates rare disease data from distinct sources
in a semantic web environment [11]. A similar EU plat-
form, RD-Connect connects databases, registries, bio-
banks and clinical bioinformatics to support research in
discovering new genes, biomarkers, and therapeutic
targets more quickly and efficiently [12]. The Monarch
Initiative as another analytic platform, semantically inte-
grates genotype and phenotype data across differing spe-
cies and sources [13], and has led to the establishment
of MONDO (Monarch Merged Disease Ontology) [14]
as a cohesive ontology for connecting many of the dis-
ease databases and resources. The integrative knowledge
graph we introduce in this study applies well-established
rare disease data drawn from GARD, Orphanet, OMIM
and MONDO as a backbone, and then expands to a
wide spectrum of additional biomedical data, including
phenotypes, genes and curated FDA approved drugs and
FDA orphan drug designations.
There are demonstrated merits and successes in using
graph database to support the management of large bio-
medical datasets. While relational databases excel at man-
aging relationships between data, graph databases provide
unique abilities to manage n-th degree relationships
among complex types of biomedical data. Furthermore,
graph databases are particularly apt at representing hier-
archical data, such as disease categories and complex se-
mantic relationships among different types of data. Neo4j
as a graph database management system [15], has been
widely applied in such use cases within the biomedical do-
main. Such as, Gratzl S, et al. demonstrated the utility of
Neo4j in developing integrated visual analysis platform for
biomedical data [16]; Himmelstein D, et al. constructed
Zhu et al. Journal of Biomedical Semantics           (2020) 11:13 Page 2 of 13
Hetionet, an integrative Neo4j network that encodes
knowledge from millions of biomedical studies to
prioritize drugs for repurposing [17]. In this paper, we
introduce this rare diseases integrative knowledge graph,
built in Neo4j as a backend graph database ingesting a
large variety of biomedical datasets. We detail data prepar-
ation and entity resolution methodologies in generating
initial insights and results, and the potential benefits for
utilizing a knowledge graph-based approach to interpret
biomedical research at a scale and pace that would be un-
sustainable when limited to the manual curation efforts
that define current processes used in curating consumer
health information.
Materials
At the time of writing, the knowledge graph integrates
34 different biomedical datasets including GARD. We
briefly describe several primary resources as below.
Rare disease related data resources
Besides GARD data retrieved from our internal database,
all other datasets were downloaded from NCBO Biopor-
tal [18].
GARD is currently managed by the Office of Rare
Diseases Research (ORDR) within the National Center
for Advancing Translational Sciences (NCATS), has
remained an important portal for patients, health-care
professionals, and researchers seeking to understand the
current state of genetic and rare diseases. GARD in-
cludes curated disease information comprised of 15 dif-
ferent sections, such as summary, diagnosis, inheritance,
etc. Notably, not all of GARD diseases have a complete
list of these 15 information sections, due to data unavail-
ability at the time of curation and update. In this study,
we extracted and applied disease specific information
sections, if applicable from our internal GARD database.
Other sections, such as Resources, Organizations will be
explored in the future [5].
Orphanet is a unique resource, gathering and improv-
ing knowledge on rare diseases so as to improve the
diagnosis, care and treatment of patients with rare dis-
eases [6].
Monarch Disease Ontology (MONDO) is a semi-
automatically constructed ontology that merges multiple
disease resources to yield a coherent merged ontology
[14].
Online Mendelian Inheritance in Man® (OMIM®) is
a comprehensive, authoritative compendium of human
genes and genetic phenotypes. The full-text, referenced
overviews in OMIM contain information on all known
mendelian disorders and over 15,000 genes [7].
Human Phenotype Ontology (HPO) provides a stan-
dardized vocabulary of phenotypic abnormalities en-
countered in human disease [19].
FDA orphan drugs
FDA orphan drug designation provides orphan desig-
nations to drugs and biologics, which are defined as
those intended for the safe and effective treatment, diag-
nosis or prevention of rare diseases/disorders [20]. In
this study, we employed orphan drug designation data
from the FDA [21], several examples of FDA orphan
drug designations retrieved from the FDA are shown in
Table 1. Specifically we utilized the associations between
FDA designated drugs (the column of Generic Name
in Table 1) and their designations (the column of Or-
phan Designation in Table 1). Although the data is pre-
sented in a structured form, orphan designation is
captured in free text, such as examples shown in Table
1. In that manner, additional curation was conducted in
this study to be able to map orphan designations to
GARD diseases and designated drugs to UNII (Unique
Ingredient Identifier).
Inxight Drugs is a drug resource developed by NCAT
S. Inxight Drugs [22] incorporates the most comprehen-
sive subset of substances and related biological mecha-
nisms pertaining to translational research and connects
them to the appropriate disease indications. As part of
Inxight Drugs, explicit connections between drugs and
conditions were manually identified from scientific arti-
cles, press releases, FDA labels, and large-scale databases
(e.g. AdisInsight [23]). For those identified associations,
the curators manually matched conditions to MeSH,
Disease Ontology (DO), and drugs to UNII (Unique In-
gredient Identifier). For example, one association pre-
senting in Inxight Drugs is as CYROMAZINE (with
UNII: CA49Y29RA9) has indication of MYIASIS,
CUTANEOUS MYIASIS OF SHEEP. In this study, we
extracted associations between FDA approved drugs and
diseases, and integrated them into our integrative know-
ledge graph.
Methods
In this paper, we detail the process of developing the in-
tegrative knowledge graph for rare diseases with inclu-
sion of multiple well-known biomedical datasets
including GARD. We also demonstrate the use of this
integrative graph to support biomedical research for rare
diseases. More details about this process is described as
below.
Data collection
GARD data is curated in two folds, manual curation by
information specialists from GARD, and programmatic
extraction from Orphanet. The curated data is stored in
a relational database, from where we extracted GARD
data for this study. GARD provides comprehensive in-
formation about rare diseases from different aspects,
including summary, sign and symptoms, treatment,
Zhu et al. Journal of Biomedical Semantics           (2020) 11:13 Page 3 of 13
RESEARCH Open Access
Temporal information extraction from
mental health records to identify duration
of untreated psychosis
Natalia Viani1*, Joyce Kam1, Lucia Yin1, André Bittar1, Rina Dutta1,2, Rashmi Patel1,2, Robert Stewart1,2 and
Sumithra Velupillai1
Abstract
Background: Duration of untreated psychosis (DUP) is an important clinical construct in the field of mental health,
as longer DUP can be associated with worse intervention outcomes. DUP estimation requires knowledge about
when psychosis symptoms first started (symptom onset), and when psychosis treatment was initiated. Electronic
health records (EHRs) represent a useful resource for retrospective clinical studies on DUP, but the core information
underlying this construct is most likely to lie in free text, meaning it is not readily available for clinical research.
Natural Language Processing (NLP) is a means to addressing this problem by automatically extracting relevant
information in a structured form. As a first step, it is important to identify appropriate documents, i.e., those that are
likely to include the information of interest. Next, temporal information extraction methods are needed to identify
RESEARCH Open Access
Ontological representation, classification
and data-driven computing of phenotypes
Alexandr Uciteli1,2* , Christoph Beger1,3, Toralf Kirsten2,4,5, Frank A. Meineke1,2 and Heinrich Herre1,2*
Abstract
Background: The successful determination and analysis of phenotypes plays a key role in the diagnostic process,
the evaluation of risk factors and the recruitment of participants for clinical and epidemiological studies. The
development of computable phenotype algorithms to solve these tasks is a challenging problem, caused by various
reasons. Firstly, the term phenotype has no generally agreed definition and its meaning depends on context.
Secondly, the phenotypes are most commonly specified as non-computable descriptive documents. Recent
attempts have shown that ontologies are a suitable way to handle phenotypes and that they can support clinical
research and decision making.
The SMITH Consortium is dedicated to rapidly establish an integrative medical informatics framework to provide
physicians with the best available data and knowledge and enable innovative use of healthcare data for research
and treatment optimisation. In the context of a methodological use case phenotype pipeline (PheP), a technology
to automatically generate phenotype classifications and annotations based on electronic health records (EHR) is
developed. A large series of phenotype algorithms will be implemented. This implies that for each algorithm a
classification scheme and its input variables have to be defined. Furthermore, a phenotype engine is required to
evaluate and execute developed algorithms.
Results: In this article, we present a Core Ontology of Phenotypes (COP) and the software Phenotype Manager
(PhenoMan), which implements a novel ontology-based method to model, classify and compute phenotypes from
already available data. Our solution includes an enhanced iterative reasoning process combining classification tasks
with mathematical calculations at runtime. The ontology as well as the reasoning method were successfully
evaluated with selected phenotypes including SOFA score, socio-economic status, body surface area and WHO BMI
classification based on available medical data.
Conclusions: We developed a novel ontology-based method to model phenotypes of living beings with the aim
of automated phenotype reasoning based on available data. This new approach can be used in clinical context,
e.g., for supporting the diagnostic process, evaluating risk factors, and recruiting appropriate participants for clinical
and epidemiological studies.
Keywords: Phenotype definition, Phenotype classification, Phenotype calculation, Phenotype ontology, Phenotype
reasoning
© The Author(s). 2020, corrected publication 2020. Open Access This article is licensed under a Creative Commons Attribution
4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as
long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,
and indicate if changes were made. The images or other third party material in this article are included in the article's Creative
Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative
Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need
to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/
licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.
0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.
* Correspondence: auciteli@imise.uni-leipzig.de; heinrich.herre@imise.uni-
leipzig.de
1Institute for Medical Informatics, Statistics and Epidemiology (IMISE),
University of Leipzig, Leipzig, Germany
Full list of author information is available at the end of the article
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 
https://doi.org/10.1186/s13326-020-00230-0
Background
Despite its long ago introduction in 1909 by Wilhelm
Johannsen, the term phenotype still has no generally
agreed definition [1]. Usually, a phenotype is consid-
ered as an observable characteristic or trait of an or-
ganism, such as its morphology, function, behaviour
or its biochemical and physiological properties [13].
Scheuermann et al. define a phenotype as a (combin-
ation of) bodily feature(s) (physical components, bod-
ily qualities or bodily processes) of an organism
determined by the interaction of its genetic make-up
and environment [4]. From the medical perspective,
clinical (clinically abnormal) and disease phenotypes
(clinical phenotype characterising a single disease) are
considered. According to Scheuermann et al., a dis-
ease phenotype can exist without being observed. Ob-
served bodily features that could be of clinical
relevance are called Sign(  observed in a physical
examination and is deemed by the clinician to be of
clinical significance) or Symptom (  observed by
the patient and is hypothesized by the patient to be a
realization of a disease) [4].
Correct determination of phenotypes plays a key
role for diagnosis of diseases, evaluation of risk fac-
tors and recruitment of patients for clinical and epi-
demiological studies [5, 6]. One challenge is to
translate phenotype algorithms, which are most com-
monly represented as non-computable descriptive
documents and knowledge artifacts [7], into
machine-readable form. This paper focuses on devel-
oping a general phenotype representation model that
can be used for data-driven phenotype computing,
i.e., software-supported determination of phenotypes
based on the data of an organism. The model to be
developed must support both the biological and the
medical views of the phenotype notion. Recent at-
tempts have shown that ontologies are suitable to
handle phenotypes and that they can support clinical
research and decision making [810].
There is a large ongoing initiative in Germany, the so
called German Medical Informatics Initiative (MII) [11,
12] that aims at making clinical data available for re-
search. Most German university hospitals participate in
one of four funded consortia. Smart Medical Informa-
tion Technology for Healthcare (SMITH) is one of these
consortia [13]. Within the ongoing SMITH project, a
phenotyping pipeline (PheP) will be established to sys-
tematically develop, evaluate and execute validated algo-
rithms and models for classifying and annotating patient
care data. These annotations and derivatives will be pro-
vided for triggering alerts and actions, data sharing and
deep analyses of patient care and outcomes. The general
design and concept of the SMITH phenotyping pipeline
is presented in [14].
In this article, we propose a novel ontology-based
method to model and compute phenotypes. Our ap-
proach provides an extended reasoning combining
phenotypic data to derive complex phenotypes based on
calculations and classifications.
Methods
Phenotypes can be derived from available data that may
have been measured (quantitative data) or observed and
qualitatively described (categorical data). The data can,
for example, come from Electronic Health Records
(EHR) (clinical data) or from a research database of a
clinical/epidemiological study (research data). In SMIT
H, the required EHR data will be integrated into a cen-
tral Health Data Storage (HDS) at each site. The inte-
grated data is homogeneously represented in each HDS
using HL7 FHIR [15] and can be queried utilising FHIR
Search [16] (Fig. 1). Structured data from different
source systems in hospitals as well as unstructured doc-
uments will be extracted, transformed and loaded into
the HDS. Natural Language Processing (NLP) techniques
are used to extract and transform relevant data from un-
structured EHR documents into structured form. In
SMITH and the German Medical Informatics Initiative,
the software tool ART-DECOR [17] is used to specify an
overarching global schema, the so-called core data set
[18]. The core data subsumes the minimal set of data el-
ements that each site (i.e., University Hospital) needs to
provide in a harmonised manner. In this way, data ele-
ments are specified based on HL7 templates, their re-
spective value sets, referenced terminologies, exemplified
use scenarios and data. These specifications are the basis
for the ontology-based phenotype representation in our
approach.
Fig. 1 Integration of the PhenoMan. The Metadata Manager models
basic data elements using ART-DECOR. The Phenotype Designer
imports the ART-DECOR specification and develops phenotype
models (PheSO) utilising the PhenoMan Editor. The PhenoMan
requests required input data from the FHIR Server, computes
phenotypes and writes the results back to the FHIR Server
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 2 of 17
HL7 FHIR and FHIR Search
Healthcare records are increasingly digitised. The
EHR must be discoverable and understandable. The
patient data must be structured and standardised to
support machine-based processing and automated
clinical decision making. The FHIR (Fast Healthcare
Interoperability Resources) specification is a HL7
standard for modelling and exchanging healthcare in-
formation [15]. FHIR provides a base set of resource
types representing relevant clinical concepts that can
be used to store and exchange data in order to solve
a wide range of healthcare related problems. For
each resource type, the corresponding information
contents and structure are specified. The resources
can be used either by themselves or combined to
complex documents representing a coherent set of
healthcare information [15]. The FHIR resource types
include, inter alia, Patient (Demographics and other
administrative information about an individual or
animal receiving care or other health-related ser-
vices.), Observation (Measurements and simple as-
sertions made about a patient, device or other
subject.) and Condition (A clinical condition, prob-
lem, diagnosis, or other event, situation, issue, or clin-
ical concept that has risen to a level of concern.)
[15].
The FHIR Search Framework [16] is part of the
HL7 FHIR standard and provides a range of opera-
tions and parameters (series of name = value pairs) to
search for existing FHIR resources in the underlying
repository. In the simplest case, a search is executed
by performing a GET operation in the RESTful
framework:
GET [base-url]/[resource-type]?name = value&...{&_
format = [mime-type]}}.
e.g., GET [base-url]/Patient?gender =male.
For numeric parameter types (number, date or quan-
tity), a value range can be defined using a prefix to the
parameter value (e.g., gt = greater than, le = less or
equal).
The & (AND) operator between single search criteria is
used to search for the intersection of resources that match
all criteria specified by each individual search parameter
(e.g., Patient?gender =male&birthdate = gt1970). To search
for resources with one of the specified parameter values
(OR), the values must be separated by a comma (e.g., Obser-
vation?code= http://loinc.org?3141-9, http://snomed.info/
sct?27113001, i.e., weight code from LOINC or SNOMED).
The following query contains AND combinations of single
criteria (code AND value-quantity) as well as OR linking of
code values and can be used to search for weight observa-
tions where the weight is greater than 75 kg:
Observation?code= http://loinc.org?3141-9, http://snome-
d.info/sct?27113001&value-quantity=gt75??kg.
ART-DECOR
ART-DECOR is an open-source tool suite that supports
the creation and maintenance of HL7 templates, value
sets, scenarios and datasets [17]. To specify and hier-
archically structure required data elements (items, con-
cepts, variables) we use the Dataset Editor of ART-
DECOR. Data elements can possess several attributes,
such as name, description (in different languages) and
value domain (including data type, unit and possible
value set) (Fig. 2a).
One of the most important components of a data
element is its terminology associations. A termin-
ology association defines the binding of dataset con-
cepts to relevant terminology [17]. To associate a
data element with a terminology concept, the corre-
sponding code (including the URI or ID of the ter-
minology) must be specified. For instance, the
concept Fasting glucose [Mass/volume] in Serum or
Plasma from LOINC (URI: http://loinc.org) has the
code 15586 (Fig. 2a).
Furthermore, additional properties of data elements
can be defined as key-value pairs. We use this function-
ality to specify the mapping between the data element
and the corresponding FHIR resource type (e.g., for fast-
ing glucose, key: FHIR, value: Observation) required
for phenotype computing. Depending on the resource
type, different FHIR Search parameters must be used to
query the relevant FHIR resources. Moreover, the differ-
ent structure of the resulting resources must be consid-
ered to extract required data.
The resulting dataset specification is available in XML
or JSON and can be parsed by our software.
Ontological architecture
Our objective was to design the PhenoMan software
according to the three-ontology method [19]. This
method is based on interactions of three different
kinds of ontologies: a task ontology (TO), a domain
ontology (DO) and a top-level ontology (TLO). The
TO serves as the conceptual model for the software,
the DO provides the domain-specific knowledge,
whereas the TLO integrates the TO and the DO and
is used as foundation of them.
In our case, the Core Ontology of Phenotypes (COP,
see section 'Core Ontology of Phenotypes (COP)') func-
tions as a TO. It describes the general structure of valid
phenotype specifications and thus enables the Pheno-
Man to create such specifications and to use them for
phenotype computing. Concrete phenotype specifica-
tions (domain-specific knowledge) are represented in
Phenotype Specification Ontologies (PheSO, see section
'Phenotype Specification Ontologies (PheSO)') playing
the role of domain ontologies (DO) in our architecture.
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 3 of 17
For the foundation of the TO we used the General
Formal Ontology (GFO) [20] as TLO. GFO has already
been successfully applied for a foundation of phenotype-
related notions. For instance, a novel approach to repre-
sent complex phenotypes in OWL was proposed im-
proving the consistency and expressiveness of formal
phenotype descriptions [10]. Another pillar of GFO for a
grounding of phenotypes is the foundational ontology of
properties, attributives and data (GFO-Data [21])
providing an extensive classification of properties (and
attributives). In the current paper, we especially refer-
ence the property notion of GFO (including distinction
between single and composite properties [22]) in our
phenotype representation model supporting data-driven
phenotype computing.
One of the advantages of the three-ontology method is
that the software only needs to implement the access to
entities (classes, properties) of the TO (COP), whereas
Fig. 2 Mapping between ART-DECOR, PheSO, FHIR Subscription and FHIR Observation entities. a Specification of the data element fasting
glucose in ART-DECOR. b Annotations of the corresponding class Fasting_Glucose after importing the ART-DECOR specification into the PheSO. c
Subscription generated for the class Fasting_Glucose. The criteria (FHIR Search query) is encoded. The original URL part is Observation?code=
http://loinc.org|1558-6. d Observation of fasting glucose provided by FHIR Server. The observation code, value, date and the referenced patient
are specified. (The same colour of the border indicates the mapping between the entities)
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 4 of 17
the entities of the corresponding DO (PheSO) are proc-
essed dynamically. The PhenoMan uses the COP as an
interface to access the PheSO entities.
Additional requirements for the ontological modelling
were:
 Developing in OWL (using OWL API [23], HermiT
[24] and Openllet [25])
 Modelling all attributes and relations that are
relevant for reasoning as object or data properties
 Modelling all attributes and relations that are not
relevant for reasoning as annotations
 Usage of general class axioms (based on subclass
of) instead of equivalence classes if only one
direction (is-a relation) is relevant for reasoning.
Software design
We defined the following main requirements for the
overall system (Fig. 1):
 The system must support a phenotype specification
providing a GUI tool (see section 'Specification of
phenotypes').
 The phenotype specifications must be saved in a
standardised ontology (see sections 'Core Ontology
of Phenotypes (COP)' and 'Phenotype Specification
Ontologies (PheSO)').
 The system must be able to correctly compute
phenotypes based on a phenotype specification
(ontology) and input data (see section 'Classification
and calculation of phenotypes').
 The system must support an additional
implementation of mapping components for
accessing required data and metadata repositories.
Example components for metadata import from
ART-DECOR as well as for interaction with FHIR
servers (e.g., SMITH HDS) must be implemented
(see sections 'Data procurement' and 'Transmission
of inferred phenotype classes to the FHIR Server').
The PhenoMan accesses the FHIR Server, extracts
phenotype-specific data, computes the specified pheno-
types and writes the results back to the FHIR Server. For
this purpose, the PhenoMan provides an API and acts as
a web service (using Dropwizard [26]) (Fig. 1). The Phe-
noMan is implemented in Java using OWL API [23] and
two reasoners, HermiT [24] and Openllet [25]. For cal-
culations we utilize the Java Expression Evaluator (Eva-
lEx) [27], but the integration of other libraries (e.g., for
executing R scripts) or rule systems (e.g., SWIRL or
Drools) is also possible. The EvalEx enables evaluating
mathematical and Boolean (inter alia, Boolean operators
and IF-THEN-ELSE structures) expressions and sup-
ports defining custom functions and operators.
The PhenoMan Editor1 is a desktop app, which is also
developed with Java and bundled with the PhenoMan
API. It offers a graphical user interface based on Java
Swing to specify attributes of phenotype classes and cat-
egories using appropriate form fields. On saving, form
content is transmitted to the PhenoMan API and written
into the ontology. The editor can be executed on a local
machine with a Java runtime environment 8 or higher
and was developed with the aim of rapidly defining
phenotype models.
Evaluation
An evaluation of our approach was designed and con-
ducted. The main objectives of the evaluation were to
prove:
1. Correct functioning of all software components
2. Faultless communication of the software with the
FHIR Server
3. Correctness of all provided phenotype specifications
4. Correct functioning of the overall system by
comparison with a corresponding SPSS
implementation of selected phenotypes.
We evaluated the PhenoMan at different levels. Firstly,
we tested all functionalities of the PhenoMan API (espe-
cially read/write in the ontology and computing pheno-
types) and the communication of the PhenoMan Service
with the FHIR Server by a set of static JUnit tests using
fixtures (i.e., example PheSOs and patient data).
Secondly, each phenotype specification is shipped with
a structured representation (spreadsheet) of test data (in-
put and output), such that the respective phenotype al-
gorithm can be automatically tested. The criterion for a
successful execution of the JUnit tests was a match be-
tween the results calculated by PhenoMan based on pro-
vided input data and the corresponding output data.
Finally, we selected some test case algorithms/deriva-
tives (such as socio-economic status [28], body mass
index [29], waist circumference and waist-hip ratio [30])
from the LIFE Adult study [31] running at the LIFE Re-
search Centre for Civilization Diseases, University of
Leipzig. There, derivatives are usually implemented by
epidemiologists, statisticians and other researchers using
the statistics software SPSS [32] and R or are database
(SQL) queries and functions, which are automatically ex-
ecuted at night based on daily captured data. The result-
ing data are directly stored within the LIFE research
database in tabular form. More details about partici-
pants, their invitation and consenting as well as
1Source code and releases of the PhenoMan Editor are available on
GitHub under the GPL-3.0 license: https://github.com/Onto-Med/Phe-
noMan-Editor
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 5 of 17
examinations, interviews, questionnaires and taken spe-
cimen can be found in [31]. We reproduced selected
SPSS derivatives using the PhenoMan and developed
parameterised JUnit tests to comparatively evaluate the
accuracy of the PhenoMan against the corresponding
SPSS implementation at the LIFE Research Centre. The
criterion for a successful comparison was a match be-
tween the results calculated by PhenoMan and SPSS
software for each dataset. The performance of our ap-
proach is not a critical issue in our use case (the pheno-
type computing could run overnight).
This evaluation included data of thousands of LIFE
participants.
Results
Core Ontology of Phenotypes (COP)
We developed the Core Ontology of Phenotypes
(COP, Fig. 3) to model, classify and calculate pheno-
types based on instance data sets (e.g., of a patient).
In this article, we consider a phenotype as a
dependent individual (in the sense of General Formal
Ontology, GFO [20]), for example, the weight of a
specific person. Hereinafter, abstract instantiable en-
tities that are instantiated by phenotypes are called
phenotype classes. For instance, the abstract property
weight possesses individual weights as instances. We
distinguish between single and composite properties,
and correspondingly, between single and composite
phenotypes. A composite property is defined as a
property that has single properties as parts [22].
Based on the definitions of single and composite
properties [22], we define single phenotypes as single
properties (e.g., age, weight, height) and composite
phenotypes as composite properties (e.g., height and
weight, BMI, SOFA score [33]) of an organism or of
one of its subsystems. Properties of an organism are
considered as all documentable information about it,
whereby the modeller is left to decide what is rele-
vant to the current situation. These can be, for ex-
ample, observable characteristics or traits of an
organism [13] or possible manifestations of clinical
phenotypes, such as signs, symptoms or dispositions
[4]. The corresponding data can be modelled using
the FHIR Observation or Condition resources.
Composite phenotypes are divided into combined
and derived phenotypes. A combined phenotype is
only a combination of corresponding phenotypes (e.g.,
a combination of height and weight), whereas a de-
rived phenotype is an additional property (e.g., BMI)
derived from the corresponding phenotypes (height
and weight). In the framework of GFO we modelled
properties using the class gfo: Property. In the present
article, composite phenotype classes are modelled
using a Boolean expression based on has_part relation
(e.g., weight and height: has_part some height and
has_part some weight). Derived phenotype classes
additionally define a calculation rule/mathematical
formula (e.g., BMI = weight [kg] / height [m]2). Fur-
thermore, combined phenotype classes can associate
certain conditions with specific predefined values
(scores), which can be used, e.g., in further formulas.
For example, if bilirubin value is greater than 12 mg/
dL, then the value 4 is used for the calculation of the
SOFA score [33].
Additionally, we distinguish between restricted and
non-restricted phenotype classes, depending on whether
their extensions (set of instances) are restricted to a cer-
tain range of individual phenotypes by defined condi-
tions or all instances are allowed. For example, the
phenotype class age is instantiated by the ages of all liv-
ing beings (non-restricted), whereas the phenotype class
young age is instantiated by the ages of the young ones,
e.g., if the age is below 30 years (restricted).
Phenotype Specification Ontologies (PheSO)
We consider a phenotype algorithm as a sequence of in-
structions (1) to classify phenotypes (single or compos-
ite) in phenotype classes or (2) to derive additional
properties (derived phenotypes) from the phenotypes of
an organism. Phenotype algorithms can be implemented,
for example, using a programming language or a statis-
tics software (e.g., SPSS or R). Our approach is to separ-
ate the specification of phenotypes (models) from the
implementation of corresponding algorithms. The COP
provides a basic model to specify phenotypes in a stan-
dardised way, while the PhenoMan implements the gen-
eral approach, common for all COP-based specifications.
It is not our aim to completely model the EHR. Instead,
our approach can support the modelling and calculation
of selected phenotypes in a user-friendly standardised
manner.
Phenotypes are modelled in Phenotype Specification
Ontologies (PheSO) using the COP. The phenotype clas-
ses and axioms (classification and calculation rules) con-
tained in the PheSO are used by PhenoMan to execute
the corresponding phenotype algorithm. PheSOs are em-
bedded in the COP in such a way that the classes of the
PheSO are subclasses of the COP classes. Every PheSO
subclass of the COP classes cop: Single_Phenotype, cop:
Combined_Phenotype or cop: Derived_Phenotype is a
phenotype class and is instantiated by phenotypes. The
Fig. 3 Core Ontology of Phenotypes (COP)
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 6 of 17
direct subclasses are non-restricted (e.g., Fasting_Glu-
cose, Fig. 4a), while the subclasses of the non-restricted
phenotype classes are restricted (e.g., Fasting_Glucose_
ABNORMAL, i.e., fasting glucose is greater equal 125
mg/dL, Fig. 4a3).
Phenotype classes possess various common attributes
(e.g., labels, descriptions and codes of external concepts).
Other attributes vary depending on the type of the
phenotype class. The following are examples of such
attributes:
 Non-restricted single phenotype (NSiP) class: unit of
measure and optional aggregate function.
 Restricted single (RSiP) and derived phenotype
(RDeP) class: restriction.
 Restricted combined phenotype (RCoP) class:
Boolean expression (based on RSiP, RCoP and RDeP
classes) and optional score value.
 Non-restricted derived phenotype (NDeP) class:
mathematical formula and Boolean expression
consisting of AND-linked variables used in the for-
mula (NSiP and non-restricted combined phenotype
(NCoP) classes). If a NCoP class is used as a vari-
able, the RCoP classes (subclasses) of the NCoP class
must have score values that should be used in the
formula.
Simple attributes of the phenotype classes are defined
as annotations. The logical relations between phenotype
classes as well as range restrictions are represented in
OWL by anonymous equivalent classes or general class
axioms based on property restrictions.
Phenotype Manager (PhenoMan)
We developed the software Phenotype Manager (Pheno-
Man), which implements a multistage reasoning ap-
proach combining standard reasoners (e.g., Pellet or
HermiT) and mathematical calculations. This section
briefly outlines the main functionality of our solution.
Specification of phenotypes
The PhenoMan Editor is an interactive user interface for
managing and developing PheSOs. The user is able to
create a new PheSO or to load an existing ontology. The
Fig. 4 Parts of the T2DM PheSO in Protégé. Middle: Phenotype classes (a Single, b Derived, c Combined). Left: Example annotations of the
phenotype classes. Right: Anonymous equivalent classes and general class axioms
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 7 of 17
PhenoMan Editor provides appropriate forms to browse,
create and edit categories and phenotype classes of the
ontology. Value range restrictions, for example, are de-
fined by selecting a comparison operator and entering
the corresponding values (Fig. 5). Boolean expressions
are built by drag-and-dropping the phenotype classes
from the left side into the expression form field and en-
tering relevant operators (Fig. 6). After submission, the
form data is transmitted to the PhenoMan API and is
stored in the actual PheSO.
Furthermore, an ART-DECOR specification (XML)
of relevant data elements can be imported in the
PheSO. For each data element, a NSiP class is gener-
ated. All relevant attributes (name, codes, FHIR re-
source type, data type, unit, etc.) specified in ART-
DECOR are defined as annotations of corresponding
classes (Fig. 2a, b).
Data procurement
After starting the PhenoMan Service, FHIR subscriptions
(rest-hooks) [34] are generated and transmitted to the
FHIR Server. The structure of the subscription resource
is very simple. The main parts of the resource are the
criteria and the channel. The FHIR Server uses the cri-
teria (FHIR Search query) to determine resources for
which notifications have to be generated. When re-
sources are identified (after creating or updating) meet-
ing the criteria, a notification is sent to the address
(endpoint) specified in the section channel.
Fig. 5 Specification of the class Fasting_Glucose_ABNORMAL with the PhenoMan Editor form. We left out some of the metadata fields for
better visibility
Fig. 6 Specification of the class T2DM_Case_3 with the PhenoMan Editor form
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 8 of 17
In the configuration file of the PhenoMan, a directory
containing all available phenotype specifications (Phe-
SOs) as well as the address (URL) of the PhenoMan ser-
vice (including the PheSO name) are defined. For each
NSiP class of each available PheSO (in the defined direc-
tory) a subscription is created. To generate the subscrip-
tion criteria (FHIR Search query), the PhenoMan uses
the resource type and codes specified in the correspond-
ing NSiP class as annotations (Fig. 2b, c). The endpoint
attribute is automatically filled with the URL of the Phe-
noMan service defined in the configuration file. The
remaining parts of the subscription resource (status,
type and payload) take default values (active, rest-
hook and application/json) (Fig. 2c).
After receiving a notification (including the
complete resource, Fig. 2d), the PhenoMan Service re-
quests further resources (for all other NSiP classes of
the corresponding PheSO) using FHIR Search. The
generated FHIR Search queries are primarily based
upon the codes specified for the NSiP classes (simi-
larly to subscription criteria), contain a reference to
the patient and can additionally support possible ag-
gregate functions.
Classification and calculation of phenotypes
After receiving required resources, the PhenoMan starts
inferring phenotypes.
First, the relevant information is extracted from re-
ceived resources and inserted into the ontology. On the
one hand, the individual properties (single phenotypes)
are inserted as instances of the direct subclasses of cop:
Single_Phenotype and the values are modelled as prop-
erty assertions based on the has_value relation. On the
other hand, a composite phenotype is defined as an in-
stance of the class cop: Composite_Phenotype, which
combines all the single phenotype instances using prop-
erty assertions based on has_part relation. Then, our
multistage reasoning algorithm is executed. The algo-
rithm consists of the following steps:
1. Classification step. A standard reasoner classifies the
existing instances (assignment to classes).
a. Single phenotype instances are classified in RSiP
classes based on property restrictions.
b. The composite phenotype instance is classified
in RCoP classes based on the specified Boolean
expression and inferred RSiP, RCoP and RDeP
classes.
c. The composite phenotype instance is classified
in NDeP classes based on the specified Boolean
expression and corresponding NSiP and NCoP
classes. In this case, all variable values required
for calculating formulas are present.
d. Available instances of NDeP classes
(representing calculated values) are classified in
RDeP classes based on property restrictions.
2. If no new NDeP classes get an instance, the
execution of the algorithm stops. All inferred
phenotypes (inferred classes including metadata and
calculated values) are returned.
3. Calculation step. The formula of each NDeP class
having an instance is calculated based on variable
values (values of the corresponding single
phenotype instances or scores of the inferred RCoP
classes). A new instance of the NDeP class
representing the calculated value is created and
associated with the composite phenotype instance
(using has_part).
The algorithm goes back to step 1.
In the case of complex phenotypes, the classification
and calculation steps can be executed several times (in a
loop). That is the case if a NDeP class has subclasses,
i.e., RDeP classes, which are in turn used in combined
phenotypes. Both steps are repeated until all formulas
are calculated and all phenotypes are classified.
The PhenoMan supports 4 primitive data types xsd:
decimal, xsd:string, xsd:boolean and xsd:date. All other
complex data types (e.g., FHIR code or quantity) are
mapped to the primitive data types (e.g., code to xsd:
string and quantity to xsd:decimal with additional unit
attribute, Fig. 2a, b). Furthermore, the PhenoMan pro-
vides, inter alia, aggregate functions, Boolean, date and
measurement unit arithmetic, integration of external ter-
minologies as well as reading and writing FHIR
resources.
Transmission of inferred phenotype classes to the FHIR
Server
PhenoMan returns all inferred combined and derived
phenotype classes as Observation resources and trans-
mits them to the FHIR Server. The generated resources
can have a numeric or a code data type. Numeric obser-
vations are used for storing numeric values (i.e., calcu-
lated values of derived phenotypes or score values of
combined phenotypes), whereas the code observations
are intended for concepts of a terminology or a value
set. The specified codes of the non-restricted phenotype
classes are utilised in the resulting resources to complete
the code attribute. The calculated values, scores or
codes of the inferred restricted phenotype classes are
used in valueQuantity or valueCodeableConcept attri-
butes (Fig. 7).
Export capabilities
The PhenoMan can export the complete PheSO or gen-
erate reasoner reports (structured descriptions of all
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 9 of 17
Fig. 7 T2DM case 3 Observation. For the Patient/103, an Observation resource was generated including the code (t2dm_case_calculated), the
value (t2dm_case_3) and the observation method (generated by PhenoMan)
Fig. 8 T2DM case 1 reasoner report. Left: Phenotype class types from the COP (single, combined, derived). Middle: Non-restricted classes of the
PheSO (the NSiP classes contain the current input value, the NDeP classes the calculated value). Right: Restricted classes inferred by PhenoMan
based on the input data
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 10 of 17
inferred phenotypes) in Excel format. A tabular rea-
soner report contains three columns: Type, Non-re-
stricted and Restricted. In the first column the type
(single, combined or derived) of the inferred pheno-
type is presented. In the next two columns, the speci-
fied or derived information about the resulting
phenotypes (restricted and non-restricted) is displayed
(Fig. 8).
Additionally, the PhenoMan can generate the graphical
representation of combined phenotype classes in the
form of decision tree (or flowchart) diagrams. The Phe-
noMan translates the Boolean expressions of every RCoP
class into disjunctive normal form and considers each
conjunction as a possible path of the tree. Then, the
paths are grouped on shared nodes and form a tree
structure (Fig. 9).
Example
We illustrate our approach by means of an example al-
gorithm for determining Type 2 Diabetes Mellitus
(T2DM) cases presented by PheKB.org [37]. The T2DM
algorithm requires the following data elements to be ex-
tracted from the EHR:
 Counts of T1DM and T2DM diagnoses (identified
by ICD-9 billing codes),
 The earliest dates of T1DM and T2DM medications
(identified by RxNorm codes),
 Laboratory values (fasting blood glucose, random
blood glucose and hemoglobin A1c, identified by
LOINC codes) as well as
 Physician-entered diagnoses (derived from
encounter or problem list sources only).
Modelling of single data elements using ART-DECOR
As a first step, required data elements (items, concepts,
variables, single phenotype classes) representing single
patient characteristics relevant for determining the
T2DM (T1DM and T2DM diagnoses, T1DM and
T2DM medications, fasting blood glucose, random blood
glucose, hemoglobin A1c as well as physician-entered
diagnoses) must be modelled using ART-DECOR. La-
bels, descriptions, codes from external terminologies,
data types, units, etc. are specified. Additionally, every
data element must be associated (as property) with a
FHIR resource type in order to ensure the correct search
and extraction of the instance values from the respective
resource. Laboratory values are represented in FHIR as
Observations, while diagnoses are usually specified using
the Condition resource. The Fig. 2a shows the filled
form of fasting glucose in ART-DECOR.
The resulting specification is provided by ART_
DECOR as a XML or JSON file.
Modelling of phenotypes using PhenoMan Editor
The user imports the ART-DECOR specification in
the ontology (PheSO) utilizing the PhenoMan Editor.
For each data element, a NSiP class (Fasting_Glucose,
HBA1c, Random_Glucose, T1DM_Diagnosis, T2DM_
Diagnosis, T1DM_Medication, T2DM_Medication and
T2DM_Diagnosis_by_Physician) including relevant an-
notations is generated (Fig. 4a, a1, a2).
Furthermore, aggregate functions (e.g., COUNT, FIRS
T, LAST, MIN, MAX) can be defined for NSiP classes.
For instance, the T2DM algorithm does not require to
process the complete data of all diagnosis resources, it is
sufficient to count the resources. Therefore, the function
COUNT is defined for the class T2DM_Diagnosis (Fig.
Fig. 9 T2DM decision tree. The tree was generated by PhenoMan in GraphML format and was processed using an automatic layout [35] of the
yEd Graph Editor [36]. Nonetheless, it is also possible to generate the complete tree as image in PNG format
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 11 of 17
4a2). The both medication classes (T1DM_Medication
and T2DM_Medication) are associated with the function
FIRST, because only the earliest date of medications is
relevant for the algorithm.
Next, the RSiP classes (Fasting_Glucose_ABNORMAL,
Random_Glucose_ABNORMAL and HBA1c_ABNORMAL)
for value range restrictions are defined as subclasses
of the NSiP classes (Fig. 4a, a3, Fig. 5). For every
RSiP class, the anonymous equivalent class is created
that represents the corresponding restriction. The re-
strictions of other NSiP classes (e.g., T2DM_Diagnosis
or T2DM_Diagnosis_by_Physician) are based on the
counts of the corresponding resources (e.g., T2DM_
Diagnosis_NO: if the count of T2DM_Diagnosis = 0 or
T2DM_Diagnosis_by_Physician_YES: if the count of
T2DM_Diagnosis_by_Physician > = 2, Fig. 4a4).
Mathematical calculations are modelled using NDeP
classes. The formula GT($T1DM_Medication, $T2DM_
Medication) (GT stands for Greater Than) defined for
the class T2DM_precedes_T1DM_Medication expresses
a comparison of the T1DM and T2DM medication dates
(Fig. 4b, b1). The dollar sign in the variable name indi-
cates that not the medication value itself but the entry
date of the medication is used in the formula. The for-
mula returns ? 1, 0 or 1 depending on whether the first
operand is less than, equal to or greater than the second
operand. The value ? 1 is also returned if one of the op-
erands is missing. The RDeP class T2DM_precedes_
T1DM_Medication_YES and the corresponding restric-
tion are specified similarly to RSiP classes (Fig. 4b, b2).
The next step is to model the abnormal lab, i.e., if ei-
ther random glucose or fasting glucose or HBA1c is ab-
normal. For this purpose, we define the non-restricted
combined phenotype (NCoP) class Abnormal_Lab and
the RCoP class Abnormal_Lab_YES with the corre-
sponding Boolean restriction (disjunction) formalised as
general class axiom (Fig. 4c, c1).
Finally, the T2DM case selection rules are modelled
using the NCoP class T2DM_Case as well as the five
RCoP classes (one for each case type) including Boolean
restrictions (Fig. 4c, c2, Fig. 6). For instance, case 1 occurs
when the T1DM diagnosis is missing but a T2DM diagno-
sis as well as both medications (T1DM and T2DM) are
present and the first T2DM medication precedes the first
T1DM medication. The abnormal lab, the presence of a
T2DM diagnosis and the absence of T1DM diagnosis and
both medications indicate the case 3.
The resulting ontology and additional material (graph-
ical and tabular representation, reasoner reports) are
publicly available [38].
Execution of the PhenoMan Service
The PheSO (OWL file) created by the PhenoMan Editor
is saved in the directory specified in the configuration
file. After starting the PhenoMan Service, subscriptions
for each NSiP class of the T2DM PheSO are created.
The subscription for the NSiP class Fasting_Glucose, for
example, is intended to identify Observation resources
with the LOINC code 15586 (criteria) (Fig. 2c). After
receiving a fasting glucose resource, other required re-
sources are queried. The query for T2DM diagnosis, for
instance, includes the additional FHIR Search parameter
_summary = count to express the aggregate function
COUNT:
Condition?code=http://hl7.org/fhir/sid/icd-9-cm|250.00,
http://hl7.org/fhir/sid/icd-9-cm|250.02&subject=Patient/1
03&_summary=count.
In this case, the server returns a bundle with only the
number of resources matching the query. To realise the
function FIRST, the combination of _sort (sorting by
date) and _count (_count = 1) is used. The following
T2DM medication query returns only the first resource
matching the criteria:
MedicationStatement?code=http://www.nlm.nih.gov/
research/umls/rxnorm|25789,http://www.nlm.nih.gov/re-
search/umls/rxnorm|10633&subject=Patient/103&_sort=
effective&_count=1
(Some codes were omitted in both queries).
As soon as all required FHIR resources are present,
the PhenoMan starts the phenotype computing. Sup-
pose, the input resource set consists of only a fasting
glucose (Fig. 2d) and a T2DM diagnosis resource. In this
case, the single phenotype instances of the classes Fast-
ing_Glucose and T2DM_Diagnosis are created and the
values are modelled as property assertions based on the
has_value relation (e.g., has_value 130 for Fasting_Glu-
cose and has_value 1 for T2DM_Diagnosis (due to the
COUNT function)). Then, a composite phenotype in-
stance is defined, which combines both single phenotype
instances using property assertions based on has_part re-
lation. In the first step (classification step), a standard
reasoner classifies the single phenotype instances in re-
stricted classes. In our example, the instance of Fasting_
Glucose is classified in the class Fasting_Glucose_AB-
NORMAL (i.e., the fasting glucose value is > = 125 mg/
dL, Fig. 4a, a3) and the instance of T2DM_Diagnosis in
the class T2DM_Diagnosis_YES (because the count of
the T2DM diagnoses is > 0, Fig. 4a, a4). Next, the com-
posite phenotype instance is classified in the RCoP clas-
ses Abnormal_Lab_YES (because the fasting glucose is
abnormal, Fig. 4c, c1) and T2DM_Case_3 (because all
conditions of the corresponding Boolean expression are
fulfilled, Fig. 4c, c2). In this case, further phenotypes can
not be derived or calculated.
Let us consider another example. Suppose, the input
data set contains a T2DM diagnosis, a T1DM and a
T2DM medication (T2DM precedes T1DM medication).
The classification step is similar to the first example.
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 12 of 17
The corresponding single phenotype instances are classi-
fied in the classes T1DM_Diagnosis_NO, T2DM_Diag-
nosis_YES, T1DM_Medication_YES and T2DM_
Medication_YES. In the next step (calculation step), the
formula of the NDeP class T2DM_precedes_T1DM_
Medication (Fig. 4b, b1) can be calculated by PhenoMan.
It inserts the variable values (the dates of the both medi-
cations) in the formula and starts the calculation. After
the calculation step, the classification step must be per-
formed again. The calculated instance of T2DM_pre-
cedes_T1DM_Medication is classified in the class
T2DM_precedes_T1DM_Medication_YES (because the
formula returns 1, Fig. 4b, b2). Then, the composite
phenotype instance is classified in the RCoP class
T2DM_Case_1 (Fig. 4c, c2) and the PhenoMan finishes
the calculation.
Finally, the PhenoMan generates the Observation re-
source for the resulting T2DM case and transmits it to
the FHIR Server. In our example, we encode the possible
T2DM cases using a code system (https://www.smith.-
care/phenoman/t2dm_case_selection_algorithm), one
code identifying the observation (t2dm_case_calculated)
and five codes for possible values (t2dm_case_1, t2dm_
case_2, t2dm_case_3, t2dm_case_4 and t2dm_case_5).
The resulting Observation resource for case 3 is illus-
trated in Fig. 7.
Additionally, we can generate a tabular reasoner report
or a decision tree diagram.
An example reasoner report for case 1 is shown in Fig. 8.
The class T2DM_Case and its subclasses (T2DM_
Case_1, T2DM_Case_2, etc.) are very suitable for the
representation as a decision tree. The decision tree gen-
erated by PhenoMan (Fig. 9) looks similar to the flow-
chart specified by PheKB.org [37].
Evaluation results
The evaluation has demonstrated that all components of
our solution function correctly. All JUnit tests were suc-
cessful and showed no difference between specified and
calculated results. The comparison between the Pheno-
Man and the SPSS calculation has also succeeded. Al-
though the performance of our approach is not a critical
issue in our use case, we measured the execution time of
the PhenoMan. The calculation of the socio-economic
status (complex algorithm requiring multiple reasoner
runs), for example, takes approximately 0.5 s per dataset.
This performance is completely sufficient for our use
case.
In summary, PhenoMan correctly computes pheno-
types based on valid phenotype specifications (PheSO)
and input data. Additionally, validating phenotype speci-
fications (PheSOs) before deploying them in a product-
ive environment is an extremely useful feature of the
PhenoMan.
Related work
We developed a novel approach to support onto-
logical modelling and reasoning of phenotypes. In
contrast to [8, 9], our solution serves to determine
and to classify phenotypes based on instance data
(e.g., EHR). Moreover, the proposed reasoning process
includes calculation of mathematical formulas at
runtime.
Very similar to our approach, Fernández-Breis et al.
[39] propose to take advantage of the best features of
EHR standards and ontologies. The authors developed
methods allowing a direct use of EHR data for the iden-
tification of patient cohorts leveraging current EHR stan-
dards and semantic web technologies. In [39], openEHR
[40] archetypes were used as EHR standard. An onto-
logical infrastructure was designed including different
ontologies for representing domain entities (colorectal-
domain), the rules for determining the risk level and the
data. The mappings between the phenotyping archetype
and the colorectal-domain ontology were defined and
are automatically executed on the archetyped data in-
stances to generate the OWL dataset. The data is then
transformed into OWL, where the classification is
performed.
Pape et al. [41] evaluated OWL/RDF for enabling
computable representations of EHR-driven phenotyping
algorithms. A proof-of-concept application using the
OWL API and the HermiT reasoner was developed. The
phenotype algorithms are specified based on a rudimen-
tary core ontology without the possibility to model and
execute mathematical calculations. The authors utilised
a simple diabetes phenotype algorithm as an example
and validated their approach against a selected set of de-
siderata proposed by Mo et al. [7]. The evaluation
showed that OWL/RDF is potentially sufficient to store
phenotyping algorithms, as it is versatile enough to meet
most of the desiderata.
We use HL7 FHIR as a standard for exchanging
healthcare information in the SMITH infrastructure. But
the main difference to the both ontological approaches
described above lies in our three-level ontological archi-
tecture. The COP is founded by GFO and provides a
framework for developing PheSOs. In this way, each par-
ticular phenotype model specified as a PheSO has the
same standardised structure and can be executed by
PhenoMan in the same manner. A further advantage of
our solution is that the PhenoMan supports classifica-
tion as well as calculation tasks and works directly with
FHIR format, so that no further transformations are re-
quired. The mapping between EHR data and ontology is
performed by PhenoMan automatically using termin-
ology associations, which are defined for each data elem-
ent in ART-DECOR (and imported into ontology) as
well as in FHIR resources (e.g., Observation).
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 13 of 17
The main objective of SHARPn [42] is to develop
methods and modular open-source resources for enab-
ling secondary use of EHR data for high-throughput
phenotyping. The phenotype algorithms are specified
based on Quality Data Model (QDM) [43] and repre-
sented in the HL7 Health Quality Measures Format
(HQMF or eMeasure) [44]. According to the authors,
there are two main challenges. Firstly, data elements in
an EHR may not be represented in a format consistent
with the QDM. Secondly, an EHR typically does not na-
tively have the capability to automatically consume and
execute eMeasure logic. To address these challenges, a
translator tool was developed that converts QDM-
defined phenotyping algorithm criteria into executable
Drools rules scripts.
The Phenotype Execution and Modeling Architecture
(PhEMA) [45] is an open-source infrastructure for
standards-based authoring, sharing, and execution of
phenotyping algorithms. Similarly to SHARPn, PhEMA
uses QDM and HQMF to model phenotype definitions.
Phenotyping algorithms are represented using the
PhEMA Authoring Tool (PhAT), are exported from the
PhAT into executable KNIME [46] workflows and are
executed against data warehouses or data repositories.
In contrast to the rule- or workflow-based description
of phenotyping algorithms, we use an ontology-based
one. Our approach is rather generic and enables a stan-
dardised and structured modelling as well as the reuse
of phenotyping algorithms and their parts (e.g., concepts
and restrictions). Furthermore, the PhenoMan is com-
patible with the native representation of EHR data (HL7
FHIR) in the SMITH infrastructure and does not need
an additional import of the data into a data warehouse.
In [47] a FHIR-compatible model was designed to sup-
port capture of cancer clinical data. Our approach allows
the modelling of different phenotypes based on a core
ontology (COP) and is independent of the EHR repre-
sentation standards. The interpretation of FHIR data
and the mapping to specified phenotypes using termin-
ology associations are provided by PhenoMan.
A method to enable automated transformation of clin-
ical data into OWL ontologies is presented in [48]. The
developed system generates OWL representations of
openEHR archetypes and automatically transforms
openEHR data to OWL individuals. In our approach, the
phenotypes are directly modelled in the ontology and
are automatically mapped to the EHR data. Moreover,
our solution supports classification as well as calculation
of phenotypes.
As described in section Phenotype Specification On-
tologies, phenotypes can possess links to concepts of ex-
ternal ontologies. For instance, they may be annotated
with concepts of anatomic structures (e.g., Foundational
Model of Anatomy [49]), or situations, respective
processes, where phenotypes are observed (e.g., electro-
cardiographic monitoring). The linkage is similar to the
Entity-Quality method [50] (entity: anatomic structure
or process, quality: phenotype) and may improve com-
parison of COP across multiple domains.
Hoehndorf et al. [9] proposed the PhenomeNET for
incorporation of phenotype ontologies from different
species. PhenomeNET can predict orthologous genes
with common pathways and common related diseases.
Apart from the different interpretation of the term
phenotype, the main focus of our attempt is to deduce
complex phenotypes from a set of basic phenotypes of
an individual.
The Human Phenotype Ontology (HPO) [8] associates
phenotypic abnormalities with underlying diseases and
participating genes, whereas COP can contain all sorts
of properties of an organism (including non-
abnormalities). Currently, COP does not offer weights
for phenotype-disease relations, like HPO does to sort
diseases for a phenotype set by relevance. We will inves-
tigate ways to add this functionality to COP in future.
Discussion
Mo et al. [7] propose 10 desired characteristics for a
flexible, computable phenotype representation model. In
this section, we discuss our implementation of the pro-
posed desiderata.
Structure clinical data into queryable forms
In the SMITH project, the patient data is structured and
integrated in a Health Data Storage (HDS) according to
HL7 FHIR. FHIR Search is used as a query language.
PhenoMan supports querying and writing FHIR re-
sources to populate the PheSO respectively to write the
inferred phenotypes back to the server. Additionally, we
investigate the future application scenarios of more ex-
pressive query languages, such as FHIRPath [51] and
Clinical Quality Language (CQL) [52].
Recommend a common data model, but also support
customization for the variability and availability of EHR
data among sites
HL7 FHIR is used as a common EHR data model. The
FHIR specification supports adaptation to particular
contexts of use by means of profiling [53].
Support both human-readable and computable
phenotype representations
The PheSOs are computable representations of pheno-
type models. They are implemented in OWL and can be
edited using the PhenoMan Editor or common ontology
editors such as Protégé. The PhenoMan interprets Phe-
SOs based on the COP and infers specified phenotypes.
Additionally, the PhenoMan provides human-readable
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 14 of 17
representations in graphic (diagrams) or tabular (spread-
sheets) form.
Implement set operations and relational algebra
The ontological representation supports intersection,
union and negation operations as well as existential and
universal quantifiers in accordance with OWL DL. To
extend the expression possibilities, First-Order Logic can
be considered in the future work.
Represent phenotype criteria with structured rules
Our multistage reasoning approach supports structured
specification of phenotype models from single pheno-
types (non-restricted and restricted) through combined
phenotypes (Boolean expressions) to derived phenotypes
(mathematical calculations). The PhenoMan iteratively
processes the individual steps (classification and calcula-
tion rules) in the proper order until all specified pheno-
types are inferred. It supports Boolean operations
(conjunction, disjunction and negation), specification of
value ranges (as intervals or enumerations) and aggre-
gate functions (for multiple values of single phenotypes
as well as for values calculated based on different
datasets).
Support defining temporal relations between events
All FHIR resource types processed by PhenoMan possess
one or more date attributes. The relevant date attributes
were implemented and associated with the correspond-
ing resource types. When the PhenoMan receives re-
sources from the FHIR Server, the dates of the resources
are considered and can be used as variable values in for-
mulas of NDeP classes (the dollar sign in the variable
name indicates the resource date as variable value, Fig.
4b1).
Use standardised terminologies, ontologies, and facilitate
reuse of value sets
In the SMITH project, the metadata (catalogue of items,
data elements) is specified using the software ART-
DECOR [17]. ART-DECOR is very suitable to specify
terminology associations (e.g., LOINC, SNOMED, ICD,
RxNorm) and value sets. The PhenoMan integrates the
metadata from ART-DECOR and uses the specified ter-
minology associations and value sets (codes) to query
the HDS and to write the inferred phenotypes.
Define representations for text searching and natural
language processing
In SMITH, a NLP module is upstream that extracts and
transforms relevant data from unstructured EHR docu-
ments into structured form. The structured data is then
integrated in the HDS, so that the PhenoMan can work
only with structured data.
Provide interfaces for external software algorithms
We use the Java Expression Evaluator (EvalEx) [27] for
mathematical calculations, but the integration of other
libraries (e.g., for executing R scripts) or rule systems
(e.g., SWIRL or Drools) is also possible and will be eval-
uated in future work.
Maintain backward compatibility
Our ontological phenotype representation model is ro-
bust to changes in EHR data, used terminologies and
standards. If, for instance, in our T2DM example both
ICD9 and ICD10 must be supported, the missing codes
(ICD10) have to be completed in the corresponding
PheSO. Additionally, we develop a component to specify
and automatically integrate required code mappings into
the ontology. The changes in the EHR data model have
no impact on the phenotype specifications (PheSOs).
The PhenoMan contains a FHIR mapping module
implementing all required functionalities to extract rele-
vant information from FHIR resources. Supported re-
source types are offered for selection during the
phenotype specification and are saved in the correspond-
ing PheSO. If the EHR data model changes, it is only ne-
cessary to re-implement the mapping module. The
switch from FHIR r3 to r4 has already been done.
Conclusion
We developed a novel ontology-based method to model
phenotypes of living beings with the aim of automated
phenotype reasoning based on instance data (e.g., patient
data from EHR). Our solution includes an enhanced it-
erative reasoning process combining classification tasks
with mathematical calculations at runtime. This new ap-
proach can be used in clinical context, e.g., for support-
ing the diagnostic process, evaluating risk factors or
recruiting appropriate participants for clinical or epi-
demiological studies. About 20 phenotype models have
already been specified and the ontology as well as the
reasoning method were successfully evaluated.
Our approach has currently the following limitations:
1. only structured data can be considered (a NLP mod-
ule is upstream); 2. mathematical calculations are only
possible for individual data sets (no statistical evalua-
tions over multiple data sets).
An integration of more complex algorithms into the
reasoning process is possible and has to be investigated
in respect of accessing external libraries (e.g., R scripts).
The current formalism will be extended in the future to
optimise the implementation of selected desiderata
expounded by Mo et al. [7].
Abbreviations
COP: Core Ontology of Phenotypes; PheSO: Phenotype Specification
Ontology; PhenoMan: Phenotype Manager; NSiP class: Non-restricted single
phenotype class; RSiP class: Restricted single phenotype class; NCoP
Uciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 15 of 17
class: Non-restricted combined phenotype class; RCoP class: Restricted
combined phenotype class; NDeP class: Non-restricted derived phenotype
class; RDeP class: Restricted derived phenotype class; TO: Task ontology;
DO: Domain ontology; TLO: Top-level ontology
Acknowledgements
An earlier version of the paper has been presented at JOWO 2019 (Joint
Ontology Workshops) / ODLS (Ontologies and Data in Life Sciences) in Graz,
Austria.
This work was supported by the German Federal Ministry of Education and
Research as part of the projects SMITH (reference number: 01ZZ1803A) and
LHA (reference number: 031 L0026).
We acknowledge support from the German Research Foundation (DFG) and
Leipzig University within the program of Open Access Publishing.
Authors contributions
AU designed and implemented the COP, the PhenoMan API and the
PhenoMan Service. CB developed the PhenoMan Editor and made
substantial contributions to the design and implementation of COP and
PhenoMan. HH was responsible for project management, conception and
semantic foundation of developed ontologies. TK and FAM focus on the
SMITH, LHA and LIFE integration. The authors read and approved the final
manuscript.
Funding
This work was supported by the German Federal Ministry of Education and
Research as part of the projects SMITH (reference number: 01ZZ1803A) and
LHA (reference number: 031 L0026). Open Access funding enabled and
organized by Projekt DEAL.
Availability of data and materials
The PhenoMan Editor, developed ontologies and generated phenotype
representations (diagrams, spreadsheets) [38] used in this paper are publicly
available. Further software components are available from the corresponding
author on reasonable request. At the end of the SMITH project, the
consortium will take a final decision about licensing and usage of developed
software components.
Ethics approval and consent to participate
Not applicable.
More details about LIFE Adult participants, their invitation and consenting as
well as examinations, interviews, questionnaires and taken specimen can be
found in [31].
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Author details
1Institute for Medical Informatics, Statistics and Epidemiology (IMISE),
University of Leipzig, Leipzig, Germany. 2SMITH Consortium of the German
Medical Informatics Initiative, Leipzig, Germany. 3Growth Network CrescNet,
University of Leipzig, Leipzig, Germany. 4Faculty of Applied Computer and
Biological Sciences, University of Applied Sciences Mittweida, Mittweida,
Germany. 5LIFE Research Centre for Civilization Diseases, University of Leipzig,
Leipzig, Germany.
Received: 5 May 2020 Accepted: 3 November 2020
Grabar et al. Journal of Biomedical Semantics            (2020) 11:7 
https://doi.org/10.1186/s13326-020-00225-x
RESEARCH Open Access
CAS: corpus of clinical cases in French
Natalia Grabar1,2*, Clément Dalloux3 and Vincent Claveau3
Abstract
Background: Textual corpora are extremely important for various NLP applications as they provide information
necessary for creating, setting and testing those applications and the corresponding tools. They are also crucial for
designing reliablemethods and reproducible results. Yet, in some areas, such as themedical area, due to confidentiality
or to ethical reasons, it is complicated or even impossible to access representative textual data. We propose the CAS
corpus built with clinical cases, such as they are reported in the published scientific literature in French.
Results: Currently, the corpus contains 4,900 clinical cases in French, totaling nearly 1.7M word occurrences. Some
clinical cases are associated with discussions. A subset of the whole set of cases is enriched with morpho-syntactic
(PoS-tagging, lemmatization) and semantic (the UMLS concepts, negation, uncertainty) annotations. The corpus is
being continuously enriched with new clinical cases and annotations. The CAS corpus has been compared with
similar clinical narratives. When computed on tokenized and lowercase words, the Jaccard index indicates that the
similarity between clinical cases and narratives reaches up to 0.9727.
Conclusion: We assume that the CAS corpus can be effectively exploited for the development and testing of NLP
tools and methods. Besides, the corpus will be used in NLP challenges and distributed to the research community.
Keywords: Medical area, Natural language processing, Corpus with clinical cases, Morpho-syntactic and semantic
annotation, Sustainability, Reproducibility
Background
Textual corpora are central for various NLP applications
as they provide information necessary for creating, set-
ting, testing and validating these applications, the cor-
responding tools, and the results. Yet, in some areas,
due to confidentiality or to ethical reasons, it is compli-
cated or even impossible to access representative textual
data typically created and used by the actors of these
areas. For instance, medical and legal areas are concerned
with these issues: in the legal area, information on law-
suits and trials remains confidential, while in the medical
area, medical confidentiality must be respected by the
medical staff. In both situations, personal data cannot
be made publicly available, which prevents corpora from
*Correspondence: natalia.grabar@univ-lille.fr
Natalia Grabar, Clé Dalloux and Vincent Claveau contributed equally to this
work.
1CNRS, UMR 8163, F-59000 Lille, France
2Univ. Lille, UMR 8163 - STL - Savoirs Textes Langage, F-59000 Lille, France
Full list of author information is available at the end of the article
being released and makes experiments non-reproducible
by other researchers and with other methods. To face such
situations, Natural Language Processing (NLP) proposes
specific methods and tools. Hence, for several years now,
anonymization and de-identification methods and tools
have been made available and provide competitive and
reliable results [14] reaching up to 90% precision and
recall. But it may still be difficult to access de-identified
documents and use them for research. One reason is that
there is a risk of re-identification of people, and more
particularly of patients [5, 6] because medical histories
can be unique. In consequence, the application of de-
identification tools on personal data often does not permit
to make the data freely available and usable within the
research context.
Yet, there is a real need for the development of methods
and tools for several applications suited for such restricted
areas. For instance, in the medical area, it is impor-
tant to design suitable tools for information retrieval and
extraction, for recruiting patients for clinical trials, for
© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate
credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were
made. The images or other third party material in this article are included in the articles Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not included in the articles Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative
Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made
available in this article, unless otherwise stated in a credit line to the data.
Grabar et al. Journal of Biomedical Semantics            (2020) 11:7 Page 2 of 10
performing several other important tasks such as index-
ing, study of temporality, negation, etc. [713]. Another
important issue is related to the reliability of tools and
to the reproducibility of study results across similar data
from different sources. The scientific research and clin-
ical communities are indeed increasingly coming under
criticism for the lack of reproducibility in the biomedical
area [1416], but notice that, for instance, psychology is
concerned with this issue as well [1719]. The first step
towards the reproducibility of results is the availability of
freely usable tools and corpora. In the current contribu-
tion, we are mainly concerned with the construction of
freely available corpora for the medical domain. Yet, we
are aware that sharing tools and methods is also impor-
tant. We assume that availability of corpora may boost the
design and dissemination of other resources, methods and
tools for biomedical tasks and applications.
The purpose of our work is to introduce the CAS cor-
pus, that contains clinical cases in French such as those
published in scientific literature or used in the education
and training of medical students. In what follows, we first
present some existing studies onmedical corpora creation
(Existing work: freely available clinical corpora), high-
lighting corpora which are freely available for research.
We then present the methods used for building, annota-
tion and analysis of the CAS corpus with clinical cases in
French (Methods). The results are presented in Results
and discussed in Discussion. We conclude with some
directions for future work (Conclusion sections). The
work presented in this article is an extended and updated
version of our previous publication [20].
Existing work: freely available clinical corpora
Within the medical area, we can distinguish two main
types of medical corpora: scientific and clinical.
 Scientific corpora are issued from scientific
publications and reporting. Such corpora are
becoming increasingly available to researchers thanks
to recent and less recent initiatives dedicated to open
publication, such as those promoted by the NLM
(National Library of Medicine) through the PUBMED
portal1 and specifically dedicated to the biomedical
area, and by the HAL2 and ISTEX3 initiatives, which
provide generic portals for accessing scientific
publications from various areas, including medicine.
Such corpora contain scientific publications that
describe research studies: motivation, methods,
results and issues on precise research questions.
Other portals may also provide access to scientific
literature aimed at specific purposes, namely indexing
1https://www.ncbi.nlm.nih.gov/pubmed
2https://hal.archives-ouvertes.fr/
3https://www.istex.fr/
reliable literature, such as proposed by HON [21],
CISMEF [22], and other similar initiatives [23]. Some
existing scientific corpora also provide annotations
and categorizations, such as PoS-tagging [24] and
negation [25]. These are often built for the purposes
of shared tasks [26, 27].
 Clinical corpora are related to hospital and clinical
events of patients. Such corpora typically contain
documents that describe medical history of patients
and the medical care they are undergoing. This kind
of corpora is typically created and used in clinical
context as part of the healthcare process. Even after
de-identification, it is complicated to obtain free
access to this kind of medical data and, for this
reason, there are very few clinical corpora freely
available for research.
In our work, we are mainly interested in clinical cor-
pora: the proposed literature review of the existing work
is aimed at clinical corpora that are freely available for
research. We present here the main existing clinical cor-
pora:
 MIMIC (Medical Information Mart for Intensive
Care), now available in its third version, provides the
largest available set of structured and unstructured
clinical data in English. MIMIC III is a single-center
database comprising information pertaining to
patients admitted in critical care units at a large
tertiary care hospital. Those data include vital signs,
medications, laboratory measurements, observations
and notes charted by care providers, fluid balance,
procedure codes, diagnostic codes, imaging reports,
hospital length of stay, survival data, and more. The
database supports applications including academic
and industrial research, quality improvement
initiatives, and higher education coursework [28].
Those data are widely used by researchers, for
instance for predicting mortality [29, 30], for
diagnosis identification and encoding [31, 32], for
studies on temporality [33] or for identifying similar
clinical notes [34], to cite just a few existing studies.
Data from these corpora are also used in challenges,
such as i2b2, n2c2 and CLEF-eHEALTH.
 i2b2 (Informatics for Integrating Biology and the
Bedside)4 is an NIH-funded initiative promoting the
development and test of NLP tools for
English-language documents with the purpose of
healthcare improvement. In order to enhance the
ability of NLP tools to process fine-grained
information from clinical records, i2b2 challenges
provide sets of fully de-identified clinical notes
enriched with specific annotations [9, 11, 35], such as:
4https://www.i2b2.org/NLP/DataSets/Main.php
Grabar et al. Journal of Biomedical Semantics            (2020) 11:7 Page 3 of 10
de-identification, smoking status, medication-related
information, semantic relations between entities, or
temporality. The clinical corpora and their
annotations built for the i2b2 NLP challenges are
available now for general research purposes.
 n2c2 (National NLP Clinical Challenges),5 held in
2018 and 2019, also address the processing of
English-language clinical documents. These
challenges are dedicated to other typical tasks when
handling clinical documents: inclusion of patients in
clinical trials, detection of adverse-drug events,
computing of textual semantic similarity, concept
normalization, and extraction of family history.
 CLEF-eHEALTH challenges6 held in 2013 and 2014
provide annotations for disorder detection and
abbreviation normalization. In 2016 the focus was on
structuring Australian free-text nurse notes. Finally,
in 2016 and 2017 death reports in French, provided
by the CépiDc,7 have been processed for death cause
extraction.
 eHealth-KD 2019 challenge8 targets human language
modelling in a scenario in which electronic health
documents in Spanish could be machine readable
from a semantic point of view. The two proposed
tasks are: identification and classification of key
phrases, and detection of semantic relations between
these key phrases.
Finally, medical data, close to those handled in the clini-
cal context, can be found in clinical trials protocols. One
example is the corpus of clinical trials annotated with
information on numerical values in English [36], and on
negation in French and Brazilian Portuguese [37, 38].
Methods
We first describe the specificity of the sources and clinical
cases from which the CAS corpus was created (Building
the corpus), then the annotation rationale (Annotation
of the corpus), and the principles of its comparison with
similar clinical narratives from Rennes University Hospi-
tal (Comparison with clinical narratives sections).
Building the corpus
The CAS corpus in French contains clinical cases as
published in scientific literature, legal or training mate-
rial. Hence, it is built using material freely available
in online sources. The collected clinical cases are pub-
lished in different journals and websites from French-
speaking countries in various continents. Those clinical
5https://n2c2.dbmi.hms.harvard.edu/
6https://sites.google.com/site/shareclefehealth/
7http://www.cepidc.inserm.fr/
8https://knowledge-learning.github.io/ehealthkd-2019
cases are related to various medical specialties (e.g. cardi-
ology, urology, oncology, obstetrics, pulmonology, gastro-
enterology...).
The purpose of clinical cases is to describe clinical sit-
uations for real de-identified or fake patients. Common
clinical cases are typically part of education programs
used for training medical students, while rare cases are
usually shared through scientific publications to illustrate
less common clinical situations. As for clinical cases which
can be found in legal sources, they usually report on situ-
ations which became complicated due to various reasons
emanating from different healthcare levels: medical doc-
tor, healthcare team, institution, health system and their
interactions.
Similarly to clinical documents, the content of clinical
cases depends on the clinical situations that are illustrated,
and on the disorders, but also on the purpose of the pre-
sented cases: description of diagnoses, treatments or pro-
cedures, evolution, family history, adverse-drug reactions,
expected audience, etc.
Data in published clinical cases are de-identified by the
authors prior to their publication. Besides, publication
is usually done with the written permission of patients.
The case reports can be related to any medical situa-
tion (diagnosis, treatment, procedure, follow-up...), to any
specialty and to any disorder. The typical structure of
scientific publications with clinical cases starts by intro-
ducing the clinical situation, then one or more clinical
cases are presented to support the situation. Schemes,
imaging, examination results, patient history, lab results,
clinical evolution, treatment, etc. can also be provided for
the illustration of clinical cases. Finally, those clinical cases
are discussed. Hence, such cases may present an exten-
sive description of medical problems. Such publications
gather medical information related to clinical discourse
(clinical cases) and to scientific discourse (introduction
Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 
https://doi.org/10.1186/s13326-019-0218-0
RESEARCH Open Access
Combining lexical and context features
for automatic ontology extension
Sara Althubaiti1,2, S¸enay Kafkas1,2 , Marwa Abdelhakim1,2 and Robert Hoehndorf1,2*
Abstract
Background: Ontologies are widely used across biology and biomedicine for the annotation of databases. Ontology
development is often a manual, time-consuming, and expensive process. Automatic or semi-automatic identification
of classes that can be added to an ontology can make ontology development more efficient.
Results: We developed a method that uses machine learning and word embeddings to identify words and phrases
that are used to refer to an ontology class in biomedical Europe PMC full-text articles. Once labels and synonyms of a
class are known, we use machine learning to identify the super-classes of a class. For this purpose, we identify lexical
term variants, use word embeddings to capture context information, and rely on automated reasoning over
ontologies to generate features, and we use an artificial neural network as classifier. We demonstrate the utility of our
approach in identifying terms that refer to diseases in the Human Disease Ontology and to distinguish between
different types of diseases.
Conclusions: Our method is capable of discovering labels that refer to a class in an ontology but are not present in an
ontology, and it can identify whether a class should be a subclass of some high-level ontology classes. Our approach
can therefore be used for the semi-automatic extension and quality control of ontologies. The algorithm, corpora and
evaluation datasets are available at https://github.com/bio-ontology-research-group/ontology-extension.
Keywords: Disease ontology, Embeddings, Neural network
Background
The biomedical community has spent significant
resources to develop biomedical ontologies which con-
tain and define the basic classes and relations that occur
within a domain. Biomedical ontologies are developed by
domain experts and are often developed in conjunction
with the needs arising in literature-based curation of
biological databases.
Manual curation of databases based on literature is a
very time-consuming task due to the massive amounts of
literature, and automated methods have been developed
early on to aid in curation [1]. One of the key tasks in
computational support for literature curation is the auto-
matic concept recognition of mentions of ontology classes
in text [2]. An ontology class is an intensionally defined
*Correspondence: robert.hoehndorf@kaust.edu.sa
1Computational Bioscience Research Center, King Abdullah University of
Science and Technology, 23955-6900 Thuwal, Saudi Arabia
2Computer, Electrical and Mathematical Sciences and Engineering Division,
King Abdullah University of Science and Technology, 23955-6900 Thuwal,
Saudi Arabia
entity that has a formal descriptionwithin an ontology and
axioms that determine its relation with other classes [3]. In
natural language, multiple terms and phrases can be used
to refer to an ontology class [4], and the formal depen-
dencies within an ontology further determine whether a
term refers to a class or not (i.e., whether a term refers to
a particular class may depend on background knowledge,
in particular subclass relations, contained in an ontol-
ogy). For example, the Disease Ontology (DO) [5] declares
Prediabetes syndrome (DOID:11716) to be a subclass
of Diabetes mellitus (DOID:9351), and based on this
information we assume that any reference to, or mention
of, Prediabetes syndrome is also a reference to Diabetes
mellitus (with respect to DO).
There are several text mining systems designed for
ontology concept recognition in text. These methods are
either based on lexical methods and therefore applicable
to a wide range of ontologies [6, 7] or they are domain-
specific and rely on machine learning [8]. Text mining
© The Author(s). 2020 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 2 of 13
based-methods can also be used to automatically or semi-
automatically construct and extend ontologies [9, 10]. For
example, Lee et al. [11] focus on text mining of relations
that are asserted in text between mentions of ontology
classes that has been used to refine ontology classes in the
Gene Ontology (GO) [12]. Text mining can also be used to
suggest new subclasses and sibling classes in ontologies,
for exampleWächter and Schroeder [13] carried out a text
mining based-system from different text sources which is
used for extending OBO ontologies by semi-automatically
generating terms, definitions and parentchild relations.
Xiang et al. [14] have developed a pattern-based system
for generating and annotating a large number of ontology
terms, following ontology design patterns and providing
logical axioms that may be added to an ontology. Recently,
clustering based on statistical co-occurrence measures
were also used to extend ontologies [15].
Here, we introduce a novel method relying on machine
learning to identify whether a word used in text refers
to a class that could be included in a particular ontol-
ogy. Essentially, our method classifies terms to determine
if they are usually mentioned in the same context as the
labels and synonyms of classes in an ontology (which are
used as seeds to train the classifier); this classifier can then
be applied to unseen terms. Furthermore, our method can
also be used to expand ontologies by suggesting terms that
are mentioned within the same context as specific classes
in an ontology.
We demonstrate the utility of our method in identi-
fying words referring to diseases from DO in full text
articles. We select the DO because the labels and syn-
onyms of DO classes are relatively easy to detect in text
and a large number of computational methods rely on
access to a comprehensive disease ontology [1619]. Our
method achieves highly accurate (F-score > 90%) and
robust results, is capable of recognizing multiple different
classes including those defined formally through logical
operators, and combines dictionary-based and context-
based features; therefore, our method is also capable of
finding new words that refer to a class. We manually
evaluate the results and suggest several additions to the
DO.
Methods
Building a disease dictionary
We built a dictionary from the labels and synonyms
of classes in the Disease Ontology (DO), downloaded
on 5 February 2018 from http://disease-ontology.org/
downloads/. The dictionary consisted of 21,788 terms
belonging to 6,831 distinct disease classes from DO. We
utilized the dictionary with the Whatizit tool [20] and
annotated the ontology class mentions along with their
identifiers in approximately 1.6 million open access full-
text articles from the Europe PMC database [21] (http://
europepmc.org/ftp/archive/v.2017.06/) and generated a
corpus annotated with mentions of classes in DO. We
preprocessed the corpus by removing stop words such as
the, a, and is as well as some punctuation characters.
Generating context-based features
We use Word2Vec [22] to generate word embedding.
Specifically, we use a skip-gram model which aims to find
word representations that are useful for predicting the
surrounding words in a given sentence or a document
consisting of sequence of words; w1,w2, ...,wK . The objec-
tive is to maximize the average log probability using the
following formula:
V (w) = 1K
K?
k=1
K?
?c?j?c;j =0
log p(wK+j|wK ) (1)
where word vectors V (w) are computed by averaging over
the number of words K and c is the size of the training
context. We generated the word embedding by using the
default parameter settings of theWord2Vec gensim imple-
mentation: vector size (dimensionality) of 100, window
size 5, minimum occurrence count of 5, and we use a
skip-gram (sg) model.
Supervised training
We carried out a set of experiments to choose the optimal
training algorithm to design our model. In our experi-
ments we used default parameters for the training algo-
rithms but different hidden layers for Artificial Neural
Networks (ANNs) [23]. Our experiments show that the
ANN model outperforms an SVM model [24] (see Addi-
tional file 1: Table 1 for full details), and our model
performs best with 200 neurons in a single hidden layer
(we tested a single hidden layer with a size of 10, 50,
100, and 200 neurons). We report results accordingly to a
model with 200 neurons in the remainder of this work. In
ANNs, multiple neurons are organized in layers. Typically,
different layers perform different kinds of transforma-
tions on their inputs [25]. In our experiments, we used
an ANN with an input layer of different sizes, a single
hidden layer that uses a sigmoid activation function, and
an output layer that differs based on the experiment. We
train each classifier in a supervised manner, using 10-fold
stratified cross-validation. Additionally, we report testing
performance on an independent 20% testing set which
we generated by randomly removing data points before
training.
Recognizing ontology classes in text
We used two approaches to recognize the mention of
ontology classes in text. Our first approach relies solely on
labels and synonyms of the classes within a given ontol-
ogy O and can be used to determine whether a word refer
Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 3 of 13
to a class in O. We first obtain an ontology O in the Web
Ontology Language (OWL) [26] format and extract a list
of class labels and synonyms L from O; we further utilize
a text corpus T as input to our method. Then, we gener-
ate word embeddings (i.e., vector-space encodings of the
contexts in which a word occurs) for all words in our text
corpus T and train a supervised machine learning model
to classify whether a word refers to a class in O or not
(using the Ls words as positive training instances and all
others as negative instances).
Figure 1 illustrates the workflow of our first approach.
Our method is generic and can, in principle, be applied
to any ontology as long as the ontology provides labels
(or synonyms), these labels can be identified in text, and
the ontology from which the labels are extracted is more
or less limited to a single domain. For example, refer-
ence ontologies in the OBO Foundry [27] are usually
single domain ontologies and therefore suitable for our
method. Ontologies that would not be suitable are appli-
cation ontologies that cover multiple domains, such as the
Experimental Factor Ontology (EFO) [28] (although our
methods can be applied to parts of it). It is most useful to
extend an existing ontology with new labels, synonyms, or
classes.
In our second approach, we rely on annotations from
the Whatizit tool [20] to identify the mention of ontology
classes in text and determine their specific superclasses in
an ontology. Our approach takes an ontology O in OWL
format, a set of ontology classes S = {C1, ...,Cn}, and a
corpus of text T as inputs.
This approach first uses Whatizit as a named entity
recognition and normalization tool to normalize class
labels and synonyms in text by replacing all mentions
of a class with the class identifier (i.e., the class URI).
We annotate 15,183 distinct terms using Whatizit; the
total dictionary consists of 21,788 terms (derived from
the labels and synonyms of classes in DO). We then train
Word2Vec model that captures the context of the men-
tion of the class and generates a vector space embedding
for that class. Given such vector space embeddings for
a set of classes in O, we use the vector space embed-
dings as input to a machine learningmethod that classifies
whether another class appears in a similar context. We
use this method to determine if a class should belong the
superclass of C in O. Figure 2 illustrates the workflow of
this approach.
The main difference between the two approaches is that
the first approach broadly identifies terms or words that
refer to classes within a domain (as defined by the sum
of classes within an ontology) while the second approach
can determine whether a term or word refers to a class
that should appear as a subclass of a more specific ontol-
ogy class. Both methods generate seed words in text and
then use these seeds first to generate context-based fea-
tures (through Word2Vec) and use these context-based
features in a supervised machine learning classifier.
Manual analysis process
We manually evaluate some of our findings. The manual
evaluation is based on the medical expert knowledge of
the evaluator who is a trained clinician, and supplemented
by literature search to validate some findings or resolve
conflicts. Mainly, results were confirmed by searching
for review papers that characterize a condition. Overall,
Fig. 1 Label-based workflow. The workflow describes how words (in red) are classified as disease or other
Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 4 of 13
Fig. 2 Annotation-based workflow. In this workflow, we first normalize the mentions of disease classes in the corpus and then apply Word2Vec to
generate embeddings for classes, not merely words
manual curation following the suggestions by our classi-
fier took 10-15 min per sample (which included identify-
ing related classes in the DO and drafting an explanation
for cases which disagree with the DO).
Results
Broad classification of domain-specific terms: application
to diseases
Our method is a workflow that can be used to identify
whether a term or phrase commonly refers to a class
that may be included in a domain-specific ontology as a
label, synonym, or a new class. To achieve this goal, we
use the existing labels and synonyms within a domain-
ontology as seeds to train a machine learning classifier
that determines whether a new term is sufficiently similar
to an existing label or synonym and may therefore also be
included in the ontology. We represent terms primarily by
the context in which they occur within a large corpus of
text; we useWord2Vec [22] for this purpose.We then train
an Artificial Neural Network classifier in a supervised
manner to distinguish between the terms already included
within a domain ontology (and therefore expected to refer
to a particular kind of phenomena) and randomly chosen
terms not included in the ontology (and therefore most
likely not referring to a phenomenon within the domain
of the ontology).
We demonstrate our method using the Human Dis-
ease Ontology (DO) [5] and applying it to the terms
occurring in a large corpus of full-text biomedical articles
(see Methods). First, we tested whether our approach is
capable of identifying words that refer to the Disease class
(DOID:4), i.e., whether our method can detect terms
that refer to a disease. We generated word embeddings
for every disease terms and other words in our corpus of
full-text articles.
Figure 3 illustrates the distribution of the terms refer-
ring to a diseases in DO and other words mentioned in
our corpus which do not belong to DO using the t-SNE
dimensionality reduction [29]. We can see that the terms
are clearly different and should be separable through a
machine learning system.
Therefore, we trained a machine learning model to
recognize whether a word refers to the disease or not
using the word embeddings as input. We split the vec-
tor space embeddings into a training and testing dataset
and consider all embeddings referring to disease as pos-
itive instances and all others as negatives. We do not
apply any filtering before selecting the positive or negative
samples. We randomly select negatives equal to the num-
ber of positives (7,932 positives and 7,932 negatives). We
withhold 20% of randomly chosen positive and negative
instances for testing, train a model on the remaining 80%
through 10-fold cross validation, and report the perfor-
mance results on the 20% test set. Evaluated on the testing
set, we can distinguish between disease and non-disease
terms with an F-score of 95% and AUC of 96% (see Table 1
and Figure 4).
To better understand the source of errors and whether
our approach can be used to reliably extend ontolo-
gies (either with additional labels and synonyms, or new
Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 5 of 13
Fig. 3 a) The visualization of the embeddings using the t-SNE for binary-classification task b) The visualization of the embeddings using the t-SNE
for classifying infectious diseases. c) The visualization of the embeddings using the t-SNE for classifying anatomical diseases. d) The visualization of
the embeddings using the t-SNE for classifying the combination of infectious and anatomical diseases
classes), we performed a manual analysis on a set of 20
false positive samples out of 197 which are not the label or
synonym of a disease class DO but are classified as disease
by our classifier (see Table 2). We found that the majority
of the 20 false positive samples refer to either diseases or
phenotypes (where phenotypes are the observable char-
acteristics of an organism that may occur manifestations,
or signs and symptoms, of a disease, but do not constitute
a disease on its own). For example, Aphthosis is a pre-
diction of our method which refers to a human disorder
that is not currently in the DO; the majority of false pos-
itives are disease-related terms that do not explicitly refer
to a disease. For example, we predictedmal-absorption as
a disease term which may refer to a phenotype in some
contexts. Our findings indicate that an ANN classifier
can identify known terms referring to diseases, and can
further suggest novel terms which may prove useful for
ontology development and extension.
Fine-grained classification: distinguishing between groups
of diseases
As our method showed capability to identify terms refer-
ring to a disease, we next tested whether our method can
also distinguish between different types of diseases. For
this purpose, we used the embeddings generated from a
pre-processed corpus in which we normalize all mentions
Table 1 F-score and AUC for our four experiments using different hidden layer sizes
Classification Hidden layer sizes 10 50 100 200
Number of classes F-score AUC F-score AUC F-score AUC F-score AUC
Diseases 2 94.65% 95.31% 94.83% 95.97% 95.32% 96.06% 94.49% 95.99%
Infectious disease 5 95.65% 95.01% 96.01% 95.74% 95.43% 95.22% 95.68% 96.42%
Anatomical disease 13 69.18% 77.22% 70.15% 80.24% 70.20% 76.98% 72.00% 85.11%
Infectious + anatomical diseases 17 71.07% 84.75% 73.13% 84.03% 72.61% 84.98% 72.67% 83.66%
The values in bold represent the highest AUC and F-score within each experiments
Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 6 of 13
Fig. 4 ROC curves for each experiment (Diseases, Infectious disease, Anatomical disease and a combination of Infectious disease + Anatomical disease)
of a disease in our corpus using Whatizit tool. The dis-
ease dictionary that we utilized with Whatizit includes a
total of 21,788 terms (labels and synonyms) from DO. We
found that 15,183 of these 21,788 terms appeared in our
corpus and we generate an embedding vector for each of
them. We then first trained a neural network model to
recognize whether a disease-term refers to the Infectious
Disease (DOID:0050117) class or not, and furthermore
whether our method is able to distinguish between the
four different types of infectious disease in DO (i.e., bac-
terial, fungal, parasitic, or viral infectious disease). As
training data, we used the word embeddings generated for
DO classes, and we used the Elk reasoner to split them
into four types of infectious diseases, and an additional
class for diseases that are not a subclass of Infectious Dis-
ease in DO. We randomly select 20% of the disease in
DO as validation set and train the neural network classi-
fier using 10-fold cross-validation on the remaining 80%
to separate diseases into one of the five classes (non-
infectious, bacterial, fungal, parasitic and viral infections).
Table 1 shows the performance achieved on the validation
set.While the performance is less than predicting whether
a term refers to a disease, our classifier can distinguish
between specific disease classes.
We manually analyzed a set of 20 false positive samples
out of 38 which are not a subclass of Infectious disease in
the DO but are classified as infectious by our classifier (see
Table 3). We found that 7 of these 20 cases can be sug-
gested to be subclasses of the specific infectious disease
they have been classified with but do not have a subclass
relation asserted or inferred in DO. For example, the term
syphilitic meningitis (DOID:10073) is a disease that our
method classify as a bacterial infectious disease but it is
not classified as infectious in the DO.
Moreover, to test the strength of our method to distin-
guish between disease classes, we further trained a neural
network model to distinguish between the 12 different
subclasses of Disease of anatomical entity (DOID:7), as
well as an additional class for diseases not classified as
subclasses of Disease of anatomical entity. We used the
Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 7 of 13
Table 2 Manually analyzed disease terms predicted as disease
Term Manual analysis result Explanation for the suggested diseases
FACTO other -
leucoencephalopathy other -
Aphthosis Disease A disease refers to a condition with repetitive mucosal ulcers
[30, 31].
Desmoid other -
metapneumovirus other -
Tracheobronchomalacia Disease A rare condition with abnormal flaccidity of both the trachea
and the bronchi which results in possibility of narrowing or
collapse of the airway [3234].
RESLES Disease A rare condition characterized by transient lesions in the cen-
tral part of the splenium of the corpus callosum (SCC), followed
by complete reversibility on follow-up magnetic resonance
imaging (MRI) after a variable period. It coincides with different
diseases [35, 36].
mal-absorption other -
acroparesthesias other -
limb-shaking other -
pineocytomas Disease A rare disease that has an Orphanet ID: ORPHA:251912. It is
one of the pineal parenchymal tumors and is considered the
least aggressive one [37, 38].
hypomineralisation other -
neurognathostomiasis Disease It is a severe form of human gnathostomiasis, DOID:11379,
which can lead to disease and death, it involves the nervous
system [3941].
Metastasis other -
myelomatosis Disease A type of cancer that begins in plasma cells that produce anti-
bodies. It could be one of the synonyms of multiple myeloma
DOID:9538 [42, 43].
AMRF Disease An OMIM disease, OMIM:254900 [44].
arthralgia other -
fibrodentinoma Disease Fibrodentinoma is a benign odontogenic tumor that occurs
in children and young adults. The disease name usually is
represented as Ameloblastic Fibrodentinoma [45, 46].
infantile-ataxia other -
knowlesi other -
The terms in bold represent the correctly validated terms (by a clinician) that classified as diseases terms using our method (in Diseases classification experiment).
same method to split the classes in training and test set as
before. Results are shown in Table 1 and demonstrate that
our method can also be useful to classify diseases in their
anatomical sub-systems.
We manually analyzed a set of 20 false positive samples
out of 127 which are not a subclass of Anatomical dis-
ease in the DO but are classified as being a subclass of a
particular anatomical system disease by our classifier (see
Table 4). We found that 12 of the 20 false positives can be
suggested to be subclasses of the specific anatomical sys-
tem disease they have been classified with but do not have
such a subclass relation asserted or inferred in DO. For
example, we classify Narcolepsy (DOID:8986) as a Ner-
vous system anatomical disease, and this may be added as
a new subclass axiom to DO.
As it is often inconvenient to train separate classifiers,
we also combined both tasks and trained a multi-class
classifier to classify disease classes either as infectious or
anatomical, or as other disease. We evaluate the perfor-
mance of this combined model (see Table 1), and our
machine learning system achieves an AUC up to 84% (see
Figure 4). These results demonstrate it may be possible to
identify new subclasses, although the performance drops
when we increase the complexity of the classification
problem by distinguishing between more subclasses.
Discussion
We developed a method to automatically expand ontolo-
gies in the biomedical domain with new classes, syn-
onyms, or axioms. We demonstrate the utility of our
Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 8 of 13
Table 3 Sample of manually analyzed disease terms predicted as infectious disease
Disease terms Ontology class assigned
by ANN
Manual analysis result Suggested additional
classification
DOID Explanation
Pelizaeus-Merzbacher
disease
Viral infectious disease Non-infectious (inherited
disorder)
- - -
Kaposis sarcoma Viral infectious disease Viral infectious disease herpes simplex DOID:8566 The disease is caused by
Human herpesvirus 8
which is Herpesviridae
infection.
maxillary sinusitis Bacterial infectious disease Bacterial infectious disease
(usually start viral and
progress to either
bacterial or fungal)
- - It is an infection in the
maxillary sinuses which
could be due to different
etiology, one of them is
bacterial [47].
keratosis follicularis Bacterial infectious disease Non-infectious (genetic
disease)
- - -
chronic rheumatic
pericarditis
Viral infectious disease The condition is triggered
by autoimmune reaction
to infection, mainly group
A streptococci.
- - -
gastroparesis Viral infectious disease In most cases the nerve is
damaged by diabetes or
surgery, however, a viral
infection might be a cause
- - A condition in which the
stomach suffers from
paresis that affects the
food movement to the
small intestine [48, 49].
osmotic diarrhea Bacterial infectious disease symptom - - -
familial cold
autoinflammatory
syndrome
Viral infectious disease Non-infectious (inherited
disease)
- - -
angular cheilitis Fungal infectious disease Etiology is controversial,
most commonly fungal or
bacterial.
- - Ambiguous.
Binder syndrome Viral infectious disease Congenital disease - - -
hypohidrosis Bacterial infectious disease Multi-causal - - -
Sjogrens syndrome Viral infectious disease autoimmune disease - - -
median rhomboid
glossitis
Fungal infectious disease Etiology is controversial,
however it is considered
as a variant of orallesion
associated with candida
infection [50].
- - Ambiguous.
Goodpasture syndrome Viral infectious disease autoimmune disease - - -
syphilitic meningitis Bacterial infectious disease Bacterial infectious disease syphilis DOID:4166 Considering the same
concept of etiology, both
diseases are caused by
bacterial infection
(Treponema pallidum).
acute diarrhea Viral infectious disease symptom - - -
WHIM syndrome Bacterial infectious disease Congenital disease - - -
erythrasma Fungal infectious disease Bacterial infection disease - - -
chronic wasting disease Parasitic infectious disease Neurodegenerative
disorder
- - -
scarlet fever Bacterial infectious disease Bacterial infectious disease rheumatic fever DOID:1586 The disease is caused by
Group A bacteria of the
genus Streptococcus,
same causative agent for
Rheumatic fever.
The terms in bold represent the correctly validated terms (by a clinician) that classified as infectious diseases terms using our method (in Infectious disease classification
experiment).
Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 9 of 13
Table 4 Sample of manually analyzed disease terms classified as affecting particular anatomical systems
Disease terms Ontology
class
Ontology
class
assigned by
ANN
Manual analysis
result
Suggested
additional
classification
DOID Explanation
Timothy
syndrome
genetic
disease
cardiovascular
system
disease
Cannot specify
(affect multiple
parts)
- - -
Familial periodic
paralysis
disease of
metabolism
cardiovascular
system
disease
musculoskeletal
system disease
- - -
Hyperprolactinemiadisease of
metabolism
endocrine
system
disease
endocrine system
disease
pituitary gland
disease
DOID:53 The pituitary gland is
the endocrine gland
responsible for
secreting prolactin.
Angiokeratoma
circumscriptum
disease of
cellular
proliferation
gastrointestinal
system
disease
cardiovascular
system disease
- - -
Zollinger-
Ellison
syndrome
syndrome gastrointestinal
system
disease
gastrointestinal
system disease
peptic ulcer disease DOID:750 It is a disease that
affects either
pancreas, duodenum,
or both of them. Both
organs are pats of the
GIT system. The
disease pathology
is mainly excessive
gastrin secretion with
subsequent peptic
ulcers.
Polycystic liver
disease
genetic
disease
gastrointestinal
system
disease
gastrointestinal
system disease
liver disease DOID:409 It is a genetic disorder
that affects primarily
the liver.
Bilirubin
metabolic
disorder
disease of
metabolism
hematopoietic
system
disease
hematopoietic
system disease
kernicterus due to
isoimmunization
DOID:12043 Bilirubin disorder
could be a result of
blood pathology,
same as for the
mentioned
classification
DOID:12043.
Alpha
thalassemia
genetic
disease
hematopoietic
system
disease
hematopoietic
system disease
hemoglobinopathy DOID:2860 The disease is mainly a
hemoglobin
disorder with
hematological
phenotypes.
Kabuki syndrome syndrome immune sys-
tem disease
Not anatomical
- multisystems
- - -
Amyloidosis disease of
metabolism
immune sys-
tem disease
Not anatomical -
multisystems
- - -
Fatty liver disease disease of
metabolism
musculoskeletal
system
disease
gastrointestinal
system disease
- - -
Renal-hepatic-
pancreatic
dysplasia
physical
disorder
musculoskeletal
system
disease
Cannot specify
(affect multiple
parts)
- - -
Radioulnar syn-
ostosis
physical
disorder
musculoskeletal
system
disease
musculoskeletal
system disease
bone development
disease/Synostosis
DOID:0080006/
DOID:11971
There is already an
entity in the DO for
synostosis under
bone development
disease.
Hypophosphatasia genetic
disease
musculoskeletal
system
disease
musculoskeletal
system disease
bone remodeling
disease
DOID:0080005 We could suggest
an additional
classification based
on the main affected
system. Our
suggestive
classification is
musculoskeletal since
Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 10 of 13
Table 4 Sample of manually analyzed disease terms classified as affecting particular anatomical systems (Continued)
the disease is mainly
affecting
mineralization of
the bone with
phenotypes similar to
those of Rickets
DOID:10609.
Narcolepsy disease of mental
health
nervous
system
disease
nervous system
disease
* * *
Aceruloplasminemia disease of metabolism nervous
system
disease
nervous system
disease
neurodegeneration
with brain iron
accumulation
DOID:0110734 The disease main
pathophysiology is
either the absence
or dysfunction of
ceruloplasmin with
subsequent iron
accumulation in
various organ, mainly
the brain.
Glomangiomatosis disease of cellular pro-
liferation
nervous
system
disease
cardiovascular
system disease
- - -
Deafness-dystonia-
optic neuronopathy
syndrome
disease of metabolism nervous sys-
tem disease
nervous system
disease
nervous system
disease; since it
covers many
subclasses to
which we can map
many aspects of
this disease
DOID:863 The diseases
phenotypes reflect
neurological affection
ofmultiple parts in the
nervous system.
Trophoblastic
neoplasm
disease of cellular
proliferation
reproductive
system
disease
reproductive
system disease
Female
reproductive organ
cancer
DOID:120 The term refers to the
group of
malignant neoplasms
that consist of
abnormal
proliferation of
trophoblastic tissues
similar to
choriocarcinoma
DOID:3596 and
gestational
trophoblastic
neoplasia
DOID:3590.
Cryptorchidism physical disorder reproductive
system
disease
reproductive
system disease
testicular disease DOID:2519 The term refers to
undescended testicle.
*Nacrolepsy: is classified as a sleep disorder which is correct, however, the class itself is a subclass to mental disorders. Since there are some neurological disorders that have
shown a strong association with sleep disorder such as: neurodenegrative disorders such as tauopathy which involve Alzheimers diseases (DOID:10652) [51],
synucleinopathy which involve Parkinsonism (DOID:14330) [52], and Genetic neurodegenerative disorders such as Machado-Joseph disease (DOID:1440) [53] or
Huntingtons disease (DOID:12858) [54]. We suggest a new classification in which sleep disorders may also be a subclass of nervous system diseases (neurodegenerative
disorder) [55] The terms in bold represent the correctly validated terms (by a clinician) that classified as anatomical diseases terms using our method (in Anatomical disease
classification experiment).
approach on the DO [5] which is widely used in biomed-
ical research [56]. As case studies, we focused on two
high-level classes in the DO: Infectious Diseases and
Anatomical Diseases. We have evaluated our method both
using common performance measures in machine learn-
ing as well as through manually investigating some of the
predicted false positives.
When applying our method to the DO, our false positive
predictions often include phenotypes or, in some cases,
pathogens. It is well-established that it is challenging to
distinguish between diseases and phenotypes in litera-
ture [5759], as evidenced by the large overlap between
disease ontologies and phenotype ontologies [19]. Simi-
larly, diseases and pathogens can often have very similar
names [60, 61], thereby making it challenging to distin-
guish between them. While a disease is defined as the
structural or functional disorder that usually results in
symptoms, signs and physical or chemical changes, phe-
notype refers to observable characteristics of an organism
and may be a part of a disease manifestation. Phenotype
Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 11 of 13
terms cover disease symptoms, signs and the investiga-
tional results that might be related to that disease. Some
phenotypic terms are more diverse; for example, con-
genital hemolytic anemia is a form of hemolytic anemia
with congenital onset. The term is included in both the
Human Phenotype Ontology (HP) (HP:0004804) and
disease ontology (DOID:589). From a clinical point of
view, it could be a type of disease under the umbrella
of hemolytic disorders with a congenital onset; however,
congenital hemolytic anemia may also be a phenotype for
certain diseases. For this reason, deciding on some terms
to be identified either as phenotypes or diseases can be
complex, challenging, and context-dependent.
Another limitation of our method is the use of theWha-
tizit tool [20] to detect and normalize mentions of ontol-
ogy classes in text. In our first use-case  the extension
of ontologies with new labels and synonyms  we classify
terms that occur in text without relying on any prior text
processing which has some drawbacks such as considering
a word as disease name within a general context. We use
Whatizit for our second use-case  the detection of sub-
class axioms  while the performance of Whatizit is less
than domain- and task-specific named entity recognition
and normalization tools [62], Whatizits key advantage
is that it is a lexical, rule-based method that does not
require any training and is able to recognize multi-word
terms.Whatizit can therefore be applied to a wide range of
ontologies without the need to generate a training dataset.
To evaluate the performance of Whatizit, we tested it on
the NCBI disease corpus [16] using their test set con-
taining 100 abstracts. In our evaluation, Whatizit has a
precision of 75% and recall of 15% and an F-score of 26%
with an accuracy of 90% (see Additional file 2). One of the
reasons for the low recall is the number of diseases which
are included in theMedical Subject Headings (MeSH) [63]
or the Online Mendelian Inheritance in Man (OMIM)
[64] vocabulary but not in DO. Furthermore, Whatizit
ignores many disease abbreviations since they are not
included in DO (and therefore in the vocabulary used by
Whatizit).
Conclusions
We presented a general method for semi-automatically
extending ontologies with new labels, synonyms, classes,
or some general subclass axioms. Our approach is based
on machine learning algorithms utilizing vector repre-
sentation of the ontology classes generated from full text
articles. We demonstrated the utility of our approach on
the Human Disease Ontology (DO), specifically by find-
ing new candidate classes, labels, and synonyms to add
to DO such as Aphthosis, and by identifying new axioms
that relate disease classes to their infectious agent or
anatomical systems. Our method can help to improve
the quality and coverage of ontologies in the ontology
development process by automatically suggesting terms
to include (either as labels of new classes or synonyms of
existing classes) and suggesting missing subclass axioms.
In the future, we plan to expand our study to other
ontologies and to defined classes to further analyze its
robustness.
Supplementary information
Supplementary information accompanies this paper at
https://doi.org/10.1186/s13326-019-0218-0.
Additional file 1: Different conducted experiments based on different
classification tasks.
Additional file 2: The evaluation of analyzing NCBI abstracts annotated
using Whatizit tool.
Abbreviations
ANNs: Artificial neural networks; AUC: Area under curve; DO: Disease ontology;
EFO: Experimental factor ontology; GO: Gene ontology; HP: human phenotype
ontology; MeSH: Medical subject headings; OMIM: Online mendelian
inheritance in man; OWL: Web ontology language; ROC: Receiver operating
characteristic; Sg: Skip-gram
Acknowledgement
Not applicable.
Authors contributions
RH conceived of the study; S¸K, SA, and RH designed the experiments. SA
conducted the experiments, implemented the software and evaluated the
results. MA manually evaluated the results. SA drafted the manuscript; S¸K and
RH contributed to revising the manuscript. All authors have read and
approved the final version of the manuscript.
Funding
This work was supported by funding from King Abdullah University of Science
and Technology (KAUST) Office of Sponsored Research (OSR) under Award
NoU?RF/1/3454-01-01, FCC/1/1976-08-01, and FCS/1/3657-02-01.
Availability of data andmaterials
All source code developed for this study is available from https://github.com/
bio-ontology-research-group/ontology-extension.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Received: 2 October 2018 Accepted: 24 December 2019
REVIEW Open Access
Natural language processing algorithms
for mapping clinical text fragments onto
ontology concepts: a systematic review
and recommendations for future studies
Martijn G. Kersloot1,2* , Florentien J. P. van Putten1, Ameen Abu-Hanna1, Ronald Cornet1 and Derk L. Arts1,2
Abstract
Background: Free-text descriptions in electronic health records (EHRs) can be of interest for clinical research
and care optimization. However, free text cannot be readily interpreted by a computer and, therefore, has
limited value. Natural Language Processing (NLP) algorithms can make free text machine-interpretable by
attaching ontology concepts to it. However, implementations of NLP algorithms are not evaluated
consistently. Therefore, the objective of this study was to review the current methods used for developing
and evaluating NLP algorithms that map clinical text fragments onto ontology concepts. To standardize the
evaluation of algorithms and reduce heterogeneity between studies, we propose a list of recommendations.
Methods: Two reviewers examined publications indexed by Scopus, IEEE, MEDLINE, EMBASE, the ACM Digital
Library, and the ACL Anthology. Publications reporting on NLP for mapping clinical text from EHRs to
ontology concepts were included. Year, country, setting, objective, evaluation and validation methods, NLP
algorithms, terminology systems, dataset size and language, performance measures, reference standard,
generalizability, operational use, and source code availability were extracted. The studies objectives were
categorized by way of induction. These results were used to define recommendations.
Results: Two thousand three hundred fifty five unique studies were identified. Two hundred fifty six studies
reported on the development of NLP algorithms for mapping free text to ontology concepts. Seventy-seven
described development and evaluation. Twenty-two studies did not perform a validation on unseen data and
68 studies did not perform external validation. Of 23 studies that claimed that their algorithm was
generalizable, 5 tested this by external validation. A list of sixteen recommendations regarding the usage of
NLP systems and algorithms, usage of data, evaluation and validation, presentation of results, and
generalizability of results was developed.
(Continued on next page)
© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if
changes were made. The images or other third party material in this article are included in the article's Creative Commons
licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons
licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain
permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the
data made available in this article, unless otherwise stated in a credit line to the data.
* Correspondence: m.g.kersloot@amsterdamumc.nl
1Amsterdam UMC, University of Amsterdam, Department of Medical
Informatics, Amsterdam Public Health Research Institute Castor EDC, Room
J1B-109, PO Box 22700, 1100 DE Amsterdam, The Netherlands
2Castor EDC, Amsterdam, The Netherlands
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 
https://doi.org/10.1186/s13326-020-00231-z
(Continued from previous page)
Conclusion: We found many heterogeneous approaches to the reporting on the development and evaluation of NLP
algorithms that map clinical text to ontology concepts. Over one-fourth of the identified publications did not perform
an evaluation. In addition, over one-fourth of the included studies did not perform a validation, and 88% did not
perform external validation. We believe that our recommendations, alongside an existing reporting standard, will
increase the reproducibility and reusability of future studies and NLP algorithms in medicine.
Keywords: Ontologies, Entity linking, Annotation, Concept mapping, Named-entity recognition, Natural language
processing, Evaluation studies, Recommendations for future studies
Background
One of the main activities of clinicians, besides providing
direct patient care, is documenting care in the electronic
health record (EHR). Currently, clinicians document clin-
ical findings and symptoms primarily as free-text descrip-
tions within clinical notes in the EHR since they are not
able to fully express complex clinical findings and nuances
of every patient in a structured format [1, 2]. These free-
text descriptions are, amongst other purposes, of interest
for clinical research [3, 4], as they cover more information
about patients than structured EHR data [5]. However,
free-text descriptions cannot be readily processed by a
computer and, therefore, have limited value in research
and care optimization.
One method to make free text machine-processable is
entity linking, also known as annotation, i.e., mapping
free-text phrases to ontology concepts that express the
phrases meaning. Ontologies are explicit formal specifica-
tions of the concepts in a domain and relations among
them [6]. In the medical domain, SNOMED CT [7] and
the Human Phenotype Ontology (HPO) [8] are examples
of widely used ontologies to annotate clinical data. After
the data has been annotated, it can be reused by clinicians
to query EHRs [9, 10], to classify patients into different
risk groups [11, 12], to detect a patients eligibility for clin-
ical trials [13], and for clinical research [14].
Natural Language Processing (NLP) can be used to
(semi-)automatically process free text. The literature indi-
cates that NLP algorithms have been broadly adopted and
implemented in the field of medicine [15, 16], including
algorithms that map clinical text to ontology concepts
[17]. Unfortunately, implementations of these algorithms
are not being evaluated consistently or according to a pre-
defined framework and limited availability of data sets and
tools hampers external validation [18].
To improve and standardize the development and evalu-
ation of NLP algorithms, a good practice guideline for
evaluating NLP implementations is desirable [19, 20].
Such a guideline would enable researchers to reduce the
heterogeneity between the evaluation methodology and
reporting of their studies. Generic reporting guidelines
such as TRIPOD [21] for prediction models, STROBE
[22] for observational studies, RECORD [23] for studies
conducted using routinely-collected health data, and
STARD [24] for diagnostic accuracy studies, are available,
but are often not used in NLP research. This is presum-
ably because some guideline elements do not apply to
NLP and some NLP-related elements are missing or un-
clear. We, therefore, believe that a list of recommenda-
tions for the evaluation methods of and reporting on
NLP studies, complementary to the generic reporting
guidelines, will help to improve the quality of future
studies.
In this study, we will systematically review the
current state of the development and evaluation of
NLP algorithms that map clinical text onto ontology
concepts, in order to quantify the heterogeneity of
methodologies used. We will propose a structured list
of recommendations, which is harmonized from exist-
ing standards and based on the outcomes of the re-
view, to support the systematic evaluation of the
algorithms in future studies.
Methods
This study consists of two phases: a systematic review of
the literature and the formation of recommendations
based on the findings of the review.
Literature review
A systematic review of the literature was performed
using the Preferred Reporting Items for Systematic re-
views and Meta-Analyses (PRISMA) statement [25].
Search strategy and study selection
We searched Scopus, IEEE, MEDLINE, EMBASE, the As-
sociation for Computing Machinery (ACM) Digital Library,
and the Association for Computational Linguistics (ACL)
Anthology for the following keywords: Natural Language
Processing, Medical Language Processing, Electronic Health
Record, reports, charts, clinical notes, clinical text, medical
notes, ontolog*, concept*, encod*, annotat*, code, and cod-
ing. We excluded the words reports and charts in the
ACL and ACM databases since these databases also contain
publications on non-medical subjects. The detailed search
strategies for each database can be found in Additional file
2. We searched until December 19, 2019 and applied the
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 2 of 21
filters English and has abstract for all databases. More-
over, we applied the filters Medicine, Health Professions,
and Nursing for Scopus, the filters Conferences, Jour-
nals, and Early Access Articles for IEEE, and the filter
Article for Scopus and EMBASE. EndNote X9 [26] and
Rayyan [27] were used to review and delete duplicates.
The selection process consisted of three phases. In the
first phase, two independent reviewers with a Medical
Informatics background (MK, FP) individually assessed
the resulting titles and abstracts and selected publica-
tions that fitted the criteria described below.
Inclusion criteria were:
 Medical language processing as the main topic of
the publication
 Use of EHR data, clinical reports, or clinical notes
 Algorithm performs annotation
 Publication is written in English
Fig. 1 PRISMA flow diagram
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 3 of 21
Some studies do not describe the application of NLP in
their study by only listing NLP as the used method, instead
of describing its specific implementation. Additionally,
some studies create their own ontology to perform NLP
tasks, instead of using an established, domain-accepted
ontology. Both approaches limit the generalizability of the
studys methods. Therefore, we defined the following exclu-
sion criteria:
 Implementation was not described
 Implementation does not use an existing established
ontology for encoding
Table 1 Induced objective tasks with their definition and an example
Induced NLP task(s) Description Example
Concept detection 1 Assign ontology concepts to phrases in free
text (i.e., entity linking or annotation)
Systolic blood pressure can be represented as SNOMED-CT
concept 271649006 | Systolic blood pressure (observable entity) |
Event detection Detect events in free text Patient visited the outpatient clinic in January 2020 is an
event of type Visit.
Relationship detection Detect semantic relationships between
concepts in free text
The concept Lung cancer in This patient was diagnosed with
recurrent lung cancer is related to the concept Recurrence.
Text normalization Transform free text into a single canonical
form
This patient was diagnosed with influenza last year. becomes
This patient be diagnose with influenza last year.
Text summarization Create a short summary of free text and
possible restructure the text based on this
summary
Last year, this patient visited the clinic and was diagnosed with
diabetes mellitus type 2, and in addition to his diabetes, the
patient was also diagnosed with hypertension becomes
Last year, this patient was diagnosed with diabetes mellitus
type 2 and hypertension.
Classification Assign categories to free text A report containing the text This patient is not diagnosed
yet will be assigned to the category Undiagnosed.
Prediction Create a predictive model based on free text Predict the outcome of the APACHE score based on the
(free-text) content in a patient chart.
Identification Identify documents (e.g., reports or patient
charts) that match a specific condition
based on the contents of the document
Find all patient charts that describe patients with hypertension
and a BMI above 30.
Software development Develop new or build upon existing NLP
software
A new algorithm was developed to map ontology concepts
to free text in clinical reports.
Software evaluation Evaluate the effectiveness of NLP software The mapping algorithm has an F-score of 0.874.
1.Also known as Medical Entity Linking and Medical Concept Normalization
Table 2 Induced objective categories with their definition and associated NLP task(s)
Induced category Induced NLP task(s) Definition
Computer-assisted coding Concept detection Perform semi-automated annotation (i.e., with a human in the loop)
Information comparison Concept detection
Event detection
Relationship detection
Compare extracted structured information to information available in free-text form
Information enrichment Concept detection
Event detection
Relationship detection
Text normalization
Text summarization
Extract structured information from free text and attach this new information to the source
Information extraction Concept detection
Event detection
Relationship detection
Extract structured information from free text
Prediction Classification
Prediction
Identification
Use structured information to classify free-text reports, predict outcomes, or identify cases
Software development
and evaluation
Software development
Software evaluation
Develop new NLP software or evaluate new or existing NLP software
Text processing Text normalization
Text summarization
Transform free text into a new, more comprehensible form
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 4 of 21
Ta
b
le
3
In
cl
ud
ed
pu
bl
ic
at
io
ns
an
d
th
ei
r
fir
st
au
th
or
,y
ea
r,
tit
le
,a
nd
co
un
tr
y
A
ut
ho
r
Y
ea
r
C
ou
nt
ry
C
ha
lle
ng
e
In
d
uc
ed
ob
je
ct
iv
e
D
at
a
or
ig
in
D
at
as
et
D
at
a
la
ng
ua
g
e
U
se
d
sy
st
em
Te
rm
.S
ys
.
In
us
e
So
ur
ce
co
d
e
Re
f
A
fs
ha
r
20
19
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
C
lin
ic
al
D
at
a
W
ar
eh
ou
se
D
at
a
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
(C
PT
,H
C
PC
S,
IC
D
-
10
,I
C
D
10
C
M
/
IC
D
9C
M
,
LO
IN
C
,M
eS
H
,S
N
O
M
ED
-
C
T,
Rx
N
or
m
)
N
ot
lis
te
d
N
o,
on
ly
lin
ks
to
cT
A
KE
S
so
ur
ce
co
de
[2
9]
A
ln
az
za
w
i
20
16
U
K
N
o
In
fo
rm
at
io
n
en
ric
hm
en
t
Ph
en
oC
H
F
co
rp
us
1
Ex
is
tin
g
En
gl
is
h
Ex
is
tin
g
U
M
LS
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[3
0]
A
tu
tx
a
20
18
Sp
ai
n
N
o
In
fo
rm
at
io
n
en
ric
hm
en
t
EH
R
do
cu
m
en
ts
O
w
n
Sp
an
is
h
N
ew
IC
D
(S
N
O
M
ED
-C
T
fo
r
no
rm
al
iz
at
io
n)
N
ot
ye
t,
ai
m
to
em
be
d
it
in
hu
m
an
-s
up
er
vi
se
d
lo
op
N
ot
lis
te
d
[3
1]
Ba
rr
et
t
20
13
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Pa
lli
at
iv
e
ca
re
co
ns
ul
t
le
tt
er
s
O
w
n
En
gl
is
h
N
ew
SN
O
M
ED
C
T
N
ot
lis
te
d
N
o,
bu
t
pl
an
ne
d
[3
2]
Be
ck
er
20
16
G
er
m
an
y
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Sh
A
Re
/C
LE
F
co
rp
us
(2
01
3)
2
Ex
is
tin
g
G
er
m
an
Ex
is
tin
g
SN
O
M
ED
C
T
(E
ng
lis
h)
,
U
M
LS
(G
er
m
an
)
N
ot
ye
t,
st
ill
un
de
r
de
ve
lo
pm
en
t
N
ot
ap
pl
ic
ab
le
[3
3]
Be
ck
er
20
19
G
er
m
an
y
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
C
lin
ic
al
no
te
s
of
pa
tie
nt
s
w
ith
kn
ow
n
co
lo
re
ct
al
ca
nc
er
O
w
n
G
er
m
an
N
ew
(+
ex
is
tin
g)
U
M
LS
Ye
s,
le
d
to
im
pr
ov
ed
qu
al
ity
of
ca
re
fo
r
co
lo
re
ct
al
pa
tie
nt
s
N
ot
lis
te
d
[3
4]
Be
ja
n
20
15
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
D
is
ch
ar
ge
su
m
m
ar
ie
s
an
d
i2
b2
/V
A
ch
al
le
ng
e
da
ta
se
t
(2
01
0)
3
O
w
n
+
Ex
is
tin
g
En
gl
is
h
Ex
is
tin
g
U
M
LS
N
o
N
ot
ap
pl
ic
ab
le
[3
5]
C
as
tr
o
20
10
Sp
ai
n
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
C
lin
ic
al
no
te
s
w
ith
m
os
t
re
le
va
nt
in
fo
rm
at
io
n
O
w
n
Sp
an
is
h
Ex
is
tin
g
SN
O
M
ED
C
T
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[3
6]
C
at
lin
g
20
18
U
K
N
o
So
ft
w
ar
e
de
ve
lo
pm
en
t
an
d
ev
al
ua
tio
n
M
IM
IC
-II
Id
at
as
et
4
Ex
is
tin
g
En
gl
is
h
N
ew
IC
D
-9
-C
M
N
ot
lis
te
d
N
ot
lis
te
d
[3
7]
C
ha
pm
an
20
04
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Em
er
ge
nc
y
de
pa
rt
m
en
t
re
po
rt
s
O
w
n
En
gl
is
h
Ex
is
tin
g
U
M
LS
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[3
8]
C
he
n
20
16
U
SA
N
o
In
fo
rm
at
io
n
en
ric
hm
en
t
D
is
ch
ar
ge
su
m
m
ar
ie
s
an
d
pr
og
re
ss
no
te
s
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
N
ot
lis
te
d
N
ot
lis
te
d
[3
9]
C
hi
ar
am
el
lo
20
16
Ita
ly
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
C
lin
ic
al
no
te
s
(c
ar
di
ol
og
y,
di
ab
et
ol
og
y,
he
pa
to
lo
gy
,n
ep
hr
ol
og
y,
an
d
on
co
lo
gy
)
O
w
n
Ita
lia
n
Ex
is
tin
g
U
M
LS
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[4
0]
C
ho
de
y
20
16
U
SA
Se
m
Ev
al
(2
01
4)
In
fo
rm
at
io
n
ex
tr
ac
tio
n
IC
U
D
at
a:
D
is
ch
ar
ge
su
m
m
ar
ie
s,
EC
G
,
ec
ho
,a
nd
ra
di
ol
og
y
Ex
is
tin
g
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
N
ot
lis
te
d
N
ot
lis
te
d
[4
1]
C
hu
ng
20
05
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Ec
ho
ca
rd
io
gr
am
re
po
rt
s
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
N
ot
ye
t,
it
w
ill
be
us
ed
to
po
pu
la
te
a
re
gi
st
ry
N
ot
lis
te
d
[4
2]
C
om
bi
20
18
Ita
ly
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Vi
gi
Se
gn
(a
dv
er
se
dr
ug
re
ac
tio
ns
)
re
po
rt
s
O
w
n
Ita
lia
n
+
En
gl
is
h
N
ew
M
ed
D
RA
Ye
s,
im
pl
em
en
te
d
in
Vi
gi
Fa
rm
ac
o
Ps
eu
do
co
de
[4
3]
D
e
Br
ui
jn
20
11
C
an
ad
a
i2
b2
/V
A
(2
01
0)
In
fo
rm
at
io
n
ex
tr
ac
tio
n
H
os
pi
ta
ld
is
ch
ar
ge
su
m
m
ar
ie
s
an
d
pr
og
re
ss
re
po
rt
s
Ex
is
tin
g
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
N
ot
lis
te
d
N
ot
lis
te
d
[4
4]
D
ei
ss
er
ot
h
20
19
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Si
x
se
ts
of
re
al
pa
tie
nt
da
ta
fro
m
fo
ur
di
ffe
re
nt
m
ed
ic
al
ce
nt
er
s.
O
w
n
En
gl
is
h
N
ew
H
PO
N
ot
lis
te
d
Ye
s
[4
5]
D
em
ne
r-
Fu
sh
m
an
20
17
U
SA
N
o
So
ft
w
ar
e
de
ve
lo
pm
en
t
an
d
ev
al
ua
tio
n
Bi
oS
co
pe
5 ,
N
C
BI
di
se
as
e
co
rp
us
6 ,
i2
b2
/
VA
ch
al
le
ng
e
co
rp
us
(2
01
0)
3 ,
Sh
A
Re
co
rp
us
7 ,
LH
C
te
st
co
lle
ct
io
n
(b
io
lo
gi
ca
l/
cl
in
ic
al
jo
ur
na
la
bs
tr
ac
ts
)
Ex
is
tin
g
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
Ye
s,
us
ed
in
ot
he
r
pa
pe
rs
id
en
tif
ie
d
in
lit
er
at
ur
e
se
ar
ch
Ye
s
[4
6]
D
iv
ita
20
14
U
SA
Pa
rt
s:
i2
b2
/V
A
So
ft
w
ar
e
Ra
nd
om
ly
se
le
ct
ed
cl
in
ic
al
re
co
rd
s
fro
m
O
w
n
En
gl
is
h
N
ew
U
M
LS
(le
ve
l0
+
9)
Ye
s,
us
ed
by
VA
Ye
s
[4
7]
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 5 of 21
Ta
b
le
3
In
cl
ud
ed
pu
bl
ic
at
io
ns
an
d
th
ei
r
fir
st
au
th
or
,y
ea
r,
tit
le
,a
nd
co
un
tr
y
(C
on
tin
ue
d)
A
ut
ho
r
Y
ea
r
C
ou
nt
ry
C
ha
lle
ng
e
In
d
uc
ed
ob
je
ct
iv
e
D
at
a
or
ig
in
D
at
as
et
D
at
a
la
ng
ua
g
e
U
se
d
sy
st
em
Te
rm
.S
ys
.
In
us
e
So
ur
ce
co
d
e
Re
f
(2
01
0)
de
ve
lo
pm
en
t
an
d
ev
al
ua
tio
n
th
e
m
os
t
fre
qu
en
t
do
cu
m
en
t
ty
pe
s
In
fo
rm
at
ic
s
an
d
C
om
pu
tin
g
In
fra
st
ru
ct
ur
e
D
ua
rt
e
20
18
Po
rt
ug
al
N
o
In
fo
rm
at
io
n
en
ric
hm
en
t
D
ea
th
ce
rt
ifi
ca
te
s,
cl
in
ic
al
bu
lle
tin
s,
an
d
au
to
ps
y
re
po
rt
s
O
w
n
Po
rt
ug
ue
se
N
ew
IC
D
-1
0
Ye
s,
us
ed
by
Po
rt
ug
es
e
M
in
is
tr
y
of
H
ea
lth
fo
r
ne
ar
re
al
-t
im
e
de
at
h
ca
us
e
su
rv
ei
lla
nc
e
N
ot
lis
te
d
[4
8]
Fa
lis
20
19
U
K
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
M
IM
IC
-II
Id
at
as
et
4
Ex
is
tin
g
En
gl
is
h
N
ew
IC
D
-9
N
ot
lis
te
d
N
ot
lis
te
d
[4
9]
Fe
rr
ão
20
13
Po
rt
ug
al
N
o
In
fo
rm
at
io
n
en
ric
hm
en
t
In
pa
tie
nt
ad
ul
t
ep
is
od
es
fro
m
th
e
EH
R
O
w
n
Po
rt
ug
ue
se
N
ew
IC
D
-9
-C
M
N
ot
lis
te
d
N
ot
lis
te
d
[5
0]
G
er
bi
er
20
11
Fr
an
ce
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
C
om
pu
te
riz
ed
em
er
ge
nc
y
de
pa
rt
m
en
t
m
ed
ic
al
re
co
rd
s
O
w
n
Fr
en
ch
N
ew
IC
D
-1
0,
C
C
A
M
,S
N
O
M
ED
C
T,
A
TC
,M
eS
H
,I
C
PC
-2
,
D
C
R
N
ot
ye
t,
w
ill
be
in
te
gr
at
ed
in
to
a
C
D
SS
N
ot
lis
te
d
[5
1]
G
oi
co
ec
he
a
Sa
la
za
r
20
13
Sp
ai
n
N
o
In
fo
rm
at
io
n
en
ric
hm
en
t
D
ia
gn
os
tic
te
xt
fro
m
pa
tie
nt
re
co
rd
s
O
w
n
Sp
an
is
h
N
ew
IC
D
-9
-C
M
N
ot
lis
te
d
N
ot
lis
te
d
[5
2]
H
am
id
20
13
U
SA
N
o
C
la
ss
ifi
ca
tio
n
N
ot
es
of
Ira
q
an
d
A
fg
ha
ni
st
an
ve
te
ra
ns
fro
m
th
e
VA
na
tio
na
lc
lin
ic
al
da
ta
ba
se
O
w
n
En
gl
is
h
Ex
is
tin
g
U
M
LS
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[5
3]
H
as
sa
nz
ad
eh
20
16
A
us
tr
al
ia
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Sh
A
Re
/C
LE
F
co
rp
us
(2
01
3)
2
Ex
is
tin
g
En
gl
is
h
Ex
is
tin
g
U
M
LS
,S
N
O
M
ED
C
T
N
ot
ap
pl
ic
ab
le
N
ot
ap
pl
ic
ab
le
[5
4]
H
el
w
e
20
17
Le
ba
no
n
N
o
C
om
pu
te
r-
as
si
st
ed
co
di
ng
M
IM
IC
-II
Id
at
as
et
Ex
is
tin
g
En
gl
is
h
N
ew
U
M
LS
,I
C
D
N
ot
lis
te
d
N
ot
lis
te
d
[5
5]
H
er
sh
20
01
U
SA
N
o
In
fo
rm
at
io
n
en
ric
hm
en
t
Ra
di
ol
og
y
im
ag
e
re
po
rt
s
O
w
n
En
gl
is
h
Ex
is
tin
g
U
M
LS
N
o,
st
ill
in
de
ve
lo
pm
en
t/
te
st
in
g
Ps
eu
do
co
de
[5
6]
H
oo
ge
nd
oo
rn
20
15
N
et
he
rla
nd
s
N
o
Pr
ed
ic
tio
n
C
on
su
lta
tio
n
no
te
s
of
pa
tie
nt
s
in
a
pr
im
ar
y
ca
re
se
tt
in
g
O
w
n
D
ut
ch
N
ew
SN
O
M
ED
-C
T,
U
M
LS
,I
C
PC
N
ot
lis
te
d
N
ot
lis
te
d
[5
7]
Jin
da
l
20
13
U
SA
i2
b2
(2
01
2)
In
fo
rm
at
io
n
ex
tr
ac
tio
n
i2
b2
ch
al
le
ng
e
co
rp
us
(2
01
2)
8
Ex
is
tin
g
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
,S
N
O
M
ED
C
T,
M
eS
H
N
ot
lis
te
d
N
ot
lis
te
d
[5
8]
Ka
ng
20
09
Ko
re
a
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
D
is
ch
ar
ge
su
m
m
ar
ie
s
O
w
n
Ko
re
an
N
ew
KO
M
ET
,U
M
LS
N
ot
lis
te
d
N
ot
lis
te
d
[5
9]
Ke
rs
lo
ot
20
19
N
et
he
rla
nd
s
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
(N
on
-s
m
al
lc
el
l)
Lu
ng
ca
nc
er
ch
ar
ts
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
SN
O
M
ED
C
T
N
ot
lis
te
d
Ye
s
[6
0]
Kö
ni
g
20
19
G
er
m
an
y
N
o
So
ft
w
ar
e
de
ve
lo
pm
en
t
an
d
ev
al
ua
tio
n
D
is
ch
ar
ge
le
tt
er
s
fro
m
BA
SE
-II
st
ud
y
O
w
n
G
er
m
an
N
ew
(+
ex
is
tin
g)
W
in
ge
rt
-N
om
en
cl
at
ur
e
N
o,
st
ill
ha
s
to
pr
ov
e
its
va
lu
e
N
ot
lis
te
d
[6
1]
Li
20
15
U
SA
N
o
In
fo
rm
at
io
n
co
m
pa
ris
on
C
lin
ic
al
no
te
s
an
d
di
sc
ha
rg
e
pr
es
cr
ip
tio
n
lis
ts
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
,S
N
O
M
ED
C
T,
Rx
N
or
m
N
ot
ye
t,
pl
an
s
to
m
ov
e
to
pr
od
uc
tio
n
Ps
eu
do
co
de
[6
2]
Li
20
19
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
EH
R
no
te
s
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
,S
N
O
M
ED
C
T,
M
ed
D
RA
N
ot
lis
te
d
N
ot
lis
te
d
[6
3]
Li
ng
re
n
20
16
U
SA
N
o
C
la
ss
ifi
ca
tio
n
St
ru
ct
ur
ed
an
d
un
st
ru
ct
ur
ed
da
ta
fro
m
tw
o
EH
R
da
ta
ba
se
s
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
,I
C
D
-9
,R
xN
or
m
N
ot
lis
te
d
N
ot
lis
te
d
[1
2]
Li
u
20
19
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
C
lin
ic
al
no
te
s
fro
m
di
ffe
re
nt
in
st
itu
tio
ns
+
Pu
bM
ed
C
as
e
re
po
rt
ab
st
ra
ct
s
O
w
n
+
Ex
is
tin
g
En
gl
is
h
Ex
is
tin
g
H
PO
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[6
4]
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 6 of 21
Ta
b
le
3
In
cl
ud
ed
pu
bl
ic
at
io
ns
an
d
th
ei
r
fir
st
au
th
or
,y
ea
r,
tit
le
,a
nd
co
un
tr
y
(C
on
tin
ue
d)
A
ut
ho
r
Y
ea
r
C
ou
nt
ry
C
ha
lle
ng
e
In
d
uc
ed
ob
je
ct
iv
e
D
at
a
or
ig
in
D
at
as
et
D
at
a
la
ng
ua
g
e
U
se
d
sy
st
em
Te
rm
.S
ys
.
In
us
e
So
ur
ce
co
d
e
Re
f
Lo
w
e
20
09
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Si
ng
le
-s
pe
ci
m
en
pa
th
ol
og
y
re
po
rt
s
O
w
n
En
gl
is
h
Ex
is
tin
g
U
M
LS
,S
N
O
M
ED
C
T
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[6
5]
Lu
o
20
14
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Pa
th
ol
og
y
re
po
rt
s
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
,S
N
O
M
ED
C
T
Ye
s,
cu
rr
en
tly
w
or
ki
ng
on
pr
oj
ec
t
in
m
ul
tip
le
ho
sp
ita
ls
N
ot
lis
te
d
[6
6]
M
ey
st
re
20
06
U
SA
N
o
In
fo
rm
at
io
n
en
ric
hm
en
t
C
lin
ic
al
do
cu
m
en
ts
fo
rm
ad
ul
t
in
pa
tie
nt
s
in
a
ca
rd
io
va
sc
ul
ar
un
it
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
(le
ve
l0
),
SN
O
M
ED
C
T
N
ot
ye
t,
te
st
in
g
in
pr
ac
tic
e
N
ot
lis
te
d
[6
7]
M
ey
st
re
20
10
U
SA
i2
b2
(2
00
9)
In
fo
rm
at
io
n
ex
tr
ac
tio
n
i2
b2
ch
al
le
ng
e
da
ta
se
t
(2
00
9)
9
Ex
is
tin
g
En
gl
is
h
N
ew
U
M
LS
N
ot
ye
t,
po
ss
ib
le
in
te
gr
at
io
n
in
re
se
ar
ch
in
fra
st
ru
ct
ur
e
N
ot
lis
te
d
[6
8]
M
in
ar
d
20
11
Fr
an
ce
i2
b2
/V
A
(2
01
0)
In
fo
rm
at
io
n
ex
tr
ac
tio
n
i2
b2
/V
A
ch
al
le
ng
e
co
rp
us
(2
01
0)
3
Ex
is
tin
g
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
N
ot
lis
te
d
N
ot
lis
te
d
[6
9]
M
is
hr
a
20
19
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
C
lin
ic
al
no
te
s
fro
m
N
IH
C
lin
ic
al
C
en
te
r
da
ta
w
ar
eh
ou
se
O
w
n
En
gl
is
h
Ex
is
tin
g
U
M
LS
,H
PO
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[7
0]
N
gu
ye
n
20
18
A
us
tr
al
ia
N
o
C
om
pu
te
r-
as
si
st
ed
co
di
ng
H
os
pi
ta
lp
ro
gr
es
s
no
te
s
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
SN
O
M
ED
C
T,
IC
D
-1
0-
A
M
N
ot
lis
te
d
N
ot
lis
te
d
[7
1]
O
el
lri
ch
20
15
U
K
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Pu
bM
ed
ab
st
ra
ct
s,
cl
in
ic
al
tr
ia
l
in
fo
rm
at
io
n,
i2
b2
/V
A
ch
al
le
ng
e
co
rp
us
(2
01
0)
3 ,
SH
A
RE
/C
LE
F
(2
01
3)
2
Ex
is
tin
g
En
gl
is
h
Ex
is
tin
g
U
M
LS
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[7
2]
Pa
tr
ic
k
20
11
A
us
tr
al
ia
i2
b2
/V
A
(2
01
0)
In
fo
rm
at
io
n
ex
tr
ac
tio
n
i2
b2
/V
A
ch
al
le
ng
e
co
rp
us
(2
01
0)
3
Ex
is
tin
g
En
gl
is
h
N
ew
U
M
LS
,S
N
O
M
ED
C
T
N
ot
lis
te
d
N
ot
lis
te
d
[7
3]
Pé
re
z
20
18
Sp
ai
n
N
o
Te
xt
pr
oc
es
si
ng
Sp
on
ta
ne
ou
s
D
Ts
ra
nd
om
ly
se
le
ct
ed
en
tr
ie
s
O
w
n
Sp
an
is
h
N
ew
IC
D
N
ot
lis
te
d
N
ot
lis
te
d
[7
4]
Re
át
eg
ui
20
18
C
an
ad
a
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
i2
b2
ch
al
le
ng
e
co
rp
us
(2
00
8)
10
Ex
is
tin
g
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
,S
N
O
M
ED
C
T,
Rx
N
or
m
N
ot
lis
te
d
N
ot
lis
te
d
[7
5]
Ro
be
rt
s
20
11
U
SA
i2
b2
/V
A
(2
01
0)
In
fo
rm
at
io
n
ex
tr
ac
tio
n
i2
b2
/V
A
ch
al
le
ng
e
co
rp
us
(2
01
0)
3
Ex
is
tin
g
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
,I
C
D
-9
N
ot
lis
te
d
N
ot
lis
te
d
[7
6]
Ro
us
se
au
20
19
U
SA
N
o
In
fo
rm
at
io
n
co
m
pa
ris
on
ED
en
co
un
te
rs
fo
r
pa
tie
nt
s
w
ith
he
ad
ac
he
s
w
ho
re
ce
iv
ed
he
ad
C
T
O
w
n
En
gl
is
h
Ex
is
tin
g
U
M
LS
:S
N
O
M
ED
C
T,
Ra
dL
ex
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[7
7]
Sa
vo
va
20
10
U
SA
i2
b2
(2
00
6,
20
08
)
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Su
bs
et
of
cl
in
ic
al
no
te
s
fro
m
th
e
EM
R
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
,S
N
O
M
ED
C
T,
Rx
N
or
m
Ye
s,
us
ed
in
ot
he
r
pa
pe
rs
id
en
tif
ie
d
in
lit
er
at
ur
e
se
ar
ch
Ye
s
[7
8]
Sh
iv
ad
e
20
15
U
SA
i2
b2
/U
TH
ea
lth
(2
01
4)
C
la
ss
ifi
ca
tio
n
i2
b2
ch
al
le
ng
e
co
rp
us
(2
01
4)
11
Ex
is
tin
g
En
gl
is
h
Ex
is
tin
g
U
M
LS
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[1
1]
Sh
oe
nb
ill
20
19
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
EH
R
no
te
s
fro
m
hy
pe
rt
en
si
on
pa
tie
nt
s
O
w
n
En
gl
is
h
Ex
is
tin
g
U
M
LS
,S
N
O
M
ED
C
T
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[7
9]
So
hn
20
14
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
C
lin
ic
al
no
te
s
w
ith
m
ed
ic
at
io
n
m
en
tio
ns
O
w
n
En
gl
is
h
N
ew
Rx
N
or
m
N
ot
lis
te
d
Ye
s
[8
0]
So
lti
20
08
U
SA
N
o
In
fo
rm
at
io
n
en
ric
hm
en
t
C
ar
di
ol
og
y
am
bu
la
to
ry
pr
og
re
ss
no
te
s
O
w
n
En
gl
is
h
Ex
is
tin
g
U
M
LS
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[8
1]
So
ria
no
20
19
Sp
ai
n
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
cl
in
ic
al
em
er
ge
nc
y
di
sc
ha
rg
e
re
po
rt
s
O
w
n
Sp
an
is
h
N
ew
SN
O
M
ED
C
T
N
ot
ye
t
Ye
s
[8
2]
So
ys
al
20
18
U
SA
Pa
rt
s:
i2
b2
So
ft
w
ar
e
D
is
ch
ar
ge
su
m
m
ar
ie
s
fro
m
th
e
i2
b2
/V
A
O
w
n
+
En
gl
is
h
N
ew
U
M
LS
Ye
s,
us
ed
by
va
rio
us
Ye
s
[8
3]
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 7 of 21
Ta
b
le
3
In
cl
ud
ed
pu
bl
ic
at
io
ns
an
d
th
ei
r
fir
st
au
th
or
,y
ea
r,
tit
le
,a
nd
co
un
tr
y
(C
on
tin
ue
d)
A
ut
ho
r
Y
ea
r
C
ou
nt
ry
C
ha
lle
ng
e
In
d
uc
ed
ob
je
ct
iv
e
D
at
a
or
ig
in
D
at
as
et
D
at
a
la
ng
ua
g
e
U
se
d
sy
st
em
Te
rm
.S
ys
.
In
us
e
So
ur
ce
co
d
e
Re
f
(2
00
9
+
20
10
),
Sh
A
Re
/C
LE
F
(2
01
3)
,S
em
-E
VA
L
(2
01
4)
de
ve
lo
pm
en
t
an
d
ev
al
ua
tio
n
ch
al
le
ng
e
co
rp
us
(2
01
0)
3 ,
ou
tp
at
ie
nt
cl
in
ic
vi
si
t
no
te
s,
m
oc
k
cl
in
ic
al
do
cu
m
en
ts
Ex
is
tin
g
in
st
itu
tio
ns
an
d
in
du
st
ria
l
en
tit
ie
s
Sp
as
i?
20
15
U
K
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
M
RI
re
po
rt
s
of
pa
tie
nt
s
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
TR
A
K,
U
M
LS
,M
ED
C
IN
,
Ra
dL
ex
N
ot
lis
te
d
Ye
s
[8
4]
St
ra
us
s
20
13
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Pa
th
ol
og
y
re
po
rt
s
of
br
ea
st
an
d
pr
os
ta
te
ca
nc
er
pa
tie
nt
s
O
w
n
En
gl
is
h
N
ew
SN
O
M
ED
C
T
N
ot
lis
te
d
Ye
s
[8
5]
Su
ng
20
18
Ta
iw
an
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
C
as
es
of
ad
ul
t
pa
tie
nt
s
w
ith
A
IS
O
w
n
En
gl
is
h
Ex
is
tin
g
U
M
LS
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[8
6]
Tc
he
ch
m
ed
jie
v
20
18
Fr
an
ce
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Q
ua
er
o
(F
re
nc
h
M
ED
LI
N
E
ab
st
ra
ct
tit
le
s
+
EM
EA
dr
ug
la
be
ls
)+
C
ép
iD
C
(IC
D
-1
0
co
di
ng
of
de
at
h
ce
rt
ifi
ca
te
s)
Ex
is
tin
g
Fr
en
ch
N
ew
(+
ex
is
tin
g)
U
M
LS
te
rm
in
ol
og
ie
s
(IC
D
-1
0)
Ye
s,
av
ai
la
bl
e
in
SI
FR
Bi
oP
or
ta
l
Ye
s
[8
7]
Te
rn
oi
s
20
18
Fr
an
ce
N
o
C
la
ss
ifi
ca
tio
n
En
do
sc
op
y
re
po
rt
s
w
rit
te
n
be
tw
ee
n
20
15
an
d
20
16
O
w
n
Fr
en
ch
N
ew
C
C
A
M
N
ot
lis
te
d
N
ot
lis
te
d
[8
8]
Tr
av
er
s
20
04
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
C
hi
ef
co
m
pl
ai
nt
te
xt
en
tr
ie
s
fo
r
al
l
em
er
ge
nc
y
de
pa
rt
m
en
t
vi
si
ts
O
w
n
En
gl
is
h
N
ew
U
M
LS
N
ot
lis
te
d
N
ot
lis
te
d
[8
9]
Tu
lk
en
s
20
19
Be
lg
iu
m
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
i2
b2
/V
A
ch
al
le
ng
e
co
rp
us
(2
01
0)
3
Ex
is
tin
g
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
N
ot
lis
te
d
Ye
s
[9
0]
U
su
i
20
18
Ja
pa
n
N
o
Pr
ed
ic
tio
n
El
ec
tr
on
ic
m
ed
ic
at
io
n
hi
st
or
y
da
ta
fro
m
ph
ar
m
ac
y
O
w
n
Ja
pa
ne
se
N
ew
IC
D
-1
0
N
ot
ye
t,
ex
pe
ct
to
us
e
it
N
ot
lis
te
d
[9
1]
Va
ltc
hi
no
v
20
19
U
SA
N
o
C
la
ss
ifi
ca
tio
n
Ra
di
ol
og
y
re
po
rt
s,
em
er
ge
nc
y
de
pa
rt
m
en
t
no
te
s
+
ot
he
r
cl
in
ic
al
re
po
rt
s
O
w
n
En
gl
is
h
Ex
is
tin
g
SN
O
M
ED
C
T,
Ra
dL
ex
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[9
2]
W
ad
ia
20
18
U
SA
N
o
C
la
ss
ifi
ca
tio
n
C
he
st
C
T
re
po
rt
s
O
w
n
En
gl
is
h
Ex
is
tin
g
SN
O
M
ED
C
T,
U
M
LS
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[9
3]
W
al
ke
r
20
19
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Tr
ea
tm
en
t
si
te
s
fro
m
EM
R
O
w
n
En
gl
is
h
N
ew
U
M
LS
N
ot
lis
te
d
N
ot
lis
te
d
[9
4]
Xi
e
20
19
C
hi
na
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
M
IM
IC
-II
Id
at
as
et
4
Ex
is
tin
g
En
gl
is
h
N
ew
IC
D
-9
-C
M
,I
C
D
-1
0
N
ot
lis
te
d
N
ot
lis
te
d
[9
5]
Xu
20
11
U
SA
N
o
C
la
ss
ifi
ca
tio
n
C
RC
pa
tie
nt
ca
se
s
fro
m
th
e
Sy
nt
he
tic
D
er
iv
at
iv
e
da
ta
ba
se
O
w
n
En
gl
is
h
Ex
is
tin
g
U
M
LS
N
o,
st
ill
un
de
r
de
ve
lo
pm
en
t
N
ot
ap
pl
ic
ab
le
[9
6]
Ya
da
v
20
13
U
SA
N
o
Pr
ed
ic
tio
n
Em
er
ge
nc
y
de
pa
rt
m
en
t
C
T
im
ag
in
g
re
po
rt
s
O
w
n
En
gl
is
h
Ex
is
tin
g
U
M
LS
N
ot
lis
te
d
Ye
s,
co
m
m
an
d
lin
e
co
m
m
an
d
[9
7]
Ya
o
20
19
U
SA
N
o
Pr
ed
ic
tio
n
i2
b2
ch
al
le
ng
e
co
rp
us
(2
00
8)
10
Ex
is
tin
g
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
N
ot
lis
te
d
Pa
rt
(S
or
l)
[9
8]
Ze
ng
20
18
U
SA
N
o
C
la
ss
ifi
ca
tio
n
Pr
og
re
ss
no
te
s
an
d
br
ea
st
ca
nc
er
su
rg
ic
al
pa
th
ol
og
y
re
po
rt
s
O
w
n
En
gl
is
h
N
ew
(+
ex
is
tin
g)
U
M
LS
N
ot
lis
te
d
N
ot
lis
te
d
[9
9]
Zh
an
g
20
13
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
i2
b2
/V
A
ch
al
le
ng
e
co
rp
us
(2
01
0)
3
an
d
G
EN
IA
co
rp
us
(M
ED
LI
N
E
ab
st
ra
ct
s)
Ex
is
tin
g
En
gl
is
h
N
ew
U
M
LS
N
ot
lis
te
d
N
ot
lis
te
d
[1
00
]
Zh
ou
20
06
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
Re
co
rd
s
of
pa
tie
nt
s
w
ith
br
ea
st
co
m
pl
ai
nt
s
O
w
n
En
gl
is
h
N
ew
U
M
LS
N
o,
st
ill
un
de
r
de
ve
lo
pm
en
t
N
ot
lis
te
d
[1
01
]
Zh
ou
20
11
U
SA
N
o
So
ft
w
ar
e
C
O
PD
an
d
C
A
D
pa
tie
nt
s
O
w
n
En
gl
is
h
N
ew
SN
O
M
ED
C
T,
Rx
N
or
m
,
Ye
s,
de
sc
rib
ed
in
ot
he
r
N
ot
lis
te
d
[1
02
]
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 8 of 21
Ta
b
le
3
In
cl
ud
ed
pu
bl
ic
at
io
ns
an
d
th
ei
r
fir
st
au
th
or
,y
ea
r,
tit
le
,a
nd
co
un
tr
y
(C
on
tin
ue
d)
A
ut
ho
r
Y
ea
r
C
ou
nt
ry
C
ha
lle
ng
e
In
d
uc
ed
ob
je
ct
iv
e
D
at
a
or
ig
in
D
at
as
et
D
at
a
la
ng
ua
g
e
U
se
d
sy
st
em
Te
rm
.S
ys
.
In
us
e
So
ur
ce
co
d
e
Re
f
de
ve
lo
pm
en
t
an
d
ev
al
ua
tio
n
U
M
LS
,P
PL
,M
D
D
,H
L7
va
lu
e
se
ts
pa
pe
r
(1
03
])
Zh
ou
20
14
U
SA
N
o
In
fo
rm
at
io
n
ex
tr
ac
tio
n
A
dm
is
si
on
no
te
s
an
d
di
sc
ha
rg
e
su
m
m
ar
ie
s
O
w
n
En
gl
is
h
Ex
is
tin
g
SN
O
M
ED
C
T,
H
L7
Ro
le
C
od
es
N
ot
lis
te
d
N
ot
ap
pl
ic
ab
le
[1
03
]
1.
Ph
en
oC
H
F
co
rp
us
:n
ar
ra
tiv
e
re
po
rt
s
fr
om
el
ec
tr
on
ic
he
al
th
re
co
rd
s
(E
H
Rs
)
an
d
lit
er
at
ur
e
ar
tic
le
s
2.
Sh
A
Re
/C
LE
F
co
rp
us
(2
01
3)
:n
ar
ra
tiv
e
cl
in
ic
al
re
po
rt
s
3.
i2
b2
/V
A
ch
al
le
ng
e
da
ta
se
t
(2
01
0)
:d
is
ch
ar
ge
su
m
m
ar
ie
s
an
d
pr
og
re
ss
re
po
rt
s
4.
M
IM
IC
-II
Id
at
as
et
:d
em
og
ra
ph
ic
s,
vi
ta
ls
ig
n
m
ea
su
re
m
en
ts
,l
ab
or
at
or
y
te
st
re
su
lts
,p
ro
ce
du
re
s,
m
ed
ic
at
io
ns
,c
ar
eg
iv
er
no
te
s,
im
ag
in
g
re
po
rt
s,
an
d
m
or
ta
lit
y
5.
Bi
oS
co
pe
co
rp
us
:m
ed
ic
al
fr
ee
te
xt
s,
bi
ol
og
ic
al
fu
ll
pa
pe
rs
an
d
bi
ol
og
ic
al
sc
ie
nt
ifi
c
ab
st
ra
ct
s
6.
N
C
BI
di
se
as
e
co
rp
us
:P
ub
M
ed
ab
st
ra
ct
s
7.
Sh
A
Re
co
rp
us
:d
ei
de
nt
ifi
ed
cl
in
ic
al
fr
ee
-t
ex
t
no
te
s
fr
om
th
e
M
IM
IC
II
da
ta
ba
se
8.
i2
b2
ch
al
le
ng
e
co
rp
us
(2
01
2)
:d
is
ch
ar
ge
su
m
m
ar
ie
s
9.
i2
b2
ch
al
le
ng
e
da
ta
se
t
(2
00
9)
:d
e-
id
en
tif
ie
d
ho
sp
ita
ld
is
ch
ar
ge
su
m
m
ar
ie
s
10
.i
2b
2
ch
al
le
ng
e
co
rp
us
(2
00
8)
:d
is
ch
ar
ge
su
m
m
ar
ie
s
of
ov
er
w
ei
gh
t
an
d
di
ab
et
ic
pa
tie
nt
s
11
.i
2b
2
ch
al
le
ng
e
co
rp
us
(2
01
4)
:l
on
gi
tu
di
na
lly
or
de
re
d
cl
in
ic
al
no
te
s
fr
om
th
re
e
co
ho
rt
s
of
di
ab
et
ic
pa
tie
nt
s
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 9 of 21
Table 4 Included publications and their evaluation methodologies
Author Year Ref. std. Validation External Generalizability a Ref
Afshar 2019 Existing EHR
data
Hold-out validation (train,
test, development)
No No, validation is needed [29]
Alnazzawi 2016 Existing
annotated
corpus
External ShARe/CLEF, NCBI disease,
Heart failure and pulmonary
embolism corpora
Yes, achieves competitive
performance on other corpora
[30]
Atutxa 2018 Manual
retrospective
review
Hold-out validation (train,
test, development)
No Yes, easily portable to other
languages
[31]
Barrett 2013 Manual
annotations
10-fold cross validation Multiple datasets (different
provider)
Yes, expect that it is generalizable [32]
Becker 2016 Existing
annotated
corpus
Not used No Not listed [33]
Becker 2019 Manual
annotations
Hold-out validation (train,
test, development)
No Not listed [34]
Bejan 2015 Manual
annotations
External i2b2 data (2010) Yes, good performance on the
i2b2 dataset, even though not
optimized on it
[35]
Castro 2010 Manual
annotations
Not used No Not listed [36]
Catling 2018 Existing
annotated
corpus
Hold-out validation (train,
test, development)
No Not listed [37]
Chapman 2004 Manual
annotations
Not used No Yes, generalizable to other domains
within and outside of bio surveillance
[38]
Chen 2016 Manual
annotations
10-fold cross validation No Not listed [39]
Chiaramello 2016 Manual
annotations
Not used No Not listed [40]
Chodey 2016 Existing
annotated
corpus
Hold-out validation (train,
test)
No Not listed [41]
Chung 2005 Manual
annotations
Hold-out validation (train,
test)
Reports from a second
hospital
Not listed [42]
Combi 2018 Manual
annotations
Not used No Not listed [43]
deBruijn 2011 Existing
annotated
corpus
15-fold cross validation No Not listed [44]
Deisseroth 2019 Manual
annotations
Hold-out validation (train,
test)
Data from a second
hospital
Yes, it can be immediately incorporated
into clinical practice
[45]
Demner-
Fushman
2017 Existing
annotated
corpus
External Multiple datasets Not listed [46]
Divita 2014 Manual
annotations
Not used No Not listed [47]
Duarte 2018 Manual
annotations
Hold-out validation (train,
test)
Second dataset Not listed [48]
Falis 2019 Existing
annotated
corpus
Hold-out validation (train,
test, development)
No Yes, method is not specific to an
ontology, and could be used for a graph
of any formation
[49]
Ferrão 2013 Existing EHR
data
Hold-out validation (train,
test)
No Not listed [50]
Gerbier 2011 Manual
annotations
Hold-out validation (train,
test)
No Yes, it could also serve other types of
clinical decision support systems
[51]
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 10 of 21
Table 4 Included publications and their evaluation methodologies (Continued)
Author Year Ref. std. Validation External Generalizability a Ref
Goicoechea
Salazar
2013 Manual
annotations
Hold-out validation (train,
test)
No Not listed [52]
Hamid 2013 Manual
annotations
10-fold cross validation No Possible, the classifier may be
applicable in academic hospital
samples
[53]
Hassanzadeh 2016 Existing
annotated
corpus
Hold-out validation (train,
test)
No Not applicable [54]
Helwe 2017 Existing
annotated
corpus
Hold-out validation (train,
test, development)
No Not listed [55]
Hersh 2001 Manual
annotations
Hold-out validation (train,
test)
No Not listed [56]
Hoogendoorn 2015 Existing EHR
data
5-fold cross validation No Not listed [57]
Jindal 2013 Existing
annotated
corpus
Hold-out validation (train,
test)
No Yes, broad applicability [58]
Kang 2009 Manual
annotations
Hold-out validation (train,
test)
No Yes, extensible to other languages [59]
Kersloot 2019 Manual
annotations
Hold-out validation
(development, test)
No Possible, but external validation
is needed
[60]
König 2019 Existing EHR
data
Not used No Still to be tested [61]
Li 2015 Manual
annotations
10-fold cross validation No Not listed [62]
Li 2019 Existing
annotated
corpus
Hold-out validation (train,
test, development)
No Not listed [63]
Lingren 2016 Manual
annotations
Hold-out validation (train,
test, development)
No Not listed [12]
Liu 2019 Manual
annotations
Not used No (but multiple datasets /
non-trained)
No, limited because of NYP/CUIMC
and Mayo notes.
[64]
Lowe 2009 Manual
retrospective
review
Hold-out validation (train,
test)
No Yes, has the potential to index other
classes of clinical documents
[65]
Luo 2014 Existing EHR
data
10-fold cross validation No No, challenging, not currently working
on it
[66]
Meystre 2006 Manual
retrospective
review
Not used No Not listed [67]
Meystre 2010 Existing
annotated
corpus
Hold-out validation (train,
test)
No Not listed [68]
Minard 2011 Existing
annotated
corpus
Hold-out validation (train,
test, development)
No Not listed [69]
Mishra 2019 Manual
annotations
Not used No Not listed [70]
Nguyen 2018 Existing EHR
data
Not listed No Not listed [71]
Oellrich 2015 Existing
annotated
corpus
External Multiple datasets Not listed [72]
Patrick 2011 Existing
annotated
10-fold cross validation No Yes, adaptable to different requirements
in clinical information extraction and
[73]
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 11 of 21
Table 4 Included publications and their evaluation methodologies (Continued)
Author Year Ref. std. Validation External Generalizability a Ref
corpus classification by choosing relevant feature
sets
Pérez 2018 Existing
annotated
corpus
Hold-out validation (train,
test, development)
No Yes, extensible to different hospital-sections
and hospitals
[74]
Reátegui 2018 Existing
annotated
corpus
Not used No Not listed [75]
Roberts 2011 Existing
annotated
corpus
Hold-out validation (train,
test)
No Not listed [76]
Rousseau 2019 Manual
annotations
Not used No Not listed [77]
Savova 2010 Manual
annotations
10-fold cross validation No Yes, implemented in several applications [78]
Shivade 2015 Manual
annotations
Hold-out validation (train,
test)
No Not listed [11]
Shoenbill 2019 Manual
annotations
Hold-out validation (train,
test)
No Yes, can allow further evaluation and
improvement in care delivery models
and treatment approaches to multiple
chronic illnesses
[79]
Sohn 2014 Manual
annotations
Hold-out validation (train,
test, development)
No Yes, with adaptions: create flexible
mechanism for adaptation process
[80]
Solti 2008 Manual
annotations
Hold-out validation (train,
test)
No Not listed [81]
Soriano 2019 Manual
annotations
Not listed No Not listed [82]
Soysal 2018 Existing
annotated
corpus
Hold-out validation (train,
test)
No Yes, can be used to quickly develop
customized clinical information extraction
pipelines
[83]
Spasi? 2015 Manual
annotations
Hold-out validation (train,
test)
No Not listed [84]
Strauss 2013 Manual
annotations
Not used No Yes, can be shared between institutions
and used to support clinical +
epidemiological research
[85]
Sung 2018 Manual
annotations
Not listed No Not listed [86]
Tchechmedjiev 2018 Existing
annotated
corpus
Hold-out validation (train,
test, development)
No Yes, but not universally [87]
Ternois 2018 Existing EHR
data
5-fold cross validation +
Hold-out validation (train,
test)
No Not listed [88]
Travers 2004 Manual
retrospective
review
Not used No Not listed [89]
Tulkens 2019 Existing
annotated
corpus
Hold-out validation (train,
test, development)
No Not listed [90]
Usui 2018 Manual
annotations
Not used No Not listed [91]
Valtchinov 2019 Manual
annotations
Not used No No [92]
Wadia 2018 Manual
annotations
Not used No Not listed [93]
Walker 2019 Manual Hold-out validation No Yes, it can be incorporated in [94]
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 12 of 21
 Not published in a peer-reviewed journal (except for
ACL and ACM publications)
In the second phase, both reviewers excluded publica-
tions where the developed NLP algorithm was not evalu-
ated by assessing the titles, abstracts, and, in case of
uncertainty, the Method section of the publication. In the
third phase, both reviewers independently evaluated the
resulting full-text articles for relevance. The reviewers
used Rayyan [27] in the first phase and Covidence [28] in
the second and third phases to store the information
about the articles and their inclusion. In all phases, both
reviewers independently reviewed all publications. After
each phase the reviewers discussed any disagreement until
consensus was reached.
Data extraction and categorization
Both reviewers categorized the implementations of the found
algorithms and noted their characteristics in a structured
form in Covidence. The objectives of the included studies
and their associated NLP tasks were categorized by way of
induction. The results were compared and merged into one
result set.
We collected the following characteristics of the stud-
ies, based on a combination of TRIPOD [21], STROBE
[22], RECORD [23], and STARD [24] statement ele-
ments (see Additional file 3): year, country, setting, ob-
jectives, evaluation methods, used NLP systems or
algorithms, used terminology systems, size of datasets,
performance measures, reference standard, language of
the free-text data, validation methods, generalizability,
operational use, and source code availability.
List of recommendations
Based on the findings of the systematic review and ele-
ments from the TRIPOD, STROBE, RECORD, and
STARD statements, we formed a list of recommenda-
tions. The recommendations focus on the development
and evaluation of NLP algorithms for mapping clinical
text fragments onto ontology concepts and the reporting
of evaluation results.
Results
The literature search generated a total of 2355 unique
publications. After reviewing the titles and abstracts, we
selected 256 publications for additional screening. Out of
the 256 publications, we excluded 65 publications, as the
described Natural Language Processing algorithms in
those publications were not evaluated. The full text of the
remaining 191 publications was assessed and 114
Table 4 Included publications and their evaluation methodologies (Continued)
Author Year Ref. std. Validation External Generalizability a Ref
retrospective
review
(development, test) institutional data warehouse
Xie 2019 Existing
annotated
corpus
Hold-out validation (train,
test, development)
No Not listed [95]
Xu 2011 Manual
annotations
Hold-out validation (train,
test)
No Yes, generable approach to combine
information from heterogeneous data
sources in EHRs
[96]
Yadav 2013 Manual
annotations
Not used No Yes, should be broadly applicate to
outcomes of clinical interest
[97]
Yao 2019 Existing
annotated
corpus
Hold-out validation (train,
test)
No Not listed [98]
Zeng 2018 Manual
annotations
5-fold cross validation +
Hold-out validation (train,
test)
No Yes, potential to be replicated [99]
Zhang 2013 Existing
annotated
corpus
External Two different sets with
same settings
Yes, can be adapted to different
semantic categories and text genres
[100]
Zhou 2006 Manual
annotations
5-fold cross validation No Not listed [101]
Zhou 2011 Manual
retrospective
review
Hold-out validation (train,
test)
No Not listed [102]
Zhou 2014 Manual
annotations
Not used No Not listed [103]
a As reported by authors
Kersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 13 of 21
Table 5 Characteristics of the included studies
RESEARCH Open Access
Identifying disease trajectories with
predicate information from a knowledge
graph
Wytze J. Vlietstra1* , Rein Vos1,2, Marjan van den Akker3,4, Erik M. van Mulligen1 and Jan A. Kors1
Abstract
Background: Knowledge graphs can represent the contents of biomedical literature and databases as subject-
predicate-object triples, thereby enabling comprehensive analyses that identify e.g. relationships between diseases.
Some diseases are often diagnosed in patients in specific temporal sequences, which are referred to as disease
trajectories. Here, we determine whether a sequence of two diseases forms a trajectory by leveraging the predicate
information from paths between (disease) proteins in a knowledge graph. Furthermore, we determine the added
value of directional information of predicates for this task. To do so, we create four feature sets, based on two
methods for representing indirect paths, and both with and without directional information of predicates (i.e.,
which protein is considered subject and which object). The added value of the directional information of predicates
is quantified by comparing the classification performance of the feature sets that include or exclude it.
Results: Our method achieved a maximum area under the ROC curve of 89.8% and 74.5% when evaluated with
two different reference sets. Use of directional information of predicates significantly improved performance by 6.5
and 2.0 percentage points respectively.
Conclusions: Our work demonstrates that predicates between proteins can be used to identify disease trajectories.
Using the directional information of predicates significantly improved performance over not using this information.
Keywords: Knowledge graph, Disease trajectories, Predicates, Temporal relationships, Directionality of predicates,
Protein-protein interactions
Background
Knowledge graphs can be used to represent the biomed-
ical knowledge published in literature and databases [1].
Knowledge is formalized as subject-predicate-object tri-
ples, where pairs of entities are related to each other by
predicates [2]. By integrating triples from a variety of
sources, knowledge graphs can be used to perform com-
putational analyses on the comprehensive body of bio-
medical knowledge [3]. Previous work has used such
analyses to identify new relationships between pairs of
entities, e.g., between drugs and diseases [4, 5], genes
and phenotypes [6, 7], or between diseases [8, 9].
Much research has been performed with knowledge
graphs that only consist of proteins, commonly referred
to as protein-protein interaction networks. Through the
involvement of proteins in metabolic, signaling, immune,
and gene-regulatory networks, protein-protein inter-
action networks can help to mechanistically explain dis-
ease and physiological processes [1012]. Even though
predicates further specify the types of interactions be-
tween proteins, thereby providing additional information
that can be analyzed, protein-protein interaction net-
works usually do not use them. Instead, most methods
analyze the network topology of proteins [12]. However,
© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if
changes were made. The images or other third party material in this article are included in the article's Creative Commons
licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons
licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain
permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the
data made available in this article, unless otherwise stated in a credit line to the data.
* Correspondence: w.vlietstra@erasmusmc.nl
1Department of Medical Informatics, Erasmus University Medical Center, Dr.
Molewaterplein 50, 3015 GE Rotterdam, the Netherlands
Full list of author information is available at the end of the article
Vlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 
https://doi.org/10.1186/s13326-020-00228-8
we have recently shown that analyses that are performed
on protein knowledge graphs benefit from predicate in-
formation [13].
By using the predicates that specify the mechanisms
by which proteins interact, temporal pathobiological re-
lationships may also be identified, although this has not
been demonstrated yet. A key application for such tem-
poral analyses is the identification of disease trajectories,
which are commonly occurring temporal sequences of
diseases diagnosed in patients [14, 15]. An example of a
disease trajectory found in a study by Jensen et al. [14] is
rheumatoid arthritis-precedes-heart failure, where pre-
cedes is defined as occurs earlier in time. [] [16]. The
occurrence of the reverse, heart failure-precedes-
rheumatoid arthritis, was found to occur significantly
less frequently in the same study, and therefore was not
classified as a trajectory.
Identifying relationships between diseases is an im-
portant and popular research topic for protein-protein
interaction networks (see Related work section). In such
analyses diseases are represented by so-called disease
proteins, which are proteins encoded by genes that are
associated with a disease [17, 18]. Often cited benefits
include an improved understanding of the biological
mechanisms underlying disease interactions [8, 19, 20],
and the ability to anticipate the next disease, thereby
providing the knowledge necessary to improve treatment
plans and interventions [14, 21]. However, the temporal
aspects of relationships between diseases still require
further investigation. We therefore aim to automatically
determine whether a given sequence of two diseases
forms a trajectory. We do so by leveraging the predicate
information from paths between (disease) proteins in a
knowledge graph. We also determine whether there is
added value in using directional information of predi-
cates for this task.
Related work
Previous authors have mostly focused on identifying un-
directed relationships between diseases with protein net-
works [1923]. For example, Kontou et al. created a
disease-disease graph, where an edge between diseases
indicated that they shared at least one disease gene [23].
Sun et al. calculated the similarity between diseases
based on their shared disease proteins, shared physio-
logical processes associated with these proteins, or the
graph structures between the proteins [20]. Li and Agar-
wal identified which biological pathways were associated
with diseases via their disease proteins, and identified re-
lationships between diseases based on the number of
shared pathways [19]. Menche et al. identified so-called
disease modules, which are clusters of closely interre-
lated disease proteins [22]. They found that short dis-
tances between the modules of diseases were predictive
for pathobiological relationships. Contrary to Kontou
et al., they demonstrated that sharing disease proteins is
not a requirement for diseases to be related to each
other.
To our knowledge, Bang et al. were the only ones to
use a directed protein-protein interaction network to
identify disease trajectories [21]. The disease proteins of
pairs of diseases were used to identify shared biomolecu-
lar pathways, after which the locations of the disease
proteins within these pathways were determined. The
disease with most upstream disease proteins was classi-
fied as the first within the sequence of diseases. Add-
itionally, 13 million Medicare records were used to
calculate two relative risk scores for each pair of dis-
eases, corresponding with the two possible temporal se-
quences of the disease pair. If the sequence determined
with the protein pathways concurred with the sequence
that generated the largest relative risk, that sequence
was identified as a trajectory. Between a total of 2604
diseases, their method suggested 61 trajectories. These
were evaluated with the biomedical literature, where fur-
ther leads were found for 16 of them. Because the au-
thors only evaluated the trajectories that were suggested
by their method, it is unclear how many trajectories the
method failed to identify.
Materials & methods
Reference sets
The ability of our method to identify disease trajectories
was evaluated with two reference sets, which have iden-
tified disease trajectories by different means. The first
reference set consisted of statistically-derived disease tra-
jectories from a large retrospective study of Danish hos-
pital data, while the second set consisted of literature-
validated disease trajectories that were based on a small
prospective study of Dutch general-practitioner data.
Jensen reference set
The first reference set was based on a study of Jensen
et al. [14]. They retrospectively identified 4014 disease
trajectories from 6.2 million electronic patient records of
Danish hospitals based on diagnoses assigned over 14.9
years. All diagnoses in these patient records were repre-
sented as International Statistical Classification of Dis-
eases and Related Health Problems 10th Revision (ICD-
10) codes. Jensen used the hierarchy within the ICD-10
to aggregate all diagnoses to a high abstraction level,
resulting in 681 two-digit codes, such as Malignant neo-
plasm of breast (C50) or Type 2 diabetes mellitus
(E11).
Jensen derived the disease trajectories from the Danish
hospital data in a two-step process. First, they identified
sequences of two diseases that were diagnosed within 5
years from each other in at least 10 patients, and which
Vlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 2 of 11
had a relative risk higher than 1. Subsequently, the direc-
tion of each sequence had to be corroborated by a bino-
mial test that compared the frequency of the sequence to
the frequency of its reversed sequence. Sequences that ful-
filled both criteria were classified as disease trajectories.
To represent the diseases in the Jensen set on the pro-
tein level, we used the expert-annotated associations be-
tween proteins and diseases from the manually curated
subset of DisGeNet [18]. The Unified Medical Language
System (UMLS) MRCONSO table was used to map the
ICD-10 codes of the Jensen trajectories to the UMLS
identifiers that are used in DisGeNet. Two diseases, Ac-
cidental poisoning by and exposure to other gases and
vapours (E47) and Influenza due to identified zoonotic
or pandemic influenza virus (J09), were lost because
their ICD-10 codes could not be mapped to a UMLS
identifier. Because only 25% of the high-level diseases in
the Jensen set were represented within DisGeNet, we
used the narrower and child relationships from the
UMLS MRREL table to identify subclasses of all diseases.
By expanding the diseases with their subclasses, the per-
centage of diseases to which disease proteins could be
assigned was increased to 68% (465 of 679 diseases).
From the 4014 disease trajectories in the Jensen set,
there were 2530 trajectories where disease proteins
could be assigned to both diseases (63%). These 2530
trajectories, which were used as positive cases in this ref-
erence set, contained 453 of the 465 diseases to which
disease proteins could be assigned (97%). On average,
diseases had 90 disease proteins assigned to them (me-
dian: 29, interquartile range: 794). Disease proteins
were on average assigned to 6.2 diseases (median: 3,
interquartile range: 28).
A set of 168,870 non-trajectories was constructed by
creating all possible sequences of the 453 included dis-
eases, minus the trajectories that were described by Jen-
sen. The set of non-trajectories thereby included
random pairs of diseases, the reversed temporal se-
quences of these random pairs, as well as the reversed
temporal sequences of the trajectories. In the following,
we will refer to the trajectories and non-trajectories as
positive and negative cases to align with common ter-
minology in the machine learning field.
Van den Akker reference set
The second reference set was based on a prospective co-
hort study on disease susceptibility by Van den Akker
et al. [24]. They followed a Dutch cohort of 3460
patients over 2 years, during which their general practi-
tioner notes were examined for sequences of Inter-
national Classification of Primary Care (ICPC) codes
that represent chronic, permanent, and recurrent dis-
eases. In the Netherlands, each citizen is registered with
a general practitioner, who acts like a gatekeeper for
secondary and tertiary medical care, and is responsible
for maintaining a complete medical history of the
patient.
A total of 473 unique sequences of diseases were
found in this cohort, containing 122 distinct diseases.
Each sequence was manually evaluated using the pub-
lished biomedical literature and medical handbooks.
There were 65 sequences of diseases where the literature
stated that the first disease increased the susceptibility of
acquiring the second disease, and 408 sequences where
no evidence of increased susceptibility was found. To
maintain consistent terminology, we will refer to se-
quences with increased susceptibility as trajectories or
positives and to sequences without increased susceptibil-
ity as non-trajectories or negatives.
To assign disease proteins to these 122 diseases we
followed the same procedure as for the Jensen set by
using the MRCONSO table to map the ICPC codes to
UMLS identifiers, after which the MRREL table was used
to group them with their subclasses. Disease proteins
could be assigned to 97 diseases, which formed 55 tra-
jectories and 316 non-trajectories. On average, diseases
had 137 disease proteins assigned to them (median: 49,
interquartile range: 17167). Disease proteins were on
average assigned to 3 diseases (median: 2, interquartile
range: 14).
To determine whether our method could also identify
the correct temporal sequence of the trajectories, 54
additional non-trajectories were created by reversing the
sequence of the diseases in the literature-supported tra-
jectories (the reverse sequence of one trajectory was
already included as a non-trajectory in the data from the
general practitioners).
Knowledge graph
The predicates between proteins were extracted from
the Euretos Knowledge Platform (EKP), a commercially
available knowledge graph (http://www.euretos.com). In
the EKP, information from more than 200 knowledge
sources from a wide variety of domains in the life sci-
ences is represented as triples. The biomedical entities
such as proteins, drugs, or diseases that form the sub-
jects and objects of these triples are represented in the
knowledge graph as vertices, each of which has one or
more identifiers associated with it from external data-
bases. Mappings between the entities described in the
different knowledge sources underlying the knowledge
graph were made by matching their identifiers. The
predicate and provenance of each triple are specified as
part of an edge between the two vertices that represent
the subject and object. The direction of the predicate
goes from subject to object. The predicates in the under-
lying knowledge sources were matched to a standardized
set of 203 predicates. If an exact match was not
Vlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 3 of 11
available, a predicate was manually mapped. If there
were no explicit predicates in a database that was used
as a knowledge source, the predicates were manually de-
rived from the database schema. A path between two
vertices is defined as a sequence of triples, or possibly a
single triple, connecting the vertices.
The contents of the EKP are represented as documents
in a NoSQL database, which allows them to be flexibly
modelled and indexed. The EKP can be run on a
reasonably-powered server, requiring an 8-core proces-
sor and 60GB of memory as a minimum. It has previ-
ously been used in pre-clinical research for drug efficacy
screening [13], prioritizing existing drugs as repurposing
candidates for autosomal dominant polycystic kidney
disease [25], and pathway enrichment [26].
Feature sets & machine learning
The paths between the disease proteins were extracted
from the EKP. To keep our graph comprehensible, we
only extracted paths that consisted of one or two triples,
i.e., paths where two disease proteins are connected by
at most one intermediate protein. Within this range,
three scenarios for the paths between the disease proteins
of two diseases A and B were distinguished (Fig. 1.):
1) Overlap, where A and B share a disease protein,
optionally with a path to itself, e.g. a disease protein
of which two copies bind with each other
(homodimerization).
2) Direct path, where a disease protein of A and a
disease protein of B are part of one triple.
3) Indirect path, where one intermediate protein
connects the disease proteins of A and B, requiring
a sequence of two triples.
Two different methods to represent indirect paths be-
tween disease proteins were compared. The first method
constructed so-called metapaths [5], where the sequence
of predicates in an indirect path was used as single feature.
The second method, which we refer to as split paths, con-
sidered each predicate in the indirect paths as a separate
feature [13]. Each method was tested both with and with-
out directional information of predicates. Predicates were
either considered to all be undirected, or predicates were
categorized as being directed or undirected based on ex-
pert assessment (described in the Assessment of predicate
directionality section below), which we refer to as the
Mixed variation. In the overlap scenario, where the subject
and the object were the same protein, predicates were al-
ways considered to be undirected.
All features were binary. Figure 2 shows the four
feature sets that are derived from the example
shown in Fig. 1. We also experimented with feature
sets where all predicates were directed as indicated
by the subject and object of the triple in the EKP.
However, because some predicates are explicitly de-
fined as being undirected, using any directional in-
formation from triples with these predicates would
contradict these definitions. Nonetheless, for the
sake of completeness we have chosen to present
these results in Additional file 1.
Random forests were trained to classify the sequences of
diseases as positive or negative. Classification performance
Fig. 1 Schematic overview of the overlap, direct, and indirect scenarios that were extracted from the knowledge graph. Both diseases A and
disease B have three disease proteins (DP) associated with them according to the manually curated subset of DisGeNet. DisGeNet describes that
DP1 is known to be associated with both diseases, while the knowledge graph describes that it has a binds with relationship to itself. DP2 and
DP4 have a direct inhibits relationship, and DP3 and DP5 are connected through an indirect path, by an intermediate protein (IP). The arrows
between the proteins indicate which protein is the subject of the inhibits predicate, and which one its object. The binds with predicate was
considered to be undirected by the experts, and therefore does not have a direction. Based on the paths in the knowledge graph, four feature
sets are created, based on two methods to represent indirect paths, and both with and without the directional information of predicates
Vlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 4 of 11
was measured with the area under the receiver operator
characteristic curve (AUC) of a 10-fold cross-validation
experiment [27, 28]. We report the mean and standard de-
viation of the AUCs of 10 repeated cross-validation exper-
iments. The same folds that were used in the experiments
with undirected predicates were also used in the experi-
ments with directed predicates, after which the differences
between the test folds were tested for significance with a
two-sided, paired t-test.
To control for the differences in prevalence and num-
ber of cases between the two reference sets, we also re-
port the classification performance after undersampling
the number of positive and negative cases in the Jensen
set to match those in the Van den Akker set.
For the best performing classifiers we also report sensi-
tivity and specificity at the probability cutoff for which the
Youden index (sensitivity + specificity  1) is largest [29].
Machine learning and evaluation of results were per-
formed in R [30] with the packages caret [31], ranger
[32], and pROC [33].
Assessment of predicate directionality
Three experts with a strong biomedical background and
familiarity with knowledge graphs assessed the direction-
ality of 47 distinct predicates that were found in the ex-
tracted paths. They were provided with definitions of
these predicates which were obtained from the Pathway
Commons resource [34]. If not available, definitions
were sought through the National Library of Medicine
[35], or the OBO foundry [36]. The assessors could
categorize each predicate as directed, undirected, or
dont know. To establish directionality, a predicate had
to be categorized as directed or undirected by a majority
(i.e., two or three) of the assessors. Predicates that con-
tain a negation (e.g., does not interact with) were auto-
matically categorized the same as the corresponding
predicate without negation (interacts with), and there-
fore not presented to the assessors. For some predicates
the categorization was straightforward. For example,
Pathway Commons defines the predicate interacts with
as This is an undirected relation between participant
Fig. 2 The four feature sets that were derived from the paths between the disease proteins in Fig. 1. All features are binary: Black fields indicate a
True value, while empty fields indicate a False value. For the Mixed feature sets, the Binds with predicate is assessed to be undirected by
experts, while the Inhibits predicate is assessed to be directed
Vlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 5 of 11
proteins of a molecular interaction. [] , and the predi-
cate catalysis precedes as This relation defines di-
rected interactions between proteins. [] [34]. Six
predicates did not reach a majority in the first round
and were anonymously commented upon by the asses-
sors to motivate their categorization. These comments
were shared between the assessors, after which they
could update their initial choice. Each predicate was
then categorized with a majority.
Table 1 shows the 12 predicates that were categorized
as undirected by the three experts. The other 35 predi-
cates were categorized as directed. A complete overview
of the predicates can be found in Additional file 2.
Results
Extracted paths
In total, 6859 distinct disease proteins were assigned to
the diseases in both reference sets, three of which could
not be mapped to the EKP. Another 430 (6.3%) of the
disease proteins were not found in any of the extracted
paths. From these disease proteins, 314 had no relation-
ship to any other protein in the EKP.
The remaining 6426 disease proteins were involved in
1,338,310 direct paths and 833,546,575 indirect paths,
while 2581 disease proteins had 7354 paths to them-
selves. All paths were based on 2,015,738 distinct triples,
which contained 17,132 different proteins and 47 differ-
ent predicates.
The overlap scenario, where the two diseases in the
trajectory share at least one disease protein (scenario 1,
Feature sets & Machine learning section), occurred in
58% of the positive cases of the Jensen set, and 95% of
the positive cases of the Van den Akker set. No indirect
paths (scenario 3, Feature sets & Machine learning sec-
tion) were found between the disease proteins of 119
positive cases (4.7%), and 18,217 negative cases of the
Jensen set (10.8%), and one positive case (1.8%) and 15
negative cases (4.1%) of the Van den Akker set.
Classification results
The classification performance for both reference sets is
shown in Table 2. Mixed metapaths performed best,
achieving mean AUCs of 89.8% for the Jensen set and
74.5% for the Van den Akker set. Overall, classification
performance on the Van den Akker set was 9.9 to 15.3
percentage points lower than on the Jensen set, while
standard deviations were 9.6 to 11.3 percentage points
higher. Metapaths performed 4.1 to 7.0 percentage
points better than split paths. The performance of the
mixed feature sets was 1.9 to 6.5 percentage points
higher than the undirected feature sets. All differences
between mixed and undirected feature sets were signifi-
cant (p-values for Jensen metapaths and split paths: <
0.001; Van den Akker metapaths: 0.02, split paths 0.001).
To quantify how much of the difference in AUC be-
tween the two reference sets could be attributed to their
difference in size, the Jensen set was undersampled to
the same number of positive and negative cases as the
Van den Akker set. With the exception of the mixed
metapaths, performance dropped below the performance
that was achieved with the Van den Akker set. The
standard deviations also increased from 0.91.7% to 8.4
12.3%. The latter values are comparable to the standard
deviations on the Van den Akker set.
Figure 3 shows the receiver operating characteristic
(ROC) curves of the mixed metapath classifiers that per-
formed best. For the Jensen set, sensitivity and specificity
at the maximum Youden index were 79.2% and 82.4%,
respectively, while for the Van den Akker set these were
73.6% and 64.3%.
Error analysis
For our best classifier (mixed metapath features, trained
on the Jensen set), we analyzed the top-15 false-positive
and the top-15 false-negative cases, searching the litera-
ture for information that might explain the errors. The
results of our analysis of the false positives are shown in
Table 3. Overall, the first 10 out of the top 15 false posi-
tives appear to be omissions from the Jensen set rather
than misclassifications. For two false-positive cases, po-
tential mechanisms have been suggested, but the current
evidence is inconclusive on whether those mechanisms
are valid. For the remaining three false-positive cases no
literature could be found, which may therefore be inter-
esting leads for further investigation.
Table 4 shows the results for the top-15 false nega-
tives. For six false negatives, the second disease was
likely to be caused by the treatment of the first disease.
For example, the radiation that is used to treat the ma-
lignant neoplasm of the larynx may compromise the
Table 1 Predicates categorized as undirected as a result of the
assessment process
Undirected Predicates
binds with
coexists with
does not coexist with
forms protein complex with
interacts with
does not interact with
is associated with
is compared with
is functionally related to
is spatially related to
is the same as
ortholog is associated with
Vlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 6 of 11
immune system around the throat and mouth, thereby
increasing susceptibility to oropharyngeal candidiasis
[54]. Two false-negative trajectories are likely to have
mechanical causes, rather than molecular pathways. The
trajectory from malignant neoplasms of the ovary to nu-
trient deficiency can be explained by the blocking of the
intestines by the ovarian tumor, thereby blocking the en-
tire digestive system [53]. For four of the false-negative
trajectories, no description could be found in the litera-
ture, making their assessment impossible. Assessment of
the three remaining false negatives is speculative. For ex-
ample, the trajectory from transient ischemic attacks
(TIA) to vitamin B12 deficiencies may be an artifact of
the medical record keeping. Vitamin B12 is known to
protect against TIAs [52], so what may often happen is
that a vitamin B12 deficiency is only diagnosed after the
more acute TIA has been treated in an emergency room.
Discussion
Our work demonstrates that disease trajectories can be
identified with the predicates between proteins in a know-
ledge graph. To do so, our machine-learning based meth-
odology needed to successfully identify both the correct
pairs of diseases, as well as their correct temporal se-
quences. Overall, representing indirect paths as metapaths
performed superior as compared to representing them as
split paths. Using the directional information of predicates
significantly improved performance over not using this
information. Undersampling the Jensen set to the same
number of positive and negative cases as the Van den
Akker set showed that its lower performance and higher
standard deviations could partially be explained by its small
size.
In previous work, Bang et al. [21] identified disease trajec-
tories by calculating the relative risk between two diseases
and combining this with the relative position of disease pro-
teins in biomolecular pathways. Their method is fully
dependent on shared disease proteins between the two dis-
eases, whereas our method also works when there are only
(in) direct paths between disease proteins. In the Jensen set,
this holds for 42% of the trajectories. Performance compari-
son of the methods is difficult because Bang et al. only vali-
dated the disease trajectories that were suggested by their
method, but did not validate the non-trajectories. Thus,
only the precision of their method can be calculated but no
insight is provided in the number of false-negative trajec-
tories. A final complication for the comparison between the
two methods is the claim of Bang et al. to discover causal
relationships between diseases, rather than only temporal
ones. Unfortunately, they refer to an example to define
causal relationships between diseases, making it difficult to
pinpoint how these differ from disease trajectories.
Although we do not foresee direct clinical application
of our work, our high performance may persuade ex-
perts to further examine the protein paths underlying
some positively classified trajectories, either known or
Table 2 Classification results for the four feature sets for both reference sets
Jensen set Jensen set - undersampled Van den Akker set
Metapaths Split paths Metapaths Split paths Metapaths Split paths
Undirected 83.3 (1.7) 78.3 (1.7) 64.2 (12.1) 61.9 (12.3) 72.5 (11.8) 68.4 (13.0)
Mixed 89.8 (0.9) 82.8 (1.2) 82.3 (8.4) 69.6 (13.1) 74.5 (10.5) 70.3 (11.4)
The values in the columns indicate the mean AUC and its standard deviation in % of 10 cross-validation experiments
Fig. 3 ROC curves of the mixed metapaths classifiers for the Jensen set and the Van den Akker set
Vlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 7 of 11
newly suggested. Interpreting these protein paths might
provide additional clues about the mechanism through
which the first disease leads to the second. Identifying
and understanding these mechanisms is likely to im-
prove prevention, prediction of disease progression,
intervention, and drug development, thereby indirectly
supporting clinical practice and health-care policy. For
now, such detailed examinations of the protein paths
were beyond the scope of this project.
A downside of working on the protein level was that not
all disease trajectories could be studied. More than a third
of the trajectories of the Jensen set, and a fifth of the Van
den Akker set was lost because disease proteins could not
be assigned to one or both of the diseases in a trajectory.
Even when disease proteins could be assigned to both dis-
eases, alternative explanations were sometimes more
plausible. For example, our analysis of the false-negative
cases suggested that some trajectories could be explained
mechanically, or were likely due to a side effect of the
treatment for the first disease. To determine the true per-
formance of our method, a validated set of trajectories that
are caused by biomolecular mechanisms would be needed.
Alternatively, the range of trajectories that can be analyzed
may be broadened by linking diseases to other types of
disease information available in the EKP, e.g., information
about drugs or physiological processes.
The two reference sets that were used in this research
were both based on patient data, but differed in many
other respects. The sequences of diseases in the Jensen
set were classified as trajectories based on statistics
calculated from 15 years of nationwide hospital data.
Despite this large volume of data, our analysis of the
false-positive cases showed that the set of trajectories
was incomplete. The literature evaluation underlying the
Van den Akker set ensures that such omissions are un-
likely to occur there. Furthermore, the negatives in the
Van den Akker set either were observed in patients, or
were reversals of literature-supported trajectories. Be-
cause the negative cases in the Jensen set were based on
randomization, this set is likely to contain pairs of dis-
eases that never co-occur within patients. Finally, the
types of diagnoses within the trajectories differ between
the two reference sets. The hospital patients in the
Jensen set are more likely to suffer from more serious
and complicated diseases than patients visiting a general
practitioner in the Van den Akker set. On the other
hand, the Van den Akker set only included chronic, per-
manent, and recurring diseases, thereby excluding dis-
eases that are acute and rapidly treatable.
Only the definitions from Pathway Commons stated
whether the predicate was directed or not. The defini-
tions of predicates from other knowledge sources,
Table 3 Assessment of the top 15 false-positive trajectories
First disease ICD-10 Second disease ICD-10 Assessment
Mental and behavioural disorders due to
use of alcohol
F10 Alzheimers disease G30 Described in literature [37]
Essential (primary) Hypertension I10 Alzheimers disease G30 Described in literature [38]
Osteoporosis without pathological fracture M81 Alzheimers disease G30 Described in literature [39]
Non-insulin-dependent diabetes mellitus E11 Alzheimers disease G30 Described in literature [40]
Other disorders of pancreatic internal
secretion
E16 Alzheimers disease G30 Described in literature [41]
Schizophrenia F20 Other septicaemia A41 Described in literature, but commonly
occurs via intermediate diseases such
as agranulocytosis and pneumonia [42]
Lupus erythematosus L93 Other disorders of
urinary system
N39 Described in literature [43]
Disorders of vestibular function H81 Alzheimers disease G30 Described in literature [44]
Lupus erythematosus L93 Respiratory failure, not
elsewhere classified
J96 Described in literature [45]
Unspecified Dementia F03 Dementia in Alzheimers
Disease
F00 Further specification of diagnosis
Retinal vascular occlusions H34 Cystitis N30 No relationship found in literature
Chronic ischaemic heart disease I25 Other septicaemia A41 Cardiac troponins are suggested to be
biomarkers for sepsis [46]
Hyperplasia of prostate N40 Alzheimers disease G30 No relationship found in literature
Hyperparathyroidism and other disorders
of parathyroid gland
E21 Alzheimers disease G30 Suggested in literature (via calcium) [47]
Asthma J45 Umbilical hernia K42 No relationship found in literature
Vlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 8 of 11
including the National Library of Medicine, left room
for interpretation. As a result, six predicates required a
second round of assessment before a majority was
achieved between the assessors. With ontologies playing
increasingly important roles in data standardization and
sharing [58], the directionality of predicates should al-
ways be clear. The Relationship Ontology already sup-
ports categorization of predicates as directed or
undirected, which it refers to as asymmetric or symmet-
ric predicates, but unfortunately is far from complete
and did not cover the predicates in our set [59].
A potential new application for our method would be
to identify trajectories for rare and low-prevalence dis-
eases, where insufficient patient data is available for
studies such as those performed by Jensen or Van den
Akker. Because our method identifies trajectories based
on a protein network, it is independent of the prevalence
of a disease. Furthermore, many of the estimated 5 to 8
thousand rare diseases have well known genetic causes
[60], making them highly suitable to be analyzed with
our method.
A possible extension of our work would be the identi-
fication of longer disease trajectories, e.g. the trajectories
consisting of sequences of four diseases that were also
described by Jensen et al. [14]. However, as far as we are
aware all available knowledge-graph methods limit
themselves to identifying relationships between two en-
tities. Expanding the current methods to identify longer
sequences should therefore be a separate investigation.
Conclusions
Our work demonstrates that disease trajectories can be
identified with the predicate information from a know-
ledge graph. We also demonstrate and quantify the
added value of using directional information of predi-
cates for this task. Our work thereby enables the discov-
ery of temporal relationships with predicate information
from knowledge graphs.
Supplementary information
Supplementary information accompanies this paper at https://doi.org/10.
1186/s13326-020-00228-8.
Additional file 1. Description and results of the directed variation
feature sets. This file describes the feature sets and classification results of
the variation where all predicates in the feature sets have a direction as
specified by their triples in the knowledge graph. Their categorization as
directed or undirected by the assessors was not used in this variation.
Figure S1 shows an example of the feature sets derived from Fig. 1, with
Table 4 Assessment of the top 15 false-negative trajectories
First disease ICD-10 Second disease ICD-10 Assessment
Thyrotoxicosis [hyperthyroidism] E05 Other disorders of eye
and adnexa
H57 Likely side effect of treatment [48]
Irritable bowel syndrome K58 Spondylosis M47 No relationship found in literature
Vitamin B12 deficiency anaemia D51 Other septicaemia A41 Vitamin B12 has been hypothesized as treatment
for sepsis [49]
Mental and behavioural disorders due
to use of alcohol
F10 Acute and transient
psychotic disorders
F23 Described in literature, but no clear role for protein
interactions [50]
Gonarthrosis [arthrosis of knee] M17 Erysipelas A46 No relationship found in literature
Senile cataract H25 Other disorders of lens H27 Likely side effect of treatment [51]
Transient cerebral ischaemic attacks
and related syndromes
G45 Vitamin B12 deficiency
anaemia
D51 Only reverse described in literature, that vitamin B12
protects against stroke [52]
Malignant neoplasm of ovary C56 Deficiency of other
nutrient elements
E61 Likely mechanical cause [53]
Malignant neoplasm of larynx C32 Candidiasis B37 Likely side effect of treatment [54]
Other intervertebral disc disorders M51 Somatoform disorders F45 No relationship found in literature
Gonarthrosis [arthrosis of knee] M17 Other local infections
of skin and subcutaneous
tissue
L08 No relationship found in literature
Benign neoplasm of brain and other
parts of central nervous system
D33 Other septicaemia A41 Likely intermediate through infection, which follows
surgery or weakening of the immune system after
(radiation) treatment
Insulin-dependent diabetes mellitus E10 Other disorders of eye
and adnexa
H57 Diabetes is a risk factor for many eye diseases [55],
but it is not clear whether these fall under this ICD-10
code
Noninflammatory disorders of ovary,
fallopian tube and broad ligament
N83 Ventral hernia K43 Likely side effect of treatment [56]
Other intervertebral disc disorders M51 Other polyneuropathies G62 Likely mechanical cause [57]
Vlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 9 of 11
the difference that in this variation the Binds with predicate also is
directed. Table S1 shows the classification performance of the directed
feature sets along with the performances of the undirected and the
mixed variations. Table S2 shows the p-values of the two-sided paired t-
tests between all variations.
Additional file 2. Overview of predicates that were found in the paths.
This file contains Table S3, which shows the 47 predicates that connect
proteins in the knowledge graph and were used to construct the
features.
Abbreviations
AUC: Area under the receiver operator characteristic curve; DP: Disease
Protein; EKP: Euretos Knowledge Platform; ICD-10: International Statistical
Classification of Diseases and Related Health Problems 10th Revision;
ICPC: International Classification of Primary Care; IP: Intermediate Protein;
ROC: Receiver Operating Characteristic curve; TIA: Transient Ischemic Attack;
UMLS: Unified Medical Language System
Acknowledgements
We would like to thank Euretos B.V. for providing access to the Euretos
Knowledge Platform, and Drs. Anneke M. Sijbers and Solène Grosdidier for
their help in assessing the predicates.
Authors contributions
WV, RV, EvM, and JK designed the study. WV created the feature sets,
performed the error analysis, and drafted the manuscript. WV and RV
performed the data analyses. MvdA and RV supplied the Van den Akker
reference set. RV, EvM and JK supervised the study and critically revised the
manuscript. All authors read and approved the final manuscript.
Funding
No funding was received for this project.
Availability of data and materials
The datasets and scripts that are used in this study are available at
https://github.com/Wytz/DiseaseTrajectories
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Author details
1Department of Medical Informatics, Erasmus University Medical Center, Dr.
Molewaterplein 50, 3015 GE Rotterdam, the Netherlands. 2Department of
Methodology & Statistics, Maastricht University, PO Box 616, 6200 MD
Maastricht, the Netherlands. 3Institute of General Practice, Johann Wolfgang
Goethe University, Theodor-Stern-Kai 7, D-60590 Frankfurt, Germany.
4Department of Family Medicine, Maastricht University, PO Box 616, 6200 MD
Maastricht, the Netherlands.
Received: 14 February 2020 Accepted: 12 August 2020
RESEARCH Open Access
Disclosing Main authors and Organisations
collaborations in bioprinting through
network maps analysis
Leonardo Azael García-García1* and Marisela Rodríguez-Salvador2
Abstract
Background: Scientific activity for 3D bioprinting has increased over the past years focusing mainly on fully functional
biological constructs to overcome issues related to organ transplants. This research performs a scientometric analysis on
bioprinting based on a competitive technology intelligence (CTI) cycle, which assesses scientific documents to establish
the publication rate of science and technology in terms of institutions, patents or journals. Although analyses of
publications can be observed in the literature, the identification of the most influential authors and affiliations has not
been addressed. This study involves the analysis of authors and affiliations, and their interactions in a global framework.
We use network collaboration maps and Betweenness Centrality (BC) to identify of the most prominent actors in
bioprinting, enhancing the CTI analysis.
Results: 2088 documents were retrieved from Scopus database from 2007 to 2017, disclosing an exponential growth
with an average publication increase of 17.5% per year. A threshold of five articles with ten or more cites was
established for authors, while the same number of articles but cited five or more times was set for affiliations. The
author with more publications was Atala A. (36 papers and a BC = 370.9), followed by Khademhosseini A. (30
documents and a BC = 2104.7), and Mironov (30 documents and BC = 2754.9). In addition, a small correlation was
observed between the number of collaborations and the number of publications. Furthermore, 1760 institutions with a
median of 10 publications were found, but only 20 within the established threshold. 30% of the 20 institutions had an
external collaboration, and institutions located in and close to the life science cluster in Massachusetts showed a strong
cooperation. The institution with more publications was the Harvard Medical School, 61 publications, followed by the
Brigham and Womens hospital, 46 papers, and the Massachusetts Institute of Technology with 37 documents.
Conclusions: Network map analysis and BC allowed the identification of the most influential authors working on
bioprinting and the collaboration between institutions was found limited. This analysis of authors and affiliations and
their collaborations offer valuable information for the identification of potential associations for bioprinting researches
and stakeholders.
Keywords: Network map analysis, Betweenness centrality, Bioprinting, Text mining, Collaboration analysis,
scientometrics, competitive technology intelligence
© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if
changes were made. The images or other third party material in this article are included in the article's Creative Commons
licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons
licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain
permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the
data made available in this article, unless otherwise stated in a credit line to the data.
* Correspondence: L.A.Garcia-Garcia@sussex.ac.uk
1University of Sussex, School of Engineering and Informatics, Falmer,
Brighton, UK
Full list of author information is available at the end of the article
García-García and Rodríguez-Salvador Journal of Biomedical Semantics
           (2020) 11:3 
https://doi.org/10.1186/s13326-020-0219-z
Background
Research articles are public documents that report scien-
tific advancements to share knowledge and promote devel-
opment in science. These documents contain fundamental
information regarding not only to research but also to the
organizations and authors involved. This data is of interest
to identify leading organizations and to map scientific
collaborations.
Scientometric tools such as co-citation analysis, biblio-
graphic coupling, or co-author analysis can help to achieve
these goals. Co-citation analysis and bibliographic coupling
are mainly used to measure the flow of information based
on the documents selected by authors, while co-author
analysis is more focused on the analysis of collaboration be-
tween authors, taking into consideration the social aspect
of the research collaboration. Furthermore, co-author ana-
lysis has been proved to be useful to determine the multi
and interdisciplinary of the institutions and their collabora-
tions [1]. Co-author analysis requires information related to
authors aliases, affiliations, publications, areas of research,
and their collaborations. This information can be obtained
from digital libraries (DL) aimed to create systems for the
identification of authors such as ORCID, which was created
by non-profit organizations, or ResearcherID, Scopus,
PubMED or Web of Science, which are companies that are
developing their unique identifiers for authors [24]. When
evaluating advances in science and technology, names of
authors and affiliations become major indicators, as 1) their
number of citations by peers correlates to their acknow-
ledgment as influential on their area of research [5] and 2)
contributes to determining the specific disciplines involved
in the research [1], both are important elements to nurture
the decision-making process. In this sense, Competitive
Intelligence (CI) acquires a relevant role, through the defin-
ition, collection, analysis, and presentation of relevant infor-
mation [6]. The CI process can be further enhanced by
incorporating feedback form experts to validate the infor-
mation obtained [7]. CI is fundamental to research and de-
velopment (R&D), including products or processes with
radical novelty, such as bioprinting.
Bioprinting is an emerging technology, a variant of addi-
tive manufacturing that involves the fabrication of 3D con-
structs for living tissues and organs [8, 9]. This discipline
is growing at an accelerated pace, involving branches of
knowledge such as biology and engineering. Bioprinting
has been developed to assist the needs of a fast-growing
population. This technique has potential social and eco-
nomic impacts [10, 11], including a huge effect in organ
transplants, where one of the main objectives is the print-
ing of functional biological structures to help in the short-
age of organs, thus overcoming long waiting lists and
issues related to the transplanted organs such as rejection
[1012]. Although there have been significant signs of pro-
gress in the past years, there are some areas of research to
be explored in this incipient technology [11]. Since acad-
emy and industry have acknowledged that bioprinting will
have a significant impact on the health-care sector in the
following years, the identification of technology trends in
3D bioprinting [13, 14], including potential printing tech-
niques [15], becomes crucial to stay competitive and to
develop new technologies in this field. With this aim,
Rodriguez-Salvador et al. [7] performed a patentometric
and scientometric analysis in bioprinting to identify trends
and to explore the knowledge landscape of this technology.
In addition, they also identified the most prolific institu-
tions, being the MIT (113 publications) the number one,
followed by Nanyang Technology University (103 publica-
tions), and Tsinghua University (93 publications); They
also found that the three first countries with more publica-
tions were USA with 1491, followed by China with 744,
and Germany with 377 [7]. These analyses are mostly
based on the frequency of documents by affiliation and
country, and no inclusion or exclusion terms were set. The
insights obtained can be enhanced with the identification
of the leading scientists and their field of expertise, thus
distinguishing the principal areas of current research and
determining potential opportunities for R&D.
In order to unveil scientific and technological trends, it is
important to face big volumes of information using text
mining. This activity can be applied to identify and extract
potentially useful information from texts. It combines tools
such as machine learning, artificial intelligence, and statis-
tics to analyse large amounts of both structured and un-
structured data. The information obtained can contribute
to understanding patterns in data by making use of tools
such as text categorization, text clustering, information ex-
traction, among others [16]. Information retrieval, word
frequency analysis, word distribution, pattern recognition,
and visualisation techniques are some of the most frequent
practices [17]. As a conclusion, text mining adds important
value to the pattern recognition by structuring the content
of data from textual sources for research, data analysis,
business or competitive intelligence (CI) [1719].
A fundamental topic for the CI is the determination of
key players, such as the main organizations and authors in-
volved in scientific advancements. Network analysis can be
used to identify the collaboration in a visual pattern, where
either the authors or affiliations are represented by nodes
and their collaborations can be seen as the connection
among them. Moreover, the nodes with common attributes
of interest for the analysis can be grouped using clusteriza-
tion. Clusterization allows to group components with simi-
lar characteristics, such as research topics or techniques.
When clustering collaborations, the closer the nodes in au-
thors or affiliations network maps, the more similarities
they share [5, 20]. Furthermore, collaboration analysis can
be strengthened with the assessment of the Betweenness
Centrality (BC) to determine the level of association of the
García-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 2 of 13
nodes according to their position in the network. A
straightforward measurement of the association level can
be the connectivity, but it fails to disclose the importance
of a node. To overcome this, BC measure can be calculated
to evaluate the importance of a node and its social inter-
action in a network as this measure counts the number of
regions in the map connected by each element, setting
their importance in the flow of information [21, 22].
Scientometric and patentometric techniques have been
used recently to analyse the number publications per year,
the main authors, and organizations to determine the main
advancements in bioprinting (methods, materials, etc.)
[2325]. Scientometric and text mining can be used to de-
tect the authors and affiliations with more publications
and more influence in bioprinting. This information can
be an input for people looking for well-known experts in
bioprinting or state-of-the-art developments in the field.
To achieve the main goal of this paper, a customised
search query was used to gather documents from Scopus.
The query included keywords highly used in the most
cited papers on bioprinting. Two network maps of collab-
orations, one for authors and one for affiliations, were
generated and analysed. Further analyses were carried out
to estimate the BC measurement, and the relationship be-
tween number of publications and the number of collabo-
rations. These parameters were used for the identification
of the most prolific (those with more publications on this
topic) and important authors and institutions involved in
the publications of advances in bioprinting.
This analysis is the first attempt to undertake a quantita-
tive analysis using a network analysis approach and the
calculation of centrality measurements to strengthen the
CI methodologies. The findings enhance the perception of
the importance of collaborations among institutions for
the generation of high-quality scientific outcomes and for
the dissemination of the knowledge generated, helping
both researchers and stakeholders in the identification of
potential opportunities for research and collaboration.
Methods
This paper is focused on the network analysis of authors
and institutions from scientific publications in bioprinting.
The analysis comprises both, a network analysis on the
collaboration among institutions and one that deals with
the collaboration among authors. The network maps were
generated in Gephi, an open-source software for network
analysis [2634]. Betweenness centrality was calculated
for both, authors and institutions collaborations.
The adequate identification of specific keywords on the
topic of interest is a determining step in the search strat-
egy, as they contribute to the appropriate establishment of
the search queries. A preliminary search in Scopus using
only the term bioprinting with no period of time defined
was the first stage of this research. Scopus was selected for
the information retrieval as this is a major scientific data-
base that includes information from more than 20,000 sci-
entific journals [35]. The ten most cited papers identified
through this search were selected, as they have been
acknowledged as referents for the topic. Table 1, García-
García[36], shows the ten articles that formed the first set
of documents. These papers were used to identify the key-
words to form the search queries. A text mining program
was specially coded to carry out the text-mining of these
publications, thus determining the most relevant keywords
on the topic. With a broader range of terms and their syn-
onyms we guarantee the inclusion of a wide range of pub-
lications compared to searches performed using only the
term bioprinting. Three different types of keywords were
searched in the whole text of the papers, being 1) the most
frequent terms, 2) terms containing the word bio, and 3)
the collocations, which are the juxtapositions of two words
with a greater frequency. A cleaning of terms was accom-
plished manually afterward to sort them out according to
specialized language of the subject. The identified key-
words were separated by subtopics (i. e. technology,
process, and application) to form the search queries. A set
of 23 searches were performed with the selected termin-
ology prior to the development of the definite query.
These searches were used to identify the correct grouping
of terms and the exclusion terms.
The search query was formed using the keywords previ-
ously identified in combination with Boolean and proxim-
ity operators, and exclusion terms. For this stage, the
definite search was carried out by defining the period of
time, from 1 January 2000 to 15 November 2017 (when
the information collection was concluded). The main
query is observed in the appendix A1. The collection activ-
ity involved the use of the query to search in title, abstract
and keywords. A quick review of titles and abstracts of the
documents found was carried out to discard those not
related to bioprinting.
The bibliographic information of the documents identi-
fied in Scopus was retrieved and exported in a CSV format
to be cleaned and analysed. A cleaning process and the
complete normalization of the data was carried out to
standardize authors and affiliations names. We performed
a manual name disambiguation for both authors and affili-
ations. The two authors analysed manually all the names
on each one of the publications gathered. Every time a
similar name was observed, name disambiguation was car-
ried out by looking to the full name, affiliation, and e-mail.
The level of agreement on the disambiguation performed
by the authors was measured using Cohens kappa [37].
Co-author analysis was limited exclusively to the informa-
tion of the publications gathered and we did not require
further information from available DLs.
The analysis to identify the most influential authors
and affiliations was carried out by setting a threshold for
García-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 3 of 13
Ta
b
le
1
C
om
pa
ris
on
of
th
e
to
p
te
n
ci
te
d
pa
pe
rs
fro
m
Sc
op
us
ob
ta
in
ed
fro
m
th
e
se
ar
ch
of
`b
io
pr
in
tin
g
an
d
th
e
de
ve
lo
pe
d
se
ar
ch
qu
er
y
in
tit
le
s,
ab
st
ra
ct
s,
or
ke
yw
or
ds
To
p
te
n
re
su
lts
us
in
g
th
e
ke
yw
or
d
bi
op
rin
tin
g
[3
6]
To
p
te
n
ar
tic
le
s
us
in
g
th
e
de
ve
lo
pe
d
se
ar
ch
st
rin
g
Ti
tle
A
ut
ho
rs
Ye
ar
So
ur
ce
C
ite
s
Ti
tle
A
ut
ho
r
Ye
ar
So
ur
ce
C
ite
s
1
3D
bi
op
rin
tin
g
of
tis
su
e
an
d
or
ga
ns
[3
8]
.
M
ur
ph
y,
S.
V.
,
A
ta
la
,A
.
20
14
N
at
ur
e
Bi
ot
ec
hn
ol
og
y
32
(8
),
pp
.7
73
7
85
14
98
3D
bi
op
rin
tin
g
of
tis
su
es
an
d
or
ga
ns
[3
8]
.
M
ur
ph
y
S.
V.
,A
ta
la
A
.
20
14
N
at
ur
e
Bi
ot
ec
hn
ol
og
y
32
(8
),
pp
.
77
3
78
5.
14
98
2
Sc
af
fo
ld
-fr
ee
va
sc
ul
ar
tis
su
e
en
gi
ne
er
in
g
us
in
g
bi
op
rin
tin
g
[3
9]
.
N
or
ot
te
,C
.,
M
ar
ga
,
F.
S.
,N
ik
la
so
n,
L.
E.
,
Fo
rg
ac
s,
G
.
20
09
Bi
om
at
er
ia
ls
30
(3
0)
,p
p.
59
10
5
91
7
60
0
M
ic
ro
sc
al
e
te
ch
no
lo
gi
es
fo
r
tis
su
e
en
gi
ne
er
in
g
an
d
bi
ol
og
y
[4
0]
.
Kh
ad
em
ho
ss
ei
ni
A
.,
La
ng
er
R.
,B
or
en
st
ei
n
J.,
Va
ca
nt
iJ
.P
.
20
06
Pr
oc
.N
at
l.
Ac
ad
.
Sc
i.
U
.S
.A
.,
10
3
(8
),
pp
.2
48
0
24
87
.
11
63
3
3D
bi
op
rin
tin
g
of
va
sc
ul
ar
iz
ed
,
he
te
ro
ge
ne
ou
s
ce
ll-
la
de
n
tis
su
e
co
ns
tr
uc
ts
[2
4]
.
Ko
le
sk
y,
D
.B
.,
Tr
ub
y,
R.
L.
,G
la
dm
an
,A
.S
.,
H
om
an
,K
.A
.,
Le
w
is
,J
.A
.
20
14
Ad
va
nc
ed
M
at
er
ia
ls
26
(1
9)
,p
p.
31
24
3
13
0
58
8
C
lin
ic
al
tr
an
sp
la
nt
at
io
n
of
a
tis
su
e-
en
gi
ne
er
ed
ai
rw
ay
[4
1]
.
M
ac
ch
ia
rin
iP
.,
Ju
ng
eb
lu
th
P.
,
G
o
T.
,A
sn
ag
hi
M
.A
.,
Re
es
L.
E.
,
C
og
an
T.
A
.,
D
od
so
n
A
.,
M
ar
to
re
ll
J.,
Be
lli
ni
S.
,P
ar
ni
go
tt
o
P.
P.
,
D
ic
ki
ns
on
S.
C
.,
H
ol
la
nd
er
A
.P
.,
M
an
te
ro
S.
,C
on
co
ni
M
.T
.,
Bi
rc
ha
ll
M
.A
.
20
08
Th
e
La
nc
et
37
2
(9
65
5)
,p
p.
20
23
2
03
0.
10
14
4
Pr
in
tin
g
an
d
pr
ot
ot
yp
in
g
of
tis
su
es
an
d
sc
af
fo
ld
s
[2
3]
.
D
er
by
,B
.
20
12
Sc
ie
nc
e
33
8
(6
10
9)
,
pp
.9
21
9
26
51
0
M
ec
ha
ni
ca
lp
ro
pe
rt
ie
s
an
d
ce
ll
cu
ltu
ra
lr
es
po
ns
e
of
po
ly
ca
pr
ol
ac
to
ne
sc
af
fo
ld
s
de
si
gn
ed
an
d
fa
br
ic
at
ed
vi
a
fu
se
d
de
po
si
tio
n
m
od
el
lin
g
[4
2]
.
H
ut
m
ac
he
r
D
.W
.,
Sc
ha
nt
z
T.
,
Ze
in
I.,
N
g
K.
W
.,
Te
oh
S.
H
.,
Ta
n
K.
C
.
20
01
Jo
ur
na
lo
f
Bi
om
ed
ic
al
M
at
er
ia
ls
Re
se
ar
ch
55
(2
),
pp
.2
03
2
16
.
93
9
5
A
dd
iti
ve
m
an
uf
ac
tu
rin
g
of
tis
su
es
an
d
or
ga
ns
[4
3]
.
M
el
ch
el
s,
F.
P.
W
.,
D
om
in
go
s,
M
.A
.N
.,
Kl
ei
n,
T.
J.,
Ba
rt
ol
o,
P.
J.,
H
ut
m
ac
he
r,
D
.W
.
20
12
Pr
og
re
ss
in
Po
ly
m
er
Sc
ie
nc
e
37
(8
),
pp
.
10
79
1
10
4
49
5
So
lid
fre
ef
or
m
fa
br
ic
at
io
n
of
th
re
e-
di
m
en
si
on
al
sc
af
fo
ld
s
fo
r
en
gi
ne
er
in
g
re
pl
ac
em
en
t
tis
su
es
an
d
or
ga
ns
[4
4]
.
Le
on
g
K.
F.
,C
he
ah
C
.M
.,
C
hu
a
C
.K
.
20
03
Bi
om
at
er
ia
ls
24
(1
3)
,p
p.
23
63
2
37
8.
73
9
6
25
th
an
ni
ve
rs
ar
y
ar
tic
le
:E
ng
in
ee
rin
g
hy
dr
og
el
s
fo
r
bi
of
ab
ric
at
io
n
[4
5]
.
M
al
da
,J
.,
Vi
ss
er
,J
.,
M
el
ch
el
s,
F.
P.
,G
ro
ll,
J.,
H
ut
m
ac
he
r,
D
.W
.
20
13
Ad
va
nc
ed
M
at
er
ia
ls
25
(3
6)
,p
p.
50
11
5
02
8
46
5
St
em
ce
ll-
ba
se
d
tis
su
e
en
gi
ne
er
in
g
w
ith
si
lk
bi
om
at
er
ia
ls
[4
6]
.
W
an
g
Y.
,K
im
H
.-J
.,
Vu
nj
ak
-
N
ov
ak
ov
ic
G
.,
Ka
pl
an
D
.L
.
20
06
Bi
om
at
er
ia
ls
27
(3
6)
,p
p.
60
64
6
08
2.
65
7
7
A
3D
bi
op
rin
tin
g
sy
st
em
to
pr
od
uc
e
hu
m
an
-s
ca
le
tis
su
e
co
ns
tr
uc
ts
w
ith
st
ru
ct
ur
al
in
te
gr
ity
[4
7]
.
Ka
ng
,H
.-W
.,
Le
e,
S.
J.,
Ko
,I
.K
.,
Yo
o,
J.J
.,
A
ta
la
,A
.
20
16
N
at
ur
e
Bi
ot
ec
hn
ol
og
y
34
(3
),
pp
.3
12
3
19
46
6
Sc
af
fo
ld
-fr
ee
va
sc
ul
ar
tis
su
e
en
gi
ne
er
in
g
us
in
g
bi
op
rin
tin
g
[3
8]
.
N
or
ot
te
C
.,
M
ar
ga
F.
S.
,
N
ik
la
so
n
L.
E.
,F
or
ga
cs
G
.
20
09
Bi
om
at
er
ia
ls
30
(3
0)
,p
p.
59
10
5
91
7
60
0
8
Pr
in
tin
g
th
re
e-
di
m
en
si
on
al
tis
su
e
an
al
og
ue
s
w
ith
de
ce
llu
la
riz
ed
ex
tr
ac
el
lu
la
r
m
at
rix
bi
oi
nk
[4
8]
.
Pa
ti,
F.
,J
an
g,
J.,
H
a,
D
.-H
.,
Ki
m
,D
.-H
.,
C
ho
,D
.-W
.
20
14
N
at
ur
e
Co
m
m
un
ic
at
io
ns
53
,9
35
41
2
O
rg
an
pr
in
tin
g:
Ti
ss
ue
sp
he
ro
id
s
as
bu
ild
in
g
bl
oc
ks
[4
9]
.
M
iro
no
v
V.
,V
is
co
nt
iR
.P
.,
Ka
sy
an
ov
V.
,F
or
ga
cs
G
.,
D
ra
ke
C
.J.
,M
ar
kw
al
d
R.
R.
20
09
Bi
om
at
er
ia
ls
30
(1
2)
,p
p.
21
64
2
17
4.
59
4
9
Ti
ss
ue
en
gi
ne
er
in
g
by
se
lf-
as
se
m
bl
y
an
d
bi
o-
pr
in
tin
g
of
liv
in
g
ce
lls
[5
0]
.
Ja
ka
b,
K.
,N
or
ot
te
,C
.,
M
ar
ga
,F
.,
Vu
nj
ak
-
N
ov
ak
ov
ic
,G
.,
Fo
rg
ac
s,
G
.
20
10
Bi
of
ab
ric
at
io
n
2
(2
),0
22
00
1
29
0
3D
bi
op
rin
tin
g
of
va
sc
ul
ar
iz
ed
,
he
te
ro
ge
ne
ou
s
ce
ll-
la
de
n
tis
su
e
co
ns
tr
uc
ts
[2
4]
.
Ko
le
sk
y
D
.B
.,
Tr
ub
y
R.
L.
,
G
la
dm
an
A
.S
.,
Bu
sb
ee
T.
A
.,
H
om
an
K.
A
.,
Le
w
is
J.A
.
20
14
Ad
va
nc
ed
M
at
er
ia
ls
26
(1
9)
,p
p.
31
24
3
13
0
58
8
10
3D
Bi
op
rin
tin
g
of
he
te
ro
ge
ne
ou
s
ao
rt
ic
va
lv
e
co
nd
ui
ts
w
ith
al
gi
na
te
/g
el
at
in
hy
dr
og
el
[5
1]
.
D
ua
n,
B.
,H
oc
ka
da
y,
L.
A
.,
Ka
ng
,K
.H
.,
Bu
tc
he
r,
J.T
.
20
13
Jo
ur
na
lo
fB
io
m
ed
ic
al
M
at
er
ia
ls
Re
se
ar
ch
-
Pa
rt
A
10
1
A
(5
),
pp
.
12
55
1
26
4
24
4
Bi
nd
in
g
an
d
co
nd
en
sa
tio
n
of
pl
as
m
id
D
N
A
on
to
fu
nc
tio
na
liz
ed
ca
rb
on
na
no
tu
be
s:
To
w
ar
d
th
e
co
ns
tr
uc
tio
n
of
na
no
tu
be
-b
as
ed
ge
ne
de
liv
er
y
ve
ct
or
s
[5
2]
.
Si
ng
h
R.
,P
an
ta
ro
tt
o
D
.,
M
cC
ar
th
y
D
.,
C
ha
lo
in
O
.,
H
oe
be
ke
J.,
Pa
rt
id
os
C
.D
.,
Br
ia
nd
J.-
P.
,P
ra
to
M
.,
Bi
an
co
A
.,
Ko
st
ar
el
os
K.
20
05
Jo
ur
na
lo
ft
he
Am
er
ic
an
Ch
em
ic
al
So
ci
et
y
12
7
(1
2)
,p
p.
43
88
4
39
6.
57
4
García-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 4 of 13
each analysis. A threshold of five documents cited at
least ten times was set for authors, while for the institu-
tions we selected those with five documents cited at least
five times. These inclusion parameters were based on
the median number of cites for the whole set of docu-
ments, which was 10.33, and the median number of pub-
lications per author was 5.76. For institutions, the mean
number of publications was 10 with the same median of
cites for the documents, 10.33; however, only three affili-
ations were within the threshold, hence the median for
citations and documents was reduced to 5 to include
more affiliations.
To identify the most prolific authors, the top ten au-
thors with more frequency within the threshold defined
were selected and a Pearson correlation was computed to
determine the relationship existing between the number
of publications and the number of co-authors. The au-
thors were clustered by the similarity of areas of research
in the network maps and those with higher networking
were identified by BC calculation. The number of times a
node is taken as a connection for the shortest paths
between two other nodes can be estimated through BC,
which measures the nodes connection to different groups
on a network map, being of a higher value the one who
connects more groups [53]. The BC is obtained using the
equation [53]:
CB vð Þ ¼
X
v?s?t
?st vð Þ
?st
Where ?st is the total of shortest paths from node s to
node t, and ?st (v) is the number of those paths that go
through v.
The information within the threshold was imported
into VOSviewer, a software for data analysis and visual-
isation [54, 55], to perform the network map analysis.
The authors or institutions are represented by nodes or
vertex in the network maps, and their connections are
represented by links or edges; in this document, the
terms are used indistinctly to refer to authors or affilia-
tions and their connections. Two undirected network
maps were constructed from two matrices, representing
only the correlation and not causality. A matrix of au-
thors and a matrix of affiliations were generated using
the visualisation of singularities (VOS) of the VOSviewer
software [55]. The clustering was performed in VOS-
viewer, computed using the default Field Independent
Clustering Model (FICM) [55]. The statistical analysis to
determine the BC of the nodes forming both maps was
performed in Gephi. The final step of the analysis was
the consultation with experts in 3D bioprinting to valid-
ate the results. Experts from UK and Asia were selected
based on their international presence and impact in the
field considering elements such as their number of cites,
publications, projects, and their availability. Instead of
providing the experts with a list of authors found on the
results of this research, we asked them to provide a list
of authors working on bioprinting according to their
own criteria. This method was used to reduce bias in
their selection, as they provided a list acknowledging
their peers based on their own experience. Is it worth
mentioning that the experts requested anonymity, there-
fore, we can only provide professional details of three of
them at the time they were consulted. One of the ex-
perts was affiliated to the School of materials at the Uni-
versity of Manchester and had more than 10,000 Scopus
citations. A second expert was affiliated to the Faculty of
Engineering at the University of Nottingham and had
more than 760 citations. A third one was affiliated to the
Singapore Centre for 3D printing at Nanyang Technol-
ogy University with more than 14,000 citations.
Results
From the initial search, where the ten most cited articles in
bioprinting from Scopus were considered, the top-cited
article is 3D bioprinting of tissue and organs [37]. This is a
review of different techniques used in bioprinting cited
1498 times, as seen in Table 1; the second most cited art-
icle is Scaffold-free vascular tissue engineering using bio-
printing [38]. This article describes a fully biological
method to fabricate tubular vascular grafts and has been
cited less than 50% of the first author, 600 times; the third
paper, entitled 3D bioprinting of vascularized heterogeneous
cell-laden tissue constructs [24] was cited 446 times and
describes methods to generate vascularized tissue con-
structs. The second and third papers are focused on one of
the biggest challenges faced to print fully functional organs,
the fabrication of scaffold-free blood vessels with mechan-
ical properties close to the naturally grown vessels. Five of
the ten articles were published in journals related to mate-
rials, four of them in general science journals (Nature Bio-
technology, Nature Communications, and Science), and
one in the journal of Biofabrication, as can be observed in
Table 1. The results of the searches in Scopus using only
the term bioprinting and those obtained using the search
query developed are listed and compared in Table 1. It can
be observed that the paper entitled 3D bioprinting of tissue
and organs still in first place in both results. The second
paper listed in the results from the search string is Micro-
scale technologies for tissue engineering biology by Khadem-
hosseini et al. [39] with 77% of the cites of the most first
publication, 1163, followed by the paper Clinical trans-
plantation of a tissue-engineered airway by Macchiarini
et al. [40], published in the Lancet. Interestingly, the first
three papers are published in three major journals covering
biology and medical-related science, and six out of the 10
papers on this search were published in journals related to
materials and one in chemical engineering.
García-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 5 of 13
Using the previously defined search query a total of
2088 publications were found from 2007 to November
15 of 2017 (when information collection activity ended).
Figure 1 shows the number of publications per year,
there is a remarkable growth, where the highest number
of publications is 339 for 2017.
After the data selection and cleaning, a total of 228
authors were found within the threshold of at least 5 docu-
ments with 10 or more citations. 89 of the authors were
found with repeated surnames. A Cohens kappa (?) of
0.62 was obtained for the agreement on the author name
disambiguation. Values from 0.61 to 0.8 are ranked as
Good [36]. A collaboration was observed in 93% of the au-
thors, being 792 the total number of connections in the
map. Regarding affiliations, a total of 20 organizations fall
into the inclusion threshold, from which only 30% had an
external collaboration.
From the analysis, only ten authors were found to have
more than 18 documents, as seen in Fig. 2, where the
number of documents and the number of co-authors for
each of them are shown. The author with more documents
falling in the threshold defined is Atala A. with 36 docu-
ments and 13 co-authors. The following author, Khadem-
hosseini A., had a total of 30 documents and more than
double of collaborations for the first author, 27 co-authors,
being the one with more connections. Mironov V. was in
third place with 30 documents, and 20 co-authors. A Pear-
son correlation analysis was performed to determine the
relationship between documents and co-authors, and a
weak positive correlation was observed, as the Pearson cor-
relation coefficient had a value r = 0.29 for the top ten au-
thors, stating a lack of relationship between the number of
co-authors and the number of publications.
Figure 3 shows the network map of the authors col-
laboration, where the nodes size is proportional to their
BC value. The nodes representing the authors were
grouped in a total of 17 clusters. From the BC calcula-
tion, the most prolific author, Atala A., was at the Wake
Forest Institute for Regenerative Medicine from the
Wake Forest University School of Medicine, Winston
Salem, United States when the information was gathered
(15 November 2017). According to Scopus altmetrics,
this author had an h-index of 89, 850 documents pub-
lished, and a total of 17,376 citations working with 150
co-authors at the time of the analysis (see Table 2). On
the other hand, under the inclusion terms, this author
published a total of 36 documents on the topic analysed,
having 13 connections, 2851 citations, and a between-
ness centrality value of 370.9.
The second most prolific author found is Khademhos-
seini Ali L.I., affiliated with the Brigham and Womens
Hospital, Department of Medicine, Boston, United States,
when the data was collected. This author had an h-index
of 88, a total of 645 papers, with a total of 16,704 citations
and 150 co-authors, as stated in the Scopus altmetrics.
Considering the inclusion terms, this author accounted
for 30 documents, 27 connections, 3047 citations, and a
betweenness centrality of 2104.9 (see Table 2).
Fig. 1 Publications per year in bioprinting
García-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 6 of 13
The third author was Mironov V., from the Laboratory
for Biotechnological Research 3D bioprinting solutions,
Moscow, Russian federation. The Scopus altmetrics showed
that this author had 105 papers, 3231 cites, and an h-index
of 31, co-authoring with 150 people. In this analysis, the au-
thor accounted for 30 documents, 20 links, 1009 citations,
and a betweenness centrality of 2754.9 (see Table 2).
According to the network map and the BC calculations,
Mironov V. was stated as the author with a higher influ-
ence in the knowledge flow of the collaboration network, as
it had the higher BC, followed by Khademhosseini A. While
Mironov was affiliated to a biotechnological research
laboratory, Atala and Khademhosseini were associated to
two of the top ten research departments in bioprinting
found on this analysis.
The authors ranked by the experts were compared
with the most influential authors disclosed in this study,
as it can be seen from Table 3.
Three of the ten top authors in this scientometric study
were considered as influential by the experts consulted,
Atala A., Mironov V., and Wei Sun; who were listed among
the top five authors in both cases. The top three authors
from this study, who are listed in Table 3, are also the main
influential authors with a higher BC (see Table 2).
Institutions research efforts can be better estimated by
the number and the quality of their publications, therefore
the affiliations with more publications on bioprinting are
here analysed. A total of 1760 affiliations were identified
in the information obtained for the 2088 documents, with
a median of 10 publications per institution and a standard
deviation of 7.8. The top ten affiliations with more publi-
cations in bioprinting are presented in Fig. 4. An interest-
ing fact is that seven of the top ten are based in the
United States, two of them are in China and one in
Singapore.
Remarkably, four of the 7 affiliations in the United
States are located in Massachusetts, and three of them
have a higher number of publications within the thresh-
old. The Harvard Medical School is the institution with
more publications in the analysis here presented, with
61 documents, followed by the Brigham and Womens
Hospital, with 46 documents. Both affiliations are lo-
cated at the Longwood Medical and Academic Area, a
medical campus in Boston with a strong life science
cluster [56]. The Brigham and Womens Hospital, is an
institution joint to the Harvard Medical School and
holds the second largest hospital-based program in the
world, pioneering in the heart valve operation and the
worlds first solid organ transplant [57].
The Massachusetts Institute of Technology (MIT), lo-
cated in Cambridge, MA, was found to be in third place
on papers related to the bioprinting, presenting 37 docu-
ments within the threshold defined. This institute holds
the fifth place in the World University Ranking 2016
2017 of the Times Higher Education [58]. The fourth in-
stitution found with more publications is the Wake For-
est University School of Medicine, an academic medical
centre ranked among the best in the United States,
Fig. 2 Number of documents and co-authors of the top ten authors in bioprinting
García-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 7 of 13
promoting research in medical areas [59]. This affiliation
shows 35 documents. Tsinghua University is placed in
fifth place, with 33 publications. This institution was on
the 35th place on the Times Higher Education World
University Rankings 2017 [58].
For the affiliations analysis, a total of 20 out of the 1760 in-
stitutions met the inclusion requirements, which at least five
documents with at least five citations each. However, only six
affiliations within this threshold were found to have a collab-
oration with external institutions. Figure 5 depicts the
collaboration network between these institutions, the size of
the nodes is proportional to their number of documents,
while the thickness of the connection line is proportional to
the strength of the link, which is equal to the number of doc-
uments they have co-authored. Within the inclusion limits
above stated, the Harvard-MIT Division of Health Science
has the first position having 37 papers, followed by the Wake
Forest Institute for Regenerative Medicine with 28 docu-
ments and the Biomaterials Innovation Research Centre from
the Brigham and Womens Hospital with 26 documents.
Fig. 3 Co-authors network map, the authors names were normalized with lower case letters
Table 2 Comparison of the Scientometric information between Scopus and the analysis performed to the top three authors with 5
or more documents with 10 or more citations
Author Documents Connections Citations BC h-
indexScopus Threshold Scopus Threshold Scopus Threshold
Atala A. 850 36 150 13 17,376 2851 370.9 89
Khademhosseini Ali L.I. 645 30 150 27 16,704 3047 2104.9 88
Mironov V 105 30 150 20 3231 1009 2754.9 31
García-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 8 of 13
On the other hand, the network map shows four institu-
tions with 5 collaborations. These institutions are the
Biomaterials Innovation Research Centre, at the Brigham
and Womens Hospital from the Harvard Medical School,
United States, the Harvard-MIT Division of Health
Sciences and Technology at the Massachusetts Institute of
Technology, United States, the Department of Physics at
the King Abdulaziz University, Saudi Arabia, and the De-
partment of Bioindustrial Technologies, Konkuk Univer-
sity, South Korea. The remainder institutions have four
connections, the Wyss Institute for Biologically Inspired
Engineering, Harvard University, United States and the
Wake Forest Institute for Regenerative Medicine, Wake
Forest School of Medicine. Interestingly, three of the six
affiliations are located in Massachusetts and two of them
are associated to the University of Harvard, the second-
best research university from the United States [60]. This
institution has a close collaboration with the top institute
in the United States, the Massachusetts Institute of Tech-
nology [58]. Both institutions have founded the Harvard-
MIT Division of Health Sciences and Technology, associ-
ated to the Massachusetts Institute of Technology. This
institute is the one with more citations, 1454, which also
has a strong collaboration with the Biomaterials
Innovation Research Centre, which has 1099 citations.
These affiliations are followed by the Wake Forest School
of Medicine with 858 citations.
The low number of institutions and the high degree of
connectivity among them is reflected on the computa-
tion of the BC for each institution. The four affiliations
with five links each had the same BC centrality, 0.25, cal-
culated with Gephis statistic tools; this low value means
that all the affiliations share the same importance for the
network. On the other hand, the remaining two affilia-
tions have four connections, and a BC equal to zero, not
contributing significantly to the network.
Fig. 4 Top ten institution and number of publications in bioprinting
Fig. 5 Network map of the collaboration between affiliations
García-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 9 of 13
Discussion
The identification leading authors and affiliations and their
collaborations were the main factors to be determined in
this study. The use of the text mining software developed
was found to have a significant contribution to the identifi-
cation of keywords to design the search queries. This step
was crucial to include the complete set of publications dis-
cussing the topic of bioprinting. The search queries created
included a wide range of terms and synonyms that are
used in articles on topics related to bioprinting. An expo-
nential growth of publications per year was observed from
the data of the documents obtained. Furthermore, as one
of the main goals of this study was to identify the most
influential authors working in bioprinting, the analysis,
including author disambiguation, was exclusively based on
the publications information specifically obtained from the
Scopus database. Although there are more scientific data-
bases available, Scopus was selected because it also con-
tains non-English coverage and its altmetrics tools were
used for a quick overview of the results. Although Data-
bases such as Web of Science cover records back to 1900,
we analysed only a specific period, which could be covered
by Scopus. Although results provide the appraisal of one of
the most complete scientific databases available, Scopus,
the information obtained can be further enhanced by
analysing data obtained from more databases to support
the findings. The author disambiguation was performed
manually for the authors with good agreement; however,
this procedure can be improved by including tools such as
similarity of pairs or features contribution. In this analysis
it was observed that although a high number of authors
were engaged in the advancement of the scientific output
on bioprinting, only a small percentage have a remarkable
productivity on the topic. Furthermore, there was found a
slight association between the number of documents and
the number of collaborations. Co-author analysis has con-
tributed to the identification of the intellectual structure of
fields and specialties [2] and to identify research groups
[3]. In this research, the network map analysis was en-
hanced with the calculation of BC to identify the authors
with more publications and the most influential authors
and institutions working in bioprinting. Although some of
the authors might be regarded as scientists with a higher
rank or seniority, this classification was beyond the scope
of this study.
The number of affiliations working in bioprinting was
found to be high, as expected. However, only a small
portion of them fulfil the inclusion requirements for the
analysis. The most prolific institutions that came across
in this study, such as Nanyang Technological University,
MIT, and Tsinghua University, were also previously re-
ported to be among the three most prolific in [15].
Moreover, a reduced number of collaborations between
the institutions in the threshold was found, an
unanticipated outcome for a multidisciplinary technol-
ogy. The institutions with more publications, The Har-
vard Medical School and the Brigham and Womens
hospital, were two of the top ten Universities in the
World University Ranking [58], which have established a
research centre close to both institutions. Besides, the
strategic geographical position of the affiliations to pro-
mote collaborations has been observed as important to
encourage scientific production. But not only the institu-
tions located closely, namely the Harvard Medical
School, Brigham and Womens Hospital and the Div-
ision of Health Science, were found to have strong col-
laborations, also the Department of Bioindustrial
Technologies of the Konkuk University and the Depart-
ment of Physics, of the King Abdulaziz University exhib-
ited a high degree of collaboration. This illustrates that
geographical positioning is an important factor to collab-
orate, but it is not crucial.
The method here presented involved the overall static
analysis of collaborations over a ten-year period, as the
change in time for both kinds of collaboration, those
among authors and those among affiliations, were not
analysed for different periods. Furthermore, the results
here shown concern the general approach of bioprinting
domain, where a wide range of methodologies and tech-
nologies are involved without special emphasis on any
particular methodology. In this sense, any agreement on
the most influential authors and institutions is more dif-
ficult to reach. The threshold set in this study was used
to determine the most influential authors and institu-
tions in bioprinting, taking into account also the depart-
ments of affiliation, thus differing from the analyse made
by Rodriguez-Salvador et al. [7], where only countries
and institutions were considered. Regarding affiliations,
insights obtained in the analysis are consistent with the
institutions reported in by Rodriguez-Salvador et al. [7].
Both analyses, this analysis and the one reported by
Rodriguez-Salvador et al. differ from the list provided by
the experts, shown in Table 3. The threshold was de-
fined to include only those authors and affiliations with
cites above the average, and this influenced the network
map analysis, by reducing significantly the number of
nodes in the map. Another aspect that influenced the re-
sults was the exclusion of those nodes with no connec-
tions. These nodes were neglected in this study as zero
links on the maps mean zero BC and do not have any ef-
fect in the overall results. Furthermore, another possible
reason for the difference between the list provided by
experts and the results here disclosed, is that expert tra-
jectories can have a subjective influence to define the
most influential authors in bioprinting.
In this research, we also disclosed the six main institu-
tions working on bioprinting and their collaboration net-
work. The use of network analysis and the calculation of
García-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 10 of 13
the BC was decisive to find the authors with a higher de-
gree of influence on the topic. However, the identifica-
tion of research areas of both authors and affiliations
was out of the scope of this research, and this can be in-
vestigated when looking for R&D opportunities for
innovation in bioprinting. This research set the basis to
determine collaborations and their position in a scien-
tific network. The knowledge obtained in our research
can provide support to researchers and stakeholders
looking for engagement in R&D projects on bioprinting.
Conclusions
Network map analysis was used here to identify the most
prominent institutions and authors. A threshold was de-
fined to disclose authors and organisations with a higher
network of collaboration, identifying authors with more
publications. Moreover, the Betweenness Centrality cal-
culation allowed us to identify the most influential au-
thors and institutions working on bioprinting. The
outcomes obtained can give strength to the perception
of the collaborations in bioprinting technologies. Al-
though the global research community in bioprinting
has grown, the most influential affiliations and authors
are located in the United States. The top three authors
have more than 29 articles each within the threshold
established. From the authors network map analysis, it
was observed that there is no direct correlation between
the BC, number of documents, and connections, as the
one with more documents in the threshold was the one
with less connections and the lower BC value. The affili-
ations with more publications are members of the top
universities in the United States and are part of medical
research programs. Individuals interested in the develop-
ment of bioprinting can benefit from the information
here disclosed to perform a trend analysis on the institu-
tions hereby mentioned. And identifying core technolo-
gies that have led them to success. The findings of this
study can offer valuable information to be used in
systematic approaches to support the decision making of
researchers and stakeholders.
Appendix
Search query
TITLE-ABS-KEY (((((3d OR 3-d OR three-dimensional)
W/1 (bioprint* OR engineer* OR print* OR tech* OR
fabricat* OR process* OR manufact* OR building OR
built)) OR ((bio-engineer* OR bioengineer* OR biofabri-
cat* OR bioprint* OR biotech* OR biomanufact*)) OR
(bioadditive W/1 manufact*) OR (3d AND bioprint*)))
W/5 (scaffold* OR construct* OR spheroid* OR channel*
OR structure* OR matr* OR crosslinking OR block* OR
aggregate* OR sheet* OR biomim* OR bioactiv* OR bio-
hybrid* OR bioresorbable OR bioscaffolds OR biosensors
OR bioassembl* OR bioartificial OR bioerodible OR bio-
patterning OR biopaper OR microextrusion) W/5 (cell*
OR tissue* OR stem OR multicellular OR organ* OR
biolog* OR embryonic OR vascular OR vessels OR cola-
gen OR bone* OR osseo* OR adipose OR vascular OR
cardiac OR heart OR cartilage* OR muscle) AND NOT
(data storage OR photonic OR pcl OR social capital))
AND (PUBYEAR > 2000 AND PUBYEAR < 2018).
Abbreviations
BC: Betweenness Centrality; CI: Competitive Intelligence; CSV: Comm
Separated Values; CTI: Competitive Intelligence; DL: Digital Libraries;
FICM: Field Independent Cluster Model; MIT: Massachusetts Institute of
Technology; R&D: Research and Development; VOS: Visualisation of
singularities
Acknowledgments
This research was supported by the research group in Advanced
Manufacturing at the Tecnologico de Monterrey and by the CONACYT
postdoctoral fellowship program.
Authors contribution
L. A. G. G. Design the study, data collection, generation of maps, data
analysis, drafting and editing of the final manuscripts. M.R. S. participated in
the study design and writing feedback. All authors read and approved the
final manuscript and agreed on its submission
Table 3 Comparison of authors in bioprinting ranked by experts versus the most influential authors disclosed in this study
Rank List of most influential authors provided by experts List of most influential authors found on this study
Author Institution Author Institution
1 Atala A. Wake Forest Institute for Regenerative Medicine Atala A. Wake Forest Institute for Regenerative Medicine
2 Mironov V. Laboratory for Biotechnological research Khademhosseini Ali L.I. Brigham and Womens hospital
3 Malda J. Utrech University Mironov V. Laboratory for Biotechnological research
4 Derby B. University of Manchester Sun W. Drexel University and Tsinghua University
5 Sun W. Drexel University and Tsinghua University Wang X. Tsinghua University
6 Lewis J. Harvard Cho D. W. Pohang University of Science and Technology
7 Yoo J. Wake Forest Institute for Regenerative Medicine. Zhang L. G. George Washington University
8 Woodfield T. University of Otago Okano T. Tokyo Womens Medical University
9 Dalton P. University of Wurzburg Zhang Y. Brigham and Womens hospital
10 M. Zanobi-Wong ETH Zurich Rezende R. A. Centre for information Technology Renato Archer
García-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 11 of 13
Funding
This work was funded by Tecnologico de Monterrey through the Escuela de
Ingenieria y Ciencias and also supported by a postdoctoral scholarship
granted by the Mexican National Council for Science and Technology
(CONACYT). The funders had no role in study design, data collection and
analysis, decision to publish, or preparation of the manuscript.
Availability of data and materials
The datasets generated and/or analysed during the current study are
available in the Open Science Framework repository, https://osf.io/ez7mv/ .
Named bioprinting_scopus_data_10.csv,
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Author details
1University of Sussex, School of Engineering and Informatics, Falmer,
Brighton, UK. 2Tecnologico de Monterrey, Escuela de Ingeniería y Ciencias,
Monterrey, Nuevo Leon, Mexico.
Received: 6 March 2018 Accepted: 30 January 2020
Keet Journal of Biomedical Semantics            (2020) 11:4 
https://doi.org/10.1186/s13326-020-00224-y
DATABASE Open Access
The African wildlife ontology tutorial
ontologies
C. Maria Keet
Abstract
Background: Most tutorial ontologies focus on illustrating one aspect of ontology development, notably language
features and automated reasoners, but ignore ontology development factors, such as emergent modelling guidelines
and ontological principles. Yet, novices replicate examples from the exercise they carry out. Not providing good
examples holistically causes the propagation of sub-optimal ontology development, which may negatively affect the
quality of a real domain ontology.
Results: We identified 22 requirements that a good tutorial ontology should satisfy regarding subject domain, logics
and reasoning, and engineering aspects. We developed a set of ontologies about African Wildlife to serve as tutorial
ontologies. A majority of the requirements have been met with the set of African Wildlife Ontology tutorial ontologies,
which are introduced in this paper. The African Wildlife Ontology is mature and has been used yearly in an ontology
engineering course or tutorial since 2010 and is included in a recent ontology engineering textbook with relevant
examples and exercises.
Conclusion: The African Wildlife Ontology provides a wide range of options concerning examples and exercises for
ontology engineering well beyond illustrating just language features and automated reasoning. It assists in
demonstrating tasks concerning ontology quality, such as alignment to a foundational ontology and satisfying
competency questions, versioning, and multilingual ontologies.
Keywords: Ontology engineering, Tutorial ontology, African wildlife
Background
The amount of educational material to learn about ontolo-
gies is increasing gradually, and there is material for dif-
ferent target audiences, including domain experts, applied
philosophers, computer scientists and software develop-
ers, and practitioners. These materials may include a tuto-
rial ontology to illustrate concepts and principles and may
be used for exercises. There are no guidelines as to what
such a tutorial ontology should be about and should look
like. The two most popular tutorial ontologies are about
wine and pizza, which are not ideal introductory subject
domains on closer inspection (discussed below), they are
limited to OWLDL only, and are over 15 years old by now,
Correspondence: mkeet@cs.uct.ac.za
Department of Computer Science, University of Cape Town, 18 University
Avenue, Rondebosch, Cape Town, South Africa
hence, neither taking into consideration the more recent
insights in ontology engineering nor the OWL 2 standard
with its additional features.
Considering subject domains in the most closely related
area, conceptual modelling for relational databases, there
is a small set of universes of discourse that are used in
teaching throughout the plethora of teaching materials
available: the video/DVD/book rentals, employees at a
company, a university, and, to a lesser extent, flights and
airplanes. Neither of these topics for databases lend them-
selves well for ontologies, for the simple reason that the
two have different purposes. It does raise the question as
to what would be suitable and, more fundamentally, what
it is that makes some subject domain suitable but not
another, and, underlying that, what the requirements are
for an ontology to be a good tutorial ontology.
© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate
credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were
made. The images or other third party material in this article are included in the articles Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not included in the articles Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative
Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made
available in this article, unless otherwise stated in a credit line to the data.
Keet Journal of Biomedical Semantics            (2020) 11:4 Page 2 of 11
Table 1 Summary of main extant tutorial ontologies
Ontology Year Stated aim Content Language Modelling issues Automated
reasoning
Current OE (e.g.,
ODP, FO)
wine 2001 novice all aspects
(methodology,
modelling, reasoning)
for OE
somewhat generic,
repetitive, limited
extensibility
frames; the
wine.owl in
OWL DL is
based on it
multiple (e.g., class vs
instance, hasX)
yes no
pizza 2004 Protégé user guide,
also illustrate OWL
and reasoning
somewhat generic,
repetitive, limited
extensibility
OWL DL multiple (e.g., hasX,
FO commitment, lack
of domain & range)
yes no
university 2005 illustrate OWL and
reasoning
generic, CDM (cf.
ontology) scope,
very small
<OWL DL
(ALCIN)
multiple (e.g., XorY,
naming of
individuals)
yes no
zooAnimals 2011 illustrate DL&OWL
and some GoodOD
modelling guidelines
generic, a lot of
detail, easily
extensible
<OWL 2 DL
(SHO)
few yes partially
(BioTopLite)
family
history
2013 illustrate OWL 2 DL
and reasoning
specific to authors
family, not extensible
OWL 2 DL multiple (e.g., hasX,
FO commitment, lack
of domain & range)
error no
shirt 2015 illustrate the design of
the FMA
generic, structure
specific to FMA,
repetitive, not
extensible
<OWL 2 DL
(ALCIQ)
few (lack of domain
& range)
none very limited
(reference
ontology)
Abbreviations: OE: ontology engineering; ODP: ontology design pattern; FO: foundational ontology; CDM: conceptual data model; FMA: foundational model of anatomy;
OWL DL is SHOIN(D) and OWL 2 DL is SROIQ(D) in DL notation
In this paper, we will first analyse existing tutorial
ontologies and highlight some issues. We then proceed
to formulate a preliminary, first, list of requirements that
tutorial ontologies should meet. The African Wildlife
Ontology (AWO) tutorial ontologies are then introduced
briefly and held against the requirements. The scope of
this paper is thus to introduce the AWO tutorial ontolo-
gies and to frame it in that context. Finally, we discuss and
conclude.
Tutorial ontologies: issues and comparison
There are several tutorial ontologies, which are sum-
marised in Table 1 and discussed in this subsection; the
next subsection that summarises the problems.
Of the six tutorial ontologies considered in detail, two
are popular, being the Wine Ontology and the Pizza
ontology, since they are part of the W3C OWL guide
and designed for the most popular ontology development
environment (Protégé), respectively. They have various
shortcomings as tutorial ontologies, however, especially
concerning modelling practices or styles (see also [1]).
The Wine ontology in its current form emanates from
the Ontology development 101 tutorial [2] with its
frames and slots that was subsequently transferred into
OWL1 and used in the OWL guide [3], which is a W3C
Recommendation. While the guide contains some good
suggestions, such as that Synonyms for the same con-
cept do not represent different classes [2], there are also
1http://www.w3.org/TR/2003/PR-owl-guide-20031209/wine
modelling issues, notably that the ontology is replete with
the class-as-instance error that is promoted by the incor-
rect statement in the tutorial Individual instances are the
most specific concepts represented in a knowledge base.
[2] (e.g., TaylorPort as instance of Port and MalbecGrape
as instance of Grape instead of as subclass of ), and the
sub-optimal object property naming scheme of hasX ,
such as adjacentRegion between two Regions rather than
the reusable and generic adjacent. Further, it uses differ-
ent desiderata in the direct subclassing of wine such as the
likes of Bordeaux and Loire (region-based) and Chardon-
nay and Cabernet Sauvignon (grape-based), and then
there are other criteria, like DessertWine (food pairing-
based grouping) and wine descriptor ones (DryWine,
RedWine, TableWine), This does make it interesting for
showing classification reasoning (except the undesirable
deduction that DryWine ? TableWine), but is not ideal
from amodelling viewpoint. Further, from a tutorial view-
point: there are many repetitions, such as very many
wineries, which distract from the principles, and it lacks
annotations.
The Pizza ontology tutorial was created for the Pro-
tégé user manual and OWL DL ontology language [4].
It reflects the state of the art at that time, yet much has
happened over the past 15 years. For instance, there are
new OWL 2 features and there are foundational ontolo-
gies that provide guidance for representing attributes (cf.
Pizzas ValuePartition). Pizzas DomainConcept throws a
learner straight into philosophical debates, which may not
be useful to start with, and, for all practical purposes,
Keet Journal of Biomedical Semantics            (2020) 11:4 Page 3 of 11
duplicates owl:Thing. Like the Wine ontology, it has
the hasX naming scheme for object properties, such as
hasTopping, including the name of the class it is sup-
posed to relate to, which is a quirk that is a combination
of a workaround for not having qualified number restric-
tions (anOWL 1 artefact) and of a sub-optimal ontological
analysis of the relation (in casu, of how the toppings really
relate to the rest of the pizza) that reduces chance of
ontology reuse and alignment. Also, this propagates into
students modelling approaches: students ontologies in
earlier instances of the authors course on ontology engi-
neering included, among others, a sandwich ontology with
hasFilling, an electrical circuit board ontology with hasIso-
lator, furniture with hasHeadboard. Modelling issues
are compounded by the statement we generally advise
against doing [domain and range declarations] in the
tutorial documentation. When one aims to get novices
to use Protégé and OWL so as not get too many error
with the automated reasoners, that might make sense,
but ontologically, fewer constraints make an ontology less
good because it admits more unintended models. Finally,
it has repetitive content to show features, which may be
distracting, and, as with Wine, there is only one final
ontology, despite that multiple releases are common in
practice.
Other tutorial ontologies include Family History,
zooAnimals, University, and Shirt. Family History [5] is
developed by the same group as Pizza and aims to teach
about advanced OWL 2 features and maximise the use
of inferencing. Loading it in Protégé 5.2 results in three
punning errors, since it mixes three object properties with
annotation properties (affecting 32 axioms), which is dis-
allowed, and trying to classify it without the three anno-
tation properties returned an OutOfMemoryError (on a
MacBookPro, 2.6 GHz and 8GB of memory), which is not
ideal to start a tutorial with. Concerning modelling issues,
ParentOfRobert illustrates one can use individuals in class
expressions, but just that the language allows it, does not
mean it is ontologically a good idea that must be taught.
It also has the hasX semantic weakness, very few anno-
tations, DomainEntity being subsumed by owl:Thing,
and multiple data properties. In contrast to Pizza and
Wine, all the declared instances are instances and the
ontology has different versions as one goes along in the
chapters. It has some subject domain aspects descending
into politics, which would render it unsuitable for teach-
ing in several countries, such as stating that Sex? Female
unionsq Male (enforcing a gender binary) and that Person 
? 2 hasParent.Person (multiple constructions are possible
biologically, societally, and legally).
The remaining tutorial ontologies have been developed
by different schools of views on ontology engineering
(OE), which is readily apparent in their scope and con-
tent. The zooAnimals tutorial ontology [6] comes closest
to our aims for a versatile tutorial ontology, demonstrat-
ing multiple OWL features, avoiding modelling issues
such as class vs instance, and it is informed by a top-
domain ontology (BioTop) as well as deep philosophical
notions such as dispositions. It puts them all together
into one ontology instead of gradual extensions, how-
ever, which is off-putting at a novice stage. One may
quibble about some of the content, such as simplifica-
tions that Plant ? ?hasProperPart.Chloroplast (notably,
some parasitic plants and all myco-heterotrophic plants
do not have chloroplasts) and there are unintended unde-
sirable deductionsi.e., logically implied, but incorrect
ontologicallysuch as marineAnimal  Omnivore since
not all such animals are omnivores. Any simplified com-
mon generic subject domain is likely to have some short-
cuts that are not 100% scientifically accurate, and it may be
a fine line between tutorial approximation and modelling
mistake.
The University ontology focuses on illustrating OWL
features and automated reasoning, rather than modelling.
For instance, it has AcademicStaff with sibling NonAca-
demicStaff where a non-X complement class is sub-
optimal, especially when there is a term for it. The repre-
sentation of Student  Person is an advanced modelling
aspect that can be improved upon with a separate branch
for roles played by an object. The Computer Science
Ontology was based on the University Ontology tuto-
rial and contains artificial classes, like unions of classes
(ProfessorinHCIorAI) and underspecified or incorrect indi-
viduals like AI and HCI (e.g., some course instance would
be CS_AI-sem1-2018 instead).
The Shirt ontology is a tutorial ontology to explain the
structure and organisation of the Foundational Model of
Anatomy in a simpler way2 and therefore does not have
the hasX naming scheme for object properties, it has no
data properties and no instances. It has many annotations
with explanations of the entities. There are no inferences.
Regarding suitability of the subject domains of the
ontologies assessed, they are mixed. Wine misses many
wine-producing regions in the Americas (e.g., Chile), in
Europe (e.g. Spain, Bulgaria), and elsewhere (e.g., South
Africa) and Pizza lacks varieties beyond Italian and Amer-
ican ones, and both are served regularly in a relatively
small part of the world, therewith reducing their appeal
internationally. Family history and a university as subject
domains veer too easily into the area of database design for
a single application, rather than application-independent
generic knowledge for an ontology. Shirts and zoo animals
do not have these shortcomings.
Finally, more or less related textbooks were consid-
ered [711]. Only the Semantic Web for the working
2http://xiphoid.biostr.washington.edu/fma/shirt_ontology/shirt_ontology_1.
php
Keet Journal of Biomedical Semantics            (2020) 11:4 Page 4 of 11
Ontologist (2nd ed.) has sample files for the books many
small examples3 with two reoccurring subject domains,
being English literature and products.
Problems to address
The previous section described several problems with
existing tutorial ontologies. Notably, the recurring short-
comings are that
i) good modelling practices are mostly ignored in
favour of demonstrating language features,
automated reasoning, and tools
ii) when good modelling practices and at least some
recent ontology engineering advances are included, it
falls short on language features and gradual
extensions.
This has a negative effect on learning about ontology
development, for tutorial ontology practices are nonethe-
less seen by students as so-called model answers even if
it were not intended to have that function.
The ontology survey does not reveal what may be the
characteristics of a good tutorial ontology and, to the best
of our knowledge, there is no such list of comprehen-
sive criteria for tutorial ontologies specifically. Schober
et al. [6] propose a partial list with seven high-level
content requirements indeed, such as a common sense
knowledge subject domain that lends itself well to demon-
strate the classic modelling challenges, but it omits the
essential components of logic, reasoning, and engineering
requirements. Scoping it more broadly, one could con-
sider modelling guidelines and automated checkers for
production level ontologies, such as [1215]. They can
inform the development of tutorial ontologies, in partic-
ular to avoid such issues as the class vs. instance error in
the provided sample ontology, but that is different from
educating students about the foundations and reasons for
such guidelines starting from a basic level of modelling
to more advanced issues. For instance, disjointness and
covering constraints among subclasses of a parent class is
indeed desirable together with coherent criteria to declare
a taxonomy [15], but that does not let students observe or
experience mistakes, i.e., learn what is suboptimal or does
not work and why. A tutorial ontology also would have
to be able to accommodate common pitfalls and grad-
ual quality improvements, among other things, which are
not covered by the general guidelines. Also, general guide-
lines tend to follow one commitment over anothere.g.,
the GoodOD guidelines favour a realist approach with
the BFO foundational ontologybut for teaching OE in
general, students need learn to be cognisant of multiple
possible commitments, their consequences when choos-
ing one or the other, and have at least one practical
3http://www.workingontologist.org/Examples.zip; Last accessed: 26-11-2018.
example of such a difference to illustrate it, which general
guidelines do not provide.
Potential benefits of the African wildlife ontology tutorial
ontologies
In order to address these problems, we introduce the
African Wildlife Ontology (AWO). The AWO has been
developed and extended over 8 years. It meets a range of
different tutorial ontology requirements, notably regard-
ing subject domain, use of language features and auto-
mated reasoning, and its link with foundational ontologies
on the one hand and engineering on the other. It aims to
take a principled approach to tutorial ontology develop-
ment, which thereby not only may assist a learner, but,
moreover from a scientific viewpoint, it might serve as a
starting point for tutorial ontology creation or improve-
ment more broadly, and therewith in the future contribute
to an experimental analysis of tutorial ontology qual-
ity. This could benefit educational material for ontology
development.
Also, educationally, there is some benefit to reusing
the same ontology to illustrate a range of aspects, rather
than introducing many small ad hoc examples, for then
later in a course, it makes it easier for the learners to see
the advances they have made. This is also illustrated with
offering multiple versions of the ontology, which clearly
indicate different types of increments.
Finally, the AWO can be used on its own or together
with the textbook An Introduction to Ontology Engineer-
ing [16], which contains examples, tasks and exercises
with the AWO.
Construction and content
The construction of the AWO tutorial ontologies has gone
through an iterative development process since 2010. This
involved various extensions and improvements by design,
mainly to address the increasing amount of requirements
to meet, and maintenance issues, such as resolving link
rot of an imported ontology. Rather than describing the
process of the iterative development cycles, we present
here a digest version of it. First, a set of tutorial ontol-
ogy requirements are presented together, then a brief
overview of the AWO content is described, and subse-
quently we turn to which of these requirements are met
by the AWO.
OE tutorial ontology requirements
Tutorials on ontologies may have different foci and it
is unlikely that an ontology used for a specific tutorial
will meet all requirements. The ontology should meet the
needs for that tutorial or course, and that should be stated
clearly. As such, this list is intended to serve as a set of con-
siderations when developing a tutorial ontology. Each item
easily can take up a paragraph of explanation. We refrain
Keet Journal of Biomedical Semantics            (2020) 11:4 Page 5 of 11
from this by assuming the reader of this paper is suffi-
ciently well-versed in ontology engineering and seeking
information on tutorial ontologies. For indicative purpose,
the requirements are categorised under three dimensions:
the subject domain of the ontology, logics & reasoning,
and engineering factors.
Subject domain
The tutorial ontologys subject domain, also called uni-
verse of discourse, should be versatile to be able to cater
plausibly for a range of modelling aspects. We specify
seven requirements for it, as follows.
1. It should be general and commonsensical domain
knowledge, so as to be sufficiently intuitive for
non-experts to be able to understand content and
add knowledge. Optionally, it may be an enjoyable
subject domain to make it look more interesting and,
perhaps, also uncontroversial4 to increase chance of
use across different settings and cultures.
2. The content should be not wrong ontologically,
neither regarding how things are represented (e.g.,
no classes as instances) nor the subject domain
semantics (e.g., whales are mammals, not fish).
3. It needs to be sufficiently international or
cross-cultural so that experimentation with a
scenario with multiple natural languages for
multilingual ontologies is plausible.
4. Its contents should demonstrate diverse aspects
succinctly when illustrating a point (in contrast to
being repetitive in content).
5. It needs to be sufficiently versatile to illustrate the
multiple aspects in ontology development (see
below), including the use of core relations (e.g.,
mereology).
6. It should permit extension to knowledge that
requires features beyond Description Logics-based
OWL species, so as to demonstrate representation
limitations and pointers to possible directions of
solutions (e.g., temporal aspects, non-monotonicity,
full first-order logic).
7. The subject domain should be able to possibly be
used in a range of use case scenarios (database
integration, science, NLP, and so on).
Logics & reasoning
Since a core feature of ontologies is their logic under-
pinning, a tutorial ontology thus also will need to meet
criteria for the representation language and automated
reasoning over it. They are as follows.
4Recent political issues include complaints with subject domains of exercises
that perpetuate stereotypes and simplifications, such as, but not limited to, the
gender binary, who can marry whom, and gendered subject domains
perceived to reside at the edges of the spectrum.
I. The ontology should be represented in a logic that has
tool support for modelling and automated reasoning.
II. The ontology should be represented in a logic that
has tool support for debugging features that
explain the deductions, meaning at least showing
the subset of axioms involved in a deduction.
III. It should permit simple classification examples and
easy examples for showing unsatisfiability and
inconsistency, such that it does not involve more
than 2-3 axioms in the explanation, and also longer
ones for an intermediate level.
IV. The standard reasoning tasks should terminate fairly
fast (< 5 s) for most basic exercises with the
ontology, with the standard reasoning tasks being
subsumption/classification, satisfiability, consistency,
querying and instance retrieval.
V. The representation language should offer some way
of importing or linking ontologies into a network of
ontologies.
VI. The language should be expressive enough to
demonstrate advanced modelling features (e.g.,
irreflexivity and role composition).
VII. The logic should be intuitive for the modelling
examples at least at the start; e.g., if there is a need for
ternary relations, then the logic should permit
n-aries with n ? 3 so that it can be represented as
such, rather than as an approximation with a
reification and a workaround pattern.
Engineering and development tasks
An ontology is an artefact, which has to be built and
maintained. To this end, there are multiple approaches,
methodologies, methods, and software tools of which at
least a subset will have to become part of an ontologists
toolbox. We identified eight broad requirements:
A. At least some ontology development methods and
tools should be able to use the ontology, be used for
improvement of the ontology, etc.
B. The ontology needs to permit short/simple
competency questions (CQs) and may permit long
and complicated CQs, which are formulated for the
ontologys content and where some can be answered
on the ontology and others cannot.
C. At least some of the top-level classes in the hierarchy
should be straight-forward enough to be easily linked
to a leaf category from a foundational ontology (e.g.,
Animal is clearly a physical object, but the ontological
status of Algorithm is not immediately obvious).
D. It should be relatable to, or usable with, or else at
least amenable to the use of, ontology design
patterns, be they content patterns or other types.
E. It is beneficial if there is at least one ontology
sufficiently related to its contents, so that it can be
Keet Journal of Biomedical Semantics            (2020) 11:4 Page 6 of 11
used for tasks such as comparison, alignment, and
ontology imports.
F. It is beneficial if there are relevant related
non-ontological resources that could be used for
bottom-up ontology development.
G. It should be able to show ontology quality
improvements gradually, stored in different files.
H. It should not violate basic ontology design principles
(e.g., classes and relations vs. implementation
decisions with data properties and data types when
representing qualities of entities, such as an animals
weight).
While this list may turn out not to be exhaustive in the
near future, it is expected to be sufficient for introduc-
tory levels of ontology development tutorials and courses.
Either way, this list of requirements are already hard
to meet in one single ontology. For instance, simplicity
(Items 3, III, and B) vs. complicated extensions and onto-
logical precision (Items 6 and C) cannot be fully met
at once. On the flip side, some requirements are closely
related or overlap, such as design principles (Item H) and
not being wrong ontologically (Item 2) since some of the
former are informed by the latter.
Content of the AWO  at a glance
The principal content of the AWO is, in the first stage
at least, intuitive knowledge about African wildlife.
This subject domain originated from an early Semantic
Web book ([8], Section 4.3.1) that was restructured and
extended slightly for its first, basic version; see Table 2
and Fig. 1. It has descriptions of typical wildlife animals,
such as Lion and Elephant, and what they eat, including
Impala (a type of antelope), and Twig or Leaf, respectively.
Basic extensions in the simple version of the ontology
(v1) include plant parts, so as to demonstrate parthood
and its transitivity, and carnivore vs. herbivore, which
make it easy to illustrate disjointness, subsumption rea-
soning, and unsatisfiable classes, and carnivorous plants
to demonstrate logical consequences of declaring domain
and range axioms 5. Most elements have been annotated
with informal descriptions, and several annotations link to
descriptions on Wikipedia.
Like the aforementioned Family History ontology, there
are several versions of the AWO that reflect different
stages of learning. In the case of the AWO, this is not
specifically with respect to OWL language features, but
one of notions of ontology quality and where one is in
the learning process. For instance, version 1a contains
answers to several competency questionsi.e., quality
5in casu, declaring eats too restrictively with as domain only Animal: then
either it will result in an unsatisfiable CarnivorousPlant (if Animal and Plant are
declared disjoint) or it will result in the undesirable deduction that
CarnivorousPlant  Animal
requirements that an ontology ought to meet [17]that
were formulated for Exercise 5.1 in the Methods and
methodologies chapter of [16]. Versions 2 and 3, on
the other hand, have the AWO aligned to the DOLCE
and BFO foundational ontologies, respectively, whose dif-
ferences and merits are discussed in Chapter 6 of the
textbook, ensuring discussion of refinements in ontologi-
cal precision with, e.g., processes and dispositions (e.g., an
Eating class with participating objects cf. an eats object
property). Their respective versions with the answers to
the related exercises have the name appended with an a
as well. Version 4 has some contents cleaned up, par-
tially based on what the OOPS! tool [14] detected; it uses
more advanced language features; and takes steps in the
direction of adhering to science more precisely with finer
granularity, such as type of carnivores and distinguishing
between types of roots.
There are also four versions in different natural lan-
guages, being in isiZulu, Afrikaans, Dutch, and Spanish,
which mainly serve the purpose of illustrating issues with
multilingual settings of ontology use, which relates to
content in Chapter 9 of the textbook.
AWO against the requirements
The AWO meets most of the requirements (see Table 3).
Concerning the subject domain, the content is general,
versatile, not wrong, sufficiently international, and not
repetitive (Items 1-4). The AWO includes the core rela-
tion of parthood for, especially, plants and their parts, with
optional straightforward extensions with the participation
relation (e.g., animals participating in a Chasing event)
and membership (animal collectives, such as Herd; see v4
of the AWO), therewithmeeting Item 5. Representation of
relevant domain knowledge beyond Description Logics-
based OWL species (Item 6) could include information
about temporal segregation of foraging or commensal-
ism, inclusion of species with distinct successive phases
with substantial morphological changes (e.g., Caterpil-
lar/Butterfly), and the notion of rigidity between what an
object is and the role it plays (e.g., Lion playing the role of
Predator; see v4 of the AWO). The subject domain is also
fertile ground for exceptions that may be represented with
non-monotonic logics; typical examples are that, gener-
ally, birds fly and plants have chlorophyl, but not all of
them (e.g., the penguin and the dodder, respectively). Use
case scenarios (Item 7) may be, among others, science
of African wildlife, activism on endangered species, and
applications such as a database integration and manage-
ment system for zoos and for tourism websites.
Regarding the logics and reasoning, the AWO is rep-
resented in OWL [19], and thus has ample tooling sup-
port for knowledge representation, reasoning, and basic
debugging/explanation, with ontology development envi-
ronment tools such as Protégé (Items I-III). The AWO
Keet Journal of Biomedical Semantics            (2020) 11:4 Page 7 of 11
Table 2 AWO ontologies, with their main differences
File name Difference
AfricanWildlifeOntology.xml This is the file from http://www.iro.umontreal.ca/~lapalme/ift6281/OWL/AfricanWildlifeOntology.xml,
that was based on the description in [8]
AfricanWildlifeOntologyWeb.owl AfricanWildlifeOntology.xml + changed the extension to .owl and appended the name
with Web. This ontology gave at the time (in 2010) a load error in the then current version of Protégé
due to the use of Collection in the definition of Herbivore
AfricanWildlifeOntology0.owl AfricanWildlifeOntologyWeb.owl + that section on Collection removed
AfricanWildlifeOntology1.owl AfricanWildlifeOntology0.owl + several classes and object properties were added (up to
SRI DL expressiveness), more annotations, URI updated (described in Example 4.1 in [16])
AfricanWildlifeOntology1a.owl AfricanWildlifeOntology1.owl + new content for a selection of the CQs in Exercise 5.1 in
[16] (its CQ5, CQ8) and awo_12 of the CQ dataset [18])
AfricanWildlifeOntology2.owl AfricanWildlifeOntology1.owl + OWL-ised DOLCE (Dolce-Lite.owl) was imported
and aligned
AfricanWildlifeOntology2a.owl AfricanWildlifeOntology2.owl + answers to the questions in Example 6.2 in [16] on
foundational ontology alignment
AfricanWildlifeOntology3.owl AfricanWildlifeOntology1.owl + BFO v1 was imported and aligned
AfricanWildlifeOntology3a.owl AfricanWildlifeOntology3.owl + answers to the questions in Example 6.2 in [16] on
foundational ontology alignment
AfricanWildlifeOntology3b.owl AfricanWildlifeOntology1.owl + BFO v2 + answer to Exercise 6.7 in v2 of [16] on refactoring
the AWO with dispositions
AfricanWildlifeOntology4.owl AfricanWildlifeOntology1.owl + some things cleaned up (e.g., consistent naming) and
added some science content, more OWL language features are used (up to SRIQ), and several
educational explanations and questions for further exploration have been added in annotation fields
AfricanWildlifeOntologyZU.owl Mostly AfricanWildlifeOntology1.owl but then in isiZulu, with IRI changed
AfricanWildlifeOntologyAF.owl AfricanWildlifeOntology1.owl but then in Afrikaans, has some IRI issues to resolve
AfricanWildlifeOntologyNL.owl AfricanWildlifeOntology1.owl in Dutch, with IRI changed
AfricanWildlifeOntologyES.owl AfricanWildlifeOntology1.owl in Spanish, same IRI but different file name
has both simple deductions and more elaborate ones
(Item III); e.g., compare Lion that is classified as a
Carnivore, having one explanation involving three axioms,
with Warthog that is classified as an Omnivore, for which
there are three justifications computed that each use, on
average, five axioms. Because the AWO is small, does
not make extensive use of individuals and high number
restrictions, the reasoner terminates fast under all stan-
dard reasoning tasks (Item IV). OWL has the language
feature to import other ontologies and it also can be
Fig. 1 The African Wildlife Ontology at a glance. The main classes and relations of the African Wildlife ontology (v1) and an illustrative selection of its
subclasses. (The relations drawn are existentially quantified.)
Keet Journal of Biomedical Semantics            (2020) 11:4 Page 8 of 11
Table 3 Summary of the evaluation of the AWO against the
requirements
Item Eval. Item Eval. Item Eval.
1 + I + A +
2 + II + B +
3 + III + C +
4 + IV + D ±
5 + V + E 
6 ± VI + F ±
7 + VII  G +
H +
The evaluation (Eval.) can be either + fully met, ± partially met (not implemented),
or  not met
used in other ontology network frameworks, notably the
Distributed Ontology, Model, and Specification Language
DOL [20] (Item V). While OWL contains expressive fea-
tures such as role chaining (Item VI), it, arguably, fails on
intuitiveness especially for novices (Item VII). Regarding
the latter, e.g., for as of yet unclear reasons, novices make
errors in the use of existential and universal quantifica-
tion [4, 13, 14], which is not known to be a problem as
such when modelling the equivalent in, say, UML Class
Diagrams, and there is the elaborate n-ary (with n ? 3)
approximation issue.
With respect to the engineering aspects, by virtue of
the AWO being represented in OWL, there are tools that
can process the ontology (Item A), and therewith ontol-
ogy quality improvement methods can be used with the
AWO. They include, e.g., the popular Protégé, and various
tools for methods and quality, such as test-driven devel-
opment [21] and OOPS! [14], and ontology development
support activities, such as visualisation and documen-
tation (e.g., [22, 23]). There are also a few competency
questions that can be answered and that can be easily
modelled to be answered, as included in AWO version
1a (Item B), and there are examples and activities to
link it to foundational ontologies (AWO versions 2 and
3) with easy examples (Item C) (see below, Utility and
Discussion). There are several versions demonstrating
various quality improvements (Item G), avoiding vio-
lating some basic design principles like data properties
and punning hacks (Item H), and touching upon some
advanced engineering issues with multilingual ontologies
(see Table 2).
It falls short at the novice level on two requirements:
an easy way to link it to another ontology (Item E) and
bottom-up development from non-ontological resources
(Item F). It is possible and feasible in a mini-project
assignment, however; e.g., one could use the freely
available wildlife trade data6 or relate it to the Biodiver-
sity Information Standards7 for application scenarios, and
link it to the Envo Environment ontology [24] or take it
easier on the domain knowledge with one of the avail-
able tourism ontologies to create an ontology network. A
bottom-up approach to knowledge acquisition for ontolo-
gies is demonstrated with cellfie8 that implements the
M2 DSL [25] so that a modeller can add content in a
spreadsheet and cellfie converts that into axioms in the
ontology, as demonstrated in Example 7.1 of the textbook.
Regarding ODPs (Item D), a content ODP with the cur-
rent contents is not immediately obvious, but other types
of ODPs, such as architectural ones, are easy to illus-
trate, alike for BioTop [26] but then at the organism-level
with an orchestration between foundation, top-domain,
and domain-level ontologies, and what are dubbed excep-
tion patterns in [6] to be used for the deviant cases when
remaining within a monotonic logic such as OWL (e.g.,
penguins as non-flying birds).
Utility and discussion
The principal utility of the AWO is to be a concrete
machine-processable artefact for the related examples and
exercises, which we shall turn to first, and subsequently
discuss the tutorial ontology.
Use in exercises and examples
The major utility of the AWO is its use in educational
activities for ontology engineering exercises and examples
that are described in the An Introduction to Ontol-
ogy Engineering" textbook [16]. It is not intended as a
real domain ontology, but it is explicitly designed as a
tutorial ontology that has a domain ontology flavour to
it. Consequently, the subject domain knowledge about
African Wildlife has been kept simple, yet amenable to
extensions.
An example of an exercise is shown in Fig. 2, which fits
within the broader scope of sensitising the student to the
notion of quality of an ontology, using the vehicle of com-
petency questions that can be used in the validation stage
when evaluating whether the ontology meets its stated
goals. It also offers a gentle acquaintance with founda-
tional ontologies with some OWL classes that are either
easy or fun to categorise or to elicitate lively debate. For
instance, impalas die in the process of being eaten by a
lion, where both are straightforwardly subclasses of Phys-
ical Object in DOLCE [27] or Independent Continuant in
BFO [9], and Death is a type of Achievement or Process
boundary, respectively. The exercises of aligning AWO
to DOLCE is additionally assisted by the D3 decision
diagram [28]. Death/dying also provides an entry point
6https://www.kaggle.com/cites/cites-wildlife-trade-database
7http://www.tdwg.org/
8https://github.com/protegeproject/cellfie-plugin
Keet Journal of Biomedical Semantics            (2020) 11:4 Page 9 of 11
Fig. 2 Section of an exercise. Screenshot of the first part of Exercise
5.1 in [16], which lets the student experiment with requirements for
the content of an ontology, trying to find that knowledge, and the
task of evaluating an ontology on its quality based on its
requirements. The high-level notion of a good ontologycompared
to less good, bad, and worsehas been introduced earlier in the
textbook, which has to be recalled and applied here
to the alternate modelling styles of process-as-relation
vs. process-as-class representation options. Another core
distinction in modelling styles are data properties vs. a
hierarchy of qualities, for which a use case of elephants
weight in zoos across the world is used (Section 6.1.1 of
[16]).
While the emphasis in this paper is on modelling
and engineering aspects, the AWO is still suitable
for teaching about OWL language features and auto-
mated reasoning, as noted before regarding the deduc-
tions (e.g., Lion  Carnivore), and use of language
features such as transitivity and (ir)reflexivity with
parthood. Straightforward examples for demonstrating
unsatisfiability are multiple inheritance of Omnivore
to the disjoint Carnivore and Herbivore or to set the
domain of eats to Animal resulting in an unsatisfiable
CarnivorousPlant.
Additional variants of the AWO are in progress, which
zoom in on subject domains with corresponding exer-
cises that are not yet covered in the introductory textbook.
Among others, a future version 5 may be the engineering
aspects of importing, aligning, and integrating, another
domain ontology rather than a foundational ontology,
such as a module of the Environment Ontology with the
habitat information or a tourism ontology, with a corre-
sponding sample answer file. The former option would
be more suitable for ontology development in ecology,
whereas the latter is a more practical option in a tuto-
rial/course for people in other disciplines. Other themes
that have not been covered explicitly yet but easily can
be applied to the AWO are modularisation [29] and
Ontology-Based Data Access with its recent tools [30],
and it could be assessed against the MIRO guidelines for
reporting on ontologies [31].
Discussion
The AWO meets most of the tutorial ontology require-
ments that evolved and extended over the years. The
AWO goes beyond extant tutorial ontologies that over-
whelmingly focus only on demonstrating language fea-
tures and automated reasoning, or how to use a specific
version of a specific tool. In particular, the AWO brings in
ontology development aspects, such as competency ques-
tions and alignment to a foundational ontology, among
others.
The illustrations of gradual quality improvements
common in ontology developmentgo beyond the notion
that a new version only uses more language features, as
in Family History [5] and University9. In particular, there
are improvements on aspects regarding, among others,
content, naming, annotations, and foundational ontology
alignment.
Also, care has been taken in representing the knowl-
edge, such as avoiding some common pitfalls like the
class-as-instance and certain naming issues like and, or
or negation in a term [13]. Unlike other tutorial ontolo-
gies, including the popular Pizza and Wine, it is richly
annotated with informal descriptions, pointers to intro-
ductory domain knowledge, and questions for further
exploration of a modelling topic.
Tutorial ontology subject domains such as ones fam-
ily history, a university, or ones pets are distinctly
focussed on individual application scenarios that may
serve database development, but do not give an educa-
tionally good flavour of typical scopes of domain ontolo-
gies. In that regard, pizzas and wines fare somewhat
better, which, however, have repetitive content, such as
listing all ingredients of pizza topping. Contrast this with
animal wildlife, where it suffices already to represent that
a lion eats animals to have it classified automatically as
a carnivore. The wildlife subject domain is generic rather
than specific for one application scenario, and therewith
less predisposed to a myopic my thing only thinking that
is prevalent when students encounter ontologies for a first
time (a regular occurrence at least in the authors classes,
carried over from software design). Last, but not least,
besides its international appeal, African wildlife is obvi-
ously relevant for South Africa, where the author and
most of her students are based, and it fits with the trend to
make curricula regionally relevant. This is also reflected
in an isiZulu and an Afrikaans version of the ontology
and introductory aspects on term use for ontologies in
a multilingual setting, as Rockdassie is not a Standard
English word yet is widely accepted in South African
English. Overall regarding the content of a tutorial ontol-
ogy, however, it is a balancing act between simplicity and
ontological precision and correctness, as [6] also noted,
and international and national relevance, as well as an esti-
mation what may be assumed to be general common sense
knowledge by novice ontologists.
9http://owl.man.ac.uk/2005/07/sssw/university.html
Keet Journal of Biomedical Semantics            (2020) 11:4 Page 10 of 11
Conclusions
The paper introduced the African Wildlife Ontology
tutorial ontologies, which is a set of ontologies used for a
variety of ontology development examples and exercises.
Considering possible desirable educational outcomes, 22
requirements were formulated that a tutorial ontology
should meet. The AWO meets most of these require-
ments, therewith improving over its predecessors espe-
cially reading the notions of evolution of ontology quality
several ontology development tasks beyond getting the
axioms into an OWL file, such as alignment to a founda-
tional ontology and satisfying competency questions.
Both the 22 requirements and the AWO are relevant
to the field of ontology engineering in particular, espe-
cially for enhancing course material, which, it is hoped,
will result in further quality improvements of the actual
ontologies that developers are building.
Availability
The AWO is freely available under a CC-BY licence through the textbooks
webpage at https://people.cs.uct.ac.za/~mkeet/OEbook/.
Abbreviations
AWO: African wildlife ontology; CDM: Conceptual data model; FMA:
Foundational model of anatomy; FO: Foundational ontology; ODP: Ontology
design pattern; OE: Ontology engineering; OWL: Web ontology language
Acknowledgements
The author would like to thank previous ontology engineering course
participants on their feedback, which assisted in refining some of the
examples and exercises with the AWO, and Ludger Jansen for feedback that
helped improve the paper.
Authors contributions
The author contributed all aspects to the paper. The author(s) read and
approved the final manuscript.
Funding
The author declares that she has not received project funding for this work.
Publication charges were financially supported by the Hasso Plattner Institute
for Digital Engineering and the University of Cape Town.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The author declares that she has no competing interests.
Received: 13 December 2018 Accepted: 9 June 2020
Moen et al. Journal of Biomedical Semantics           (2020) 11:10 
https://doi.org/10.1186/s13326-020-00229-7
RESEARCH Open Access
Assisting nurses in care documentation:
from automated sentence classification to
coherent document structures with subject
headings
Hans Moen1* , Kai Hakala1,2, Laura-Maria Peltonen3, Hanna-Maria Matinolli3, Henry Suhonen3,4,
Kirsi Terho3,4, Riitta Danielsson-Ojala3,4, Maija Valta4, Filip Ginter1, Tapio Salakoski1 and Sanna Salanterä3,4
Abstract
Background: Up to 35% of nurses working time is spent on care documentation. We describe the evaluation of a
system aimed at assisting nurses in documenting patient care and potentially reducing the documentation workload.
Our goal is to enable nurses to write or dictate nursing notes in a narrative manner without having to manually
structure their text under subject headings. In the current care classification standard used in the targeted hospital,
there aremore than 500 subject headings to choose from, making it challenging and time consuming for nurses to use.
Methods: The task of the presented system is to automatically group sentences into paragraphs and assign subject
headings. For classification the system relies on a neural network-based text classification model. The nursing notes
are initially classified on sentence level. Subsequently coherent paragraphs are constructed from related sentences.
Results: Based on a manual evaluation conducted by a group of three domain experts, we find that in about 69% of
the paragraphs formed by the system the topics of the sentences are coherent and the assigned paragraph headings
correctly describe the topics. We also show that the use of a paragraph merging step reduces the number of
paragraphs produced by 23% without affecting the performance of the system.
Conclusions: The study shows that the presented system produces a coherent and logical structure for freely written
nursing narratives and has the potential to reduce the time and effort nurses are currently spending on documenting
care in hospitals.
Keywords: Patient care documentation, Nursing documentation, Electronic health records, Text classification,
Natural language processing, Neural networks, Model interpretation
*Correspondence: hnsmoen@gmail.com
Hans Moen and Kai Hakala contributed equally to this work.
1Department of Future Technologies, University of Turku, Vesilinnantie 5,
20500 Turku, Finland
Full list of author information is available at the end of the article
© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate
credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were
made. The images or other third party material in this article are included in the articles Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not included in the articles Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative
Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made
available in this article, unless otherwise stated in a credit line to the data.
Moen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 2 of 12
Background
Care documentation is important for supporting the con-
tinuity of care in hospitals. According to literature, nurses
spend up to 35% (with an average of 19%) of their working
time on documentation [1]. Naturally, if we can reduce the
time that nurses spend on documentation, more time will
be available for direct patient care.
To support tasks such as navigation, planning and sta-
tistical analysis, nurses in many countries are required
to perform structuring of the information they write [2].
Such structuring approaches include the use of documen-
tation standards, classifications and standardized termi-
nologies [3]. However, this usually adds certain restric-
tions and requirements to the documentation process
compared to writing the information in an unstruc-
tured narrative manner. In Finland, nurses are nowa-
days expected to structure the information they write
by using subject headings from the Finnish Care Clas-
sification (FinCC) standard [4]. This includes selecting
the correct subject headings and writing the associated
information underneath. In this way, each subject head-
ing forms a paragraph in the nursing note. As an example,
if a nurse wants to write something about administrated
wound care, he/she will first have to select an appropri-
ate heading, e.g. Wound. FinCC consists primarily of two
taxonomy resources, the Finnish Classification of Nurs-
ing Diagnoses (FiCND) and the Finnish Classification of
Nursing Interventions (FiCNI), and both of these have a
three-level hierarchy. For example, one branch in FiCND
is: Tissue integrity (level 1), Chronic wound (level 2)
and Infected wound (level 3). Another example, a branch
from FiCNI is: Medication (level 1), Pharmacotherapy
(level 2) and Pharmaceutical treatment, oral instructions
(level 3). However, FinCC consists of more than 500 sub-
ject headings, covering both interventions and diagnoses.
This makes it potentially challenging and time consuming
for nurses to use since they are required to memorize, use
and structure the information they write according to a
large number of subject headings [5].
What we are aiming for is to develop a system that can
assist nurses in selecting suitable subject headings and in
structuring the text accordingly.We hypothesize that such
a system has the potential to save time and effort required
for documentation and ultimately free up more time for
other tasks. We see two use-cases for such a system: One
is where the system assists nurses in selecting appropri-
ate headings when they write, in a suggestive manner, e.g.,
per sentence or paragraph; A second use-case is where
nurses are allowed to write or dictate (by voice to text)
in a fully unstructured narrative manner, without having
to take into consideration the structure or the use of sub-
ject headings. Instead the system assigns subject headings
afterwards and restructures the text into paragraphs. In
this study we focus on the second use-case.
This is the continuation of a previously reported study
that focused on assessing how an earlier version of the
system performs on the level of sentences [6]. The main
conclusion of that study is that a sentence classification
model trained on semi-structured nursing notes can be
applied on unstructured free nursing narratives without a
substantial decline in accuracy.
This time we focus on paragraph-level assessment,
where we also explore a post-processing step aimed at
reducing the number of paragraphs initially generated
by the system. To evaluate our system, a team of three
domain experts (aka evaluators) conduct a manual evalu-
ation to assess both the grouping of sentences into para-
graphs and the correctness of the assigned headings. In
addition we analyze the classification model in an attempt
to identify conflicts between the actual use of the sub-
ject headings and the intended use according to the FinCC
taxonomy.
At the core of our system is a text classification model
based on a bidirectional long short-termmemory (LSTM)
recurrent neural network architecture [7, 8]. As train-
ing data we use a large collection of nursing notes from
a Finnish hospital which contain subject headings and
which are structured accordingly. Further, to acquire the
type of narrative text that we would like to use as input
to the system, without a bias towards a particular struc-
ture and subject headings, we made a set of nursing notes
based on artificial patients that we use for testing.
Related work
As we focus on classifying individual sentences, the work
is closely related to other short text classification studies.
However, most of the prior work focuses on texts col-
lected from social media or other online sources [911].
Interestingly, Zhang et al. [12] conclude that the optimal
text classification method is strongly dependent on the
selected task, warranting domain specific research on this
topic.
In the clinical domain, a common objective for text
classification has been the automated assignment of ICD
codes to the target documents [1315]. For instance Xie et
al. [16] use a neural model for mapping diagnosis descrip-
tions extracted from discharge notes to the corresponding
ICD codes. Similarly Koopman et al. [17] assign ICD-10
codes to death certificates, but limit the scope to various
cancer types.
For cases where available training data is scarce, Wang
et al. [18] propose a system for producing weakly labeled
training data, where simple rules are initially used to
label a large set of unlabeled clinical documents and
these labels are subsequently used as training targets for
machine learning based classifiers. The approach is eval-
uated on smoking status and hip fracture classification,
but shows mixed results, with a rule-based baseline being
Moen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 3 of 12
the strongest model in some cases. As our training dataset
inherently contains the used classification labels, we have
not considered such weak supervision in our research.
To our knowledge the most recent systematic review
on clinical text classification was conducted by Mujtaba
et al. [19]. In addition to comparing the classification
approaches utilized in different studies, the review focuses
on the differences in the selected datasets. Their study
indicates that along with the medical literature, clinical
text classification research mostly focuses on pathology,
radiology and autopsy reports, whereas other clinical doc-
uments such as nursing care records are far less stud-
ied. Moreover, the vast majority of the reviewed studies
only evaluate their methods on English data, leading to
Mujtaba et al. suggesting wider range of languages to be
included in these studies.
As an additional note, Mujtaba et al. also conclude that
deep learning methods are still relatively poorly studied in
this domain. However, lately neural approaches have been
suggested for a wide range of medical text classification
purposes [2022].
More related to our research are prior studies on clinical
note segmentation. Denny et al. [23] present an approach
for detecting section headers in clinical notes based on the
free text. More precisely, they focus on history and phys-
ical examination documents where the goal is to identify
and normalize section headers as well as to detect section
boundaries. Li et al. [24] present a system that catego-
rizes sections in clinical notes into one of 15 pre-defined
section labels for notes already split into sections. Their
approach relies on modelling the dependencies of consec-
utive section labels with Hidden Markov Models. In [25]
coarse topics are assigned to the sections found in clin-
ical notes. These topics are here seen as separate from
the section headings used by the clinicians when writing,
thus the section headings are considered as input to the
classifier along with the free text.
A distinction between our study and the prior work
is that we operate with an order of magnitude larger
set of section labels. Additionally, we rely on semi-
structured nursing notes as training data with the devel-
oped method subsequently being applied on unstruc-
tured notes. Thus, we do not utilize any prior knowledge
about paragraphs/sections. Grouping the text into sensi-
ble paragraphs is instead a task for the presented system 
together with assigning subject headings.
Methods
Our ultimate goal is to develop a system that is able
to automatically identify and classify, on sentence level,
interventions and diagnoses mentioned in nursing narra-
tives, and further capable of grouping the text into sensible
paragraphs with subject headings reflecting their topics.
In other words, we are aiming for a system that can let
nurses simply write or dictate in a narrative manner with-
out having to plan and structure the text with respect to
paragraphs and subject headings. In pursuing this goal
we have implemented a prototype system with a neural
network-based text classification model at its core. In this
section we describe the data and methods used in the
implementation and evaluation.
Data
The data set used for training is a collection of approxi-
mately 0.5 million patients nursing notes extracted from a
university hospital in Finland. The selection criteria were
patients with any type of heart-related problem in the
period 2005 to 2009 and nursing notes from all units vis-
ited during their hospital stay are included. The data is
collected during a transition period between two classi-
fication standards, the latter being the mentioned FinCC
standard. This means that our training data contains a
mixture of headings from these two. We only use sen-
tences occurring in paragraphs with subject headings,
which amounts to approximately 5.5 million sentences,
133,890 unique tokens and approximately 38.5 million
tokens in total. We exclude all subject headings used less
than 100 times, resulting in 676 unique subject headings,
where their frequency count range from 100 to 222,984,
with an average of 4,896. The individual sentences are
used as a training example with the corresponding subject
heading as the target class to be predicted. The average
sentence length is 7 tokens1 and the average number of
sentences per paragraph is 2.1. The data set is split into
training (60%), development (20%) and test (20%) sets and
further used to train and optimize the text classification
model.
Text classification model
The classification task is approached as a sentence-level
multiclass classification task, where each sentence is
assumed to have one correct subject heading (label). Our
text classification model is based on a bidirectional short-
term memory (LSTM) recurrent neural network archi-
tecture [7, 8]. The model receives a sequence of words
as its input and encodes them into latent feature vectors
(dimensionality 300). These vectors are subsequently used
as the input for a bidirectional LSTM layer (dimensional-
ity 600 per direction). As the final layer a fully connected
layer with the dimensionality corresponding to the num-
ber of target subject headings is used. The word embed-
dings are pretrained with Word2vec [26]. The model is
optimized for categorical cross-entropy with Adam opti-
mizer [27], stopping early based on the development set
performance. As machine learning tools we primarily use
1Space separated units.
Moen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 4 of 12
the Keras deep learning library [28], with TensorFlow
library [29] as backend.
We want to emphasize that the focus of this paper is not
to find the optimal text classificationmethod and parame-
ter settings for this task. This has instead been the focus of
another study [30], where a range of different state-of-the-
art and baseline text classification methods are tested and
compared. Results from the mentioned study indicate that
a bidirectional version of LSTM networks performs best
when compared to other classification methods/models,
including convolutional neural networks, support vector
machines and random forests [3133].
On the test set, when the classifier is allowed to suggest
one subject heading per sentence, it suggests the correct
heading for 54.35% of the sentences according to auto-
mated evaluation. When allowed to suggest 10 headings
per sentence, the correct one is among these 89.54% of the
time (see [30] for more details).
Subject heading prediction and grouping into paragraphs
Since our prototype system relies primarily on a sentence-
level classification model, it starts by classifying each sen-
tence individually before grouping them into paragraphs.
However, this might arguably be the opposite order of how
a human would approach this task. The systems opera-
tion can be described as a four-step process. Step 1: First
the text is split into sentences. For this we rely on a com-
bination of the NLTK tokenizers for Finnish [34] and a set
of regular expressions tailored for the clinical text. Step 2:
Next the classification model is used to classify each sen-
tence individually and assign the top predicted heading
(the one with the highest confidence value). Step 3: As
a third step the sentences with the same assigned sub-
ject heading are grouped into paragraphs. Step 4: The
fourth step focuses on merging paragraphs whose content
and assigned headings are close to each other in terms of
meaning. This fourth step is included to potentially reduce
the number of paragraphs to more closely simulate how
nurses document. Below we explain in more detail how
this fourth step is done.
Paragraphmerging explained: In the previous study [6],
the evaluators reported that the system showed a ten-
dency to assign subject headings with a high level of
specificity, and sometimes even too specific to be prac-
tical. For example, for two or more sentences describing
different aspects of pain management in the same nurs-
ing note, such as treatment and medication, the system
would in some cases assign these to different subject
headings, possibly headings of different level of speci-
ficity/abstraction. This meant that, in some cases, unnec-
essarily many unique headings, thus paragraphs, were
assigned to each nursing note.
In an attempt to reduce the number of paragraphs cre-
ated, to more closely simulate how nurses document, we
have implemented an experimental post-processing step
that enables the system to merge paragraphs (within a
nursing note) that it finds to have similar subject head-
ings. For this we primarily rely on the confidence values
of the classification model, as well as extracted vec-
tor representations of each subject heading. The LSTM
layer outputs 600 dimensional sentence encodings for
both directions of the input sequence, resulting in 1200
dimensional vectors representations for the subject head-
ings. These we use to calculate heading similarity by
applying the cosine distance metric. See the Data anal-
ysis section for further description of these heading
vectors.
First a paragraph-to-paragraph similarity matrix is
formed reflecting how each paragraph would consider the
subject headings from the other paragraphs (from step
3) as a likely candidate heading. To this end we define
a simple asymmetric similarity function which measures
how inclined a paragraph (source) is towards the head-
ing of another paragraph (target) in the same nursing
note. For each sentence in a given source paragraph we
take the classifiers confidence of the sentence belong-
ing to the target heading and subtract the difference in
the confidence between predicting the source heading
and the target heading. The individual sentence scores
are averaged and further summed with the cosine dis-
tance between the source and target headings and the
relative size of the target paragraph (compared against
the whole nursing note). The first component, relying
on the confidence values of the classifier, describes how
well the sentences fit in the target paragraph. The sec-
ond component measures how semantically similar the
compared paragraph headings are, more similar headings
being more likely to be merged. The third component
increases preference towards retaining the headings of
the larger paragraphs. This scoring function produces
values in the range 3 to minus 2. Note that it is not
symmetrical.
To determine if two paragraphs are to be merged,
we require that the similarity between these two para-
graphs, in both directions, is above a given threshold.
If the threshold is exceeded, the two most similar para-
graphs are merged, keeping the heading of the para-
graph with the lowest score out of the two. Subsequently
the similarity matrix is recalculated, and the process is
repeated until no paragraph pairs can bemerged.We opti-
mize this threshold on a sample of nursing notes from
the test data where paragraph information and head-
ings are removed. A threshold is found that enables the
system to generate approximately the same number of
paragraphs as in the original versions of these nursing
notes.
Moen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 5 of 12
System evaluation
In this experiment the focus is on evaluating how the
system performs at the intended task. Two versions of
the system are manually evaluated, NoMerging and
WithMerging, where the difference is that NoMerging
only performs steps 13, while WithMerging also per-
forms step 4. This comparison is done to see if the para-
graph merging (step 4) can be done without reducing
system performance according to the evaluators assess-
ments. To perform the evaluation two domain experts
with nursing background first evaluated the paragraphs
individually. Then we consulted a third domain expert
who provided a third opinion for the instances where the
two disagreed. Finally the three of them agreed on the final
consensus version which we report here.
The evaluation focuses on two aspects of the struc-
tured notes produced by the system: 1) The correctness of
the assigned subject headings at paragraph level. Table 1
shows the classes used by the evaluators; 2) The quality of
the formed paragraphs, i.e. sentence grouping. The classes
used in this assessment are shown in Table 2.
The nursing notes from the training data have been
planned, structured and written with subject headings in
mind. One could argue that by simply removing headings
and paragraph information, automated evaluation could
be implemented. However, we found that the sentences
here, which are structured under subject headings, have a
tendency to be biased towards the topic of their headings,
and sometimes their meaning can only be interpreted in
the context of their headings. Also, this structuring forces
the nurses to write very short and concise things, whereas
when given the freedom to write in a narrative manner,
more complex sentence structures are present. Thus, to
obtain relevant nursing notes for evaluation of our use
case  notes written in a free narrative style without plan-
ning for or considering the use of paragraphs and subject
headings  we asked five domain experts with nursing
background to write notes based on made up artificial
patients. In total, 40 nursing notes, each note representing
one day of provided care for a patient, were generated. The
top part of Fig. 1 shows an example of one such nursing
note.
Table 1 Classes used by the evaluators when assessing the
headings assigned by the system
Class Description
1 Correct: the subject heading suits the text in this paragraph.
2 Partly correct: the subject heading only suits some of the text,
not all.
3 Incorrect: the subject heading does not seem to suit any of
the text.
4 Unable to assess: unable to asses whether or not this subject
heading is suitable.
Table 2 Classes used by the evaluators when assessing the
paragraphs formed by the system
Class Description
a Sensible grouping: it makes sense to have these sentences
grouped together as a separate paragraph based on their
topic(s) (even if the subject heading may not fit).
b Inconsistent/problematic grouping  alt1: one or more
sentences in this paragraph would fit better in other para-
graph(s) in this note.
c Inconsistent/problematic grouping  alt2: one or more
sentences in this paragraph do not belong in this or any of the
other paragraphs in this note.
d Unable to assess: unable to evaluate this paragraph.
These 40 nursing notes were fed to the two versions of
the system, NoMerging and WithMerging. For eval-
uation purposes the output was stored as spreadsheets,
one for each system, each containing both the origi-
nal and the generated/structured version of each nursing
note.
Statistical analyzes were performed to investigate differ-
ences in themanual evaluations of the two system versions
(Pearsons chi-squared test), as well as to see if there is
a possible correlation between manual evaluations and
the classification models confidence values (Spearmans
rho).
To gain some qualitative feedback on the system, we also
asked the evaluators to answer the following open-ended
questions:
Q1: Can you mention the main strengths that you found
with the system(s)?
Q2: Can you mention the main weaknesses that you
found with the system(s)?
Q3: Do you, or do you not, think that this kind of a
system would be helpful when it comes to nursing
documentation, and why?
Data analysis
We hypothesize that the large amount of subject headings
in the FinCC classification standard may cause confusion
among the nurses in terms of what headings should be
used in documenting the various aspects of the admin-
istrated care. Thus, to obtain a deeper understanding of
the evaluated sentence classification model and the care
documentation conventions of the nurses, we analyze
the heading representations learned by the classification
model  reflecting how they have been used  and how
this may differ from their description and intended use
based on FinCC.
The weights of the fully connected output layer of the
trained classifier can be seen as semantic representations
of the subject headings since the weights corresponding
Moen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 6 of 12
Fig. 1 Nursing note example. Top: Without any particular structure or assigned subject headings. Input to the system. Bottom: Grouped into
paragraphs with assigned headings. Output from the system. This has been translated from Finnish to English
to a given heading define how strongly the heading is
activated for a given input sentence, compared against
other possible headings. Thus, two headings with similar
weights will have similar probabilities of being assigned
to a given input sentence. Inversely, under the assump-
tion that the model has learned the classification task well,
it can be hypothesized that if two headings have similar
weights, the sentences assigned under these headings in
the training data are also similar. Note that these represen-
tations are not based on the names of the subject headings,
but instead on the actual sentences written under the
headings.
Our main goal in this analysis is to verify whether we are
able to find subject headings which are semantically sim-
ilar according to our classification model, but far apart in
the used FinCC taxonomy, or vice versa. This allows us to
identify conflicts between the actual use of headings and
their intended use according to the taxonomy. To mea-
sure the distances of the subject heading representations
we simply calculate the cosine distance across all heading
pairs.
The used FinCC classification standard is comprised of
3 top level categories: nursing diagnoses, nursing inter-
ventions and nursing outcomes, however the nursing out-
Moen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 7 of 12
come headings are not present in the used data. Both
nursing diagnoses and interventions use a hierarchical
structure with maximum depth of 3. To form a single tree,
we connect the diagnoses and interventions categories
with an artificial root node. This combined tree has amax-
imum depth of 4. To measure the distances of headings in
FinCC we calculate the shortest path between the head-
ing nodes in the tree. Although simple, this approach has
shown strong performance in measuring concept similar-
ities in other biomedical ontologies [35].
Once we have the two distances calculated for all sub-
ject heading pairs  cosine distance and distance in the
tree  we rank each pair based on these two, resulting
in two distinct rankings. The conflicting pairs that we
select for further analysis are the ones being furthest apart
according to these two rankings.
Since the nursing notes include the used subject head-
ings as plain text, without containing the actual FinCC
identifiers, we use strict string matching to map the head-
ings to the corresponding FinCC concepts. This leaves us
with 263 headings for this analysis out of the total 676
headings in our data set. The excluded headings either
originate from the older classification standard or contain
spelling variations.
Results
In this section we first present the results from the sys-
tem evaluation. Next we highlight some of the observa-
tions from the analysis of subject heading representations
according to the classification model and the underlying
classification standard.
System evaluation results
This experiment provided insight into how the system
performs at the intended task of assigning applicable sub-
ject headings and grouping sentences into paragraphs.
Table 3 shows how well the assigned subject headings fit
the text in the paragraphs. Table 4 reflects what the eval-
uators think about the integrity of the paragraphs formed
by the system.
See Fig. 1 for an example showing a input note to the
system (top) and the output (bottom) where the text is
grouped into paragraphs with assigned subject headings.
Table 3 Subject headings evaluation results. See Table 1 for an
explanation of the classes
Class NoMerging n(tot=396) WithMerging n(tot=305)
1 70.45% 279 71.15% 217
2 14.65% 58 16.72% 51
3 14.14% 56 11.80% 36
4 0.76% 3 0.33% 1
1+2 85.10% 337 87.87% 268
Table 4 Paragraph (sentence grouping) evaluation results. See
Table 2 for an explanation of the classes
Class NoMerging n(tot=396) WithMerging n(tot=305)
a 79.55% 315 79.02% 241
b 15.66% 62 12.13% 37
c 3.79% 15 8.52% 26
d 1.01% 4 0.33% 1
Overall these results show that the system is able to
provide suitable subject headings for about 71% of the
paragraphs (class 1). They also indicate that about 79%
of the paragraphs formed are sensible (class a). By sensi-
ble paragraphs we mean that all the sentences within are
related to the same topic and that none of them would fit
better elsewhere in the corresponding nursing note.
When using NoMerging the number of paragraphs
formed is 396, with an average of 9.9 per note (min=3,
max=19). When using WithMerging, which also per-
forms the paragraph merging step, the number of para-
graphs is reduced by 23%, down to 305, with the average
per note being 7.6 (min=2, max=17).
We also calculated how many of the formed para-
graphs were consistent (class a) while also having a
suitable subject heading (class 1). The result is seen in
Table 5 and shows that 66.67% (NoMerging) and 68.85%
(WithMerging) of the paragraphs are both sensible and
have a correctly describing subject heading assigned to
them. These results show that the merging step results in
basically no loss in performance.
Pearsons chi-squared tests were performed to see
whether there are statistically significant differences
between the evaluation results of NoMerging and
WithMerging based on 1) the subject heading correct-
ness evaluations, and 2) the paragraph (sentence merging)
quality evaluation results2. The evaluation of 1) does not
seem to be dependent on what system version was used
(X2 (2, N = 697) = 1.20, p = 0.55). However, there is a
statistically significant difference between the two when
looking at 2) (X2 (2, N = 696) = 8.12, p = 0.02).
It is possible that the confidence values of the classifier
may provide some indication of paragraph correctness in
that there is a correlation between the classifiers confi-
dence value for an assigned heading and the paragraph
being correct according to the manual evaluation results.
Using Spearmans rho to compare the manual evaluation
results of WithMerging with the classifiers confidence
values for each paragraphs assigned heading (average
across sentences), we found there to be a negative correla-
tion between classifier confidence values and the heading
assignment ratings (Spearmans rho = -0.42, moderate);
2Here we excluded classes 4 and d due to their low frequency (n<5).
Moen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 8 of 12
Table 5 Results showing the percentage of sensible paragraphs
(i.e. sentence groupings) with correct headings assigned
Class NoMerging n(tot=396) WithMerging n(tot=305)
1 and a 66.67% 264 68.85% 210
as well as between classifier confidence values and the rat-
ings of the quality of the formed paragraphs (Spearmans
rho = -0.29, weak).
Based on the open-ended questions posed to the evalu-
ators, they reported the following.
A1: As strengths they reported that the system does an
overall (surprisingly) good job and usually provides
good enough results.
A2: Its main weakness and challenge is that people tend
to write information about more than one topic into
the same sentence. This sometimes makes it
challenging for the system since it is tasked with
classifying the entire sentence. They suggest that
some sort of smart sentence splitting, which has the
ability to split such sentences into two or more
phrases, could help. Further, they also noticed that
the basic sentence splitting performed by the system
was not always correct, which sometimes resulted in
the main message of a sentence being lost. One
reported observation suggests that the system seems
to perform worse on the more atypical and complex
nursing notes.
A3: On the question regarding whether or not the system
could be helpful, they report that they think it could
be (very) helpful since nurses would not need to
consider where to write the information or what
subject headings to choose. This would reduce time
and effort required for nurses documentation duties.
It is also suggested that this kind of system would
work well when the documentation is done via
dictation (speech to text). Another suggested
consequence of using this system is that it could
increase consistency in how headings are being used
for similar information. However, it was also
mentioned that having the ability to first select a set
of subject headings can sometimes be helpful to
remember what to report. The evaluators suggest
that increased performance could be gained through
fine-tuning the model for the different units at the
hospital, possibly by limiting the pool of headings to
select from.
Data analysis results
Before looking in detail at the measured similarities
between heading pairs, we examine the overall quality of
the heading representations and the agreement between
the two used rankings. To visually inspect the representa-
tions we form a dendrogram from a hierarchical clustering
of the headings. Figure 2 shows an example subtree of the
dendrogram with two high level clusters. The first one
focuses on breathing, containing 10 headings overall, all
of which are related to the topic. The second cluster con-
tains 9 headings related to patients activity where most of
the formed clusters are meaningful.
Although the heading vectors seem to offer good
semantic representations, and the shortest paths have
been used for measuring semantic similarity with other
biomedical ontologies, there seems to be a strong dis-
Fig. 2 Heading Dendrogram. A subtree of the heading dendrogram formed with hierarchical clustering of the subject heading representations
derived from the neural network classification model
Moen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 9 of 12
agreement between these two approaches. For instance
the Spearmans rho between the two formed rankings is
only 0.12. To gain an insight into why these two rank-
ings are heavily conflicted, we look at heading pairs that
the classifier has identified as similar, but for which the
corresponding distance in FinCC is high. It turns out
that all top 1000 pairs with the largest difference in the
ranks in this setting are pairs where one of the headings
originates from the nursing diagnosis category and the
other from the nursing interventions. The top conflicting
pairs include headings such as Nursing Diagnosis: Uri-
nary Incontinence  Nursing Intervention: Treatment
of Urinary Incontinence, Nursing Diagnosis: Changes in
Oral Mucosa  Nursing Intervention: Basic Care of Oral
and Other Mucosa and Nursing Diagnosis: Swelling 
Nursing Intervention: Monitoring Swelling.
To show that these headings are not similar accord-
ing to our model only due to its incapability to distin-
guish the semantic differences between diagnoses and
interventions, we look into the actual sentences writ-
ten under these headings. For instance the Nursing
Diagnosis: Swelling heading is assigned by nurses to
sentences such as Swelling of right arm. and Severe
swelling of shins. whereas Nursing Intervention: Moni-
toring Swelling heading contains sentences such as Shins
somewhat swollen. and Shins still swollen, feet not as much.
In fact, sentences such as Legs swollen. occur identically
under both of these headings.
Similar trend can be seen by just looking at the most
similar headings according to the classifier, ignoring the
conflicting distance in the taxonomy. The most similar
heading pair in the whole representation space is Nursing
Intervention: Providing Additional Nutrition  Nursing
Intervention: Offering Supplements. Both of these head-
ings again contain identical sentences, such as Renilon
1 can at 11 am and PreOp 2 cans, 6 oclock, yet these
headings are not closely related in the FinCC taxonomy.
In classification tasks it is often the case that the
amount of training data have a clear correlation with
classification model performance, and that this can also
be observed on the level of individual classes (i.e.,
subject headings in our case). However, based on the
automated evaluation data, we do not observe a lin-
ear dependence between heading-specific model accu-
racy and the amount of training data for each head-
ing (Pearsons r = 0.04), probably due to the relatively
large number of training examples for most classes.
There is neither a linear dependence between accuracy
and heading specificity (depth) in the FinCC taxonomy
(Pearsons r = 0.02).
Discussion
Overall, the results suggest that the system is doing a rel-
atively good job at the task of grouping nursing note text
into paragraphs and labelling them with subject headings.
According to the manual evaluation, shown in Table 5,
68.85% of the paragraphs formed by the WithMerging
system variant are sensible and have been assigned subject
headings that correctly describes the text therein. Even
though the results are not yet perfect, we believe that the
system could already be helpful by producing an initial
structured version that the users can correct afterwards if
needed.
We found that there is a correlation between the
paragraph-level manual evaluation results and the classi-
fiers confidence values for the assigned headings. Thus,
for practical use, it could be helpful to the user to see these
confidence values, for each paragraph and/or sentence,
when assessing whether or not to retrospectively correct
the initial structured version of a nursing note. As a future
work we are considering training a separate model for the
purpose of classifying the quality of formed paragraphs 
e.g. as good or bad.
In a previous study we conducted a manual evalua-
tion of the same classification model as used here, but
this focused on sentence-level evaluation instead of para-
graphs [6]. In that study the evaluation was conducted on
20 nursing notes3 instead of 40, as in the present study.
This previous evaluation showed that between 68.05% to
88.40% of the sentences had been assigned a suitable head-
ing. In the present study, focusing on paragraphs, the
results are similar, and equivalent to classes 1 (71.15%)
and 1+2 (87.87%) in Table 3, accordingly.
These similarities are as expected for the NoMerging
variant, since it merely groups together the sentences
with the same assigned subject headings. More interest-
ing is the observation that WithMerging has about the
same performance as NoMerging (0.70 percentage point
increase for class 1 and 2.77 for class 1+2). This indi-
cates that the merging step performed by WithMerging
does not result in less suitable headings being assigned
to the paragraphs. We also observe that there is not a
statistically significant difference between these evalua-
tions. Further, by looking at Table 4, we see that the
differences between the two system versions in terms of
paragraph (sentence grouping) quality are small (e.g., only
0.53 percentage point decrease for class a when com-
paring WithMerging to NoMerging). However, the
Pearsons chi-squared test shows that these differences can
be considered as being statistically significant.We observe
that the main differences between the assessments of the
two system versions are found in classes b and c, indi-
cating that the paragraphs produced by WithMerging
has fewer sentences which should be moved to other
formed paragraphs within a nursing note (b), but instead
more sentences do not group well with any of the para-
3These 20 nursing notes are a subset of the 40 ones used in the present study.
Moen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 10 of 12
graphs it produces (c). Whether this difference is of
practical relevance is something that requires further
investigation.
As already mentioned, this system has the potential
to save nurses time and effort when it comes to docu-
mentation. As suggested by the evaluators, this system
can be helpful when the documentation is performed via
speech-to-text dictation  which alone has been shown to
decrease documentation time [36, 37].With amicrophone
being the main interface for the user (instead of a key-
board), it will be evenmore difficult tomanually select and
insert subject headings and structure the resulting text
accordingly. Thus, with the use of such a system, nursing
documentation could potentially be done, e.g., at the point
of care and still produce nursing notes that follow the
ruling documentation standard. The use of the proposed
system also has the potential to increase the consistency
in the use of subject headings for similar information and,
as a consequence, improve the documentation quality.
Regardless of how the text is produced, a classification-
based model like we use here could additionally serve as
a reminder system that reminds the user about possible
missing information in the nursing notes being written.
For example, if a unit requires that certain topics should
be mentioned in the nursing notes, the system will be
able to detect (with a certain confidence) if something
has not been reported yet. Similarly, the system could be
used to notify users if a sentence already written under
one heading/paragraph might better fit under another
heading.
We did not put any limitations on the units and wards
from where the nursing notes used in this study come
from. Still, it is difficult to say how this system generalizes
to the various units at the hospital. However, asmentioned
in the answers from the evaluators, performance of such
a system is likely to improve if it were to be tailored for
individual units at the hospital. We believe that separate
versions of the system could be used at the different units
and wards at the hospital. In this way, the training data and
what the classification model learns would more closely
reflect the local documentation practices.
In the classification model used in our system, all train-
ing examples contained a sentence as input and a subject
heading from the classification standard to predict as out-
put. However, we have also observed that some of the
text that the nurses document may not necessarily belong
under a specific subject heading. Examples include meta
information regarding the unit/ward, dates and names. As
a future work we plan to also include such information as
training examples for the model, and thus allow the sys-
tem to suggest that some text does not need to be assigned
a subject heading.
The paragraph merging step used here is rather prim-
itive. Further, the system (WithMerging) is currently
only allowed to merge the initially formed paragraphs. As
a future work we plan to develop this merging algorithm
further, where initially formed paragraphs may be split up
to form new ones, and with the possibility of introducing
new headings in the process. One idea could be to apply
some sort of centrality-based algorithm.
The exploration of the heading representations formed
by the classification model reveals a drastic discrepancy
between the FinCC taxonomy and the actual use of the
subject headings. The most prominent observation is that
neither the classification model nor the nurses differen-
tiate between diagnosis and intervention headings, but
instead the same textual content is often documented
under both variants of otherwise similar headings, e.g.
Swelling (diagnosis) and Monitoring Swelling (inter-
vention). Similar indistinguishable heading pairs can be
detected within the main categories.
We believe these observations can be beneficial in devel-
oping future versions of FinCC as they provide a semi-
automated method for identifying problematic taxonomy
definitions based on a large collection of nursing notes,
whereas the prior development has relied on small-scale
questionnaires [38]. Since FinCC is derived from the inter-
national Clinical Care Classification (CCC) System [39],
these issues are most likely not specific to FinCC, but also
present in other patient care frameworks.
Conclusions
In this study we have described the evaluation of a sys-
tem aimed at assisting nurses in documenting patient care.
The aim is to allow nurses to write the information they
want to document without having to manually structure
the text under subject headings which they select from a
large taxonomy. Instead, the system automatically groups
sentences into paragraphs and assigns subject headings. In
68.85% of the paragraphs formed by the system, the topics
of the sentences are coherent and the assigned headings
correctly describe the topics. Further, we show that the use
of a paragraph merging step reduces the number of para-
graphs produced by 23% without affecting the quality of
the patient documentation, resulting in a more coherent
outcome.
Finally, we show that interpreting the internal work-
ings of the used neural classifier provides insights into the
actual use of the subject headings in care documentation
and can be used to pinpoint where the documentation
practices deviate from the intended use of the care clas-
sification standards. Such observations can be utilized
in improving the usability of the underlying clinical care
taxonomy.
This study shows that the use of text classification
applied to clinical nursing notes has the potential to
reduce the time and effort that hospital nurses are cur-
rently spending on care documentation.
Moen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 11 of 12
Abbreviations
FinCC: Finnish Care Classification standard; FiCND: Finnish Classification of
Nursing Diagnoses; FiCNI: Finnish Classification of Nursing Interventions; LSTM:
Long Short-Term Memory recurrent neural network; ICD: International
Classification of Diseases; CCC: Clinical Care Classification
Acknowledgements
We would like to thank the anonymous reviewers for their valuable comments
and suggestions which led to improvements to the originally submitted
manuscript.
Authors contributions
HM initiated the study and was, together with KH, LMP and SS, responsible for
the overall design. HM and KH conducted the system implementations,
performed the machine learning experiments, data preprocessing and
automated evaluations under the supervision of FG and TS. LMP, HMM, HS,
RDO, KT and MV developed the data set used for testing the system and,
together with SS, provided the required domain expertise. LMP, HMM, HS were
responsible for the manual evaluations of the system. HM and KH outlined the
first draft of the manuscript. All authors contributed to editing and revising the
manuscript. All authors read and approved the final manuscript and there are
no other persons who satisfy the criteria for authorship but are not listed. The
order of authors listed in the manuscript has been approved by all authors.
Funding
This research was supported by Academy of Finland (315376). The Titan Xp
GPU used in this research was donated by the NVIDIA Corporation.
Availability of data andmaterials
The datasets generated and/or analysed during the current study are not
publicly available due to their sensitive nature but more information are
available from the corresponding author on reasonable request.
Ethics approval and consent to participate
Ethical approval for using the data was obtained from the hospital districts
ethics committee (17.02.2009 §67) and research approval was obtained from
the medical director of the hospital district (02/2009).
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Author details
1Department of Future Technologies, University of Turku, Vesilinnantie 5,
20500 Turku, Finland. 2University of Turku Graduate School, University of
Turku, Hämeenkatu 4, 20500 Turku, Finland. 3Department of Nursing Science,
University of Turku, Joukahaisenkatu 3-5, 20520 Turku, Finland. 4Turku
University Hospital, Kiinamyllynkatu 4-8, 20521 Turku, Finland.
Received: 19 May 2019 Accepted: 14 August 2020
Wolff et al. Journal of Biomedical Semantics           (2020) 11:12 
https://doi.org/10.1186/s13326-020-00226-w
RESEARCH Open Access
Methodologically grounded semantic
analysis of large volume of chilean medical
literature data applied to the analysis of
medical research funding efficiency in Chile
Patricio Wolff1, Sebastián Ríos1, David Clavijo1, Manuel Graña2* and Miguel Carrasco3
Abstract
Background: Medical knowledge is accumulated in scientific research papers along time. In order to exploit this
knowledge by automated systems, there is a growing interest in developing text mining methodologies to extract,
structure, and analyze in the shortest time possible the knowledge encoded in the large volume of medical literature.
In this paper, we use the Latent Dirichlet Allocation approach to analyze the correlation between funding efforts and
actually published research results in order to provide the policy makers with a systematic and rigorous tool to assess
the efficiency of funding programs in the medical area.
Results: We have tested our methodology in the Revista Médica de Chile, years 2012-2015. 50 relevant semantic
topics were identified within 643 medical scientific research papers. Relationships between the identified semantic
topics were uncovered using visualization methods. We have also been able to analyze the funding patterns of
scientific research underlying these publications. We found that only 29% of the publications declare funding sources,
and we identified five topic clusters that concentrate 86% of the declared funds.
Conclusions: Our methodology allows analyzing and interpreting the current state of medical research at a national
level. The funding source analysis may be useful at the policy making level in order to assess the impact of actual
funding policies, and to design new policies.
Keywords: Data science, Machine learning, Latent Dirichlet allocation, Healthcare management, Strategy
Background
Due to the speed of innovation and change of research
trends in the medical community, research topic tax-
onomies published by governmental agencies for funding
calls often diverge from the reality of the research practice.
Our working hypothesis is that semantic topic analysis
provides an unbiased and accurate portrait of the actual
research topics that are generating published results. In
this paper we exploit the information from a national
*Correspondence: manuel.grana@ehu.es
2Computational Intelligence Group, University of Basque Country, P. Manuel
Lardizabal 1, 20018 San Sebastián, Spain
Full list of author information is available at the end of the article
medical publication, described below, to identify the areas
of active research, correlating them with the acknowl-
edged funding sources, and non-funded personal effort
backing these scientific results. This analysis provides the
policymaker with a systematic, unbiased, and automated
tool for the evaluation of the results of funding programs,
allowing to assess the coherence of the national research
funding policies with the actual research outcomes.
Methodology background
