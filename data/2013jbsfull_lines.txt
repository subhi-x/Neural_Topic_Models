JOURNAL OF
BIOMEDICAL SEMANTICS
Wu et al. Journal of Biomedical Semantics 2013, 4:1
http://www.jbiomedsem.com/content/4/1/1RESEARCH Open AccessA common type system for clinical natural
language processing
Stephen T Wu1*, Vinod C Kaggal1, Dmitriy Dligach2, James J Masanz1, Pei Chen2, Lee Becker3,
Wendy W Chapman4, Guergana K Savova2, Hongfang Liu1 and Christopher G Chute1Abstract
Background: One challenge in reusing clinical data stored in electronic medical records is that these data are
heterogenous. Clinical Natural Language Processing (NLP) plays an important role in transforming information in
clinical text to a standard representation that is comparable and interoperable. Information may be processed and
shared when a type system specifies the allowable data structures. Therefore, we aim to define a common type
system for clinical NLP that enables interoperability between structured and unstructured data generated in
different clinical settings.
Results: We describe a common type system for clinical NLP that has an end target of deep semantics based on
Clinical Element Models (CEMs), thus interoperating with structured data and accommodating diverse NLP
approaches. The type system has been implemented in UIMA (Unstructured Information Management Architecture)
and is fully functional in a popular open-source clinical NLP system, cTAKES (clinical Text Analysis and Knowledge
Extraction System) versions 2.0 and later.
Conclusions: We have created a type system that targets deep semantics, thereby allowing for NLP systems to
encapsulate knowledge from text and share it alongside heterogenous clinical data sources. Rather than surface
semantics that are typically the end product of NLP algorithms, CEM-based semantics explicitly build in deep
clinical semantics as the point of interoperability with more structured data types.
Keywords: Natural Language Processing, Standards and interoperability, Clinical information extraction, Clinical
Element Models, Common type systemBackground
Electronic medical records (EMRs) hold immense prom-
ise for improving both practice and research. Area 4 of
the Strategic Healthcare IT Advanced Research Project
(SHARP 4, or SHARPn) aims to reuse data from the
EMR, analyzing records on a large scale  an effort
known as high throughput phenoytyping. Many large-
scale applications are dependent on high throughput
phenotyping, such as characterizing the prevalence of a
disease, or finding patients who fit the criteria for a clin-
ical or epidemiological study. A prerequisite is that
information across patients, areas of practice, and insti-
tutions must be comparable and interoperable. SHARP
4 has adopted Intermountain Healthcares Clinical* Correspondence: wu.stephen@mayo.edu
1Mayo Clinic, Rochester, Rochester, MN, USA
Full list of author information is available at the end of the article
© 2013 Wu et al.; licensee BioMed Central Ltd
Commons Attribution License (http://creativec
reproduction in any medium, provided the orElement Models (CEMs) as the standardized format for
information aggregation and comparison. This represen-
tation is both concrete and specific, yet allows for some
of the ambiguity that is inherent in clinicians explan-
ation of a clinical situation.
However, a significant amount of information in the
EMR is not available in any form that could be easily
mapped to CEMs. It is no surprise that health care
professionals prefer to record a significant proportion of
their information in the format of human language,
rather than more structured formats like CEMs. There-
fore, Natural Language Processing (NLP) techniques are
necessary to tap into this extensive source of clinical
information. The goals for NLP in SHARPn are to
normalize information from clinical text into the struc-
tured CEMs, which are more conducive to computation
at a large scale.. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Wu et al. Journal of Biomedical Semantics 2013, 4:1 Page 2 of 12
http://www.jbiomedsem.com/content/4/1/1A type system specifies data structures that may be
used for the processing and sharing of information. In
this work, we define a type system whose key innovation
is that it implements a comprehensive model of clinical
semantics types, based on CEMs. This deep semantic
target is integrated with a comprehensive brush of types
for existing language analysis tools, allowing the type
system to be used for arbitrary clinical use cases and to
be compatible with a diversity of underlying NLP
approaches. Therefore, we call it a common type system,
with highly structured output semantics intended to
interoperate with structured data from the EMR. Add-
itionally, NLP components that use the type system will
be interchangeable with each other. The type system was
initially designed for practical NLP use in UIMA (Un-
structured Information Management Architecture [1]),
which allows for flexible passing of input and output
data types between components of an NLP system.
Our preliminary work [2] has been fully adopted by
Mayo Clinics popular open source NLP tool, cTAKES
(clinical Text Analysis and Knowledge Extraction System
[3]), as of cTAKES 2.0. The current work presents a full
picture of the type system, alongside a thorough example
of how the type system may be used in practice to house
SHARPn-style CEMs. Our description is consistent with
the implementation in cTAKES v2.5 (http://sourceforge.
net/projects/ohnlp/files/cTAKES/).
UIMA and type systems
UIMA was originally designed by IBM to process text,
speech, or video [1]. Here, we concern ourselves with
clinical text as our domain of input. Each clinical docu-
ment that is processed within UIMA is automatically
marked up (annotated) by components called Analysis
Engines, which are often arranged in a pipeline. Analysis
Engines may be interchanged if they solve the problems
and annotate the data in the same way.
However, the structure of the markup must be defined
in order for Analysis Engines to be interoperable. A type
system defines the structure for possible markup, provid-
ing the necessary data types for downstream com-
ponents to make use of partially processed text, and
gives upstream components a target representation for
markup data. For example, after sentence detection, a
document will have identified types called SENTENCE;
after tokenization, a document will have identified types
called WORDTOKEN. Each type may have associated
features, which give additional information about the
structure. For example, a WordToken could have an
associated part-of-speech VB (verb). In this article, we
will use feature and a related term, attribute,
interchangeably.
The data are then passed between Analysis Engines in
an efficient framework, the Common Analysis Structure(CAS), which includes the original document, the results
of the analysis, and indices for efficient searching of
these results. To facilitate outputs from and inputs to
UIMA, the CAS can also be efficiently serialized and de-
serialized. With this architecture, UIMA enables inter-
operability between Analysis Engines and encourages a
development of best-of-breed components.
All UIMA-based techniques will have a type system
[4-6], and other tools (such as the General Architecture
for Text Engineering (GATE) [7]) typically have analo-
gous schemata for artifacts. Most of these type systems
encode the same basic information as our common type
system, including types for storing text span annotations,
syntax, and document annotations. In a few cases, types
and features (e.g., a LIST structure) were introduced into
our common type system based on an analysis of these
systems.
The reported work within SHARP 4 is an attempt to
provide a common type system for diverse NLP use
cases centering around clinical texts and domain seman-
tics. Therefore, the our most significant contributions
are the extensive semantic model based on CEMs and
the separation between textual semantic types and refer-
ential (referring to the real-world) semantic types. These
contributions enable a development of diverse technolo-
gies that serve different clinical use cases.Deep semantics with clinical element models
From a linguistic perspective, this common type system
embeds a deep semantic representation analogous to
those that have been used in the computational seman-
tics and dialogue systems communities [8,9]. It distin-
guishes between semantic content that refers to real-
world phenomena and the textual surface form used to
communicate the semantics. However, we might expect
the impact of a mature, deep semantic representation
for Clinical NLP to be much greater, since this is an en-
abling technology for many downstream tasks like pa-
tient classification and high-throughput phenotyping.
Designing the type system to account for these deep
semantics as output gives room for technological inno-
vations around the CEM structure.
In addition to providing a well-developed semantic
data model, the common type system provides a wide
range of data types to bridge from text and linguistic
structure to deep semantics. In doing so, it allows for
downstream access to both the more raw, textual data
types and the deeper semantic representation.
For SHARPn, six core CEMs have been identified
and are under continuing development: Anatomical
Sites, Diseases and Disorders, Signs and Symptoms, Pro-
cedures, Medications, and Labs. A specific CEM for
cough would have the same basic structure as any
Wu et al. Journal of Biomedical Semantics 2013, 4:1 Page 3 of 12
http://www.jbiomedsem.com/content/4/1/1other Signs and Symptoms, as defined by the Signs and
Symptoms core CEM:
<<cetype kind="statement"
name="CoughAssert" xmlns="">
<key code="Assertion_KEY_ECID" />
<data domain="CoughType_VALUESET_ECID"
type="cwe" />
. . .
<qual card="0-M" name="periodicity"
type="Periodicity" />
<qual card="0-1" name="course" type="Course" />
<qual card="0-1" name="severity" type="Severity" />
. . .
<mod card="0-1" name="subject" type="Subject" />
<mod card="0-1" name="negationInd"
type="NegationInd" />
<mod card="0-1" name="uncertainty"
type="Uncertainty" />
<att card="0-1" name="observed" type="Observed" />
<att card="0-1" name="reportedReceived"
type="ReportedReceived" />
<att card="0-1" name="verified" type="Verified" />
</cetype>
The basic structure of a CEM consists of a type, a key,
and a value choice; qualifiers, modifiers, and attributions
give further detail. The Type is a coded value that repre-
sents the constraints to which all instances of a given
model will conform (e.g., cwe  coded with extensions,
or pq  physical quantity). The key is a coded value for
the real world concept that is important to what an in-
stance is attempting to describe (e.g., since we are mod-
eling text, Assertion is a common key). Finally, the value
choice is a choice between a data property or items,
where the former is a derivative of the HL7 version 3
data type ANY, and the latter is a sequence of one or
more clinical elements (e.g., CoughType is a data prop-
erty, constraining data values).
A qualifier captures information that does not change
the meaning of the value choice (e.g., the periodicity of
a cough). A modifier adds information that changes the
meaning of the value choice (e.g., negationInd may re-
verse the asserted CoughType). An attribution defines
an action and the contextual information for the action
(e.g., observed gives the context of the Cough).
In the end, this work reports, alongside other NLP-
relevant types, a casting of CEMs from the above struc-
ture into a UIMA type system.
Methods
To define our common type system, we began with
the types in the cTAKES v1.1 type system. These types
had primarily been developed ad hoc while doinginformation extraction tasks in UIMA. They were there-
fore loosely arranged around a typical information
extraction pipeline (including components such as sen-
tence detection, tokenization, lemmatization, named en-
tity recognition (NER), and negation/status detection).
We analyzed a number of existing UIMA type systems
to find useful types that caused us to augment or modify
the original type definitions. We eventually modified
existing types and categorized them into 4 groupings:
Utilities, Text Spans, Syntax, and Text Semantics. We
also created 3 new groupings: Structured Data, Rela-
tions, and Referential Semantics.
In this breakdown of 7 groupings, we have ensured
that the resulting semantic model distinguishes clearly
between text semantics and referential semantics. This
distinction is important for several reasons. The NLP
task of Named Entity Recognition may define semantics
in terms of a semantic type or even a mapping to an
ontology code (possible with text semantics types), but
additional structure is necessary when populating post-
coordination or attribute templates (possible with refer-
ential semantics types). Furthermore, there is no clear
interpretation for the results of coreference resolution
with only text semantics. Especially in the clinical con-
text, a chain of related text mentions does not define an
event; for example, if a patient has a severe cough with
sputum, but coreferring mentions in the text do not
mention the sputum, this does not mean that there is no
sputum. Finally, deep semantic structure is necessary be-
cause it enables interoperability with all other types of
medical information. With this CEM-based referential
semantic model, structured data and NLP results can
then be used together seamlessly in high-throughput
phenotyping efforts.
The effort in the type system definition was that of
creating types for the referential semantics grouping, in
which our goal was to represent the core CEM tem-
plates. Notably, we created a separate type for each of
the 6 core CEMs. The distinction between qualifiers,
modifiers, and attributions in the core CEMs was
dropped, and all were considered to be features. Com-
mon features between the core CEMs were moved into
the Element supertype. The text semantic model was
adjusted to complement the referential semantic model.
Outside of the semantic model, new types were also cre-
ated for standard NLP tasks, such as constituent and de-
pendency parsing, relation extraction, and temporal
relations.
Overall, the type system design attempts to follow best
practices for UIMA type systems, as recommended by
UIMAs original developers. These have to do with ease
and completeness of representation, as well as computa-
tional cost. For example, defining a subtype is an effi-
cient way to subset data because indices for types are
Wu et al. Journal of Biomedical Semantics 2013, 4:1 Page 4 of 12
http://www.jbiomedsem.com/content/4/1/1reliably calculated. However, we do not put locally used
(component-specific) types in the CAS, as there is no
garbage collection in UIMA and extra types only bloat
the type system. Where possible, we assumed existing
standards for NLP tasks (e.g., types for constituent
parses should be consistent with Treebank II).
Results
We present the SHARPn common type system in its en-
tirety, as released with cTAKES 2.5. This type system is
an extensive update of the cTAKES type system, with
modifications, restructuring, and additions. While there
are carry-over types from previous iterations of the
cTAKES type systems, use case specific types were
dropped, as they would be local to custom components
at the end of a pipeline.
The type systems seven groups correspond to 7 name-
spaces. We will thoroughly describe these groups, focus-
ing especially on syntax, text semantics, relations, and
referential semantics with examples to highlight our
contribution.
Notationally, we introduce types with small caps and
in their namespaces (e.g., TEXTSPAN.SENTENCE), but refer
to them informally by just capitalizing just the first letter
(e.g., Sentence). Features (attributes) of each type are
introduced with colons (e.g., SENTENCE:begin). Using the
equivalent data structures outside of UIMA do not
strictly require these namespaces or groupings. Also, in
the following figures, dark gray boxes indicate types are
in a different namespace, but are necessary to fully de-
scribe the inheritance of another type (e.g., UIMA.TCAS.
ANNOTATION).
Structured data types
Unstructured clinical text is generated in the wider con-
text of clinical settings. Structured data can provideFigure 1 Types and features for 3 namespaces. Structured data types, U
indicates types that are not in the namespace but are included to show inuseful information about the clinical context, both to
improve information extraction and to more easily re-
trieve results. Structured data types are shown on the
left side of Figure 1.
Utility types
This minimal grouping (bottom right, Figure 1)
replaces a type from cTAKES v1.1 called PROPERTY
with the UTIL.PAIR type. Each PAIR:attribute corre-
sponds with some PAIR:value, and a UTIL.PAIRS type
stores multiple ones. This is tacitly a brute-force im-
plementation of a probability distribution as well,
hence the UTIL.PROBABILITYDISTRIBUTION type. More ef-
ficient means of defining probability distributions are
possible, but this is not easily done in a generalizable
type system to be used with UIMA.
Text span types
Text span types shown on the top-right side of Figure 1
are typically discourse-level subdivisions of a text docu-
ment into organizational components. The type DOCU-
MENT spans a whole document and subsumes other text
span types. In UIMA implementations of the type sys-
tem, this DOCUMENT type is by default available from the
UIMA itself (hence the gray box); non-UIMA imple-
mentations would need DOCUMENT to be introduced ex-
plicitly. Other types break down the text into spans of
decreasing size: TEXTSPAN.SEGMENT (e.g., sections of a
clinical note), TEXTSPAN.PARAGRAPH, TEXTSPAN.LIST, and
TEXTSPAN.SENTENCE.
Syntactic types
The syntax namespace in Figure 2 shows two major
groupings: Morphology (light blue) and Syntactic Struc-
ture (darker blue). The Morphology grouping deals with
the internal characteristics of words and borderstility types, and Text span types. Dark gray background coloring
heritance. Arrows indicate inheritance.
Figure 2 The syntax namespace: types for morphology and syntax.
Wu et al. Journal of Biomedical Semantics 2013, 4:1 Page 5 of 12
http://www.jbiomedsem.com/content/4/1/1between words. It centers around SYNTAX.BASETOKEN, a
supertype for word, punctuation, symbol, newline,
contraction, or number tokens. It includes parts of
speech in BASETOKEN:partOfSpeech, which are grammat-
ical categories, e.g., noun (NN) or preposition (IN) that
use Penn Treebank tagsa with a few additions. BASETOKEN:
normalizedForm stores a final normalized form, including
processes such as lemmatization and abbreviation
expansion.
Because syntactic processing has been well studied in
NLP, the common type system adopts established stan-
dards for the majority of its grammatical types. Phrase-
level syntactic categories found by the process of shallow
parsing (chunking) are stored in SYNTAX.CHUNK types.
The CHUNK:chunkType feature draws from phrasal Penn
Treebank II categories. Many of these phrasal tags
(SYNTAX.ADJP, SYNTAX.NP, SYNTAX.VP, etc.) are included
as child types to SYNTAX.CHUNK for ease of indexing.
More robust syntactic structure is embodied in
SYNTAX.CONLLDEPENDENCYNODE, a dependency parse
node spanning a single BaseToken. This follows the
CONLL-X Shared Task [10] format with 10 features. De-
pendency parses are produced sentence-by-sentence.
Each node of a parsed sentence refers to its syntactic
head, i.e., another ConllDependencyNode pointed to
from CONLLDEPENDENCYNODE:head.Like shallow parses, deep (constituent) parses can be
represented consistently with Penn Treebank II stan-
dards [11]. Here, we use SYNTAX.TREEBANKNODE and two
convenience subtypes, SYNTAX.TERMINALTREEBANKNODE
and SYNTAX.TOPTREEBANKNODE. The syntactic constitu-
ent can be found in TREEBANKNODE:nodeType or
TREEBANKNODE:nodeValue.
Stanford dependencies [12] are formally triples of two
tokens plus the relationship between them. Thus, they
are represented as SYNTAX.STANFORDDEPENDENCY, bin-
ary relation types that relate heads to dependents,
where the inherited STANFORDDEPENDENCY:arg1 is the
head, STANFORDDEPENDENCY:arg2 is the dependent, and
STANFORDDEPENDENCY:category stores the type of rela-
tion (e.g., nsubj).
Textual semantic types
The main intent of textual semantic types is for spans of
text to house a shallow sense meaning or function.
These types are shown in Figure 3, progressing from
simpler meaning on the left to more complex meaning
on the right.
Context-sensitive annotations
On the left of Figure 3, several simple types indicate
spans of text that are of use in clinical context. The
Figure 3 The textsem namespace: spanned types for shallow semantics.
Wu et al. Journal of Biomedical Semantics 2013, 4:1 Page 6 of 12
http://www.jbiomedsem.com/content/4/1/1TEXTSEM.CONTEXTANNOTATION type is a lightweight,
spanned type that tags the context surrounding an entity
or event. It may be used for quick iteration and search.
The feature CONTEXTANNOTATION:Scope has example
values like left, right, or both, indicating on which
side of a CONTEXTANNOTATION:FocusText the entity or
event might be found.
A set of subtypes of Annotation (DATE, FRACTION,
MEASUREMENT, PERSONTITLE, RANGE, ROMANNUMERAL,
TIME) is included for quick indexing. These are fre-
quently found in clinical texts that report laboratory and
procedure values, proper names with titles, and temporal
information.
Mentions in text
The middle section of Figure 3 has types that are similar
to Named Entities and Events as defined by the Auto-
matic Content Extraction and Message Understanding
Conference (MUC-7) tasks, and emerging ISO stan-
dards. Notably, TEXTSEM.IDENTIFIEDANNOTATION is a span
of text that must be discovered, generalizing traditional
Named Entities. Children of IdentifiedAnnotation are
intended to bridge the gap between a text-centric/shal-
low-semantic view of the data, versus a concept-centric/
deep-semantic view of the data. For example, Patient
has pain in the abdomen. . . pain increases with pres-
sure has two (co-referring) text spans for pain, but
would have one unified referential semantic representa-
tion. Thus, subtypes of IdentifiedAnnotation each have
an attribute that refers to the deeper semantic represen-
tation, to be described in the Referential semantics
section.
For a shallow semantic representation, IDENTIFIEDAN-
NOTATION:ontologyConceptArr allows an array of hy-
potheses about how the text should be mapped to anontology. The array allows (but does not require) users
to utilize techniques that separate word sense disam-
biguation (WSD) from the initial recognition. Other fea-
tures give some measure of how the annotation is
presented in context  IDENTIFIEDANNOTATION:polarity
(stated with negation), IDENTIFIEDANNOTATION:condi-
tional (e.g., . . . should return if any rash occurs),
IDENTIFIEDANNOTATION:uncertainty (stated with doubt),
IDENTIFIEDANNOTATION:subject (stated in reference to an
entity, e.g., the patient), and IDENTIFIEDANNOTATION:gen-
eric (stated without a real-world instance, e.g., lupus
clinic).
Other administrative features are included as well. The
semantic type (e.g., drugs, disorders, etc.) may be stored
in IDENTIFIEDANNOTATION:typeID; the segmentID and
sentenceID features provide an indexing to the section
and sentence within which the annotation is found; the
means of discovering the IdentifiedAnnotation is stored
in IDENTIFIEDANNOTATION:discoveryTechnique; and for
automatic extraction methods, a confidence score may
be stored in IDENTIFIEDANNOTATION:confidence.
Two subtypes of primary importance are TEXTSEM.
ENTITYMENTION and TEXTSEM.EVENTMENTION. The
former is an IdentifiedAnnotation that refers to a real-
world entity (embodied in ENTITYMENTION:entity). Simi-
larly, EventMentions are spans that refer to real-world
events as captured by EVENTMENTION:event. We have
built in a distinction that Events have a temporal nature
(see REFSEM.EVENTPROPERTIES) whereas Entities do not. In
clinical text, the large majority of relevant mentions are
temporally active.
Other IdentifiedAnnotations may not refer to entities or
events, but may instead represent attributes of those ele-
ments. For example, if a medication is prescribed for one
month, this may be discovered in text as 1 month, a
Wu et al. Journal of Biomedical Semantics 2013, 4:1 Page 7 of 12
http://www.jbiomedsem.com/content/4/1/1month, 1 mo, etc. We store this text in TEXTSEM.MODI-
FIER and point it to a normalized version in REFSEM.ATTRIB-
UTE. Time annotations have their own subtype as well,
TEXTSEM.TIMEMENTION, since the time stamp of an event is
an important attribute that may or may not be set from text.
Semantic role labeling
Semantic role labeling [13] is a standard NLP task [14] that
gives a shallow representation of the semantics of a sen-
tence. We follow the conventions of PropBank [15] in defin-
ing the semantic roles. TEXTSEM.SEMANTICARGUMENT is
used for the arguments in predicate-argument structures
(SemanticRoleRelations). The SEMANTICARGUMENT:label fea-
tures should contain the type of semantic role (e.g., ARG0,
ARGM) that this argument has (with respect to the predi-
cate). The predicate and other information about the seman-
tic role are available through the SemanticRoleRelation type.Figure 4 The refsem namespace, with deep semantic types and a moWe follow PropBank standards for TEXTSEM.PREDICATE
with a few clinical additions. Predicates are typically
verbs and may participate in semantic role relations
(accessible via PREDICATE:relations). The corresponding
TEXTSEM.SEMANTICROLERELATION type embodies the se-
mantic role labeling output as a binary Relation between
SEMANTICROLERELATION:predicate and SEMANTICROLERE-
LATION:argument.Referential semantic types
The types in Figure 4 are a deep semantic representation
that are meant to refer to something in the real world.
Unlike the text semantics grouping, referential semantic
types do not inherit from Annotation, and they are thus
document-level objects (assuming that one document is
processed per CAS).del of core CEMs.
Wu et al. Journal of Biomedical Semantics 2013, 4:1 Page 8 of 12
http://www.jbiomedsem.com/content/4/1/1General referential semantics
We begin referential semantics with the REFSEM.ONTOLO-
GYCONCEPT and REFSEM.UMLSCONCEPT types. Ontologies
(e.g., the Systematized Nomenclature of Medicine 
Clinical Terms, or SNOMED-CT) provide precise,
expert-curated semantic specifications for concepts.
IdentifiedAnnotations and Elements may point to
these normalized concept representations to indicate
normalization to clinical concepts. Concepts in the
Unified Medical Language System (UMLS) Metathe-
saurus have a concept unique identifier (CUI) and a
type unique identifier (TUI, i.e., semantic type)
which are curated, normalized codes. For example,
pain would have a UMLSCONCEPT:cui = C0030193
and UMLSCONCEPT:tui = T184. Instead of recreating
these ontological structures within the common type
system, concepts are created by reference to any
existing resource, as needed by an application.
Ontology concepts may exist, irrespective of whether
they are instantiated in a particular clinical document.
To capture and summarize what is actually indicated by
a clinical text, we introduce REFSEM.ELEMENT, our foun-
dational structure for deep semantics in the context of
clinical care. An Element contains some of the same
semantic information as IdentifiedAnnotation types,
namely ELEMENT:polarity, ELEMENT:conditional, ELEMENT:
uncertainty, ELEMENT:generic, and ELEMENT:subject.
However, it assumes a single, disambiguated word sense
to be set as an ELEMENT:ontologyConcept. Because mul-
tiple IdentifiedAnnotations in the text can refer to the
same thing in the real world, there is an ELEMENT:men-
tion array that allows quick access to mapped text. This
illustrates the difference between the textsem and refsem
namespaces; for example, a REFSEM.EVENT would aggre-
gate all the features of co-referring TEXTSEM.EVENTMEN-
TION objects. Note that some of the attributes may
remain unused for subtypes of Element, such as the
ELEMENT:polarity feature for Time and Date.
Similar to IdentifiedAnnotations, Elements have the
non-temporal subtype REFSEM.ENTITY and the temporal
subtype REFSEM.EVENT. In the constrained clinical con-
text, most concepts are discussed as instances with some
temporal component, e.g., Medications, Labs, and Dis-
eases, while AnatomicalSites are Entities.
Other subtypes include REFSEM.TIME, which refers to a
real-world time instance and stores a timestamp in
TIME:normalizedForm. Similarly, REFSEM.DATE repre-
sents dates and puts them in a structured normalized
form with DATE:day, DATE:month, and DATE:year.
Core clinical elements
Although the Element type is general enough to handle
real world entities of many kinds, we take special effort
to develop structure for things in the clinical domain.SHARPn has identified 6 general NLP-relevant CEMs
that are here converted into types: REFSEM.ANATOMICAL-
SITE, REFSEM.DISEASEDISORDER, REFSEM.LAB, REFSEM.MEDI-
CATION, REFSEM.PROCEDURE, and REFSEM.SIGNSYMPTOM.
These 6 groups are intended to house concepts from
UMLS semantic groups, [16] with a few exceptions;
Signs & Symptoms is a subgroup within Diseases & Dis-
orders, Labs is a subgroup within Procedures, and Medi-
cations are the subset of Chemicals & Drugs that are
present in RxNORM.
In the type system, these clinical elements primarily
differ from each other in their attributes; for example, a
strength feature as used in Medications would be ir-
relevant for Procedures.
Core clinical attributes
The REFSEM.ATTRIBUTE type forms the basis for post-
coordination in our clinical semantics. In the current
model, these types simply contain strings or numbers
with units. Instead of strong typing (e.g., lab values must
be physical quantities in some standards), default value
sets are for the most part included in a file of constants
that augment the type system. Again, not all Attributes
apply to all Element types; also, some of the important
features in the core clinical element types are not Attri-
butes (e.g., SIGNSYMPTOM:startTime). This means that
the link between Attributes and their respective allow-
able values is not enforced by the type system itself; ex-
perience has shown us that hard-coded value sets tend
to hamper development of downstream application-
specific tools.
A few Attributes are common to multiple core CEMs.
REFSEM.BODYLATERALITY (medial vs. lateral, distal vs.
proximal, etc.) and REFSEM.BODYSIDE (left, right, bilateral)
have to do with relative physical location. The progress
or decline of a condition is marked through REFSEM.
COURSE (changed, increased, decreased, improved, wor-
sened, resolved, unmarked), and explicit indications of
the seriousness of a condition are accounted as REFSEM.
SEVERITY (severe, moderate, slight, unmarked).
Procedures, Labs, and Medications each have more
specific attributes, and these are further documented in
the type system implementation.
Relation types
Relation types in the common type system connect data
structures at both the level of text semantics (connecting
to the textsem namespace) and referential semantics
(the refsem namespace). Shown in Figure 5, both of
these are based on the RELATION.RELATION type. Relations
themselves are not tied to specific spans of text. The Re-
lation type ensures that subtypes all have a category at-
tribute (the label or semantic type of the relation), and
the ability to represent negative (polarity) or uncertain
Figure 5 The relation namespace, with both text relations (spanned) and referential semantic (unspanned) relations.
Table 1 Distribution of types in the common type system
Type subdivisions # of types # of features
Structured 4 24
Syntax 26 33
RefSem 31 96
TextSem 17 33
TextSpan 5 5
Util 3 3
Relation 14 13
Total 100 207
Wu et al. Journal of Biomedical Semantics 2013, 4:1 Page 9 of 12
http://www.jbiomedsem.com/content/4/1/1(uncertainty) relationships. For example, cancer has
not spread to the lymph nodes would yield a negative
locationOf relationship.
Text relation subtypes (RELATION.BINARYTEXTRELATION
and RELATION.COLLECTIONRELATION) use the RELATION.
RELATIONARGUMENT type, which wraps an Annotation
and therefore implies a text span. Subtypes of Binary-
TextRelation, such as RELATION.UMLSRELATION, tie to-
gether two text spans and are further distinguished (e.g.,
when a medication treats a disease) via the inherited
RELATION:category.
Coreference (encapsulated in RELATION.COREFERENCERE-
LATION) is another text relation, and is particularly im-
portant because it bridges between the text semantics
and referential semantics worlds by indicating that two
text spans (mentions) actually refer to the same under-
lying real-world instance.
RELATION.ELEMENTRELATION and RELATION.ATTRIBUTER-
ELATION extend the Relation type rather than the Binary-
TextRelation type; thus, they operate without a spanned
location. The types of relationships they are designed to
express are those between real-world objects. For ex-
ample, a coreference relation would be inappropriate for
this type of relation, because it links multiple textual
mentions as referring to the same entity. This type of re-
lation might instead be a higher-level relationship be-
tween two such coreference-resolved entities.
ElementRelations are used for things like RELATION.
TEMPORALRELATION that link two Time, Date, or Event
annotations. Other subtypes of ElementRelation include
specific UMLS relations between referential semantic
objects (rather than text spans): RELATION.DEGREEOF,RELATION.AFFECTS, RELATION.LOCATIONOF, RELATION.RESUL-
TOF, and RELATION.MANIFESTATIONOF.
Statistics
As shown in Table 1, the defined common type system
contains a total of 100 types and 207 attributes. This is
expanded from 60 types in cTAKES v1.1 and 97 types in
our preliminary common type system. 38 of the types
are modified, 22 are deleted (from Use Cases), and 58
are newly defined as detailed in previous sections. The
average number of attributes per type has also increased
from 1.63 in cTAKES v1.1, to 2.07 attributes per type.
The detailed semantic CEMs are a significant factor
in this.
Discussion
Semantic pipeline example
The foregoing type system allows for NLP techniques to
reach a depth of clinical semantic normalization that
Wu et al. Journal of Biomedical Semantics 2013, 4:1 Page 10 of 12
http://www.jbiomedsem.com/content/4/1/1was not previously possible, and which we believe will
be the primary gathering point for interoperable systems.
We illustrate this semantic depth here by an example,
which considers the clinical text: Chief complaint: se-
vere cough and fever. Cough started 2 days ago, no
expectoration.
Figure 6 shows semantic annotation that has typically
been discovered in clinical NLP algorithms. In the figure,
we have grouped instantiated types by their namespaces
(text semantic types  green, relation types  red). Itali-
cized features are inherited features, but many are omitted.
JOURNAL OF
BIOMEDICAL SEMANTICS
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27
http://www.jbiomedsem.com/content/4/1/27RESEARCH Open AccessFlexiTerm: a flexible term recognition method
Irena Spasi?1*, Mark Greenwood1, Alun Preece1, Nick Francis2 and Glyn Elwyn2,3Abstract
Background: The increasing amount of textual information in biomedicine requires effective term recognition
methods to identify textual representations of domain-specific concepts as the first step toward automating its
semantic interpretation. The dictionary look-up approaches may not always be suitable for dynamic domains such
as biomedicine or the newly emerging types of media such as patient blogs, the main obstacles being the use of
non-standardised terminology and high degree of term variation.
Results: In this paper, we describe FlexiTerm, a method for automatic term recognition from a domain-specific
corpus, and evaluate its performance against five manually annotated corpora. FlexiTerm performs term recognition in
two steps: linguistic filtering is used to select term candidates followed by calculation of termhood, a frequency-based
measure used as evidence to qualify a candidate as a term. In order to improve the quality of termhood calculation,
which may be affected by the term variation phenomena, FlexiTerm uses a range of methods to neutralise the main
sources of variation in biomedical terms. It manages syntactic variation by processing candidates using a bag-of-words
approach. Orthographic and morphological variations are dealt with using stemming in combination with lexical and
phonetic similarity measures. The method was evaluated on five biomedical corpora. The highest values for precision
(94.56%), recall (71.31%) and F-measure (81.31%) were achieved on a corpus of clinical notes.
Conclusions: FlexiTerm is an open-source software tool for automatic term recognition. It incorporates a simple
term variant normalisation method. The method proved to be more robust than the baseline against less formally
structured texts, such as those found in patient blogs or medical notes. The software can be downloaded freely
at http://www.cs.cf.ac.uk/flexiterm.Background
Terms are means of conveying scientific and technical
information [1]. More precisely, terms are linguistic repre-
sentations of domain-specific concepts [2]. For practical
purposes, they are often defined as phrases (typically
nominal [3,4]) that frequently occur in texts restricted to a
specific domain and have special meaning in a given
domain. Terms are distinguished from other salient
phrases by the measures of their unithood and termhood
[4]. Unithood is defined as the degree of collocational
stability (each term has a stable inner structure), while
termhood refers to the degree of correspondence to
domain-specific concepts (each term corresponds to at
least one domain-specific concept). Termhood implies
that terms carry heavier information load compared to
other phrases used in a sublanguage, and as such they* Correspondence: i.spasic@cs.cardiff.ac.uk
1School of Computer Science & Informatics, Cardiff University, Queen's
Buildings, 5 The Parade, Cardiff, UK
Full list of author information is available at the end of the article
© 2013 Spasi? et al.; licensee BioMed Central L
Commons Attribution License (http://creativec
reproduction in any medium, provided the orcan be used to: provide support for natural language
understanding, correctly index domain-specific documents,
identify text phrases to be useful for automatic summarisa-
tion of domain-specific documents, efficiently skim through
documents obtained through information retrieval, identify
slot fillers for the information extraction tasks, etc. It
is, thus, essential to build and maintain terminologies
in order to enhance the performance of many natural
language processing (NLP) applications.Automatic term recognition
Bearing in mind the potentially unlimited number of
different domains and the dynamic nature of some domains
(many of which expand rapidly together with the cor-
responding terminologies [5,6]), the need for efficient
term recognition becomes apparent. Manual term recogni-
tion approaches are time-consuming, labour-intensive and
prone to error due to subjective judgement. Therefore,
automatic term recognition (ATR) methods are needed
to efficiently annotate electronic documents with a settd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 2 of 15
http://www.jbiomedsem.com/content/4/1/27of terms they mention [7]. Note that here ATR refers to
automatic extraction of terms from a domain-specific
corpus [2] rather than matching a corpus against a diction-
ary of terms (e.g. [8]). Dictionary-based approaches are
too static for dynamic domains such as biology or the
newly emerging types of media such as blogs, where
lay users may discuss topics from a specialised domain
(e.g. medicine), but may not necessarily use a standardised
terminology. Therefore, many biomedical terms cannot be
identified in text using a dictionary look-up approach [9].
It is also important to differentiate between two related
problems: ATR and keyphrase extraction. Both approaches
aim to extract terms from text. The ultimate goal of ATR
is to extract all terms from a corpus of documents, whereas
keyphrase extraction targets only those terms that can
summarise and characterise a single document. The two
tasks will have similar approaches to candidate selection
(e.g. noun phrases), after which the respective methods
will diverge. Keyphrase extraction typically relies on super-
vised machine learning [10,11], while ATR is more likely
to use unsupervised methods in order to explore the
terminology space.
Manual term recognition is performed by relying on the
conceptual knowledge, where human experts use tacit
knowledge to identify terms by relating them to the corre-
sponding concepts. On the other hand, ATR approaches
resort to other types of knowledge that can provide clues
about the terminological status of a given natural language
utterance [12], e.g. morphological, syntactic, semantic
and/or statistical knowledge about terms and/or their
constituents (nested terms, words, morphemes). In general,
there are two basic approaches to ATR [3]: linguistic (or
symbolic) and statistical.
Linguistic approaches to ATR rely on the recognition
of term formation patterns, but patterns alone are not
sufficient for discriminating between terms and non-terms,
i.e. there is no lexico-syntactic pattern according to which
it could be inferred whether a phrase matching it is a term
or not [2]. However, they provide useful clues that can
be used to identify term candidates if not terms themselves.
Linguistic ATR approaches usually involve pattern
matching algorithms to recognise candidate terms by
checking if their internal syntactic structure conforms to a
predefined set of morpho-syntactic rules [13], e.g. cyclic/JJ
adenosine/NN monophosphate/NN matches the pattern
(JJ | NN)+ NN (JJ and NN are part-of-speech tags used to
denote adjectives and nouns respectively). Others simply
focus on noun phrases of certain length: 2 (word bigrams),
3 (word trigrams) and 4 (word quadgrams) [14]. However,
both approaches depend strongly on the ability to reliably
identify noun phrases, a task that has proven to be
problematic in the biological domain mainly due to the
lack of highly accurate part-of-speech (POS) taggers for
biomedical text [15].Statistical ATR methods rely on the following hypotheses
regarding the usage of terms [4]: specificity (terms are
likely to be confined to a single or few domains), absolute
frequency (terms tend to appear frequently in their do-
main), and relative frequency (terms tend to appear more
frequently in their domain than in general). In most of the
methods, two types of frequencies are used: frequency of
occurrence in isolation and frequency of co-occurrence.
One of the measures that combines this information is
mutual information, which can be used to measure the
unithood of a candidate term, i.e. how strongly its constit-
uents are associated with one another [16]. Similarly, the
Tanimoto's coefficient can be used to locate the words that
appear more frequently in co-occurrence than isolated
[17]. Statistical approaches are prone to extracting not only
terms, but also other types of collocations: functional,
semantic, thematic and other [18]. This problem is typic-
ally remedied by employing linguistic filters in the form of
morpho-syntactic patterns in order to extract candidate
terms from a corpus, which are then ranked using statis-
tical information. A popular example of such an approach
is C-value [19], a method which combines linguistic
knowledge and statistical analysis. First, POS tagging is
performed, since the syntactic information is needed in
order to apply syntactic pattern matching against a corpus.
The role of these patterns is to extract only those words
sequences that conform to syntactic rules that describe a
typical inner structure of terms. In the statistical part of
the C-value method, each term candidate is quantified by
its termhood following the idea of a cost-criteria based
measure originally introduced for automatic collocation
extraction [20]. C-value is calculated as a combination of
the terms numerical characteristics: length as the number
of tokens, absolute frequency and two types of frequencies
relative to the set of candidate terms containing the nested
candidate term (frequency of occurrence nested inside
other candidate terms and the number of different term
candidates containing the nested candidate term). Formally,
if T is a set of all candidate terms, t ? T, | t | is the number
of words in t, f: T ? N is the frequency function, P(T) is
the power set of T, S: T ? P(T) is a function that maps a
candidate term to the set of all other candidate terms
containing it as a substring, then the termhood, denoted as
C-value(t), is calculated as follows:
C?value tð Þ ¼ In tj j?f tð Þ
In tj j?ðf tð Þ? 1S tð Þj j
X
s?S tð Þ
f sð ÞÞ
;if S tð Þ¼?
;if S tð Þ??
8><
>:
ð1Þ
The method favours longer, more frequently and inde-
pendently occurring term candidates. Better results have
been reported when the limited paradigmatic modifiability
was used as a measure of termhood, which is based on the
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 3 of 15
http://www.jbiomedsem.com/content/4/1/27probability with which specific slots in a term candidate
can be filled by other tokens, i.e. the tendency not to let
other tokens occur in particular slots [14].
Term variation
Both methods will perform well to identify terms that are
used consistently in the corpus, i.e. where their occurrences
do not vary in structure and content. However, terms
typically vary in several ways:
 morphological variation, where the transformation
of the content words involves inflection
(e.g. lateral meniscus vs. lateral menisci) or
derivation (e.g. meniscal tear vs. meniscus tear),
 syntactic variation, where the content words are
preserved in their original form (e.g. stone in kidney
vs. kidney stone),
 semantic variation, where the transformation of the
content words involves a semantic relation
(e.g. dietary supplement vs. nutritional supplement).
It is estimated that approximately one third of an English
scientific corpus accounts for term variants, the majority of
which (approximately 59%) are semantic variants, while
morphological and syntactic variants account for around
17% and 24% respectively [1]. The large number of term
variants emphasises the necessity for ATR to address the
problem of term variation. In particular, statistically
based ATR methods should include term normalisation
(the process of associating term variants with one another)
in order to aggregate occurrence frequencies at the
semantic level rather than dispersing them across separate
variants at the linguistic level [21].
Lexical programs distributed with the UMLS know-
ledge sources [22] incorporate an effective method for
neutralising term variation [23]. Orthographic, mor-
phological and syntactic term variants are normalised
simply by tokenising each term, lowercasing each token,
converting each word to its base form (lemmatisation),
ignoring punctuation, ignoring tokens shorter than three
characters, removing stop words (i.e. common English
words such as of, and, with etc.) and sorting the remaining
tokens alphabetically. For example, the genitive (possessive)
forms are neutralised by this approach: Alzheimers disease
is first tokenised to (Alzheimer,  , s, disease), then lowercased
(alzheimer,  , s, disease), after which punctuation and short
tokens are removed, and the remaining tokens finally
sorted to obtain the normalised term representative
(alzheimer, disease). The normalisation of the variant
Alzheimer disease results in the same normalised form,
so the two variants are matched through their normalised
forms. Similarly, the genitive usage of the preposition of
can be neutralised. For example, aneurysm of splenic artery
and splenic artery aneurysm share the same normalisedform. Note that such an approach may lead to overgeneral-
isation, e.g. Venetian blind and blind Venetian vary only
in order, but have unrelated meanings. However, few such
examples have been reported in practice [23]. Derivational
and inflectional variation of individual tokens is addressed
by rules which define mapping between suffixes across dif-
ferent lexical categories. For example, the rule a|NN|al|
JJ maps between nouns ending with a and adjectives
ending with al that match on the remaining parts (e.g.
bacteria and bacterial), while the rule us|NN|i|NN
matches inflected noun forms that end with us and i
(e.g. fungus and fungi).
Methods
Method overview
FlexiTerm is an open-source, stand-alone application
developed to address the task of automatically identifying
terms in textual documents. Similarly to C-value [24], our
approach performs term recognition in two stages. First,
lexicosyntactic information is used to select term
candidates, after which term candidates are scored using a
formula that estimates their collocation stability, but taking
into account possible syntactic, morphological, derivational
and orthographic variation. What differentiates FlexiTerm
from C-value is the flexibility with which term candidates
are compared to one another. Namely, C-value relies
on exact token matching to measure the overlap be-
tween term candidates in order to identify the longest
collocationally stable phrases, also taking into account
the exact order in which these tokens occur. The order
condition has been relaxed in later versions of C-value
in order to address the term variation problem using
transformation rules to explicitly map between different
types of syntactic variants (e.g. stone in kidney is mapped
to kidney stone using the rule NN1 PREP NN2?NN2
NN1) [25]. FlexiTerm uses flexible comparison of term
candidates by treating them as bags of words, thus com-
pletely ignoring the order of tokens, following a more
pragmatic approach to neutralising term variation, which
has been successfully used in practice [23] (see the
Background section for details). Still, the C-value approach
relies on exact token matching, which may be too rigid for
types of documents that are prone to typographical errors
and spelling mistakes, e.g. medical notes [26] and patient
blogs [27]. Therefore, FlexiTerm adds additional flexibility
to term candidate comparison by allowing approximate
token matching based on lexical and phonetic similarity,
which often indicates not only semantically equivalent
words (e.g. hemoglobin vs. haemoglobin), but also seman-
tically related ones (e.g. hypoglycemia vs. hyperglycemia).
Edit distance (ED) has been widely applied in NLP for
approximate string matching, where the distance between
identical strings is equal to zero and it increases as the
strings get more dissimilar with respect to the characters
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 4 of 15
http://www.jbiomedsem.com/content/4/1/27they contain and the order in which they appear. ED is
defined as the minimal number (or cost) of changes needed
to transform one string into the other. These changes may
include the following edit operations: insertion of a single
character, deletion of a single character, replacement
(substitution) of two corresponding characters in the
two strings being compared, and transposition (reversal or
swap) of two adjacent characters in one of the strings [28].
This approach has been successfully utilised in NLP appli-
cations to deal with alternate spellings, misspellings, the
use of white spaces as means of formatting, the use of
upper- and lower-case letters and other orthographic
variations. For example, 80% of the spelling mistakes
can be identified and corrected automatically by consider-
ing a single omission, insertion, substitution or reversal
[28]. ED can be practically computed using a dynamic pro-
gramming approach [29]. FlexiTerm applies ED to improve
token matching, thus allowing different morphological,
derivational and orthographic variants together with statis-
tical information attached to them to be aggregated.
Linguistic pre-processing
Our approach to ATR takes advantage of lexicosyntactic
information to identify term candidates. Therefore, the in-
put documents need to undergo linguistic preprocessing
in order to annotate them with relevant lexicosyntactic
information. This process includes sentence splitting,
tokenisation and POS tagging. Practically, text is first
processed using the Stanford log-linear POS tagger [30,31],
which splits text into sentences and tokens, which are then
annotated with POS information, i.e. lexical categories such
as noun, verb, adjective, etc. The output of linguistic pre-
processing is a document in which sentences and lexical
categories of individual tokens (e.g. nouns, verbs, etc.) are
marked up. We used the Penn Treebank tag set [32]
throughout this article (e.g. NN, JJ, NP, etc.).
Term candidate extraction and normalisation
Once input documents have been pre-processed, term
candidates are extracted by matching patterns that specify
the syntactic structure of targeted noun phrases (NPs).
These patterns are the parameters of the method and may
be modified if needed. In our experiments, we used the
following three patterns:
1. (JJ | NN)+ NN, e.g. chronic obstructive pulmonary
disease
2. (NN | JJ)* NN POS (NN | JJ)* NN, e.g. Hoffa's fat pad
3. (NN | JJ)* NN IN (NN | JJ)* NN, e.g. acute
exacerbation of chronic bronchitis
Further, lexical information is used to improve boundary
detection of term candidates by trimming leading and
trailing stop words, which include common English words(e.g. any), but also frequent modifiers of biomedical terms
(e.g. small in small Baker's cyst).
In order to neutralise morphological and syntactic
variation, all term candidates are normalised. The nor-
malisation process is similar to the one described in
[23] and consists of the following steps: (1) Remove
punctuation (e.g. ' in possessives), numbers and stop words
including prepositions (e.g. of) (2) Remove any lowercase
tokens with ?2 characters. (3) Stem each remaining token.
For example, this process would map term candidates such
as hypoxia at rest and resting hypoxia to the same
normalised form {hypoxia, rest}, thus neutralising both
morphological and syntactic variation resulting in two
linguistic representations of the same medical concept.
The normalised candidate is used to aggregate the relevant
information associated with the original candidates, e.g.
their frequency of occurrence. This means that subsequent
calculation of termhood is performed against normalised
term candidates.
It should be noted that the step 2 removes only lowercase
tokens. This approach effectively removes possessive s
in Baker's cyst, but not D in vitamin D as uppercase tokens
generally convey more important information, which
is therefore preserved in this approach. Also note
that removing tokens longer than 2 characters would
be too aggressive in deleting not only possessives and
some prepositions (e.g. of ), but also essential term constit-
uents as it would be the case with fat pad, in which both
tokens would be lost, thus completely ignoring it as a
potential term.
Token-level similarity
While many types of morphological variation are effectively
neutralised with stemming used as part of the normalisa-
tion process (e.g. transplant and transplantation will be
reduced to the same stem), exact token matching will still
fail to match synonyms that differ due to orthographic
variation (e.g. haemorrhage and hemorrhage are stemmed
to haemorrhag and hemorrhag respectively). On the
other hand, such variations can be easily identified using
approximate string matching. For example, the ED between
the two stems is only 1  a single insertion of the character
a: h[a]emorrhag. In general, token similarity can be used
to boost the termhood of related terms by aggregating
statistical information attached to them. For example, when
terms such as asymptomatic HIV infection and symptom-
atic HIV infection are considered separately, the frequency
of nested term HIV infection, which also occurs independ-
ently, will be much greater than that of either of the
longer terms. This introduces a strong bias towards
shorter terms (often a hypernym of the longer terms),
which may cause longer terms not to be identified as
such, thus overgeneralising the semantic content. How-
ever, the lexical similarity between the constituent tokens
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 5 of 15
http://www.jbiomedsem.com/content/4/1/27asymptomatic and symptomatic (one deletion operation)
combined with the other two identical tokens indicates
high similarity between the candidate terms, which can be
used to aggregate the associated information and reduce
the bias towards shorter terms.
The normalisation process continues by expanding previ-
ously normalised term candidates with similar tokens found
in the corpus. In the previous example, the two normalised
candidates {asymptomat, hiv, infect} and {symptomat, hiv,
infect} would both be expanded to the same normalised
form {asymptomat, symptomat, hiv, infect}. In our imple-
mentation, similar tokens are identified based on their
phonetic and lexical similarity calculated with Jazzy [33]
(a spell checker API). Jazzy is based on ED [28] described
earlier in more detail, but it also includes two more edit
operations to swap adjacent characters and to change the
case of a letter. Apart from string similarity, Jazzy supports
phonetic matching with the Metaphone algorithm [34],
which aims to match words that sound similar without
necessarily being lexically similar. This capability is
important in dealing with new phenomena such as SMS
language, in which the original words are often replaced
by phonetically similar ones to achieve brevity (e.g. l8 and
late). This phenomenon is becoming increasingly present
in online media (e.g. patient blogs) and needs to be taken
into account in modern NLP applications.
Termhood calculation
The termhood calculation is based on the C-value formula
given in (1) [19]. A major difference in relation to the ori-
ginal C-value method is the way in which term candidates
are normalised. In the C-value approach the notion of
nestedness, as part of determining the set S(t), is based on
substrings nested in a term candidate t treated as a string.
In our approach, a term candidate is treated as a bag of
words, which allows nestedness to be determined using
subsets instead of substrings. This effectively bypasses the
problem of syntactic variation, where individual tokens do
not need to appear in the same order (e.g. kidney stone vs.
stone in kidney). Other causes of term variability (mainly
morphological and orthographic variation) are addressed
by automatically adding similar tokens to normalised term
candidates, which means that nestedness can be detected
between lexically similar phrases using the subset oper-
ation. For example, exact matching would fail to detect
posterolateral corner as nested in postero-lateral corner
sprain because of hyphenation (a special case of ortho-
graphic variation). In our approach, these two term candi-
dates would be represented as {postero-later, posterolater,
corner} and {postero-later, posterolater, corner, sprain} re-
spectively, where similar stems postero-later and posterolater
have been automatically detected in the corpus and used
to expand normalised term candidates. In this case,
nestedness is detected by simply checking the followingcondition: {postero-later, posterolater, corner} ? {postero-
later, posterolater, corner, sprain}.
The FlexiTerm method is summarised with the following
pseudocode:
1. Pre-process text to annotate it with lexico-syntactic
information.
2. Select term candidates using pattern matching on
POS tagged text.
3. Normalise term candidates by performing the
following steps.
a. Remove punctuation, numbers and stop words.
b. Remove any lowercase tokens with ?2 characters.
c. Stem each remaining token.
4. Extract distinct token stems from normalised term
candidates.
5. Compare token stems using lexical and phonetic
similarity calculated with Jazzy API.
6. Expand normalised term candidates by adding
similar token stems determined in step 5.
7. For each normalised term candidate t:
a. Determine set S(t) of all normalised term
candidates that contain t as a subset.
b. Calculate C-value(t) according to formula (1).
8. Rank normalised term candidates using their
C-value.
Output
Once terms are recognised, FlexiTerm produces output
that can be used by either a human user or other NLP
applications. Three types of output are produced: (1) a
ranked list of terms with their termhood scores presented
as table in the HTML format, (2) a plain list of terms that
can be utilised as a lexicon by other NLP applications, and
(3) a list of regular expressions in Mixup (My Information
eXtraction and Understanding Package), a simple pattern-
matching language [35]. Figure 1 shows a portion of the
HTML output in which term variants with the same
normalised form are grouped together and assigned a
single termhood score. Lowercased term variants are
given as they occurred in the corpus and are ordered
by their frequency of occurrence. In effect, the plain
text output presents the middle column of the HTML
output. The term list can be utilised in a dictionary
matching approach (e.g. [36]) to annotate all term occur-
rences in a corpus. Rather than annotating occurrences in
text, we opted for this approach as it is more flexible and
avoids conflict with other annotations produced by other
applications. Still, for quick overview of terms and the
context in which they appeared, the Mixup output can be
used by MinorThird, a collection of Java classes for anno-
tating text [35], to visualise the results (see Figure 2) and
save the stand-off annotations, which include document
name, start position of a term occurrence and its length.
Figure 1 Sample output of FlexiTerm. A ranked list of terms and their variants based on their termhood scores.
Figure 2 Annotated occurrences of terms recognised by FlexiTerm. The annotations are visualised using MinorThird.
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 6 of 15
http://www.jbiomedsem.com/content/4/1/27
Table 2 Data sets used in evaluation
Data
set
Size (KB) Documents Sentences Tokens Distinct
tokens
Distinct
stems
1 145 100 906 24,096 3,430 2,720
2 150 100 949 26,174 3,837 3,049
3 169 100 1,949 40,461 4,404 3,422
4 300 100 3,022 55,845 5,402 4,504
5 73 100 960 13,093 946 824
Quantitative description of the corpora.
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 7 of 15
http://www.jbiomedsem.com/content/4/1/27Results
Data
FlexiTerm is a domain independent ATR method, that
is  it does not rely on any domain specific knowledge
(e.g. rules or dictionaries) to recognise terms in a domain
specific corpus. A comprehensive study of subdomain
variation in biomedical language has highlighted significant
implications for NLP applications, in particular standard
training and evaluation procedures for biomedical NLP
tools [37]. This study revealed that the commonly used
molecular biology subdomain is not representative of
the overall biomedical domain, meaning that the results
obtained using a corpus from this subdomain (e.g. [38])
cannot be generalised in terms of expecting comparable
performance with other types of biomedical text. In par-
ticular, a comparative evaluation of ATR algorithms indi-
cated that choice, design, quality and size of corpora have
a significant impact on their performance [39]. Therefore,
in order to demonstrate the portability of our method
across sublanguages, i.e. languages confined to specialised
domains [40], we used multiple data sets from different
biomedical subdomains (e.g. molecular biology, medical
diagnostic imaging or respiratory diseases) as well as text
written by different types of authors and/or aimed at
different audience (e.g. scientists, healthcare professionals
or patients). We used five data sets (see Tables 1 and 2 for
basic description).
Data set 1 refers to 100 documents randomly selected
from GENIA, a semantically annotated corpus for NLP
applications, which consists of molecular biology abstracts
retrieved from the PubMed database using human, blood
cell and transcription factor as search terms [38]. Similarly,
data set 2 consists of 100 abstracts retrieved from PubMed,
but on a different topic. Unlike data set 1, which belongs
to biomolecular domain, data set 2 belongs to clinical
domain, more specifically chronic obstructive pulmonary
disease (COPD), and it has been collected using the
following PubMed query: "pulmonary disease, chronic
obstructive" [MeSH Terms]. This distinction is relevant
given the considerable differences between biomolecular
and clinical sublanguages [41].
Furthermore, apart from topical difference, we wanted
to explore differences in the writing style. Therefore, weTable 1 Data sets used in evaluation
Data set Topic Document type Source
1 molecular biology abstract PubMed
2 COPD abstract PubMed
3 COPD blog post Web
4 obesity, diabetes clinical narrative i2b2
5 knee MRI scan clinical narrative NHS
Qualitative description of the corpora.collected text data from the same clinical domain (i.e.
COPD), but written by non-medical experts, i.e. patients
or caregivers. Data set 3 represents a collection of 100 blog
posts, which have been collected from blogs identified with
blog search engines (Google Blog Search and Technorati)
using a set of COPDrelated search terms. Query results
were reviewed manually in order to identify blogs with pa-
tient contributions and exclude blogs written by medical
practitioners or those set up for marketing purposes.
Finally, we wanted to contrast the clinical sublanguage
used in clinical practice against that used in scientific
literature (see data set 2). Lexical analysis of a large corpus
of various types of medical records (discharge summaries,
radiology reports, progress notes, emergency room reports
and letters) revealed that clinical narratives are charac-
terised by a high degree of misspellings, abbreviations and
idioms and as such pose considerable challenges for NLP
applications [26]. A particular challenge for ATR, especially
when dictionary-based, is the fact that over 20% of the
words in the given corpus were unrecognisable i.e. were
not recognizable medical words, common words or names,
and could not be algorithmically or contextually converted
to such words. Almost 78% of unrecognisable words were
judged to be probably correctly spelled medical words. To
test the flexibility of our method in coping with irregular-
ities of clinical sublanguages, we used two additional data
sets, which were anonymized prior to their distribution.
Data set 4 represents a collection of narratives extracted
from hospital discharge summaries of patients with history
of obesity or diabetes, which were distributed for the i2b2
Challenge in NLP for Clinical Data [42]. Hospital discharge
summaries were split into sections by matching the mostSearch terms
human, blood cell, transcription factor
"pulmonary disease, chronic obstructive" [MeSH Terms]
COPD, chronic obstructive {pulmonary | lung | airways | respiratory}
disease, bronchitis, emphysema
N/A
N/A
Table 3 Contingency tables for interannotator agreement
B
Yes No Total
A Yes n11 n12 n1.
No n21 n22 n2.
Total n.1 n.2 N
B
Yes No Total
A Yes p11 p12 p1.
No p21 p22 p2.
Total p.1 p.2 p
General structure of a contingency table, where n and p annotate the total
numbers and proportions respectively.
Table 5 Contingency tables for interannotator agreement
on data set 2
B
Yes No Total
A Yes 7,256 1,100 8,356
No 1,062 16,756 17,818
Total 8,318 17,856 26,174
B
Yes No Total
A Yes 0.277 0.042 0.319
No 0.041 0.640 0.681
Total 0.318 0.682 1
Agreement at the token level.
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 8 of 15
http://www.jbiomedsem.com/content/4/1/27frequent keywords used in section titles [43], after which
the narrative sections referring to history of present illness
and hospital course were extracted automatically. Finally,
data set 5 represents a collection of magnetic resonance
imaging (MRI) reports acquired from a National Health
Service (NHS) hospital. They describe knee images taken
following an acute injury.
Gold standard
Terms, defined here as noun phrases referring to concepts
relevant in a considered domain, were annotated by two
independent annotators (labelled A and B in Tables 3, 4, 5,
6, 7, 8). The annotation exercise was performed using
MinorThird, a collection of Java classes for annotating text
[35]. Each annotated term was automatically tokenised in
order to enable token-level evaluation later on (see the
following subsection for details). Therefore, the annotation
task resulted in each token being annotated as being part
of a term, either single or multi word.
Cohen's Kappa coefficient [44] was used to measure the
inter-annotator agreement. After producing contingency
tables following the structure described in Table 3, theTable 4 Contingency tables for interannotator agreement
on data set 1
B
Yes No Total
A Yes 11,948 346 12,294
No 1,664 10,138 11,802
Total 13,612 10,484 24,096
B
Yes No Total
A Yes 0.496 0.014 0.510
No 0.069 0.421 0.490
Total 0.565 0.435 1
Agreement at the token level.Kappa coefficient was calculated according to the following
formula:
? ¼ Ao?Ae
1?Ae
where Ao = p11 + p22 is observed agreement and Ae =
p1.?p.1 + p2.?p.2 is expected agreement by chance. The
Kappa coefficient of 1 indicates perfect agreement, whereas
0 indicates chance agreement. Therefore, higher values
indicate better agreement. Different scales have been
proposed to interpret the Kappa coefficient [45,46]. In
most interpretations, the values over 0.8 are generally
agreed to indicate almost perfect agreement.
Based on the contingency tables produced for each data
set (see Tables 4, 5, 6, 7, 8), we calculated the Kappa coeffi-
cient values given in Table 9, which ranged from 0.809 to
0.918, thus indicating very high agreement. Gold standard
for each data set was then created as the intersection of
positive annotations. In other words, gold standard repre-
sents a set of all tokens that were annotated as being part
of a domain-specific term by both annotators.Table 6 Contingency tables for interannotator agreement
on data set 3
B
Yes No Total
A Yes 2,325 204 2,529
No 436 37,496 37,932
Total 2,761 37,700 40,461
B
Yes No Total
A Yes 0.057 0.005 0.062
No 0.011 0.927 0.938
Total 0.068 0.932 1
Agreement at the token level.
Table 7 Contingency tables for interannotator agreement
on data set 4
B
Yes No Total
A Yes 14,396 1,454 15,850
No 2,269 37,726 39,995
Total 16,665 39,180 55,845
B
Yes No Total
A Yes 0.258 0.026 0.284
No 0.040 0.676 0.716
Total 0.298 0.702 1
Agreement at the token level.
Table 9 Interannotator agreement
Data set Observed
agreement (Ao)
Expected
agreement (Ae)
Kappa
coefficient (?)
1 0.917 0.501 0.834
2 0.917 0.566 0.809
3 0.984 0.878 0.869
4 0.934 0.587 0.840
5 0.960 0.511 0.918
The values of three agreement measures.
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 9 of 15
http://www.jbiomedsem.com/content/4/1/27The extent of terminological content across the five data
sets illustrates great variation in biomedical language and
justifies the need for multiple data sets in order to general-
ise the results [37]. To illustrate this point we converted the
information from Tables 4, 5, 6, 7, 8 to a histogram shown
in Figure 3. Terms account for a massive 50% in PubMed
abstracts in molecular biology (data set 1), whereas the
same type of documents in medicine (data set 2) includes
28% of terminological content. Not surprisingly, terms
account for only 6% in medical information reported by
laymen (data set 3). Finally, the terminological content
of medical notes also varies significantly with 26% in
hospital discharge summaries (data set 4) compared to
41% in radiology reports (data set 5). These variations
should be kept in mind later on when the evaluation results
for the top k automatically recognised terms are reported
(k = 10, 20, , 500).
Evaluation measures
ATR can be viewed as an information extraction (IE)
task, where term occurrences constitute information
to be extracted from text, and thus can be evaluatedTable 8 Contingency tables for interannotator agreement
on data set 5
B
Yes No Total
A Yes 5,312 278 5,590
No 252 7,251 7,503
Total 5,564 7,529 13,093
B
Yes No Total
A Yes 0.406 0.021 0.427
No 0.019 0.554 0.573
Total 0.425 0.575 1
Agreement at the token level.using the contingency table model [47]. Information
extracted by the system is classified either as a true
positive if it is indeed a term or as a false positive if it is
not. Conversely, each term occurrence is classified as a
false negative if it is not extracted by the system. Given
the total numbers of true positives (TP), false positives
(FP) and false negatives (FN), precision (P) and recall
(R) are calculated as the following ratios:
P ¼ TP
TP þ FP R ¼
TP
TP þ FN
In other words, precision represents the proportion
of correctly extracted term occurrences, while recall
represents the proportion of term occurrences that are
extracted by the system. Given the precision and recall
values, F-measure is calculated as their harmonic mean:
F ¼ 2?P?R
P þ R
An important question that remains to be answered is
what counts as a correctly recognised term. It is natural to
assume that it would match an annotated term occurrence
exactly. Such an approach is suitable for common IE
task such as named entity recognition (e.g. protein nameFigure 3 The size and distribution of data sets. Comparison of
terminological and non terminological content.
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 10 of 15
http://www.jbiomedsem.com/content/4/1/27recognition), where it is easier to define the exact
boundaries of the names occurring in text. However, it
is less suitable for ATR, since terms are often formed
by combining other terms. Consider for example a term
such as protein kinase C activation pathway, where protein,
protein kinase, protein kinase C, activation, pathway, pro-
tein activation pathway and protein kinase C activation
pathway are all terms defined in the UMLS [22]. This
fact makes the annotation task more complex and conse-
quently more subjective. Even if we simplified the task byFigure 4 Evaluation results. Comparison to the baseline method with res
represents the number of proposed terms k (k = 10, 20, , 500).focusing only on the most specific concepts, i.e. the ones
described by the longest term encompassing all other
nested terms, it would be difficult to justify the recogni-
tion of subsumed terms as term recognition errors.
For these reasons, it may be more appropriate to apply
token-level evaluation, which effectively evaluates the
degree of overlap between automatically extracted terms
and those manually annotated in the gold standard. Similar
approach has been used for IE evaluation in i2b2 NLP
challenges [48], as it may provide more detailed insightpect to the precision, recall and F-measure. The horizontal axis
Table 10 A comparison to the baseline on data set 1
Rank FlexiTerm TerMine
1 transcription factor t cell
transcription factors
transcriptional factors
2 nf-kappa b transcription factor
3 gene expression nf-kappa b
expression of genes
4 transcriptional activity gene expression
activator of transcription
transcriptional activation
activating transcription
activators of transcription
transcription activation
transcriptional activator
5 nf-kappab activation cell line
nf-kappab activity
6 human t cells t lymphocyte
human cells
7 cell lines human monocyte
cell line
8 human monocytes dna binding
9 activation of nf-kappa b tyrosine phosphorylation
nf-kappa b activation
nf-kappa b activity
10 protein kinase b cell
Top 10 ranked terms by the two methods.
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 11 of 15
http://www.jbiomedsem.com/content/4/1/27into the IE performance. We adapted this approach for
ATR evaluation to calculate token-level precision and
recall. The same contingency table model is applied to
individual tokens that are part of term occurrences ei-
ther automatically extracted by the system or manually
annotated in the gold standard. Each token extracted as
part of a presumed term is classified as a true positive if it
is annotated in the gold standard; otherwise it is classified
as a false positive. Similarly, each token annotated in the
gold standard is classified as a false negative if it is not
extracted by the system as part of an automatically
recognised term. Precision, recall and F-measure are then
calculated as before.
Evaluation results and discussion
The evaluation was performed using the gold standard
and the evaluation measures described previously. The
evaluation results provided for our method were compared
to those achieved by a baseline method. We used TerMine
[49], a freely available service from the academic domain
based on C-value [24], as the baseline method. The values
of all evaluation measures achieved on top k (k = 10, 20, ,
500) proposed terms are plotted for both methods in
Figure 4. Tables 10, 11, 12, 13, 14 illustrate the ATR
results by providing top 10 terms as ranked by the two
methods. Here we provide a more detailed analysis of
the results achieved.
Our method underperformed on all three evaluation
measures only on data set 1, a subset of the GENIA
corpus [38]. The precision of our method was worse on
the literature data in both domains, i.e. biology (data set 1)
and medicine (data set 2). We hypothesise that the better
performance of the baseline in terms of precision may
stem from the highly regular nature of scientific language
in terms of grammatical correctness, e.g. fewer syntactic
and typographic errors compared to patient blogs (data
set 3) and medical notes (data sets 4 and 5), where the
flexibility of our approach in neutralising such errors and
other sources of term variation may not be necessarily
beneficial. The precision achieved on the remaining data
sets does not contradict this hypothesis.
An alternative explanation for better precision of the
baseline method is potentially better term candidate
extraction prior to termhood calculation since TerMine
uses GENIA tagger, which is specifically tuned for bio-
medical text such as PubMed abstracts [50]. On the other
hand, we used Stanford log-linear POS tagger [30,31]
using a left-three-words tagging model of general English.
This may pose limitation on the performance in the bio-
medical domain, but also makes the FlexiTerm method
more readily portable between domains.
The third reason contributing to poorer precision is
the way in which prepositions were annotated in the
gold standard and the fact that the baseline method doesnot include prepositional phrases as part of term candi-
dates. Our method does recognise prepositional phrases
as term components, which in effect will tend to favour
longer phrases such as exacerbation of chronic obstructive
pulmonary disease recognised by our method, but not the
baseline (see Table 11). Due to the problems with complex-
ity and subjectivity associated with the annotation of com-
pound terms (i.e. the ones which contain nested terms) as
explained in the previous subsection, prepositions are likely
not to be consistently annotated. In the given example this
means that if one annotator failed to annotated the whole
phrase and instead annotated exacerbation and chronic
obstructive pulmonary disease as separate terms, the prep-
osition of would be counted as a false positive in token-
level evaluation. Therefore, prepositions that are syntactic
constituents of terms partly account for the drop in
precision. However, prepositions do need to be considered
during term recognition and this in fact may boost the
performance in terms of both precision and recall. We
illustrate this point by the following examples. Data sets 2
and 3 are in the same domain (COPD), but written from
different perspectives and by different types of authors. As
Table 11 A comparison to the baseline on data set 2
Rank FlexiTerm TerMine
1 chronic obstructive pulmonary disease chronic obstructive pulmonary disease
2 patients with copd obstructive pulmonary disease
copd patients
3 pulmonary disease pulmonary disease
4 acute exacerbation copd patient
acute exacerbations
5 copd exacerbation acute exacerbation
copd exacerbations
exacerbations of copd
exacerbation of copd
6 patients with chronic obstructive pulmonary disease severe copd
patients with chronic obstructive pulmonary diseases
7 lung function copd exacerbation
8 exacerbations of chronic obstructive pulmonary disease lung function
chronic obstructive pulmonary disease exacerbations
exacerbation of chronic obstructive pulmonary disease
9 quality of life airway inflammation
10 airway inflammation exercise capacity
Top 10 ranked terms by the two methods.
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 12 of 15
http://www.jbiomedsem.com/content/4/1/27they share the same domain, they naturally share some of
the terminology used. Tables 11 and 12 show that the
phrase quality of life is ranked highly by our method in
both data sets. We checked the terminological status of
the hypothesised term by looking it up in the UMLSTable 12 A comparison to the baseline on data set 3
Rank FlexiTerm TerMine
1 pulmonary rehab pulmonary rehab
pulmanory rehab
2 breathe easy breathe easy
3 vitamin d vitamin d
4 lung transplantation lung function
lung transplant
lung transplants
lung transplantations
5 breathe easy groups severe copd
breath easy groups
breathe easy group
6 chest infection blood pressure
chest infections
7 quality of life lung disease
8 blood pressure lung transplant
9 lung function chest infection
10 rehab room rehab room
Top 10 ranked terms by the two methods.where it is indeed defined as "A generic concept reflecting
concern with the modification and enhancement of life
attributes, e.g., physical, political, moral and social envir-
onment; the overall condition of a human life." Nonethe-
less, the inspection of the complete results proved that the
baseline method does not recognise it at all. The results
on data set 4 (see Table 13) provide a similar example,
shortness of breath, listed as a synonym of dyspnea in the
UMLS, which was ranked third by our method, but againTable 13 A comparison to the baseline on data set 4
Rank FlexiTerm TerMine
1 hospital course hospital course
course of hospitalization
2 chest pain present illness
3 shortness of breath chest pain
4 coronary artery coronary artery
coronary arteries
5 present illness blood pressure
6 blood pressure ejection fraction
blood pressures
7 coronary artery disease coronary artery disease
8 congestive heart failure myocardial infarction
9 myocardial infarction congestive heart failure
10 ejection fraction cardiac catheterization
Top 10 ranked terms by the two methods.
Table 14 A comparison to the baseline on data set 5
Rank FlexiTerm TerMine
1 mri knee collateral ligament
2 collateral ligaments medial meniscus
3 medial meniscus lateral meniscus
medial mensicus
4 lateral meniscus hyaline cartilage
5 hyaline cartilage posterior horn
6 posterior horn femoral condyle
7 joint effusion joint effusion
8 mri rt knee mri lt knee
mri knee rt
9 mri lt knee lateral femoral condyle
mri knee lt
10 lateral femoral condyle medial femoral condyle
Top 10 ranked terms by the two methods.
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 13 of 15
http://www.jbiomedsem.com/content/4/1/27not recognised at all by the baseline. Failure to include
prepositions therefore may completely overlook extremely
important concepts in a domain. In less extreme cases, it
may skew the term recognition results with less severe but
still significant effects. For example, the difference in
ranking of copd exacerbation in data set 2 may not seem
significant. It was ranked seventh by the baseline method
and slightly higher at five by our method due to the fact
that the information obtained for two variants copd ex-
acerbation and exacerbation of copd was aggregated. The
difference in ranking of the same term in data set 3, where
it is used less often, becomes more prominent (16 in our
method compared to 47 in the baseline method), thus
signifying the importance of aggregation for sparse data.
The importance of aggregation is nicely illustrated with
the increase of precision in data set 5 (see Table 14), which
exhibits high degree of derivational and orthographic vari-
ation often as a result of typographical errors. For example,
the third ranked term medial meniscus also includes
its misspelled variant medial mensicus, which otherwise
would not be recognised in isolation due to its low fre-
quency. The 11th ranked term includes two orthographic
variants postero-lateral corner and posterolateral corner
in our results, while the baseline method ranks them
separately at 18 and 55 respectively. Another interestingTable 15 Computational performance
Data set Linguistic pre-processing Term recognition
1 14 sec 101 sec
2 13 sec 96 sec
3 10 sec 59 sec
4 26 sec 290 sec
5 12 sec 32 sec
Completion times across five datasets.example is the 14th ranked term, which includes three
variants infrapatellar fat pad, infra-patella fat pad and
infra-patellar fat pad, the first one ranked 20th by the
baseline method and the remaining two ranked as low
as 281. The results on this data set demonstrate how
flexible aggregation of term variants with the same or
related meaning can significantly improve the precision
of ATR (see Figure 4).
In general, with the exception of the literature data
sets, the precision of our method is either comparable
(an improvement rate of 0.71 percentage points on data
set 3) or better (an improvement rate of 2.02 and 3.29
percentage points on data sets 4 and 5 respectively) than
that of baseline. The natural drop in precision as the recall
increases also seems to be less steep on all five data sets.
Interestingly, the precision of both methods is rising on
data set 4 and very soon stabilises to almost constant level.
On another type of clinical text data (data set 5) where the
recall values were nearly identical, the aggregation of term
variants and their frequencies significantly boosts the
precision as the recall increases.
A similar effect can be observed in boosting the recall,
which is either comparable (a drop by 0.96 percentage
points on data set 5) or better than the baseline (an im-
provement rate of 3.77, 3.96 and 2.43 percentage points
on data sets 24 respectively). The boost in recall is
most obvious on terminologically sparse data set 3. When
precision and recall are combined, the F-measure is better
than that of the baseline with the exception of data set 1.
It is significantly better on data sets 3 and 4 (an improve-
ment rate of 2.73 and 2.77 percentage points respectively)
where both precision and recall were improved.
In conclusion, both methods perform comparably well
on literature and clinical notes. However, based on the
results achieved on data set 3, it appears that the flexibility
incorporated into the FlexiTerm method makes it more
robust for less formal types of text data where the termin-
ology is sparse and not necessarily used in the standard
way. The underperformance on data set 1 in comparison to
performance on other data sets does show that the results
on this corpus cannot be generalised for other biomedical
domains and language types as suggested in [37].
Computational efficiency
Computational efficiency of FlexiTerm is a function of
three variables: the size of the dataset, the number of term
candidates and the number of unique stemmed tokens
that are part of term candidates. The size of the dataset
will be reflected in the time required to linguistically
pre-process all documents, including POS tagging and
stemming. Additional time will be spent on term recogni-
tion including the selection of term candidates based on
a set of regular expressions and their normalisation
based on token similarity. Similarity calculation is the
Spasi? et al. Journal of Biomedical Semantics 2013, 4:27 Page 14 of 15
http://www.jbiomedsem.com/content/4/1/27most computationally intensive operation and its complex-
ity is quadratic to the number of unique stemmed tokens
extracted from term candidates. According to Zipf's law,
which states that a few words occur very often while others
occur rarely, the number of unique tokens is not expected
to rise proportionally with the corpus size. Therefore,
the similarity calculation should not affect the scalability
of the overall approach. Table 15 provides execution times
recorded on five datasets used in evaluation.
Conclusions
In this paper, we described a new ATR approach and
demonstrated that its performance is comparable to that
of the baseline method. Substantial improvement over the
baseline has been noticed on sparse and non-standardised
text data due to the flexibility in the way in which
termhood is calculated. While the syntactic structure
of terms is an important factor in distinguishing between
terms and non-terms, the results show that it need not be
part of termhood calculation. Therefore, we suggest that
the analysis of syntactic structure should be confined to
linguistic filters used to select term candidates, after which
they should be treated using a bag-of-word approach.
We also suggest grouping semantically related term
candidates to further improve the termhood calculation
for sparse terms. Such grouping can be achieved using
phonetic and lexical similarity as a proxy for semantic
similarity. Further improvement of semantic grouping can
be achieved by using other methods to measure semantic
relatedness between words. Latent semantic analysis, which
statistically analyses contextual information over a large
corpus in order to link related words [51], is an obvious
choice and incorporating it into the FlexiTerm framework
will be the subject of future work. To further improve the
results of terminologically processing the data retrieved
from the Web, we will conduct experiments with the
Google distance [52], a semantic similarity measure
calculated as a function of hits returned by the Google
search engine for the given words, where words with
similar meaning tend to appear close in this measure.
The improved performance of term recognition on data
obtained from the Web and social media in particular may
facilitate consumer health informatics research [53] by
efficiently extracting consumer health vocabulary [54], thus
effectively bridging the consumer-professional gap in com-
munication. The extracted terminology can support trad-
itional qualitative research techniques such as content
analysis (e.g. [55,56]) by highlighting the most important
concepts mentioned. More importantly, it can support
large-scale processing with text mining. For example, ATR
in combination with sentiment analysis can quickly reveal
major concerns faced by specific patient populations, thus
providing essential information for health policy makers be-
yond that obtained with the traditional survey techniques.Availability and requirements
Project name: FlexiTerm
Project home page: http://www.cs.cf.ac.uk/flexiterm
Operating system(s): Platform independent
Programming language: Java
Other requirements: None
License: FreeBSD
Any restrictions to use by non-academics: None
Competing interests
To the best knowledge of the authors, there are no competing interests.
Authors contributions
IS conceived the overall study, designed and implemented the application
and drafted the manuscript. MG contributed to the implementation,
collected the data and coordinated the evaluation. AP was consulted
throughout the project on all development issues. NF and GE lent their
medical expertise to interpret the results. All authors read and approved the
final manuscript.
Acknowledgements
MG gratefully acknowledges the support of the Presidents Research
Scholarships. We are thankful to Dr Kate Button for the provision of the MRI
data set together with the ethical approval granted from the South East
Wales Research Ethics Committee (ref: 10/WSE03/5) and School of Healthcare
Studies Research Ethics Committee, Cardiff University. We would also like to
thank David Rogers and Kieran Evans for their assistance in testing the
software on data collected from Twitter.
Author details
1School of Computer Science & Informatics, Cardiff University, Queen's
Buildings, 5 The Parade, Cardiff, UK. 2The Cochrane Institute for Primary Care
and Public Health, Cardiff University, Heath Park, Cardiff, UK. 3Dartmouth
Center for Health Care Delivery Science, Dartmouth College, Hanover, NH,
USA.
Received: 26 June 2013 Accepted: 3 October 2013
Published: 10 October 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Park et al. Journal of Biomedical Semantics 2013, 4:13
http://www.jbiomedsem.com/content/4/1/13DATABASE Open AccessThe Vertebrate Trait Ontology: a controlled
vocabulary for the annotation of trait data across
species
Carissa A Park1, Susan M Bello2, Cynthia L Smith2, Zhi-Liang Hu1, Diane H Munzenmaier3,4, Rajni Nigam3,
Jennifer R Smith3, Mary Shimoyama3,5, Janan T Eppig2 and James M Reecy1*Abstract
Background: The use of ontologies to standardize biological data and facilitate comparisons among datasets has
steadily grown as the complexity and amount of available data have increased. Despite the numerous ontologies
available, one area currently lacking a robust ontology is the description of vertebrate traits. A trait is defined as any
measurable or observable characteristic pertaining to an organism or any of its substructures. While there are
several ontologies to describe entities and processes in phenotypes, diseases, and clinical measurements, one has
not been developed for vertebrate traits; the Vertebrate Trait Ontology (VT) was created to fill this void.
Description: Significant inconsistencies in trait nomenclature exist in the literature, and additional difficulties arise
when trait data are compared across species. The VT is a unified trait vocabulary created to aid in the transfer of
data within and between species and to facilitate investigation of the genetic basis of traits. Trait information
provides a valuable link between the measurements that are used to assess the trait, the phenotypes related to the
traits, and the diseases associated with one or more phenotypes. Because multiple clinical and morphological
measurements are often used to assess a single trait, and a single measurement can be used to assess multiple
physiological processes, providing investigators with standardized annotations for trait data will allow them to
investigate connections among these data types.
Conclusions: The annotation of genomic data with ontology terms provides unique opportunities for data mining
and analysis. Links between data in disparate databases can be identified and explored, a strategy that is particularly
useful for cross-species comparisons or in situations involving inconsistent terminology. The VT provides a common
basis for the description of traits in multiple vertebrate species. It is being used in the Rat Genome Database and
Animal QTL Database for annotation of QTL data for rat, cattle, chicken, swine, sheep, and rainbow trout, and in the
Mouse Phenome Database to annotate strain characterization data. In these databases, data are also cross-referenced
to applicable terms from other ontologies, providing additional avenues for data mining and analysis. The ontology is
available at http://bioportal.bioontology.org/ontologies/50138.
Keywords: Quantitative trait loci, Gene association, Trait ontology* Correspondence: jreecy@iastate.edu
1Department of Animal Science, Iowa State University, Ames, IA, USA
Full list of author information is available at the end of the article
© 2013 Park et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Figure 1 Vertebrate Trait Ontology hierarchy showing upper
level terms.
Park et al. Journal of Biomedical Semantics 2013, 4:13 Page 2 of 9
http://www.jbiomedsem.com/content/4/1/13Background
The use of ontologies (formal, standardized vocabularies
identifying the relationships between terms related to a
particular subject matter) to standardize biological data
and facilitate comparisons among datasets and across or-
ganisms has steadily grown as the complexity and
amount of data available for researchers to analyze have
increased. The hierarchical structure of ontologies makes
them both machine readable and meaningful to human
users, which results in more intuitive query and data dis-
play tools for investigators.
One of the largest and most widely used biological on-
tologies is the Gene Ontology (GO), which consists of
three distinct controlled vocabularies used to describe
the molecular functions, biological processes, and cellu-
lar components associated with gene products [1].
Ontologies have also been created to describe pheno-
types [2,3], anatomy [4-7], cell types [8], chemical com-
pounds [9], and proteins [10]. New ontologies continue
to be developed at a rapid pace as evidenced by the
National Center for Biomedical Ontology (NCBO; [11]),
where the number of ontologies has increased from 72
in early 2008 [12] to 339 in April 2013.
Despite the numerous ontologies available, one area
currently lacking a robust ontology is the description of
vertebrate traits. A trait can be defined as any measur-
able or observable characteristic pertaining to an organ-
ism or any of its substructures. A search of ontologies to
address the trait domain shows that while there are sev-
eral ontologies that represent entities and processes in
phenotypes, diseases, and clinical measurements, there
has not been one for vertebrate traits; the Vertebrate
Trait Ontology (VT) was developed to fill this void.
Impetus for this project came from multiple groups in-
cluding the Rat Genome Database (RGD; [13]), Mouse
Genome Informatics (MGI; [14]), and the Animal QTL
Database (QTLdb; [15]), and it began as a way to
standardize descriptions and definitions of quantitative
trait loci (QTL) for cross-species comparisons and other
analyses. In addition, the need to link various levels of
data connected with physiological processes, phenotypes,
and disease mechanisms was identified.
The concepts of phenotype and trait are closely
aligned, to the extent that some might consider them
synonymous. However, while several phenotype ontol-
ogies exist, including the Mammalian Phenotype (MP)
Ontology [2], the Human Phenotype (HP) Ontology [3],
and the Phenotypic Quality Ontology (PATO; [16]),
there are fundamental differences between the content
and/or structure of these ontologies and the VT which
make them less than ideal for expressing trait data. Nei-
ther the MP nor the HP fulfills this need because both
ontologies are designed to express phenotypic variation
from a normal state. For instance, although the HPmode of inheritance branch includes unaltered pheno-
types, the other two branches, onset and clinical course
and phenotypic abnormality, clearly indicate a more or
less anomalous state. Likewise, the MP was specifically
developed as a means to define the abnormal changes
caused by mutations. Traits, on the other hand, do not
indicate an abnormal state or process.
PATO is constructed in such a way that it would be
possible to use it to express the normal state or process,
but it differs from the VT in that it was created to annotate
phenotypes using a combinatorial approach, in which a
phenotypic character is composed of an entity (e.g., limb)
and a quality, or attribute (e.g., length). PATO requires
entities to be drawn from other ontologies, such as
those describing anatomy or cell types [16]. Phenotype
composition can be done either during ontology cre-
ation (pre-composition) or at the time of annotation
(post-composition). One ontology that is pre-composed
using PATO is the Fission Yeast Phenotype Ontology
Table 1 Problems and their fixes using VT
Problem Original trait/subtrait or term name Current VT term Current VT ID
Original QTL trait is not a trait: Hypothalamic-pituitary-adrenal axis/ Blood glucocorticoid amount VT:0003366
corticosterone
White spotting on belly Coat/hair pigmentation trait VT:0010463
Name contains sample information: Hormone level/ Blood aldosterone amount VT:0005346
aldosterone, females
Name contains experimental condition information: Blood pressure/ Arterial blood pressure trait VT:2000000
salt-depleted
Percentage live sperm after thawing Sperm quantity VT:0002673
Name contains measurement information: Blood pressure/ Arterial blood pressure trait VT:2000000
pulse pressure
Average daily gain Postnatal growth trait VT:0001731
Name contains both condition and
measurement information:
Post-weaning average daily gain Postnatal growth trait VT:0001731
Name contains both method and
measurement information:
Blood pressure/ Arterial blood pressure trait VT:2000000
direct systolic
Name contains disease information: Glucose level/ Blood glucose amount VT:0000188
insulin-dependent
Tibial dyschondroplasia Tibia morphology trait VT:0000558
Same trait described in two different ways: Gland mass/pancreas Pancreas mass VT:0010144
Pancreas weight/(none) Pancreas mass VT:0010144
Park et al. Journal of Biomedical Semantics 2013, 4:13 Page 3 of 9
http://www.jbiomedsem.com/content/4/1/13(FYPO; [17]). An example of a group that performs
post-composition using PATO is the Zebrafish Information
Network (ZFIN; [18]). Although the post-compositional ap-
proach facilitates computational analysis, it increases com-
plexity and decreases ease of use for human users [19]. It
also impedes curation, because more time is required for a
curator to consult multiple ontologies to construct a single
trait term. In addition, it increases the potential for ambigu-
ity, since a compound term could be created in many ways
depending on which ontologies the component terms are
selected from (e.g., one may generate the term circulating
sugars amount as an alternative to blood glucose amount).
Disease ontologies such as the Human Disease Ontology
[20], SNOMED Clinical Terms [21], and the International
Classification of Diseases [22] are not appropriate to ex-
press traits because the disease state is, by definition, ab-
normal. In addition, multiple traits may be associated with
a disease and vice versa. While the Clinical Measurement
Ontology (CMO) [23] does represent measurable entities,
it is designed to describe the actual measurements taken
which result in a quantitative or qualitative result and not
the trait that the measurement is used to assess.
Trait information provides a valuable link between the
measurements that are used to assess the trait, the phe-
notypes related to the traits, and the diseases associated
with one or more phenotypes. A trait, such as erythro-
cyte size, is distinct from phenotype (a description of the
manifestation of the trait; e.g., increased erythrocyte size)and measurement (a quantification or assessment of the
trait; e.g., mean corpuscular volume). Significant incon-
sistencies exist in the literature when it comes to trait
nomenclature. Even within species, multiple terms may
be used to refer to the same trait (e.g., subcutaneous fat
depth, subcutaneous adipose thickness, backfat thick-
ness, etc.). Complexity increases when attempts are made
to compare traits across species. Because multiple clinical
and morphological measurements are often used to assess
a single trait, and a single measurement can be used to
assess multiple physiological processes, providing investi-
gators with standardized annotations for trait data will
allow them to investigate connections among these differ-
ent types of data. Therefore, the Vertebrate Trait Ontology
was developed to describe the measurable or observable
characteristics pertaining to the morphology, physiology,
and development of vertebrate organisms. It is available
for public browsing and download via BioPortal (http://
bioportal.bioontology.org/ontologies/50138).
Construction and content
The VT was originally developed as an outgrowth of
naming conventions and trait vocabularies utilized to
characterize QTL. Its intended purpose was to assist in
the discovery of cross-species syntenic regions identified
as being associated with the same or similar traits. Be-
cause experimental techniques can differ widely depend-
ing on organism, and because many QTL were originally
Table 2 Standardization of traits
Trait_name Subtrait_name
Blood pressure None
Blood pressure Arterial
Blood pressure Diastolic
Blood pressure Diastolic, daytime
Blood pressure Direct systolic
Blood pressure Indirect systolic
Blood pressure Mean arterial
Blood pressure Mean arterial pressure
Blood pressure Mean arterial pressure, stress related changes
Blood pressure NaCl-loaded systolic blood pressure
Blood pressure Post nitric oxide system block
Blood pressure Post renin-angiotensin system block
Blood pressure Post sympathetic nervous system block
Blood pressure Pulse pressure
Blood pressure Response to intrathecal cytisine
Blood pressure Salt-depleted
Blood pressure Salt-loaded
Blood pressure Salt-loaded mean arterial
Blood pressure Salt-loaded systolic
Blood pressure Systolic
Blood pressure Systolic, nighttime
Original RGD trait and subtrait assignments for QTL now annotated with the
VT term arterial blood pressure trait.
Park et al. Journal of Biomedical Semantics 2013, 4:13 Page 4 of 9
http://www.jbiomedsem.com/content/4/1/13named and annotated according to terms used by au-
thors, this cross-comparison proved difficult for many
researchers. While individual entities such as MGI,
RGD, QTLdb, and the French National Institute for
Agricultural Research (INRA) each created limited
naming conventions and vocabularies to more or less
standardize QTL data within their own databases, there
was little commonality among the groups. In addition,
naming and trait assignment included disease terms,
abnormal phenotype terms, measurements, and method
terms, causing additional confusion.
The Vertebrate Trait Ontology was designed to create
consistency in annotation across species and to provide
a navigational layer among data types. Capitalizing on
previous development efforts, the Mammalian Phenotype
JOURNAL OF
BIOMEDICAL SEMANTICS
Gündel et al. Journal of Biomedical Semantics 2013, 4:35
http://www.jbiomedsem.com/content/4/1/35RESEARCH Open AccessHuPSON: the human physiology simulation
ontology
Michaela Gündel1,2*, Erfan Younesi1,2, Ashutosh Malhotra1,2, Jiali Wang1, Hui Li1, Bijun Zhang1, Bernard de Bono3,
Heinz-Theodor Mevissen1 and Martin Hofmann-Apitius1,2Abstract
Background: Large biomedical simulation initiatives, such as the Virtual Physiological Human (VPH), are substantially
dependent on controlled vocabularies to facilitate the exchange of information, of data and of models. Hindering
these initiatives is a lack of a comprehensive ontology that covers the essential concepts of the simulation domain.
Results: We propose a first version of a newly constructed ontology, HuPSON, as a basis for shared semantics and
interoperability of simulations, of models, of algorithms and of other resources in this domain. The ontology is based
on the Basic Formal Ontology, and adheres to the MIREOT principles; the constructed ontology has been evaluated via
structural features, competency questions and use case scenarios.
The ontology is freely available at: http://www.scai.fraunhofer.de/en/business-research-areas/bioinformatics/downloads.
html (owl files) and http://bishop.scai.fraunhofer.de/scaiview/ (browser).
Conclusions: HuPSON provides a framework for a) annotating simulation experiments, b) retrieving relevant information
that are required for modelling, c) enabling interoperability of algorithmic approaches used in biomedical simulation,
d) comparing simulation results and e) linking knowledge-based approaches to simulation-based approaches. It is
meant to foster a more rapid uptake of semantic technologies in the modelling and simulation domain, with particular
focus on the VPH domain.
Keywords: Simulation, Algorithm, Interoperability, Ontology, Semantics, Text miningBackground
Biomedical ontologies have proven their value in diverse
applications as metadata annotation and data integration
[1], knowledge representation [2], and knowledge discovery
[3]. Ontologies also play a fundamental role in harmoniz-
ing name spaces, shared semantics and standardization of
data and of model resources [4]. Recently, analysis of
mechanical problems in a human body under disease con-
ditions, using computational algorithms and models, has
gained momentum in biomechanics research [5].
Many well-established ontologies exist in the biomedical
domain that can be used to annotate simulation experi-
ments on the anatomical, molecular, chemical, phenotypic* Correspondence: michaela.guendel@scai-extern.fraunhofer.de
Equal contributors
1Fraunhofer Institute for Algorithms and Scientific Computing (SCAI), Schloss
Birlinghoven, Sankt Augustin, Germany
2Bonn-Aachen International Center for Information Technology (B-IT),
University of Bonn, Bonn, Germany
Full list of author information is available at the end of the article
© 2013 Gündel et al.; licensee BioMed Central
Commons Attribution License (http://creativec
reproduction in any medium, provided the orlevels (see, e.g., the BioPortal repository [6]). However,
despite the fast growth in the number of biomechanical
studies, there exist only a few semantic frameworks expli-
citly developed for simulation experiments and models.
Examples include the Kinetic Simulation Algorithm
Ontology (KiSAO) [7], the Terminology for the Descrip-
tion of Dynamics (TEDDY) [7], the Discrete-Event Model-
ing Ontology (DeMO) [8,9] and the Systems Biology
Ontology (SBO) [7,10]. DeMO formalizes information
only related to discrete systems, KISAO is limited in scope
to kinetic models and algorithms, TEDDY deals with clas-
sification of dynamic features in simulation and SBO rep-
resents model components. There also exists the Living
Human Digital Library (LHDL) domain ontology [11,12]
that serves as a foundation for coherent annotation of
LHDL resources and their retrieval and traceability.
Subsequently, it is very specific to the LHDL project
requirements.
The RICORDO interoperable anatomy and physiology
project [13] provides tools that help physiology andLtd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Gündel et al. Journal of Biomedical Semantics 2013, 4:35 Page 2 of 9
http://www.jbiomedsem.com/content/4/1/35pharmacology researchers and medical students in the
semantic interoperability of clinical data and model re-
sources. RICORDO combines concepts from standard
ontologies to form composites, thus creating more com-
plex concepts such as venous return [13]. The approach
of composite annotations is also proposed by Gennari
et al. [14]. The authors explicitly avoid constructing a bio-
simulation ontology, instead they leverage established on-
tologies to circumvent the combinatorial challenge of
having to include all possible multi-term class names,
such as aortic blood pressure. The SemSim approach
[15] makes use of such composite annotations, annotating
model parameters, variables and other observables against
terms from reference ontologies. The aim of SemSim is to
create semantic interoperability of biosimulation models
by creating machine-readable definitions. While this is a
valid approach to creating interoperability and the integra-
tion of resources, the problem remains that semantic in-
formation is spread among different external sources and
an additional tool (e.g. SemGen [14], the RICORDO tool-
kit [13]) is needed.
None of the above works provides a comprehensive
ontology that covers simulations and algorithmic ap-
proaches. We believe that a stand-alone ontology, ver-
sus semantic tools that leverage existing ontologies in a
distributed way, that covers the biosimulation domain
and algorithmic approaches will be a useful tool and will
serve interested groups involved in cross-disciplinary
simulation initiatives. An example of such an initiative is
the VPH [16]. The VPH foresees that modelling and
simulations will enable a better understanding of the
humans bodys functioning and its pathological pro-
cesses, as well as help develop therapies and tools that
can aid disease diagnosis, treatment and prevention.
Thus, in order to support these types of initiatives, we
developed and evaluated an initial version of the Human
Physiology Simulation Ontology (HuPSON).
Results
Scope and purpose
HuPSON provides a framework for a) annotation of simu-
lation experiments with standard ontology terms, b) text-
mining based information retrieval that is required for
modelling, c) interoperability of algorithmic approaches
used in biomedical simulation, d) comparability of simula-
tion results and interoperability on different structural
scales (from the human anatomy down to cells and mole-
cules) and e) linking knowledge-based approaches (e.g.
ontologies) to simulation-based approaches (e.g. differen-
tial equation-based approaches).
The current primary use of HuPSON is to aid in text-
mining (scope b)). Scopes a) and b) are validated in the
Results section below, whereas for a discussion of scopes
c)-e), the reader is referred to the Discussion section.Ontology contents
The ontology was modelled using a UML-type of diagram
as shown in Figure 1. A computer simulation consists of
simulation steps that use algorithms and scientific tech-
niques and is performed on a model. A model mathemat-
ically describes some modelled thing, which can be an
anatomical part, a process, function, or a quality. A model
has equations and parameters. A list of definitions of these
main ontology classes is given in Table 1.
The ontology (cf. Figure 2) contains 2,920 classes and a
total of 7,262 synonyms. 1,067 (36%) of these classes were
added manually, whereas the other 64% of classes were in-
tegrated from related ontologies (Figure 3). Wherever pos-
sible, leaf equation classes were annotated via an
annotation property with their corresponding MathML
[17] expression. Approximately 55% of the 108 equations
have a MathML expression associated to them. In
addition to textual definitions, axioms have been inserted
wherever they are deemed meaningful (both necessary
and sufficient axioms and class-descriptive axioms). For
instance, the class computational fluid dynamics (CFD)
model is described via has_part_equation some numer-
ical equation and mathematically_describes some hydro-
dynamic quality, allowing the reasoner to infer that it is
both a hydrodynamic model and a numerical model, as
those classes are defined via according necessary and suffi-
cient axioms.
Validation
The HermiT reasoner [19] was used to ensure ontology
consistency. The ontology was evaluated based on struc-
tural featuresa and with regard to its performance on
text-mining tasks. Relatively high values of class number
(2,920), leaves (1,927), maximum width (727) and aver-
age width (270.05), along with a fanout factor of 0.71,
are indicative of the ontology's broad coverage; similarly,
the depth values of 10 (max.) and 5.5 (avg.) are indicators
of a relatively good specificity of types to the domain.
The screenshot provided as Additional file 1 is an ex-
ample of a PubMed abstract annotation using HuPSON
terms, and is an example of how HuPSON can be used
in regard to scope a). Such annotations, applied to real
simulation settings, also pave the grounds for compar-
ability of simulation experiments by leveraging the se-
mantics from the ontology (scope d)).
As an example of HuPSONs applicability to relevant
text-mining tasks (scope b)), 700 PubMed abstracts
about simulations in the VPH context were downloaded
from MEDLINE [20] and used to produce our own gold
standard (i.e. training and test sets) for evaluation. This
gold standard consists of the set of annotations that are
expected when running a text-mining tool that queries
for the HuPSON terms over the abstracts. Calculation of
the system performance resulted in a recall, a precision
Figure 1 Extract of diagram used for modelling HuPSON. UML-like diagram used for the design of HuPSON  relationships between the
upper-level classes model, biomedical computer simulation, equation, parameter, scientific technique, algorithm, and related classes; normal arrows
denote subsumption relations, dotted arrows denote object properties that hold between the two classes.
Gündel et al. Journal of Biomedical Semantics 2013, 4:35 Page 3 of 9
http://www.jbiomedsem.com/content/4/1/35and an F-score of around 0.66 in the test set. Further-
more, participants from different working groups, whom
participated in the VPH Network of Excellence, were
asked to provide queries typical for the VPH domain
(see competency questions/queries in Table 2). To study
these real-use case scenarios, ProMiner [21], using the
HuPSON dictionary (see Methods section) as input, was
applied to the complete MEDLINE abstracts for theTable 1 Main ontology classes
Ontology class Definition
Computer simulation A broad collection of methods used to stu
systems. Simulation studies are performed,
model of the system created for the purpo
Simulation step A specific stage of progression through a
Algorithm An algorithm is a set of instructions, somet
certain task. []
Scientific technique A scientific technique is any systematic me
desired material or product. []
Model A mathematical model is the use of mathe
model usually describes a system by a set o
the variables.  []
Modelled thing Thing that is mathematically described via
Equation A statement asserting the equality of two e
into left and right sides and joined by an e
Parameter Any value passed to a program by the user
particular purpose. []
Definition of main HuPSON classes.identification of specific knowledge. The recognized con-
cepts from the HuPSON dictionary were visualized
using SCAIView semantic search engine [22]. Table 3
shows that both ontology-based queries resulted in more
true positive hits than their PubMed counterparts. These
abstracts are considered to represent an information
gain compared to the PubMed query results. Moreover,
HuPSON was used in SCAIView to retrieve studies thatdy and analyze the behavior and performance of actual or theoretical
not on the real-world system, but on a (usually computer-based)
se of studying certain system dynamics and characteristics. []
sequential process of a simulation.
imes called a procedure or a function, that is used to perform a
thod to obtain information of a scientific nature or to obtain a
matical language to describe the behaviour of a system. A mathematical
f variables and a set of equations that establish relationships between
a model.
xpressions, usually written as a linear array of symbols that are separated
qual sign.
or by another program in order to customize the program for a
Figure 2 HuPSON hierarchy plugged into BFO. HuPSON class hierarchy depicting classes biomedical computer simulation, algorithm, equation,
model and related classes inside the BFO hierarchy, displayed with OBO Graph View [18] inside Protégé; blue arrows diplaying i: subclass relations;
light blue; has_part_simstep; light green: mathematically_describes; green: has_part_equation; yellow: uses; orange: is_mathematically_described_by;
brown: application_generates; light violet: is_performed_on; violet: is_param_used_in (bottom)/solves_equation (top).
Figure 3 Class provenance in HuPSON. This diagram shows the
provenance of classes. 36% of all classes were added manually
inside the HuPSON namespace, the other 64% stem from related
ontologies. other includes further ontologies/taxonomies such as
the NCBI taxonomy, Ontology for Biomedical Investigations (OBI),
Unit Ontology (UO) and Medical Dictionary for Regulatory Affairs
(MEDDRA) (all available from BioPortal).
Gündel et al. Journal of Biomedical Semantics 2013, 4:35 Page 4 of 9
http://www.jbiomedsem.com/content/4/1/35report on heart biomechanics modelling, with a specific
focus on the application of mechanical pump models to
supporting blood circulation in human hearts. Starting
with the query [heart AND pump model AND
blood circulation], the retrieved studies were further
filtered for Homo sapiens, resulting in 9 identified
documents that correctly describe blood pump models
and their application to blood circulation in human
hearts (i.e. PMIDs: 10203406, 18002874, 7872572,
17938774, 17015490,15802261, 2752563, 18401072, and
11940364). The retrieved information can help experts
improve their understanding of the applicability of such
models and the underlying mechanical theory (for exam-
ples, see findings in [23] (PMID: 18002874) and [24]
(PMID: 11940364), Additional file 2). Note that using an
ontology-driven semantic system to search the know-
ledge space of publications, using complex queries, out-
performs traditional search engines such as that offered
by the PubMed system in targeted information retrieval.
Exemplifying this is that PubMed, using the same search
query as described above, finds only one abstract (i.e.
PMID: 10203406).
Lastly, in order to show the applicability of HuPSON
to independent domains, we applied it to Alzheimers
disease by challenging the system to retrieve and seman-
tically filter the published knowledge related to simula-
tion and modelling within this domain. Alzheimers
Table 2 Competency questions
Query for competency question expressed in free text HuPSON-based query Query in PubMed
Search the literature for fluid structure interaction models
of the aneurysm simulating the pressure and its link to rupture
((fluidstructure interaction (FSI) model)
AND pressure AND ruptured AND aneurysm)
(fluidstructure interaction model OR
fluid structure interaction model) and
aneurysm and pressure and ruptured
Find publications on velocity of blood flow and rupture
outcomes of aneurysms
(velocity AND (ruptured OR unruptured)
AND aneurysm AND (blood circulation))
velocity AND (ruptured OR unruptured)
AND aneurysm AND blood circulation
Selected competency questions formulated by VPH experts and transformed into HuPSON-based queries and PubMed queries.
Gündel et al. Journal of Biomedical Semantics 2013, 4:35 Page 5 of 9
http://www.jbiomedsem.com/content/4/1/35disease is a common neurological disorder afflicting the
elderly, whose clinical diagnosis is problematic because
of overlapping early symptoms with other diseases.
However, structural imaging has been recently shown to
be a valuable tool in differential diagnosis of most de-
mentias [25]. To identify studies reporting the applica-
tion of image analysis models to the differential
diagnosis of Alzheimers using MRI, we used the MeSH
terminology in conjunction with HuPSON and per-
formed a query in the SCAIView environment. 18 of the
23 retrieved abstracts were relevant to the query and
correctly identified such studies. From these documents,
we were able to extract what specific model types are
used in the query context (e.g. network diffusion
models and logistic regression models). This kind of
information can help model developers choose an ap-
propriate model for their research.
Discussion
HuPSON provides ontology classes that describe things
that can be modelled. These include a humans anatom-
ical parts, from gross anatomy down to the molecular
level, physiological processes, functions and qualities. It
brings together, into one comprehensive ontology, exter-
nal ontologies and adds new classes that are not avail-
able elsewhere, but are important for simulations.
Classes have been chosen in a methodological way from
relevant literature and complemented by terms consid-
ered important by representatives of the VPH commu-
nity. Such selection helps to ensure that the terms
contained in the ontology reflect the way that they are
commonly expressed and used by the community.
Moreover, it ensures that those composites that are most
commonly mentioned in the literature are contained inTable 3 Evaluation via competency questions
Query expressed in free text
Search the literature for fluid structure interaction models of the aneurysm si
and its link to rupture
Find publications on velocity of blood flow and rupture outcomes of aneury
Competency questions evaluated in SCAIView based on HuPSON, and PubMed que
acorresponding PMIDs: 16712729, 18568827, 16221475, 16121537, 16500664, 16153
bcorresponding PMIDs: 9647316, 19563706, 21096182, 1644550, 19675980, 1932915
18787954, 19553143, 12695182, 21071533, 20508183, 21161794, 17416810, 178852
20300847, 19936925.the ontology. The approach of converting the ontology
classes and their synonyms into a dictionary file make
the ontology ready for use in text mining approaches.
Re-use of external ontology class URIs makes it inter-
operable with external established ontologies. The hier-
archical mathematical model types are associated to the
equation types that are solved inside them, the equa-
tions, in turn, are associated to their MathML descrip-
tions (approach similar to that described by Ivchenko
et al. [26]). The equations are thus computer-readable
and are, furthermore, placed in their correct hierarchical
context. This makes them available to semantically-
aware computer processing. In doing so, we propose a
solution to connect the semantics and knowledge-driven
approaches to the simulation approaches that typically
employ differential equations (scopes c)-e)).
One reason for relatively low values of precision and
recall in its evaluation lies in the simulation domains
broadness and the complexity of the terms used therein;
a term such as mechanical, trileaflet heart valve pros-
thesis, even though specific to the domain, does not ap-
pear in many scientific simulation-related texts and thus,
is not present among the synonyms.
Conclusions
HuPSON is meant to foster a more rapid uptake of se-
mantic technologies in the modelling and simulation do-
main in general, with a particular focus in the VPH
domain. The ontology is suited to link the mathematics
and algorithmics behind biomedical simulations and the
communication dealing with simulation experiments. It
can be used to systematically detect various types of
statements in scientific reports and publications. One fu-
ture application of the ontology could be the systematicHits of SCAIView
query
Hits of PubMed
query
mulating the pressure 8/9TP* a 0/0 TP*
sms 29/59 TP* b 2/3 TP*
ries; *TP meaning informative and relevant to the query.
654, 21722905, 21088917.
2, 16783935, 18350286, 16813443, 17047283, 21233477, 10447563, 10414574,
39, 18977588, 18622621, 10472991, 16321205, 20435277, 19762460,
Gündel et al. Journal of Biomedical Semantics 2013, 4:35 Page 6 of 9
http://www.jbiomedsem.com/content/4/1/35detection of assumptions made in modelling and simula-
tions. This is quite challenging since most assumptions
are implicitly made. The importance of making assump-
tions explicit in biosimulation models was recently
discussed in context to the formulation of a models se-
mantics (the authors call this meaning facets) [27]. In
HuPSON terms, for instance, one might detect the mod-
elling assumption of Newtonian blood viscosity that is
made for a model that mathematically_describes some
blood circulation and has_part some Newtonian fluid
dynamic equation (from the latter the reasoner auto-
matically infers it to be a Newtonian model).
Finally, the perspective of reasoning over algorithmic
approaches, based on HuPSONs hierarchy of equations
that are directly accessible to computer processing via
MathML, is quite fascinating. We invite the modelling
and simulation community to provide use cases to en-
able us to explore this possibility further. For instance,
an interesting feature will be to improve the semantic
enrichment of equations and to connect them with more
detail to variable or constant types or instances.
Note that HuPSON is meant to be a draft ontology
that is proposed to the modelling and simulation com-
munity. Ontologies represent a certain view on a topic
and a certain state of knowledge within a domain. The
authors explicitly express that their view on the simula-
tion domain is not the only one. Moreover, the authors
are aware of the fact that new knowledge, including new
algorithmic approaches, is constantly added to the bio-
medical simulation area. Therefore, we encourage the
community to actively take up and optimize this first
version of the ontology (via the BioPortal project web
site), including its evaluation in real use case scenarios.
Methods
Use of tools and reasoning
To construct the OWL ontology, Protégé 4.1.9 (Build 209)
[28] together with its inbuilt HermiT 1.3.3 reasoner were
used. For evaluation purposes, ProMiner was used as a
named entity recognition (NER) tool and SCAIView as a
literature mining environment that allows for a context-
sensitive document retrieval based on ontologies.
Although there does not exist any single standard for
the evaluation of ontologies (cf., NCBO Ontology Summit
2013 [29] on ontology evaluation), there are various pro-
posals for how an ontology might be evaluated (e.g.,
[29,30], and [31], or the discussion by Hoehndorf et al.
[32]). In [31], the authors state that good ontologies are
the ones that serve their purpose and in [32] it is stated
that evaluation of (applied) ontology will depend on the
desired application. As the current primary purpose of
HuPSON is to aid in text-mining, its evaluation was
focused mainly on how it performed with regard to
literature-based mining of simulation knowledge. Thiswas accomplished using competency questions formulated
in advance by VPH experts and by use cases. For gold
standard creation (i.e. a training set and a test set), 700
PubMed abstracts about simulations in the VPH context
were downloaded from MEDLINE. The ontology class la-
bels and synonyms were converted into a dictionary for-
mat, then these terms were searched in both training set
and test set using ProMiner. The NER search was per-
formed using case-insensitive, word order-sensitive and
longest string exact match search constraints. For calcula-
tion of precision, recall and F-score of the test set, the fol-
lowing formulas were used:
Precisionf ¼ TPc = TPþ FPd 
Recallg ¼ TP = TPþ FNeð Þ
F?scoreh ¼ 2  Precision  Recallð Þ = Precisionþ Recallð Þ:
The MathML code contained within the ontology was
generated from equations collected from the literature
and encoded with the help of SnuggleTeX 1.2.2 [33].
SnuggleTeX is an open-source java library that converts
LaTeX into semantically enriched MathML, or Content-
MathML wherever the conversion can be done automat-
ically. Equations that have been annotated with MathML
code via an annotation property also have a textual def-
inition and are annotated with a PubMed ID pointing to
relevant literature.
Ranking of n-grams was performed using the Porter
Stemmer [34]. Noun phrase chunking was done using a
chunker based on the OpenNLP system [35].
The reasoner was used to subsume types with class-
descriptive axioms to be a subtype of formally defined
ones via necessary and sufficient axioms. In other words,
(secondary) classification is left to the reasoner and
ontology maintenance is eased through avoidance of dir-
ect multiple inheritance assertions, as proposed as a good
practice for modularised ontology construction [36].
Axioms necessary for this purpose were added manually,
for instance, to classes with composite multi-term labels.
Knowledge acquisition and conceptualization
In order to identify relevant entities and to ensure that
HuPSON will cover the most important terms from exist-
ing related work, standards for simulation and modelling
(such as SED-ML, Cell-ML, SBML, MIASE, MIRIAM, cf.
[16]), domain ontologies [6] in the field (cf. External ontol-
ogies section) and relevant literature were studied. A cor-
pus of pertinent literature articles and publications in the
context of the official VPH Network of Excellence and
other VPH projects was collected and analysed manually
for candidate upper-level classes. Around 32,000 relevant
PubMed abstracts were queried for candidate subclasses
of these upper-level classes (bigram to 5-gram word
Gündel et al. Journal of Biomedical Semantics 2013, 4:35 Page 7 of 9
http://www.jbiomedsem.com/content/4/1/35combinations containing the top-level class terms as the
last word of the n-gram, using a Java program written for
this purpose). Found n-grams were sorted by occurrence
and subsequently ranked. To ensure the ontology covers
the most important entities in the simulation context, ap-
proximately 15,000 of the abstracts from various resources
including the ones used in the n-gram search, VPH pro-
ject websites (e.g., VPH NoE, Biomed Town, LDL) and
extra information disseminated through existing VPH
projects (e.g., RICORDO, euHeart, VPHOP, ARTreat, pre-
DiCT and othersb) were analysed using a noun phrase
chunker. Thus, composite terms that are often used in the
literature, and subsequently important for text mining,
found their way into the ontology. For synonym enrich-
ment of ontology classes, an approach was chosen that
combines manual synonym annotations with the use of
external annotation services offered by the National
Center for Biomedical Ontology (NCBO) [37].
External ontologies
URIs of external ontologies have been re-used, where
appropriate, according to the Minimum Information to
Reference an External Ontology Term (MIREOT) princi-
ples [38] (cf. Figure 3). These include: CellMLBio Ontol-
ogy [39], DeMO [8,9], KiSAO [7], the Phenotypic Quality
Ontology (PATO) [40], Systems Biology Ontology (SBO)
[7] and LHDL Master Ontology [11,12]; Gene Ontology
(GO) [41], Chemical Entities of Biological Interest (ChEBI)
[42], Human disease ontology (DOID) [43], Cell type
ontology (CL) [44] and the Foundational Model of Anat-
omy (FMA) [45]. For model types, algorithm types and
qualities, the entire DeMO, KiSAO and PATO hierarchical
structures were included in HuPSON. Further information
on included external ontology classes is provided separ-
ately (Additional file 3).
The Basic Formal Ontology (BFO) [46] was preferred
over other upper-level ontologies (e.g. DOLCE [47],
SUMO [48], the General Formal Ontology [49] and Cyc
[50]) because of its use within the OBO community that
follows the OBO principles [51], its large user base and
the many ontologies that meanwhile have been con-
structed on BFO under the OBO Foundry [51] umbrella.
Using BFO upper levels, interoperability to those re-
sources is ensured. Relations were also adopted from
established standards, such as rdf-schema [52], Dublin
Core (DC) [53] and the OBO Foundry Relation Ontol-
ogy (RO) [54], as far as possible.
Endnotes
anumber classes (without owl:Thing): 2920; number
roots: 10; number leaves: 1927; max width/breadth: 727;
avg. width/breadth: 270.05; max depth: 10; total no. chil-
dren: 2885; avg. number children: 1.068; avg. depth (avg.
root-to-leaf distance): 5.486; depth variance (var(d) = E[d^2]-E[d]^2): 2.637; width/breadth variance (var(w) = E
[w^2]- E[w]^2): 55455850; tangledness (no. nodes with 2+
parents/total no. nodes): 0.060; fanout factor (no. leaf clas-
ses/number classes): 0.713.
bfor a complete list see http://www.vph-noe.eu/vph-
projects.
cnumber of true positive hits correctly found, i.e.,
matching the annotation in the gold standard.
dnumber of false positive hits, i.e., hits found but not
contained in the gold standard.
enumber of false negative hits, i.e., entities not found
but contained in the gold standard.
fproportion of correct hits out of all hits.
gproportion of correct hits out of all terms that should
have been correctly found.
hoverall measure of accuracy (harmonic mean of preci-
sion and recall).
Additional file
Additional file 1: Abstract of a simulation publication regarding
wall sheer stress in aortic coarctation patients annotated with
HuPSON terms, displayed in SCAIView environment.
Additional file 2: HuPSON-driven information retrieval scenario for
the application of mechanical pump models to supporting blood
circulation in human hearts, displayed in SCAIView environment.
The screenshot shows an exemplary document retrieved by the following
HuPSON-driven query: [heart AND pump model AND blood circulation].
HuPSON classes found in the PubMed abstract are highlighted in green.
Additional file 3: External ontologies.
Competing interests
The authors declare that they have no competing interests. This work was
not funded by the EU VPH programme.
Authors contributions
MG designed and coded the ontology, contributed to its evaluation and
drafted the manuscript. EY and AM contributed to evaluation and to
manuscript drafting. JW, HL and BZ carried out text annotations. BdB
contributed to ontology design. HTM performed text mining. MHA
participated in the design of the study and revised the paper critically. All
authors read and approved the final manuscript.
Acknowledgements
This work was conducted using the Protégé resource, which is supported by
grant LM007885 from the United States National Library of Medicine.
The authors wish to thank the following persons for their assistance: Marco
Viceconti from Istituto Ortopedico Rizzoli/the VPH Institute, Gerhard
Engelbrecht from the Center for Computational Imaging & Simulation
Technologies in Biomedicine, Universitat Pompeu Fabra, and Richard Lycett
from the School of Medicine and Biomedical Sciences, University of Sheffield,
for their valuable contributions providing queries useful for the evaluation of
the ontology; Roman Klinger from Fraunhofer SCAI for his contribution to
noun phrase chunking; Dirk Reith from Fraunhofer SCAI for his tips and
explanations with regard to the design of the UML class diagram and
regarding modelling and molecular computer simulations; Karl N. Kirschner
from Fraunhofer SCAI for his valuable hints and proofreading.
Author details
1Fraunhofer Institute for Algorithms and Scientific Computing (SCAI), Schloss
Birlinghoven, Sankt Augustin, Germany. 2Bonn-Aachen International Center
for Information Technology (B-IT), University of Bonn, Bonn, Germany.
3University College London (UCI), Gower Street, WC1E 6BT, London, UK.
Gündel et al. Journal of Biomedical Semantics 2013, 4:35 Page 8 of 9
http://www.jbiomedsem.com/content/4/1/35Received: 15 May 2013 Accepted: 7 October 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Cook et al. Journal of Biomedical Semantics 2013, 4:41
http://www.jbiomedsem.com/content/4/1/41RESEARCH Open AccessOntology of physics for biology: representing
physical dependencies as a basis for biological
processes
Daniel L Cook1*, Maxwell L Neal3, Fred L Bookstein4 and John H Gennari2Abstract
Background: In prior work, we presented the Ontology of Physics for Biology (OPB) as a computational ontology
for use in the annotation and representations of biophysical knowledge encoded in repositories of physics-based
biosimulation models. We introduced OPB:Physical entity and OPB:Physical property classes that extend available
spatiotemporal representations of physical entities and processes to explicitly represent the thermodynamics
and dynamics of physiological processes. Our utilitarian, long-term aim is to develop computational tools for
creating and querying formalized physiological knowledge for use by multiscale physiome projects such as the
EUs Virtual Physiological Human (VPH) and NIHs Virtual Physiological Rat (VPR).
Results: Here we describe the OPB:Physical dependency taxonomy of classes that represent of the laws of
classical physics that are the rules by which physical properties of physical entities change during occurrences
of physical processes. For example, the fluid analog of Ohms law (as for electric currents) is used to describe
how a blood flow rate depends on a blood pressure gradient. Hookes law (as in elastic deformations of springs)
is used to describe how an increase in vascular volume increases blood pressure. We classify such dependencies
according to the flow, transformation, and storage of thermodynamic energy that occurs during processes
governed by the dependencies.
Conclusions: We have developed the OPB and annotation methods to represent the meaningthe biophysical
semanticsof the mathematical statements of physiological analysis and the biophysical content of models and
datasets. Here we describe and discuss our approach to an ontological representation of physical laws (as
dependencies) and properties as encoded for the mathematical analysis of biophysical processes.Background and aims
Physiological knowledge is based on physically observable
properties of biological entities and how those properties
change values during biological processes. The parsing of
biological function into physical entities and processes is fun-
damental to how we observe, represent, and analyze biology
using a range of expressions from the purely descrip-
tive (e.g., increased blood pressure increases blood
flow) to formal quantitative mathematical expressions
(e.g., a pressure gradient is related to blood flow via
the fluid version of Ohms law). Whether described
and illustrated in textbooks of physiology or formalized* Correspondence: dcook@uw.edu
1Department of Physiology & Biophysics, University of Washington, Seattle
98195, USA
Full list of author information is available at the end of the article
© 2013 Cook et al.; licensee BioMed Central L
Commons Attribution License (http://creativec
reproduction in any medium, provided the orin complex, rigorous mathematical biosimulation models,
the semantics of physiological processes and their depend-
ence on thermodynamics are generally implicit in the
representations.
In prior work, we described how the OPB extends and
adapts classes from the Basic Formal Ontology (BFO)
[1] and the General Formal Ontology (GFO) [2] to rep-
resent OPB:Physical entity and OPB:Physical property
classes [3,4] that extend BFO and GFO spatiotemporal
representations of physical entities and processes to ex-
plicitly represent the thermodynamics and dynamics of
physiological processes. Here we take the next step by
describing OPB:Physical dependency classes as formal
representations of the laws of classical physics that are
the rules by which physical properties of physical
enities change during occurrences of physical processes.td. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Cook et al. Journal of Biomedical Semantics 2013, 4:41 Page 2 of 8
http://www.jbiomedsem.com/content/4/1/41For example, the fluid analog of Ohms law (as in electric
currents) is used to describe how a blood flow rate de-
pends on a blood pressure gradient. Hookes law (as in
elastic deformations of springs) is used to describe how
an increase in vascular volume increases blood pressure.
Our very utilitarian, long-term aim is to develop
computational tools for creating and querying formal-
ized physiological knowledge [5] for use by multiscale
physiome projects such as the EUs Virtual Physio-
logical Human (VPH) and NIHs Virtual Physiological
Rat (VPR). A knowledge representation problem com-
mon to such projects is that knowledge must be shared
between domain-specific silos that employ different
data formats and computational languages. Expressed
most simply, our working hypothesis is that an onto-
logical formalization of the mathematical language of
classical physics can provide a syntax and semantics
for logically representing the biological content of datasets
and analytical models according to their biophysical
meanings better than ad hoc documentation and local
naming/coding schemata. Thus, our goal for the OPB
is to formally represent the biophysical semantics of
the mathematical statements of physiological analysis
and simulation to formally map and query the biophys-
ical content of models and datasets. As example use-
cases, we are developing and testing this approach in
the domains of cardiovascular physiology [6,7] and sys-
tems biology [8] using our SemGen software and
knowledge structures [9] to cast systemic and cellular
physiology models into a prototype semantic human
physiome [5].
We have successfully built on this premise by develop-
ing OPB-based computational tools and a workflow with
which we annotate, aggregate, and query [8,10,11] the
biophysical content of available physiological models
written in the JSim language [12] (available from the
National Simulation Resource [13]), models written in
CellML [14] (available from PMR2 model repository
[15]), models written in SBML [16] (available from the
BioModels Database [17]), and BioPAX pathway data
[18]. We are currently testing these tools in the con-
text of the VPR multiscale physiological modeling pro-
ject [6]. These tools have used only the OPB:Physical
property classes [3] to create composite annotations
[9,19,20] to annotate data and variables (e.g., blood
pressure and flow rate). However, if we annotate only
physical properties of participating entities (e.g., things
such as hearts and portions of blood), then we ignore
the dependencies amongst property values (e.g., physical
laws and axioms) by which physical processes occur.
Thus, we have now extended OPB to represent a tax-
onomy of such physical dependencies by which property
values change during processes as a first step toward
representing a taxonomy of physical processes.We anticipate that the hierarchy of logically-defined
OPB:Physical dependency classes we present here will
accelerate physiome-level modeling efforts in several
ways. First, it will help automate the cumbersome model
annotation process. Furthermore, OPB:Physical depend-
ency annotations will communicate valuable information
about the assumptions underlying a models mathemat-
ical structure. This is crucial within the context of
community-level model reuse because, when repurpos-
ing a model, modelers must determine whether its
underlying assumptions preclude its use for a modeling
project. Incorporating dependency annotations into models
also provides a basis for performing qualitative, up/down
perturbation experiments on the modeled system without
the need for a numerical solver (see Discussion and next
steps).
We extend our prior work in semantic annotation to
not only annotate model variables but also computations
amongst variables (i.e. the equations) against OPB:Phys-
ical dependency classes that semantically represent the
meaning of the computations in terms of biophysical
systems dynamics. For example, a model that simulates
blood flow will, typically, calculate the dependence of
blood flow rates (OPB:Fluid flow rate) on blood pressure
differentials (OPB:Fluid pressure) along flow paths using,
in the simplest case, a fluid analogue of Ohms law (OPB:
Resistive flow dependency >OPB:Fluid flow dependency).
Similarly, that model will typically calculate blood pressures
as functions of the volumes of blood in a vessels according
to fluid analogues of Hookes law (OPB:Capacitive
force dependency > OPB:Fluid capacitive dependency).
Thus, the specific goals of the current effort are to: (1)
define the OPB:Physical dependency class to represent
the various quantitative dependencies between values of
OPB:Physical properties (as in Figure 1), (2) classify de-
pendencies according to a conceptual framework based on
system dynamical theory (Figures 2, 3), and (3) subclass
the dependencies to apply to single and multiple biophys-
ical domains (OPB:Physical domain; e.g., fluid flow, chem-
ical kinetics; Table 1). In future work, we propose to map
OPB:Physical dependency classes to corresponding OPB:
Physical process classes as a basis for creating a semantic
map of the human physiome built from the physiological
knowledge extracted semiautomatically from available bio-
simulation models as SemSim models [7] and integrated
as extended PhysioMaps [5,10].
Approach and scope
OPB is curated in the web ontology language (OWL
[21]) using the Protege-OWL [22] ontology editor. OPB
is available from BioPortal [23]. Currently, OPB encom-
passes the physics of discrete entities that can be ana-
lyzed with algebraic or ordinary differential equations
(ODE); we have deferred work on the representation of
Physical 
entity 
hasDependency 
Rate 
dependency 
Physical 
process 
hasProperty 
hasProperty 
hasDependency 
State 
dependency 
hasParticipant Boundary dependency 
Rate 
property 
State 
property 
Figure 1 OPB classes represent physical entities and processes
(blue icons) that have physical properties (purple) which are
players (blue arrows) in biophysical dependencies that
represent biophysical mathematical computations. Continuants
are above the gray boundary bar; processural entities, below.
Spatiotemporal categories are at left, biophysical classes at right.
Flow
Amount 
Flow rate 
inductive L 
Capacitive C 
Resistive R 
Force
Momentum 
Force 
Amount-driven K 
Conservation
Summation
State RateBoundary Dynamical Constitutive 
dependency property 
Figure 3 Schematic map that expands on Figure 1 to show
OPB:hasPropertyPlayer relations (gray lines) between key OPB:
Dynamical dependencies classes as rectangles for OPB:
Dynamical state dependency, OPB:Boundary dependency, and
OPB:Dynamical flow rate dependency. OPB:Physical property players
in these dependencies are shown as blue ovals for OPB:Dynamical
properties and gray ovals for OPB:Constitutive properties. K, R, C, and L
constitutive properties are, respectively: OPB:Reaction rate constant,
OPB:Resistance, OPB:Capacitance, and OPB:Inductance.
Cook et al. Journal of Biomedical Semantics 2013, 4:41 Page 3 of 8
http://www.jbiomedsem.com/content/4/1/41continuum physics requiring partial differential equa-
tions (PDE) of spatial gradients. As a domain strategy,
we base the OPB on theories of classical physics as
expressed in texts of basic, classical physics and as ap-
plied to physiological systems including biomechanics
(e.g., [24]), electrophysiology (e.g., [25]), chemical bio-
physics (e.g., [26]), and large-scale physiological integra-
tion (e.g., [27]).Figure 2 OPB:Physical dependency class taxonomy showing
selected OPB:Dynamical dependency classes as discussed in this
paper; see the OPB.owl file for others.As a curatorial strategy, we base our representation on
an extensive range of use-cases encountered in our pro-
ject to establish a semantic physiome that consists of
semantic simulation (SemSim) models [28] derived by
semiautomatically parsing and annotating biosimulation
models as available from model repositories in a variety
of computational languages. These sources afford us a wide
range of use-cases that span multiple structural scales and
biophysical domains with challenging abstractions.
We generalize engineering-oriented approaches to rep-
resent multiscale, multidomain physiological processes
as a useful knowledge tool that leverages recent progress
in computational biomedical ontology [5]. Thus our
goals and approaches differ from other ontological ap-
proaches to physical phenomena such as naive physics
as developed for artificial intelligence applications [29],
the representation of the mathematics of engineering
analysis [30], or strictly spatiotemporal representations
of entities and events as articulated by Dorato et al. [31].
Foundational representation of biophysics
Using the representational schema illustrated in Figure 1,
the OPB is designed to express the physical intuition that,
at any instant in time, any physical entity (e.g., a portion of
blood, cell) exists in a physical state that is defined, in
part, by the values of its state properties (e.g., fluid vol-
ume, cell location). State property values change dur-
ing physical processes (e.g., blood volumes change as
blood flows, bones move as forces are applied) at
process rates (e.g., blood flow rate, bone velocity) de-
termined, in turn, by the state properties of the entities
Table 1 Examples of OPB:Dynamical dependency classes with property players and corresponding constitutive
proportionality classes
Dynamical dependency Rate property player Other property player(s) Constitutive proportionality
Constitutive flow dependency
Amount-driven flow dependency Flow-rate ? Amount Resistance
Force-driven flow dependency Flow-rate ? Force Rate constant
Resistive flow dependency Flow-rate  Resistance
Coupled flow dependency Flow-rates  Modulus
Constitutive storage dependency
Capacitive dependency ? Force Amount Capacitance
Inductive dependency ? Force Momentum Inductance
Transactor dependency Either Any Coefficient
Cook et al. Journal of Biomedical Semantics 2013, 4:41 Page 4 of 8
http://www.jbiomedsem.com/content/4/1/41that participate in the process. This merely restates the
Laplacian precept that future physical states are fully
determined by current physical states. Our aim is not
to author a formal model of the real world as a kind
of physical realism but to produce a semantic model
of physical concepts as they are used by biophysicists,
bioengineers, and physiologists to analyze and model
the real world. Thus, OPB should be capable of
representing statements of physics that concern such
unreal entities as frictionless planes, inviscid blood or
spherical hearts.
As a first step, we adopt the foundational distinction
between physical entities (continuants) and the physical
processes (occurrents) by which the entities change.
Whereas BFO represents the spatiotemporal aspects of
reality, OPB represents the complementary and orthog-
onal thermodynamic and dynamical aspects of the same
reality. Thus OPB:Physical entities participate (via the has-
Participant relation) in OPB:Physical processes wherein
thermodynamic energy flows amongst participants (see
below). Entity and process classes are attributed with
physical properties (via the hasProperty relation) that are
classified as, respectively, OPB:Dynamical state properties
and OPB:Dynamical rate properties, so-named because
each determines, respectively, the states and rates of
change of the thermodynamic energy content of partici-
pating physical entities (see OPB:Thermodynamic depend-
ency, below). The values of these properties depend upon
one another according to the laws of physics, expressed as
mathematical relations and used to compute the quantita-
tive implications of biophysical hypotheses.
In prior work [3], we introduced the OPB:Physical
property taxonomy. Here we extend our representation
to OPB:Physical dependency entity classes (Figure 1)
whose main subclass, OPB:Dynamical dependency, is
defined as a mathematical relationship by which the
values of dynamical properties of entities that are role
players in the dependency determine the flow ordistribution of energy amongst physical entities that
are bearers of the properties. That is, physical de-
pendencies encompass the rules by which physical
entities property values change and energy flows during
occurrences of physical processes. As such, OPB:Physical
dependency entity may qualify a subclass of GFO: Concept
(i.e., categories that are expressed by linguistic expres-
sions and which are represented as meanings in someones
mind [32]). However, as the top class in the OPB is de-
fined as formal abstraction of the real world created for
the science of classical physics for describing and analyzing
real physical entities and processes, the OPB would seem
to be apart from the BFO representation of reality [33].
System dynamical analogies span biophysical domains
The OPB representational schema illustrated in Figure 3
is based on well-developed theories of system dynamics
[34-37] that recognize and represent analogies between
physical phenomena in different physical domains (OPB:
Physical domain). For example, Ohms law, originally de-
rived for modeling electrical current flow, describes
analogous relations for viscous resistance to fluid flow
and mechanical translation. Similarly, Hookes law de-
rived for elastic spring compression applies to electrical
capacitors and extensible fluid vessels. Following Borst
et al. [34], we have formalized such analogies for classify-
ing physical properties (e.g., fluid pressure, chemical
concentration) [38] and here present a taxonomy of the
physical laws that are the basis for the analogies. For ex-
ample, OPB:Resistive flow dependencies (Resistive in
the figure) are analogues of Ohms law that relate a flow
rate (classically, electrical current in amperes) to a force
(an electrical potential difference, in volts) and a resistive
constitutive property (typically a resistance parameter in
Ohms). OPB:Capacitive force dependencies (Capacitive
in the figure) are analogues of Hookes law that relate an
amount (e.g., the volume of fluid in an elastic vessel) to a
force (e.g., the fluid pressure in the vessel) across multiple
Cook et al. Journal of Biomedical Semantics 2013, 4:41 Page 5 of 8
http://www.jbiomedsem.com/content/4/1/41dynamical domains. We have formalized and extended
these dynamical analogies from the engineering domain to
the domains of physiological processes.
Physical dependencies represent physical laws
Physical dependencies represent fundamental rules by
which material, electrical charge, and thermodynamic
energy are transfered and stored in and amongst partici-
pants in physical processes. Thus, an OPB:Physical de-
pendency is an axiom, definition, or empirical law of
classical physics that relates the values of physical prop-
erties of physical entities and processes to one another.
For example, the area of a circle depends on its diam-
eter; electrical current flow in a wire depends on its volt-
age differential and a resistance to flow; the change of
fluid volume in a vessel depends on net fluid inflow and
outflow rates and the elasticity of the vessel. OPB:Phys-
ical dependencies logically represent the quantitative
relationships between the values of physical properties
of the physical entities that participate in physical pro-
cesses. In parallel with the RO:has_participant rela-
tionships [39] of entities to procsses as in Figure 1,
OPB defines the OPB:hasPropertyPlayer (blue arrows
in Figure 1) that relate physical properties to the de-
pendencies that describe how the values of the proper-
ties depend upon one another.
Thus, for example, a dependency representing Ohms
law would have as property players a current (I), a volt-
age differential (E), and a resistance (R). Just as process
participants must be distinguished according to their
participatory role (RO:has_participant), players and their
roles in dependencies are related by OPB:hasProperty-
Player object relations. For example, I, E, and R are re-
lated by OPB:hasFlowPlayer, OPB:hasForcePlayer, and
OPB:hasConstitutiveProportionality for a representation
of Ohms law where Flow, Force and Constitutive
refer to OPB superclasses for electrical current, voltage,
and resistance, respectively [3]. We will describe selected
examples of three OPB:Dynamical dependencies classes:
OPB:Boundary dependency, OPB:Dynamical state de-
pendency, and OPB:Dynamical flow rate dependency as
shown in the class taxonomy in Figure 2.
Boundary dependencies
A boundary dependency represents how a change in
an amount or momentum (i.e., a state property) of a
physical entity depends on a flow rate or force that tra-
verses or impinges on the boundary of the entity over
a span of time. For example, blood flows across the
boundary from one portion of fluid (OPB:Portion of fluid,
e.g., blood in the proximal aorta) to another (e.g., blood in
the distal aorta) reducing and increasing, respectively, the
amount of blood in each portion. This basic principle, an
application of Stokes theorem to discrete systems, isrepresented by subclasses of OPB:Boundary flow depend-
ency which holds for the flow of conserved quantities in
multiple biophysical domains such as mass, charge,
and energy flows across entity boundaries. For discrete
entities, this dependency takes the mathematical form,
?volume = ?(volume_flow_rate) dt, or in the derivative
form (d(volume)/dt = volume_flow_rate). These math-
ematical relations simply state that the change in amount
of stuff (e.g., volume of blood; OPB:Fluid volume) in an
entity changes over a span of time as the temporal integral
of the flow rate of stuff (e.g., volume flow rate of blood;
OPB:Fluid flow rate). This dependency applies irrespective
of the structural constitution and material properties of
the participating entities and depends solely on the fluxes
across the boundary (OPB:Physical boundary) between
entities.
The OPB:Boundary force dependency is an analogous,
but less familiar, dependency that represents a change in
the value of a momentum state property (OPB:Momen-
tum property) as the temporal integral of a force (OPB:
Force property) over a span of time. For example, the
longer a force is applied to a ball, the faster the ball will
roll; i.e., the more momentum it will have. When tem-
porally differentiated, this dependency is a restatement
of Newtons Second Law (i.e., that force is a product of
mass and acceleration; f = ma).
State dependencies
According to boundary dependencies, flows or forces
acting across the boundary of a physical entity necessar-
ily change the entitys amounts or momenta, respect-
ively, which are changes of physical state (OPB:Physical
state) that may change other state properties. For ex-
ample, the net flow of blood into a vessel changes the
vessels volume (OPB:Fluid volume) which changes
values of other spatial properties such as the vessels
length or diameter. Such dependencies amongst an
entitys state properties are represented as subclasses
of OPB:Dynamical state dependency that determines
how attributes of a physical entity are related at a
temporal instant and thus represent static relation-
ships amongst entity state properties. In addition to
OPB:Spatial dependency (as in our vascualar example),
OPB:Dynamical state dependency also includes the
subclass OPB:Summation dependency, which applies to
multiscale entities and, for example, accounts for how,
at any instant, the volume of an entity is the sum of
the volumes of its proper parts.
Rate dependencies
OPB:Dynamical flow rate dependencies represent a broad
variety of biophysical mechanisms by which physical en-
tities transfer or control the flow of thermodynamic
energy amongst physical process participants. Each
Cook et al. Journal of Biomedical Semantics 2013, 4:41 Page 6 of 8
http://www.jbiomedsem.com/content/4/1/41represents a dependency of a rate property (i.e., flow-
rate or force) on another dynamical property (i.e., rate
or state) and are classified, as in Table 1, according to
how energy is exchanged, stored, or controlled during
the process. Whereas RO [39] and Process Specifica-
tion Language (PSL) Ontology [40] represent the spa-
tiotemporal consequences of physical process such as
the fusion, fission of participants, rate dependencies
define the underlying thermodynamic forces that drive
such changes. For the sake of brevity, only selected
OPB:Rate dependency classes will be discussed as ex-
amples of classes that have, so far, been represented in
OPB.
OPB:Constitutive flow dependency classes represent
dependence of a flow-rate of matter (or charge or en-
ergy) on the driving force (fluid pressure) differential
that drives blood flow from atrium to ventricle in a
heart. A key subclass are the OPB:Force-driven flow de-
pendencies that are analogues of Ohms law which ex-
presses the dependence electrical current flow-rate on a
electrical potential (voltage, a force) difference across an
electrically conductive pathway. For an ideal conductor,
flow-rates are proportional to force differential and can
be quantitatively characterized by an electrical resistance
proportionality (OPB:Resistance;) or its reciprocal, con-
ductance (OPB:Conductance). Analogues of Ohms law
apply to the dependence of the velocity of a moving
structure to the viscous force resisting the motion, or
the dependence of a chemical reaction flux on the chem-
ical potentials of pools of the reactants [26].
In two cases, flow-rates between participants are for-
mulated to depend not on force differences but on differ-
ences in the amount properties of participants and are
represented by OPB:Amount-driven flow dependencies.
Its subclasses are OPB:Chemical mass-action rate de-
pendency and OPB:Diffusion gradient rate dependency
that represent, respectively, the many classes of chemical
mass-action rate laws (e.g., Michaelis-Menten enzyme
rate law) and Ficks diffusion rate law.
OPB:Coupled flow dependencies are subclasses of OPB:
Force-driven flow dependency that represent mechanisms
by which energy flows from one participant to another
in a manner analogous to an electrical transformer. That
is, the product of force times flow-rate for one element
is proportional (as a modulus parameter) to the prod-
uct of force times flow-rate of the other. The OPB:Mech-
anical transformer dependency represents participants
in a single domain, OPB:Solid mechanical domain, in
which, for example a biceps muscle tendon moves a
weight held in a hand. Other transformer dependencies
cross domains. For example, OPB:Chemo-mechanical
transducer dependency applies to process in which
chemical potential energy is transduced into mechan-
ical energy as when ATP is consumed to contractmyofibrillar proteins. OPB:Fluid-mechanical transduc-
tion occurs when the mechanical strain energy of the
myocardium is transduced into fluid potential energy of
the ventricular blood.
OPB:Constitutive storage dependencies (Table 1) repre-
sent how kinetic or potential energy is stored by partici-
pants in processes. The OPB:Capacitive storage dependency
represent analogues of Hookes law that relates the amount
(e.g., mechanical displacement) of a participant to forces act-
ing upon it. For example, OPB:Fluid capacitive dependency
represents how the fluid pressure (OPB:Fluid pressure) of a
portion of ventricular blood depends on the amount (OPB:
Fluid amount) of blood in the ventricle. The more blood,
the more fluid potential energy is associated with the blood
portion. OPB:Inductive flow dependencies represent how ele-
ments that behave as analogues of electrical inductors store
kinetic energy. For example, OPB:Fluid inductive depend-
ency represents the dependency of the momentum of a
moving portion of fluid on the fluid pressures that move the
fluid just as OPB:Mechanical inductive dependency relates
the solid momentum of a solid object to the forces that ac-
celerate it.
OPB:Transactor dependency classes represent a broad
and pervasive class of dependencies for which the actual
thermodynamic forces are either unknown or negligible.
They are dynamical wild cards for representing de-
pendencies that are known to exist but whose mechanisms
are unknown, are too complex, or are thermodynamically
negligible. Thus, the baroreceptor reflex, the neural control
of heart rate and contractility by aortic blood pressure,
which involves complex neural signal transmission and
transduction is often modelled as a simple proportional
dependency. Virtually any combination of dynamical prop-
erty dependencies can and have been modeled by such
simple A affects B formulations.
Next, we briefly discuss the constitutive properties of
rate dependencies that are subclasses of OPB:Physical
property.
Constitutive properties of rate dependencies
OPB:Rate dependencies are constitutive in that the
functional form of the dependency depends on both the
structural and material constitution of the device de-
scribed by the dependency. For example, the slope of a
dependency between voltage differential and current for
a wire depends on structural composition (length, diam-
eter) and material composition (the resistivity of the
conducting material). Similarly, vascular blood flow
driven by a pressure gradient is a more complex de-
pendency between vessels length, diameter, and wall
thickness, the walls material elastance properties, and
the bloods viscosity. We have introduced such bio-
physical rate dependencies as analogues of ideal
electrical circuit elementse.g., resistors, capacitors,
Cook et al. Journal of Biomedical Semantics 2013, 4:41 Page 7 of 8
http://www.jbiomedsem.com/content/4/1/41inductors, transformerswhose operating character-
istics are simple proportionalities between the values
of physical property players. For such ideal dependen-
cies, we use the OPB:hasConstitutiveProportionality
object property to link to subclasses of OPB:Constitu-
tive proportionality, a kind of OPB:Physical property,
that includes OPB:Conductance (along with its reciprocal
property, OPB:Resistance) and OPB:Reaction rate con-
stant (as related to an OPB:Chemical mass-action rate
dependency).
Biophysicists, however, have to routinely contend with
decidedly non-proportional dependencies whose operat-
ing curves and algebraic description may entail multiple,
empirically-determined coefficients. For example, the re-
sistive flow dependency that relates fluid volume flow-
rate to input pressure of a vascular blood flow process is
particularly non-linear because the vessel cross-sectional
area depends, itself, on input pressure, and because
blood is a non-Newtonian fluid whose viscosity depends
on flow-rate (see [24]). Furthermore, the conductance of
even the simplest membrane ion channel has distinctly
non-linear conductive properties that require complex
algebraic descriptions that may include dynamical de-
pendencies such as expressed by the Hodgkin-Huxley
ion-channel gating equations [25]. Even the familiar
Michaelis-Menten rate equation describes a dependency
that is non-proportional that requires two coefficients
(a half-maximal concentration, and a maximum flow
rate) as expressed in SBO. Representing such complex-
ities in broad and complete form is currently beyond
the scope of the OPB but will be handled on an as-
needed basis as specific use-cases arise.
Discussion and next steps
The OPB addresses the needs of physiologists for a for-
mal semantics of biophysical processes as needed for the
annotation and reuse of biosimulation modeling and
data resources [6,8,28]. The OPB schema (Figure 1) ex-
tends the classification of continuant and processural
spatiotemporal entities (Figure 1, left side; as in BFO,
GFO) by introducing (Figure 1, right side) the biophys-
ical abstractions used to quantitatively represent and
computationally explain how biological processes occur.
We continue to work with collaborators in the biosimu-
lation community to apply and extend OPB and SemSim
architectures to the physics-based annotation and ana-
lysis of biological processes [6,7]. For such projects we
use our SemGen software to parse the code of available
biosimulation models to generate a semantic simulation
(SemSim) model of each source model based on OPBs
class structure and relational schema. We are able to as-
semble sets of SemSim models of into an aggregate net-
work knowledgebase (tentatively termed SemPhysKB
[5]). In preliminary work, we have abstracted the contentsof such a knowledgebase into a network of physical en-
tities (as nodes) and the processes (as linking arrows) that
we call PhysioMaps [10,11] that we propose to analyze
with quantitative modeling and qualitative inferencing as
in our prior Chalkboard application [41]).
The OPB representational framework is based on gen-
eral theories of classical physics and network thermody-
namics [36,37] as represented in bond-graph theory [35]
and the PhysSys ontology [34]. We have exploited this
generality to provide an integrated representation of the
physical properties and principles that apply to the
broad spatiotemporal scales and multiple biophysical do-
mains that are required for quantitative analysis of the
physiome [27]. Whereas we recognize that OPB could
be generalized from discrete to continuum physics, we
are deferring such a generalization pending use-cases in
the modeling community for which continuum models
can accelerate and facilitate continuum modeling projects.
A challenge we faced was to develop a representation
of dynamical properties [3] and dependencies that is
both orthogonal to and consistent with existing spatio-
temporal representations of biological processes as in
BFO and GFO. Given the span and generality of OPB,
however, there are bound to be some overlaps. For one
example, OPB:Fluid pressure maps to PATO:pressure in
the Phenotype Ontology (PATO [20]). OPB is a com-
plement to other biomedical ontologies and semantic
resources as have been reviewed in [42]  OPB gener-
alizes, in both scale and domain, on the chemical kin-
etic focus of SBO (Systems Biology Ontology) while
KiSAO (Kinetic Simulation Algorithm Ontology) de-
scribes a variety of simulation algorithms, and TEDDY
(Terminology for the Description of Dynamics) offers
descriptors of the behaviors of simulation results.
OPB current limitations, next steps
We are aware of a number of limitations that direct our
current OPB research and development. First, whereas
we have provided (for the most part) human-readable
definitions to the new dependency classes, our OWL
description-logic implementation of physical dependency
classes is far from complete.
A next step is to implement has_property_player rela-
tions for dependencies (analogous to has_participant re-
lations for processes) that link property instances of
physical entities according to the particular mathemat-
ical role they play the dependency computation. For
example, calculating fluid flow-rate from a portion-of-
fluid-A to portion-of-fluid-B holds only for the fluid
pressure of A, the fluid pressure of B, and the fluid
flow resistance from A-to-B; other pressures and resis-
tances are irrelevant. Second, our current collaborations
amongst the biosimulation community aim to test the
generality, utility, and applicability of the current OPB
Cook et al. Journal of Biomedical Semantics 2013, 4:41 Page 8 of 8
http://www.jbiomedsem.com/content/4/1/41class structure and object relations. Another key issue is
how to more fully support the non-proportional dependen-
cies across biophysical domains to model non-proportional
kinetic rate equations (as in SBO) for modeling chemical
mass-action kinetics.
Competing interests
The authors have no competing interests in this work.
Authors contributions
All coauthors participated in numerous discussions leading to the
development of the OPB. DLC is the developer of the ontology and led the
writing of this manuscript. JHG, FLB, and MLN contributed to the writing and
editing of the manuscript. All authors read and approved the final
manuscript.
Acknowledgements
This work was partially funded by the EU-VPH Network of excellence, EC FP7
project #248502, and by the NIH-NIGMS GM094503.
Author details
1Department of Physiology & Biophysics, University of Washington, Seattle
98195, USA. 2Department of Biomedical & Health Informatics, University of
Washington, Seattle 98195, USA. 3Department of Bioengineering, University
of Washington, Seattle 98195, USA. 4Department of Statistics, University of
Washington, Seattle 98195, USA.
Received: 6 September 2013 Accepted: 19 November 2013
Published: 2 December 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Rebholz-Schuhmann et al. Journal of Biomedical Semantics 2013, 4:19
http://www.jbiomedsem.com/content/4/1/19
SOFTWARE Open Access
Monitoring named entity recognition: the
League Table
Dietrich Rebholz-Schuhmann1,2*, Senay Kafkas2, Jee-Hyub Kim2, Antonio Jimeno Yepes2,3 and Ian Lewin2,4
Abstract
Background: Named entity recognition (NER) is an essential step in automatic text processing pipelines. A number
of solutions have been presented and evaluated against gold standard corpora (GSC). The benchmarking against
GSCs is crucial, but left to the individual researcher. Herewith we present a League Table web site, which benchmarks
NER solutions against selected public GSCs, maintains a ranked list and archives the annotated corpus for future
comparisons.
Results: The web site enables access to the different GSCs in a standardized format (IeXML). Upon submission of the
annotated corpus the user has to describe the specification of the used solution and then uploads the annotated
corpus for evaluation. The performance of the system is measured against one or more GSCs and the results are then
added to the web site (League Table). It displays currently the results from publicly available NER solutions from the
Whatizit infrastructure for future comparisons.
Conclusion: The League Table enables the evaluation of NER solutions in a standardized infrastructure and monitors
the results long-term. For access please go to http://wwwdev.ebi.ac.uk/Rebholz-srv/calbc/assessmentGSC/. Contact:
rebholz@ifi.uzh.ch.
Keywords: Text mining, Gold standard corpus, Evaluation, Named entity
Background
Benchmarking components of text mining solutions
against gold standard corpora (GSCs) is mandatory to
achieve long-term progress in text mining [1]. The
biomedical text mining community has engaged into the
development of a selection of GSCs as a requirement for
public competitions [2,3]. We now propose to benchmark
the annotated corpora with the help of a dedicated sub-
mission site that not only benchmarks the performances,
but also generates a ranked list of all-time performances
(the League Table) and keeps hold of the submitted
annotated corpora for future comparisons.
The following GSCs have been made available for the
identification of gene and protein names (PGN) in the
scientific literature: JNLPBA, FSUPRGE, BioCreative II
and PennBioIe, and further GSCs have been prepared for
*Correspondence: rebholz@ifi.uzh.ch
1Department of Computational Linguistics, University of Zurich, Zürich,
Switzerland
2European Bioinformatics Institute, Wellcome Trust Genome Campus,
Hinxton, Cambridge CB10 1SD, UK
Full list of author information is available at the end of the article
chemical entities and disease mentions [4-6]. However,
the evaluation of a novel NER solution against one or
several GSCs is a tedious task and it is the researchers
responsibility to perform all evaluations. The final results
are reported in the corresponding scientific publication
without delivering the annotated corpus to the public and
without keeping track of the scores in combination with
the delivered corpus.
The inclusion or exclusion of features into the NER
approach decides on the performance of the solution
against the GSC. It can be expected that progress in the
development of NER solutions can be improved by mak-
ing the annotated GSC available in combination with
the systems description and the performance measures
against the used GSC. In addition, having all GSCs repre-
sented in a standard format and measuring performances
through a shared submission site should reduce the error
rate in all reporting. Last, the web site can act as an
inventory for the annotation results related to a journal
submission. Users of the site can investigate on the system
descriptions and the annotation results.
© 2013 Rebholz-Schuhmann et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the
Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,
distribution, and reproduction in any medium, provided the original work is properly cited.
Rebholz-Schuhmann et al. Journal of Biomedical Semantics 2013, 4:19 Page 2 of 5
http://www.jbiomedsem.com/content/4/1/19
Automatic evaluation has been performed as part of
different challenges (e.g., LLL and BioNLP shared task),
but no League Table is generated over time. The machine
learning community (e.g., http://mlcomp.org) has pro-
posed such an approach, but the GSCs for the annotation
of biomedical named entities requires different evaluation
methods.
Here we describe the interface of the submission site
and the technology behind. A number of publicly available
GSCs have been normalized into a shared representation
and are available for download [7,8].
Implementation
Selection of GSCs
The CALBC League Table hosts GSCs for genes and pro-
teins, for diseases and chemical entities, after serving
as submission site for the CALBC challenge (Collabora-
tive Annotation of a Large-scale Biomedical Corpus, [9]).
The following GSCs for proteins and genes are accessi-
ble from the web site: (1) JNLPBA corpus (from 2004,
produced from the Genia corpus), (2) BC-II (2005, test
data for human gene and protein NER), (3) the PennBioIE
corpus (2008, oncology), and (4) the FSU-PRGE cor-
pus (2009, gene-regulatory events) [4-6]. All corpora
deliver complete Medline abstracts as annotated docu-
ments, except the BC-II GSC which consists of individual
sentences. In addition the Arizona corpus for disease
annotations and the SCAI corpus for chemical entities
have been normalised and uploaded to the submission
site [10,11].
Transformation to IeXML
IeXML has been used to standardize the annotations in
the GSCs, which is also suitable for the alignment of
the corpora. The users have to provide their annotations
in the IeXML format, then upload the corpus and after
a short processing time they receive the evaluation of
their annotations against the corpus. The submitter is
requested to have a description of the annotation solution
with the uploaded annotated corpus.
Other formats have been suggested that could be
used as an alternative, but would not serve the same
purpose as the IeXML format. The BIO/IOB formats
are very popular and have been well supported by
the CoNLL challenges. The letters stand for (B)egin,
(I)nside and (O)utside which represent the tag set used
for marking up the different tokens of a term (B, I)
and the surrounding tokens (O). Unfortunately, there is
not a single standardized BIO/IOB format, i.e. differ-
ent variants exist. There are leaner formats (. . .the_O
protein_B HZF-1_I is_O. . . ) and richer for-
mats, which include part-of-speech information. It is
possible to anticipate an XML format for BIO/IOB
(<w iob="o">the</w><w iob="b">protein</w>
<w iob="i">HZF-1</w><w iob="o">is</w>
<w iob="o">), which then could be transformed
into IeXML  or even used as it is  to calculate the
alignments efficiently.
Second, BIO/IOB requires that the stream of text is
tokenized and usually the single tokens are delivered on
separate lines. IeXML only marks and analyses the bound-
aries and does not consider nor evaluate the tokenisation
leading to a solution whose purpose is more generic.
Third, BIO/IOB  in contrast to IeXML  cannot deal
with nested annotations nor with overlapping annota-
tions, which plays an important role in biomedical text
mining. For example, the phrase left lung cancer treat-
ment can be annotated as a long noun phrase ("BIII"),
but a more sophisticated solution would allow alternative
interpretations as well which could result from the use of
different terminological resources: left/B lung/I cancer/B
treatment/I ("BIBI", a cancer treatment of an organ)
and left/B lung/I cancer/I treatment/B ("BIIB", a treat-
ment of a lung cancer type possibly located outside of the
lungs) would both be valid solutions. In the best case the
annotation solution would account for all, which cannot
be achieved with BIO/IOB.
Last, BIO/IOB has so far not been used to consider
the semantic type. For the sake of supporting different
research communities, a transformation from BIO/IOB
into IeXML is under development and will be provided in
the future.
Alignment and evaluation
The annotated corpora undergo sentence-based align-
ment to then achieve NE-based alignment with the NEs
of the corresponding GSC using the Whatizit Finite State
Automata infrastructure [12]. Alignment is performed
right after submission and on the fly on a Sun Fire V40z 8-
cpu opteron server with 128 GB RAM. A summary file is
generated that gathers the frequency of the different error
types and produces the required statistical results. Eventu-
ally, the standard statistical parameters such as precision,
recall and F-measure of the annotated corpus against the
GSC are calculated.
In principle, different alignments are available that pro-
duce either exact, cos98 or nested matching of the anno-
tated entities against the pre-annotated entities in the
GSC [9]. The preferred evaluation uses exact matching,
since this annotation solution is the standard in public
challenges. Alternative measures can be selected, such as
cos98 matching and nested matching, to relax the bound-
ary condition in the evaluation. Cos98 matching is a
symmetrical measure and counts two annotations as simi-
lar, if they only haveminor differences in their boundaries,
i.e. the existance or lack of an extension such as a deter-
miner or a frequently encountered term such as protein.
Nested matching is an asymmetric measure which counts
Reb
holz-Schuhm
ann
etal.JournalofBiom
edicalSem
antics
2013,4:19
Page
3
of5
http
://w
w
w
.jb
iom
edsem
.com
/content/4/1/19Table 1 The table shows the League Table for annotation solutions that have been tested against the JNLPBA GSC*
Top performing system
User Reference file Assessment file # of Precision Recall F-score Alignment Date
annotations type
jhkim JNLPBA.Gold.xml JNLPBA.20100730.AbnerNLPBA.xml 6142 74.70% 66.52% 70.37% Exact 2012-02-16
jhkim JNLPBA.Gold.xml JNLPBA.20100730.Abner.xml 6142 61.07% 63.01% 62.03% Exact 2012-02-16
jhkim JNLPBA.Gold.xml JNLPBA.20100730.chang2.xml 6142 60.27% 59.51% 59.89% Exact 2012-02-16
jhkim JNLPBA.Gold.xml JNLPBA.20100730.biolexicon.xml 6142 49.17% 33.29% 39.70% Exact 2012-02-16
jhkim JNLPBA.Gold.xml jnlpba.whatizitUkpmcPRGE.xml 6142 34.40% 44.45% 38.78% Exact 2012-02-16
jhkim JNLPBA.Gold.xml jnlpba.swissprot70.xml 6142 39.82% 36.93% 38.32% Exact 2012-02-16
jhkim JNLPBA.Gold.xml jnlpba.geneProt70.xml 6142 51.11% 30.25% 38.00% Exact 2012-02-16
jhkim JNLPBA.Gold.xml JNLPBA.20100730.whatizitUkPmcGenesProteins.xml 6142 32.43% 43.87% 37.29% Exact 2012-02-16
jhkim JNLPBA.Gold.xml EBI.JNLPBA.Test.xml 6142 32.53% 42.78% 36.96% Exact 2012-01-31
*The same table will be shown from the League Table web interface.
Rebholz-Schuhmann et al. Journal of Biomedical Semantics 2013, 4:19 Page 4 of 5
http://www.jbiomedsem.com/content/4/1/19
as positive, if either the GSC annotation is fully contained
in the annotation of the submitted corpus, or vice versa.
In the case of BC-II, only the gene list is considered. The
inclusion of the alternative gene list would lead to results
that cannot be compared directly to the outcomes against
the other GSCs.
Results and discussion
The user has to select, download and annotate the
GSC that fits best the users annotation solution. All
annotations have to comply with the IeXML format
for inline annotations. Standoff annotations could be
used as an alternative but have proven to be less
robust in challenge evaluations. The annotated corpus
is submitted to the site and automatically aligned with
the annotations from the GSC leading to the iden-
tification of false positive and false negative annota-
tions. Finally the precision, recall and F-measure are
determined.
The user is requested to supply a description of the
annotation solution together with the annotated corpus.
Currently, EBIs publicly available annotation solutions
have been applied to the GSCs and the annotated corpora
have been uploaded into the League Table.
Table 1 gives an overview of the first results in the
League Table. All results are sorted according to the
F-measure that has been determined through the align-
ment of the annotated corpus against the GSC. The com-
parison of different PGN NER solutions has shown that
their performances vary from one GSC to the next and
that they achieve higher performances in the identifi-
cation of PGN NER on GSCs with newer release dates
[8]. Furthermore, different PGN taggers with the same F-
measure performance on a given GSC can have different
profiles in terms of their precision and recall perfor-
mances on the GSC.
The League Table approach can be applied to a variety
of NE types as shown and to any selection of GSCs or sil-
ver standard corpora (SSCs). The collection of annotated
corpora tagged by different tagging solutions in combi-
nation with their descriptions helps to better understand
which features in the annotation solutions produce the
best results.
Currently, only the U-Compare solution has been made
available for comparative evaluation of annotation solu-
tions [13]. U-Compare allows comparisons of NER solu-
tions against publicly available tagging solutions that can
be executed within U-compare, e.g., ABNER, GENIA tag-
ger, etc., over different corpora, e.g., AImed, BioIE, and
others [13]. However, U-Compare does not maintain a
repository of annotated corpora and does not generate a
list of performances against the GSC.
Competitions have been proposed for other tasks in
computational biology, such as protein structure predic-
tion (CASP) and the prediction of protein network rep-
resentations from experimental data (DREAM) [14,15].
Furthermore, submission sites are available for generic
machine-learning problems and solutions such as the
MLcomp Web site [16], but this approach has not yet
attracted any biomedical researchers that investigate into
the semantics of the proposed task including approaches
that make use of biomedical data resources. So far, the
CALBC League Table is the only solution available that
gathers the research community in biomedical textmining
and data integration.
Conclusions
Altogether, the CALBC League Table contributes to the
development of NER solutions, since all overhead is
reduced to the submission of an annotated corpus in a
standardised format, and users can follow-up on their own
submissions in the future. For access please go to [17]. The
League Table Web interface guides all data exchange and
only requires a standard Web browser for its execution.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
AJY produced the core alignment engine for the assessment of submission
and transformed the gold standard corpora into the standard format. AJY and
JHK developed the submission site. SK and IL contributed significant efforts
towards the quality control of the whole submission system. DRS supervised
the work and wrote the manuscript. All authors read and approved the final
manuscript.
Acknowledgements
This work was funded by the EU Support Action grant 231727 (CALBC, www.
calbc.eu) under the 7th EU Framework Programme (ICT 2007.4.2).
Author details
1Department of Computational Linguistics, University of Zurich, Zürich,
Switzerland. 2European Bioinformatics Institute, Wellcome Trust Genome
Campus, Hinxton, Cambridge CB10 1SD, UK. 3NICTA Victoria Research Lab,
Melbourne VIC 3010, Australia. 4Linguamatics Ltd, 324 Cambridge Science
Park, Milton Road, Cambridge CB4 0WG, UK.
Received: 15 November 2012 Accepted: 25 July 2013
Published: 13 September 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2
http://www.jbiomedsem.com/content/4/1/2
SOFTWARE Open Access
OPPL-Galaxy, a Galaxy tool for enhancing
ontology exploitation as part of bioinformatics
workflows
Mikel Egana Aranguren1,2*, Jesualdo Toma´s Ferna´ndez-Breis3, Chris Mungall4, Erick Antezana5,
Alejandro Rodr´?guez Gonza´lez2 and Mark D Wilkinson2
Abstract
Background: Biomedical ontologies are key elements for building up the Life Sciences Semantic Web. Reusing and
building biomedical ontologies requires flexible and versatile tools to manipulate them efficiently, in particular for
enriching their axiomatic content. The Ontology Pre Processor Language (OPPL) is an OWL-based language for
automating the changes to be performed in an ontology. OPPL augments the ontologists toolbox by providing a
more efficient, and less error-prone, mechanism for enriching a biomedical ontology than that obtained by a manual
treatment.
Results: We present OPPL-Galaxy, a wrapper for using OPPL within Galaxy. The functionality delivered by OPPL
(i.e. automated ontology manipulation) can be combined with the tools and workflows devised within the Galaxy
framework, resulting in an enhancement of OPPL. Use cases are provided in order to demonstrate OPPL-Galaxys
capability for enriching, modifying and querying biomedical ontologies.
Conclusions: Coupling OPPL-Galaxy with other bioinformatics tools of the Galaxy framework results in a system that
is more than the sum of its parts. OPPL-Galaxy opens a new dimension of analyses and exploitation of biomedical
ontologies, including automated reasoning, paving the way towards advanced biological data analyses.
Background
Among the various steps that a typical life-sciences
research cycle comprises, information extraction from
raw data (and its dissemination to the community)
remains as one of the most relevant ones. New biological
insights are generated by combining information from dif-
ferent sources with the expertise of scientists. Neverthe-
less, integrating information and generating knowledge
out of it is still a challenging task, as the information is fre-
quently captured in computationally opaque formats and
dispersed over the Web in resources with idiosyncratic
schemas.
The Semantic Web [1] aims to overcome the issue of
computationally opaque and disperse information in the
*Correspondence: mikel.egana.aranguren@upm.es
1Ontology Engineering Group, School of Computer Science, Technical
University of Madrid (UPM), Boadilla del Monte, 28660, Spain
2Biological Informatics Group, Centre for Plant Biotechnology and Genomics
(CBGP), Technical University of Madrid (UPM), Pozuelo de Alarco´n, 28223, Spain
Full list of author information is available at the end of the article
Web with a set of technologies and standards defined by
the W3C: RDF [2], SPARQL [3] and OWL [4]. Therefore,
these standards are increasingly used by the Life Sciences
community to integrate information (RDF), to query it
(SPARQL), and to axiomatically encode consensus knowl-
edge about such information in ontologies (OWL), in the
so-called Life Sciences Semantic Web [5].
Biomedical ontologies are essential for the Life Sciences
Semantic Web since they offer computationally process-
able and often Web-oriented representations of agreed-
upon domain knowledge. The Gene Ontology (GO) [6]
stands out as one of the most intensely curated and
used biomedical ontologies; other important biomedi-
cal ontologies can be found at the Open Biological and
Biomedical Ontologies Foundry [7], a project that hosts
biomedical ontologies that follow certain design prin-
ciples (reusability, orthogonality, etc.). Additionally, the
National Center for Biomedical Ontology (NCBO) offers
access to biomedical ontologies through BioPortal [8],
including a set of Web Services.
© 2013 Egana Aranguren et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the
Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,
distribution, and reproduction in any medium, provided the original work is properly cited.
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 2 of 16
http://www.jbiomedsem.com/content/4/1/2
Current biomedical ontologies support a broad range of
tasks: axiomatically rich ontologies are used for intense
automated reasoning [9], axiomatically lean ontologies
act as vocabularies for Linked Data [10], and typically
other functions in between [11]. In order to fulfill such
functions, biomedical ontologies should be adapted to
fit scientists requirements, especially when reusing pre-
existing ontologies: addition or removal of axioms and
entities, inference in relation to external ontologies, selec-
tive materialisation of inferred axioms, complex querying,
and so forth.
Manipulating biomedical ontologies can be a labori-
ous task since they are regularly growing in size [12]
and axiomatic complexity [13]. Therefore, advanced tools
are needed for efficiently performing such manipulation
[14]. The Ontology Pre Processor Language (OPPL) [15]
offers the possibility of automating this kind of ontology
manipulation. By using OPPL, the ontologist can define
the intended manipulation in an OPPL script as a series
of additions or removals of axioms to be performed in a
concrete ontology. Therefore, the use of OPPL makes the
ontology manipulation process more efficient, sustainable
and less error-prone.
OPPL capabilities have already been demonstrated: it
has been used to build an ontology transformation service
[16] and for applying [17-20] or detecting [21] Ontol-
ogy Design Patterns (ODPs). Also, it is part of Populous,
an application for adding content from spreadsheets to
ontologies [22].
OPPLs versatility and functionality cannot be exploited
directly within the typical bioinformatics analyses. Galaxy,
a Web server for combining various genomic-oriented
tools into workflows [23], offers an ideal platform for
making OPPL part of bioinformatics analyses. There-
fore, we have developed OPPL-Galaxy, a tool to execute
OPPL scripts from within Galaxy. OPPL-Galaxy enhances
OPPLs functionality, i.e. automated ontology manipula-
tion, by providing the possibility of dynamically sending
OPPLs output, that is, an improved ontology, to other
Galaxy tools (and making OPPL capable of consuming
ontologies as input from other Galaxy tools).
This paper presents an overview of OPPL-Galaxys
design and implementation, including tested use cases
that provide a basis for creating more complex analy-
ses. OPPL-Galaxy is also compared to other tools and its
benefits and limitations are discussed.
Implementation
OPPL
OPPL implements its own syntax: an extension of the
Manchester OWL Syntax (MOS) [24] that includes key-
words like ADD (to add an axiom), REMOVE (to remove an
axiom), SELECT (to select entities), and so on. An OPPL
script defines a query and some actions that should be
performed against the retrieved entities (see Basic usage
use case in Section Results). A query can combine vari-
ables (to be bound by a set of named entities) and actual
named entities of the target ontology (OWL classes, prop-
erties, or individuals). An important constraint in OPPL
specifies that every variable must resolve to a group of
named entities (or none), not an anonymousOWL expres-
sion, to ensure that queries can be answered. The follow-
ing types of queries can be defined inOPPL (all the queries
mix variables with OWL expressions):
 OWL queries that exploit automated reasoning.
 Syntactic OWL queries that only work with the
asserted axioms.
 Queries that use a regular expression to match
annotation values like rdfs:label.
The actions are based on the addition or removal of
axioms of any complexity to/from entities retrieved by
the query (OWL classes, properties, or instances). Once
an OPPL script has been defined, the OPPL engine is
passed this script and the ontology to be modified. The
OPPL engine, in turn, modifies the ontology according to
the changes defined in the OPPL script, generating a new
ontology (Figures 1 and 2).
Galaxy
Galaxy offers an open, Web-based platform for perform-
ing genomic analyses [23]. In Galaxy several tools can
be combined, ranging from simple data manipulations
(e.g. text manipulation) to complex analyses (e.g. statistical
analysis of Next-Generation Sequencing data). Such a tool
orchestration can be executed from within a single Web
interface: the output of a tool can be sent to other tools
as input, easing the construction of workflows by combin-
ing recurrent tasks. Moreover, a history of all performed
actions is stored, so the analyses can be reproduced at any
time and shared with other users. Galaxy workflows can
be built from the users history and shared. Finally, the
workflows can be migrated to other systems, like other
Galaxy servers or myExperiment [25].
Apart from its functionality and ease of use, another
appealing feature of Galaxy is its extensibility, allowing
a straightforward integration of command-line tools: the
only requirement is to create an XML file containing a
description of the tools Web interface and inputs/outputs
[26].
OPPL-Galaxy
OPPL can be executed through the graphical interface of
Prote´ge´ [27] and Populous. Despite those possible means
of manipulating ontologies, OPPL cannot be used as
part of a workflow, limiting the possibilities of including
other bioinformatics analysis tools, unless a tailored Java
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 3 of 16
http://www.jbiomedsem.com/content/4/1/2
Figure 1 Toy ontology for OWL rendering convention. Toy ontology to illustrate the convention for representing abstract OWL structures in
Figures depicting use cases. Above, the ontology is rendered using MOS; below, the ontology is rendered with the same convention as in Figures 2,
5, 6, 10 and 14. In those Figures, however, names of OWL entities are not included in the ontologies, since OPPL scripts act on absract structures (any
axiomatic pattern that matches the query). Solid circle: named class; dotted circle: anonymous class; dot: named individual; solid arrow:
subClassOf axiom; dotted arrow: triple (relation between individuals); line ending in circle: restriction (the small circle points to the filler class;
there is no distinction between necessary and necessary/sufficient conditions)a.
program is written using the OPPLAPI. OPPL-Galaxy fills
that gap by offering an enhanced version of OPPL that
can be used in combination with other Galaxy tools. To
that end, an OPPL wrapper was developed as a mediator
between Galaxy and both the OPPL 2 API [28] and the
OWL API [29] (Figure 3).
OPPL-Galaxy takes as input a target ontology and an
OPPL script: both artefacts are uploaded to Galaxy by the
user or produced as output by another Galaxy tool. It gen-
erates a new ontology that has been changed according to
the instructions defined in the OPPL script, thus axioms
are added or removed. The OPPL-Galaxy Web interface
presents the following options (Figure 4):
 Target ontology: the input ontology that will be
modified by the OPPL script. Since OPPL-Galaxy
Figure 2 OPPL pipeline. The OPPL engine takes an ontology (circle group on the left) and an OPPL script (dotted square) as inputs, and performs
the changes defined by the OPPL script on the input ontology, thereby generating a new output ontology (modified ontology, on the right).
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 4 of 16
http://www.jbiomedsem.com/content/4/1/2
Figure 3 OPPL-Galaxy architecture. The inner circle represents the OPPL wrapper and the outer one Galaxy. Galaxy manages the data and
parameters that will be passed to the OPPL wrapper. In order to pass, for instance, an ontology to the OPPL wrapper, the ontology must be first
uploaded to Galaxy (or passed to it from the output of another Galaxy tool). Also, Galaxy manages the output of the OPPL wrapper: it can be
redirected to other Galaxy tools or downloaded and saved as a standalone file. The OPPL wrapper coordinates the OPPL API (to parse the OPPL
script and execute it), the OWL API (to read/write ontologies from stdin/to stdout and perform changes), and the chosen reasoner (to perform
inferences).
relies on the OWL API for loading and saving
ontologies, it can load ontologies in the following
formats: OBOF [30], OWL (RDF/XML, OWL/XML,
Functional OWL Syntax, MOS), Turtle, and KRSS.
 OPPL script: a flat file containing the OPPL script
that, when executed, will perform the desired
changes in the target ontology. This file may be
created by using the Prote´ge´ OPPL plugin via the
OPPL text editor (with autocompletion), the OPPL
script builder, or the OPPL macros tab (see the OPPL
manual [31] for details on how to create OPPL
scripts).
 Output format: the format of the output ontology,
either OBOF or OWL (RDF/XML).
 Choose reasoner: the reasoner to be used for
performing the inference, Pellet [32], HermiT [33],
FaCT++ [34], or Elk [35].
The output ontology can be reused as input for other
Galaxy tools like ONTO-toolkit [36], or downloaded from
the Galaxy Web interface so that it can be used outside
Galaxy, for example with Prote´ge´ or OBO-Edit [37].
OPPL-Galaxy includes various modules with diverse
functionality, apart from executing OPPL scripts. Addi-
tionally, other tools are exploited as part of the use cases
(NCBO-Galaxy [38], SPARQL-Galaxy, GO::TermFinder).
See Table 1 for details.
Figure 4 OPPL-Galaxy Web interface. The OPPL-Galaxy Web interface is displayed in the middle pane. In the left pane, a list of standard Galaxy
tools is shown; in the right pane, a sample of a history of the executed tasks is shown.
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 5 of 16
http://www.jbiomedsem.com/content/4/1/2
Table 1 OPPL-Galaxy distribution and related Galaxy tools
OPPL-Galaxy bundle
OPPL Executes OPPL scripts
OWL Query Perform DL (Description Logics) queries against OWL ontologies, returning a list of named
entities that satisfy the query
OPPL Query Perform OPPL queries, thus, queries that mix MOS with variables
Inference Add the inferred axioms to the input ontology as asserted axioms, generating a new ontology
that includes all the axioms
Merge Resolves the import axioms and adds the imported ontology to the input ontology file
NCBO-Galaxy bundle
The NCBO-Galaxy bundle includes modules for retrieving ontologies, extracting subtrees from ontologies,
search for terms in ontologies, annotate texts against ontologies, etc. using NCBO Web services. See [38] for details
SPARQL-Galaxy bundle
SPARQL-Galaxy includes a tool for performing SPARQL queries on an OWL (RDF/XML) ontology;
it can be downloaded from the Galaxy Tool Shed (http://toolshed.g2.bx.psu.edu), under Ontology manipulation.
Galaxy-OBO
Galaxy-OBO [39] is a fork of Galaxy that includes wrappers for common tools like GO::TermFinder [40]
This table provides a detailed list of the OPPL-Galaxy tools and other tools that are executed in the workflows of the use cases.
Results
This section provides use cases not only demonstrating
the utility of OPPL-Galaxy but also showing, through
examples, how to use it. The use cases are described in
detail in [41]. All the use cases are provided as Galaxy
workflows for users to be able to execute them without
having to rebuild the use case from scratch. The URLs of
the workflows are summarised at Table 2.
Basic usage
The OPPL-Galaxy bundle includes a simple OPPL script
for testing purposes that works with the test ontology
also included in the bundle (Figure 5). The OPPL script
is described as follows to help the reader understand the
remainder of the use cases (more OPPL examples can be
found at the OPPL scripts collection [42]):
1 ?agent:CLASS,
2 ?process:CLASS
3 SELECT ?agent SubClassOf participates\_in
some ?process
4 WHERE ?agent != Nothing
5 BEGIN
6 ADD ?agent SubClassOf participates\_in
only ?process
7 END;
Table 2 Galaxy workflows for reproducing the use cases
Name Galaxy workflow
Basic usage http://biordf.org:8090/u/mikel-egana-aranguren/w/basic-usage-1
Ontology debugging and
evaluation?
http://biordf.org:8090/u/mikel-egana-aranguren/w/ontology-debugging-and-evaluation
Complex querying of GO http://biordf.org:8090/u/mikel-egana-aranguren/w/complex-querying-of-go
Expansion of gene product
annotations through GO
structure
http://biordf.org:8090/u/mikel-egana-aranguren/w/expansion-of-gene-product-annotations-through-go-structure
Selective extraction of modules
from GO for term enrichment
http://biordf.org:8090/u/mikel-egana-aranguren/w/selective-extraction-of-modules-from-go-for-term-enrichment
OWL TBox to ABox
transformation for assisting
SPARQL queries
http://biordf.org:8090/u/mikel-egana-aranguren/w/owl-tbox-to-abox-transformation-for-assisting-sparql-queries
The name of the use case (as per section name) is provided in the left column; the URL of the Galaxy workflow is provided in the right column. In order to execute a
workflow, the datasets (ontologies, OPPL scripts, GAFs, etc.) must be taken from the history (http://biordf.org:8090/u/mikel-egana-aranguren/h/oppl-galaxy-use-
cases-for-jbs) or the workflow can be reproduced manually with the same datasets, by uploading them. The workflow Ontology debugging and evaluation obtains
the ontologies directly from NCBO services. For detailed instructions, see http://wilkinsonlab.info/OPPL-Galaxy. All the workflows can be reproduced in a local Galaxy
installation; in order to do so, the workflows and datasets can be downloaded from http://biordf.org:8080/JBSusecases.tar.gz.
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 6 of 16
http://www.jbiomedsem.com/content/4/1/2
Figure 5 Basic usage. The OPPL engine takes the target ontology and OPPL script as inputs, and generates a new ontology changed according to
the OPPL script. The OPPL script queries the reasoner for a class with a certain restriction (SELECT ... WHERE clause, blue) and adds another
restriction to the retrieved class (ADD clause, red).
Lines 1 and 2 show the declaration of two variables
(?process and ?agent) and their type (CLASS). These
variables represent (sets of ) OWL classes. Then, line 3
introduces a SELECT clause, which is processed by OPPL
and sent to the reasoner asking for the classes that are sub-
classes of the anonymous expression participates in
some ?process: the expression is written in MOS
and it mixes named entities of the ontology (the prop-
erty participates in) with variables (?process and
?agent, representing sets of classes). Later, in line 4,
the classes retrieved as members of the variable ?agent
are checked for satisfiability (?agent != Nothing).
Finally, the axiom SubClassOf participates in
only ?process is added (ADD) to the input ontology,
resolving ?agent and ?process to all the classes that
have been bound and combinations thereof.
Ontology debugging and evaluation
Ontology debugging (the process of fixing defects in an
ontology) can be a daunting activity, especially when the
ontology the scientist is working with has not been devel-
oped in-house and/or if it presents a complex axiomati-
sation over many entities. OPPL-Galaxy can be used for
detecting and fixing certain structures that are consid-
ered bad practice (antipatterns) or at least suspicious. The
detection of antipatterns also offers a picture of the ontol-
ogy: it can be used to evaluate the overall structure of
the ontology as one of the criteria to judge its quality.
OPPL-Galaxy provides a means of defining antipatterns as
test units that can be run automatically against a set of
ontologies, as part of Galaxy workflows.
The notion of antipatterns in ontologies has already
been introduced [43,44]. For example, [44] mentions
using the OWL universal restriction (only) without any
other restriction on the same property (e.g. some) as a
potential antipattern (exclusive universal). This is due to
the fact that, the only restriction, on its own, can be
trivially satisfied by an unsatisfiable (empty) class, e.g. A
subclassof p only (B and C) can be satisfiable
even when B disjointWith C, since the semantics of
only state that if there is a relation, it must be to (B and
C), or none: (B and C) is empty and therefore would
satisfy the none case.
The exclusive universal structure can be easily detected
in, for example, BioPAX [45], by the following OPPL script
(Figure 6):
1 ?target:CLASS,
2 ?prop:OBJECTPROPERTY,
3 ?filler:CLASS
4 SELECT ASSERTED ?target SubClassOf ?prop
only ?filler
5 WHERE FAIL ?target SubClassOf ?prop some
?filler
6 BEGIN
7 ADD ?target SubClassOf !OnlyBadPractice-
Result
8 END;
This script detects the exclusive universal structureb
and adds all the classes that present it as subclasses of
OnlyBadPracticeResult, a class created on the fly
if it does not exist in the ontology (! symbol). Note the
use of the ASSERTED keyword (only the asserted axioms,
not the inferred ones, are taken into account: the rea-
soner is deactivated for querying in order to improve
performance) and the FAIL keyword (negation as failure,
which is out of OWL semantics, is used to detect absent
existential restrictions).
The ontology can also be simply queried, without mod-
ifying it, by using the OPPL-Query tool (See Table 1 and
Figure 7):
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 7 of 16
http://www.jbiomedsem.com/content/4/1/2
Figure 6 Ontology debugging and evaluation. This script detects any class that has a universal restriction without an existential restriction
(dotted blue line). It adds a subClassOf OnlyBadPracticeResult axiom (red arrow) to any matching class.
1 ?target:CLASS,
2 ?prop:OBJECTPROPERTY,
3 ?filler:CLASS
4 SELECT ASSERTED ?target SubClassOf ?prop
only ?filler
5 WHERE FAIL ?target SubClassOf ?prop some
?filler
The exclusive universal structure can also be modi-
fied by adding an existential restriction to every universal
restriction:
1 ?target:CLASS,
2 ?prop:OBJECTPROPERTY,
3 ?filler:CLASS
4 SELECT ASSERTED ?target SubClassOf ?prop
only ?filler
5 WHERE FAIL ?target SubClassOf ?prop some
?filler
6 BEGIN
7 ADD ?target SubClassOf ?prop some
?filler
8 END;
Even though the exclusive universal structure might
be considered as a legitimate modelling decision, it is
recommendable, to make sure there is no trivially sat-
isfiable classes, to add existential restrictions on the fly
(and possibly to make entities disjoint), apply reason-
ing to detect trivially satisfiable classes, and then remove
the existential restrictions again. Such procedure can be
automatically performed using OPPL-Galaxy. An alter-
native would be to check the consistency of the filler,
e.g. ?filler subClassOf owl:Nothing, with the
reasoner activated, instead of checking for the exclusive
universal structure [46].
More antipatterns can be found in the collection pre-
sented in [43]:
 Logical Antipatterns (LAP): modelling errors that are
detectable by an automated reasoner, e.g. unsatisfiable
classes.
 Non-Logical Antipatterns (NLAP): modelling errors
that are not detectable using a reasoner, usually
created by the developer due to a misunderstanding
of the language semantics (the logical consequences
of the axioms stated in the ontology).
 Guidelines (G): alternative, simpler axiomatic
expressions of the same knowledge.
Synonym Of Equivalence (SOE) is an example of a
NLAP. Such type of antipattern describes the situation in
which two classes are declared as being equivalent and
both pertain to the same ontology (i.e., they have not
been imported). Generally, that means that the devel-
oper intends to model a synonym, which should be an
rdfs:label string, as a whole class. Such structure
can be easily detected, for example, in the NIF Gross
Anatomy ontology [47], using the following script (which
also removes the non-desired structure):
1 ?target:CLASS,
2 ?filler:CLASS
3 SELECT ASSERTED ?target equivalentTo
?filler
4 BEGIN
5 REMOVE ?target equivalentTo ?filler
6 END;
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 8 of 16
http://www.jbiomedsem.com/content/4/1/2
Figure 7 OPPL query tool.Web interface of the OPPL query tool.
We do not claim that these structures (exclusive uni-
versal in BioPAX and SOE in NIF Gross Anatomy) are
erroneous per se. We rather state that, according to the
experience of the authors of [43,44], and ours, they are
modelling practices that may yield unexpected results
when automated reasoning is applied downstream. There-
fore, a scientist who might reuse those ontologies should
be aware of the existence of the mentioned antipatterns.
OPPL-Galaxy is a straightforward, powerful and flexi-
ble tool to detect antipatterns en masse when executed
as a Galaxy workflow: a scientist can have a collection of
antipatterns of her choice ready to be applied in any ontol-
ogy she wants to reuse (any antipattern can be defined by
her, since OPPL is, roughly, a superset of OWL). The full
process can be automated, defining once what ontologies
to obtain and then adding antipatterns to the collection
as needed. Once the workflow has been executed and
the antipatterns detected in the target ontology, she can
decide if the ontology meets her requirements. Addition-
ally, OPPL-Galaxy can be used to modify the ontologies
that do not meet her requirements, within the same work-
flow.
Complex querying of GO
OPPL-Galaxy can be combined with other Galaxy-
enabled tools to build advanced workflows such as the one
shown in Figures 8 and 9. This workflow can be used by a
scientist to pose a complex question against GO, namely
What are the proteins that act on processes that involve
hepatocytes and are part of or regulate other biological
processes?. Posing such a complex question requires dif-
ferent steps that can be performed with OPPL and stored
for further analysis with the help of Galaxy.
The workflow executes the OPPL query tool and the
Galaxy tool for comparing two data sets (included in
the standard Galaxy distribution, in Join, subtract and
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 9 of 16
http://www.jbiomedsem.com/content/4/1/2
Figure 8 Complex querying of GO (as shown in Galaxy). OPPL-query workflow for quering GO against GAFs. The result is a list of proteins of
interest.
group). Thus, this workflow combines Galaxy tools to
retrieve exactly the proteins that the scientist defined in
her plain-english query, which is translated into amachine
interpretable form, as discussed below.
The OPPL script queries GO for the terms that have
Hepatocyte as part of their names and that are related,
via part of or regulates, to a biological process:
1 ?hepatocyte\_process:CLASS,
2 ?hepatocyte\_process\_label:CONSTANT =
MATCH(".?hepatocyte.+"),
3 ?part\_of\_or\_regulates:OBJECTPROPERTY
4 SELECT ?hepatocyte\_process.IRI label
?hepatocyte\_process\_label,
5 ?hepatocyte\_process subClassOf
?part\_of\_or\_regulates some
GO\_0008150
Then, the Galaxy tool for comparing two data sets is
used to extract the proteins involved in the resulting pro-
cesses of interest, using the GO terms as keys against
a Gene Association File (GAF) [48]. The result of this
comparison is a list of the protein identified as of interest.
This workflow demonstrates some of the main advan-
tages provided by OPPL-Galaxy: on one hand, this type
of analysis can only be performed, effectively, with OPPL
(see below). On the other hand, the unique capabilities of
OPPL are enhanced due to the fact that they are executed
Figure 9 Complex querying of GO (details). Detailed depiction of the workflow shown in Figure 8.
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 10 of 16
http://www.jbiomedsem.com/content/4/1/2
within Galaxy: the process can be repeated with any new
version of GO or GAFs, it can be shared with other sci-
entists, combined with other tools, and modified or ran in
parallel with minimum effort.
OPPL enables a unique set of capabilities for
analysing ontologies. It can mix, for instance, text
manipulation (in this case the regular expression
(".?hepatocyte.+")) and automated reasoning (in
this case subPropertyOf axioms, and subClassOf
and part of transitivity) as part of the same query.
It also enables the ability to refer to groups of entities
via variables, a feature which is outside the standard
OWL semantics, unless explicit axioms are codified
into the ontology (e.g. equivalent property axioms):
part of and regulates are represented by the
same variable ?part of or regulates, includ-
ing the subproperties negatively regulates and
positively regulates, due to the OWL semantics
(subPropertyOf).
Expansion of gene product annotations through GO
structure
GO annotations are provided independently of the ontol-
ogy itself, in GAFs. However, being able to access gene
products linked to GO through annotations is a useful
feature for queries and other analyses [49]. One of the
tools that can be used to merge GAFs with GO is OORT
(OBO Ontology Release Tool) [50]: it offers, for a given
ontology version, the possibility of checking its syntactic
and semantic quality, before releasing it. It also includes
the functionality to transform GAFs into ontologies,
in doing so linking, in the same ontology, gene prod-
ucts with their GO terms. This gives the possibility of
directly exploiting the structure of GO against the gene
product data: For example, if gene product G is capable
of function F and F is part of P (as per GO structure),
then G is also capable of G. Such semantic expansion
of gene product information can be performed using
OPPL-Galaxy, providing an ontology generated by OORT
that includes the link between gene products and their
GO terms as input. For example, the relations of the
gene product Atu0514 (subClassOf has prototype
some (actively participates in some
chemotaxis on or near host involved in
symbiotic interaction)) can be expanded with
the following script (this use case was obtained from [51],
see Figure 10):
?process:CLASS,}
?parent\_process:CLASS,
?gene:CLASS
SELECT ?gene subClassOf RO\_0002214 some
(RO\_0002217 some (?process and
BFO\_0000050 some ?parent\_process))
WHERE ?parent\_process != GO\_0008150
BEGIN
ADD ?gene SubClassOf RO\_0002217 some
?parent\_process
END;
This script queries the ontology and expands any gene
product - GO term relation according to the partonomy
hierarchy. As a result, the new axioms for Atu0514 read as
follows:
Atu0514 subClassOf
actively participates\_in some interspecies
interaction between organis}
actively participates\_in some multi-
organism process
actively participates\_in some symbiosis,
encompassing mutualism through parasitism
This new ontology can be used for further analyses.
Figure 10 Expansion of gene product annotations through GO structure. This workllow starts from an OWL ontology that includes GAF
information, produced by OORT. The script detects the structure ?gene subClassOf RO 0002214 some (RO 0002217 some
(?process and BFO 0000050 some ?parent process)) (Simplified depiction) and adds a new restriction to every matching class.
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 11 of 16
http://www.jbiomedsem.com/content/4/1/2
Selective extraction of modules from GO for term
enrichment
A typical use for GO is to perform an over-representation
analysis of genes expressed in micro-array experiments,
also known as enrichment analysis. To that end, a module
or subset from GO is usually extracted, as recommended
in [36], so that the statistical values of the analysis could be
sounder (i.e., the bias that might be introduced by consid-
ering other modules is diminished since the gene product
space is smaller).
OPPL-Galaxy can be combined with OWL-Query-
Galaxy to extract a module (Figure 11). The extent
of such module can specified with OPPL-Galaxy, for
example by adding transitivity to the regulates
object property (as a result the module holds more
terms):
1 ?regulates:OBJECTPROPERTY
2 SELECT ASSERTED negatively\_regulates
SubPropertyOf ?regulates
3 BEGIN
4 ADD Transitive ?regulates
5 END;
The resulting ontology can be later queried with the
OWL-Query-Galaxy tool (also part of OPPL-Galaxy, see
Figure 12), to obtain the module, i.e. a list of GO terms,
that can be then used to perform the enrichment analysis
by using other Galaxy tools like GO::TermFinder:
(regulates some GO\_0007049) or
GO\_0007049
OPPL performs, in this case, the same function as
ONTO-toolkit but in a more flexible way. Another
advantage of this procedure is that it can be executed
every time GO is updated, i.e., scientists can easily extract
different modules with a few clicks, and compare them
using Galaxy tools.
OWL TBox to ABox transformation for assisting SPARQL
queries
Making SPARQL queries against TBox axioms of an
RDF/XMLOWLontology is awkward. OWLpunning (see
bellow) can be used to add an instance to every class and
be able to do succinct SPARQL queries while retaining
the original TBox semantics [52] (However, the resulting
ontology has new semantics due to the addition of ABox
assertions).
OWL punning is a feature provided by OWL 2 that
makes it possible for different entities to share the same
URI [53]. The punned entities that share the same URI
are differentiated by the reasoner using their axiomatic
context. Punning can only be used within precisely
defined limits: for instance, the same URI cannot be
shared by both a class and a data type property.
Therefore, to have both classes (for DL or OWL syn-
tactic queries) and individuals (for more comfortable
SPARQL queries), it makes sense to add, for every class,
an individual with the same URI, i.e. to use OWL punning
in the ontology. The following OPPL script can be used for
such a task (Figures 13 and 14):
1 ?x:CLASS,
2 ?y:INDIVIDUAL = create(?x.RENDERING)
Figure 11 Selective extraction of modules from GO for term enrichment (as shown in Galaxy). In this workflow a reduced GAF is obtained by
querying GO (i.e., extracting a module) and comparing the retrieved GO terms with the GO terms from the GAF. The resulting reduced GAF is used
to perform an enrichment analysis with GO::TermFinder.
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 12 of 16
http://www.jbiomedsem.com/content/4/1/2
Figure 12 OWL query tool.Web interface of the OWL query tool.
3 SELECT ?x SubClassOf Thing
4 WHERE ?x != Nothing, ?x != Thing
5 BEGIN
6 ADD ?y Type ?x
7 END;
By applying this simple script a punned ontology can be
quickly obtained: the script adds an individual as a mem-
ber of each class, with the same URI as the class, except
in the case of owl:Thing and owl:Nothing (line 4). It
is worthy noting that the RENDERING keyword in OPPL
refers to the rendering method used in Prote´ge´ 4 for enti-
ties: URI fragment, rdfs:label, QName, etc. (OPPL-
Galaxy uses the default, URI fragment). As a result, an
ontology in which each class has an individual with the
same URI is obtained. An RDF triple for every existential
restriction can be added to the punned ontology by exe-
cuting the following script (using the punned ontology as
input):
1 ?x:CLASS,
2 ?y:INDIVIDUAL,
Figure 13 OWL TBox to ABox transformation for assisting SPARQL queries (as shown in Galaxy). In this workflow two OPPL scripts are used:
the first one adds an instance to every class with the same URI and the second one adds an RDF triple for every existential restriction.
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 13 of 16
http://www.jbiomedsem.com/content/4/1/2
Figure 14 OWL TBox to ABox transformation for assisting SPARQL queries (details). Detailed depiction of the workflow shown in Figure 13.
3 ?z:CLASS,
4 ?w:INDIVIDUAL,
5 ?p:OBJECTPROPERTY
6 SELECT ASSERTED ?x SubClassOf ?p some ?z,
7 ASSERTED ?y Type ?x, ASSERTED ?w Type ?z
8 WHERE ?x != Nothing, ?x != Thing
9 BEGIN
10 ADD ?y ?p ?w
11 END;
This script will only work for existential restrictions,
i.e. it will not transform universal restrictions to triplesc.
Therefore, it will completely transform an ontology that
only presents existential restrictions, like GO. By using
such scripts sequentially in a Galaxy workflow, a ready-
to-use (OWL) RDF representation can be obtained to be
submitted to a Galaxy tool for executing SPARQL queries
(Table 1).
Discussion
One of the most important applications of OPPL is the
axiomatic expansion of an existing ontology. The defi-
nition of complex modelling made by an ontologist is
expanded, through the script execution, to different parts
of the ontology itself, saving in this way time and effort.
Such complex modelling can be stored in a script, which
can be reused at any time in order to (re)apply precisely
defined ontology patterns. Thus, OPPL abstracts away
the repetitive task of implementing common axiom pat-
terns found in ontologies and parameterising them with
concrete entities. Using OPPL when building ontologies
ensures the repeatability and style consistency of themod-
elling since such modelling is performed by executing
a script. Moreover, OPPL allows experimentation with
modelling choices: design options can be stored in a
script and by simply executing such script and inspect-
ing the results, the ontologist can rapidly try out complex
modelling and revise decisions as necessary.
OPPL provides a simple, flexible and expressive lan-
guage for maintaining ontologies as well as for keeping
track of the changes themselves. By using OPPL, in con-
trast to a direct OWL API implementation, users profit
from less complex scripting that does not require the
overhead of a Java program, yet retains the complexity
and capabilities needed to work with OWL ontologies in
a fully expressive manner. OPPL scripting is not a sim-
ple task; nonetheless, OPPL scripts do afford a unique
programmatic way to manipulate OWL ontologies in a
pattern based manner that avoids many of the issues with
manual crafting of individual axioms.
The only tool that offers a functionality similar to OPPL
is Thea [54]. Thea, however, requires the ontologist to be
able to program her axioms in Prolog. OPPL, in contrast,
requires a knowledge of its scripting syntax, which is
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 14 of 16
http://www.jbiomedsem.com/content/4/1/2
an extension of MOS (which in turn is an OWL syn-
tax designed for human use and readability) based on an
intuitive set of keywords (such as ADD, REMOVE, etc.).
Therefore, the OPPL syntax learning curve is not that
steep for an ontologist who is familiar with the OWL syn-
tax. On the other hand, Galaxy enhances the mentioned
features of OPPL by embedding them in an infrastructure
that provides persistence, shareability and reproducibil-
ity of analyses, combination with other tools, etc. To the
best of our knowledge, there is no other Galaxy tool com-
parable to OPPL-Galaxy, except ONTO-toolkit. However,
ONTO-toolkit offers different, complementary function-
alities to the ones offered by OPPL-Galaxy and as a matter
of fact they can be combined to obtain meaningful results.
OPPL-Galaxy is a seminal prototype that is regularly
improved. The following list collects a set of prospective
features:
 Loading local imported ontologies by uploading
them to Galaxy (Currently only remote URIs are
resolved).
 Load ontologies by their URI.
 Configurable querying and rendering (URI fragment,
rdfs:label, QName, etc.).
 Standalone OPPL assertions processing (e.g. ADD
phagocyte subClassOf cell).
 Support for OWLlink [55] and RACER [56] reasoners.
 Other output formats apart from RDF/XML.
 In the case of the inference module, support for more
inferences like data property assertions, different
individuals assertions, etc.
 A tool for wrapping the ontology modularisation
function of the OWL API.
Performance might be an issue while working with
OPPL-Galaxy [18], since automated reasoning on espe-
cially large, complex biomedical ontologies is usually
resource demanding [57], even considering that OPPL-
Galaxy will normally work in a server with considerable
memory. As performance typically depends on the imple-
mentation of the automated reasoners, it is expected to
improve in the future, since reasoners are becoming more
efficient. Also, Galaxy can used in a cloud computing
setting such as Amazon EC2 [58].
Conclusions
The success of the application of the Semantic Web
technologies in Life Sciences not only relies on build-
ing ontologies and fine-tuning or setting standards, but
also on augmenting the scientists toolbox with tools that
can be easily plugged into frequently-used data analy-
sis environments such as Galaxy. Galaxy facilitates the
combination of several bioinformatics tools within a sin-
gle Web interface. Since OPPL-Galaxy can be used as
part of the Galaxy framework as an ontology manipula-
tion tool, it can be exploited in combination with other
Galaxy tools. That is, precisely, what sets OPPL-Galaxy
apart from other ontology tools that offer similar func-
tionality: it can be used with the actual data and tools
that life scientists use on a daily-basis, rather than in iso-
lation. By embedding tools like OPPL in genomic science
frameworks like Galaxy, the user awareness of such type of
application of the semantic technologies in Life Sciences
could increase, thus enabling more sophisticated analyses
of biomedical information.
The OPPL syntax extends that of OWL with a set of
intuitive keywords; therefore, the learning curve of any
userminimally fluent inOWL should be relatively shallow.
This means that OPPL-Galaxy provides a powerful and
(indirectly) familiar tool for automating ontology curation
processes that otherwise would need considerable human
resources and/or might produce incomplete or erroneous
results. The OPPL scripts described in the results section
are relatively simple, yet they show how users could ben-
efit from this tool to enhance their ontology development
and exploitation tasks, like debugging, rewriting and per-
forming axiomatic enrichment via ODPs. Specially in the
case of ODPs, a well-known ontology engineering prac-
tice, OPPL-Galaxy offers the ideal setting for their applica-
tion, since such ODPs can be shared as ready-to-execute
Galaxy workflows, saving time and effort. More complex
OPPL scripts would undoubtedly yield even greater bene-
fits, particularly if combined in workflows (e.g. debugging
and rewriting sequentially and sending the output to other
Galaxy tools).
Examples of Galaxy workflows that combine differ-
ent OPPL scripts with other Galaxy tools are provided
in the use cases Complex querying of GO, Selective
extraction of modules from GO for term enrichment,
and OWL TBox to ABox transformation for assisting
SPARQL queries. Other sophisticated analyses can be
performed with workflows exploiting OPPL-Galaxy, like
more fine-grained axiomatic enrichment of biomedical
ontologies [18,59-61]. The diversity and functionality of
Galaxy workflows involving OPPL-Galaxy depend only on
the user.
In summary, OPPL-Galaxy offers the possibility of
automating ontology manipulations in a reproducible,
versatile, persistent and shareable fashion, within a con-
text in which the result of such manipulations can be sent
directly to other tools in order to further build or enhance
analysis workflows. Therefore, OPPL-Galaxy should, on
the one hand, be of interest for the life scientists that
exploit ontologies to analyse biomedical information, and,
on the other hand, for bio-ontologists that continuously
maintain ontologies and are concerned by their quality.
Egana Aranguren et al. Journal of Biomedical Semantics 2013, 4:2 Page 15 of 16
http://www.jbiomedsem.com/content/4/1/2
Endnotes
aStrictly following this convention would result in restric-
tions being represented as lines going out of dotted circles
(A condition in an OWL class is the anonymous class
formed by the individuals that have the relation). However
restrictions have been simplified, omitting the anonymous
class, for the sake of clarity.
bThis script detects any case in which a universal restric-
tion is used in the absence of an existential restriction.
Therefore, it would (wrongly) flag as an instance of the
antipattern, for example, a universal restriction and an
exactly restriction used together. A more thorough
script is feasible but out of the scope of this paper.
cThe reason for not including universal restrictions is that,
in the case of GO, only existential restrictions are present
in the ontology; nothing prevents the user from adding a
further statement so as to also capture universal restric-
tions, but in the case of GO no entities would be retrieved.
Availability and requirements
 Project name: OPPL-Galaxy.
 Project home page: http://wilkinsonlab.info/OPPL-
Galaxy. We provide a public instance of Galaxy with
OPPL-Galaxy installed on it, including Galaxy tools
related to the use cases (ONTO-toolkit,
NCBO-Galaxy, Annotation, SPARQL-Galaxy):
http://biordf.org:8090. The Galaxy bundle for local
installation can be downloaded at http://toolshed.g2.
bx.psu.edu/, under the category Ontology
manipulation. The bundle includes the software itself
(along with the necessary third-party libraries and
XML tool files), sample scripts and ontologies, and
instructions on installation and usage.
 Operating system(s): it is recommended that
OPPL-Galaxy be deployed in a UNIX-based machine
(GNU/Linux, Mac OS X, BSD, etc.) since it uses
standard UNIX redirection (MSWindowsTM is not
officially supported by Galaxy).
 Programming language: Java and Python.
 Other requirements: a working Galaxy installation
is needed (http://galaxy.psu.edu/).
 License: General Public License (http://www.gnu.
org/copyleft/gpl.html). Source available at the Galaxy
tool shed mercurial repository (http://toolshed.g2.bx.
psu.edu/repos/mikel-egana-aranguren/oppl).
Abbreviations
DL: Description Logics; BioPAX: Biological Pathway Exchange; GAF: Gene
Association File; GO: Gene Ontology; KB: Knowledge Base; MOS: Manchester
OWL Syntax; NCBO: National Center for Biomedical Ontology; NLAP:
Non-Logical Antipattern; OBO: Open Biomedical Ontologies; ODP: Ontology
Design Pattern; OORT: OBO Ontology Release Tool; OPPL: Ontology Pre
Processor Language; OWL: Web Ontology Language; RACER: Renamed ABox
and Concept Expression Reasoner; RDF: Resource Description Framework;
SOE: Synonym Of Equivalence; SPARQL: SPARQL Protocol and RDF Query
Language; URI: Uniform Resource Identifier; W3C: World Wide Web
Consortium; XML: eXtensible Markup Language.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MEA developed OPPL-Galaxy, designed some use cases, and contributed to
the text. JTFB and EA tested the use cases, created the Web for them and
contributed to the text. CM contributed to the use cases and to the text. ARG
developed SPARQL-Galaxy. MDW revised the manuscript and supervises the
Biological Informatics Group and funding. All authors read and approved the
final manuscript.
Acknowledgements
Mikel Egana Aranguren and Mark D Wilkinson are funded by the Marie Curie
Cofund programme (FP7) of the European Union, including the charges for
publishing this manuscript. Jesualdo Toma´s Ferna´ndez-Breis is funded by the
Spanish Ministry of Science and Innovation (TIN2010-21388-C02-02) and
co-funded by the FEDER Programme. Chris Mungall is funded by National
Human Genome Research Institute (NHGRI) [5P41HG002273-09]. Alejandro
Rodr´?guez Gonza´lez is funded by the Isaac Peral Programme.
Luigi Iannone and Ignazio Palmisano offered extensive technical help for the
development of OPPL- Galaxy. The SWAT4LS 2011 reviewers and attendees
offered helpful comments that contributed to the current version of the
manuscript and OPPL-Galaxy.
Author details
1Ontology Engineering Group, School of Computer Science, Technical
University of Madrid (UPM), Boadilla del Monte, 28660, Spain. 2Biological
Informatics Group, Centre for Plant Biotechnology and Genomics (CBGP),
Technical University of Madrid (UPM), Pozuelo de Alarco´n, 28223, Spain.
3School of Computer Science, University of Murcia (UM), Murcia, 30100, Spain.
4Genomics Division, Lawrence Berkeley National Laboratory, Berkeley, CA,
94720, US. 5Department of Biology, Norwegian University of Science and
Technology (NTNU), Høgskoleringen 5, N-7491, Trondheim, Norway.
Received: 2 April 2012 Accepted: 27 December 2012
Published: 4 January 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Jensen et al. Journal of Biomedical Semantics 2013, 4:42
http://www.jbiomedsem.com/content/4/1/42DATABASE Open AccessThe neurological disease ontology
Mark Jensen1, Alexander P Cox1, Naveed Chaudhry2, Marcus Ng2, Donat Sule2, William Duncan1, Patrick Ray1,
Bianca Weinstock-Guttman2, Barry Smith1,2, Alan Ruttenberg3, Kinga Szigeti2 and Alexander D Diehl2*Abstract
Background: We are developing the Neurological Disease Ontology (ND) to provide a framework to enable
representation of aspects of neurological diseases that are relevant to their treatment and study. ND is a
representational tool that addresses the need for unambiguous annotation, storage, and retrieval of data associated
with the treatment and study of neurological diseases. ND is being developed in compliance with the Open
Biomedical Ontology Foundry principles and builds upon the paradigm established by the Ontology for General
Medical Science (OGMS) for the representation of entities in the domain of disease and medical practice. Initial
applications of ND will include the annotation and analysis of large data sets and patient records for Alzheimers
disease, multiple sclerosis, and stroke.
Description: ND is implemented in OWL 2 and currently has more than 450 terms that refer to and describe
various aspects of neurological diseases. ND directly imports the development version of OGMS, which uses BFO 2.
Term development in ND has primarily extended the OGMS terms disease, diagnosis, disease course, and
disorder. We have imported and utilize over 700 classes from related ontology efforts including the Foundational
Model of Anatomy, Ontology for Biomedical Investigations, and Protein Ontology. ND terms are annotated with
ontology metadata such as a label (term name), term editors, textual definition, definition source, curation status,
and alternative terms (synonyms). Many terms have logical definitions in addition to these annotations. Current
development has focused on the establishment of the upper-level structure of the ND hierarchy, as well as on the
representation of Alzheimers disease, multiple sclerosis, and stroke. The ontology is available as a version-controlled
file at http://code.google.com/p/neurological-disease-ontology along with a discussion list and an issue tracker.
Conclusion: ND seeks to provide a formal foundation for the representation of clinical and research data pertaining
to neurological diseases. ND will enable its users to connect data in a robust way with related data that is
annotated using other terminologies and ontologies in the biomedical domain.Background
Neurology is concerned with diseases related to the func-
tioning of the nervous system. These diseases may present
acutely or chronically, have transient or progressive courses,
and affect a variety of anatomical regions and cell types.
They are realized through diverse mechanisms, including
cell-autonomous disorders, unregulated protein aggregation,
autoimmune conditions, or vascular pathology [1,2]. There
are a variety of ways to classify neurological diseases, such as
by symptomology or pathology. Several classificatory sys-
tems and terminologies are currently available, such as* Correspondence: addiehl@buffalo.edu
Equal contributors
2Department of Neurology, University at Buffalo School of Medicine and
Biomedical Sciences, 701 Ellicott Street, Buffalo, NY 14203, USA
Full list of author information is available at the end of the article
© 2013 Jensen et al.; licensee BioMed Central
Commons Attribution License (http://creativec
reproduction in any medium, provided the orNIFSTD, ICD-10, SNOMED CT, MeSH, and the Disease
Ontology [3-7]. Although some of these classificatory sys-
tems and terminologies are widely used for purposes such
as billing and medical messaging, they do not satisfy current
best practices in ontology development and do not provide
the level of detail needed for precise annotation of and rea-
soning over data.
Definitions for terms in medical terminologies are often
ambiguous or vague, and often the meanings of such
terms are defined for use only in one particular domain.
Clinical or research data is thus rarely encoded in a way
that will allow the linking of various types of data together
in a coherent fashion that will also support computation.
Yet, computer-aided reasoning has become increasingly
important to medical research due, in part, to the vast
amount of data being generated [8]. This means that it isLtd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Jensen et al. Journal of Biomedical Semantics 2013, 4:42 Page 2 of 10
http://www.jbiomedsem.com/content/4/1/42more important than ever for data to be annotated in a
clear and unambiguous manner in order to facilitate inte-
gration across diverse sources and thereby maximize the
benefit of scientific investigation. The Open Biomedical
Ontology (OBO) Foundry promotes the development of
consistent formal ontologies based on a common upper-
level reference ontology to address this need [9]. The suc-
cess of the Gene Ontology has shown how a controlled
and properly curated ontology can benefit and extend re-
search in medicine [10].
The Neurological Disease Ontology (ND) is being de-
veloped as an extension of the Ontology for General
Medical Science (OGMS). OGMS represents entities in
the domain of medicine and disease and addresses the
need to integrate biomedical data [11,12]. The OGMS
framework consists of approximately 100 terms that de-
scribe fundamental aspects of medicine, such as dis-
order, diagnosis, disease course, clinical encounter and
syndrome. OGMS utilizes a template for generating un-
ambiguous textual and formal logical definitions for
terms in an effort to promote the integration of scientific
data. ND seeks to produce definitions in a similar man-
ner within the domain of neurological disease.
We are developing the Neurological Disease Ontology
to provide a framework for the representation of key as-
pects of neurological disease. ND is compliant with
OBO Foundry principles and builds upon the paradigm
established by OGMS. ND is an ongoing collaborative
project that aims to establish a formal structure to en-
able precise representation of a variety of neurological
diseases and disorders. Our ultimate goal is to accurately
represent for each disease its molecular, genetic and en-
vironmental origins, the processes involved in its eti-
ology and course of progression, as well as its clinical
presentation and phenotypes, including associated signs,
symptoms, syndromes, diagnostic criteria, treatment, and
testing methods. ND has three initial areas of focus:
1. Alzheimers disease and diseases resulting in
dementia
2. multiple sclerosis and demyelinating diseases
3. stroke and cerebrovascular events
Construction and content
Development of ND
ND is being developed using both a top-down and
bottom-up approach to term creation. We use the top-
down approach to create high-level classes in ND by
analyzing the types of neurological diseases presented in
clinical literature and determining how to classify them
within the ontology. This approach involves determining
what additional core entities should be part of the ND
framework in order to allow for a more complete repre-
sentation of the domain, including relationships betweenupper-level classes. NIF_Dysfunction was used as start-
ing point for developing the class hierarchy for neuro-
logical disease in ND [13]. The bottom-up approach
involves reviewing primary research articles, review arti-
cles, texts and other sources to inform the development
of ND. Domain experts and clinical collaborators provide
constructive feedback and guide decision making on con-
troversial material. We examine samples of available data
sources, such as forms for recording clinical history, func-
tional assessments, and diagnostic charts, to let the data
inform term creation and refinement. This approach has
provided the majority of classes and definitions in ND.
The ontology currently contains approximately 450
ND classes. Primary work has been done under the
OGMS classes disease, diagnosis, disease course, and
disorder. Figure 1 shows a subset of classes illustrating
a portion of the asserted is_a hierarchy between ND,
OGMS, and the Basic Formal Ontology (BFO) [14]. In
particular, we have built a large hierarchy of subclasses
of neurological disease as illustrated in Figure 2. The
182 subclasses of neurological disease are further subdi-
vided into additional classifications such as neurodegen-
erative disease, demyelinating disease, and autoimmune
neurological disease. We assert a single inheritance hier-
archy of disease and capture the complexity of many
neurological diseases through the use of logical definitions
that capture additional disease characteristics as onto-
logical differentia (see below). We have also imported over
700 classes from external ontologies, such as the Protein
Ontology (PR), Foundational Model of Anatomy (FMA),
and Ontology for Biomedical Investigations (OBI) [15-17].
ND terms are annotated with ontology metadata such as a
label (term name), term editors, definition, definition
source, curation status, and alternative terms (synonyms).
JOURNAL OF
BIOMEDICAL SEMANTICS
Zhang et al. Journal of Biomedical Semantics 2013, 4:33
http://www.jbiomedsem.com/content/4/1/33RESEARCH Open AccessNetwork-based analysis of vaccine-related
associations reveals consistent knowledge with
the vaccine ontology
Yuji Zhang1*, Cui Tao2*, Yongqun He3, Pradip Kanjamala1 and Hongfang Liu1Abstract
Background: Ontologies are useful in many branches of biomedical research. For instance, in the vaccine domain,
the community-based Vaccine Ontology (VO) has been widely used to promote vaccine data standardization,
integration, and computer-assisted reasoning. However, a major challenge in the VO has been to construct
ontologies of vaccine functions, given incomplete vaccine knowledge and inconsistencies in how this knowledge
is manually curated.
Results: In this study, we show that network-based analysis of vaccine-related networks can identify underlying
structural information consistent with that captured by the VO, and commonalities in the vaccine adverse events
for vaccines and for diseases to produce new hypotheses about pathomechanisms involving the vaccine and the
disease status. First, a vaccine-vaccine network was inferred by applying a bipartite network projection strategy to the
vaccine-disease network extracted from the Semantic MEDLINE database. In total, 76 vaccines and 573 relationships
were identified to construct the vaccine network. The shortest paths between all pairs of vaccines were calculated
within the vaccine network. The correlation between the shortest paths of vaccine pairs and their semantic similarities
in the VO was then investigated. Second, a vaccine-gene network was also constructed. In this network, 4 genes were
identified as hubs interacting with at least 3 vaccines, and 4 vaccines were identified as hubs associated with at least 3
genes. These findings correlate with existing knowledge and provide new hypotheses in the fundamental interaction
mechanisms involving vaccines, diseases, and genes.
Conclusions: In this study, we demonstrated that a combinatorial analysis using a literature knowledgebase, semantic
technology, and ontology is able to reveal important unidentified knowledge critical to biomedical research and public
health and to generate testable hypotheses for future experimental verification. As the associations from Semantic
MEDLINE remain incomplete, we expect to extend this work by (1) integrating additional association databases to
complement Semantic MEDLINE knowledge, (2) extending the neighbor genes of vaccine-associated genes, and
(3) assigning confidence weights to different types of associations or associations from different sources.Background
Vaccines have been one of the most successful public
health interventions to date, with most vaccine-preventable
diseases having declined in the United States by 95% to
99% [1]. However, vaccine development has become more
difficult as more complex organisms become vaccine
targets. In recent years, drug repositioning has increased* Correspondence: Zhang.Yuji@Mayo.edu; Cui.Tao@uth.tmc.edu
1Division of Biomedical Statistics and Informatics, Department of Health
Sciences Research, Mayo Clinic, Rochester, MN 55905, USA
2School of Biomedical Informatics, University of Texas Health Science Center
at Houston, Houston, TX 77030, USA
Full list of author information is available at the end of the article
© 2013 Zhang et al.; licensee BioMed Central
Commons Attribution License (http://creativec
reproduction in any medium, provided the orrapidly and achieved a number of successes for existing
drugs, such as sildenafil [2] and thalidomide [3]. By
definition, drug repositioning is the process of finding
new uses outside the scope of the original medical
indications for existing drugs or compounds [4]. In
2009, more than 30% of 51 new medicines and vaccines
were developed on the basis of previously marketed
products. This suggests that drug repositioning has
drawn great attention from both industry and academic
institutions [5]. However, many drug repositioning
examples were based on serendipitous discoveries [6]
or on observable clinical phenotypes, without systematicLtd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Zhang et al. Journal of Biomedical Semantics 2013, 4:33 Page 2 of 8
http://www.jbiomedsem.com/content/4/1/33identification of new targets. Recent research has shown
that bioinformatics-based approaches can aid in reposi-
tioning drugs based on the complex relationships among
drugs, diseases, and genes [7].
In recent years, high-throughput biological data
and computational systems biology approaches have
provided an unprecedented opportunity to understand
disease etiology and its underlying cellular subsystems.
Biological knowledge, such as drug-disease networks
and biomedical ontologies, have accelerated the devel-
opment of network-based approaches to understanding
disease etiologies [8,9] and drug actions (network
pharmacology) [10,11]. Such approaches could also be
applied to vaccine research, aiming to investigate vaccine-
related associations derived from public knowledgebases,
such as PubMed. For example, Vaccine Ontology (VO)
based literature mining research last year studied all
potential gene interactions associated with fever alone
or both fever and vaccine [12]. This study focused on
the retrieval of gene-gene associations identified on the
basis of their direct interactions in the context of fever
and vaccine. The centrality-based network approach
[13] evaluated the level of importance for each gene in
the extracted gene interaction network. Novel gene
interactions were identified to be essential in fever- or
vaccine-related networks that could not be found before.
A similar VO and centrality-based literature mining
approach was used to analyze a vaccine-associated
interferon ? gene interaction network [14].
Ball et al. [15] compiled a network consisting of 6,428
nodes (74 vaccines and 6,354 adverse events) and more
than 1.4 million interlinkages, derived from the Vaccine
Adverse Event Reporting System. This network demon-
strated a scale-free property, in which certain vaccines
and adverse events acted as hubs. Such network analysis
approaches complement current statistical techniques
by offering a novel way to visualize and evaluate vaccine
adverse event data. However, the relationships among
different vaccines in the context of vaccine-vaccine and
vaccine-gene networks have not been well studied. A
systematic investigation of such relationships will improve
understanding of how vaccines are related to each other
and whether such information can complement existing
knowledge, such as the VO, therefore providing possible
directions for future drug-repositioning.
To analyze the possible commonly shared protective
immunity or adverse event mechanisms among different
vaccines, it is critical to study all possible vaccine-vaccine
and vaccine-gene associations using network analysis
approaches. The hypotheses behind this are 2-fold: (1)
if 2 vaccines have a coupling relationship with common
disease(s) or gene(s), they are linked in the vaccine
network; and (2) the closer 2 vaccines are in the vaccine
network, the more similar they are in the context of aliterature knowledgebase, such as Semantic MEDLINE
[16]. In this study, we proposed a network-based approach
to investigate the underlying relationships among vaccines
in the context of the vaccine-related network derived
from Semantic MEDLINE. The distances of the vaccines
were further compared with their semantic similarities
in the VO. The results demonstrated that the structure
information in the vaccine network is consistent with
that captured by the VO. Such network-based analysis
can serve as an independent data resource to construct
and evaluate biomedical ontologies. In addition, the
vaccine-gene network was constructed also on the basis
of Semantic MEDLINE information, in which important
vaccine-related genes were identified and investigated by
the VO and related independent resources. Both analyses
demonstrated the potential to serve as the basis for future
drug discovery and repositioning. The results of this
study will establish associations that may kindle hypothesis
generation about drug repositioning.
The rest of the paper is organized as follows. Section 2
introduces the data resources and the proposed network-
based framework. Section 3 illustrates the results gen-
erated from each step in the proposed computational
framework. Section 4 provides a thorough discussion of
the results and concludes the paper.Materials and methods
In this section, we first describe the data resources and
preprocessing method in this work. We then introduce
our proposed network-based approach for investigating
vaccine-related associations derived from literature
abstracts in PubMed. The evaluation of the discovered
vaccine-vaccine and vaccine-gene relationships is based
on the VO hierarchy and logical definitions. Figure 1
illustrates the steps of the proposed approach.Data sources and preprocessing
Data resources
In this study, we used Semantic MEDLINE as the data
resource to build the networks. Semantic MEDLINE [16]
is a National Library of Medicineinitiated project, which
provides a publicly available database that contains
comprehensive resources, with structured annotations
for information extracted from more than 19 million
MEDLINE abstracts. Since Semantic MEDLINE is a
comprehensive resource that contains heterogeneous data
with different features extracted, our previous research
has reorganized this data source and optimized it for
informatics analysis [17]. Using the Unified Medical
Language System semantic types and groups [18], we
extracted unique associations among diseases, genes, and
drugs and represented them in 6 Resource Description
Framework (RDF) graphs. In this study, we used our
Figure 1 Overview of the proposed framework. The proposed method consists of 3 steps: (1) extraction of vaccine-related associations from
Semantic MEDLINE using ontology-based terms; (2) network-based analyses to identify vaccine-vaccine associations and vaccine-gene associations;
and (3) evaluation of inferred vaccine-vaccine and vaccine-gene relationships using VO hierarchical structure and literature validation.
Zhang et al. Journal of Biomedical Semantics 2013, 4:33 Page 3 of 8
http://www.jbiomedsem.com/content/4/1/33optimized Semantic MEDLINE RDF data as the data
source to perform network analysis for vaccine-related
networks.
Our RDF-based Semantic MEDLINE resource currently
contains 843,000 disease-disease, 111,000 disease-gene,
1,277,000 disease-drug, 248,000 drug-gene, 1,900,000
drug-drug, and 49,000 gene-gene associations. Since
this resource contains high-level terms (eg, gene, protein,
disease) that are not useful for network analysis, we
manually filtered out these terms. For disease terms, we
used only those terms that are included in International
Classification of Diseases, Ninth Revision (ICD 9). For
gene terms, we included only those terms that have an
Entrez Gene ID.
Data extraction
We identified the associations relevant to vaccines only.
Specifically, vaccine terms were identified on the basis of
Systematized Nomenclature of MedicineClinical Terms
(SNOMED CT) (http://www.ihtsdo.org/snomed-ct). All
the terms under the SNOMED CT term Vaccine (CUI:
C0042210) were first extracted. Manual review by 3 experts
further removed common terms (eg, bacteria, vaccine) and
animal vaccine terms.
Network analysis of vaccine network
Projection of bipartite vaccine-disease network
In graph theory, a bipartite network is composed of 2
non-overlapping sets of nodes and links that connect 1
node in the first node set with 1 node in the second
node set. The properties of bipartite networks are often
investigated by considering the 1-mode projection of the
bipartite network. The 1-mode projection network can
be created by connecting 2 nodes in the same node set
if they have at least 1 common neighboring node in
the other node set. For instance, the vaccine-disease
association network is 1 bipartite network: vaccines and
diseases constitute 2 node sets, and links are generated
between vaccine and disease if they are associated inthe Semantic MEDLINE. Therefore, the vaccine-vaccine
network can be investigated by projecting vaccine-disease
associations to vaccine-vaccine associations, in which 2
vaccines are connected if they are associated with at
least 1 same disease. In this work, all links were generated
on the basis of associations extracted from Semantic
MEDLINE as described in the preceding section, Data
Sources and Preprocessing. A vaccine-vaccine network
was generated consisting of all the links identified in
vaccine-disease associations.
Network distance between vaccines
The distance between any two vaccines in the vaccine
network was calculated as the length of the shortest path
between them [19]. The hierarchical clustering analysis
was performed on the distance matrix of all vaccines
[20]. Compared to other clustering methods (e.g., k-mean
clustering), hhierarchical clustering doesnt need specify
the number of clusters in advance, generates small clusters
that are more biologically meaningful, and produces a
bottom-up hierarchical structure that is indicative of
similarities among variables.
A heatmap is an effective way to visually display multi-
variate data, which combines the use of color to distin-
guish the magnitude of measurements and dendrograms
to show clustering of variables [21]. Heatmaps are often
used for gene expression data to identify the similairty
among genes or samples. In this study, we used heatmaps
to investigate the hierarchical associations among vaccine
terms.
Analysis of vaccine-gene network
The vaccine-gene network was constructed by vaccine-
gene associations extracted from the drug-gene associa-
tions in our RDF-based data resource. The important
vaccine-related genes were identified by their significant
higher node degree compared with other vaccines or
genes in the same network. The Cytoscape tool [22] was
used to visualize the network. Cytoscape is an open-source
Zhang et al. Journal of Biomedical Semantics 2013, 4:33 Page 4 of 8
http://www.jbiomedsem.com/content/4/1/33platform for integration, visualization, and analysis of
biological networks. Its functionalities can be extended
through Cytoscape plugins. Scientists from different re-
search fields have contributed more than 160 useful
plugins so far. These comprehensive features allow us to
perform thorough network-level analyses, visualization
of our association tables, and integration with other
biological networks in the future.
Analysis of vaccine groups using VO
The community-based VO includes more than 4,000
vaccine-specific terms, namely, all licensed human and
veterinary vaccines currently used in the United States.
Logical axioms have been defined in the VO to represent
the relations among vaccine terms [14]. The Semantic
MEDLINE analysis uses SNOMED terms to represent
various vaccines. The VO has established automatic
mapping between SNOMED vaccine terms and VO
terms. On the basis of the mapping, we first extracted
all vaccine terms from the Semantic MEDLINE and
mapped to the VO. The ontology term retrieval tool
OntoFox [23] was then applied to obtain the hierarchies
of the total vaccines or subgroups of the vaccines iden-
tified in this study.
Results
The overall network view
In total, 76 vaccines, annotated by the SNOMED CT
term Vaccine (CUI: C0042210), were used to extract
related vaccine-disease and vaccine-gene associations
from the drug-disease and drug-gene association tables,
respectively. In the vaccine-disease network, there
were 1,038 nodes (104 vaccines and 934 diseases) and
1,693 vaccine-disease associations (Additional file 1).
In the vaccine-gene network, there were 170 nodes (85
vaccines and 85 genes) (Additional file 2) and 94
vaccine-gene associations. One vaccine network was
generated by the projection of the vaccine-disease
bipartite network, consisting of 76 vaccines and 573
associations (Additional file 3). This vaccine network
was then used to analyze the vaccine relationships. The
derived vaccine-gene network was also investigated by
the VO knowledge.
Analysis of vaccine network
Figure 2 shows a heat map of hierarchical analysis results,
providing direct visualization of potential vaccine-vaccine
associations. Here we selected 4 relatively large vaccine-
vaccine association groups on the diagonal from Figure 2
and explain them in detail:
Vaccine-vaccine association 1
This group contains 18 widely studied vaccines. Many
interesting results have been obtained from the analysisof this group of vaccine-disease-vaccine associations.
For example, the results from this group show that
influenza vaccines and rabies vaccines have been asso-
ciated with the induction of a severe adverse event,
Guillain-Barré syndrome [24,25]. Guillain-Barré syndrome
is a rare disorder in which a persons own immune system
damages the nerve cells, causing muscle weakness and
sometimes paralysis. This group also includes 5 other
vaccines associated with nervous system disorders, includ-
ing pertussis vaccine [26], diphtheria and tetanus toxoids
and pertussis (DTP) vaccine [27], hepatitis B vaccine [28],
chickenpox vaccine [29], and poliovirus vaccine [30,31].
As shown by a VO hierarchical structure layout (Figure 3),
these 7 vaccines belong to different bacterial and viral
vaccine groups. For instance, the DTP vaccine is a
combination vaccine that contains 3 individual vaccine
components, including a pertussis vaccine. DTP is asserted
in the VO as a subclass of diphtheria and tetanus toxoids
vaccine. In SNOMED, DTP is asserted as subclass of a
Diphteria + tetanus vaccine, which is a subclass of
tetanus vaccine and diphtheria vaccine. However, the
DTP term in SNOMED does not include any axiom
that makes the association of the vaccine to any of the
pathogens that causes the three diseases. Different from
SNOMED, the VO logically defines vaccines by associating
each vaccine to the pathogen organisms defined in the
NCBI Taxonomy Ontology. For example, a VO vaccine
for diphtheria has the following axiom definition: vaccine
immunization against microbe some Corynebacterium
diphtheriae. This logical definition links the VO vaccine
to the bacterium Corynebacterium diphtheriae defined in
the NCBOTaxon ontology with the ID: NCBITaxon_1717.
Since multiple inheritances are not used in the VO, an
inference applying an ontology reasoner was used to
infer that the DTP is also Bordetella pertussis vaccine
(ie, pertussis vaccine) (Figure 3). It is likely that the
association of the combination vaccine DTP with a
neurologic disorder is at least partially attributable to
the pertussis vaccine component.
Our study also identified many other diseases associated
with different vaccines. For example, 5 vaccines (eg,
pertussis vaccine) were found to be associated with various
types of antimicrobial susceptibility, and 8 vaccines
(eg, influenza vaccine) have been co-studied in patients
with the asthma condition. Because of the relatively
poor annotation of the vaccine data in the Semantic
MEDLINE system, the vaccines identified in the se-
mantic analysis were poorly classified. Our previous
study [32] also confirmed that the vaccine annotations
in VO are more granular than that in Semantic MED-
LINE or MeSH. The incorporation of the VO in the
study clearly classifies these vaccines, leading to better
understanding of the result of the Semantic MEDLINE
analysis.
Figure 2 The heat map of vaccine-vaccine associations. The shortest path matrix of all vaccine pairs was used to generate the heat map.
Each row (column) represents a vaccine term. The color scale represents the shortest path between any vaccine pair.
Zhang et al. Journal of Biomedical Semantics 2013, 4:33 Page 5 of 8
http://www.jbiomedsem.com/content/4/1/33Vaccine-vaccine association 2
This group of vaccines, including Q fever vaccine, parvo-
virus vaccine, and tick-borne encephalitis vaccine, is asso-
ciated with the common disease delayed hypersensitivity.
Delayed-type reactions may occur at days after vaccination
and often raise serious safety concerns. Delayed hypersen-
sitivity is not antibody mediated but rather is a type ofFigure 3 The VO hierarchical structure of the seven vaccines
associating with neurological disorder. A reasoning process
assigned the Diphtheria-Tetanus-Pertussis vaccine under Bordetella
pertussis vaccine. The Protégé-OWL editor 4.2 was used for the
figure generation.cell-mediated response. The study of common vaccines
and gene and pathway features related to the delayed
reaction will help to reveal the cause of a delayed-type
reaction and eventually prevent it. While these vaccines
are developed against different bacterial or viral diseases,
there may be similarities among the vaccines, such as
common vaccine ingredients (eg, adjuvant) and a shared
target in some common biological pathway in human.
Identification of these common features may indicate a
common cause of a delayed-type reaction.
Vaccine-vaccine association 3
This group of vaccines is associated with the common
disease mumps. The vaccines in this group include mumps
vaccine, measles, mumps, rubella, and varicella virus
vaccine, and DTP-Haemophilus b conjugate vaccine
(DTP-Hib). The first 2 vaccines protect against mumps.
The DTP-Hib vaccine was compared with a mumps
vaccine in a study [33].
Vaccine-vaccine association 4
This vaccine group consists of 7 vaccines (eg, Brucella
abortus vaccine and bovine rhinotracheitis vaccine) with
direct associations between them. They are all associated
with the common term calf in the literature abstracts.
Since calf disease is a synonym Scheuermann disease,
Zhang et al. Journal of Biomedical Semantics 2013, 4:33 Page 6 of 8
http://www.jbiomedsem.com/content/4/1/33these vaccines have all been linked to Scheuermann
disease because of the ambiguity of the Natural Language
Processing (NLP) process. This ambiguity can be improved
by future advancement in the disambiguity capacity of
the NLP tools.
Vaccine-gene network
In the vaccine-gene network (Additional file 4), many
genes were found to interact with different vaccines.
Four genes (TH1L, CD40LG, TFPI, and CD79A) have
been identified as interacting with at least 3 vaccines.
These 4 genes can be considered hub genes in the
vaccine-gene network. Among them, CD40LG (CD40
ligand) is closely associated with 5 vaccines: diphtheria
toxoid vaccine, cholera vaccine, tetanus toxoid vaccine,
chickenpox vaccine, and inactivated poliovirus vaccine
(Figure 4). CD40LG plays an important role in antigen
presentation and stimulation of cytotoxic T lymphocytes
[34]. CD40LG can also be used in rational vaccine adju-
vant design [35]. It is likely that all 5 of these vaccines
stimulate protective immunity through the same CD40LG-
mediated pathway. Our finding confirms the important
role of CD40LG and provides a possible hypothesis on
how different bacterial and viral vaccines stimulate a
protective immune response in the host. Another example
is CD79A (immunoglobulin-associated alpha, also known
as mb-1), a phosphoprotein that is a component of the
CD79a/CD79b dimer associated with membrane-bound
immunoglobulin in B cells. This dimer interacts with
the B-cell antigen receptor and enables the cell to respondFigure 4 A vaccine-gene subnetwork. The associations between vaccine
Rectangular purple nodes represent vaccines, and round yellow nodes repto the antigens on the cell surface [36]. The presence of
the CD79a suggests strong B-cellmediated antibody
response. Interestingly, both CD40LG and CD79A interact
with the same inactivated poliovirus vaccine. It is likely
that the inactivated poliovirus vaccine induces both
CD40LG-mediated cellular immune response and CD79A-
mediated B-cell antibody response.
In the gene-vaccine network, many vaccines also inter-
act with multiple genes. Four vaccines (diphtheria toxoid
vaccine, DTP vaccine, inactivated poliovirus vaccine,
and influenza virus vaccine) were found to interact with
at least 3 genes. According to the VO, diphtheria toxoid
vaccine protects against infection by Corynebacterium
diphtheriae. In addition to C diphtheriae, the DTP vaccine
also protects against infection by Clostridium tenani
and Bordetella pertussis. Both of these vaccines interact
with CD40LG and TH1L. So it is likely that these 2 pro-
teins specifically interact with the same C diphtheriae
component in diphtheria toxoid vaccine, but not with
the other components in the DTP vaccine.
Conclusions and future work
In this paper, we proposed a novel network-based ap-
proach to investigate vaccine relationships in the context
of a vaccine network extracted from abstracts from
the literature posted to PubMed. These investigations
of vaccine-vaccine, vaccine-disease, and vaccine-gene
networks demonstrated that such literature-based as-
sociations can be better analyzed using the VO and
such a combinatorial analysis is able to reveal thes and related genes were visualized by the Cytoscape tool [22].
resent genes.
Zhang et al. Journal of Biomedical Semantics 2013, 4:33 Page 7 of 8
http://www.jbiomedsem.com/content/4/1/33association patterns and new knowledge. The identified
vaccine-vaccine associations based on vaccine-disease
distance analysis were consistent with their VO categories
and often lead to the generation of new hypotheses.
Our studies identified some novel vaccine-vaccine rela-
tionships by discovering a group of vaccines associated
with some common diseases, as demonstrated in the heat
map analysis in the Results section. Because information
in the literature is incomplete, such vaccine-vaccine
associations need further validation in independent
databases or through future experimental studies. For
example, while our analysis revealed associations between
a group of vaccines and neurologic adverse events, the
evidence of these associations, although reported by some
PubMed abstracts, has not necessarily been commonly
acknowledged [37]. More analysis may be required for
clarification. As such relationships are further validated
by independent data resource such as Electronic Health
Record (EHR), they can complement current relationships
in VO and provide additional underlying targets for
vaccine repositioning in the future.
Extensions of this work include the following: (1) inte-
gration of more comprehensive vaccine-disease associ-
ation databases (e.g., the Vaccine Adverse Event Reporting
System) to construct more complete vaccine-related
networks; (2) generation of a vaccine-related gene network
by extending analysis to the neighbor genes of vaccine-
associated genes; (3) network-based investigation of the
relationships among vaccines and other drugs using
vaccine-drug associations; and (4) investigation of pos-
sible ways to improve the network by assigning metrics
such as weights or confidence rates to different types of
associations or associations from different sources.
Additional files
Additional file 1: The vaccine-disease network consisting of 1,038
nodes (104 vaccines and 934 diseases) and 1,693 vaccine-disease
associations.
Additional file 2: The vaccine-gene network consisting of 170
nodes (85 vaccines and 85 genes) and 94 vaccine-gene associations.
Additional file 3: The vaccine network consisting of 76 vaccines
and 573 associations.
Additional file 4: The cys file (Cytoscape format) of the vaccine-
gene network.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
YZ and CT led the study design and analysis, and drafted the manuscript. YH
contributed to the analysis of VO. YH and HL provided institutional support
and manuscript editing. All authors read and approved the final manuscript.
Acknowledgements
This project was supported by the National Institutes of Health grants
5R01LM009959-02 to HL, and R01AI081062 to YH.Author details
1Division of Biomedical Statistics and Informatics, Department of Health
Sciences Research, Mayo Clinic, Rochester, MN 55905, USA. 2School of
Biomedical Informatics, University of Texas Health Science Center at Houston,
Houston, TX 77030, USA. 3Unit of Laboratory of Animal Medicine, University
of Michigan, Ann Arbor, MI 48109, USA.
Received: 15 August 2013 Accepted: 4 November 2013
PROCEEDINGS Open Access
Automated Patent Categorization and Guided
Patent Search using IPC as Inspired by
MeSH and PubMed
Daniel Eisinger1,2*, George Tsatsaronis1, Markus Bundschus2, Ulrich Wieneke2, Michael Schroeder1
From Bio-Ontologies 2012
Long Beach, CA, USA. 13-14 July 2012
* Correspondence: daniel.
eisinger@biotec.tu-dresden.de
1TU Dresden, BIOTEC, Tatzberg 47/
49, 01307 Dresden, Germany
Abstract
Document search on PubMed, the pre-eminent database for biomedical literature,
relies on the annotation of its documents with relevant terms from the Medical
Subject Headings ontology (MeSH) for improving recall through query expansion.
Patent documents are another important information source, though they are
considerably less accessible. One option to expand patent search beyond pure
keywords is the inclusion of classification information: Since every patent is assigned
at least one class code, it should be possible for these assignments to be
automatically used in a similar way as the MeSH annotations in PubMed. In order to
develop a system for this task, it is necessary to have a good understanding of the
properties of both classification systems. This report describes our comparative
analysis of MeSH and the main patent classification system, the International Patent
Classification (IPC). We investigate the hierarchical structures as well as the properties
of the terms/classes respectively, and we compare the assignment of IPC codes to
patents with the annotation of PubMed documents with MeSH terms.
Our analysis shows a strong structural similarity of the hierarchies, but significant
differences of terms and annotations. The low number of IPC class assignments and
the lack of occurrences of class labels in patent texts imply that current patent
search is severely limited. To overcome these limits, we evaluate a method for the
automated assignment of additional classes to patent documents, and we propose a
system for guided patent search based on the use of class co-occurrence
information and external resources.
Background
As evidenced by a growing number of reports about various high-profile patent trials
in recent years, having the necessary information about all relevant competitor patents
can be vital to a companys interests. At the same time, current research results are
often first published in a patent and only afterwards in a journal. This makes patents
also a potentially valuable source for academic research, although to our knowledge,
most academic researchers are not using patents. The number of patent applications
continues to rise, reaching an all-time high of almost 2 million worldwide in 2010
alone, with the number of granted patents in that year also setting a new record with
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2013 Eisinger et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in
any medium, provided the original work is properly cited.
more than 900,000 grants [1]. As can be seen in Figure 1 that was adapted from the
World Intellectual Property Report 2011 [2] (published by the World Intellectual Prop-
erty Organization (WIPO) [3]), five of the six top patent offices have experienced
strong growth in the number of accepted patents over the last two decades, with Japan
being the sole exception. While the figure shows small declines for all offices except
China at the end of the last decade, these are believed to be consequences of the global
financial crisis. Despite the continued effects of the crisis, the declines are expected to
be temporary; this hypothesis is also supported by the record numbers mentioned
above [1].
It is therefore important to have systems for patent search that are both comprehen-
sive and accessible. The most natural way to search almost any document collection is
by using keywords. Unfortunately, there are some obstacles to the sole use of keywords
for searching patents. In particular, patent language is often extremely complicated.
Additionally, many companies use very unspecific vocabulary in order to make the
scope of their patents as broad as possible. At the very least, finding most or all rele-
vant keywords will require a large time investment.
International Patent Classification
As a consequence of the problems with keywords, professional searchers rely on the use
of classification information [4,5]. Patents are classified into hierarchical systems of cate-
gories by patent offices. The International Patent Classification (IPC) [6] is the most
common system - it is used by over 100 patent-issuing bodies worldwide [7] - and its
Figure 1 Number of patent applications per year for six top patent offices until 2010. With the exception
of Japan, all offices are showing strong growth over the last two decades, (adapted from World Intellectual
Property Report 2011)
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 2 of 23
hierarchy constitutes the base of other important systems such as the Japanese File
Index [8], the German Deutsche Feinklassifikation (DEKLA) [9] and the new Coop-
erative Patent Classification (CPC) [10] that is now used by the United States Patent and
Trademark Office (USPTO) as well as the European Patent Office (EPO). The IPC hier-
archy is divided into eight sections that correspond to very general categories such as
Human necessities (section A) or Chemistry/Metallurgy (section C). Each section is
made up of numerous classes (e.g., A61), and each class contains multiple subclasses
such as A61K. Each subclass is again divided into main groups (e.g., A61K 38/00), and
for most main groups there are additional subgroups such as A61K 38/17. In order to
improve readability, we will refer to all individual entries of the IPC as classes instead
of the correct term for the respective level of the hierarchy.
As this example shows, individual entries of the hierarchy are mainly represented by
alphanumeric codes. The corresponding definitions are complicated and often depend
on each other. The following list represents the complete definition tree for the sub-
group code from the example above.
 A
Human necessities
 A61
Medical or veterinary science; Hygiene
 A61K
Preparations for medical, dental or toilet purposes
 A61K 38/00
Medicinal preparations containing peptides
 A61K 38/16
Peptides having more than 20 amino acids; Gastrins; Somatostatins; Melanotropins;
Derivatives thereof
 A61K 38/17
from animals; from humans
This example illustrates the necessity to also consider the superordinate code defini-
tions in order to understand what kind of invention is represented by the given code.
It also shows that the code alone does not accurately represent the hierarchy in all
cases: While class 38/17 is directly subordinate to 38/16, there is no direct hierarchical
connection between classes 38/16 and 38/15 (definition: Depsipeptides; Derivatives
thereof). Finding the most relevant classification codes to be used for search therefore
constitutes a significant challenge, especially for users with little experience.
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 3 of 23
As a consequence of the high complexity of searching patents, major pharma compa-
nies employ patent professionals to relieve their scientists of this difficult and time-
consuming task. Their patent searches often combine keywords and classification
codes (and possibly additional metadata) in a single query. Many researchers without
access to such resources ignore patents in favor of more accessible scientific literature.
However, as we mentioned above, they thereby risk missing a lot of current research
results. This study intends to investigate ways for
1. assisting patent professionals in finding the relevant elements (keywords and clas-
sification codes) for their search queries more quickly and
2. enabling non-professionals to start using patents as an important additional infor-
mation source.
In order to test the validity of using patent classification information for these tasks,
we analyzed the code assignment in the patent domain and compared it with the assign-
ment of Medical Subject Headings to documents in the biomedical literature database
PubMed. Although PubMed has a considerably more narrow focus than the patents do,
we consider this comparison a useful approach for the following reasons: First, PubMed
represents (to our knowledge) the largest freely accessible collection of scientific docu-
ments (or more precisely, abstracts) indexed with a controlled vocabulary, making it a
natural target for our comparison of document annotations. Second, as we will describe
in the next subsection in more detail, the assigned terms are already used for improving
PubMed searches, mirroring our plan for the IPC codes. And third, although our patent
corpus contains patents from many different fields, we are mainly trying to improve
patent search for the biomedical domain.
Medical Subject Headings
The Medical Subject Headings (MeSH) are a controlled vocabulary thesaurus of bio-
medical terms curated by the National Library of Medicine (NLM). Similar to the
IPC, the hierarchy starts with 16 very broad categories such as Anatomy or
Organisms and gets much more specific in deeper hierarchical levels. The MeSH
terms are used in the biomedical literature database PubMed as a document index-
ing system, i.e., for annotating documents with relevant terms that describe their
content. PubMed users can therefore restrict their search to documents that have
been annotated with some very specific terms in which they are interested. On top
of that, MeSH terms are used to automatically improve the recall of PubMed
searches through query expansion: By mapping keywords from a search query to
MeSH terms, relevant documents are included in the search results even if they only
contain synonyms or hyponyms of the original keyword. That means that even
PubMed users who are completely unfamiliar with MeSH can benefit from the
search improvements it makes possible.
The use of MeSH for PubMed searches is just one example for a controlled vocabulary
that is assisting document search. Many text mining and document retrieval applications
rely on text annotations with terms from existing taxonomies or ontologies - either by
using existing manual annotations or by automatically assigning relevant terms. Systems
based on this principle include GoPubMed [11] as well as EBIMed [12] and its sister appli-
cation Whatizit [13]. All these systems are mainly or exclusively intended for use with
Medline/PubMed documents; there is no comparable system for patents. Consequently, it
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 4 of 23
is desirable to offer scientists an easier option to formulate patent queries that include
classification information. In order to provide such assistance, it is important to have a
clear understanding of the properties of both classification systems. In this paper, we
therefore investigate differences between the IPC and the established MeSH hierarchy and
their implications for patent search. As a solution to problems we discovered through our
analysis, we propose two approaches: a system for the automated assignment of additional
classes to patent documents and a guided patent search system that assists the user by
offering query expansion suggestions derived from class co-occurrence data or using exist-
ing knowledge from external sources.
Related work
The importance of MeSH for the biomedical field has led to extensive research: There
are mature approaches for automatically assigning MeSH terms to documents [14,15],
and MeSH terms are successfully used for query expansion [16]. MeSH has also been
used in combination with patents, e.g., for tagging diseases [17].
IPC-related research is much more limited than for MeSH, but scientific interest has
been growing over the last decade. Some publications explain the professional
approach to patent search and classification information [18,19] while others identify
problems and suggest solutions: Annies [20] points out that the limits of using key-
words for chemical searches necessitate the inclusion of classification information, but
warns that class assignments are incomplete. Parisi et al. [21] emphasize the same
point even more, saying that existing assignments may be subjective, incomplete or
inconsistent and sometimes even random. Despite these problems, Becks et al. [22]
report drastic recall improvements for patent retrieval from the inclusion of classifica-
tion information. Many publications cover methods for the use of existing assignment
information for prior art search [23].
The automated assignment of classes to patents is an important issue for all patent
offices. It is therefore not surprising that most of the initial research had direct con-
nections to patent offices such as the European Patent Office (EPO) [24] and WIPO
[25,26]. In later years, different workshops such as the Japanese NTCIR [27] and more
recently the CLEF-IP evaluation track [28] added patent categorization tasks. The
results from the published approaches vary depending on the hierarchical level that
was used: Trappey et al. [29] report precision values slightly above 0.9 for a small sub-
set of IPC subclasses and main groups, Tikk et al. [30] correctly identify up to 37% of
main groups, and Verberne et al. [31] reach an F1-score of 0.7 for the subclass level in
their best run. To our knowledge, there is only one prior effort to classify patents
down to the lowest level of the IPC: Chen et al. [32] report 36% accuracy for that diffi-
cult task. To date, there is no in-depth analysis of either hierarchy and no systematic
comparison of both hierarchies, although there are some papers dealing with MeSH
[33,34] or IPC [5,35-37] in a more general way.
Results and discussion
This section reports the results of our analysis of MeSH and IPC. In order to tackle
problems we discovered in this comparison, we then discuss our efforts to automati-
cally assign additional classes to patents and our investigation into possibilities for
assisting patent searchers in utilising the classification for their search.
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 5 of 23
Comparative analysis
Our analysis of MeSH and IPC can be divided into two parts: The first part concerns
the respective hierarchies and terms of the systems themselves, while the second part
examines their usage for document categorization. We analyzed the latter by collecting
classification information from all patent applications to the European Patent Office
(EPO) between 1982 and 2005 (over one million) as well as the annotations to all
PubMed documents published by early 2011 (over 20 million). Our analysis has the
goal of assisting patent search; we are therefore less interested in the reasons for any
discrepancies than in their implications for search. Table 1 summarizes some core
results of our analysis, and the following subsections give more detailed reports.
Hierarchies and terms
As Table 1 shows, the structural comparison of the hierarchies did not reveal any sig-
nificant differences: Their sizes are in the same range (about 70000 IPC classes and
54000 entries in the MeSH tree), they have almost the same depth (14 levels for IPC,
13 for MeSH) and the node distributions are similar (cf. Figure 2). The main difference
of the hierarchies is reflected in Table 1 in the distinction between the number of hier-
archy entries and unique entries: While each IPC class can only be directly subordinate
to exactly one other entry, MeSH allows its entries to have more than one father. This
means that the same MeSH term can occur in multiple places inside the hierarchy.
The terms on the other hand show two major differences:
 Emphasis on terms/concepts versus identifiers:
While MeSH assigns identifiers to its headings, the emphasis is clearly on the term
itself. The IPC on the other hand is first and foremost a collection of alphanumeric
codes which are signifying their place in the hierarchy. Unlike MeSH terms, these
codes do not give an uninformed user any useful information about the patents
that should be assigned to this class. This information is instead contained in addi-
tional class definitions that are more akin to MeSHs scope notes. As an example,
looking up the MeSH headings for a document about insects on PubMed will lead
the user to the term Insects, not its identifier D007313. However, a patent
about an immunoassay is assigned to class G01N 33/53 (or one of its subclasses),
Table 1 Comparative analysis MeSH vs. IPC. The hierarchical structures are similar, but
MeSH terms are shorter and more likely to occur in text. The number of MeSH
annotations per document far surpasses the number of classes per patent.
Property MeSH IPC
number of hierarchy entries 54095 69487
number of unique entries 26581 69487
number of hierarchy levels 13 14
average string length main labels/class definitions 18 50
string length longest main label/class definition 104 596
string length shortest main label/class definition 2 3
average number of synonyms 8 0
occurrence of class labels in text frequent very rare
average number of annotations per document 9 2
number of unique annotations 25646 56599
proportion of documents with multiple annotations 86% 53%
proportion of documents with related annotations (same hierarchy tree) 81% 46%
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 6 of 23
and the definition of this class has to be checked separately if the user does not
know it.
 Length of terms/occurrence in text:
As Table 1 shows, IPC definitions are usually much longer than MeSH terms. Most
of them are also considerably more abstract and complicated, and many of them
depend on their ancestor classes for a complete understanding, e.g., from animal;
from humans (class A61K 38/17; cf. section Background). Unlike MeSH, the IPC
also does not include any synonyms for the class definitions. All of these differ-
ences contribute to a very low probability of occurrence of class labels in text,
while MeSH terms occur frequently [38,39]. In order to quantify how rarely IPC
codes occur in text, we searched the complete texts of all 14600 documents from
our test corpus C73 (for a definition see Training Corpora) for their class defini-
tions. Less than 2% of the documents contained their respective definition, and
most of these hits were for class names that werent informative on their own (e.g.,
class C07K 14/47, from mammals).
As a consequence, IPC classes cannot be assigned to patents by simply extracting
them from text. This is one of the main reasons for the much more extensive use
of automated (pre-)annotation of PubMed documents compared to patents. One
possible approach to solving this problem is the assignment of classes using
machine learning methods, i.e., training a classifier on existing classification data to
predict assignments for new data.
Usage for document classification
IPC and MeSH are both used as classification/annotation systems for documents: all
patent applications are assigned at least one IPC code, and all articles in participating
journals are annotated with appropriate MeSH terms. As Figure 3 shows, the average
Figure 2 Terms/classes per hierarchy level. Both hierarchies expand in similar ways.
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 7 of 23
number of MeSH annotations per document is much higher than the average number
of IPC annotations per patent. Patents from our corpus had less than two assigned
IPC classes on average, while PubMed documents have almost nine MeSH terms (cf.
also Table 1). We also measured the diversity of IPC classes/MeSH terms assigned to
the same document as follows: Given a hierarchy H (in our case either MeSH or IPC)
and two entries a and b of the hierarchy, we define the distance between a and b as
the length of the shortest path between them in H. For a subset A of H consisting of
all annotations to a single document, we then define the maximum (minimum) anno-
tation distance as the maximum (minimum) over the pairwise distances of elements of
A. Since both MeSH and IPC are organized as a union of trees, we inserted one artifi-
cial root node into each hierarchy. For example, the most distant IPC classes assigned
to one patent from our corpus have the definitions Chemical Analysis and [Immu-
noassay] using isolate of tissue []. These classes are directly related, and the shortest
path connecting them in the IPC hierarchy has length 3. On the other hand, many
PubMed documents have annotations from different parts of the MeSH hierarchy: One
example document is annotated with Cyanides from the Chemicals and Drugs tree
as well as with Risk Factors from the Health Care tree. These terms show multiple
aspects of the document, and their distance in MeSH is 14. As these examples show,
the distance between terms hints at whether they belong to the same main tree of the
hierarchy; below, we examine this property of the annotation terms more precisely.
However, the distance also allows us to estimate the diversity of annotations from the
same tree. Figures 4 and 5 show the minimum and maximum differences for PubMed
as well as our patent corpus.
As Figure 4 shows, many PubMed documents are annotated with very similar MeSH
headings, in many cases even pairing a term with its direct parent (i.e., annotations have
distance 1). Arguably, this means that there is more redundancy contained in the MeSH
annotations, since the parent term is implicitly assigned together with the child term.
This situation is less common in our patent corpus, although there are also many
patents that are annotated with closely related classes. The analysis of maximum dis-
tances (Figure 5) has the opposite result: While the maximum distances for patents do
not differ too much from the minimum distances, the maximum distances for PubMed
documents are much larger. The higher number of PubMed annotations is certain to
Figure 3 Percentage of documents with number of annotations. The average number of MeSH
annotations per PubMed document is much higher than the number of IPC classes per patent.
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 8 of 23
play an important role in these differences. This result indicates that PubMed annota-
tions cover a broader spectrum of aspects than the assigned patent classes. In addition
to the path lengths between annotations, we examined the relations between annotations
more directly by checking how often terms were co-assigned with closely related terms.
Figure 6 shows the percentage of documents (among those with multiple annotations)
that were assigned pairs of annotations with , it is much more common for MeSH anno-
tations than for IPC codes to be co-assigned with sibling terms (i.e., terms that have the
same parent term), their direct parent terms or more distant ancestors. On the other
hand, it is very common for patents to get assigned to multiple classes from the same
tree: Including patents with just one annotation, over 83% of all patents are classified
into only one of the eight main sections of IPC. Since the main trees correspond to
extremely general domains such as Human necessities, we believe that some aspects of
many patents are not covered by the currently assigned classes.
Problems for IPC-based search
It could be argued that some of the described discrepancies are likely caused by differ-
ences in classification guidelines between patent offices and PubMed and may
Figure 4 Minimum hierarchical distances of multiple annotations assigned to the same document.
PubMed documents have more very closely related annotations than patents.
Figure 5 Maximum hierarchical distances of multiple annotations assigned to the same document. The
maximum distance is considerably larger for PubMed documents than for patents, as is the difference
between maximum and minimum distances.
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 9 of 23
therefore be intended. However, that does not change the fact that both IPC and
MeSH are used to improve search results on document corpora, and we believe that
the use of IPC for that purpose comes with two serious disadvantages:
 Complexity of the classification system:
The complexity of the IPC terms causes significant problems for non-professional
patent searchers. On the other hand, the exclusive use of keywords for searching
patent databases often leads to bad results due to the complicated language used in
patents.
 Sparse class assignments:
The low number of class assignments in combination with the relatively close rela-
tion of co-assigned classes indicates that relevant aspects of many patents are not
covered by the existing class assignments. Consequently, the recall of patent
searches using the classification may be lower than expected.
Given these disadvantages, patent search engines should offer additional functionality
for helping the user find the required results. Since the class definitions are needed to
understand the meaning of the class codes, the system must include easy access to
them. Additionally, since many definitions depend on their ancestor classes, the engine
should give the user an easy overview over the relevant parts of the hierarchy. Unfortu-
nately, many popular existing engines such as Espacenet [40], Google Patents [41] and
FreePatentsOnline [42] do not display this basic information on the same page as the
patent.
Assigning additional classes to patents
In the last section, we hypothesized that the low average number of class assignments
to patents results in recall problems for patent search. The most straightforward way
of dealing with this problem would be the assignment of additional classes, but due to
the high number of patents as well as the high complexity of the classification system,
this can only be done automatically.
Figure 6 Hierarchical relationships of terms assigned to the same document. PubMed documents have
considerably more closely related annotations, but the percentage of documents with annotations from
the same main tree is almost equal for patents.
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 10 of 23
As we described in the Related Works section, there have been multiple published
approaches for the assignment of IPC codes to patents. These approaches are usually
restricted to higher levels of the hierarchy such as the class or subclass level [29,31] or
the main group level [29,30]. The WIPO has also made a patent categorization tool
available, offering users the possibility to have documents categorized to any of these
levelS [43]. To our knowledge, there is only one prior effort to classify patents down to
the lowest level of the IPC [32]. While all previous approaches were solely focused on
automatically recreating the existing assignments, it is our goal to find additional rele-
vant classes that the patent was not originally assigned to. Consequently, we evaluate
our method both for its ability to recreate assignments and the quality of additional
class proposals.
Our system is based on the approach that was used by [15] for the automated assign-
ment of MeSH terms to PubMed documents. After using the Maximum Entropy
approach to build one binary classifier for each class, each learned model is applied to
all documents that are supposed to be categorized. For all classifiers that put the docu-
ment into the positive category with high confidence, the document is added to the
corresponding class. As a result, we retrieve a set of classes for each document. We
explain the method in detail in the Methods section.
Training corpora
In order to evaluate the results of our categorization efforts, we constructed training
corpora from the EPO dataset that was also the basis of our previous analysis. We
used three parameters to choose the sets of classes and documents that we used for
these corpora:
 number of patents
Since the Maximum Entropy method improves with a growing number of training
documents [15], it is reasonable to restrict the categorization task to classes that
were used to annotate some minimum number of patents. We therefore excluded
classes that were assigned to fewer patents.
 text length
While the EPO dataset contains the bibliographic data to all European patents, it
doesnt always include the complete texts. Since the classifiers rely on the text, we
only used documents that surpassed a certain minimum text length.
 only primary classification/also secondary classification
Of the classes assigned to a patent, one is always emphasized as the primary one; i.
e., the one that is supposed to correspond to the central aspect of the patent.
When choosing training documents for a class, it might therefore be advisable to
concentrate on patents with the primary classification.
We constructed one corpus with strict requirements (only widely used classes, long
patents and primary classification) and another with more relaxed requirements (also
less widely used classes, shorter patents and secondary classification). The details are
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 11 of 23
presented in the Methods section. As a result of applying these requirements to our set
of patents, the first corpus contains 73 classes while the second one is much larger
with 1205 classes. In order to enhance the readability of this paper, we will refer to the
first corpus as C73 and the second one as C1205 for the remaining sections of this
paper. This size difference in connection with the expected higher quality of the docu-
ments due to the constraints we mentioned above should lead to better categorization
results for C73 than for C1205.
Evaluation
With our initial evaluation, we tested our methods ability to retrieve the classes that
were actually assigned to the patents. Therefore, all of these classes were considered
correct while everything else was considered wrong. Table 2 shows the macro-average
scores (precision, recall and F1-measure) of all classifiers using 10-fold cross-validation
for the confidence threshold 0.5; the use of other values is investigated below.
As Table 2 shows, the results are for the most part encouraging, with most values
approaching 0.9. The recall value is 6% higher for the smaller corpus. Applying t-test
to the recall values from the common classes of both corpora (i.e., the 73 classes
from C73) confirmed that this difference is statistically significant with extremely high
confidence (a < 0.001). However, despite the size differences between both corpora,
the precision values are equal. This may suggest that the C73 results are about as good
as can be expected from the use of the Maximum-Entropy method on patent texts.
Table 3 shows the most influential word features for five models trained on IPC
classes with biomedical relevance. While the positive features are in general very
Table 2 Evaluation results for confidence threshold 0.5. The precision values are
identical for both corpora, but recall is considerably higher for the smaller corpus.
Corpus Precision Recall F1-measure
C73 0.88 0.90 0.89
C1205 0.88 0.84 0.86
Table 3 Most influential positive and negative classifier features. Features were
extracted from five binary Maximum-Entropy classifiers trained on IPC classes with
biomedical significance, leaving out stopwords and words with three or less characters.
The occurrence of positive/negative feature words makes a document more/less likely
to be assigned to the class.
IPC code A61B 5/00 A61B 17/70 A61M 25/
00
BO1L 3/00 G01N 33/543
class definition
(abbrev.)
Measurement for diagnostic
purposes
Spinal
positioners
Catheter Laboratory
glassware
Immunoassay
light bone catheter sample binding
sensor portion distal fluid analyte
Pos. Features blood member tube channel sample
patient screw lumen chamber surface
tissue spinal portion surface antibody
layer data data data data
acid information time information information
Neg. Features sequence signal signal image network
network method information signal channel
cells cell control recording transmission
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 12 of 23
representative of the respective IPC class, most of the negative features seem to be use-
ful mainly for excluding patents about information technology. We plan to investigate
the influence that different methods for choosing negative documents (cf. Methods sec-
tion) have on these feature sets.
While precision and recall values around 0.9 are generally acceptable when a single
classifier is used, these values are problematic in our situation - especially when
applied to the corpus with many classes. It is important to note that there are two dif-
ferent learning tasks:
1. Given a class, find documents for this class.
2. Given a document, find classes for this document.
For the first learning task, we have very promising results with precision, recall and
F1-measure close to 0.9. The second task is more problematic however. Since we apply
all classification models to all documents, most documents are assigned more than one
hundred classes in the case of C1205. While this may appear to contradict our claim of
the good performance of the individual classifiers, it does not: Even if every classifier
makes the right decision in nine out of ten cases, applying more than 1000 such classi-
fiers will still lead to many wrong decisions. This means that although the performance
of the individual classifiers is satisfactory, we have to take additional steps in order to
make its use for our intended application feasible. In order to reduce the number of
class suggestions, we tried various higher values for the confidence threshold. In the
PubMed/MeSH experiments detailed in [15], the highest F1-measure was reached for
the confidence threshold 0.6. Unfortunately, our patent classifiers react less positively
to raising the threshold, as can be seen in Figure 7: While raising the value from 0.5 to
0.6 clearly has a positive effect on precision, the corresponding drop in recall is much
more severe and leads to a significantly lower F1-measure. Raising the value further
only has negligible effects on the classification quality, leading to very slight precision
increases and recall decreases.
Figure 7 Classification results for corpus C73 depending on the confidence threshold. The F1 measure is
highest for the value 0.5 due to a rapidly decreasing recall. Increasing the threshold further after the value
0.6 only leads to small changes.
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 13 of 23
Due to the high number of patent classes, our methods precision would have to be
very close to 1 in order to make the assignment of additional classes to all documents
feasible. Unfortunately, since raising the confidence threshold only leads to moderate
increases in precision, we cannot reach a value high enough for practical application of
the method by itself. Still, since most queries also include a keyword component, it is
possible to use the described approach to improve recall for such combined searches.
Despite that, we also tried to filter the assignments of our approach in order to make
it useful by itself: As before, we applied every classifier to every model. However,
instead of setting a confidence threshold for the classification score, we decided in
advance how many classes were supposed to be assigned to each document. After cal-
culating all classification scores, we only retained the pre-determined number of high-
est-ranking classes. Figure 8 shows the recall of the method depending on the number
of assigned classes, both for the exact class (i.e., the subgroup) and the more general
main group. We calculated the main group recall by considering all subgroups below
the closest main group as correct; in terms of our example from the Background sec-
tion, for a patent from class A61K 38/17, also classes such as A61K 38/00 and A61K
38/16 are accepted. We chose to have the method assign ten classes in order to strike
a balance between recall and precision. A small-scale manual evaluation of the results
revealed that this method is able to recreate some assignments and to add relevant
classes that were not assigned. As an example, patent EP1286824 about an apparatus
for clamping and releasing contact lens molds was correctly assigned to class B29D
11/00 about the production of optical elements, and it was also assigned to the rele-
vant class G02C 7/02 about lens systems - this class was not among the original
assignments. However, even among the ten assigned classes for each patent, there were
usually at least five completely irrelevant ones. The example patent about contact lens
production was also assigned to class A61K 31/485 which is about medicinal prepara-
tions involving morphinan derivatives as well as class B29D 30/06 which is about
pneumatic tyres or parts thereof. These results make the practical application of the
method without any other filters doubtful. They are however not unexpected consider-
ing the slow precision growth that we pointed out in Figure 7: Our method effectively
increases the confidence threshold further, reaching different values for each classifier.
Figure 8 Recall for corpus C1205 depending on the number of assigned classes. The value grows rapidly
until around ten classes, then continues growing at a slower pace.
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 14 of 23
But since precision remains almost constant (albeit at a high level), this is still not
enough to remove all irrelevant assignments.
However, we plan to investigate another possibility to filter out incorrect class
assignments. As we will explain in Proposing Additional Classes for Search, we also did
an analysis on pairs of classes that were frequently assigned to the same patent docu-
ment. This data should also be useful for this task, since it is likely that classes that
were never assigned to the same document do not cover similar subjects. An initial
test had the result we were hoping for: For the example patent about contact lens pro-
duction, our approach was able to filter out the incorrectly assigned classes about mor-
phine and pneumatic tyres, since they had never been assigned to the same patent as
class B29D 11/00 that had been assigned by the patent office. On the other hand, the
newly assigned relevant class about lens systems did co-occur with class B29D 11/00
and was therefore not filtered out.
Guided patent search
An additional possible way for tackling the problem of low class assignment is the
expansion of user queries to make up for the missing assignments. Since professional
patent search queries are a combination of class codes and keywords in most cases, we
investigate ways to expand both of these components in the following subsections.
Proposing additional classes for search
If a user query contains a class code, it can be assumed that the user is confident of the
relevance of that class. In order to find closely related classes to suggest to the user, we
analyzed the class co-assignments in our patent corpus. We collected all pairs of classes
that were assigned to the same patent and ranked them both on the absolute number of
co-assignments and the relative number in the form of their Jaccard-Index. We hypothe-
size that pairs of classes with high ranks in either ranking are related closely enough that
many searches for one of the classes will also have additional relevant results in the sec-
ond class. We therefore propose to suggest these frequently co-occurring classes to the
user for query expansion. In order to ensure the quality of our suggestions, we based the
co-assignment statistics solely on the existing EPO assignments. However, we plan to
investigate the effect of including classes that were added by our automated method.
Many resulting class suggestions are from the same hierarchical tree but not directly
related, i.e., they cover patents with very similar aspects to the ones searched for by
the user. Additionally, the rankings include pairs of classes from completely separate
parts of the hierarchy that are also highly related; in many cases, they can be consid-
ered to represent different points of view. Figure 9 shows one example of such a pair
of classes, including their definition hierarchy. The left class is clearly more applica-
tion-oriented than the right one, since it deals with medical preparations containing
peptides while the right class concentrates on the peptides themselves. However, we
argue that many searchers interested in patents from one class will also find relevant
patents in the other one. We used the professional patent search tool Thomson Inno-
vation to find out how recall is affected when only one class is used for search. For
these example classes, searching for only the first class leads to over 50% missed possi-
ble results, and searching only for the second still leads to 25% missed results. The
situation is similar for the pair of classes shown in Figure 10, also detected using co-
assignment information.
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 15 of 23
In order to give a more general evaluation of how meaningful the class co-occur-
rences are, we used our categorization results. Since we trained binary classifiers for a
large number of classes, we can compare their feature sets to each other. We chose
the 100 most frequently co-occurring pairs of classes as well as 100 additional random
classes. We then calculated the number of common features between different classes
among the 100 top features for each classifier. Figure 11 shows these numbers for the
co-occurring pairs compared to the average for all 100 random classes. For all co-
occurring class pairs, the overlap is considerably higher than for the randomly chosen
classes. On average, the co-occurring classes have 35 common features, while the ran-
dom classes only have nine. This shows that the co-occurrence information is useful
for finding related classes.
Proposing additional keywords for patent searches
As we showed above, it is possible to find additional relevant classes to expand user
queries based on class co-occurrence. Another option for query expansion would be
the suggestion of additional keywords to the user. The following subsections describe
our efforts to expand patent queries with additional keywords extracted from various
sources.
1. Extracting keywords from classes The most straightforward way of turning class
codes into keyword suggestions is by considering the corresponding IPC definitions -
both by extracting keywords directly and exploiting the morphosyntactic structure of
definitions where possible. For definitions containing lists of related terms, we use the
system described in (Fabian et al., 2012) to find additional terms with the same relation
and suggest the top-ranking ones to the user. As an example, the suggestions for the
IPC definition Orthopaedic devices [] such as splints, casts or braces include the
relevant terms slings, collars, and crutches. For a baseline Boolean keyword query
Figure 9 Example for semantically related IPC classes without any hierarchical relation, detected using co-
assignment information.
Figure 10 Second example for semantically related IPC classes without any hierarchical relation, detected
using co-assignment information.
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 16 of 23
simply connecting the terms with OR, the result set almost doubles in size after the
inclusion of the generated sibling terms. Our system detected 3053 IPC classes (? 4%)
that contain enumerations and can therefore in principle be used in this way for query
expansion. Taking this approach one step further, established NLP techniques can be
used to extract keywords from the patent texts that belong to the query classes. In a
way, this is also what we did for our categorization efforts: Each classifier gives weight
parameters to the words contained in patent documents, with high values correspond-
ing to words that are typical for the class. Table 3 shows the word features with the
highest values for five IPC classes with biomedical relevance, demonstrating that this
approach is able to discover useful search terms.
2. Extracting keywords from external ontologies Existing ontologies are another possi-
ble source for additional keywords. If an ontology term can be matched to an IPC class
definition, any additional information contained in the ontology about the term (e.g., its
synonyms) can be used to add suggestions for the user. As a proof of concept, we used the
annotation pipeline from [11] to map MeSH terms to an IPC subset with biomedical rele-
vance. For that purpose, we selected all subclasses of the IPC class A61K with the defini-
tion Preparation for medical, dental or toilet purposes (981 subclasses). The annotation
results provided at least one MeSH term for 865 of these classes (88%), and three or more
terms for 466 classes (48%). Many IPC classes were matched with very relevant MeSH
terms, e.g., class A61K 48/00 (medicinal preparations containing genetic material which
is inserted into cells of the living body to treat genetic diseases; gene therapy) with MeSH
terms including Genes, Cells and Gene Therapy. On the other hand, there were also
incorrect matches, often due to shortened MeSH synonyms. For the example class, the
MeSH term Containment of Biohazards was considered a match because the word con-
taining in the class definition was mapped to Containment which MeSH lists as a syno-
nym. Since our system proposes expansion terms to the user instead of automatically
adding them, this high level of coverage represents a valuable addition despite the inclu-
sion of some false positive annotations.
Figure 11 Classifier feature overlap among the Top 100 features for frequently co-occurring and random
classes. The overlap is generally much higher for co-occurring classes, showing the significance of co-
occurrence information.
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 17 of 23
The availability of a domain ontology also makes enhanced sibling generation possi-
ble: If an IPC definition contains a MeSH term as well as one of its child terms in the
form of an example, it is reasonable to assume that all other child terms are also rele-
vant. Following this intuition, IPC definitions of this form (e.g., Sulfonylureas, e.g.
glibenclamide, tolbutamide, chlorpropamide) lead to term suggestions with very high
precision (for the example: Carbutamide, Acetohexamide, etc.). Of the biomedical
IPC subset, this was possible for 72 classes (7%).
3. Repurposing class-keyword mappings for class suggestion After keywords have
been mapped to IPC classes in the proposed ways, the mapping data can also be
used in the opposite direction: If the user enters a keyword that has been mapped to
an IPC class, this class can be suggested to the user for expanding his query. If the
class definition is displayed with the suggested class code, even users unfamiliar with
the IPC can profit from classification information. This is especially true for the bio-
medical domain, since the availability of detailed domain ontologies leads to very pre-
cise class suggestions. The same approach applies again for the keywords that were
selected by our classifiers ass representative of their class. The WIPO website used to
offer similar functionality but it was not made clear what the systems class proposals
were based on and the service was stopped in November of 2012 without further
explanation.
Conclusions
We investigated possibilities for giving patent searchers access to the same advantages
that are offered for PubMed through MeSH annotations. Our analysis of MeSH and
IPC showed some unique characteristics of the patent domain, most importantly com-
plex class definitions that rarely occur in text as well as a low number of class assign-
ments. These discrepancies must be considered during the development of a patent
retrieval system. We proposed ways to overcome these problems by combining two
complementary approaches: the assignment of additional patent classes as well as the
development of specialized components for a guided patent search system. Our experi-
ments showed that automated patent categorization using the Maximum Entropy
approach offers promising results with F1-measure values above 0.85 for individual
classifiers. Including these newly assigned classes in a patent retrieval sytem by com-
bining them with search keywords can offer considerable improvements to patent
search. Additionally, we demonstrated that class co-occurrence data can provide valu-
able information to users and that existing ontologies and taxonomies such as MeSH
can benefit the patent searcher by taking existing domain knowledge into account.
Methods
This section describes in more detail the methods we used for our experiments. The
first subsection explains our analyses of MeSH and IPC, and the following subsection
concerns our experiments with patent categorization.
Analysis of MeSH and IPC
Both analyses were carried out in two steps: We first retrieved and analyzed the terms
and their hierarchical relationships, and then the annotations to our document
corpora.
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 18 of 23
MeSH
For our analysis of the MeSH hierarchy, we used the XML version of MeSH 2012
retrieved from the MeSH homepage [44]. We extracted all MeSH terms with their
MeSH IDs as well as the tree numbers from the file. The tree numbers were then used
for reconstructing the hierarchy. We implemented graph-based methods for calculating
different hierarchical properties such as the number of nodes per hierarchy level. For
the PubMed/MeSH annotation analysis, we used the complete Medline dataset with
MeSH annotations, downloaded from PubMed on September 22, 2011. After extracting
the necessary information about documents and annotations, we analyzed it using a
custom implementation, calculating different characteristics of the data such as the
average number of annotations per document.
IPC
We reconstructed the IPC hierarchy using HTML files available from the WIPO
homepage [45]. After manually entering the eight sections of the IPC with their defini-
tions as top nodes of the hierarchy, our implementation automatically extended the
hierarchy step by step: Each section file (e.g., A.htm) contained all main classes of
the section, allowing us both to add them to our representation of the hierarchy and
to retrieve the corresponding HTML files. Then the subclasses were extracted from
the main class files (e.g., A01.htm), and the main groups and subgroups from the
subclass files (e.g., A01B.htm). Since the class codes do not correctly reflect the
father/child relationship between entries at the subgroup level (cf. section Background),
we used the dot representation in the files to ensure the accuracy of our representation
of the hierarchy. Class definitions were also extracted from the files in string form;
images contained in a number of chemistry-related class definitions as well as refer-
ences to related classes were removed. The analysis of the hierarchy was carried out
using a slightly modified version of our MeSH implementation, leading to directly
comparable results.
For the patent annotation analysis, we used XML files published by the EPO via their
subscription-based Product 14.12 [46]. We used the complete set of patent applica-
tions from the years 1981 to 2005, and we extracted document numbers as well as all
classification information from the files. The reason for our exclusion of more recent
patents was a change in EPO publication policies and data formats. Since there have
been multiple updates to the IPC that are not reflected in the EPOs files, we decided
to use the 2006 version of IPC in order to minimize the number of class assignments
that could not be matched. The document numbers were used to make sure that dif-
ferent versions of the same patent were not counted multiple times. For the classifica-
tion information, we extracted both primary and secondary classification codes and
combined them into one set of codes per patent. We again used a modification of the
PubMed implementation to perform our analyses of the data.
Automated patent categorization
As we described in section Results and Discussion, our approach to assigning addi-
tional classes to patents was based on [15], where a Maximum Entropy (MaxEnt)
approach was used for document annotation with MeSH terms: For each MeSH term
that was to be used for document annotation, a binary MaxEnt model was learned
from already annotated documents and applied to new ones. We applied the same
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 19 of 23
principle to patent classification, learning IPC classifiers from existing patent docu-
ments with classes manually assigned by professional patent examiners. The following
subsections describe the Maximum Entropy approach in general as well as the details
of our implementation.
Maximum Entropy
The Maximum Entropy approach estimates a probability distribution from existing
data, based on the assumption that the distribution should be as uniform as possible
if no external knowledge is available. This principle also gave the approach its name:
Entropy measures the uncertainty of the outcome of a random variable, and its value is
maximized if the random variable is uniformly distributed. Intuitively, this can be seen
through the example of a coin toss: Its uncertainty is largest for a fair coin; if the coin
is known to have a higher probability to show heads, it is easier to guess the next toss.
Maximum Entropy has been used for various tasks in Natural Language Processing (e.
g., language modelling [47] and part-of-speech tagging [48]) since the mid-nineties and
was first proposed for text classification in 1999 by [49].
For this purpose, the existing data are documents that have been labeled with certain
categories (the training set), and the probability distribution that is estimated by the
approach is used to assign classes to new documents (the test set). In order to do that,
features are extracted from the training set. A feature is a measurable property of the
documents, e.g., the number of occurrences of a certain word in the text or the year in
which the document was published. For estimating the probability distribution, each
feature fi is assigned a parameter li with initial value 0. Based on the relationship
between feature values and class assignments in the training documents, these para-
meters are then updated iteratively until they converge. The result is a probability dis-
tribution based on the chosen features weighted by the corresponding parameters. In
order to assign classes to a new document, it is then only necessary to input the docu-
ments feature values into the distribution to get a classification score.
MaxEnt can be used for binary classification (i.e., one of two classes is assigned) as
well as multi-class classification (one of multiple classes). Since our goal is multi-label
classification (i.e., a subset of all classes should be assigned), we trained one binary
classifier for each class and applied all classifiers to each document.
Implementation
Our corpus was again a subset of the EPO dataset we used for the IPC analysis. For
the classification, we used patents published after 2005 and before July 2012. As we
described in section Results and Discussion, we constructed two corpora by applying
three different criteria: the number of patents, the text length and the optional restric-
tion to primary classification. We included all classes that had the required number of
patents fulfilling the text length requirement. In the first corpus, only patents that had
the class as primary classification were counted. We then collected the required num-
ber of patents for each class by randomly choosing from the complete set. Table 4
shows the values we chose for the parameters as well as the number of classes that ful-
filled the requirements and the resulting number of patents per corpus.
We used the Java API of the open-source machine-learning toolkit Mallet (version
2.0.7) [50] for our classification efforts. The pre-processing was done in two steps: For
each of the patent documents that were chosen from the EPO corpus for inclusion in
C73 or C1205, we first created a text file that contained all text fields from the
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 20 of 23
corresponding XML file. We then created a feature vector from each text file by using
both existing and custom implementations of Mallets Pipe interface. The classifiers
were trained by executing the train method from the Mallet class MaxEntTrainer.
The training sets for each classifier were constructed as follows: For the positive set,
all patents that the corpus contained for the respective class were included. For the
negative set, a few different approaches were investigated by [15]. Since the differences
were very small, we decided to use the most simple option: We randomly chose the
same total number of patents as in the positive set from the set of all other classes. In
order to avoid the over-representation of individual classes, we shuffled all these classes
and randomly selected one document from each of them in turn. Despite taking that
step, the negative features seem to be overly concentrated on separating very distant
technological fields and less useful for detecting subtle differences between classes (cf.
Table 3). We plan to investigate different possibilities for constructing the negative set,
e.g., increasing the number of documents from fairly similar classes. However, while
this may help fine-tune the negative features, it is possible that the currently high qual-
ity of the positive features will suffer.
We used 10-fold cross-validation and calculated the macro-average scores (cf. Table 2).
Since the cross-validation methods that are included in Mallet do not conserve the ratio
of positive and negative training documents, we implemented a custom method for this
task as well as for the evaluation of the categorization results.
Since both our approach and our objective for patent categorization differ consider-
ably from the previous approaches we mentioned in the Background section, compar-
ing the results directly is not possible: Almost all existing approaches are restricted to
higher levels of the hierarchy, and all of them are used for assigning one single class
instead of sets of classes.
Authors contributions
DE, MS conceived the ideas, analysed the data, and wrote the manuscript. DE implemented code. MB, UW, GT
contributed to discussions.
Competing Interests
The authors state that they have no competing interests.
Acknowledgements
The authors wish to thank Thomas Wächter for very helpful discussions concerning guided patent search.
Declarations
This work was supported by Roche Diagnostics GmbH and the DFG projects GeneCloud and Hybris.
This article has been published as part of Journal of Biomedical Semantics Volume 4 Supplement 1, 2013: Proceedings
of the Bio-Ontologies Special Interest Group 2012. The full contents of the supplement are available online at http://
www.jbiomedsem.com/supplements/4/S1
Author details
1TU Dresden, BIOTEC, Tatzberg 47/49, 01307 Dresden, Germany. 2Roche Diagnostics GmbH, Nonnenwald 2, 82377
Penzberg, Germany.
Published: 15 April 2013
Table 4 Training corpora for patent categorization. C73 has more patents per class with
longer text and only primary classification.
training
corpus
number of
patents/class
minimum text
length
restricted to primary
classification
number of
classes
total number of
patents
C73 200 8000 characters yes 73 14600
C1205 100 2000 characters no 1205 120500
Eisinger et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S3
http://www.jbiomedsem.com/content/4/S1/S3
Page 21 of 23
JOURNAL OF
BIOMEDICAL SEMANTICS
Hayamizu et al. Journal of Biomedical Semantics 2013, 4:15
http://www.jbiomedsem.com/content/4/1/15
SHORT REPORT Open Access
EMAP/EMAPA ontology of mouse
developmental anatomy: 2013 update
Terry F Hayamizu1*, Michael N Wicks2, Duncan R Davidson2, Albert Burger2,3, Martin Ringwald1 and
Richard A Baldock2
Abstract
Background: The Edinburgh Mouse Atlas Project (EMAP) ontology of mouse developmental anatomy provides a
standard nomenclature for describing normal and mutant mouse embryo anatomy. The ontology forms the core of
the EMAP atlas and is used for annotating gene expression data by the mouse Gene Expression Database (GXD),
Edinburgh Mouse Atlas of Gene Expression (EMAGE) and other database resources.
Findings: The original EMAP ontology listed anatomical entities for each developmental stage separately, presented
as uniparental graphs organized as a strict partonomy. An abstract (i.e. non-stage-specific) representation of mouse
developmental anatomy has since been developed. In this version (EMAPA) all instances for a given anatomical entity
are presented as a single term, together with the first and last stage at which it is considered to be present.
Timed-component anatomies are now derived using staging information in the primary non-timed version.
Anatomical entities are presented as a directed acyclic graph enabling multiple parental relationships. Subsumption
classification as well as partonomic and other types of relationships can now be represented. Most concept names are
unique, with compound names constructed using standardized nomenclature conventions, and alternative names
associated as synonyms.
Conclusions: The ontology has been extended and refined in a collaborative effort between EMAP and GXD, with
additional input from others. Efforts are also underway to improve the revision process with regards to updating and
editorial control. The revised EMAPA ontology is freely available from the OBO Foundry resource, with descriptive
information and other documentation presented in associated Wiki pages (www.obofoundry.org/wiki/index.php/
EMAPA:Main_Page).
Keywords: Mouse development, Anatomy ontology, Developmental anatomy, OBO format
Findings
EMAP ontology
The ontology of mouse developmental anatomy was orig-
inally developed by Jonathan Bard and his colleagues
as part of the Edinburgh Mouse Atlas Project (EMAP;
www.emouseatlas.org) in order to provide a structured
controlled vocabulary of stage-specific anatomical struc-
tures for the developing laboratory mouse [1]. In order
to construct the original dictionary of anatomy terms,
histologically distinguishable anatomical entities were
identified and organized as simple, strictly uniparental
hierarchies (trees). Initial selection of terms was based
*Correspondence: Terry.Hayamizu@jax.org
1The Jackson Laboratory, Bar Harbor, USA
Full list of author information is available at the end of the article
on the tissue index for The Atlas of Mouse Develop-
ment [2]. Subsequently, the list of anatomical terms was
substantially extended. Term names were assigned based
on what was considered to be most generally accepted
names, with synonyms included as appropriate. Individual
term labels were not necessarily unique, but each com-
ponent could be unambiguously specified by its full
name, which included its ordered hierarchical path, as
well as by a unique numerical identifier (i.e. EMAP ID).
For example, the term for epithelium associated with id
EMAP:969 could be specified by its full hierarchical path,
i.e. TS14/mouse/organ system/visceral organ/alimentary
system/gut/midgut/epithelium.
The original hierarchy only utilized part-of relation-
ships, based primarily on structural subdivisions. The
© 2013 Hayamizu et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Hayamizu et al. Journal of Biomedical Semantics 2013, 4:15 Page 2 of 5
http://www.jbiomedsem.com/content/4/1/15
intent was to describe the whole embryo as a tree
of anatomical structures successively divided into non-
overlapping named parts. Sets of anatomical terms for
each standardised developmental stage (Theiler Stage,
TS) [3], were presented as separate hierarchical trees.
For example, at TS20, the mouse embryo has parts (e.g.
head, limb, trunk and tail) which are progressively sub-
divided, e.g. limb > forelimb > handplate > digit 1 >
mesenchyme.
EMAP terms, organized within trees for each Theiler
stage, have been adopted for the annotation of expres-
sion data by the Gene Expression Database for Mouse
Development (GXD; www.informatics.jax.org/expression.
shtml), part of the Mouse Genome Informatics (MGI)
resource at The Jackson Laboratory, and the Edin-
burgh Mouse Atlas of Gene Expression (EMAGE;
www.emouseatlas.org/emage). Figure 1 illustrates the
role of EMAP as the means of integration between
GXD and EMAGE. Other database resources cur-
rently utilizing EMAP ontology terms include Eur-
Express (www.eurexpress.org) and the GenitoUrinary
Molecular Anatomy Project (GUDMAP; www.gudmap.
org). In addition, the EMAP ontology forms the core
of the EMAP anatomical atlas (www.emouseatlas.org/
emap) and will be an important element of the online
version of the Atlas of Mouse Development [2]. Finally,
EMAP terms as well as the hierarchical organization of
the ontology were used as a framework for construc-
tion of an anatomy ontology for the postnatal mouse
by GXD [4]. This has enabled consistency of nomen-
clature and will facilitate future integration of these
ontologies.
The abstract mouse
From the onset, the design of the EMAP database
identified each Theiler stage-dependent term as a
timed-component with a hidden abstract mouse [1]
comprised of a set of stage-independent terms with
partonomic relationships. The abstract mouse anatomy
ontology was algorithmically derived from the exist-
ing stage-dependent anatomy hierarchies by forming the
union of all stage-dependent graphs where nodes repre-
sent anatomical structures and edges represent part-of
links [5]. Nodes in the abstract mouse graph represent
anatomical structures that exist during some time of
embryo development and broadly correspond to so-
called material continuants [6] (code.google.com/p/
obo-relations/). Originally invented as a schema design
for the object-oriented database system used to store the
anatomy, the idea of an abstract mouse has subsequently
proved useful at a conceptual level. The non-timed ver-
sion of the mouse developmental anatomy ontology has
previously been made available on a limited basis, with
unique identifiers included as persistent, trackable IDs.
Figure 1 EMAP ontology: Mouse developmental anatomy and gene expression data. The original EMAP ontology has been and is still being
used for standardised description of anatomical entities by the e-Mouse Atlas (EMA), an anatomical atlas of mouse embryo development, and for
annotation of gene expression data by the Gene Expression Database for Mouse Development (GXD) and the e-Mouse Atlas of Gene Expression
(EMAGE).
Hayamizu et al. Journal of Biomedical Semantics 2013, 4:15 Page 3 of 5
http://www.jbiomedsem.com/content/4/1/15
Updates to EMAPA
The stage-dependent EMAP hierarchies have provided a
valuable basis for data annotation and integration, but var-
ious inherent limitations have been encountered. Early
on, it became apparent that the ability to provide alter-
nate representations of the anatomy would be required,
with different hierarchial views enabling classification and
other types of relationships. Also problematic were the
inherent constraints in cases where the embryonic age
or stage was poorly or not specified. Another issue was
the fact that term labels, such as epithelium were orig-
inally not necessarily unique, nor specific. It was clear
that the ontology would benefit from a series of modifi-
cations. Pursuant to these goals, the abstract version of
the mouse developmental anatomy has since been further
developed.
The uniparental hierarchy was converted to a directed
acyclic graph (DAG) enabling multiple parental relation-
ships (see Figure 2). This allowed the representation of
anatomical concepts that were otherwise not possible. For
example, brain can be represented as a part of head
as well as a part of central nervous system. The DAG
format also supported the inclusion of other types of
relationships as well as partonomic ones. Subsumption
classification and other relationship types can now be
represented. In the revised EMAPA representation, all
instances for a given anatomical entity are presented as
a single term, together with the first and last stage at
which the entity is considered to be present in the devel-
oping embryo. Stage-specific EMAP anatomy hierarchies
are now derived using staging information associated with
terms in the primary non-timed EMAPA version. The
ontology has also been transformed into a more support-
able format based on openly available relational database
technology, coupled with a standard input/output format
developed by the Open Biological Ontologies (OBO) con-
sortium. These changes have and will continue to facilitate
further development of the ontology.
In extensions to the EMAPA ontology, is-a relations
have been introduced (Figure 2) in situations where this
relationship was determined to be more appropriate than
part-of (e.g., nervous system is-a organ system). The
use of is-a relations has also been used in extensions to
the ontology to facilitate data annotation and to support
subsumption classification of anatomical entities in the
ontology. In general, modeling of hierarchial relationships
has followed the conventions identified by GXD in devel-
oping the ontology for postnatal mouse anatomy (MA) [4].
These conventions also parallel those being adopted for
anatomy ontologies by other model organism databases,
as well as scientific community-wide efforts developing
multispecies ontologies (see below).
In the original version of the EMAP ontology, individual
term labels were not necessarily unique, often requiring
Figure 2 EMAP and EMAPA ontologies provide stage-specific and stage-independent representations of mouse embryo anatomy.
Originally constructed as uniparental partonomic hierarchies with anatomic entities for each Theiler stage of embryonic development, the anatomy
ontology for mouse development has been revised and is now comprised of directed acyclic graphs (DAGs) with both stage-independent and
stage-specific representation for mouse developmental anatomy.
Hayamizu et al. Journal of Biomedical Semantics 2013, 4:15 Page 4 of 5
http://www.jbiomedsem.com/content/4/1/15
knowledge of the hierarchical path for disambiguation.
Because it was impractical to display the full path names in
user interfaces, shortened print names have been imple-
mented. For example, to represent expression results for
the above mentioned anatomical structure EMAP:969 in
GXD, the print name TS14;midgut epithelium was dis-
played rather than the the full path name or the ambigu-
ous term label epithelium. Term identification based on
parental hierarchy was further complicated by the intro-
duction of multiple parentage. Consequently, in an effort
to provide unique names for all terms, every term name
in the ontology was evaluated for uniqueness. In numer-
ous cases, modified compound names were constructed
for many terms using standardized nomenclature conven-
tions [4]. Alternative names will continue to be added as
synonyms. The evaluation of this and other nomencla-
ture considerations will remain as part of the editorial
process.
Furthermore, the ontology has been substantially
extended and refined in collaborative efforts between
EMAP and GXD. The original EMAP ontology contained
more than 14,200 stage-specific terms for anatomical
entities in the mouse embryo, corresponding to about
3,400 abstract anatomy terms. Since then, terms have
been added, predominantly in response to the require-
ments of substantial amounts of gene expression data
curation by both GXD [7] and EMAGE [8]. In addi-
tion, urinary and reproductive systems have been exten-
sively extended and refined by curators from GUDMAP
[9]. Based on the information contained in the EMAPA
file, stage-specific terms with associated EMAP identi-
fiers have been instantiated. The resulting set of EMAP
terms and identifiers includes and is consistent with
previous versions of the mouse developmental anatomy.
Currently, the EMAPA ontology includes 5,590 anatomy
terms, corresponding to over 35,000 stage-specific
EMAP terms.
The anatomical ontology for the developing mouse will
continue to be expanded and refined based on additional
resources, as well as the needs of the scientific com-
munity. The revised EMAPA ontology has been made
freely available as a text file in OBO format via the OBO
Foundry resource (www.obofoundry.org). Obo-formatted
files containing EMAP ontology hierarchies for each of the
Theiler stages for mouse development, presented as sep-
arate DAGs, will also be available. In addition, in order to
facilitate interoperability of resources using different sets
of mouse anatomy terms, a mapping file has been cre-
ated in which all corresponding EMAP and EMAPA terms
have been specified. Descriptive information and other
documentation relevant to these files is provided in asso-
ciated Wiki pages. Stage-specific EMAP and abstract
EMAPA ontologies can also be accessed at the EMAP
site (www.emouseatlas.org/emap/ema/DAOAnatomyJSP/
abstract.html) using a browser which enables search-
ing for terms directly as well as browsing through the
respective hierarchies.
Future directions
The EMAPA ontology, along with instantiated stage-
specific EMAP components, will continue to be expanded
and refined according to the requirements of data curation
and input from the scientific community at large. Opti-
mally, as in the case of the GUDMAP contributions, this
will include editing of specific areas of the ontology with
domain expert involvement. Efforts to improve the revi-
sion process with regards to updating and editorial control
are also underway. Plans are being developed in order to
facilitate term requests, and to enable appropriate edi-
torial tracking and version control. Future development
of the EMAPA ontology itself will also involve extension
and refinement of relationships between concepts, includ-
ing further development of the subsumption classification
hierarchy, as well as introduction of other types of rela-
tionships. Particularly, the develops-from relationships
will be included to support the analysis of differentiation
pathways in databases that deal with expression, pheno-
typic, and disease-related information. Another goal is the
inclusion of a set of textual definitions, computable logical
definitions that can be used by automated reasoners, and
other forms of metadata. Further efforts are underway
towards adhering to basic ontological principles such as
those set forth by the OBO Foundry [10].
The new EMAPA ontology will be used by GXD,
EMAGE, and EMAP, as well as by other resources that
have employed previous versions of the ontology to
describe gene expression patterns and other biological
data pertinent to mouse anatomy. These include Gene
Ontology (GO) [11] for annotation of mouse gene prod-
ucts, as well as several efforts utilizing the entity-quality
(EQ) approach [12] to describe data annotated using
the Mammalian Phenotype Ontology (MP) [13]. EMAPA
terms and identifiers are also included in bridging exten-
sions to the Uberon multispecies anatomy ontology [14],
which will further serve to facilitate integration of mouse
developmental data within the broader scientific domain.
New research has also been initiated to study how an
ontology such as EMAP can be used to integrate experi-
mental data from model organisms, such as the EMAGE
database, with a computational framework of human
physiological modelling for eHealth purposes (part of the
Virtual Physiological Human programme), though this
work is still very preliminary [15].
Conclusion
Here we have presented the recently updated and
extended EMAP ontology of mouse developmental
anatomy. The ontology has been in active use for many
Hayamizu et al. Journal of Biomedical Semantics 2013, 4:15 Page 5 of 5
http://www.jbiomedsem.com/content/4/1/15
years in GXD and EMAGE for annotation of gene-
expression data and as part of the Edinburgh Mouse Atlas
model framework. Since the original development of the
ontology, the modelling emphasis has shifted from a series
of time-dependent ontologies to a single abstract time-
independent ontology (EMAPA), where the former can
now be automatically derived from the latter. The ontol-
ogy is available from the OBO Foundry web-site and is
under continuous revision to include new terms and rela-
tionships. In particular the ontology will be updated to
ensure a full class hierarchy for each tissue term and
extension of the lineage information encoded via the
develops-from relationship. This extension will enable
automated consistency checking and validation in addi-
tion to the semantic checking provided by the editorial
review group.
Abbreviations
DAG: Directed acyclic graph; EMAGE: Edinburgh Mouse Atlas of Gene
Expression; EMAP: Edinburgh Mouse Atlas Project; GXD: Gene Expression
Database for mouse development at MGI; GUDMAP: GenitoUrinary Molecular
Anatomy Project; MGI: Mouse Genome Informatics at The Jackson Laboratory,
USA; OBO: Open Biological Ontologies.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
TFH undertook the primary effort to update and extend the EMAPA ontology
to its present form and wrote the first draft of this manuscript. MNW has
developed the underlying database and associated code to provide validation
of the ontology and production of the new version with consistent UIDs
maintaining persistence as required. DRD, AB, MR and RAB have contributed in
the overall ontology design and provided expert input either in terms of
developmental anatomy or ontology formalisation aspects or both. All authors
read and approved the final manuscript.
Acknowledgements
The effort in Edinburgh for this work was supported under the MRC core
funded Mouse Atlas Programme (U.1275.2.4.4.1) at the MRC Human Genetics
Unit. Work at The Jackson Laboratory was done as part of the GXD project
which is supported by NIH/NICHD grant HD062499.
Author details
1The Jackson Laboratory, Bar Harbor, USA. 2MRC Human Genetics Unit,
Institute of Genetics and Molecular Medicine, Edinburgh, UK. 3Department of
Computer Science, Heriot-Watt University, Edinburgh, UK.
Received: 21 June 2013 Accepted: 19 August 2013
Published: 26 August 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Kontonatsios et al. Journal of Biomedical Semantics 2013, 4:7
http://www.jbiomedsem.com/content/4/1/7
SOFTWARE Open Access
Deploying and sharing U-Compare workflows
as web services
Georgios Kontonatsios*, Ioannis Korkontzelos, BalaKrishna Kolluru, Paul Thompson and Sophia Ananiadou
Abstract
Background: U-Compare is a text mining platform that allows the construction, evaluation and comparison of text
mining workflows. U-Compare contains a large library of components that are tuned to the biomedical domain. Users
can rapidly develop biomedical text mining workflows by mixing and matching U-Compares components. Workflows
developed using U-Compare can be exported and sent to other users who, in turn, can import and re-use them.
However, the resulting workflows are standalone applications, i.e., software tools that run and are accessible only via a
local machine, and that can only be run with the U-Compare platform.
Results: We address the above issues by extending U-Compare to convert standalone workflows into web services
automatically, via a two-click process. The resulting web services can be registered on a central server and made
publicly available. Alternatively, users can make web services available on their own servers, after installing the web
application framework, which is part of the extension to U-Compare. We have performed a user-oriented evaluation
of the proposed extension, by asking users who have tested the enhanced functionality of U-Compare to complete
questionnaires that assess its functionality, reliability, usability, efficiency and maintainability. The results obtained
reveal that the new functionality is well received by users.
Conclusions: The web services produced by U-Compare are built on top of open standards, i.e., REST and SOAP
protocols, and therefore, they are decoupled from the underlying platform. Exported workflows can be integrated
with any application that supports these open standards. We demonstrate how the newly extended U-Compare
enhances the cross-platform interoperability of workflows, by seamlessly importing a number of text mining workflow
web services exported from U-Compare into Taverna, i.e., a generic scientific workflow construction platform.
Keywords: UIMA, U-Compare, Web service, Annotation, Workflow, Text mining, Components
Background
The vast majority of text mining systems adopt modu-
lar approaches, which combine a number of components,
each of which solves a particular subtask, to facilitate
robust, scalable text analysis. Individually, these compo-
nents do not normally address a complete text mining
task. However, when combined together into workflows,
they become much more powerful. For instance, although
the output of a sentence splitter component is not par-
ticularly useful on its own, the use of such a component
is a vital pre-processing step for a large number of more
complex tasks, such as syntactic parsing, named entity
*Correspondence: georgios.kontonatsios@cs.man.ac.uk
Equal contributors
National Centre for Text Mining & School of Computer Science, The University
of Manchester, Manchester, M1 7DN, UK
recognition, etc. Text mining workflows provide users
with the ability to mix and match a variety of compo-
nents within a workflow. However, certain combinations
of components may result in a suboptimal workflow that
affects the overall performance of a text mining system
[1]. Thus, it is critical that developers are able to evaluate
and compare different workflows [2], in order to discover
potential problems and to determine the best performing
workflow.
Currently, there exist a number of workflow construc-
tion platforms that facilitate the development of software
tools for a range of different domains, e.g., natural lan-
guage processing (NLP), text mining, chemoinformatics
and bioinformatics. Such platforms are exploited not only
by developers but also by end-users, who can create their
own applications by combining existing components into
pipelines to carry out various tasks. Often, users need to
© 2013 Kontonatsios et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Kontonatsios et al. Journal of Biomedical Semantics 2013, 4:7 Page 2 of 14
http://www.jbiomedsem.com/content/4/1/7
share applications that they have developed with other
users. To facilitate this, most existing platforms offer an
import/export mechanism. However, workflows are nor-
mally shareable only within the boundaries of the specific
platform. This can make it difficult to use workflows inde-
pendently of the platform in which they were developed,
and violates the principles of wide software applicability
and reusability. In response to this, we propose a frame-
work for exporting text mining workflows as web services.
The resulting web services are freely and publicly avail-
able, fully compatible with open web standards, i.e., REST
protocols and accessible via any web browser.
Bioinformatics resources such as ontologies, web ser-
vices, controlled vocabularies, text mining and visual-
ization tools are becoming a necessity for life science
applications. Given the overwhelming amount of biomed-
ical knowledge recorded in textual form, i.e., full papers
or abstracts, there is a need for techniques that can iden-
tify, extract, manage and interpret this knowledge [3]. Text
mining provides a handle on isolating the relevant data
from the mountain of biomedical literature.
The Unstructured Information Management Architec-
ture (UIMA) is a framework that enables interoperability
of text analysis components, to promote their widespread
adoption. Amongst its advantages, UIMA defines a stan-
dard workflow metadata format, which has attracted
numerous text mining developers, including commercial
vendors, who are willing to distribute their own UIMA-
compliant components and systems [2,4,5]. The UIMA
framework is only intended to provide an abstract-level
formal framework for text mining component interop-
erability. It leaves the actual implementation to third
party developers, but does not sufficiently address poten-
tial incompatibilities between tools produced by different
developers.
U-Compare [2] is a text mining framework built on top
of UIMA, meaning that components developed within
the framework are compatible with any UIMA applica-
tion. U-Compare comes packaged with the worlds largest
repository of ready-to-use text mining components. A
major feature of U-Compare is that users can create
workflows using a drag-and-drop graphical user interface.
This means that different workflows can be constructed
rapidly, with no requirement for programming skills. In
addition, U-Compare provides special facilities for eval-
uating and comparing the performance of similar work-
flows. The U-Compare Type System, which models a wide
range of NLP data types, e.g., sentences, tokens, parts-of-
speech, named entities, etc., aims to address gaps in the
UIMA framework concerning the compatibility of tools
produced by different developers. UIMA components that
make use of the U-Compare Type System can be freely
combined into workflows, thus enhancing interoperabil-
ity. Although U-Compare workflows can be constructed
using both native and web-based components, the final
workflows are standalone applications.
In this paper, we propose a framework to convert U-
Compare workflows into web services that are accessible
through HTTP GET/POST requests. To perform this
transformation, we employ Apache Simple Server [6]. In
addition to its fundamental transformation functional-
ity, the proposed framework benefits from the following
facilities:
- Access to U-Compares library of ready-to-use
components, consisting of specialised bioinformatics
tools, e.g., biomedical named entity recognisers
(NERs), and NLP components, e.g., sentence splitters,
tokenisers, POS taggers supporting a number of
European languages, i.e., English, Spanish,
Portuguese, Maltese, Romanian and Calatan.
- The U-Compare Type System, which models a wide
range of NLP data types.
- A validation mechanism that verifies the integrity of
the uploaded web services, e.g., certifying the content
of the uploaded workflows.
- A post-processing component, that transforms the
resulting in-line UIMA annotations into stand-off
annotations. Although UIMA outputs stand-off
annotations, the proposed transformation using
SimpleServer imposes in-line annotations. For
reasons of presentation, we map them back to the
original stand-off format.
- A human-readable access mechanism that generates
a web-based visualisation of the stand-off annotations
generated by the above post-processing component.
Related work
Workflow construction platforms allow the integration of
both local and remote resources into multi-step applica-
tions. The resulting workflows are becoming a popular
way of conducting scientific experiments, consisting of
distinct computational steps, in a wide range of domains.
Examples of such platforms include:
- Taverna [7] and Galaxy [8], useful for bioinformatics
and chemoinformatics,
- Discovery Net [9], intended for molecular biology,
- Kepler [10], for environmental analysis,
- The Konstanz Information Miner (KNIME) [11], for
data analytics,
- The commercial system Pipeline-Pilot [12] for
business intelligence,
- U-Compare and Argo [13], both UIMA-based
platforms, for text mining and NLP.
All of the above workflow construction platforms
address the need to export and share workflows among
their users, and offer different functions and services to
Kontonatsios et al. Journal of Biomedical Semantics 2013, 4:7 Page 3 of 14
http://www.jbiomedsem.com/content/4/1/7
facilitate this. Taverna offers a process for converting stan-
dalone workflows into web services, which is comparable
to the extension to U-Compare described in this paper.
However, in contrast to the U-Compare extension, the
Taverna process is not automated, and requires additional
programming work from users. Furthermore, Taverna is
linked with myExperiment [14], an online repository of
workflows that facilitates the discovery and distribution
of Taverna workflows. Users must manually upload their
Taverna workflows to myExperiment in order to make
them available to the community. A further requirement is
that myExperiment users need to install Taverna on their
local machines before they are able to use the distributed
workflows.
The Galaxy platform is complemented by the Galaxy
free public server, an on-line version of the platform that
allows users to create, execute and share workflows. Since
workflows are executed remotely at the Galaxy free pub-
lic server, the only requirement for using Galaxy is a
web browser. The Konstanz Information Miner (KNIME)
offers the KNIME Team Space, an online service that
allows users to share not only workflows but also other
resources, e.g., data files. Discovery Net, one of the earliest
workflow construction platforms, includes Data Access
and Storage Service repositories, allowing data and work-
flows to be reused by different applications. Kepler work-
flows can be exported using a specific file format, i.e., the
Kepler Archive file, and then shared via a central repos-
itory, the Kepler Component Repository. Pipeline-Pilot
includes a web-based repository for sharing workflows,
i.e., Pipeline Pilot Web Port.
Although all of the above platforms allow users to share
workflows and resources, the distributed workflows are
accessible only via the on-line interfaces provided by the
individual platforms. In addition, web-based workflows
are restricted to the workflow platform in which they
were developed, meaning that their interoperability is lim-
ited. In contrast to previous efforts, the work described
in this paper completely abstracts the exported web-based
workflows, not only from programming languages or soft-
ware library dependencies, but also from the underlying
platform, i.e., U-Compare.
Standalone workflows, although sharable, are typically
platform-dependent and can be discovered by other
potential users through web-pages and forums. To be
reusable in applications other than the platform in which
they were originally developed, they require extra work,
mainly due to incompatibilities of data types and plat-
forms. In contrast, web services are inherently compatible
with each other and therefore facilitate interoperability
[15,16]. Such interoperability can simplify the construc-
tion of new networked and pipelined applications. In
addition, web services typically run on servers and can
be accessed from devices with limited processing power,
such as smartphones and netbooks. In the domain of life
sciences, there is an active and on-going interest in web
services. Bioinformatics tools are being made available as
web services, e.g., the Basic Local Alignment Search Tool
(BLAST) [17], and accessible through online repositories,
e.g., the European Bioinformatics Institute Web Services
[18], Biocatalogue [19,20], while web service frameworks,
e.g., BioMoby [21], allow the interaction of web services in
an interoperable way.
In this paper, we present a web application framework
to create web services automatically from U-Compare
workflows. The framework is directly linked with the U-
Compare user interface, thus allowing users to create a
web-based, publicly accessible version of their workflow,
using only two clicks of the mouse.
The rest of the paper is organised as follows: In the
Methods section, a discussion of user requirements and
design objectives of the U-Compare extension is followed
by an overview and technical details about the integrated
system, which combines the web application framework
with U-Compare. Subsequently, a description of the archi-
tecture of the framework is given. In the Results and
discussion section, we provide details of the 14 web ser-
vices that have been created using the extended version
of U-Compare, which allow the processing of text belong-
ing to different domains and written in different European
languages. We then describe the user-centred evalua-
tion of the extended U-Compare system. Finally, in the
Conclusions section, we summarise our contribution and
propose some directions for future work.
Implementation
In this section, we firstly discuss the user requirements
and design objectives of the proposed extension. Subse-
quently, we present an overview of the integrated system,
which combines the web application framework with U-
Compare, and then provide details of themechanisms that
allow the integration of the infrastructures. Finally, we
describe the architecture of the framework.
Requirements and design objectives
Often, researchers must download and install software
libraries before being able to use standalone applications,
which is a potential drawback for those looking for out-
of-the-box solutions. In contrast, web services are loosely
coupled components that enhance information accessibil-
ity, allow interpretation of resources and are suitable for
the creation of workflows. The only prerequisite is that
the input and output types of combined components are
known and must match with each other.
Based on the advantages that web services offer, we have
implemented a U-Compare extension that allows users to
create web services from their standalone workflows. This
is done completely automatically, and with the minimum
Kontonatsios et al. Journal of Biomedical Semantics 2013, 4:7 Page 4 of 14
http://www.jbiomedsem.com/content/4/1/7
of effort. The extension consists of two parts, based on
server/client operations, as follows:
- A modification of the U-Compare interface, to allow
it to generate all the necessary information to
automatically deploy a web service and to upload the
exported workflow to a server (client side).
- A web application framework that is responsible for
the actual deployment of a standalone workflow as a
web service (server side).
For the client side module of the infrastructure, devel-
oped as part of the U-Compare platform, the only design
objective we identify is to allow users to create web ser-
vices from workflows as easily as possible. Based on this,
the only information required from users is the provi-
sion of a name for the web service. Optionally, users can
manually add a description of their workflow, to allow sub-
sequent searching. U-Compare will then try to produce
metadata for each exported web service automatically, by
looking at the descriptor files of the components that
are present in the workflow. This metadata is used for
documentation purposes.
Clear documentation of each web service is a funda-
mental design objective of the infrastructure, since users
of the services need to understand their capabilities before
deciding whether to use them. Based on these objectives,
the U-Compare extension generates an XML file that con-
tains a description of the workflow and its functionality,
JOURNAL OF
BIOMEDICAL SEMANTICS
Brochhausen et al. Journal of Biomedical Semantics 2013, 4:23
http://www.jbiomedsem.com/content/4/1/23RESEARCH Open AccessDeveloping a semantically rich ontology for the
biobank-administration domain
Mathias Brochhausen1*, Martin N Fransson2, Nitin V Kanaskar3, Mikael Eriksson2, Roxana Merino-Martinez2,
Roger A Hall1, Loreana Norlin2, Sanela Kjellqvist2, Maria Hortlund2, Umit Topaloglu1, William R Hogan1
and Jan-Eric Litton2Abstract
Background: Biobanks are a critical resource for translational science. Recently, semantic web technologies such as
ontologies have been found useful in retrieving research data from biobanks. However, recent research has also
shown that there is a lack of data about the administrative aspects of biobanks. These data would be helpful to
answer research-relevant questions such as what is the scope of specimens collected in a biobank, what is the
curation status of the specimens, and what is the contact information for curators of biobanks. Our use cases
include giving researchers the ability to retrieve key administrative data (e.g. contact information, contact's
affiliation, etc.) about the biobanks where specific specimens of interest are stored. Thus, our goal is to provide an
ontology that represents the administrative entities in biobanking and their relations. We base our ontology
development on a set of 53 data attributes called MIABIS, which were in part the result of semantic integration
efforts of the European Biobanking and Biomolecular Resources Research Infrastructure (BBMRI). The previous work
on MIABIS provided the domain analysis for our ontology. We report on a test of our ontology against competency
questions that we derived from the initial BBMRI use cases. Future work includes additional ontology development
to answer additional competency questions from these use cases.
Results: We created an open-source ontology of biobank administration called Ontologized MIABIS (OMIABIS) coded
in OWL 2.0 and developed according to the principles of the OBO Foundry. It re-uses pre-existing ontologies when
possible in cooperation with developers of other ontologies in related domains, such as the Ontology of
Biomedical Investigation. OMIABIS provides a formalized representation of biobanks and their administration. Using
the ontology and a set of Description Logic queries derived from the competency questions that we identified, we
were able to retrieve test data with perfect accuracy. In addition, we began development of a mapping from the
ontology to pre-existing biobank data structures commonly used in the U.S.
Conclusions: In conclusion, we created OMIABIS, an ontology of biobank administration. We found that basing its
development on pre-existing resources to meet the BBMRI use cases resulted in a biobanking ontology that is re-useable
in environments other than BBMRI. Our ontology retrieved all true positives and no false positives when queried
according to the competency questions we derived from the BBMRI use cases. Mapping OMIABIS to a data structure
used for biospecimen collections in a medical center in Little Rock, AR showed adequate coverage of our ontology.* Correspondence: mbrochhausen@uams.edu
1Division of Biomedical Informatics, University of Arkansas for Medical
Sciences, Little Rock, AR, USA
Full list of author information is available at the end of the article
© 2013 Brochhausen et al.; licensee BioMed Central Ltd. This is an open access article distributed under the terms of the
Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,
distribution, and reproduction in any medium, provided the original work is properly cited.
Brochhausen et al. Journal of Biomedical Semantics 2013, 4:23 Page 2 of 9
http://www.jbiomedsem.com/content/4/1/23Introduction
Biobanks are a critical resource in translational science,
such as translational oncology, as they provide speci-
mens essential to the identification of novel biomarkers
for specific therapies [1]. Recent research has provided
compelling examples of using semantic web technolo-
gies, such as ontologies, to retrieve research-relevant
data from biobanks [2,3]. However, [4] point out that
little attention is paid to collecting data about the dif-
ferent ways in which biobanks are organized. This lack
is apparent in both of the ontologies considered by the
authors of [2,3]: Neither the Ontology of Biomedical
Investigation (OBI)a, nor the Translational Medicine
Ontology (TMO)b represent biobanks, biobank orga-
nizations, or related entities. This situation makes it
impossible to query biobanks with respect to orga-
nizational structures, ownership of biobanks and speci-
mens, and the curation status of specimens. Thus, our
goal was to provide an ontology that represents the
administrative aspects of the biobanking domain to
enable querying biobank data from both the specimen
or population perspective and the administrative pers-
pective. Our ontology is called Ontologized MIABIS
(OMIABIS) and is named after the Minimum Informa-
tion About BIobank data Sharing (MIABIS) [5]. The
latter provided the starting point for our ontology de-
velopment. We recently released the initial version of
OMIABIS coded in Web Ontology Language 2.0. It can
be downloaded from http://purl.obolibrary.org/obo/
omiabis.owl. The ontology is open source and we invite
the community to develop it further with us.
In the background section we introduce MIABIS and
its use cases. In the methods section we describe our
approach to ontology development including the re-use
of existing ontologies. In addition, we introduce our
approach to testing the ability of the ontology to an-
swer competency questions derived from our use cases.
In the results section, we show the basic features of our
ontology and present the results of our evaluation of its
adequacy. Finally, we discuss future work and potential
uses of the ontology, as well as its connections to
ongoing efforts in biomedical ontology.
Background
Introducing BBMRI
For an initial domain analysis we relied on the work on
data integration done by the European Biobanking and
Biomolecular Resources Research Infrastructure (BBMRI).
During the so-called preparatory phase of BBMRI, be-
tween 20082011, the initiative comprised 54 different
partners across Europe and more than 225 associated
organizations representing over 30 countries. One of the
aims of the BBMRI is to provide the necessary formats to
compare biobank information at different levels of detail[6]. Work on data integration within BBMRI used at
least two approaches; a survey of the samples and data
of European biobanks using questionnairesresulting
in the Catalogue of European Biobanks [7], and the de-
velopment of a common information model for a hub-
and-spokes structure for national or regional biobank
nodes [8]. Because biobank data is often related to per-
sonal health data, management and sharing must fol-
low legal jurisdiction, according to Directive 95/46/EC
in the European context. In combination with several
other integration issues identified in [9], the establish-
ment of an information model for sharing biobank data
on an international level will require future effort. In
the meantime, and to meet the demand of the biobank
community to understand what data should be stored
in relation to biological samples, a minimum list of
data attributes was drafted as one of the last activities
in the preparatory phase of BBMRI. One of the activ-
ities in the Swedish BBMRI, i.e., BBMRI.se, has been to
continue the development of the minimum information
list from the European BBMRI. The updated version is
called MIABIS  Minimum Information About BIo-
bank data Sharing  and consists of fifty-two attributes
considered important for establishing a system of data
discovery for biobanks and sample collections. To
avoid legal issues related to individual subjects, cases
or samples are not considered at present [5]. The attri-
butes employ existing standards, e.g., the Sample
PREanalytical Code (SPREC) [10], ICD Codesc, and
definitions developed by the Public Population Project
in Genomics (P3G)d and the International Society for
Biological and Environmental Repositories (ISBER)e.
Use cases for MIABIS & OMIABIS
MIABIS was developed in the context of several use
cases described by invited researchers as part of the
BBMRI project. Our two example use cases stem from
the development of MIABIS:
a) Search for tissue samples from donors diagnosed
with nemaline myopathy. Determine the age group.
What are the sample storage conditions? Contact
the biobank for detailed information about the
biopsy samples and whether myoblast cell cultures
have been grown from these samples.
b) Search for sample collections having at least 10
cases with tissue from the thoracic aorta as well
as blood, serum, or plasma from the same
donor. Also check if clinical data has been
registered for the donors such as physical
measurements. Contact the person responsible
for the sample collection to obtain detailed
information on the specific kind of thoracic
aorta biopsies of interest. Also assure that the
Brochhausen et al. Journal of Biomedical Semantics 2013, 4:23 Page 3 of 9
http://www.jbiomedsem.com/content/4/1/23biopsies were performed +/? one week in
relation to the blood sampling.
Use case b) would require inclusion of individual-
level data. As mentioned above, the attributes for
representing data about individual donors and speci-
mens were dropped during MIABIS development due
to regulatory issues.
Already, MIABIS is being used in a structured Scandi-
navian survey to gather information about sample col-
lections stored in biobanks in a searchable database
(www.bbmriregister.se). Increasing the total searchable
information could include uploading new data directly
to the existing system, and/or developing external data-
bases that structure the information according to MIABIS.
In the latter case, an ontologized version of MIABIS will
be used to perform a federated search across the multiple
databases. This search capability will minimize the effort a
researcher must expend to search for sample collections of
interest, by avoiding the need to query several separate da-
tabases one by one. Hence, the University of Arkansas for
Medical Sciences and Karolinska Institutet, representing
BBMRI.se, decided to initiate a biobank ontology develop-
ment project as a joint effort.
Methods
Our aim is to provide a semantically rich representa-
tion of biobank administration to facilitate the sharing
of biobank data. We based our development on an ana-
lysis of the minimum requirements for sharing biobank
data done within the BBMRI as captured by MIABIS.
Hence, we named our ontology OMIABIS, standing for
Ontologized MIABIS. To make the ontology easily ac-
cessible and implementable, we chose Web Ontology
Language (OWL) 2 [11] for implementation. To facili-
tate re-use and harmonization across ontologies, we
used Basic Formal Ontology (BFO)f as the upper ontol-
ogy [12,13]. In addition, the entire ontology develop-
ment followed the principles of ontology development
as set forth by the OBO Foundry [14]g.
Re-use of preexisting ontologies is key among the
OBO Foundry principles. In creating OMIABIS we
imported the Proper Name Ontology (PNO)h in its en-
tirety. PNO is based on the Information Artifact Ontol-
ogy (IAO)i. It is a formal representation of proper names
based on Devitt's theory of designation [15]. Thus,
OMIABIS is an extension of IAO. In addition, multiple
entities from other ontologies, namely the Ontology of
Biomedical Investigations (OBI)j and the Ontology of
Medically Relevant Social Entities (OMRSE)k are im-
ported using a tool based on the MIREOT methodology
[16], which was developed in a joint endeavor between
the University of Arkansas for Medical Sciences and the
University of Arkansas at Little Rock [17].We chose to re-use the ontologies mentioned above
based on the fact that they are members of the OBO
Foundry and, thus, are built according to the same basic
principles and extend the same upper ontology (BFO).
Our aim is to create ontological representations that
facilitate the integration of biobank administrative data
with biomedical research data. The latter often is anno-
tated with terms from Gene Ontology (GO) or OBI.
Thus, choosing ontologies from the very same orthog-
onal ontology library (OBO Foundry) of which the latter
are members appears to be the best strategy to accom-
plish this integration.
All directly imported ontologies (BFO, PNO, IAO) will
update automatically. MIREOT, so far, does not have a
strategy for automated updates. However, the developers
of the MIREOT plugin plan to include this functionality
in a future release.
In addition to these ontologies, the development of
OMIABIS was informed by other pre-existing ontologies
in the biobanking domain mentioned in the Discussion
section of this paper.
Because existing ontologies already represent speci-
mens, clinical studies and populations, OMIABIS repre-
sents the domain of biobank administration. Together
with terms from these specimen-focused ontologies,
OMIABIS needs to allow the level of semantic integra-
tion required by the use cases described above.
OMIABIS was developed using Protégé 4.1.0, Build 239l.
The MIREOT Plugin is Version 1.0.1. The consistency of
our ontology was verified using the HermiT 1.3.6 reasonerm.
To test the adequacy of our ontology for the BBMRI
use cases (s. Background section) we derived a set of com-
petency questions from them. Because the focus of the
ontology is the administrative aspects of biobanks, the use
cases entail some competency questions that fall outside
the scope of our ontology at this point (namely all ques-
tions related to the different donor subpopulations).
The competency questions we address and evaluate
here are:
 Which biobanks hold frozen specimens?
 Which biobanks hold blood, plasma and serum?
 Which blood plasma specimens are owned by one
specific biobank organization?
 Which departments of a specific university have
members that are serving as biobank contacts?
 What are the e-mail addresses of all biobank contact
persons at one specific biobank organization?
These competency questions were approved by the
domain experts from Karolinska Institute.
To perform DL queries that test the adequacy of the
ontology to retrieve data that answer the competency
questions, we populated an OWL file (that imports
Brochhausen et al. Journal of Biomedical Semantics 2013, 4:23 Page 4 of 9
http://www.jbiomedsem.com/content/4/1/23OMIABIS) with instances or individuals from a made-
up biobank example. In OWL it is possible to represent
the individual members of classes. OMIABIS per se
does not represent any individuals, but it imports 326
individuals from GEO that represent nations and their
administrative subdivisions (to enable capture of the
mailing addresses of biobank contacts). We included
both true positives and false positives to the instance-
level OWL file, to ensure that queries did not retrieve
incorrect information. This file is called CompetencyTest.
owl, and can be downloaded from: http://omiabis-dev.
googlecode.com/svn/branches/CompetencyTest.owl. In
addition, we submitted the file to this journal as
Additional file 1.
The actual queries we ran together with the results
can be found in Table 1.
Results
Implementation of OMIABIS
The latest release of OMIABIS in OWL can be down-
loaded from the permanent URL http://purl.obolibrary.
org/obo/omiabis.owl. In our research we focused on
representing the MIABIS data attributes focused on
biobanks and studies/sample collections, which com-
prises all classes and object properties closely related to
administrative aspects.
The central class of any biobank ontology ought to be
the class of biobanks or biorepositories. MIABIS differ-
entiates biobanks from the organizations that own them.
Accordingly, OMIABIS defines "biobank" as follows: "A
biobank is a collection of samples of biological sub-
stances (e.g. tissue, blood, DNA) which are linked to
data about the samples and their donors. They have a
dual nature as collections of samples and data." The def-
inition is derived from the definition for human biobank
in [18]. The latter does not define "biobank in general,
but we generalized their definition to be applicable to
any kind of biobank. The class is formally restricted to
be the equivalent ofn:
Notably, the biobank as such is neither an organization
nor a facility, but the aggregate of the specimens and the
data regarding these specimens. OMIABIS also represents"biobank organization". Its textual definition is: "A biobank
organization is an organization bearing legal personality
that owns or administrates a biobank". "Biobank orga-
nization" is equivalent to:
Referring to the class "legal person role" from
OMRSE is necessary due to the fact that the definition
of organization in OBI does not refer to legal
personalityp. Any group of human beings that has
some organizational rules fulfills the textual definition
according to OBI. However, for our use case legal per-
sonality is crucial, since within the BBMRI framework
we are concerned with management of certain rights
and obligations, which are held by legal persons. The
formal description of biobank organization uses two
object properties which have been specifically created
for OMIABIS:
1. "owns"
Elucidation: This is a primitive relation. This relation
is the foundation to the owners right to have the owned
entity at his/her full disposal.
Domain: Homo sapiens
OR organization
OR collection of humans
OR aggregate of organizations
Range: information content entity
OR material_entity
Characteristics: asymmetric
The elucidation for this primitive relation is based on
Reinach's legal ontology [19]. For further material on the
ontology of claims and obligations see [20].
2. "administrates"
Definition: "a administrates b if c owns b and some
rights and obligations grounded in the owning relation
regarding b are transferred[q] from c to a."
Domain: Homo sapiens
OR organization
OR collection of humans
OR aggregate of organizations
Range: information content entity
OR material_entity
Characteristics: asymmetric
OMIABIS includes a total of 249 classes and 64 object
properties. Of the 249 classes 34 classes are restricted
by an equivalent class axiom. 35 classes and object
properties were newly created for the initial version of
Table 1 DL Queries executed on the Competency Test OWL file and results
Competency question DL Query Recall Precision Ratio Reasoning
time (in ms)
Which biobanks hold frozen specimens? biobank and has_part some 'frozen specimen' 100% 100% 6/6 76.9
Which biobanks hold blood, plasma
and serum?
biobank and has_part some 'blood plasma specimen' and
has_part some 'blood serum specimen' and has_part some
'blood specimen'
100% 100% 5/5 53.2
Which blood plasma specimens are owned
by one specific biobank organization?
'blood plasma specimen' and part_of some (biobank and 'is
owned by' some {'Unseen University'})
100% 100% 6/6 45.4
Which departments of a specific university
have members that are serving as
biobank contacts?
department and 'has organization member' some (bearer_of
some 'biobank contact role')
100% 100% 6/6 30.2
What are the e-mail addresses of all
biobank contact persons at one specific
biobank organization?
'email address' and 'is contact information about' some
(bearer_of some 'biobank contact role' and 'is member of
organization' some {'Unseen University'})
100% 100% 6/6 55.2
Each query is encoded in a separate test method in Scala using the Java OWL-API. Each method reloads the ontology, builds the necessary axiom from API calls,
adds the axiom, executes the query, and removes the axiom from the ontology. Each trial was conducted by running each method 100 times and reporting the
average running time in milliseconds. Ten trials were conducted.
Brochhausen et al. Journal of Biomedical Semantics 2013, 4:23 Page 5 of 9
http://www.jbiomedsem.com/content/4/1/23OMIABIS. A textual definition is given for all newly
created classes and object properties. Figure 1 shows a
semantic network for the central classes of OMIABIS
and how they are used in retrieving data matching the
competency questions.
The OMIABIS labels tend to be very long, since they
are referring to the ontological hierarchy. However, we
foresee that for future use cases we might add more and
shorter labels for those classes to accommodate devel-
opers and users. In OMIABIS the MIABIS attributes are
given as "alternative name" for the class in question.
Performance of OMIABIS regarding the competency
questions
Table 1 shows the DL queries we executed using the DL
query tab of Protégé and their results. The test ontology
based on OMIABIS and populated with example individ-
uals performed flawlessly in answering the competency
questions as specified in the Methods section.
Discussion
OMIABIS in relation to pre-existing efforts in
biobank ontology
Ontologies have been identified as a key technology to
overcome the lack of semantic integration of biobank-
related data [21]. [3] demonstrates how pre-existing on-
tologies, namely the Ontology of Biomedical Investiga-
tion (OBI) [22] and BioTop [23] can be efficiently used
to represent data regarding samples and sample curation
in a semantically rich way. The methodology used, and
the criteria applied, by [3] overlap with our approach to
ontology development. In our research we focused on
administrative data regarding biobanks, sample collec-
tions, and studies producing sample collections, whereas
[3] focuses on individual specimens or samples. We plan
to use a similar approach and integrate their work in sub-
sequent research that will address the issue of propertiesof individual samples [24]. Developed an ontology-based
architecture to integrate data from heterogeneous
biobanks by unifying metadata. Since the outcome of their
development is not open source, we contacted the devel-
opers and aim to cooperate with them on the OMIABIS
project.
Another ontology that represents biobanks/biorepo-
sitories is the eagle-i resource ontology (ERO)r, which was
created for the eagle-i project. The aim of the eagle-i pro-
ject is to "create a searchable inventory of unique, rare or
otherwise hard-to-find biomedical resources  to foster
sharing and linking of resources in the larger scientific
community". ERO is used to integrate data about biomed-
ical resources and make the search functionality more
flexible [25]. However, due to its use case ERO is relatively
sparse with respect to axiomatic representation of its clas-
ses. Our goal was to provide a semantically rich ontology
that allows extensive reasoning, so re-use of ERO classes
was not an option. In addition, we found ambiguities and
lack of clarity in its representation of biobanks, specifically
the fact that it defines biobank organization instead of
biobank.. We have since begun collaborating with the
ERO developers on the branches of ERO related to
biobanks and their management.
Performance of OMIABIS regarding the competency
questions
The fact that all true positives were retrieved and none of
the false positives was, hints to the fact that the ontology
performs well. Based on our timing results when running
the queries, we suspect that the axiomatic definition of
"biobank" (given in Results section) is computationally
"expensive". Relatively simple queries that used this class
ran slower that complex queries that did not refer to it.
We are aware that the number of individuals in the
competency test ontology is small. Both (1) the initial
use cases from BBMRI and (2) the usage of OMBIABIS
Figure 1 Illustration of the central OMIABIS classes. The figure shows the central classes of OMIABIS and the object properties connecting
them. Light blue rectangles are classes; light blue arrows are object properties. Dark blue circles and edges represent instances that can be
retrieved using OMIABIS.
Brochhausen et al. Journal of Biomedical Semantics 2013, 4:23 Page 6 of 9
http://www.jbiomedsem.com/content/4/1/23in i2b2, which we present below, include federated search
in multiple databases. This raises the question of how the
ontology will be used to query across large data sets. Our
scenarios focus on researchers retrieving data about pos-
sible sources of specimens (BBMRI) or specific specimens
(i2b2) to do research. This task is part of a study's plan-
ning phase. It is not related to patient-related activities or
the performance of lab work. Thus, we believe, it is rea-
sonable to provide the researcher with the benefit of a fed-
erated search at the cost of speed. The query results could
be sent to the researcher once they are available. There
does not seem to be the need for immediate recall. None-
theless, we do want to keep reasoning time to a minimum
once we start running queries on large data sets. We
therefore plan to implement or develop methods to
ensure timely recall.
Ontological challenges regarding the MIABIS attribute
"biobank type"
Taking into consideration the immediately biobank-
related attributes in MIABIS, we found one attribute to be
challenging from the perspective of ontology develop-
ment: biobank type. Among the values for this attribute in
MIABIS are for example Pathology, Cytology, Gynecology
etc. There are strong indications from MIABIS users that
this list is not exhaustive. The rationale behind this attri-
bute and its current values is to allow the person submit-
ting data about a biobank to easily select something that
seems plausible to her. However, the downside of this ap-
proach is a certain difficulty for end users to find relevant
biobanks and studies for her research. The possible values
for biobank type in MIABIS are under elaboration andwill be updated as time progresses. A particular specimen
collection, by virtue of the type of specimens stored,
might be of interest to both pathologists and virologists,
or gynecologists and cytologists, and so on. In order to
provide useful ontological representation of these classes
we need users to specify which characteristics of a
biobank make it useful for which specialty of medicine or
which research domain.
Using OMIABIS to annotate data in i2b2
In addition to putting OMIABIS to use within the BBMRI
framework, we plan to use it for biobank data manage-
ment at University of Arkansas for Medical Sciences
(UAMS) and the Arkansas Childrens Hospital Research
Institute (ACHRI). UAMS has a Tissue Procurement
Facility and several, relatively smaller individual research
labs (i.e. the Myeloma Institute, the "Spit for the Cure"
Project). In addition, ACHRI has several independent labs
similarly managing specimens, including the Center
for Birth Defects Research, Section of Developmental-
Behavioral and Rehabilitative Pediatrics (autism research),
and the Women's Mental Health Program. Both UAMS
and ACHRI would like to share their collected specimens
and annotated data for research purposes while keeping
the operations of each lab independent. Recently, UAMS
created an Enterprise Data Warehouse (EDW) to facilitate
access to and integration of clinical, basic-science, and
other data for research and quality reporting. Retrieving
de-identified data from the EDW is done using Informa-
tics for Integrating Biology and the Bedside (i2b2) [26,27],
an open-source software application. i2b2 was designed
primarily for cohort identification, allowing users to
Brochhausen et al. Journal of Biomedical Semantics 2013, 4:23 Page 7 of 9
http://www.jbiomedsem.com/content/4/1/23perform queries to determine the existence of a set of
patients meeting certain inclusion or exclusion criteria.
Researchers have requested adding the ability to search
for specimens to the data warehouse.
To ensure semantic integration of data from multiple
biobanks with research relevant patient data, i2b2 requires
an ontology to which the data will be mapped in i2b2's
Ontology Cell. Because the management, the operations,
and the data collected in the biobanks are heterogeneous,
manual mapping of the data into a single i2b2 instance is
a challenge. Instead, a federated architecture where quer-
ies are distributed to individual nodes and the results
merged is the more promising approach. This approach
requires a common ontology like OMIABIS.
Currently the biobanks at UAMS use caTissue [28], an
open-source biospecimen management tool. caTissue is
developed under the cancer Biomedical Informatics Grid
(caGRID) initiative of the National Cancer Institute (NCI).Figure 2 Mapping between OMIABIS classes and caTissue data elemeIt facilitates the process of locating and analyzing tissue
specimens by cancer researchers based on clinical, tissue,
and genomic characteristics. caTissue Annotation forms
store clinical and other related data about specimens. Also
called Dynamic Extensions, this component allows the
creation of new forms that contain fields a site wishes to
collect about each specimen.
Despite using a single software application, integration
of data is not guaranteed in this approach because each
biobank creates its own specimen annotation forms with
different data elements. To ensure and optimize seman-
tic integration, we will incorporate an ontology into
caTissues annotation forms for all UAMS/ACHRI bio-
banks and the biobank administration data model. Then,
the data in separate caTissue instances for the biobanks
can be easily incorporated into the EDW i2b2 instance,
and queried with common semantics. The researchers run-
ning the EDW have identified OMIABIS as the ontology itnts.
Brochhausen et al. Journal of Biomedical Semantics 2013, 4:23 Page 8 of 9
http://www.jbiomedsem.com/content/4/1/23will use for biobank data. Figure 2 shows the mapping of
OMIABIS terms to caTissue data elements previously used
by UAMS' EDW.
McCusker et al. [29] have studied an option that would
convert NCIt curated Unified Modelling Language (UML)
annotations to OWL using semCDI. semCDI query for-
mulation uses a view of caBIG semantic concepts, meta-
data, and data as an ontology [30]. The result was that
OWL annotation properties are used to represent meta-
data on OWL constructs and are not considered for rea-
soning purposes. So, McCusker et al. have indeed created
their own UML-to-OWL transformation that does not
model attributes as datatype properties and does not
model NCIt annotations of UML classes using subsump-
tion. This methodology limits the expressivity and limits
reasoning ability. In addition, this approach did not
consider multiple biobanks.
To fulfill all requirements of biobank data integration
within the UAMS/ACHRI framework, in the future
OMIABIS representations will need to be integrated with
ontologies representing individual specimens and donors.
Our next step is to cooperate with other biobank pro-
jects and biobank ontologies to extend OMIABIS and to
work towards a domain ontology for biobanking as a
whole. OMIABIS will be curated and maintained as an
open-source artifact using subversion on an ongoing
basis, with periodic releases of new versions.Conclusions
In conclusion, we created OMIABIS, an ontology of
biobank administration. We found that basing its devel-
opment on pre-existing resources to meet the BBMRI
use cases resulted in a biobanking ontology that is
re-useable in environments other than BBMRI.. With re-
spect to answering the competency questions, our quer-
ies against an OMIABIS-based ontology, populated with
a small set of hypothetical test cases, retrieved only true
positives and did not miss any true positives. In addition,
the mapping to a pre-existing data structure in the
open-source caTissue application used for biospecimen
collections in a medical center in Little Rock, AR dem-
onstrated the adequacy of the coverage of OMIABIS.Endnotes
ahttp://purl.obolibrary.org/obo/obi.owl
bhttp://translationalmedicineontology.googlecode.com/
svn/trunk/ontology/tmo.owl
chttp://apps.who.int/classifications/icd10/browse/2010/en
dThe Public Population Project in Genomics (P3G):
http://www.p3g.org.
eThe International Society for Biological and Environ-
mental Repositories (ISBER): http://www.isber.org.
fBasic Formal Ontology (BFO): http://ifomis.org/1.1gPrinciples of the OBO Foundry: http://obofoundry.
org/crit.shtml
hThe Proper Name Ontology (PNO): http://purl.
obolibrary.org/obo/iao/pno.owl
iThe Information Artifact Ontology (IAO): http://purl.
obolibrary.org/obo/iao.owl
jThe Ontology of Biomedical Investigation (OBI):
http://purl.obofoundry.org/obo/obi.owl
kThe Ontology of Medically Related Social Entities
(OMRSE): http://purl.obolibrary.org/obo/omrse.owl
lThe Protégé Ontology Editor and Knowledge Acquisi-
tion System: http://protege.stanford.edu
mHermiT Reasoner: http://www.hermit-reasoner.com
nclasses printed bold, object properties in italics and
OPERATORS all caps. Definitions of classes referred to
here can be found in Table 1
oNote that this class description is based on object
properties and classes from BFO, IAO and OBI.
phttp://purl.obolibrary.org/obo/OBI_0000245
qThe 'transfers' object property is represented in
Document Acts Ontology (d-acts): http://purl.obolibrary.
org/obo/iao/d-acts.owl
rThe eagle-i Resource Ontology (ERO): http://purl.
obolibrary.org/obo/ero.owl
Additional file
Additional file 1: OMIABIS Competency Test.
Competing interests
The authors declare that they have no competing interest.
Authors' contributions
MB is the creator of the OWL file, provided the ontology-related background
of the paper and edited the paper in its entirety. In addition, he provided
the competency test. MB & WRH did the main ontological analysis of the
domain and authored the OWL implementation. MNF, ME, RMM, LN, SK, MH,
UT, WRH, JEL contributed to the ontology development and reviewed the
ontology. RAH provided the method calculating the reasoning times and ran
the measurements. NVK and UT provided the mapping to i2b2 and its
integration into i2b2. All authors reviewed and commented on the paper
until there was agreement. All authors read and approved the final
manuscript.
Acknowledgments
The work is partially funded by the Arkansas Biosciences Institute, the major
research component of the Arkansas Tobacco Settlement Proceeds Act of
2000 and by award number UL1TR000039 from the National Center for
Advancing Translational Sciences (NCATS). The content is solely the
responsibility of the authors and does not necessarily represent the official
views of NCATS or the National Institutes of Health. We would like to thank
the people involved in the European BBMRI preparatory phase, financially
supported by the European Commission (grant agreement 212111) and the
Swedish Research Council for granting the BBMRI.se project (grant
agreement 829-2009-6285). The authors would also like to thank Joseph
Baligh, Josh Hanna, three anonymous OBML 2012 reviewers and three
anonymous JBMS reviewers for their valuable comments.
Author details
1Division of Biomedical Informatics, University of Arkansas for Medical
Sciences, Little Rock, AR, USA. 2Department of Medical Epidemiology and
Brochhausen et al. Journal of Biomedical Semantics 2013, 4:23 Page 9 of 9
http://www.jbiomedsem.com/content/4/1/23Biostatistics, Karolinska Institutet, Stockholm, Sweden. 3Department of IT
Research, University of Arkansas for Medical Sciences, Little Rock, AR, USA.
Received: 17 January 2013 Accepted: 15 May 2013
PROCEEDINGS Open Access
Representing physiological processes and their
participants with PhysioMaps
Daniel L. Cook1*, Maxwell L. Neal2, Robert Hoehndorf3,4, Georgios V. Gkoutos4, John H. Gennari1
From Bio-Ontologies 2012
Long Beach, CA, USA. 13-14 July 2012
* Correspondence: dcook@uw.edu
1Biomedical & Health Informatics,
Univ. of Washington, USA
Abstract
Background: As the number and size of biological knowledge resources for physiology
grows, researchers need improved tools for searching and integrating knowledge and
physiological models. Unfortunately, current resourcesdatabases, simulation models,
and knowledge bases, for exampleare only occasionally and idiosyncratically explicit
about the semantics of the biological entities and processes that they describe.
Results: We present a formal approach, based on the semantics of biophysics as
represented in the Ontology of Physics for Biology, that divides physiological
knowledge into three partitions: structural knowledge, process knowledge and
biophysical knowledge. We then computationally integrate these partitions across
multiple structural and biophysical domains as computable ontologies by which such
knowledge can be archived, reused, and displayed. Our key result is the semi-
automatic parsing of biosimulation model code into PhysioMaps that can be
displayed and interrogated for qualitative responses to hypothetical perturbations.
Conclusions: Strong, explicit semantics of biophysics can provide a formal,
computational basis for integrating physiological knowledge in a manner that
supports visualization of the physiological content of biosimulation models across
spatial scales and biophysical domains.
Background
Researchers developing large scale, integrative projects such as the Physiome[1], the Vir-
tual Physiological Human (www.vph-noe.eu), and the Virtual Physiological Rat[2] have
aimed to use biomedical ontologies to improve access to biomedical knowledge resources
and to analyze and even integrate some of their content. As contributors to some of these
projects, we have aimed to create computable knowledge networks of biological processes
and their participants, which we term a PhysioMap. These PhysioMaps represent and
explain physiological hypotheses that are embodied in biosimulation models, and are
designed to aid in information retrieval and model integration across biomedical disci-
plines and knowledge resources. PhysioMaps are formalized versions of the kinds of infor-
mal diagrams that are routinely used in papers and presentations for representing the
physiological content of datasets, models, and research domains. Examples of PhysioMaps
are the reaction pathway diagrams as generated by KEGG[3], Reactome[4], and the Bio-
Models[5] resources. See for example, the SBML layout package, sbmllayout.sourceforge.
Cook et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S2
http://www.jbiomedsem.com/content/4/S1/S2 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2013 Cook et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in
any medium, provided the original work is properly cited.
net, for visualizations of SBML models. Similarly, for Reactome, the BioPAX formalism [6]
provides the basis for a node-and-arc representation of processes. In all of these, nodes
represent portions of chemicals that are linked by arcs that represent reaction pathways.
We argue that this node-arc-node representation of physiological processes generalizes
across temporal and structural scales as well as biophysical domains such that it is applic-
able to chemical diffusion, heat flow, transmembrane ion currents, as well as more familiar
domains such as fluid flow.
Projects that aim to integrate biomedical knowledge across multiple scales would benefit
from holistic PhysioMaps that represent the known (or hypothesized) connections
between, say, the expression of a gene, its impacts on cell signaling, and ultimately its
effects on macroscopic physiology and pathology. Current resources, such as the Gene
Ontology, are useful for annotating data and models in terms of defined biological process
classes, yet their underlying knowledge architecture lacks formal relations for linking pro-
cesses to their physical participants or for representing causal chains of processes. Our
goal has been to generalize such mappings to all structural/temporal scales and to other
biophysical domains to serve as valuable knowledge resources for displaying the scope of
integrative projects in biomedicine (as in the physiome, VPH, VPR) and as a channel for
communication between mathematical biophysicists, who express their ideas in computer
code, and nonmathematical experimentalists who express their ideas diagrammatically
and qualitatively (as illustrated in Figure 1).
With these goals, we have developed a software workflow (Figure 1) and a knowledge
architecture (Figure 2, next section)by which modelers using SemGen software[7]can
read and parse biosimulation model code (in SBML, JSim, or CellML) into a SemSim
model from which a PhysioMap file is extracted. Finally, our Chalkboard software[8]
can import this PhysioMap file for display as well as qualitative, cause-effect explora-
tion. Our approach begins with the physics-based physiological knowledge already for-
mally expressed as biosimulation code by physiologists and bioengineers.
Biosimulation models as formalized physiological knowledge
Biosimulation models are developed and curated to formally express physiological
hypotheses about how complex biological systems work and to quantitatively test these
Figure 1 Workflow by which mathematical modelers can derive PhysioMaps, using SemGen software, for
experimentalists to display and query in Chalkboard software.
Cook et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S2
http://www.jbiomedsem.com/content/4/S1/S2
Page 2 of 9
hypotheses against experimental data. Thus, for generations, physiologists and biophy-
sicists have encoded and archived models in a variety of computational languages
including SBML (sbml.org), CellML (www.cellml.org), and JSims MML(physiome.org/
jsim/), as well as more general-purpose languages that are not designed for the biome-
dical domain (e.g., MATLAB, Fortran, C++, etc.). Such biosimulation models represent
formal, physics-based expressions of one or more key biomedical hypotheses that are
of such interest as to warrant the difficult, time-consuming, and error-prone effort
required to encode, debug, and evaluate the model. By focusing first on such physics-
based simulation models, we expect to stress-test our approach against formally correct
biophysics to assure ourselves that our methods are, in fact, sufficiently rigorous to be
applied to any physical representation of physiological processes.
Results
In the following we describe our logical schema and computational architecture for
PhysioMaps, describe our methods for creating PhysioMaps from biosimulation model
code, and then present initial visualizations of example PhysioMaps derived from
models.
PhysioMap knowledge architecture
PhysioMaps and SemSim models are based on a foundation provided by the Ontology
of Physics for Biology (OPB)[9]. The OPB represents entities and relations used in
engineering systems dynamics[10,11] and biological network thermodynamics[12]. It
thus leverages formal analogies between physical properties and their quantitative
dependencies to span multiple structural and temporal scales and across multiple
Figure 2 Our knowledge architecture illustrates how PhysioMaps relate to a tripartite structure for
physiology (processes, structural entities, and biophysical dependencies) that is based on the OPB.
Cook et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S2
http://www.jbiomedsem.com/content/4/S1/S2
Page 3 of 9
biophysical domains including chemical reactions, fluid flow, diffusion, electrophysiol-
ogy, etc. For example, the dependency of chemical reaction rates on reactant concen-
trations is analogous to the dependency of fluid flow rates on fluid pressures (or ion
currents on electrochemical gradients, and so forth).As in many upper-level ontologies
(e.g., the Basic Formal Ontology, ifomis.org/bfo/) OPB physical entities are continuants
that participate in processes (occurrents). Participant types can be annotated as
classes in various biomedical ontologies such as FMA[13], CL[14]), GO[15], and ChEBI
[16]. The OPB is orthogonal to other ontologies because entities and processes are
defined dynamically in terms of thermodynamic quantities so that OPB:Dynamical
entity is defined as the bearer of a portion of thermodynamic energy and OPB:Dyna-
mical process as the flow of thermodynamic energy between participating dynamical
entities.
Figure 2 illustrates the tripartite representational schema by which PhysioMaps are
based on the OPB representational architecture: (a) dynamical entities and their
structural relations (e.g., parthood), (b) dynamical processes and their temporal rela-
tions(e.g., temporal parthood), and (c) the biophysical dependencies that constrain
how the properties of entities change over time and govern the time-course of pro-
cesses as they occur in time. These dependencies may be definitions such as Newtons
law or constitutive laws such as Ohms law that describe empirically observed depen-
dencies between forces and flows. Next, we describe each of these three classes in
more detail.
Dynamical entity classes (OPB:Dynamical entity) formally represent the physical enti-
ties (e.g., organs, cells, molecules) that participate in biological processes in the
domains of anatomy, biochemistry and physiology. Structural relations among these
physical entities include part-of and connected-to [17] whereby, for example, a car-
diomyocyte is part-of wall of left ventricle that is connected-to wall of right ventricle.
Process classes(OPB:Dynamical process) represent occurrences whereby physical enti-
ties undergo changes of composition (e.g., losing a part) or magnitudes of a physical
property (e.g., size or material flow rate). Processes are temporally related by relations
such as precedes and has-process-part that are analogous to structural relations. Prop-
erty classes (OPB:Dynamical property) represent physical observable attributes of dyna-
mical entities and processes by which the physical state of an entity and the progress
of a process is observed to occur. OPB:Physical property dependencies represent the
quantitative relations by which the values of physical properties depend upon one
another according to the definitions and laws of physics. Ohms law, for example, is
encoded as OPB:Electrical resistive flow dependency which is defined to be a relation
between the difference between two voltages and the flow of electrical current. OPB:
Fluid resistive flow dependency is an analogous dependency for fluid pressures and
flows. In each case, such dependencies relate the rate of a physical process to the phy-
sical states of their participating physical entities where, for example, the rate of a
blood flow process depends on the fluid pressure in the source blood pool and on the
pressure in the sink blood pool. It is precisely these dependency relations by which our
software identifies participants and processes by parsing the annotated mathematical
code of simulation models.
Building from this foundational OPB schema, we work with three computational arti-
facts used in the workflow of Figure 1: (1) PhysioMaps, (2) model code, and (3)
Cook et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S2
http://www.jbiomedsem.com/content/4/S1/S2
Page 4 of 9
SemSim models. PhysioMaps are files, currently encoded in XML, that consist of a set
of dynamical processes linked by the set of dynamical entities that are its participants.
Model code is the code-level implementation of biosimulation models as written in
any of the various modeling languages available (including SBML, JSim, and CellML).
SemSim (semantic simulation) models are OWL-encoded ontologies that map model
variables and equations to OPB property and dependency classes (see rightmost part of
Figure 2) while retaining the model equations and parameter values and a link to the
model file itself.
As developed above, a SemSim model makes explicit the biophysical knowledge
implicit in biosimulation model code. SemSim models combine biophysical and
structural views of particular biosimulation models in which each model variable and
parameter is annotated using a composite annotation[18,19] against a defined subset
of orthogonal biomedical ontologies (we have primarily used OPB, ChEBI, and the
FMA). Furthermore, SemSim creates a map of the mathematical dependencies
between variables so that, for example, a chemical reaction rate variable will depend
on a concentration variable for each reactant and on one or more rate-law
parameters.
Creating PhysioMaps from model code
At this stage of PhysioMap development, we aim to represent only two kinds of physi-
cal process. First, we consider physical flows (subclasses of OPB:Energy flow process)
during which a quantity of stuff (e.g., blood, molecules; attended by a corresponding
amount of energy) flows from one physical entity to another (e.g., blood flow from
aorta to femoral artery). Second, we consider OPB:Modulation processes during which
the value of one physical property (of an entity or process) directly affects the value of
some other physical property without significant energy flow. For example, in pathway
databases (e.g., Reactome, KEGG) enzyme reaction process rates are modulated by the
concentrations of activating or inhibiting molecules without regard to the details of the
actual reaction kinetics.
For flow processes, we provide two examples (see Figure 3) to demonstrate how we
generate PhysioMaps from biosimulation models. For macroscopic flow processes, con-
sider a model of cardiovascular dynamics. First, we map model variables to physical
property classes (e.g., OPB:Fluid pressure) that are linked via property-of relations to
the physical entity (e.g. FMA:Blood in aorta) that bears the property. For example, a
flow-rate variable (e.g., FLV-aorta) representing the flow of blood from one FMA:Por-
tion of blood (e.g., in left ventricle) to another (e.g., in the aorta) is annotated as an
individual of class OPB:Fluid flow rate. Such a variable is, identically, an attribute of
the flow-source entity (e.g., FMA:Blood in left ventricle), the flow-sink entity (e.g.,
FMA:Blood in aorta) and the flow process itself (e.g., OPB:Fluid flow process). Such
flow variables are annotated as subclasses of OPB:Dynamical flow rate such as OPB:
Fluid flow rate (e.g., for blood or air flows) or OPB:Chemical flow rate (e.g., for molar
chemical flows) that are attributes of a OPB:Fluid flow processor of a OPB:Chemical
flow process, respectively. In subsequent work we plan to generalize our multiscale and
multidomain approach to include OPB:Physical process classes such as OPB:Fluid
capacitive process and OPB:Transducer process by which thermodynamic energy is
stored and redistributed amongst process participants. For each such flow property,
Cook et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S2
http://www.jbiomedsem.com/content/4/S1/S2
Page 5 of 9
SemGen infers and instantiates its corresponding flow process as a PhysioMap
element.
For molecular flow processes, consider a models of glycolysis. For example, Figure 3a
shows a fragment of the glycolysis pathway derived from a curated, annotated SBML
model from the BioModels database (model ID: BIOMD0000000236). In some pathway
models, such as this one, reactants, products, and reaction modifiers are explicitly
tagged, and thus, SemGen can automatically generate a PhysioMap from the tags. In
cases where such explicit semantic tags are unavailable, SemGen derives a computa-
tional dependency network by parsing the SBML-encoded equations that include flow
process variables (i.e., those annotated as OPB:Dynamical flow rate subclasses).
SemGen then: (1) identifies physical property variables upon which the flow variable
depends and which are annotated to be of the same biophysical domain as the flow
variable (e.g., OPB:Fluid kinetic domain) and (2), identifies force (OPB:Force property)
or amount (OPB:Amount property) variables that can play the role of a driver for the
flow. From such relations, SemGen infers that the entities that bear the force or
amount properties are thus participants in the process. SemGen then identifies down-
stream process participants by finding force and amount variables that are mathemati-
cally dependent on the flow term.
Displaying and interrogating PhysioMaps
Having created a SemSim model, we derive and export a PhysioMaps from SemGen
into Chalkboard for display and cause/effect querying. Chalkboard is an editor for the
BioD biological description language[20] that is similar in design intent to SBGN Pro-
cess Description language [21], SBML Layout Tools (http://sbmllayout.sourceforge.net/
SBMLLayout/Welcome.html), and ChiBE for BioPAX models[22]. In our prototype
demonstrations, we have extended Chalkboard to read PhysioMap files and to repre-
sent processes as rectangles with arrows that link to circle icons that represent partici-
pating entities. Figure 3a shows, on the left side, a PhysioMap representation of a
Figure 3 Prototype PhysioMaps as visualized by the Chalkboard system (Cook et al., 2007): (a) shows a
simple model of glycolysis and (b) shows blood flow processes in a cardiovascular (CV) model.
Cook et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S2
http://www.jbiomedsem.com/content/4/S1/S2
Page 6 of 9
portion of glycolysis, derived from the BioModels model BIOMD0000000236, and on
the right side, a portion of a cardiovascular model derived from a lumped-parameter
model of cardiovascular dynamics [23]. In Figure 3a, the arrows represent molar che-
mical flows (except for the modulatory processes), whereas in Figure 3b, the arrows
are fluid flows between portions of blood located in a circuit of blood vessels and
heart chambers.
PhysioMaps in Chalkboard can be interrogated about qualitative perturbations intro-
duced anywhere in the network. Chalkboards Path Tracing feature supports thought
experiments to display the consequences of researchers interpretations of how a
dynamic system might behave for a given set of hypothetical experimental conditions.
Thus, experimental perturbations (e.g., an increment in intracellular glucose in Figure
3a, or in the amount of aortic blood in Figure 3b) can be propagated through a func-
tional network as increments and decrements in the amounts or flow-rates of con-
nected participants and processes. To do this effectively, all modulators (e.g. enzymes
in biochemical reactions) must be annotated with a polarityare the modulators inhi-
bitors or stimulators? As implemented in Chalkboard, Path Tracing can trace A-to-B
pathways in complex networks, detect positive- and negative-feedback loops, and dis-
play the qualitative (up or down) responses of all affected processes and participants in
the network.
Discussion and conclusions
We have developed the idea of PhysioMaps and some prototype implementations that
extend our current SemSim technology to make explicit the connections among biolo-
gical processes, the physical entities that participate in those processes, and the biophy-
sics that determine how biological processes occur over time. This tripartite formal
view of physiology (as in Figure 2) will better enable knowledge and model integration
across resources, which in turn will enable improved understanding and better models
of physiological processes.
Our approach is domain- and scale-independent. Prior process visualization efforts
such as for BioPAX or SBML models apply only to a single biophysical domainmass
action chemical kinetics at a single scalesubcellular biochemical reactions. In con-
trast, because our approach is based on foundational theories of systems dynamics and
classical physics, PhysioMaps generalize across all biophysical process domains (e.g.,
fluid flow, diffusion, or electrophysiology) as required for integrated analysis of multi-
scale physiological systems.
PhysioMap next steps
We are aware of a number of limitations of our work to date, and these help direct the
next steps in our research. First, our current Chalkboard implementation does not
leverage modern graph layout algorithms and drawing packages; although these user
interface details do not affect our theory and approach to modeling, they certain do
affect the ability of outside users to test or use our software.
Second, although we have generated a number of example PhysioMap files, we have
not yet carried out any sort of exhaustive survey of biosimulation models to under-
stand where are methods work well and where they do not. We expect that the gen-
eration of PhysioMaps can never be a fully automatic process for all models. As should
Cook et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S2
http://www.jbiomedsem.com/content/4/S1/S2
Page 7 of 9
be clear from Figure 2, a PhysioMap is an abstraction, both of the SemSim model, and
even more so, of the underlying biosimulation model. Thus, choosing what to abstract
and what to retain will always require some human guidance. However, our expecta-
tion is that we can nearly automate the process for certain classes of biosimulation
models, by leveraging common assumptions and characteristics of those models (e.g.,
the class of all biochemical reaction models that are encoded in SBML).
Third, a challenge we face is that our approach depends on understanding the
sources, sinks and mediators of specific flows in a model. Where models include expli-
cit annotations to specify these (as in many models in the BioModels repository), we
can generate PhysioMaps in a fully automatic manner. However, lacking these annota-
tions, our approach depends on parsing and deriving these semantics based on the
potentially complex mathematical dependencies among flow process variables and the
forces and amounts of participating physical entities. Such dependencies can obscured
by intervening variables and equations that may not have clear semantics. Future ver-
sions of our SemGen tool will make it possible to create and edit such links between
processes and their participants to ensure that PhysioMaps generated from SemSim
models accurately represent the physiological architecture of the model.
The ability to visualize and perturb biosimulation models (via thought-experiments)
is a significant step toward demystifying biosimulation modeling results, and can
potentially improve how biosimulation is used to direct experimental research. How-
ever, to develop and refine their hypotheses, experimenters will need to be able to
dynamically combine and modify PhysioMaps. If a researcher integrates multiple Phy-
sioMaps that represent the same biophysical process (with the same participants and
properties), then this process should be uniquely represented in the merged system.
Our prior work in merging SemSim models may be applicable to this challenge since
integrating SemSim models requires identifying and resolving the components of the
models that are semantically identical. In the long run, for large, integrative projects to
succeed (e.g., the Virtual Physiological Human or the Virtual Physiological Rat[2]), this
sort of PhysioMap merging and visualization capability is essential.
Our vision is to develop multiscale/multidomain physiological pathway maps, pat-
terned after biochemical pathway maps, that represent the physiological content of bio-
logical models and datasets for use in large-scale physiological integration projects.
These PhysioMaps should help researchers (a) understand the physiology implicit in
the mathematics of biosimulation models, (b) manipulate and perturb those models in
a graphical manner, and (c) combine and modify PhysioMaps to develop new experi-
mental ideas or hypotheses that will drive research forward toward a more comprehen-
sive understanding of biological processes.
Authors contributions
All coauthors participated in numerous discussions leading to the development of these ideas. MLN led all of the
development work with SemGen and SemSim models, including the annotation and development of the example
models. DLC led the writing of this manuscript, as well as all Chalkboard and OPB development work. JHG and MLN
contributed to the writing and editing of the manuscript.
Competing interests
The authors have no competing interests in this work.
Acknowledgements
This work was partially funded by the VPH Network of excellence, EC FP7, project #248502.
Cook et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S2
http://www.jbiomedsem.com/content/4/S1/S2
Page 8 of 9
Declarations
This supplement was partially funded by the VPH Network of excellence, EC FP7, project #248502.
This article has been published as part of Journal of Biomedical Semantics Volume 4 Supplement 1, 2013: Proceedings
of the Bio-Ontologies Special Interest Group 2012. The full contents of the supplement are available online at http://
www.jbiomedsem.com/supplements/4/S1
Author details
1Biomedical & Health Informatics, Univ. of Washington, USA. 2Bioengineering, Univ. of Washington, USA. 3Dept of
Physiology, Development & Neuroscience, Univ. of Cambridge, UK. 4Computer Science Dept., Univ. of Aberystwyth,
UK.
Published: 15 April 2013
PROCEEDINGS Open Access
A task-based approach for Gene Ontology
evaluation
Erik L Clarke, Salvatore Loguercio, Benjamin M Good, Andrew I Su*
From Bio-Ontologies 2012
Long Beach, CA, USA. 13-14 July 2012
* Correspondence: asu@scripps.edu
The Scripps Research Institute, La
Jolla, CA, USA
Abstract
Background: The Gene Ontology and its associated annotations are critical tools for
interpreting lists of genes. Here, we introduce a method for evaluating the Gene
Ontology annotations and structure based on the impact they have on gene set
enrichment analysis, along with an example implementation. This task-based
approach yields quantitative assessments grounded in experimental data and
anchored tightly to the primary use of the annotations.
Results: Applied to specific areas of biological interest, our framework allowed us to
understand the progress of annotation and structural ontology changes from 2004
to 2012. Our framework was also able to determine that the quality of annotations
and structure in the area under test have been improving in their ability to recall
underlying biological traits. Furthermore, we were able to distinguish between the
impact of changes to the annotation sets and ontology structure.
Conclusion: Our framework and implementation lay the groundwork for a powerful
tool in evaluating the usefulness of the Gene Ontology. We demonstrate both the
flexibility and the power of this approach in evaluating the current and past state of
the Gene Ontology as well as its applicability in developing new methods for
creating gene annotations.
Background
Introduction
The Gene Ontology [1] (GO) provides a resource for systematically classifying and anno-
tating gene function. The annotations associated with the GO play a critical role in mod-
ern biology and cover many organisms. Many of these annotations, especially for human,
are the product of manual and automated annotation by the Gene Ontology Annotation
(GOA) team at UniProt [2]. For the human genome, over 10,000 GO terms are used to
annotate gene function in over 200,000 annotations.
Annotations in GOA come from a variety of sources. Broadly, they either derive from
manual curation, or from automatic inference based on pre-existing annotations and
resources. Currently over half of the human GO annotations are the result of manual
curation as opposed to the automatic electronically inferred annotations (IEAs) [3].
Historically, manually-added annotations are considered to be of higher quality [4] than
IEAs, though recent work is challenging this conception [5]. Both sets of annotations are
Clarke et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S4
http://www.jbiomedsem.com/content/4/S1/S4 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2013 Clarke et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in
any medium, provided the original work is properly cited.
continually revised and expanded based on published advances in the literature and sus-
tained biocuration efforts. The ever-increasing size of the human GO annotation dataset
(Figure 1) suggests that the structured representations of gene function are still very
much in flux.
Due to the importance of GO annotation in modern biology, significant effort has
been expended to assess the quality of these annotations. Measures of annotation com-
pleteness, accuracy, and precision are important tools for the GO developers, and are
critical if researchers are to use the annotations in real-world applications with confi-
dence. Numerous methods for assessing annotation error rate and accuracy have been
developed [6-8]. However, most of these focus on the creation of ad-hoc qualitative
metrics based on annotation evidence codes and term specificity. Such methods, (e.g.
[7]) have significant drawbacks: for one, it is conceivable to construct a random and
artificial ontology that would score highly on these metrics without any bearing on the
real world. Likewise, measures of accuracy based on term specificity have been called
into question [5]. Other approaches that address annotation error rates or accuracy
such as [6] and [8] downplay the role of ontology structural quality, and ignore the
effect that the ontology structure can have on real-world applications.
An approach that quantified the performance of the ontology at common tasks
would allow us to understand the strengths and weaknesses of the ontology and its
annotations directly [9]. In this paper, we present an approach that assesses the quality
and utility of GO and its annotations through their performance in a common use-
case, namely gene-set enrichment analysis.
A task-based approach
Enrichment analysis is the process of using a collection of gene annotations to determine
what terms are enriched or over-represented in a set of genes, which are often pro-
duced from genome-scale experiments (e.g., gene expression analysis, genome sequen-
cing, etc.). For instance, a cancer researcher, presented with a set of genes implicated in
neuroblastoma metastasis, could use enrichment analysis and the GOA to determine
what biological processes are most associated with her gene set. This process is among
Figure 1 Cumulative annotations from 2002 through 2011 The increase in individual gene:term
annotation pairs from the start of 2002 through the start of 2012. These numbers were created by
progressively filtering earlier annotations from the 2012 human annotation file available at [3].
Clarke et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S4
http://www.jbiomedsem.com/content/4/S1/S4
Page 2 of 11
the most common applications of the GO annotations, and is a critical resource for the
analysis of genome-scale profiling experiments [10][11][12].
Our framework uses enrichment analysis to determine the effectiveness of the GO
annotations at providing biologically accurate results (Figure 2). This produces a mea-
sure of utility based on a real-world application, and helps us understand the strengths
and weaknesses of the GO and its annotations. Broadly, our framework consists of the
following workflow: First, we select one or more terms to test that are representative
of our area of interest. Next, select a dataset that is clearly representative of those
terms, and use standard statistical methods to define a list of differentially expressed
genes. Conduct an enrichment analysis, using the GO annotations and the chosen
dataset, and obtain a list of terms that are significantly enriched among the list of dif-
ferentially expressed genes. The performance of the GO knowledgebase is related to
the significance of the term of interest in the resulting list. Notice that the perfor-
mance measured in this manuscript focuses on the specific biological context chosen,
and not the quality of the GO knowledgebase as a whole. Rather, we provide a
Figure 2 Analytical framework for evaluation of GO performance Using our method, the user first
identifies a GO term or biological area of interest. He or she then collects experimental data known to be
representative of that area, and performs an enrichment analysis with the gene list and appropriate GO
annotation sets. By identifying the significance (or lack therof) of the term or terms associated with the area of
interest, the user can quantify the performance of the GO and its annotations for this domain.
Clarke et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S4
http://www.jbiomedsem.com/content/4/S1/S4
Page 3 of 11
framework for evaluating its precision and coverage in specific domains where compel-
ling experimental evidence is available.
Implementation and testing
To demonstrate the framework, we present an example implementation and apply it
towards a selected term of interest. The methodology and results of this experiment
are discussed below.
Results
Creating the Analytical Framework
Our framework consists of the following steps:
1 Identify term(s) of interest T. These may be representative terms of an area of interest
or a sample across the GO structure.
2 Collect experimental data that are clearly related to T. For example, if T was angio-
genesis, one might use gene expression datasets related to highly angiogenic tissues to
derive a list of overexpressed genes.
3 Use standard statistical methods to define a list of differentially expressed genes.
4 Conduct an enrichment analysis on each gene list from (3).
5 From the list of enriched terms resulting from (4), identify the rank and p-value of T.
The expectation is that T will be significantly enriched against the background terms;
whether or not that expectation is met, along with Ts relative rank and score, indicates
the efficacy of Ts annotation set.
The framework as presented here is generalizable. For instance, it is up to the user to
determine what enrichment method to perform in Step 4. It is also up to the user to
determine how to best identify terms of interest and the associated datasets.
One basic use case is to evaluate how well the GO annotations perform at reproducing
biological expectations for a dataset. A researcher interested in angiogenesis may collect a
series of datasets derived from tumor samples that are known to express traits associated
with blood vessel formation, for instance. He or she would then use these datasets in an
enrichment analysis with the current version of the GO annotations, and observe how
significant the angiogenesis-related terms were in the resulting list of enriched terms. The
researcher could compare the rank of angiogenesis terms to other unassociated biological
processes to get an idea of how well the GO annotations cover this area. This procedure
could be used in any area for which experimental data are available. To test these various
analyses, we implemented the framework as a set of Python scripts called the GO Evalua-
tion Suite (GOES). This resource is available as open-source software [13].
Test 1: Evaluating current GO performance on a selected dataset
In our tests, we selected the GO term angiogenesis (GO:0001525) as our term of interest.
Angiogenesis, the formation of blood vessels, is a well-known trait of a type of brain
tumor called glioblastoma multiforme. Glioblastoma multiforme, a high-grade glioma, is
unusual for tumors because of its highly vascular nature [14]. We then searched the NCBI
Gene Expression Omnibus [15] (GEO) for expression microarray datasets involving glio-
blastoma and selected one such dataset, GDS1962 [16]. GDS1962 compares general gene
expression in various gliomas, including glioblastoma, to non-tumor control samples. The
structure of the experiment that produced this dataset and the tissues it tested made us
Clarke et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S4
http://www.jbiomedsem.com/content/4/S1/S4
Page 4 of 11
reasonably confident that angiogenesis was a highly significant trait of the glioblastoma
samples. We therefore expect that conducting an enrichment analysis using the Biological
Process branch of the GO would yield angiogenesis as a significant term. Our expectations
were validated: an enrichment analysis using the protocol (outlined in more detail in
Methods) yielded a p-value of 1.07 x 10-8 for the term angiogenesis. Even after applying a
conservative Bonferroni multiple-testing correction, this enrichment for angiogenesis was
still significant in GDS1962 (p = 6.3 x 10-5).
Designing a time-based analysis
We can expand this method to assess the changes made to the GO and its annotations
over time. Because the GO is constantly changing, it is important to be able to determine
if the alterations improve or decrease its performance in common use-cases. To assess the
quality of the GO and its annotations over time, we performed the following experiment:
1 Select terms and representative datasets as in Steps 1 and 2 in the original
framework.
2 By using the version-control system of the Gene Ontology Consortium [17] retrieve
versions of the GO structure for each time point under consideration. For instance, to
look at the changes from 2004 - 2012, we would collect nine iterations of the GO, one
from each year.
3 Retrieve matching iterations of the GO annotations from [18].
4 For each time point, complete the remaining steps in the original framework. Note
the significance of the term(s) of interest in each time point.
5 Plot or otherwise compare the changing significance of the term across the time
points.
Test 2: Evaluating GO performance over time
We implemented the time-based analysis by sampling the structure of the GO in May
of each year from 2004 through 2012 from the GO version control repository and the
human GO annotations from concurrent time points in the GOA repository. We pro-
duced each terms gene set by associating that years annotations to the terms in the
ontology. The rest of the procedure was identical to the previously-described process,
and yielded the significance of the term angiogenesis for GDS1962 from 2004 to 2012.
Figure 3A (red line) shows the changing p-value for angiogenesis in the glioblastoma
sample during this time. From 2007 to 2012, the p-value of angiogenesis decreased by
nearly five orders of magnitude (0.024 to 1.07 x 10-8).
Tracking the significance of the most relevant terms over time
Another variation of the framework is the omission of pre-identified terms of interest in
favor of observing the most significant terms for a dataset. More specifically, instead of
a priori identifying a term of interest, we select a gene list of interest and run an enrich-
ment analysis on those genes. We then select the most significant terms and observe how
their significance changes as the structure and annotations change (as in the time-based
analysis). The benefit of this approach is that it yields a broader picture of the GO evolu-
tion by identifying what was reported as being most significant for a dataset in past ver-
sions of the GO. It allows researchers to see how recently a term became significant, and
which terms have become less relevant.
Clarke et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S4
http://www.jbiomedsem.com/content/4/S1/S4
Page 5 of 11
Figure 3 P-values of angiogenesis and ten most significant terms for glioblastoma samples over
time A) The red line shows the change in p-value of the angiogenesis term from 0.024 (2007) to
1.07x10-8 (2012). The grey lines are the p-values for the ten most significant terms in enrichment
analysis from 2012 traced through time (see Table 1). The vertical axis is on a log scale. Some lines start
after 2004 or end before 2012 due to filtering in the enrichment analysis (only terms annotated to
between 3 and 500 genes). The final p-values have had a Bonferroni multiple-testing correction applied.
B) The same as A but using the most significant terms from enrichment analysis done with 2006
versions of the ontology and annotations.
Clarke et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S4
http://www.jbiomedsem.com/content/4/S1/S4
Page 6 of 11
Test 3: Behavior of the most significant terms over time
To see if the most significant terms for GDS1962 in 2012 had always been the most sig-
nificant through the years, we followed the generalized dataset analysis approach
described in Methods. First, we selected the top 10 most significant terms (i.e. lowest
p-values) from an enrichment analysis using up-to-date versions of the ontology and
annotations. We then conducted a time-based analysis to track the significance of these
terms in past years, shown in Figure 3A (grey lines). Because the study that produced
GDS1962 was published in 2006, we also examined how the top terms of an identical
enrichment analysis done in 2006 would perform in later years (Figure 3B, grey lines). In
these figures, terms that had a p-value of 1 or were annotated to fewer than 3 genes
were omitted, resulting in the grey lines beginning only when the terms started appear-
ing. In Figure 3A, we can see that some terms are consistently highly significant from
2007 onwards, while some only appear as recently as 2010. In Figure 3B, the behavior of
the top terms in 2006 is inconsistent: while some improve in significance, others become
less relevant.
Comparison of changes to ontology structure and changes to annotations
The Gene Ontology and its annotations are distinct entities. Changes to the ontology
structure, such as the addition or clarification of a term, or the movement of a term within
the graph, may affect the results of enrichment analyses independently from annotation
changes[19]. Our time-based framework can be modified to assess the impact of these dif-
ferent kinds of changes in the following manner.
Before, we combined the ontology structure for a given time point with its contempor-
ary annotations, recreating the resource as it would have been used during that time. By
only varying the ontology structure, and using a single set of annotations, we can
observe the impact of structural changes on performance. Conversely, we can observe
the impact of annotation changes alone by using various annotation sets while using a
set ontology structure. We then use these modified resources in the time-based analysis
specified above.
Test 4: Assessing the difference between changes to ontology structure and annotation
We created gene sets reflecting ontology structure changes by merging the 2012 anno-
tations with each years version of the GO. Similarly, we produced gene sets reflecting
annotation changes by merging each years annotations with the 2012 GO structure. We
then performed an enrichment analysis on each collection of gene sets and looked at the
performance of both angiogenesis and the previously-identified top 10 most significant
terms for glioblastomas in GDS1962. Figure 4 shows the results when considering only
changes to the annotations (Fig. 4A) or only changes to the ontology structure (Fig. 4B).
Generally, changes to the ontology structure did not have as significant an effect as
changes to the annotation sets, as evidenced by the relatively unchanging significance
levels in Fig. 4B compared to Fig. 4A.
Discussion
Here, we presented a task-based framework for GO annotation evaluation and applied it
to an angiogenesis-enriched gene list derived from a glioblastoma dataset. We showed
how our method can be used to analyze the historical performance of the GO, providing
Clarke et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S4
http://www.jbiomedsem.com/content/4/S1/S4
Page 7 of 11
us with an understanding of how the ontology and annotations have changed over time.
Our framework also allowed us to separate the effects of changes to the ontology struc-
ture from changes to the annotations, and to see how each affects the performance of
the GO in a real-world task.
Figure 4 Effect of changing only the annotations or ontology structure on angiogenesis and most
significant terms A) The same analysis as in Figure 3A, but with only the annotations changing over time
(the ontology structure used was from 2012). The final p-values have had a Bonferroni multiple-testing
correction applied. B) The same analysis as Figure 3A, but with only the ontology structure changed (the
annotation set used was from 2012).
Clarke et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S4
http://www.jbiomedsem.com/content/4/S1/S4
Page 8 of 11
Our implementation of the framework shown here presents us with some noteworthy
results. Our term of interest, angiogenesis, rose in significance over time, as we had
expected. What we had not expected was how recently some of the most significant
terms in the 2012 analysis even started appearing in results (a term appeared if it had a
p-value of 1 or annotated to more than 3 genes)- some as late as 2010. Other terms
show a dramatic increase in p-value from 2010 to 2012. To determine whether changes
to the ontology or the annotations were responsible for these trends, we can consider
our structure-vs-annotations experiment shown in Figure 4. Here, we see that the graph
where only the annotations changed (Fig. 4A) shows the same trends of interest as
Fig. 3A, while the graph with only ontology changes lacks these trends. We can safely
assert that annotation changes, in this instance, are largely responsible for the later
dramatic changes. These results only hold for our dataset under test; an interesting fol-
low-up to this would be to identify a GO term with low annotation activity and see if
ontology structure changes affect it more strongly.
In Figure 3B, we see that the significance of the most relevant terms in the 2006 enrich-
ment analysis do not consistently improve, and are not especially significant in later years.
In fact, Table 1 shows that there is no overlap between these terms and their counterparts
in 2012. However, some terms are closely related and have more specific counterparts in
2012, e.g. transmission of nerve impulse becomes regulation of transmission of nerve impulse.
For this dataset, our results suggest that the GO and its annotations have become more
effective at representing the underlying biological facts of the data. We can assert this
based on the rising significance of a key tissue phenotype, angiogenesis, and the increasing
specificity of its most significant terms. Questions on the overall efficacy of the GO and
the human GO annotations are not answered by our implementation.
Discussion of the framework
These results illustrate the behavior of the GO as applied to a single dataset, which begs the
question of whether these results would hold in a more general analysis. The demonstrable
flexibility of the framework would allow its use in a large-scale effort where representative
terms or areas of interest are selected and tested. For instance, with datasets that are speci-
fically crafted to represent particular traits like cell division or apoptosis, we could deter-
mine empirically how accurate and useful the GO and its annotations are at representing
biological truths.
Table 1 Top Ten Biological Process Terms for 2006 and 2012
2006 2012
System development axon guidance
nervous system development regulation of synaptic transmission
transmission of nerve impulse regulation of transmission of nerve impulse
mitotic cell cycle M phase of mitotic cell cycle
intracellular protein kinase cascade nuclear division
metal ion transport mitosis
cell morphogenesis organelle fission
cytoskeleton organization regulation of vesicle-mediated transport
regulation of cell cycle regulation of neurological system process
potassium ion transport learning or memory
The p-values for 2006 range from 3.15E-21 to 6.70E-7. The p-values for 2012 range from 1.11E-16 to 2.26E-10. The terms
are listed in order of p-value.
Clarke et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S4
http://www.jbiomedsem.com/content/4/S1/S4
Page 9 of 11
On a smaller scale, the framework allows researchers to test modifications to the ontol-
ogy or annotations in real-time. Let us assume a new method of gene annotation, for
example through text-mining, has been developed and we wish to test its efficacy. Let us
also assume that the previous versions of the GO are worse at the task of enrichment ana-
lysis than later versions (as is arguably the case with GDS1962 and angiogenesis). We
could then combine the novel annotations with previous versions of the GO annotations
and observe if key terms rose or fell in significance, and if the new results more closely
resemble current results. If so, then we have evidence that the new method of gene anno-
tation is at least as correct as the efforts of existing curators. This would be a powerful
new tool for the development of future automated annotation methods.
Similar methods to the ones described here were used in an analysis of a long-term
annotation initiative in which the authors examined the impact of the new annotations
on standard enrichment analyses [20]. As with our results, they found that the new
annotations significantly increased the number of enriched terms, many of which were
not present at all before the annotation efforts. Their results are an example of the
divergent behavior we would expect from high annotation activity.
Conclusions
In this framework, we have a quantitative way to examine the GO and its annotations in
the context of real-world applications. We have demonstrated its ability to shed light on
the evolution of the GO over time, to separately quantify changes in ontology structure
and annotation composition, and test the performance of the GO in key applications.
Our framework is flexible enough to address many questions facing GO developers and
annotators and can be applied across disparate regions of the GO, multiple species, and
various enrichment analyses. The methodology presented here should become a valuable
tool in the development of novel annotation algorithms and many other applications.
Methods
To identify a list of genes that were overexpressed in the GDS1962 dataset, we first filtered
out microarray probes whose maximum value across samples was less than the median
probe value across the entire dataset. We then took the natural log of each probe value.
To identify differentially-expressed genes, we used an independent T-test comparing the
glioblastoma sample against the control. The final list of differentially expressed genes
included those whose Bonferroni-corrected p-value was significant (p < 0.05). We used an
enrichment analysis based on Fishers exact test for the glioblastoma samples [21]. In the
Fishers exact test, we used only terms that were annotated to at least 3 and no more than
500 genes.
Authors contributions
AS, BG, and EC conceived of the studies; EC and SL performed the analyses; EC, SL and AS wrote the manuscript. All
authors read and approved the final manuscript.
Competing Interests
The authors declare that they have no competing interests.
Acknowledgements
The authors acknowledge support from the National Institute of General Medical Sciences (GM089820 and GM083924
to A.I.S).
Declarations
This research and the publication costs were funded by the National Institute of General Medical Sciences (GM089820
and GM083924 to A.I.S).
Clarke et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S4
http://www.jbiomedsem.com/content/4/S1/S4
Page 10 of 11
This article has been published as part of Journal of Biomedical Semantics Volume 4 Supplement 1, 2013: Proceedings
of the Bio-Ontologies Special Interest Group 2012. The full contents of the supplement are available online at http://
www.jbiomedsem.com/supplements/4/S1
Published: 15 April 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Wagholikar et al. Journal of Biomedical Semantics 2013, 4:3
http://www.jbiomedsem.com/content/4/1/3RESEARCH Open AccessPooling annotated corpora for clinical concept
extraction
Kavishwar B Wagholikar1*, Manabu Torii2, Siddhartha R Jonnalagadda1 and Hongfang Liu1Abstract
Background: The availability of annotated corpora has facilitated the application of machine learning algorithms to
concept extraction from clinical notes. However, high expenditure and labor are required for creating the
annotations. A potential alternative is to reuse existing corpora from other institutions by pooling with local
corpora, for training machine taggers. In this paper we have investigated the latter approach by pooling corpora
from 2010 i2b2/VA NLP challenge and Mayo Clinic Rochester, to evaluate taggers for recognition of medical
problems. The corpora were annotated for medical problems, but with different guidelines. The taggers were
constructed using an existing tagging system MedTagger that consisted of dictionary lookup, part of speech (POS)
tagging and machine learning for named entity prediction and concept extraction. We hope that our current work
will be a useful case study for facilitating reuse of annotated corpora across institutions.
Results: We found that pooling was effective when the size of the local corpus was small and after some of the
guideline differences were reconciled. The benefits of pooling, however, diminished as more locally annotated
documents were included in the training data. We examined the annotation guidelines to identify factors that
determine the effect of pooling.
Conclusions: The effectiveness of pooling corpora, is dependent on several factors, which include compatibility of
annotation guidelines, distribution of report types and size of local and foreign corpora. Simple methods to rectify
some of the guideline differences can facilitate pooling. Our findings need to be confirmed with further studies on
different corpora. To facilitate the pooling and reuse of annotated corpora, we suggest that  i) the NLP community
should develop a standard annotation guideline that addresses the potential areas of guideline differences that are
partly identified in this paper; ii) corpora should be annotated with a two-pass method that focuses first on concept
recognition, followed by normalization to existing ontologies; and iii) metadata such as type of the report should
be created during the annotation process.Background
Development of Natural Language Processing (NLP) tools
generally requires a set of annotated documents in the
application domain [1]. The annotations serve as a reference
for constructing rule-based NLP systems and as a training
corpus to derive machine learning models for concept
extraction. However, in the clinical domain, annotated
corpora are often difficult to develop due to high cost of
manual annotation involving domain experts and med-
ical practitioners, and also due to concerns for patient
confidentiality [2]. Due to high demand, such corpora* Correspondence: wagholikar.kavishwar@mayo.edu
1Division of Biomedical Statistics and Informatics, Mayo Clinic, Rochester, MN,
USA
Full list of author information is available at the end of the article
© 2013 Wagholikar et al.; licensee BioMed Cen
Creative Commons Attribution License (http:/
distribution, and reproduction in any mediumhave been recently created with pioneering effort of
some research groups and made available to the scien-
tific community to support studies in clinical NLP [3-5].
Availability of the annotated corpora has fostered the
application of machine learning algorithms to concept
extraction from clinical notes [6,7]. Supervised machine
learning taggers that achieve an accuracy of more than
80% have been developed [8,9], given their great success
for general English text [10] and biomedical literature
[11-13]. These taggers were developed as an alternative
to earlier systems that use dictionaries and rules, e.g.
MetaMap [14], MedLEE [15], and SymText/MPLUS [16].
However, machine learning methods are sensitive to the
distribution of data, such as the distribution of words in the
vocabulary and grammar styles, which could significantlytral Ltd. This is an Open Access article distributed under the terms of the
/creativecommons.org/licenses/by/2.0), which permits unrestricted use,
, provided the original work is properly cited.
Wagholikar et al. Journal of Biomedical Semantics 2013, 4:3 Page 2 of 10
http://www.jbiomedsem.com/content/4/1/3affect the portability of a trained machine learning system
across institutions and, thus the value of annotated corpora.
Given the barriers for preparing a large annotated cor-
pus in individual institutions, consolidation of annotation
efforts has the potential to advance clinical NLP. One way
of leveraging existing efforts is to pool annotated corpora
across institutions. Pooling of the annotations to train
machine learning taggers may increase performance of
the taggers [17]. However there has been little research
on associated issues. In this paper we have investigated
whether pooling of similar corpora from two different
sources can improve performance of resultant machine
learning taggers for medical problem detection. We hope
that our study will be a useful guide for facilitating reuse
of annotated corpora across institutions.
Pooling Biomedical Corpora
There have been similar efforts to pool corpora in the bio-
medical domain. Johnson et al. [18] semi-automatically
changed the format of the Protein Design Group corpus
into two new formats (WordFreak and embedded XML),
without altering the semantics, to increase the usage of the
corpus. The process of altering the format without change
in the semantics was called re-factoring. Ohta et al. [19]
extended the annotation of their GENIA corpus to inte-
grate the annotation style of the GENTAG corpus, which is
the other prominent and widely used biomedical corpus, so
that their corpus can been pooled with others following the
same format. As an extension of this work, Wang et al. [20]
pooled these corpora (and a third known as AIMED) [21]
hoping to achieve better performance using the large cor-
pus. However, the performance dropped by 12%. Subse-
quently they analyzed incompatibilities among the corpora.
After removing the incompatibilities, they obtained promis-
ing performance using the pooled corpora [22]. Recently,
Huang et al. [23] have reported significant performance
improvement of machine learning based part-of-speech
(POS) taggers, by training them on pooled dataset.
We have used publicly available resources, such as the
UMLS MetaThesaurus as a term dictionary, GENIA tagger
for POS tagging, programs in Mallet machine learning
software suite for sequence tagging and clinical corpora
from the i2b2/VA challenge, to investigate the feasibility of
pooling annotations for clinical concept extraction. Our
current effort can potentially benefit research on develop-
ment of clinical taggers at healthcare institutions, by facili-
tating use of annotated corpora from other institutions. In
the next subsection, we briefly explain the process of
annotation for readers who are new to this field.
Annotation of clinical text
Machine learning based taggers for detecting phrases
that convey particular concepts, requires the availability
of reports that have been manually marked (annotated)for the phrases. For instance, in the sentence The
patient denies any abdominal pain, the phrase denoting a
medical problem has been marked by the underline. The
exercise to manually create such a set of marked reports is
initiated with the development of a guideline, which
defines what to mark and also how to mark. A group of
human annotators then independently follow the guideline
to carry out the annotation exercise.
Researchers developing a machine learning tagger at an
institute have the option of training the tagger on i) the
in-house set of reports that have been manually annotated,
ii) reports annotated at another institution or iii) a pooled
set constructed by combining i and ii. While the develop-
ment of the in-house corpus requires several hundred
hours of human effort and the associated expenses, the
corpus from other institutions may not be portable. In this
paper, we have examined the factors associated with the
use of corpus from other institutions.Overview of current work
We trained and tested taggers on a corpus from Mayo
Clinic Rochester [24] and a corpus from the 2010 i2b2/VA
NLP challenge [25], and examined the effect of pooling the
corpora [26]. These corpora share the property that they
were annotated for the same task of developing taggers
for detecting medical problems. However the corpora
were constructed with different annotation guidelines.
The experiments were carried out using an existing ma-
chine learning-based tagging system, MedTagger [9], that
participated in the 2010 i2b2/VA NLP challenge. In an
earlier study, we had reported performance gain for
machine learning taggers by pooling corpora across
institutions and report types [17]. The corpora used in
that study were subsets of the i2b2 corpus and were
annotated with the same annotation guideline.Results
Figure 1 summarizes the results of the experiments. Detailed
results are tabulated in the Additional file 1.Intra-corpus testing
Taggers performed the best when the training and test
sets were from the same corpus. F1-scores for MCR and
i2b2 cross-validations were 0.58 and 0.79 for exact span
evaluation, respectively. The higher F1-scores for i2b2 as
compared to MCR could be due to a lower diversity of
report types and annotations. Addition of more reports
to MCR corpus might lead to improved performance for
MCR. For the overlap spans, the F1-scores were 0.82 for
MCR and 0.89 for i2b2. The performance patterns were
similar for recall and precision for the exact and overlap
span evaluations.
Figure 1 Performance measures of the taggers. The plots A, B
and C show the F1-score, precision and recall respectively. Each line
in the figure corresponds to the test set (MCR or i2b2) and the
evaluation method: Exact (E) with solid lines or Overlap (O) with
dashed lines. The horizontal axis indicates the training sets: MCR,
i2b2 and combined (MCR + i2b2).
Wagholikar et al. Journal of Biomedical Semantics 2013, 4:3 Page 3 of 10
http://www.jbiomedsem.com/content/4/1/3Inter-corpora testing
Performance of the taggers was poor when they were
trained exclusively on reports from the other corpora.
For tagger trained on i2b2 and tested on MCR, the F1
score was 0.38 for exact spans. Similarly for tagger
trained on MCR and tested on i2b2, the scores was 0.40.Supplementation of the training set with reports from
other corpora decreased the performance, by 12% points
for MCR and 4% points for i2b2. The greater degrad-
ation for MCR is likely due to small size of the corpus as
compared to the i2b2 corpus, i.e. the proportion of data
supplemented to MCR training set was much larger than
that for i2b2. The pattern was similar for precision and
recall, and for the overlap span evaluation. An excep-
tion was the improvement in the recall on MCR corpus
when it was supplemented with i2b2 corpus, using over-
lap span evaluation.
Results suggest that the corpora are incompatible for
simple pooling. In an earlier study we had reported per-
formance gain for machine learning taggers by pooling
corpora across institutions and report types [17]. The
corpora used in that study were subsets of the i2b2 cor-
pus and were annotated with the same guideline. Con-
trastingly, in the current study, the corpora differ in
their annotation guidelines, and also have different
distributions of report types. Also the corpora sizes are
different from the ones examined in the earlier study.
Hence we investigated the effect of differences in the
annotation guidelines, distributions of report types and
corpora sizes, on the performance of taggers trained on
pooled corpora, as described in the following sub-sections.
Guideline differences
We examined the annotation guidelines to identify factors
that contributed to the performance degradation of taggers
trained on pooled corpora.
Concept definition
Annotation guidelines for the two corpora differed slightly
in definition of concepts. i2b2 annotation guideline extends
definition of medical problem beyond the semantic type of
signs/symptoms and disease/syndrome (disorder), to in-
clude pathologic functions, mental dysfunction, molecular
dysfunction, congenital abnormality, acquired abnormality,
neoplastic process, and virus/bacterium. It also allows
the annotator to mark medical problems not covered in
the UMLS.
MCR annotation guideline defined signs/symptoms and
disorders, which we mapped to the problem class. Signs/
symptoms were defined as concepts that mapped to
SNOMED-CT subset of semantic type signs/symptoms.
Disease/syndrome had a looser definition that extended
beyond the semantic types for i2b2 medical problem, to
include injury or poisoning, behavioral dysfunction, cell
dysfunction, experimental model of disease and anatom-
ical abnormality but excluded virus/bacterium.
Articles
The i2b2 annotations included articles, e.g. the cough
and a fever, while MCR annotations did not. Nearly
Figure 2 Distribution of number of tokens per annotation in
the two corpora. MCR annotations (red line) are shorter than the
i2b2 annotations (blue dashed line).
Wagholikar et al. Journal of Biomedical Semantics 2013, 4:3 Page 4 of 10
http://www.jbiomedsem.com/content/4/1/311% of the i2b2 annotations began with an article
(Table 1). This contributes to the generally longer length
of the i2b2 annotations (Figure 2).
Possessive pronouns
i2b2 annotations included possessive pronouns, e.g. his
cancer, while MCR annotations did not. 3% of i2b2
annotations began with his (174) or her (200).
Concepts not pertaining to patients
MCR annotations included concepts that were not
pertaining to patients. For instance, disease names in
organizational unit names, e.g. cancer in cancer de-
partment. Concepts that are not directly related to the
patient were not annotated in i2b2 corpus.
Prepositional phrases
The i2b2 guidelines specify that one prepositional phrase
following a concept can be included in the annotation if
it does not contain a markable concept and/either
indicates an organ/body part. Also a preposition can be
included in a concept phrase if words therein can be
rearranged to express the same concept without the
preposition. For example, pain in chest is a viable con-
cept phrase according to the i2b2 guidelines schema
because it indicates body part and can be rephrased
without in as chest pain. In contrast, the text segment
removal of mass is annotated as two concept phrases
as it cannot be rearranged to express the same concept
without the preposition. MCR guidelines did not expli-
citly address this issue.
Conjunctions
The i2b2 guidelines specify that conjunctions that de-
note lists are included if they occur within the modifiers
or are connected by a common set of modifiers. If the
portions of the lists are otherwise independent, they
should not be included. For example, the text segment
metastases in the liver and pancreas is a valid concept
phrase including and, while the segment diarrhea, nau-
sea and vomiting is not valid. The latter is annotated as
three concept phrases. MCR guidelines did not explicitly
address this issue.Table 1 Frequency of i2b2 annotations that begin with
an article
Article Frequency
a 562
the 269
any 154
some 134
an 126
this 25Reconciliation of annotations differences
To investigate the effect of annotation differences due to
the differing guidelines, we considered curation of the
annotations. Rectification of the differences in all guide-
line factors would require considerable manual effort.
Hence, we restricted our effort to automated rectifica-
tion of a subset of the factors. Specifically, we removed
articles and possessive pronouns from i2b2 annotations.
Fifteen percent of the i2b2 annotated phrases were
modified. When this partially rectified corpus was used
to supplement training data for the MCR corpus, there
was lesser degradation of the performance measures.
The F1-score degraded by 6% points instead of 12% points
for exact match and 2% points instead of 3% points for
overlapping match (Figure 3 and Additional file 1).
Table 2 shows the overlap in the annotated phrases in
the two corpora. Before rectification of the annotation
differences, 42.9% of MCR annotations exactly matched
i2b2 annotations, i.e. from the 2,076 concept annotations
in MCR, there were 890 annotations that exactly matched
with an i2b2 annotation. When one start word was ignored
55.1% of the annotations matched and when one word was
ignored 55.7% matched. After the i2b2 corpus was curated
to partially rectify the annotation differences, there was
an improvement in the overlap of the corpora, as shown
in Table 2.
The annotations in MCR corpus were mapped to the
highest level of granularity-- to the UMLS CUIs. The
MCR annotations were restricted strictly to the UMLS.
Consequently, these annotations can be expected to in-
herit the limitations of UMLS which includes lack of
concept coverage. This would possibly be the reason
why the taggers trained on MCR corpus and tested on
Figure 3 Performance measures of the taggers trained on the
curated i2b2 corpus and tested on MCR corpus. The plots A, B
and C show the F1-score, precision and recall respectively. There are
two lines in each plot that correspond to the two evaluation
methods: Exact (E) with solid lines and Overlap (O) with dashed
lines. The horizontal axis indicates the training sets: i2b2, i2b2C
(curated), MCR + i2b2 (combined) and MCR + i2b2C (MCR combined
with curated i2b2).
Wagholikar et al. Journal of Biomedical Semantics 2013, 4:3 Page 5 of 10
http://www.jbiomedsem.com/content/4/1/3i2b2 corpus showed greater degradation in performance
than vice-versa. The annotation guideline for the i2b2
corpus advocated a more intuitive approach for annota-
tion. I2b2 annotators used UMLS definitions to guide
the annotation, which allowed them greater flexibility toannotate and even include phrases that were not covered
in the UMLS.
Hence, to facilitate reuse of the annotations for developing
machine learning models for concept recognition, we
suggest the following two-step approach for annotation.
Annotators should first mark the phrases that corres-
pond to the concept of interest (perform concept recog-
nition), and then normalize the annotations by mapping
each annotation to the set of ontology nodes with the
highest possible granularity. When normalization is not
possible to any ontology node, the phrase should be
marked as a novel concept.
For developing machine learning applications it is crit-
ical that all the phrases that map to the concept of inter-
est are annotated, by ensuring that even those which are
not covered in the reference ontology are marked up.
The first pass of concept recognition would ensure
that all the concepts are covered. The second pass of
normalization will facilitate the filtering/sub-classing
of annotations for developing machine learning taggers
for a particular sub-class. This two-pass annotation
method will facilitate the pooling of corpus with other
similar corpora. Also the novel annotation class will be
useful for adding new ontology terms.
Report type
In addition to the differences in the annotation guidelines,
the performance of the taggers could be affected by the
distribution of report types in the corpora. i2b2 corpus
included discharge summaries and progress notes, while
MCR corpus had a wider variety, since the reports were
randomly selected from the EMR system for annotation.
Named entities may vary in their distributions on report
types. For instance the history and examination reports
will have a high density of patient symptoms as compared
to the progress notes that will mainly refer to the
symptoms addressed by the current treatment. Also the
progress notes will perhaps contain more medical termin-
ology instead of ordinary English words reported by the
patient in the history and examination reports. The distri-
bution of the medical terminologies on the report types
may also depend on the institution, as many institutions
have their own report formats. However the reports in
either corpus did not have meta-data about the type of
report. Hence, the authors could not investigate the
report-type factor further.
Corpus size
To examine the effect of size, we measured the tagger
performance on subsets of MCR corpus of various sizes,
by performing 5 fold cross-validation experiments. This
was compared to the performance after pooling the
MCR subsets with the original and curated i2b2 corpus
for training, i.e. the i2b2 corpus was used to supplement
Table 2 Overlap of annotations in the corpora
Annotation set Exact match Ignoring one start word Ignoring one word
MCR 42.9/43.7 55.1/55.7 56.0/56.7
i2b2 22.6/25.2 32.6/33.6 33.6/34.7
Each cell in the table gives the percentage of annotations that matched with the other corpus before and after automated curation of the i2b2 corpus to partially
rectify the annotation differences.
Wagholikar et al. Journal of Biomedical Semantics 2013, 4:3 Page 6 of 10
http://www.jbiomedsem.com/content/4/1/3the training fraction of the MCR corpus during the
cross-validations. Each experiment was repeated 5 times
and the average performance measures were computed,
i.e. 5 times 5-fold cross-validation was performed. We
had increased the runs for cross-validating the subsets
from 3 to 5. The subsets were smaller in size, which
increased the variation of the accuracy measurements.
The additional runs were required to compensate for
the increase in variation, so as to provide adequate
confidence of the accuracy measurements. The results
are summarized in Figure 4 and tabulated in the
Additional file 1.
The performance of taggers trained and tested on
MCR corpus increases in F1-score and recall as the cor-
pus size increases. The increase is rapid at first with in-
crement in the corpus size, but later forms a plateau.
The precision, on the other hand, is nearly unaffected by
the corpus size.
Pooling with the i2b2 corpus nearly always increases
the recall. However as the precision always degrades on
pooling, the F1-score first increases with pooling and
then degrades. A possible explanation is that the smaller
subsets of MCR corpus are deficient in all the annota-
tion patterns, and when these are supplemented by the
i2b2 curated corpus, there is an improvement in recall.
The improvement in recall for smaller sizes of MCR
subsets is greater than the degradation in precision that
occurs due to pooling, which translates to an improve-
ment in F1-score. When the MCR subset size crosses a
threshold, the improvement in recall cannot surpass the
degradation in precision, which lowers the F1-score.
This result leads to a hypothesis that pooling with a
compatible corpus from another institution may be
beneficial and an economically favorable alternative to
extending the in-house annotated corpus, only when the
in-house corpus is below a critical size. However our
analysis is limited to a single observation and further
studies on other corpora are needed to investigate the
combined effect of differences in the annotation guidelines,
distributions of report type and sizes.
The tagger performance with the curated i2b2 cor-
pus was greater than with the original corpus for all
subsets of MCR corpus. The simple approach of
automated annotation, described earlier increased the
threshold of the MCR subset size where the F1-score
dips on pooling.Summary
In summary, simple pooling of corpora was overall
found to reduce the tagger performance. We examined
the annotation guidelines for the corpora to delineate
several inconsistencies that include concept definition,
articles, possessive pronouns, unrelated concepts, prep-
ositional phrases and conjunctions. Rectification of a
subset of the annotation differences using an automatic
approach reduced the performance degradation that
occurred on pooling. The effect of distribution of re-
port types could not be studied as the corpora were
not annotated for report type. The effect of pooling
was found to depend on the corpus size, as pooling
was found to improve tagger performance for smaller
subsets of the MCR corpus. This result suggests that
pooling with corpora from another institution may be
beneficial when the in-house corpus is below a critical
size. Further studies on different corpora are needed to
elucidate the relationship between the above mentioned
factors and performance of taggers trained on pooled cor-
pora. The investigation of these relationships would be a
useful guide for researchers to develop machine learning
taggers for clinical text.
Conclusions
We investigated whether pooling of corpora from two
different sources, can improve performance and port-
ability of resultant machine learning taggers. The effect
of pooling local and foreign corpora, is dependent on
several factors that include compatibility of annotation
guidelines, distribution of report types and corpus size.
Simple automatic methods to rectify some of the guide-
line differences can be useful to facilitate pooling. Prac-
tically useful machine taggers can be possibly developed
by annotating a small local corpus, and pooling it with
a large similar corpus, available for reuse from another
institution. The benefits of pooling diminish as more
local annotations are created. Our findings need to be
confirmed with further studies using different corpora
and machine taggers.
Future directions
Studies on different corpora are needed to elucidate the
relationship between the above mentioned factors and
performance of taggers trained on pooled corpora. We
plan to investigate whether weighting of features for the
Figure 4 The plots A, B and C show the F1-score, precision and
recall of the taggers respectively. The horizontal axis indicates the
size of the MCR subsets used in the cross-validation. In each plot the
colors of the horizontal lines correspond to the corpus used in the
cross-validation experiment, viz. MCR, MCR + i2b2 (MCR with training
fraction supplemented by i2b2) and MCR + i2b2C (MCR with training
fraction supplemented by curated i2b2). The two types of evaluation
methods are represented using different line styles: Exact (E) with
solid lines and Overlap (O) with dashed lines.
Wagholikar et al. Journal of Biomedical Semantics 2013, 4:3 Page 7 of 10
http://www.jbiomedsem.com/content/4/1/3machine learning tagger and filtering of the annotations
using a dictionary lookup, can improve the tagger per-
formance on pooling the corpora. Another interesting
direction of investigation would be to train machinetaggers separately on local and foreign corpora and then
to combine the taggers using machine learning.
We suggest that future initiatives for clinical annotations
should consider guideline factors delineated in this paper
for development of the annotation guidelines, so that their
annotation effort can be utilized by others. Moreover the
guidelines should also include instructions to annotate
metadata about the reports, so that reports of the same
type from different corpora can be readily pooled for en-
hancing machine learning based taggers. The authors are
aware of an effort in this direction [27], but there needs to
be consensus for wider utilization of the standard for an-
notation of new corpora. The annotation groups involved
in clinical research should come together to develop a
standard annotation guideline that facilitates reuse of an-
notation efforts. We also suggest a two-pass annotation
method that focuses first on concept recognition, followed
by normalization to existing ontologies, to facilitate the
pooling of corpora.
Methods
Corpora
Two annotated corpora were used in this study (Table 3).
The corpora differed in their sources as well their annota-
tion guidelines, but contained annotations for the same
concept type, i.e. medical problems.
The first corpus consisted of 349 clinical reports from
the 2010 i2b2/VA challenge on concept assertions and
relations in clinical text [25] that were provided to the
participants for training. Partners Healthcare, Beth Israel
Deaconess Medical Center and University of Pittsburgh
Medical Center contributed discharge summaries for
this corpus, and University of Pittsburgh Medical Center
also contributed progress reports. This corpus was
annotated for patient medical problems (signs/symptoms
and disorders), treatments and tests.
The second corpus had 160 clinical notes from Mayo
Clinic Rochester (MCR) [24,28]. These were annotated for
signs/symptoms, disorders, medications and procedures.
The annotation class problem from the first corpus
was equivalent to the combination of classes -- signs/
symptoms and disorders in the second corpus and we
carried out experiments with reference to this class.
MedTagger
The experiments reported in this paper were carried
out using an existing tagging system, MedTagger that
participated in the 2010 i2b2/VA NLP challenge. This is
an adaptation of the BioTagger-GM system that was
originally developed to identify gene/protein names in
biomedical literature [9]. The pipeline of this system
consisted of dictionary lookup, part of speech (POS)
tagging and machine learning for named entity predic-
tion and concept extraction.
Table 3 Summary statistics for the corpora
Set name Documents Lines Tokens Concepts % of tokens included in concept annotation
i2b2/VA 349 30,673 260,570 11,967 10.9
Mayo 160 2,487 40,988 2,076 11.3
Wagholikar et al. Journal of Biomedical Semantics 2013, 4:3 Page 8 of 10
http://www.jbiomedsem.com/content/4/1/3Dictionary lookup
We used the UMLS MetaThesaurus [29] and a collec-
tion of words used in a clinical vocabulary viewer [30]
as the domain dictionary. The input text and dictionary
were normalized to facilitate flexible matching. The
normalization process included (a) converting words to
base form using the UMLS SPECIALIST lexicon, (b)
changing letters to lower case, (c) removing punctuation
symbols, and (d) converting digits and Greek letters to 9
and G respectively. The dictionary lookup tagged all
phrase occurrences, including overlapping phrases.POS tagging
We used GENIA tagger for labeling parts of speech to
all input tokens. GENIA tagger [31] is based on max-
imum entropy models trained on biomedical text as well
as generic English text.Machine learning
Using the dictionary lookup and POS tagging results, we
derived a set of features for each token. These were
collated with other commonly used features for named
entity recognition, such as words, word affixes and word
shapes. The features were fed to a sequence tagger using
a conditional random field (CRF) model [32] with a
token window size of five.Figure 5 Design for different training/testing experiment reported in
representing a direct evaluation on the test set, b) The dotted lines represe
tested is excluded from the training set.Experiment
We designed our experiments to examine effect of using
pooled training sets on the performance of machine
learning taggers for concept extraction (Figure 5). The
taggers were trained to recognize medical problems, in-
cluding signs/symptoms and disorders. First, we trained
the tagger on i2b2 corpus and tested it on MCR corpus
and vice versa. We then performed 5-fold cross valid-
ation experiments on MCR, i2b2 and the combined
(i2b2 +MCR) corpora. We repeated the cross-validation
on MCR corpus after supplementing the training frac-
tion with the i2b2 corpus during each of the cross valid-
ation runs. This design was repeated for the i2b2 corpus
by using MCR corpus to supplement the training. The
cross-validation experiments were repeated three times
to average the performance scores, i.e. 3 times 5-fold
cross-validation was performed.
Two kinds of evaluations were preformed: exact span
matching and overlap span matching. In exact span
matching, the annotations were counted as matches if
begin and end spans matched. In case of overlap span
evaluation the annotations were counted as matches, if
there was any overlap in the span ranges. The overlap
span is a more lenient measure of performance. It was
expected to be more useful in this study, because a wide
variation of phrases was anticipated between the corpora,
as the corpora originated from different institutions and
used different annotation guidelines.this paper. Two types of arrows point to the test sets: a) Solid lines
nt 5 fold cross-validation in which the fraction of the corpus being
Wagholikar et al. Journal of Biomedical Semantics 2013, 4:3 Page 9 of 10
http://www.jbiomedsem.com/content/4/1/3Performance measures of precision, recall and F1 score
were computed for the experiments (Figure 1). The
measures are defined as follows:
Precision ¼ truepositives= truepositivesþ falsepositivesð Þ
Recall ¼ truepositives= truepositivesþ falsenegativesð Þ
F1 ¼ 2 Recall Precisionð Þ= Recallþ Precisionð Þ
Additional files
Additional file 1: Appendix. Tables detailing the figures included in
the paper.
Abbreviations
NLP: Natural language processing; EMR: Electronic medical record;
MCR: MAYO clinic Rochester; CRF: Conditional random field; POS: Part of
speech.
Competing interests
The authors declare that they have no competing interest.
Authors contribution
KW carried out the experiments, led the study design and analysis and
drafted the manuscript. MT helped with the experiments, participated in the
study analysis and manuscript drafting. SJ participated in the study analysis
and manuscript drafting. HL conceived the study, helped with the
experiments, participated in the study design and analysis, and drafting of
the manuscript. All authors read and approved the final manuscript.
Acknowledgements
We are thankful to the people who developed and made available the
machine learning tools: GENIA tagger and Mallet, terminology services: UMLS
and the labeled corpora used in this study. The i2b2 corpus of deidentified
clinical reports used in this research were provided by the i2b2 National
Center for Biomedical Computing funded by U54LM008748 and were
originally prepared for the Shared Tasks for Challenges in NLP for Clinical
Data organized by Dr. Ozlem Uzuner, i2b2 and SUNY. We are also thankful
for the pioneering effort of the Mayo NLP team, for developing the
resources which made the current study possible. The authors are thankful
for the constructive comments of the reviewers.
This study was supported by National Science Foundation ABI:0845523,
Strategic Health IT Advanced Research Projects (SHARP) award (90TR0002) to
Mayo Clinic from Health and Human Services (HHS) and National Library of
Medicine (NLM:1K99LM011389, 5R01LM009959-02) grants.
Author details
1Division of Biomedical Statistics and Informatics, Mayo Clinic, Rochester, MN,
USA. 2Department of Radiology, Georgetown University Medical Center,
Washington, DC, USA.
Received: 26 June 2012 Accepted: 2 January 2013
Published: 8 January 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Fuellen et al. Journal of Biomedical Semantics 2013, 4:25
http://www.jbiomedsem.com/content/4/1/25RESEARCH Open AccessUsing ontologies to study cell transitions
Georg Fuellen1*, Ludger Jansen2,6, Ulf Leser3 and Andreas Kurtz4,5Abstract
Background: Understanding, modelling and influencing the transition between different states of cells, be it
reprogramming of somatic cells to pluripotency or trans-differentiation between cells, is a hot topic in current
biomedical and cell-biological research. Nevertheless, the large body of published knowledge in this area is
underused, as most results are only represented in natural language, impeding their finding, comparison,
aggregation, and usage. Scientific understanding of the complex molecular mechanisms underlying cell transitions
could be improved by making essential pieces of knowledge available in a formal (and thus computable) manner.
Results: We describe the outline of two ontologies for cell phenotypes and for cellular mechanisms which together
enable the representation of data curated from the literature or obtained by bioinformatics analyses and thus for
building a knowledge base on mechanisms involved in cellular reprogramming. In particular, we discuss how
comprehensive ontologies of cell phenotypes and of changes in mechanisms can be designed using the entity-quality
(EQ) model.
Conclusions: We show that the principles for building cellular ontologies published in this work allow deeper insights
into the relations between the continuants (cell phenotypes) and the occurrents (cell mechanism changes) involved in
cellular reprogramming, although implementation remains for future work. Further, our design principles lead to
ontologies that allow the meaningful application of similarity searches in the spaces of cell phenotypes and of
mechanisms, and, especially, of changes of mechanisms during cellular transitions.Background
The (artificial) induction of cell transitions has recently
attracted a lot of attention. A cell phenotype (or cell
type) can be defined by the cells repertoire of molecules
and structural components at a certain time, together
with the specific morphology and function they bring
with them. A cell transition is a change in a cell that re-
sults in a new phenotype. For example, the phenotype of
epithelial cells is distinct from the phenotype of fibro-
blasts. Programming of cells is the induction of a cell
phenotype transition, e.g. from fibroblast to epithelial
cell. Reprogramming is the artificially induced transition
of a cell to a cell phenotype, which it (or its predecessor)
had in the past. Potency can be defined as the dispos-
ition of a cell to transition into another cell phenotype;
pluripotency is the ability of a cell to transition naturally
into any of the cell phenotypes of an organism (where a
transition is natural if it is not triggered by a technical* Correspondence: fuellen@uni-rostock.de
Equal contributors
1Institute for Biostatistics and Informatics in Medicine and Ageing Research,
Rostock Medical School, Ernst-Heydemann-Str. 8, 18057 Rostock, Germany
Full list of author information is available at the end of the article
© 2013 Fuellen et al.; licensee BioMed Central
Commons Attribution License (http://creativec
reproduction in any medium, provided the orintervention). Since Takahashi and Yamanaka described
cell reprogramming of fibroblasts back to pluripotency
(also known as generation of iPS, induced pluripotent
stem cells) [1], hundreds of papers have dissected the re-
programming process and the cellular disposition of
pluripotency at an ever-increasing resolution, reviewed
in, e.g., [2] and [3]. This corpus is currently underused
as there is no formal representation of the reported
findings.
Several ontologies already exist in the domain of cell
biology, such as the well-known Gene Ontology (GO)
[4] and the cell type ontology (CL; cf. [5,6]). Bard et al.
[5] proposed formal definitions for CL classes, referring
to properties of cells such as expressed proteins, acti-
vated biological processes, or phenotypic characteristics.
Further cell-related knowledge projects include the Vir-
tual Physiological Human project (http://www.vph-noe.eu/)
that attempts to provide interoperability between differ-
ent databases and tools related to human physiology and
gene expression; the associated software Phenomeblast
(code.google.com/p/phenomeblast) is an ontology-based
tool for aligning and comparing phenotypes across species.Ltd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Fuellen et al. Journal of Biomedical Semantics 2013, 4:25 Page 2 of 8
http://www.jbiomedsem.com/content/4/1/25However, many efforts in formal modelling of biological
phenomena of organisms focus on anatomical features and
only rarely address the cell level (cf. [7-10] and [11]). What
is missing is a comprehensive tool to represent and to
compare cellular phenotypes and their dynamics.
Results and discussion
Cell phenotypes and cell mechanisms
We distinguish between two types of processes going on
in a cell: microscale mechanisms and macroscale changes
thereof. Microscale mechanisms are the interactions be-
tween molecules going on in a cell at a certain time,
while a macroscale change is the transition from one set
of microscale mechanisms going on at one point of time
to another such set at a later time. In order to transfer
ontology-based annotation and search strategies from
phenotypes at the anatomical level [12] to the domain
of cell phenotypes and mechanism changes, we need to
be able to formally describe both (a) cell phenotypes
and (b) mechanism changes. Phenotypes are usually de-
scribed by means of the entity-quality syntax (EQ) using
the Phenotypic Quality Ontology PATO for anatomic
phenotypes [13,14]. To apply the EQ syntax to the cell
level, we outlined two ontologies, an ontology of cell
parts (Figure 1) and an ontology of microscale mechanismsFigure 1 Outline of an ontology of cell parts and its use to describe c
phenotypes, here for epithelial cells, mesenchymal cells and embryonic ste
on the left hand side) and PATO-analogous quality modifiers (shown on th
terms relating to structures in red, to ultrastructures in blue, and to molecu
have an all-some syntax, i.e. Tight junction has_part Occludin means: For
instance of the type Occludin such that x has part y.(Figure 2) to be used in combination with a small set of
standardized modifiers (as qualities).
Most of the classes that are needed for the ontology of
cell parts can be found in CELDA, the ontology developed
by the CellFinder project (http://cellfinder.org/about/
ontology/) [15], that itself integrates ontologies like the
Cell Ontology (CL), the Cell Line Ontology (CLO), the
Foundational Model of Anatomy (FMA), the Gene Ontol-
ogy (GO) and Mouse Anatomy. Additional classes can be
taken from the Cellular Phenotype Ontology [16] (CPO).
For the ontology of cell mechanisms, we can re-use
(portions of ) the Interaction Network Ontology (http://
bioportal.bioontology.org/ontologies/1515) and the GO
subontology for biological processes (http://www.gene
ontology.org). The current GO (Biological Process), how-
ever, does not capture the hierarchical relationships de-
scribed in Figure 2, connecting molecular events such as
the interaction of Occludin and JAM to ultrastructural
events such as the formation of a tight junction. Here, we
need to explicitly encode the interconnections of molecu-
lar events and ultrastructural events. The ontology of cell
parts (Figure 1) is designed to handle exactly the same
challenge, on the level of the continuants. While the focus
of the CPO is on phenotype abnormalities, we can still re-
use it to provide distinct morphological and associatedell phenotypes. The figure shows a structure by which cell
m cells (ESC), can be formally represented, using entity terms (shown
e right hand side). Terms referring to cells are indicated in yellow,
les in green. With the exception of is_a, all relations are meant to
all instances x of the type Tight junction there is some y that is an
Figure 2 Outline of an ontology of cell mechanisms and its use to describe cell transitions. The figure shows a structure by which
mechanism changes can be formally represented, using entity terms (shown on the left hand side) and quality modifiers (shown on the right
hand side). The colour code follows the code used in Figure 3: Occurrents relevant for cell phenotypes are indicated in yellow, occurrents
relevant for ultrastructures in blue, and occurrents directly involving molecules in green. Up and down are intended to indicate relative
changes: Interaction Occludin-JAM Up states that there is a development in the cell to feature more interactions of this kind, no matter how
many of them there have been before. With the exception of is_a, all relations are meant to have an all-some syntax (cf. Figure 1).
Fuellen et al. Journal of Biomedical Semantics 2013, 4:25 Page 3 of 8
http://www.jbiomedsem.com/content/4/1/25physiological phenotypes of cells and their components.
Again, the hierarchical interconnections between the mo-
lecular entities and cellular parts (components, anatomical
structures, cell types) need to be explicitly established, for
example, between Occludin in tight junctions to the anat-
omy of specific cell types.
To describe cell phenotypes and transitions, we refer
to entities belonging to distinct ontological top-level cat-
egories [17]:
(1)Independent continuants: Cells and their organelles as
well as molecules are three-dimensional entities; they
are present with all their spatial parts at every time of
their existence.
(2)Dependent continuants: Any property of a cell or a
molecule, be it a quality or a disposition, also exists as a
whole at every time of its existence. However, any such
property is ontologically dependent on its particular
bearer: It cannot exist without it.
(3)Occurrents: Interactions, inhibitions, stimulations as
well as transitions are temporally extended processes.
They have temporal parts that occur at different
times; hence they do not exist as a whole at any single
point of time.We can, for example, describe the state of a cell at a
certain time by enumerating all of its parts and contents
(independent continuants), or by enumerating all of its
properties (dependent continuants), or by enumerating
all the events going on at this time (occurrents), which
could then be connected with parts and contents of the
cell as their participants, e.g. with organelles or mole-
cules. All of these categories are needed to integrate the
data available: Cellular data describe continuants (like
cellular components and dispositions) as well as occur-
rents, namely the molecular interactions (microscale
mechanisms) going on in a cell at a certain time. While
these continuants are covered by the phenotype ontol-
ogy scheme, the interactions (microscale mechanisms)
are covered by the mechanism ontology. Cell transition
data describe occurrents, namely macroscale changes of
microscale mechanisms. Within the EQ framework, we
can describe such macroscale changes of microscale
mechanisms by pairing terms for microscale mechanisms
(as entities) with specific change modifiers (as qualities).
In Figure 2 we illustrate this with one possible annotation
pattern for a cell transition. In this annotation pattern, the
entity term Interaction Occludin-JAM from the mechan-
ism ontology is used as a subject term in combination
Fuellen et al. Journal of Biomedical Semantics 2013, 4:25 Page 4 of 8
http://www.jbiomedsem.com/content/4/1/25with the qualifier up in order to express that in a certain
time step the interaction between Occludin and JAM has
been upregulated.
In our framework, a pluripotent cell can then be char-
acterized by its expression data (about genes, proteins
etc.), from which relevant microscale mechanisms can
be inferred. A cell transition from one cell phenotype
into another (e.g., of a fibroblast into a pluripotent cell)
can be described by comparing the expression data of
both cell phenotypes, which capture macroscale changes
in microscale mechanisms. Such expression data include
the start-up of the interactions between genes/proteins
relevant for the induction of pluripotency. Such a start-up
may happen because the cell starts to produce more in-
stances of the molecule types participating in this type of
interaction. In our framework, a pluripotent cell realizes
dispositions for mechanisms relevant for pluripotency that
may be described by a network of interactions. Further, a
cell transition from fibroblast to pluripotent cell realizes
dispositions for changes in mechanisms. After transition,
the cell is characterized by the microscale mechanisms
relevant for the pluripotent phenotype.
The use of the ontologies within the EQ framework
Our ontologies are designed to be used together with
specific modifiers within the EQ framework. As shown
on the right-hand side of Figure 1, the ontology of cell
phenotypes can be used to collect annotations for cell
phenotypes such as fibroblast, epithelial cell and pluripo-
tent stem cell. We can set up annotation profiles of cells,
consisting of sets of EQ pairs that describe them. ForFigure 3 Using EQ syntax to represent cellular dynamics. The diagram
mechanism changes (MECH) work together with the EQ syntax in order to
annotation profiles for three cell types (A, B, C) and for two macroscale cha
profiles is time-stamped: A is the phenotype of the cell in question before
phenotype after 2 hours. In each annotation profile we use, e.g., terms for
represent the participants of the microscale mechanisms that are actively o
cell will normally vary through time, and such a series of profiles can be us
we use terms for mechanisms together with modifiers like up and down
mechanism going on within the cell.example, the profile of epithelial cells includes the infor-
mation that the genes/proteins Occludin, the Junctional
adhesion molecule (JAM), Claudin as well as tight junc-
tions (TJs) are present, and cell membranes are joined.
For this purpose, we can use a number of standardized
modifiers like present, absent, up, moderately up, and
down, which can be integrated within an ontology like
PATO [13]. The quality terms used in a particular annota-
tion profile are derived from the data being annotated,
describing, e.g., a specific set of epithelial cells in a specific
culture medium.
The ontology of cell mechanisms, on the other hand,
is designed to be used together with modifiers like up
and down in order to yield descriptions for macroscale
changes of the microscale mechanisms going on within
a cell (up for start-up; down for shutdown). The right-
hand side of Figure 2 features, e.g., the specific changes
(up for start-up) of the microscale mechanisms relevant
for TJs, which are the macroscale changes associated
with TJ formation. Within the framework of the EQ syn-
tax, qualities help to describe these changes of the mi-
croscale mechanisms. The example hierarchy in Figure 2
reflects our example of the epithelial-mesenchymal transition
(EMT) and its reversal (MET, observed during reprogram-
ming). Within an EQ-framework, an MET can be coded
using network of mechanisms relevant for epithelial cell
as the entity term and up as the quality modifier; the net-
work of mechanisms relevant for mesenchymal cell goes
down simultaneously. Figure 3 shows how the ontology
of phenotypes and the ontology of mechanisms can be
combined in order to represent temporal dynamics.shows how the two ontologies for cell phenotypes (PHEN) and
represent the dynamics of cellular processes. It shows fictive
nges (from type A to B, and from type B to C). Each of the cell type
a certain intervention, B the phenotype after 1 hour and C the
molecular entities together with modifiers like present and absent to
ngoing in a cell at a certain point of time. The phenotypic profile of a
ed to adequately describe the history of a cell in an experiment. Here,
in order to describe the macroscale changes of the microscale
Fuellen et al. Journal of Biomedical Semantics 2013, 4:25 Page 5 of 8
http://www.jbiomedsem.com/content/4/1/25Annotation propagation
Both ontologies allow for annotation propagation. In
[12], annotations for anatomical entities are propagated
up a hierarchy of is_a and part_of relations, such that a
parent receives all the annotations of its children. The
rationale for this is the following. Every finger is part of
a hand; hence any information about a finger is also
information about some hand. Hence, whatever is expli-
citly annotated with finger is, implicitly, also about
some hand. Annotation propagation makes these impli-
cit annotations explicit via automated reasoning. In our
domain, however, universal part_of relations are rare: As
opposed to anatomical entities, molecules (like Occludin)
and organelles are not usually restricted to one specific
part of a cell or a specific cell, and process parts can be-
long to different process wholes. As a consequence of this,
the mereological hierarchy cannot be used in the same
way as in [12] for cell phenotypes and mechanism
changes. As there is an implicit universal quantification
over all instances of the first class in an assertion in an
ontology description language like the Web Ontology
Language (OWL) [18], we have to use has_part instead
of part_of. In our example, a molecular entity like
Occludin can belong to a range of cellular structures
and phenotypes, while a certain cellular structure or
phenotype has to possess certain molecular entities. Put
in general terms, a cellular structure necessitates its es-
sential molecular parts. That is, the whole determines
the parts, and for this reason we need to use the
has_part relation. The has_part relation is also appro-
priate for occurrents like cell mechanisms. This is be-
cause any initial temporal part of an event can happen
without the event being completed. For example, not
every S-phase needs to be part of a mitosis: the cell
cycle can be disrupted, e.g. by the destruction of the cell
that is about to divide, resulting in an S-phase that is
not followed by a cell division at all. In contrast to this,
every mitosis has an S-phase as one of its temporal
parts. Again, we need to employ the has_part hierarchy,
from whole processes to their necessary parts (e.g., from
Network_of_mechanisms_relevant_for_TJ to the Interaction_
Occludin_JAM). When employing annotation propagation,
therefore, as a rule, a whole process will have a higher in-
formation content than its necessary parts.
Similarity searches
The ontologies outlined above enable similarity searches
across cell phenotypes and mechanism changes in ana-
logy to [12]. In particular, we wish to estimate the simi-
larity of cell types and of cell transitions across time. In
this setting, macroscale changes are processes happening
from one state at a certain time to another state at a later
time. What we call microscale mechanisms are activities
around a certain time, i.e. activities that we suppose tohappen at some (maybe small) interval around that time.
Microscale mechanisms are typically described as undir-
ected activities (interactions), while macroscale changes
are of necessity directed to a certain end state. The EQ-
syntax is used to build up annotation profiles for the cell
types under scrutiny. If appropriate, they will be time-
stamped in order to mark how much time has elapsed
after a certain intervention (e.g., two days after interven-
tion X). If a macroscale change is the transition from cell
type A to cell type B, it can inherit the timestamps from
the annotation profiles of A and B as its start and end
time, respectively (see also Figure 3).
Similarity searches may then compare, e.g., EMT/MET
and reprogramming data. In simplified terms, an MET
(mesenchymal-epithelial transition) [19] consists in, first,
the formation of adherens junctions (AJs) and, second,
the formation of tight junctions (TJs). We represent the
MET as the start-up of the microscale mechanisms rele-
vant for an epithelial cell, which has as one of its parts
TJ formation that is, in turn, represented as the start-up
of the mechanisms relevant for a TJ. This is the inverse of
an EMT (epithelial-mesenchymal transition, which hap-
pens in development, metastasis and fibrosis). ExprEssence
and related tools ([20-23]) can be employed for generating
annotations about mechanism changes relevant for a
certain transition by means of high-throughput data
analysis, and the more mechanisms are annotated, the
better we can estimate how similar biological processes
are. Ultimately, any set of cell transitions can be com-
pared (using data coded in EQ syntax) with respect to
the underlying mechanisms, demonstrating the power
of our approach. Our ontology design principles thus
enable a kind of BLAST search in the space of annotations
(for mechanisms), with similar goals such as highlighting
relationships (between mechanisms, based on basic
mechanisms as building blocks), and eventually estimating
their evolutionary history.
The support for similarity search that we envision is
illustrated in Figure 4. From left to right, three possible
annotation profiles are shown that could be derived from
data on MET processes. For the sake of simplicity, the
qualities considered are restricted to up and down, mark-
ing upregulation of a mechanism (derived from high-
throughput data or literature curation) by a black tick
mark. Further assertions are then derived by automatic
annotation propagation: Our ontology of cell mechanisms
enables to infer (by subsumption reasoning, following the
idea of [12]) the magenta tick marks. Our example sug-
gests that similarity estimates after ontology-based reason-
ing are better and more reliable than without reasoning.
Clustering of cell phenotypes
While our individuation criteria for cell phenotype are
very fine-grained (even a tiny change in the molecular
Figure 4 Example of an ontology-supported similarity search. Three possible annotation profiles derived from data on MET processes are
exemplified, from left to right. Black tickmarks label mechanisms upregulated according to the data, and magenta tickmarks are inferred. After
reasoning, more plausible similarity estimates become possible.
Fuellen et al. Journal of Biomedical Semantics 2013, 4:25 Page 6 of 8
http://www.jbiomedsem.com/content/4/1/25repertoire constitutes a change in phenotype), we can
construct more coarse-grained cell types by clustering of
cell phenotypes based on similarity, considering the pres-
ence or absence of morphological or (ultra-)structural
components and of molecular entities. In addition, we can
also cluster the macroscale changes that transition cells
from one phenotype to another. A cluster of cells shares
aspects of components and microscale mechanisms. Gen-
erally speaking, similar phenotypes correspond to similar
cells, and similar mechanism changes correspond to simi-
lar cell transitions (cf. Figure 5). Thus, boundaries between
clusters of cells that are next neighbours (e.g. pluripotent
embryonic stem cells and epiblast stem cells) as well as be-
tween cells on opposite ends of a developmental spectrumFigure 5 Quasi-potential landscapes and cell phenotype and
mechanism change clustering. The quasi-potential landscape,
adapted from [24,25], illustrates how clusters of cells (cell phenotypes)
and clusters of mechanism changes (cell transitions) are related.(e.g. mesenchymal cells and epithelial cells) can be de-
fined by clustering based on expert annotations and on
bioinformatics analyses of experimental data. Clustering
of mechanism changes (that is, of macroscale changes
in microscale mechanisms) will in turn generate clusters
of similar mechanism changes with a large distance be-
tween the clusters. The cause for this large distance
then is the existence of strongly dissimilar cells.
We suggest that the improvement in similarity estimates
afforded by our ontologies (see Figure 4) enables the
plausible clustering of both cell phenotypes and cellular
mechanisms. Then it should become possible to cluster
cell phenotype and mechanism data sufficiently well to de-
rive the clusters exemplified in Figure 5.
Conclusion
We outlined how to design ontologies that enable to (1)
formally represent cell phenotypes and mechanism changes
behind cell transitions such as (re-)programming, and to
(2) develop algorithms exploiting this framework, includ-
ing clustering and searching for similar cell phenotypes
and mechanism changes. Both ontologies support manual
curation of publication data, annotation propagation and
information content measurement, as well as the inclusion
of results from high-throughput data analysis.
Our use of EQ-syntax allows the systematic encoding
of annotation profiles of cell phenotypes and mechanism
changes. The terms for both types of entities are organized
in hierarchies ranging from molecular to (ultra)structural
to morphological entities. Annotation profiles can then be
obtained using (1) data curation from publications or by
(2) high-throughput data analysis. In ontological terms,
Fuellen et al. Journal of Biomedical Semantics 2013, 4:25 Page 7 of 8
http://www.jbiomedsem.com/content/4/1/25bioinformatics tools such as ExprEssence can be used as
an instrument for deriving mechanistic information from
high-throughput data, turning information about con-
tinuants into information about occurrents by differen-
tial analysis. The starting point for expert curation,
possibly supported by text mining, must be a set of care-
fully selected papers.
Given a rich annotated knowledge base, existing ap-
proaches for ontology-based similarity measurements [12]
can be applied to the domains of cell phenotypes and cel-
lular mechanism changes. This would yield two important
functionalities: It allows clustering of cell phenotypes (and
of mechanism changes) by similarity, providing important
information for an operational definition of cell pheno-
types, and it allows similarity search in the spaces of
mechanism changes and of cell phenotypes.
To further refine and populate the ontologies, we cur-
rently explore the option to work together with collabo-
rators in the DFG SPP 1356 (http://www.spp1356.de) on
pluripotency and cellular reprogramming, and similar
initiatives, and we are looking for further collaborations.
The size of the final artifacts is obviously a function of
time and efforts invested in their development. While
the number of relevant entities is limited for cell anat-
omy and cell types (several thousands), it is very large
and virtually unlimited for molecular entities.
To evaluate our approach, we intent to compare simi-
larity search results based on high-throughput data ana-
lysis only to results based on employing the ontologies
integrating high-throughput data, (ultra)structural data
and morphological data, and further to compare both
sets of results with the expectations of domain experts.
We expect that in particular the relationships between
molecular events (which may be derived from filtering
high-throughput data) and ultrastructural events (cu-
rated from the literature) yield improvements for simi-
larity searches (see Figure 4). To avoid a garbage-in,
garbage-out scenario, the application domain must be
strictly limited, e.g. to data describing reprogramming
and EMT experiments, so that the input data can all be
validated by domain experts. Ultimately we envision a
community-based crowd-sourcing approach.Competing interests
The authors declare that they have no competing interests.Authors contributions
Starting from research by GF, LJ and GF developed the ontologies, while AK
and UL provided domain knowledge. LJ wrote the first version of this paper,
drawing on a larger unpublished manuscript by the authors. All authors read
and approved the manuscript.Acknowledgements
DFG support to AK and UL (AK 851/3-1, LE 1428/4-1), GF (FU 583/2-1, FU 583/2-2)
and LJ (JA 1904/2-1) is gratefully acknowledged.Author details
1Institute for Biostatistics and Informatics in Medicine and Ageing Research,
Rostock Medical School, Ernst-Heydemann-Str. 8, 18057 Rostock, Germany.
2Institute of Philosophy, University of Rostock, 18051 Rostock, Germany.
3Humboldt Universität zu Berlin, Unter den Linden 6, 10099 Berlin, Germany.
4Charite Berlin, BCRT, Augustenburger Platz 1, 13353 Berlin, Germany.
5College of Veterinary Medicine and Research Institute for Veterinary Science,
Seoul National University, Seoul, Republic of Korea. 6Department of
Philosophy, University of Münster, Domplatz 6, 48143 Münster, Germany.
Received: 21 January 2013 Accepted: 19 August 2013
Published: 8 October 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Dameron et al. Journal of Biomedical Semantics 2013, 4:17
http://www.jbiomedsem.com/content/4/1/17
RESEARCH Open Access
OWLmodel of clinical trial eligibility criteria
compatible with partially-known information
Olivier Dameron1,2*, Paolo Besana1,2, Oussama Zekri3, Annabel Bourdé1,2, Anita Burgun1,2
and Marc Cuggia1,2
Abstract
Background: Clinical trials are important for patients, for researchers and for companies. One of the major
bottlenecks is patient recruitment. This task requires the matching of a large volume of information about the patient
with numerous eligibility criteria, in a logically-complex combination. Moreover, some of the patients information
necessary to determine the status of the eligibility criteria may not be available at the time of pre-screening.
Results: We showed that the classic approach based on negation as failure over-estimates rejection when
confronted with partially-known information about the eligibility criteria because it ignores the distinction between a
trial for which patient eligibility should be rejected and trials for which patient eligibility cannot be asserted. We have
also shown that 58.64% of the values were unknown in the 286 prostate cancer cases examined during the weekly
urology multidisciplinary meetings at Rennes university hospital between October 2008 and March 2009.
We propose an OWL design pattern for modeling eligibility criteria based on the open world assumption to address
the missing information problem. We validate our model on a fictitious clinical trial and evaluate it on two real clinical
trials. Our approach successfully distinguished clinical trials for which the patient is eligible, clinical trials for which we
know that the patient is not eligible and clinical trials for which the patient may be eligible provided that further
pieces of information (which we can identify) can be obtained.
Conclusions: OWL-based reasoning based on the open world assumption provides an adequate framework for
distinguishing those patients who can confidently be rejected from those whose status cannot be determined. The
expected benefits are a reduction of the workload of the physicians and a higher efficiency by allowing them to focus
on the patients whose eligibility actually require expertise.
Introduction
Patient recruitment is a major focus in all clinical tri-
als. Adequate enrollment provides a base for projected
participant retention, resulting in evaluative patient data.
Identification of eligible patients for clinical trials (from
the principal investigators perspective) or identification
of clinical trials in which the patient can be enrolled (from
the patients perspective) is an essential phase of clini-
cal research and an active area of medical informatics
research. TheNational Cancer Institute has identified sev-
eral barriers that health care professionals claim in regard
to clinical trial participation [1]. Among those barriers,
*Correspondence: olivier.dameron@univ-rennes1.fr
1Université de Rennes1, UMR936, F-35000 Rennes, France
2INSERM UMR936, F-35000 Rennes, France
Full list of author information is available at the end of the article
lack of awareness of appropriate clinical trials is frequently
mentioned.
Automated tools that help perform a systematic screen-
ing either of the potential clinical trials for a patient, or
of the potential patients for a clinical trial could overcome
this barrier [2]. Efforts have been dedicated to provide
a uniform access to heterogeneous data from different
sources. The Biomedical Translational Research Infor-
mation System (BTRIS) is being developed at NIH to
consolidate clinical research data [3]. It is intended to sim-
plify data access and analysis of data from active clinical
trials and to facilitate reuse of existing data to answer
new questions. STRIDE [4] is a platform supporting clin-
ical and translational research consisting of a clinical data
warehouse, an application development framework for
building research data management applications and a
© 2013 Dameron et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Dameron et al. Journal of Biomedical Semantics 2013, 4:17 Page 2 of 10
http://www.jbiomedsem.com/content/4/1/17
biospecimen data management system. The i2b2 frame-
work integrates medical records and clinical research
data [5] and SHRINE [6] handles several sources by pro-
viding a federated query tool for clinical data reposito-
ries. The ObTiMA system relies on OWL and SWRL
to perform semantic mediation between heterogeneous
data sources [7]. Lezcano et al. propose an architec-
ture based on OWL to represent patients data from
archetypes, and on SWRL rules to perform the reason-
ing [8]. Several other efforts have been dedicated to
the formal representation of clinical trials eligibility cri-
teria to support automated reasoning [9]. Weng et al.
performed an extensive literature review [10]. They con-
firmed that although eligibility criteria are usually written
in free text to be human-readble, standard-based com-
putable knowledge representations for eligibility criteria
are necessary to clinical and research tasks. They iden-
tified five key aspects of eligibility criteria representa-
tion, three of which being essential for knowledge-based
representation of eligibility criteria: expression language
for representing eligibility rules, the encoding of eligi-
bility concepts and patient data modeling. Milian et al.
developed a method for automatic formalization of eli-
gibility criteria and comparison of their restrictiveness
[11,12]. Their goal is to support the design of eligibil-
ity criteria, enable their reuse and provide meaningful
suggestions of relaxing them based on previous trials.
They processed eligibility criteria from 300 clinical trials,
and created a library of structured conditions cover-
ing 18% of encountered inclusion and exclusion criteria.
Ross et al. conducted a survey of 1,000 criteria randomly
selected from ClinicalTrials.gov and found that 80% of
them had a significant semantic complexity [13], with
40% involving some temporal reasoning. Tu et al. pro-
posed an approach to convert free text eligibility criteria
into the computable ERGO formalism [14]. OConnor
et al. developed a solution based on OWL and SWRL
that supports temporal reasoning and bridges the gap
between patients specific data and more general eligibility
criteria [15].
The ASTEC (Automatic Selection of clinical Trials
based on Eligibility Criteria) project aims at automat-
ing the search of prostate cancer clinical trials to which
patients could be enrolled to [16]. It features syntac-
tic and semantic interoperability between the oncologic
electronic medical records and the recruitment deci-
sion system using a set of international standards (HL7
and NCIT), and the inference method is based on
ERGO [17].
The EHR4CR project aims at facilitating clinical trial
design and patient recruitment by developing tools and
services that reuse data from heterogeneous electronic
health records [18]. The TRANSFoRm project has similar
objectives for primary care [19,20].
All these studies on data and criteria representa-
tion, integration and reasoning are motivated by the
requirement to have the necessary information available
at the time of processing the patients data, and assume
that somehow, that will be the case.
Missing information that is required for deciding
whether a criterion is met leads to recruitment being
underestimated.
Solutions for circumventing this difficulty consist either
in making assumptions about the undecided criteria, or
in having a pre-screening phase considering a subset of
the criteria for which patients data are assumed to be
available.
Bayesian belief networks have been used to address the
former [21] but require a sensible choice of probability
values and may lead to the wrong asumption in particular
cases.
The latter leaves most of the decision task to human
expertise, which provides little added value (if an expert
has to handle the difficult criteria, automatically process-
ing the simple pre-screening ones is only a little weight
off his shoulders) and is still susceptible to the problem of
missing information for the pre-screening criteria.
We propose an OWL design pattern for modeling clin-
ical trial eligibility criteria. This design pattern is based
on the open world assumption for handling missing infor-
mation. It infers whether a patient is eligible or not
for a clinical trial, or if no definitive conclusion can be
reached.
Background
Modeling eligibility criteria
A clinical trial can bemodeled as a pair< (Ii)ni=0, (Ej)mj=0 >
where (Ii)ni=0 is the set of the inclusion criteria, and (Ej)mj=0
is the set of the exclusion criteria. All the eligibility crite-
ria from (Ii)ni=0 ? (Ej)mj=0 are supposed to be independent
from one another (at least in the weak sense: the value
of criterion Ck cannot be infered from the combined val-
ues of other criteria). Each criterion can be modeled as
an unary predicate C(p), where the variable p represents
all the information available for the patient. C(p) is true if
and only if the criterion is met.
A patient is deemed eligible for a clinical trial if all the
inclusion criteria and none of the exclusion criteria are
met.
patient eligible ? n?
i=0 Ii(p) ? ¬(
m?
j=0Ej(p)) (1)
Before making the final decision on the list of clinical
trials for which a patient is eligible for, there are interme-
diate pre-screening phases where only the main eligibility
criteria of each clinical trial are considered. Such pre-
screening sessions rely on subsets of (Ii)ni=0 and (Ej)mj=0,
but the decision process remains the same.
Dameron et al. Journal of Biomedical Semantics 2013, 4:17 Page 3 of 10
http://www.jbiomedsem.com/content/4/1/17
For the sake of clarity, in addition to the general case,
we will consider a simple clinical trial with two inclusion
criteria I0 and I1, and two exclusion criteria E0 and E1.
patient eligible ? I0(p)? I1(p)?¬(E0(p)?E1(p)) (2)
For example, these criteria could be:
 I0: evidence of a prostate adenocarcinoma;
 I1: absence of metastasis;
 E0: patient older than 70 years old;
 E1: evidence of diabetes.
According to equation 2, a patient would be eligible for
the clinical trial if and only if he has a prostate adenocar-
cinoma and has no metastasis and is neither older than 70
years old nor suffers from diabetes.
Because of De Morgans laws, equation 1 is equivalent
to:
patient eligible ? ( n?
i=0 Ii(p)) ? (
m?
j=0¬Ej(p)) (3)
Even though equation 1 and equation 3 are logically
equivalent, the latter is often preferred because it is an
uniform conjunction of criteria. Note that the negations
in front of the exclusion criteria are purely formal, as
both inclusion and exclusion criteria can represent an
asserted presence (e.g. prostate adenocarcinoma for I0 or
of diabetes for E1) or an asserted absence (e.g. metastasis
for I1).
For our example:
patient eligible ? I0(p)? I1(p)? (¬E0(p))? (¬E1(p))
(4)
According to equation 3, a patient would be eligible for
the clinical trial if and only if he has a prostate adenocarci-
noma and has no metastasis and is not older than 70 years
old and does not suffer from diabetes.
The problem of unknown information
Distinction between the patients that we know are not
eligible and those that we do not know if they are eligible
When a part of the information necessary for determining
if at least one criterion is met is unknown, the conjunction
of equation 3 can never be true. This necessarily makes
the patient not eligible for the clinical trial, whereas the
correct interpretation of the situation is that the patient
cannot be proven to be eligible. This is different from
proving that the patient is not eligible, and indeed, in real-
ity the patient can sometimes be included by assuming the
missing values (cf. next section).
For our fictitious clinical trial, we consider a population
of nine patients covering all the combinations of True,
False or Unknown for the inclusion criterion I1 and
the exclusion criterion E1. Table 1 presents the value of
Table 1 Differences between the logical evaluation of the
criteria conjunction and the correct inclusion decision
when only a portion of the necessary information is
known: evaluation of equation 4 and correct inclusion
decision for all the possible values of I1 and E1, with
possibly unknown information
Patient I0 I1 E0 E1 I0 ? I1? Decision
¬E0 ? ¬E1
p0 T T F T F Exclude
(E1)
p1 T T F F T Include
p2 T T F ? F Propose
cannot (assume ¬E1)
assert ¬E1
p3 T F F T F Exclude
(both ¬I1 and E1)
p4 T F F F F Exclude
(¬I1)
p5 T F F ? F Exclude
(¬I1)
p6 T ? F T F Exclude
(E1)
p7 T ? F F F Propose
cannot (assume I1)
assert I1
F
p8 T ? F ? cannot Propose
assert I1
cannot (assume both
assert ¬E1 I1 and ¬E1)
equation 4 and correct inclusion decision for the nine
combinations. Among the five patients (p2, p5, p6, p7 and
p8) for which at least a part of the information is unknown,
three (p2, p7 and p8) illustrate a conflict between the value
of equation 4 and expected inclusion decision. A strict
interpretation of equation 4 leads to the exclusion of the
eight patients:
Dameron et al. Journal of Biomedical Semantics 2013, 4:17 Page 4 of 10
http://www.jbiomedsem.com/content/4/1/17
 for three of them (p0, p3 and p4), all the information
is available;
 for two of them (p5 and p6), some information is
unknown, but the available information is sufficient
to conclude that the patients are not eligible;
 for the three others (p2, p7 and p8), however, the
cause of rejection is either because one of the
inclusion criteria cannot be proven (I1 for p7 and p8)
or because one of the exclusion criteria cannot be
proven to be false (E1 for p2 and p8).
In the case of unknown information, equation 3 alone is
not enough to make the distinction between the patients
we know are not eligible (the first two categories, so this
also includes patients for whom a part of the information
is unknown) and those we do not know if they are eligible
(the third category). This is a problem because patients
from the first two categories should be excluded from the
clinical trial, whereas those from the third category should
be considered for inclusion.
Assuming values for criteria
Currently, the case of each patient diagnosed with can-
cer is examined in a multidisciplinary meeting (MDM)
gathering experts (oncologists, pathologists, surgeons,...).
The goal is to determine collectively the best therapeutic
strategy for the patient, including consideration of poten-
tial inclusion into clinical trials. This preliminary stage is
called pre-screening because it takes place before obtain-
ing the patients informed consent (i.e., before enroll-
ment). It mainly relies on retrospective data coming from
the patient health record. At this point, all the information
necessary for determining the status of each inclusion and
exclusion criteria may not be available, but the rationale is
to focus on the clinical trials for which the patient may be
eligible for. It should be noted that the missing items may
differ between patients. One solution could be to assume
the values of the unknown criteria in order to go back to a
situation where inclusion or exclusion could be computed
using equation 3.
In this case:
 inclusion criteria for which the available information
is not sufficient to compute the status are considered
to be met;
 exclusion criteria for which the available information
is not sufficient to compute the status are considered
not to be met.
Therefore, in the case where the available information is
not sufficient to compute the status of a criterion, a differ-
ent status is assumed depending on whether the criterion
determines inclusion or exclusion.
Referring to our fictitious clinical trial, the lack of infor-
mation about the absence of metastasis would lead to the
assumption that I1 is true, whereas the lack of informa-
tion about diabetes would lead to the assumption that E1
is false.
This situation raises several issues:
 a different status is assumed depending on whether
the criterion determines inclusion or exclusion;
 the assumed status depends on the nature of the
criterion (i.e. inclusion or exclusion) and not on its
probability;
 one has to remember that the value for at least a
criterion has been assumed in order to qualify the
inferred eligibility (adamant for p0 or p1 vs under the
assumption that... for p2, p7 and p8);
 this qualification can be difficult to compute (the
status of E1 is unknown for both p2 and p5, but p5
can be confidently excluded whereas p2 can be
included assuming E1).
The extent of the missing information problem
To determine the extent of the missing information prob-
lem, we analyzed the 286 prostate cancer cases examined
during the weekly urology multidisciplinary meetings at
Rennes university hospital between October 2008 and
March 2009. This involved 252 patients: 25 of them
were examined during two different MDM, and 5 were
examined during three different MDM. Before the MDM,
the patients data are collected in a form with 65 fields.
The form supports the distinction between known and
unknown values (e.g. for antecedent of neoplasm, the
possible answer are yes, no, not specified).
Overall, 11,323 values (60.90%) were not specified. On
average, for each case studied in a MDM, 39.6 fields
(among 65) had an unknown value.
All of the 286 cases studied had at least some of the 65
fields with an unknown value. Indeed, the case with the
most fields filled still missed 22 of them.
59 fields (90.77% of 65) had a missing value in at least
one of the 286 cases. The six fields that were systemat-
ically filled were: the patient identifier, the MDM date,
the patients birth date, the patients gender, the tumor
anatomic site and the primary histological type.
During this period, 4 clinical trials related to prostate
cancer running at Rennes Comprehensive Cancer Centre
were considered during the MDM. Table 2 presents the
composition of the clinical trials fields and their propor-
tion of missing information. It shows that for each clinical
trial, all the patients had at least one missing field that pre-
vented formula 3 to be true (regardless of the values of the
known fields).
Methods
We propose an OWL design pattern for modeling clinical
trial eligibility criteria. We then explain how the reasoning
Dameron et al. Journal of Biomedical Semantics 2013, 4:17 Page 5 of 10
http://www.jbiomedsem.com/content/4/1/17
Table 2 Importance of unknown information during pre-screening for the four clinical trials of interest: importance of
unknown information during pre-screening for the four clinical trials of interest
CT1 CT2 CT3 CT4
Nb inclusion fields 15 19 16 10
Nb exclusion fields 10 9 8 11
Nb common fields 3 0 2 3
Missing values 50.06% 61.72% 56.52% 42.99%
Nb patients with all inclusion fields known 0 0 1 1
Nb patients with all exclusion fields known 4 3 0 1
Nb patients with all fields known 0 0 0 0
Nb eligible patients 30 23 6 2
unfolds using the fictitious clinical trial from Table 1. We
validate our approach by verifying if the inferred outcome
corresponds to the expected value from Table 1. We eval-
uate our approach on two of the four clinical trials related
to prostate cancer and the 286 cases mentioned in the
previous section. This allows us to quantify the impact of
missing information on inclusion rates, as we have seen
that in some cases, even partially-known information can
lead to certain rejection.
We reused anonymized data from the patients medical
records and did not conduct any experimental study. The
study was approved by Rennes Hospital ethics evaluation
committee institutional review board under the reference
13-26 (2013).
Results
Eligibility criteria design pattern
 for each criterion, create a class C_i (at this point, we
do not care if it is an inclusion or an exclusion
criteria, or both) and possibly add a necessary and
sufficient definition representing the criterion itself
(or use SWRL);
 for each criterion, create a class Not_C_i defined as
Not_C_i ? Criterion ¬ C_i. This process can
be automated;
 for each clinical trial, create a class Ct_k
(placeholder);
 for each clinical trial, create a class Ct_k_include
as a subclass of Ct_k with a necessary and sufficient
definition representing the conjunction of the
inclusion criteria and of the exclusion criteria (cf.
equation 3) (Ct_k_include ? n
i=0 I_i 
m
j=0
Not_E_j);
 for each clinical trial, create a class Ct_k_exclude
(placeholder) as a subclass of Ct_k;
 for each clinical trial, create a class
Ct_k_exclude_at_least_one_exclusion_
criterion as a subclass of
Ct_k_exclude with a necessary and sufficient
definition representing the disjunction of the
exclusion criteria
(Ct_k_exclude_at_least_one_exclusion_
criterion ? munionsq
j=0 E_j);
 for each clinical trial, create a class
Ct_k_exclude_at_least_one_failed_
inclusion_criterion as a subclass of
Ct_k_exclude with a necessary and sufficient
definition representing the disjunction of the negated
inclusion criteria
(Ct_k_exclude_at_least_one_failed_
incl_criterion ? nunionsq
i=0 Not_I_i);
 represent the patients data with instances (Figures 1
and 2). For the sake of simplicity, we will make the
patient an instance of as many C_i as we know he
matches criteria, and as many Not_C_j classes as we
know he does not match criteria, even if this is
ontologically questionable (a patient is not an
instance of a criterion). How the patients data are
reconciled with the criteria by making the patient an
instance of the criteria is not specified here: it can be
manually, or automatically with OWL necessary and
sufficient definitions or SWRL rules for the C_i and
Not_C_j classes.
Reasoning
If all the required information is available, after classifica-
tion, for each criterion the patient will be an instance of
each C_i or Not_C_i, and therefore will also be instan-
tiated as either Ct_k_include (like p1 in Figure 3),
Ct_k_exclude_at_least_one_exclusion_criterion
or Ct_k_exclude_at_least_one_failed_inclusion_
criterion (so at least we are doing as well as the other
systems).
If not all the information is available, because of the
open world assumption, there will be some criteria for
which the patient will neither be classified as an instance
of C_i nor of Not_C_i (e.g. in Figure 2, p2 is neither
Dameron et al. Journal of Biomedical Semantics 2013, 4:17 Page 6 of 10
http://www.jbiomedsem.com/content/4/1/17
Figure 1 A patient for whom all the information is available.
an instance of E_1 nor of Not_E_1), so he will not
be classified as an instance of Ct_k_include either.
However, the patient may be classified as an instance of
Ct_k_exclude_at_least_one_exclusion_criterion
or of Ct_k_exclude_at_least_one_failed_inclusion_
criterion. As both are subclasses of Ct_k_exclude,
we will conclude that the patient is not eligible for
the clinical trial. We will even know if it is because
he matched an exclusion criterion (like p0, p3 and p6
in Figure 4), because he failed to match an inclusion
Figure 2 A patient for whom some information is unknown (here about E1).
Dameron et al. Journal of Biomedical Semantics 2013, 4:17 Page 7 of 10
http://www.jbiomedsem.com/content/4/1/17
Figure 3 The class modeling clinical trial inclusion after classification (here patient p1 can be included).
criterion (like p3, p4 and p5 in Figure 5), or both
(like p3).
If the patient is neither classified as an instance of
Ct_k_include nor of Ct_k_exclude (or its sub-
classes), then we will conclude that the patient can be
considered for the clinical trial, assuming the missing
information will not prevent it (like p2, p7 and p8, who do
not appear in Figures 3, 4 and 5, consistently with Table 1.
By retrieving the criteria for which the patient is neither
an instance of C_i nor of Not_C_i, we will know which
information is missing.
Validation
We modeled our fictitious clinical trial as well as the nine
combinations of values (Additional file 1). All the results
were identical to the decision of Table 1.
Evaluation
We evaluated our model on the first (Additional file 2) and
third (Additional file 3) clinical trials.
First clinical trial
According to our system, among the 286 cases, 0 were for-
mally eligible, 149 were potentially eligible, and 137 were
not eligible. The 30 cases that were identified as eligible
by the experts during the multidisciplinary meetings were
all among the 149 proposed by our system (precision was
0.20; recall was 1.0).
It should be noted that the a posteriori analysis of the
119 cases proposed by our model but not by the MDM
revealed that several were not proposed even if they for-
mally met the eligibility criteria because their Gleason
score was deemed too low. We added an inclusion crite-
rion requiring patients to have a Gleason score superior
or equal to 7. This resulted in 67 cases potentially eligible,
among which were 24 of the 30 actually eligible (precision
was 0.36; recall was 0.80). The six false negative cases had
a Gleason score of 6. Among the 43 false positive, at least
15 were rejected during the MDM because of additional
information not available at the time of pre-screening: 8
because new results indicated that they did not have can-
cer, 3 because too much information was missing and
4 because other elements such as a relatively young age
resulted in proposing a surgical treatment instead of the
clinical trial.
Third clinical trial
According to our system, among the 286 cases, 0 were for-
mally eligible, 34 were potentially eligible, and 252 were
not eligible. The 6 cases that were identified as eligible by
the experts during the multidisciplinary meetings were all
among the 34 proposed by our system (precision was 0.18;
recall was 1.0). Among the 28 false positive, 6 cases were
rejected during the MDM because of additional informa-
tion not available at the time of pre-screening, 5 were
rejected on the basis of information present in their report
Figure 4 The class modeling clinical trial exclusion because at least one of the exclusion criteria has beenmet after classification (here
patients p0, p3 and p6 match the definition).
Dameron et al. Journal of Biomedical Semantics 2013, 4:17 Page 8 of 10
http://www.jbiomedsem.com/content/4/1/17
Figure 5 The class modeling clinical trial exclusion because at least one of the inclusion criteria failed to be met after classification (here
patients p3, p4 and p5 match the definition).
but erroneously missing in the database, 15 were rejected
because there was no evidence of recurring cancer (not all
the cases examined during the MDM of urology have can-
cer even if most do), and 2 cases were rejected because too
much information was missing.
Adding implicit inclusion criteria for performing the
same post-processing as the first clinical trial resulted in
only 17 potentially eligible cases, among which were 3 of
the 6 identified by the experts (precision was 0.18; recall
was 0.5). This shows that this strategy is not relevant for
this clinical trial.
Discussion
The observed proportion of missing information is com-
patible with results from other studies [22]. Köpcke et al.
compared the information from 706 patient to 351 eligi-
bility criteria from 15 clinical trials. They reported that the
total completeness of EHR data for recruitment purposes
was 35%.
The analysis of the first clinical trial demonstrates that
missing information would have led to the rejection of
all the patients proposed as eligible by the experts during
the multidisciplinary meetings. Our approach identified
potentially eligible patients (149 for the first clinical trial,
and 34 for the third), among which were all the patients
deemed eligible by the experts (30 for the first clinical trial,
and 6 for the third).
This shows that our system confidently rejects non-
eligible cases, which leaves more time to examine the
others during the multidisciplinary meetings. Moreover,
in the first clinical trial, precision can be significatively
improved by adding pragmatic criteria that further dis-
criminate the patients who would not be considered as
eligible even if they meet the pre-screening criteria. Note
that this second step can be kept separate from the for-
mal determination of eligibility but is useful both for the
acceptance of the system by the experts and for maintain-
ing the efficiency of the multidisciplinary meetings.
Missing information can partially be handled even with
reasoning based on negation as failure using ad hoc
conversion between inclusion and exclusion criteria. For
example, the inclusion criterion absence of ischemic heart
disease can be converted into the exclusion criterion
presence of ischemic heart disease. The former will prob-
ably never bemet because a patients record onlymentions
ischemic heart disease when they are present, whereas
the latter will (correctly) only exclude those patients hav-
ing evidence of ischemic heart disease. The problem is
that if absence of ischemic heart disease had been an
exclusion criterion, it would likewise have been converted
into the inclusion criterion presence of ischemic heart
disease and the system would have (incorrectly, at least
during pre-screening) rejected patients whose record does
not mention the presence nor the absence of ischemic
heart disease. Moreover, a criterion can be an inclusion
criterion for a clinical trial and an exclusion criterion for
another trial, so this strategy is not a general solution to
the problem of missing information.
Reasoning about the conjunction of the eligibility cri-
teria should be handled by OWL, which supports the
open world assumption, rather than by related technolo-
gies such as SWRL which do not. It would be possible
to write a SWRL rule that represents the conjunction of
criteria (cf. formula 3). However, it is impossible to distin-
guish situations where we know that one criterion is not
met from those where we cannot determine if it is met,
because in both cases the rule will not fire.
Applying our criteria modeling design pattern to real
clinical trials and real patients data was a manual pro-
cess. The reasoning part of our contribution focused on
combining the status of the eligibility criteria when some
of then can not be determined, not on determining the
statuses themselves. However, both points are of impor-
tance. Our design pattern consisted in modeling each
criterion by two classes representing the certain presence
and the certain absence of the criterion for a patient. As we
have seen in this article, this first modeling part was easy,
can be automated, and addressed the problem of missing
information as one of the causes of patient recruitment
underestimation. When evaluating our system on real
clinical trials and real patients data, we had to determine
for each patient whether each criterion was met. This
Dameron et al. Journal of Biomedical Semantics 2013, 4:17 Page 9 of 10
http://www.jbiomedsem.com/content/4/1/17
required both the occasional decomposition of complex
criteria into logical combinations of simpler conditions,
and the binding with the patients data representation in
the local EHR. The first step is generic and rather straight-
forward. It only has to be done once, and can be reused
shared between hospitals or reused if a criterion appears
in several clinical trials. The second step is clearly depen-
dent on the local representation of patients data, and was
more difficult and labor-intensive. It also required to write
the functions that process the data, which took a couple
of days for each clinical trial (a portion of the code written
for the first CT could be reused for the second one).
The standardization of data elements would provide
a significant help to the challenge of connecting the
patients data with the eligibility criteria. The main stan-
dard organizations (HL7,OpenEHR/EN213606 for clini-
cal care) and CDISC [23] (for clinical research domain)
define their own semantic interoperability framework to
structure and encode data elements with reference ter-
minologies. Moreover recent initiatives have been carried
out to fill the gap between clinical data sources com-
ing from EHRs and Clinical Data Management Systems
(CDMS) including Recruitment Support Systems. For
instance, the Joint Initiative Council was formed as a
partnership between HL7, CDISC, ISO TC 215, IHTSDO,
and CEN TC 251 with the stated goal of increasing col-
laboration between standards organizations based on the
recognition of a common goal of computable seman-
tic interoperability. Clinical Data Acquisition Standards
Harmonization (CDASH) is an initiative that specifies
the unambiguous semantics of a number of common
data elements that are deemed common to all trials.
As such, CDASH represents a significant first-step in
achieving cross-trial semantic interoperability. BRIDG
[24] (Biomedical Research Integrated Domain Group)
model which, on one side, contains representations of
clinical research data with underlying mappings to the
HL7 RIM and, on the other side, covers a superset of
the scope defined by CDASH. Currently, several projects
around the world are currently using these standards such
as REUSE [25], EHR4CR [18,26], TRANSFORM [19,20] or
CaBIG [27].
The use of RDF-based (Resource Description Frame-
work) Semantic Web formats (hopefully standardized)
data elements and eligibility criteria would also make
their integration easier. RDF proved to be a key elements
for data integration in more general contexts. Associated
querying and reasoning techniques based on SPARQL
(SPARQL Protocol and RDF Query Language) and SPIN
(SPARQL Inference Notation) for determining the status
of eligibility criteria would have the advantage of having
the rules represented in the same language as the schema
and data to which those rules are attached, as well as hav-
ing sustainable computation performances. On the other
hand, these strategies usually rely on closed-world reason-
ing. Future work should focus on studying the benefits
of such an approach and on determining how well it can
address the problem of missing information.
Potential applications of our approach are not limited
to clinical trials [21]. They cover all clinical decision sit-
uations where some information may be missing. We are
currently adapting this approach for the determination of
pacemaker alerts severity [28]. Electronic health records
and clinical reports have been shown to exhibit large
amounts of redundant information [29,30], but Pakhomov
et al. observed a discordance between patient-reported
symptoms and their (lack of) documentation in the elec-
tronic medical records [31]. They noted that this has
important implications for research studies that rely on
symptom information for patient identification and may
have clinical implications that must be evaluated for
potential impact on quality of care, patient safety, and
outcomes.
Conclusions
We have shown that ignoring the missing information
problem for automatic determination of clinical trial
eligibility led to over-estimate rejection. Systems based on
negation as failure infer that the patient is not eligible if
it cannot be proved that he is eligible, whereas the situ-
ations where it cannot be determined that the patient is
eligible nor that he is not eligible should be identified and
treated separately. A retrospective analysis of 252 patients
with prostate cancer showed that for the four clinical trials
of interest, all the patients had at least one missing value
that resulted in their rejection whereas 62 of them were
actually eligible for at least one of the clinical trials.
We proposed a modeling strategy of eligibility criteria
in OWL that leveraged the open world assumption to
address the missing information problem. Our approach
was able to distinguish a clinical trial for which the patient
is eligible, a clinical trial for which we know that the
patient is not eligible and a clinical trial for which the
patient may be eligible provided that further pieces of
information (which we can identify) can be obtained.
By confidently rejecting some of the non-eligible cases,
our approach leaves more time to examine those requiring
medical expertise during the multidisciplinary meetings.
Additional files
Additional file 1: OWL files for the validation set. The file
clinicalTrial-validation.tgz is a zipped tarball containing a
readme.txt and the OWL files modeling the criteria and the patients
data from the validation set.
Additional file 2: OWL files for the first clinical trial of evalution set.
The file clinicalTrial-getug14.tgz is a zipped tarball containing
a readme.txt and the OWL files modeling the criteria and the patients
data for the first clinical trial from the evaluation set.
Dameron et al. Journal of Biomedical Semantics 2013, 4:17 Page 10 of 10
http://www.jbiomedsem.com/content/4/1/17
Additional file 3: OWL files for the third clinical trial of evalution set.
The file clinicalTrial-getug16.tgz is a zipped tarball containing
a readme.txt and the OWL files modeling the criteria and the patients
data for the first clinical trial from the evaluation set.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
OD designed the study, implemented the reasoning mechanism and drafted
the manuscript. PB and AB implemented the patients data repository. OZ
collected the patients data and provided medical expertise. AB and MC
revised the manuscript. All authors read and approved the final manuscript.
Acknowledgements
The ASTEC project is funded by the French Agence Nationale pour la
Recherche (ANR 08-TECS-002).
Author details
1Université de Rennes1, UMR936, F-35000 Rennes, France. 2INSERM UMR936,
F-35000 Rennes, France. 3Centre Régional de Lutte Contre le Cancer Eugène
Marquis, F-35000 Rennes, France.
Received: 30 April 2013 Accepted: 21 August 2013
Published: 13 September 2013
PROCEEDINGS Open Access
NCBO Technology: Powering semantically aware
applications
Patricia L Whetzel1*, NCBO Team1,2,3,4
From Bio-Ontologies 2012
Long Beach, CA, USA. 13-14 July 2012
* Correspondence:
whetzel@stanford.edu
1Stanford Center for Biomedical
Informatics Research, Stanford
University, Stanford, CA 94305, USA
Abstract
As new biomedical technologies are developed, the amount of publically available
biomedical data continues to increase. To help manage these vast and disparate
data sources, researchers have turned to the Semantic Web. Specifically, ontologies
are used in data annotation, natural language processing, information retrieval,
clinical decision support, and data integration tasks. The development of software
applications to perform these tasks requires the integration of Web services to
incorporate the wide variety of ontologies used in the health care and life sciences.
The National Center for Biomedical Ontology, a National Center for Biomedical
Computing created under the NIH Roadmap, developed BioPortal, which provides
access to one of the largest repositories of biomedical ontologies. The NCBO Web
services provide programmtic access to these ontologies and can be grouped into
four categories; Ontology, Mapping, Annotation, and Data Access. The Ontology Web
services provide access to ontologies, their metadata, ontology versions, downloads,
navigation of the class hierarchy (parents, children, siblings) and details of each term.
The Mapping Web services provide access to the millions of ontology mappings
published in BioPortal. The NCBO Annotator Web service tags text automatically
with terms from ontologies in BioPortal, and the NCBO Resource Index Web services
provides access to an ontology-based index of public, online data resources. The
NCBO Widgets package the Ontology Web services for use directly in Web sites. The
functionality of the NCBO Web services and widgets are incorporated into
semantically aware applications for ontology development and visualization, data
annotation, and data integration. This overview will describe these classes of
applications, discuss a few examples of each type, and which NCBO Web services are
used by these applications.
NCBO Technology overview
BioPortal is an open repository of biomedical ontologies that stores ontologies devel-
oped in various formats, such as OWL, OBO format, Protégé frames, and the Rich
release format, and provides access to this content via Web browsers and Web services
[1,2]. The BioPortal Web interface allows users to browse the list of ontologies, search
and comment on terms in ontologies, annotate text with ontology terms, and search
an ontology-based index of biomedical resources. The BioPortal architecture currently
includes both LexEVS (http://informatics.mayo.edu/LexGrid) and the Protégé database
Whetzel and Journal of Biomedical Semantics 2013, 4(Suppl 1):S8
http://www.jbiomedsem.com/content/4/S1/S8 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2013 Whetzel and NCBO Team; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the
Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
(http://protege.stanford.edu), however work is underway to replace the dual database
backend with a RDF database. A beta version of the BioPortal RDF database is avail-
able at: http://sparql.bioontology.org.
The functionality of the BioPortal Web site is driven by the NCBO Web services,
which include the Ontology, Mapping, Annotator, and Resource Index Web services
(Figure 1). The Ontology Web services provide access to ontologies, their metadata,
ontology versions, navigation of the class hierarchy (parents, children, siblings) and
details of each term. These services also allow download of the ontology (in the origi-
nal format and in RDF), provide the ability to search for terms in an ontology, to
extract subsets of an ontology and to provide comments and propose new terms as
metadata to the ontology. The Widgets package the functionality of the Ontology Web
services in order to provide embeddable code for Web sites. These widgets include a
term autocomplete widget and ontology visualization widgets. The Mapping Web ser-
vices provides access to a variety of mappings published in BioPortal. The mapping
data includes mappings from UMLS based on shared Concept Unique Identifiers, map-
pings specified within ontologies, user submitted mappings, and automatically gener-
ated mappings using the Lexical OWL Ontology Matcher (LOOM), which generates
mappings based on lexical similarity of the preferred name and synonyms between
pairs of ontologies [3]. The Mapping Web services are parameterized to allow a high
degree of flexibility to access the data. For example, mappings can be accessed for one
ontology mapped to all other ontologies, between pairs of ontologies, for one term
mapped to all other terms, and between pairs of terms. This Web service can also be
used to submit mappings directly to BioPortal. The NCBO Annotator Web service
Figure 1 NCBO Technology The NCBO Web services and widgets provide access to ontologies in
BioPortal. The Web services can be grouped into four categories; Ontology, Mapping, Annotation and Data
Access.
Whetzel and Journal of Biomedical Semantics 2013, 4(Suppl 1):S8
http://www.jbiomedsem.com/content/4/S1/S8
Page 2 of 10
processes text to recognize terms from ontologies in BioPortal that exist within the
text [4]. The Annotator Web service uses the entity recognizer Mgrep [5], which out-
performs MetaMap in almost all cases evaluated for precision [6]. The Web service
parameters can be set to limit results to a particular ontology or to certain UMLS
semantic types and characterisitcs of the term matches can also be parameterized, e.g.
to recognize both preferred name and synonyms, match terms greater than X charac-
ters in length, and the ability to include a custom list of stopwords. The NCBO Anno-
tator Web service was used to generate an ontology-based index of several online
biomedical data repositories (e.g., GEO, ClinicalTrials.gov, dbGaP, DrugBank,
PharmGKB, and Reactome) resulting in the NCBO Resource Index [7,8]. The textual
metadata of data records from these resources was annotated with terms from ontolo-
gies in BioPortal and then stored locally for query efficiency. Therefore, data records
across databases are linked together via their shared ontology annotations. These
linkages take advantage of the semantic relationships within the ontology, including
subsumption relationships among ontology entities and mappings between entities in
different ontologies. The Resource Index is designed to provide updates in both new
resource data records and new ontology versions. The NCBO Resource Index Web ser-
vices provide a mechanism for programmatic search of the index using ontology terms.
For example, one can search for all experiments and clinical trials related to malignant
melanoma from GEO and ClinicalTrials.gov. The NCBO Web services are documen-
ted at: http://www.bioontology.org/wiki/index.php/NCBO_REST_services
Classes of applications incorporating ontologies via NCBO Technology
Ontology development and visualization
With the growing interest in the use of ontologies in the health care and life sciences,
additional tools are being developed to support the development of ontologies within
new biomedical domains and the re-use of existing ontologies to build application
ontologies. To this end, new plugins for ontology editing tools such as Protégé and
OBO-Edit have been developed. These plugins use the NCBO Web services to aid in
term re-use, to automatically generate ontology terms from text, provide an infrastruc-
ture for collaborative ontology development, and provide ways to visualize ontologies.
The BioPortal Import plugin [9] enables re-use of ontology terms by allowing the
ontology developer to search for terms in BioPortal directly from Protégé 3 (Figure 2).
The terms of interest can be directly imported enabling the re-use of terms rather than
creating new terms with new URIs. The developer can import an entire subtree of terms
and specify the desired depth of child terms to import. The annotation properties of the
imported terms can be specified in order to harmonize these properties with existing
terms in the new application ontology. The BioPortal Reference plugin [10] also
JOURNAL OF
BIOMEDICAL SEMANTICS
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9
http://www.jbiomedsem.com/content/4/1/9
RESEARCH Open Access
Semantic querying of relational data for
clinical intelligence: a semantic web
services-based approach
Alexandre Riazanov1*, Artjom Klein1, Arash Shaban-Nejad2, Gregory W Rose3, Alan J Forster3,
David L Buckeridge2 and Christopher JO Baker1
Abstract
Background: Clinical Intelligence, as a research and engineering discipline, is dedicated to the development of tools
for data analysis for the purposes of clinical research, surveillance, and effective health care management. Self-service
ad hoc querying of clinical data is one desirable type of functionality. Since most of the data are currently stored in
relational or similar form, ad hoc querying is problematic as it requires specialised technical skills and the knowledge
of particular data schemas.
Results: A possible solution is semantic querying where the user formulates queries in terms of domain ontologies
that are much easier to navigate and comprehend than data schemas. In this article, we are exploring the possibility of
using SADI Semantic Web services for semantic querying of clinical data. We have developed a prototype of a semantic
querying infrastructure for the surveillance of, and research on, hospital-acquired infections.
Conclusions: Our results suggest that SADI can support ad-hoc, self-service, semantic queries of relational data in a
Clinical Intelligence context. The use of SADI compares favourably with approaches based on declarative semantic
mappings from data schemas to ontologies, such as query rewriting and RDFizing by materialisation, because it can
easily cope with situations when (i) some computation is required to turn relational data into RDF or OWL, e.g., to
implement temporal reasoning, or (ii) integration with external data sources is necessary.
Background
Clinical intelligence and ad hoc querying of relational data
Clinical Intelligence (CI) is essentially Business Intelli-
gence applied to clinical data, i.e., it is a business vertical
and a research and engineering field aimed at the devel-
opment of methods and tools for deriving insights from
clinical data, required for research, surveillance and ratio-
nal health caremanagement (see, e.g., [1-5] to get a flavour
of different directions of CI work). A typical example of
CI is the use of patient records for selecting cohorts to
be used in clinical trials of drugs and other treatments.
Researchers could use CI tools on massive amounts of
data to facilitate the discovery of knowledge that can
improve existing treatment methods or help to define new
*Correspondence: alexandre.riazanov@gmail.com
1Department of Computer Science and Applied Statistics,University of New
Brunswick,Saint John,NB,Canada
Full list of author information is available at the end of the article
treatments. Health care management can use CI to objec-
tively evaluate the performance of hospital units or sepa-
rate professionals and allocate resources more efficiently.
The work of the Roundtable on Value and Science-Driven
Health Care of the US Institute of Medicine, and in par-
ticular their work on the Learning Healthcare System [6]
provides evidence of the need for such work.
One of the most useful modes of using clinical data is
ad hoc querying: in many scenarios, such as clinical trial
cohort selection, its very difficult to predict the kind of
queries than need to be answered and preprogram them
(see, e.g., eligibility criteria for various trials from [7]).
Since the data is usually stored in relational or similar
form, we can identify ad hoc querying of relational data as
an important technical problem within CI.
Semantic querying: problem and existing approaches
To be economical and therefore accessible, ad hoc query-
ing has to be self-service, so that non-technical users 
© 2013 Riazanov et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 2 of 19
http://www.jbiomedsem.com/content/4/1/9
clinical researchers, surveillance practitioners, health care
managers, etc.  can query clinical data directly, with-
out help from database programmers.With the traditional
database methods, this type of querying is problematic
because writing correct queries requires good under-
standing of technical details of data schemas and the
knowledge of query languages like SQL. Non-technical
users almost never have such skills.
A viable solution to the problem, as demostrated, e.g.,
by the case study [4] conducted in Cleveland Clinic, is
semantic querying (SQ) based on automatic application
of domain knowledge written in the form of ontologi-
cal axioms and, possibly, rules. Existing approaches to
SQ (see, e.g., [8-11]) typically allow database program-
mers to define mappings between data schemas and
domain knowledge bases, in the form of logical axioms
or similar declarative constructs. The axioms map con-
crete data into virtual models usually based on RDF
and OWL, so that the databases can be queried as RDF
graphs or OWLABoxes. End users formulate their queries
in the terminology of their domain, without any knowl-
edge of how the underlying data is structured. Seman-
tic querying systems then use the semantic mappings
to translate the semantic queries into queries directly
executable on the data, or to translate the data itself
to RDF.
Note that our assumption that semantic querying may
be self-service is currently just a vision that may or may
not be realised in the future, as commercial production-
quality implementations are very scarce yet (a notable
exception is the Cyc Analytic Environment by Cycorp
Inc, used in [4]). As several other research projects, e.g.,
[4,8,12], our work aims to help to realise this vision.
The most basic form of semantic querying is query-
ing semantic data, e.g., RDF, with languages like SPARQL,
and our work is focused on this core problem. However,
SPARQL querying is, by itself, not sufficient to fully realise
the vision of self-service querying because it is difficult to
expect that many non-technical users will be able to write
SPARQL queries. More friendly graphical or keyword-
based query interfaces, as in [4,12], have to be used on
top of SPARQL. However, this problem is outside the
scope of this article. Likewise, we do not discuss technical
problems that have to be solved before semantic query-
ing software can be deployed in clinical settings, such as
security or profiled data access.
When declarative mappings are not enough
Hypothetically, when the database design and the corre-
sponding formalised domain terminologies reasonably fit
together, the use of declarative semantic mappings may be
both sufficient for many types of queries and cost effec-
tive, because declarative mappings are relatively easy to
write, examine and edit. In most realistic clinical settings,
however, declarative mappings cannot cope with all user
requirements, specifically because certain things cannot
be done declaratively and require programming.
One example of such a requirement, which is also highly
relevant to our work, is the necessity of temporal reason-
ing. Many interesting queries to clinical data impose tem-
poral constraints specifying how certain events, activities,
or procedures of interest are temporally situated or tem-
porally related to each other. For example, the user may
be interested in retrieving only medical tests performed
prior to some diagnoses of interest, within a limited time
period.
Query engines processing such queries need to invoke
some code doing essentially temporal arithmetics, i.e.,
comparison of concrete temporal values, or only slightly
more complex temporal reasoning. This cannot be easily
done by just extending the virtual RDF graphs represent-
ing the data if only declarative semantic mapping means
are used  some computation has to happen dynamically
during the query processing time.
We would like to emphasise that temporal reasoning
is only one, although very important, example of a chal-
lenge for declarative mappings. In general, all data in the
virtual RDF graph generated by a declarative mapping
is constructed by simple transformations of data present
in the database. Very often this data is not enough. A
typical example is when some queries may require Body
Mass Index (BMI) of patients, which are not necessar-
ily directly stored in the database, but can be computed
on the fly from patients weight and height, or various
cardiovascular risk scores that can be computed on vari-
ous patient attributes. Another typical problem related to
Clinical Intelligence is the large amount of useful clinical
information stored in free text form in various abstracts
and reports. This kind of information can only be accessed
via specialised text mining algorithms. However, in this
article we focus our attention on the temporal reasoning
problem as it seems to be required in a larger number of
use cases and also demonstrated the capabilities of SADI
well.
Another significant challenge for declarative approaches
is integrating relational data with data from heteroge-
neous non-relational data sources using representations
ranging from various ad hoc text file-based formats,
popular with public biomedical database providers,
to HTML content that has to be scraped. Querying
such sources is inherently algorithmic and is beyond
reach of current declarative mapping languages and
implementations.
Note also that the axiomatic semantic mappings for
real-life databases are often very complex, given that most
non-trivial concepts expressed in the semantic schema
need to be deduced or inferred from many data elements.
This adds difficulty to their practical use.
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 3 of 19
http://www.jbiomedsem.com/content/4/1/9
SADI and SHARE
The SADI framework [13] is a set of conventions for
creating HTTP-based Semantic Web services that can
be automatically discovered and orchestrated. SADI ser-
vices consume RDF documents as input and produce RDF
documents as output, which solves the syntactic interop-
erability problem as all services speak one language. This
is also convenient for client programs that can leverage
existing APIs for RDF to represent the data onwhich SADI
services operate.
In brief, SADI services operate by attaching new proper-
ties to input URIs described in the input RDF document,
and the most important feature of SADI is that these
properties are fixed for each service. A declaration of
these predicates, available online, constitutes a seman-
tic description of the service. For example, if a service is
declared with the predicate is_performed_for described in
an ontology as a predicate linking diagnoses to patients,
the client software knows that it can call the service to
retrieve the patient for whom a given diagnosis was made.
The SADI framework uses the OWL class syntax to
specify conditions on the input nodes a service con-
sumes, and declare the predicates the service attaches.
Such declarations of inputs and outputs of services enable
completely automatic discovery and composition of SADI
services (see, e.g., [13]). Practically, it means that one can
have a query engine for SPARQL or a similar language
that can answer queries by automatically discovering nec-
essary SADI services and calling them. SHARE [14] is a
proof-of-concept implementation of this approach. From
the user point of view, it is a SPARQL engine that com-
putes queries by picking and calling suitable SADI services
from some registry. In a typical scenario, the user first
looks up predicates he needs for his query, in the list of
predicates declared as provided by SADI services in a reg-
istry, and also related classes and property predicates in
the referenced ontologies. Then he uses the available con-
cepts to form a regular SPARQL query, and sends it to a
SHARE endpoint for execution.
For more technical details on SADI and SHARE,
the reader is referred to [13], and Bioinformatics- and
Chemoinformatics-related case studies can be found in
[15-18].
Article outline
These limitations of the declarative mapping-based
approaches motivate us to look for other possibilities. In
this article we explore an approach to semantic querying
based on the use of Semantic Web services that can be
automatically discovered, composed and invoked. More
specifically, we are looking at the SADI [13] services as
a possible medium for semantic querying of clinical data
in relational form. We describe a prototype based on the
SADI technology, we created to experiment with semantic
querying, and report the results of a case study performed
for several scenarios related to the surveillance of, and
research on hospital-acquired infections, using an extract
from a hospital datawarehouse.
Methods
The hydra query engine
We used SHARE in our early experiments on HAI-
related semantic querying for Clinical Intelligence pur-
poses, reported in [19]. At some point we reached the
limits of SHARE performance  some of our queries were
taking hours to complete, although the general amount of
processed RDF data and the numbers of SADI service calls
were moderate. Another feature of SHARE that hinders
experimentation is the fact that the user has to wait until
absolutely all answers to the query are computed before
he can see any answers at all. Together with the low query
execution speed, this makes query debugging very time
consuming. For these reasons, we have opted to use both
SHARE and Hydra [20] in our experiments.
Hydra mirrors SHARE, in terms of functionality, in
that it executes queries over collections of SADI services.
One notable extension is the ability of Hydra to draw
data from multiple arbitrary SPARQL endpoints, giving
end users the ability to use existing online data as input
data. Unlike SHARE, which is Open Source, Hydra is a
commercial prototype, based on a complex but scalable
architecture and is being developed by IPSNP Computing
Inc to address the lack of commercial quality clients for
SADI. Despite being an early proof-of-architecture proto-
type, the system shows acceptable performance even on
relatively complex queries in our use cases, and sets of ser-
vices. A notable feature is that Hydra often returns first
answers in quasi-real time, in minutes or even seconds,
which facilitates experimentation with different forms of
queries. In contrast SHARE returns answers in batch form
when all possible services have responded. This can lead
to lengthy delays or query timeouts. Two of the four exper-
iments reported in this article were done exclusively with
Hydra.
Semantic querying of relational data with SADI
The SADI framework primarily facilitates the feder-
ated querying of multiple heterogeneous, distributed and
autonomous data sources, such as online databases and
algorithmic resources, as illustrated by several case stud-
ies [15-18]. The work presented here, however, explores a
different avenue  using SADI services as a medium for
semantic querying of single relational databases. If suc-
cessful, our effort will add a new approach to the pool of
existing practical methods for semantic querying of RDB,
at least in the Clinical Intelligence context.
The general approach is to write a set of SADI ser-
vices over a relational database that users would like to
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 4 of 19
http://www.jbiomedsem.com/content/4/1/9
query semantically, and leverage the services collectively
to answer ad hoc SPARQL queries, drawing the necessary
data from the database. In this article we report on pre-
liminary progress in implementing this idea on a clinical
datawarehouse.
Experiment settings: querying The Ottawa Hospital DW
We are testing our approach in a Clinical Intelligence
scenario dedicated to the surveillance for, and research
on Hospital-Acquired Infections (HAI). To this end,
we are prototyping a SADI-based infrastructure for
semantic querying of a relational database used by The
Ottawa Hospital (TOH) and containing an extract from
the large TOH datawarehouse accumulating data from
the most important IT systems of the hospital (see,
e.g., [21,22]). Our infrastructure consists of an ontology
defining concepts suitable for reasoning about Hospital-
Acquired Infections, and a number of SADI services
drawing data from the DB, as well as several gen-
eral purpose services dealing with information about
drugs, diseases and infectious agents. In our experi-
ments, we use SHARE and Hydra to run test queries
over this network of SADI services. By using queries
that HAI surveillance specialists or researchers may be
interested in, we are trying to demonstrate the via-
bility of our approach for real-life Clinical Intelligence
tasks.
Application: hospital-acquired infections
Infections acquired by patients in health care institutions
are a serious practical problem as they result in thousands
of deaths and hundreds of millions of dollars in addi-
tional expenses each year in Canada alone: at least one in
twenty patients admitted to a Canadian hospital acquires
an infection. Surveillance for, and research on HAI are
essential to develop prevention methods, evaluate them
and control their deployment. Currently, HAI surveillance
based on clinical data is mostly manual and, as a conse-
quence, is limited in scope and costly. Therefore, the use of
adequate Clinical Intelligence tools promises to increase
the effectiveness of HAI surveillance and research efforts
and bring the cost down, which, in particular, justifies our
effort to enable agile querying of HAI-related data with
SADI.
Data
Our experimental efforts are focused on querying the
TOH datawarehouse extract containing potentially HAI-
related data from several clinical data sources, such
as microbiology and clinical chemistry test results,
information on drug prescriptions and surgical proce-
dures, operating room information, and patient demo-
graphics and movement information. Our datawarehouse
extract, referred to as DW throughout the rest of the
article, contains information for 715 cardiac surgery
patients, 6132 encounters, 12275 diagnoses and 6029
procedures, and has been previously used in a Clini-
cal Intelligence effort [22] dedicated to cardiac surgical
site infections. The data we used were selected from a
cohort of all clean cardiac surgeries performed at the
University of Ottawa Heart Institute in 2004-2007. In
terms of design, the datawarehouse is a single Relational
DB with tables representing patients, encounters, proce-
dures and drug prescriptions, etc (the schema is available
at [23]).
Target query types
From the perspective of HAI surveillance and research, we
would like to be able to use the available data to answer
HAI case identification and enumeration requests, ques-
tions aimed at identification or evaluation of HAI risk
factors or causative factors, and questions aimed at identi-
fication or evaluation of diagnostic factors. Our target set
of questions at this stage is as follows:
(1) Which patients diagnosed with surgical site
infections (SSI) had older age as a risk factor?
Questions of this type allow surveillance practitioners
to estimate the prevalence of particular risk factors.
(2) Howmany patients were infected with
methicillin-resistant Staphylococcus aureus
(MRSA) in Quarter 1 of 2007? Questions of this
type allow health care managers to evaluate anti-HAI
measures.
(3) What patients received a diagnosis of sepsis
within 30 days of being diagnosed with SSI?
Researchers may use such questions to develop
methods for predicting complications of HAI.
(4) Howmany SSI were diagnosed during quarter 1
of 2011 for patients who had undergone heart
bypass surgery within 30 days prior to the SSI
diagnoses? Questions of this type allow to estimate
the incidence of HAI for particular categories of
patients.
(5) Howmany Catheter Associated Urinary Tract
Infection (CAUTI) incidents in January 2011
were diagnosed within 30 days following
procedures involving Foley catheters?
Surveillance personnel can ask such questions to
evaluate the risks associated with new types of
equipment.
(6) Which patients were diagnosed with SSI while
they were taking corticosteroids systemically?
Questions of this type make it possible to identify
patient risk groups based on the types of medications
they take.
(7) Howmany cases of hypoxia (decreased oxygen
supply) were not accompanied by
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 5 of 19
http://www.jbiomedsem.com/content/4/1/9
hypoalbuminemia (reduced serum albumin
concentration) and SSI diagnoses? Researchers
can ask questions of this type allow to discover
interactions between various risk factors.
(8) Howmany CAUTI diagnoses were made within 7
days from the time a test indicated the patients
serum glucose level was between 7.0 and 7.5
mmol/L? Questions of this type allow to identify
dose-response relationships between numerical
findings of medical tests and the risk of HAI.
(9) Howmany diabetic patients were diagnosed with
SSI? In general, such questions are aimed at
identifying comorbidity factors for HAI.
(10) Elevation of which proteins was indicated by
blood test results within 7 days prior to the
patients being diagnosed with HAI? Questions of
this type help to discover new diagnostic factors for
HAI.
(11) Which patients using drugs with
anti-inflammatory side effects, had an operation
and were diagnosed post-operatic abscess
formation not detected preciously by a
tomography? Questions of this type allow to
identify weaknesses of existing diagnostic methods.
These sample questions illustrate the ad hoc nature of
the queries we would like to compute over our dataware-
house and, in general, over similar sources of clini-
cal data. Although such questions can be implemented
with SQL queries, this approach is uneconomical as it
requires involvement of database programmers. Ideally,
we would like to enable non-technical users, such as
surveillance practitioners and researchers, to write ad
hoc queries by themselves. The solution we are advocat-
ing is semantic querying where users formulate queries
in terms of domain ontologies that are much easier
to navigate and comprehend than relational database
schemas.
Other kinds of requirements associated with our use
cases are either trivial, e.g., its clear that extreme querying
speed is not necessary, or can only be identified through
experimental deployment of some software prototypes,
for which our research is still in too early a stage.
Ethical consent
The work reported in this paper was carried out, in part,
using data from a secondary source previously made avail-
able, citation [22], and no additional patient consent or
ethical approval was necessary.
The HAIKU project was reviewed and approved by the
Institutional Review Boards at the OHRI and at McGill
University. Since we are using individual patient records
(even if personally identifying information is removed),
IRB approval was required and was obtained.
Results
Outline of the infrastructure
HAI Ontology
In general, semantic querying implies the use of for-
malised terminologies as sources of query primitives.
For this reason, we are developing the HAI Ontology
(HAIO) [24,25] that defines a number of HAI-specific
concepts, such as Surgical_site_infection and Hospital-
acquired_tuberculosis, adds a small hierarchy of general
health care-related concepts, such as Disease and Medi-
cal_test, and aligns the resulting ontology with a number
of third party ontologies, both general and specialised.
The ontology design is fairly straightforward. Its core
is a hierarchy of classes formalising concepts from the
HAI domain and a flat hierarchy of object properties rep-
resenting relations of interest between entities, such as
goes_through to link patients to the operative procedures
they undergo, and is_performed_for to link diagnoses to
the patients they were made for.
We are using the Semanticscience Integrated Ontol-
ogy (SIO) [26] as the upper ontology to provide access
to our classes and properties via more general classes
and properties. Currently, HAIO is undergoing the align-
ment to SIO and eventually most of our primitives will
be mapped to appropriate places in the SIO hierarchy.
We are also removing datatype properties to follow the
SIO convention that the only datatype property is has
value (SIO_000300) and all data-valued attributes are
represented as individuals.
Apart from SIO, we use several other separate ontolo-
gies most of which are large biomedical nomenclatures.
For example, to be able to use the Canadian version of the
ICD-10 [27] nomenclature of diseases, we have created
an OWL version [28] of the ICD-10 hierarchy. Also, to
be able to reason about drugs, we created OWL versions
[28] of the Anatomical Therapeutic Chemical (ATC) clas-
sification that provides a hierarchy of active ingredients,
and of the Canadian Drug Identification Number (DIN)
nomenclature. We also developed the Extra Simple Time
Ontology (ESTO) [29], to be able to specify temporal coor-
dinates of activities and events and compare them, which
will be discussed in more detail in SectionMain difficulty:
temporal reasoning.
To give the reader some flavour of the kind of mod-
elling that HAIO supports, we provide an RDF example
in Figure 1. This RDF graph is a description of the patient
identified as :patient. The node :diagnosis and the sub-
graph around it represents a diagnosis linked to :patient
with the predicate haio:is_performed_for. The diagnosed
disease is identified as :incident. Since the disease is a sur-
gical site infection, it must be a consequence of an oper-
ative procedure. In our example, the procedure :surgery
is an instance of haio:Coronary_artery_bypass_graft. The
rest of the example describes a blood culture test :test
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 6 of 19
http://www.jbiomedsem.com/content/4/1/9
Figure 1 Figure 1 HAI Ontology modelling fragment. This RDF graph illustrates how HAI Ontology concepts can be used to semantically model
data from the datawarehouse in particular, and from the HAI domain in general.
that revealed the presence of Serratia proteamaculans in
the patients blood, and a prescription of a drug (DIN
00888222) to the patient.
Mapping the DW schema to HAIO
A key ingredient of any implementation of semantic
querying over a relational database is the semantic map-
ping of the database schema to the corresponding ontolo-
gies, specifying, in one form or another, how the database
contents are modelled semantically, e.g., as a virtual RDF
graph. We are mapping the relevant parts of the Ottawa
Hospital DW schema [23] to the HAI Ontology. This is
done by specifying how classes and properties fromHAIO
are populated, based on the contents of relevant tables in
the DW. In all cases we have encountered so far, the pop-
ulation of the RDF graph representing an ABox for HAIO,
can be done by running one or more SQL queries over the
DW and converting the results of the SQL queries into
RDF by creating necessary resource URIs and data literals,
and asserting RDF triples on them.We illustrate this by the
following examples.
Example 1. populating haio:Patient. Populating the
class haio:Patient with instances amounts to identifying all
patients in our DW and assigning URIs to them. Note that
the patient URI generation from the DW keys for patients
must be invertible, so that SADI services that accept
patient URIs as input are able to identify the relevant DW
records. This consideration applies to all URI generation
schemes in our experiments.
The datawarehouse schema has table Npatient con-
taining basic information about all patients, such as
names, dates of birth, medical record numbers, etc. The
tables primary key consists of one integer-valued attribute
patWID which is used throughout the datawarehouse to
identify patients. We simply map these integer patient IDs
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 7 of 19
http://www.jbiomedsem.com/content/4/1/9
to URIs as follows. Suppose, the patWID value is 123. The
correspondingURI ishaiku:Patient_by_patWID?-
wid=123, where the namespace haiku corresponds to
http://cbakerlab.unbsj.ca:8080/haiku/.
In general, we simply attach the patWID as a query
parameter in the URI, so that the URI can be viewed as a
function of the patWID. The function is easily invertible
since the patWID value can be extracted as the value of the
wid parameter of a given patient URI.We use this primary
key-to-URI mapping scheme for most entities that have to
be identified with URIs in the virtual HAIO ABox, such as
diagnoses, diseases, medical tests, drug prescriptions, etc.
Example 2. populating haio:is_performed_for and
haio:identifies. Our goal now is to specify how patients
are linked to their diagnoses and disease incidents, based
on the DW contents. The DW has a table NhrDiagno-
sis with information about diagnoses, but its records do
not directly specify the patients. However, NhrDiagnosis
uses foreign key hdgHraEncWID to table Nencounter rep-
resenting encounters, where the attribute encPatWID is
the foreign key to Npatient. The attribute NhrDiagno-
sis.hdgCd also specifies the ICD-10 [27] code of the disease.
The following SQL query can be used to enumerate triples
containing patient and diagnosis IDs and disease codes:
SELECT Nencounter.encPatWID as patientID,
NhrDiagnosis.hdgWID as diagnosisID,
NhrDiagnosis.hdgCd as diseaseCode
FROM Nencounter, NhrDiagnosis
WHERE Nencounter.encWID =
NhrDiagnosis.hdgHraEncWID
The corresponding part of the virtual RDF graph can be
formed by asserting
Diagnosis_by_hdgWID(diagnosisID)
a haio:Diagnosis;
haio:is_performed_for Patient_by_patWID
(patientID);
haio:identifies Disease_by_diagnosis
(diagnosisID).
Disease_by_diagnosis(diagnosisID)
a haio:Disease ;
a disease_class_by_ICD10(diseaseCode) .
for each triple (patientID,diagnosisID,diseaseCode). Note
that in this pseudo-RDF Diagnosis_by_hdgWID,
Patient_by_patWID, and Disease_by_diagnosis are func-
tions constructing URIs from numeric and string-
valued parameters similarly to how it was done for
patient URIs in the previous example. The function
disease_class_by_ICD10 represents a slightly differ-
ent case as it constructs disease class URIs according
to our OWL version of the ICD-10 nomenclature
(Canadian edition). For example, diseaseCode=A40
(Streptococcal septicaemia) is mapped to the URI
ont:icd10ca.owl#A40, where the namespace ont is
defined as http://cbakerlab.unbsj.ca:8080/
ontologies/.
It is important to clarify that, although we have pre-
sented an SQL query, we do not actually execute it and
construct explicit RDF from the results. The SQL and
the pseudo-RDF are only written to document our under-
standing of how the data is mapped. Our SADI services
implementing the mapping use slightly different SQL
queries because a service always has some piece of data-
e.g., a patient URI or diagnosis URI  given to them
as input.
In fact, we dont even literally write SQL to docu-
ment our semantic mapping. We use PSOA RuleML
[30]  an expressive rule language  to express the
semantic mapping. For example, the population of
haio:is_performed_for and haio:identifies is captured by
the following PSOA RuleML rule:
1 And (
2 ?diagnosis # haio:Diagnosis
3 haio:is_performed_for(?diagnosis
?patient)
4 haio:identifies(?diagnosis ?disease)
5 ?disease # haio:Disease
6 ?disease # ?diseaseClass )
7 :-
8 And (
9 ?encounterRow # dwt:Nencounter
(dwa:encWID->?encounterID
10 dwa:encPatWID->?patientID)
11 ?diagnosisRow # dwt:NhrDiagnosis
(dwa:hdgWID->?diagnosisID
12 dwa:hdgHraEncWID->?encounterID
13 dwa:hdgCd->?diseaseCode)
14 ?patient = External
(modf:Patient_by_patWID(?patientID))
15 ?diagnosis = External(modf:Diagnosis_
by_hdgWID(?diagnosisID))
16 ?diseaseClass = External(modf:disease_
class_by_ICD10(?diseaseCode))
17 ?incident = External(modf:Disease_
by_diagnosis(?diagnosisID)) )
Lines 913 essentially represent the SQL query given
above, and lines 1-6 and 1417 capture the meaning of
the pseudo-RDF. We provide the rule only as an illustra-
tion, so readers interested in details of PSOA RuleML are
referred to [30].
We would like to avoid making a false impression that
semantic mapping of relational data schemas is simple.
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 8 of 19
http://www.jbiomedsem.com/content/4/1/9
Typical challenges are, e.g., associated with insufficiently
expressive ontologies and the general difficulty of seman-
tic modelling of data. Since this activity deserves a sepa-
rate investigation, in this article we restrict ourselves with
a relatively brief discussion of it.
SADI services
The core part of our semantic querying infrastructure will
be a set of SADI services providing several HAIO predi-
cates according to the semantic mapping discussed above,
by drawing the necessary data from the DW. The general
scheme for the execution of these services is as follows.
The code of a service (i) accepts some input RDF describ-
ing certain entity identified with a URI, (ii) decomposes
the description into a set of values and embeds them
as parameters in one or more predefined service-specific
SQL queries, (iii) runs the SQL queries over the DW, (iv)
converts the result values to a new RDF description of the
input entity and returns the description as its output.
To illustrate this scheme, we anatomise one of the
already written services  getDiagnosisByPatient  that
enumerates all known diagnoses for a specified patient.
It implements a part of the semantic mapping defined in
Example 2 from the previous section. The input class of
the service is just haio:Patient and any URI qualified as an
instance of haio:Patient is legitimate input to the service.
The service only processes URIs of the form described in
Example 1, in a meaningful way. As soon as it receives
such a URI, it extracts the integer DW key for the patient
from the URI. The service then executes the SQL query
SELECT NhrDiagnosis.hdgWID as diagnosisID
FROM Nencounter, NhrDiagnosis
WHERE Nencounter.encPatWID = ?
AND Nencounter.encWID =
NhrDiagnosis.hdgHraEncWID
where the parameter ? is replaced with the patient key
value. Then, for each value of diagnosisID in the result
relation, the service constructs the corresponding diagno-
sis URI (as Diagnosis_by_hdgWID(diagnosisID)), qualifies
it as an instance of haio:Diagnosis and attaches it as the
value of the property haio:patient_has_diagnosis to the
patient URI. The output class of the service, in Protégé
syntax, is (patient_has_diagnosis some Diagnosis). Note
that, in HAIO, the property haio:patient_has_diagnosis is
defined as the inverse for haio:is_performed_for, so the
service indeed implements a part of the mapping from
Example 2.
To illustrate the composition of SADI services, con-
sider another service getDiagnosisInfo which annotates
a specified diagnosis URI with the disease incident
diagnosed, through the predicate haio:identifies, and
with the time when the diagnosis was made, through
haio:situation_has_time. So the input class of getDiagno-
sisInfo is haio:Diagnosis and the output class is (identi-
fies some Disease) and (situation_has_time some Fully-
DefinedTimeInterval). Because a part of the output from
getDiagnosisByPatient is a URI explicitly qualified as an
instance of haio:Diagnosis, this URI can be directly sub-
mitted as input to getDiagnosisInfo. Practically, if some
client program has an instance :patient of haio:Patient
and wants to know disease incidents diagnosed for the
patient, it can first call getDiagnosisByPatient to retrieve
the relevant diagnoses, and then call getDiagnosisInfo on
these diagnosis URIs.
Overall, we currently have fourteen finished services
similar to getDiagnosisByPatient and getDiagnosisInfo,
and this number is likely to double as our infrastructure
matures. In terms of the scope, the services compute,
or will compute, instances of HAIO classes for patients,
diagnoses, procedures, drug prescriptions, medical tests,
etc, and provide information about such entities, such as
patient names, time of procedures, links between patients
and diagnoses and medical tests, etc. In terms of the
complexity, most of the services we already have are sim-
ilar or slightly more complex than getDiagnosisByPatient:
typically, the service execution amounts to instantiation
of a few SQL query templates and a fairly straightfor-
ward transformation of the query results into output RDF.
This type of programming can be easily done by moder-
ately skilled DB administrators or programmers. Finally,
in addition to SADI services working on the DW and
created specifically for HAIKU  our project on ontology-
supported HAI surveillance, we are using a number of
external general purpose SADI services, dealing mostly
with drug and disease information, and a set of services for
temporal reasoning, that will be discussed in the Temporal
Reasoning section.
Querying with Hydra and SHARE
The end users of our infrastructure will access the SADI
services and, through the services, the underlying data in
the DW, by querying our network of SADI services in
the SPARQL language. For our current experiments, we
have been using Hydra and, to some extent, SHARE  two
prototypes that implement such functionality.
To enable service discovery, we register all relevant ser-
vices in a private registry, and the query engines are
configured to query this registry for services provid-
ing particular predicates necessary to resolve SPARQL
queries being executed. Users interact with the DW only
via the semantic front end. They need not know any-
thing about the DW schema or write SQL queries, and
have no direct access to the relational data. To form
queries, users look up concepts of interest in HAIO
and accompanying ontologies, including the ontologies
referred to from the descriptions of non HAIKU-specific
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 9 of 19
http://www.jbiomedsem.com/content/4/1/9
services in the registry. For browsing convenience, we
have created an umbrella ontology [31] that simply
imports all relevant ontologies, can be browsed with the
help of standard ontology editors such as Protégé, and
works, effectively, as a kind of semantic schema for the
DW. The user finds relevant classes and properties in
the ontology, and uses their URIs to manually form a
SPARQL query. Note that this is only a temporary solu-
tion to facilitate early stage experimentation  in the
future we will look for more productive ways to create
queries.
Finally, we would like to mention that, in con-
trast with SPARQL querying of static RDF triplestores,
Hydra, as well as SHARE, can only start calling ser-
vices if it already has some seed data. This seed
data comes from RDF files specified with their URLs
in FROM sections of queries. Examples of this will
be given in the evaluation section. Hydra can also get
the seed data from one or more specified SPARQL
endpoints, but we did not need this feature in our
experiments.
Major difficulty: temporal reasoning
Our very first efforts to analyse the problem of seman-
tic querying for the purposes of HAI surveillance and
research revealed that most questions we would like to
answer based on the DW contents, have temporal com-
ponents. A simple example is question (2) from the
section introducing our HAI application: Howmany inci-
dents of diseases caused by meticillin-resistant Staphy-
lococcus aureus bacteria, were diagnosed in Quarter 1
of 2007?. The temporal constraint embedded in this
question requires the disease incidents to be diagnosed
between Jan 1 and Mar 31, 2007. A slightly more complex
example is question (6) Which patients were diagnosed
with SSI while they were taking corticosteroids systemi-
cally?. In correct answers, the times of surgical site infec-
tion diagnosesmust fall within some administered periods
for corticosteroids for the same patients. To accommo-
date this kind of requirements, we have to solve two
problems.
First, we need a way to define temporal entities, such
as time intervals and instants, and to express relations
between them, i.e., we need a time ontology. We also need
predicates in HAIO to link activity- and event-like enti-
ties, such as diagnoses and procedures, to their temporal
coordinates. These features should enable our SADI ser-
vice to return temporal information for various entities,
as well as enable users to capture temporal constraints in
SPARQL.
Second, we need to be able to resolve temporal con-
straints in queries, which requires temporal reasoning at
least in the form of temporal arithmetics on the temporal
entities that can be defined.
Time ontology
As a solution to the first problem, we have created the
Extra Simple Time Ontology (ESTO) [29]. We can define
instants by specifying their XSD dateTime value:
:noon a esto:Instant;
esto:hasValue
"2011-10-02T12:00:00"^^xsd:dateTime
We can define proper time intervals by specifying their
ends:
:january2011 a esto:TimeInterval;
esto:timeRegionHasStart:january2011_start;
esto:timeRegionHasEnd:january2011_end .
:january2011_start a esto:Instant;
esto:hasValue
"2011-01-01T00:00:00"^^xsd:dateTime .
:january2011_end a esto:Instant;
esto:hasValue
"2011-01-31T24:00:00"^^xsd:dateTime .
We also have two dedicated URIs denoting infinite
points in the past and future. Time intervals in our model
are defined by specifying their ends as instants. Infinite
time intervals can be specified by using infinities as their
ends. Duration values can be associated with intervals and
instants are treated as intervals of zero duration.
The core of ESTO is a set of properties for comparison
of time intervals. The main properties are just the Allens
temporal predicates [32]. For example, we can check in a
query if a time interval is subsumed by another interval by
using Allens predicate during:
?DiagnosisTime esto:during
?AdministeredPeriod .
Temporal reasoning
Several of our SADI services supply temporal infor-
mation on various entities. For example, service
getDiagnosisInfo provides, among others, predicate
haio:situation_has_time specifying the instant when the
corresponding diagnosis was made. Likewise, service
getPharmacyServiceInfo attaches a time interval as an
instance of haio:Administered_period via the predicate
haio:has_specification. We can have a query with the
following fragment:
?Diagnosis
haio:situation_has_time ?DiagnosisTime.
?PharmacyService
haio:has_specification?
AdministeredPeriod.
?AdministeredPeriod a haio:
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 10 of 19
http://www.jbiomedsem.com/content/4/1/9
Administered_period .
?DiagnosisTime esto:during?
AdministeredPeriod.
Assuming that Hydra or SHARE instantiates the vari-
ables ?DiagnosisTime and ?AdministeredPeriod with some
time interval URIs, e.g., by calling getDiagnosisInfo and
getPharmacyserviceInfo, it only needs to test the predicate
esto:during on these values. However, there is no SADI
specification-compliant way to do this because SADI
framework currently only supports services that attach
predicates and supports neither services that would test
specific predicates on specified arguments, nor services
that would accept more than one input parameter and
establish some facts about them. These SADI deficiencies
motivated us to look for a workaround.
One obvious possibility is not to use high-level predi-
cates like haio:during. We could replace our query frag-
ment with another fragment that invokes the built-in
arithmetics on XSD dateTime values via the SPARQL
FILTER construct:
?Diagnosis
haio:situation_has_time ?DiagnosisTime .
?PharmacyService
haio:has_specification ?
AdministeredPeriod .
?AdministeredPeriod a haio:
Administered_period .
?DiagnosisTime
esto:hasValue?DiagnosisTimeValue .
?AdministeredPeriod
esto:timeRegionHasStart
[esto:hasValue?AdministeredPeriodStart].
?AdministeredPeriod
esto:timeRegionHasEnd
[esto:hasValue ?AdministeredPeriodStart].
FILTER(?AdministeredPeriodStart
<= ?DiagnosisTimeValue && ?
DiagnosisTimeValue
<= ?AdministeredPeriodEnd)
The main problem with this solution is that it requires
significant extra work and too much technical knowledge
from the end user, which is exactly what we are trying to
avoid by using semantic querying. Recall that our ultimate
goal is to facilitate self-service querying. We are assuming
that using high-level temporal predicates like esto:during
with obvious semantics would not present any extra diffi-
culty for non-technical users, whereas the FILTER-based
solution would require them to remember the details of
the time period representation and make the query com-
position more error prone. So, we chose to try different
solutions that avoid complicating queries by leveraging
one of the main features of SADI  the ability of SADI
services to do computation behind semantic interfaces.
Before we describe the solutions, wewould like to briefly
discuss possibilities of implementing temporal reasoning
more complex than temporal arithmetics. In one of the
first versions of our infrastructure we experimented with
SADI services that did not require the input intervals to be
fully defined, i.e., specified with their ends given as specific
date-time values. These services could also work on inter-
vals that are only partially defined by by specifying how
they relate to other intervals, by implementing, in a lim-
ited manner, the axiomatic semantics of Allens temporal
predicates [32]. We skip the discussion of this possibil-
ity primarily because temporal arithmetics is sufficient for
our goal of illustrating SADI capabilities, but also because
none of our use cases so far required such reasoning.
Query ID-based solution
Our first attempt to embed temporal reasoning into the
DW querying resulted in the creation of a temporal rea-
soner in the form of a set of SADI services, providing
Allens predicates from ESTO and working on a shared
cache of time intervals.Wewill explain this by anatomising
two of the services  getTimeIntervalsDuringTimeInterval
and getTimeIntervalsContainingTimeInterval  providing,
respectively, Allens predicates contains and during, that
are inverse to each other. Suppose SHARE or Hydra
instantiates the variables ?DiagnosisTime and ?Adminis-
teredPeriod with :diag_time and :adm_period respectively,
and then decides to call getTimeIntervalsContaining-
TimeInterval on the description of :diag_time to identify
all intervals, known to the temporal reasoner, that can
be attached to :diag_time via esto:during. Its possible that
:adm_period is not yet known to the temporal reasoner,
i.e., it is not yet in the cache, and the call to getTimeInter-
valsContainingTimeInterval will not confirm
:diag_time esto:during :adm_period
which is required to resolve the temporal constraint
?DiagnosisTime esto:during ?
AdministeredPeriod .
in the query. However, the temporal reasoner will now
remember the interval :diag_time and, when the other
service getTimeIntervalsDuringTimeInterval is called on
:adm_period, it will attach esto:contains :diag_time to
:adm_period, which is sufficient to resolve the constraint
because esto:contains is inverse to esto:during.
Obviously, this approach cannot be practical unless
there is a way to segment the time interval cache in
the temporal reasoner into sub-caches corresponding
to different queries because otherwise intervals from
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 11 of 19
http://www.jbiomedsem.com/content/4/1/9
different queries will be returned by temporal services and
pollute the working memory of the query engine. SADI
does not support any notion of query IDs, so we had to
devise a workaround for this.
To this end, we created a small ontology [33] pro-
viding class cont:Context and property cont:hasContext,
where cont is an abbreviation for the ontology names-
pace. Instances of cont:Context can represent query IDs
and cont:hasContext links various resources to the IDs of
the relevant queries. We assume that all SADI services to
be used for querying over the HAI DW can accept inputs
with attached cont:hasContext and propagate the query
IDs to all URIs in the output. We also require users to
attach a unique query ID to all URIs mentioned in a query
and in seed data. These conventions guarantee that all
time intervals submitted to the temporal reasoner services
carry query IDs with them and the services use this IDs to
segment the cache.
Candidate list-based solution
Although the query ID-based approach solves the prob-
lem in principle, it suffers from two major drawbacks.
First, the requirement to propagate query IDs imposed on
services strongly hinders the use of external SADI services
in queries: the query engine can only use an external ser-
vice without query ID propagation if the data returned
by the service does not have to be submitted as input to
services requiring cont:hasContext. Second, the temporal
reasoning services based on comparing input time inter-
vals with absolutely all known time intervals, including
instances, tend to produce too large outputs. In one of
our experiments involving processing several thousands
of diagnoses from the DW, a batch call to getTimeIn-
tervalsDuringTimeInterval produced 750,000 RDF triples
which took several minutes to transmit from the service
to the query engine over the Internet. These problems
motivated us to look for a more efficient solution.
Hydra experimentally supports the following exten-
sion to the SADI framework. When it realises that
testing some predicate, say esto:contains, on two spe-
cific resources, say t1 and t2, could help to satisfy
the query being executed, it asserts an additional fact
t1 ext:isComparedToObject t2 in its working mem-
ory, where ext:isComparedToObject is a special predicate
introduced specifically for this purpose. This predicate
can be used to implement predicate-testing services as
follows. Suppose, we want a SADI service testing the pred-
icate esto:contains, i.e., somehow given two time interval
descriptions x and y, the service has to compare them and
if the condition x esto:contains y holds, report this.
With the help of ext:isComparedToObject, the pair of val-
ues can be submitted bymaking x themain input node and
adding x ext:isComparedToObject y to its description.
Our service, say getTimeIntervalsDuringTimeInterval,
having received x, looks for all resources yi attached to it
via ext:isComparedToObject and compares x to them by
doing temporal arithmetics, e.g., by extracting specific val-
ues for interval ends and comparing them with appropri-
ate Java methods. For all yj satisfying x esto:contains yj ,
the service adds the triple x esto:contains yj to the out-
put description of x. In other words, the client program
sends a list of candidates yi to the service by attaching
them via ext:isComparedToObject, and the service selects
the output property values from the provide list rather
than computes completely new values.
This output candidate list-based solution is free from
the drawbacks of the query ID-based solution discussed
above. First, only the services that are meant to be
predicate-testing services need to support the predicate
ext:isComparedToObject in the input. No other services
are affected and any external services can be used as
long as they implement compatible data modelling. Sec-
ond, there is no combinatorial explosion in the out-
puts caused by irrelevant comparisons of many temporal
entities to many temporal entities: services supporting
ext:isComparedToObject only perform comparisons that
are relevant to the query and directly requested by the
query engine. The cost of this elegance and corresponding
performance gain is on-compliance with the current SADI
specification: services based on ext:isComparedToObject
will be useless with clients not supporting this spe-
cial predicate. However, once this solution is tested in
more scenarios, the corresponding extension to the SADI
framework will be officially proposed.
Evaluation: pilot use cases
To empirically test the utility of our SADI-based approach
and infrastructure for semantic querying of The Ottawa
Hospital DW, we formulated four SPARQL queries imple-
menting different questions from the list in the Target
query types section above. Two of the queries were exe-
cuted with SHARE, as was reported earlier in [19], albeit
using service implementations modified to only return
a small part of the generated output data in order to
increase SHARE performance. The other two queries
were executed with Hydra with full versions of the ser-
vices. Since our research task is to demonstrate the prin-
ciple possibility of using SADI Semantic Web services for
semantic querying of clinical data, we do not pay much
attention to the query performance yet. We would only
like to briefly mention that the two queries executed with
SHARE took several hours and Hydra took between 20
minutes and 1 hour (on a commodity laptop) to compute
all answers of the two queries it was tested on. Although
the first answers are obtained by Hydra in minutes, this
kind of performance is still unlikely to be sufficient in
practice. However, note that Hydra is in its infancy and
its performance is expected to improve by several orders
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 12 of 19
http://www.jbiomedsem.com/content/4/1/9
of magnitude when it matures into a commercial quality
system.
Let us now discuss the implemented queries.
Use case 1
Our first query intended to implement Question (9) How
many diabetic patients were diagnosed with surgical site
infections (SSI)?, is as follows:
1 SELECT DISTINCT ?Patient ?Incident
?Surgery
2 FROM <http://cbakerlab.unbsj.ca:
8080/haiku/query1_as_context.rdf>
3 FROM <http://cbakerlab.unbsj.ca:
8080/haiku/operative_procedure.rdf>
4 WHERE {
5 haio:Operative_procedure haio:
procedure_type_has_instance ?Surgery .
6 # Select surgeries that cause SSI:
7 ?Surgery haio:has_consequence
[haio:disease_has_type haio:SSI] .
8 # Retrieve patients:
9 ?Surgery haio:is_done_on ?Patient .
10 # Retrieve diabetes diagnoses for
the patient:
11 ?Patient haio:patient_has_diagnosis ?
DiabetesDiagnosis .
12 ?DiabetesDiagnosis haio:identifies
13 [haio:disease_has_type haio:
Diabetes_mellitus] .
14 # Check that the surgery was performed
15 # after the diabetes diagnosis:
16 ?DiabetesDiagnosis haio:
situation_has_time ?DiagTime .
17 ?Surgery haio:situation_has_time ?
SurgeryTime .
18 ?SurgeryTime esto:after ?DiagTime .
19 # Check that the surgery was
performed within
20 # 1 week after the diabetes diagnosis:
21 ?DiagTime esto:meets ?IntervalBetween .
22 ?IntervalBetween esto:meets ?
SurgeryTime .
23 ?IntervalBetween esto:
timeIntervalHasDuration
[esto:hasValue ?Duration] .
24 FILTER(?Duration <= "P7D"^^xsd:
duration) }
Consider the FROM clauses first. File query1_as_
context.rdf contains a description of a query ID
URI as an instance of cont:Context and operative_
procedure.rdf attaches the query ID to the URI
haio:Operative_procedure to make it comply with the
conventions described in the Temporal Reasoning section.
This is not necessary when the query is executed with
Hydra and predicate-testing versions of the temporal
reasoning services.
Let us now discuss the ontological primitives that
form the query. In line 5, we use haio:Operative_
procedure to specify the class of all surgeries, and
haio:procedure_type_has_instance to link it to its
instances. We use haio:has_consequence to link surgeries
to their complications and haio:disease_has_type, a spe-
cialisation of rdf:type, to test if a complication is an SSI.
Predicate haio:is_done_on is sufficient to retrieve the
patients who undergo the corresponding surgeries. Predi-
cates haio:patient_has_diagnosis and haio:identifies serve
to retrieve diagnoses and diseases being diagnosed, and
haio:disease_has_type checks if the disease is of the type
haio:Diabetes_mellitus. In lines 1618, we retrieve the
times of the surgery and diabetes diagnosis, and compare
them. In lines 2124, we identify the interval between
those times and test its duration.
The query execution starts with a call to the service
getProcedureByClass that enumerates URIs of all proce-
dures in the DW whose CCI [34] codes indicate that they
are of the specified procedure class. Line 7 in the query
is resolved with a call to getSSIBySurgery that retrieves
incidents of infections that are suspected complication of
the specified operative procedure. For this purpose, the
current service prototype extracts from the DW all diag-
noses made for the same patient within 30 days from
the operation, and selects only incidents with ICD-10
codes indicating complications of surgeries. Service get-
PatientByProcedure instantiates ?Patient in line 9, then
getDiagnosisByPatient instantiates ?DiabetesIncident with
all diseases the patient was diagnosed with. These ser-
vices use fairly obvious SQL queries to the DW. Testing
that ?DiabetesIncident is a diabetes mellitus (lines 1213)
is done by calling getDiseaseClass that will assign sub-
classes of haio:Disease based on the ICD-10 codes from
the DW table for diagnoses, although the current pro-
totype can only deal with haio:Diabetes_mellitus. Alter-
natively, to improve performance, we could implement
it as a predicate-testing service supporting the predicate
ext:isComparedToObject. Lines 16-17 are resolved with
getDiagnosisInfo and getProcedureInfo that retrieve the
necessary times from the records of encounters referred
to from the DW tables for diagnoses and procedures.
Lines 1823 are resolved with several calls to temporal
reasoning services.
Finally, to illustrate the benefits of semantic querying,
we provide a simplified SQL query equivalent to our
SPARQL query:
SELECT proc_enc.encPatWID, hprcWID,
diab_diag.hdgWID
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 13 of 19
http://www.jbiomedsem.com/content/4/1/9
-- patient ID, procedure ID and diagnosis
ID
FROM NhrProcedure, Nencounter proc_enc,
NhrDiagnosis diab_diag,
Nencounter diab_diag_enc, NhrDiagnosis
ssi_diag,
Nencounter ssi_diag_enc
WHERE hprcCd LIKE 1
AND proc_enc.encWID=NhrProcedure.
hprcHraEncWID
AND proc_enc.encPatWID = diab_diag_enc.
encPatWID
AND diab_diag_enc.encWID=diab_diag.
hdgHraEncWID
-- prefixes for coded for diabetes
mellitus:
AND (diab_diag.hdgCd LIKE E10
diab_diag.hdgCd LIKE E12
diab_diag.hdgCd LIKE E14
AND proc_enc.encPatWID = ssi_diag_enc.
encPatWID
AND NhrProcedure.hprcHraEncWID = proc_enc.
encWID
AND ssi_diag_enc.encWID = ssi_diag.
hdgHraEncWID
-- SSI must be diagnosed in this time
interval:
AND (ssi_diag_enc.encStartDtm -
NhrProcedure.hprcDtm) >=(INTERVAL 3 DAY)
AND (ssi_diag_enc.encStartDtm -
NhrProcedure.hprcDtm) <(INTERVAL 30 DAY)
-- SSI codes prefixes:
AND (ssi_diag.hdgCd LIKE T802
ssi_diag.hdgCd LIKE T826
ssi_diag.hdgCd LIKE T835
ssi_diag.hdgCd LIKE T845
ssi_diag.hdgCd LIKE T847
ssi_diag.hdgCd LIKE T874
-- diabetis must be diagnosed within
a week before the surgery
AND (diab_diag_enc.encStartDtm -
NhrProcedure.hprcDtm) <= (INTERVAL 7 DAY)
It is clear that the user composing such an SQL query
would have to know some SQL and, more importantly, be
able to navigate the complex DW schema and learn to use
rather non-mnemonic table and attribute identifiers. The
query composition process is also quite error prone, e.g.,
the user would have to be very accurate not to omit some
of the diabetes or SSI-related code prefixes. In contrtast,
the SPARQL query hides all such technicalities behind
a relatively simple and mnemonic notation. In seman-
tic querying, the burden of sorting out the technicalities
lies on the engineers implementing it and, although it
may require a considerable effort, the results can be later
reused by multiple users in multiple scenarios.
Use case 2
Our second query, implementing Question (6) What
patients were diagnosed with SSI while they were taking
corticosteroids systemically?, is as follows:
1 SELECT DISTINCT ?Patient
2 FROM <http://cbakerlab.unbsj.ca:
8080/haiku/query1_as_context.rdf>
3 FROM <http://cbakerlab.unbsj.ca:
8080/haiku/corticosteroids.rdf>
4 WHERE
5 { # corticosteroids for systemic use
6 <http://cbakerlab.unbsj.ca:
8080/ontologies/atc.owl#H02>
7 atcso:hasSubClass ?DrugClass .
8 # is referred to by
9 ?DrugClass sio:SIO_000212 ?
DIN_Record .
10 # has attribute
11 ?DIN_Record a lsrn:DIN_Record; sio:
SIO_000008 ?DIN_Identifier .
12 # has value
13 ?DIN_Identifier a lsrn:DIN_Identifier;
sio:SIO_000300 ?DIN .
14 # is about
15 ?DIN_Record sio:SIO_000332 ?DrugProduct.
16 ?PharmService haio:manages ?DrugProduct;
haio:is_service_for ?Patient .
17 # Select patients with SSI:
18 ?Patient haio:patient_has_diagnosis ?
Diagnosis .
19 ?Diagnosis haio:identifies
[haio:disease_has_type haio:SSI] .
20 # Temporal check:
21 ?PharmService haio:has_specification ?
AdminPeriod .
22 ?AdminPeriod a haio:Administered_period.
23 ?Diagnosis haio:situation_has_time[esto:
during ?AdminPeriod] . }
The file corticosteroids.rdf simply attaches a query ID
to the URI for the class corticosteroids for systemic use
from the ATC ontology. The line with hasSubClass is
resolved with a call to http://cbakerlab.unbsj.ca:8080/
atc-sadi/getSubclassByATCDrugClass, which is an exter-
nal, i.e., non DW-specific service. Note, however, that
we had to modify the service code for our experiments
with SHARE, to make it propagate query IDs if they are
provided in the input. This is not necessary for tempo-
ral reasoning with Hydra, as explained in the section on
temporal reasoning.
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 14 of 19
http://www.jbiomedsem.com/content/4/1/9
In lines 915, we use SIO primitives and LSRN [35]
identifiers to leverage the SIO modeling for database
records. In our situation, an ATC drug class is referred
to by a DIN record which has a DIN ID as an attribute
and whose subject is a drug product, identified with
a URI. Lines 913 are resolved with another general
purpose service http://cbakerlab.unbsj.ca:8080/din-sadi/
getDINByATCDrugClass, also conforming to the context
propagation convention.
In the first half of line 16, we link pharmacy services to
the drug product via haio:manages. It is resolved, together
with line 15, by a call to service getPharmacyServiceBy-
DIN, which obtains the necessary data from table Nph-
mIngredient in the DW containing DINs of prescribed
drugs as a separate attribute.
The predicate haio:is_service_for links services to the
corresponding patients and is provided by getPatient-
ByPharmacyService, trivially implemented as a join of
appropriate DW tables. Primitives from the remaining
lines, and the services implementing them, have already
been discussed.
Use case 3
The following query implements Question (2) How many
patients were infected with methicillin-resistant Staphylo-
coccus aureus (MRSA) in Quarter 1 of 2007?:
1 SELECT ?incident ?kegg_org_rec ?
kegg_disease_rec ?icd10_id
2 FROM <http://cbakerlab.unbsj.ca:
8080/haiku/quarter1_2007.rdf>
3 FROM <http://cbakerlab.unbsj.ca:
8080/haiku/staphylococcus_aureus.rdf>
4 WHERE
5 {
6 # name
7 ?bacteria_name_attr a sio:SIO_000116.
8 # has value
9 ?bacteria_name_attr sio:SIO_000300 "
Staphylococcus aureus N315 (MRSA/VSSA)".
10 # KEGG Organism record for the bacteria:
11 # has value
12 #?bacteria_name_attr sio:SIO_000300 ?
kegg_org_name;
13 # is attribute of
14 sio:SIO_000011 ?bacteria_type.
15 # organism is subject of
16 ?bacteria_type rdfs:subClassOf sio:
SIO_010000; sio:SIO_000629?kegg_org_rec.
17 # has attribute
18 ?kegg_org_rec a lsrn:KEGG_Organism_
Record;sio:SIO_000008 ?kegg_org_id_attr.
19 # has value
20 ?kegg_org_id_attr a lsrn:KEGG_Organism_
Identifier; sio:SIO_000300 ?kegg_org_id.
21 # KEGG DISEASE corresponding to the
bacteria:
22 ?bacteria_type keggso:pathogen_causes_
disease_incident [a ?kegg_disease_type].
23 # is subject of
24 ?kegg_disease_type sio:SIO_000629 ?
kegg_disease_rec .
24 ?kegg_disease_rec a lsrn:KEGG_DISEASE_
Record;
25 # has attribute
26 sio:SIO_000008 ?kegg_disease_id_attr .
27 ?kegg_disease_id_attr a lsrn:KEGG_
DISEASE_Identifier;
28 # has value
29 sio:SIO_000300 ?kegg_disease_id .
30 # Corresponding ICD-10 class and ID:
31 ?common_incident a ?kegg_disease_type;
a ?icd10_disease_type .
32 # is subject of
33 ?icd10_disease_type sio:SIO_000629 ?
icd10_rec .
34 # has attribute has value
35 ?icd10_rec a lsrn:ICD10CA_Record; sio:
SIO_000008 [sio:SIO_000300 ?icd10_id] .
36 # Known incidents:
37 ?icd10_disease_type haio:
disease_type_has_instance ?incident .
38 ?incident a haio:Disease; haio:
identified_through ?diagnosis .
39 ?diagnosis a haio:Diagnosis .
40 # Temporal check:
41 ?diagnosis haio:situation_has_time
[esto:during esto:quarter1_2007] . }
The files in the FROM clauses contain RDF descrip-
tions of the time interval esto:quarter1_2007 represent-
ing Quarter 1 of 2007, and a resource representing
a name with the value Staphylococcus aureus N315
(MRSA/VSSA).
The main feature distinguishing this query from the
two previous queries is the extensive use of external
SADI services: all conditions in Lines 1235 require
calls to such services. The external (not based on our
DW) services used by Hydra provide the following func-
tionality: (a) finding a KEGG Organism record and
ID by the bacteria name (Lines 1220), (b) retrieving
KEGG DISEASE records and IDs for conditions caused
by organisms specified with their KEGG Organism IDs
(Lines 2229), and (c) ICD-10 disease classes correspond-
ing to diseases specified with their KEGG DISEASE IDs
(Lines 3135).
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 15 of 19
http://www.jbiomedsem.com/content/4/1/9
Note that by using the external services we go far
beyond what is possible with SQL querying of the DW.
This use case demonstrated that the data integration
aspect of SADI is an important advantage. However, we
refrain from a more thorough discussion of this because
data integration with SADI has been studied in several
specialised projects (see, e.g., [15-18]).
Use case 4
The following query implements Question (3) What
patients received a diagnosis of sepsis within 30 days of
being diagnosed with SSI?:
1 SELECT ?sepsis_incident ?icd10_id ?
ssi_incident ?duration
2 FROM <http://cbakerlab.unbsj.ca:
8080/haiku/sepsis.rdf>
3 WHERE
4 {
5 # ICD-10 disease classes related to
sepsis:
6 ?sepsis_keyword sio:SIO_000300
"sepsis" .
7 # is similar to
8 ?sepsis_keyword sio:SIO_000283 ?
disease_superclass_name_attr .
9 # name
10 ?disease_superclass_name_attr a sio:
SIO_000116;
11 # has value
12 sio:SIO_000300 ?disease_superclass_
name;
13 # is attribute of
14 sio:SIO_000011 ?disease_superclass .
15 # disease
16 ?disease_superclass rdfs:subClassOf sio:
SIO_010299 .
17 ?disease_class rdfs:subClassOf ?
disease_superclass;
18 # is subject of
19 sio:SIO_000629 ?icd10_record .
20 # has attribute
21 ?icd10_record a lsrn:ICD10CA_Record;
sio: SIO_000008 ?icd10_id_attr .
22 # has value
23 ?icd10_id_attr a lsrn:ICD10CA_
Identifier; sio:SIO_000300 ?icd10_id .
24 # Patients with sepsis diagnoses:
25 ?disease_class haio:disease_type_has_
instance ?sepsis_incident .
26 ?sepsis_incident a haio:Disease; haio:
identified_through ?sepsis_diagnosis .
27 ?sepsis_diagnosis a haio:Diagnosis;
haio: is_performed_for ?patient .
28 # SSI diagnoses for the same patient:
29 ?patient haio:patient_has_diagnosis
[haio:identifies ?ssi_incident] .
30 ?ssi_incident haio:disease_has_type
haio:SSI .
31 # Sepsis was diagnosed shortly after
the SSI?
32 ?sepsis_diagnosis haio:situation_has
_time ? sepsis_diagnosis_time .
33 ?ssi_diagnosis haio:situation_has_time ?
ssi_diagnosis_time .
34 ?sepsis_diagnosis_time esto:after ?
ssi_diagnosis_time .
35 ?ssi_diagnosis_time esto:meets ?
between .
36 ?between esto:meets ?
sepsis_diagnosis_time .
37 ?between esto:timeIntervalHasDuration
[esto:hasValue ?duration] .
38 FILTER(?duration <= "P30D"^^xsd:
duration) }
In the previous query in Use case 3, a specific bacteria
name Staphylococcus aureus N315 (MRSA/VSSA) was
used, which assumed that the user somehow knows the
precise name from the KEGG Organism nomenclature.
Sometimes this assumption is too strong and we want
the user to be able to formulate less precise questions as
SPARQL queries. The current use case illustrates this: we
assume that the user is interested in various disease names
mentioning sepsis. The work of finding such names is
done by an external SADI service that resolves the con-
ditions in lines 816. Other external SADI services used
by Hydra provide the following functionality: (a) finding
URIs of disease classes from our ICD-10 ontology (see
[28]), given precise names of the disease classes, (b) enu-
merating subclasses of a disease class specified with an
ICD-10 URI  this is necessary because potentially a dis-
ease class related to sepsis may have subclasses not having
sepsis in their names, and (c) mapping ICD-10 disease
class URIs to ICD-10 records and IDs.
Related work
We are not aware of any work on semantic querying of
relational databases via Semantic Web services. However,
there have been a lot of attempts, since as early as 1993
[36], to implement semantic querying of RDB. We cannot
give a comprehensive overview of all related publications,
so we will focus our discussion on the key concepts and
methods.
Declarative semantic mappings
Practically all existing approaches to semantic querying of
RDB use some sort of declarative mappings from source
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 16 of 19
http://www.jbiomedsem.com/content/4/1/9
relational data schemas to target semantic schemas.
The most popular state-of-the-art approaches use OWL
ontologies or RDF vocabularies as target schemas (see,
e.g., [8-10,36]), but different approaches have also been
tried  e.g., in [37] the target semantic schema is essen-
tially a regular relational schema with tables correspond-
ing to well-defined concepts with mnemonic names, that
are easier to understand for a non-programmer user than
the source schema.
The purpose of the semantic mappings is to define an
interpretation of data from an RDB in terms of the con-
cepts and relations defined in the target schema. The
implementations then use these mappings to either com-
pletely translate the relational data or by applying them in
query time, which will be discussed below in more detail.
Note that we are also using declarative mappings to doc-
ument the semantic modelling of our data (see Section
Mapping the DW schema to HAIO above).
Declarative mappings are expressed in the form of
axioms postulating how facts (rows) in the source RDB
are related to instances and facts in the target schema.
Typically, the axioms are Horn rules in some specific syn-
tactic form, such as OWL axioms, as in [9], or SPARQL
CONSTRUCT queries, as in SWObjects [38]. However,
there are approaches that support much more expressive
semantic mapping and query languages: e.g., [11,39] can
deal with any first-order logic axioms.
Declarative semantic mappings for RDB are often cre-
ated manually by programmers. D2R [40] and Virtuoso
[41] can also automatically generate a preliminary map-
ping that can be later modified by the user, e.g., in order to
align it with the target schema. The RDB2RDF Working
Group [42] atW3C is taking this approach further by stan-
dartising both the language R2RML for mapping RDB to
RDF, and defining a standard way to map schemas (Direct
Mapping). The virtual direct graph produced by applying
the Direct Mapping to a relational schema can be further
mapped to the target schema by additional axioms, e.g.,
SWObjects implements SPARQL CONSTRUCT for this
purpose.
The declarative nature of axiom-like mappings makes
them relatively easy to create and maintain, at least com-
pared to our approach based on hard-coding mappings in
Java or other programming languages. However, declara-
tive mappings have some limitations that are hard to over-
come practically: certain things are much easier to define
programmatically than axiomatically, due to natural lim-
itations of mapping languages and implementations. For
example, defining Allens predicates for temporal com-
parison in a practical fashion, or defining the Body Mass
Index function is likely to be challenging with some of the
existing declarative approaches. Mapping and retrieving
data from non-relational sources is even harder. Over-
all, the SADI-based approach is more flexible  even
very tricky relationships between the source and target
schemas can be captured by programming.
Materialisation
Defining a semantic mapping for an RDB creates a virtual
database instantiating the target semantic schema. Since
most of the state-of-the-art approaches map relational
data to RDF, for simplicity we can speak about virtual RDF
graphs. A straight forward way to use a virtual RDF graph,
e.g., for query answering, is to actually materialise it, i.e.,
to create a triplestore representing it and make it available
for querying or browsing. This approach is implemented
at least in Triplify [43] and Minerva [44].
An obvious advantage of this approach is that even
large RDF graphs materialised in a triplestore can be pro-
cessed very efficiently, due to the availability of mature
highly optimised implementations. One obvious limita-
tion of this approach is its inability to deal with live data:
the data has to be transformed into RDF before it can
be queried. However, most Clinical Intelligence scenar-
ios other than surveillance, dont require querying live
data.
A more significant practical problem is the limited
scope of the data that can be stored in the triplestore:
there are only so many data sources that can be integrated
into it. This limits the scope of analyses possible with
the data without additional processing layers. Our SADI-
based approach is free from this limitation  thousands
of information sources may be wrapped as SADI services
and immediately available for ad hoc analyses.
Query rewriting
An alternative to materialisation is query rewriting, when
queries in terms of the target semantic schema are
converted to equivalent queries in terms of the source
relational schema. This approach is very popular and
is implemented at least in [9,11,37,40,41,45]. It allows
querying live data and requires lighter infrastructure that
materialisation: there is no need to set up and maintain a
triplestore and data translation utilities. However, it suf-
fers from the same drawback as materialisation, since, by
itself, it does not support data integration.
Relation to data integration
We would like to mention that some of the prior exper-
imental efforts on semantic querying of RDB specifically
targeted the data integration task rather than self-service
ad hoc querying of isolated RDB. For example, SIMS [36]
and TAMBIS [8]  early and well-cited semantic querying
projects  were focused on data integration. The integra-
tion aspect of semantic querying is especially important
in the Clinical Intelligence context because many analyses
of clinical data require drawing data from external sources
of information about diseases, drugs, infectious agents,
Riazanov et al. Journal of Biomedical Semantics 2013, 4:9 Page 17 of 19
http://www.jbiomedsem.com/content/4/1/9
medical equipment, etc, as exemplified by the last three of
our four use cases.
Currently, publishing relational data as Linked Data
seems to be the most popular approach to semantic inte-
gration of relational data (see, e.g., [40,41]). For example,
the Virtuoso SPARQL engine can crawl theWeb by resolv-
ing the URIs of RDF resources and using the obtained
data together with the underlying relational data to answer
SPARQL queries. However, although its Web-like nature
makes Linked Data easy to surf, efficient implementa-
tion of querying of Linked Data is still a major technical
challenge.
Another possibility is federated querying of multiple
distributed SPARQL endpoints each representing a sep-
arate RDB or data source of a different type. This pos-
sibility is currently almost hypothetical because efficient
federated querying is difficult: the optimisation methods
(mostly inherited from relational databases) that work
well on triplestores, do not transfer well to the case of
distributed endpoints.
Since SADI is originally designed with the data federa-
tion task in mind, its not surprising that our SADI-based
approach copes well with the integration challenge. Our
experiments with Use cases 3-4 suggest that if the external
data sources are available via SADI services, the integra-
tion is automatic and transparent to the user.
Temporal reasoning
Although we discuss temporal reasoning in this article
primarily to illustrate the capabilities of SADI, in par-
ticular the extensibility of our approach, because of the
practical importance of the temporal reasoning task, we
would like to briefly discuss existing work in this direc-
tion. All publications related to SPARQL querying with
temporal constraints we have found address the prob-
lem of defining mechanisms for specifying validity time of
assertions in RDF, and study algorithms that can answer
queries extended accordingly. For example, [46], which is
INTRODUCTION Open Access
Selected papers from the 15th Annual
Bio-Ontologies Special Interest Group Meeting
Larisa N Soldatova1*, Susanna-Assunta Sansone2, Michel Dumontier3, Nigam H Shah4
From Bio-Ontologies 2012
Long Beach, CA, USA. 13-14 July 2012
* Correspondence: larisa.
soldatova@brunel.ac.uk
1Brunel University, London, UK
Abstract
Over the 15 years, the Bio-Ontologies SIG at ISMB has provided a forum for
discussion of the latest and most innovative research in the bio-ontologies
development, its applications to biomedicine and more generally the organisation,
presentation and dissemination of knowledge in biomedicine and the life sciences.
The seven papers and the commentary selected for this supplement span a wide
range of topics including: web-based querying over multiple ontologies, integration
of data, annotating patent records, NCBO Web services, ontology developments for
probabilistic reasoning and for physiological processes, and analysis of the progress
of annotation and structural GO changes.
Summary of selected papers
In 2012, the SIG received 20 paper submissions and 9 flash updates. 15 papers and all
the flash updates were selected for presentation at the meeting, out of which 7 papers
and 1 commentary appear in this supplement.
The seven papers and the commentary selected for this supplement are extended ver-
sions of the original papers presented at the 2012 SIG. The papers include research on
web-based querying over multiple ontologies [1,2,7], analysis of the Gene Ontology [3],
reports on advances in knowledge representations, e.g. the representation of physiological
processes [4] and probabilistic reasoning [6], advances in the annotation of patent records
[5], and Web services from the National Center for Biomedical Ontology [8].
The paper titled Ontology-Based Querying with Bio2RDFs Linked Open Data by Call-
ahan et al reports on an update to Bio2RDF [1]. Nineteen new and updated RDF datasets
have been mapped to the Semanticscience Integrated Ontology (SIO) to enable federated
queries across multiple Bio2RDF endpoints. The new datasets include BioModelsan EBI
resource providing details on published computational models primarily from systems
biology, BioPortala collection of over 300 bio-ontologies from multiple providers, NDC
(the National Drug Code Directory)a Food and Drug Administration (FDA) resource
providing a current list of all drugs produced or otherwise processed for distribution by
drug companies, and others. Each dataset in the Bio2RDF network is linked to all the
other datasets. Federated queries make it possible to formulate a query across connected
datasets that reside in separate SPARQL endpoints. Several example SPARQL queries are
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):I1
http://www.jbiomedsem.com/content/4/S1/I1 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2013 Soldatova et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
discussed in the paper and Bio2RDF conversion scripts are available at a GitHub reposi-
tory http://github.com/bio2rdf/bio2rdf-scripts.
The paper Biotea: RDFizing PubMed Central in Support for the Paper as an Interface to
the Web of Data by Castro et al demonstrates an approach to the generation of intero-
perable, interlinked, and self-describing documents in the biomedical domain [2]. The
proposed semantic processing approach has been applied to the full-text, open-access sub-
set of PubMed Central. The resulting RDF dataset exploits existing ontologies and seman-
tic enrichment services. The semantic processing of biomedical literature presented in this
paper embeds documents within the Web of Data and facilitates the execution of con-
cept-based queries against the entire digital library. The proposed approach delivers a set
of tools for metadata declaration and semantic processing of biomedical documents.
The model, services, prototype, and datasets are available at http://biotea.idiginfo.org/.
Clarke et al in the paper titled A task-based approach for Gene Ontology (GO) eva-
luation introduce a method for evaluating the GO annotations based on the impact
they have on gene set enrichment analysis [3]. The proposed framework uses enrich-
ment analysis to determine the effectiveness of the GO annotations in providing biolo-
gically accurate results. As a use case of the evaluation the authors examine how well
the GO annotations perform at reproducing biological expectations for a dataset. They
demonstrate that the proposed framework enabled the analysis of the progress of
annotation and structural GO changes from 2004 to 2012. The authors were also able
to determine that the quality of annotations and structure have been improving in
terms of their ability to recall underlying biological traits.
The paper titled Representing physiological processes and their participants with Phy-
sioMaps by Cook et al presents computable knowledge networks of biological processes
and their participantsPhysioMaps [4]. PhysioMaps have originated from the large-scale
projects such as the Physiome, the Virtual Physiological Human, and the Virtual Physiolo-
gical Rat. PhysioMap and SemSim (semantic simulation) models are based on the Ontol-
ogy of Physics for Biology (OPB). The simulation models are XML files that specify a set
of dynamical processes and their participants. Currently the proposed approach supports
two types of processes, physical flows and modulation processes. Illustrative examples are
provided. The key result is the semi-automatic parsing of biosimulation model code into
PhysioMaps that can be displayed and interrogated for qualitative responses to hypotheti-
cal perturbations. SemSim project materials are available at http://sbp.bhi.washington.edu/
projects/semsim. A tool for creating, annotating, composing and decomposing SemSim
models SemGen is available at http://sbp.bhi.washington.edu/projects/semgen.
Eisinger et al in the paper titled Automated Patent Categorization and Guided
Patent Search using IPC as Inspired by MeSH and PubMed provide a comparative
analysis of the Medical Subject Headings ontology (MeSH) and the main patent classi-
fication system, the International Patent Classification (IPC) [5]. MeSH supports and
improves the document search on PubMed, while patent documents are considerably
less accessible. The analysis shows a strong structural similarity of the MeSH and IPC
hierarchies, but also some significant differences. The use of IPC to support the patent
search comes with two serious disadvantages: complexity of the classification system
and sparse class assignments. The low number of IPC class assignments and the lack
of occurrences of class labels in patent texts result in the limitations in the patent
search. To overcome these limits, the authors propose a system for guided patent
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):I1
http://www.jbiomedsem.com/content/4/S1/I1
Page 2 of 4
search based on the use of class co-occurrence information and the assigning of addi-
tional classes to patents.
The paper Representation of probabilistic scientific knowledge by Soldatova et al dis-
cusses the probabilistic nature of biomedical knowledge and the necessity for an ontolo-
gical support for probabilistic reasoning with scientific knowledge [6]. The authors
propose an ontology HELO (HypothEses and Laws Ontology) to model the key entities
of the theory of probability. HELO is designed to consistently accommodate scientific
hypotheses and laws collected from different sources: interviews with scientists, web
pages, research papers, databases, program codes. The authors demonstrate the utility of
HELO on three worked examples: changes in the probability of the hypothesis that sir-
tuins regulate human life span; changes in the probability of hypotheses about gene
functions in the S. cerevisiae aromatic amino acid pathway; and the use of active learning
in drug design, where a strategy for the selection of compounds with the highest prob-
ability of improving on the best known compound was used. HELO is available at
https://github.com/larisa-soldatova/HELO.
Vita et al in the commentary Query enhancement through the practical application of
ontology: the IEDB and OBI outline their experiences in utilizing bio-medical ontologies
to provide enhanced database search functionality [7]. The authors analyse the benefits of
the information captured by a formal ontology implemented directly into the user web
interface for querying databases. The authors discuss the long-term goal of enabling a true
semantic integration of data and knowledge in the biomedical domain. Vita et al describe
their progress towards this goal and the main obstacles. The discussed approach is consid-
ered on the example of the Immune Epitope Database (IEDB, www.iedb.org) that utilizes
the Ontology for Biomedical Investigations (OBI) and several additional ontologies to
represent immune epitope mapping experiments.
Whetzel on behalf of the NCBO Team in the review paper NCBO Technology:
Powering semantically aware applications provides an overview of technology devel-
oped by the National Center for Biomedical Ontology (NCBO), a National Center for
Biomedical Computing created under the NIH Roadmap [8]. The NCBO developes
Web services, which provide access to one of the largest repositories of biomedical
ontologies. This overview describes typical services provided by NCBO for the
research community. For example, the Ontology Web services provide access to ontol-
ogies, navigation of the class hierarchy and details of each term and the NCBO Anno-
tator Web service tags text automatically with terms from ontologies in BioPortal.
The NCBO Widgets package enables the Ontology Web services to be used directly in
web sites. The functionality of the NCBO Web services and widgets are incorporated
into semantically aware applications for ontology development and visualization, data
annotation, and data integration. The NCBO Web services are documented at: http://
www.bioontology.org/wiki/index.php/NCBO _REST_services.
Competing interests
The authors have no competing interests to declare.
Acknowledgements
As editors of this supplement, we thank all the authors who submitted papers, the Program Committee members and
the reviewers for their excellent work. We are grateful for help from Sarah Headley from BioMed Central in putting
this supplement together.
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):I1
http://www.jbiomedsem.com/content/4/S1/I1
Page 3 of 4
Declarations
This article has been published as part of Journal of Biomedical Semantics Volume 4 Supplement 1, 2013: Proceedings
of the Bio-Ontologies Special Interest Group 2012. The full contents of the supplement are available online at http://
www.jbiomedsem.com/supplements/4/S1
Author details
1Brunel University, London, UK. 2University of Oxford, Oxford e-Research Centre, UK. 3Carleton University, Ottawa,
Canada. 4Stanford University, CA, USA.
Published: 15 April 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Karlsson and Trelles Journal of Biomedical Semantics 2013, 4:4
http://www.jbiomedsem.com/content/4/1/4SOFTWARE Open AccessMAPI: a software framework for distributed
biomedical applications
Johan Karlsson and Oswaldo Trelles*Abstract
Background: The amount of web-based resources (databases, tools etc.) in biomedicine has increased, but the
integrated usage of those resources is complex due to differences in access protocols and data formats. However,
distributed data processing is becoming inevitable in several domains, in particular in biomedicine, where
researchers face rapidly increasing data sizes. This big data is difficult to process locally because of the large
processing, memory and storage capacity required.
Results: This manuscript describes a framework, called MAPI, which provides a uniform representation of resources
available over the Internet, in particular for Web Services. The framework enhances their interoperability and
collaborative use by enabling a uniform and remote access. The framework functionality is organized in modules
that can be combined and configured in different ways to fulfil concrete development requirements.
Conclusions: The framework has been tested in the biomedical application domain where it has been a base for
developing several clients that are able to integrate different web resources. The MAPI binaries and documentation
are freely available at http://www.bitlab-es.com/mapi under the Creative Commons Attribution-No Derivative Works
2.5 Spain License. The MAPI source code is available by request (GPL v3 license).
Keywords: Service-oriented architectures, Web-service integration, Software frameworkBackground
The World Wide Web (WWW) has emerged as a gallery
of resources, such as Web Services (WS) and datasets,
which can be discovered, combined and exploited to en-
hance our capacity of producing new knowledge. One
prominent example is the BioCatalogue [1] repository with
metadata describing over 2200 WS (November 2011).
The potential of using WS to support biomedical re-
search has been widely reported. For example, in [2,3],
WS are used to establish genome-disease associations
which are necessary for patient genome sequencing to
support determination of diagnosis or therapy. In [4],
semantics are used to enrich the patient record system, in
particular for tasks related to drug prescription (drug
interactions, medical insurance coverage for the drug etc.).
The authors show how web-services can be used to com-
municate information between the legacy systems and
databases.* Correspondence: ortrelles@uma.es
Computer Architecture Department, University of Málaga, Complejo
Tecnológico, Campus de Teatinos, Málaga 29080, Spain
© 2013 Karlsson and Trelles; licensee Biomed
Creative Commons Attribution License (http:/
distribution, and reproduction in any mediumHowever, sending the output from a WS to another WS
(i.e. as workflows or pipelines) is complex because of differ-
ences in WS communication protocols (varying from
SOAP [5] to WS using REST [6] principles) and data for-
mats (for example FASTA [7], GenBank [8] and FASTQ
[9]). WS metadata describing tool inputs and outputs and
syntax description of formats (datatypes) in shared reposi-
tories simplifies the development of user-friendly client
software that can combine WS as workflows [10]. With
such metadata, it is also possible to apply tools such as
ReadSeq [11] to automatically transform biological se-
quence data between formats.
This paper describes a software framework (MAPI)
which provides support for WS integration. MAPI
addresses the following aspects of WS integration and
usage:
 Management and discovery of WS instances in
metadata registries
 Unification of WS metadata
 WS invocation (execution) and data format
conversion.Cental Ltd. This is an Open Access article distributed under the terms of the
/creativecommons.org/licenses/by/2.0), which permits unrestricted use,
, provided the original work is properly cited.
Karlsson and Trelles Journal of Biomedical Semantics 2013, 4:4 Page 2 of 12
http://www.jbiomedsem.com/content/4/1/4In [12] we showed how MAPI facilitates client develop-
ment by allowing the developers to focus on GUI aspects.
This paper gives complementary background and details, in
particular with respect to the metadata schema in MAPI
(see Section Common (shared) model) and aspects
related to addressing heterogeneity in WS and user data
(see Section Seamless data format transformation).
Additionally, this paper exemplifies the usage of MAPI
functions for a simple use case (see Section Use Case
Homologous Protein Finder) and discusses the role of
MAPI in biomedical settings where there are strong
requirements for security (see Section Potential of using
WS in biomedicine).
We will discuss the role of MAPI in the biomedicine do-
main where a wide variety of formats, protocols and tools
are used [13]. As a proof of concept, MAPI provides sup-
port for BioMOBY WS [14], WSDL  described SOAP WS
(for example, from European Bioinformatics Institute, EBI
[15] and DNA Data Bank of Japan, DDBJ [16]), Taverna
[17] workflows, WS from INB [18] and ACGT [19]
projects.Implementation
This section gives an overview of the MAPI software
framework and its novel characteristics. The MAPI frame-
work covers functionality related to service-oriented archi-
tectures, in particular management of metadata for WS,
datatypes, data, files and users. We will describe the mod-
ules (components) and their overall functionality.Figure 1 MAPI architecture. The figure shows the different software com
or several accesses. The Workers, Formatters and Loaders enable the Execu
protocols and data formats respectively.BioMOBY datatype taxonomy
It is quite common that the results from one WS invoca-
tion must be further analyzed using other web-services.
The standard BioMOBY [14] aims to simplify this task by
defining a shared datatype taxonomy and a standardized
web-service protocol. The taxonomy follows the object-
oriented paradigm where data types are related to other
data types. Data types can inherit parts from another data
type and add additional structure/attributes by including
(containing) or consisting of arrays of other data types. For
example, the datatype GenericSequence from the datatype
taxonomy of BioMOBY MobyCentral inherits from
VirtualSequence the attributes id (String) and namespace
(String) and length (Integer). GenericSequence adds a new
attribute called SequenceString (String) which contains
the actual sequence characters.
Modules
MAPI modules and their metadata schemas are based on
concepts related to WS (e.g., tool, datatype, endpoint, par-
ameter, etc.) and relationships between WS concepts
(e.g., the datatype of a parameter). For details about the
schemas, please see Figure 1 and Additional file 1: Supple-
mentary material (Internal data models).
The following modules are available:
 Tool: A tool is an abstract grouping of software
components used to solve a specific type of problem.
Several types of tools are supported in this module;
examples include locally available applicationsponents which comprise the overall framework. Each module has one
tion and Data modules to invoke services following different service
Karlsson and Trelles Journal of Biomedical Semantics 2013, 4:4 Page 3 of 12
http://www.jbiomedsem.com/content/4/1/4(on a client machine), remotely accessible WS or even
complex workflows. Each tool has one or more
operations. Each operation has one or more input/
output parameters. Each parameter is associated with
a specific datatype.
 ToolLocation: Each Tool instance can have one or
many ToolLocation instances representing service
endpoints (mirrors). Note that the instances can
specify different WS communication protocols
(for example BioMOBY or SOAP) for the same
abstract Tool. Multiple endpoints help to create
robust and fault-tolerant clients because it is
possible to call another endpoint when one or more
are not available. The module also provides access to
information about the host machine of the endpoint
(such as memory, bandwidth etc.).
 Datatype: This module manages the shared
taxonomy of datatypes. Using such taxonomy is
essential for WS interoperability because it obligates
WS providers to adhere to the taxonomy. By
declaring that a WS works with a specific datatype
in the ontology, WS providers guarantee that the
WS is able to process data of a specific datatype
(or compatible datatypes based on inheritance as
declared in the taxonomy). MAPI has taken the
approach used in BioMOBY a as base for the
DataType module: inheritance (IS relation) and
HAS and HASA relations.
 Functional categories: This module organizes
functional categories in taxonomy. A functional
category is a keyword with semantic properties that
can be hierarchically arranged. The arrangement is
structured in such a way that each resource can be
annotated with one or more keywords, from
descriptions that range from very specific to generic.
 Namespaces: The namespace module stores
information about data provenance (data sources).
Namespaces provide a method to place resources in
context by qualifying elements and attribute names.
 Data: This module deals with the management of
structured data. Internally, the module transforms user
data from different data formats to a common,
structured data format (as defined in the datatype
module). Clients can programmatically extract different
parts of the structured data (using components called
Loaders) and/or export data to different formats (using
components called Formatters).
 File System: This module provides an abstract view
of files and folders and permits client software to
read and write files/directories regardless of where
or how they are physically stored.
 Execution: This module provides mechanisms to
invoke the tools defined in the Tool and ToolLocation
modules. The set of supported tools can be extendedby independent plug-ins called Workers and are in
charge of actually invoking the tools.
 Statistics: This module is used to record and provide
statistics about tool usage. This information can be
used to analyze the behavior of endpoints (mirrors),
identify which endpoints is most frequently used etc.
in order to implement more efficient scheduling
algorithms.
 Users: This module provides the functionality
required for handling information about persons
(users, data owners or tool providers) and
institutions associated with resources. The main
strength of this module is its ability to combine with
any of the others modules to produce secure
versions, where the access to information
(read/write) is restricted based on user rights.Functionality
The functionality of each module has been designed
around the resource it manages (e.g., users, files, tools,
data types). Each module provides methods for acces-
sing, querying and editing metadata. The main function-
alities of the modules are:
1. Retrieval of Resources/Information (all modules):
metadata for a specific resource or all resources
(lists) can be retrieved.
2. Filtering (all modules): all lists of resources retrieved
by the modules can additionally be filtered so that the
resources satisfy different criteria (extendable by
writing new filters).
3.Hierarchical Browsing (File system and Functional
categories): for modules that handle resources
organized in a taxonomy, the framework provides the
functions needed to browse the resources as a tree
and change the parent/child relations of the
resources (i.e. modifying the taxonomy).
4.General Editing (all the modules): every module has
methods for adding new resources. In the same way,
resources can be deleted and it is possible to
configure whether dependent resources will also be
deleted in a cascade fashion or whether deletion will
be rejected while the dependences exist. Finally, the
values of resource attributes can be modified.
5. Compatibility Search (Tools, Data and Datatypes):
the framework provides functions for finding
compatible WS based on the parameter datatypes.
6.Data Formatting (Data): the data module has
functions for managing the formatters available in
the system and converting user data between
different formats.
7. Task Invocation (Execution): The Execution module
manages sub-components (workers) which are able
Karlsson and Trelles Journal of Biomedical Semantics 2013, 4:4 Page 4 of 12
http://www.jbiomedsem.com/content/4/1/4to execute/invoke different types of tools (WS,
workflows etc.).
8. Task Querying (Execution): This module lets software
developers query status and obtain results/statistics
from service executions.
A detailed list of the functionality of each module is
available at http://www.bitlab-es.com/mapi/.Model characteristics
This section describes the main characteristics of the
MAPI framework. The design of MAPI has been focused
on providing a common and generic model of WS meta-
data, with the minimal set of metadata necessary to con-
struct client software.A flexible modular model
Models for different aspects of WS metadata are separated
in different modules and can be combined to adapt to a
specific requirement. Because some modules require
information from another module, the modules are notFigure 2 Setup for two clients. Two clients have instantiated MAPI modu
Interface layer communicates with repository A through Access A and ano
2 only has one instance which communicates to Repository B through Acc
regardless whether they are communicating with repository A or B. The Ac
be developed only once). The specific configuration in each client is contro
more modules than only the Tool Module (which needs, at least, the ToolL
simplification we only show instances of the Tool Module.totally independent. For example, the Tool module
requires the Datatype module to provide the definition of
data types used for the parameters of tools defined in the
Tool module.
Each module has two layers: Access and Interface. The
Access layer is in charge of mapping the information from
the data model used in the source repository to the model
used in MAPI, while the Interface defines the protocol
and programmatic method used by clients to access the
functionality of the module (i.e. the public API).
Communication between these two layers is carried
out using a common, internal interface.
In addition to the two main layers, it is possible to add
more layers (following the same internal interface) to
supply new features, such as a cache. Adding more
layers does not affect the Interface layer (for example,
software clients only need to update the configuration to
enable caching).
This separation in layers allows flexible clients to be
developed. This can be seen in Figure 2, where two dif-
ferent clients share the same access code but are config-
ured to use a different set of tools.les. Client 1 has two instances of the Tool Module, one where the
ther where it communicates with repository B through Access B. Client
ess B. Note that the interface for both clients is always the same,
cess B code is also the same for both clients (i.e. the access needs to
lled through the configuration file. Note that it is necessary to use
ocation, DataType and FunctionalCategory modules). However, for
Karlsson and Trelles Journal of Biomedical Semantics 2013, 4:4 Page 5 of 12
http://www.jbiomedsem.com/content/4/1/4Common (shared) model
By allowing software developers to work with a shared
and common model of WS metadata, it becomes much
simpler to construct client software which is able to work
with different types of WS because the complexity and
heterogeneity in service-oriented architectures are dealt
with by MAPI, not by the client software code directly.
Configurations for several taxonomies are supplied
with the standard distribution of MAPI (for example,
the main BioMOBY MobyCentral taxonomy http://www.
biomoby.org or the INB taxonomy http://www.inab.org).
However, please note that MAPI is not limited to using
a specific datatype or functional category taxonomy
since writing new configurations can extend the set of
taxonomies available for MAPI.
Similar conditions apply to functional categories. In it-
self, MAPI does not specify any functional categories but
can be configured to use external sources, such as those
found in BioMOBY MobyCentral. For example, a user
interested in finding services performing a certain task
could browse the service tree using a graphical tool which,
in turn, uses MAPI functions getFunctionalCategoryRoots
to obtain the roots of the taxonomy and recursively call
the getChildren and getTools methods for the Functional-
Category instance to obtain the functional category
instances (children) which inherit from the instance and
the tools annotated with the instance respectively. Client
software uses the same API calls to obtain this information,
regardless of which service catalogue is used. MAPI will
use the relevant access depending on the configuration.Extensibility of data model
Naturally, it is not realistic to establish a data model which
successfully predicts all future requirements and WS stan-
dards. Therefore, the data model in MAPI is extensible.
Additional modules can be implemented for new concepts
without affecting existing modules. If some feature is ne-
cessary for an existing module, it is possible to extend the
existing module (with a new module) and add the new fea-
ture (i.e. feature inheritance between modules).Seamless data format transformation
As has been mentioned earlier, biomedicine is an ex-
ample of a research field where a multitude of tools pro-
duce and consume many different data formats. One
example of such dispersion is a multitude of formats for
biological sequence data. This dispersion limits the feasi-
bility of interconnecting WS which require or produce
data in different sequence formats.
The data module in MAPI represents structured user
data with methods to read, navigate and modify the data
structure (this can be compared to APIs to navigate and
modify XML DOM trees).Modifications to the data structure can be applied auto-
matically with two types of MAPI components, Loaders
and Formatters. Both types of components are applied
based on the datatype taxonomy (inheritance) without user
intervention. Components configured to work on a specific
parent datatype will also be applied to any data of child
datatypes.
Loaders are able to modify the data structure (as defined
in the datatype module) of user data. This enables data to
be represented in a generic way, but still be compatible
with requirements of particular data standards. For ex-
ample, in BioMOBY, sequences contain several fields be-
sides the actual sequence data: every data object in
BioMOBY must have two attributes (identifier and name-
space), and every sequence must also contain explicitly the
length of the sequence data. When MAPI loads raw se-
quence data and is asked to produce this data in the Bio-
MOBY format, two loaders are applied seamlessly (without
the need for programmers to specify this) to include the
required attributes and calculate/add the length of the se-
quence data. The loader component which adds the two
required attributes is always invoked for all BioMOBY data
(it is configured to be applied to any data which inherits
from the base class; in essence all BioMOBY data). It is
possible to develop new loaders and to configure when
they are used.
Formatters, also extensible and configurable, are respon-
sible for the serialization of the generic data structure to
the actual data format (for example, BioMOBY data is
serialized as XML).
Efforts to provide mechanisms for data format trans-
formation exist (see for example [11]), but the approach in
MAPI is  to our knowledge  unique in the sense that
software developers can specify a set of formatters and loa-
ders (in essence a set of rules) which are applied seamlessly
when connecting services.
Results and discussion
This section will discuss some aspects of the design and
implementation of MAPI. The first part provides a case
study and gives an overview of the features provided by
other systems in comparison with MAPI. The second part
comments on different types of service and data hetero-
geneity, and the mechanisms that MAPI provides to ad-
dress this.
Clients implemented using MAPI
MAPI is a software framework aiming to simplify WS
integration and client development. The usefulness of
MAPI in practice for biomedicine is therefore best
represented in higher-level clients implemented using
MAPI. In [12], we showed how the MAPI framework
can be used to build complex clients. One notable ex-
ample is jORCA [20] which uses MAPI in different ways:
Karlsson and Trelles Journal of Biomedical Semantics 2013, 4:4 Page 6 of 12
http://www.jbiomedsem.com/content/4/1/4for example the Datatypes, Namespaces and WS trees
are built using the DataTypes, Namespaces, Tools and
FunctionalCategory modules. In this sub-section, please
refer to Table 1 for functionalities referenced. For each
module, jORCA asks for the list of tools, datatypes and
namespaces (functionality 1) and for relations with the
FunctionalCategories (functionality 3). jORCA also
makes use of the Filtering functionality (functionality 2)
for the quick-search tool; and using the ToolModule,
jORCA is able to quickly retrieve the list of compatible
tools for a selected datatype (functionality 5). The Exe-
cution module along with the ToolLocation module is
essential to execute and monitor tools in a transparent
way to the final user (functionality 7 and 8).
The functions for information retrieval and filtering
can be used to implement software for WS discovery
and composition, such as Magallanes [21]. Magallanes is
a discovery engine that uses MAPI to access information
(functionality 1) stored in different repositories. MAPI
also makes intensive use of the search for compatible
WS with a datatype in the Tool module for the auto-
matic generation of workflows (functionality 5). Magal-
lanes is also available as a plugin for jORCA.Use case  homologous protein finder
In this scenario, a bioinformatician has obtained a protein
sequence and wishes to know whether this protein has
been isolated in another species or even if the protein has
any isoform into the same species being studied. The
bioinformatician knows the protein identifier and wishes
to search in a database for additional information. This ex-
ample is obviously basic and only involves retrieving the
sequence from a database and then comparing that se-
quence against other known sequences but the purpose ofTable 1 Features available in different systems. Legend: [?] S
Functionality MAPI BioMOBY Glo
1. Retrieval resources ? ? ?
2. Querying ? ? ?
3. Filtering ? L ?
4. Compatibility search ? ? NA
5. Retrieval information ? ? ?
6. Browsing tree ? L NA
7. Data Formatting ? NA NA
8. Task invocation ? ? ?
9. Task query ? ? NA
10. Task scheduling OG NA ?
11. Adding resources ? ? ?
12. Delete resources ? L ?
13. General aditing ? NA NA
14. Support reasoners OG ? NAthis use case is to illustrate the usage of MAPI functional-
ity (API calls).
Actors
 WS provider
 Bioinformatician
Steps
1. The WS provider deploys two BioMOBY WS with
the following metadata:upp
busa. Name getAminoAcidSequence: input id type Object,
output sequence type AminoAcidSequence
b.Name runRPSBlast: input sequence type
GenericSequence, output blast_report type BLAST-
Text
2. The WS provider registers (see Figure 3) the
corresponding WS metadata using the Flipper
application [22] which, in turn, uses the following
MAPI functions ToolModule:newTool, Tool:
addOperation, Tool:addParameter (in that order)
to add an abstract definition of the WS and
ToolLocationModule:newToolLocation to add
specific details related to the protocol (in this case
BioMOBY), such as the endpoint where the WS
was deployed.
3. The bioinformatician uses the WS client jORCA
(please see Section Clients implemented using
MAPI). She searches for a potential path from
input datatype Object, output datatype BLAST-
Text. This is performed by the Magallanes
components (see Figure 4) which, using MAPI
functions, obtains a representation of the outputorted; [NA] Not Available; [L] Limited; [OG] On-Going
UDDI Feta WSMX SADI
? ? ? ?
? ? ? ?
NA NA ? ?
NA ? ? ?
? ? ? ?
NA NA L NA
NA NA ? NA
NA NA ? NA
NA NA ? NA
NA NA L NA
? ? ? ?
? ? ? ?
? L ? ?
NA ? ? ?
Fig
use
Karlsson and Trelles Journal of Biomedical Semantics 2013, 4:4 Page 7 of 12
http://www.jbiomedsem.com/content/4/1/4datatype (DataTypeModule:getDataType), asks the
ToolModule which tools produce an instance of
this datatype (using calls to ToolModule:
getToolList, Tool:getOperations, Operation:
getParameters to obtain instances of Parameter),
looks at the datatypes of those parameters etc.
until it finds an optimum path between the
requested input and output datatypes. In this case,
the datatypes differ slightly (the input datatype of
runRPSBlast is not the same as the output
datatype of getAminoAcidSequence). However,
since tools in BioMOBY can accept data instances
with subtypes of their declared input datatypes,
Magallanes can determine that the services are
compatible using calls to DataType:isSubtypeOf
(AminoAcidSequence is a subtype of
GenericSequence). The datatype ontology from INB
is a good example of an ontology that would give
good results for this service composition.
Obviously this pipeline is very simple (only two
services) but a more advanced example, togetherure 3 Registering a service using Flipper. This shows the metadata neces
d in this screenshot utilizes MAPI functions to register the service in a BioMOwith details about this procedure, can be found in
[21]. More complex service compositions could be
imagined for other services and datatypes where
the formats differ (in this specific example both
services required BioMOBY formatted XML). In
more complex cases, MAPI would apply
Formatters and Loaders to (if possible) make the
data compatible for the next service in the
pipeline. Please see the MAPI API documentation
and example code snippets in additional files from
[9] for further details on this process.
4. Once the tool composition (i.e. pipeline) has been
identified, the bioinformatician can enact the
pipeline from within jORCA. jORCA knows which
input parameters are necessary by using MAPI to
obtain the necessary parameters for the first WS
using the MAPI function Operation:
generateInterface.
5. Once the bioinformatician has provided the
necessary input (see Figure 5) and started
execution with jORCA, the relevant MAPI workersary for registering the service runRPSBlast. The application Flipper
BY service registry.
Fig
MA
the
be
Gen
Karlsson and Trelles Journal of Biomedical Semantics 2013, 4:4 Page 8 of 12
http://www.jbiomedsem.com/content/4/1/4for the service will be called automatically when
calling the ExecutionModule:addTask method.
6. Once running, jORCA will continuously ask the
Task object produced by the previous addTask
method for the status and, eventually, the results.This brief example shows the main functionality of
MAPI with regards to WS registration, discovery and
invocation.
Dealing with heterogeneity
The main goal of the MAPI framework is to enhance
interoperability and compatibility between different tech-
nologies by acting as a bridge for their combined use. It is
necessary to consider the following aspects in order to
combine WS:
1. Syntactic heterogeneity: differences in data formats
and service protocols
2. Semantic heterogeneity: differences in the meaning of
concepts
MAPI provides metadata which can be used for syntax-
based searches (plain-text descriptions of WS functionality)
and searches based on semantics (classification of func-
tionalities or WS input/output datatypes). Such datatypeure 4 Discovering a workflow using Magallanes. This shows how a
PI functions to discover the workflow. In many cases, there are several p
most appropriate service (see [17] for details). Please note that MAPI re
connected even if the output datatype of getAminoAcidSequence is Ami
ericSequence. This is possible because of the inheritance relation betwemetadata can be used to dynamically compose different
WS into workflows.
If we combine WS which use and produce data in differ-
ent formats, we have to take into account several aspects:
 Different names/identifiers, which could be handled in
MAPI by adding another Access layer where the
identifiers are mapped. For example, most DDBJ [16]
service parameters were declared in their WSDL as
plain strings. Therefore, we have annotated the
parameters with (semantic) datatypes used in the INB
service registry in the Access layer implemented for
this repository. This enables clients to perform more
exact WS discovery (and WS composition).
 Differences in data structure and format, which
could be handled by the use of Loaders in MAPI.
Loaders are able to modify the structure of a given
datatype in order to adapt it to a new structure. For
example, as was noted earlier, sequences in
BioMOBY contain information about the length of
the sequence, whereas standard formats such as
FASTA do not. When using the BioMOBY datatype
taxonomy in MAPI, sequences in FASTA format are
loaded and structured according to the datatype
model for sequences (one part with the sequence
and another with the calculated length of thepipeline can be generated using Magallanes. The application uses
ossible paths (compatible services). In those cases, the user can select
cognizes that the services getAminoAcidSequence and runRPSBlast can
noAcidSequence and the input datatype of runRPSBlast is
en the datatypes (AminoAcidSequence ISA GenericSequence).
Karlsson and Trelles Journal of Biomedical Semantics 2013, 4:4 Page 9 of 12
http://www.jbiomedsem.com/content/4/1/4sequence). We have several implementations of
heuristics based on rules and regular expressions
[23], which can recognize biological sequences in
different formats. Other heuristics can be specified
to recognize further formats like PDB formats,
Blast- or ClustalW-outputs. These heuristics allow
software clients to recognize raw input data and
suggest a reasonable classification to the user.
Similar efforts have been undertaken with shim-
services in myGrid [24].
Limitations
In this section, we will discuss limitations in the MAPI
approach and give examples of specific solutions.
Model differences
To add support for new standards or systems, it is neces-
sary to map the new information to be added to the mo-
del of the module in question. For example, for mapping
WSDL-described WS, it was necessary to map the contents
in WSDL to the Tool, ToolLocation and Datatype modules
(this task took one person approximately one week to
complete these accesses). Note that once the code has been
developed, it can be re-used for different services: for ex-
ample, MAPI uses the same access to obtain metadata
about EBI web-services as it does for WABI web-services.Figure 5 Enacting a workflow using jORCA. This shows how the workflo
required to enter the initial input data for getAminoAcidSequence and can,However, in some cases, it is not possible to make a
complete mapping from the original source to MAPI. For
example, in the data model of MAPI, all WS are consid-
ered to have operations but BioMOBY WS do not concep-
tually provide operations. Therefore, we created a virtual
operation in the MAPI representation of BioMOBY WS.
So far, we have not encountered major problems related
to modelling differences when developing access compo-
nents for new service types.Functionality differences
As can be seen in Table 1, it is difficult to agree on a gen-
eral set of features for all systems. For example, in the case
of BioMOBY, metadata editing is not supported in the
API. Therefore, the BioMOBY access provides this func-
tionality by de-registering and registering the metadata
instance again with the modified information in a trans-
parent way for the software developer. This method raises
the possibility of information inconsistency because the
resource can be related with other resources. In this case,
de-registering temporarily leaves the repository in an in-
consistent state. However, as de-registering and subse-
quent re-registering is almost immediate, no information
inconsistency in the BioMOBY repository has been
reported so far.w generated in Figure 4 can be enacted using jORCA. The user is
if necessary, modify secondary parameters to fine-tune the enactment.
Karlsson and Trelles Journal of Biomedical Semantics 2013, 4:4 Page 10 of 12
http://www.jbiomedsem.com/content/4/1/4Format conversions
MAPI supports plug-ins, which facilitate the interoperation
of data in multiple formats. This is possible with many
formats, but in some cases the conversion is incomplete
since the target format does not support the complete data.
For example, a gene sequence can be extracted from a
GenBank record and then exported as a BioMOBY object,
but the process cannot be reversed since the information
discarded from the GenBank record cannot be recovered.
This problem is, however, not specific to MAPI and is im-
possible to avoid because the data formats support differ-
ent amounts of information.
Format conversion simplifies the integration of heteroge-
neous WS in many cases, but the approach with loaders/
formatters is not possible in all cases. For example, because
the Loaders load the full user data into the main memory,
huge data sets are not feasible because of memory limita-
tions. However, this depends on the loader implementa-
tion. MAPI only provides the interface and does allow
extensions of the functionality. For larger data sets, it
would be possible to implement a loader which only loads
parts of the data on-demand into memory and avoids
loading the entire dataset at once.
Future work
In our opinion, WS for biomedical applications must sup-
port user authentication, transport encryption, call-by-
JOURNAL OF
BIOMEDICAL SEMANTICS
Osumi-Sutherland et al. Journal of Biomedical Semantics 2013, 4:30
http://www.jbiomedsem.com/content/4/1/30
RESEARCH Open Access
The Drosophila phenotype ontology
David Osumi-Sutherland1*, Steven J Marygold1, Gillian H Millburn1, Peter A McQuilton1,
Laura Ponting1, Raymund Stefancsik1, Kathleen Falls4, Nicholas H Brown3,1 and Georgios V Gkoutos2
Abstract
Background: Phenotype ontologies are queryable classifications of phenotypes. They provide a widely-used means
for annotating phenotypes in a form that is human-readable, programatically accessible and that can be used to
group annotations in biologically meaningful ways. Accurate manual annotation requires clear textual definitions for
terms. Accurate grouping and fruitful programatic usage require high-quality formal definitions that can be used to
automate classification. The Drosophila phenotype ontology (DPO) has been used to annotate over 159,000
phenotypes in FlyBase to date, but until recently lacked textual or formal definitions.
Results: We have composed textual definitions for all DPO terms and formal definitions for 77% of them. Formal
definitions reference terms from a range of widely-used ontologies including the Phenotype and Trait Ontology
(PATO), the Gene Ontology (GO) and the Cell Ontology (CL). We also describe a generally applicable system, devised
for the DPO, for recording and reasoning about the timing of death in populations. As a result of the new
formalisations, 85% of classifications in the DPO are now inferred rather than asserted, with much of this classification
leveraging the structure of the GO. This work has significantly improved the accuracy and completeness of
classification and made further development of the DPO more sustainable.
Conclusions: The DPO provides a set of well-defined terms for annotating Drosophila phenotypes and for grouping
and querying the resulting annotation sets in biologically meaningful ways. Such queries have already resulted in
successful function predictions from phenotype annotation. Moreover, such formalisations make extended queries
possible, including cross-species queries via the external ontologies used in formal definitions. The DPO is openly
available under an open source license in both OBO and OWL formats. There is good potential for it to be used more
broadly by the Drosophila community, which may ultimately result in its extension to cover a broader range of
phenotypes.
Keywords: Drosophila, Phenotype, Ontology, OWL, OBO, Gene ontology, FlyBase
Background
Drosophila Phenotype Ontology (DPO)
Drosophila melanogaster is one of the most widely used
model organisms for genetics, with a wealth of genetic and
phenotypic data generated over the past hundred years.
FlyBase, the model organism database for Drosophila
genetics, curates and maintains a near-comprehensive
set of records of non-molecular Drosophila phenotypes
using a combination of formal annotation strategies
and free text. Formal curation of phenotypes takes one
of two forms: phenotypes affecting specific anatomical
*Correspondence: djs93@gen.cam.ac.uk
1FlyBase, Department of Genetics, University of Cambridge, Downing Street,
Cambridge, UK
Full list of author information is available at the end of the article
structures are curated using terms from the Drosophila
anatomy ontology (DAO) [1]; other phenotypes, includ-
ing those affecting behaviour and biological processes
such as cell division, are curated using terms from the
Drosophila Phenotype Ontology (DPO), which is limited
to a relatively small number (<200) of high-level and com-
monly described phenotypic classes. To date, this ontol-
ogy has been used to annotate over 159,000 phenotypes.
It is openly available under a Creative Commons attri-
bution license (CC-BY) in both OBO and OWL formats
(See Table 1 for download options).
Biomedical ontologies
Biomedical ontologies are queryable classifications of bio-
logical entities such as anatomical structures, processes,
© 2013 Osumi-Sutherland et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the
Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,
distribution, and reproduction in any medium, provided the original work is properly cited.
Osumi-Sutherland et al. Journal of Biomedical Semantics 2013, 4:30 Page 2 of 10
http://www.jbiomedsem.com/content/4/1/30
Table 1 Accessing the DPO
Target Base URL extension
Homepage fbcv
Term request tracker fbcv/tracker
Pre-reasoned OBO version fbcv/dpo-simple.obo
Full OWL version fbcv/dpo-non-classified.owl
Full details of all available versions fbcv/downloads
Individual term details for FBcv_0000423 FBcv_0000423
DPO files, content and related resources can all be accessed via Persistent URLs
(PURLs). The base URL for all PURLS is http://purl.obolibrary.org/obo/. Column 1
describes the targets of various PURLs specified in Column 2 as extensions to
this base URL. Individual term details resolve to html when viewed in a browser,
but resolve to XML when accessed programatically.
behaviours and phenotypes. They are commonly used by
bioinformatics resources to provide controlled vocabular-
ies for annotating a range of entities (such as research
papers, genes and genotypes) with assertions about, for
example, gene function, phenotypes and gene expression
patterns [2-5]. Class and part hierarchies in ontologies
provide terms with a range of specificity allowing cura-
tors to choose an appropriately specific term depending
on the information available. Term names on their own
are frequently ambiguous, so textual definitions of terms
are needed to ensure consistent and accurate manual
annotation.
The semantics of ontologies are used to group anno-
tations in biologically meaningful ways. Typically, this
is done by grouping annotations using class and part
hierarchies (partonomy). For example, a query for genes
expressed in the Drosophila leg could return gene expres-
sion annotated with the termmiddle leg (a subclass of leg)
and claw (a part of the leg) as well as with the term leg.
The usefulness of such grouping depends on the accuracy
of classification and of assertions about partonomy.
Most highly-used biomedical ontologies have been
developed in OBO format [6]. Historically, these
ontologies have been poorly formalised and manually
maintained. Improvements to the expressiveness of OBO
format and the definition of OBO format semantics via
mapping to OWL2 [6,7] have made it possible to for-
malise definitions so that OWL reasoners can be used
to automate classification, check for consistency and
run queries. Where formal definitions reference terms
from external ontologies, OWL reasoners can leverage
the formal structure of ontologies from which terms are
imported to automate classification, check consistency
and run queries. This approach is already being used
to improve the GO [8,9], the DAO and a number of
phenotype ontologies [10-12]. Improved formalisation
can also make more sophisticated systems for grouping
annotations and querying ontology content possible.
For example, the Virtual Fly Brain project (VFB) [13,14]
uses a set of custom formalisations for representing neu-
roanatomy to drive custom queries and to enrich the
results of queries of expression and phenotype data.
When coupled with modularisation [15], formalisa-
tion can facilitate integrative approaches to reason and
compare across disparate species. For example, the Phe-
nomeNet approach aligns phenotypes across species and
enables the generation of a single, unified, and logi-
cally consistent representation of phenotype data for
multiple species. By combining the anatomy and pheno-
type ontologies of six species (yeast, worm, fly, mouse,
zebrafish, rat) alongside human disease phenotypes, Phe-
nomeNet generates a cross-species network of phenotype
similarity between genotypes and diseases [16].
PATO
The Phenotype And Trait Ontology (PATO) [17] is an
ontology of phenotype-related qualities that comprise the
basic entities that we can perceive and/or measure such
as color, size, mass, length etc. Qualities inhere in entities:
every entity comes with certain qualities that exist as
long as the entity exists. PATO allows for the description
of affected entities by combining various ontologies that
describe the entities that have been affected, such as the
various anatomical ontologies, the GO [18] and the Cell
Ontology [19], with the various qualities it provides for
defining how these entities were affected. For instance, to
describe a brown eye phenotype, we could combine the
PATO term brown with an anatomy ontology term for an
eye.
Defining and formalising the DPO
Many of the terms that make up the DPO were originally
developed and maintained in an informal hierarchy [20].
This became an explicit classification hierarchy following
the adoption of OBO format circa 2006 but initially no
further formalisation was added. No textual definitions
were provided for terms in the original hierarchy, and this
remained the case until recently. We have now developed
textual definitions for all DPO terms and formal defini-
tions in OWL for 77% of them. Here we describe the
results of this work and how it has improved the accuracy
of the ontology, its usefulness for grouping and query-
ing annotations, and its potential utility in cross-species
querying of phenotypes.
Results
Defining terms that are already widely used is challeng-
ing. New definitions either need to be consistent with
existing annotations or existing annotations need to be
updated to conform to new definitions. To ensure con-
sistency between the new DPO definitions and existing
annotation, the process of developing definitions involved
collaboration between ontology developers and curators,
Osumi-Sutherland et al. Journal of Biomedical Semantics 2013, 4:30 Page 3 of 10
http://www.jbiomedsem.com/content/4/1/30
making use of both the tacit knowledge of curators
and the extensive free-text descriptions of phenotypes in
FlyBase. During this process, we discovered inconsisten-
cies in existing annotations and invested considerable
effort to correct these and, where necessary, to modify
annotations to conform to new terms.
We have largely followed formalisation patterns devel-
oped for other phenotype ontologies [10-12,21] with all
phenotypes being subclasses of PATO quality and particu-
lar qualities having an inheres_in (RO_0000052) relation-
ship to some entity class. Types of entity are referred to
using terms from other widely-used bio-ontologies such
as the GO [18] and the cell ontology (CL) [19]. Re-using
standard patterns provides interoperability with both the
entity ontologies and other phenotype ontologies, pro-
viding good potential for more sophisticated queries of
Drosophila data and for cross-species querying.
Broadly, a phenotype can be defined as an observ-
able attribute of an organism. However, model organism
geneticists, such as those working in Drosophila genetics,
typically use the term phenotype to refer to an abnor-
mality in some anatomical structure, process or behavior
in a specified genotype compared to wild-type. Accord-
ingly, we define phenotype, the root term of our ontol-
ogy, as a quality (PATO_0000001) of some anatomical
structure, process or behavior that differs from wild-
type. Following the definition pattern developed for other
phenotype ontologies (e.g. [21]), we record this formally
using a qualifier relationship to the PATO term abnormal
(PATO_0000460):
phenotype EquivalentTo quality that qualifier some
abnormal
We have not attempted to formalise the comparative
nature of phenotypes more explicitly. All phenotypes in
the DPO are manually classified under this root term and
so inherit the assertion of abnormality.
Processual and Behavioral abnormalities
The DPO contains a variety of terms that describe phe-
notypes that are defects in biological processes. For such
terms, a GO process term is linked to a PATO term that
describes how this process was affected.
For example, the DPO term radiation resistant
(FBcv_0000439) is defined as a decreased sensitivity of a
process (PATO_0001552), inhering in response to radia-
tion (GO_0009314):
radiation resistant EquivalentTo decreased sensitivity
of a process that inheres_in some response to
radiation
By using the GO to define processual abnormalities, we
can leverage classification within it to infer much of the
classification of processual phenotypic classes. This has
led to new classifications not originally present in the orig-
inal, asserted classification. For example, stress response
defective (FBcv_0000408) originally had only 2 asserted
subclasses. We now define it using the GO term response
to stress (GO_0006950):
stress response defective EquivalentTo phenotype that
inheres_in some response to stress and qualifier
some abnormal
After auto-classification, this class has 8 subclasses
(see Figure 1A) including a number, such as DNA repair
defective (FBcv_0000423), that were not initially obvious.
Other inferred subclasses of stress response defective
have additional inferred superclasses. For example, cold
stress response defective (FBcv_0000684) is an inferred
subclass of both stress response defective (FBcv_0000408)
and temperature response defective (FBcv_0000683).
Maintaining such multiple classification by hand is well
known to be difficult, error prone and poorly scalable.
Auto-classification based on assertion of properties is
much less error prone and can scale well [22].
Figure 1 Autoclassification of processual phenotypes. Auto-classification of processual phenotypes, leveraging the GO. Terms in bold have
equivalent class definitions. Panel A shows classification of stress response phenotypes. Panel B shows a portion of the behavioral phenotype
classification.
Osumi-Sutherland et al. Journal of Biomedical Semantics 2013, 4:30 Page 4 of 10
http://www.jbiomedsem.com/content/4/1/30
The DPO also contains a range of terms for behavioral
phenotypes (Figure 1B We define a grouping class behav-
ior defective (FBcv_0000387) using the GO term behavior
(GO_0007610)
behavior defective EquivalentTo quality that
inheres_in some behavior and qualifier some
abnormal
This allows us to defer the thorny decision of what to
class as behavior [23] to the GO. With automated clas-
sification, this has resulted in a number of classes being
moved out from under the behavioral classification. This
includes a set of terms that refer to defects in percep-
tion, which the GO classifies as a neurological process but
not as behavior. It also includes the general class circa-
dian rhythm defective (FBcv_0000394), originally classi-
fied under behavior defective because circadian rhythm
defects are commonly assayed using behavior. However,
many non-behavioral processes are under circadian con-
trol. We have added a new term, circadian behavior defec-
tive (FBcv_0000679) for specifically behavioral circadian
phenotypes.
For processual and behavioral phenotypes, the evidence
for disruption is commonly indirect. A defect in the
process of segmentation during embryogenesis might be
inferred from disruption to segmental pattern in the cuti-
cle, formed many hours after the segmentation process,
with many developmental processes acting in between.
Likewise, the disruption of a behavioral reflex might be
inferred from the absence of a reflex reaction, but this
absence could also be due to disruption of muscles or
sensory perception. With appropriate extra evidence and
controls, the case for disruption of the process or behav-
ior can be compelling, but in the absence of this, it
may be more appropriate to simply record the directly
observed phenotype. A system for recording phenotypes
from the literature has to cater for both types of assertion.
Where the evidence is an observation of anatomy, this can
be recorded directly using the the Drosophila anatomy
ontology. Where the evidence is an observation of an
animals movement, we give annotators a choice of
DPO terms, one of which is neutral about whether
the phenotype is behavioral. For example, the jump
response (GO_0007630), a well characterised reflex
escape response behavior in flies, is used to define
the term jump response defective. Annotation to jump
response defective is only warranted if flies fail to
jump in a standard assay for this reponse, and con-
trols have been done which discount simple physical
explanations, such as defective legs or leg muscula-
ture. A broader term, jumping defective (FBcv_0000415),
is available for cases where no such controls are in
place.
Automated textual definitions for processual and behavioral
phenotypes
To keep definitions up-to-date with changes in the GO, we
automatically derive human-readable textual definitions
from GO terms for classes defined using the pattern:
EquivalentTo quality that inheres_in some <GO
process class>and qualifier some abnormal
For example, stress response defective gets the textual
definition:
A phenotype that is a defect in response to stress
(GO_0006950). The GO term response to stress is
defined as: Any process that results in a change in state
or activity of a cell or an organism (in terms of
movement, secretion, enzyme production, gene
expression, etc.) as a result of a disturbance in
organismal or cellular homeostasis, usually, but not
necessarily, exogenous (e.g. temperature, humidity,
ionizing radiation).
We only use this mechanism for terms that do not have
a manually supplied definition. We do not use it where
formal definitions use more specific PATO terms, as it
has proven difficult to reliably derive human readable
definitions for these cases.
Phenotypes of cells andmulti-cellular structures
The DPO contains a number of terms for cell phenotypes
such as increased cell size (FBcv_0000363), increased
cell number (FBcv_0000362) and cell death defective
(FBcv_0000425).We define these with reference to the cell
type ontology term cell (CL_0000000) or to some subclass
of cellular process (GO_0009987).
An increased cell size phenotype can be the result of
a variety of abnormal underlying biological processes
including defects in cell growth or changes in the rate of
cell division. In the absence of evidence for an underly-
ing mechanism, curators need to be able to record this
observation directly. We therefore define increased cell
size using the terms increased size (PATO_0000586) and
cell (CL_0000000):
increased cell size EquivalentTo increased size that
inheres_in some cell
The DPO class increased cell number was originally
classified as a subclass of size defective (FBcv_0000357).
But there is a complicated relationship between size
and cell number: an increase in cell number need not
result in larger size if it is accompanied by a decrease
in cell size. An increase in cell number is a phenotype
that can only be exhibited by a multicellular structure
(FBbt_00100313). We define it using has extra parts of
type (PATO_0002002)a and cell (CL_0000000) as follows:
Osumi-Sutherland et al. Journal of Biomedical Semantics 2013, 4:30 Page 5 of 10
http://www.jbiomedsem.com/content/4/1/30
increased cell number EquivalentTo has extra parts of
type towards some cell and inheres_in some
multicellular structure
The phenotype cell death defective (FBcv_0000425) pro-
vides an interesting example of the difficulty of defin-
ing widely-used terms based on their names alone. We
initially defined this class using programmed cell death
(GO_0012501) as:
EquivalentTo quality that inheres_in some
programmed cell death and qualifier some abnormal
But analysis of free text phenotype descriptions and
feedback from curators quickly made it clear that exist-
ing usage consisted of cases where the amount of cell
death occurring in one or more multicellular structures
was abnormal. In many cases it was not clear whether this
was due to a defect in regulation of cell death in the tissue
or due to a defect in the core processes of cell death. So,
we instead chose to define this class as a union:
EquivalentTo (quality that inheres_in some
programmed cell death and qualifier some abnormal)
OR (quality that inheres_in some regulation of
programmed cell death and qualifier some abnormal)
Lethality and stage
Following typical usage by Drosophila geneticists, we use
the term lethal (FBcv_0000351) to refer to a phenotype
in which, to a good approximation, all animals in a pop-
ulation do not survive to become mature adults. We
use partially lethal - majority die (FBcv_0000352) (AKA
semi-lethal) to refer to a phenotype where most but not all
animals die before mature adulthood:
lethal: A phenotype of a population that is the death of
all animals in that population at some stage or stages
prior to becoming a mature adult.
partially lethal - majority die: A phenotype of a
population that is the death a majority of animals in
that population prior to becoming a mature adult.
To record that animals die before mature adulthood
says nothing about the stages of development when death
occurs, but this information is of great practical impor-
tance. Geneticists working on stages before mature adult-
hood need to be able to find genotypes that survive to a
stage suitable for their experiments. Knowing the various
stages at which significant number of animals of a particu-
lar genotype die can also be useful in allowing researchers
to home-in on stages to characterise for defects.
FlyBase historically recorded information about the
stages of death due to specific genotypes in a semi-
controlled form by combining terms like lethal and semi-
lethal with terms from an ontology of developmental
stage. However, the semantics of these combinations were
never codified and so the resulting annotations in FlyBase
were not reliably useful for queries about the stages at
which death occurs.
We devised a set of phenotype terms, with a formal
semantics in OWL, for recording and reasoning about the
stages at which death occurs in a population. Our aim was
a system that separated assertions about the percentage of
animals dying at various stages, which could apply to pop-
ulations with significant adult survivors, from use of the
term lethal, which always refers to the lack of survival to
mature adulthood.
These phenotype terms are defined using a combi-
nation of four elements. First we define a population
of Drosophila using a term from the DAO, organism
(FBbt_00000001) for an individual member of the species.
population of Drosophila EquivalentTo
population
that (has_member some organism)
and (has_member only organism)
To define the stage or age of members of a population,
we use a set of terms for life stages from the Drosophila
stage ontology (http://purl.obolibrary.org/obo/fbdv.owl)
(see Figure 2A) along with a set of relations and axioms
for reasoning about relative timing based on a subset of
the Allen Interval Algebra [24]. For example, we can refer
to a population of Drosophila in which all members are
younger than the third instar larval stage using:
population of Drosophila
that has_member some (organism and ((has_age some
(precedes some third instar larval stage))))
and (has_member only (organism and (has_age some
(precedes some third instar larval stage))))b
Finally, we use an OWL data property, has_increased_
mortality_rate, to record the overall percent mortality
rate, excluding the wild-type death rate for the stage in
questionc. An alternative approach would be to use a data
property designed to record the penetrance of any phe-
notype. But there is, to our knowledge, no logically sound
way to link percent penetrance recorded using an OWL
data property to a specific phenotype. In contrast, using
has_increased_mortality allows us to specify a pheno-
type and its penetrance in a single assertion, so linking the
two is not an issue. Expressing death rates as an increase
over wild-type allows this system to be used to define
the phenotypic class short lived (FBcv_0000385) - which
needs to cover stages of life for which wild-type death
rates will be high.
The root term increased mortality (FBcv_0002004) is
defined, without any stage restriction, as:
Osumi-Sutherland et al. Journal of Biomedical Semantics 2013, 4:30 Page 6 of 10
http://www.jbiomedsem.com/content/4/1/30
Figure 2 Lethal phase phenotypes.When do they die? - classes for recording and reasoning about the timing of death in lethal phenotypes. -
Panel A shows the temporal relationships between Drosophila life stages from the Drosophila stage ontology. The P icon stands for
immediately_preceded_by, which corresponds to the Allen relation meets. Panel B shows a set of lethal phenotype terms prior to
auto-classification. Panel C shows the same set of classes after auto-classification using the HermiT reasoner.
A phenotype that is an increase in the rate of death in
a population at any any stage of life (during
development or adulthood), over the rate seen in a
wild-type control population.
It has a formal definition that specifies a minimum
increase in mortality of 5%:
increased mortality EquivalentTo (
has_increased_mortality_rate some short [>=5]
and inheres_in some population of Drosophila)
increased mortality SubClassOf phenotype
For this modelling approach to work, we need a hard,
non-zero cut-off and chose 5% as a reasonable figure
based on community usage. These hard percentage cut-
offs are considered rules-of-thumb for annotators, rather
than strict rules as sometimes only qualitative assertions
are available. Similarly, we use a cutoff of 98% for defin-
ing lethal to take into account that this term is used
even when there are rare escaper animals that survive to
adulthood:
lethal EquivalentTo (
has_increased_mortality_rate some short[<= 98])
that inheres_in some (
population of Drosophila
that has_member some (organism that has_age some
(precedes some mature adult stage))
and has_member only (organism that has_age some
(precedes some mature adult stage )))
For each of the major life stages, we define a term for
recording that some animals (>= 5%) die during that stage
and another for recording that some animals die before
the end of that stage. We define similar pairs of terms for
recording that most (>= 50%) or that all (>= 98%) die
during or before the end of a specified stage.
We also define terms for partial lethality:
partially lethal - majority die EquivalentTo (
has_increased_mortality_rate some short[>50, <=
98]) that inheres_in some ( population of Drosophila
that has_member some (organism that has_age some
(precedes some mature adult stage)) and
has_member only (organism that has_age some
(precedes some mature adult stage )))
The resulting list of terms is completely flat prior
to reasoning, but forms a deeply nested classification
following OWL reasoning. Figure 2 shows subclasses of
the term lethal before and after reasoning, illustrating
a small part of this inferred classification. Simply group-
ing annotations using the class hierarchy generated by
classification with an OWL reasoner, one can query for
genotypes that cause all (>=98%) animals to die before
some specified stage, or significant numbers of animals
to die before or during some specified stage. This allows,
Osumi-Sutherland et al. Journal of Biomedical Semantics 2013, 4:30 Page 7 of 10
http://www.jbiomedsem.com/content/4/1/30
for the first time, accurate grouping of DPO annotations
based on the stage of lethality.
We are in the process of converting our existing
annotation set of over 30,000 lethal and semi-lethal
phenotype assertions to the new system, thus improv-
ing the accuracy and usefulness of this entire data-set.
Over 17000 of these assertions involve some specifica-
tion of the stage of lethality and of these around 13000
proved amenable to scripted migration to the new sys-
tem. This leaves around 4000 requiring manual review
of free text phenotype descriptions before a sensible
choice can be made about how to annotate using the
new system.
Discussion
Formal definitions in the DPO mostly follow patterns
established for other phenotype ontologies [10-12]. In
the DPO, we assert as little classification as possible
and rely on a reasoner to infer classifications. These
inferred classifications are instantiated in the -simple
release versions (see Table 1). Where classifications
are asserted it is because the DPO or external ref-
erenced ontologies currently have insufficient formali-
sation to infer them. This approach is in contrast to
most other phenotype ontologies which have kept their
asserted classifications largely in-tact, although reasoning
has been used to assess the validity of these assertions
[25].
The biggest divergence between DPO and other phe-
notype ontologies is the system for specifying mortality
rates during development. The Human Phenotype Ontol-
ogy (HPO) [26], the Mammalian Phenotype Ontology
(MPO) [27] and the (nematode) Worm Phenotype Ontol-
ogy (WPO) [28] all include terms for recording the stage
of death. Some of these terms have formal definitions, but
these are not useful for reasoning about the amount or
timing of death in populations. One example of increased
clarity that adoption of our system in other phenotype
ontologies would bring is that is that it would make
explicit the different uses of the term lethal by differ-
ent communities of biologists. For example, in contrast
to the specialized meaning this term has to Drosophila
biologists, lethal in the worm phenotype ontology covers
increased mortality at any point in the life cycle including
mature adulthood. It could therefore be formally defined
following the pattern used for increased mortality in the
DPO.
The only other ontological framework for recording
mortality rates that we are currently aware of, an
impressively detailed and well-axiomatised proposal from
Sanatana and colleages [29], is not suited to our needs:
it is based on a different upper ontology to the DPO; it
assumes that death is due to injury or disease; and it does
not include axioms for reasoning about relative timings.
The definition pattern we propose in this system ismod-
erately complicated compared to the other formalisms
we use. Adding new terms using these patterns manu-
ally is more laborious and potentially error-prone than
for simpler formalisms. But the pattern is highly stereo-
typed and each definition only contains two elements
that change: the age of animals the population referred to
and the proportion of animals in that population that die
(excluding the wild-type death rate). It would therefore
be easy to specify a template-based system for creating
new terms along the lines of the TermGenie system devel-
oped by the Gene Ontology (http://go.termgenie.org/).
Another potential drawback is the scaling of reasoning
time as the ontology grows. The formalisation uses ele-
ments, such as inverse object properties and universal
quantification (only), that are outside the EL profile of
OWL2 [30]. As a result, it is not possible to completely
classify the DPO with fast, scalable, concurrent reason-
ers available for OWL2EL such as ELK [31]. However, at
the current scale, classification is sufficiently fast, under
40 seconds with the DL reasoner HermiT (http://www.
hermit-reasoner.com/), that it can be run frequently dur-
ing ontology development.
Future work
Efforts to increase the number of formal definitions
are ongoing. We have deliberately taken a conservative
approach to adding formal definitions specifying neces-
sary and sufficient conditions for class membership. It
is important to guard against applying simple definition
patterns that make membership of a class overly broad,
leading to serious errors in annotation grouping and
query results. As a result, 23% of terms in the DPO still
lack formal definitions specifying necessary and sufficient
conditions for class membership.
Among these are phenotypic classes such as touch sen-
sitive that group phenotypes according to performance in
some assay but are agnostic about the underlying etiol-
ogy. In other cases, the etiology is clear, but the phenotype
is still hard to formalize. Classic segmentation pheno-
types are a good example of this. A gap phenotype is
defined as:
Embryonic/larval segmentation phenotype that is the
complete loss of a contiguous stretch of 2 or more
segments.
We can record that a defect in the process of segmen-
tation is a necessary condition for classification as a gap
phenotype, but defining additional clauses for a com-
plete set of necessary and sufficient conditions for class
membership is much more challenging. We have also,
so far, avoided the challenge of defining complex phe-
notypes that have multiple features, such as the Minute
Osumi-Sutherland et al. Journal of Biomedical Semantics 2013, 4:30 Page 8 of 10
http://www.jbiomedsem.com/content/4/1/30
(FBcv_0000443) phenotype, which combines slow devel-
opment and short bristles.
Conclusions
The presence of textual definitions for all terms in the
DPO ensures the accuracy of future curation with this
ontology both by FlyBase and by any other group who use
it. The process of composing both textual and formal def-
initions for DPO terms has involved extensive analysis of
existing annotations. As a result of this, we have improved
the DPO to more closely fit curator need, and improved
the existing annotation set to be more consistent and
coherent.
Composing formal definitions for terms in the DPO
using high-quality, external ontologies, such as the GO,
has allowed us to leverage classification and other for-
malisations in these ontologies to classify phenotypes. As
a result, 85% (258/305) classifications are inferred rather
than asserted. This has resulted in much more accurate
and complete grouping of phenotype annotations using
the DPO. For example, using the old manual classification,
a query of the current FlyBase CHADO database [32] for
stress response defective (FBcv_0000408) phenotypes finds
only 344 phenotypes (481 alleles), whereas with the latest
DPO release it finds 859 phenotypes (910 alleles).
The formalisation presented here increases the possi-
bilities for sophisticated and accurate queries to be made
against the very large, rich dataset of DPO annotations
curated and maintained by FlyBase. One way to do this
is to pipe the results of OWL queries of the DPO into
SQL queries of the open FlyBase CHADO SQL server
maintained by FlyBase. A guide for how to access DPO
annotated phenotypes in the FlyBase CHADO database,
including sample SQL queries, can be found at https://
sourceforge.net/p/fbcv/wiki/chado_query_guide/. A sim-
ilar approach has recently been successfully used to pre-
dict gene function from phenotype annotation in FlyBase
and other model organism databases [33]. Similar formali-
sations for other phenotype ontologies have been success-
fully used for cross-species prediction of gene function
and to search for disease models [10,16,34].
So far, the DPO has only been used by FlyBase, but
it is freely available under an open source license, and
there is no reason that it could not be used more widely
and extended to cover a broader range of phenotypes in
collaboration with interested parties.
Methods
The DPO was largely developed in OBO format as part
of a range of controlled vocabularies sharing the FBcv
ID namespace. It still shares this ID namespace but is
now available as a separate file. Most development has
used OBOEdit, but with continuous conversion to OWL
during development so that the results can be checked,
browsed and queried with an OWL reasoner. Conver-
sion to OWL and import of modules from from other
ontologies was done using the OBO Ontology release
Tool (OORT) (https://code.google.com/p/owltools/wiki/
OortIntro) under the control of a continuous integra-
tion (CI) server which was also used to trigger a suite of
Perl syntax checking and derivation scripts. As a result,
each commit to our ontology development repository
triggered syntax and consistency checking, rolled new tex-
tual definitions and generated various flavours of DPO
file in OBO and OWL. The recent introduction of data-
properties and universal quantification to our formalisa-
tion has prompted a shift to develop some components
of the DPO in OWL - with the different component files
being knitted together by OORT during CI. We antici-
pate that development will move fully to OWL in the near
future.
Our module generation strategy works as follows: For
every term from an external ontology used in a DPO
axiom we import all terms and axioms on paths to root
from a pre-reasoned version of the external ontology using
OORT.
See Table 1 for URLs for accessing DPO files, terms and
information.
Conventions used in this paper: All OWL entities in are
identified in free text using their label in italics (for classes)
or bold (for Object Properties), followed by their OWL
short form ID in brackets. Following the OBO foundry
ID standard (http://www.obofoundry.org/id-policy.shtml)
a full URI can be generated by prepending http://purl.
obolibrary.org/obo/ to his ID. In most cases this URI will
resolve to OntoBee (http://www.ontobee.org/), return-
ing XML if accessed programatically. OBO IDs can be
derived by converting the underscore in a short-form
ID to a colon. All formal axioms are expressed in OWL
Manchester syntax (OWL-MS). OWL-MS keywords are
italicised. Object properties (relations) are in bold. The
names of OWL entities (e.g. classes, object properties) are
quoted only if they contain spaces.
Allen interval algebra: For reasoning about relative tim-
ing, we use relations based on a subset of the Allen interval
relations: precedes (p), preceded by (P), met (m), met
by (M), during (d) starts (e), finishes (f ). p inverseOf P, m
inverseOf M. Transitive properties: M, P, d. Key axioms
from the Allen composition table are represented using
OWL property chains.
Endnotes
ahas extra parts of type is a PATO relational quality.
We use this relational property in combination with the
relation towards. It would be simpler to use a single
relation in place of this combination, but we use this
pattern order to conform to standards established for
other phenotype ontologies.
Osumi-Sutherland et al. Journal of Biomedical Semantics 2013, 4:30 Page 9 of 10
http://www.jbiomedsem.com/content/4/1/30
bAge is a non-rigid property - individuals retain their
identity when it changes. Using such properites has some
modelling disadvantages [35]. For example, it is not
possible in OWL to both track the identity of individuals
as they age, and to classify them appropriately under
classes whose formal definition includes an age
restriction.
cThe range of has_increased_mortality_rate is a
whole number (datatype short) between 0 and 100
inclusive. It would make more sense to allow fractional
percentage values, but current limitations of reasoners
exclude this option.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
GVG proposed the initial set of formalisations, except those for recording the
timing of lethality. These were then refined by DOS in consultation with
FlyBase genetic curators (SJM, GM, PAM, LP and RS) and NB. Formal definitions
of terms for recording the timing of lethality were designed by DOS in
consultation with FlyBase genetic curators. SJM, GM, PAM, LP, RS and NB all
made significant contributions to the textual definitions of many terms and to
validating the structure of the ontology. Their work also ensured that the new
definitions complied with current and past usage. KF played a critical role in
retrofitting phenotype annotations in FlyBase following changes to the DPO.
This paper was primarily written by DOS with edits and suggestions from all
other authors. All authors read and approved the final manuscript.
Acknowledgements
We are heavily indebted to Michael Ashburner for his pioneering work on
earlier versions of the DPO and the various ontologies we use for formalisation
(the Cell Ontology, Drosophila Anatomy Ontology and the Gene Ontology.)
This work would also not have been possible without the FlyBase Consortium,
which provided the infrastructure and context, and which generates and
maintains the massive annotation set of Drosophila Phenotypes used in the
work described here.
Funding
George Gkoutos work on this project was funded by a BBSRC grant:
BBG0043581. FlyBase support for this project was provided by an NHGRI / NIH
grant HG000739 (W. Gelbart, Harvard University, PI, NHB, coPI).
Author details
1FlyBase, Department of Genetics, University of Cambridge, Downing Street,
Cambridge, UK. 2Department of Genetics, University of Cambridge, Downing
Street, Cambridge, UK. 3Gurdon Institute & Department of Physiology,
Development and Neuroscience, University of Cambridge, Tennis Court Road,
Cambridge, UK. 4The Biological Laboratories, Harvard University, 16 Divinity
Avenue, Cambridge, MA, USA.
Received: 1 July 2013 Accepted: 11 October 2013
Published: 18 October 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Hassanpour et al. Journal of Biomedical Semantics 2013, 4:14
http://www.jbiomedsem.com/content/4/1/14RESEARCH Open AccessA semantic-based method for extracting concept
definitions from scientific publications: evaluation
in the autism phenotype domain
Saeed Hassanpour1*, Martin J OConnor1 and Amar K Das2Abstract
Background: A variety of informatics approaches have been developed that use information retrieval, NLP and
text-mining techniques to identify biomedical concepts and relations within scientific publications or their
sentences. These approaches have not typically addressed the challenge of extracting more complex knowledge
such as biomedical definitions. In our efforts to facilitate knowledge acquisition of rule-based definitions of autism
phenotypes, we have developed a novel semantic-based text-mining approach that can automatically identify such
definitions within text.
Results: Using an existing knowledge base of 156 autism phenotype definitions and an annotated corpus of 26
source articles containing such definitions, we evaluated and compared the average rank of correctly identified rule
definition or corresponding rule template using both our semantic-based approach and a standard term-based
approach. We examined three separate scenarios: (1) the snippet of text contained a definition already in the
knowledge base; (2) the snippet contained an alternative definition for a concept in the knowledge base; and (3)
the snippet contained a definition not in the knowledge base. Our semantic-based approach had a higher average
rank than the term-based approach for each of the three scenarios (scenario 1: 3.8 vs. 5.0; scenario 2: 2.8 vs. 4.9; and
scenario 3: 4.5 vs. 6.2), with each comparison significant at the p-value of 0.05 using the Wilcoxon signed-rank test.
Conclusions: Our work shows that leveraging existing domain knowledge in the information extraction of
biomedical definitions significantly improves the correct identification of such knowledge within sentences. Our
method can thus help researchers rapidly acquire knowledge about biomedical definitions that are specified and
evolving within an ever-growing corpus of scientific publications.
Keywords: Knowledge acquisition, Ontologies, Rules, Biomedical definitions, Autism phenotypesBackground
Biomedical knowledge is growing rapidly, and the majority
of it is in an unstructured form in text. Researchers face
a number of difficulties when trying to find relevant
knowledge in this flood of information and to formalize it
in a computational form for applications ranging from
annotated publication repositories to automated decision
support. In extracting relevant knowledge from scientific
publications, the initial problem is finding articles relevant
to a particular domain-related query. A secondary problem
is to identify the portion of text within a retrieved* Correspondence: saeedhp@gmail.com
1Stanford Center for Biomedical Informatics Research, Stanford, CA 94305,
USA
Full list of author information is available at the end of the article
© 2013 Hassanpour et al.; licensee BioMed Ce
Creative Commons Attribution License (http:/
distribution, and reproduction in any mediumarticle that contains relevant domain knowledge. Prior
information extraction approaches in the biomedical
domain, such as GoPubMed [1] and Textpresso [2],
have shown that the use of pre-existing knowledge,
encoded as class hierarchies, can address these two
challenges. These past semantic-based methods, however,
fall short in resolving a third problem: helping users identify
specific instances of structured domain knowledge, not just
the presence of biomedical concepts, within relevant text.
We have addressed this problem in our efforts to assist
the information extraction needs of mental health experts
who are developing a knowledge-based catalog of autism
phenotypes [3]. Such phenotype concepts are represented
as classes within a domain ontology and defined more
precisely as rules expressing numeric or temporal cut-offsntral Ltd. This is an Open Access article distributed under the terms of the
/creativecommons.org/licenses/by/2.0), which permits unrestricted use,
, provided the original work is properly cited.
Hassanpour et al. Journal of Biomedical Semantics 2013, 4:14 Page 2 of 10
http://www.jbiomedsem.com/content/4/1/14of measurements on standardized diagnostic tests [3]. The
goal of our efforts is to provide an approach that can help
researchers accurately identify existing phenotype
definitions within text or facilitate the acquisition of
knowledge about newly published phenotype definitions.
Assisting clinical and genetics researchers acquire and
maintain such rule-based classifications of phenotypes can
facilitate their cataloging, comparison, and validation
and ultimately enable the use of standardized biomedical
definitions for robust, reproducible phenotype-genotype
analyses.
In resolving this problem, we must address a number of
challenges in information extraction, which we illustrate
using following verbatim examples of phenotype definitions
in two snippets taken from the same publication by
Hus et al. [4].
Snippet Example 1
One construct commonly used to stratify samples is
age of language acquisition, based on age of first
words or phrases. Delayed language is defined on the
ADI-R by age of first words ? 24 and age of first
phrases ? 3336 months. 
Snippet Example 2
Language Acquisition Groups defined based on
ADI-R items 9 (Age of First Words) and 10
(Age of First Phrases). Individuals were grouped as
follows:
 NDW (not delayed words): acquired words
? 24 months
 DW (delayed words): acquired words > 24 months
 NW (no words): no words at time of ADI-R
 NDP (not delayed phrases): acquired phrases
? 33 months)
 DP (delayed phrases): acquired phrases > 33 months)
 NP (no phrases): no phrases at time of ADI-R)
The first snippet appears in the introductory
section of the article, whereas as the second snippet
appears in the articles Methods section. Both
contain multiple definitions of autism phenotypes
related to language acquisition, such as delayed words
and delayed phrases, which are based on the autism
diagnostic instrument, ADI-R. The intertwined defini-
tions of multiple phenotypes within a single snippet
indicate why methods using pattern-based concept
recognition within a single sentence, such as used by
Textpresso, or textual entailment in the form of a concept
is defined as would be limited in identifying such
complex concept definitions.In this paper, we have focused on the extraction of
phenotype definitions from scientific articles and their
acquisition as Semantic Web Rule Language (SWRL) [5]
rule statements in pre-existing Web Ontology Language
(OWL) [6] ontologies. In our work, autism phenotypes
are categorized as an OWL class hierarchy and defined
by a set of tests and measurements in SWRL rules.
Considering the example of Delayed Words phenotype,
which is defined in the above two text snippets, Figure 1
shows this phenotype and its relationship with other
phenotypes in a small part of the autism ontologys class
hierarchy. As can be seen in this figure, Delayed Words
phenotype is the direct descendent of Status of Age of
Words and indirect descendent of Language Acquisition
and Autism Phenotype Level concepts. Delayed Words
phenotype SWRL rule definition specifies this phenotype
according the snippet example 2 based on the criteria
from definition ADI-R test. This rule indicates that if a
child does not acquires the ability to speak words by the
age of 24 months or earlier, then there is a delayed devel-
opment in word acquisition and asserts this finding in the
record of the subject who had that ADI-R questionnaire
completed for him or her.
Delayed Words phenotype SWRL rule definition
ADI-R(?a) ^ adi-r2003:ADI_2003_acqorlossoflang_
aword(?a, ?wordage) ^ swrlb:greaterThan(?wordage,
24) ^ adi_r2003:SubjectKey(?a, ?subjectID) ^ swrlx:
createOWLThing(?phenotype, ?subjectID) ? Delayed
word(?phenotype) ^ autism-core:subject_has_quality_
or_disposition(?subjectID, ?phenotype)
In prior work, we have presented semantic-based infor-
mation retrieval that uses previously encoded knowledge
about a domain, specifically a domain ontology and rule
base, to identify papers relevant to phenotypes and to
extract snippets of text most likely to contain their defini-
tions [7,8]. In this paper, we present a novel semantic-
based approach to identify which exact definition or
related definition exists within a returned snippet of text
in an article. Our approach must allow the knowledge
base developer to handle three different scenarios in infor-
mation extraction. In the first scenario, the snippet con-
tains one or more rule-based definitions that are
already encoded within the domain knowledge. In this
situation, we provide the developer a set of concepts
and their rules and allow the developer to simply as-
sociate the new text with the existing rule. In the sec-
ond scenario, the concept exists with the domain
ontology but the criteria used for defining that concept
differ from that in the snippet. For example, there is
a slight difference in the cut-off used in defining the
concepts of delayed words and delayed phrases in the
Language 
Acquisition
Status of Age of  
Words
Status of Age of  
Phrases
Delayed Words No Words Delayed Phrases No Phrases No Delayed PhrasesNo Delayed Words 
Restrictive & 
Repetitive Behavior
Savant Skill
Autism Phenotype 
Level
Figure 1 A part of autism phenotype class hierarchy. A portion of the class hierarchy showing the encoding of Delayed Words phenotype
and other phenotype concepts defined in the two example snippets in the Background section.
Hassanpour et al. Journal of Biomedical Semantics 2013, 4:14 Page 3 of 10
http://www.jbiomedsem.com/content/4/1/14example snippets. We again provide the developer a
set of concepts and their rules and permit the user to
modify an existing rule to create a new alternative
rule for that concept that matches the definition
within the snippet. Finally, in the third scenario, the
text contains a concept definition that has not been
previously encoded within the domain knowledge. In
this scenario, we present the developer a set of
existing concepts that may be related, and assume
that one of the existing rules for these concepts can
be used as a template to acquire the rule for the new
concept. Of note, all three of these scenarios may exist
within a single snippet of relevant text.
In this paper, we show how we address the information
extraction challenge of each of these three scenarios using
a single vector-space modeling approach that incorporates
terms and their weights based on previously encoded
domain knowledge. In each scenario, we use theFigure 2 Comparing semantic- and term-based methods in the first s
for the semantic- and term-based methods.vector-space model to find the set of concepts and
their rules that most closely match the biomedical
definitions within the given text. We compare our
semantic-based approach with a more conventional
term-based mechanism and show that incorporating
domain knowledge significantly improves the relevance of
the ranked results. We have evaluated our framework by
using it to extract autism phenotype definitions from text.
Although we presented this work in the domain of autism
phenotyping, our techniques do not depend on this
particular domain, and are not restricted to OWL
and SWRL frameworks. Our method is applicable to
other domains and any structured knowledge format
that consists of class hierarchies and rules.
Related work
Efforts to automatically acquire rule-like knowledge have
a long history in computer science and informaticscenario. Average ranks of the correct phenotypes found in a snippet
Hassanpour et al. Journal of Biomedical Semantics 2013, 4:14 Page 4 of 10
http://www.jbiomedsem.com/content/4/1/14research. The association rule-mining field, in particular,
has developed an extensive array of such techniques [9].
The general aim is to discover significant relationships
between variables in structured data and to encode these
relationships as rules. The expansion of online information
repositories has steered work towards extracting knowledge
from less structured data sources. In the biomedical
domain, the availability of a large number of abstracts
and full-text publications from sources like PubMed
has provided an impetus to the development of new
techniques. Early work focused on automatically classifying
the subjects of papers to help guide searches in particular
domains. Many of these efforts have used ontologies
in the key role of providing structured terminologies
for this classification process, or indeed have extracted
ontologies themselves [10-12]. In some cases, domain
ontologies provide an initial controlled vocabulary for
identifying terms.
More recent work has employed natural language
processing techniques to infer relationships between
the concepts described in a corpus. For example,
Rinaldi et al. [13] described a method for extracting
interactions between proteins from publications. Using
related techniques, researchers have begun to attempt
to automatically derive ontology hierarchies from text
by extracting domain terms from a corpus and finding
pairwise relationships between them [14-17]. Again, a
variety of techniques are used to build these hierarchies:
(1) documents are analyzed for syntactic patterns that
indicate relationships between terms; (2) template-based
approaches are used to describe syntactic patterns,
which are then used to find relationships between terms;
and (3) statistical methods are used to detect term
co-occurrence, which can often indicate relationships.
In contrast, acquiring domain knowledge in the form of
rules is more challenging because the logical relationships
that can be modeled as rules can be significantly more
complex [18]. Instead of simply detecting relationships
between concepts pairs, the method must automatically
link a series of these relationships together to build
composite requirements, which can then be encoded as
rules. Some work in rule acquisition used statistical
methods to extract simple term-to-term entailment rules
from text [19]. However such simple rules are very limited
in modeling most typical domain knowledge. A more
elaborate variant of this approach attempts to extract
first-order Horn clauses from the text [20]. This
method needs to be applied on a large corpus of text
to gather statistical evidences to discover rules. Related
efforts have attempted to extract first order logic rules by
using machine learning methods [21]. These rules are
learned from a set of positive and negative examples
therefore are not suitable for use on free text. Also, rule
editors such as SemEx [22], provide the functionalityto annotate text with common business terms to help
put together simple business rules in a semiautomatic
manner.
Only a small number of approaches using ontology-
based rule extraction methods are described in the
literature. Duboue and McKeown described a system
for capturing content selection rules from text that
identify parts of a corpus that are relevant to a
certain topic [23]. The system used a frame-based
knowledge representation format to drive statistical
methods to produce these types of rules from short
segments of user-supplied text. This text is assumed
to contain relevant rule-like information. Manine
et al. [24] presented an approach for acquiring gene
interaction rules from text, which were then encoded
using ontologies. The approach used an existing
ontology as input to an inductive logic algorithm,
which used it to learn inference rules from pre-selected
text. Park and Lee [25] developed an ontology-based
method to extract rules semi-automatically from web
documents. Like other rule acquisition approaches,
the method required an existing domain ontology and
manual selection of relevant web pages as method
input. This approach used very basic WordNet-based
NLP techniques, so was limited in its ability to handle
complex text. Recent work by the authors involved
automatically extracting car rental requirements from
online text by using a domain knowledge base
encoded in ontologies [26]. However, this method
required a manually derived ontology to capture
almost all domain terms, so it was not immediately
suitable for general-purpose use.Methods
The goal of our method is to find existing rules that
define the phenotype in a text snippet or to present
closely related rules or rule templates that can be used
to construct new concept definitions. Our method uses
three techniques. First, we build a model of the rules
and text snippets. Second, we use a similarity metric to
compute the relatedness of rules to a snippet. Third, we
find rules or rule templates that are the closest match to
the text definition.Modeling rules and snippets as vectors
Our basic approach in the method is to compute the
similarity between a snippet and the existing encoded
rules for phenotype definitions. To perform this computa-
tion, we represent both the snippets and rules as vectors
using a vector space model approach. Vector space model-
ing is widely used for information retrieval applications,
since it provides an efficient and scalable computational
approach for converting a text-based corpus to a standard
Hassanpour et al. Journal of Biomedical Semantics 2013, 4:14 Page 5 of 10
http://www.jbiomedsem.com/content/4/1/14mathematical format and then for searching for terms in
that corpus.
Standard modeling of text snippets
We use the standard vector space model to represent
each snippet as a vector in Euclidian space, where each
dimension of a vector corresponds to an individual term
in the overall corpus of snippets. If a snippet includes a
term, its value in the vector is given a non-zero weight for
that term. We use the most common method to compute
this weight, the term frequency-inverse document
frequency (tf-idf) weighting. In this weighting scheme,
weights increase proportionally to the number of the term
appearances in the document but are scaled down by the
frequency of the term in the corpus. The tf-idf formula
used in this work is:
wi;d¼tf i;dlog n=df ið Þ
Where wi,d is the tf-idf weight of term i in snippetd, tfi,
d is the frequency of term i in snippetd, n in the total
number of snippets in the corpus, and dfi is the number
of snippets that contain term i. Tf-idf weighs are
computed for every term in a snippet. As an example
we describe tf-idf weight computation for the term
language from the sample snippet 1 in the Background
section. The frequency of the term language in this snippet
is 2. Assuming there are 10 snippets in our snippet corpus
and 5 of them contain the term language, the tf-idf weight
for this term is 2log2 ? 0.6. In the modeling process, tf-idf
weights for all terms in a snippet are aggregated as the
vector presentation of that snippet.
Semantic-based modeling of phenotypes
A mathematical representation of the phenotype rules is
also necessary to determine its closeness to a snippet.
Again, we use the vector space modeling technique for
this purpose. The terms for ontology classes and properties
that are used in each rule are represented in the vector
space. As described previously [27], we use the hierarchies
of classes and properties in the ontology to extract
indirectly related concepts and incorporate them in
the vector representation. The weight of these related
concepts is determined by using a semantic similarity
metric, which exponentially decreases for each term based
on its distance from the phenotype in the hierarchy graph.
The formula for the semantic similarity is:
Sim C1;C2ð Þ¼ 2?ShortestPath
Here, Sim(C1, C2) is the semantic similarity between
concepts C1 and C2, and ShortestPath is the minimum
distance between them in the class hierarchy graph. For
example in our semantic-based modeling, the weight of
the term Words in the vector representation of phenotypeDelayed Words according to the class hierarchy in Figure 1
is 1.5. This is because the term Words has weight 1 in the
phenotypes name and weight 0.5 in the immediate
parents name in the class hierarchy, based on the above
semantic similarity formula.
Term-based modeling of phenotypes
To evaluate whether semantic based modeling provides
more relevant results in information extraction of
phenotype definitions, we use an alternative approach
for modeling phenotypes as vectors that is simply based
on the terms used in the rule for that vector. If the
rule contains a particular term, the value for that
term is based simply on its frequency within the rule.
The term-based approach does not include additional
terms found in the class hierarchy and does not use
semantic similarity measures as weights. As an example,
the representation of Delayed Words phenotype in the
term-based modeling is a vector with weights 1 for
the terms Delayed and Words.
Computing similarity
The information extraction task of identifying which
phenotype concepts and/or related rule-based definitions
may exist within a snippet can simply be done by meas-
uring the similarity of vectors for existing rules within
the encoded knowledge with the vector for a given snippet.
Since both snippets and phenotype rules are modeled as
vectors in a vector space, this similarity is calculated
using the standard cosine similarity calculation. The
cosine similarity for two vectors is the cosine of the
angle between them, and ranges from 0 for orthogonal vec-
tors to 1 for parallel vectors. The mathematical formula for
cosine similarity is:
Similarity a; bð Þ ¼ cos ?ð Þ ¼ a?bð Þ= ak k bk kÞð
Where a and b are two vectors in the Euclidean space,
? is the angle between them, a · b is their dot product,
and ||a|| and ||b|| are the magnitudes of the vectors.
Because cosine similarity is normalized by the sizes of
the vectors, it is stable and independent of input vectors
sizes and thus provides a robust measure of closeness.
Evaluation and results
A key component of our experiment is to demonstrate
whether including background knowledge improves the
ranking of the correct rules, alternative rules, or rule
templates for formalizing phenotype definitions in text.
We thus compared our semantic-based method to
the standard term-based method. As is common in
text-mining, the term-based method uses the frequency of
terms to model text and pre-existing rules as vectors. In
this method the correlations between text sections and
Hassanpour et al. Journal of Biomedical Semantics 2013, 4:14 Page 6 of 10
http://www.jbiomedsem.com/content/4/1/14existing phenotype rule definitions is used to present the
corresponding rules or rule templates to users to facilitate
knowledge acquisition. In the semantic-based method
related concepts to phenotypes are extracted from the
class hierarchy in the domain knowledge base, and their
relevance is quantified by a measure of semantic similarity.
These relevance weights are incorporated in the vector rep-
resentation of phenotypes, and similar to the term-based
method, the correlation between text sections and existing
rules is used to present the corresponding rules or rule
templates for rule extraction. Using the domain ontology
and the semantic similarity measure allow us to incorporate
the domain context or semantics in our phenotype
modeling, and is the basis for the naming of our
method. In this work formalized phenotype definitions
are equivalent to defining phenotype rules in the domain
knowledge base, and we investigated the relevance of our
resultsseparately in all possible scenarios.
Autism phenotype knowledge base
We evaluated our method for the task of developing and
maintaining a knowledge base in the domain of autism
phenotyping. The initial autism phenotype knowledge base
was composed of an ontology that was developed over a
two-year period by a group of mental health experts,
including the author AKD, and a knowledge-modeling
expert [3]. The ontology contains both a class hierarchy
that defines the terms and relationships among nine major
categories of autism phenotypes such as language, social
interaction, and behavioral abnormalities and a rule
base that defines these concepts as value restrictions
on research or clinical data collected through standardized
diagnostic instruments. The scope of domain knowledge
was initially defined by the experts who manually reviewed
26 relevant articles on autism phenotypes found in
PubMed [3]. The experts then used OWL and SWRL to
encode the phenotype definitions within the class hierarchy
and rule base. The resulting knowledge base contains 1726
classes and properties and includes 156 SWRL rules
that describe 145 unique phenotypic concepts for autism
patients. In the process of knowledge acquisition, the
domain experts identified the text sections within each
paper that contained one or more autism phenotype
definitions and associated the encoded rules to those
definitions in each snippet. In our evaluation, these
text sections are used as input snippets for our method,
and the expert-confirmed associations between rules and
snippets are considered to define the gold standard in
each case. The annotations generated by the domain
experts are not used in our method techniques and
are only used for evaluation.
Our method outputs the most related rules in the
existing knowledge base for each snippet, sorted by their
cosine similarity; we have undertaken an evaluation ofthese results based on the three possible scenarios that
we outlined in the Background section. We have focused
our analysis on 53 phenotype concepts that use multiple
criteria as part of their rule definition, providing the
most difficult information extraction challenge. As is
shown in the snippet examples 1 and 2 in the Background
section, often several phenotypes are defined in a single
snippet. Each snippet that is used in our evaluation
contains several phenotype definitions. In total 10 snippets
covered the definition of 53 phenotypes. The average
length of these snippets is 98 terms and the standard
deviation of their length is 42 terms.
Scenario 1: existing rule definition
In the first scenario, the phenotype defined in the snippet is
already present in the domain rule base. We evaluated this
scenario by comparing the rank of rules for phenotypes that
are known to exist within the snippet based on the domain
experts mapping. For instance, assuming Delayed Words
phenotype SWRL rule definition exists in the domain
knowledge base, the snippet example 2 in the Background
section is a snippet with an existing rule definition in the
knowledge base. The gold standard in this scenario is to
rank the existing rule definitions on the top of the re-
sult list. In our example Delayed Words phenotype SWRL
rule definition should be ranked on the top.
We investigated this scenario by considering 53
domain expert-specified complex phenotype definitions
in the autism phenotyping publications text. We then
computed the average rank of correct phenotype rule in
the returned sorted list of rule candidates as a measure
of methods accuracy. As can be seen in Figure 2, the
average correct rank for the semantic-based method is
better than the term-based method. The Wilcoxon signed-
rank test shows that this difference is significant with
p-value of 0.028. The average pairwise difference between
correct phenotypes for these two methods is 1.15.
Scenario 2: alternative rule definition
In this scenario, the exact definition of the phenotype
concept present in the snippet does not exist in the rule
base. Instead, the snippet defines a phenotype concept that
has a different definition from the one encoded in existing
rules for that concept. In this case, we undertook an evalu-
ation by creating a set of different knowledge bases in
which alternatively one of the existing rules for the con-
cepts that have multiple rule definitions was removed. We
then determined the rank of the alternative rules for the
phenotype concept known to exist within a snippet.
Delayed Phrases phenotype SWRL rule definition shows
the rule definition of Delayed Phrases phenotype according
the snippet example 2 in the Background section. As an ex-
ample of this scenario, Delayed Phrases phenotype defined
in the sample snippet 1 has a different criterion (age
Figure 3 Comparing semantic- and term-based methods in the second scenario. Average ranks of the alternative phenotypes found in a
snippet for the semantic- and term-based methods.
Hassanpour et al. Journal of Biomedical Semantics 2013, 4:14 Page 7 of 10
http://www.jbiomedsem.com/content/4/1/14of first phrases ? 3336 months) from Delayed
Phrases phenotype SWRL rule definition (age of first
phrases > 33 months). Assuming the corresponding
rule for the phenotype defined in the sample snippet
1 does not exist in the rule base, the gold standard is
to rank the alternative rule definitions on the top of
the result list. In our example Delayed PhrasesFigure 4 Comparing semantic- and term-based methods in the third
snippet for the semantic- and term-based methods.phenotype SWRL rule definition should be ranked on
the top.
The autism phenotype knowledge base contains 29
phenotype concepts with alternative rule definitions.
To compare the semantic-based and the term-based
methods, we computed the average rank for the cor-
rect results in this scenario. Figure 3 shows that thescenario. Average ranks of the rule templates for a concept in a
Hassanpour et al. Journal of Biomedical Semantics 2013, 4:14 Page 8 of 10
http://www.jbiomedsem.com/content/4/1/14average rank of the correct answer in the semantic-
based method is 2.8 and in the term-based method is
4.9. According to the Wilcoxon signed-rank test, p-
value is 0.032. The average pairwise difference for
ranks in these two methods is 2.8.
Delayed Phrases phenotype SWRL rule definition
ADI-R(?a) ^ adi-r2003:ADI_2003_acqorlossoflang_
aphrase(?a, ?phraseage) ^ swrlb:greaterThan(?phraseage,
33) ^ adi_r2003:SubjectKey(?a, ?subjectID) ^ swrlx:
createOWLThing(?phenotype, ?subjectID)? Delayed
phrases(?phenotype) ^ autism-core:subject_has_quality_
or_disposition(?subjectID, ?phenotype)
Scenario 3: matching rule template
In this scenario, the snippet defines a phenotype that
is not represented within the class hierarchy of the
ontology and thus has no corresponding definition as
a rule in the knowledge base. In this case, our method will
still provide a set of ranked rules for concepts in the
snippet, even though the actual concept does not exist in
the ontology. We expect that the developer will recognize
this situation by comparing the text with the returned
results. We provide the developer the option of using
a pre-defined template to enter the rule for the new
concept. These templates derive from our prior work
[28], in which we have found that SWRL rules can be
represented by syntactic signatures based on the types of
classes and properties they contain and their relationships.
In a rule signature rules elements are represented by their
types, and are grouped together if they are about a
common subject. For example, if a rule defines a
member from a class and assigns a property value to
that member, the classes and property are grouped
together as (CD) in the rule signature, where C represents
the class and D represents the data value property. Full
description of rule signatures requires close familiarity
with SWRLsyntax [28], which is off-topic in this paper.
However, in the case of the sample Delayed Words and
Phrases phenotypes SWRL rule definitions, even an inex-
perienced user can observe the similarity between the
rules structures and conclude that they have the same
template. The syntactic analysis of several large SWRL
rule bases indicated that they contained only a limited
number of syntactic signatures, which we showed could
be used as templates to acquire new rules. Our syn-
tactic analysis of the autism phenotype rules found
only 5 distinct signatures, and each of these mapped
to semantically similar types of rule definitions [28].
We exploited this knowledge in our evaluation strategy
for the third scenario. We created a set of 53 knowledge
bases in which we alternatively removed one of the 53
distinct phenotype concepts and its corresponding rules.For each snippet that contained one of the concepts
known to be missing in that version of the knowledge
base, we compared the rank of the rule template for the
returned rules with the correct rule template for the
definition of the missing concept. As an example of
this scenario, assuming Delayed Words phenotype
does not exist in the domain ontology and rule base,
the phenotype definition in the snippet examples 1
and 2 do not have the corresponding phenotype in
the domain knowledge base. The gold standard in this
scenario is to rank Delayed Phrases rule definition on
the top of the result lists for these snippets as the
rule with the similar template. Here, the goal is to
provide phenotype rules that have a similar structure
to the new phenotypes, so their syntactic templates can
facilitate the acquisition of the new definitions.
We compared the average rank for the correct
templates in this scenario, for the semantic and the
term-based method. As can be seen in Figure 4, the average
rank of the correct template is 4.5 in the semantic-based
method and it is 6.2 in the term-based method.
According to the Wilcoxon signed-rank test, the p-value for
the comparison is 0.003. The average difference between
correct template ranks between two methods is 1.7.
Discussion
For researchers wishing to identify and encode phenotype
definitions into formal knowledge, finding relevant articles
and the related text fragments are only the first steps. The
nextand most difficultstep is formally encoding
the definitions contained in these text fragments. The
current state-of-the art is simply to manually encode
new definitions using the retrieved text as a guide.
The ideal, of course, would be to automatically
acquire the phenotype definition from text. As we
showed in the Background section, the variety of ways
phenotypes can be defined in free text makes this
task very hard in practice. We thus have focused on
developing a method that at least partially automates
this authoring process and thus greatly assists users. This
paper outlines the approach that we have developed to
facilitate this knowledge acquisition process.
Our approach use semantic-based information retrieval
techniques to help users both to identify known defini-
tions of phenotypes in free text and to formalize the new
definitions of phenotypes present in the text. Our work
addresses the shortcomings of prior work where extracted
knowledge is largely in the form of concept hierarchies.
Biomedical knowledge about rule-based definitions, which
are commonly used for criteria-based diagnoses, cannot
be represented using such hierarchies. Our approach
combines domain rule bases and ontologies with NLP
techniques to capture this knowledge within vector
space modeling. We have compared this approach
Hassanpour et al. Journal of Biomedical Semantics 2013, 4:14 Page 9 of 10
http://www.jbiomedsem.com/content/4/1/14with a more conventional term-based mechanism and our
results have shown that incorporating domain knowledge
into the information extraction method significantly
improves the relevance of the results.
Our evaluation shows the text sections containing
phenotype definitions often include several other pheno-
types names and information without explicitly defining
them. These occurrences in addition to the ambiguity and
complexity of phenotype definitions in the form of free text
complicate the rule extraction even in the case of existing
phenotypes in the knowledge base. In the future, we are
planning to improve the accuracy of our method by text
understanding and natural language processing techniques.
A limitation of the work is that we have only evaluated it in
a single domain. We plan to further evaluate our approach
in a variety of biomedical domains. In particular, we will
evaluate the accuracy of our rule matching method with
clinical guidelines. These sources typically contain a signifi-
cant amount of proscriptive information, which is amenable
to a rule-like representation. The ultimate goal is to produce
a fully automated mechanism for finding and generating
rules from text. By combining our information extraction
approaches, we plan to create an overall workflow that
starts with an existing ontology containing a set of rules,
identify publication texts corresponding to new or related
rules in the domain, and automatically extract new rules.
Conclusions
The work described in this paper has demonstrated that
the use of formally encoded domain knowledge can dra-
matically improve information extraction methods.
Knowledge acquisition methods can also leverage this
formal knowledge to provide a set of fully or partially
automated strategies for generating new knowledge
from text. Ultimately, these methods can help scientists
to rapidly formalize the complex domain knowledge that
is emerging in published research findings. They can also
be applied to other information extraction challenges
where there is a need to accurately capture computer-
interpretable definitions, constraints, and policies that are
specified in text.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
All three authors were involved in the designing of the study, the analysis of
the results, and drafting the manuscript. SH implemented the experiment
and gathered the results. All authors read and approved the final manuscript.
Acknowledgements
The authors would like to thank Samson Tu, Lakshika Tennakoon, Richard
Waldinger and Joachim Hallmayer for their help on the ontology
development. Also, they would like to acknowledge support from the
National Database for Autism Research. This research was supported in part
by grants R01LM009607 and R01MH87756 from the National Institutes of
Health.Author details
1Stanford Center for Biomedical Informatics Research, Stanford, CA 94305,
USA. 2Geisel School of Medicine at Dartmouth, 46 Centerra Drive, Suite 330,
Lebanon, NH 03766, USA.
Received: 29 September 2012 Accepted: 4 July 2013
Published: 12 August 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Boyce et al. Journal of Biomedical Semantics 2013, 4:5
http://www.jbiomedsem.com/content/4/1/5
RESEARCH Open Access
Dynamic enhancement of drug product
labels to support drug safety, efficacy,
and effectiveness
Richard D Boyce1*, John R Horn2, Oktie Hassanzadeh3, Anita de Waard4, Jodi Schneider5,
Joanne S Luciano6, Majid Rastegar-Mojarad7 and Maria Liakata8,9
Abstract
Out-of-date or incomplete drug product labeling information may increase the risk of otherwise preventable adverse
drug events. In recognition of these concerns, the United States Federal Drug Administration (FDA) requires drug
product labels to include specific information. Unfortunately, several studies have found that drug product labeling
fails to keep current with the scientific literature. We present a novel approach to addressing this issue. The primary
goal of this novel approach is to better meet the information needs of persons who consult the drug product label for
information on a drugs efficacy, effectiveness, and safety. Using FDA product label regulations as a guide, the
approach links drug claims present in drug information sources available on the Semantic Web with specific product
label sections. Here we report on pilot work that establishes the baseline performance characteristics of a
proof-of-concept system implementing the novel approach. Claims from three drug information sources were linked
to the Clinical Studies, Drug Interactions, and Clinical Pharmacology sections of the labels for drug products that contain
one of 29 psychotropic drugs. The resulting Linked Data set maps 409 efficacy/effectiveness study results, 784
drug-drug interactions, and 112 metabolic pathway assertions derived from three clinically-oriented drug information
sources (ClinicalTrials.gov, the National Drug File  Reference Terminology, and the Drug Interaction Knowledge Base)
to the sections of 1,102 product labels. Proof-of-concept web pages were created for all 1,102 drug product labels
that demonstrate one possible approach to presenting information that dynamically enhances drug product labeling.
We found that approximately one in five efficacy/effectiveness claims were relevant to the Clinical Studies section of a
psychotropic drug product, with most relevant claims providing new information. We also identified several cases
where all of the drug-drug interaction claims linked to the Drug Interactions section for a drug were potentially novel.
The baseline performance characteristics of the proof-of-concept will enable further technical and user-centered
research on robust methods for scaling the approach to themany thousands of product labels currently on themarket.
Keywords: Regulatory science, Drug information services, Drug labeling, Linked data, Scientific discourse ontologies,
Drug interactions, Pharmacokinetics, Treatment efficacy, Treatment effectiveness, Comparative effectiveness research
Introduction
The drug product label (also called package insert)
is a major source of information intended to help
clinicians prescribe drugs in a safe and effective man-
ner. Out-of-date or incomplete product label informa-
tion may increase the risk of otherwise preventable
*Correspondence: rdb20@pitt.edu
1Department of Biomedical Informatics, University of Pittsburgh, Offices at
Baum, 5607 Baum Blvd, Pittsburgh, PA, USA
Full list of author information is available at the end of the article
adverse drug events (ADEs). This is because many pre-
scribers and pharmacists refer to drug product labeling
for information that can help them make safe prescrib-
ing decisions [1,2]. A prescribing decision might be
negatively affected if the label fails to provide infor-
mation that is needed for safe dosing, or to prop-
erly manage (or avoid) the co-prescribing of drugs
known to interact. Prescribing decision-making might
also be indirectly affected if 1) the clinician depends
on third-party drug information sources, and 2) these
sources fail to add information that is available in
© 2013 Boyce et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Boyce et al. Journal of Biomedical Semantics 2013, 4:5 Page 2 of 21
http://www.jbiomedsem.com/content/4/1/5
the scientific literature but not present in the product
label.
In recognition of these concerns, the US Federal Drug
Administration (FDA) Code of Federal Regulations (CFR)
Title 21 Part 201 Section 57 requires drug labels to
include specific information for FDA-approved drugs
[3]. Mandated information includes clinical studies that
support a drugs efficacy for its approved indications,
known pharmacokinetic properties, clearance data for
special populations, and known clinically-relevant drug-
drug interactions. Unfortunately, for each of these types
of information, product labeling fails to keep current with
the scientific literature. For example:
 Marroum and Gobburu noted deficiencies in the
pharmacokinetic information provided by product
labels, especially for drugs approved in the 1980s [1],
 Boyce et al. found that the product label provided
quantitative data on age-related clearance reductions
for only four of the 13 antidepressants for which such
data was available [4],
 Steinmetz et al. found that quantitative information
on clearance changes in the elderly was present in
only 8% of 50 product inserts that they analyzed, [5],
and
 Hines et al. noted drug-drug interaction information
deficiencies in 15% of the product labels for drugs
that interact with the narrow therapeutic range drug
warfarin [6].
We present a novel approach to addressing prod-
uct labeling information limitations such as those listed
above. The primary goal of this novel approach is to bet-
ter meet the information needs of persons who consult
the drug product label for information on a drugs efficacy,
effectiveness, and safety. The approach is based on the
hypothesis that a computable representation of the drug
effectiveness and safety claims present in product labels
and other high quality sources will enable novel methods
for drug information retrieval that do a better job of help-
ing drug experts, clinicians, and patients find complete
and current drug information than current search engines
and bibliographic databases.
Figure 1 is an overview of the system that we envi-
sion. Claims about drugs are currently present in sources
of drug information such as the drug product label,
studies and experiments published in the scientific lit-
erature, premarket studies and experiments reported in
FDA approval documents, and post-market data sources
such as drug effectiveness reviews and drug informa-
tion databases. Many of these sources are available, or
are becoming available, on the Semantic Web. Using
FDA product label regulations as a guide [3], a new
linked data set would be created that links claims present
in drug information sources available on the Seman-
tic Web to relevant product label sections. The linked
data set would create and automatically update claim-
evidence networks [7-11] to make transparent the moti-
vation behind specific claims. Customized views of the
linked dataset would be created for drug experts including
clinicians, researchers, and persons who maintain ter-
tiary drug information resources (i.e., proprietary drug
information products).
The objective of this paper is to report on our
pilot work that establishes the feasibility of the novel
approach and the baseline performance characteris-
tics of a proof-of-concept system. Because there is a
broad range of content written into product labels,
and the novel approach requires synthesizing research
from multiple areas of research, we have organized
this paper to report progress in three complementary
areas:
1. Linking relevant Semantic Web resources to the
product label: We describe a basic proof-of-concept
that demonstrates the Semantic Web technologies
and Linked Data principles [12,13] that we think are
necessary components of a full-scale system. The
proof-of-concept consists of a set of web pages
created using existing Semantic Web datasets, and
demonstrates one possible approach to presenting
information that dynamically enhances particular
product label sections.
2. First steps towards the automated extraction of
drug efficacy and effectiveness claims: Focusing on
drug efficacy and effectiveness studies registered with
ClinicalTrials.gov, we describe the methods and
baseline performance characteristics of a pilot
pipeline that automatically obtains claims from the
scientific literature and links it to the Clinical
Studies section of the product label for psychotropic
drugs.
3. A descriptive summary of challenges to the
automated claim extraction of metabolic pathways:
We provide a descriptive analysis of the challenges to
the automated identification of claims about a drugs
metabolic pathways in full text scientific articles. The
analysis is based on manual identification of these
claims for a single psychotropic drug.
Results
Linking relevant semantic web resources to the product
label
Twenty-nine active ingredients used in psychotropic drug
products (i.e., antipsychotics, antidepressants, and seda-
tive/hypnotics) that were marketed in the United States at
Boyce et al. Journal of Biomedical Semantics 2013, 4:5 Page 3 of 21
http://www.jbiomedsem.com/content/4/1/5
Figure 1 The general architecture of a system to provide dynamically enhanced views of drug product labeling using Semantic Web
technologies.
the time of this study were selected as the target for the
proof-of-concept.a These drugs were chosen because they
are very widely prescribed and a number of these newer
psychotropic drugs are involved in drug-drug interactions
[14]. Figure 2 shows the architecture of the proof-of-
concept system that we developed for these drugs. As the
figure shows, four data sources were used in the proof-
of-concept. One of the sources (DailyMed) contained the
text content of the three product label sections that were
the focus of this study (Clinical Studies, Drug Interactions,
and Clinical Pharmacology). The other three sources were
chosen because they contain rigorous scientific claims
that we expected to be relevant to pharmacists seeking
information about the efficacy, effectiveness, and safety
of a drug. These three resources, and the claims they
provided, were:
1. LinkedCT:b Drug efficacy and effectiveness studies
registered with ClinicalTrials.gov that have published
results (as indicated by an article indexed in
PubMed) [15,16]
2. National Drug File  Reference Terminology
(NDF-RT):c Drug-drug interactions listed as critical
or significant in the Veterans Administration
[17,18]
3. The Drug Interaction Knowledge Base (DIKB):d
Pharmacokinetic properties observed in
pharmacokinetic studies involving humans [19].
In order for the proof-of-concept to link claims from
these three sources to sections from the product labels
for the chosen drugs, we first implemented a Linked Data
representation of all product labels for the psychotropic
drugs used in our study. We constructed the Linked Data
set from the Structured Product Labels (SPLs) available
in the National Library of Medicines DailyMed resource.e
A total of 36,344 unique SPLs were transformed into an
RDF graph and loaded into an RDF store that provides a
SPARQL endpoint.f We refer to this resource as Linked-
SPLs throughout the remainder of this text. LinkedSPLs
contained product labels for all 29 psychotropic drugs in
this study.
We then created a separate RDF graph with map-
pings between product label sections and claims present
in the three drug information sources. This graph was
imported it into the same RDF store as LinkedSPLs. The
graph has a total of 209,698 triples and maps 409 effi-
cacy/effectiveness study results, 784 NDF-RT drug-drug
interactions, and 112 DIKB pathway claims to the sections
of 1,102 product labels.g Consideringmappings on a label-
by-label basis (see Listing 1), the graph has an average
of 50 mappings per product label (mean:50, median:50).
Twenty-four labels had the fewest number of mappings
(2), and two had greatest number of mappings (135).
Table 1 shows the counts for all mappings grouped by each
drug in the study. The next three sections provide more
detail on the specific mappings created for each product
label section.
Figure 2 The architecture of the proof-of-concept system described in this paper that demonstrates the dynamic enhancement of drug
product labels using Semantic Web technologies.
Boyce
etal.JournalofBiom
edicalSem
antics
2013,4:5
Page
4
of21
http
://w
w
w
.jb
iom
edsem
.com
/content/4/1/5
Table 1 Counts of product labels and all linked claims
Drug Number of product labels Number of VANDFRT Number of DIKB inhibits/substrate ClinicalTrials.gov Published results
for products containing DDIs found for of assertions with evidence studies involving the from ClinicalTrials.gov
the drug the drug found for the drug drug studies involving the drug
Significant Critical Evidence for Evidence against
Antidepressants
Amitriptyline 57 16 8 0 0 1 1
Amoxapine 2 15 8 0 0 0 0
Bupropion 111 7 4 2 0 5 44
Citalopram 85 25 9 2* 4* 4 25
Desipramine 15 16 10 0 0 0 0
Doxepin 32 15 9 0 0 0 0
Duloxetine 17 26 8 3 4 4 4
Escitalopram 20 13 3 4* 5* 6 9
Fluoxetine 90 51 14 2 0 8 22
Imipramine 19 18 10 0 0 1 4
Mirtazapine 55 2 5 4 9 1 22
Nefazodone 5 39 20 3 6 0 0
Nortriptyline 29 16 11 0 0 3 24
Paroxetine 60 33 11 2 0 3 40
Selegiline 11 2 47 0 0 1 1
Sertraline 74 28 8 2 0 3 27
Tranylcypromine 2 3 61 0 0 3 71
Trazodone 38 8 10 1 0 2 2
Trimipramine 2 17 10 0 0 0 0
Venlafaxine 66 21 6 3 3 2 2
Antipsychotics
Aripiprazole 15 4 0 2 13 3 3
Clozapine 9 29 2 3 1 3 9
Olanzapine 42 0 1 1 0 5 13
Quetiapine 33 8 0 1 9 4 9
Risperidone 71 13 0 2 1 23 70
Ziprasidone 22 54 23 2* 9* 1 6
Boyce
etal.JournalofBiom
edicalSem
antics
2013,4:5
Page
5
of21
http
://w
w
w
.jb
iom
edsem
.com
/content/4/1/5
Table 1 Counts of product labels and all linked claims (Continued)
Sedative Hypnotics
Eszopiclone 11 7 0 1 7 1 1
Zaleplon 24 0 0 1 1 0 0
Zolpidem 85 0 0 2 0 0 0
*Citalopram, escitalopram, and ziprasidone were each mapped to one claim for which there was both supporting and refuting evidence in the DIKB. Counts of product labels for each drug and claims that were linked to drug
product labeling from three Linked Data drug information sources.
Boyce et al. Journal of Biomedical Semantics 2013, 4:5 Page 6 of 21
http://www.jbiomedsem.com/content/4/1/5
Listing 1 The total number of claim mappings present in
the proof-of-concept RDF graph by drug product label
PREFIX poc:<http://purl.org/net/nlprepository/dynamic-spl-enhancement-poc#>
SELECT ?spl COUNT(DISTINCT ?mapping) WHERE {
{
## mappings for the Clinical Studies section ##
poc:linkedct-result-map ?spl ?mapping.
?mapping poc:linkedct-result-drug ?drug.
} UNION {
## mappings for the Drug Interactions section ##
poc:ndfrt-ddi-map ?spl ?mapping.
?mapping poc:ndfrt-ddi-drug ?drug.
} UNION {
## mappings for the Clinical Pharmacology section ##
poc:dikb-pk-map ?spl ?mapping.
?mapping poc:dikb-pk-drug ?drug.
}}
GROUP BY ?spl
ORDER BY ?spl
Automatic linking of study abstracts from ClinicalTrials.gov
to the Clinical Studies section
The Clinical Studies section of the product label could be
mapped to the abstract of at least one published result for
22 of the 29 psychotropic drugs (76%) (see Table 1). Seven
drugs (24%) were not mapped to any published result. The
largest number of mappings was for risperidone, with 70
published results mapped to 71 product labels. There was
a considerable difference between the mean and median
number of published results that were mapped when such
a mapping was possible (mean: 19, median: 9).
Automatic linking of VANDF-RT drug-drug interactions to the
Drug Interactions section
The Drug Interactions section of the product label could
be mapped to at least one NDF-RT drug-drug interac-
tion for 27 of the 29 psychotropic drugs (93%). Table 1
shows the counts for all published result mappings for
each drug in the study. The number of mappings to drug-
drug interactions labeled Significant in the NDF-RT (see
Section Methods for explanation) ranged from 2 (mir-
tazapine and selegiline) to as many as 54 (ziprasidone)
with a mean of 19 and a median of 16. For Critical drug-
drug interactions, the number of mappings ranged from
one (olanzapine) to 61 (tranylcypromine) with a mean of
13 and median of 9.
Table 2 shows the counts and proportion of linked
drug-drug interaction claims that were noted as poten-
tially novel to the Drug Interaction section of at least one
antidepressant product label. For these drugs, a poten-
tially novel interaction was an NDF-RT interaction that
1) was not mentioned in the Drug Interaction section of
a product label based on a case-insensitive string match,
and 2) was not listed as an interacting drug based on our
review (prior to the study) of a single manually-reviewed
product label for the listed drug (see Section Methods
for further details). At least one potentially novel interac-
tion was linked to a product label for products containing
each of the 20 antidepressants. The largest number of
potentially novel Significant interactions was for nefa-
zodone and fluoxetine (31 and 28 respectively), while
tranylcypromine and selegiline had the largest number of
potentially novel Critical interactions (33 and 23 respec-
tively). All of the Significant drug interactions mapped
to seven antidepressants (35%) were novel, while all of
the Critical interactions mapped to five antidepressants
(25%) were novel. These results are exploratory and it is
not known how many of the potentially novel interactions
are truly novel.
Automatic linking ofmetabolic pathways claims from the
drug interaction knowledge base to the Clinical
Pharmacology section
The Clinical Pharmacology section of the product label
could be mapped to at least one metabolic pathway claim
for 20 of the 29 psychotropic drugs (69%). Table 1 shows
the counts for all pathway mappings for every drug in the
study stratified by whether the DIKB provided supporting
or refuting evidence for the mapped claim. Thirteen of the
20 drugs that were mapped to pathway claims with sup-
porting evidence were alsomapped to claims with refuting
evidence. In most cases, these mappings were to different
pathway claims, as only three drugs (citalopram, escitalo-
pram, and ziprasidone) were mapped to individual claims
with both supporting and refuting evidence. Three path-
way claims had both supporting and refuting evidence,
40 pathway claims had only supporting evidence, and 69
claims had only refuting evidence.
Generation of web pagemashups
The mappings described above were used to gener-
ate web pages that demonstrate one possible way that
users could be presented with information that dynam-
ically enhances product label sections. A total of 1,102
web pages were generated by the proof-of-concept using
a version of LinkedSPLs that was synchronized with
DailyMed content as of October 25, 2012. The web pages
are publicly viewable at http://purl.org/net/nlprepository/
outfiles-poc.h Figures 3, 4 and 5 show examples of the
web pages generated by the proof-of-concept for the three
sections we chose to focus on.
First steps towards the automated extraction of drug
efficacy and effectiveness claims
It is important to note that, for drug efficacy and
effectiveness claims, the proof-of-concept implements
Boyce et al. Journal of Biomedical Semantics 2013, 4:5 Page 7 of 21
http://www.jbiomedsem.com/content/4/1/5
Table 2 Counts of potentially novel drug-drug interaction claims
Drug Number of VA-NDFRT DDIs Number of VA-NDFRT DDIs that were potentially novel
found for the drug to at least one product label. N ( % )
Significant Critical Significant Critical
Amitriptyline 16 8 8(50) 3 (38)
Amoxapine 15 8 11 (73) 4 (50)
Bupropion 7 4 5 (71) 3 (75)
Citalopram 25 9 5 (20) 4 (44)
Desipramine 16 10 16 (100) 6 (60)
Doxepin 15 9 15 (100) 9 (100)
Duloxetine 26 8 12 (46) 3 (38)
Escitalopram 13 3 3 (23) 1 (33)
Fluoxetine 51 14 28 (55) 8 (57)
Imipramine 18 10 18 (100) 6 (60)
Mirtazapine 2 5 1 (50) 1 (20)
Nefazodone 39 20 31 (80) 11 (55)
Nortriptyline 16 11 16 (100) 11 (100)
Paroxetine 33 11 15 (46) 5 (45
Selegiline 2 47 1 (50) 23 (49)
Sertraline 28 8 7 (25) 3 (38)
Tranylcypromine 3 61 1 (33) 33 (54)
Trazodone 8 10 8 (100) 10 (100)
Trimipramine 17 10 17 (100) 10 (100)
Venlafaxine 21 6 21 (100) 6 (100)
The number and proportion of VA NDF-RT drug-drug interactions that were noted as potentially novel to the Drug Interaction section of at least one antidepressant
product label. For these drugs, a potentially novel interaction was an NDF-RT interaction that was 1) not mentioned in the Drug Interaction section of a drugs product
label based on a case-insensitive string match, and 2) not listed as an interacting drug based on our review (prior to the study) of a single manually-reviewed product
label the listed drug.
Figure 3 A Clinical Study section from an escitalopram product label as shown in the proof-of-concept. In this example, an efficacy claim is
being shown that was routed from the abstract of a published result for study registered in ClinicalTrials.gov.
Boyce et al. Journal of Biomedical Semantics 2013, 4:5 Page 8 of 21
http://www.jbiomedsem.com/content/4/1/5
Figure 4 A Drug Interactions section from an escitalopram product label as shown in the proof-of-concept. In this example, several
Significant NDF-RT drug-drug interactions are being shown. The interaction marked as New to Section? was not found by manual inspection of a
single product label for an escitalopram drug product, nor by an automated case-insensitive string search of the Drug Interactions section of the
escitalopram product label.
only one of the two steps that are needed to imple-
ment a fully automated claim extraction process. While
the proof-of-concept retrieves text sources from which
drug efficacy and effectiveness claims can be extracted
(i.e., PubMed abstracts), these claims remain written in
unstructured text. We hypothesized that sentences con-
taining claims could be automatically extracted using a
pipeline that processed the text of the abstracts returned
from the LinkedCT query using an algorithm that auto-
matically identifies sentences stating conclusions. To test
the precision and recall of this approach, we first cre-
ated a reference standard of these conclusion claims for a
randomly chosen subset of psychotropic drugs. We then
evaluated a publicly-available system called SAPIENTA
[20] that can automatically identify conclusion sentences
in unstructured scientific text.
Development of a reference standard of relevant claims
Figure 6 shows the results of identifying relevant and novel
conclusion claims from efficacy and effectiveness stud-
ies routed to the Clinical Studies section via LinkedCT.
Table 3 lists results for each of the nine randomly-
selected psychotropic drugs. A total of 170 abstracts were
routed from PubMed to the Clinical Studies section of
Figure 5 A Clinical Pharmacology section from an escitalopram product label as shown in the proof-of-concept. In this example, an DIKB
metabolic pathway claim with supporting evidence is being shown.
Boyce et al. Journal of Biomedical Semantics 2013, 4:5 Page 9 of 21
http://www.jbiomedsem.com/content/4/1/5
the products labels for the nine randomly sampled psy-
chotropics. Four of the abstracts were either not clinical
studies, or provided no other text content besides the title.
These were dropped from further analysis. Of the 166
remaining conclusions, two were not interpretable with-
out reading the full text article and 113 were judged to
not be relevant to a pharmacist viewing the Clinical Stud-
ies section. For the remaining 51 relevant conclusions,
the inter-rater agreement prior to reaching consensus was
0.69, reflecting substantial agreement according to the
criteria of Landis and Koch [21].
Twelve of the 51 relevant conclusions were judged
to apply to uses of the drug other than those for
which it was approved for by the FDA. Of the 39 rel-
evant conclusions that applied to an approved indica-
tion, 30 were judged to be novel to the Clinical Studies
section of at least one product label for a product con-
taining the drug. Inter-rater agreement prior to reach-
ing consensus on the novelty of these 30 relevant and
novel conclusions was also substantial with a Kappa of
0.72.
Determination of the precision and recall of an automated
extractionmethod
Figure 7 shows the results of determining the base-
line information retrieval performance of the proof-of-
concept system. SAPIENTA processed the same 170
abstracts mentioned in the previous section that were
routed from PubMed to the Clinical Studies section of
the product labels for the nine randomly sampled psy-
chotropics. Of the more than 2,000 sentences in the 170
abstracts, the program automatically classified 266 sen-
tences as Conclusions. In comparison, the conclusion
claims extracted manually from the abstracts consisted
of 318 sentences. Using these sentences as the reference
standard, the recall, precision, and balanced F-measure
for SAPIENTA was 0.63, 0.75, and 0.68 respectively. By
combining these results with the precision of routing Clin-
icalTrials.gov study results to the Clinical Studies section
via LinkedCT results in an overall pipeline precision of
0.23.
A descriptive summary of challenges to the automated
extraction of claims about a drugs metabolic pathways
Although the proof-of-concept made links from claims
about a drugs metabolic pathways present in the DIKB
resource to the Clinical Pharmacology section of the
product label, the DIKB has claims for only a small sub-
set (<100) of the thousands of drugs currently on the
market. To further investigate the feasibility of auto-
matically extracting claims about a drugs pharmacoki-
netic properties, we manually traced the evidence for a
small number of claims pertaining to the pharmacoki-
netics of escitalopram that the proof-of-concept linked
from the DIKB to drug product labels. The goal of
this effort was to see if there were particular pat-
terns that we might use in future language analytics
systems.
We found that the inhibition and substrate claims are
derived from two texts, one describing a set of experi-
ments to deduce the metabolic properties (i.e., biotrans-
formation and enzyme inhibition) for escitalopram [22],
and one a product label produced by Forest Labs [23]. As
an example, there are two pieces of evidence against the
claim escitalopram inhibits CYP2C19  first, from the
Forest Labs text...
In vitro enzyme inhibition data did not reveal an
inhibitory effect of escitalopram on CYP3A4, -1A2,
-2C9, -2C19, and -2E1. Based on in vitro data,
escitalopram would be expected to have little
inhibitory effect on in vivo metabolism mediated by
these cytochromes.
...and second, from the Moltke et al. paper:
CYP2C19. R- and S-CT were very weak inhibitors,
with less than 50 percent inhibition of S-mephenytoin
hydroxylation even at 100micM. R- and S-DCT also
were weak inhibitors. R- and S-DDCT were moderate
inhibitors, with mean IC50 values of 18.7 and
12.1micM, respectively. Omeprazole was a strong
inhibitor of CYP2C19, as was the SSRI fluvoxamine
(see Table 2).
The claim escitalopram is a substrate of CYP2C19 is
motivated by the following evidence in Moltke et al.:
At 10micM R- or S-CT, ketoconazole reduced reaction
velocity to 55 to 60 per cent of control, quinidine to 80
per cent of control, and omeprazole to 80 to 85 per
cent of control (Figure 6). When the R- and S-CT
concentration was increased to 100 M, the degree of
inhibition by ketoconazole increased, while inhibition
by quinidine decreased (Figure 6). These findings are
consistent with the data from heterologously expressed
CYP isoforms.
The validity of this claim depends on an assumption
(omeprazole is an in vitro selective inhibitor of enzyme
CYP2C19) which is a separate DIKB claim, supported by
a draft FDA guidance document [24].
The next claim is that escitaloprams primary clearance
route is not by renal excretion and it is derived from the
following sentence in the Forest Laboratories text:
Following oral administrations of escitalopram, the
fraction of drug recovered in the urine as escitalopram
and S-demethylcitalopram (S-DCT) is about 8 per cent
Boyce et al. Journal of Biomedical Semantics 2013, 4:5 Page 10 of 21
http://www.jbiomedsem.com/content/4/1/5
Figure 6 A flow diagram of the process and results of identifying relevant and novel conclusions from efficacy and effectiveness studies
routed to the product label Clinical Studies section via LinkedCT.
Table 3 Relevance and novelty of conclusion claims based onmanual validation
Drug ClinicalTrials.gov Published results from ClinicalTrials.gov
studies involving the drug studies involving the drug
N Relevant N ( % ) Novel (indication) Novel (off-label use)
Antidepressants
Citalopram 4 25 5 (20) 5
Duloxetine 4 4 4 (100) 3
Escitalopram 6 9 3 (33) 1 2
Mirtazapine 1 22 1 (5) 1 0
Nortriptyline 3 24 2 (8) 1 1
Venlafaxine 2 2 2 (100) 1 1
Antipsychotics
Olanzapine 5 13 7 (54) 6 1
Risperidone 23 70 26 (37) 21 5
Sedative Hypnotics
Eszopiclone 1 1 1 (100) 0 1
The relevance and novelty of conclusion claims linked from three Linked Data drug information sources to the product labeling for nine randomly selected
psychotropic drugs.
Boyce et al. Journal of Biomedical Semantics 2013, 4:5 Page 11 of 21
http://www.jbiomedsem.com/content/4/1/5
and 10 per cent, respectively. The oral clearance of
escitalopram is 600 mL/min, with approximately 7 per
cent of that due to renal clearance.
The connection between the evidence and the claim
requires the domain knowledge that renal excretion is
roughly the same as the fraction of dose recovered in
urine.
Finally, the evidence for claims pertaining to escitalo-
prams metabolites again comes from the Forest Labs
text:
Escitalopram is metabolized to S-DCT and
S-didemethylcitalopram (S-DDCT).
From these examples, we ascertained four issues that
present major challenges for the automated extraction of
drug claims from a text source:
Self-referencing and anaphora. In narrative text,
coherence is often created by creating anaphoric
co-reference chains - where entities at other
locations in the text are referred to by pronouns
(it, they) and determiners (these, this). This makes
sentences such as these very easy for humans to read:
R-CT and its metabolites, studied using the same
procedures, had properties very similar to those of
the corresponding S-enantiomers.
However, automatically identifying the entities
referred by these referents its metabolites,
the same procedures, similar properties, and
the corresponding S-enantiomers is a non-trivial
task.
Use of ellipsis Often statements are presented in a
compact manner, where the full relations between
drugs and proteins are omitted, as in this
example:
Based on established index reactions, S-CT and
S-DCT were negligible inhibitors (IC50 > 100
?M) of CYP1A2, -2C9, -2C19, -2E1, and -3A, and
weakly inhibited CYP2D6 (IC50 = 70 - 80 ?M)
A computational system would need to unpack this
statement to read the following list of relations (a
total of 12 statements).
 S-CT (escitalopram) was a negligible inhibitor
((IC50>100 ?M) of CYP1A2
Figure 7 Determining the baseline information retrieval performance of the proof-of-concept system.
Boyce et al. Journal of Biomedical Semantics 2013, 4:5 Page 12 of 21
http://www.jbiomedsem.com/content/4/1/5
 S-CT (escitalopram) was a negligible inhibitor
((IC50>100 ?M) of CYP2C9
 ...
Domain knowledge is needed to be able to
resolve anaphora. The metabolites referred to in
the phrase R-CT and its metabolites, above, which
is referred to six times in the text, are not explicitly
described in the text. For even a human to be able to
define what they are it is necessary that they know
that the following sentence contains a definition of
the metabolites studied:
Transformation of escitalopram (S-CT), the
pharmacologically active S-enantiomer of
citalopram, to S-desmethyl-CT (S-DCT), and of
S-DCT to S-didesmethyl-CT (S-DDCT), was
studied in human liver microsomes and in
expressed cytochromes (CYPs).
Interestingly, this information is given only in the
abstract of the paper.
Key components are provided in other papers. As
with textual coherence, inter-textual coherence,
embedding the current text in the corpus of known
literature, is an important function of the text. In
certain cases key elements of the paper, such as the
methods, are entirely described through a reference,
e.g.:
Average relative in vivo abundances [... ] were
estimated using methods described in detail
previously (Crespi, 1995; Venkatakrishnan et al.,
1998 a,c, 1999, 2000, 2001; von Moltke et al., 1999
a,b; Störmer et al., 2000).
There is of course no way to ascertain what methods
were used without (computational) access to these
JOURNAL OF
BIOMEDICAL SEMANTICS
Katayama et al. Journal of Biomedical Semantics 2013, 4:6
http://www.jbiomedsem.com/content/4/1/6REVIEW Open AccessThe 3rd DBCLS BioHackathon: improving life
science data integration with Semantic Web
technologies
Toshiaki Katayama*, Mark D Wilkinson, Gos Micklem, Shuichi Kawashima, Atsuko Yamaguchi, Mitsuteru Nakao,
Yasunori Yamamoto, Shinobu Okamoto, Kenta Oouchida, Hong-Woo Chun, Jan Aerts, Hammad Afzal,
Erick Antezana, Kazuharu Arakawa, Bruno Aranda, Francois Belleau, Jerven Bolleman, Raoul JP Bonnal,
Brad Chapman, Peter JA Cock, Tore Eriksson, Paul MK Gordon, Naohisa Goto, Kazuhiro Hayashi, Heiko Horn,
Ryosuke Ishiwata, Eli Kaminuma, Arek Kasprzyk, Hideya Kawaji, Nobuhiro Kido, Young Joo Kim, Akira R Kinjo,
Fumikazu Konishi, Kyung-Hoon Kwon, Alberto Labarga, Anna-Lena Lamprecht, Yu Lin, Pierre Lindenbaum,
Luke McCarthy, Hideyuki Morita, Katsuhiko Murakami, Koji Nagao, Kozo Nishida, Kunihiro Nishimura,
Tatsuya Nishizawa, Soichi Ogishima, Keiichiro Ono, Kazuki Oshita, Keun-Joon Park, Pjotr Prins, Taro L Saito,
Matthias Samwald, Venkata P Satagopam, Yasumasa Shigemoto, Richard Smith, Andrea Splendiani,
Hideaki Sugawara, James Taylor, Rutger A Vos, David Withers, Chisato Yamasaki, Christian M Zmasek,
Shoko Kawamoto, Kosaku Okubo, Kiyoshi Asai and Toshihisa TakagiAbstract
Background: BioHackathon 2010 was the third in a series of meetings hosted by the Database Center for Life
Sciences (DBCLS) in Tokyo, Japan. The overall goal of the BioHackathon series is to improve the quality and
accessibility of life science research data on the Web by bringing together representatives from public databases,
analytical tool providers, and cyber-infrastructure researchers to jointly tackle important challenges in the area of in
silico biological research.
Results: The theme of BioHackathon 2010 was the 'Semantic Web', and all attendees gathered with the shared
goal of producing Semantic Web data from their respective resources, and/or consuming or interacting those data
using their tools and interfaces. We discussed on topics including guidelines for designing semantic data and
interoperability of resources. We consequently developed tools and clients for analysis and visualization.
Conclusion: We provide a meeting report from BioHackathon 2010, in which we describe the discussions,
decisions, and breakthroughs made as we moved towards compliance with Semantic Web technologies - from
source provider, through middleware, to the end-consumer.
Keywords: BioHackathon, Open source, Software, Semantic Web, Databases, Data integration, Data visualization,
Web services, Interfaces* Correspondence: ktym@dbcls.jp
Database Center for Life Science, Research Organization of Information and
Systems, 2-11-16, Yayoi, Bunkyo-ku, Tokyo 113-0032, Japan
© 2013 Katayama et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Katayama et al. Journal of Biomedical Semantics 2013, 4:6 Page 2 of 17
http://www.jbiomedsem.com/content/4/1/6Background
The term Semantic Web refers to those parts of the World
Wide Web in which information is explicitly encoded in a
machine-readable syntax, and the relationships between en-
tities are explicitly constructed and labeled using machine-
readable links. The most significant difference between the
Semantic Web and the current World Wide Web is that
Semantic Web is intended to be accessed by machines, ra-
ther than by people. As such, it concerns itself primarily
with the structured representation of data and knowledge
in ways that can be automatically interpreted and tra-
versed without human intervention. This should therefore
support more complex, cross-domain, and cross-resource
investigations by putting the burden of data discovery and
integration on the machine, rather than on the individual.
Until now, complex cross-domain querying has tended
to only be supported by large-scale data warehouses, in-
cluding BioMart [1] and InterMine [2]. However, these can
only respond to questions within the confines of the data
collected in that warehouse. By simplifying and automating
dynamic and distributed data discovery and integration,
the Semantic Web should encourage a researchers curios-
ity and support them in pursuing spontaneous questions
outside of the scope of existing and pre-constructed data
warehouses.
Early-adopters of Semantic Web technologies have put
together demonstrations showing its power over traditional
data and knowledge frameworks. Among the most prom-
inent of these early-adopters have been the life and health
science communities [3] where numerous Linked Data
initiatives have emerged. Notable examples include the
Semantic Web Healthcare and Life Science Interest Group
(HCLSIG) with their creation of the Clinical Observations
Interoperability demo [4] and Linked Open Drug Data
demo [5]; and the Bio2RDF project [6,7], which integrates
more than 50 biological databases into a Linked Data en-
vironment. While these motivational projects demonstrate
the power of Linked Data [8], the warehouse approach they
adopt, which ensures data consistency, avoids the desirable
and intended distributed nature of the Semantic Web. The
next step in the Semantic Web evolution involves the
source providers themselves making their resources avail-
able as Linked Data. This was the theme of BioHackathon
2010 - to our knowledge the first time in the life sciences
that such a large group of non-affiliated data providers,
cyber-infrastructure projects, and client-application pro-
jects have come together for this purpose.
We use the terms Linked Data and Semantic Web to
refer to two different aspects of the construction of a
distributed web of data and knowledge, and it is useful to
go into some detail about how each layer contributes to
the overall vision of machine-readable data and knowledge.
Linked Data refers to data exposed on the Web following
a particular set of technologies and conventions aimed atimproving mechanized access and processing. For example,
in Linked Data, Uniform Resource Identifiers (URIs) are
used to refer to all data entities; moreover, by convention
and best practice, these URIs should (in general) resolve to
additional Linked Data describing that entity, and linking
that entitys URI to other URIs using explicitly labeled
relationships. These Linked Data conventions provide sig-
nificant advantages for both data providers and data
consumers. First, through naming data by a URI, the
means for retrieving that data, regardless of its provider or
location, become uniform - defined by the HTTP proto-
col. Similarly, there is a separation of how data is stored
(web pages, flat-files, relational databases, etc.) and how it
is accessed and consumed. In much the same way as the
HTTP protocol created a uniform access layer for the
Web, allowing the creation of generic tools such as Web
browsers, Linked Data ensures that, regardless of under-
lying format, exposed information is uniformly accessible
through a common query language - SPARQL. More im-
portantly, Linked Data can also be integrated site-to-site
across multiple independent providers via queries that
span multiple data-endpoints. Linked Data technologies
and conventions, therefore, facilitate data exploration and
evaluation by removing the need to design an integrative
schema, download, homogenize, and finally warehouse
data subsets in order to ask common domain-spanning
questions.
The Semantic Web extends the concept of and is built
on Linked Data, but is additionally concerned with de-
fining machine-interpretable semantics for entities and
relations that might appear in a Linked Dataset. The
precise meanings of these concepts and relations are
defined in an ontology, and this ontology can be utilized
by software called a reasoner to evaluate the data-types
and properties within a Linked Dataset against a possible
interpretation of that dataset, defined by the ontology.
Through this process, the aggregated data is automatic-
ally classified or categorized according to the concepts
defined by a given ontology - the ontology provides a
view of the data, and different ontologies can be used
to provide differing views, depending on the nature of
the study or the question of interest. Moreover, new
knowledge can be automatically derived as reasoners
detect new instances of ontologically-defined concepts
within aggregated Linked Data.
For both biological researchers and data managers, the
significance of these new paradigms cannot be over-stated.
Laboratories that currently invest significant resources in
creating one-off integrated data warehouses, and then
manually interpreting them, would be able to create such
datasets with a single Web-wide query. Moreover, carefully
crafted ontologies could then be employed to automatically
classify the resulting integrated data into conceptual cat-
egories of interest to that researcher. Beyond keyword
Katayama et al. Journal of Biomedical Semantics 2013, 4:6 Page 3 of 17
http://www.jbiomedsem.com/content/4/1/6searches, or even traditional database queries, Semantic
Web technologies facilitate querying datasets at a concep-
tual level, where the concepts of interest can be defined
post-facto, after the act of data query and integration. The
ease and low-cost of creating and analyzing such datasets
would encourage researchers to re-ask their questions in
light of new discoveries or new datasets. Alternately, they
may choose to re-formulate their research question as they
progress toward a new discovery, in the knowledge that re-
trieving and integrating additional third-party data into
their existing warehouse can be achieved with very little
cost. While these kinds of integrative investigations can be
(and are being) conducted on the current Web, the current
cost and complexity of cross-domain data integration
hinders exploration, inhibiting researchers from pursuing
their own curiosities.
The goal of the 2010 BioHackathon, therefore, was to
bring together data providers and tool authors to discuss
the path toward making their resources available using
these powerful new integrative standards and frameworks.
Meeting outline
A total of 56 attendees participated in BioHackathon
2010, including representatives from at least 10 major
biological databases, major cyber-infrastructure pro-
jects such as BioRuby [9,10], Biopython [11,12], SADI
[13,14], G-language [15,16], BioMart and InterMine,
and client providers such as Taverna [17], FlyMine
[18,19], Bio-jETI [20,21], and Cytoscape [22,23]. The
full list of BioHackathon participants, and their respect-
ive projects, is available [24].
Many important life science data providers were
present at the BioHackathon, including UniProt [25,26],
Korean HapMap [27], TreeBASE [28], DDBJ [29,30],
INSD [30], PDBj [31], KEGG [32,33], DBCLS [34],
IntAct [35,36], as well as developers of data integration
projects including Bio2RDF [6,7], BioGateway [37,38],
DERI [39], PSICQUIC [40,41] and the HUPO Proteomics
Standards Initiative [42].
BioHackathon 2010 revolved around discussion and
hands-on exploration of approaches to structuring life sci-
ence Linked Data such that it maximizes the power of
semantics, while at the same time minimizing the burden
placed on data providers. The lessons learned and best
practices that emerged from these discussions are detailed
below and follow the flow of Figure 1 from designing data
through querying to analysis, visualization and browsing.
Methods
Ontologies
Ontologies and ontology-like knowledge structures have
been part of life science research and practice for centur-
ies. For example, both Merriam-Webster and the Oxford
English dictionary suggest that the first use of the termnosology (a formal classification of diseases) occurred
in approximately 1721, and of course formal property-
based taxonomies are almost as old as biology itself.
Medical and clinical terminologies such as SNOMED
CT [43] and OpenGALEN [44] have been developed
over the past decades. However, the term ontology, in
its more modern usage in the life sciences, arguably began
with the establishment of the Gene Ontology project in
1998 [45] which aimed to set-up a classification system for
genes. Subsequently, the idea of using ontologies to
categorize and annotate all types of life science data rap-
idly has become widely adopted. To encompass all of the
potential types and uses for ontologies, some very gene-
ralized definitions of ontology have been proposed, with
perhaps the most widely cited being a specification of a
conceptualization [46] - quite simply, to be formal and
explicit about the things, concepts and relationships that
exist in a domain of interest. The Artificial Intelligence
community further specified this definition for their own
field, as the models that capture and describe specific
domains [47].
At the intersection of the life sciences and the com-
puter sciences, we find ontologies being used for a wide
variety of purposes, such as annotation (e.g., plant anat-
omy [48]), supporting biological analysis (e.g., systems
biology modeling [49]); data integration and improved
shareability (e.g., the Gene Ontology [45]); and decision
support (e.g., Clinical Practice Guidelines [50]).
Given the wide range of use-cases and disparate de-
velopment communities, a variety of standards and
structures emerged within which to capture this expli-
cit ontological knowledge. The Semantic Web initiative of
the World Wide Web Consortium (W3C) consolidated
these into three core standards for data representation,
knowledge representation, and querying, described as
follows.
RDF
The Resource Description Framework (RDF) is a data
model proposed by the W3C to implement and support
the Semantic Web infrastructure. RDF consists of three
components - Resources, Relationships, and Literals. A
Resource is any nameable thing (e.g., an entity or a con-
cept) and RDF Resources are always named by Universal
Resource Indicators (URIs). Optimally, every Resource
should only have one URI that is shared throughout the
Web, though in practice this is quite difficult to achieve
(and, in fact, was a topic of significant discussion at the
BioHackathon, as will be described below). The second
component is the Relationship, also called the Predicate.
Relationships are used to describe how two Resources are
related to one another; these are also named by a URI, and
this named relationship should also be optimally unique
and shared throughout the Web. The combination of two
Figure 1 A schematic flow from data through querying to analysis, visualization and browsing that were collaboratively endeavored in
the BioHackathon 2010. Databases, services, projects represented and/or utilized are shown in green ovals.
Katayama et al. Journal of Biomedical Semantics 2013, 4:6 Page 4 of 17
http://www.jbiomedsem.com/content/4/1/6Resources (subject and object) and their Relationship
(predicate) is called a Triple - the smallest unit of infor-
mation that can be represented on the Semantic Web. For
example:
<http://example.org/event/BH10> <http://example.org/
attendee> <http://example.org/people/TK>
The final component is the Literal - effectively, a numer-
ical value or a set of characters. Literals are intended to
provide concrete data about a particular Resource. As with
Resource-to-Resource connections, Literals are connected
to a Resource by an appropriate and well-defined Rela-
tionship URI, for example:<http://example.org/event/BH10> <http://example.org/year>
2010
Literals cannot be the subject of a Triple, and therefore
cannot be connected to one another. A set of Triples is
called a Graph, and Triples are generally stored in a
database-like triple-store. These databases can then be
exposed on the Web as endpoints available for query-
ing by a variety of tools.
RDF can be represented in various ways for the purpose
of passing data from one machine to another, or for human
consumption. One of the most common representations is
XML [51]. Another common serialization is N3 [52], which
is much more compact.
Katayama et al. Journal of Biomedical Semantics 2013, 4:6 Page 5 of 17
http://www.jbiomedsem.com/content/4/1/6OWL
The second of the core W3C Semantic Web standards is
the Web Ontology Language (OWL). OWL is a language
for encoding (a) how Classes and Predicates should
be interpreted, and (b) how specific combinations of
Resources and Predicates can be inferred to represent a
particular concept. For example, the concept of (puta-
tive) TransmembraneProtein might be simplistically
defined in pseudo-OWL as follows:
TransmembraneProtein is: a:Protein located_in
a:Membrane and has_sequence (a:Sequence and
[has_motif a:Helix or a:Barrel])
Subsequently, if the following triples were found on
the Web:
1. ex:molecule type a:Protein
2. ex:molecule has_sequence ex:sequence
3. ex:sequence has_motif a:Helix
4. ex:molecule located_in a:Membrane
It would be possible for a reasoner (a program that
analyses the logical statements in RDF and OWL) to
conclude that ex:molecule is of ontologically-defined
type TransmembraneProtein.
What may not be obvious from this example is that
triples 14 might come from entirely different places on
the Web. However, it is possible that triple 3 is brought
from bioinformatics analysis and triple 4 is acquired
experimentally. Because they are sharing URIs, the
independently-derived triples can be easily combined
into a Graph. Moreover, OWL and reasoning can then
be applied to interpret (discover) the emergent new
information contained in that integrated dataset. This
idea is extremely powerful. However, to achieve this
power, consensus must be reached on how to represent
the data in RDF such that it can be integrated as easily
as just described, and this was a major theme of the
BioHackathon.
SPARQL
SPARQL (SPARQL Protocol and RDF Query Language)
[53] is a standard language for querying RDF data,
allowing the information stored in triple stores to be
explored and retrieved in a manner akin to how SQL is
used to retrieve data from relational databases. A triple
store that is queryable by the SPARQL language is referred
to as a SPARQL endpoint. The life science community
consisted of early adopters of this technology, providing
SPARQL endpoints even before this language became an
official W3C recommendation [37,38]. SPARQL queries
consist of a series of triple-patterns, where any component
of the triple might be a variable, and these triple-patternscan be combined into graph-patterns. SPARQL engines
then look for sub-graphs that match the graph-pattern
specified in the query. For example, the triple-pattern:
?protein <http://www.semantic-systems-biology.org/
SSB#located_in> "nuclear membrane"
could be used to find all of the proteins in a given SPARQL
endpoint that are located in the nuclear membrane.
To help construct SPARQL queries, efforts to bring these
technologies closer to end-users are emerging, providing
straightforward interfaces to domain-specific triple stores
(e.g., Cell Cycle Ontology [54]).Results
Designing semantic data
A major focus of the BioHackathon was to look at RDF
from the data provider's perspective. We discussed, and
in some cases came-up with possible answers to, ques-
tions such as What Semantic Web-enabled life science
data should be provided?, Should I convert everything
into RDF, or just some types of data?, What data is
already available as RDF and how do I link into that?, I
am a publisher of RDF already, how might I improve my
current offerings? This section, therefore, first describes
a number of guidelines that were established at the
BioHackathon, then examines the state of currently
available data, and concludes with the open questions
that remained at the end of the BioHackathon.
The guidelines center around two issues: making data
available in ways that are easy to integrate and query,
and making data descriptive and explicit. Addressing the
former, the participants agreed that providers should
ideally both:
 provide a SPARQL endpoint, not necessarily hosted
by the data provider themselves, but officially
supported by the provider, and
 make dumps of raw data available in RDF from their
HTTP or FTP server.
Further, the BioHackathon attendees agreed that in
order to provide descriptive and explicit data, providers
should strive to:
 use standard/shared URIs, and
 use standard/shared predicates.
UniProt is an example of an existing provider who is
following all of these guidelines. It offers RDF dumps on
their FTP server [55], and also allows dynamic retrieval of
RDF representations of specific resources (via adding '.rdf'
to the URL or via HTTP content negotiation) and/or
Katayama et al. Journal of Biomedical Semantics 2013, 4:6 Page 6 of 17
http://www.jbiomedsem.com/content/4/1/6query results (via appending '&format=rdf'). TreeBASE
does the latter as well, and can make an RDF dump avail-
able easily. Representatives from DDBJ provided the Gene
Trek in Prokaryote Space (GTPS) [56] data as an RDF
dump and are also working to convert their other re-
sources into RDF. PDBj made available their entire struc-
ture database in the RDF format [57]. The KEGG group
also attempted to convert their data into RDF but the
work is temporally discontinued due to the recent change
of their licensing policy. TogoWS [58,59] has imple-
mented on-the-fly RDF conversion for databases stored in
TogoDB [60] and several external data sources including
NCBI PubMed (e.g., http://togows.dbcls.jp/entry/pubmed/
20472643.ttl).
Even though the size of the RDF representation of data
can generally become larger than the original one, users
can benefit from advanced-level search capabilities based
on the semantics explicitly embedded in the RDF. There-
fore, providers agreed to make RDF dumps available for
download. However, providing a SPARQL endpoint re-
quires additional time and resources, and these will likely
take longer to appear from the individual data hosts.
There are a number of SPARQL endpoints provided by
third-parties, many of which are warehouses of RDF data
from these individual providers. Examples include the
LODD [5] and the Bio2RDF [6,7]; however, the RDF in
these warehouses generally is a project-specific conversion
from the source data host, and may not resemble RDF
provided by the host itself, if such is available. Finally,
though many are overlapping in their scope, these third-
party warehouse projects have created independent mo-
dels for the RDF data, and independent standards for
URIs, so are difficult to integrate with one another. The
problem of URI standardization was discussed extensively
at the BioHackathon, and is the next topic of discussion
here.
URIs and global integration
The URI is the core technology upon which all aspects
of the Semantic Web are built. Thus, as with all Seman-
tic Web projects, it was inevitable that the first decision
that needed to be made at BioHackathon 2010 was
related to URIs. The BioHackathon attempted to use a
community-consensus approach to find a solution to
generating commonly accepted URIs, and a consensus
decision was achieved among the attendees in a surpris-
ingly short time. The consensus is described as follows:
 The BioHackathon community recognized the need
for, and strongly endorsed Cool URIs [61] as the
behavior that they would strive to adhere to when
naming their own entities. A Cool URI has a
variety of behaviors, but most relevant to this
meeting were that Cool URIs do not change, areprecise in what they identified (i.e., is it the real-
world thing or a document describing the thing),
must resolve through HTTP GET, and can provide
an appropriate representation of the thing through
content-negotiation (e.g., HTML for a browser, and
RDF for a Semantic Web tool). An example of a
Cool URI is http://identifiers.org/obo.go/
GO:0006915, which can be resolved to an RDF
representation with the Accept application rdf+
xml HTTP header, or resolved to an HTML
representation with the Accept text/html HTTP
header.
 It was recognized that, in order to link data, it is
often necessary to refer to the entities of third-
parties using common URIs, and that, for the time
being, many of these third-party providers do not
have Cool URIs for the entities we wish to cross-
reference, but it is important to be polite [62]
when referring to their data in our RDF. Effectively,
when a data provider publishes a URI, he/she does
so expecting people to use that URI in their own
RDF Triples, so if he/she is publishing a URI about
someone elses data (i.e. he/she is naming anothers
data elements) then he/she should attempt to be
polite about it.
What it means to be polite was discussed over sev-
eral days, and the following recommendations were
adopted by the attendees:
1. A registry of bioinformatics URI patterns has been
set-up in FreeBase [63].
2. Native providers of Cool URIs should register their
URI pattern in FreeBase, with an indication that it is
approved for use by third-parties, and has some
guarantee of stability/resolvability in the long-term.
3. A provider who must refer to a third-party entity in
their RDF, should first check FreeBase to determine
which URI pattern has been registered there.
4. If they find no existing entry for that provider, they
should attempt to determine which URI pattern from
that provider is most likely to be stable (giving
preference to Cool-URI-style URIs over GET strings
with parameters) and then register that pattern in
the FreeBase repository. This is the pattern that will
then be used by the rest of the community until such
time as the third-party provides Cool-URI-style
identifiers.
5. When such third-parties do begin to produce Cool-
URIs, they are strongly encouraged to register a
mapping scheme to assist the community in
accurately translating from the URIs they have been
using in their RDF to the URIs that are now
approved by that third party.
Katayama et al. Journal of Biomedical Semantics 2013, 4:6 Page 7 of 17
http://www.jbiomedsem.com/content/4/1/6This first-come-first-served approach is intended to be
purely pragmatic. It ensures that nobody is blocked from
making progress on their own data representation by the
need for community consensus around third-party URI
patterns, and is highly scalable by distributing the curator-
ial burden over the entire community in a needs-based
manner. However, at the time of this writing, this proposal
has not been implemented even by BioHackathon atten-
dees, suggesting that this solution was impractical, un-
desirable, or both. As such, referring to and linking to
third-party data in RDF remains a challenge for many
providers.
While there was some discussion regarding exactly how
to structure the RDF returned by a URI when derefe-
renced, there was general agreement that this discussion
was better left to a future meeting. Two providers of
Semantic Web data - the Life Science Resource Names
(LSRN [64] - soon to be supplanted by Identifiers.org
[65]) and SADI projects - have decided to adopt the
Semanticscience Integrated Ontology (SIO) [66] to model
at least core metadata about the nature of the URI that
has been resolved. SIO is an ontology under active devel-
opment specifically aimed at describing scientific data and
data-set-composition, for example, the type of thing
being described (a GenBank record, a SwissProt record,
etc.) and/or the dataset to which it belongs (spot informa-
tion from a particular microarray study, for example).
Data providers
The success of any Semantic Web initiative in the life
sciences will depend on the participation of the source data
providers. As such, their involvement in the BioHackathon
was key to many of the other activities undertaken during
the event. Here we describe some of the successes achieved
by the various domains of bio/molecular data resource
providers.
Molecular databases
Participants representing molecular database providers
indicated that they expected to produce their public
resources officially in the RDF format in the near future.
As mentioned earlier, UniProt already provides the raw
data in RDF. During the BioHackathon, the DDBJ group
developed a prototype of a converter which returns the
RDF version of a DNA sequence entry; however, because
the International Nucleotide Sequence Database Collab-
oration (INSDC) data format is very complicated, there
is a need to improve the design of the RDF representa-
tion to maximize usability. The KEGG group attempted
to extract cross-referencing information between path-
ways, genes, and Protein Data Bank (PDB) entries and to
capture these in RDF. In addition, the PDBj group
developed an Extensible Stylesheet Language (XSL) that
converted the entirety of the PDB Markup Language(PDBML) file into RDF. Since the PDBML is based on
the macromolecular Crystallographic Information File
(mmCIF) format - the original format of the PDB data-
base - and mmCIF itself is defined as an ontology,
mmCIF categories and items are easily re-used as pre-
dicates in an RDF representation of PDBML [57].
Molecular interactions
The Proteomics Standard Initiative Common QUery
InterfaCe (PSICQUIC) project [40,41] is moving towards
an RDF/XML output format. PSICQUIC is a standard
that defines a common way to access Molecular Inter-
action resources (IntAct [35,36], MINT [67], BioGrid
[68], iRefIndex [69], MatrixDB [70], Reactome [71],
MPIDB [72], ChEMBL [73]), including more than 1.7
million interactions in 2010/2011, and at the time of this
writing this number had grown to 150 million inter-
actions, spanning 27 PSICQUIC services over 19 inde-
pendent organizations [74]. This rapid and widespread
adoption was due in large part to the notable successes
of the proteomics community in coordinating their data
sharing efforts via PSICQUIC development over previ-
ous BioHackathon events, and we believe this is an
excellent demonstration of the utility and power of cre-
ating communities of developers and providers through
BioHackathon events.
The evolution of the standard that was enacted during
BioHackathon 2010 was to design PSICQUIC services that
return Biological Pathway Exchange (BioPAX) [75] Level 3
data. As the underlying data was in Proteomics Standards
Initiative Molecular Interaction (PSI-MI XML) format
[76], a converter was created using the Apache Jena frame-
work [77], which is designed to simplify manipulation of
RDF in Java. At the end of the BioHackathon, PSICQUIC
was able to return RDF data in a variety of formats (RDF/
XML, N3, and N-Triples). The data returned contains
interaction data including information about the inter-
action participants and their individual cross-referencing
information. To ensure data integrity and correct out-
put formatting, the output was successfully visualized in
Cytoscape 2.7.0 beta 3 (Figure 2).
Systems biology
In systems biology, most curated models are developed by
a graphical tool (e.g., CellDesigner [78]) and described in
Systems Biology Markup Language (SBML) format [79].
Each pathway model is published in a traditional manu-
script, as well as a corresponding SBML model file. To re-
veal relationships among pathways and overall network
structure, integration of those published models is required.
For this purpose, BioPAX can be used as a common ex-
change format for biological pathway models. Users can
use Semantic Web technologies to explore the integrated
pathways since BioPAX data is represented in OWL.
Figure 2 Visualizing PSICQUIC RDF output in Cytoscape.
Katayama et al. Journal of Biomedical Semantics 2013, 4:6 Page 8 of 17
http://www.jbiomedsem.com/content/4/1/6To evaluate this approach in practice, we selected
Alzheimers disease as a target domain, since represen-
tatives at the BioHackathon had already been involved in
developing AlzPathway [80,81] which collects signaling
and metabolic pathways relevant to this disease. Im-
portantly, many pathways implicated to have a role in
Alzheimers disease have other critical functions for nor-
mal cell and tissue growth and development. For example,
the apoptosis pathway is known to be closely related to
Alzheimers disease. Therefore, integration of apoptosis
and AlzPathway models might bring novel insights and/or
provide new hypotheses on a formerly unknown relation-
ship between these pathways.
The sbml2biopax program [82] was used to convert
the apoptosis pathway, which is published in BioPAX
format as supplement material [83], into the SBML for-
mat. Both AlzPathway and apoptosis pathway data in
BioPAX Level 2 format are stored in an instance of
4store [84] and SPARQL queries against the triple store
were performed. The resulting integrated apoptosis and
AlzPathway model was explored using RDF store and
SPARQL queries. It was noted that mapping of names
among models in RDF is also essential for successful in-
tegration in this SPARQL endpoint.
Taxonomy
Another application of Semantic Web technologies pur-
sued at the BioHackathon was related to systematics and
taxonomy [85]. Taxonomic concepts are at the heart of
such well-established disciplines as zoology and botanyand are crucial to the field of biodiversity informatics
[86]. Recently they are becoming increasingly important in
molecular biology and genomics as well, for example in the
numerous metagenomics projects that deal with hundreds
of taxonomic entities simultaneously (e.g., [87,88]) as well
as the increased availability of genome data from non-
traditional model organisms that becomes available for
comparative-genomics studies. Simply representing a tax-
onomy as a string has a number of shortcomings. Chiefly
among them is the inability to link to other data, the exist-
ence of synonyms (taxa with multiple names) and different
taxa with the same name (homonyms). Using taxonomy-
specific controlled vocabularies makes it possible to repre-
sent taxonomic information in the form of RDF triples and
provides a means to link taxonomic names to other en-
tities, such as molecular sequences, type collections and
geographic information. Ontologies which have been de-
signed for taxonomy and biodiversity informatics are the
Taxonomic Database Working Group (TDWG) ontology
[89] and DarwinCore [90]. Furthermore, the Comparative
Data Analysis Ontology (CDAO) [91] is an ontology de-
signed for the associated fields of evolutionary biology and
phyloinformatics and enables the description of concepts
related to comparative evolutionary analysis, including the
analysis of molecular sequences. It is, however, worth not-
ing that there is no authoritative source for taxonomy. This
is mainly due to the fact that multiple needs get conflated.
This issue has had a slight negative impact on semantic-
based integrations so far (and usually an ad-hoc taxonomy
is built based on extant resources).
Katayama et al. Journal of Biomedical Semantics 2013, 4:6 Page 9 of 17
http://www.jbiomedsem.com/content/4/1/6Text-mining
At the present time, biological databases alone cannot
capture the richness of scientific information and argu-
mentation contained in the literature, nor can they pro-
vide support for the novel ways in which scientists wish
to interrogate that data. A considerable fraction of the
existing data in biology consists of natural language texts
used to describe and communicate new discoveries, and
therefore scientific papers constitute a resource with
crucial importance for life sciences. As the amount of
scholarly communication increases, it is increasingly dif-
ficult for specific core scientific statements to be found,
connected, and curated.
The biomedical text mining community has been
working for a long time on the development of reliable
information extraction applications. Both named entity
recognition and conceptual analysis are needed in order
to map from natural language texts to a formal represen-
tation of the objects and concepts represented by the
text, with direct links to online resources that explicitly
expose those concepts as semantics.
Different Web tools allow researchers to search literature
databases and integrate semantic information extracted
from text with external databases and ontologies. At the
time of the BioHackathon, Whatizit [92,93], Reflect [94-96]
and Medie [97,98] provided Web APIs for free without any
registration, and the developers and collaborators of the
latter two services were members of this sub-group. There-
fore, this BioHackathon sub-group concentrated on using
these three tools to develop a methodology to provide RDF
triples for PubMed literature. Further, we investigated how
to embed annotations into the XHTML output from these
tools, and selected RDFa (RDF in attributes [99]) a suitable
technology for this. RDFa enables the embedding of RDF
triples within XHTML documents, and also enables the
extraction of RDF triples by compliant user-agents.
Data querying and interoperability of resources
Implementation
The BioHackathon participants investigated many differ-
ent facets of the existing Semantic Web technologies to
help data provision and querying in the life sciences.
Working with semantic data requires a variety of spe-
cialized software and libraries:
 Libraries for reading, transforming and writing RDF
 SPARQL servers (endpoints) for querying
 Semantic registries or indices enabling discovery and
querying of data, e.g., SADI
 User interfaces and visualization tools such as
RDFScape [100].
Although SPARQL offers the possibility of large-scale
distributed flexible discovery and querying of data, itrelies on widespread provision of RDF data through
SPARQL endpoints. Realistically, it will take time to mi-
grate existing resources to these new standards because,
while producing RDF is not difficult in principle, produ-
cing RDF that has the positive features of Linked Data
(like Cool URIs) and well-planned predicates is not triv-
ial. Therefore, discussion centered on whether there are
useful improvements that can be made in advance of
any wholesale switch to RDF generation. We considered
whether communication between Cytoscape [22,23], the
data warehouse systems BioMart and InterMine, and the
workflow design tool Galaxy, could be made more robust
and lower maintenance. It was felt that the current situ-
ation could be improved by avoiding bespoke plugins for
communication. For Galaxy [101,102], the user is respon-
sible for making sure the data is of the correct type and
format for the analysis being performed. For communica-
tion between Galaxy and BioMart neither side deals
with semantics yet, but generally interoperation was
considered to be sufficient. This was also the case for
interoperation between InterMine-based systems and
Galaxy. For Cytoscape and BioMart interoperation, the
existing REST API was deemed sufficient for data re-
trieval. We now detail the various new and changed
features in these various tools.
Individual BioMart deployers decide on their own meta-
data layer, and the BioMart Graphical User Interface
(GUI) uses this layer to plan/construct queries, rather than
the data itself. Although the GUI provides a number of
options, there is nothing preventing a deployer from using
non-standard names. Thus, to achieve interoperation be-
tween different BioMart instances, it is essential to har-
monize the semantics used by each deployer. For example,
if two BioMart instances call a data field a UniProt identi-
fier, it is the responsibility of the two deployers to con-
firm that they are, in fact, referring to the same data-type
values. As a lightweight solution, the Hackathon attendees
suggested that it would be useful to have a controlled
name-space for such use and/or a hand-shake between
integrated instances to check that matching values are
being used. Similarly, instances of the InterMine ware-
house can ask each other what data they provide but do
not confirm that the name-spaces are compatible. It
would be beneficial if this could be checked automatically.
Likewise, interoperation would be easier if the InterMine
and BioMart could automatically discover the data that
are provided by any given warehouse instance. It was
agreed that it would help to have a more formal descrip-
tion of data and services and this would make planned
interoperation between InterMine and BioMart easier to
implement. One simple improvement would be passing
headers with data column descriptions. Experience at the
Ontario Institute for Cancer Research (OICR) suggests
that available data-describing controlled vocabularies are
Katayama et al. Journal of Biomedical Semantics 2013, 4:6 Page 10 of 17
http://www.jbiomedsem.com/content/4/1/6rather limited. As such, a move towards Semantic Web
technologies in the near future would be extremely benefi-
cial to these two projects.
Data exchange
File formats
The preceding section demonstrates that agreement on
semantics is critical regardless of the use of formal
Semantic Web technologies for representation and ex-
change of information. This is useful even at the relatively
primitive level of using consistent terms to describe differ-
ent file formats. For instance, the Open Bio* projects
(BioPerl, Biopython, BioRuby, BioJava and EMBOSS) have
effectively agreed to a file-format naming system covering
major sequence formats, alignment formats, and assorted
tool output formats. Similarly the XML descriptions of
different tools in Galaxy describe valid input formats and
the generated output formats by string based names.
It was felt that, to facilitate automated interoperability, it
would be useful to have a common machine-readable
namespace that allows one to assert something is a par-
ticular file format. However, agreeing on shared terms for
different file formats is still only a first-step; the content of
that file can also influence how a piece of software might
need to operate on it. For example, the FASTA format can
represent quite diverse data: nucleotide vs protein sequen-
ces, single records vs multiple records, use of ambiguity
codes or not, soft masked vs plain sequence, and gapped
vs ungapped. Using a more extensive and formal naming
scheme able to capture these kinds of details, such as a
URI, would enable the encapsulation of meta-data about
the type of file and its contents, enabling validation and
providing a means to prevent inappropriate actions such
as using protein queries for a nucleotide sequence search.
Likewise, if a set of records from two sources were
received, it would be possible to ensure that they were
conforming to the same representation as well as to the
same biological entity. The EDAM ontology (described
below) was selected as an appropriate annotation vocabu-
lary for this purpose.
File formats vs. labeling of data itself
As mentioned above, combined analysis of separate data
sources involves cross-referencing record fields whose
names are under the control of individual data providers,
and therefore may not be shared in common, or may be
shared, but used to refer to different data entities. To
alleviate this, BioMart and InterMine agreed to begin
comparing their database column-headings to find those
with a common intent, and work jointly towards a com-
mon name. For example, both provide data headed "GO
identifier", but it is also necessary to assert, to software,
that the columns from the two sources are truly equiva-
lent. A URI-fragment was chosen as the solution to thisproblem; the URI-fragment indicates the namespace of
data in that column, and when concatenated with the
identifier in that column, becomes the URI of the entity in
that column cell. Columns between two datasets can then
be compared based on these URI-fragments. The URIs
used to label columns were chosen from the EBI Ontology
Lookup Service (OLS) [103]. For instance, http://www.ebi.
ac.uk/ontology-lookup/?termId=MI%3A0448 provides an
identifier, MI:0448, for Gene Ontology. Consideration was
given to the need for a central naming authority/name-
space provider. As well as the OLS, RDF data is provided
by UniProt, and thus the EBI and corresponding institutes
in Japan (DDBJ) and the US (NCBI) were considered nat-
ural coordinators for this role. It was further agreed that
these decisions would not be imposed on individual
providers, but that all providers would be encouraged to
make such a migration as quickly as possible.
Versioning of data
A related point was that of being explicit about which ver-
sion of the data is being referred to in a given RDF triple
(e.g., the genome build/annotation release). For example,
there are extant issues with public resources being
generated from specific, now legacy, versions of genome
datasets, e.g., Affymetrix microarrays. It is neither easy,
nor desirable to enforce the use of just one version of the
genome and the corresponding updating of older
resources, and therefore there must be some clear way of
tracking URIs relating to the same, re-versioned entity.
Ensembl [104,105] has addressed this problem for all of
its hosted genomes, where the mapping of their versions
to similar resources hosted by themselves and others (e.g.,
from UCSC [106] and between Ensembl releases) are
systemized. A combination of the assembly version and
Ensembl gene-build version are sufficient to resolve all
ambiguities. Therefore, it was discussed that, when min-
ting URIs for an RDF representation of Ensembl data, the
URI schema would be based on Ensembl genome/annota-
tion versions until another approach was deemed neces-
sary. In addition, these URIs would be associated to other
versions (both local and remote) using an appropriately
named predicate.
modMine [107], an InterMine instance that is the central
data integration point within the modENCODE project,
records the genome version against which data were
generated. However, at the time of the BioHackathon,
exported data did not include the genome version, and it
was therefore recommended that genome version informa-
tion be included in all exported data.
Galaxy preserves data provided for each run of a Galaxy
workflow. This is useful with respect to reproducibility and
clarity about the analysis that was done on any given day,
even over the course of a multi-year project. However,
while Galaxy encourages users to record important
Katayama et al. Journal of Biomedical Semantics 2013, 4:6 Page 11 of 17
http://www.jbiomedsem.com/content/4/1/6metadata such as the genome build version, it does not
enforce this. As such, users should be aware that it is their
responsibility, even in Galaxys metadata-preserving envir-
onment, to provide the metadata critical to comprehen-
sively describing the experiments they are running using
the Galaxy tool.
Finally, BioMart and the UCSC genome browser pro-
vide version information, but they do not use the same
namespace to describe this version information in every
case. Thus, it can be difficult to automatically determine
that two datasets are from the same version/build.
Data exchange conclusions
 Current systems are arbitrary but in fact can be
made to work with close coordination of the
development teams. This, however, is an unusual
case that may only be applicable in a BioHackathon
situation. Moreover, other software will have to
engage in a similar re-engineering process to use
these data sources.
 A namespace for file formats would be useful -
candidates include the EDAM (EMBRACE Data and
Methods) ontology [108] and myGrid-Moby Web
service ontologies [109], which lists several of the
more popular formats (e.g., GenBank flatfile) and
could be extended with additional formats over time
(by the time of this writing, the myGrid-Moby data-
type ontology has been non-redundantly imported
into the appropriate portion of the EDAM ontology,
so this dichotomous choice no longer exists, and
EDAM should be considered canonical in preference
to the BioMOBY ontologies).
 A namespace for columns of tabular data would be
useful. This utility could also be extended to
describe data in more generic formats such as XML,
though this was not deemed necessary at this time.
 At the BioHackathon there was general agreement
among these sub-group members that the
identification of the data column itself was more
immediately important for data exchange and
interoperability than assigning URIs to each data
element in that column; however the latter seems
plausible to automate if the column headers are
themselves URIs. Shared, human-readable column
names, however, provide a temporary solution to
this integration problem through specific
engineering of the client software to recognize and
respond to these names correctly.
RDF for interoperation
If all resources expressed their data as RDF, using a
shared URI scheme for third-party data elements, large-
scale data integration would be dramatically facilitated,whether that be a conventional warehouse or a triple
store. This, however, leads to the barrier discussed earl-
ier relating to agreement on URI schemes, and how to
accommodate RDF-linking to data from third-parties
who have not yet made a commitment to either RDF or
even stable URIs. In practice, RDF warehouses (such as
Bio2RDF) are making these decisions on their own, and
(effectively) imposing their own URIs schemes on data
elements that they do not own. Source providers, how-
ever, find this objectionable, and even in the context of
the BioHackathon community, would not agree to use
these imposed URIs as their own URIs, even in cases
where they did not publish stable URIs. This will lead to
quite serious conflicts when the individual data pro-
viders begin publishing their own RDF, since statements
(triples) will have been made throughout the Semantic
Web that involve the non-canonical URIs coming from
the RDF warehouses. Each of these non-canonical state-
ments will then have to be explicitly mapped to the
provider-published URI at some later point in time - a
task that is effectively impossible to achieve.
This remains a difficult problem to overcome since at
its core is the different pressures of need versus resour-
cing. For instance, the modENCODE and OICR Cancer
Genome projects, represented at the BioHackathon, pro-
vide original data and therefore could generate their
own canonical URIs; however, the pressures of delivering
for their target communities specific needs interferes
with their desire to transition to RDF publishing, due to
both the additional infrastructure required, as well as
the additional rigor that RDF publishing can, when done
with care, necessitate. As such, this BioHackathon group
suggested that a balance was needed between cleanli-
ness/rigor and making the data available immediately in
RDF. Nevertheless, they recognized that an advantage of
generating RDF was that it forced producers to think
more clearly about how they defined their data elements,
and that this was of long-term benefit to both the
providers themselves as well as the community.
Care is still needed, however, since low quality or un-
stable RDF can severely damage interoperability long-
term; RDF is specifically intended to be built-upon and
extended by third-parties on the Semantic Web. When
dramatic changes are made to either the URIs or the na-
ture of the statements being made about them, contradict-
ory, even nonsensical information can result. For example,
if the provider first publishes a URI that represents a gene
locus record, and later realizes that this URI more accur-
ately represents a specific Open Reading Frame (ORF),
third party statements made about that URI as a locus will
now resolve to statements about that ORF. Providers are
cautioned, however, that these semantic shifts can also be
quite subtle. For example, when a provider publishes a
URI representing a protein (which might be the source of a
Katayama et al. Journal of Biomedical Semantics 2013, 4:6 Page 12 of 17
http://www.jbiomedsem.com/content/4/1/6protein-binding interaction triple in RDF) and then changes
the meaning of that URI such that it now represents the
database record for a protein (which might be the subject
of triples about authorship, editorial changes, record-length,
etc.), a Semantic Web agent can make extremely trouble-
some data integration errors. Since on the traditional Web,
these semantic differences are never taken into account
(because humans can automatically make the distinctions),
publishing RDF requires additional care on the part of the
provider compared to traditional Web publishing.Tools and clients for analysis and visualization
Open Bio*
A rare opportunity provided by BioHackathon 2010 was
that representatives from the BioRuby, Biopython, BioPerl
and BioJava communities were in attendance with a com-
mon goal - the production and consumption of RDF and
associated technologies. An initial survey among differ-
ent languages reported that the core RDF data format
is very well supported by Java, Perl, Python, and Ruby
libraries (Table 1). Considering RDF as a data model
used mainly in the back-end for building a knowledge
base, or in the middleware for data exchange, the prior-
ity of Open Bio* projects is the capacity to consume
SPARQL endpoints easily, providing support to the end
user for a better experience. To achieve this goal and
for better uniformity, the BioHackathon encourages de-
velopment of a set of shared APIs among the Open
Bio* projects.
With respect to supporting SPARQL endpoints, query
systems such as the one provided by BioMart and FlyMine
were considered good starting points from the perspective
of interface behavior. The notable thing in these systems
is that the user can explore the data/knowledge while dy-
namically building the result set. Using a similar approach,
it was proposed that Open Bio* projects should access in-
dividual SPARQL endpoints with the goal of dynamically
generating cross-domain queries fragment-by-fragment.
This design, known as a Builder Pattern, accumulates
filters and attributes stepwise (e.g., add_filter as an API
method) during exploration of large, dispersed data-sets
such as BioMart, InterMine, and Bio2RDF; this object
can then be converted into one or more SPARQL queries
to reproduce the exploration process in its entirety. The
Biopython and BioRuby projects have implementedTable 1 RDF/SPARQL libraries for each programming languag
Language Library
Ruby ActiveRDF, RDF.rb
Java Jena
Python RDFLib
Perl PerlRDFversions of such an interface, and others are under
development.
As the number of SPARQL endpoints grows, such
interfaces will become increasingly useful. The approach
of providing an object-oriented adaptor API on top of
SPARQL endpoints, which has a similar look and feel to
other OpenBio* project APIs, eases the transition to using
Semantic data. Current limitations are, not unexpectedly,
that custom code must be written for endpoints that do
not use shared ontologies and graph structures, and that
currently the system does not generalize to arbitrarily
complex SPARQL queries. Nevertheless, its primary utility
is to encapsulate common query cases into a familiar
interface that encourages skilled users to use and explore
data provided by Semantic Web data sources.
Visualization
This section focuses on the development of tools and
methods to help the end-user - the researcher - consume
data exposed over the Semantic Web. This sub-groups
BioHackathon activity was centered around Cytoscape
and the development of plugins to make it possible to
consume RDF data on the Cytoscape platform. Our efforts
have been complemented by the development of inter-
faces in the G-language project [15], and discussions
on the RelFinder [110,111] which is especially useful to
explore locally-stored RDF data and high-performance
SPARQL endpoints.
RDFScape [100] is a tool that allows the use of
Cytoscape as an interface to browse and query Semantic
Web knowledgebases in an interactive way. This visuali-
zation can be tuned via a variety of features, ranging from
coloring or filtering information by namespace, treating
datatypes as attributes, or even providing a semantic view
of the content of the knowledge base, via the application
of custom inference rules (Figure 2). Our sub-groups
efforts centered around the re-design of RDFScape with a
reference use case from the molecular interaction session
(described in the Molecular interactions section).
During the BioHackathon, the general robustness and
the quality of the user experience was improved in the
following ways: First the installation procedure has been
dramatically simplified, such that RDFScape can be in-
stalled in the same way as any other Cytoscape plugin.
In addition, we have re-designed the user interfaces, ra-
tionalizing the layout of commands and improving thee
Web site
http://activerdf.org/, http://ruby-rdf.github.com/rdf/
http://jena.apache.org/
http://www.rdflib.net/
http://www.perlrdf.org/
Katayama et al. Journal of Biomedical Semantics 2013, 4:6 Page 13 of 17
http://www.jbiomedsem.com/content/4/1/6flow of user interaction, where many actions now have an
intuitive default. RDFScape provides analysis contexts or
workspaces - pre-assembled settings for specific know-
ledge bases. If one of these analysis contexts is provided,
the new user interaction flow does not require any deci-
sion to be taken from the user in order to begin exploring
the knowledge base. Finally, the features of RDFScape
were extended, and the user experience simplified, by pro-
viding three different RDFScape modes of operation, to be
selected at start-up: 1) as an interface to query and
visualize remote SPARQL endpoints, 2) to visualize and
analyse a local knowledge base, and 3) for interactive ana-
lysis and reasoning on ontologies in the context of experi-
mental data.
User interaction
Because Semantic Web data is stored in the form of
triples, a feasible interface to query the data might be to
begin with a keyword, display a list of available predicates
related to that keyword, and from there provide the list of
possible objects. A ring interface is an effective implemen-
tation for this purpose, whereby the predicates and objects
are displayed as a series of rings. Each ring contains a
limited set of possible connections for the given data,
shown graphically with icons.
Searches and queries of biological data take place pri-
marily via the Web. Therefore, a generic querying inter-
face should be able to run on any website. Moreover, it
should run without any installation of specialized soft-
ware or plugins. The G-language Bookmarklet [112] is
implemented as a bookmarklet coded with HTML/CSS/
JavaScript, runs on almost all browsers, and can be in-
voked from any website. By selecting keywords of inter-
est within any webpage and opening the G-language
Bookmarklet, an array of icons in the shape of a ring
appears with animation on top of the webpage that the
user is currently browsing (Figure 3).
Most biological data is well-curated and cross-referenced
by major database providers. An emergent Semantic Web,
therefore, would complement this existing data by making
JOURNAL OF
BIOMEDICAL SEMANTICS
Jiang et al. Journal of Biomedical Semantics 2013, 4:11
http://www.jbiomedsem.com/content/4/1/11RESEARCH Open AccessUsing Semantic Web technology to support
icd-11 textual definitions authoring
Guoqian Jiang*, Harold R Solbrig and Christopher G ChuteAbstract
The beta phase of the 11th revision of International Classification of Diseases (ICD-11) intends to accept public input
through a distributed model of authoring. One of the core use cases is to create textual definitions for the ICD
categories. The objective of the present study is to design, develop, and evaluate approaches to support ICD-11
textual definitions authoring using Semantic Web technology. We investigated a number of heterogeneous
resources related to the definitions of diseases, including the linked open data (LOD) from DBpedia, the textual
definitions from the Unified Medical Language System (UMLS) and the formal definitions of the Systematized
Nomenclature of MedicineClinical Terms (SNOMED CT). We integrated them in a Semantic Web framework
(i.e., the Linked Data in a Resource Description Framework [RDF] triple store), which is being proposed as a backend
in a prototype platform for collaborative authoring of ICD-11 beta. We performed a preliminary evaluation on the
usefulness of our approaches and discussed the potential challenges from both technical and clinical perspectives.
Keywords: Semantic Web Technology, RDF, SPARQL, ICD-11, SNOMED CT, DBpediaIntroduction
The 11th revision of International Classification of Diseases
(ICD-11) was officially launched by the World Health
Organization (WHO) in March 2007 [1]. A 3-tiered con-
tent model (see more details in Background section) has
been proposed and discussed under WHO Topic Advisory
Group on Health Informatics and Modeling [2]. The pur-
pose of the ICD-11 content model is to present the know-
ledge that underlies the definitions of an ICD entity.
Starting in May 2012, the beta phase of the ICD-11 revi-
sion intends to accept public input through a distributed
model of authoring. One of the core use cases is to create
the textual definitions for each ICD category. The param-
eter textual definitions is described by WHO as, Each
ICD concept will be accompanied by a written definition
of its descriptive characteristics. This full text definition en-
ables human users to understand the meaning of a concept
for classification, translation and other reasons [2].
The provision of textual definitions has been regarded
as one of important criteria for measuring the quality of
a terminology/ontology [3]. A well-structured human-
readable definition, by distinguishing one entity from* Correspondence: jiang.guoqian@mayo.edu
Department of Health Sciences Research, Mayo Clinic, 200 First St SW,
Rochester, MN 55905, USA
© 2013 Jiang et al.; licensee BioMed Central L
Commons Attribution License (http://creativec
reproduction in any medium, provided the oranother, may serve as the basis for the formal definition
(i.e., a computational definition of a class or category, usu-
ally expressed in description logic) of an entity. While
human-readable definitions may be more complete and de-
tailed than formal definitions, there still should not be any
discordance between them.
The objective of the present study is to design, develop,
and evaluate approaches to support ICD-11 textual defini-
tions authoring using Semantic Web technology. We in-
vestigate a number of heterogeneous resources related to
the definitions of diseases, including the linked open data
(LOD) from DBpedia, the textual definitions from the Uni-
fied Medical Language System (UMLS) and the formal def-
initions of the Systematized Nomenclature of Medicine
Clinical Terms (SNOMED CT). We integrate them in a
Semantic Web framework (i.e., the Linked Data in a Re-
source Description Framework [RDF] triple store), which
is being proposed as a backend in a prototype platform for
collaborative authoring of ICD-11 beta. We perform a pre-
liminary evaluation on the usefulness of our approaches
and discuss the potential challenges from both technical
and clinical perspectives.td. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Jiang et al. Journal of Biomedical Semantics 2013, 4:11 Page 2 of 9
http://www.jbiomedsem.com/content/4/1/11Background
ICD-11 and its content model
Historically, ICD was developed to support international
comparison of mortality statistics. WHO has embraced a
broadened set of use cases to drive ICD-11 development,
including scientific consensus of clinical phenotype
(definition and criteria), public health surveillance (e.g.,
mortality and morbidity), and clinical data aggregation [4].
Each ICD entity can be seen from different dimen-
sions. The content model represents each one of these
dimensions as a parameter. Currently, there are 13 de-
fined main parameters in the content model to describe
a category in ICD. Table 1 illustrates that Textual Defi-
nitions is one of main parameters for describing an ICD
category.SNOMED CT and its canonical forms
SNOMED CT is the most comprehensive, clinically ori-
ented medical terminology system. It is owned and
maintained by the International Health Terminology
Standard Development Organization (IHTSDO) [5], and is
now specified in the US, UK, and several other countries
as a preferred or required terminology for coding clinical
problems and other aspects of the electronic health record.
IHTSDO and WHO signed a collaborative agreement in
July 2010, aimed at enabling harmonization of WHO Clas-
sifications and SNOMED CT, which essentially establishes
SNOMED CT as the core of the ontological component of
ICD [6].
SNOMED CT adopted a description logic foundation
that has allowed its curators to formally represent con-
cept meanings and relationships. SNOMED CT pro-
posed the canonical (or normal) forms for its concept
codes [7]. A normal form is a view that can be generated
by maximally decomposing any valid expression byTable 1 The ICD-11 content model main parameters
1 ICD Entity Title
2 Classification Properties
3 Textual Definitions
4 Terms
5 Body System/Structure Description
6 Temporal Properties
7 Severity of Subtype Properties
8 Manifestation Properties
9 Causal Properties
10 Functioning Properties
11 Specific Condition Properties
12 Treatment Properties
13 Diagnostic Criteriaapplying a set of logical transformation rules. The
purpose of generating normal forms is to facilitate
complete and accurate retrieval of precoordinated and
postcoordinated SNOMED CT expressions from clinical
records or other resources. Two alternative normal
forms are proposed: the long canonical form and the
short canonical form. We used the short canonical form
to generate the structured definition for a SNOMED CT
code (see the Methods section).
iCAT and ICD-11 alpha authoring
WHO initially adopted Web-Protégé for the alpha phase
of ICD-11 development and the tool is called iCAT.
iCAT is a variant of Web-Protégé, which is a web-based
application using Google Web Toolkit (GWT) technol-
ogy [8].
For the alpha process, the user community is relatively
small as the main task is to augment rubric definitions
and review of elements in the foundation component of
the ICD. However, in the beta phase, the ICD will be
reviewed publicly and the number of user community
could potentially be large. The scalability issue of the
iCAT tool will be challenged, as multiple users work on
the same copy of an evolving ICD category in that tool.
LexWiki and the proposal-based mechanism
LexWiki is an effort led by Mayo Clinic for development
of a collaborative authoring platform for large-scale bio-
medical terminologies [9]. The LexWiki environment
based on Semantic MediaWiki [10] enables the wider
community to make both structured and unstructured
proposals on the definitions of classes and property values,
suggest new values, and make corrections to the current
ones. LexWiki currently is at the core of community-
based development of Biomedical Grid Terminology [11]
and has also been successfully implemented to support
the Common Terminology Criteria for Adverse Events re-
vision project [12] and the Clinical Data Interchange Stan-
dards Consortium (CDISC) Shared Health and Research
Electronic Library project [13].
Semantic Web technology
The World Wide Web Consortium (W3C) is the main
international standards organization for the World Wide
Web. Its goal is to develop interoperable technologies
and tools as well as specifications and guidelines to lead
the Web to its full potential. W3C recommendations
have several maturity levels: Working Draft, Candidate
Recommendation, Proposed Recommendation, and W3C
Recommendation. RDF, a W3C recommendation, is a
directed, labeled graph data format for representing infor-
mation in the Web [14]. The Linked Data uses the RDF
data model that encodes data in the form of subject,
predicate, and object triples. SPARQL (SPARQL Protocol
Jiang et al. Journal of Biomedical Semantics 2013, 4:11 Page 3 of 9
http://www.jbiomedsem.com/content/4/1/11and RDF Query Language) is a query language for RDF
graphs. SPARQL queries are expressed as constraints on
graphs, and return RDF graphs or sets as results. SPARQL
1.0 has been a W3C recommendation whereas SPARQL
1.1 is a Working Draft [15,16]. Triplestore is a database
for the storage and retrieval of RDF metadata, ideally
through standard SPARQL query language.
Methods
Proposed collaborative authoring framework for ICD-11
Beta
Figure 1 shows the system architecture we proposed for
the ICD-11 beta collaborative authoring platform. In the
client side, we chose to use the SmartGWT rich widget
library [17] and Liferay portal system [18] to develop the
user interface. In the server side, we chose to use an
RDF store for ICD-11 contents and metadata persist-
ence. We used the GWT Remote Procedure Calls tech-
nology to realize the communication between the client
and the server. Besides utilizing the ICD-11 content
model, we enable a proposal provenance model. The
model is used to represent the provenance data required
for the implementation of a proposal-based authoring
mechanism informed by our previous work on the
LexWiki system [9].
As the ICD-11 beta will be based upon the contents of
the ICD-11 alpha, we developed a transformation pipe-
line to convert the ICD-11 alpha data into the Semantic
Web format. The ICD-11 alpha data is rendered in the
MySQL relational database format and the db dump is
available for download [19]. We utilized the D2R
technology [20], defined a D2RQ mapping file, andFigure 1 System architecture of proposed ICD-11 beta collaborative aconverted the relational database to the RDF triples.
Since the D2R server did not support those SPARQL 1.1
features required for the authoring purpose (i.e., the
RDF graph update operations), we dumped the RDF
triples utilizing the RDF dump feature of the D2R.
In a prototype implementation, we adopted the 4store
that is a scalable open source RDF database developed at
the Garlik [21]. We loaded the RDF dump from the
D2RQ transformation using the 4store built-in import
script. With the ICD-11 contents loaded in the RDF
store, we were able to define the standard SPARQL
queries to access the contents through the 4store built-
in SPARQL endpoint, and to utilize its SPARQL 1.1
features for the authoring purpose. Table 2 shows a
SPARQL query example to get all chapter labels and
codes of ICD-11. The similar queries are used to extract
the data to build the ICD-11 category hierarchy in the
user interface.
Proposed system design for textual definitions authoring
Figure 2 shows the system design for the use case of
textual definitions authoring, chosen as an initial proto-
type. We integrated 3 heterogeneous resources related
to the definitions of diseases, including the LOD from
DBpedia [22], the textual definitions from the UMLS
[23], and the formal definitions of SNOMED CT [5].
Textual definitions from DBpedia
To utilize the LOD data in DBpedia, we accessed its
SPARQL endpoint at http://dbpedia.org/sparql. We de-
fined a SPARQL query and extracted those instances
with the type of disease. Table 3 shows the SPARQLuthoring platform.
Table 2 The SPARQL query example to get all chapter labels and codes of ICD-11
SPARQL Query Note
SELECT DISTINCT ?label ?code To get all chapter labels and codes of ICD-11
{ GRAPH <http://who.int/icd>
{ <http://who.int/icd#ICDCategory>
<http://who.int/icd/vocab/resource/DIRECT-SUBCLASSES> ?child.
?child <http://who.int/icd/vocab/resource/DIRECT-SUPERCLASSES> ?parent.
?child rdfs:label ?label .
?child <http://who.int/icd#icdCode> ?code.
} } ORDER BY ?label
Jiang et al. Journal of Biomedical Semantics 2013, 4:11 Page 4 of 9
http://www.jbiomedsem.com/content/4/1/11query that retrieves the information of label, abstract,
MeSH ID, and the corresponding WikiPedia link for the
instances with the type of disease (defined in the
DBpedia ontology as http://dbpedia.org/ontology/Dis-
ease). We consider the abstract information closely cor-
responds to the definition. We used the MeSH ID as an
anchor to map the DBPedia disease definitions to those
corresponding codes in both SNOMED CT and ICD-10
through the UMLS concept unique identifiers (CUIs).
Textual definitions from UMLS
The textual definitions from UMLS had already been
imported into the original ICD-11 alpha database for a
portion of ICD categories. Therefore, we were able to ac-
cess the textual definitions just by defining the SPARQL
queries against the RDF dump that was loaded into the
ICD-11 RDF store as described in above section. Table 4Figure 2 System design for the use case of textual definitions authorshows a SPARQL query example that extracts the defin-
ition and its metadata for a specific ICD category
A19  Miliary tuberculosis. Table 5 shows the query
results.
Structured definitions from the SNOMED CT
We utilized the data files and the canonical table file of
the 20100731 International Release of SNOMED CT.
We defined simple grammatical rules that can be used
to render those elements in the short canonical form
into the structured definition that is more human-
readable to the domain professionals. Table 6 shows the
structured definition of Acute myocardial infarction
derived from its short canonical form.
We mapped the SNOMED CT codes and their corre-
sponding structured definitions with the ICD categories
represented by the ICD-10 codes through using theing.
Table 3 A SPARQL query against the SPARQL endpoint of the DBpedia to extract the disease definition information in
the language of English
SPARQL Query Note
SELECT DISTINCT ?label ?abstract ?meshId ?wikipediaLink To extract the disease
definition information
in the language of English
from Dbpedia
WHERE {
?s a <http://dbpedia.org/ontology/Disease>.
?s rdfs:label ?label.
?s <http://dbpedia.org/ontology/abstract> ?abstract.
?s <http://dbpedia.org/ontology/meshId> ?meshId .
?wikipediaLink <http://xmlns.com/foaf/0.1/primaryTopic> ?s.
FILTER (langMatches(lang(?label), "en") && langMatches(lang(?abstract), "en"))
}
Jiang et al. Journal of Biomedical Semantics 2013, 4:11 Page 5 of 9
http://www.jbiomedsem.com/content/4/1/11UMLS CUIs. We then rendered the mappings and defi-
nitions into the RDF triples and loaded them into the
ICD-11 RDF store in a separate graph model using the
4store built-in import script. Table 7 shows the mapping
between the ICD category I21 and the SNOMED CT
code 57054005 and its structured definition rendered
in the RDF triples.
System evaluation
We performed a preliminary evaluation on the useful-
ness of our approaches on textual definitions authoring
in the following aspects. First, we evaluated the coverage
of each definition resource. Second, we performed a case
study on 2 example ICD categories. We linked the defi-
nitions extracted from all 3 resources with each of the 2
categories and profiled the definitions using the ICD-11
content model. The purpose of this evaluation is to illus-
trate the potential gap between the textual definitions
and the formal definitions.
Results
We successfully transformed the ICD-11 contents into
the Linked Data in a RDF store, which is utilized as the
backend in a prototype of our proposed collaborativeTable 4 A SPARQL query example to extract the definition an
tuberculosis
SPARQL Query
SELECT DISTINCT ?label ?definitionContent ?ontologyId ?termId
{ GRAPH <http://who.int/icd>
{ <http://who.int/icd#A19> <http://who.int/icd#definitionPrefilled> ?pre
<http://who.int/icd#A19> rdfs:label ?label .
?prefilledDefinition <http://who.int/icd#label> ?definitionContent;
<http://who.int/icd#ontologyId> ?ontologyId;
<http://who.int/icd#termId> ?termId;
}}authoring system for ICD-11 beta project. To support
the use case of textual definitions authoring, we devel-
oped the approaches that integrated 3 resources using
Semantic Web technology. The resources comprised the
disease definitions from the LOD data in the DBpedia,
the textual definitions from the UMLS and the struc-
tured definitions from the SNOMED CT. Figure 3 shows
a screenshot of an initial user interface prototype illus-
trating how the textual definitions are leveraged in our
proposed collaborative authoring system.
From the LOD data in DBpedia, we extracted 2,735
distinct disease definitions and labels in the language of
English, as well as their corresponding MeSH Ids and
Wikipedia links. Using the MeSH IDs, we were able to
link the textual definition from the DBpedia with ICD
categories through the UMLS CUIs. In total, the disease
labels and definitions correspond to 2,463 distinct MeSH
IDs, which were mapped to 1,069 ICD categories repre-
sented by the ICD-10 codes.
From the ICD-11 RDF store, we identified 1,487 textual
definitions for 1,278 distinct ICD categories. The textual
definitions were mainly from 7 different coding schemes
of the UMLS, including the NCI Thesaurus (UMLS/
NCI2007_05E), the MeSH (UMLS/MSH2008_2008_02_04),d its metadata for a specific ICD category, A19  Miliary
Note
To extract the definition and
its metadata for a specific ICD
category, A19  Miliary
tuberculosisfilledDefinition .
Table 5 The query results for the definition of the ICD category A19  Miliary tuberculosis
Label DefinitionContent OntologId TermId
A19. Miliary tuberculosis An acute form of TUBERCULOSIS
in which minute tubercles are formed
in a number of organs of the body due
to dissemination of the bacilli through
the blood stream.
UMLS/MSH2008_2008_02_04 C0041321
Jiang et al. Journal of Biomedical Semantics 2013, 4:11 Page 6 of 9
http://www.jbiomedsem.com/content/4/1/11the Gene Ontology (UMLS/GO2007_02_01), and the Com-
puter Retrieval of Information on Scientific Projects
(UMLS/CSP2006), etc.
From the canonical table of the SNOMED CT, it
contained the short canonical forms of 96,235 SNOMED
CT concept ids from the branch of Clinical Finding.
Utilizing the grammatical rules we defined, we were able
to transform the short canonical forms into the struc-
tured definition for each of the codes. Through the
UMLS CUIs, we mapped 5,778 ICD categories repre-
sented by the ICD-10 codes to 6,122 SNOMED CT con-
cept ids.
As a case study, we randomly selected 2 ICD categor-
ies that had the definitions from all 3 sources, the I35.0
Aortic (valve) stenosis and the N17-N19 Renal failure.
Each category had 5 definition entries. We profiled each
entry of the definitions using the ICD-11 content model
parameters. Table 8 and Table 9 show the profiling re-
sults. The results indicated that the textual definitions
were more detailed than the structured definitions de-
rived from the formal definitions. In addition, we found
that most of definitions specified the supertypes but the
supertypes varied in different granularity. Taking the ex-
ample from Table 8, the supertypes specified for the
Aortic valve stenosis include a valvular heart disease,
a pathological constriction or a disease.
Discussion
In this study, we demonstrated that how Semantic Web
technology was leveraged to integrate heterogeneous dis-
ease definition data to support ICD-11 textual defini-
tions authoring. With the capacity of the RDF store, we
were able to integrate multiple, heterogeneous disease
definition resources in an agile manner. The underlying
RDF model encoding of knowledge in the form of triples
plays a key role on this as the RDF can be used as a
schema-less data representation format. This ensures theTable 6 The structured definition of Acute myocardial infarc
Definition
Acute myocardial infarction
is a Disease
that has Clinical course of Sudden onset AND/OR short duration
that has Associated morphology of Acute infarct
and has Finding site of Myocardium structureflexibility of our system. Using the powerful SPARQL
query language, we were able to access the definition
elements in the ICD-11 RDF store, as well as the exter-
nal LOD data services.
The textual definitions extracted from DBpedia are a
typical example of traditional human readable definitions
generated using a crowdsourcing model. The definitions
are actually harvested by DBpedia from Wikipedia, one
of the largest collaborative authoring platforms in the
world. DBpedia is a Linked Data project aiming to
extract structured contents from the information created
as part of the Wikipedia project. DBpedia allow users to
query relationships and properties associated with
Wikipedia resources, including links to other related
datasets [24].
Using the LOD service of DBpedia, we can easily
extract the shared definition data through standard
SPARQL queries for the purpose of the ICD-11 use case.
We found that the type Disease and the predicate
meshId defined in DBpedia ontology are very useful
for the extraction process. The MeSH IDs provided a
mapping bridge between the coding schemes like
SNOMED CTand ICD, which are utilized in this project.
In addition, the multilingual definitions are available
for most of disease instances in DBpedia, though we just
extracted those in the language of English. For example,
the definitions of the Aortic valve stenosis were avail-
able in 12 languages in DBpedia. We consider this may
provide added values for the ICD-11 project, as the
multilingual support is one of critical requirements for
the ICD-11 content authoring.
We also argue that the ICD-11 project may potentially
take advantage of the crowdsourcing model of Wikipedia.
Using this model, each ICD-11 category would be seeded
as a Wikipedia page for public input and the definitions of
the categories would be harvested using the DBpedia. And
then the WHO Topic Advisory Groups may just play ation derived from its short canonical form
Note
The structured definition of
Acute myocardial infarction
derived from its short canonical form
Table 7 The RDF triples in Turtle format rendered for the mapping between the ICD category I21 and the SNOMED
CT code 57054005and its structured definition
RDF Triples Note
<http://who.int/icd#I21> <http://who.int/icd#icdCode> "I21"; The RDF triples in Turtle
format rendered for the
mapping between the ICD
category I21 and the SNOMED
CT code 57054005and its
structured definition
<http://who.int/icd#definitionPrefilled> _:b0672.
_:b0672 <http://who.int/icd#label>
"Acute myocardial infarction Is a Disease and
has Clinical course of Sudden onset AND/OR short duration
that has Associated morphology of Acute infarct
and has Finding site of Myocardium structure " ;
<http://who.int/icd#ontologyId> "SNOMED CT" ;
<http://who.int/icd#termId> "C0155626";
<http://who.int/icd#sctId> "57054005".
Note that the label text is wrapped for the display purpose.
Jiang et al. Journal of Biomedical Semantics 2013, 4:11 Page 7 of 9
http://www.jbiomedsem.com/content/4/1/11role in reviewing the harvested definitions to ensure the
quality of the data.
The textual definitions from the UMLS had been
extracted using the mappings between the ICD-10 and
other coding schemes in the UMLS through their shared
CUIs. As the example illustrated in above section, an
ICD code can have multiple definitions from multiple
coding schemes identified. We consider this an important
JOURNAL OF
BIOMEDICAL SEMANTICS
Buttigieg et al. Journal of Biomedical Semantics 2013, 4:43
http://www.jbiomedsem.com/content/4/1/43RESEARCH Open AccessThe environment ontology: contextualising
biological and biomedical entities
Pier Luigi Buttigieg1*, Norman Morrison4, Barry Smith3, Christopher J Mungall2, Suzanna E Lewis2
and the ENVO ConsortiumAbstract
As biological and biomedical research increasingly reference the environmental context of the biological entities
under study, the need for formalisation and standardisation of environment descriptors is growing. The
Environment Ontology (ENVO; www.environmentontology.org) is a community-led, open project which seeks to
provide an ontology for specifying a wide range of environments relevant to multiple life science disciplines and,
through an open participation model, to accommodate the terminological requirements of all those needing to
annotate data using ontology classes. This paper summarises ENVOs motivation, content, structure, adoption, and
governance approach. The ontology is available from http://purl.obolibrary.org/obo/envo.owl - an OBO format
version is also available by switching the file suffix to obo.
Keywords: Environment, Ecosystem, Biome, OntologyBackground
Biologically motivated research is generating [1-3] and
archiving [4,5] ever-larger quantities of computerised data
from environmental samples. Simultaneously, biomedical
researchers have begun to take particular interest in the
physical environment of organisms at all scales, from
microbes to patients [6-9], while scientists in epidemi-
ology and public health are developing a stronger inter-
est in location- and environment-based information for
purposes of disease tracking [10,11]. In these complex
and data-rich fields, the need to describe systematically
the environmental context of biological entities is being
increasingly acknowledged as a means to mobilise data
for environment-aware analyses (see e.g. [12]).
It was the need for consistent description of the envir-
onmental origins of tissue, pathogen, and metagenomics
samples, together with a parallel need in the labeling
of samples and artifacts in museum collections that
precipitated the creation of the Environment Ontology
(ENVO). A series of meetings and workshops laid the
foundation for addressing these needs by establishing* Correspondence: pbuttigi@mpi-bremen.de
1HGF-MPG Research Group on Deep-Sea Ecology and Technology, Alfred
Wegener Institute, Helmholtz Centre for Polar and Marine Research, Am
Handelshafen 12, Bremerhaven 27570, Germany
Full list of author information is available at the end of the article
© 2013 Buttigieg et al.; licensee BioMed Centr
Commons Attribution License (http://creativec
reproduction in any medium, provided the orthe ENVO consortium and the ontology itself. ENVO is
comprised of classes (terms) referring to key environment-
types that may be used to facilitate the retrieval and inte-
gration of a broad range of biological data. In developing
ENVO, we recognized the many existing resources which
address, among other entities, environment-types [13-16]
and were motivated by the value of unifying such re-
sources in a foundational, or building block, ontology
developed within a federated framework and exclusively
concerned with the specification of environment types,
independent of any particular application. Thus, ENVO
was developed with the goal of interoperability with the
numerous biological and biomedical ontologies compli-
ant with Open Biomedical and Biological Ontologies
(OBO) Foundry principles [17,18] and is being aligned
to the Basic Formal Ontology (BFO 2.0 [19]; see below)
in aid of semantic homogeneity. Lastly, ENVO is designed
as an open project, poised to respond to the needs of its
users and draw from their insights. We hope that ENVO
will offer benefits similar to those of the Gene Ontology
(GO; [20]) in allowing a standardized and semantically
controlled representation of a domain central to life
science research in an open, community-led manner.
Classes describing natural environments currently dom-
inate ENVOs content as the ontology is geared towards
use in the biological domain. Nevertheless, ENVO isal Ltd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Buttigieg et al. Journal of Biomedical Semantics 2013, 4:43 Page 2 of 9
http://www.jbiomedsem.com/content/4/1/43suitable for the annotation of any record that has an envir-
onmental component. For example, one may use ENVO
classes to provide information on the environment of
remote sensing devices or of photographic image content.
Indeed, classes corresponding to man-made objects, for
example hypodermic needle [ENVO_ 02000000]a, umbrella
[ENVO_ 02000052], or terrarium [ENVO_00000349], are
included in the ontology. Further, ENVO offers termin-
ology resources both for specialists and for non-experts,
a feature particularly useful in scenarios where citizen
scientists and volunteers are involved in sampling or
observational campaigns (for example as described in [21]).
In this paper, we briefly describe ENVOs current con-
tent, structure, adoption, and governance model in order
to orient potential users and contributors. Readers should
be aware that ENVO is a living ontology shaped by mul-
tiple contributors and thus subject to change. However,
the ontology is under version control in a Google Code
repository [22] and historical changes are fully tracked.
More information is present in the Downloads section,
below.Results and discussion
In what follows, ontology classes (or synonymously,
terms), written in italics, are taken from ENVO unless
otherwise marked through the provision of an appropri-
ate namespace, as in PATO:cellular motility. The name-
space and unique identifier of each terms OBO Foundry
Uniform Resource Identifier, e.g. ENVO_00002297 for
environmental feature, will be included on first mention of
any class. Full URIs are of the form: http://purl.obolibrary.
org/obo/ENVO_00002297, and are resolved to OWL as
well as to human-readable web pages.Semantics of environment terms
While all biologists have an intuitive understanding of
what is meant by environment, a rigorous definition of
this class is non-trivial (see e.g. [23,24]). For example,
when taken simply as the surrounding space of an
entity, the causal relevance of an environment to that
entity as well as its boundaries are unclear. Consider a
population of humans in Biosphere 2 [25,26]. While it is
surrounded by the Santa Catalina Mountains (AZ, USA),
many environmental factors of this region have little
relevance to this populations biology and behaviour.
The ecosystems within Biosphere 2, however, are of
greater causal relevance and thus more appropriately
identified as the populations environments. Further,
confusion often arises when attempting to distinguish an
environment from a habitat or niche: the environment
an organism was observed in or isolated from may have
little to do with its habitat or its niche, as described, for
example, in [27].In an effort to clarify these concepts, work has been
done to align ENVOs four top-level classes to classes
from the Basic Formal Ontology (BFO; [19]), an upper-
level ontology that provides a semantic foundation for a
wide range of domain ontologiesb. Through this exercise,
a new subclass of BFO:material entity [BFO_0000040],
system, has been proposed to describe causally integrated
yet multi-component entities such as environments.
We propose that an environment (synonymous with an
environmental system [ENVO_01000254]) is a certain
sort of system which has the disposition to environ, that
is to contain within its BFO:site [BFO_0000029] and
causally integrate, some BFO:material entity. Examples
of environments range from rainforests to gut lumens
to the interiors of virally infected cells. As described
below, the subclasses of environmental system will refer-
ence environment-types familiar to most biologists.
ENVOs biome [ENVO_00000428] and habitat [ENVO_
00002036] classes are subclasses of environmental system.
The biome class represents environmental systems to
which resident ecological communities have evolved
adaptations. Thus, a biome may be thought of as a
community-centric ecosystem, whose extent is defined
by the presence of the communities adapted to it. This
requires that a biome possesses a degree of spatial and
temporal stability that has allowed at least some of its
constituent communities to adapt. Classes such as tundra
biome [ENVO_01000180] and coniferous forest biome
[ENVO_01000196] are included in ENVO. Currently,
the biome branch of the ontology makes no commit-
ment to a specific spatial or temporal scale. While bi-
omes are community-centric, ENVO treats habitats in
a population-centric manner: habitats refer to envi-
ronmental systems which include those components
needed to allow the survival and growth of a specific
ecological population. Our objective is to differentiate
between habitats and other environment types following
considerations such as those in [18]. The subclasses of
ENVOs habitat class are currently under review.
The environment-types described above are useful in
ecological settings; however, environments are often de-
scribed by referencing a single entity that has a strong
causal influence on its surrounding space. For example,
a coral reef environment is determined by the presence
and influence of a coral reef [ENVO_00000150]. Similarly,
the human gut environment is determined by the human
gut. Removal of either the coral reef or the human gut
would cause the associated environmental system to col-
lapse. Environmental systems of this kind make no specific
reference to ecological communities or populations (as do
biomes and habitats resp.), but to some central, supporting
feature. Entities that act in this way as the causal hubs or
supports of a given environmental system are referenced
by classes in ENVOs top-level environmental feature
Figure 1 Subclasses of ENVOs environmental condition may be
used as differentiae when defining subclasses of classes in the
biome (shown), environmental feature, or environmental material
hierarchies. Retrieval of entities annotated with ENVO classes that
satisfy a given condition is thus facilitated.
Buttigieg et al. Journal of Biomedical Semantics 2013, 4:43 Page 3 of 9
http://www.jbiomedsem.com/content/4/1/43[ENVO_00002297] hierarchy. For example, the envir-
onmental feature seamount [ENVO_00000264] would
support a seamount environment, i.e. an environmental
system which is supported by, and whose properties are
determined by, the presence of a seamount. Currently,
ENVO only includes classes for environmental features
and not the environmental systems associated with them.
Work to arrive at a formal definition of environmental
feature is ongoing. Current considerations are focused on
differentiating the environmental feature class from the BFO:
material entity class by defining a BFO:role [BFO_0000023]
which declares the environment-supporting nature of a
environmental feature.
In contrast to the classes above, which identify count-
able entities, the subclasses of the top-level environmental
material [ENVO_00010483] class refer to masses, vol-
umes, or other portions of some medium included in an
environmental system (for a full discussion of medium
see: [28]). A portion of environmental material is under-
stood to be more complex and variable in composition
than a simple collection of material entities (e.g. a collec-
tion of silicate particles). For example, the environmental
material soil [ENVO_00001998] typically contains aggre-
gates of fine rock particles, sand grains, clay particles, silt
particles, communities of animals, plants, fungi and mi-
crobes, small parts of organisms, organic matter, water in-
clusions, and airspaces. As is the case with environmental
feature, work on the definition of this class is ongoing.
This class is likely to be defined as a subclass of BFO:fiat
object [BFO_0000024] which forms the medium or part of
the medium an environmental system.
Lastly, ENVO includes the top-level class, environmental
condition [ENVO_01000203]. Subclasses of environmental
condition define specific ranges of determinate qualities
(e.g. a temperature range of 20  37°C, a solar irradiation
range of 426 W/m2 - 773 W/m2) or combination of qual-
ities that are present in an environmental system. These
may be used as differentiae with biome, environmental
feature, or environmental material classes as genera. For
example, the class subtropical broadleaf forest biome
[ENVO_01000201], includes the differentia has_condition
subtropical [ENVO_01000205] (Figure 1). Note that sub-
classes of environmental condition such as tropical, tem-
perate [ENVO_01000206], and polar [ENVO_01000238]
are intended to reflect qualities such as the degree of solar
irradiation received by an environment rather than refer-
ence geographic regions. A complete definition of these
classes has yet to be finalised and will be derived from
BFO:quality [BFO_0000019].
Where possible, the semantics of ENVO classes are
JOURNAL OF
BIOMEDICAL SEMANTICS
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37
http://www.jbiomedsem.com/content/4/1/37RESEARCH Open AccessPAV ontology: provenance, authoring and
versioning
Paolo Ciccarese1,2*, Stian Soiland-Reyes3, Khalid Belhajjame3, Alasdair JG Gray3, Carole Goble3 and Tim Clark1,2,3Abstract
Background: Provenance is a critical ingredient for establishing trust of published scientific content. This is true
whether we are considering a data set, a computational workflow, a peer-reviewed publication or a simple scientific
claim with supportive evidence. Existing vocabularies such as Dublin Core Terms (DC Terms) and the W3C
Provenance Ontology (PROV-O) are domain-independent and general-purpose and they allow and encourage for
extensions to cover more specific needs. In particular, to track authoring and versioning information of web resources,
PROV-O provides a basic methodology but not any specific classes and properties for identifying or distinguishing
between the various roles assumed by agents manipulating digital artifacts, such as author, contributor and curator.
Results: We present the Provenance, Authoring and Versioning ontology (PAV, namespace http://purl.org/pav/): a
lightweight ontology for capturing just enough descriptions essential for tracking the provenance, authoring and
versioning of web resources. We argue that such descriptions are essential for digital scientific content. PAV
distinguishes between contributors, authors and curators of content and creators of representations in addition to the
provenance of originating resources that have been accessed, transformed and consumed. We explore five projects
(and communities) that have adopted PAV illustrating their usage through concrete examples. Moreover, we present
mappings that show how PAV extends the W3C PROV-O ontology to support broader interoperability.
Method: The initial design of the PAV ontology was driven by requirements from the AlzSWAN project with further
requirements incorporated later from other projects detailed in this paper. The authors strived to keep PAV lightweight
and compact by including only those terms that have demonstrated to be pragmatically useful in existing applications,
and by recommending terms from existing ontologies when plausible.
Discussion: We analyze and compare PAV with related approaches, namely Provenance Vocabulary (PRV), DC Terms
and BIBFRAME. We identify similarities and analyze differences between those vocabularies and PAV, outlining strengths
and weaknesses of our proposed model. We specify SKOS mappings that align PAV with DC Terms. We conclude the
paper with general remarks on the applicability of PAV.
Keywords: Provenance, Authoring, Versioning, Annotation, Semantic web, AttributionBackground
Research in the life sciences is becoming increasingly
digital and collaborative. Scientists tend to conduct
their investigations and reporting using digital resources
(e.g., data artifacts, articles, etc.) obtained by aggregat-
ing existing resources (potentially generated as a result
of other research investigations conducted by other* Correspondence: paolo.ciccarese@gmail.com
Equal contributors
1Department of Neurology, Massachusetts General Hospital, 55 Fruit Street,
Boston, MA 02114, USA
2Harvard Medical School, 25 Shattuck Street, Boston, MA 02115, USA
Full list of author information is available at the end of the article
© 2013 Ciccarese et al.; licensee BioMed Centr
Commons Attribution License (http://creativec
reproduction in any medium, provided the orscientists), and processed and analyzed using manual or
automated workflows.
In such a context, scientists require a systematic means
to organize and annotate resources [1]. This might require
them, amongst other things, to: (i) trace the origin of a
given resource; (ii) specify its previous and subsequent ver-
sions; and (iii) identify the creators (be they humans or ma-
chines) responsible for the existence of the resource, as well
as the contributors who enriched and updated its content.
For the most general use, a provenance vocabulary that
meets these criteria must also be relatively compact and
terse.al Ltd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 2 of 22
http://www.jbiomedsem.com/content/4/1/37Several provenance vocabularies including Dublin
Core Terms (DC Terms) [2], PROV-O [3], OPM [4], and
Provenance Vocabulary [5] partially address these gen-
eral needs, at varying levels of richness, complexity and
maturity. In this section, after discussing the original use
case, we discuss the strengths and weaknesses of these
vocabularies and gaps in usage coverage, which led us to
develop the Provenance Authoring and Versioning
(PAV) ontology. In the Results section we also provide
mappings from PAV to PROV-O and DC Terms.
Original use case: SWAN platform
The Semantic Web Applications in Neuromedicine
(SWAN) web-based collaborative platform [6] is an ex-
ample of an application that embodies many of the
above requirements. SWAN aims to organize and anno-
tate scientific knowledge regarding neurodegenerative
disorders and to facilitate the formation, development
and testing of hypotheses. In particular, the AlzSWAN
[7] knowledge base (AlzSWAN KB), a collaboration of
SWANs developers with the AlzForum web community
of Alzheimer Disease researchers [8], is an instance of
SWAN configured to allow the scientific community of
Alzheimer Disease (AD) researchers to author, curate
and connect a diversity of data and ideas about AD. The
AlzSWAN curators typically read carefully a scientific
article, usually representing a hypothesis on AD, and
produce a linear representation of the embedded scien-
tific discourse: claims, hypotheses and questions. For
each of the discourse elements the curator selects re-
lated publications, proteins and genes. Knowledge in the
AlzSWAN KB is shared using the SWAN Ontology [9]
for interoperability.
One of the goals of the AlzSWAN KB consists in
clearly recording the provenance of the digital artifacts
as well as the provenance of the content or knowledge
elements represented by the artifacts, and the agents (or-
ganizations, people and software) involved in creating
and manipulating those artifacts. There is a clear distinc-
tion between the roles of the authors and curators, and
the source of content:
Authors are the primary originators of scientific state-
ments, originally conceiving the content (e.g. a tabular
dataset).
Curators collect the knowledge published by the au-
thors, interpreting and transforming the content of
a textual document into SWAN research statements
(hypothesis, claim or research question). They re-
structure the previously authored content and shape it
to be appropriate for the intended representation (e.g. by
normalizing the fields for being represented in a spread-
sheet). Curators create the SWAN KB version that
embodies the authors work; thus they are contributing
to the knowledge representation. However, the mainintellectual property remains attributed to the original
authors.
Artifact creators take care of physically creating the
digital artifact by entering the statements and their links
into the platform, (e.g. saves the spreadsheet as an .xlsx
file).
External sources are the external data- and knowledge-
bases such as PubMed [10] and UniProt [11,12] that
AlzSWAN draws upon for metadata and for integrated
data. Some of this metadata are retrieved and cached as
they are, while some are imported after one or more trans-
formations. It is important to track the original source and
how it was incorporated in the knowledge base.
As depicted in Figure 1, the AlzSWAN knowledge cap-
ture and curation process consists of several steps:
A PhD-level neuroscientist (the curator) reads care-
fully an article (written by authors) usually representing
a hypothesis on AD.
Based on the reading, the curator produces a textual
document with a linear knowledge representation of the
scientific discourse of the article, by building an ordered
list of claims, hypotheses and questions.
For each of those elements, the curator identifies ex-
ternal resources such as related publications, proteins
and genes. These resources provide data that can be
retrieved in unmodified form, or imported after a
transformation.
When possible, the formed representation is shared
with the authors of the original article for collecting
feedback.
The knowledge map is entered in AlzSWAN by a sec-
ond person (the artifact creator) through a web user
interface, which eventually encodes the textual content
according to the SWAN ontology.
While step 5 represents the straightforward creation of
the digital artifact, steps 14 represent the curation of
the knowledge that the authors expressed in the journal
article. Curation involves high-level domain knowledge
and acts of judgement and creative composition. Both
authors and curators originate digital content; author-
ship denotes the role of creative invention of a work,
while the artifact creator of a work in our terminology is
responsible for accurate transcription and encoding into
final digital form.
Figure 1 also depicts tasks of revision, publishing and
feedback collection. In particular, the feedback might
motivate the generation of a new version of the encoded
knowledge. Normally, in the case of AlzSWAN a new
version might include newly available evidence support-
ing a given claim.
Existing provenance vocabularies
There are existing vocabularies that at first appear to be
promising to address the needs of AlzSWAN and similar
1. Reading
AlzSWAN Workbench
Curator
4. Validation
Authors
Creator
AlzSWAN Browser
8. Publishing
9. Collecting feedback
Claims, Hypothesis, 
Research Questions,
Discourse List
 Citations, Genes,
Proteins
+
2. Knowledge Extraction
3. Linking
5. Encoding
6. Revision
7. Connecting
Authors
Journal Article
Pubmed, Entrez Gene, 
UniProt
Figure 1 Depiction of the AlzSWAN knowledge creation and publishing process.
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 3 of 22
http://www.jbiomedsem.com/content/4/1/37applications: Dublin Core Terms (DC Terms) [2],
PROV-O [3], OPM [4], and Provenance Vocabulary [5].
The Dublin Core Metadata Initiative (DCMI) pro-
vides core metadata vocabularies to support inter-
operable solutions for discovering and managing
resources. In particular, DC Terms provides terms for
specifying the entities that create and/or contribute to
the existence of a given resource. While terms such
as dct:contributor and dct:creator are useful and
popular, we argue that they conflate what we consider
as the distinct roles of contributor/author/curator and
creator of the representation. For instance, a person
who converts a web page from HTML 3 to HTML 5
could be said to be the dct:creator or dct:contributor
of the new document, even when that person has
not modified the human readable content of the
document.
DC Terms provides a means for specifying deriv-
ation and representational differences (dct:source, dct:
isVersionOf, dct:isFormatOf ), but these do not clearly
distinguish between a resource which was simplycopied, one which was transformed to give rise to a
new resource, or one which was further derived by
adding additional content (for example new scientific
evidence). For versioning DC Terms has dct:isVer-
sionOf, which implies substantive changes in content
(e.g. a movie can be a version of a theater play), and
dct:replaces, which indicates a superseded resource.
However, these statements do not quite fit with deal-
ing with and distinguishing between: smaller updates
(e.g. spelling mistakes); larger derivations (which
might no longer be versions of the original); and lin-
ear revision history without necessarily indicating pre-
vious versions as outdated. Consider, for example, a
health authority where Dr. Doe authored and imple-
mented the clinical guidelines for treating hyperten-
sive patients and sent it to Dr. Green for comments.
Green edits his copy to suggest extending a drug
treatment from 2 weeks to 4 weeks. Because Greens
guideline has not been approved yet, we cannot state
that his document replaces (dct:replaces) Does docu-
ment. Also, Greens document is in the same form
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 4 of 22
http://www.jbiomedsem.com/content/4/1/37and does not introduce substantive changes (dct:
isVersionOf ) from Does document.
PROV-O [3], an OWL ontology developed by the
W3C Provenance Working group [13] aims to provide a
standard for representing and exchanging domain-
independent provenance information between applica-
tions and systems. PROV-O provides terms that can be
used to trace the origin of a given resource, its deriv-
ation history, as well as the relationship between the re-
sources, and the entities that contributed to the
existence of the resource. PROV-O can be used at de-
tailed process level with activity-agent-entity interac-
tions, or at a higher level with shortcuts for entity-entity
and entity-agent relations such as prov:wasDerivedFrom
and prov:wasAttributedTo.
Overall PROV-O is a generic provenance data model,
which can be extended to give domain-specific proven-
ance, for instance by subclasses of prov:Activity or sub-
properties of prov:wasAttributedTo. PROV-O does not
itself provide any distinctions between authors, curators,
contributors or artifact creators.
PROV-O specifies a set of common extensions, which
at first glance would seem to cover some of our require-
ments: i) prov:hadPrimarySource: a kind of derivation
relation from secondary materials to their primary
sources, which were produced by some agent with direct
experience and knowledge about the topic, ii) prov:was-
QuotedFrom, the repeat of (some or all of ) an entity,
such as text or image, by someone who may or may not
be its original author.
However, on closer examination these terms are inad-
equate. For example, in the AlzSWAN project, curators
consulted papers on the web. Such an action cannot be
described using prov:hadPrimarySource, because the
documents consulted by the curators were not necessar-
ily primary sources [3] such as witness statements, re-
ports or interviews; we found the definition of primary
sources to be too narrow to cover most academic publi-
cations. Instead, we need a means to describe the fact
that the curators simply accessed a document. Curators
can also download a file from a source on the web, such
as UniProt. Although prov:wasQuotedFrom can be used
to relate the document to the source it was downloaded
from, it does not reflect that the document is a complete
and exact copy of the document in the source.
The Open Provenance Model [4] predates PROV-O,
and has a very similar approach to modeling provenance
by relating agents, artifacts and processes. By and large,
the concepts of OPM are covered by equivalent PROV-
O concepts, therefore the above analysis of PROV-O ap-
plies also to OPM.
DC Terms, PROV-O and OPM are domain-independent
and general-purpose vocabularies. Therefore the limi-
tations we identified above should not be perceivedas issues that need to be addressed within these vo-
cabularies. In fact, the PROV-O authors do not claim
that the vocabulary is complete. Instead, they encour-
age users to extend it with terms that capture their
domain needs.
The Provenance Vocabulary [5] describes data access,
creation, retrieval and publishing as detailed chains of prov:
Activity and prov:Entity relations. Terms like prv:accesse-
dResource, prv:createdBy, prv:retrievedBy seem relevant
for our use cases, but as detailed in the Discussion section,
this approach come at the cost of increased verbosity,
which we argue reduces the ability to query the provenance
in a consistent way.
For describing the provenance of the AlzSWAN use
cases such as the one depicted in Figure 1, we designed
the PAV (Provenance, Authoring and Versioning) ontol-
ogy, whose most recent version, PAV 2, maps to PROV-O.
In PAV, we do not attempt to model the whole chain of
process-oriented provenance like Open Provenance
Model (OPM) processes [4] and PROV-O activities [3],
or the many forms of metadata as covered by Dublin
Core Terms (DC Terms) [2]. Rather, in PAV we focus
on the provenance of a digital resource in terms of its
relationships with other digital resources and agents in-
volved in their creation, authoring and manipulation,
and we abstract away from the description of the activ-
ities (process) that manipulate and transform the digital
resources.Results
In this section we present the PAV ontology, describing
its structure and constituent terms. We go on to present
the systems and communities that have adopted PAV,
and to discuss how they use it. Finally, we present a col-
lection of mappings, specifying how PAV extends the
W3C PROV-O vocabulary.PAV Ontology
PAV is a lightweight vocabulary, for capturing just
enough descriptions essential for web resources repre-
senting digitized knowledge. PAV is intended to specify
Provenance, Authoring and Versioning information. Ac-
cordingly, this section is organized into three subsec-
tions. PAV properties are outlined in bold where they
are defined, described or exemplified; and in italics else-
where. Properties belonging to other vocabularies are al-
ways in italics.
The pav: prefix indicates the namespace http://purl.
org/pav/ [14], which also resolves to the latest version of
the PAV ontology as OWL. PAV is currently in version
2.2 (OWL importable from http://purl.org/pav/2.2 [15]).
Further details of versions and changes are listed on the
wiki [16].
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 5 of 22
http://www.jbiomedsem.com/content/4/1/37Authoring
In scholarly communication, it is crucial to be able to
precisely attribute the several forms of authorship (intel-
lectual property) or contributions of the knowledge con-
tent and of its representation [1]. The PAV ontology
provides properties for tracking intellectual property in-
formation, which are described in Table 1.
As suggested by the properties listed in Table 1, in
PAV we distinguish between authors that originate or
creatively invent a work that is expressed in a digital
resource (pav:authoredBy), e.g., the authors of a scien-
tific publication or of a novel scientific hypothesis; and
curators (pav:curatedBy), who are content specialists
responsible for shaping the expression in an appropriate
format. When talking about knowledge artifacts the
authors are contributing the primary knowledge and
the curators are those responsible for updating the
knowledge base. Contributors (identified by the super-
property pav:contributedBy) cover both authors and
curators, as well as agents that generically provide some
help in conceiving the resource or in the expressed
knowledge creation/extraction. For example, a scien-
tist who performed some biological experiments andTable 1 PAV authoring properties
pav:authoredBy Indicates an agent that originated or gave existence to t
content of a resource may be different from the creator
the same.
pav:authoredBy is more specific than its superproperty d
creation of the representation of the artifact.
The author is usually not a software agent (which would
unless the software actually authored the content itself;
music or a machine learning algorithm that authored a
pav:authoredOn Indicates the date this resource was authored by the ag
pav:createdOn, although their values are often the same.
This property is normally used in a functional way, indic
this.
pav:curatedBy Specifies an agent specialist responsible for shaping the
responsible for ensuring the quality of the representatio
creator of the digital resource (pav:createdBy). The curato
software which adds hyperlinks for recognized genome
pav:curatedOn Specifies the date this resource was curated. pav:curated
This property is normally used in a functional way, indic
pav:contributedBy Specifies an agent that provided any sort of help in con
Contributions can take many forms, of which PAV define
specific roles could also be specified by pav:contributedB
managing the underlying data source. Contributions can
prov:hadRole.
Note that pav:contributedBy identifies only agents that co
agents that made the digital artifact or representation (p
pav:authoredBy and pav:curatedBy above.
pav:contributedBy is more specific than its superproperty
contributions to making the representation of the artifac
pav:contributedOn Indicates the date this resource was contributed on. pav
superproperty of pav:authoredOn and pav:curatedOn, bu
investigating.published the results in a paper is considered an author
(pav:authoredBy) as she produced novel results. The
agent that analyzes the paper, extracts and organizes
some of the scientific discourse in argumentation  hy-
potheses, claims, etc.  is the curator (pav:curatedBy).
Finally the person that enters such knowledge in a hy-
potheses management application is the creator of the
knowledge artifacts (pav:createdBy), as defined in the
next subsection.
As illustrated in Table 1, PAV authoring properties can
be associated with a timestamp using the following
properties: pav:authoredOn, pav:curatedOn, and pav:
contributedOn.
For describing the publication process of the created
resource, we recommend adopting the following DC
Terms properties: dct:publisher, dct:issued, dct:dateSub-
mitted, dct:dateAccepted and dct:dateCopyrighted. For
instance, dct:publisher identifies an entity responsible
for making the resource available [2]. It is important to
note that these properties describe the publication of the
particular resource (say a knowledge graph), not the
publication of the original work that this resource might
have been imported or derived from. The PAV term pav:he work that is expressed by the digital resource. The author of the
of that resource representation (pav:createdBy), although they are often
ct:creator - which might or might not be interpreted to also cover the
be indicated with pav:createdWith, pav:createdBy or pav:importedBy),
for instance an artificial intelligence algorithm which authored a piece of
classification of a tumor sample [17].
ents given by pav:authoredBy. Note that pav:authoredOn is different from
ating the last time of authoring, although PAV does not formally restrict
expression in an appropriate format. Often the primary agent
n. The curator may be different from the author (pav:authoredBy) and
r may in some cases be a software agent, for instance text mining
names.
By gives the agents that performed the curation.
ating the last curation date, although PAV does not formally restrict this.
ceiving the work that is expressed by the digital artifact.
the subproperties pav:authoredBy and pav:curatedBy; however other
y or custom subproperties, such as illustrating, investigating or
additionally be expressed in detail using prov:qualifiedAttribution and
ntributed to the work, knowledge or intellectual property, and not
av:createdBy), thus the considerations for software agents is similar to for
dct:contributor - which might or might not be interpreted to also cover
t.
:contributedBy specifies the agents that contributed. This term is a
t can also be used for other kinds of contributions, such as illustrating or
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 6 of 22
http://www.jbiomedsem.com/content/4/1/37providedBy (see next subsection) gives a shortcut to ex-
press the publisher of the original work.
Figure 2 is an example illustrating the usage of PAV
authoring terms. The figure depicts a partial representation
of a hypothesis taken from the AlzSWAN knowledge base
and published by the AlzSWAN team on behalf of Alz-
Forum. Such a claim has been derived from a scientific
publication and therefore is recorded as authored by the
publication author. A curator performed on a particular
date the task of encoding the research statements into the
SWAN format.
Provenance
To encode provenance information specifying creation,
retrieval, import and source access, PAV provides the
properties presented in Table 2.
While authoring terms like pav:authoredBy and pav:
contributedBy describe who brought the underlying
knowledge to light, the provenance term pav:createdBy
describes who created the digital resource. For instance,
in Figure 2, the creator is the user who formally encodes
the claim, while the author is the person who wrote the
original published article. In PAV, the digital artifact was
pav:createdOn a given date, and it was pav:createdWith
a specific software application. Similarly, a digitization of
Charles Darwins Galápagos notebook could be pav:create-
dOn 2006-10-06 T09:49:12Z and pav:authoredOn 1835-
03-06 T00:00:00Z. (Note that this xsd:dateTime string uses
the convention of a zero timestamp as the exact time of
day is unknown)."Mutations in APP, PS1 and 
PS2, or variant ApoE4 alter 
A  production that promotes 
amyloid aggregation."
swan:Claim
dct:description
http://tinyurl.com/7zo
Golde T.
Wong G.
pav:authoredBy
pav:curatedBy
pav:curatedOn
rdf:type
dct:
"2009-02-25"
Content
"2008-06-02"
pav:authoredOn
pav
R
Figure 2 A claim published in the AlzSWAN knowledge base, authore
date by the human agent Wong G. The artifact has been created by a
the AlzSWAN Team.In PAV, we also distinguish between retrieving a re-
source as is (pav:retrievedFrom), such as caching or
downloading; importing a resource through a data trans-
formation (pav:importedFrom) in order to fit it into an
existing model, e.g., when converting a CSV file to an
Excel spreadsheet; and accessing a resource (pav:sour-
ceAccessedAt). The latter is useful when resources such
as webpages are accessed but not cached or imported
into the system.
As well as the above properties, PAV allows us to specify
the agent that performed the task  pav:retrievedBy, pav:
importedBy, pav:sourceAccessedBy  and the time when
the task was performed  pav:retrievedOn, pav:importe-
dOn and pav:sourceAccessedOn. For example, Figure 3 is
a snippet specifying the record of a protein generated by
importing data from the EntrezGene database [18]. The re-
lationship pav:importedFrom is used to assert that the rec-
ord is the result of a transformation process. In this specific
case, an XML file has been translated into RDF according
to a specific model (for example the lses namespace from
the SWAN Ontology). The transformation is attributed to
the agent that performed it  in this particular case a soft-
ware agent  through the relationship pav:importedBy.
Additional properties can be used to enrich the proven-
ance data. For instance pav:createdAt provides the geolo-
cation of the agent when the artifact has been created.
Versioning and evolution
To avoid complexity, PAV adopts a snapshot-based ap-
proach as opposed to a detailed process-oriented approach.2zwt
"AlzSWAN Team"
publisher
"2009-02-26"
"AlzSWAN Workbench"
pav:createdWith
pav:createdOn
Wu E.
:createdBy
epresentation
http://...
rdfs:label
http://...
rdfs:label
d by the human agent Golde T. and curated on the indicated
human agent Wu E. with the AlzSWAN Workbench and published by
Table 2 PAV provenance properties
pav:createdBy An agent primarily responsible for encoding the digital artifact or resource representation. This creation is distinct from
forming the content, which is indicated with pav:contributedBy or its subproperties.
pav:createdBy is more specific than its superproperty dct:creator - which might or might not be interpreted to also cover the
creation of the content of the artifact.
For instance, the author wrote this species has bigger wings than normal in his log book. The curator, going through the
log book and identifying important knowledge, formalizes this as locus perculus has wingspan > 0.5 m. The artifact creator
enters this knowledge as a digital resource in the knowledge system, thus creating the digital artifact (say as JSON, RDF, XML
or HTML).
A different example is a news article. pav:authoredBy indicates the journalist who wrote the article. pav:contributedBy can
indicate the artist who added an illustration. pav:curatedBy can indicate the editor who made the article conform to the
news papers language style. pav:createdBy can indicate who put the article on the web site.
The software tool used by the creator to make the digital resource (say Protege, Wordpress or OpenOffice) can be indicated
with pav:createdWith.
pav:createdOn The date of creation of the digital artifact or resource representation. The agents responsible can be indicated with pav:
createdBy.
This property is normally used in a functional way, indicating the time of creation, although PAV does not formally restrict
this. pav:lastUpdateOn can be used to indicate minor updates that did not affect the creating date.
pav:createdWith The software/tool used by the creator (pav:createdBy) when making the digital resource, for instance a word processor or an
annotation tool. A more independent software agent that creates the resource without direct interactions by a human
creator should instead be indicated using pav:createdBy.
pav:createdAt The geo-location of the agents when creating the resource (pav:createdBy). For instance, a photographer takes a picture of
the Eiffel Tower while standing in front of it.
pav:retrievedFrom The URI where a resource has been retrieved from. Retrieval indicates that this resource has the same representation as the
original resource. If the resource has been somewhat transformed, pav:importedFrom should be used instead. This property is
normally used in a functional way, although PAV does not formally restrict this.
pav:retrievedBy An entity responsible for retrieving the data from an external source. The retrieving agent is usually a software entity, which
has done the retrieval from the original source without performing any transcription.
Retrieval indicates that this resource has the same representation as the original resource. If the resource has been
somewhat transformed, use pav:importedFrom instead.
pav:retrievedOn The date the source for this resource was retrieved. This property is normally used in a functional way, although PAV does
not formally restrict this.
pav:importedFrom The original source of imported information. Import means that the content has been preserved, but transcribed somehow,
for instance to fit a different representation model by converting formats. The imported resource does not have to be
complete but should be consistent with the knowledge conveyed by the original resource.
pav:importedBy An agent responsible for importing data from a source given by pav:importedFrom. The importer is usually a software agent
which has done the transcription from the original source. Note that pav:importedBy may overlap with pav:createdWith.
pav:importedOn The date the resource was imported from a source given by pav:importedFrom. This property is normally used in a functional
way, indicating the first import date, although PAV does not formally restrict this.
This property is normally used in a functional way, although PAV does not formally restrict this. If the resource is later
reimported, this should instead be indicated with pav:lastRefreshedOn.
pav:lastRefreshedOn The date of the last import of the resource. This property is used if this version has been updated due to a re-import, rather
than the import creating new resources related using pav:previousVersion.
pav:providedBy The original provider of the encoded information (e.g. PubMed, UniProt, Science Commons).
The provider might not coincide with the dct:publisher, which would describe the current publisher of the resource. For
instance if the resource was retrieved, imported or derived from a source, that source was published by the original provider.
pav:providedBy provides a shortcut to indicate that original provider on the new resource.
pav:sourceAccessedAt A source which was accessed or consulted (but not retrieved, imported or derived from). For instance, a curator (pav:
curatedBy) might have consulted figures in a published paper to confirm that a dataset was correctly pav:importedFrom the
papers supplementary CSV file.
Another example: I can access the page for tomorrow weather in Boston (http://www.weather.com/weather/tomorrow/
Boston+MA+02143) and I can blog tomorrow is going to be nice. The source does not make any claims about the nice
weather, that is my interpretation; therefore the blog post has pav:sourceAccessedAt the weather page.
pav:sourceAccessedBy The agent who accessed the source given by pav:sourceAcccessedAt .
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 7 of 22
http://www.jbiomedsem.com/content/4/1/37
Table 2 PAV provenance properties (Continued)
pav:
sourceAccessedOn
The date when the original source given by pav:sourceAccessedAt was accessed to create the resource.
For instance, if the source accessed described the weather forecast for the next day, the time of source access can be crucial
information.
This property is normally used in a functional way, although PAV does not formally restrict this. If the source is subsequently
checked again (say to verify validity), this should be indicated with pav:sourceLastAccessedOn.
pav:
sourceLastAccessedOn
The date when the original source given by pav:sourceAccessedAt was last accessed and verified, especially when the source
has previously been pav:sourceAccessedOn when creating the resource. This property is normally used in a functional way,
although PAV does not formally restrict this.
This property can be useful together with pav:lastRefreshedOn or pav:lastUpdateOn, but could also be used alone, for instance
when a source was verified and no further action was taken for the resource.
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 8 of 22
http://www.jbiomedsem.com/content/4/1/37Snapshots are identified by a URI and the free text property
pav:version, and several snapshots of the same resource
are related using pav:previousVersion, linking a version of
the resource with the previous one of the same lineage. We
use pav:derivedFrom to indicate an artifact as a derivation
of another, not necessarily of the same lineage. Table 3 pre-
sents and describes PAV versioning properties.
As an illustrative example, Figure 4 specifies that the Hy-
pothesis A1 is first created and then updated into A1 by
the same human agent. A1 and A1 are two representation
of the same version of the same hypothesis at two different
points in time. The version A2 of the same hypothesis is
created by another human agent having the same access
rights. From the digital artifact standpoint it is accurate to
say that the second version (A2) was created by a different
agent than the first one, even if both versions are of the
same lineage. In PAV the relationship pav:createdBy does
not have any content authorship connotation. Computing
the differences between the two versions allows attributing
the intellectual property to the authors/curators who origi-
nated it. It is also possible to branch the lineage of digital
artifacts through the relationship pav:derivedFrom. The
property pav:lastUpdateOn is used to date when the digital
artifact was last updated, indicating minor changes that did
not signify a change of version (and therefore a new re-
source), such as fixing a typographical error.
Dublin Core Terms provides the property dct:source,
which is used to specify A Reference to a resource fromFigure 3 Example of import from the EntrezGene database
expressed using Turtle notation [19]. The two namespaces lses
(life science entities [http://purl.org/swan/1.2/lses/] and agents
[http://purl.org/swan/1.2/agents/] are part of the SWAN suite
of ontologies.which the present resource is derived. The present re-
source may be derived from the Source resource in
whole or part. Note, however, that this is more permis-
sive, and therefore less specific, than pav:derivedFrom as
it encompasses also format conversions.
Multiplicity
The properties defined by PAV do not have any multiplicity
constraints. That means it is valid, for instance, to specify
multiple authors using pav:authoredBy, or multiple contri-
bution dates using pav:contributedOn. Some of the proper-
ties, like pav:retrievedFrom, pav:lastRefreshedOn or pav:
version should still primarily be used in a functional way (as
indicated in Tables 1, 2 and 3), as their interpretation could
be difficult with multiple values. However PAV does not
formally add functionality constraints to the OWL ontology.
Combinations of multiple agents, sources and dates
may be used with PAV, but would mean that finer details
are not fully preserved, such as who accessed which
source when. This is the result of a compromise between
simplicity and completeness when designing PAV. It is
recommended that applications that want to keep those
details also provide an accompanying PROV-O trace, as
exemplified in Figure 5.
In this example PAV will tell us there are 3 authors,
and but the resource has only got a single authoring date
(the last time of authoring). We do not know from PAV
alone when the different authors participated, but the
expanded PROV-O trace, which qualifies the implied
prov:wasAttributedTo relations, can detail those dates
using prov:atTime. Here the nature of the individual at-
tributions are also indicated using prov:hadRole and a
custom role vocabulary ex:, and the prov:atLocation indi-
cates that :khalid and :stian were in the same office,
authoring at the same time.
In some cases it might not be appropriate to attribute the
individual agents directly, for instance a report authored by
a committee where the individual members have discussed
and voted over the content. We recommend for such cases
to be represented as a single identified agent, typically a
prov:Organization, and the individual members represented
using the Collection Ontology [20], as shown in Figure 6.
Table 3 PAV versioning properties
pav:version The version identifier of a resource. This is a free text string, typical values are 1.5 or 21. The URI identifying the previous version
can be provided using pav:previousVersion. This property is normally used in a functional way, although PAV does not formally
restrict this.
pav:
previousVersion
The previous version of a resource in a lineage. For instance a news article updated to correct factual information would point to
the previous version of the article with pav:previousVersion. If, however, the content has significantly changed so that the two
resources no longer share lineage (say a new article that talks about the same facts) they can instead be related using pav:
derivedFrom.
This property is normally used in a functional way, although PAV does not formally restrict this. A version identifier for a resource
can be provided using the data property pav:version.
pav:derivedFrom Derived from a different resource. Derivation concerns itself with derived knowledge. If this resource has the same content as the
other resource, but has simply been transcribed to fit a different model (like XML to RDF or SQL to CSV), use pav:importedFrom. If
the content has been further refined or modified, use pav:derivedFrom.
Details about who performed the derivation (e.g. who did the refining or modifications) may be indicated with pav:contributedBy
and its subproperties.
pav:
lastUpdateOn
The date of the last update of the resource. An update is a change which did not warrant making a new resource related using
pav:previousVersion, for instance correcting a spelling mistake. This property is normally used in a functional way, although PAV
does not formally restrict this.
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 9 of 22
http://www.jbiomedsem.com/content/4/1/37Contribution roles
While PAV properties like pav:retrievedBy and pav:created-
With are fairly specific, yet generally applicable; different do-
mains will vary in their understanding of what in PAV
would constitute the roles of pav:authoredBy, pav:curatedBy
or pav:contributedBy. For instance, the International Work-
shop on Contributorship and Scholarly Attribution 2012
[21] presented a survey where authorship was found to
regularly include roles such as design of experimental
methods and statistical analysis, but also software develop-
ment, preparing graphics and managing a laboratory. The
presented text analysis on acknowledgement sections in
academic papers identified non-author contributions like
funding, technical assistance, data contribution, and animal
assistance.
It is out of scope for PAV to try to model this wide range
of contributorships, but we do note that ontologies such as
SPARs Publishing Roles Ontology [22] define roles like pro:Hypothesis
A1
1
pav:version
Hypothesis
A1'
Feb 26 2009 19:49
pav:lastUpdat
1
pav:version
pav:createdBy
Ciccarese P
Feb 26 2009 14:49:12 EST
pav:lastUpdatedOn
pav:createdBy
Ciccarese P
Figure 4 Example illustrating versioning with PAV. Hypothesis A1 and
resource at different points in time.illustrator, pro:critic and pro:editor which would be appropri-
ate to use with prov:hadRole in the pattern shown in
Figure 5. Additionally, third-party subproperties, e.g. of pav:
contributedBy and pav:authoredBy, can be created to further
specify the form of the contribution, utilizing PAV as a com-
mon platform for attributions across domains.
Who is using PAV
PAV has successfully been applied by several projects in aca-
demia and in industry due to it being compact and easy to
understand. Besides the SWAN project, for which PAV was
originally developed, it has been used in the Annotation
Ontology (AO) [23]; the Domeo Annotation Tool [24]; the
Nanopublications specification [25]; the Open PHACTS
dataset description specification [26-28]; the Wf4Ever
Research Objects [29]; and the Elsevier Satellite article anno-
tation format [30]. For its most recent release, we updated
PAV to include a mapping to the PROV-O ontology [3],Hypothesis
A2
pav:createdBy
pav:previousVersion
Clark T
Feb 28 2010 12:49:12 EST
pav:createdOn
2
pav:version
:12 EST
edOn
Hypothesis
B
pav:derivedFrom
0.1
pav:version
Hypothesis A1 have the same URI, they are representing the same
Figure 5 Example illustrating how PROV-O can be combined
with PAV in order to provide a more detailed
provenance record.
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 10 of 22
http://www.jbiomedsem.com/content/4/1/37enabling a level of compatibility for all PROV-based tools
for analyzing and combining provenance. Figure 7 illustrates
PAVs relation to these projects.Annotation ontology and Domeo annotation tool
PAV is used extensively by many of the applications
making use of the Annotation Ontology (AO) [23] for
anchoring annotations to online resources. One such ap-
plication is the Domeo Web Annotation Toolkit [24], a
collection of software components that provides a rich
set of features including:
 semantically annotating online HTML and XML
documents;
 sharing the annotation in RDF; and
 searching the annotation while leveraging semantic
inference.
Domeo, as well as the other AO applications, provide
a constant stream of requirements and feedback for test-
ing and improving the PAV model.
In Figure 8 we represent a common scenario where the
annotation artifact is digitizing an annotation that has been
originally performed on the physical manifestation of a pic-
ture. In other words: Khalid scribbled a note on a classic
printed picture; Paolo found a digital version of that picture
and interpreted the handwritten note by Khalid and passed
it along to Stian who, using Domeo, created an AO artifact
representing the whole scenario. Khalid is the author (pav:
authoredBy) of the original note, Paolo is the curator (pav:Figure 6 Example illustrating how Collection Ontology (CO) can be ucuratedBy) of that content and Stian is the creator (pav:cre-
atedBy) of the digital artifact.
Open PHACTS dataset descriptions
The aim of the Open PHACTS project [26,27] is to facili-
tate improvements in drug discovery using semantic web
standards and technologies. Drug discovery requires the in-
tegration of data from many data sources covering chemical
compounds (e.g. ChemSpider [31] and ChEMBL [32]), pro-
teins (e.g. UniProt [11,12]), and drug interactions (e.g.
DrugBank [33]). As such, the project requires accurate de-
scriptions of the datasets they have used, identifying the
particular versions, so that data provenance can be
returned to the users.
In the specification [28], Open PHACTS recommend the
use of existing vocabularies: Vocabulary of Interlinked Data-
sets (VoID) [34]; Dublin Core Terms (DC Terms) [2]; Friend
of a Friend (FOAF) [35]; and PAV. VoID itself does not define
any new provenance terms, but specifies a pattern of using
DC Terms properties for purposes of recording provenance,
such as dct:creator, dct:contributor and dct:source [36].
The Open PHACTS dataset specification, which uses a
specialization of VoID, also specifies patterns of recording
provenance, but from the provenance-related terms of
DC Terms, only uses dct:publisher and dct:issued, and pri-
marily recommends the PAV properties: pav:version, pav:
previousVersion, pav:retrievedFrom, pav:importedFrom, pav:
importedOn, pav:importedBy, pav:derivedFrom, pav:create-
dOn, pav:createdBy, pav:createdWith, pav:authoredBy, pav:
authoredOn, pav:lastRefreshedOn and pav:lastUpdateOn.
The Open PHACTS VoID editor [37], shown in Figure 9,
provides a wizard-like web interface for generating dataset
descriptions, including the above-mentioned PAV properties.
Nanopublications
Nanopublications [38] have been proposed by the Con-
ceptWeb Alliance and the Open PHACTS project as a
new means for publishing and citing specific core scien-
tific statements. A nanopublication is composed of two
basic elements: an assertion, and the provenance of that
assertion. The former is used to express a single scien-
tific fact, whereas the second is used to provide support-
ing evidence, in addition to the nanopublication
attribution, i.e. its authors and other metadata informa-
tion. To express provenance information, the Nanopu-
blication specification [25] recommend the use of PAV
and Open Provenance Model (OPM) [4]. For instances,
the examples of nanopublications advertised on thesed with PAV in order to encode a collection (set) of people.
PAV PROV
SATELLITES
Extends
SWAN
Used by
Used by
Used by Used by
Used by
Nanopub.org
Used by
Annotation
Ontology
Used by
Figure 7 Relationships between the PAV ontology, the PROV ontology and all the projects listed in this article making use of PAV.
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 11 of 22
http://www.jbiomedsem.com/content/4/1/37nanopublication official website [39] make use of the fol-
lowing PAV properties: pav:authoredBy, pav:createdBy,
pav:version.
To illustrate this use of PAV, the RDF snippets in
Figures 10, 11 and 12 specify a nanopublication assertion
and its provenance. These snippets are taken from a real
world nanopublication [40], here slightly simplified
for brevity. Notice that the attribution information
(Figure 12) uses pav:authoredBy and pav:createdBy to
distinguish the author (the scientist making the claim)
from the artifact creator (who formed the nanopublica-
tion as RDF). The nanopublication itself is given a ver-
sion number with pav:version.
Note that the process of structuring a nanopublication
could also be considered a kind of curation, as in order
to serialize the RDF the creator has to also look up iden-
tifiers for genes and symptoms, a content refinement asFigure 8 Example of annotation representation using
Annotation Ontology and PAV.such identifiers presumably where not in the original
content. The nanopublication we found as an example
did however not specify pav:curatedBy, so from the
above we cant tell anything more about the curation as-
pect of the nanopublication.
Elsevier satellite
PAV has been used, in conjunction with DC Terms, as
the provenance ontology in the Elsevier Satellite annota-
tion format [30]. The Satellite format is a linked data
compliant data format to capture, store and expose
metadata objects using open standards based metadata
frameworks e.g. SKOS [43], DCMI and SWAN. Satellite
format uses PAV 1.2 as originally specified in the SWAN
suite of ontologies [9].
In Satellite format, PAV differentiates the provenance
properties used for the metadata container and those
used for the contained metadata items. Satellite uses the
dct:date and dct:creator predicates to provide informa-
tion on the item being described by the metadata. Satel-
lite uses the pav:createdOn and pav:createdBy predicates
in a header to describe the origin of the Satellite meta-
data itself.
Research objects
The notion of Research Object [29] was coined by the
Wf4Ever project as an abstraction for the management
of containers of sets of objects. In Wf4Ever, research ob-
jects bundle investigation-related resources such as doc-
uments, presentations, workflows, datasets, etc. Research
Objects, their constituent resources, and their relation-
ships, are described and annotated using existing vo-
cabularies, including PAV. Specifically, the Wf4Ever
Research Object Model [44] uses PAV to:
Figure 9 The Open PHACTS VoID editor [37], a web-based wizard for creating a VoID dataset description, here representing a data
format conversion by using PAV properties pav:importedFrom, pav:importedOn, pav:importedBy. The VoID description itself (the generated
RDF) has its own provenance, using pav:createdBy, pav:createdOn.
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 12 of 22
http://www.jbiomedsem.com/content/4/1/37 Identify and distinguish the people (Agents) who
contributed to a Research Object (and its
constituent resources) from those who created
them. For instance, a hypothesis document could
have been authored by a PhD student, but uploaded
to a research object by their supervisor (thus
creating the digital resource)
 State the time at which a Research Object, or a
resource thereof, was last updated.
 Track the versions and origin of replicated resources,
such as recording provenance for resources and userFigure 10 Gene disease nanopublication example, in TriG format, ada
graphs: the Assertion which expresses the claim of this nanopublication (Fig
assertion (Figure 12), and Provenance, relating this nanopublication to the oannotations which have been imported from and
retrieved from third-party repositories using auto-
mated tools.
PROV-O and PAV mapping
The earlier version 1.2 of PAV was recognized (as a
component of the SWAN ontology) by the W3C Prov-
enance Incubator Group [45] and was one of the foun-
dational models [46] on which the requirements for the
general provenance model later defined by the W3C
Provenance Working Group.pted from [40]. The nanopublication is expressed as three named
ure 11), the PublicationInfo, which details the attributions of this
riginal data it was derived from (shown above).
Figure 11 Nanopublication assertion, adapted from [40]. Statistical association between gene and disease expressed using Bio2RDF and SIO
ontology. [41,42].
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 13 of 22
http://www.jbiomedsem.com/content/4/1/37The W3C Provenance working group later released
the PROV-O Ontology [3] as a Recommendation.
PROV-O provides a general way to describe provenance
relations using OWL. It describes provenance as a set of
interactions between Entities, Agents and Activities. In-
teractions can be described using direct relations like
prov:wasGeneratedBy; or they may be described with
qualified indirect relationships, using classes like prov:
Generation. The latter allows assignment of roles to
agent participations and other details such as timestamp
and location.
PROV-O is a generic framework for describing prov-
enance in a whole range of applications, but used alone
it lacks necessary detail for the more specific provenance
of authoring and versioning that arise from our use
cases. PAV can be a useful specialization of PROV-O by
providing simple relationships for expressing common
provenance for digital artifacts. Therefore PAV, starting
from version 2.1, introduced a mapping from PAV to
PROV-O using subproperties, as detailed in Table 4:
In PROV-O, entities are considered immutable, and
different states for the purposes of provenance are repre-
sented as different entities, each with their own proven-
ance. The W3C Provenance Working Group has
published a note Dublin Core to PROV Mapping [47],
proposing a subproperty mapping from Dublin CoreFigure 12 Nanopublication attribution specifying attribution informa
distinguish between the author of the nanopublication (the scientists who
digital representation, who in this case expressed the assertion as an RDF g
dcterms:hasVersion. (The original RDF uses the PAV 1.2 term versionNumberTerms to PROV-O, in addition to a complex mapping
by using SPARQL CONSTRUCT to create detailed
PROV-O traces. This note highlights the difference be-
tween Dublin Core and PROV-O resources: while the
former conflates more than one version or state of the
resource in a single entity, the latter proposes to separate
all of them.
We have provided a subproperty mapping from PAV
to PROV-O, which implies a similar entity conflation, by
attaching all properties to the same resource rather than
introducing intermediate entities, which would be re-
quired to give a detailed PROV-O trace of the activities
that lead to the generation of the final resource state.
The combination of OWL/RDFS reasoning and the
PROV inference rules [48] means we can infer further
PROV statements such as entity generation and activity
association from a single PAV statement, shown in
Figure 13.
Note that some information is not preserved in these
inferred statements, e.g. the distinction between authors
and curators; and the PAV statements of authorship and
authorship time are detangled into separate existential
variables and PROV-O statements.
A more integrated mapping from PAV properties to
such chains of PROV-O activities and entities could be
formulated in a similar fashion to that shown in [47],tion of the nanopublication in Figures 10 and 11. PAV is used to
made the assertion expressed in Figure 10), and the creator of its
raph. The nanopublication is given a pav:version, also identified using
 which was renamed to version in PAV 2.0.).
Table 4 Mapping from PAV to PROV-O
PROV-O superproperty PAV property Rationale
prov:wasAttributedTo pav:createdBy The creator agent participated in some activity that generated the entity.
pav:createdWith The software agent participated in some activity that generated the entity.
pav:
contributedBy
The contributor participated in some activity that generated the entity.
pav:authoredBy The author participated in some activity that generated the entity.
pav:curatedBy The curator participated in some activity that generated the entity.
pav:importedBy The agent (usually software in this case) participated in some import activity, which generated the
entity.
pav:retrievedBy The agent (usually software in this case) participated in some retrieval activity, which generated the
entity.
prov:wasDerivedFrom &
prov:alternateOf
pav:
importedFrom
Import is a transformation of an entity into another. As the resulting entity is presenting aspects of
the same thing, it is also an prov:alternateOf the original.
pav:
retrievedFrom
Retrieval is construction of an entity into another. As the resulting entity is essentially (bytewise) the
same, i.e. presenting aspects of the same thing, it is also an prov:alternateOf the original. Some aspects
of the original entity (like its dct:publisher) might not be shared, and therefore prov:specializationOf is
not an appropriate superproperty.
prov:wasDerivedFrom pav:derivedFrom Derivation is an update of an entity, resulting of a new one. Note that pav:derivedFrom is more
specific than prov:wasDerivedFrom, and does not cover minor derivations as with pav:importedFrom
and pav:retrievedFrom. PAV derivation implies that additional knowledge has been contributed,
curated or authored.
prov:wasRevisionOf pav:
previousVersion
The new version is a revised version of the original. pav:previousVersion is more specific than prov:
wasRevisionOf because it is intended for minor updates and corrections, and typically would be used
with linearly incremental pav:version numbers. Significant changes (contributing new knowledge)
should be indicated with pav:derivedFrom.
prov:wasInfluencedBy pav:
sourceAccessedAt
The source Entity has an effect on the character, or development of the entity. The PAV term is a weak
indication that another resource was consulted (for instance as part of curation), but without the
new entity being directly derived from the source. The prov:hadPrimarySource is not an appropriate
superproperty, as it implies a stronger statement, giving the source the status of a primary source
and derivation. As the resource is not necessarily based on or transformed from the consulted
source, we cant assume prov:wasDerivedFrom as a superproperty.
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 14 of 22
http://www.jbiomedsem.com/content/4/1/37which explores complex mappings of Dublin Core
Terms to detailed PROV-O patterns. For instance,
unrolling PAV import statements to PROV-O activities
could create triples as shown in Figure 14.
Detailing such a mapping is currently work in pro-
gress, and would have to balance logical correctness vs.
usefulness, for instance the above assumes that all PAV
import statements describe the same activity, but if there
are multiple pav:importedFrom statements and multiple
pav:importedBy statements we cannot be certain about
the extent of that import activity.Figure 13 Inferences from PAV authorship to existential PROV-O activ
according to PROV constraint attribution-inference imply that there existed
associated with.The authors believe that the current PROV-O sub-
property mapping is liberal enough to allow PAV to
complement more detailed provenance traces using
PROV-O, while enabling inferences to compatible PROV-O
statements.
In order to demonstrate PAV interoperability with
PROV-O, we wanted to make use of the PROV-O map-
ping, so that PROV-O statements could be inferred from
PAV statements using a standard OWL reasoner. We
then wanted to test if a PROV-O consuming tool was
able to understand the statements.ities. pav:authoredBy is subproperty of prov:wasAttributedBy, which,
some _:activity that generated the resource and which :paolo was
Figure 14 Visualization of ChemSpider VoID provenance. Both subsets are attributed to chemspider.com (pav:retrievedBy), and derived from
gz/zip files (pav:retrievedFrom). The VoID file itself is attributed to me (pav:createdBy) and derived from void.rdf (pav:derivedFrom). Note that the
labels above are generated by ProvToolbox based on the URIs  the n-prefix indicates a prov:Entity. Figure converted from SVG diagram which
was produced by Taverna workflow [46].
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 15 of 22
http://www.jbiomedsem.com/content/4/1/37To perform this experiment, we used Taverna Work-
bench v2.4 [49] to build a workflow [50] using the OWL
reasoner Pellet v2.3.0 [51], and then visualized the in-
ferred statements as a diagram (SVG) by using Prov-
Toolbox v0.1.2 [52], which understands PROV-O. We
executed this workflow against the VoID example [53] in
the Open PHACTS dataset specification [28] to visualize
its PAV statements. The generated provenance diagram
is shown in Figure 14.
Method
The PAV ontology was developed with the aim of enabling
traceability of scientific results and their representations.
The design was driven by real requirements, which initially
stemmed from the AlzSWAN project, and later were ex-
panded to requirements from other projects. DC terms and
Open Provenance Model [4] were available at that time,
and could have been used. However, they were found not
to be suitable. Specifically, DC Terms conflates different
agent roles that the participants in the SWAN project want
to distinguish, in particular, authorship, creation and contri-
bution. OPM adopts a process-oriented view of lineage by
detailing the processes whereby artifacts are used and gen-
erated, and the agents that controlled those processes. In-
stead, the requirements elicited in the context of the
SWAN project, and other projects later on, targeted mainly
the expression of lineage in terms of the relationships be-
tween the artifacts, and the relationships between the arti-
facts and the agents involved in their creation, authorship
or curation.
For these reasons, we decided to develop a new vo-
cabulary that specifically addressed the SWAN project
requirements. In doing so, the following principles were
followed:
Keep the ontology lightweight: Experience suggests that
a complex and large vocabulary (albeit well crafted) is
likely not to be adopted by users. Therefore, the authors
of PAV were keen to have a minimal set of terms (prop-
erties) that cater for the needs identified in the context
of the AlzSWAN project. In particular, PAV does not at-
tempt to model the complete chain of process-oriented
provenance.
Favoring Incremental (and organic) development: Ra-
ther than trying to design an ontology and convince theusers to utilize it, the development of the PAV ontology
went through cycles in which the ontology designers
communicated with end users and examined how the
ontology is used in practice. Modifications and additions
were then made based on the observations made in each
cycle.
Reuse and recommend existing vocabularies when they
cater for given requirements: For those requirements
supported by existing vocabularies, the authors of the
PAV ontology strived to either recommend their reuse
or (when necessary) extend them. In this respect, we
have shown in the previous section, how PAV extends
terms from the PROV-O ontology.
The PAV ontology is currently in its second version.
Since its inception, PAV has gained momentum, and it is
now adopted by several vocabularies and projects. It is
increasingly viewed as one of the main vocabularies for
specifying provenance information in the biomedical se-
mantics field. Some of the applications and projects that
have adopted PAV are indicated in the Results section.
In 2009, PAV was used as one of its starting vocabularies
by the W3C Provenance Incubator group. The Incubator
group preceded the W3C Provenance Working Group,
which made the current PROV specifications.
Discussion
In this section, we analyze and compare three proposals
we have found to be relevant to PAV: Dublin Core
Terms, BIBFRAME and Provenance Vocabulary (PRV).
We then close the article by presenting some concluding
remarks.
Dublin core terms
The Dublin Core Terms vocabulary provide terms such
as dct:contributor and its subproperty dct:creator, and
we have argued that they conflate the roles of content
authoring, knowledge curation and representation cre-
ation. Although our presented use-cases highlights the
importance of distinguishing these in the setting of for-
mal knowledge representation, the ambiguous definition
of dct:creator also means that its value for stating con-
sistent provenance is significantly reduced on the web in
general. In cases where the content author and represen-
tation creator are different, common use of DC Terms
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 16 of 22
http://www.jbiomedsem.com/content/4/1/37[2] is often to attribute only one of these, but which one
depends on the application. A use case could be a cor-
porate blog where a webmaster (Bob) types in an an-
nouncement, which the CEO (Alice) sent in an email.
Some blog platforms would automatically represent the
currently logged in user (Bob) as the dct:creator, other
platforms might allow the webmaster to select an author
(Alice) from the corporate directory and would instead
represent her as the dct:creator.
Using PAV, the blog platform can be more precise
about the provenance of the post. When the platform
has no user interface for describing the author, the safest
would be to present pav:createdBy for the current user.
If the user interface allows selecting a different author,
then both pav:authoredBy and pav:createdBy can be
supplied. Enterprise publishing platforms could also in-
dicate curation (e.g. hyperlinks and textual formatting)
with pav:curatedBy and additional contributions (such
as adding an illustration) with pav:contributedBy.
Dublin Core Terms defines terms that may cover
some provenance aspects (dct:isFormatOf, dct:source,
dct:isVersionOf, dct:replaces), however DC Terms con-
cerns itself primarily with catalogue metadata for a
resource, while PAV has a bigger focus on entity-agent-
driven provenance.
For instance, dct:isFormatOf is an existential statement
that there is a different representation of the same con-
tent, while pav:importedFrom also implies directionality
and a transformation step which was performed by an
agent, indicated with pav:importedBy. The former term
is useful for finding alternate representations, while the
latter PAV relation gives lineage to the resource, which
can be beneficial for instance to track down the source
of an inconsistency or to verify that data is current.
As dct:creator can be seen to cover both content
authoring and creating its representation, we have de-
fined both pav:authoredBy and pav:createdBy to be sub-
properties of dct:creator, while its superproperty dct:
contributor is a superproperty of pav:contributedBy; here
the PAV term only covers contributions to the work or
content, while dct:contributor may also cover representa-
tional contributions such as scaling an image or convert-
ing HTML to PDF.
Other PAV properties have not been mapped to DC
Terms in the OWL ontology. Part of the reason for this
is that the DC Terms vocabulary is not fully OWL com-
patible (e.g. properties are not declared as either annota-
tion or object properties), another is that we found the
more bibliographic DC Terms to be hard to align with
the provenance oriented aspect of PAV using strict
OWL property hierarchies.
In order to clarify the differences between the
remaining properties that might seem similar between
PAV and Dublin Core Terms and to relate the twovocabularies in detail, we defined a SKOS mapping [54].
As the differences are often conceptual we found the use
of SKOS [43] more beneficial than a formal OWL map-
ping. The most significant mappings are shown in
Table 5 with their rationale:
BIBFRAME
The Library of Congress officially launched its Biblio-
graphic Framework Transition Initiative (BIBFRAME)
[55,56] initiative in May 2011. The initiative aims to re-
envision the current standard for bibliographic exchange
(MARC 21) [57] and implement a new bibliographic en-
vironment for libraries that makes the network central
and interconnectedness commonplace, using semantic
web technologies. As PAV can be used to express attri-
bution of both digital resources and traditional publica-
tions, and BIBFRAME is an emerging standard within
the library community, we here explore BIBFRAME and
compare it with PAV.
BIBFRAME revolves around two main concepts: the
Creative Work; and the Instance, reflecting an individual,
material embodiment of the Work. These are similar
distinctions to the Work and Manifestation dichotomy
in the original Functional Requirements for Biblio-
graphic Records (FRBR) [58] model, or in other related
models such as the FRBR-aligned Bibliographic Ontol-
ogy (FaBiO) [59]. An example of the BIBFRAME two-
level model is depicted in Figure 15.
Although PAV itself does not distinguish between
work and instances, the distinction between content and
its representation is at the core of PAV; exemplified by
pav:authoredBy vs. pav:createdBy. There is however
nothing inherent with PAV itself that prevents its usage
with separate Work and Instance resources. In fact, the
PROV-O property prov:specializationOf is intended for
modeling abstraction levels, so if a bf:Work is pav:
authoredBy Alice, and a bf:Instance is a prov:specializa-
tionOf the work, then the instance can be implied to also
be pav:authoredBy Alice.
For bibliographic data, multiple abstractions levels
such as in BIBFRAME and FRBR are elegant and useful,
but for many other use cases, such as for provenance of
a blog post or nanopublication, the separation of in-
stance and work can be inconvenient, hard or even im-
possible to achieve. PAV, as a general vocabulary for
provenance and authoring of resources, is applicable in
both approaches.
Provenance vocabulary
While PAV allows expression of data sources (pav:sour-
ceAccessedAt) and derivations (pav:derivedFrom, pav:
importedFrom), the Provenance Vocabulary (PRV) [5,61]
is an extension of PROV-O to express more detailed
provenance of data items on the web, by forming chains
Table 5 SKOS mappings of applicable PAV terms to Dublin core terms
SKOS mapping Rationale
pav:authoredBy skos:broadMatch dct:
creator
Broad match due to the common usage of dct:creator to mean the creator of the Work rather than just
the creator of the particular resource. Solely creating the representation of a resource is in PAV covered
instead by pav:createdBy, but would often also be covered by dct:creator, therefore this is not a skos:
closeMatch.
pav:contributedBy skos:closeMatch dct:
contributor
Close match due its the common usage to mean someone who added to the Work of the resource
(usually not just the digital representation), but not skos:exactMatch as purely representational
contributions represented with dct:contributor should be mapped to pav:createdBy.
pav:createdBy skos:broadMatch dct:
creator
A PAV creator is a particular kind of DC Terms creator, which made the digital representation of the
resource.
pav:importedFrom skos:broadMatch
dct:source
Imported is a specialization of being derived from the related resource in whole.
pav:importedFrom skos:broadMatch
dct:isFormatOf
The resulting resource is substantially the same as the source, but in another format. However imported also
implies provenance of a directed transformation from the original, at a given time and performed by an
agent, and hence this is a broad match.
pav:importedBy skos:broadMatch dct:
creator
The agent importing is essentially a specialized creator of the new resource, hence has close match dct:
creator. In common use of DC Terms the extent of the transformation work might however affect whether
a dct:creator corresponds to an importer or author.
pav:derivedFrom skos:broadMatch dct:
source
A related resource from which the described resource is derived, but pav:derivedFrom is more specific (skos:
broadMatch) than dct:source, as it requires further contributions to the content, and does not cover say
pav:importedFrom or pav:retrievedFrom.
pav:derivedFrom skos:narrowMatch
dct:isVersionOf
pav:derivedFrom do point to a resource of which the described resource is a version, edition, or adaptation,
but also allow further derivations, and so has a narrow match dct:isVersionOf. The pav:derivedFrom does
require such contributions to be in the form of content and not just representation, which corresponds
closely to dct:isFormatOf s description Changes in version imply substantive changes in content rather than
differences in format.
pav:previousVersion skos:narrowMatch
dct:replaces
dct:replaces is a stronger statement (skos:narrowMatch) than pav:previousVersion, as the PAV statement does
not necessarily imply the original was superseded. For instance, a draft specification may be pav:
previousVersion a previously published specification, but it is not dct:replaces the previous version as the
draft is not official yet.
pav:previousVersion skos:relatedMatch
dct:isVersionOf
pav:previousVersion is only considered to have a related match dct:isVersionOf, as pav:previousVersion does
not generally cover substantive changes in content.
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 17 of 22
http://www.jbiomedsem.com/content/4/1/37of prov:Activity to detail data creation, retrieval, access
and publication. Each activity can be associated with
prov:Agents which directly or indirectly perform the
work. Additional PRV modules allow expression of data-
base queries, HTTP retrieval and file operations, and
therefore PRV might at first glance seem like an alterna-
tive to PAV. We here consider a use case where Steiner
et al. adopted PRV. Below, we explore the complexity of
querying the process-oriented PRV approach and we
demonstrate how PAV can complement such detailed
provenance and simplify queries.
PRV was conceived in 2009, and has been adapted to de-
scribe provenance of a range of internet resources, from
OpenStreetMap [62] and readings in sensor networks [63]
to reified RDF statements [64] and Facebook posts [65].
Here we explore the last case, which presents a browser ex-
tension and a REST service for annotating Facebook micro-
posts by combining several natural language processing
(NLP) APIs to tag posts with semantic terms from vocabu-
laries like dbpedia.org [66]. The service uses the Provenance
Vocabulary (PRV) to indicate how the underlying text min-
ing APIs have contributed to its tagging. This provenance is
expressed in rich details of the processes of data creationand multiple data retrievals, including individual API calls,
embedding details of their HTTP transactions using the
HTTP Vocabulary [67]. An abbreviated example of the
resulting tag is included in Figure 16.
The authors of [65] are conscious of the need to re-
duce the verbosity of their provenance trace, and list this
as a consequence of using the PRV vocabulary. We have
explored their use of the PRV model and based on their
example listing, formulated how one could find out: (i)
the APIs called to create the tagging <tag1>, (ii) when
the tag was made, and (iii) which agent created the tag.
In order to answer this, we have to query through the
individual processes of data creation, retrieval and ac-
cess, as shown in Figure 17.
Using PRV and process-oriented modeling allows the
service to express such provenance in detail, but forming
this query requires in-depth knowledge about the par-
ticular graph structure, which mirrors how the service
creates, retrieve and access data. As such, the underlying
structure might change significantly if the mechanisms
of the service are modified, requiring query rewrites.
The equivalent PAV statements can be queried in a
simpler way, as shown in Figure 18.
Figure 15 Example of BIBFRAME representation of a book as a creative work (sample:16300892) and its paperback instance (sample:
instance42) which have features such as dimensions and pages. Note how this work contains parts (tales) that themselves are works, each
having individual bc:creators, and how sample:16300892 (the bibliographical record, not the work) is bf:derivedFrom another bibliographical record.
Adapted from RDF/XML example at [60].
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 18 of 22
http://www.jbiomedsem.com/content/4/1/37We argue that PAV can provide a simpler way to de-
scribe provenance from the perspective of the interesting
resource. This allows writing general provenance queries
without a pre-existing understanding of the specific
mechanisms that made the resource.
This simplification does come at a small cost: If mul-
tiple? api resources have been imported for the same
tag, it is not possible with PAV alone to express when
each individual? api was accessed, as import details such
as? when and? agent are expressed directly on the result-
ing resource. We believe this trade-off is reasonable as
for common cases there will be a single resource for
each of pav:importedFrom, pav:importedOn and pav:
importedBy.
Our design decision to not express the implied activ-
ities is reflected in all PAV properties such as pav:
authoredOn, pav:authoredBy; or pav:sourceAccessedAt,
pav:sourceAccessedBy, pav:sourceAccessedOn; and this
reflects the simplicity approach of PAV.This simplicity of PAVs approach does not preclude
the concurrent expression of more detailed provenance
using other vocabularies such as PRV; as we showed in
Figures 18 and 19, more specific details can be expressed
by unrolling a PAV statement into a chain of corre-
sponding PROV-O activities and entities. We believe
PRV can be used such to compliment PAV for details
(and vice versa), and as both ontologies specialize
PROV-O without enforcing significant constraints, a
PROV-O aware client can follow the traces across both
vocabularies; although without gaining the specialized
understanding expressed using PAV or PRV.
Conclusions
In this article we have presented the PAV ontology, a
lightweight vocabulary for capturing provenance, author-
ship and versioning of resources on the Web. PAV dis-
tinguishes between the roles of content contributors
(including authors and curators) and creators of
Figure 16 Example of tagging a Facebook post with DBPedia terms using the Common Tag vocabulary [41]. The provenance of the
graph that contains the ctag statement expresses a chain of PRV data creation and access activities [61]. In TriG format, abbreviated from figure
in [65].
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 19 of 22
http://www.jbiomedsem.com/content/4/1/37representations. Additionally PAV can describe which
sources have been accessed, transformed or consumed
in order to create the resource.
As well as the ontology, we have listed examples of
projects that have adopted PAV, illustrating their usage
through concrete examples. Furthermore, we have pre-
sented how PAV extends the W3C recommendationFigure 17 SPARQL query over PRV provenance to find data
creation, retrieval and access of a Facebook tag. The query finds
the activity the tag was prv:createdBy, which was prv:performedBy
the agent and prv:usedData that were prv:retrievedBy another
activity, which prv:accessedResource the given REST API, prv:
performedAt the given time.PROV-O, and how this enables detailed provenance
traces in PROV-O to be combined with PAVs direct re-
lationships to the origins of a resource.
Originally created in 2006 with curated knowledge
bases (such as AlzSWAN) in mind, PAV has evolved and
is now used to document a wide variety of digital re-
sources. PAV introduces terms for clearly attributing the
intellectual property of the content, and also deals with
other aspects crucial for representing scientific contentFigure 18 SPARQL query over PAV provenance to find data
creation, retrieval and access of a Facebook tag (equivalent to
Figure 16). The tag was pav:importedFrom the given REST API, by
the importing agent at the given import time.
Figure 19 Example of SPARQL CONSTRUCT generating PROV-O
activities from PAV imports.
Ciccarese et al. Journal of Biomedical Semantics 2013, 4:37 Page 20 of 22
http://www.jbiomedsem.com/content/4/1/37in a federated environment, such as versions and re-
source retrieval. PAV does not specify detail about the
chain of processes that produced the current state of the
resource, but gives a view of attribution metadata that is
uniform across a multitude of implementations.
At the core of PAV is the distinction between author-
ing knowledge (content) and creating representations.
This is highlighted by the mapping to DC Terms, which
shows how PAV properties can provide more precise at-
tributions. Equally important, PAV derivation properties
distinguish between plain retrieval, versioned updates,
transformational imports, and more structural deriv-
ation. These distinctions are essential for attributing re-
sources in the complex real world of curated knowledge
bases and datasets, and PAV has been adapted for these
purposes by representation models like Open PHACTS
dataset descriptions, Nanopublications, Wf4Ever Re-
search Objects and Elseviers Satellite annotations.
We believe PAV is a good complement to existing
provenance vocabularies, such as OPM and PROV-O.
Indeed, use of such vocabularies often focus on describ-
ing the chain of activities that were performed to trans-
form given resources into other resources, and the role
of agents associated with those activities. In PAV, the
emphasis is put on the provenance of the resources:
PAV describes the lineage from other resources, and just
as important, the role of agents involved with creating
and maintaining the resource.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
PC conceived and authored PAV 1.0 as a spin-off of the SWAN Ontology. PC
co-developed the SWAN platform and is also the main author of Annotation
Ontology and the architect of the Domeo Annotation Tool. PC is one of the
main authors of this article and co-developed PAV 2.x with SSR. SSR is one of
the main authors of this article, and co-developed PAV 2.x with PC. SSR is
one of the authors of the Research Object model together with KB, and one
of the contributors of PROV-O as a member of the W3C Provenance Working
Group. In this article, SSR is the author of the sections on Open PHACTS
datasets, Nanopublications, PROV mapping, DC Terms and Provenance
Vocabulary. He also maintains the OWL representation of PAV and the
accompanying descriptions shown in the Ontology section. KB is, together
with SSR, co-author of the Research Object model and the W3C PROV-O
ontology. KB drafted the background and method sections, and contributed
to refining the results section. He also participated in improving the
descriptions of ontology terms, and by restructuring and improving theorganization of the article as a whole. AG is the author of the Open PHACTS
dataset descriptions, and helped validate PAVs documentation and OWL
implementation. AG co-wrote the sections on Open PHACTS datasets and
Nanopublications, and has helped revise the final article. CG is the Principal
Investigator of the Wf4Ever project and co-investigator of the Open PHACTS
project. She supervised the research and edited the final article. TC is the
Principal Investigator of the SWAN project and of the Domeo Annotation
Tool. He supervised the research and edited the final article. All authors read
and approved the final manuscript.
Authors information
Paolo Ciccarese
URL: http://orcid.org/0000-0002-5156-2703
Stian Soiland-Reyes
URL: http://orcid.org/0000-0001-9842-9718
Khalid Belhajjame
URL: http://orcid.org/0000-0001-6938-0820
Alasdair JG Gray
URL: http://orcid.org/0000-0002-5711-4872
Carole Goble
URL: http://orcid.org/0000-0003-1219-2137
Tim Clark
URL: http://orcid.org/0000-0003-4060-7360
Acknowledgements
We would like to thank Marco Ocana, Gwen Wong, Elizabeth Wu, and June
Kinoshita for their input during the development of the SWAN ontology. We
also would like to thank Paul Groth for helping us validate the mappings
between PAV and the PROV model. We thank the Journal of Biomedical
Semantics reviewers, who provided valuable feedback which led to further
improvements of the ontology and this paper.
Stian Soiland-Reyes and Khalid Belhajjame are funded for the Wf4Ever project
by the European Commissions 7th FWP FP7-ICT-2007-6 270192.
Alasdair Gray received support from the Innovative Medicines Initiative Joint
Undertaking under grant agreement number 115191, resources of which are
composed of financial contribution from the European Unions Seventh
Framework Programme (FP7/2007- 2013) and EFPIA companies in kind
contribution.
Author details
1Department of Neurology, Massachusetts General Hospital, 55 Fruit Street,
Boston, MA 02114, USA. 2Harvard Medical School, 25 Shattuck Street, Boston,
MA 02115, USA. 3School of Computer Science, University of Manchester,
Oxford Road, Manchester M13 9PL, UK.
Received: 26 April 2013 Accepted: 7 October 2013
Published: 22 November 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Rebholz-Schuhmann et al. Journal of Biomedical Semantics 2013, 4:28
http://www.jbiomedsem.com/content/4/1/28
RESEARCH Open Access
Evaluating gold standard corpora against
gene/protein tagging solutions and lexical
resources
Dietrich Rebholz-Schuhmann1,2*, Senay Kafkas2, Jee-Hyub Kim2, Chen Li2, Antonio Jimeno Yepes2,3,
Robert Hoehndorf4, Rolf Backofen5 and Ian Lewin6,2
Abstract
Motivation: The identification of protein and gene names (PGNs) from the scientific literature requires semantic
resources: Terminological and lexical resources deliver the term candidates into PGN tagging solutions and the gold
standard corpora (GSC) train them to identify term parameters and contextual features. Ideally all three resources,
i.e. corpora, lexica and taggers, cover the same domain knowledge, and thus support identification of the same types
of PGNs and cover all of them. Unfortunately, none of the three serves as a predominant standard and for this reason it
is worth exploring, how these three resources comply with each other. We systematically compare different PGN
taggers against publicly available corpora and analyze the impact of the included lexical resource in their
performance. In particular, we determine the performance gains through false positive filtering, which contributes to
the disambiguation of identified PGNs.
Results: In general, machine learning approaches (ML-Tag) for PGN tagging show higher F1-measure performance
against the BioCreative-II and Jnlpba GSCs (exact matching), whereas the lexicon based approaches (LexTag) in
combination with disambiguation methods show better results on FsuPrge and PennBio. TheML-Tag solutions
balance precision and recall, whereas the LexTag solutions have different precision and recall profiles at the same
F1-measure across all corpora. Higher recall is achieved with larger lexical resources, which also introduce more noise
(false positive results). TheML-Tag solutions certainly perform best, if the test corpus is from the same GSC as the
training corpus. As expected, the false negative errors characterize the test corpora and  on the other hand  the
profiles of the false positive mistakes characterize the tagging solutions. Lex-Tag solutions that are based on a large
terminological resource in combination with false positive filtering produce better results, which, in addition, provide
concept identifiers from a knowledge source in contrast toML-Tag solutions.
Conclusion: The standardML-Tag solutions achieve high performance, but not across all corpora, and thus should
be trained using several different corpora to reduce possible biases. The LexTag solutions have different profiles for
their precision and recall performance, but with similar F1-measure. This result is surprising and suggests that they
cover a portion of the most common naming standards, but cope differently with the term variability across the
corpora. The false positive filtering applied to LexTag solutions does improve the results by increasing their precision
without compromising significantly their recall. The harmonisation of the annotation schemes in combination with
standardized lexical resources in the tagging solutions will enable their comparability and will pave the way for a
shared standard.
*Correspondence: rebholz@ifi.uzh.ch
1Institute of Computational Linguistics, University of Zürich, CH-8050 Zürich,
Switzerland
2European Bioinformatics Institute, Wellcome Trust Genome Campus,
Hinxton, Cambridge, CB10 1SD, UK
Full list of author information is available at the end of the article
© 2013 Rebholz-Schuhmann et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the
Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,
distribution, and reproduction in any medium, provided the original work is properly cited.
Rebholz-Schuhmann et al. Journal of Biomedical Semantics 2013, 4:28 Page 2 of 18
http://www.jbiomedsem.com/content/4/1/28
Introduction
The integration of the biomedical literature into the
infrastructure of other biomedical data resources requires
the identification of entities and concepts, such as pro-
teins and gene named entities [1,2]. Furthermore, the
entities have to be linked to specific database entries to
achieve interoperability between all data resources (called
grounding or normalisation) [3-5]. Once genes and
other entities have been correctly identified, the different
data resources and information types can be explored in
concerto: Genes can be attributed with known facts such
as associated diseases or novel drug candidates [6,7].
Which gene did youmean?
Biologically, a gene is given by one or several genetic
sequences that reside on one or several locations on the
DNA (genetic loci). According to semantic formalisms
a gene is a disposition which is described by the given
genetic sequence. By contrast, a protein, a gene product,
is a class of molecules with specific properties: composed
of amino acids, having a structure and specific functions.
A gene can be analysed by sequencing, but the observa-
tion of a protein and its functions requires more complex
experimental setups. Eventually, observations about pro-
teins (and genes) are kept in reference databases, like
UniProt [8] for proteins or the NCBI Gene database [9]
(formerly known as Entrez Gene), which also contribute
with unique identifiers for grounding or normalisation
[10].
Scientists frequently do not distinguish between the
mention of a gene or a protein in their publications, since
the gene is  in general  transcribed and translated into a
protein anyways, i.e. the gene produces multiple instances
of this particular protein type, which then exposes their
function [11]. In principle, we should have for every pro-
tein type a unique database entry which contains the
reference information such as a unique identifier, the pro-
tein properties and also the names (labels) that would
help to recognize the protein type in text, called pro-
tein and gene names (PGNs). Unfortunately, the existing
resources are not quite as comprehensive yet.
More than 500,000 database entities are available that
refer to protein types, which also contribute with sev-
eral million names, synonyms, acronyms, hypernyms and
morphological term variants [12]. These names certainly
comply with established nomenclature guidelines, but the
reporting in the scientific literature is not similarly stan-
dardised and produces new term variants on a daily basis
[13-15].
Standards for protein and gene names
There are several standard approaches to validate a
PGN identified in text. A first approach is to con-
sult the naming guidelines [16-18], to discover that
the name (1) may represent a recognized macro-
molecule (e.g., hemoglobulin, prolactin), (2) may state
the function of the gene or protein (e.g., methyl-
transferase), (3) may state part of the structure of
a protein (e.g., Cytochrome c oxidase subunit 2),
(4) may indicate that the protein is part of a pro-
cess (e.g., Mitochondrial fission process protein 1),
(5) may indicate its action on a target (e.g., DNA gyrase
inhibitor), (6) may induce a phenotype (e.g., protein
hunchback), (7) may express chemical or physical prop-
erties (e.g., 37.8 kD protein), or (8) may just be similar
to another known protein or gene (e.g., Myc homolog
protein).
A second and simpler approach, which is another stan-
dard, would be looking up the name in a reference
database, where the PGN may be linked to a single entry
(unique name), to several data entries (polysemous
term), or to no entry at all (unknown PGN). A third
approach would imply using a PGN tagger, e.g. Banner
[19] a state of the art machine learning based tagger, and
any finding of a PGN would be considered a true positive
ignoring for the moment missed terms (false negatives,
FN) and the wrongly identified terms (false positives, FP).
Other tagging solutions have been suggested and tested as
well, but may be used to a lesser degree [20]. A prelim-
inary comparison of ML-Tag solutions has already been
performed [21], but was executed to judgemethods for the
identification of protein interactions from the scientific
literature, and as a consequence did not cover the same
scope as this analysis.
As a last approach, the different GSCs could be con-
sulted, which have been produced to denote the correct
mention of a PGN in the scientific literature. The fol-
lowing GSCs have to be considered (cf. Table 1): (1)
Jnlpba [22] (2004) which stems from the Genia corpus,
(2) BioCreative-II [23] (BC2, 2007) for human PGNs, (3)
PennBio corpus [24] (20062007) about oncology, and
(4) FsuPrge corpus [25] (2009) on gene-regulatory events.
The CRAFT corpus [26] has not been considered in this
study, since it covers full text articles and was not yet
available during the experimental phase of this work.
Considering the different solutions to represent the
same or similar standards, the question arises, how these
Table 1 A number of gold standard corpora have been
delivered to the public for the evaluation of PGN tagging
solutions
Name Release # Annot. # Units Topic
Jnlpba 2004 6,142 401 abs. Subset of Genia
BioCreative-II 2005 5,144 4,171 sent. Human proteins
PennBio 200607 18,148 1,414 abs. Oncology
FsuPrge 2009 59,483 3,236 abs. Gene regulatory
processes
Rebholz-Schuhmann et al. Journal of Biomedical Semantics 2013, 4:28 Page 3 of 18
http://www.jbiomedsem.com/content/4/1/28
different solutions relate to each other. Can we character-
ize, which portion of the semantics representation they
share and where they differ? Although Banner is only one
piece in the puzzle, it can be used as the starting point:
first as a means to judge the compliance of the GSCs and
second, as a means to compare against other PGN tagging
solutions measured across all corpora.
Note that all evaluations are concerned with the surface
forms of PGNs in text and in the other semantic resources,
thus our analysis is focused to the identification of gene
mentions. From a different angle, the presented study still
allows to derive, whether or not a PGN mention given
by a tagging solution is linked to a concept identifier (see
Table 2), which is deemed more precious than the pure
identification of a PGN.
Is Banner the best tagging solution?
In the search of the best tagging solution, a number of
hypotheses and test scenarios can be evaluated. State of
the art PGN tagging solutions like Banner have not fully
solved the PGN tagging task yet, the question remains
whether it is possible to introduce a solution that per-
forms better than Banner. It is easy to derive that Banner
will certainly perform best on the BioCreative II corpus,
which has been used to train the tagging solution, but how
big is the performance loss, when we evaluate it on other
GSCs?
It is also possible to evaluate PGN tagging solutions
that have been trained on other GSC, e.g. Abner [20]
(BioCreative-I) and Abner (Jnlpba), and compare them
against Banner, leading to the obvious assumption that
each solution may achieve better performance on the test
corpus of the GSC they have been trained on. But which
PGN tagger will perform best across all GSCs? The answer
to this question requires that all PGN tagging solutions are
tested against all GSCs.
It is also possible to explore PGN tagging solutions
that have not been trained on any of the GSCs, e.g. dic-
tionary lookup based solutions from the Whatizit [27]
infrastructure. Several taggers are available that can be
distinguished according to the lexical resources that have
been incorporated into the annotation solution. Two
questions arise: Which one performs best and on which
GSC, and is it possible to correlate the tagging perfor-
mance with the type of lexical resource that has been
incorporated?
Another observation results from the previous consid-
erations. If we test a selection of PGN tagging solutions
across different GSCs, then we will find out that the tag-
gers miss annotations (FNs) that are specific for one GSC
in contrast to another, and that some tagging solutions
are more alike in the production of unconfirmed anno-
tations (FPs) than others. It becomes clear, that the PGN
taggers may have similar FP profiles and a given GSC may
induce similar FN profiles. Are these profiles giving us
better insights, which setup for a PGN tagging solution is
required to achieve the best performance on a given cor-
pus or are selected corpora even more characteristic for a
given set of PGNs after all? Even beyond this result, can
we better analyse what distribution of PGN representa-
tions we would have to expect in the scientific literature in
general?
It becomes clear that the comparison of as many PGN
tagging solutions against as many GSCs as available leads
to a matrix where each solution shows its best fit to a GSC
and eventually, it becomes possible to reverse engineer
Table 2 The PGN tagging solutions are incorporate different components, i.e. lexical resources or trained
machine-learning based entity recognizers
Tagger Acronym Tagger Lexical # Lexical Id Training FP
name type resource entries data filter
Banner ML   No BC2 Banner
Chang2 Ch2 ML   No BC2 Chang2
Abner (BC1) ML   No BC1 Abner (BC1)
Abner (Jnlpba) ML   No Jnlpba Abner (Jnlpba)
SwissProt SP Lex SwissProt 228,893 Yes  BNC
SwissProt (GP7) Lex GP7 868,050 Yes  BNC
BioLexicon Lex BioLexicon 653,212 Yes  BNC
GeneProt 7.0 GP7 Lex GP7 1,725,500 Yes  BNC
Wh-Ukpmc Lex+ML SwissProt 228,893 Yes  BNC, Chang2
Wh-Ukpmc (GP7) WH7 Lex+ML GP7 868,050 Yes  BNC, Chang2
Gnat (human) Lex+ML Human genes Yes BC2 
Gnat (all) Lex+ML 11 species 80,000 Yes BC2 
Gnat-GN (all) Lex+ML 11 species 80,000 Yes BC2 
Rebholz-Schuhmann et al. Journal of Biomedical Semantics 2013, 4:28 Page 4 of 18
http://www.jbiomedsem.com/content/4/1/28
which resources induce a PGN tagging solution that
produces the best results against the given GSC.
How to evaluate a PGNmention?
A priori, it is only necessary to apply all PGN tagging
solutions against all corpora, count the true positives,
false positives and false negatives and finally calculate
precision, recall and F1-measure. Any GSC  in its own
rights  would claim this result as the only correct solu-
tion. Unfortunately, as it is often the case, a more complex
solution is required to produce results that are indepen-
dent from a single corpus.
Ideally, we would like to measure the correct assign-
ment of an identifier to the PGN mention in the text,
where the identifier points to an entry in the reference
database [10]. Since PGN mentions are often polysemous
and have been reused in a number of database entries, it
is impossible to derive automatically the identifier which
should be the preferred choice for the author. A recent
experiment has also shown that authors as well fail to
use database entries  even when supported by experts
for doing so  and rather prefer to use terminology only
when specifying PGNs in theirs manuscripts [28-30]. It
is in the same way impossible to make a clear distinc-
tion between gene mentions and protein mentions, and
name ambiguities between genes and proteins, e.g. hypA
versus HypA, may result from the annotation guidelines
[11].
The term boundaries are certainly crucial for the char-
acterization of the term and its semantics. Again, it
has been claimed that the difficulties in determining
the correct term boundaries do result from the com-
plexity of the naming guidelines, which  in this case
 have been used for the production of a GSC [11].
As an example, the organism names preceding a pro-
tein name may or may not be part of the protein name.
The authors address the problems by defining the tag-
ging of proteins as the prime target of the extraction
task, and also distinguish the protein tags and the long-
form tags as two annotation types, where the latter is
formed by optionally extending the boundaries of the pro-
tein tag whenever the name boundaries are difficult to
determine.
In essence, exact boundary matching gives clear evalua-
tion guidelines, but will ignore term variability, i.e. ignores
alternative annotations by a PGN tagging solution that
does cope with term variability.When relaxing the bound-
ary criteria, either by treating morphological term vari-
ability as a single entry (e.g. HZF-7 vs. HZF-7 protein)
or by the use of flexible matching [31] (e.g. cosine similar-
ity, nestedness) or by allowing alternative term mentions
(see BioCreative II, Alternative Gene List) the perfor-
mance of the tagging solutions increases, but the precise
interpretation of the results is reduced.
Background
The following section gives an overview on different
solutions for PGN tagging. In principle, a classifier can
be trained on a GSC and then correctly identifies the
PGN boundaries (mention identification), but would not
attribute a database identifier to the PGN mention. The
latter requires at least the use of one or more lexical
resources.
PGN tagging solutions
Yamamoto distinguishes three different basic approaches
for the identification of bio-entities: (a) exact and approx-
imate string matching of entities [32,33], (b) handcrafted
rule-based approaches [34,35], and (c) machine learning
solutions [36,37]. In addition, hybrid methods exist that
integrate dictionaries with machine-learning approaches
[38-40].
In principle all gene taggers fulfill a number of specific
tasks. In the first place they identify a stretch of text that
appears to represent a gene [41]. This judgment requires
that the morphological and syntactical structure of the
gene is identified from text [42,43]. The second step is
the use of the contextual information to achieve term dis-
ambiguation or normalization of the named entity to a
concept identifier [44,45]. Both results can be achieved
with a single processing step, for example in the use of
dictionary based methods, but under-perform in terms of
context-sensitive disambiguation of terms and entities in
comparison to the ML approaches.
One solution for the gene identification is the use of a
terminological resource that covers the full set of PGNs
[45]. Such resources have been generated from public sci-
entific databases, e.g. from UniProtKb, or from a set of
public resources in combination or can also be produced
from the scientific literature [46-48]. The size of the ter-
minological resources is crucial for the identification task,
since only a comprehensive resource can ensure that all
entities are identified [49]. On the other hand, a larger ter-
minological resource increases the noise ratio of the gene
tagger, since less common and ambiguous gene names
raise the false positive rate. Last but not least, the use
of existing terminological resources leads to the advan-
tage that the gene can be linked to their database resource
[15,44,50,51], which provides further relevant information
about the gene.
As mentioned before, typical machine-learning solu-
tions solve the mention identification task and have been
trained and tested on a GSC [19,52,53]. Hybrid methods
require the integration of a dictionary and a machine-
learning approach [38-40].
Normalising gene entities
The PGN tagging solutions provide one or several types of
normalisation. The first type of normalisation only affects
Rebholz-Schuhmann et al. Journal of Biomedical Semantics 2013, 4:28 Page 5 of 18
http://www.jbiomedsem.com/content/4/1/28
the mention in text. The PGN tagger could provide a left-
most or right-most longest match to a PGN or could just
instantiate the normalisation according to a GSC (men-
tion normalisation). In the second type of normalisation,
the PGN tagger would ensure that the mention exists in
a terminological resource (lexical lookup normalisation,
see Table 3), where thematchmay ormay not consider any
morphological variability (HZF 7 vs. HZF1-7 vs. HZF/7)
or other modifications to the term.
The dictionary-based tagging solutions make use of dif-
ferent terminological resources and the lexical resources
have been compared as described here (Table 3). One lex-
ical resource has been used as a component in a tagging
solution (called tagger in the in Table 3) and has been
applied to the lexical resource to annotate the content
(called corpus). Two different methods for the match-
ing have been applied: (1) exact matching (upper part of
the table), and (2) matching with morphological variabil-
ity (alias matching, lower part). The cross-tagging shows
the intersections between the different resources and
overall it becomes clear, which terminological resource
comprises fully or partially one of the other resources.
The third type of normalisation, which is the most
advanced one, would select the most appropriate refer-
ence to a database entry ("concept normalisation"), but
JOURNAL OF
BIOMEDICAL SEMANTICS
Yamamoto et al. Journal of Biomedical Semantics 2013, 4:8
http://www.jbiomedsem.com/content/4/1/8RESEARCH Open AccessBuilding Linked Open Data towards integration of
biomedical scientific literature with DBpedia
Yasunori Yamamoto*, Atsuko Yamaguchi and Akinori YonezawaAbstract
Background: There is a growing need for efficient and integrated access to databases provided by diverse
institutions. Using a linked data design pattern allows the diverse data on the Internet to be linked effectively and
accessed efficiently by computers. Previously, we developed the Allie database, which stores pairs of abbreviations
and long forms (LFs, or expanded forms) used in the life sciences. LFs define the semantics of abbreviations, and
Allie provides a Web-based search service for researchers to look up the LF of an unfamiliar abbreviation. This
service encounters two problems. First, it does not display each LFs definition, which could help the user to
disambiguate and learn the abbreviations more easily. Furthermore, there are too many LFs for us to prepare a full
dictionary from scratch. On the other hand, DBpedia has made the contents of Wikipedia available in the Resource
Description Framework (RDF), which is expected to contain a significant number of entries corresponding to LFs.
Therefore, linking the Allie LFs to DBpedia entries may present a solution to the Allies problems. This requires a
method that is capable of matching large numbers of string pairs within a reasonable period of time because Allie
and DBpedia are frequently updated.
Results: We built a Linked Open Data set that links LFs to DBpedia titles by applying key collision methods
(i.e., fingerprint and n-gram fingerprint) to their literals, which are simple approximate string-matching methods. In
addition, we used UMLS resources to normalise the life science terms. As a result, combining the key collision
methods with the domain-specific resources performed best, and 44,027 LFs have links to DBpedia titles. We
manually evaluated the accuracy of the string matching by randomly sampling 1200 LFs, and our approach
achieved an F-measure of 0.98. In addition, our experiments revealed the following. (1) Performances were similar
independently from the frequency of the LFs in MEDLINE. (2) There is a relationship (r2 = 0.96, P < 0.01) between
the occurrence frequencies of LFs in MEDLINE and their presence probabilities in DBpedia titles.
Conclusions: The obtained results help Allie users locate the correct LFs. Because the methods are computationally
simple and yield a high performance and because the most frequently used LFs in MEDLINE appear more often in
DBpedia titles, we can continually and reasonably update the linked dataset to reflect the latest publications and
additions to DBpedia. Joining LFs between scientific literature and DBpedia enables cross-resource exploration for
mutual benefits.Background
Linked data in bioinformatics and systems biology
Because of the rapid developments in the life sciences
and the large amounts of open data available on the
Internet, a rising number of databases are being released.
Currently, 1,380 databases are listed on the 2012 NAR
Database Summary Paper Alphabetic List [1], which
have been carefully selected by Nucleic Acids Research* Correspondence: yy@dbcls.rois.ac.jp
Database Center for Life Science, Faculty of Engineering Bldg. 12,
The University of Tokyo, 2-11-16, Yayoi, Bunkyo-ku, Tokyo, Japan
© 2013 Yamamoto et al.; licensee BioMed Cen
Creative Commons Attribution License (http:/
distribution, and reproduction in any mediumeditors [2]. A single organisation or institution cannot
build or maintain all of these databases, but none of the
databases is all-encompassing; each researcher must
identify multiple databases that are relevant to his/her
research and learn how to use them to look up specific,
designated entries. In such a situation, linking related
entries would make the research process more efficient.
For example, a systems biology researcher would need
to access databases of chemical biology and drug data to
discover a new drug. This type of interdisciplinary work
has successfully addressed various complicated researchtral Ltd. This is an Open Access article distributed under the terms of the
/creativecommons.org/licenses/by/2.0), which permits unrestricted use,
, provided the original work is properly cited.
Yamamoto et al. Journal of Biomedical Semantics 2013, 4:8 Page 2 of 7
http://www.jbiomedsem.com/content/4/1/8challenges [3]. Linking related research outcomes be-
yond field-specific boundaries would further the pro-
gress of interdisciplinary studies.
The scientific literature, the entities as abbreviations and
long-forms
To link heterogeneous databases and provide users with
access in an integrated manner, publishing datasets fol-
lowing the linked data design pattern [4] has increasing
appeal to database developers and users. One reason for
this is that it utilises well-known open standards, such
as the Resource Description Framework (RDF) [5] or
Hypertext Transfer Protocol (HTTP). Within this con-
text, we decided to make our abbreviation database,
Allie, downloadable in RDF format. The Allie database
stores life science abbreviations and their long forms
(LFs, or expanded forms) [6]. We have been providing
a Web interface to lookup the candidate LFs of given
abbreviations since 2008, and we began to provide a
SPARQL endpoint in 2011. The dataset is updated
monthly to keep up with the latest publications, and it is
publicly available free of charge. Our motivation for de-
veloping this database is based on the following facts:
Life science researchers often have difficulty in under-
standing papers that are outside their area of expertise
partly because abbreviations are commonly used in
those papers and polysemous ones appear frequently.
We use the ALICE tool [7] to extract pairs of abbrevia-
tions and their corresponding LFs from the entire
MEDLINE database. Allie stores each pair with its asso-
ciated PubMed IDs such that users can easily find its
provenance. Although Allie displays several contexts in
which each pair is used (e.g., bibliographic data, co-
occurring abbreviations, and a main research area), Allie
does not define the LF itself. We assumed that providing
a description of each LF would be beneficial to the users;
however, creating descriptions for every LF from scratch
is impractical at our institution.
Wikipedia as background information
Wikipedia [8] is an open, collaboratively developed en-
cyclopedia project and the largest, most popular general
reference work on the Internet [9]; it is expected to con-
tain a considerable number of entries corresponding to
LFs. Therefore, using this content as a reference for the
descriptions of each LF may provide a solution. Further-
more, there are advantages to using Wikipedia. First, its
content is licensed under Creative Commons Attribution
Share-Alike 3.0, and we can freely obtain the entire
dataset. Second, in the form of DBpedia [10], Wikipedia
is also used as a hub for the Linked Open Data (LOD)
cloud [11]; Allie users can access related information
that would otherwise be difficult to find by following a
series of links from an abbreviation-LF pair. The LODcloud is an outcome of the Linking Open Data project
[12], which is a grassroots community effort founded in
2007 to identify datasets available under open licenses,
re-publish them in RDF on the Web, and interlink them
with each other. The latest version contains 295 datasets
that consist of 31 billion RDF triples, which are inter-
linked by approximately 504 million RDF links [13]. In
the cloud, 42 datasets are categorised as life science
datasets. Third, the titles of Wikipedia entries can be
used as a gold standard set to normalise the lexical vari-
ants of LFs because they are the results of the collabora-
tive knowledge building.
Following these discussions, we decided to link Allie
LFs to DBpedia entries. Although this task appears ra-
ther straightforward, as is frequently the case with nat-
ural language processing, it is difficult to cope with the
lexical variants. In addition, the numbers of LFs and
Wikipedia entries are large (1,768,718 LFs and 8,826,375
entries, respectively), and new abbreviations and Wiki-
pedia entries are continually generated independently.
Integration of literature and Wikipedia enables resear-
chers at different levels of expertise to better exploit
either resource by linking established knowledge with
latest research results. Therefore, the linking process
must be automated and completed within a reasonable
time. Thus, our task consists of checking whether each
LF matches any of the DBpedia titles, efficiently taking
their lexical variants into consideration and making a
link using the owl: sameAs property, which is a built-in
Web Ontology Language (OWL) property.
To make links effectively and efficiently between Allie
and DBpedia, we took a strategy consisting of two ap-
proaches, that is, approximate string matching and
dictionary-based term normalisation to take domain
knowledge into consideration. We used Google Refine
[14] for the former, which is a tool used to clean data
that can also be used for data reconciliation. Google Re-
fine has several functions, including data distribution
analysis and values editing. In addition, it provides clus-
tering functionality within the values of a column to
help users identify lexical variants or typographical er-
rors, and the Java source code is available for free. We
can choose several well-studied clustering algorithms
and approximate string-matching methods for use in
this clustering [15]. For the latter, we used the Unified
Medical Language System (UMLS) dictionaries and a
tool (UMLS resources) [16] to map an inflectional form
to its corresponding base form (i.e., normalisation).
Our contributions to the biomedical semantics com-
munity include publishing a Linked Open Data set for
the life sciences, demonstrating a method of efficiently
and effectively constructing links between large and rap-
idly changing datasets, and enriching the Allie service.
In addition, we confirmed that there is a statistically
Figure 1 Link results. Link results for five methods/conditions: exact match, the key collision methods of fingerprint and bi-gram fingerprint, and
the combined method with and without using the UMLS resources. The link ratio indicates the number of LFs with a link to their corresponding
DBpedia titles divided by the total number of LFs at each bin.
Table 1 F-measures for each bin and method
Bin Exact match Fingerprint Bi-gram fingerprint Combined
1 0.80 0.98 0.99 0.99
2 0.83 0.92 0.94 0.96
3 0.86 0.97 0.98 0.99
4 0.80 0.96 0.95 0.99
5 0.85 0.96 0.98 0.98
6 0.88 0.94 0.97 0.97
7 0.88 0.97 0.99 0.99
8 0.92 0.96 0.97 0.97
9 0.86 0.97 0.97 0.98
10 0.93 0.98 0.98 1.00
11 0.90 0.97 0.98 0.99
12 0.91 0.98 0.98 0.99
Each F-measure expresses the accuracy of the string match. Note that all the
data have been obtained using the UMLS-based term normalisation pre-
processes, and these do not indicate how many LFs have links to their
corresponding DBpedia titles.
Yamamoto et al. Journal of Biomedical Semantics 2013, 4:8 Page 3 of 7
http://www.jbiomedsem.com/content/4/1/8supported relationship between abbreviation-LF usage in
the life science literature and the probability of its ap-
pearance in DBpedia titles.
Results
To see a relationship between the occurrence frequen-
cies of LFs in MEDLINE and the probabilities of their
appearance in DBpedia titles, we split the LFs used in
the evaluation into 12 bins according to the numbers of
their occurrences in MEDLINE. Then, we sampled 100
LFs randomly from each bin; therefore, we manually
evaluated 1200 randomly sampled LFs.
Our experiments resulted in two sets of data: a set of
link (match) ratios between the LFs and DBpedia titles
for each bin and a set of match performance results. A
link ratio means here that the number of the generated
links divided by the total number of LFs in a bin. The
former set demonstrates the extent to which our
methods can produce links between LFs and DBpedia ti-
tles (these may include false links), and the latter set de-
scribes their accuracy; Figure 1 and Table 1 indicate
these values, respectively.
Figure 1 illustrates the link ratios for each bin and
each method. For the results of the exact match, finger-
print, and bi-gram fingerprint methods, the UMLS re-
sources were not used; for the combined method, the
results with and without the UMLS resources are shown.
The results indicate that as the number of occurrences
of an LF in MEDLINE increases, the link ratio also
increases, with the exception of the exact match me-
thod. This outcome suggests that approximate string-
matching methods (i.e., the key collision methods of
fingerprint and bi-gram fingerprint) significantly contrib-
ute to the increase in link ratios. In addition, combining
the key collision methods also improves the ratios, andusing the UMLS resources is effective for all the bins.
Consequently, the best link ratios can be obtained with
the combined method and using the UMLS resources.
Note that we opted for bi-gram for the n-gram finger-
print method subsequent to our preliminary experi-
ments. In addition, we also experimented the case of
using the UMLS resources only (i.e., the exact match
method with the UMLS resources), but the result was
worse than that of the combined method.
Table 1 presents the F-measures for each bin and
method. This indicates that the match performance is
high (from 0.92 to 1.00) for all of the bins and methods,
with the exception of the exact match (from 0.80 to
0.93). Table 2 indicates the numbers of false negatives,
false positives, and true positives. Here, we identify a
false negative if an LF does not have a link to its
Table 2 The numbers of false negatives (NPs), false
positives (FPs), and true positives (TPs) for each method
FN FP TP
Exact match 221 2 773
Fingerprint 59 9 928
Bi-gram fingerprint 40 9 945
Combined 28 4 967
Note that all of the data have been obtained using the UMLS-based term
normalisation pre-processes.
Yamamoto et al. Journal of Biomedical Semantics 2013, 4:8 Page 4 of 7
http://www.jbiomedsem.com/content/4/1/8corresponding DBpedia title (where there should be a
link) and a false positive if an LF is erroneously linked to
DBpedia titles.
These results indicate that the lower link ratios of the
less frequently appearing LFs are not attributable to the
drawbacks of the methods that were used; rather,
the LFs that do not have a link to DBpedia do not have
corresponding DBpedia titles.
We investigated the relationship between the occur-
rence frequencies of LFs in MEDLINE and their prob-
abilities of presence in DBpedia titles. As a result, there
is a linear relationship (r2 = 0.96, P < 0.01) between the
occurrence frequencies of LFs and the negative loga-
rithm of their probabilities of absence from DBpedia ti-
tles. More precisely, the relationship obtained based on
simple linear regression analysis is
log 1 yð Þ ¼ 0:2xþ 0:92;
where x is the frequency-based bin number of an LF and
y is the probability of its presence in DBpedia titles. In
other words, the equation is
1 y ¼ exp 0:2 xþ 4:6ð Þ:
The detailed data and calculations used to obtain this
result are available in the Additional file 1. Note that be-
cause the frequency ranges of bins 1 and 2 are different
from those of bins 3 to 11, we replaced them with a sin-
gle bin whose link ratio is the average of those of the
two bins. The range of bin 12 is also different, but the
numbers of the LFs become marginal if we split the bin,
and we believe that the effect of using that bin is insig-
nificant toward obtaining the relationship. We then
reassigned the bin numbers from 0 to 10 (i.e., the former
bins 1 and 2 become bin 0, bin 3 becomes bin 1, and
so on).
Discussion
Using the two simplest approximate string-matching
methods and the domain-specific resources, we de-
termined that our hypothesis was supported by positive
results. The key collision methods that we used are sim-
pler and faster than other approximate string-matching
methods, such as the Dice coefficient [17], the Jaro-Winkler distance [18], or the Levenshtein distance [19].
These methods are more flexible and are capable of
finding less similar but possibly related strings; however,
these methods also raise the probability of false positives
and are more computationally intensive. Therefore, the
methods can be used most effectively if there is a dra-
matic increase in the matching performance. We used
the Levenshtein distance, but we terminated the process
before completion after it had continued for more than
three days. In contrast, the key collision methods took
less than ten minutes under the same circumstances and
machine environment.
Although the matching performance was high, our fu-
ture research will benefit from a few alterations. There
are some typical causes of false negatives and false posi-
tives. Three issues commonly result in false negatives
(each example denotes a pair of an LF in Allie and its
corresponding DBpedia title):
a. Presence/absence of an additive term
Examples include c-fos protein and C-Fos,
bronchiolitis obliterans syndrome and
Bronchiolitis obliterans, or natural killer T and
Natural Killer T cell.
b. Abbreviation/LF
Examples include Programmed death-1 and PD-1,
reverse transcription PCR and Reverse
transcription polymerase chain reaction, or
endothelial NO synthase and Endothelial NOS.
c. Variant/synonym
Examples include cytochrome P450 1A1 and
Cytochrome P450, family 1, member A1;
epithelial to mesenchymal transition and
Epithelial mesenchymal transition; or
transoesophageal echocardiography and
Transesophageal echocardiogram.
To resolve the first issue, we require a simple diction-
ary of these domain-specific additive terms (for proteins,
syndromes, or cells). This dictionary can be used to
match the last word of a compound; if there is a match
for a term (compound), the matched word can be ig-
nored. The second issue is caused by the difference be-
tween an abbreviation and its LF and can be resolved
using Allies dictionary. For example, there is a pair of
PD-1 and programmed death-1 in Allie; therefore, the
both can be linked. However, we must ensure that the
PD-1 used as a DBpedia title definitely describes pro-
grammed death-1 because this may be a polysemous
word. The third issue is caused by synonymy, and the
UMLS resources can partially help if they can success-
fully enumerate the relevant synonyms.
In addition, DBpedia data (other than titles) can be
used to address these issues. For example, the predicates
Yamamoto et al. Journal of Biomedical Semantics 2013, 4:8 Page 5 of 7
http://www.jbiomedsem.com/content/4/1/8http://dbpedia.org/ontology/wikiPageDisambiguates and
http://dbpedia.org/ontology/wikiPageRedirects denote
that their subjects and objects have synonymous rela-
tionships one another. Therefore, if there is a triple stat-
ing that PD-1 is a synonym for programmed death-1, the
problem described above can be solved.
Although there are far fewer false positives than false
negatives, the following examples are relevant to this do-
main. The first example is mitogen-activated protein kin-
ase kinase and Mitogen-activated protein kinases. In
general, it is considered erroneous if a term appears con-
secutively multiple times, and the key collision methods
do not distinguish the former from the latter. The sec-
ond example is RNA polymerase II and Rna polymerase
iii. In this case, the methods do not distinguish between
the two words II and iii, in which the difference is the
number of appearances of a repeated letter. These cases
require unique fixes.
As for our choice of DBpedia, some of the 42 life sci-
ence datasets in the LOD cloud might be more relevant
to the Allie database. However, Allie covers all of the life
science research areas as MEDLINE does and needs the
description of each LF; there is not such one in the 42
datasets. Furthermore, as DBpedia is the hub, once suc-
cessfully linking Allie to it, we can access to the relevant
datasets via DBpedia.
Regarding constructing linked data, we can use an in-
tegrated development environment called Silk [20] to
build linked data between any two sets of RDF data for
which SPARQL endpoints have been provided. Although
Silk provides several types of string-matching methods,
such as the Jaccard similarity coefficient, the Jaro dis-
tance, or the Levenshtein distance, key collision methods
are not included. In addition, the current version (Ver-
sion 2.5.3) of Silk does not provide a way to use external
resources to match a pair of strings, such as the UMLS
resources.
First, we assumed that the more frequently the LFs ap-
pear in MEDLINE, the more likely they are to appear in
DBpedia as titles. Once this assumption has been sup-
ported, we can expect that if an LF will be used more
frequently in literature whose bibliographic data are in
MEDLINE, the LF will also be used as a title in DBpedia.
We believe that this feature is reasonable for use as an
enhancement for Allie because the LFs that more re-
searchers want to look up are more likely to have their
corresponding titles in DBpedia.
Next, we confirmed that our assumption was sup-
ported based on the relationship between the occurrence
frequencies of LFs in MEDLINE and their presence
probabilities from DBpedia titles. This result can be
understood to indicate that the likelihood of an LF
appearing in the DBpedia titles is proportional to the ab-
sence probabilities of the LFs in DBpedia titles whoseoccurrence frequencies in MEDLINE are the same as
that of the LF. Therefore, if an LF is continually used in
the life science literature, it is likely to appear in the
DBpedia titles.
We set the threshold cut-off to 10 for the occurrence
frequencies of LFs. This might be too high, and we need
to investigate cases below it. Our preliminary survey in-
dicated that more false negatives occurred owing to the
case c (variant/synonym) mentioned above.
Conclusions
First, we showed that the LFs that are most frequently
used in MEDLINE titles or abstracts are more likely to
appear as titles in DBpedia; LFs that do not currently
have a link to DBpedia titles are expected to obtain a
link if they are used more frequently in MEDLINE. This
finding yields useful knowledge for Allie users.
Second, we proposed an effective linked data-building
process that uses the two key collision methods (i.e., fin-
gerprint and bi-gram fingerprint) and the UMLS re-
sources, based on our attempt to link LFs in the Allie
database to DBpedia titles. We performed the experi-
mental comparisons of exact match, fingerprint, bi-gram
fingerprint, and the method combining the two key col-
lision methods with the UMLS-based pre-processing.
The results demonstrate that of the four methods, the
combined method performed best, yielding a very high
performance with an F-measure of 0.98. Because the key
collision methods are known to be much faster than
other approximate string-matching methods, using them
is an effective way to build linked data, even for very
large datasets.
Together with the obtained relationship, this outcome
is promising as a means of managing the rapid growth
of data in MEDLINE and DBpedia; it allows us to con-
tinually update the linked dataset.
We analysed examples of false negatives and found
that they exhibit certain typical patterns, which will be
explored in the future. From the perspective of Linked
Open Data, we plan to place the newly acquired links on
the results page of the Allie search service and add them
to the Allie RDF datasets, which are publicly down-
loadable for free and accessible at the SPARQL end-
point [21].
Methods
Data
a. Allie
Currently, the Allie database has 1,768,718 LFs,
which were automatically extracted from
MEDLINE (titles and abstracts; more than 20
million entries) by ALICE. Although ALICEs
performance is fairly high (a recall of 95% and a
Yamamoto et al. Journal of Biomedical Semantics 2013, 4:8 Page 6 of 7
http://www.jbiomedsem.com/content/4/1/8precision of 97% on randomly selected MEDLINE
data), erroneous expressions are included,
especially when the number of their appearances
in MEDLINE is low. Therefore, we decided to use
expressions that appear more than 10 times in
MEDLINE as a target dataset to be linked to
DBpedia. As a result, the number of the extracted
LFs is 91,573.
b. DBpedia
We used the English titles of DBpedia version 3.7
in N-Triples (labels_en.nt.bz2). There are
8,826,375 titles.
c. UMLS
UMLS is a set of files and software that brings
many health and biomedical vocabularies and
standards together developed and is maintained
by the U.S. National Library of Medicine
(NLM). To absorb some of the lexical variants
in the LFs and DBpedia titles, we used the
UMLS SPECIALIST Lexicon and the lvg norm
tool. For the Lexicon, we used the Agreement
and Inflection and the Spelling Variants files
to map each term to its basic form. If a basic
form is found in which the inflectional form
exactly matches an LF or a DBpedia title, it is
replaced with the basic form. The lvg tool is
suitable for this purpose, but our survey
showed that it fails to normalise some terms
that can be handled with the Lexicon mapping.Table 3 Distribution of LF appearances in MEDLINE
Bin number Frequency range
of appearances in MEDLINE
Number of
unique LFs
1 10 - 49 69 416
2 50 - 99 10 293
3 100 - 199 5 691
4 200 - 299 2 030
5 300 - 399 1 033
6 400 - 499 639
7 500 - 599 444
8 600 - 699 329
9 700 - 799 233
10 800 - 899 185
11 900 - 999 154
12 >= 1 000 1 126
The frequency range indicates that the number of appearances of an LF in
MEDLINE falls within that range.String matching
We used two key collision methods for approximate
string matching: fingerprint and n-gram fingerprint [15].
In addition, as a baseline to compare the results, exact
string matching was used. The key collision methods are
explained well in [15]; a brief explanation is provided
here. For each string, its alternative expression (key) is
generated and matched against another strings key to
determine whether they are identical. The key collision
methods are fast: their computational complexity is lin-
ear in the number of values processed. The difference
between fingerprint and n-gram fingerprint is the way in
which keys are generated. The former method consists
of a series of string manipulation processes to generate a
key for each string, such as splitting the string into
whitespace-separated tokens, sorting the tokens and re-
moving duplicates, and joining the tokens back together.
This method absorbs word order variations and is the
least likely to produce false positives. The latter method
is similar to the fingerprint method, but instead of using
whitespace-separated tokens, it uses n-grams in which
the n (or the size in chars of the token) can be specified
by the user. In our task, we use size two, based on acomparison of the results from the use of bi-gram
and tri-gram.Experiments
To test our assumption (i.e., the relationship between
the numbers of appearance of LFs in MEDLINE and the
probability of their appearance in DBpedia), we split the
LF data into 12 bins according to their occurrence fre-
quencies in MEDLINE (Table 3). For example, the LF
lentigo maligna melanoma appears 63 times in
MEDLINE and is therefore in bin 2 (whose range is 50
to 99). The number of unique LFs (non duplicate LFs)
that reside in bin 2 is 10,293. For each bin, we then
obtained the matching results between the LFs and the
DBpedia entry titles in the four ways, that is, exact
match, fingerprint, n-gram fingerprint, and combining
these two key collision methods. In the fourth way, a
link between an LF and a DBpedia entry title is accepted
if the fingerprint or the n-gram fingerprint methods find
a match; if the two methods find different matches be-
tween an LF and DBpedia titles, the shorter matched
title is accepted. Furthermore, if both string lengths are
the same, the one in which the ratio of the numbers of
upper to lower case characters is closest to that of the
LF is accepted. For each method, we also obtained re-
sults using the UMLS resources.
To evaluate the results, we randomly sampled 100 LFs
per bin and manually checked whether each link was ap-
propriate. Furthermore, for each LF without any links,
we determined whether there was a corresponding
DBpedia title by searching for any corresponding strings
in Wikipedia using Google.
Yamamoto et al. Journal of Biomedical Semantics 2013, 4:8 Page 7 of 7
http://www.jbiomedsem.com/content/4/1/8Additional file
Additional file 1: Supplementary Data.
Abbreviations
LF: Long form; RDF: Resource Description Framework; HTTP: Hypertext
Transfer Protocol; UMLS: Unified Medical Language System.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
YY designed this work, and ATY discussed the experiments and outcomes.
AKY oversaw this work. All authors read and approved the final manuscript.
Acknowledgements
This work is funded by the Integrated Database Project, Ministry of
Education, Culture, Sports, Science and Technology of Japan and National
Bioscience Database Center (NBDC) of Japan Science and Technology
Agency (JST). We thank Mr. Toyofumi Fujiwara for helping with the
development of the RDF version of the Allie database.
Received: 31 March 2012 Accepted: 10 March 2013
Published: 13 March 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Midford et al. Journal of Biomedical Semantics 2013, 4:34
http://www.jbiomedsem.com/content/4/1/34DATABASE Open AccessThe vertebrate taxonomy ontology: a framework
for reasoning across model organism and species
phenotypes
Peter E Midford1,2, Thomas Alex Dececchi3, James P Balhoff2,4, Wasila M Dahdul2,3, Nizar Ibrahim5, Hilmar Lapp2,
John G Lundberg6, Paula M Mabee3, Paul C Sereno5, Monte Westerfield7, Todd J Vision2,4 and David C Blackburn8*Abstract
Background: A hierarchical taxonomy of organisms is a prerequisite for semantic integration of biodiversity data.
Ideally, there would be a single, expansive, authoritative taxonomy that includes extinct and extant taxa,
information on synonyms and common names, and monophyletic supraspecific taxa that reflect our current
understanding of phylogenetic relationships.
Description: As a step towards development of such a resource, and to enable large-scale integration of
phenotypic data across vertebrates, we created the Vertebrate Taxonomy Ontology (VTO), a semantically
defined taxonomic resource derived from the integration of existing taxonomic compilations, and freely
distributed under a Creative Commons Zero (CC0) public domain waiver. The VTO includes both extant and
extinct vertebrates and currently contains 106,947 taxonomic terms, 22 taxonomic ranks, 104,736 synonyms,
JOURNAL OF
BIOMEDICAL SEMANTICS
Costa et al. Journal of Biomedical Semantics 2013, 4:32
http://www.jbiomedsem.com/content/4/1/32
RESEARCH Open Access
The Drosophila anatomy ontology
Marta Costa1, Simon Reeve1, Gary Grumbling2 and David Osumi-Sutherland1*
Abstract
Background: Anatomy ontologies are query-able classifications of anatomical structures. They provide a widely-used
means for standardising the annotation of phenotypes and expression in both human-readable and
programmatically accessible forms. They are also frequently used to group annotations in biologically meaningful
ways. Accurate annotation requires clear textual definitions for terms, ideally accompanied by images. Accurate
grouping and fruitful programmatic usage requires high-quality formal definitions that can be used to automate
classification and check for errors. The Drosophila anatomy ontology (DAO) consists of over 8000 classes with broad
coverage of Drosophila anatomy. It has been used extensively for annotation by a range of resources, but until
recently it was poorly formalised and had few textual definitions.
Results: We have transformed the DAO into an ontology rich in formal and textual definitions in which the majority
of classifications are automated and extensive error checking ensures quality. Here we present an overview of the
content of the DAO, the patterns used in its formalisation, and the various uses it has been put to.
Conclusions: As a result of the work described here, the DAO provides a high-quality, queryable reference for the
wild-type anatomy of Drosophila melanogaster and a set of terms to annotate data related to that anatomy. Extensive,
well referenced textual definitions make it both a reliable and useful reference and ensure accurate use in annotation.
Wide use of formal axioms allows a large proportion of classification to be automated and the use of consistency
checking to eliminate errors. This increased formalisation has resulted in significant improvements to the
completeness and accuracy of classification. The broad use of both formal and informal definitions make further
development of the ontology sustainable and scalable. The patterns of formalisation used in the DAO are likely to be
useful to developers of other anatomy ontologies.
Keywords: Drosophila, Anatomy, Ontology, OWL, OBO, Gene Ontology, FlyBase, Description logic
Background
Anatomy ontologies
Anatomy ontologies are queryable classifications of
anatomical structures. They are commonly used by bioin-
formatics resources to provide controlled vocabularies for
annotating a range of entities (such as research papers,
genes and genotypes). Typically curation is done manually
and consists of assertions about phenotypes and expres-
sion patterns [1-4] but many other types of assertion are
possible. For manual annotation, class and part hierar-
chies in ontologies provide terms with a range of speci-
ficity allowing curators to choose an appropriately precise
term depending on the information available. Term names
*Correspondence: djs93@gen.cam.ac.uk
1FlyBase, Department of Genetics, University of Cambridge, Downing Street,
Cambridge, UK
Full list of author information is available at the end of the article
alone are frequently ambiguous. Textual definitions of
terms, ideally supplemented with images, are therefore
important for consistent and accurate manual annotation.
Anatomy ontologies are also used to group annotations
in biologically meaningful ways. This is commonly done
by grouping annotations using class and part hierarchies
(partonomy). For example, a query for genes expressed in
the Drosophila leg would return gene expression anno-
tated with the term middle leg (a subclass of leg) and
claw (a part of the leg) as well as with the term leg. The
usefulness of such grouping depends on the accuracy of
classification and of assertions about partonomy. More
sophisticated groupings can be achieved by taking advan-
tage of ontology semantics expressed in a formal language
such asOWL. For example, Virtual Fly Brain (VFB) groups
annotations based on inference of overlap between neu-
rons and gross neuro-anatomical structures as well as
using partonomy and classification [5,6].
© 2013 Costa et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Costa et al. Journal of Biomedical Semantics 2013, 4:32 Page 2 of 11
http://www.jbiomedsem.com/content/4/1/32
Various resources, including model organism databases,
use anatomy ontologies as searchable stores of informa-
tion about anatomy. Annotation of terms with synonyms
provides a means for users to search for anatomical
structures using the various names used for them in
the literature. Textual definitions provide human read-
JOURNAL OF
BIOMEDICAL SEMANTICS
Livingston et al. Journal of Biomedical Semantics 2013, 4:38
http://www.jbiomedsem.com/content/4/1/38RESEARCH Open AccessRepresenting annotation compositionality and
provenance for the Semantic Web
Kevin M Livingston1*, Michael Bada1, Lawrence E Hunter1 and Karin Verspoor2,3Abstract
Background: Though the annotation of digital artifacts with metadata has a long history, the bulk of that work
focuses on the association of single terms or concepts to single targets. As annotation efforts expand to capture
more complex information, annotations will need to be able to refer to knowledge structures formally defined in
terms of more atomic knowledge structures. Existing provenance efforts in the Semantic Web domain primarily
focus on tracking provenance at the level of whole triples and do not provide enough detail to track how
individual triple elements of annotations were derived from triple elements of other annotations.
Results: We present a task- and domain-independent ontological model for capturing annotations and their linkage
to their denoted knowledge representations, which can be singular concepts or more complex sets of assertions.
We have implemented this model as an extension of the Information Artifact Ontology in OWL and made it freely
available, and we show how it can be integrated with several prominent annotation and provenance models. We
present several application areas for the model, ranging from linguistic annotation of text to the annotation of
disease-associations in genome sequences.
Conclusions: With this model, progressively more complex annotations can be composed from other annotations,
and the provenance of compositional annotations can be represented at the annotation level or at the level of
individual elements of the RDF triples composing the annotations. This in turn allows for progressively richer
annotations to be constructed from previous annotation efforts, the precise provenance recording of which
facilitates evidence-based inference and error tracking.
Keywords: Ontology, Conceptual data modeling, Annotation, Markup, Provenance, OWL, RDFBackground
Annotation of artifacts such as documents and images
with metadata is a scholarly practice with a long history. A
wide variety of annotations have been represented in a
wide range of formats. In the bulk of that work, each an-
notation consists of a basic association of one conceptual
resource (e.g., an ontology class, schema element, database
identifier) with one target (e.g., document, text span, data-
base entry) via an explicit or implicit relationship. Single-
concept annotations have proven very useful, for example,
in computing term enrichment [1] or for indexing for
search [2]; however, they do not provide a detailed repre-
sentation of the content they are describing. As information* Correspondence: kevin.livingston@ucdenver.edu
Equal contributors
1Department of Pharmacology, University of Colorado Anschutz Medical
Campus, Aurora, CO, USA
Full list of author information is available at the end of the article
© 2013 Livingston et al.; licensee BioMed Cen
Commons Attribution License (http://creativec
reproduction in any medium, provided the orneeds increase and annotation efforts expand to capture
more complex information, complex knowledge structures
formally defined in terms of more atomic knowledge struc-
tures will need to be represented.
Where they exist, more structured annotations tend to
be represented in ad hoc formats suited to one particular
type of annotation or task but are not broadly applicable
or interoperable. Several prominent annotation models
not limited to specific types of tasks or information have
been created, and components that enable annotations
to denote knowledge structures more complex than
atomic concepts have been added very recently to these
models [3,4]. Yet there have been no mechanisms put
forth by which these more complex annotations can
refer to other annotations and by which their prov-
enance can be unambiguously recorded. There have also
been prominent efforts in scientific workflow prov-
enance [5,6]. That work, however, primarily focuses ontral Ltd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 2 of 15
http://www.jbiomedsem.com/content/4/1/38annotating experimental data, typically annotating lists
of identifiers or numeric data with their origins, not on
annotating with dynamically composed and compo-
sitional knowledge structures.
An effective annotation model, in addition to being ap-
plicable to many annotation use cases and supporting
the specification of complex knowledge structures, needs
to be able to unambiguously represent annotation prov-
enance. While ontologies strive to be complete, it is
likely that specific applications will require dynamic con-
struction of concepts, either through data-driven methods
[7] or compositional concept formation [8]. To support
and document the provenance of these more complex
annotations, annotators (both human and computational)
need the ability to refer to existing annotations as the basis
of more complex annotations. For example, in the lin-
guistic domain, an annotation representing part of a
syntactic parse tree may wish to build upon existing
token or part-of-speech annotations. Similarly, in the
biomedical domain, a protein interaction event annotation
may wish to leverage existing annotations identifying spe-
cific proteins. As annotation efforts become more ambi-
tious, they will naturally build upon previous annotation
efforts, and tracking the provenance of constructed know-
ledge representations being used for annotations at a fine-
grained level will be important to facilitate inference and
error analysis.
This paper proposes a task- and domain-independent
formal ontological model for the creation of annotations
and their linkage to their denoted knowledge representa-
tions, which can be singular concepts or more complex
knowledge in the form of sets of RDF assertions. With this
model, progressively more complex annotations can be
composed from other annotations, and this provenance
can be unambiguously represented at either a coarse- or
fine-grained level. We have designed our annotation
model to be generic so as to facilitate the concurrent use
of multiple types of annotations (e.g., syntactic annotation
and semantic annotation). Additionally, it allows for the
creation of arbitrarily complex annotations, both in terms
of their denoted knowledge and of any other annotations
upon which they rely. All of this information can be loss-
lessly recorded, facilitating inference and error tracking in
large computational annotation efforts. We have imple-
mented this model as an extension of the Information
Artifact Ontology in OWL and made it freely available.
We also show how it can be integrated with several other
prominent generic annotation models.
Results
Overview
A central aspect of our model is the capability to accurately
capture the provenance of annotations, in terms of precur-
sor annotations, created by a human or computationalannotator. The annotation model we present here is gener-
ally applicable to arbitrarily complex, structured annota-
tions applied to any content in any context. It is not
specific to text annotations, although our primary use cases
are related to understanding biomedical text.
Our model provides two key contributions above exist-
ing annotation and provenance models. First, we provide
a generic model for complex and compositional annota-
tions that extends existing general-purpose annotation
models. Second, we provide a model for documenting
the provenance of the construction of the triples used as
the denoted knowledge representations by these annota-
tions. Our model goes beyond modeling the provenance
of whole triples (for which there are sufficient existing
methods, as discussed in the Related Work section) and
extends the provenance modeling to document the
source of individual statement elements that are used to
construct triples.
This proposal is neutral with respect to annotation
template; i.e., the choice of terminologies, ontologies, or
schemas used for annotation and the nature of the de-
noted knowledge representations is left to the annotator.
Several existing annotation models handle the associ-
ation of annotations to text or other targets, and we dis-
cuss the integration of their representations with our
model in Additional file 1 and Additional file 2. Addi-
tionally, as this proposal focuses on the linkage of anno-
tations to their denoted knowledge representations and
on the provenance of these knowledge representations,
details about the recording of other types of annotation
metadata such as author and creation date (for which
there are existing proposals, e.g., [3,4,9]) are largely
elided from this paper. Finally, our ontological model is
neutral with respect to the methodology by which any
such annotations are created.
We reuse or extend existing community-curated on-
tologies where possible, and we therefore present our
proposal as an extension of the Information Artifact
Ontology (IAO), which is a member of the Open Bio-
medical Ontologies library of ontologies [10] (though
not all of the concepts of these ontologies are specific to
the biomedical realm). The IAO focuses on the repre-
sentation of types of information content entities, which
are defined to stand in relation of aboutness to other
entities; that is, an information content entity is in some
way about some other concept(s). For example, within
the biomedical domain, data, images, and text are all in
some way about sets of biomedical concepts. The IAO
provides a hierarchy of types of information content en-
tities as well as types of aboutness, including denotation,
in which the information content entity specifically re-
fers to some other concept (e.g., the word apple de-
notes either a specific apple or the more general concept
of an apple). We hold that an annotation is a type of
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 3 of 15
http://www.jbiomedsem.com/content/4/1/38information content entity, as it is in some way about
the entity it is annotating. We are engaged in the on-
going process of submitting our model to the IAO for
inclusion. An OWL representation of our model as an
extension of the IAO is provided in Additional file 3.
Namespace and notation
Our in-house knowledge base of biomedicine (KaBOB)
is the aggregator of our work. KaBOB extensions of an
ontology are named by prefixing the ontologys name-
space with the letter k; the namespace kiao: is therefore
used for our extension of the IAO (whose namespace is
iao:), and the ex: namespace is used for examples. In this
document, fixed-width font will be used to identify
concepts. Class names begin with a capital letter, while in-
stance and property names begin with a lowercase letter.
Additionally, instances are named mnemonically with let-
ters corresponding to their class names; e.g., instances of
the class RdfResourceAnnotation have names
starting with ra. RDF triples and quads are presented
using an abbreviated n-triple/quad format for readabil-
ity, using name-space-abbreviation:local-name
instead of full URIs.
Representation of annotations
We have created a top-level Annotation class, defin-
ing an annotation as an information content entity that
is used to concisely describe, comment on, or otherwise
make an assertion or set of assertions about an existing
information content entity. Thus, for example, a linguis-
tic part-of-speech tag can be used to annotate a word
within a piece of text to describe its syntactic or mor-
phological behavior; a Java keyword (e.g., @depre-
cated) can be used to annotate a segment of Java
source code to specify a property of a Java class, method,
variable, parameter, or package; and a GO term can be
used to annotate a digital representation of a gene or
gene product to make an assertion about some aspect of
the biological functionality of the latter. Conciseness
seems to be a common trait among the many types of
annotations we have considered, so, e.g., a book written
about a poem would seem to be beyond the bounds of
what most would consider an annotation. Additionally,
an annotation provides additional information about an
entity but is typically not fundamental to the entity;
therefore, we would not consider a title of a journal art-
icle to be an annotation of the article: Even though it
concisely describes an existing information content en-
tity, i.e., the body of the journal article, it is a canonically
required part of the article. Furthermore, an annotation
is typically either incorporated into the entity (e.g., in the
classical case of annotation of writing in the margins of
a book, which becomes a physical part of the book) or
can be otherwise retrieved along with the entity it isannotating (e.g., in the case of GO-term annotations of
database entries of proteins).
A subclass of Annotation could be defined for any
type of information content entity used to annotate another
entity (e.g., PartOfSpeechTagAnnotation, JavaKey-
wordAnnotation, GoTermAnnotation). However,
since we are motivated toward utility for the Semantic
Web, we introduce only two subclasses, RdfResour-
ceAnnotation and RdfGraphAnnotation, repre-
senting RDF resources and graphs, respectively, that are
used to annotate other information content entities. These
two subclasses should be all that is needed for the repre-
sentation of annotations in RDF stores, in which every-
thing should be an RDF resource or graph. Furthermore,
as long as information content entities used for annotation
are offered as RDF constructs (so that they can be used in
RDF stores), other annotation subclasses should not be
needed for their representation in RDF stores. For ex-
ample, since GO terms are also offered as RDF resources,
GO-term annotations can be stored as instances of
RdfResourceAnnotation, obviating the need for a
GoTermAnnotation class (unless there is further de-
sired axiomatization for GO-term annotations).
Resource annotations
In our model, a resource annotation is an annotation
that associates a single rdfs:Resource with a target.
A resource annotation is modeled as rdf:type kiao:
RdfResourceAnnotation. The relation iao:deno-
tes is used to associate a given annotation with the
concept being used to annotate the target. This property
relates an information content entity (in this case a re-
source annotation) to something to which it is specific-
ally intended to refer.
One of the primary types of text annotation is syntactic
annotation, which is often produced by text mining systems
(e.g., [11]). To demonstrate the applicability of our model to
syntactic annotation, we use a fragment of the example
sentence used by Liu et al. in their study of dependency
parsing for information extraction [12], i.e., the phrase
Interferons inhibit activation of STAT6. (For the purposes
of an example, we have taken some liberty in creating
example classes and relations that we believe are faithful to
the native dependency parse representations [13].)
Common tasks at the beginning of text mining pipelines
include tokenization and part-of-speech tagging [14].
Figure 1 depicts four resource annotations: ra1, ra2,
ra3, and ra4. The concepts in the object positions of the
denotes assertions are part of the domain model used
by the annotator and are not part of the proposed annota-
tion model itself. ra1 and ra2 denote specific instances
of tokens (represented here as instances of the class
Token), while ra3 and ra4 denote plural nouns and sin-
gular present-tense verbs, respectively (represented here
Figure 1 Example syntactic annotations. This figure depicts five
syntactic annotations as bold ovals with underlined labels: four
RdfResourceAnnotation instances, each with the prefix ra,
and one RdfGraphAnnotation instance, prefixed with ga.
Rectangles represent classes, while instances have rounded corners.
Double-lined arrows depict basedOn assertions. Thin gray arrows
are used to provide reference to the text, although their representation
is elided in this paper. The statements inside brackets are contained
within the corresponding RDF graph.
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 4 of 15
http://www.jbiomedsem.com/content/4/1/38by their Penn Treebank part-of-speech tags [15]). In this
example, the annotator made the domain-specific repre-
sentational choice to model the tokens as instances so that
they can be specifically referred to later by subsequent an-
notations, as will be shown in the next section. Abstract
relations connecting the resource annotations to text
spans are shown in Figure 1 as gray arrows, with gray
brackets representing the text spans. Existing models for
linking annotations to the object being annotated can be
used with our model, for example, the relations oa:has-
Target [3] or ao:context [4] could be used to model
these gray arrows. As our model is neutral relative to these
representational decisions, this aspect of modeling the
example annotations is elided from this document for
simplicity and clarity.
The following are RDF triples representing two of these
annotations, asserting that ra1 and ra3 are resource
annotations that denote a particular instance of a token
(represented here as t1) and plural nouns (represented
here as NNS), respectively.
Semantic annotations of text fragments, such as those
in the CRAFT Corpus [16], are another primary use case
for the model presented here. In Figure 2, the examplesentence fragment from Figure 1 has been annotated
with semantic classes in the manner of CRAFT annota-
tion. (In some cases, we have used ontologies and classes
not used in CRAFT in order to simplify the biology and
therefore the example.) The biomedical classes and
properties used to model the examples in this paper are
not part of the proposed annotation model. In Figure 2,
the three example resource annotations ra5, ra6, and
ra7 denote relevant biological concepts: ra5 denotes
interferons, a group of proteins represented here by
Interferon (IPR000471) in the InterPro database
of protein sequence signatures and families [17]; ra6 de-
notes the upregulation of biological processes, represented
here by positive regulation of biological
process (GO:0048518)a in the Gene Ontology [18]; and
ra7 denotes STAT6 proteins, represented here by STAT6
(PR:000001933) in the Protein Ontology [19]. The
following are RDF triples for two of these annotations,
specifically asserting that ra6 and ra7 are resource
annotations that denote positive regulation of biologi-
cal processes (represented here as GO:0048518) and
STAT6 proteins (represented here as PR:000001933),
respectively.Graph annotations
While a resource annotation relies on a single RDF
resource for annotation, a graph annotation is an RDF
graph, composed of a set of one or more RDF state-
ments, that is being used to annotate another informa-
tion content entity. A graph annotation is modeled as
rdf:type kiao:RdfGraphAnnotation. A graph
annotation is connected to a named graph of RDF state-
ments using the property iao:denotes. While a graph
annotation is directly linked to a named graph, it
actually denotes the content of the named graph (i.e., the
RDF graph that the named graph encodes or represents)
and not the named graph itself; this is consistent with
the semantics of named graphs proposed by Carroll
et al. [20], which states that any assertion in RDF about
the graph structure of a named graph is understood to
refer to the underlying RDF graph. As before, the nature
of the denoted knowledge representations (i.e., the set of
RDF statements) is left to the user, as our metamodel
focuses on the linkage of annotations to such represen-
tations and, as presented in the next section, the prov-
enance of compositional annotations.
Linguistic annotation is frequently done in a pipeline
where subsequent stages build upon the annotations pro-
duced by earlier stages. In addition to the aforementioned
Figure 2 Example biomedical semantic annotations. This figure depicts five semantic annotations as bold ovals with underlined labels: three
RdfResourceAnnotation instances and two RdfGraphAnnotation instances. (See the caption of Figure 1 for explanation of shapes
and arrows).
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 5 of 15
http://www.jbiomedsem.com/content/4/1/38resource annotations, Figures 1 and 2 depict several
RdfGraphAnnotation instances. In Figure 1, there is
one graph annotation that denotes the subject dependency
of token t2 on token t1, represented here as a graph
(g1) containing one RDF triple with subject t2, property
hasSubjectDependent, and object t1. The following
are triples/quads for graph annotation ga1:
Since the annotator made the domain-specific model-
ing choice to represent resource annotations ra1 and
ra2 as denoting instances of tokens, a dependency
assertion among them (i.e., that token t2 syntactically
depends on token t1 as the subject of the sentence, as
seen in Figure 1) was able to be created. If the annotator
had pointed these resource annotations directly to the
class Token (analogous to the direct pointing of re-
source annotations ra3 and ra4 to the classes NNS and
VBP, respectively), then it could only have been asserted
at the class level that Token hasSubjectDependent
Token rather than the assertions relating the specific
tokens. It is important to remember that this representation
of syntactic dependency is of our own choosing for this ex-
ample and that any user of our metamodel of annotationsis free to represent syntactic dependency (or any other
knowledge denoted by the annotations) as he chooses.
Semantic concept annotation, such as the manual
annotation performed on the CRAFT Corpus or annota-
tions created by text mining systems, can also be built in
layers. Figure 2 depicts two RdfGraphAnnotation in-
stances ga2 and ga3. The former denotes the positive
regulation of STAT6 protein, represented here as a
graph g2 containing a dynamically constructed subclass
P1 of the GO class representing positive regulation
(GO:0048518) in which STAT6 (PR:000001933) is
regulated. The graph ga3 builds upon the denoted
knowledge representation of ga2 and denotes the nega-
tive regulation of the positive regulation of STAT6 pro-
tein by an interferon, represented here as a dynamically
constructed subclass N1 of the GO class representing
negative regulation (GO:0048519) in which the regulat-
ing entity is an interferon (IPR000471) and the regu-
lated process is a positive regulation of STAT6 protein.
The following are triples/quads for graph annotation
ga2:
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 6 of 15
http://www.jbiomedsem.com/content/4/1/38The use of graphs has the advantage of separating the
representation of the annotation from the representation
of the denoted content and thus protects users of a
given annotation from committing to or believing the
propositions represented in the annotation unless
desired. For example, a given annotation could denote
the fact that STAT6 can bind calcium ions as one of its
functionalities, which could be represented as, e.g., one
RDF triple (STAT6 hasFunction CalciumIon-
Binding) in an RDF graph. Since this triple is placed in
its own graph, a reader of this annotation is not commit-
ted to believing that STAT6 can bind calcium ions; what
has been effectively represented is that this particular
annotation says that STAT6 can bind calcium ions. Just
as our model is agnostic with respect to the denoted
knowledge representations of annotations, we do not
seek here to explicitly represent confidence, trust, or
other epistemological or modal information; however,
such information could be modeled independently and
added orthogonally or compositionally to our proposed
model.Provenance of compositional annotations
As annotations become more complex, tracking their
provenance becomes increasingly important. Provenance
tracking is necessary in order to know which other an-
notations were used in constructing an annotation and
how their individual denoted representations were com-
posed into larger knowledge structures. Additionally,
provenance is needed for error analysis and blame attri-
bution. For example, referring to the example in Figure 2,
if ga2 is found to be incorrect only because it refers to
the wrong protein, the error and blame should be prop-
erly attributed to the author of ra7, as the latter was
the source of the incorrect protein identification. Like-
wise if ra7 is determined to be incorrect, annotations
dependent on that annotation could be identified and
retracted or updated as well.
Provenance information can be equally useful for dis-
ambiguation. Consider the case in which there are mul-
tiple competing annotations for the text STAT6 (e.g.,
those denoting human, mouse, and rat homologs of the
STAT6 protein, which are represented as distinct entities
in prominent biological repositories such as UniProt
[21] but are all canonically referred to as STAT6). If, as
in this example, there are multiple competing annota-
tions for the specific type of protein but only one is used
as the provenance for a larger annotation, such as ga2,
then this provenance can be tracked to resolve the ambi-
guity caused by the competing annotations. This is one
of the ways language understanding systems can
successfully resolve ambiguity [22]. An annotation model
such as ours that captures this information andprovenance can be used to document these choices and
facilitate error analysis.
In order to track provenance, we introduce the tran-
sitive relation kiao:basedOn, which is used to track
both coarse-grained, annotation-level provenance and fine-
grained, statement-element-level provenance in our model.
We propose that this relation holds between two
information content entities; therefore, the value of
rdfs:domain and rdfs:range for this relation is
iao:information content entity. Informally,
kiao:basedOn holds between subject and object in-
formation content entities when the subject entity has
been created relying in whole or in part on the already
existing object entity. In this proposal, we are interested in
making assertions of annotation provenance by recording
that specific annotations have been created wholly or
partly relying on other specific annotations. We make no
restriction on the cardinality of kiao:basedOn, so a
subject information content entity can be based on mul-
tiple object entities, and multiple subject information con-
tent entities can be based on the same object entity.
Annotation-level provenance
The simplest way to record provenance is to make
coarse-grained basedOn assertions between annota-
tions. A basedOn statement can be made between two
annotations either when there is a direct relationship be-
tween the annotations, such as one directly using one or
more elements of another, or when there is an indirect
relationship, such as one being used as the justification
for anothers existence even though no part is explicitly
shared (e.g., an annotation of a text span with a specific
protein class being used to justify an annotation of the
same text span with the top-level class protein
(PR:000000001) from the Protein Ontology).
Most syntactic dependency parsers use tokenization and
part-of-speech tags produced by other annotation systems
as input. Figure 1 depicts six different annotation-level
basedOn assertions between syntactic resource annota-
tions. Those from ra3 to ra1 and from ra4 to ra2 have
been asserted because ra3 and ra4, denoting parts of
speech, were created based on ra1 and ra2, denoting
tokenization, respectively. The following triples represent
the assertions of provenance among these four resource
annotations:
Note that these assertions are made even though there
are no direct relationships among the denoted tokens
and parts of speech, e.g., between the concepts denoted
by ra3 and ra1 (i.e., between plural nouns, represented
Figure 3 Example of statement-element provenance. This figure
depicts an example of statement-element-level provenance. The
RdfStatement and the three RdfStatementElement
instances have bold ovals and underlined labels. This figure is an
extension of Figure 1, and some of the parts of that figure have
been preserved here but grayed out. Dashed lines show assertions
that can be inferred. (See the caption of Figure 1 for explanation of
shapes and arrows).
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 7 of 15
http://www.jbiomedsem.com/content/4/1/38here by the part-of-speech tag NNS, and token t1); how-
ever, ra1 was instrumental in the creation of ra3, and
the basedOn statement documents this. The part of the
syntactic dependency parse annotated by the graph an-
notation ga1 used both the tokenization annotations
(ra1 and ra2) and the part-of-speech annotations (ra3
and ra4) to determine that token t1 is the subject of
token t2, and thus ga1 is modeled with a basedOn re-
lation to each of these four resource annotations. The
following four triples represent the annotation-level
provenance of graph annotation ga1:
Just as layers of annotation can build upon each other
in syntactic annotation, semantic annotation can also be
composed compositionally. Provenance relations are
analogously depicted among the semantic annotations in
Figure 2. Graph annotation ga2 was built using infor-
mation from resource annotations ra6 and ra7. Simi-
larly, the larger graph annotation ga3 records that it
was built using information from resource annotation
ra5 and from graph annotation ga2. The provenance
information can be traced from annotation to annota-
tion, and in this case one can see that ga3 is (partly)
based on ga2, which in turn is based on ra6 and ra7.
It is important to note that basedOn is general in that
it can be used to create an annotation-level assertion of
provenance from either a resource or graph annotation
to a set of any combination of resource and/or graph an-
notations. Instances of RdfGraphAnnotation need
not be strictly compositional; that is the statements in a
graph annotation do not need to be based on other an-
notations and can incorporate new information not yet
annotated elsewhere. For example, ga3 uses additional
information not explicitly previously annotated, which is
shown in Figure 2 by an additional gray arrow pointing
to a segment of the text not previously annotated.
Statement-element-level provenance
The second type of provenance represented in our
model records detail at a more fine-grained level. Refer-
ring back to Figure 2, while the assertion ga2 basedOn
ra7 is sufficient to model that at least some part of ga2
was based on ra7, it does not capture which elements
of ga2 are based on ra7. Analogously, in Figure 1, the
graph annotation ga1 documents that it is based on re-
source annotation ra1 but nothing more specific than
this. If the author of ga1 wishes to document that the
object element (which denotes token t1) of the RDFstatement of ga1 is based on ra1 (which also denotes
token t1), then recording provenance at the annotation
level is insufficient. In addition to documenting how
compositional annotations were constructed for under-
standing or training purposes, this type of provenance is
necessary to perform detailed error analysis.
In RDF, the typical way to make statements about state-
ments is to reify the statement itself as an instance of rdf:
Statement. An RDF statement identifies its subject,
property, and object via the relations rdf:subject, rdf:
property, and rdf:object, respectively. However,
RDF statements and their elements are conceptual repre-
sentations; for example, in Figure 1, the RDF statement t2
hasSubjectDependent t1 represents the assertion
that token t2 has as its subject token t1. To explicitly
represent RDF statements as information content entities,
we introduce the class kiao:RdfStatement, which
is rdfs:subClassOf iao:information content
entity. An example of a reified kiao:RdfStatement
is the instance s1 in Figure 3. A graph annotation can then
be connected to each reified statement of the graph annota-
tion using the property obo:has_part.
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 8 of 15
http://www.jbiomedsem.com/content/4/1/38In order to record the provenance about individual
parts of a statement, these parts must also be reified as
instances of kiao:RdfStatementElement. A re-
ified statement is linked to its component instances of
kiao:RdfStatementElement using three proper-
ties that mirror the properties used to reify the
rdf:Statement itself (i.e., rdf:subject, rdf:
property, and rdf:object): kiao:subject-
Element, kiao:propertyElement, and kiao:ob-
jectElement, each of which is rdfs:subProper
tyOf obo:has_part; that is, a reified statement has
these subject, property, and object elements as parts.c
Two reified instances of kiao:RdfStatementEl-
ement, se1 and se2, can be seen in Figure 3. The
corresponding iao:denotes assertions from these state-
ment elements to their denoted concepts (i.e., tokens t1
and t2, respectively) are also depicted.
With an assertion from a graph annotation to a state-
ment via obo:has_part and another assertion from the
statement to a statement element via kiao:subjectEl-
ement, kiao:propertyElement, or kiao:object-
Element (each a subproperty of obo:has_part), an
obo:has_part assertion from the graph annotation to
the statement element can also be inferred. The follow-
ing axiom holds for subject elements, and correspond-
ing axioms hold for property and object elements:
This is simply applying obo:has_part transitively.
Figure 3 shows two derived obo:has_part assertions:
Since graph annotation ga1 has_part statement s1,
and s1 is linked to its component statement elements
se1 and se2 via subjectElement and objectEl-
ement, respectively, it can be inferred that ga1 has
these statement elements as parts.
Now that the reified statement has been decom-
posed into and appropriately linked to reified
RdfStatementElement instances, the provenance
of these individual pieces can be recorded. In the ex-
ample in Figure 3, se1 is documented as being based
on resource annotation ra1, and se2 is documented
as being based on ra2. This is analogous to the use
of the basedOn among annotations, except in this
case the relation is being used among more fine-
grained components of the model.
Our original model [23] used a more complex set of
properties to record the same amount and type of infor-
mation. The model proposed in this paper simplifies the
representation of this information and the cognitive load
of using the model significantly. The following triplesrepresent statement s1 decomposed into statement ele-
ments se1, se2, and se3, along with the provenance of
se1, as rendered in Figure 3:The first two triples above represent that graph an-
notation ga1 has statement s1 as a part and that s1 is
an RDF statement, and the next six triples decompose
s1 into RdfStatementElement instances and spec-
ify their denotations. The relations subjectElement,
propertyElement, and objectElement have an
rdfs:domain of kiao:RdfStatement and an rdfs:
range of kiao:RdfStatementElement, and thus
type information can be inferred using RDFS reason-
ing, which we have omitted for conciseness. The sev-
enth and eighth triples reify the object position of
this statement, and the final triple documents that
this statement element is based on resource annota-
tion ra1. In this way, the annotator constructing
ga1 can explicitly document the origin of every com-
ponent piece.
Just as RdfStatementElement instances can be
based on instances of RdfResourceAnnotation,
they can also be based on other instances of Rdf-
StatementElement. As the composition of anno-
tations becomes more complex and the layers of anno-
tation get deeper, graph annotations will build on other
graph annotations. This is especially true for annotations
produced and used by computational language under-
standing systems [24]. For example, in Figure 4, state-
ment element se9 is based on statement element se6,
which is in turn based on resource annotation ra7.
Figure 4 only shows the provenance of statement elem-
ent se6 of statement s2 along with the provenance of
statement element se9 of statement s3. Although not
depicted, statement-element-level provenance could
analogously be recorded for all elements of these state-
ments, as well as for all statements of graph annotations
ga2 and ga3. The following are triples representing an-
notation information for statements s2 and s3 and
statement elements se6 and se9, including their proven-
ance, rendered in Figure 4:
Figure 4 Extended example of statement-element provenance. This figure depicts an extended example of statement-element-level
provenance. One RdfStatement from each graph and the six RdfStatementElement instances have bold ovals with underlined labels.
This figure is an extension of Figure 2, and some of the parts of that figure have been preserved here but grayed out. Dashed lines show
assertions that can be inferred. (See the caption of Figure 1 for explanation of shapes and arrows).
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 9 of 15
http://www.jbiomedsem.com/content/4/1/38The first group of eight triples and the ninth triple
above are exactly analogous to the triples used in the
previous example. As in the previous example, here
there is one reified statement (s2) that is part of a graph
annotation (ga2). Analogously, the reified statement s3
is a part of graph annotation ga3, as can be seen in the
third group of (eight) triples. However, in this example,
there is an extended statement-element-level assertion
of provenance: In the last triple, a statement element(se9) is asserted to be based on another statement
element (se6), which was already created to partly
document the provenance of a graph annotation (ga2).
In this way, the annotator creating graph annotation
ga3 can unambiguously document the specific element
of the specific statement of graph annotation ga2 from
which its reference to the protein STAT6 derives. This
low-level provenance is essential for understanding the
dependencies between layers of complex composi-
tional annotations and for being able to unwind these
layers to perform tasks such as error analysis and blame
attribution.
Discussion
Use cases
In this section, we discuss types of tasks that our anno-
tation model enables, along with specific examples of
such tasks, including projects on which we are working
as well as external efforts.
Integrating different types of annotations
A wide variety of annotation models and formats have
been created for a wide range of tasks; however, the
large majority of these are suited to one particular type
of annotation or task and are not broadly applicable or
interoperable. Our proposal is a generic metamodel of
annotations and their linkage to their denoted know-
ledge representations. As such, it is neutral with respect
to annotation template (i.e., the choice of terminologies,
ontologies, or schemas used for annotation), the nature
of the denoted knowledge representations created, and
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 10 of 15
http://www.jbiomedsem.com/content/4/1/38the methodology by which annotations and denoted
knowledge representations are created. As a result, our
model can be generically used to integrate different types
of annotations in a common representation, in turn
leading to enhanced interoperability and queryability
among these different types of annotations. Such inte-
gration is of substantial interest to us in the context of
our efforts with the Colorado Richly Annotated Full-
Text (CRAFT) Corpus, a collection of full-text biomed-
ical journal articles that we have extensively marked up
with a wide range of types of annotations, including
those specifying mentions of biomedical concepts,
coreference, discourse, as well as a variety of syntax,
including sentence segmentation, tokenization, part-of-
speech tagging, and Penn TreeBank tagging [16,25].
Relying on our generic metamodel, we are able to
represent these disparate types of annotations and their
denoted knowledge representations in a unified way.
This, in turn, is required to enable matching of different
types of annotations to various elements of formal
natural-language patterns for automated understanding
of biomedical text. This enables querying over multiple
annotation types simultaneously, for example looking for
all the noun-phrases that overlap with annotations to
specific ontology terms, which might be a query for
learning new vocabulary or patterns for identifying
ontology terms in text.
Text is not the only artifact for aggregating multiple
types of annotation. Tools such as the UCSC Genome
Browser which layer annotation tracks over a visualization
of the genome [26] demonstrate a clear need for integrat-
ing various types of genomic annotation, such as dbSNP
[27], COSMIC [28], OMIM [29], and the GWAS catalog
[30]. Use of our model would enable such annotations to
be more easily integrated into such tool via a standard rep-
resentation, and would further enable the integration of
annotations to be usable beyond the scope of specific tools
(e.g., visualization) for new queries yet to envisioned by
researchers.Connecting annotations to the Semantic Web
Many types of annotations are represented in ad hoc, idio-
syncratic formats (e.g., Penn Treebank [15] for parts of
speech, GAF 2.0 [31] for gene function, VCF [32] for
SNPs) that, in addition to hindering interoperability, are
obstacles to integration with the Semantic Web. We have
implemented our annotation metamodel as a formal
OWL ontology, and specific annotations are created as in-
stances of relevant OWL classes. Consequently, linkage of
these annotations and the data and knowledge they specify
to existing ontologies [10], other RDF repositories [33],
and the broader Semantic Web is considerably facilitated.
Using our model, reasoning over annotation structure andtheir denoted semantics simultaneously is also enabled
through the use of RDF- and OWL-based querying sys-
tems. For example, it is possible to query for annotations
that are based on an annotation from a specific source
that mention a subclass of a specific ontology term. Such
a query might be used if a specific annotator is known to
be highly accurate or inaccurate at a certain task. Such
queries cannot be easily performed, if they can be per-
formed at all, using combinations of tools on more idio-
syncratic formats.
Linking annotations to arbitrarily complex knowledge
representations
In most annotation efforts, each annotation consists of a
basic association of one conceptual resource (e.g., an
ontology class, schema element, database identifier) with
one target (e.g., document, text span, database entry).
However, as information needs increase and annotation
efforts expand to capture more complex information,
more complex knowledge structures will need to be repre-
sented. For example, although most GO annotations of
genes/gene products straightforwardly link GO terms to
database entries representing these genes/gene products,
there has been a call to associate the biological-functionality
annotations of the genes/gene products with more specific
contexts, such as the types of cellular locations or cells
in which these biological functionalities were observed
[31]. Similar calls for representing complex structures
in linguistic annotation are also being made, for ex-
ample representing semantic frames composed from
other annotations [34]. In our own work, the next layer
of annotation planned for the CRAFT corpus will also
require the ability to dynamically construct concepts
for use in assertional annotation [8]. To capture such
information, annotations must point to knowledge struc-
tures more complex than singular concepts. We have rep-
resented two fundamental types of annotations: resource
annotations, each of which points to a single RDF re-
source, and graph annotations, each of which points to an
RDF graph encapsulating one or more RDF statements.
Using our model, a user can create any combination of re-
source annotations and/or graph annotations, as moti-
vated by the complexity of information that is sought to
be captured in a given annotation effort.
Documenting the composition of annotations
As the annotations become more complex annotators
(both human and computational) need the ability to
refer to existing annotations as the basis of more com-
plex annotations. While ontologies and other vocabular-
ies used for annotation strive to be complete, it is likely
that specific applications will require dynamic construc-
tion of concepts, either through data-driven methods
[7,14]. or compositional concept formation [8].
Table 1 Counts of triples required to represent annotations
Annotation Annotation
triples
Annotation
provenance
triples
Statement
element
provenance
triples
Text
span
triples
ga1 2 (3) 4 9 (13) 16
ga2 3 (4) 2 16 (24) 16
ga3 6 (7) 3 32 (48) 30
Counts of triples required to represent annotations (and total counts including
inferable type triples), their statement-element-level provenance, and their
associated text spans.
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 11 of 15
http://www.jbiomedsem.com/content/4/1/38Natural language processing (NLP) pipelines are fre-
quently composed of sequences of components that each
produce output based on the output of earlier annotation
layers. For example, the output of a tokenizer might be fed
into a part-of-speech detector, and both of which are fed
into a named-entity recognizer, each component of which
is dependent on the output of some or all of the previous
components. There are generalized frameworks for building
annotation pipelines, such as UIMA [35,36]; however these
pipelines do not provide standardized models for docu-
menting the compositional provenance of annotations.
Other systems for language understanding, such as Direct
Memory Access Parsing systems [37,38] including Open-
DMAP [39] and REDMAP [24], use hierarchical patterns
to compose semantic annotations. These systems produce
knowledge structures that are analogous to those presented
in Figure 2; however, they have no standard methods for
documenting provenance. All of these use cases are covered
in a generalized way by our model, which enables the track-
ing of both coarse-grained, annotation-level provenance
and fine-grained, statement-element-level provenance.
Understanding the genetic basis of disease is a major
focus of current biological and bioinformatics research
and requires the integration of numerous types of anno-
tation. For example, epistasis captures interactions be-
tween genes that affect function and phenotype, and
compositional epistasis has been introduced [40] as a
way to model multiple genes affecting a phenotype. A
typical method for identifying epistasis starts with anno-
tations of SNPs (single-nucleotide polymorphisms) and
then applies a procedure for determining interactions
among them [41,42]; the inputs to such procedures are
SNP annotations and the outputs can be modeled as a
higher-order annotation over the genome sequence that
connects two or more of the SNP annotations. Captur-
ing the dependency of the epistasis relation on the prior
SNP annotation is important; SNP identification (variant
calling) is a process that is dependent on the initial se-
quencing and assembly, the reference genome, and other
factors and as such the SNP annotations of a genome
may vary with different analyses. Our model would en-
able identifying and distinguishing epistasis relationships
determined on the basis of one variant analysis from
those based on another analysis performed under differ-
ing conditions.
Analyzing annotation errors and attributing blame
A common use of provenance information is for error
analysis and blame attribution tasks. For example, if an
annotation is deemed incorrect, the method by which
that annotation was constructed needs to be investi-
gated. This often starts with identifying all the annota-
tions that contributed to its generation. It is possible
that a lower-level annotation is incorrect and its usealone led to the larger annotation being incorrect. In the
case of DMAP-style pattern recognition, this type of
analysis is critical both for debugging during develop-
ment as well as for analysis of results such as that done
in the evaluation of REDMAP [24]. For example, in that
evaluation it was important to identify whether errors
were due to named-entity recognizers improperly identi-
fying entities in the text or to larger patterns being im-
properly applied. Working in the opposite direction, if a
lower-level annotation is deemed incorrect, it is impor-
tant to identify all downstream annotations that are
based on that annotation so that they too can be identi-
fied as incorrect and retracted. (Please see later section
titled Querying Using the Model for examples of
SPARQL queries that extract this type of provenance in-
formation using our model.)Efficiency of the model
Modeling statement-element-level provenance comes
with the cost of reifying the statements and statement
elements. In the worst case, this cost is 10 triples per
statement in the graph annotation, or 14 triples if infer-
able type triples are explicitly represented as well: 7 tri-
ples are required to reify the statement, and 1 triple is
needed each for the subject, property, and object of the
statement to record its provenance. (However, in our ex-
perience, it is rare to record provenance for the prop-
erty.) For example, as rendered in Figure 4, ga1 requires
2 triples for the annotation (3 with type information), 4
triples to record annotation-level provenance, and 9 tri-
ples (13 with types) to record the statement-element-
level provenance. In contrast in the OA model [3] it
takes 7 triples per text span to anchor an annotation to
a piece of text. If the text has multiple spans there is an
additional 2-triple overhead. To connect ga1 to text in
the OA model requires 16 triples, 7 triples for each of
the two text spans plus 2 triples of overhead for having
multiple spans. Table 1 shows the number of triples re-
quired to model the example graph annotations used in
this document and their provenance. It also shows the
number of triples required to anchor these annotations
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 12 of 15
http://www.jbiomedsem.com/content/4/1/38to their corresponding spans of text using the OA
model. In terms of counts of RDF triples required, it can
be seen that recording statement-element-level prove-
nance is comparable to associating annotations with text.
We aimed in this work to lay down a low-level foun-
dation for annotation provenance, which can then serve
as the building blocks for higher-level models or
axiomatization. We acknowledge that the use of reifica-
tion to explicitly identify all of the low-level parts in
our model leads to the production of additional
triples. However, as more support for reasoning with ax-
iomatizations in triple stores becomes available, exten-
sions and abstractions of our model that reduce the
counts of triples required could be defined. For example,
if triples are reused from one graph to another, as is the
case for statements s2 and s3 in Figure 4, a relation
such as copyOf could be defined and used to directly
connect these statements so as to obviate the need to
reify and relate all of the corresponding RdfState-
mentElement instances from both statement triples.
As the model is applied in practice, other patterns may
emerge that point to additional optimizations or refine-
ments. Mappings could also be constructed from our
model to nanopublications [43,44] or RDF Molecules [45]
to potentially reduce the number of redundant triples.
Querying using the model
Due to the manner in which the model was integrated with
the Relation Ontology (RO) and the Information Artifact
Ontology (IAO), querying of the model can be quite
straightforward in SPARQL. A common provenance-
tracking task might be to identify all the annotations that
are based on a given annotation that is suspected of being
incorrect. For example, the following SPARQL 1.1 query
returns all of the annotations that are based on ra7:
Similarly, a researcher may be interested in all of the
statements that are based on a given annotation. If these
RdfGraphAnnotation instances have had their
statement-element-level provenance recorded using our
model, such statements could be queried for directly. Forexample, the following SPARQL 1.1 query would retrieve
all statements each of which has at least one statement
element that is based on resource annotation ra7:
The first three namespaces are needed as part of the
annotation model, and the last three are specific to the
example domain and only used for more conveniently
rendering the results. While this query and the one be-
fore are modeled using SPARQL 1.1 property paths,
there is nothing in our model that requires them for
querying. For example, using SPARQL 1.0, the property
paths could be expanded using blank nodes or variables
that are not captured in the results.
Related work
Efforts in the representation of more structured anno-
tations have tended to be idiosyncratic, specific to a
particular type of annotation or task, and not broadly
interoperable. For example, for the task of Gene Ontology
(GO) annotation, in which the functionalities of genes and
gene products represented in biomedical databases are as-
sociated to GO terms [46], the Gene Association File
format (GAF 2.0) [31] enables the representation of con-
straints on the context in which a given annotation might
be valid (e.g., the type of cell in which the functionality is
asserted to be present); however, this format is specific to
this narrow task. Analogously, the corpus and computa-
tional linguistics communities have developed solutions
for representing complex syntax and semantics for docu-
ments, e.g., the Penn Treebank format [15], but these
representations are mostly idiosyncratic and not interoper-
able. The Linguistic Annotation Framework (LAF) [34],
along with its XML-based serialization GrAF [47] and
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 13 of 15
http://www.jbiomedsem.com/content/4/1/38the RDF-based representation DADA [48], allow for the
markup of a wider range of linguistic phenomena, but they
only permit the specification of functional (single-valued)
properties. POWLA [49] is another linguistic corpora an-
notation formalism based in RDF and OWL, but, like
LAF/GrAF, it introduces an ambiguity by using the same
identifier to anchor information about both the annotation
and what it denotes; hence, the formalism cannot clearly
capture which information applies to the annotation and
which applies to the denoted knowledge [50].
Several prominent efforts have focused on general rep-
resentations of annotation, notably the Open Annotation
(OA) model [3] and the Annotation Ontology (AO) [4].
These models represent three basic pieces of informa-
tion for a given annotation: the thing being annotated
(e.g., a span of text), the denoted knowledge representa-
tion (i.e., the concept or set of assertions denoted by the
annotation), and the annotation itself, which connects
the other two. These three things can be broadly aligned
across the two models as well as with our model for
the linkage of annotations to their denoted knowledge
representations. A proposed integration of our model with
the Open Annotation model can be found in Additional
file 1 and an integration with the Annotation Ontology
model in Additional file 2. In this paper, we have elided
discussion of metadata such as author and creation date
as well the connection of annotations to their respective
targets, and our annotation model makes no constraints
or requirements as to how these pieces of information are
represented. The relations used by the Open Annotation
model and the Annotation Ontology would both work
well, and for most of such metadata the two models
largely capture the same details. Though constructs that
can denote complex knowledge structures have very re-
cently been added to these models, there have been no
mechanisms put forth by which complex annotations can
be composed of more atomic annotations with their prov-
enance unambiguously recorded.
Both the OA and AO annotation models seem to sup-
port one annotation pointing to multiple targets. However,
it is ambiguous as to whether the annotation applies
equally and independently to each target (e.g., as for an an-
notation targeting the text spans of multiple mentions of
STAT6 in a piece of text with the corresponding Protein
Ontology class (PR:000001933)) or if it is the union of
the targets that is being annotated (e.g., as for an annota-
tion targeting the two discontinuous spans of text c-ter-
minal and tails from the phrase c-terminal cytoplasmic
tails with the Sequence Ontology class c_terminal_
region (SO:0100015) [51]). We strongly assert that an
annotation with multiple targets should be interpreted as
a single discontinuous annotation and that the alternate
shared-annotation interpretation should be disallowed by
all models. On its surface, the shared-annotationinterpretation seems to be beneficial in that it saves triples
and seems easier to create. However, it muddles informa-
tion represented for the purposes of provenance tracking
or error analysis; for example, if three out of four of the
shared targets for an annotation are correct, but the fourth
target is incorrect, this information could not be accu-
rately captured. Furthermore, in the case of compositional
annotations, it could not be clearly represented which of
the shared annotations and targets are connected via
basedOn links and which are not. When considering in-
creasingly complex annotations and how annotations will
be used by downstream applications and models, it is clear
that one annotation to one Annotation instance is the
only lossless approach.
Numerous other models of triple-level provenance also
exist, for example, PaCE [52] and RDF coloring [53], but
these models require more complicated URI-minting pro-
cedures and systems that can understand the composi-
tional URIs they produce. The most related triple-
provenance model is that for nanopublications [43,44],
which is compatible with our GraphAnnotation model
in that it provides a method for recording triple-level
provenance and annotating sets of triples with metadata.
However, the primary purpose of nanopublications is to
enable attribution and validation of scientific statements,
and as such it does not model resource annotations, tar-
geting annotations to other content such as text, or fine-
grained statement-element-level provenance. Our approach
is complementary to microattribution proposals to attribute
data such as disease-implicated genetic variants to the sci-
entists who determine them [54].
Also related to our research is work being done by the
scientific workflow provenance community. Proof Markup
Language [55] models the justifications of reasoning results
from Semantic Web services, while work such as Provair
[5] aims to document work-flow provenance. Trust and
authenticity are also active areas of research [56]. Through
provenance workshops [57] and challenge meetings [58],
the Open Provenance Model (OPM) [59] has been devel-
oped. Other community efforts have led to the creation of
the PROV [60] model, which provides a data model for
building representations of the entities, people and pro-
cesses involved in producing a piece of data or thing. A
proposed integration of our provenance relations with the
object-centric portion of PROV-O (the OWL-ontology
version of PROV) [61] is provided in Additional file 4.
Conclusions
We have presented a model for representing compositional
annotations and annotation provenance, and provided ex-
amples of application areas for the model. The model can
be used to link annotations to their denoted knowledge
representations, and we have divided the annotation space
into resource annotations, in which RDF resources are used
Livingston et al. Journal of Biomedical Semantics 2013, 4:38 Page 14 of 15
http://www.jbiomedsem.com/content/4/1/38to annotate targets, and graph annotations, in which graphs
composed of one or more RDF triples are used. With this
model, progressively more complex annotations can be
composed from other annotations, and this provenance can
be unambiguously represented at either a coarse- or fine-
grained level. We have designed our annotation model to
be generic so as to facilitate the concurrent use of multiple
types of annotations. Additionally, it allows for the creation
of arbitrarily complex annotations, both in terms of their
denoted knowledge and of any other annotations upon
which they rely. All of this information can be losslessly re-
corded, thus facilitating inference and error tracking in
large computational annotation efforts. We have provided
an OWL representation of our model integrated with the
Information Artifact Ontology, as well as proposed integra-
tions with the Open Annotation model, the Annotation
Ontology, and the PROV Ontology.
Endnotes
aAlthough activation (mentioned in the example sen-
tence fragment in Figure 2) is semantically narrower
than positive regulation, we use the GO class posi-
tive regulation of biological process here
for simplicity, as there is no more specific subclass in
the GO that generically represents the activation of a
biological process. Similarly, inhibition is semantically
narrower than the GO class negative regulation
of biological process.
b The OBO Relation Ontology, upon which the ontol-
ogies of the OBO library rely, uses the obo: namespace.
We extend the Relation Ontology using the namespace
kro:.
c While relations are typically named as verbs or verb
phrases, we modeled these relation names to be analo-
gous to the core RDF statement model.
Additional files
Additional file 1: Appendix A. Alignment with the Open Annotation
Model. Appendix describing an alignment from the proposed model to
the Open Annotation model.
Additional file 2: Appendix B. Alignment with the Annotation
Ontology. Appendix describing an alignment from the proposed model
to the Annotation Ontology model.
Additional file 3: OWL file containing proposed model as an
extension of the IAO.
Additional file 4: Appendix C. Alignment with the PROV Ontology.
Appendix describing an alignment from the proposed model to the
PROV OWL model.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
KV and KML conceived the project. KML and MB were the primary
developers of the proposed models. KV contributed to the text-mining use
cases and to the alignment with existing annotation models. LEHcontributed to the alignment with IAO and other OBO efforts. KML, MB, and
KV contributed to the writing of the manuscript. All authors reviewed and
approved the work.Acknowledgements
This work was supported by Award No. 2011- 0204805 from the Andrew W.
Mellon Foundation to KV. Additional support came from NIH grant 3 T15
LM00945103S1 to KML. KV receives funding through NICTA, which is funded
by the Australian Government as represented by the Department of
Broadband, Communications and the Digital Economy and the Australian
Research Council through the ICT Centre of Excellence program.
Author details
1Department of Pharmacology, University of Colorado Anschutz Medical
Campus, Aurora, CO, USA. 2National ICT Australia, Victoria Research
Laboratory, Melbourne 3010 VIC, Australia. 3Department of Computing and
Information Systems, The University of Melbourne, Melbourne 3010 VIC,
Australia.
Received: 12 April 2013 Accepted: 20 September 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Smith et al. Journal of Biomedical Semantics 2013, 4:26
http://www.jbiomedsem.com/content/4/1/26RESEARCH Open AccessThe clinical measurement, measurement method
and experimental condition ontologies:
expansion, improvements and new applications
Jennifer R Smith1, Carissa A Park2, Rajni Nigam1, Stanley JF Laulederkind1, G Thomas Hayman1, Shur-Jen Wang1,
Timothy F Lowry1, Victoria Petri1, Jeff De Pons1, Marek Tutaj1, Weisong Liu1, Elizabeth A Worthey1,3,
Mary Shimoyama1,4* and Melinda R Dwinell1,5Abstract
Background: The Clinical Measurement Ontology (CMO), Measurement Method Ontology (MMO), and
Experimental Condition Ontology (XCO) were originally developed at the Rat Genome Database (RGD) to
standardize quantitative rat phenotype data in order to integrate results from multiple studies into the PhenoMiner
database and data mining tool. These ontologies provide the framework for presenting what was measured, how it
was measured, and under what conditions it was measured.
Results: There has been a continuing expansion of subdomains in each ontology with a parallel 23 fold increase
in the total number of terms, substantially increasing the size and improving the scope of the ontologies. The
proportion of terms with textual definitions has increased from ~60% to over 80% with greater synchronization of
format and content throughout the three ontologies. Representation of definition source Uniform Resource
Identifiers (URI) has been standardized, including the removal of all non-URI characters, and systematic versioning of
all ontology files has been implemented. The continued expansion and success of these ontologies has facilitated
the integration of more than 60,000 records into the RGD PhenoMiner database. In addition, new applications of
these ontologies, such as annotation of Quantitative Trait Loci (QTL), have been added at the sites actively using
them, including RGD and the Animal QTL Database.
Conclusions: The improvements to these three ontologies have been substantial, and development is ongoing.
New terms and expansions to the ontologies continue to be added as a result of active curation efforts at RGD and
the Animal QTL database. Use of these vocabularies to standardize data representation for quantitative phenotypes
and quantitative trait loci across databases for multiple species has demonstrated their utility for integrating diverse
data types from multiple sources. These ontologies are freely available for download and use from the NCBO
BioPortal website at http://bioportal.bioontology.org/ontologies/1583 (CMO), http://bioportal.bioontology.org/
ontologies/1584 (MMO), and http://bioportal.bioontology.org/ontologies/1585 (XCO), or from the RGD ftp site at
ftp://rgd.mcw.edu/pub/ontology/.* Correspondence: shimoyama@mcw.edu
1Human and Molecular Genetics Center, Medical College of Wisconsin,
Milwaukee, WI, USA
4Department of Surgery, Medical College of Wisconsin, Milwaukee, WI, USA
Full list of author information is available at the end of the article
© 2013 Smith et al.; licensee BioMed Central Ltd. This is an open access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Smith et al. Journal of Biomedical Semantics 2013, 4:26 Page 2 of 12
http://www.jbiomedsem.com/content/4/1/26Background
Integrating phenotype data from multiple experiments
and sources is challenging because of the general lack of
standardization in how such data is presented. The
Clinical Measurement (CMO), Measurement Method
(MMO), and Experimental Condition (XCO) Ontologies
were developed at the Rat Genome Database (RGD)
[1-3] to meet this challenge [4]. The CMO, MMO, and
XCO constitute a suite of ontologies designed to provide
detailed descriptions of specific, quantitative phenotype
data and the experiments that produced them by indi-
cating (1) what was measured, (2) how it was measured,
and (3) under what conditions it was measured. Along
with the Rat Strain Ontology, these form the basis of the
RGD PhenoMiner tool for mining and visualizing quan-
titative phenotype data [5].
Because the ontologies were designed to work to-
gether, their development was originally, and continues
to be, coordinated. They were first used to integrate and
standardize high-throughput rat phenotype data from
the PhysGen Programs for Genomic Applications (PGA)
[6,7] and the National BioResource Project for the Rat in
Kyoto, Japan (NBRP) [8], and by the COVER project at
Washington University in St. Louis in the integration of
human cardiovascular phenotype data [4]. The success
of these efforts has prompted further development of
these ontologies, resulting in expansions of their size,
their scope, and their usage. This paper will present
details regarding these improvements to the ontologies
and information about applications of the ontologies
which have recently been implemented.
Results and discussion
Increases in the size and scope of the clinical
measurement ontology
As stated when this ontology was originally released, the
Clinical Measurement Ontology was primarily orga-
nized on the highest level according to the body system
in which the measurement is made [4]. This is still theTable 1 Comparison of ontology statistics between 2012 and
Clinical measurement
2012 2013
Total # terms 523 1691
Defined terms 328 1427
Percent defined 62% 84%
Maximum depth 7 11
% terms with 2 or more parents 2.0% 15.8%
% terms with single subclass 10.3% 15.6%
Average branching factor 0.98 0.86
Table 1 displays the total number of terms for each ontology as well as the numbe
original publication in May 2012 and as of July/August 2013. In addition, basic stati
degree of branching for each are included.case. However, both the size and the scope of the ontol-
ogy have substantially increased [9]. Between 2012 and
2013, the number of terms has grown from a total of
523 to 1691, the maximum depth of the ontology has
now increased to 11, the percentage of classes with a
single subclass is 15.6%, and the average branching
factor is 0.86 (Table 1). This table also shows that the
percentage of classes with two or more parents has in-
creased to 15.8%. Although it is a common practice to
limit the number of asserted parents to a single one for
each ontology term, the applications for which these
ontologies were designed are largely geared toward
physiological and clinical researchers. As such and in
keeping with our decision from the beginning to use a
pragmatic approach to the design of these ontologies, in
cases where it seems clear that a researcher would
expect to find a relationship between terms it is our
practice to assert that parentage. Also, as will be
discussed later, we have found that some groups have
begun to extract only a small subset of terms from one
of these ontologies, to use according to their needs. This
practice is facilitated by the assertion of parentage rather
than limiting those assertions to a single parent and
trusting semantic reasoners to supply the missing rela-
tionships. For these applications, artificially limiting the
assertions of parentage or conforming to a formalized
ontology design pattern (ODP) or structured upper level
ontology such as the Basic Formal Ontology (BFO),
while perhaps improving the ontologys logical structure,
renders the ontology opaque to many researchers
attempts to browse the vocabulary to find the term(s)
they need. In this respect, as Lord and Stevens com-
mented, while realist principles may enable straight-
forward modelling for some topics, there are crucial
aspects of science and the phenomena it studies that do
not fit into this approach; realism appears to be over-
simplistic which, perversely, results in overly complex
ontological models [10]. The structures of the ontolo-
gies are therefore based on their contents and the2013
Measurement method Experimental condition
2012 2013 2012 2013
195 402 110 346
116 326 76 320
59% 81% 69% 92%
6 8 5 8
<1% 7.2% 1.0% 14.2%
7.7% 14.9% 10.9% 17.1%
1.00 0.93 0.99 0.87
r and percentage of those terms with textual definitions, at the time of the
stics such as the maximum depth of each ontology and information about the
Smith et al. Journal of Biomedical Semantics 2013, 4:26 Page 3 of 12
http://www.jbiomedsem.com/content/4/1/26established organizational hierarchies understood by the
research communities that are using the vocabularies.
As shown in Table 2 and Figure 1A, the scope has
expanded from 13 direct subclasses of the term clinical
measurement to 26. The new branches include coverage
for additional organ systems (alimentary/gastrointestinal
system, endocrine/exocrine system, immune system, mus-
culoskeletal system, nervous system, and skin) as well as
coverage for measurements that do not necessarily come
to mind as phenotypes. These include branches related
to disease population measurements such as incidence
and prevalence, to disease processes such as onset/diagno-
sis and progression, and to mortality and survival. In these
cases, quantitative values are commonly assessed and
reported in the literature. For example, researchers will
give a number for the percentage of a study population
that develop a disease within a given period of time, report
the age at which a disease state is detected, or track theTable 2 Expansion of the scope of the clinical
measurement ontology
Original branches
of the CMO:
New branches added to the CMO:
Organ
systems:
Cardiovascular
system
Alimentary/gastrointestinal system
Liver/biliary system Endocrine/exocrine system
Renal/urinary
system
Musculoskeletal system
Reproduction Nervous system
Respiratory system Skin
Immune system
Other
branches:
Blood measurement Body movement measurement
Body morphological
measurement
Chemical response/sensitivity
measurement
Body temperature Disease population measurement
(incidence/prevalence)
Cell measurement Disease process measurement
(onset/diagnosis, progression, severity)
Consumption
measurement
Exudate measurement
Growth
measurement
Mortality/survival measurement
Tissue composition
measurement
Organ measurement
Tumor
measurement
Table 2 lists all of the direct children of the parent CMO term clinical
measurement as of July 2013, v2.5. These are divided into the list of terms
that existed at the time of the original publication of the ontology on the left
and those which have subsequently been added to the ontology on the right.
Increases in the scope of the CMO have almost doubled the number of direct
subclasses of the parent.proportion of animals in a study population which are
surviving at a series of time points.
A branch for chemical response and sensitivity mea-
surements which covers the results of a variety of both
in vivo and ex vivo drug and chemical tests was added,
as was a branch for exudate measurements for use
with measurements made on extravasated fluid or other
substances.
The term originally labeled as organ measurement
(CMO:0000068) was changed to organ morphological
measurement in keeping with its placement under
body morphological measurement and above terms
which described only organ morphology. A new term for
organ measurement (CMO:0000669) was created
directly under the root term clinical measurement and
linked as a parent to organ morphological measure-
ment (CMO:0000068) (Figure 1B). These changes have
allowed inclusion of physiological measurements related
to the specified organs in addition to their correspond-
ing morphological measurements.
A new branch for body movement measurement
addresses a common area of study in rodent research
that was not covered in the earlier version of the CMO.
This branch is designed to include both involuntary
movements, such as measurement of the acoustic startle
response (CMO:0001519), and voluntary movements. In
the rodent research literature, measurements of volun-
tary movements such as locomotor behavior in an open
field apparatus, rearing, or freezing are often presented
as measurements of the emotional state of the animal
(e.g., anxiety [11]). Although this is a common inter-
pretation of the results, the psychological state is not
the actual quantity being measured. Additionally, such
movement measurements can be used in other con-
texts. A cursory search of the rat literature resulted
in articles in which movement in an open field appa-
ratus was used to assess learning/memory [12],
ethanol-related hyperactivity [13], the sedative effects
of drug treatments [14], the locomotor effects of ves-
tibular dysfunction [15], and the effects of cholinergic
denervation of the hippocampus [16]. This being the case,
the branch was developed as representing measurements
of movement in general, not of psychology or emotional-
ity. Also, because the same measurements are made across
a number of different types of apparatus, the specifics of
the apparatus are assigned via the MMO rather than being
included in the CMO terms.
Collaboration with the Animal QTL Database (QTLdb)
[17,18] has led to the addition of a substantial number of
CMO terms related to agricultural animal assessments.
These include terms for measurements commonly used
by the agricultural community to assess the composition
and yield of milk for cattle and sheep, as well as measure-
ments of fowl eggs, of fat and muscle morphology and fat
2012: 13 direct 
children of "clinical 
measurement"
2013: 26 direct 
children of "clinical 
measurement"
A
B
Figure 1 The clinical measurement ontology 2012 vs. 2013. A. Additions and improvements to the CMO have resulted in an expansion of
both the number of terms and the scope of the ontology. In May of 2012, there were 13 direct child terms under the root clinical
measurement. As of July 2013, this had increased to 26. The vertical arrows point to the level in the display which corresponds to the vocabulary
nodes directly under the root. B. Adjustments to the branch for body morphological measurement and addition of a new branch for organ
measurement clarified the morphological terms and allowed for addition of organ-specific physiological terms.
Smith et al. Journal of Biomedical Semantics 2013, 4:26 Page 4 of 12
http://www.jbiomedsem.com/content/4/1/26composition in cattle and pigs, and of feed intake and
weight gain in cattle, pigs, sheep, and chickens.
Increases in the size and scope of the measurement
method ontology
The Measurement Method Ontology covers the domain
of the specific methods used to make the measurements
represented by the CMO terms, i.e., how it was mea-
sured. This being the case, development of this ontol-
ogy is closely coordinated with the development of the
CMO and it has likewise increased in size and scope
[19]. The number of terms in the MMO has increased
from 195 to 402, the maximum depth of the ontology
has increased to 8, the percentage of classes with two or
more parents has risen to 7.2%, the percentage of classeswith a single subclass is 14.9%, and the average
branching factor is 0.93 (Table 1). The MMO is sub-
divided into two major branches: in vivo method for
methods performed in or on a living body, and ex vivo
method for procedures performed outside the living
body. Improvements to the in vivo branch include the
addition of terms for body movement methods, such as
subbranches for types of test enclosures, mazes, and
treadmills, and addition of more general branches for
flowmetry and body fluid collection method. New
subbranches under ex vivo method include radioacti-
vity and volume measurement methods, as well as a
branch for isolated cell method which corresponds to
expansion of the CMO cell measurement branch. In
several cases, what was originally a single term has been
Smith et al. Journal of Biomedical Semantics 2013, 4:26 Page 5 of 12
http://www.jbiomedsem.com/content/4/1/26expanded into a larger branch. For instance gel electro-
phoresis, originally a direct child of ex vivo method,
now appears within the molecular separation method
branch of the MMO (Figure 2).
Increases in the size and scope of the experimental
condition ontology
Because incorporation of data from new areas of re-
search requires the addition of new condition terms, the
Experimental Condition Ontology has expanded from
110 to 346 terms, the maximum depth of the ontology
has now increased to 8, the percentage of classes with
two or more parents is 14.2%, the percentage of classes
with a single subclass is 17.1% and the average bran-
ching factor is 0.87 (Table 1) [20]. New branches under
experimental condition include controlled visible light
condition, controlled in situ organ condition and
pathogen. New terms include sample resting period,
which was necessitated by experiments in which sepa-
rate measurements were made on a sample before and
after the sample was allowed to sit for a specified period
of time. The only difference in the conditions between
the two values was the sample resting period. In
addition, a term for perfusate was added within the
more general solution branch to describe experiments
performed on isolated organs. The terms surgical im-
plantation and surgical removal were moved under the
new surgical manipulation term, and fasting was incor-
porated into the existing diet branch.
The most extensive additions to the XCO were made in
the existing chemical branch. Originally, the branchFigure 2 The measurement method ontology 2012 vs. 2013. Addition
molecular separation method branch under ex vivo method. The term 
moved from being a direct child of ex vivo method into the new branchincluded four subclasses: anesthetic, neoplasm inducing
agent, polycyclic arene, and steroid. As the number of
subclasses increased (at one point reaching 25 direct
children of chemical) it became clear that a better
organizational strategy was needed. Table 3 compares the
original children of chemical with the current structure
of the branch. Following the lead of the Chemical Entities
of Biological Interest (ChEBI) ontology [21,22], the branch
was split into two major subbranches: chemical with
specified function and chemical with specified struc-
ture. Classes representing functional roles such as
disease inducing chemical and neurotransmitter have
been moved under the former term. Those representing
structural classifications such as hydrocarbon and sul-
fonamide are now found under the latter. This structure
facilitates browsing for researchers. In this way, whether a
chemist is looking for a nitrosourea or a physiologist is
looking for a mutation inducing agent both will find the
commonly used mutagen N-ethyl-N-nitrosourea (ENU)
where they would intuitively expect it to be. Note that the
term polycyclic arene, previously a direct child of che-
mical, has been moved to the appropriate level within the
more general hydrocarbon subbranch. Where possible,
the corresponding ChEBI ID is given as a cross-reference
for the term in the XCO.
Consideration was given to simply using the ChEBI
ontology for chemical conditions. However, ChEBI is
fundamentally an ontology of chemical structures. We
would argue that the concept of the use of a chemical as
an experimental condition is qualitatively different than
that of a chemical as a structure or molecule. Inof new terms such as chromatography necessitated the creation of a
gel electrophoresis, as a type of molecular separation method, was
.
Table 3 Expansion of the chemical branch of the
experimental condition ontology
Original subclasses of
chemical in the XCO
Current organization of terms under
chemical in the XCO
Anesthetic Chemical with specified function
Neoplasm inducing agent Activator
Polycyclic arene Anesthetic
Steroid Antigen
Antioxidant
Buffer
Disease inducing chemical
Diuretic
Enzyme substrate
Hormone
Indicator
Inhibitor
Mutation inducing agent
Neoplasm inducing agent
Neurotransmitter
Toxic substance
Vasoactive chemical
Chemical with specified structure
Alcohol
Amino acid
Carbohydrate
Chemical nanoparticle
Hydrocarbon
Ion/salt
Labeled chemical
Nitrosourea
Nucleic acid
Peptide/protein
Steroid
Sulfonamide
Table 3 highlights the expansion and reorganization of the XCO branch under
the subclass chemical. The original four subclasses are shown on the left. As
of August 2013, v3.0, the branch has been divided into two major
subcategories: chemical with specified function and chemical with specified
structure and the terms which were previously direct children of chemical
have been moved under one of these two. In addition to more closely
following the familiar structure of the ChEBI ontology, the new organization
facilitates browsing.
Smith et al. Journal of Biomedical Semantics 2013, 4:26 Page 6 of 12
http://www.jbiomedsem.com/content/4/1/26addition, ChEBI is often used in annotation of molecular
level gene-chemical interactions which differs from an
annotation of a chemical bolus or solution being adminis-
tered as an experimental stressor. The decision was there-
fore made to include terms for chemical conditions in the
XCO and express the relationship between such a condi-
tion and the structure and role of the referenced chemical
JOURNAL OF
BIOMEDICAL SEMANTICS
Roncaglia et al. Journal of Biomedical Semantics 2013, 4:20
http://www.jbiomedsem.com/content/4/1/20DATABASE Open AccessThe Gene Ontology (GO) Cellular Component
Ontology: integration with SAO (Subcellular
Anatomy Ontology) and other recent
developments
Paola Roncaglia1,2*, Maryann E Martone3, David P Hill2,4, Tanya Z Berardini2,5, Rebecca E Foulger1,2, Fahim T Imam3,
Harold Drabkin2,4,7, Christopher J Mungall2,6 and Jane Lomax1,2Abstract
Background: The Gene Ontology (GO) (http://www.geneontology.org/) contains a set of terms for describing
the activity and actions of gene products across all kingdoms of life. Each of these activities is executed in a
location within a cell or in the vicinity of a cell. In order to capture this context, the GO includes a sub-ontology
called the Cellular Component (CC) ontology (GO-CCO). The primary use of this ontology is for GO annotation,
but it has also been used for phenotype annotation, and for the annotation of images. Another ontology with
similar scope to the GO-CCO is the Subcellular Anatomy Ontology (SAO), part of the Neuroscience Information
Framework Standard (NIFSTD) suite of ontologies. The SAO also covers cell components, but in the domain
of neuroscience.
Description: Recently, the GO-CCO was enriched in content and links to the Biological Process and Molecular
Function branches of GO as well as to other ontologies. This was achieved in several ways. We carried out an
amalgamation of SAO terms with GO-CCO ones; as a result, nearly 100 new neuroscience-related terms were
added to the GO. The GO-CCO also contains relationships to GO Biological Process and Molecular Function
terms, as well as connecting to external ontologies such as the Cell Ontology (CL). Terms representing protein
complexes in the Protein Ontology (PRO) reference GO-CCO terms for their species-generic counterparts.
GO-CCO terms can also be used to search a variety of databases.
Conclusions: In this publication we provide an overview of the GO-CCO, its overall design, and some recent
extensions that make use of additional spatial information. One of the most recent developments of the GO-CCO
was the merging in of the SAO, resulting in a single unified ontology designed to serve the needs of GO
annotators as well as the specific needs of the neuroscience community.
Keywords: Gene ontology, Cellular component ontology, Subcellular anatomy ontology, Neuroscience,
Annotation, Ontology language, Ontology integration, Neuroscience information framework* Correspondence: paola@ebi.ac.uk
1European Molecular Biology Laboratory, European Bioinformatics Institute
(EMBL-EBI), Wellcome Trust Genome Campus, Hinxton CB10 1SD, UK
2The Gene Ontology Consortium, European Molecular Biology Laboratory,
European Bioinformatics Institute (EMBL-EBI), Wellcome Trust Genome
Campus, Hinxton CB10 1SD, UK
Full list of author information is available at the end of the article
© 2013 Roncaglia et al.; licensee BioMed Central Ltd. This is an open access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Roncaglia et al. Journal of Biomedical Semantics 2013, 4:20 Page 2 of 11
http://www.jbiomedsem.com/content/4/1/20Background
The Gene Ontology (GO) [1,2] contains a set of terms
for describing the activity and actions of gene products
across all kingdoms of life. Each of these activities is ex-
ecuted in a cellular location or a location outside in the
vicinity of a cell. In order to capture this context, the GO
includes, since its inception, a sub-ontology called the Cel-
lular Component Ontology (GO-CCO). GO-CCO terms
describe parts of cells and structures associated with cells
throughout the taxonomy range. The primary use of this
ontology is for GO annotation, but it has also been used
for phenotype annotation. Another ontology with a similar
scope to the GO-CCO is the Subcellular Anatomy Ontol-
ogy (SAO) [3], part of the Neuroscience Information
Framework Standard (NIFSTD) [4] suite of ontologies.
The SAO covers cellular components in the domain of
neuroscience and was designed as a model for describ-
ing relationships among subcellular structures that
would be encountered in an electron micrograph, for
example a neuropil. In the nervous system, there are
numerous examples of named subcellular structures
that are composed of parts of multiple cell types, e.g.,
synapses, the Node of Ranvier, the glia limitans. SAO
thus has a richer set of spatial relationships than the
GO, modeled in part after the Foundational Model of
Anatomy (FMA) [5].
At the time the SAO was constructed, circa 20052006,
tools for import and reuse of existing ontologies were lim-
ited; in addition the SAO was composed in OWL (Web
Ontology Language), while the GO-CCO was in OBO
(Open Biomedical Ontologies) format. At this time, the se-
mantics of OBO format were not yet aligned with those of
OWL. Thus, the SAO had developed an independent set
of cell component terms, with a heavy focus on those en-
countered in the nervous system. More recently, with the
advent of a more detailed specification of OBO format
(which clarifies the semantics of OBO format as a subset
of OWL2) and the development of OBO/OWL con-
verters, the native format of an ontology is less relevant.
This has allowed us to work together on the same ontol-
ogy by incorporating the SAO into the GO-CCO.
The SAO was used primarily within prototype segmenta-
tion and annotation tools developed for electron tomog-
raphy data [6] to enhance search within the NIF across
federated data [4] and, as described below, to annotate data
derived from imaging and the literature on phenotypes as-
sociated with neurodegenerative disease [7]. To ensure that
these annotations are not lost, NIF maintains a mapping
between SAO and GO-CCO within a bridge file (for details
on the use of bridge files in NIFSTD, see [8]).
In this paper, we describe an overview of the GO-CCO,
a description of the amalgamation of the GO-CCO with
the SAO, followed by a sketch of how the GO-CCO fits in
with other ontologies. The last part of the paper describesapplications and uses of the GO-CCO. Our aim is to pro-
vide a single unified cellular component ontology that can
serve the needs of a diverse scientific community. The bio-
medical and bioinformatics communities may also benefit
from the links between the GO-CCO and other ontologies.
The URL for the Gene Ontology (GO) is http://www.
geneontology.org/. GO files are publicly available for download
at http://geneontology.org/GO.downloads.ontology.shtml.
Overview of the Cellular Component Ontology
The Cellular Component Ontology describes subcellular
structures and macromolecular complexes. GO-CCO
terms may thus be used to annotate cellular locations of
gene products. Examples of cellular components include
nuclear inner membrane (Figure 1) and the ubiquitin lig-
ase complex, with several subtypes of this complex repre-
sented as descendants. The GO-CCO is not taxonomically
restricted, and includes terms for both core components
found across all domains of life (for example, the species-
generic chromosome) and components specific to par-
ticular lineages (for example, Nebenkern, a mitochondrial
formation found in insects, and thylakoid, a compartment
inside chloroplasts and cyanobacteria).
The two core relationship types used in the GO-CCO are
is_a and part_of . The is_a relation (also known as
SubClassOf) represents the relationship between a more
generic term and a specialized term (for example, between
membrane and plasma membrane), whereas the part_of 
relationship describes how sub-structures are assembled
into larger structures (for example, between nucleolus and
nucleus) [9].
Generally, experimental results or computational predic-
tions support statements that a gene product is located in
or is a subcomponent of a particular cellular component.
The GO-CCO includes multi-subunit enzymes and other
protein complexes, but not individual proteins or nucleic
acids. (Terms describing protein complexes are further
discussed below.) Whilst the GO-CCO includes cell struc-
tures, it excludes cell types, which are instead represented
in the Cell Ontology (CL) [10] or the plant cell branch
of the Plant Ontology (PO) [11]. The GO-CCO also ex-
cludes multicellular anatomical terms, with such struc-
tures being described by either species-specific ontologies
(e.g., Zebrafish anatomy ontology [12], Mouse gross anat-
omy ontology [13]) or taxonomically broad anatomical on-
tologies (e.g., Uberon [14], PO).
The 2013-06-18 release of the GO contains 3332 CC
ontology terms. Approximately half of these terms repre-
sent protein complexes, with the other half representing
larger units.
Amalgamation with SAO
The SAO was incorporated into the Neuroscience Infor-
mation Framework standard ontologies when they were
Figure 1 Diagram and ontology placement of 'nuclear inner membrane'. (A) Diagram of human cell nucleus, including the nuclear inner
membrane. (Taken from Wikimedia commons, http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Diagram_human_cell_nucleus.svg/
2000px-Diagram_human_cell_nucleus.svg.png). (B) Placement of the Gene Ontology term GO:0005637 'nuclear inner membrane', drawn using
the ontology editing tool OBO-Edit (see Methods). Due to space limitations, not all ancestor and descendant terms are shown. Is_a links are
indicated by "I"; part_of links are indicated by "P" (see main text for explanation).
Roncaglia et al. Journal of Biomedical Semantics 2013, 4:20 Page 3 of 11
http://www.jbiomedsem.com/content/4/1/20originally assembled (NIFSTD) [15]. The NIF project
[16] was charged with providing a semantic framework
for describing and searching neuroscience data.
NIFSTD was built from community ontologies when
possible, but as noted above, working with community
ontologies was often a challenge when the project
began. Over the course of the project, NIF gradually re-
placed its custom ontologies with more general com-
munity ontologies when they became available, both to
benefit from the continued enrichment of these ontol-
ogies by the life sciences community and to ensure that
annotations in the NIF would be compatible with the
larger life sciences community. In this case, a reconcili-
ation of the NIF and the GO-CCO was required.
Through this reconciliation, not only would NIFs data
federation and search benefit from the on-going devel-
opment and extensive use of the GO for annotations,
but the community ontologies would become enriched
with the neuroscience-specific content developed by
NIF. The SAO-GO-CCO integration is an example of
this type of harmonization.
We started from a list of about 400 terms from the
NIF Subcellular Anatomy Ontology (SAO) representing
sub-cellular locations that required integration into the
GO-CCO. GO editors carefully examined the list and
considered each term as appropriate. The following cat-
egories were identified:
1) Terms that were already in the GO;
2) Terms that needed to be added to the GO;
3) Terms that were out of scope for the GO.Terms that were already in the GO were:
- high-level GO-CCO terms that were included in the
SAO to provide some structure (e.g. plasma
membrane);
- recent additions to the GO that had not yet been
documented in SAO; in these cases, the NIFSTD IDs have
COMMENTARIES Open Access
Query enhancement through the practical
application of ontology: the IEDB and OBI
Randi Vita1*, James A. Overton1, Jason A. Greenbaum1, Alessandro Sette1, OBI consortium2, Bjoern Peters1
From Bio-Ontologies 2012
Long Beach, CA, USA. 13-14 July 2012
* Correspondence: rvita@liai.org
1Division of Vaccine Discovery, La
Jolla Institute for Allergy and
Immunology, 9420 Athena Circle,
La Jolla, CA 92037, USA
Abstract
Ontologies categorize entities, express relationships between them, and provide
standardized definitions. Thus, they can be used to present and enforce the specific
relationships between database components. The Immune Epitope Database (IEDB,
www.iedb.org) utilizes the Ontology for Biomedical Investigations (OBI) and several
additional ontologies to represent immune epitope mapping experiments. Here, we
describe our experiences utilizing this representation in order to provide enhanced
database search functionality. We applied a simple approach to incorporate the
benefits of the information captured in a formal ontology directly into the user web
interface, resulting in an improved user experience with minimal changes to the
database itself. The integration is easy to maintain, provides standardized terms and
definitions, and allows for subsumption queries. In addition to these immediate
benefits, our long-term goal is to enable true semantic integration of data and
knowledge in the biomedical domain. We describe our progress towards that goal
and what we perceive as the main obstacles.
The IEDB is a free resource that catalogs published experimental data regarding the
recognition of epitopes by adaptive immune receptors. This is accomplished by a team
of curator scientists who read relevant manuscripts and extract the data following a set
of established guidelines [1]. In order to represent the data in a systematic and intero-
perable manner, the IEDB actively contributes to a number of Open Biological and
Biomedical Ontology (OBO) projects [2]. The closest of these collaborations has been
with the Ontology for Biomedical Investigations (OBI), an integrated ontology for the
description of biological and clinical investigations [3]. OBI represents the experimental
design, protocols, materials, and instruments used in biomedical investigations, the
data generated and the type of analyses performed. Previously, we utilized OBI to
model and export the immunological experiments described in the IEDB, allowing rea-
soning and driving database schema redesign [4,5]. Here we describe a deeper integra-
tion of IEDB assay types with OBI and discuss the many benefits for usability and
querying. We close with a general discussion of the costs and benefits of integrating
scientific data with ontologies, looking beyond the scope of IEDB, and offer our per-
spective on future prospects.
Vita et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S6
http://www.jbiomedsem.com/content/4/S1/S6 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2013 Vita et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in
any medium, provided the original work is properly cited.
Using ontologies to represent assays in the IEDB
IEDB curators collect a wide range of information about the experiments they curate.
An accurate classification of the type of assays performed is crucial for a proper under-
standing of an experiment and the reported results. Initially, IEDB curators surveyed
relevant epitope publications and itemized the types of assays that were encountered.
Some assay types were straightforward and easily described, such as tetramer staining,
while others were more complex, such as assays that assess the effect on disease devel-
opment of immunizing mice with an epitope. A list of assay types was produced and
used as a controlled vocabulary by curators adding new experiments to the database
(Figure 1a).
This list-based approach was quite limited and did not capture the inherent similari-
ties between different assay types. For example, the same method (e.g. surface plasmon
resonance) may be used to measure different aspects of the same antibody: antigen
binding event (e.g. equilibrium association and disassociation constants). Conversely,
different methods such as ELISA and ELISPOT assays can be used to measure the
same kind of biological event, such as IFN-g production by T cells. The different assay
types utilized by the IEDB have relationships to each other that were not adequately
described in a flat list of assays.
OBO projects such as OBI each provide a hierarchy of types (terms) as the primary
axis of classification, and then enrich the hierarchy with additional relations. One
branch of OBI is its hierarchy of assays. OBI defines assay (OBI:0000070) as A
planned process with the objective to produce information about an evaluant and
gives the following logical definition (in part) using the Web Ontology Language
(OWL) [6]:
SubClass Of:
has_specified_input some (material_entity and 
(has_role some evaluant role))
has_specified_output some (information content entity and
(is about some (continuant and 
(has_role some evaluant role))))
IEDB assay types are more focused than this general type, involving specific biologi-
cal events and measurement techniques, but each is a descendant of assay.
Figure 1 Conversion of IEDB assay types from a static list to terms in OBI used to generate new search
interface utilizing a hierarchical tree.
Vita et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S6
http://www.jbiomedsem.com/content/4/S1/S6
Page 2 of 10
Incorporating IEDB assays into OBI was an iterative and collaborative process, requir-
ing careful consideration of term names, definitions, and relationships between terms
in OBI and in other ontologies such as the Gene Ontology (GO) [7]. For example, B
cell epitope specific surface plasmon resonance (SPR) measuring KA [1/nM] assay
(OBI:0001730) is logically defined as:
Equivalent To:
direct binding assay
and (has part some surface plasmon resonance assay)
and (has_specified_input some immunoglobulin complex)
and (has_specified_output some
(measurement datum
and (equilibrium association constant (KA)
and (is about some immunoglobulin binding to epitope)
and (has measurement unit label value 
count per nanomolar))))
This logical definition refers to several terms from OBI, the Information Artifact
Ontology [8], and other ontologies: two OBI assay types, direct binding assay
(OBI:0001591) and surface plasmon resonance assay (OBI:0000923); a GO cellular
component, immunoglobulin complex (GO:0019814); a GO biological process, immu-
noglobulin binding to epitope (OBI:0001702); several terms describing the output data,
including measurement datum (IAO:0000109), equilibrium association constant (KA)
(OBI:0001548), and count per nanomolar (UO:0000284) from the Ontology of Units
of Measurement (UO) [9]. The carefully defined relationships among terms provide a
rich structure that supports multi-faceted classification and querying of the assay types
and their instances.
For each assay type we identified the input materials, evaluants, outputs, and the
information that was being produced (Figure 1b). Metadata such as a definition and an
example are also required. The information produced by these immunological assays
always relates either directly to a biological process or to some readout that is proxy
for that process having occurred. Therefore, each assay was linked to a GO biological
process (GO:0008150) that the information content entity generated by the assay
is_about. Other external ontologies were also referenced as appropriate, such as UO
when an assay has a specified readout of known units, e.g. nM. Term requests to
outside ontologies were often required. The production of certain cytokines by T cells,
including CCL4, CCL5, and CCL9, and antibody activities such as immune complex
formation and neutralization of antigen are examples of terms that were added to GO
based on our requests. In this way, the process of integration of our assay types into
OBI enriched the content of several additional ontology projects.
The logical definitions of assays were often quite complex. However, despite the
diversity of IEDB assay types, we discovered strong patterns among them. These
design patterns allowed us to build a system of templates for efficiently describing
specific assay types. The hierarchical organization of assay types also allowed much of
the information about specific assay types to be inherited from their more general
ancestor assay types. The following is part of a template used to generate term
OBI:0001730 (mentioned above). Much of the information required for the logical defi-
nition is inherited from the more general assay types direct binding assay
(OBI:0001591) and surface plasmon resonance assay (OBI:0000923).
Vita et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S6
http://www.jbiomedsem.com/content/4/S1/S6
Page 3 of 10
 assay_type_id: 242
 label: B cell epitope specific surface plasmon resonance (SPR) measuring
KA [1/nM] assay
 textual definition: surface plasmon resonance assay measuring epitope specific
immunoglobulin equilibrium association constant (KA) in nM^-1
 IEDB alternative term full: surface plasmon resonance (SPR) measuring KA [1/nM]
 is a: direct binding assay (OBI:0001591)
 has part: surface plasmon resonance assay (OBI:0000923)
 measurement of: equilibrium association constant (KA) (OBI:0001548)
 has units: nM?-1 (UO:0000284)
The Quick Term Template (QTT) method was used to generate OBI terms for the
hundreds of assay types used in the IEDB [10]. This was easily accomplished by enter-
ing the metadata, such as definition, synonym, term editor, etc, associated with each
assay type into the columns of Microsoft Excel spreadsheets. We then inserted these
terms into OBI using a set of mapping expressions that formulate the logical defini-
tions of each term. The Mapping Master plug-in of the Protégé ontology editor was
used for this task [11], but the method is quite general can be implemented in many
ways. Data can be drawn from a spreadsheet, relational database, text-file, etc. and
inserted into a template using basic string-manipulation tools available in any pro-
gramming language. The template itself can use one of several representations of
OWL or RDF data, such as the Turtle syntax discussed below.
OBI has broad scope and a diversity of users. To ensure that each OBI term is well
understood by a wide audience, the term labels are deliberately verbose. Within the
context of IEDB and its website, where users expect to see information related to epi-
topes, these verbose labels are a drawback. We therefore created IEDB alternative
terms to provide the names commonly used by immunologists to describe assay types
(Figure 1c). For example, the OBI term ELISA of epitope specific granulocyte colony
stimulating factor production by T cells is displayed by its shorter IEDB alternative
term of G-CSF release by ELISA because IEDB users expect all data to be epitope
specific, are familiar with the abbreviation G-CSF, and know that all T cell assays will
reflect a T cell response. Thus the benefits of a formal ontology (i.e., the standardized
definitions, hierarchical tree, and term relationships) are provided while we avoid con-
fusing end users with ontological jargon.
With the term design accomplished and the QTT templates developed, the task of
migrating the hierarchical organization of assays in the ontology into IEDB and its
search interface was straightforward. First, each of the existing IEDB assay types was
mapped to an OBI identifier. The assay finder on the IEDB website is built using
OWL API to read from the OBI.owl file [12]. Only descendants of the term immune
epitope assay are used in the assay finder, since this is the scope of the IEDB. The
IEDB finder application is similar to tools provided by National Center for Biomedical
Ontology (NCBO) Bioportal [13] which allow user interfaces to be populated using
ontologies housed in Bioportal.
The overall effort involved in the implementation described here breaks down as fol-
lows. To replace the 244 assays in the IEDB with corresponding terms in OBI took a total
of 4 man-months. The majority of this work went into analyzing how to precisely and for-
mally model the assays in our list and their relations to one another. This involved reading
Vita et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S6
http://www.jbiomedsem.com/content/4/S1/S6
Page 4 of 10
papers in which they were used and talking to experts in the field. The technical steps of
integration took 2 man-months, and were largely a one-off effort to enable our developers
to read OWL files and convert the data to the formats required for the browsing and
search interfaces of the IEDB website.
As the IEDB encounters new assay types in the literature, each is easily added to OBI
utilizing the same QTT method. Once a new OBI.owl file is generated, the branch
under immune epitope assay simply replaces the existing one in use by the IEDBs
search interface. Updates are integrated into the build process, and require no human
intervention.
Immediate benefits from ontology integration
The conversion of the list of IEDB assays into an ontological hierarchy was time con-
suming, but in our opinion, the benefits have been significant and widespread, includ-
ing: improved definitions, documentation, and understanding by curators and users;
removal of duplicate assay types; improved curation accuracy; improved search by
assay technique and biological event; and improved usability with hierarchical search.
Chief among these is improved understanding of the assay types by the IEDB cura-
tors and users. All IEDB assay types are now clearly documented, with textual and
logical definitions. Having to clearly specify what makes two assay types different based
upon the biological processes measured or the techniques applied has clarified curation
rules. An exact definition allows a meaningful discussion of which type of assay is
actually used in an investigation instead of arguing about labels for assays without defi-
nitions. Viewing groups of assays as siblings and seeing their parents also gives cura-
tors and users additional insight into the relationships between assays and improves
understanding.
Selection from hierarchical structure is also better suited for curation than selection
from a flat list of assays. If the description in a manuscript is not sufficient for a cura-
tor to decide which of two assays to pick, the curator can now select the parent class
of those assays instead. For example, a manuscript may state that an epitope induced
T cell degranulation, but not mention whether perforin or granzyme B was released. In
the past, the curator would be forced to select an assay describing release of one of the
two proteins. Using the new tree, the curator can select the parent class of cytotoxic T
cell degranulation instead, which more accurately reflects the information presented in
the paper.
Automated reasoning over the ontology produces an inferred version of the hierar-
chy that allows for assays to appear in multiple locations. For example, all assays that
use surface plasmon resonance will appear under the term surface plasmon resonance
assay, regardless of what they measure (KA, KD, kon, etc.), while any surface plasmon
resonance assay that also measures a KA will additionally appear under an organiza-
tional term representing assays measuring equilibrium association constant (KA). The
rich information in the logical definitions of the assay types supports this multi-faceted
organization with no additional effort.
The hierarchical organization of assay types not only improves curation, but also
enhances usability for browsing and search of IEDB. End users are now able to view all
of the previously curated data in a hierarchically organized manner (Figure 1c). For-
merly, end users were not able to select all assays that shared a parent, such as all
Vita et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S6
http://www.jbiomedsem.com/content/4/S1/S6
Page 5 of 10
assays that measure KA. Using the new tree, one may select all of a higher level of
assay type, such as ELISA, or refine their criteria to a subset (ELISA with binding con-
stant) or single assay type (ELISA with KD). Thus, hierarchical search significantly
improves usability.
The enriched assay definitions also allow search options to include both what is mea-
sured (GO biological process) and how it is measured (OBI assay type). New content is
being made available as each assay type now links, via the OBI identifier, to its meta-
data provided by OBI, giving users the option of viewing definitions and examples for
the provided search terms.
Logical definitions have allowed us to remove duplicate assay types from the IEDB.
Automated reasoners were able to infer from the logical definitions that several assay
types were redundant. For example, because new assay types were added to the pre-
vious assay list as they were encountered in the literature, one assay measuring che-
mokine (C-X-C motif) ligand 9 release and one measuring MIG release were
separately added to the list. The process of creating logical definitions for these assays
based on GO biological processes followed by reasoning identified that the two assays
were logically equivalent as the two terms are in fact referring to the same cytokine.
Prospective benefits from ontology integration
A significant future benefit of integration of a formal ontology into the IEDB is the
creation of rule-based validation. The logical restrictions and definitions of terms in
OBI and other ontologies can be used to formulate curation rules. For instance, if an
assay type is defined in OBI as requiring a virus as an input, then the curator must
enter an input variable that is a virus. These rules can be extended to the external
ontologies, such as GO. For example, if GO defines a certain cytokine as being pro-
duced only by CD4+ T cells, then an assay measuring that cytokine should not have
CD8+ T cells curated as the effector cell.
Formal representation of all of the IEDBs assay types within OBI has been one
among a number of ways in which the IEDB builds on existing ontologies. Wherever
possible, we are collaborating with existing projects and linking to other resources
through ontological identifiers. We are in the process of integrating many of our classi-
fications: cell types with the Cell Type Ontology [14]; tissue types with the Founda-
tional Model of Anatomy [15]; diseases with the Human Disease Ontology [16];
organisms with NCBI Taxonomy [17]; proteins with the Protein Ontology [18]; and
non-protein molecules from Chemical Entities of Biological Interest (ChEBI) [19]. One
of the greatest benefits of these technologies is that they allow an enhanced range of
queries across a variety of classification systems. For example, it becomes possible to
use the GO biological process hierarchy to query for assays that measure chemokine
responses and distinguish them from other cytokine responses even though the IEDB
does not distinguish which cytokines are chemokines. As additional relevant ontologies
are developed and imported, more sophisticated queries may be performed, providing
new insights into the data of the IEDB.
To enable queries of the IEDB data that take advantage of ontology term definitions,
we translate selected IEDB data into RDF statements and then query the results
(Figure 2) [20]. We use the Terse Triple Notation or Turtle syntax for representing
RDF statements, and use a templating method similar to the QTT method discussed
Vita et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S6
http://www.jbiomedsem.com/content/4/S1/S6
Page 6 of 10
above [21]. We can then use SPARQL (a query language for RDF) to query the data set
[22]. Figure 2 illustrates the use of ChEBI identifiers for epitope molecules in the IEDB.
Using the information captured on the roles of different molecules in ChEBI, it
becomes possible to ask which molecules are targeted by T cell immune responses and
are also used as pharmaceuticals (a ChEBI:role). Similarly, using the Vaccine Ontology,
we can ask what pathogens for which a human vaccine exists have been characterized
in terms of their T cell and B cell responses after natural infection [23].
The URL for a SPARQL query endpoint can be found at http://ontology.iedb.org. We
envision that the SPARQL endpoint will initially be used as a proof of concept by the
relatively few users familiar with this technology. It allows us and others to test
whether biological queries requested by end users can indeed be better answered by
this approach. Queries that are deemed useful will be integrated into the standard
IEDB web interface, which does not require our end users to have any knowledge of
the ontology or these linked data technologies.
Conclusions and perspective
The IEDB has been pursuing integration with ontologies for seven years. We have been
fortunate to find many ontology projects relevant to the data housed by the IEDB, and
continue to actively seek out further collaborations. We have tried to learn from others
working on similar problems, such as (among many others) the eagle-i discovery system
[24], NIF [25], and EuPathDB [26]. Our investments of time and resources have resulted
in a number of immediate benefits for the IEDB described above, but the long term pro-
mise of seamless data integration across different projects has not yet been realized.
The problems faced by the IEDB are general ones, and widely shared. Data need to
be classified. The literature contains a diversity of terminology. Rather than using an
Figure 2 Export of IEDB data into RDF format makes SPARQL queries possible. This example demonstrates
the ability to utilize the role branch of the ChEBI ontology to determine pharmaceuticals with T cell
responses in the IEDB. See http://ontology.iedb.org.
Vita et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S6
http://www.jbiomedsem.com/content/4/S1/S6
Page 7 of 10
ad hoc list of terms, there are benefits to collaborating on shared standards. Policies
such as standardized identifiers and IRIs, and technologies such as RDF and OWL,
make it easier to name terms, annotate them, and encode information in rich networks
of terms. The QTT method makes it easier to add large sets of similar terms to ontol-
ogies. Turtle templates make it easier to move data from tabular-form into RDF
graphs. Projects using shared best practices will find it easier to merge RDF graphs
into broad networks of linked data.
While progress has been made over the years, it has often been slow. We believe that
there are three primary reasons for the slow progress: The speed (or lack thereof) of
community based ontology development, gaps in tools and best practices, and a lack of
examples for advanced ontology utilization in database projects. To a certain extent,
the slow speed of community based ontology development is unavoidable. It is difficult
to find a consensus on how knowledge should be represented in a way that satisfies
experts from different domains. However, these problems are amplified by the lack of
direct funding for ontology development. OBO projects are modeled after open-source
software projects and, like most open-source projects; development is done on a volun-
teer basis. Contributors come from various scientific projects, such as IEDB, and have
specific goals to accomplish for their home projects. The result is often a lack of docu-
mentation, technical debts such as poor testing and release procedures, and a lack of
shared focus. The combination of scientific expertise and specialized technical skills
required to create and maintain an ontology project is relatively rare. The learning
curve is relatively steep. Ontologies such as OBI are built by small, changing groups of
active volunteers, within a larger circle of contributors who have a long-term interest
but lack the time or the skills to play a larger role.
However, the benefits of open ontology projects are easy to share, requiring only a
little knowledge and no special skills. Ontologies and linked data projects benefit from
network effects, where the advantage of joining a large network is much greater than
joining a small one. Investments in this sort of informatics infrastructure for science
may yield widespread savings, as individual scientific projects are saved from having to
reinvent systems of classification, and as the value of linked data increases. We would
like to see greater support for community ontology projects from granting agencies.
We would also like to see the costs of development for ontology project lowered by
further development of shared best practices and supporting tools. The OBO Foundry
provides a number of policies and best practices that facilitate interoperability between
ontologies and the scientific projects that use them. We have found these policies use-
ful and would like to see them expanded in scope, not because they are perfect, but
rather because having any standard (and a mechanism for changing it) is preferable to
having none. There are also several tools and services that facilitate ontology develop-
ment, such as Protégé and Bioportal. Investments in the continued development of
such tools for building high-quality ontologies should reduce the barriers to entry for
scientists, thus increasing participation in ontology development and speeding progress.
Finally, we believe that providing working examples of ontology based data integra-
tion across large data repositories is necessary to demonstrate the feasibility and use-
fulness of the approach. Our own efforts are aimed at providing one such example.
We hope that eventually this will lead to a tipping point, when the benefits of
advanced ontology use will be obvious while the costs for new projects to follow these
Vita et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S6
http://www.jbiomedsem.com/content/4/S1/S6
Page 8 of 10
practices will have dropped substantially based on the ability to re-use existing ontolo-
gies, standards and tools.
Authors contributions
RV participated in ontology modeling, created/edited ontology terms, and wrote and edited the manuscript and
figures. JAO designed the Turtle template, performed the export of data and SPARQL queries, and wrote and edited
the manuscript and figures. JAG participated in ontology modeling, created/edited ontology terms, participated in
data export, and wrote and edited the manuscript. AS evaluated and improved the ontology modeling as a domain
expert. BP conceived and oversaw the entire project, participated in ontology modeling, created/edited ontology
terms, and wrote and edited the manuscript. All authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Acknowledgements
We gratefully acknowledge the IEDB team and funding by the National Institutes of Health contract
HHSN2272201200010C.
Declarations
All fees and funding for this article were paid by the National Institutes of Health contract HHSN2272201200010C.
This article has been published as part of Journal of Biomedical Semantics Volume 4 Supplement 1, 2013: Proceedings
of the Bio-Ontologies Special Interest Group 2012. The full contents of the supplement are available online at http://
www.jbiomedsem.com/supplements/4/S1
Author details
1Division of Vaccine Discovery, La Jolla Institute for Allergy and Immunology, 9420 Athena Circle, La Jolla, CA 92037,
USA. 2http://obi-ontology.org.
Published: 15 April 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Aoki-Kinoshita et al. Journal of Biomedical Semantics 2013, 4:39
http://www.jbiomedsem.com/content/4/1/39SHORT REPORT Open AccessIntroducing glycomics data into the
Semantic Web
Kiyoko F Aoki-Kinoshita1, Jerven Bolleman2, Matthew P Campbell3, Shin Kawano4, Jin-Dong Kim4, Thomas Lütteke5,
Masaaki Matsubara6, Shujiro Okuda7,8, Rene Ranzinger9, Hiromichi Sawaki10, Toshihide Shikanai10,
Daisuke Shinmachi10, Yoshinori Suzuki10, Philip Toukach11, Issaku Yamada6, Nicolle H Packer3
and Hisashi Narimatsu10*Abstract
Background: Glycoscience is a research field focusing on complex carbohydrates (otherwise known as glycans)a,
which can, for example, serve as switches that toggle between different functions of a glycoprotein or glycolipid.
Due to the advancement of glycomics technologies that are used to characterize glycan structures, many glycomics
databases are now publicly available and provide useful information for glycoscience research. However, these
databases have almost no link to other life science databases.
Results: In order to implement support for the Semantic Web most efficiently for glycomics research, the
developers of major glycomics databases agreed on a minimal standard for representing glycan structure and
annotation information using RDF (Resource Description Framework). Moreover, all of the participants implemented
this standard prototype and generated preliminary RDF versions of their data. To test the utility of the converted
data, all of the data sets were uploaded into a Virtuoso triple store, and several SPARQL queries were tested as
proofs-of-concept to illustrate the utility of the Semantic Web in querying across databases which were originally
difficult to implement.
Conclusions: We were able to successfully retrieve information by linking UniCarbKB, GlycomeDB and JCGGDB in a
single SPARQL query to obtain our target information. We also tested queries linking UniProt with GlycoEpitope as
well as lectin data with GlycomeDB through PDB. As a result, we have been able to link proteomics data with
glycomics data through the implementation of Semantic Web technologies, allowing for more flexible queries
across these domains.
Keywords: BioHackathon, Carbohydrate, Data integration, Glycan, Glycoconjugate, SPARQL, RDF standard,
Carbohydrate structure databaseBackground
It is widely acknowledged that developing a mechanism
to handle multiple databases in an integrated manner is
key to making glycomics accessible to other -omic disci-
plines. The National Academy of Science published a
report called Transforming Glycoscience: A Roadmap
for the Future that exemplifies the hurdles and* Correspondence: h.narimatsu@aist.go.jp
10Research Center for Medical Glycoscience, National Institute of Advanced
Industrial Science and Technology, Tsukuba Central-2, Umezono 1-1-1, Tsukuba
305-8568, Japan
Full list of author information is available at the end of the article
© 2013 Aoki-Kinoshita et al.; licensee BioMed
Creative Commons Attribution License (http:/
distribution, and reproduction in any mediumproblems faced by the Glycomics research community
due to the disconnected and incomplete nature of exist-
ing databases [1]. Within the last decade, a large num-
ber of carbohydrate structure (sequence) databases have
become available on the web, all providing their own
unique data resources and functionalities [2]. After the
conclusion of the CarbBank project [3], the German
Cancer Research Center used the available data to de-
velop their GLYCOSCIENCES.de database [4], which in
general focuses on the three-dimensional conformations
of carbohydrates. KEGG GLYCAN was added to theCentral Ltd. This is an open access article distributed under the terms of the
/creativecommons.org/licenses/by/2.0), which permits unrestricted use,
, provided the original work is properly cited.
Aoki-Kinoshita et al. Journal of Biomedical Semantics 2013, 4:39 Page 2 of 7
http://www.jbiomedsem.com/content/4/1/39KEGG resources as a new glycan structure database that
is linked to their genomic and pathway information [5].
The Consortium for Functional Glycomics also devel-
oped a glycan structure database to supplement their
data resources storing experimental data from glycan
array, glycan profiling from mass spectrometry, glyco-
gene knockout mouse and glyco-gene microarray
[6]. In Russia, the Bacterial Carbohydrate Structure
Database (BCSDB) was developed, which contains
carbohydrate structures from bacterial species col-
lected from the scientific literature [7]. Additionally,
small databases used in local laboratories have been
developed, and so the GlycomeDB database was devel-
oped to integrate all the records in these databases
to provide a web portal that allows researchers to
search across all supported databases for particular
structures [8]. The developers of GlycomeDB were
a part of the EUROCarbDB project, which was an
EU-funded initiative for developing a framework for
storing and sharing experimental data of carbohy-
drates [9]. Several resources were developed under the
EUROCarbDB framework including, a database for or-
ganizing monosaccharide information was developed,
called MonosaccharideDB [10] and the HPLC-focused
database GlycoBase [11]. MonosaccharideDB is an
important database for integrating carbohydrate struc-
tures from different resources, since oftentimes differ-
ent representations are used for the same monosaccharides.
Unfortunately, funding-support for the EUROCarbDB
project ended, however the data resources and soft-
ware, which are all available as open source software,
were taken on by the UniCarbKB project [12]. Meanwhile
in Japan, the Japan Consortium for Glycobiology and
Glycotechnology Database (JCGGDB) was developed to
integrate all the carbohydrate resources in Japan [13].
However, despite all of these efforts to develop useful and
valuable glycomics databases, a lack of interoperability
is hampering the development of mashup applications
that are capable of integrating glycan related data with
other -omics data.
Almost all databases mentioned above provide their
information using web pages restricting the query pos-
sibilities to the limited search options provided by the
developers. In addition only a few databases provide
web services that allow retrieval of data in a machine-
readable non-HTML format. The few implemented
web service interfaces return proprietary non-standard
formats making it hard to retrieve and integrate data
from several resources into a single result. Despite
some efforts to standardize and exchange their data
[14,15], most glycomics databases are still regarded as
disconnected islands [1]. Standardization of carbohy-
drate primary structures is more difficult than genom-
ics or proteomics, mainly because of the inherentstructural complexity of oligosaccharides exemplified
by complex branching, glycosidic linkages, anomericity
and residue modifications. Individual databases devel-
oped their own formats to cope with these problems
and encode glycan primary structures in a machine
readable way [2].
Collaboration agreement
In order to integrate data in the life sciences using
RDF (Resource Description Framework), several an-
nual BioHackathons (Biology + Hacking + Marathon)
sponsored by the National Bioscience Database Center
(NBDC) and Database Center for Life Science (DBCLS) in
Japan have been held since 2008. The 5th BioHackathon
was held in Toyama city, Japan, from September 2nd to
7th, 2012 [16]. The glycan RDF subgroup convened in
Toyama to discuss and implement the initial version of a
contextualized RDF document (GlycoRDF) representing
the respective glycan database contents in a standardized
RDF format.
For a better understanding of the processes that gly-
cans are involved in, the participants all agreed that
not only should the information on primary structures
be available but also associated metadata such as the
biological contexts the glycans have been found in
(including information on the proteins that glycans
are linked to), specification of glycan-binding proteins,
associated publications and experimental data must
be taken into consideration. Such data are spread
over the various resources, which are (e.g. in the con-
text of proteins) not limited to only glyco-related
databases. A better integration of all these data collec-
tions will allow researchers to answer more complex
biological questions than simply using individual data-
bases or only cross-linking primary structures. Con-
necting glycomics resources with other kinds of life
science data will also significantly improve the inte-
gration of glycan information into systems biology
approaches.
Each of the glycan databases already has an existing tool
chain and infrastructure in place. Therefore, the glycan
databases were first translated into an agreed-upon RDF
data model. This RDFication process is unique for each
resource due to their respective data contents. However, a
minimal agreement was made by which the databases
could be linked with one another. The following gene-
ralization illustrates some examples of the RDF data gene-
rated by the databases used in the proof-of-concept
queries. Note that a unified prefix glyco: was agreed
upon, as well as the use of identifiers.org as the URI to be
used when referencing external databases. As a result, gly-
can structures, monosaccharides, biological sources,
PROCEEDINGS Open Access
Ontology-Based Querying with Bio2RDFs Linked
Open Data
Alison Callahan1, José Cruz-Toledo1, Michel Dumontier1,2,3*
From Bio-Ontologies 2012
Long Beach, CA, USA. 13-14 July 2012
* Correspondence:
michel_dumontier@carleton.ca
1Department of Biology, Carleton
University, 1125 Colonel By Drive,
Ottawa, ON, Canada
Abstract
Background: A key activity for life scientists in this post -omics age involves
searching for and integrating biological data from a multitude of independent
databases. However, our ability to find relevant data is hampered by non-standard
web and database interfaces backed by an enormous variety of data formats. This
heterogeneity presents an overwhelming barrier to the discovery and reuse of
resources which have been developed at great public expense.To address this issue,
the open-source Bio2RDF project promotes a simple convention to integrate diverse
biological data using Semantic Web technologies. However, querying Bio2RDF
remains difficult due to the lack of uniformity in the representation of Bio2RDF
datasets.
Results: We describe an update to Bio2RDF that includes tighter integration across
19 new and updated RDF datasets. All available open-source scripts were first
consolidated to a single GitHub repository and then redeveloped using a common
API that generates normalized IRIs using a centralized dataset registry. We then
mapped dataset specific types and relations to the Semanticscience Integrated
Ontology (SIO) and demonstrate simplified federated queries across multiple Bio2RDF
endpoints.
Conclusions: This coordinated release marks an important milestone for the Bio2RDF
open source linked data framework. Principally, it improves the quality of linked data
in the Bio2RDF network and makes it easier to access or recreate the linked data
locally. We hope to continue improving the Bio2RDF network of linked data by
identifying priority databases and increasing the vocabulary coverage to additional
dataset vocabularies beyond SIO.
Background
A key activity for life scientists in this post -omics age involves searching for and
integrating biological data from the multitude of independent online biological data-
bases. This task usually involves a tedious manual search and assimilation of isolated
and diverse collections of life sciences data hosted by both large organizations such as
the National Center for Biotechnology Information (NCBI) [1] and the European
Bioinformatics Institute (EBI) [2], as well as smaller groups such as the one that pub-
lishes iRefIndex [3], a database of molecular interactions aggregated from 13 data
sources. While some resources provide links to other databases (e.g. UniProt links its
Callahan et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S1
http://www.jbiomedsem.com/content/4/S1/S1 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2013 Callahan et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
entries to hundreds of other databases [4]), these often lack a semantic richness
required to understand the intent or limitation of the linkage. With thousands of biolo-
gical databases and hundreds of thousands if not millions of datasets, our ability to
find relevant data is hampered by non-standard interfaces backed by an enormous
diversity of data formats [5]. Our inability to easily navigate through available data and
databases presents an overwhelming barrier to their reuse.
The open-source Bio2RDF project [6-8] uses Semantic Web technologies and a set of
conventions to provide linked data [9] for the life sciences. It consists of scripts that
automatically download and convert well known biological data sets into the Resource
Description Framework (RDF) from their original formats, whether it be flat-files, tab-
delimited files, XML or SQL. Using the powerful SPARQL Protocol and RDF Query
Language (SPARQL), Bio2RDF linked data can be uniformly explored and queried.
Although there are several efforts for provisioning life science linked data such as
Neurocommons [10], LinkedLifeData [11], W3C HCLS [12], Chem2Bio2RDF [13] and
BioLOD [14], Bio2RDF is unique in several ways. First, Bio2RDF attempts to capture
the intended meaning serialized by the original data providers in both content and
structure. Each Bio2RDF dataset has a unique linked data vocabulary and topology and
does not attempt to marshal the data into a common schema. Second, Bio2RDF relies
on a set of basic guidelines to produce syntactically interoperable linked data across all
datasets. Third, Bio2RDF infrastructure provides a federated network of SPARQL end-
points and provisions the community with an expandable global network of mirrors
that host Bio2RDF datasets. Finally, Bio2RDF is open source and freely available for to
use, modify or redistribute.
Although Bio2RDF facilitates integration of and programmatic access to otherwise
heterogeneous datasets (in both content and format), a complete syntactic and seman-
tic normalization across the numerous datasets has yet to be fully realized. This is par-
tially because, as stated above, each Bio2RDF dataset has a unique structure and
vocabulary. Linked data serialized as RDF also inherently lacks complex formal seman-
tics that would allow a reasoner to infer the relationship between data items in differ-
ent datasets. As such, the meaning of types and relations in linked data records and
between entities from different datasets are, at best, weakly defined. Consequently,
there is no integration at the level of relations or types, and as a result Bio2RDF data
cannot currently be queried with a universal data model.
Several approaches for integrating biological data have been reported [15-17], but
they rely on a variety of data formats and standards, making sustainable large-scale
integration challenging [18]. Ontologies have also been used as a means to integrate
data at a global level [19-23] but these efforts typically involve creating a new applica-
tion ontology or re-using domain specific ontologies. Here, we report the mapping of
Bio2RDF dataset vocabularies to the Semanticscience Integrated Ontology (SIO) [24],
an ontology that is being used to integrate SADI-based Semantic Web services [25-27].
At its core, SIO focuses on three kinds of basic entities: objects, processes and their
attributes, from which over a thousand more specific kinds of entities are available.
Importantly, SIO provides a coordinated set of relationships that can be used to richly
and axiomatically describe entity types. Thus, if Bio2RDF types and relations were for-
mally mapped to SIO, it would be possible to use SIO to query across and within
Bio2RDF datasets.
Callahan et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S1
http://www.jbiomedsem.com/content/4/S1/S1
Page 2 of 13
Here, we describe a major update to the Bio2RDF project including consolidation
and improvements to conversion scripts, new and updated datasets, and the ability to
uniformly query Bio2RDF datasets using an integrated ontology for the sciences.
Results
This section contains an overview of new and updated Bio2RDF datasets. We describe
SPARQL 1.1 federated queries and SIO-based querying of Bio2RDF datasets.
New and updated Bio2RDF linked data
Table 1 lists theBio2RDF datasets that are currently available at 19 SPARQL endpoints
and for download. The following datasets are new to the Bio2RDF network:
1. BioModels an EBI resource providing details on published computational mod-
els primarily from systems biology
2. InterPro  an EBI resource that describes predicted protein classifications,
domains and biologically significant sites
3. BioPortal A collection of over 300 bio-ontologies from multiple providers
Table 1 Bio2RDF datasets currently available
Dataset Namespace # of triples # of unique
subjects
# of unique
predicates
# of unique
objects
Affymetrix affymetrix 44469611 1370219 79 13097194
Biomodels* biomodels 589753 87671 38 209005
Comparative
Toxicogenomics Database
ctd 141845167 12840989 27 13347992
DrugBank drugbank 1121468 172084 75 526976
NCBI Gene ncbigene 394026267 12543449 60 121538103
Gene Ontology Annotations goa 80028873 4710165 28 19924391
HUGO Gene Nomenclature
Committee
hgnc 836060 37320 63 519628
Homologene homologene 1281881 43605 17 1011783
InterPro* interpro 999031 23794 34 211346
iProClass iproclass 211365460 11680053 29 97484111
iRefIndex irefindex 31042135 1933717 32 4276466
Medical Subject Headings mesh 4172230 232573 60 1405919
National Center for
Biomedical Ontology*
ncbo 15384622 4425342 191 7668644
National Drug Code
Directory*
ndc 17814216 301654 30 650650
Online Mendelian
Inheritance in Man
omim 1848729 205821 61 1305149
Pharmacogenomics
Knowledge Base
pharmgkb 37949275 5157921 43 10852303
SABIO-RK* sabiork 2618288 393157 41 797554
Saccharomyces Genome
Database
sgd 5551009 725694 62 1175694
NCBI Taxonomy taxon 17814216 965020 33 2467675
Total 19 1010758291 57850248 1003 298470583
The Bio2RDF datasets currently available for SPARQL querying and download at http://bio2rdf.org. The total number of
triples, number of unique subject, number of unique predicates and number of unique objects are listed along with the
Bio2RDF namespace for each dataset.
* Datasets new to the Bio2RDF network
 InterPro contains 13 domain resources, iRefIndex contains 13 interaction resources, and NCBO contains 107 OBO
ontologies.
Callahan et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S1
http://www.jbiomedsem.com/content/4/S1/S1
Page 3 of 13
4. NDC  The National Drug Code Directory is a Food and Drug Administration
(FDA) resource providing a current list of all drugs produced or otherwise processed
for distribution by drug companies
5. SABIO-RK  An expert-curated biochemical reactions kinetics database that
includes information about reaction participants, conditions and kinetics
Of the 19 datasets, InterPro, BioPortal and iRefIndex are collections of multiple indi-
vidual datasets. InterPro contains 12 datasets: CATH, Gene3D, PANTHER, PIRSF,
Pfam, PRINTS, ProDom, PROSITE, HAMAP, SMART, SUPERFAMILY and TIGR-
FAMs. iRefIndex consists of 13 datasets (BIND, BioGRID, CORUM, DIP, HPRD, Inna-
teDB, IntAct, MatrixDB, MINT, MPact, MPIDB, MPPI and OPHID). The Bio2RDF
version of BioPortal currently only consists of 107 Open Biomedical Ontologies (OBO)
ontologies including ChEBI, Protein Ontology and the Gene Ontology, and efforts are
being made to coordinate with the NCBO BioPortal team to make use of their emer-
ging SPARQL endpoint.Each dataset in the Bio2RDF network is connected to all the
other datasets, either directly through a named reference or indirectly through some
path through the data. Figure 1A shows how the new and updated Bio2RDF datasets
are interconnected while Figure 1B shows detailed connectivity for the Pharmacoge-
nomics Knowledge Base (PharmGKB). PharmGKB links to the following datasets (with
their corresponding namespace): Allele Frequency Database (alfred), BindingDB (bin-
dingdb), Chemical Entities of Biological Interest (chebi), Comparative Toxicogenomics
Database (ctd), NLMs DailyMed (dailymed), Health Canada Drug Product Database
(dpd), DrugBank (drugbank), Ensembl (ensembl), GenAtlas (genatlas), NCBI Gene
(geneid), PDB Heteroatom Vocabulary (het), Database of Human Unidentified Gene-
Encoded Large Proteins Analyzed (huge), Human CYC (humancyc), International
Union of Pharmacology Ligands (iupharligand), Kyoto Encyclopedia of Genes and Gen-
omes (kegg), Medical Subject Headings (mesh), ModBase Database of Comparative
Protein Structure Models (modbase), National Drug Code Directory (ndc), Online
Mendelian Inheritance in Man (omim), PubChem (pubchem), Refseq (refseq), Systema-
tized Nomenclature of Medicine (snomed), HGNC Gene Symbols (symbol),Therapeutic
Figure 1 Bio2RDF datasets showing namespace connectivity Circles represent Bio2RDF datasets and
the links between them represent a relation between one dataset and another based on IRI namespaces.
Datasets with many links may be considered hubs that serve to connect multiple datasets in the Bio2RDF
network. A All Bio2RDF datasets and their namespace connectivity. B Detail of namespace connectivity for
the PharmGKB dataset.
Callahan et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S1
http://www.jbiomedsem.com/content/4/S1/S1
Page 4 of 13
Targets Database (ttd), and Uniprot (uniprot). Several of these datasets (chebi, ctd,
drugbank, geneid, kegg, mesh, ndc, omim, pubchem, refseq, snomed, symbol, uniprot)
are part of the Bio2RDF network, and can be further explored by following the linked
data or through federated queries.
Federated queries make it possible to formulate a query across connected datasets
that reside in separate SPARQL endpoints. The following SPARQL query makes it pos-
sible to query SGD, Gene Ontology Annotations and the Gene Ontology to obtain the
set of genes that encode proteins involved in zinc ion binding:
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX dc: <http://purl.org/dc/terms/>
PREFIX sgd_vocabulary: <http://bio2rdf.org/sgd_vocabulary:>
SELECT ?gene_id ?gene_label ?description ?protein_id
WHERE {
?gene_id sgd_vocabulary:prefLabel ?gene_label .
?gene_id dc:description ?description .
?gene_id sgd_vocabulary:encodes ?protein_id .
?annotation rdf:type sgd_vocabulary:GO-Annotation .
?annotation rdf:subject ?protein_id .
?annotation rdf:object ?go_identifier .
SERVICE <http://ncbo.bio2rdf.org/sparql> {
?go_identifier rdfs:label ?go_label .
FILTER regex ?go_label,"zinc ion binding"? ?
}
}
The first five results of the query are presented in Table 2. This query returns four
variables for each result, defined by a SELECT statement: the gene IRI, the gene label,
a description provided for the gene, and the protein IRI. These possible values of these
variables are restricted by the conditions in the WHERE clause, which uses structure
of the SGD linked dataset to retrieve the desired results. The query uses a SERVICE
[28] clause to query the remote SPARQL endpoint for GO identifiers with the label
Table 2 Top 5 results of a federated SPARQL query to search for SGD genes related to
the GO function with label zinc ion binding
Gene identifier Gene
label
Description Protein identifier
http://bio2rdf.org/sgd:
S000004688
YMR083W Mitochondrial alcohol dehydrogenase
isozyme III
http://bio2rdf.org/sgd:
S000004688gp
http://bio2rdf.org/sgd:
S000000349
YBR145W Alcohol dehydrogenase isoenzyme V http://bio2rdf.org/sgd:
S000000349gp
http://bio2rdf.org/sgd:
S000002624
YDR216W Carbon source-responsive zinc-finger
transcription factor
http://bio2rdf.org/sgd:
S000002624gp
http://bio2rdf.org/sgd:
S000000819
YER017C Component of the mitochondrial inner
membrane m-AAA protease
http://bio2rdf.org/sgd:
S000000819gp
http://bio2rdf.org/sgd:
S000001306
YIL044C ADP-ribosylation factor (ARF) GTPase
activating protein (GAP) effector
http://bio2rdf.org/sgd:
S000001306gp
SGD gene identifier, label, partial description and corresponding protein identifier for each query result
Callahan et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S1
http://www.jbiomedsem.com/content/4/S1/S1
Page 5 of 13
zinc ion binding, and uses the resulting GO identifiers to restrict the top part of the
query, which searches for SGD genes with corresponding GO annotations.
Bio2RDF vocabulary mappings to SIO
Table 3 lists the Bio2RDF datasets whose vocabularies (dataset-specific types and rela-
tions) have been manually mapped to SIO. A total of 136 classes and 407 object prop-
erties across all Bio2RDF datasets were mapped to SIO. Table 3 contains the number
of classes and object properties in the corresponding dataset vocabulary ontology as
well as the number of exact and intermediate subclass matches. Exact matches are
those mappings for which a SIO class was found to be the most specific parent class
for the corresponding Bio2RDF dataset vocabulary class. Intermediate matches are
mappings for which a SIO class was determined to be a parent class for the dataset
vocabulary but for which a more specific parent could not be identified in SIO.
Querying Bio2RDF linked data using SIO
The availability of Bio2RDF-SIO mappings makes it possible to compose data source
independent SPARQL queries that can be applied to all SPARQL endpoints, as
opposed to a priori formulation of dataset specific queries against targeted endpoints.
For instance, using the Comparative Toxicogenomics Database (CTD), SGD and the
Gene Ontology, we ask for a chemical that participates in a process with an object that
encodes a protein:
Table 3 Bio2RDF vocabulary and SIO mapping metrics
Dataset # of
classes
# of object
properties
# of class exact
mappings
# of class intermediate
mappings
Affymetrix 1 15 0 1
BioModels 0 2 0 0
CTD 4 3 3 1
DrugBank 15 58 6 9
GO
Annotations
1 2 0 1
HGNC 1 30 0 1
Homologene 1 5 0 1
InterPro 7 21 2 5
iRefIndex 5 6 0 5
MeSH 3 46 0 3
OMIM 7 47 3 4
NCBI
Taxonomy
4 16 1 3
NCBI Gene 18 89 1 17
NDC 11 16 0 11
PharmGKB 16 8 6 10
SABIO-RK 0 3 0 0
SGD 42 40 7 33
The number of classes and object properties mapped to SIO for each Bio2RDF dataset vocabulary ontology, as well as
details on quality of class mappings. Exact matches indicate that the Bio2RDF vocabulary class is a direct subclass of its
mapped parent SIO class. Intermediate matches are those that are not exact matches in the SIO class hierarchy but for
which there was not a more precise parent class candidate.
Callahan et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S1
http://www.jbiomedsem.com/content/4/S1/S1
Page 6 of 13
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX sio: <http://semanticscience.org/resource/>
SELECT *
FROM <http://bio2rdf.org/ctd>
WHERE {
?chemical a sio:SIO_010004.
#SIO_010004: chemical entity
?chemical rdfs:label ?chemicalLabel.
?chemical sio:SIO_000062 ?process.
#SIO_000062: is participant in
?process rdfs:label ?processLabel.
SERVICE <http://sgd.bio2rdf.org/sparql>{
?protein a sio:SIO_010043.
#SIO_010043: protein
?protein sio:SIO_000062 ?process.
?gene sio:SIO_010078 ?protein.
#SIO_010078: encodes
?gene rdfs:label ?geneLabel.
}
}
This query returns all bound variables specified in the WHERE clause, as indicated by
the SELECT * statement. This query again uses the SERVICE keyword to execute a fed-
erated query, in this case over the remote SGD SPARQL endpoint. In this query, the a
keyword is used as a short form for rdf:type. Among its answers, this query returns Vin-
clozolin (mesh:025643) and U2snRNP associated splicing factor (sgd:S000004901),
both of which are found to participate in RNA splicing (go:0008380) (Figure 2). This
query is possible because the predicates sgd_vocabulary:is-participant-in and ctd_voca-
bulary:is-participant-in have both been mapped to the corresponding SIO object prop-
erty is participant in, while ctd_vocabulary:Chemical has been mapped to SIO class
chemical entity and sgd_vocabulary:Protein has been mapped to SIO class protein.
Discussion
While simple in structure, flat data files are charged with implicit semantics, especially
in datasets where relationships between biological entities are taken for granted. Con-
sider, for example, that the SGD entry for gene BCY1 (sgd:S000001295) provides func-
tional annotations for its encoded gene products, primarily a protein product (which
may not have a standard identifier). Encoding such semantics explicitly is a challenge
when generating linked data from other data formats, but one which may have signifi-
cant impact on how the data is subsequently incorporated for analysis. The consolida-
tion, review and update of Bio2RDF conversion scripts created an opportunity to
ensure that minimal local semantics were asserted. Where applicable, we disambigu-
ated between dataset, record and entity level provenance, such that information that
pertains to datasets (release versions) was recorded separately from record-level infor-
mation (e.g. curators/editors, dates of creation, updates, etc.) and from entity-level
information (biological or informational relationships).
Callahan et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S1
http://www.jbiomedsem.com/content/4/S1/S1
Page 7 of 13
Ontologies constructed using the Web Ontology Language (OWL) can be used to
define the meaning of types and relations used in Linked Data. Although OWL for-
mally differentiates classes, object properties (which relate two individuals), data prop-
erties (which have as value only literals) and individuals, the same cannot be said of all
Bio2RDF datasets. In the case where a literal represents a label or a quantity, there is a
need to transform it into an instance of a type (e.g. sio:label, sio:quantity, sio:descrip-
tion, etc.), nominally a SIO information content entity, with the literal attached to sio:
has-value, which is SIOs only datatype property. So while Bio2RDF datatype relations
were mapped to SIO object properties, the challenge of rendering these literals using
sio:has-value still remains. One solution would involve transforming SIO-containing
queries to datatype mappings, and another is to transform all literal data into an
instantiated class. In this way, it would become possible to categorize each kind of lit-
eral, as has been demonstrated with the Chemical Information Ontology [29].
Ontologies also enable data integration through querying using a unified vocabulary.
By making use of SIO to map Bio2RDF vocabularies, not only can previously uncon-
nected datasets be queried to discover the relationships between them, but the
intended semantics of the classes and object properties they use are formally defined
such that computational reasoning and inference is possible. Unlike upper level ontolo-
gies such as the Basic Formal Ontology (BFO) [30] and its associated Relation Ontol-
ogy [31], SIO contains unified and rich axiomatic descriptions of its classes and
properties. SIO also enables the representation of cumulative-constitutively organized
material entities [32], which are common in biological domains but not captured by
BFO. SIO also contrasts OBO domain ontologies such as the Vaccine Ontology [33] in
that it does not suffer from a proliferation of classes and object properties as a result
of attempting to integrate multiple OBO ontologies. Instead, SIO provides a well-
described upper level that acts as an anchor for other domain specific classes and
object properties as well as design patterns [34] for developing new classes and their
Figure 2 Graphical representation of results of the SPARQL query for CTD chemicals that
participate in the same molecular process as SGD proteins. The SGD gene S000004901 encodes a
gene product that is typed as protein in the SGD dataset. This protein is annotated to be a participant in
the GO process RNA splicing. Chemicals from the CTD dataset that are also participants in this GO
process are retrieved. This query is possible because of cross-dataset links between CTD and SGD that
result from mapping their Bio2RDF vocabularies to SIO: the SGD type protein was mapped as a subclass
of the SIO protein class, while the CTD type chemical was mapped as a subclass of the SIO chemical
entity class. The SGD dataset uses SIO relation encodes to relate genes and gene products, and both the
SGD and CTD datasets use the SIO relation is participant in to link entities to GO processes. Square boxes
are SIO OWL classes. Rounded boxes are RDF resources that are part of Bio2RDF linked datasets.
Callahan et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S1
http://www.jbiomedsem.com/content/4/S1/S1
Page 8 of 13
axiomatic definitions. Finally, another advantage of mapping Bio2RDF vocabularies to
SIO is that once new candidate classes are identified they can be added to SIO, thus
improving the coverage of the ontology.
The process of manually mapping Bio2RDF dataset vocabulary classes to SIO identi-
fied branches of the SIO class hierarchy that can be further developed. Specifically,
Table 3 indicates that many Bio2RDF dataset vocabulary classes do not have exact par-
ent matches in SIO. For example, the Bio2RDF National Drug Code (NDC) vocabulary
class vaccine was mapped to SIO heterogeneous substance (a subclass of material
entity) as its direct parent. A search for the term vaccine in NCBO BioPortals ontol-
ogy index returns many hits, including a class in the Ontology for Biomedical Investi-
gations (OBI) [35] and in SNOMED Clinical Terms [36]. Closer examination of the
OBI vaccine class reveals that its direct superclass is material entity, which is less
specific than SIOs heterogeneous substance. On the other hand, SNOMEDCTs
vaccine class has the following class lineage:substance  biological substance  immu-
nologic substance  immunologic agent  vaccine, immunoglobulin, and/or anti-
serum vaccine. This indicates that the SNOMEDCT vaccine class is more
semantically granular than that of SIO. However, SIO material entity has necessary
(but not sufficient) axioms associated with it, specifically has attribute some mass and
has proper part only material entity, which are lacking in both OBI and SNOMEDCT.
Other Bio2RDF dataset vocabularies, such as OMIM, have exact superclass matches
in SIO. For example, the OMIM vocabulary class gene is mapped as a direct subclass
of the SIO class gene and OMIM class entity is mapped as a direct subclass of SIO
class entity. SIO gene is a subclass of nucleic acid part, which is partially defined by
the following axioms: has direct part some nucleotide residue and is part of some
nucleic acid. When a Bio2RDF vocabulary class has an exact parent match in SIO it
becomes possible to take advantage of its rich hierarchy and axiomatic descriptions,
but as shown above, in the case of less specific matches there may be better candidate
parent classes in other bio-ontologies.
With access to more normalized structured data free of multi-valued fields, it
becomes possible to consider converting Bio2RDFdatasets into full-fledged OWL
ontologies. This has the significant advantage of allowing sophisticated reasoning to
classify and check the consistency of the data itself, as was demonstrated in finding
curation errors in the BioModels database [15]. Since there are only a handful of data-
bases that contain raw experimental data (e.g. omics data such as that in PRIDE, a
mass spectrometry database, or ArrayExpress, a microarray database) that may best
remain as instance data, the rest could be fully formalized and gain the benefit of auto-
mated reasoning. Systematic conversion of Bio2RDF datasets to OWL may one day be
possible, but will require significant effort and coordination to produce a unified
knowledge base.
In the future we would like to facilitate integration of Bio2RDF data with other avail-
able bio-ontologies. The NCBO BioPortal team has mapped SIO to the domain ontolo-
gies they provide, based on lexical matching of class labels. This enables the querying
of Bio2RDF resources using NCBO bio-ontologies, and provides an approach for inte-
grating OBO ontologies with the Bio2RDF network via SIO. A related NCBO effort is
its Resource Index [22], which uses the NCBO Annotator [37] to annotate text fields
in records from 27 biological databases (including DrugBank, OMIM, PharmGKB,
Callahan et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S1
http://www.jbiomedsem.com/content/4/S1/S1
Page 9 of 13
PubMed and UniProt) to link their content to NCBO bio-ontologies. We intend to
contribute Bio2RDF to this index by executing the NCBO Annotator over Bio2RDF
Linked Data. The resulting index would have the advantage that, unlike the databases
currently included, NCBO ontologies would be linked to Bio2RDF data and the rela-
tions among datasets and ontologies would also be automatically available via the con-
nectivity of Bio2RDF datasets.
Inter-namespace connectivity among datasets is useful for identifying how dataset
content is related, but also for ranking candidate resources for addition to the Bio2RDF
network. Specifically, if multiple Bio2RDF datasets contain relations to entities from a
single biological database that is not currently in Bio2RDF, this database can be con-
sidered high priority for addition. Identifying candidate additions in this way is possible
because of the Bio2RDF approach to consistent IRI creation as well as the resource
registry which contains descriptions and preferred short names for datasets both in
and not yet part of Bio2RDF.
Key to the continued growth of Bio2RDF is participation from the scientific commu-
nity that produces and consumes the data that forms the Bio2RDF network. Housing
the latest versions of all Bio2RDF data conversion scripts in GitHub allows others to
both use Bio2RDF scripts to generate their own linked data graphs, but also to contri-
bute new scripts that build on existing ones or convert datasets not currently in
Bio2RDF. It should be noted that all Bio2RDF scripts are licensed using a free software
license that permits re-use and modification with attribution. The GitHub repository
may also act as a place for discussion in the community with regard to the Bio2RDF
data modelling practices as well as additions to the project.
Conclusions
Bio2RDF is an open source project to coordinate the provision of linked data for the
life sciences. The use of an internally consistent IRI scheme across all datasets, map-
pings to other terminologies and publishing linked data at SPARQL endpoints facili-
tates the arduous task of data integration and knowledge discovery. Future work will
focus on further integration of existing bio-ontologies with Bio2RDF datasets as well as
prioritizing new databases to add to the Bio2RDF network.
Methods
In this section we describe the tools and methods used to publish Bio2RDF data con-
version scripts and linked data for the scientific community. We also present the
Bio2RDF approach for modelling dataset-specific vocabularies with examples. Finally,
we describe the mapping of Bio2RDF linked data vocabularies to SIO to enable query-
ing across multiple datasets.
Generating Bio2RDF linked data
A GitHub repository (http://github.com/bio2rdf/bio2rdf-scripts) was created to house
Bio2RDF conversion scripts. We aggregated all known scripts (30 PHP scripts, 1 Java
program and 1 Ruby gem) and updated them to address changes in the underlying
data formats and content. GitHub users can download a copy of the repository, add or
edit code and submit new scripts or changes, which will be reviewed by repository
moderators.
Bio2RDF identifies data items using the following pattern:
Callahan et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S1
http://www.jbiomedsem.com/content/4/S1/S1
Page 10 of 13
http://bio2rdf.org/namespace:identifier
where the namespace is drawn from a resource registry of data providers and their
preferred short name that currently maintained as part of PHP-LIB project [38]. For
example, the Bio2RDF Internationalized Resource Identifier (IRI) for the UniProt entry
with the identifier P26838 would be:
http://bio2rdf.org/uniprot:P26838
Two additional namespace patterns are used to specify new resources created in gen-
erating Bio2RDF data. The first is for dataset-specific types and predicates and follows
the pattern:
http://bio2rdf.org/namespace_vocabulary:identifier
For example, the Saccharomyces Genome Database (SGD) describes genes and their
protein products, which are typed as
http://bio2rdf.org/sgd_vocabulary:Protein
The second involves other resources created to convert n-ary relations into a set of
binary relations, and follows the pattern:
http://bio2rdf.org/namespace_resource:identifier
For example, the Pharmacogenomics Knowledge Base (PharmGKB) describes associa-
tions between diseases, genes and drugs, but does not specify an identifier for these
associations, and hence we assign a new stable identifier for each, such as
http://bio2rdf.org/pharmgkb_resource:association_PA445019_PA126
for the gene-disease association between cytochrome P450, family 2, subfamily C,
polypeptide 9 (pharmgkb:PA126) and Myocardial Infarction (pharmgkb:PA445019).
Bio2RDF scripts were executed to generate linked data from the latest version of all
available datasets (as of September 15, 2012).
Mapping Bio2RDF types and predicates to SIO
To facilitate dataset-independent querying, types and predicates used in Bio2RDF data-
sets were manually mapped to the Semanticscience Integrated Ontology (SIO). Dataset
types and predicates were declared as Web Ontology Language (OWL) classes or
object properties as appropriate using the following SPARQL queries:
CONSTRUCT {
?o rdf:type owl:Class
} WHERE {
?s rdf:type ?o.
FILTER regex ?o, "bio2rdf"
}
CONSTRUCT {
?p rdf:type owl:Obje
? ?
ctProperty
} WHERE {
?s ?p ?o .
FILTER regex ?p, "bio2rdf"
}
? ?
The resulting vocabularies were manually mapped to corresponding SIO classes and
object properties. The mapping process involved asserting the rdfs:subClassOf relation
Callahan et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S1
http://www.jbiomedsem.com/content/4/S1/S1
Page 11 of 13
for Bio2RDF vocabulary classes (e.g. sgd_vocabulary:Chemical is a subclass of sio:-
chemical entity), as well as owl:equivalentProperty and owl:superProperty relations as
appropriate. All types used in Bio2RDF datasets were added in the corresponding vocabu-
lary ontology and mapped to SIO. Only predicates belonging to a [dataset]_vocabulary
namespace, however, were mapped to SIO. RDF, RDFS, OWL and Dublin Core predicates
were not included in the dataset vocabularies or mapped to SIO. Each Bio2RDF dataset
vocabulary is serialized as an OWL ontology, and its mappings to SIO are serialized as a
separate mapping ontology that imports the corresponding vocabulary ontology and SIO.
Provisioning Bio2RDF datasets and SPARQL endpoints
Each Bio2RDF linked dataset was loaded into a unique SPARQL endpoint for querying,
using OpenLink Virtuoso Community Edition build 06.01.3127 with the faceted brow-
ser, SPARQL 1.1 query federation, and Cross-Origin Resource Sharing (CORS) enabled.
SPARQL endpoints are accessible at http://[namespace].bio2rdf.org. For example, the
Saccharomyces Genome Database (SGD) SPARQL endpoint is available at http://sgd.
bio2rdf.org. The list of all available endpoints can be found at http://bio2rdf.org. All
updated Bio2RDF linked data is also available for download as N-Triples at http://down-
load.bio2rdf.org. We have implemented a versioning and release protocol for Bio2RDF
datasets and mappings. The current Release 2 of Bio2RDF data sets, featuring updates to
19 datasets, is available at http://download.bio2rdf.org/current/. The corresponding
vocabulary-SIO mapping files are available at the GitHub bio2rdf-mapping project page,
at http://github.com/bio2rdf/bio2rdf-mapping/tree/master/2/[namespace] e.g. https://
github.com/bio2rdf/bio2rdf-mapping/tree/master/2/ctd.
Authors contributions
AC, JCT and MD carried out updates to Bio2RDF data conversion scripts and migration to GitHub, as well as managed
loading of datasets into Virtuoso. AC and JCT mapped Bio2RDF vocabularies to SIO, developed dataset queries,
collected vocabulary metrics, and drafted the manuscript. MD conceived of the study, participated in its design and
helped to draft the manuscript. All authors read and approved the final manuscript.
Competing interests
The authors have no competing interests to declare.
Acknowledgements
This research was supported by an NSERC CGSD for AC, and an NSERC Discovery Grant and Ontario Early Researcher
Award for AC, JCT and MD. We also acknowledge useful discussions and technical support from Marc-Alexandre Nolin
and Peter Ansell.
Declarations
Publication in this supplement was supported by an Ontario Early Researcher Award to MD.
This article has been published as part of Journal of Biomedical Semantics Volume 4 Supplement 1, 2013: Proceedings
of the Bio-Ontologies Special Interest Group 2012. The full contents of the supplement are available online at http://
www.jbiomedsem.com/supplements/4/S1
Author details
1Department of Biology, Carleton University, 1125 Colonel By Drive, Ottawa, ON, Canada. 2Institute of Biochemistry,
Carleton University, 1125 Colonel By Drive, Ottawa, ON, Canada. 3School of Computer Science Carleton University,
1125 Colonel By Drive, Ottawa, ON, Canada.
Published: 15 April 2013
PROCEEDINGS Open Access
Representation of probabilistic scientific
knowledge
Larisa N. Soldatova1*, Andrey Rzhetsky2, Kurt De Grave3, Ross D King4
From Bio-Ontologies 2012
Long Beach, CA, USA. 13-14 July 2012
* Correspondence: larisa.
soldatova@brunel.ac.uk
1Department of Information
Systems and Computing, Brunel
University, London, UK
Abstract
The theory of probability is widely used in biomedical research for data analysis and
modelling. In previous work the probabilities of the research hypotheses have been
recorded as experimental metadata. The ontology HELO is designed to support
probabilistic reasoning, and provides semantic descriptors for reporting on research
that involves operations with probabilities. HELO explicitly links research statements
such as hypotheses, models, laws, conclusions, etc. to the associated probabilities of
these statements being true. HELO enables the explicit semantic representation and
accurate recording of probabilities in hypotheses, as well as the inference methods
used to generate and update those hypotheses. We demonstrate the utility of HELO
on three worked examples: changes in the probability of the hypothesis that sirtuins
regulate human life span; changes in the probability of hypotheses about gene
functions in the S. cerevisiae aromatic amino acid pathway; and the use of active
learning in drug design (quantitative structure activity relation learning), where a
strategy for the selection of compounds with the highest probability of improving
on the best known compound was used. HELO is open source and available at
https://github.com/larisa-soldatova/HELO
Introduction
All knowledge resolves itself into probability.
David Hume, in a treatise of Human Nature (1888), 181-182.
Scientific knowledge is inherently uncertain: experimental observations may be cor-
rupted by noise, and no matter how many times a theory has been tested there is still
the possibility that new experimental observations will refute it  as famously hap-
pened to Newtonian mechanics. Probability theory has from its conception been
utilized to represent this uncertainty in scientific knowledge. However the role of prob-
ability theory has proved controversial, with for example the great philosopher of
science Karl Popper arguing that probabilities cannot be applied to scientific theories
on the grounds that an infinite number of theories can explain any scientific data,
therefore their a priori probabilities are zero. This view is now generally disregarded
and a Bayesian approach to the use of probabilities in science is widely accepted. In
Bayesian reasoning a priori probability estimates for hypotheses are updated through
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S7
http://www.jbiomedsem.com/content/4/S1/S7 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2013 Soldatova et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
observation of additional evidence [1]. The Bayesian approach is arguably the only
rational method for updating beliefs [2,3].
Despite the undoubted importance of probabilities in science it is unfortunately the
case that conventional knowledge representations in bio-medicine are insufficient to
support probabilistic reasoning. The best available representation, in our view, is the
Evidence Code Ontology (ECO) [4]. ECO enables the recording of evidence that sup-
ports scientific statements, e.g. experimental evidence, sequence similarity, curator
inference; and also by what method the evidence was obtained, e.g. through computa-
tional combinatorial analysis, inference from background knowledge, non-traceable
author statement. This information enables researchers to qualitatively evaluate the
degree of uncertainty of scientific statements. However, such evaluations are rarely
recorded, not checked for consistency with other relevant evaluations, and therefore
are difficult to use for probabilistic reasoning. There is a need for a resource that
would enable the explicit quantitative recording of probabilities associated with
research statements. To address this need we propose the ontology HELO (HypothEsis
and Law Ontology) that supports probabilistic reasoning about bio-medical research
statements.
HELO aims
The HELO ontology was originally designed to support development of Robot Scientists,
these are physically implemented laboratory automation systems that exploit techniques
from the field of artificial intelligence to execute cycles of scientific experimentation.
A probability that a research statement is true may vary greatly depending on the
source of the statement. While experimental data from good laboratories are likely to
be true, even research statements extracted from very high impact journals are not
necessarily valid. C.G. Bengley and L.M. Ellis in their recent article in Nature report
that scientific findings have been confirmed only in 6 out of 53 landmark studies in
haematology and oncology [5]. This is consistent with results in other areas. For exam-
ple Prinz et. al report that only 25% of published pre-clinical studies could be validated
[6]. The authors stressed that validation attempts could fail for various reasons, includ-
ing technical differences. HELO aims to provide a framework for the recording of
probabilities that research statements are true, and for probabilistic reasoning with
such statements.
The key HELO classes
Research statements
The HELO representation of research statements is based on the representation of
research hypothesis as PREDICATE(entityi, entityj) defined in an ontology LABORS,
where predicate is a relation and entity is a class or instance defined in a domain
ontology [7]. HELO enables one to formulate complex research statements, where
basic (atomic) statements like PREDICATE(entityi, entityj) are combined by logical
operators ?, ?, ¬, ®, ?. Entities that form research statements may be replaced by
more generic entities (parent classes) and/or be specialized by their properties: indivi-
dual gene names could be replaced with classes from Gene Ontology (GO) [8]; specific
environmental factors could be replaced with general terms such as increased/
decreased temperature, carbon source, addition of drugs, etc.; and measurable
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S7
http://www.jbiomedsem.com/content/4/S1/S7
Page 2 of 12
phenotypes could be replaced with general terms such as relate to growth, cell shape,
etc. The following complex statement about yeast strains could be then represented:
If all genes with lactase activity are deleted from a yeast strain and if this strain is
grown in medium with lactose as the sole carbon source, then the phenotype will be no
growth. This could be expressed in logic using terms defined in various ontologies as:
((( gene, yeast_strain, x
HAS-FUNCTION _a
? ? ? |
( ,gene lactase ctivity
yeast strain gene
process deletion
)
( , )
( , )
?
?
?
HAS-PART _
IS-A
HAS-PARTICIPANT
HAS-OUTPUT _
( , )
( ,
gene deletion
deletion yeast s
?
train
growth medium lactose
lacto
)
( , )
(
?
?HAS-PART _
HAS-FUNCTION se carbon source
growth medium x
phenotyp
, )
( , )
(
_
HAS-PART _
IS-A
?
?
e no growth
x carbon source
, )
, _
_
HAS-FUNCTION( ))
?
?
In combination with a logical model of metabolism these statements would enable
deduction of the fact:
o (HAS-QUALITY( _ _ )).yeast strain no growth,
HELO defines a hierarchy of research statements: research hypothesis, hypotheses set
(a collection of hypotheses with a total probability 1, it usually combines research
hypotheses, negative hypotheses, and alternative hypotheses, see [7] for more detail),
assumption, conclusion, scientific law (models and generic rules, including Bayes rule),
theorem (including Bayes theorem). Research laws may be represented as production
rules (statementi ® statementj), where statements correspond to hypotheses, evidence,
conclusions. For example,
INTERACT-PHYSICALLY( )
INTERACT-
gene product gene producti j 
o
,
EPISTATICALLY( ).gene genei j,
Research laws may be models that are produced for example by the Eureka system
that outputs laws of nature [9].
HELO is designed to consistently accommodate scientific hypotheses and laws col-
lected from different sources: interviews with scientists, web pages, research papers,
databases, program codes. Any research statement in HELO has an associated prob-
ability of being true.
Probability
Probabilistic reasoning is essential in biomedicine, e.g. the Ontology of Adverse Events
(OAE) models causal adverse event probability (an information content entity that
represents a probability that an adverse event is caused (induced) by a medical interven-
tion) [10], the Mass Spectrometry (MS) structured controlled vocabulary developed by
the HUPO Proteomics Standards Initiative models modification probability (a priori
probability of a modification) [11]. However, the concept of a probability is not modeled
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S7
http://www.jbiomedsem.com/content/4/S1/S7
Page 3 of 12
consistently in biomedical ontologies. MS models the class probability as a subclass of
the class modification parameters. The Parasite Experiment Ontology (PEO) defines the
concept as a subclass of the class statistical measure and data collection (a statistical
way of expressing knowledge or belief that an event will occur or has occurred) [12].
Computational Neuroscience Ontology (CNO) defines probability as the subclass of the
class model parameter [13]. CNO has the class mathematical concept (a thing that
represents the different mathematical concepts used to represent models), but for some
reason probability is not considered as a mathematical concept. The Semanticscience
Integrated Ontology (SIO) [14] defines the class probability as the subclass of the class
description. SIO has the class mathematical entity, but again probability is not consid-
ered as such.
HELO follows the theory of probability [15], [16] and defines the class probability as
a subclass of the class mathematical function to enable mathematical operations with
probabilities (a mathematical function expressing knowledge or belief that a research
statement is true). This definition covers frequentist probabilities (taken as a limiting
frequency of experimental observations), and also Bayesian subjective probabilities, or
beliefs.
Reliable statistical estimates of the probability of a statement being true are often
unavailable. In the subjective Bayesian framework human experts are expected to pro-
vide priors that capture scientific background knowledge and intuition. Obtaining such
probabilities is notoriously difficult and there is an extensive literature on the subject.
Once prior probabilities are given the probabilities of scientific statements may be then
iteratively updated (increased or decreased) with new evidence. It is important to
record these changes in value and how they were inferred.
HELO enables the recording of how probabilities were obtained. The class method of
probability estimation has subclasses Bayesian inference, expert estimation, statistical
calculation, deduction, abduction, induction, homological inference; and linked to the
class procedure that records a specific algorithm implementation for obtaining a prob-
ability of a research statement. The class probability has the subclasses prior probabil-
ity and posterior probability. A research statement is linked to an associated probability
via the functional relation HAS-PROBABILTY.
HELO imports from SIO the relations refutes, supports, disputes to link research
statements, and the relations HAS-DISPUTING-EVIDENCE, HAS-REFUTING-EVI-
DENCE, HAS-SUPPORTING-EVIDENCE to link research statements and evidence
(see Fig.1).
An ontology of the theory of probability
HELO defines the key entities of the theory of probability to enable logically consistent
recording of operations that involve probabilities. HELO includes such classes as vari-
able, probability distribution function, probability mass function, mean, variance, and
such qualities as independent, random (variable), joint (probability). In order to orga-
nize these classes into a hierarchical system, HELO imports the following top-level
classes: continuant (from BFO [17]), information content entity (from IAO [18]), plan
specification (from OBI [19]), procedure (from LABORS [20]), representation (from
LABORS and SIO [14]) (see Fig.2).
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S7
http://www.jbiomedsem.com/content/4/S1/S7
Page 4 of 12
Additionally, the class random event is defined as an upper class, because the con-
cept of an event ontologically differs from the notion of a process or any other notion.
It may involve a process and participants and it has an associated time point, e.g. the
end of the process. The theory of probability deals with random events defined on a
sample space of all possible outcomes of a random event.
HELO is expressed in OWL-DL. It has been checked for logical consistency with the
reasoners HermiT 1.3.6 and FaCT++. HELO is open source and available at https://
github.com/larisa-soldatova/HELO.
Figure 1 An example of the HELO representation of a research statement. The figure shows the
representation of the values of the prior and posterior probabilities of the research statement about
sirtuins, and also the supporting and refuting evidence.
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S7
http://www.jbiomedsem.com/content/4/S1/S7
Page 5 of 12
Worked examples
The S. cerevisiae aromatic amino acid pathway
This example demonstrates how a probability that a research hypothesis is true is used
for automated experimentation.
King et al. (2009) demonstrated the full automation of scientific discovery [20]. The
Robot Scientist Adam employed abductive inference to formulate a set of 8 hypoth-
eses based on its logical model of the S. cerevisiae aromatic amino acid (AAA) pathway
concerning which gene had been deleted (see Supplementary Information in [20] for
more detail). The prior probability of each hypothesis from the set of being correct,
(using a uniform distribution) was 1/8. Adam then planned and executed cycles of aux-
otrophic experiments to test these hypotheses. Each cycle resulted in the rejection of
one or more hypotheses, and the probabilities of the remaining hypotheses were
increased with each cycle. The experiments were executed until only one hypothesis
was left. The posterior probability of the remaining hypothesis was 1 and all of the
others - 0.
In making its decision about which experiment to execute in each cycle Adam used
the probabilities of the hypotheses being true, the cost of the compounds required in
the experiments to test those hypotheses, and the predicted information gain in testing
the hypotheses. Previously, probabilities of research hypotheses were represented and
recorded as associated with the experiments metadata [21]. HELO enables the direct
recording of prior and posterior probabilities as properties of research hypotheses. This
makes the representation of probabilistic knowledge explicit, and streamlines probabil-
istic reasoning, decision making, and automated experimentation.
Sirtuins
We use the example of sirtuins as an example of how to utilise HELO for probabilistic
representation of research statements. We are interested in recording and automating
the argumentation involved in the sirtuin case, both to direct our own research into
aging, but also as an exemplar of biological reasoning. This example is typical in how
Figure 2 An overview of the ontology HELO. The figure shows the top-level classes of HELO and some
of their extentions.
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S7
http://www.jbiomedsem.com/content/4/S1/S7
Page 6 of 12
the probability of scientific statements varies over time with the observation of new
experiments. The example also illustrates the use of homologous inference, which is
the basis of much biological reasoning, and which is essentially probabilistic.
Sirtuins are highly conserved NAD+ - dependent deacetylases that are believed to
play a role in regulating lifespan in many organisms. The potential role of sirtuins in
extending human lifespan has led to extensive research into the human gene SIRT1
and its orthologs. For example in 2001 Tissenbaum & Guarente showed that increased
dosage of the SIRT1 homolog extends lifespan in the nematode (Caenorhabditis ele-
gans) [22]. Increasing sirtuin level through genetic manipulation has been observed to
extend lifespan in C. elegans, the yeast (Saccharomyces cerevisiae), the fruitfly (Droso-
phila melanogaster), and the mouse (Mus musculus) [23,24]. This research sparked
commercial interest and in 2008 Sirtris Pharmaceuticals Inc., working on exploiting
sirtuin modulation for the treatment of human disease, was bought by GlaxoSmithK-
line for approximately USD 720 million.
This excitement about the potential of sirtuins suffered a major setback in 2011
when Burnett et al. reported that overexpressing the sirtuin gene in two model organ-
isms, C. elegans and D. melanogaster did not in fact boost longevity as had been pre-
viously reported [25]. The situation changed again in 2012 when Kanfi et al. reported
that the sirtuin SIRT6 regulates lifespan in male mice, but not in female ones [23].
Therefore the probability of the research hypothesis that sirtuins regulate organism
lifespan has increased and decreased over the last decade.
The primary research hypothesis h1 we are interested in is: SIRT1 regulates human
life span (SIRT1 is a sirtuin gene in humans). HELO enables the recording of this
research statement:
IS-A( )
HAS-QUALITY( - )
REGUL
human organism
organism life span
,
,
?
?
ATES( - ).SIRT life span1,
However, it is difficult to directly test this hypothesis, so most evidence relating to it
comes from laboratory experiments using model eukaryotes. For example a hypothesis
h2 is about C. elegans:
IS-A
HAS-QUALITY -
R
( . , )
( , )
C elegans organism
organism life span
?
?
EGULATES( - ).SIRT life span2,
and a hypothesis h3 is about S. cere?isiae:
IS-A  
HAS-QUALITY -
( . , )
( ,
S cerevisiae organism
organism life spa
?
n
SIRT life span
)
,
?
REGULATES( - ).2
The evidence about h2 and h3 is then related to h1 by probabilistic reasoning (homo-
logical inference):
HAS-PROBABILITY( ) HAS-PROBABILITY( )
HAS-PROBABIL
h p h p2 2 1 12, ,o
ITY( ) HAS-PROBABILITY( ).h p h p3 3 1 13, ,o
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S7
http://www.jbiomedsem.com/content/4/S1/S7
Page 7 of 12
SIR2 is the S. cere?isiae homolog of SIRT1. The research hypothesis h3 SIR2 regu-
lates yeast life span is very well supported by the scientific literature. The dataset
yeast70 is a subset of Gene Ways 7.0 database [26]. The Gene Ways 7.0 database was
produced through automated analysis of 368,331 full-text research articles and
8,039,972 article abstracts from the PubMed database, using the GeneWays system.
The database covers a wide spectrum of molecular interactions, such as bind, phos-
phorylate, glycosylate, and activate (nearly 500 relations in total). The dataset yeast70
has 1,135 sentences containing the keyword aging and yeast gene names, 492 of
them contain SIR2, the gene SIR1 is not mentioned, and SIR3 is mentioned 42 times.
Examining these papers suggests that the probability of h2 is close to 1.0.
The probability of scientific hypotheses changes with new evidence, and we wish to
use HELO to represent this. For example Burnett et al. results directly decreased the
probability of the hypotheses regarding the function of the SIRT1 homologs in Caenor-
habditis elegans and Drosophila melanogaster, and these indirectly decreased the prob-
ability of h1. The situation changed again in 2012 with Kanfi et al. where the evidence
directly increased the probability of the hypotheses regarding the function of the
SIRT1 homolog in Mus musculus, and this indirectly increased the probability of h1
(see Fig.1).
Of course the weight of the evidence in these papers on h1 depends on a host of fac-
tors other than simply the model species involved: the amount and variety of evidence,
its statistical confidence, the lab where the work was done, the publisher, etc. Taking
all these into account an expert estimate of the probability that h1 is held after the
publication of the paper [22] is 0.8 (see Fig.1). It should be noted that the exact prob-
ability of h1, say 0.8 or 0.82, is not that critical. What is important is the ball-park
figure, and the direction of change with new evidence. Our idea is that addition of
more and more evidence and inferences to the argument constrain the probabilities to
reasonable numbers. It is our contention that all human scientists make such implicit
inferences and much is to be gained by making them explicit. In addition these prob-
ability can be used for further automated inference and experimentation.
Experiments with a Sir2 deletant strain run within the Robot Scientists project
showed no difference between the wild type, while yeast strains with NAD+ grew to a
significantly higher biomass than the wild type. The experiments demonstrate that Sir2
functions differently from other NAD+ genes, and this indirectly supports the hypoth-
esis h1. It is clear that further experimentation is required to accept or reject the
hypothesis h1.
Active learning for drug discovery
This example demonstrates the recording of probabilities in drug discovery experi-
ments. The goal of these experiments was to find the best compound (with respect to
a biomedical assay, e.g. for treating cancer) without having to test all the compounds
against the assay. This involved learning quantitative structure activity relationships
(QSARs). These are functions that take as input the structure of a compound and out-
put an estimate of how well the compound will perform in a biomedical assay. The
investigation was computational and used existing assay results.
The task of finding the best instance (e.g. compounds, parameters) as evaluated on
an unknown target function (e.g. high biological activity, minimal costs) using limited
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S7
http://www.jbiomedsem.com/content/4/S1/S7
Page 8 of 12
resources (e.g. time) is important to many scientific disciplines. In drug discovery it is
not sufficient to find just a single best compound or lead as several leads improve
the chances of finding a compound that passes toxicology tests. The challenge there-
fore is to identify the k best performing instances (= compounds in this context) using
as few experiments as possible. We refer to this task as active k-optimization.
We applied machine learning to solve the active k-optimization task and to propose
the best candidates for screening [27]. We considered several selection strategies for
the best instances: Cox and Johns lower confidence bound criterion [28] (we refer to
it as the optimistic strategy), the most probable improvement (MPI) of the current
solution strategy [29], the maximum expected improvement (MEI) strategy, and also
the random choice (see [27] for more detail).
These strategies were evaluated on the US National Cancer Institute 60 anticancer
drug screen (NCI60) dataset [30]. This repository contains measurements of the inhibi-
tory power of tens of thousands of chemical compounds against 59 different cancer
cell lines (one of the originally 60 cell lines was evicted because it was essentially a
replicate of another one [31]). NCI reports the negative log-concentration required for
50% cancer cell growth inhibition (pGI50) as well as cytostatic and cytotoxic effect
measures, but we only used the pGI50.
The goal is to find compounds in a library that have a high pGI50, and to do so using
as few pGI50 measurements as possible. The program bootstraps by selecting 10 ran-
dom compounds and measuring their pGI50. In each subsequent step, a current QSAR
model is fitted to all available pGI50 values. The model is used to predict the pGI50 for
all remaining (untested) compounds in the library. The model is a Gaussian process,
which outputs a (Normal) distribution for the pGI50 value rather than only a point pre-
diction. This enables the implementation of the previously listed strategies. For exam-
ple, for the MPI strategy, one computes the probability that a compound has a pGI50
which is larger than the current k-th best one. The compound with the highest prob-
ability is selected for the next measurement of pGI50.
The table in Figure 3 illustrates MPI for a particular cell line 786-0, for a specific
bootstrap, and for k = 1. The first column of the table shows the number of known
pGI50 values at that time. P1 is the probability, given the current evidence, that a parti-
cular compound NSC 642567 will have a pGI50 better than the best bootstrap com-
pound. The subsequent column shows what is the probability P2 that NSC 642567 has
a better pGI50 than the current best value. The third column shows the highest such
probability P3 for any of the compounds remaining in the library.
Each computational experiment was repeated 20 times and the results were averaged.
Overall, on the NCI60 datasets, the optimistic strategy was most robust. In all situa-
tions considered, it performed either best or not significantly worse than the best strat-
egy (see [27] for detail and diagrams). The performance of MPI is competitive for
medium experimental budgets, but it may fail to find more than one good compound
when constrained to low budgets, and it does not optimally exploit high budgets. MEI
is a very good strategy when about 10 compounds are needed. The random selection
strategy performs worse than all other selection methods in all settings. Actively choos-
ing compounds substantially speeds up the finding of the compounds with high pGI50.
HELO enables the recording of these important results in a semantically defined way.
The following semantic descriptors are required for the reporting of this study:
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S7
http://www.jbiomedsem.com/content/4/S1/S7
Page 9 of 12
Gaussian distribution, zero mean, variance, prior belief, posterior probability, random
variable, likelihood, estimated probability. HELO contains exact matching terms or
equivalent synonyms of the required semantic descriptors, for example HAS-VALUE
(mean, 0) is equivalent to zero mean.
Conclusion
Scientific knowledge is inherently uncertain. There is therefore a need for a representa-
tion that focuses on the probabilistic features of research statements, and supports prob-
abilistic reasoning. In order to address this need we proposed the ontology HELO that
supports probabilistic reasoning over uncertain scientific statements. HELO defines a
hierarchy of typical research statements and links them to their associated probabilities,
and methods of obtaining those probabilities. We demonstrated HELO on the represen-
tation of scientific belief that sirtuins regulate organism life span, and regarding deleted
genes in the S. cere?isiae aromatic amino acid pathway. In both cases the probability of
research statements changed with new evidence, and it is clearly important to employ
the most updated probability estimate for making decisions about research involving
these genes. The active learning for drug discovery study is based on operations with
probabilities. The probabilities of having a high pGI50 were iteratively computed for all
compounds in the library, and the best compounds were chosen for further study.
HELO enables accurate recording of supporting and refuting evidence of research state-
ments, and how they participate in the process of updating probability values.
Figure 3 The probabilities that the selected compounds have high GI50
Soldatova et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S7
http://www.jbiomedsem.com/content/4/S1/S7
Page 10 of 12
HELO is specifically designed to support the cycles of automatic scientific discovery
that incorporate text mining, machine learning, robotic automation, and knowledge
representation, and may be of use for other of research that involve probabilistic
reasoning.
Authors contributions
LNS originated the idea of ontological representation of the key entities of the theory
of probability and worked on the ontology HELO. AR originated the idea of annotating
research statements extracted from natural language text with semantic descriptors
indicating the level of truthfulness of those statements, and also the recording of com-
peting and contradictory statements. KDeG applied HELO for the reporting on the
active learning for high throughput screening study. RDK originated the idea of using
probabilities of hypotheses for the choice of experiments in automated experimenta-
tion. He also contributed to the development of HELO and other worked examples.
Acknowledgements
This work was partially supported by grant BB/F008228/1 from the UK Biotechnology & Biological Sciences Research
Council, from the European Commission under the FP7 Collaborative Programme, UNICELLSYS, KU Leuven GOA/08/
008 and ERC Starting Grant 240186.
Declarations
The publication costs for this article were funded by the corresponding authors institution, Brunei University, London.
This article has been published as part of Journal of Biomedical Semantics Volume 4 Supplement 1, 2013: Proceedings
of the Bio-Ontologies Special Interest Group 2012. The full contents of the supplement are available online at http://
www.jbiomedsem.com/supplements/4/S1
Author details
1Department of Information Systems and Computing, Brunel University, London, UK. 2Department of Medicine &
Department of Human Genetics, the University of Chicago, US. 3Department of Computer Science, KU Leuven,
Belgium. 4Manchester Institute of Biotechnology, the University of Manchester, UK.
Published: 15 April 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Pérez et al. Journal of Biomedical Semantics 2013, 4:12
http://www.jbiomedsem.com/content/4/1/12
RESEARCH Open Access
BioUSeR: a semantic-based tool for retrieving
Life Science web resources driven by text-rich
user requirements
María Pérez1*, Rafael Berlanga2, Ismael Sanz1 and María José Aramburu1
Abstract
Background: Open metadata registries are a fundamental tool for researchers in the Life Sciences trying to locate
resources. While most current registries assume that resources are annotated with well-structured metadata, evidence
shows that most of the resource annotations simply consists of informal free text. This reality must be taken into
account in order to develop effective techniques for resource discovery in Life Sciences.
Results: BioUSeR is a semantic-based tool aimed at retrieving Life Sciences resources described in free text. The
retrieval process is driven by the user requirements, which consist of a target task and a set of facets of interest, both
expressed in free text. BioUSeR is able to effectively exploit the available textual descriptions to find relevant resources
by using semantic-aware techniques.
Conclusions: BioUSeR overcomes the limitations of the current registries thanks to: (i) rich specification of user
information needs, (ii) use of semantics to manage textual descriptions, (iii) retrieval and ranking of resources based on
user requirements.
Keywords: Resources discovery, Semantic annotation, Information retrieval, Life science
Background
In this section we introduce the context of our work and
our motivation. Then, we summarize the related work and
we present the rationale of our proposal.
Introduction andmotivation
In recent years, the research activity of the Life Sciences
community has produced a huge amount of data as well
as many resources and tools to manage it. Nowadays, the
success of many research tasks in the Life Science depends
on the integration of the proper resources and tools which
can be accessed through the Internet. As an example task,
let us consider the combination of DNA sequencing with
reference databases available on the web [1], which is fol-
lowed by complex analysis workflows that rely on highly
specific algorithms, often available as web services [2]. In
this scenario the amount of data produced and consumed
*Correspondence: mcatalan@uji.es
1Department of Computer Science and Engineering, Universitat Jaume I,
Castellón, Spain
Full list of author information is available at the end of the article
is prodigious, and the sheer amount of available resources
to manage research data is a source of severe difficulties.
In this work, we consider web resources as any appli-
cation, information source, service or site that can be
identified or handled in theWeb and which provides func-
tional and processable metadata about its functionality
and features.
A web resource registry is a repository in which
providers register their resources (e.g., web services,
datasets and so on) with the aim that other users can
discover and use them. As a result of different research
efforts, currently there are many registries with resources
related to Life Sciences. Table 1 shows the compari-
son of some of the most frequently used ones. This
comparison is based on how users specify their require-
ments, the type of search, the use of semantics in the
discovery process and functionalities related to resource
composition.
Most of them provide search based on keywords or fil-
ters, which implies that users have to know the vocabulary
© 2013 Pérez et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 2 of 15
http://www.jbiomedsem.com/content/4/1/12
Table 1 Comparative table of registries of web resources in Life Sciences
Registry User Discovery Semantics Composition
requirements
Feta [3] Keywords Input, output, Manually N/A
from ontology operation type,
task
BioMoby [4] Keywords Resource type, I/O Resource type, N/A
object type
EMBRACE [5] Keywords String matching Syntactically annotated N/A
with BioXSD
BioCatalogue [6] Keywords String matching, Categories, some tags N/A
categories, filters
SSWAP [7] Keywords, RDF Third-party ontologies, N/A
Resource reasoning
Query Graph
Magallanes [8] Keywords String matching in N/A Yes
data type,
resource type
myExperiment Keywords String matching, Tags N/A
[9] filters
Taverna [10] Keywords String matching, BioMoby metadata Workflow
ontology concepts composition
SADI [11] SPARQL RDF Third-party ontologies Yes
This comparative table presents the main characteristics of the most popular registries in Life Sciences.
used to describe the web resources and,moreover, the suc-
cess of the search depends on the available information
about the resource.
Another limitation is that most of the registries assume
the availability of well-defined metadata about the fea-
tures of the resources, e.g., input and output data types.
However, in open registries relevant information about
the features of a resource (e.g., input/output data types,
method and species involved) is usually described in the
textual description and, therefore, it is not expressed as
relevant metadata.
In this paper we present BioUSeR (Bioinformatics User-
driven discovery of Semantically-enriched Resources), a
tool to assist the researcher in the discovery of the most
suitable resources to her information needs and that
overcomes the limitations presented above by: (i) allow-
ing the user to provide a text-rich specification of her
requirements including the target task and important fea-
tures of the resources, (ii) exploiting text-rich descriptions
to discover and classify relevant information about the
resources to better characterize them and use this infor-
mation as facets for the search, (iii) using semantic anno-
tation to allow mappings between information written in
free text.
Related work
Next, we provide a brief description of the features of
current web resource registries in the Life Sciences.
Feta [3] is a faceted retrieval system for Life Sciences
resources in which the user queries are based on the input
and output data types, the method or type of an operation,
or a phrase contained in the description of an operation.
Feta requires that resources have to be manually anno-
tated with the myGrid ontology, and then, searches must
be also based on this ontology. In this work, ranking is not
applied because they argue the registry is very small.
BioMoby [4] is an open-source research project whose
aim is to implement a web-resources registry to facilitate
the discovery and sharing of biological data. Resources are
registered in MOBY Central by using a model that allows
search and retrieval based on object and resource hierar-
chies. Users may request a search for available resources
based on their input, output, resource type or authority by
using keywords.
EMBRACE Resource Registry [5] is a Life Sciences
web resources registry with built-in resource testing.
Resources are syntactically annotated using BioXSD
[12]. The search is based on the string matching of
keywords. This registry is the prelude to BioCatalogue.
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 3 of 15
http://www.jbiomedsem.com/content/4/1/12
BioCatalogue [6] is a Life Sciences registry that pro-
vides a common interface for registering, browsing and
annotating Life Sciences web resources. Web resources in
BioCatalogue can be annotated with categories, tags and
descriptions. These annotations are manually provided by
the resource providers and the user community plus some
monitoring and usage analysis data obtained automati-
cally by BioCatalogue servers. However, at the moment,
most of these annotations are expressed as free text with-
out following any controlled vocabulary. The resource
discovery is mainly based on both keyword search and fil-
tering mechanisms. Filters can be applied over: resource
type, provider, submitter and country. To enhance its
accessibility and usability, BioCatalogue is indexed by
search engines such as GoogleTM. It also provides a pro-
grammable API which is used by third-party applications
such as Taverna [10].
SSWAP [7] proposes an architecture, a protocol and a
platform to semantically discover and integrate heteroge-
neous disparate resources on the web. Unfortunately, this
approach heavily relies on the provided metadata, which
is usually poorly described. SSWAP provides two types
of searches: keyword search and resource query graph.
The keyword search presents the problems related with
the selection of keywords and the lack of useful meta-
data. Resource graph query requires training to learn how
to build the graph and how to express the queries with
this format, which means a high effort for those users not
familiar with these technologies.
Magallanes [8] is a library of algorithms aimed at discov-
ering bioinformatics web resources. The search is based
on a GoogleTM-like approach, in which the user keywords
are matched to metadata descriptions improved by the
Did you mean...? algorithm which helps the user to build
the query. Search can be performed on data type, resource
and resource type fields and it is improved by a learn-
ing process from users feedback. Moreover, Magallanes
provides a way of composing compatible resources into
workflows.
myExperiment [9] is a Life Science repository whose
main resources are workflows but other research objects
can also be registered in it. It has been developed in
the same project as BioCatalogue. The workflows can be
annotated with tags, a description, object type and other
information about the provider like the country, etc. The
search is based on keywords and filters over the previous
fields. myExperiment provides also information about the
popularity of the resource, such as the times the resource
has been viewed or downloaded, and a rating scale that
reports about the quality of the resource.
Taverna [10] is a workflow construction environment
and execution engine designed to support in silico experi-
ments developed by the European Bioinformatics Institute
(EBI) and University of Manchester. Taverna is part of
myGrid project and so is aligned to BioCatalogue and
myExperiment. It is able to build complex workflows, to
execute them and to display the different types of results.
The user selects the resources with a keyword search.
Taverna contains the BioMoby resources and, therefore,
the input and output data types are well defined. Other
resources can be imported to Taverna.
SADI [11] framework is a registry that uses standard-
compliant Semantic Web Resource design patterns that
simplify the publication of resources and their sub-
sequent discovery in domains such as bioinformatics.
Providers have to follow SADI conventions to pub-
lish their resources, and users have to create SPARQL
queries in order to discover the desired resources, which
implies that users have to know the SPARQL query
language, which supposes an extra effort for exam-
ple for biologists, that may not be experts in these
technologies.
Rationale
From the related work presented above, we can conclude
that these registries limit the users in the specification of
their requirements since they have to use specific key-
words usually expressed in an application ontology like
myGrid or create queries in specific query languages such
as SPARQL. Moreover, most of them base the discov-
ery on specific metadata such as input and output data
types assuming them available but that rarely appear. As
the above text shows, open-metadata registries (e.g., Bio-
Catalogue and SSWAP) hardly provide rich metadata as
stand-alone applications do (e.g., Magallanes). It is also
worth noting that none of them exploits the descrip-
tion of the resources in order to find relevant metadata
of the resources, with the consequent loss of relevant
information.
To solve these limitations, there are some key aspects
that must be addressed: the non-intuitive specification of
the users requirements in current registries, the under-
use of available standards in the description of web
resources, and the lack of automatic mechanisms to deter-
mine the degree of suitability of the discovered resources.
As it is shown in this paper, BioUSeR addresses these
aspects by using both semantic annotations as a nor-
malization process consisting in the association of for-
mal knowledge to the available text descriptions, and
information extraction techniques in order to obtain
information about the facets from the annotated task
descriptions.
Results and discussion
In this section we present BioUSeR, a prototype that has
been developed to show the usefulness of our approach,
and we demonstrate its usefulness with a case study. Then,
we evaluate it and, finally, we discuss the results.
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 4 of 15
http://www.jbiomedsem.com/content/4/1/12
BioUSeR
We have developed a prototype called BioUSeR (Bioin-
formatics User-driven discovery of Semantically-enriched
Resources) that assists users of web resource registries in
each step of the retrieval process, from the requirements
specification until the resource selection. This prototype
is user-oriented and one of its main characteristics is that
it allows the user to configure all the process in an easy
and intuitive way.
The current prototype is focused on the Bioinformatics
domain and we have selected BioCatalogue as the refer-
ence web resource catalogue. However, other catalogues
(e.g., SSWAP) can be easily integrated by only register-
ing the information of the resources in our repository.
BioCatalogue contains 2081 registered resources (as of
November 2011). Although some resources are described
through a set of predefined categories, most of them
have no metadata and just provide a free text description
and/or the web resource documentation. Some resources
do not provide any kind of information but just the URL
to their web sites. For these cases, we have downloaded
the web site main pages and used them as the resource
descriptions. We remark that these limitations motivate
the use of our approach.
The retrieval process in BioUSeR is divided in three
phases, as shown in Figure 1: (i) requirements specifi-
cation, (ii) normalization and facets extraction and (iii)
resource retrieval and selection. Next, we present a case
study to show the results of each one of these phases.
Case study
To demonstrate the usefulness of BioUSeR, we use it to
develop a Life Science case study extracted from [13].
The case study concerns biological research that analyzes
the presence of specific genes involved in the genesis
of Parkinsons disease, called LRRK2 genes, in different
organisms. Next, each step of the process is explained with
a brief description and a snapshot of BioUSeR.
1. Requirements specification. The user needs to
compare specific genes in different organisms as part
of a study of the presence of the LRRK2 genes in the
organism N .Vectensis, since previous studies have
shown that this is a key organism to trace the origin
of these genes. With BioUSeR, the users query would
be a requirements specification consisting of the goal
specific genes in different organisms to be compared
plus a set of tasks specified in a requirements model
by using natural language descriptions. In this case,
the user defines five tasks in order to achieve the
main goal: (i) to search similar sequences given a
protein sequence, (ii) to predict the gene structure;
(iii) to align protein sequences; (iv) to build a
phylogenetic tree; and (v) to analyze the domains in
protein sequences. Figure 2 shows a fragment of the
requirements model specified by the user in which
the tasks predict gene structure, align protein
sequences ,build phylogenetic trees and analyze
domains given a protein sequence are shown.
This case study also shows that in the requirements
specification the value of the facets can be: (i)
implicitly described in the task, e.g. search similar
sequences given a protein sequence, (ii) determined
directly as the value of the facet as happens in the
output of the task build phylogenetic tree as it is
shown in Figure 3, or (iii) implicitly determined by
the dependencies between tasks, e.g., the task build
phylogenetic trees has as input the data generated
by the task align protein sequences.
2. Normalization and facets extraction. Once the
user has completed her requirements specification,
the next step is to normalize and analyze it. First, the
descriptions of the tasks are semantically annotated
with the unified knowledge resources (KR) and
BioUSeR provides the user with the concepts of the
annotations. Then, the user can reject those that are
not appropriate due to ambiguity or to errors of the
annotator. Moreover, BioUSeR also allows the user to
select the ancestors or descendants of these concepts.
Finally, the requirements specification is translated
into a semantic vector that contains the selected
concepts. In Figure 3, the Annotations section shows
the annotations of the task build phylogenetic trees
and the selection dialogue prompted to the user.
Additionally, from the annotated task descriptions,
information about the facets is extracted by using the
information extraction patterns shown in Table 2. In
the task search a similar sequence given a protein
sequence, the input type protein sequence is
implicitly described and it is extracted by the
extraction pattern given noun-phrase.
Figure 1 Overview of the proposed approach. The approach is divided on three phases: requirements specification, normalization and web
resource discovery.
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 5 of 15
http://www.jbiomedsem.com/content/4/1/12
Figure 2 Requirements model. This figure shows the requirements model defined by a user who wants to compare specific genes in different
organisms.
Figure 3 Information of a normalized task. This figure shows the information of the user-defined tasks once they have been normalized. For each
task, it shows the facets values, the semantic annotations and the selected resource.
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 6 of 15
http://www.jbiomedsem.com/content/4/1/12
Table 2 Example of extraction patterns for facets
Facet NP R
Input Inputs? (is|are)
- Given
Taking
(((of)? E)+ | it) gets
Output Outputs? (is|are)
(((of)? E)+|it) Constructs
Finds
Retrieves
Calculates
Contains
Produces
Extracts
Returns
Method (((of)? E)+ | it) Maps
Executes
Performs
Implements
Applies
Applying
Runs
Running
Based on
Computes
Carries out
Processes
It shows some of the extraction patterns specified for the facets: input, output
and method.
3. Resources retrieval and selection. The retrieval of
the most appropriate resources is carried out by a
semantic mapping between the semantic vectors of
the requirements specification and the semantic
vectors of the resources.
At the end, the user gets a ranked list of resources for
each task and can visualize metadata about each
resource which helps her in the selection of the most
appropriate ones. The Resource section of Figure 3,
the first ranked resource is selected by default, but
the user can select any other resource by pressing the
Choose button. Table 3 shows the information for
each task, the defined facets and the resources
selected. For the task build phylogenetic tree, the
selected resource is the fifth ranked resource, but it is
the one that best fulfils the task and the facets
required by the user.
BioUSeR assists and is assisted by the user during the
whole retrieval process, from the requirements specifi-
cation until the resources selection, taking advantage of
her knowledge and expertise on the field, and provid-
ing her with useful information like annotation concepts,
resources metadata and ranked lists of resources. We
want to remark that the user guides each step: select-
ing the appropriate concepts, choosing the most suitable
resources or even redefining the initial requirements.
Moreover, the user can also specify additional features of
the desired resources. The use of facets improves the suit-
ability of the results since the final list is ranked according
to the task and to the facets.
Evaluation
In this section, we first evaluate the effectiveness of the
discovery system and, then, we make a more specific anal-
ysis of the facets extraction method. Finally, we further
Table 3 Results for the case study
Task Facets Resource Rank
Retrieve gene sequence Input: gene getColiCardIDs_by_ 1
Output: sequence InteractingPartnersResource
Search similar sequences Input: sequence Database of Protein 1
Output: blast report Subcellular Localization Resource
Predict gene structure Input: gene GlimmerResource 1
Output: gene model
Align protein sequences Input: protein sequence T-Coffee 1
Output: sequence alignment
Build phylogenetic trees Input: sequence alignment INB:www.bioinfo.uma.es: 5
Output: phylogenetic tree runCreateTreeFromClustalw
Analyze domains Input: protein sequence INB:inb.bsc.es:parseRulesFromMotif 1
This table shows the selected resources for each user-defined task.
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 7 of 15
http://www.jbiomedsem.com/content/4/1/12
evaluate our system by comparing it to BioCatalogue, one
of the most popular open registries in Life Science.
BioUSeR evaluation
The evaluation of BioUSeR has been carried out by exe-
cuting a set of heterogeneous queries (i.e. task description
examples) that captures different ways to describe bioin-
formatics tasks, thus reflecting the variability in the users
information needs. The query pool [14] has been created
by selecting more than 250 short descriptions extracted
from other Life Sciences resource catalogues such as
OBRC [15] and ExPaSy [16].
These queries have been evaluated over a gold standard
(GS) due to the difficulties to determine the whole set
of relevant results for each query. The GS [17] has been
built with 443 resources (out of 2081 registered resources),
but only for seven base tasks that can be unambiguously
related to BioCatalogue categories. Moreover, we have
manually revised it in order to ensure the quality of the
final set.
Table 4 shows the precision, the recall and the F-
measure of the results obtained by executing the queries
from the query pool. These results show that the top-
ranked results are, in most cases, appropriate for the users
requirement and, moreover, the recall shows that most of
the relevant resources are provided to the user.
Moreover, we have also evaluated the use of semantics
in the normalization process in order to know how the
semantic annotations improve the search. To that end, we
have evaluated the results of the queries from the query
pool without semantic annotations, that is, the retrieval
is based on words and not in concepts. The precision is
in average 32% and the recall is in average 38%. There-
fore, we can conclude that semantic annotations improve
significantly the web resources discovery.
Facets extraction evaluation
In order to evaluate the quality of extracted facets, we have
set up a GS data set with information about the facets
of the resources registered in BioCatalogue. BioCatalogue
Table 5 Facets gold standard
Facet Tags Resources
Input 52 48
Output 47 48
Method 135 434
Disease 7 5
Species 27 61
This table shows for each facet: the number of BioCatalogue tags in the facets
gold standard (GS) and the number of resources that are tagged with them.
allows users to assign tags to resources in order to describe
some aspects of them. Currently there are 855 tags for
describing 2081 web resources. This GS has been built
as follows. For input/output facets, we have automatically
selected the tags assigned to the input/output descrip-
tions. For the other facets, we have manually classified the
tags into method, species and disease facets. A summary
of the number of tags and involved resources for each facet
is shown in Table 5. Notice the low number of resources
having tags for the input/output descriptions in BioCata-
logue, which confirms the lack of processable metadata in
this kind of open registries.
Table 6 presents the number of concepts that have been
automatically extracted for each facet by using extraction
patterns and semantic types, and the number of resources
that are annotated with these concepts.
The facet method is the most utilized by users for
describing resources, whereas the disease facet is seldom
described via tags. However, our tool detects a greater
number of values for these facets, although it works worst
for the method facet. The latter issue is due to the poor
coverage of the reference ontologies with respect to bioin-
formatics algorithms and methods.
We have also evaluated the precision, recall and F-
measure of the input, output and method extracted facets
with respect to the GS and the results are shown in
Table 6.We have not evaluated these measures for the dis-
ease and species facets due to their poor representation in
the GS.
Table 4 BioUSeR evaluation
Base task P@5 P@10 P@20 P R F
Search proteins with a functional domain 0.79 0.76 0.73 0.45 0.63 0.53
Search similar sequences 0.91 0.85 0.77 0.22 0.72 0.33
Analyze transgenic model organism 0.93 0.94 0.89 0.58 0.89 0.7
Find genes with functional relationships 0.78 0.75 0.66 0.34 0.39 0.36
Predict structure 0.91 0.91 0.85 0.47 0.29 0.36
Analyze phylogeny 0.8 0.8 0.79 0.52 0.36 0.43
Align sequences 0.73 0.76 0.74 0.62 0.3 0.41
This table shows the precision, recall and F-measure of the results obtained for the queries of the query pool, associated to the 7 base tasks of the gold standard (GS).
It also includes the precision for the top-5, top-10 and top-20 results.
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 8 of 15
http://www.jbiomedsem.com/content/4/1/12
Table 6 Facets extraction evaluation
Facet Concepts Resources Precision Recall F-measure
Input 259 399 0.69 0.93 0.73
Output 266 274 0.6 0.94 0.64
Method 136 210 0.44 0.53 0.35
Disease 142 144 - - -
Species 292 287 - - -
This table shows the number of concepts that our approach has automatically
extracted for each facet and the number of resources that are annotated with
those concepts. Moreover, for the input/output and method facets the
precision, recall and F measures are shown. These measures have been
calculated for the automatically extracted information with respect to the GS.
The disease and species facets have not been evaluated because of their poor
representation in the GS.
Comparisonwith BioCatalogue
With the aim of validating our approach, we compare it
with the BioCatalogue search engine. We have selected
BioCatalogue for several reasons: first, nowadays it is
one of the most popular open registries in Life Sci-
ence; second, BioUSeR has been evaluated with a GS set
up with the resources registered in BioCatalogue and,
third, because BioCatalogue provides an API that allows
users to query it programmatically. BioCatalogue provides
two types of search: (i) keyword-based search and (ii)
navigational-based search using categories. Each type of
search has been evaluated separately. In both cases, the
results have been evaluated using the GS described above.
Next, we describe with more details each evaluation.
Keyword search is based on string matching techniques
that use all the information available about the resources.
This type of search supposes an extra effort to the user
since she has to summarize her informational needs in a
set of words and these words have to make a complete
matching with the words in the resource information. For
instance, the query metabolic pathways does not retrieve
any resource, however its singular form metabolic path-
way retrieves some resources. Table 7 shows the precision,
recall and F-measure of the results obtained by manually
built keyword queries that try to express the informa-
tional needs described in the requirements. This table
also shows the cost of edition, that is, the average num-
ber of failed queries we have executed before getting
some results, which is in average 2.89, and the number
of keywords per query, which is in average 2.94. Con-
sidering the precision and the recall, keyword queries do
not provide good results considering users requirements.
Our approach presents better precision and recall, that
is, it retrieves more relevant results, moreover, without
transforming the original requirements.
Navigational search allows the user to navigate through
the BioCatalogue taxonomy of categories, i.e., the most
common bioinformatics tasks. When the user selects
a category, BioCatalogue filters the resources that are
tagged with that category. BioCatalogue allows to select
several categories, but it does not combine navigational
search with keyword search. An important limitation of
this search is that it does not retrieve those resources that
are not categorized, even when the category appears in
their description expressed in natural language. Another
limitation is the broadness of the categories, which does
not allow the user to express more specific tasks. To evalu-
ate the navigational search, we have manually selected the
most suitable categories for each query in the query pool.
Table 8 shows the precision, recall and the F-measure of
the results, and the cost of edition of the queries. In this
type of search, the cost of edition is represented by the
depth of the category in the taxonomy and the number
of siblings of the selected category, describing in this way
the steps required to select the most appropriate category.
Table 7 BioCatalogue keyword search evaluation
Task P@5 P@10 P@20 P R F Edition Keywords
cost
Search proteins with 0.4 0.41 0.41 0.41 0.02 0.04 2.45 3.4
a functional domain
Search similar sequences 0.4 0.4 0.4 0.36 0.07 0.12 2.87 3.8
Analyze transgenic 0.74 0.71 0.71 0.71 0.17 0.27 3.25 2.94
model organism
Find genes with 0.27 0.26 0.27 0.26 0.04 0.07 3.15 2.13
functional relationships
Predict structure 0.67 0.66 0.65 0.64 0.04 0.07 3.27 2.93
Analyze phylogeny 0.18 0.2 0.2 0.18 0.01 0.02 2.8 2.56
Align sequences 0.72 0.72 0.75 0.69 0.07 0.13 2.48 4.16
This table shows the evaluation of the keyword-based search of BioCatalogue. The evaluation has been made using the queries in our GS and precision, recall and F
measures have been calculated for each topic. The edition cost is the cost of translating the requirements into keywords queries, which corresponds to the number of
failed queries executed before getting results. Finally, keywords is the average number of keywords of the successful queries.
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 9 of 15
http://www.jbiomedsem.com/content/4/1/12
Table 8 BioCatalogue navigational search evaluation
Task P@5 P@10 P@20 P R F Edition cost
Search proteins with a functional domain 0.92 0.92 0.82 0.75 0.15 0.25 2.67/3.3
Search similar sequences 1 1 1 1 0.3 0.46 2.0/4.25
Analyze transgenic model organism 0.8 0.9 0.95 0.94 0.4 0.56 0.03/10.77
Find genes with functional relationships 0.91 0.95 0.89 0.89 0.26 0.4 1.0/3.0
Predict structure 0.87 0.93 0.96 0.9 0.1 0.18 2.29/3.42
Analyze phylogeny 0.8 0.88 0.88 0.88 0.03 0.06 0.0/11.0
Align sequences 0.98 0.99 0.99 0.99 0.06 0.11 2.94/2.1
This table shows the evaluation of the BioCatalogue navigational search based on categories. The evaluation has been made using the queries in our GS and the
average precision, recall and F-measure have been calculated for each topic. The cost of edition of the queries is represented by the depth of the category in the
taxonomy of categories and the number of siblings of the selected category.
The higher the depth, the more specific the category is.
On average, the precision is high but it is not possible
to know if the retrieved results perform the specific task
described in the requirement. Our approach presents a
lower precision but a higher recall, that is, it retrieves
relevant resources that the navigational search does not
retrieve, e.g., those that are not categorized. Moreover,
our approach retrieves resources that perform the specific
tasks described in the requirements, which is not possible
with the navigational search.
Another important limitation of both types of search
is that they do not provide a ranked list, so the user has
to manually check all the results. Nevertheless, BioUSeR
provides the user with a ranked list of resources depend-
ing on their suitability to the requirement.
Regarding facets, BioCatalogue allows the user to search
by introducing input or output data examples, retrieving
those resources that require or produce these data. How-
ever, they do not combine this search with the others and,
therefore, the user cannot specify which task she wants to
perform. In BioUSeR, the user can describe the required
functionality and information about the facets in the same
query.
We can conclude that our approach improves BioCat-
alogue search engine by using natural language queries,
which describe the task and the features of the resources,
avoiding the selection of keywords or general categories
that do not describe specific tasks. Moreover, the seman-
tic annotation addresses the problem of using different
vocabularies or string mismatchings.
Discussion
Most of the current registries base the discovery of
resources on keywords or concepts coming from their
own ontologies that describe the tasks or the input
and output data types. There are approaches that use
string matching techniques with all the available infor-
mation, and others that are based only on the metadata
of the resources. The former assume that users know
the vocabulary with which the resources are described,
since they have to specify the correct keywords. The lat-
ter do not take into account all the information available
in the resource descriptions and documentation which
are expressed in natural language, so they assume that
resources are provided with useful metadata and, as we
have mentioned before, this does not happen in current
open-metadata registries for Life Sciences.
Our approach allows the user to specify her information
needs as rich textual descriptions.While current registries
do not allow the user to combine in the same query infor-
mation about the task and the features of the resources, in
BioUSeR the user can provide a description of the func-
tional tasks she needs to be executed to achieve a goal
together with the set of relevant features that the retrieved
resources must have. Currently, BioUSeR supports the fol-
lowing facets: input and output data types, the method
and the disease and species involved, but a new facet can
be easily added by only determining its adequate informa-
tion extraction patterns and the involved semantic types.
Considering the features of the resources, we bring to our
tool the well-known advantages of using facets to restrict
the search and enhance the efficiency and precision of
current information retrieval systems [18].
As a result, BioUSeR makes possible that users without
any special training can specify, with rich textual descrip-
tions, all the features of the required solution for their
information needs. BioUSeR allows this kind of search
because of the normalization of data.
Moreover, most of the resources metadata are expressed
in textual descriptions and few well-defined metadata
are available on open registries. As many Life Sci-
ence researchers recognize, to manually describe their
resources by using standards is a very complex task that
usually produces incomplete and imprecise descriptions,
being this the reason for which almost always they pre-
fer to describe them by means of free texts with non-
standard vocabularies. In BioUSeR, the user requirements
specification and all the available resources metadata are
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 10 of 15
http://www.jbiomedsem.com/content/4/1/12
semantically annotated with widely accepted Life Science
ontologies such as UMLS andmyGrid. Additionally, infor-
mation extraction techniques [19] are used to identify in
the resources metadata relevant information about the
set of facets supported by the system. BioUSeR looks for
information about the facets in all the available metadata
of the resource, and not only on specific fields as most
current registries search engines do. The automatic nor-
malization enriches the system information with formal
knowledge and provides two main benefits to the system:
(i) it avoids the problem of the use of specific vocabular-
ies and (ii) it allows the system to exploit all the available
information of the web resources independently of the
characteristics of the metadata.
BioUSeR retrieves the resources by the semantic map-
ping of the normalized user requirement specification and
the normalized resources metadata, and provides the user
with a ranking of the retrieved resources, while current
registries only provide a list. Both the retrieval and the
ranking of resources are driven by the functional task
and the set of user-defined facets. Thus, for each task
described in the requirements specification, the system
prompts to the user a short ranked list of web resources
that could be used to execute it. Then, the user can eas-
ily select a resource for each task and to define a sequence
of resources that can be seen as a workflow specifica-
tion. In order to assist the user in the selection, the
available metadata of each resource can be visualized. As
the discovery process is a cyclic process, if some of the
retrieved resources are not considered adequate for the
user requirements, the user can modify the initial require-
ments specification so that alternative resources can be
explored.
Conclusions
In this paper we present BioUSeR, a tool that assists
researchers in Life Sciences in the discovery of the most
appropriate web resources for their well-defined require-
ments. With BioUSeR, users can easily find out web
resources that were previously unknown to them because
fell out of the scope of their main field of interest, or were
poorly categorized with existing tags.
BioUSeR assists all kinds of users from the requirements
specification until the selection of the most appropri-
ate resources, not only by allowing the customization of
the queries, but also by making the specification of the
information needs easier to non-expert users.
The main novelty of BioUSeR with respect to existing
registries is that it deals with text-rich descriptions of the
registered resources apart from the provided metadata.
For this purpose, BioUSeR applies automatic semantic
annotation and information extraction processes. As a
result, this tool automatically generates two kinds ofmeta-
data: semantic annotations and facet-value pairs. Thus,
BioUSeR aims to create metadata that are useful to fulfill
the user requirements, which are usually stated as free text
descriptions and facet-based requests.
Future work will be mainly focused on improving both
the annotation system and the extraction of facet-values.
The annotation system needs to be extended with new
knowledge resources containing specific Bioinformatics
algorithms and methods that are now poorly covered
by the selected ontologies. Also, it is necessary to treat
ambiguous annotations that can produce noise in the
retrieval of resources. As for the extraction of facets, an
automatic method to find out relevant patterns for a facet
should be designed. In this way, the definition of new
facets and its inclusion into the tool will be even eas-
ier. Another interesting issue for future work is to study
new methods for re-ranking retrieval results according
to the existing relations between tasks. The main aim of
this re-ranking is to improve the compatibility between
the retrieved resources of each task. Finally, our final aim
is to build an unified repository with existing ones (e.g.,
BioCatalogue, SSWAP, myExperiment and so on), and
integrate it with Taverna [20] through BioUSeR.
Methods
Our approach consists of three phases as depicted in
Figure 1. The main purpose of our method is to normalize
both the user requirements and the web resources meta-
data in order to compare them and to discover the web
resources that best match the users needs. In this section
we explain the methods and techniques applied at each
phase.
User requirements specification
Most current registries, as shown on Table 1, only pro-
vide keyword search or searching by filters. In this kind of
registry, the users find limitations when specifying their
requirements, e.g., the selection of the words to make the
search, the available information of specific fields and so
on. However, due to the experience and the knowledge
users have on their research fields, they can easily provide
natural language descriptions of their requirements and
the tasks that would bemanually performed tomeet them.
In our approach, we have adopted a hierarchical model
to specify user requirements in a formal way so that they
can be automatically used in the subsequent phases of the
resource retrieval process.
User requirements are represented by means of goals
and task elements in a formal specification called the
Requirements model. This requirements specification is
based on the i? formalism [21,22], which is both a goal-
oriented and an agent-oriented language. We use this
framework because it provides the functionality required
to obtain a formal specification of the users require-
ments without taking into account the characteristics of
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 11 of 15
http://www.jbiomedsem.com/content/4/1/12
the system. The goal and the task elements of the Strate-
gic Rationale (SR) model of the i* framework capture the
users information requirements and the steps to achieve
them. This model generalizes the work in [23] to allow the
specification of the user requirements in the context of
finding appropriate similarity measures for XML data.
To better describe the desired resources, the user can
specify additional features of the resources by determin-
ing values for the facets of interest of each task. A faceted
search system presents users with key-value metadata
that is used for query refinement. In our approach, we
propose a faceted search to discover the resources that
best cover the user-defined facets, more specifically: the
input and output types, the method, the diseases and the
species involved in the resources. It is worth noting that
the set of facets can be easily extended to cover other user
requirements.
Additionally, thanks to the hierarchical structure of the
Requirements model, in case a task has not explicitly
defined the input/output types, they can be automatically
set since they can be implicitly determined by the previ-
ous/next related tasks. In this way, the model is describing
the sequence of tasks that would execute the functionality
required by the user.
Normalization
User requirements can be easily described when the user
can express them in natural language, without the lim-
itation of using specific vocabularies as in most web
resources discovery approaches. Unfortunately, natural
language presents heterogeneity, ambiguity and implic-
itness issues, which make them hard to process auto-
matically. In our approach, we use automatic semantic
annotation (SA) to normalize textual descriptions of user
requirements and web resources with respect to a set of
reference knowledge resources.
SA can be seen as the process of linking the entitiesmen-
tioned in a text to their semantic descriptions, which are
usually stored in knowledge resources (KRs) such as the-
sauri and domain ontologies [24]. Former approaches to
SA were mainly guided by users (e.g., [25]) through seed
documents, manually tagged examples or ad hoc extrac-
tion patterns. However, in our scenario, we require that
the SA process is fully automatic and unsupervised. This
is because the volume of data to be processed is huge
and the set of possible user requirements is unknown a
priori. There are few approaches performing fully unsu-
pervised SA, and they are mainly based on dictionary
look-up methods or adhoc extraction patterns (see [26]
for a review of SA concepts and approaches).
Our SA process consists of three main steps. In the first
step, the KR is processed to generate a lexicon, which con-
tains lexical variants with which each concept is expressed
in the written texts. We denote the set of variants of a
concept C as lex(C). The second step consists of applying
some mapping function between the text chunks likely to
contain an entity and the KRs lexicon, in order to obtain
the list of concepts that are potentially associated. Notice
that entities usually appear in noun phrases, thus, the text
chunks to be considered are restricted to these syntactic
structures. Finally, in the third step, the concepts whose
lexical forms best fit to each text chunk are selected to
generate the corresponding semantic annotation.
Knowledge resources
As our method relies on the SA of both the user require-
ment specifications and the web resource metadata, we
need to establish the reference KRs from which con-
cepts are brought. Unfortunately, a unique comprehensive
ontology for this application domain does not exist, and
therefore we need to combine several existing resources.
For this purpose, we have selected as main KR the refer-
ence ontologies of BioCatalogue (i.e., myGrid ontologies)
and EDAM Ontology [27], that improves the annota-
tions of the myGrid ontologies. We have also used the
whole UMLS Meta-thesaurus (version 2010AA) to cover
the concepts about procedures, anatomy, diseases, pro-
teomics and genomics. Finally, in order to cover broadly
the names of the algorithms and methods involved in
Bioinformatics, we have included as concepts the entries
of the Wikipedia that have as category some sub-category
of the Bioinformatics category.
For tagging purposes, all these KRs are loosely inte-
grated into a concept repository which consists of an
inventory of concepts, their taxonomical relationships
(i.e., is_a relationship) and the lexical variants associated
with each concept (e.g., alternative labels, synonyms, and
so on) [28].
Normalization through semantic annotation
In order to reconcile the users requirements and the
resources, we need to normalize their representation
under a well-defined semantic space. This normalization
process involves the annotation of all the descriptions with
the concepts of the unified knowledge resource KR. The
purpose of the annotation process is to identify the best-
suited concepts for each description found in either web
resource metadata or user-requirements specification. To
achieve this, we have adopted the automatic annotation
method presented in [29], which was tested within the
CALBC competition [30]. As mentioned before, this pro-
cess consists of a mapping function between each text
chunk, denoted with T, and the lexical variants of each KR
concept, denoted with lex(C). This function is defined as
follows:
sim(C,T) = maxS?lex(C)
[ info(S ? T) ? info(S ? T)
info(S)
]
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 12 of 15
http://www.jbiomedsem.com/content/4/1/12
The set lex(C) is a set of lexical strings associated to the
concept C. The operation "S intersect T" means the set
of tokens both strings S and T share. This function mea-
sures the information coverage of T with respect to each
lexical variant of a concept C. Notice that we assume that
text chunks and lexical strings are represented as bags of
words. Information is measured with an estimation of the
string words entropy:
info(S) = ?
?
w?S
log(P(w|Background))
We have estimated word probabilities over the whole
Wikipedia as background.
All these definitions are inspired by the information-
theoretic matching function presented in [31] and the
word content evidence defined in [32].
The set of annotations associated to each text chunk T
are those concepts that maximize both sim(C,T) and the
word coverage of T. That is, the system selects the top
ranked concepts whose lexical variants best cover the text
chunk T. In order to avoid spurious and incomplete anno-
tations, a minimum threshold for sim(C,T) is required
(usually above 0.7).
From the annotations set of each description, we define
a semantic vector weighted by the tf ? idf score, where
tf (C) is the frequency of the concept C in the description
and idf (C) is calculated as follows:
idf (C) = maxS?lex(C)info(S)
Considering the concept reference formats of Table 9,
the annotations generated for the example task described
as Build phylogenetic trees, the metadata of the web
resource Blast (DDBJ), and their semantic vectors are
shown in Figure 4.
Facets extraction
In this work, we assume that resource descriptions usu-
ally lack facet-like metadata, which is very helpful to
define user requirements. This kind of information can
be implicitly found in the textual resource descriptions,
and therefore some kind of information extraction must
be performed to obtain those implicit facets. For example,
descriptions may contain information about the inputs
and the outputs of a resource, the algorithm behind a
resource, or the species involved in a public database. We
use two techniques to extract information about facets: (i)
extraction patterns and (ii) use of the annotation semantic
types. Thus, each facet has associated a set of extraction
patterns and a set of semantic types related to it.
1. Extraction patterns. Extraction patterns are applied
over the semantically annotated descriptions in order
to identify the relevant concepts of each facet. After
inspecting some resources, we conclude that the
basic extraction pattern for facets is as follows:
(noun-phrase)? relation E-noun-phrase
Where E-noun-phrase denotes any noun phrase
containing at least one semantic annotation. Each
facet will define the allowed noun phrases and
relations of the above generic pattern. For example,
Table 2 shows some extraction patterns for the
input/output and method facets.
Regarding the example of Figure 4, the patterns of
Table 2 extract the following instance:
(Facet = Output, NP = BLAST, R = finds,
C = {C1514562, C2348205})
Where NP and R are the identified noun phrase and
relation of the pattern, and C is the set of concepts
contained in E-noun-phrase part of the pattern.
2. Semantic types. The semantic types can also be
used to extract information about a facet. Our KR
concepts have associated a semantic type which can
be very useful to identify relevant information about
a facet. For example, a resource can manage
information about a specific set of species which are
explicitly mentioned in the resource description.
Once normalized, their corresponding annotations
will have the semantic types of relevant species (e.g.,
bacteria, virus, mammal, etc.) and, will directly define
the species facet. By using their semantic types of
annotations the user can define any facet associated
to them.
Table 9 Concept reference formats
Source Concept reference format Comment
UMLS UMLS:C<number>: STypes STypes are the semantic types associated to UMLS concepts (e.g. Disease,
Protein, etc.)
Wikipedia Wiki:W<number>: Categs Categs are the categories associated to the page entry of the referred
concept.
myGRID myGR:D<number>: These concepts are extracted from the myGRID ontologies.
EDAM EDAM_<number>: These concepts are extracted from the EDAM ontology.
This table shows the concept reference formats used for the different semantic sources. The generic format for a reference is
Source:ConceptID:SemanticTags.
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 13 of 15
http://www.jbiomedsem.com/content/4/1/12
Figure 4 Semantic annotation of a task and a resource description. This table shows the semantic annotation and the corresponding semantic
vector of the task build phylogenetic trees and a fragment of the description of the resource Blast. We have used the IeXML notation [33] to show
the generated annotations.
Finally, each facet is represented with the semantic
vector obtained from the union of all the concepts asso-
ciated to that facet in the textual description at hand.
This process is applied to user requirements and to the
descriptions of the resources.
Requirements refinement
Once the requirements have been semantically annotated,
the user can tune the annotations to better describe her
queries. Each task is represented by a semantic vector
which can be modified by the user as follows:
1. Selection of the most appropriate concepts. The user
can choose which concepts of those in the semantic
vector are going to be used in the resource retrieval
process. In this way, the user disregards wrongly
annotated or irrelevant concepts.
2. Selection of more specific concepts. The system
prompts to the user a list of narrower concepts to
define a more specific query.
3. Selection of more general concepts. The system
prompts to the user a list of broader concepts to
define a less specific query.
Web resources retrieval and ranking
The retrieval process of the suitable web resources accord-
ing to researchers requirements is based on the matching
between the annotations of the query, including all the
facets, and themetadata of the resources. This matching is
performed over the semantic vectors associated to them.
For example, we could apply the cosine measure to cal-
culate the similarity between web resource descriptions
and user requirements, or a concept-based probabilistic
model like that presented in [34]. However, these mea-
sures do not take into account the relevance of each
concept in the context of the tasks to which web resources
and requirements are aimed at.
For example, in the queries define structurally and
functionally important domains of the membrane, pre-
dict gene functions and compare functional relation-
ships, the concept function does not have the same rel-
evance. In the first query, functionally describes only a
characteristic of the domain, in the second one, function
is the key concept in the query, since it is the object that
must be predicted and, finally in the third one, functional
specifies the type of relationship that must be compared.
Therefore, the relevance of the same concept in different
queries varies depending on the context.
To be able to exploit this contextual information,
our approach is based on a topic-based ranking model
described in [35]. Using this topic-based model, we can
estimate the conditional probabilities of each concept c
under a pre-defined set of topics tk(1 ? k ? n) (where
n is the number of topics) which roughly corresponds
to bioinformatics generic tasks [36], such as sequence
analysis, protein identification, etc.
Web resource ranking
Given a query q, which consists of a set of concepts ci ? q
derived from the semantic annotation of a user require-
ment description, the ranking of resources for a facet f is
provided with the following probabilities:
P(q|wsj, f ) =
?
ci?q
?
tk
P(ci ? f |tk) · P(tk|wsj, f )
Pérez et al. Journal of Biomedical Semantics 2013, 4:12 Page 14 of 15
http://www.jbiomedsem.com/content/4/1/12
Here, P(ci ? f |tk) is the probability of the concept c for
the facet f given the topic tk , and P(tk|wsj, f ) is estimated
with the joint probability of resource wsj and the task tk
distributions for the facet f.
The final similarity between a faceted query q and a
web resource wsj is given by the linear combination of the
probabilities of the facets in the query.
P(q|wsj) = ? · P(q|wsj) +
?
f
?f · P(q|wsj, f )
where
? +
?
f
?f = 1
Top ranked resources according to these probabilities
are deemed the most appropriate for fulfilling the user
requirement query.
Evaluation
Given a GS, we have evaluated the results obtained for
each one of the queries from our query pool with the pre-
cision, recall and F-measure. These measures have been
calculated as follows:
precision = |relevant_resources ? retrieved_resources||retrieved_resources|
recall = |relevant_resources ? retrieved_resources||relevant_resources|
F = 2 · precision · recallprecision + recall
Facets extraction evaluation
The evaluation of the facets extraction method has been
carried out for each one of facets by calculating the preci-
sion, recall and F-measure as explained next.
For a given facet F (e.g., input) we denote with tags(F)
the BioCatalogue tags in the GS assigned to F, and with
concepts(F) the automatically extracted concepts for facet
F. Each tag t ? tags(F) has associated the set of resources
annotated with it for the facet F, which is denoted with
resourcesF(t).
Similarly, each concept c ? concepts(F) has associ-
ated the set of resources having c as value of the facet F,
denoted as above.
We calculate precision and recall for each pair (t, c), t ?
tags(F) and c ? concepts(F), as follows:
PF(t, c) = resourcesF(t) ? resourcesF(c)resourcesF(c)
RF(t, c) = resourcesF(t) ? resourcesF(c)resourcesF(t)
FF(t, c) = 2 · PF(t, c) · RF(t, c)PF(c, t) + RF(c, t)
The global precision and recall is calculated as a macro-
average over the best (t, c) mappings, which is defined as:
PF =
?
t?tags(F)
P(t, argmaxc?concepts(F)(FF (t, c)) · 1|tags(F)|
RF =
?
t?tags(F)
R(t, argmaxc?concepts(F)(FF (t, c)) · 1|tags(F)|
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MP developed the framework of the presented discovery method, conducted
the experiments and wrote the first draft of the article. RB developed the
semantic annotator and provided subject matter expertise critical to the
development of the retrieval model. IS developed the interface of BioUSeR. MJ
provided useful and very interesting suggestions to improve the design and
development of BioUSeR as well as to improve the structure and content of
this article. RB and MJ supervised and coordinated the project. All authors read
and approved the final manuscript.
Acknowledgements
This work has been partially funded by the Ministerio de Economía y
Competitividad" with contract number TIN2011-24147, and the Fundació
Caixa Castelló project P1-1B2010-49. María Pérez has been supported by
Universitat Jaume I predoctoral grant PREDOC/2007/41.
Author details
1Department of Computer Science and Engineering, Universitat Jaume I,
Castellón, Spain. 2Department of Computer Languages and Systems,
Universitat Jaume I, Castellón, Spain.
Received: 2 April 2012 Accepted: 18 April 2013
Published: 1 May 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10
http://www.jbiomedsem.com/content/4/1/10
RESEARCH Open Access
A unified approach for debugging is-a
structure and mappings in networked
taxonomies
Patrick Lambrix* and Valentina Ivanova
Abstract
Background: With the increased use of ontologies and ontology mappings in semantically-enabled applications
such as ontology-based search and data integration, the issue of detecting and repairing defects in ontologies and
ontology mappings has become increasingly important. These defects can lead to wrong or incomplete results for
the applications.
Results: We propose a unified framework for debugging the is-a structure of and mappings between taxonomies,
the most used kind of ontologies. We present theory and algorithms as well as an implemented system RepOSE, that
supports a domain expert in detecting and repairing missing and wrong is-a relations and mappings. We also discuss
two experiments performed by domain experts: an experiment on the Anatomy ontologies from the Ontology
Alignment Evaluation Initiative, and a debugging session for the Swedish National Food Agency.
Conclusions: Semantically-enabled applications need high quality ontologies and ontology mappings. One key
aspect is the detection and removal of defects in the ontologies and ontology mappings. Our system RepOSE
provides an environment that supports domain experts to deal with this issue. We have shown the usefulness of the
approach in two experiments by detecting and repairing circa 200 and 30 defects, respectively.
Background
In recent years many biomedical ontologies (e.g., [1]) have
been developed. Intuitively, ontologies can be seen as
defining the basic terms and relations of a domain of inter-
est, as well as the rules for combining these terms and
relations [2]. They are a key technology for the Seman-
tic Web. The benefits of using ontologies include reuse,
sharing and portability of knowledge across platforms,
and improved documentation, maintenance, and reliabil-
ity. Ontologies lead to a better understanding of a field and
to more effective and efficient handling of information in
that field. The work on ontologies is recognized as essen-
tial in some of the grand challenges of genomics research
[3] and there is much international research cooperation
for the development of ontologies.
Often we would want to be able to use multiple ontolo-
gies. For instance, companies may want to use community
*Correspondence: Patrick.Lambrix@liu.se
Department of Computer and Information Science / Swedish e-Science
Research Centre, Linko¨ping University, 581 83 Linko¨ping, Sweden
standard ontologies and use them together with company-
specific ontologies. Applications may need to use ontolo-
gies from different areas or from different views on one
area. Ontology builders may want to use already existing
ontologies as the basis for the creation of new ontolo-
gies by extending the existing ontologies or by combin-
ing knowledge from different smaller ontologies. In each
of these cases it is important to know the relationships
between the terms in the different ontologies. Further, the
data in different data sources in the same domain may
have been annotated with different but similar ontologies.
Knowledge of the inter-ontology relationships would in
this case lead to improvements in search, integration and
analysis of data. It has been realized that this is a major
issue and much research has recently been done on ontol-
ogy alignment, i.e., finding mappings between terms in
different ontologies (e.g., [4]).
Neither developing ontologies nor aligning ontologies
are easy tasks, and often the resulting ontologies and
alignments are not consistent or complete. Such ontolo-
gies and alignments, although often useful, also lead to
© 2013 Lambrix and Ivanova; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the
Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,
distribution, and reproduction in any medium, provided the original work is properly cited.
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 2 of 19
http://www.jbiomedsem.com/content/4/1/10
problemswhen used in semantically-enabled applications.
Wrong conclusions may be derived or valid conclusions
may be missed. Defects in ontologies can take different
forms (e.g., [5]). Syntactic defects are usually easy to find
and to resolve. Defects regarding style include such things
as unintended redundancy. More interesting and severe
defects are the modeling defects which require domain
knowledge to detect and resolve, and semantic defects
such as unsatisfiable concepts and inconsistent ontolo-
gies. Most work up to date has focused on debugging (i.e.,
detecting and repairing) the semantic defects in an ontol-
ogy (e.g., [5-8]). Modeling defects have been discussed in
[9-11]. Recent work has also started looking at repairing
semantic defects in a set of mapped ontologies [11,12]
or the mappings between ontologies themselves [13-15].
In this paper we tackle the problems of debugging the
is-a structure of a fundamental kind of ontologies, i.e.,
taxonomies, as well as the debugging of the mappings
between taxonomies in a unified approach.
In addition to its importance for the correct model-
ing of a domain, the structural information in ontologies
is also important in semantically-enabled applications.
For instance, the is-a structure is used in ontology-based
search and annotation. In ontology-based search, queries
are refined and expanded by moving up and down the
hierarchy of concepts. Incomplete and wrong structure
in ontologies influences the quality of the search results.
As an example, suppose we want to find articles in the
MeSH (Medical Subject Headings [16], controlled vocab-
ulary of the National Library of Medicine, US) Database
of PubMed [17] using the term Scleral Diseases in MeSH.
By default the query will follow the hierarchy of MeSH
and include more specific terms for searching, such as
Scleritis. If the relation between Scleral Diseases and Scle-
ritis is missing in MeSH, we will miss 738 articles in the
search result, which is about 55% of the original result set.
The structural information is also important in ontology
engineering research. For instance, most current ontol-
ogy alignment systems use structure-based strategies to
find mappings between the terms in different ontologies
(e.g., overview in [18]) and the modeling defects in the
structure of the ontologies have an important influence
on the quality of the ontology alignment results [19]. Also
incomplete alignments and wrong mappings lead to prob-
lems for semantically-enabled applications. For instance,
the lack of a mapping can lead to the fact that informa-
tion about similar entities in different databases cannot be
integrated. Wrong mappings will lead to wrong results in
data integration.
As the ontologies grow in size, it is difficult to ensure
the correctness and completeness of the structure of the
ontologies. Some structural relations may be missing or
some existing or derivable relations may be unintended.
This is not an uncommon case. It is well known that
people who are not expert in knowledge representation
often misuse and confuse equivalence, is-a and part-of
(e.g., [20]), which leads to problems in the structure of
the ontologies. For instance, we have shown in [21] that
many is-a relations were missing in the 2008 and 2009 ver-
sions of the ontologies Adult Mouse Anatomy Dictionary
(AMA, [22]) and the NCI Thesaurus anatomy (NCI-A,
[23]) in the Anatomy track of the Ontology Alignment
Evaluation Initiative (OAEI, [24]). Further, it is also diffi-
cult to ensure the correctness and completeness for the
mappings. Often, the mappings are generated by ontology
alignment systems. The system that performed best in the
2011 version of the OAEI Anatomy track had a precision
of 0.943 (i.e., there are wrong mappings), a recall of 0.892
(i.e., the alignment is incomplete) and a recall+ (recall for
non-trivial cases) of 0.728. The organizers also state that
less than half of the participating systems produced good
or acceptable results [25].
Semantically-enabled applications require high-quality
ontologies and mappings. A key step towards this is
debugging the ontologies and their alignments. In this
paper we deal with taxonomies connected via mappings
in an ontology network. We propose a semi-automatic
approach which helps a domain expert to debug the is-
a structure of the ontologies and the mappings between
them. This includes the detection of defects as well as
the repairing of defects. An advantage of our approach is
that no external knowledge is needed and, as we will see,
much debugging can already be done in this way. How-
ever, in the case external knowledge is available, this can
be used as well. For instance, in our work we use exter-
nal resources, such as WordNet [26] and UMLS [27], to
recommend user actions. We also note that, although we
focus on ontologies in a network, the repairing methods
can also be used for single ontologies when wrong and
missing is-a relations are available.
The remainder of this paper is organized as follows. In
Section Methods we present the setting, an overview of
our approach, the algorithms and our implemented sys-
tem. Further, we present experiments in Section Exper-
iments and a discussion and related work in Section
Discussion and related work. The paper concludes in
Section Conclusions. For formal definitions of the pre-
sented notions and proofs of statements, we refer to the
Additional file 1.
Methods
Preliminaries
The ontologies that we study are taxonomies, which are
defined using named concepts and subsumption axioms
(is-a relations between concepts). Most ontologies contain
such axioms and many of the most well-known and used
ontologies in the life sciences are covered by this setting.
The ontologies are connected into a network through
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 3 of 19
http://www.jbiomedsem.com/content/4/1/10
alignments which are sets of mappingsa between con-
cepts from two different ontologies.We currently consider
mappings of the type equivalent (?), subsumed-by (?)
and subsumes (?). The concepts that participate in map-
pings we callmapped concepts. Concepts can participate
in multiple mappings. The domain knowledge of an ontol-
ogy network is represented by its induced ontology. The
induced ontology for an ontology network is an ontology,
whose concepts are the concepts of the ontologies in the
network, and its set of asserted is-a relations contains the
asserted is-a relations of the ontologies together with the
is-a relations representing the mappings in the alignments
in the network.
Figure 1 shows a small ontology network with two
ontologies (concepts are represented by nodes and the is-
a structures are represented by directed edges) and an
alignment (represented by dashed edges).b The alignment
consists of 7 equivalence mappings. One of these map-
pings represents the fact that the concept bone in the first
ontology is equivalent to the concept bone in the second
ontology. As these two concepts appear in amapping, they
are mapped concepts.
Debugging workflow
In this Subsection, we give an overview of our debugging
approach. As illustrated in Figure 2, the process consists of
6 phases, where the first two phases are for the detection
and validation of possible defects, and the last four are for
the repairing. The input is a network of ontologies. The
output is the set of repaired ontologies and alignments.
As discussed before, in this paper we focus on detecting
wrong and missing is-a relations and mappings in the
ontology network, based on knowledge that is intrinsic
to the network. Therefore, given an ontology network,
we use the domain knowledge represented by the ontol-
ogy network to detect the deduced is-a relations in the
network. For each ontology in the network, the set of
candidate missing is-a relations derivable from the
ontology network (CMIs) consists then of is-a relations
between two concepts of the ontology, which can be
inferred using logical derivation from the induced ontol-
ogy of the network, but not from the ontology alone.
Similarly, for each pair of ontologies in the network, the
set of candidate missing mappings derivable from
the ontology network (CMMs) consists of mappings
between concepts in the two ontologies, which can be
inferred using logical derivation from the induced ontol-
ogy of the network, but not from the two ontologies and
their alignment alone. Therefore, one way to start the
debugging process is to choose an ontology in the net-
work and in Phase 1 CMIs are detected in this ontology.
Another way to start the process is to select a pair of
ontologies in the network and their alignment. In this
case in Phase 1 CMMs are detected.
Since the structure of the ontologies may contain wrong
is-a relations and the alignments may contain wrong map-
pings, some of the CMIs and CMMs may be derived due
to some wrong is-a relations and mappings. Therefore,
we need to validate the CMIs for all ontologies and par-
tition them into missing is-a relations and wrong is-a
relations. Similarly, the CMMs are validated and par-
titioned into missing mappings and wrong mappings.
This validation (possibly based on recommendations from
the debugging system) should be done by a domain expert
and is performed in Phase 2 of the debugging process.
We note that each validation leads to a debugging oppor-
tunity. In the case of a wrong is-a relation or mapping,
some is-a relations and/or mappings need to be removed.
In the case of a missing is-a relation or mapping, some
is-a relations and/or mappings need to be added. This
is a consequence and an advantage of our logic-based
approach using the knowledge intrinsic to the network.
Figure 1 (Part of an) Ontology network. Example of an ontology network with two ontologies (with roots bone) and an alignment.
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 4 of 19
http://www.jbiomedsem.com/content/4/1/10
Figure 2 Debugging workflow. Overview of debugging workflow with different phases.
The algorithm for detecting the CMIs and CMMs and
the procedure for validating are explained in Subsection
Detecting and validating candidate missing is-a relations
and mappings.
Once missing and wrong is-a relations and mappings
have been obtained, we need to repair them (Phase 3).c
For each ontology in the network, we want to repair the
is-a structure in such a way that (i) the missing is-a rela-
tions can be derived from their repaired host ontologies
and (ii) the wrong is-a relations can no longer be derived
from the repaired ontology network. In addition, for each
pair of ontologies, we want to repair the mappings in such
a way that (iii) the missing mappings can be derived from
the repaired host ontologies of their mapped concepts and
the repaired alignment between the host ontologies of the
mapped concepts and (iv) the wrong mappings can no
longer be derived from the repaired ontology network. To
satisfy requirement (i), we need to add a set of is-a rela-
tions to the host ontology. To satisfy requirement (iii), we
need to add a set of is-a relations to the host ontologies of
the mapped concepts and/or mappings to the alignment
between the host ontologies of the mapped concepts. To
satisfy requirements (ii) and (iv), a set of asserted is-a rela-
tions and/or mappings should be removed from the ontol-
ogy network. The notion of structural repair formalizes
this. It contains is-a relations and mappings that should
be added to or removed from the ontologies and align-
ments to satisfy these requirements. These is-a relations
and mappings are called repairing actions.
As explained in our previous work [10] regarding miss-
ing is-a relations, not all structural repairs are equally
useful or interesting for a domain expert. Therefore, we
defined several heuristics of which we use extensions in
this work. The first heuristic (pref1) states that we want
to use repairing actions that contribute to the repairing.
Secondly, we want to repair with as informative as pos-
sible repairing actions (pref2). As an example, consider
the missing is-a relation (nasal bone, bone) in the first
ontology in Figure 1. Knowing that nasal bone ? vis-
cerocranium bone, according to the definition of more
informative (see Additional file 1), we know that (visce-
rocranium bone, bone) is more informative than (nasal
bone, bone). As viscerocranium bone actually is a sub-
concept of bone according to the domain, a domain expert
would prefer to use the more informative repairing action
for the given missing is-a relation.d The third heuris-
tic (pref3) prefers not to introduce equivalence relations
between concepts A and B when in the original ontol-
ogy there is an is-a relation betwen A and B. Finally,
the single relation heuristic (pref4) assumes that it is
more likely that the ontology developers have missed
to add single is-a relations, rather than chains of is-a
relations.
A naive way of repairing would be to compute all pos-
sible structural repairs for the network with respect to
the validated missing is-a relations and mappings for all
the ontologies in the network. This is infeasible in prac-
tice as it involves all the ontologies and alignments and
all the missing and wrong is-a relations and mappings
in the network. It is also hard for domain experts to
choose between structural repairs containing large sets
of repairing actions for all the ontologies and alignments.
Therefore, in our approach, we repair ontologies and
alignments one at a time.
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 5 of 19
http://www.jbiomedsem.com/content/4/1/10
For the selected ontology (for repairing is-a relations)
or for the selected alignment and its pair of ontologies
(for repairing mappings), a user can choose to repair
the missing or the wrong is-a relations/mappings (Phase
3.1-3.4). Although the algorithms for repairing are dif-
ferent for missing and wrong is-a relations/mappings, the
repairing goes through the phases of generation of repair-
ing actions, the ranking of is-a relations/mappings, the
recommendation of repairing actions and finally, the exe-
cution of repairing actions. In Phase 3.1 repairing actions
are generated. For missing is-a relations and mappings
these are is-a relations and/or mappings to add, while for
wrong is-a relations and mappings, these are is-a rela-
tions and/or mappings to remove. In general, there will
be many is-a relations/mappings that need to be repaired
and some of them may be easier to start with such as
the ones with fewer repairing actions. We therefore rank
them with respect to the number of possible repairing
actions (Phase 3.2). After this, the user can select an is-
a relation/mapping to repair and choose among possible
repairing actions. To facilitate this process, we devel-
oped methods to recommend repairing actions (Phase
3.3). Once the user decides on repairing actions, the
chosen repairing actions are then removed (for wrong
is-a relations/mappings) from or added (for missing is-a
relations/mappings) to the relevant ontologies and align-
ments and the consequences are computed (Phase 3.4).
For instance, by repairing one is-a relation/mapping some
other missing or wrong is-a relations/mappings may also
be repaired or their repairing actions may change. Fur-
ther, newCMIs andCMMsmay be found. A description of
the algorithms and components of our implemented sys-
tem RepOSE for Phases 3.1-3.4 are found in Subsection
Repairing wrong is-a relations and mappings (related to
wrong is-a relations/mappings) and Subsection Repairing
missing is-a relations and mappings (related to missing
is-a relations/mappings).
We also note that at any time during the process, the
user can switch between different ontologies, start earlier
phases, or switch between the repairing of wrong is-a rela-
tions, the repairing of missing is-a relations, the repairing
of wrong mappings and the repairing of missing map-
pings. The process ends when there are no more CMIs,
CMMs, missing or wrong is-a relations and mappings to
deal with.
Algorithms
Detecting and validating candidatemissing is-a relations
andmappings
The CMIs and CMMs could be found using a brute-force
method by checking each pair of concepts in the network.
If the concepts in a pair belong to the same ontology,
and an is-a relation is not derivable from the ontology
but derivable from the network, then it is a CMI. If the
concepts in the pair belong to different ontologies, and
a mapping is derivable from the network, but not from
the host ontologies of the concepts and their alignment,
then it is a CMM. However, for large ontologies or ontol-
ogy networks, this is infeasible. Further, some of these
CMIs and CMMs are redundant in the sense that they can
be repaired by the repairing of other CMIs and CMMs.
Therefore, instead of checking all pairs of concepts in
the network we define a subset of the set of all pairs of
concepts in the network that we will consider for generat-
ing CMIs and CMMs. This subset will initially consist of
all pairs of mapped concepts. The reason for this choice
is that, in the restricted setting where we assume that
all existing is-a relations in the ontologies and all exist-
ing mappings in the alignments are correct (and thus the
debugging problem does not need to consider wrong is-
a relations and mappings), it can be shown that all CMIs
and CMMs will be repaired when we repair the CMIs and
CMMs between mapped concepts (see Additional file 1).
This guarantees that for the part of the network for which
the is-a structure and mappings are correct, we find all
CMIs and CMMswhen using the set of all pairs of mapped
concepts. In addition, we may generate CMIs that were
derived using wrong information. These may later be val-
idated as correct or wrong. As our debugging approach
is iterative, after repairing, larger and larger parts of the
network will contain only correct is-a structure and map-
pings. When finally all the network contains only correct
is-a structure and mappings, it is guaranteed that all
defects that can be found using the knowledge intrinsic to
the network, are found using our approach.
Therefore, our detection algorithm is applied tomapped
conceptse. First, we initialize a knowledge basef for the
ontology network (KBN ). Then, we initialize for each
ontology a knowledge base. Further, we initialize a knowl-
edge base for each pair of ontologies and their alignment.
For each pair of mapped concepts within the same ontol-
ogy, we check whether an is-a relation between the pair
can be derived from the knowledge base of the network,
but not from the knowledge base of the ontology, and if
so, it is a CMI. Similarly, for each pair of mapped concepts
belonging to two different ontologies, we check whether
an is-a relation between the pair can be derived from the
knowledge base of the network, but not from the knowl-
edge base of the two ontologies and their alignment, and
if so, it is a CMM.
Among these generated CMIs and CMMs, we also
remove redundant ones. The remaining CMIs and CMMs
should then be validated. As noted before, every CMI or
CMM that is generated by our approach gives an oppor-
tunity for debugging. If a CMI or CMM is validated to
be correct, then information is missing and is-a rela-
tions or mappings need to be added; otherwise, some
existing information is wrong and is-a relations and/or
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 6 of 19
http://www.jbiomedsem.com/content/4/1/10
mappings need to be removed. After repairing, new CMIs
and CMMs may be generated.
Initialization of the repairing phase
1. Initialize KBN with ontology networkN ;
2. For k := 1 .. n:
initialize KBk with ontologyOk ;
3. For i := 1 .. n-1: for j := i+1 .. n:
initialize KBij with ontologiesOi andOj;
for every mapping (m, n) ? Mij: add the
axiomm ? n to KBij;
4. For k:= 1 .. n:
for every missing is-a relation (a, b) ? MIk :
add the axiom a ? b to KBN ;
add the axiom a ? b to KBk ;
for i := 1 .. k-1:
add the axiom a ? b to KBik ;
for i := k+1 .. n:
add the axiom a ? b to KBki;
5. For i := 1 .. n-1: for j := i+1 .. n:
for every missing mapping (m, n) ? MMij:
add the axiomm ? n to KBN ;
add the axiomm ? n to KBij;
6. MI := MIN ;WI := WIN ;MM := MMN ;
WM := WMN ;
7. R+I := ?;R?I := ?;R+M := ?;R?M := ?;
8. CMI := ?; CMM := ?;
Given a set of missing and wrong is-a relations and
mappings for the ontology network, as an initial step we
initialize knowledge bases for the ontology network (KBN ,
step 1), for each of the ontologies (KBk , step 2), and for
each pair of ontologies with their alignments (KBij, step 3).
Then, we add all missing is-a relations (step 4) and map-
pings (step 5) to the relevant knowledge bases. As these
are validated to be correct, this is extra knowledge that
should be used in the repairing process. Adding the miss-
ing is-a relations and mappings essentially means that we
have repaired these using the least informative repair-
ing actions (see definition of more informative in the
Additional file 1). In Subsection Repairing missing is-a
relations and mappings we try to improve this and find
more informative repairing actions.
Further, we initialize global variables for the current sets
of missing (MI) and wrong (WI) is-a relations, the cur-
rent sets of missing (MM) and wrong (WM) mappings
in step 6, the repairing actions (R+I and R?I for is-a rela-
tions, and R+M and R?M for mappings) in step 7, and the
current sets of CMIs (CMI) and CMMs (CMM) in step.
Repairing wrong is-a relations andmappings
1. Compute AllJust(w, r,Oe)
whereOe = (Ce, Ie) such that Ce = ?nk=1Ck andIe = ((?nk=1Ik) ? (?ni,j=1;i<jMij) ?MIN ?
MMN ?R+I ?R+M) \ (R?I ?R?M);
2. For every I ? ? AllJust(w, r,Oe):
choose one element from
I ? \ (MIN ?MMN ?R+I ?R+M) to remove;
The algorithm for generating repairing actions for a
wrong is-a relation or mapping is run for all wrong is-
a relations (elements in WI) and mappings (elements
in WM). It computes all justifications (AllJust) for the
wrong is-a relation or mapping (w,r) in the current ontol-
ogy network (Oe). The current network is the original
network where the repairs up to now have been taken
into account (i.e., all missing is-a relations are repaired by
adding them while some are repaired using more infor-
mative repairing actions in R+I , missing mappings have
been repaired by adding them or by repairing actions in
R+M, and some wrong is-a relations and mappings are
already repaired by removing is-a relations and mappings
in R?I and R?M, respectively). A justification for a wrong
is-a relation or mapping can be seen as an explanation
for why this is-a relation or mapping is derivable from
the network (definition in Additional file 1). For instance,
for the wrong is-a relation (brain grey matter, white mat-
ter) in AMA (experiment in Section Experiment 1 -
OAEI Anatomy and Figure 3), there is one justification:
{(brain grey matter (AMA), Brain White Matter (NCI-
A)), (Brain White Matter (NCI-A), White Matter
(NCI-A)), (White Matter (NCI-A), white matter (AMA))}.
In general, however, there may be several justifications
for a wrong is-a relation or mapping. The algorithm to
compute justifications initializes a knowledge base with
the original ontology and the repairing actions up to now.
To compute the justifications all derivation paths (see
Additional file 1) are computed and the minimal ones are
retained. The wrong is-a relation or mapping can then
be repaired by removing at least one element in every
justification. However, missing is-a relations, missing
mappings, and added repairing actions (is-a relations in
ontologies and mappings) cannot be removed. Using this
algorithm structural repairs are generated that include
only contributing repairing actions (pref1 in Subsection
Debugging workflow).
When the repairing is executed, a number of updates
need to be done. First, the wrong is-a relation (or map-
ping) is removed from WI (or WM). The chosen
repairing actions that are is-a relations in an ontology are
added to R?I and repairing actions that are mappings are
added to R?M. Some other wrong is-a relations or map-
pingsmay also have been repaired by repairing the current
wrong is-a relation or mapping (update WI and WM).
Also, some repaired missing is-a relations and mappings
may also become missing again (update MI and MM).
Further, new CMIs and CMMs may appear (update CMI
and CMM - and after validation update CMI , MI ,
WI , CMM,MM andWM). In other cases the possible
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 7 of 19
http://www.jbiomedsem.com/content/4/1/10
Figure 3 An example of repairing wrong is-a relations. Repairing wrong is-a relations: the case of (brain grey matter, white matter), (cerebellum
white matter, brain grey matter) and (cerebral white matter, brain grey matter).
repairing actions for wrong and missing is-a relations and
mappings may change (update justifications and sets of
possible repairing actions for missing is-a relations and
mappings). We also need to update the knowledge bases.
Repairingmissing is-a relations andmappings
Repair missing is-a relation (a,b) with a ? Ok and b ? Ok :
Choose an element from GenerateRepairingActions
(a, b, KBk);
Repair missing mapping (a,b) with a ? Oi and b ? Oj:
Choose an element from GenerateRepairingActions
(a, b, KBij);
GenerateRepairingActions(a, b, KB):
1. Source(a,b) := super-concepts(a)?super-concepts(b )
in KB;
2. Target(a, b) := sub-concepts(b ) ? sub-concepts(a) in
KB;
3. Repair(a, b) := Source(a, b) × Target(a, b);
4. For each (s, t) ? Source(a, b) × Target(a, b):
if (s, t) ? WI ?WM ?R?I ?R?M then remove
(s, t) from Repair(a, b);
else if ?(u, v) ? WI ?WM ?R?I ?R?M : (s, t)
is more informative than (u, v) in KB
and u ? s and t ? v are derivable from
validated to be correct only is-a relations
and/or mappings
then remove (s, t) from Repair(a, b);
5. return Repair(a, b);
The algorithm for the computation of repairing actions
for a missing is-a relation or mapping, an extension of the
algorithm in [10], takes into consideration that all missing
is-a relations and missing mappings will be repaired (least
informative repairing action), but it does not take into
account the consequences of the actual (possibly more
informative) repairing actions that will be performed for
other missing is-a relations and other missing mappings.
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 8 of 19
http://www.jbiomedsem.com/content/4/1/10
The main component of the algorithm (GenerateRe-
pairingActions) takes a missing is-a relation or mapping
as input together with a knowledge base. For a missing
is-a relation this is the knowledge base corresponding to
the host ontology of the missing is-a relation; for a miss-
ing mapping this is the knowledge base corresponding to
the host ontologies of the mapped concepts in the miss-
ing mapping and their alignment. In this component for
a missing is-a relation or mapping we compute the more
general concepts of the first concept (Source) and the
more specific concepts of the second concept (Target) in
the knowledge base. To avoid introducing non-validated
equivalence relations where in the original ontologies and
alignments there are only is-a relations, we remove the
super-concepts of the second concept from Source, and
the sub-concepts of the first concept from Target. Adding
an element from Source × Target to the knowledge base
makes the missing is-a relation or mapping derivable.
However, some elements in Source × Target may con-
flict with already known wrong is-a relations or mappings.
Therefore, in Repair, we take the wrong is-a relations and
mappings and the former repairing actions for wrong is-
a relations and mappings into account. The missing is-a
relation ormapping can then be repaired using an element
in Repair. We note that for missing is-a relations, the ele-
ments in Repair are is-a relations in the host ontology for
the missing is-a relation. For missing mappings, the ele-
ments in Repair can bemappings as well as is-a relations in
each of the host ontologies of the mapped concepts of the
missing mapping. Using this algorithm structural repairs
are generated that include only contributing repairing
actions, and repairing actions of the form (a, t) or (s, b) for
missing is-a relation or mapping (a, b) do not introduce
non-validated equivalence relations (see pref1 and pref3 in
Subsection Debugging workflow). Further, the solutions
follow the single relations heuristic (pref4).
As an example, for the missing is-a relation (lower res-
piratory track cartilage, cartilage) in AMA (experiment in
Section Experiment 1 - OAEI Anatomy and Figure 4) a
Source set of 2 elements and a Target set of 21 elements
are generated and this results in 42 possible repairing
actions. Each of the repairing actions when added to
Figure 4 An example of repairing missing is-a relations. Repairing missing is-a relations: the case of (lower respiratory tract cartilage, cartilage).
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 9 of 19
http://www.jbiomedsem.com/content/4/1/10
AMA, would make the missing is-a relation derivable
from AMA. In this example a domain expert would select
the more informative repairing action (respiratory system
cartilage, cartilage).
When the selected repairing action is in Repair(a, b),
the repairing action is executed, and a number of updates
need to be done. First, the missing is-a relation (or map-
ping) is removed from MI (or MM) and the chosen
repairing action is added to R+I or R+M depending on
whether it is an is-a relation within an ontology or a
mapping. Further, new CMIs and CMMs may appear.
Some other missing is-a relations or mappings may
also have been repaired by repairing the current miss-
ing is-a relation or mapping. Some repaired wrong
is-a relations and mappings may also become derivable
again. In other cases the possible repairing actions for
wrong and missing is-a relations and mappings may
change. We also need to update the knowledge bases.
Implemented system
Detecting and validating candidatemissing is-a relations
In RepOSE, the user loads the ontologies and alignments.
Then the user can use the tab Step1: Generate and Val-
idate Candidate Missing is-a Relations (Figure 5) and
choose an ontology for which the CMIs are computed.
The user can validate all or some of the CMIs as well
as switch to another ontology or another tab. Showing
all CMIs at once would lead to information overload and
difficult visualization. Showing them one at the time has
the disadvantage that we do not have information about
the interaction with other is-a relations. Therefore, as a
trade-off we show the CMIs in groups where for each
Figure 5 An example of generating and validating candidate missing is-a relations. Generating and validating candidate missing is-a
relations: the case of bone and joint.
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 10 of 19
http://www.jbiomedsem.com/content/4/1/10
member of the group at least one of the concepts sub-
sumes or is subsumed by a concept of another member
in the group. Additionally, the CMIs can be chosen from
a drop-down list. Initially, CMIs are shown using arrows
labeled by ? (as in Figure 5 for (acetabulum, joint)) which
the user can toggle to W for wrong relations and M
for missing relations. For each CMI the justification in
the ontology network is shown as an extra aid for the
user. For instance, in Figure 5 (palatine bone, bone) is
selected and its justifications shown in the justifications
panel. Concepts in different ontologies are presented with
different background color. Further, we implemented a
recommendation algorithm for validation. As is-a and
part-of are often confused, the user can ask for a rec-
ommendation based on existing part-of relations in the
ontology or in external domain knowledge (WordNet). If
a part-of relation exists between the concepts of a CMI,
it is likely a wrong is-a relation (the ? label is replaced
by a W? label as in Figure 5 for (upper jaw, jaw)). Sim-
ilarly, the existence of is-a relations in external domain
knowledge (WordNet and UMLSg) may indicate that a
CMI is indeed a missing is-a relation (the ? label is
replaced by a M? label as in Figure 5 for (elbow joint,
joint)). When a user decides to finalize the validation of
a group of CMIs, RepOSE checks for contradictions in
the current validation as well as with previous decisions
and if contradictions are found, the current validation
will not be allowed and a message window is shown to
the user.
A similar tab Step 2: Generate and Validate Candi-
date Missing Mappings can be used to choose a pair of
ontologies and their alignment for which the CMMs are
generated and, to validate them. The recommendation
algorithm for mappings uses WordNet and UMLS.
Repairing wrong is-a relations andmappings
Figure 3 shows the RepOSE tab Step 3: Repair Wrong is-a
Relations for repairing wrong is-a relations. Clicking on
the Generate Repairing Actions button, results
in the computation of repairing actions for each wrong
is-a relation of the ontology under repair. The wrong is-a
relations are then ranked in ascending order according
to the number of possible repairing actions and shown
in a drop-down list. Then, the user can select a wrong
is-a relation and repair it using an interactive display.
The user can choose to repair all wrong is-a relations
in groups or one by one. The display shows a directed
graph representing the justifications. The nodes repre-
sent concepts. As mentioned before, concepts in different
ontologies are presented with different background color.
The concepts in the is-a relation under repair are shown
in red. The edges represent is-a relations in the justi-
fications. These is-a relations may be existing asserted
is-a relations (shown in grey), mappings (shown in
brown), unrepaired missing is-a relations (shown in blue)
and the added repairing actions for the repaired missing
is-a relations (shown in black).
In Figure 3 the user has chosen to repair several wrong
is-a relations at the same time, i.e., (brain grey matter,
white matter), (cerebellum white matter, brain grey mat-
ter), and (cerebral white matter, brain grey matter). In
this example we can repair these wrong is-a relations by
removing the mappings between brain grey matter and
Brain White Matter. We note that, when removing these
mappings, all these wrong is-relations will be repaired at
the same time.
For the wrong is-a relations under repair, the user can
choose, by clicking, multiple existing asserted is-a rela-
tions andmappings on the display as repairing actions and
click the Repair button. RepOSE ensures that only exist-
ing asserted is-a relations and mappings are selectable,
and when the user finalizes the repair decision, RepOSE
ensures that the wrong is-a relations under repair and
every selected is-a relation and mapping will not be deriv-
able from the ontology network after the repairing. Fur-
ther, all consequences of the repair are computed (such as
changes in the repairing actions of other is-a relations and
mappings and changes in the lists of wrong and missing
is-a relations and mappings).
During the repairing, the user can choose to use
the recommendation feature by enabling the Show
Recommendation check box. The recommendation
algorithm will then compute hitting sets for all the jus-
tifications of the wrong is-a relations under repair. Each
hitting set contains a minimal set of is-a relations and
mappings to remove so as to repair a wrong is-a rela-
tion (formal definition and algorithm in [28]). The rec-
ommendation algorithm then assigns a priority to each
possible repairing action based on how often it occurs
in the hitting sets and its importance in already repaired
is-a relations and mappings. In the example in Figure 3
the highest priority (indicated by pink labels marked Pn,
where n reflects the priority ranking) is given to the map-
ping (Brain White Matter, brain grey matter), as this is
the only way to repair more than one wrong is-a rela-
tion at the same time. (Both (cerebellum white matter,
brain grey matter) and (cerebral white matter, brain grey
matter)would be repaired). Upon the selection of a repair-
ing action, the recommendations are recalculated and
the labels are updated. As long as there are labels, more
repairing actions need to be chosen.
A similar tab (Step 4: Repair Wrong Mappings) is used
for repairing wrong mappings.
Repairingmissing is-a relations andmappings
Figure 4 shows the RepOSE tab Step 5: Repair Missing
is-a Relations for repairing missing is-a relations. Click-
ing on the Generate Repairing Actions button,
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 11 of 19
http://www.jbiomedsem.com/content/4/1/10
results in the computation of repairing actions for the
missing is-a relations of the ontology under repair. For
easy visualization, these are shown to the user as Source
and Target sets (instead of Repair). Once the Source and
Target sets are computed, the missing is-a relations are
ranked with respect to the number of possible repairing
actions. The first missing is-a relation in the list has the
fewest possible repairing actions, and may therefore be
a good starting point. When the user chooses a missing
is-a relation, its Source and Target sets are displayed on
the left and right, respectively, within the Repairing
Actions panel (Figure 4). Both have zoom control and
can be opened in a separate window. Similarly to the
displays for wrong is-a relations, concepts in the miss-
ing is-a relations are highlighted in red, existing asserted
is-a relations are shown in grey, unrepaired missing is-a
relations in blue and added repairing actions for the miss-
ing is-a relations in black. For instance, Figure 4 shows
the Source and Target sets for the missing is-a relation
(lower respiratory tract cartilage, cartilage), which contain
2 and 21 concepts, respectively. The Target panel shows
also the unrepaired missing is-a relation (nasal septum,
nasal cartilage). The Justifications of current
relation panel is a read-only panel that displays the jus-
tifications of the current missing is-a relation as an extra
aid.
For the selected missing is-a relation, the user can also
ask for recommended repairing actions by clicking the
Recommend button. The recommendation algorithm (as
defined in [10]) computes for missing is-a relation (a, b)
themost informative repairing actions from Source(a, b)×
Target(a, b) that are supported by domain knowledge
(WordNet). In general, the system presents a list of rec-
ommendations. By selecting an element in the list, the
concepts in the recommended repairing action are iden-
tified by round boxes in the panels. For instance, for
the case in Figure 4, the recommendation algorithm pro-
poses to add (respiratory system cartilage, cartilage). Using
the recommendation algorithm we recommend structural
repairs that try to use as informative repairing actions as
possible (pref2 in Subsection Debugging workflow). The
user can repair the missing is-a relation by selecting a con-
cept in the Source panel and a concept in the Target panel
and clicking on the Repair button. When the selected
repairing action is not in Repair(a, b), the repairing will
not be allowed and amessage window is shown to the user.
Further, all consequences of a chosen repair are computed
(such as changes in the repairing actions of other is-a rela-
tions and mappings and changes in the lists of wrong and
missing is-a relations and mappings).
The tab Step 6: Repair Missing Mappings is used for
repairing missing mappings. The main difference with the
tab for repairing missing is-a relations is that we deal with
two ontologies and their alignment and that the repairing
actions can be is-a relations within an ontology as well as
mappings.
Experiments
In this Section we discuss two debugging sessions. The
first is a debugging session for two well-known ontologies
in the anatomy domain. The second is work that we per-
formed for the Swedish National Food Agency. In both
cases the debugging was performed by domain experts.
Further, we also discuss related work.
Experiment 1 - OAEI Anatomy
In the first experiment a domain expert ran a complete
debugging session on a network consisting of the two
ontologies and the reference alignment from the Anatomy
track in OAEI 2010. These ontologies as well as the ref-
erence alignment were developed by domain experts. For
the 2010 version of OAEI, AMA contains 2,744 con-
cepts and 1,807 asserted is-a relations, while NCI-A con-
tains 3,304 concepts and 3,761 asserted is-a relations.
The reference alignment contains 986 equivalence and 1
subsumption mapping between AMA and NCI-A. This
information is summarized in Table 1.
The experiment was performed on an Intel Core i7-950
Processor 3.07GHz with 6 GB DDR2 memory underWin-
dows 7 Ultimate operating system and Java 1.7 compiler.
The domain expert completed debugging this network
within 2 days. As the system has a good responsiveness,
much of this time was spent on making decisions for
validation and repairing (essentially looking up and ana-
lyzing information to make decisions) and interactions
with RepOSE.
Table 2 summarizes the results of the detection and
repairing of defects in the is-a structures of the ontolo-
gies and the mappings. The system detected 200 CMIs in
AMA of which 123 were non-redundant. Of these non-
redundant CMIs 102 were validated to be missing is-a
relations and 21 were validated to be wrong is-a rela-
tions. For NCI-A 127 CMIs, of which 80 non-redundant,
were detected. Of these non-redundant CMIs 61 were val-
idated to be missing is-a relations and 19 were validated
to be wrong is-a relations. To repair these defects 85 is-
a relations were added to AMA and 57 to NCI-A, 13 is-a
relations were removed from AMA and 12 from NCI-
A, and 12 mappings were removed from the reference
Table 1 Experiment 1 - ontologies and alignment
Concepts Asserted is-
a relations
Asserted
equivalence
mappings
Asserted is-
a mappings
AMA 2744 1807 - -
NCI-A 3304 3761 - -
Alignment - - 986 1
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 12 of 19
http://www.jbiomedsem.com/content/4/1/10
Table 2 Experiment 1 results - final result
Candidate missing Missing Wrong Added Repair missing Removed Removed
all/non-redundant is-a relations more informative is-a relations mappings
AMA 200/123 102 21 85 22 13 -
NCI-A 127/80 61 19 57 8 12 -
Alignment - - - - - - 12
alignment. In 22 cases in AMA and 8 cases in NCI-A a
missing is-a relation was repaired using a more informa-
tive repairing action, thereby adding new knowledge to
the network.
The ranking and recommendations seemed useful.
Table 3 summarizes the recommendation results. Regard-
ing CMIs, 81 and 27 recommendations that the rela-
tion should be validated as a missing is-a relation, were
accepted for AMA, respectively NCI-A, while 8 and 2
were rejected.When the system recommended that a CMI
should be validated as a wrong is-a relation, the recom-
mendation was accepted in 7 out of 20 cases for AMA and
6 out of 8 cases for NCI-A. The recommendations regard-
ing repairingmissing is-a relations were accepted in 69 out
of 85 cases for AMA and 43 out of 57 cases for NCI-A.
We note that the system may not always give a recom-
mendation. This is the case, for instance, when there is no
information about the is-a relation under consideration in
the external sources.
In the remainder of this Subsection we discuss the
session and the results in more details.
Detecting and validating candidatemissing is-a relations for
the first time
After loading AMA, NCI-A and the reference alignment,
it took less than 30 seconds for each of the ontologies to
detect all its CMIs. As a result, RepOSE found 192 CMIs
in AMA and 122 in NCI-A. Among these CMIs, 115 in
AMA and 75 in NCI-A are displayed in 24 groups and 18
groups, respectively, for validation, while the remaining 77
in AMA and 47 in NCI-A are redundant and thus ignored.
With the help of the recommendations, the domain expert
identified 20 wrong is-a relations and 95 missing is-a rela-
tions in AMA. For NCI-A the domain expert identified
17 wrong and 58 missing is-a relations. These results are
summarized in Table 4. As for the recommendation, the
use of asserted part-of relations in ontologies together
Table 3 Experiment 1 results - recommendations
Candidate missing Candidate missing Repair missing
missing accept/ wrong accept/ accept/reject
reject reject
AMA 81/8 7/13 69/16
NCI-A 27/2 6/2 43/14
with WordNet recommended 20 possible wrong is-a rela-
tions in AMA and 8 in NCI-A, of which 7 in AMA and 6 in
NCI-A were accepted as decisions. WordNet and UMLS
recommended 84 possible missing is-a relations in AMA
and 29 in NCI-A, of which 77 in AMA and 27 in NCI-A
were accepted as decisions.
Repairing wrong is-a relations for the first time
After the validation phase, the domain expert continued
with the repairing of wrong is-a relations. In this exper-
iment, for the 20 wrong is-a relations in AMA and 17
in NCI-A, each wrong is-a relation has only one justi-
fication, consisting of two or more mappings and one
or more asserted is-a relations in the other ontology.
Therefore, the repairing is done by removing the involved
asserted is-a relations and/or mappings (Table 4). For
example, for the wrong is-a relation (Ascending Colon,
Colon) in NCI-A (which is actually a part-of relation), its
justification contains two equivalence mappings (between
Ascending Colon and ascending colon, and between Colon
and colon) and an asserted is-a relation (ascending colon,
colon) in AMA. The repairing was done by removing
(ascending colon, colon) from AMA. As shown before, the
wrong is-a relation (brain grey matter, white matter) in
AMA (Figure 3) was repaired by removing the mappings
between Brain White Matter and brain grey matter.
We note that 11 mappings were removed, 8 of them
as a result of wrong is-a relations in AMA and 3 as a
result of the debugging of NCI-A. Further, several wrong
is-a relations were repaired by repairing other wrong is-a
relations.
Repairingmissing is-a relations in AMA and NCI-A for the first
time
As the next step, the domain expert proceeded with the
repairing of missing is-a relations in AMA. At this point
there were 95 missing is-a relations to repair, and it took
less than 10 seconds to generate the repairing actions
for them. Almost all Source and Target sets were small
enough to allow a good visualization. For 59 missing
is-a relations, the domain expert used the missing is-a
relation itself as the repairing action (i.e., the least infor-
mative repairing actions). For 19missing is-a relations, the
domain expert used more informative repairing actions,
which also repaired 17 other missing is-a relations. These
results are summarized in the last column of Table 4.
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 13 of 19
http://www.jbiomedsem.com/content/4/1/10
Table 4 Experiment 1 results - first iteration
Candidate missing Missing Wrong Repair wrong Repair missing
all/non-redundant removed self/more informative/other
AMA 192/115 95 20 12 59/19/17
NCI-A 122/75 58 17 11 49/5/4
Alignment - - - 11 -
The recommendation algorithm was used in 78 cases.
In 63 of them the selected repairing action was among
the recommended repairing actions and in 9 of them the
recommendation algorithm suggested more informative
repairing actions.
The domain expert then continued with the repair-
ing of missing is-a relations in NCI-A. For the 58
missing is-a relations to repair, 49 missing is-a rela-
tions were repaired using themselves as the repairing
actions, 5 were repaired using more informative repair-
ing actions, and 4 were repaired by the repairing of
others (Table 4). For example, for the repairing of
missing is-a relation (Epiglottic Cartilage, Laryngeal
Connective Tissue) in NCI-A, the domain expert used
more information repairing action (Laryngeal Cartilage,
Laryngeal Connective Tissue), where Laryngeal Cartilage
is a super-concept of Epiglottic Cartilage in NCI-A. This
repairing also repaired 3 other missing is-a relations, i.e.,
(Cricoid Cartilage, Laryngeal Connective Tissue), (Ary-
tenoid Cartilage, Laryngeal Connective Tissue) and (Th-
yroid Cartilage, Laryngeal Connective Tissue), where
Cricoid Cartilage, Arytenoid Cartilage and Thyroid
Cartilage are sub-concepts of Laryngeal Cartilage in
NCI-A. The recommendation algorithm was used in 54
cases. In 42 of them the selected repairing action was
among the recommended repairing actions and in 3 of
them the recommendation algorithm suggested more
informative repairing actions.
We observe that at this point for 19 missing is-a rela-
tions in AMA and 5 in NCI-A, the domain expert has
used repairing actions that are more informative than the
missing is-a relation itself. This means that for each of
these the domain expert has added knowledge that was
not intrinsic to (i.e., derivable from) the network. Thus the
knowledge represented by the ontologies and the network
has increased.
The subsequent debugging process
The repairing of the wrong and the missing is-a relations
in both ontologies resulted in 6 non-redundant new CMIs
in AMA and 4 in NCI-A. In each ontology 1 of those was
validated as wrong and the others as missing. 2 of the
5 missing is-a relations in AMA were repaired by them-
selves and 3 usingmore informative repairing actions. The
wrong is-a relation was repaired by removing an is-a rela-
tion in NCI-A. The 3 missing is-a relations in NCI-A were
repaired by using more informative repairing actions. The
wrong is-a relation was repaired by removing a map-
ping from the reference alignment. The repairing of these
newly found relations led to two more CMIs in AMA,
which were validated as correct and repaired by them-
selves, and one CMI in NCI-A, which was validated as
wrong and repaired by removing an is-a relation in AMA.
At this point there were no more CMIs to validate, and no
more wrong or missing is-a relations to repair.
Experiment 2 - ToxOntology, MeSH and their alignment
In this Subsection we describe a debugging session that
was performed for and with the Swedish National Food
Agency [29]. As part of an initiative to facilitate adequate
identification and display of substance-associated health
effects a toxicological ontology - ToxOntology - was cre-
ated. ToxOntology is an OWL2 ontology, encompassing
263 concepts and 266 asserted is-a relations. The task was
to help the Swedish National Food Agency to create an
alignment with MeSH and debug the ontologies.
As MeSH contains many descriptors not related to the
domain of toxicology, we used parts from the Diseases
[C], Analytical, Diagnostic and Therapeutic Techniques
and Equipment [E] and Phenomena and Processes [G]
branches of MeSH. Further, the hierarchical relation in
MeSH does not necessarily represent the is-a relation. For
this experiment we therefore created an ontology (which
we will call MeSH in this experiment) that contains 9,878
concepts which are related to descriptors in the [C], [E]
and [G] branches and 15,786 asserted is-a relations which
relate to hierarchical relations. We note that the asserted
is-a relations therefore may not always be correct with
respect to the domain.
Aligning ToxOntology andMeSH
Mapping suggestions were created using our ontology
alignment system SAMBO [30]. During the validation
phase a domain expert classified the mapping suggestions
into: equivalence mapping, is-a mapping (ToxOntology
term is-a MeSH term and MeSH term is-a ToxOntology
term), related terms mapping and wrong mapping. The
resulting validated alignment consists of 41 equivalence
mappings, 43 is-a mappings between a ToxOntology term
and aMeSH term, 49 is-amappings between aMeSH term
and a ToxOntology term and 243 related terms mappings.
Further, there is information about 1136 wrong mappings.
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 14 of 19
http://www.jbiomedsem.com/content/4/1/10
Table 5 Experiment 2 - Changes in the alignment (equivalencemapping (?), ToxOntology term is-a MeSH term (?),
MeSH term is-a ToxOntology term (?), related terms (R), wrongmapping (W))
ToxOntology MeSH Original Final Final Final
alignment alignment alignment alignment
manual RepOSE
Metabolism Metabolism ? ? ? removed ?
Photosensitisation Photosensitivity disorders ? R R removed ?, ?
Phototoxicity Dermatitis phototoxic ? R R removed ?, ?
Inhalation Administration inhalation ? W W removed ?, ?
Urticaria Urticaria pigmentosa ? W W removed ?
Autoimmunity Diabetes mellitus type 1 ? R R removed ?
Autoimmunity Hepatitis autoimmune ? R R removed ?
Autoimmunity Thyroiditis autoimmune ? R R removed ?
Gastrointestinal metabolism Carbohydrate metabolism ? W W removed ?
Gastrointestinal metabolism Lipid metabolism ? W W removed ?
Cirrhosis Fibrosis ? R R removed ?, ?
Cirrhosis Liver cirrhosis ? ? ? -
Metabolism Biotransformation ? ? ? -
Metabolism Carbohydrate metabolism ? W W -
Metabolism Lipid metabolism ? W W -
Hepatic porphyria Porphyrias ? ? W removed ?
Hepatic porphyria Drug induced liver injury ? R - removed ?
Debugging using validated alignment
It was not considered feasible to identify defects man-
ually. Therefore, we used the detection mechanisms of
RepOSE. RepOSE computed CMIs, which were then val-
idated by domain experts. As there initially were only
29 CMIs, we decided to repair the ontologies and their
alignment independently in two ways. First, the CMIs
and their justifications were given to the domain experts
who manually repaired the ontologies and their align-
ment. Second, the repairing mechanisms of RepOSE
were used.
A summary of the changes in the alignment and in Tox-
Ontology due to the debugging sessions are summarized
in Table 5 columns original alignment and final align-
ment, and Table 6 column final, respectively. There are
also 5 missing is-a relations for MeSH. In the remainder of
this Subsection we describe the detection and repairing in
more details and compare the manual repairing with the
repairing using RepOSE.
Detection using RepOSE
As input to RepOSE we used ToxOntology and the part
of MeSH described earlier. Further, we used the validated
part of the alignment discussed in Section Aligning Tox-
Ontology and MeSH, that contains the 41 equivalence
mappings, the 43 is-a mappings between a ToxOntol-
ogy term and a MeSH term and the 49 is-a mappings
between aMeSH term and a ToxOntology term.h RepOSE
generated 12 non-redundant CMIs for ToxOntology
(34 in total) of which 9 were validated by the domain
experts as missing and 3 as wrong. For MeSH, RepOSE
generated 17 non-redundant CMIs (among which 2 rela-
tions represented one equivalence relation - 32 CMIs in
total) of which 5 were validated as missing and the rest
as wrong.
Table 6 Experiment 2 - Changes in the structure of
ToxOntology
Added is-a relations Final Manual RepOSE
Absorption ? physicochemical
parameter
Yes Yes Yes
Hydrolysis ? metabolism Yes Yes Yes
Toxic epidermal necrolysis ?
hypersensitivity
Yes Yes Yes
Urticaria ? hypersensitivity Yes Yes Yes
Asthma ? hypersensitivity Yes Yes Yes
Asthma ? respiratory toxicity Yes Yes No
Allergic contact dermatitis ?
hypersensitivity
Yes Yes Yes
Subcutaneous absorption ?
dermal absorption
Yes Yes Yes
Oxidation ? metabolism Yes Yes Yes
Oxidation ? physicochemical
parameter
Yes Yes Yes
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 15 of 19
http://www.jbiomedsem.com/content/4/1/10
Manual repair
The domain experts focused on repairment of ToxOn-
tology and the alignment. Regarding the 9 missing is-a
relations in ToxOntology, these were all added to the
ontology. Further, another is-a relation, (asthma, respira-
tory toxicity), was added, in addition to (asthma, hypersen-
sitivity), based on an analogy of this case with the already
existing is-a relation (urticaria, dermal toxicity) and the
added is-a relation (urticaria, hypersensitivity). This is
summarized in Table 6 column manual. The domain
experts also removed two asserted is-a relations ((asthma,
immunotoxicity) and (subcutaneous absorption, absorp-
tion)) for reasons of redundancy. These is-a relations are
valid and they are derivable in ToxOntology.
The wrong is-a relations for MeSH and ToxOntology
were all repaired by removing mappings in the align-
ment (Table 5 column final alignment manual). In 5
cases a mapping was changed from equivalence or is-a
into related. In one of the cases (concerning cirrhosis in
ToxicOntology and fibrosis and liver cirrhosis in MeSH)
a further study led to the change of cirrhosis ? liver
cirrhosis into cirrhosis ? liver cirrhosis.
The wrong is-a relations involving metabolism in Tox-
Ontology, invoked a deeper study of the use of this
term in ToxOntology and in MeSH. The domain experts
concluded that the ToxOntology term metabolism is
equivalent to the MeSH term biotransformation and
a sub-concept of the MeSH term metabolism. This
observation led to a repair of the mappings related to
metabolism.
Further, some mappings were changed from an equiva-
lence or is-a mapping to a wrong mapping. In these cases
(e.g., between urticaria in ToxOntology and urticaria pig-
mentosa in MeSH) the terms were syntactically similar
and were initially validated wrongly during the alignment
phase.
Repairing using RepOSE
In this second way of repairing the domain expert used
RepOSE. For the 9 missing is-a relations in ToxOntol-
ogy and the 5 missing is-a relations in MeSH, possible
repairing actions (using Source and Target sets) were gen-
erated. For most of these missing is-a relations the Source
and Target sets were small, although for some there were
too many elements in the set to provide for good visual-
ization. For all these missing is-a relations the repairing
constituted of adding themissing is-a relations themselves
(Table 6 column RepOSE). In all but three cases this is
what RepOSE recommended based on external knowl-
edge from WordNet and UMLS. In 3 cases the system
recommended to add other is-a relations, that were not
considered correct by the domain experts (and thus wrong
or based on a different view of the domain in the external
domain knowledge).
For the 3 wrong is-a relations for ToxOntology and the
12 wrong is-a relations for MeSH, the justifications were
shown to the domain experts. The justifications for a
wrong is-a relation contained at least 2 mappings and 0
or 1 is-a relations in the other ontology. In each of these
cases the justification contained at least one mapping that
the domain expert validated to be wrong or related and
the wrong is-a relations were repaired by removing these
mappings (see Table 5 column final alignment RepOSE,
except last row). In some cases repairing one wrong is-
a relation also repaired others (e.g., removing mapping
hepatic porphyria ? porphyrias, repairs two wrong is-
a relations in MeSH: (porphyrias, porhyrias hepatic) and
(porphyrias, drug induced liver injury)).
After this repairing, we detected one newCMI inMeSH.
This was validated as a wrong is-a relation and resulted
in the removal of one more mapping (see Table 5 column
final alignment RepOSE last row).
Debugging using non-validated alignment
In the previous Subsection the validated alignment was
used as input. As a domain expert validated the mappings,
they could be considered of high quality, although we
showed that defects in themappings were detected. In this
Subsection we discuss an experiment with a non-validated
alignment; we used the 41 mapping suggestions generated
by SAMBO with a similarity value higher than or equal to
0.8 and used them initially as equivalence mappings.i
Using RepOSE (in 2 iterations) 16 non-redundant CMIs
(27 in total), were computed for ToxOntology of which 6
were also computed in the debugging session described
earlier. For MeSH 6 non-redundant CMIs (10 in total)
were computed of which 2 were also computed earlier. As
expected, the newly computed CMIs were all validated as
wrong is-a relations and their computation was a result of
wrong mappings. During the repairing 5 of the 7 wrong
mappings were removed, and 2 initial mappings were
changed into is-a mappings. RepOSE can thus be helpful
in the validation of non-validated alignments - a domain
expert will be able to remove wrong mappings that lead to
wrong is-a relations, but other wrong mappings may not
be found.
Discussion and related work
Discussion
Generally, detecting defects in ontologies without the sup-
port of a dedicated system is cumbersome and unreliable.
According to the domain experts, in the cases outlined
in this paper RepOSE clearly provided a necessary sup-
port. Further, visualization of the justifications of possible
defects was very helpful to have at hand as well as a graph-
ical display of the possible defects within their contexts
in the ontologies. Moreover, RepOSE stored information
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 16 of 19
http://www.jbiomedsem.com/content/4/1/10
about all changes made and their consequences as well as
the remaining defects needing amendment.
As the set of CMIs in the second experiment was rela-
tively small, it was possible for domain experts to perform
a manual repair. They could focus on the pieces of Tox-
Ontology that were related to the missing and wrong is-a
relations. This allowed us to compare results of manual
repair with those of repairment using RepOSE. Regard-
ing the changes in the alignment, for 11 term pairs the
mapping was removed or changed in both approaches.
For 2 term pairs the manual approach changed an is-a
relation into an equivalence and for 2 other term pairs
an is-a relation was changed into a wrong relation. These
changes were not logically derivable and could not be
found by RepOSE. For 3 of these term pairs the change
came after the domain experts realized (using the justi-
fications of the CMIs) that metabolism in MeSH has a
different meaning than metabolism in ToxOntology. For
1 term pair (one but last row in Table 5) the equiva-
lence mapping was changed into a wrong mapping by the
domain experts, while using RepOSE it was changed into
an is-a relation. In the final alignment the RepOSE result
was used. Further, through a second round of detection,
using RepOSE an additional wrong mapping was detected
and repaired, which was not found in the manual repair.
Regarding the addition of is-a relations to ToxOntology,
the domain experts added one more is-a relation in the
manual approach than in the approach using RepOSE. It
could not be logically derived that (asthma, respiratory
toxicity) was missing, but it was added by the domain
experts in analogy to the repairing of another missing
is-a relation.
In some cases, when using RepOSE, the justification
for a missing is-a relation was removed after a wrong
is-a relation was repaired by removing a mapping. For
instance, after removing metabolism (ToxicOntology) ?
metabolism (MeSH) in the second experiment, there was
no more justification for the missing is-a relation (hydrol-
ysis, metabolism). However, one advantage of RepOSE
is that once a relation is validated as missing, RepOSE
requires that it be repaired and thus, this knowledge will
be added, even without a justification.
Another advantage of RepOSE is that, for repairing
a wrong is-a relation, it allows to remove multiple is-a
relations and mappings in the justification, even though
sometimes it is sufficient to remove one. This was used, for
instance, in the repair of the wrong is-a relation (phototox-
icity, photosensitisation) in ToxOntology where photosen-
sitisation ? photosensitivity disorders and phototoxicity ?
dermatitis phototoxic were removed. Further, the repair-
ing of one defect can lead to other defects being repaired.
For instance, the removal of these two mappings also
repaired the wrong is-a relation (photosensitivity disor-
ders, dermatitis phototoxic) in MeSH. In general, RepOSE
facilitates the computation and understanding of the con-
sequences of repairing actions.
The repairing of is-a relations in the two experiments
was quite different. To repair wrong is-a relations, in the
second experiment only mappings were removed. This
indicates that the ontology developers modeled the is-a
structure decently. In the first experiment, however, 25
is-a relations and 12 mappings were removed. To repair
missing is-a relations, in the second experiment all miss-
ing is-a relations were repaired by adding the missing is-a
relations themselves. In the first experiment, however, in
30 cases a missing is-a relation was repaired using a more
informative repairing action, thereby adding new knowl-
edge that was not derivable from the ontologies and their
alignment.
As we need at least 3 ontologies and 2 alignments to
find CMMs, there were no such in the discussed exper-
iments. In another experiment [31] in the bibliography
domain with 5 ontologies and 4 alignments, our approach
found CMIs and CMMs and the repairing was done by
adding and removing is-a relations andmappings. Further,
we note that in this setting RepOSE can also be used to
align ontologies. Indeed, in the bibliography experiment
alignments were generated for each pair of ontologies for
which no alignment existed previously.
Our approach for detection CMIs and CMMs is logic-
based. The advantage of this approach is that it is guaran-
teed that all missing is-a relations and mappings derivable
from the network will be found after a number of itera-
tions. A disadvantage is the fact that we need is-a relations
in the ontologies. In the second experiment we used
the hierarchy in MeSH as if it represents is-a relations
although, in general, it does not. Therefore, some identi-
fied wrong is-a relations in MeSH may actually not be is-a
relations in MeSH.
Another constraint of RepOSE pertains to the fact
that adding and removing is-a relations and mappings
not appearing in the computations in RepOSE can be a
demanding undertaking. Currently, these changes need to
be conducted in the ontology files, but it would be use-
ful to allow a user to do this via the system. For instance,
in the second experiment it would have been useful to
add (asthma, respiratory toxicity) via RepOSE. An integra-
tion of RepOSE functionality and functionality of ontology
development systems would solve such issues.
Related work
The approach in this paper is an extension of our pre-
vious work on debugging missing is-a relations in single
taxonomies [10] where we assumed that the existing is-a
structure is correct. This work was extended in [11] where
we dealt with missing and wrong is-a relations in tax-
onomies in a network. The assumption in that work was
that the existing mappings were correct. In this work we
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 17 of 19
http://www.jbiomedsem.com/content/4/1/10
extended the approach to a unified approach that deals
with missing and wrong is-a relations as well as with
missing and wrong mappings and their interactions.
There is not much work on debugging modeling defects
in networked ontologies. The work closest to our own
regarding missing is-a relations is [9], where a similar
method for detection is used and a validation of the results
is also needed. However, repair consists only of adding
the missing is-a relations. The work in [32] discusses
the alignment of AMA and NCI-A and uses the notion
of structural validation to remove mappings that cannot
be structurally validated. Structural validation could be
used to detect candidate missing is-a relations. In [33]
we showed that the problem of repairing missing is-a
relations can be formalized as an abduction problem.
In [8] the authors propose an approach for detect-
ing modeling and semantic defects within an ontology
based on patterns and antipatterns. The patterns and
antipatterns are logic-based and mainly deal with logical
constructs not available in taxonomies. Some sugges-
tions for repairing are also given. In [34] a pattern-based
approach is presented for the detection of wrong and
missing mappings. The approach was tested on the OAEI
Anatomy 2010 data and the results incorporated in the
OAEI Anatomy 2011 data. We have compared the results
of our approach with the results of [34] regarding wrong
mappings. Of the 25 wrong mappings identified by [34],
our approach can identify 21 wrong mappings using the
fullj reference alignment. Our approach identified, fur-
thermore, 8 additional wrong mappings.
There is more work that addresses semantic defects in
ontologies. Most of it aims at identifying and removing
logical contradictions from an ontology. Standard reason-
ers are used to identify the existence of a contradiction,
and provide support for resolving and eliminating it [35].
In [7] minimal sets of axioms are identified which need to
be removed to render an ontology coherent. An algorithm
for finding solutions is proposed which uses a variant of
the single relation heuristic. Similarly, in [5,6] strategies
are described for repairing unsatisfiable concepts detected
by reasoners, explanation of errors, ranking erroneous
axioms, and generating repair plans. The generated solu-
tions, however, are based on other heuristics than [7] and
our work. In [36] the focus is on maintaining the con-
sistency as the ontology evolves through a formalization
of the semantics of change for ontologies. In [13-15] the
setting is extended to repairing ontologies connected by
mappings. In this case, semantic defects may be intro-
duced by integrating ontologies. All approaches assume
that ontologies are more reliable than the mappings and
try to remove some of the mappings to restore consis-
tency. In [13,15] the solutions are based on the computa-
tion ofminimal unsatisfiability-preserving sets orminimal
conflict sets. While [13] proposes solutions based on a
heuristic using distance in WordNet, [15] allows the user
to choose between all, some or one solution. In [14] the
authors focus on the detection of certain kinds of defects
and redundancy. The work in [37] further characterizes
the problem as mapping revision. Using belief revision
theory, the authors give an analysis for the logical prop-
erties of the revision algorithms. The approach in [12]
deals with the inconsistencies introduced by the integra-
tion of ontologies, and unintended entailments validated
by the user. We note that most of these approaches can
deal with ontologies represented in more expressive lan-
guages than in our work. However, few approaches have
implemented systems and the approaches are usually only
tested on small ontologies.
A different setting is the area of modular ontologies
where the ontologies are connected by directional map-
pings and where knowledge propagation only occurs
in one direction. Regarding the detection of semantic
defects, within a framework based on distributed descrip-
tion logics, it is possible to restrict the propagation of local
inconsistency to the whole set of ontologies (e.g., [38]).
Related to the detection of missing relations, there is
much work on finding relationships between terms in the
ontology learning area [39]. In this setting, new ontol-
ogy elements are derived from text using knowledge
acquisition techniques. Regarding the discovery of sub-
sumption relations, one paradigm is based on linguistics
using lexico-syntactic patterns. The pioneering research
conducted in this line is in [40], which defines a set of
patterns indicating is-a relationships between words in
the text. Another paradigm is based on machine learning
and statistical methods. Compared with these approaches,
our detection method uses the ontology network as the
domain knowledge for the discovery of is-a relations. It
is able to deal with multiple ontologies at the same time
rather than a single ontology. However, these approaches
are complementary to our detection method, in that,
results from them, after validation, could be used as input
to the repairing phase.
Conclusions
To obtain satisfactory results in the semantically-enabled
applications, high-quality ontologies and alignments are
both necessary. A key step towards this is debugging the
ontologies and their alignments. In this paper we have
proposed an approach for debugging the is-a structure
of ontologies and mappings between ontologies in a net-
work of taxonomies. We defined important notions and
developed algorithms for detection and repair of the miss-
ing and wrong is-a structure and mappings. We also
implemented a system and showed the usefulness of the
approach through two experiments.
We note that the repairing algorithms can also be used
for single ontologies when initial sets of missing and
Lambrix and Ivanova Journal of Biomedical Semantics 2013, 4:10 Page 18 of 19
http://www.jbiomedsem.com/content/4/1/10
wrong is-a relations are given. Further, the detection phase
in the framework can easily be extended to approaches
using external knowledge to detect missing and wrong
is-a relations and mappings. A first interesting direction
for future work is therefore to integrate such approaches
with RepOSE. For instance, we intend to use our ontol-
ogy alignment system SAMBO [30], that will provide
candidate missing mappings.
Another connection between ontology debugging and
ontology alignment is given in [21] where one of the ontol-
ogy alignment approaches includes detecting missing is-a
relations by using the structure of the ontologies and a
set of correct mappings. The missing is-a relations were
repaired by adding them to the ontologies before start-
ing the actual alignment process. In the future we intend
to study this interaction between ontology debugging and
ontology alignment in a deeper way.
A further interesting direction is to deal with ontologies
represented in more expressive representation languages.
The techniques described in this paper may be partly used
for these ontologies, but a number of conditions (such as
the consequences of negation and disjointness) will need
to be taken into account. We also want to study the inter-
action between repairing actions. For instance, it may be
more important to repair the top level in the ontology first
and in this case, the ranking approach should reflect this.
Endnotes
aThese mappings may be generated by an ontology
alignment system ormanually created by a domain expert.
In general, eachmappingmay be correct or wrong accord-
ing to the intended model of the domains. One of the aims
of this paper is to detect and repair the wrong mappings.
bThe first ontology is a part of AMA, the second ontol-
ogy is a part of NCI-A, and the alignment is a part of the
alignment between AMA and NCI-A as defined in OAEI
2010.
cWe note that the repairing does not require that the
missing and wrong is-a relations and mappings be deter-
mined using the technique for detection described above.
They may have been generated using external knowledge
(e.g., by an ontology alignment system for the mappings)
and then validated by a domain expert or they may have
been provided directly by a domain expert.
dWe also note that using (viscerocranium bone, bone)
as repairing action would also immediately repair the
missing is-a relations (maxilla, bone) and (lacrimal bone,
bone).
eIn the worst case scenario the number of mapped con-
cept pairs is equal to the total number of concept pairs.
In practice, the use of mapped concepts may significantly
reduce the search space, e.g., when some ontologies are
smaller than other ontologies in the network or when not
all concepts participate in mappings. For instance, in the
experiment in Section Experiment 1 - OAEI Anatomy the
search space is reduced by almost 90%.
fSee Additional file 1 for our definition of knowledge
base as well as the initialization.
gIt is well-known that UMLS contains semantic and
modeling defects (e.g., [41,42]). Therefore, we only use
the external resources in the recommendation of the val-
idation of CMIs (and in Section Repairing missing is-a
relations and mappings in the recommendation of repair-
ing actions), but not in the generation. The validation
(and in Section Repairing missing is-a relations and map-
pings the choice of repairing actions) is always the domain
experts responsibility and the recommendations should
only be considered as an aid.
hThe related terms mappings cannot be used in logical
derivation related to the is-a structure of the ontologies
and are therefore not included in the alignment used in
RepOSE.
iFrom the validation we know that these actually con-
tain 29 equivalence mappings, 2 is-a mappings between
a ToxOntology term and a MeSH term, 2 is-a mappings
between a MeSH term and a ToxOntology term, 1 related
terms mapping and 7 wrong mappings.
jThe reference alignment in experiment 1 is a partial
alignment. The full reference alignment was not available
at the time of the experiment.
Additional file
Additional file 1: Supplementary material.
Competing interests
Both authors declare that they have no competing interests.
Authors contributions
PL defined most of the theory. VI did the implementation work. The other
tasks were performed by both authors. Both authors read and approved the
final manuscript.
Acknowledgements
We acknowledge the Swedish National Graduate School in Computer Science
(CUGS), the Swedish Research Council (VR) and the Swedish e-Science
Research Centre (SeRC). We thank Qiang Liu for the implementation work on
an earlier version of the RepOSE system. We thank Ulf Hammerling and Jonas
Laurila Bergman from the Swedish National Food Agency for the cooperation
on the experiments.
Received: 30 March 2012 Accepted: 10 March 2013
Published: 31 March 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Oellrich et al. Journal of Biomedical Semantics 2013, 4:29
http://www.jbiomedsem.com/content/4/1/29
RESEARCH Open Access
Automatically transforming pre- to
post-composed phenotypes: EQ-lising HPO
and MP
Anika Oellrich1*, Christoph Grabmüller2 and Dietrich Rebholz-Schuhmann2,3
Abstract
Background: Large-scale mutagenesis projects are ongoing to improve our understanding about the pathology and
subsequently the treatment of diseases. Such projects do not only record the genotype but also report phenotype
descriptions of the genetically modified organisms under investigation. Thus far, phenotype data is stored in
species-specific databases that lack coherence and interoperability in their phenotype representations. One
suggestion to overcome the lack of integration are Entity-Quality (EQ) statements. However, a reliable automated
transformation of the phenotype annotations from the databases into EQ statements is still missing.
Results: Here, we report on our ongoing efforts to develop a method (called EQ-liser) for the automated generation
of EQ representations from phenotype ontology concept labels. We implemented the suggested method in a
prototype and applied it to a subset of Mammalian and Human Phenotype Ontology concepts. In the case of MP, we
were able to identify the correct EQ representation in over 52% of structure and process phenotypes. However,
applying the EQ-liser prototype to the Human Phenotype Ontology yields a correct EQ representation in only 13.3% of
the investigated cases.
Conclusions: With the application of the prototype to two phenotype ontologies, we were able to identify common
patterns of mistakes when generating the EQ representation. Correcting these mistakes will pave the way to a
species-independent solution to automatically derive EQ representations from phenotype ontology concept labels.
Furthermore, we were able to identify inconsistencies in the existing manually defined EQ representations of current
phenotype ontologies. Correcting these inconsistencies will improve the quality of the manually defined EQ
statements.
Background
Advances in sequencing technologies have opened up new
ways for the systematic exploration of species-specific
phenotypic traits linked to selected mutations of a given
genome, for example the International Mouse Pheno-
typing Consortium (IMPC) analyses systematically the
mouse genome to this end [1,2]. Phenotype descriptions
from such mutagenesis experiments are kept in species-
specific Model Organism Databases (MODs) to ensure
that the representation of the phenotype data is well-
structured in support of further research in compara-
tive phenomics [3]. As the number of available MODs
*Correspondence: ao5@sanger.ac.uk
1European Bioinformatics Institute, Wellcome Trust Genome Campus,
Hinxton, Cambridgeshire, CB10 1SD, UK
Full list of author information is available at the end of the article
increased [4-6], the same happened to the number of
species-specific phenotype ontologies, which nowadays
comprise, amongst others, the Mammalian Phenotype
Ontology (MP) [7], the Human Phenotype Ontology
(HPO) [8] and the Worm Phenotype Ontology (WBPhe-
notype) [9]. The phenotype ontologies serve as resources
for well-chosen and standardised concepts, which sup-
port the annotation work. Since the concepts have been
prepared prior to the curation work, these ontologies are
therefore categorised as pre-composed ontologies. How-
ever, these species-dependent phenotype ontologies are
very specific to a single species, and thus do not serve well
the integration of phenotype data across MODs. In order
to facilitate the comparability and exchange of data across
all MODs and to support knowledge discovery across all
species, other phenotype representations are required.
© 2013 Oellrich et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Oellrich et al. Journal of Biomedical Semantics 2013, 4:29 Page 2 of 7
http://www.jbiomedsem.com/content/4/1/29
In principle, there are two ways to achieve interop-
erability between phenotype ontologies: (1) automatic
ontology alignment algorithms, and (2) standardized phe-
notype representations across all species, i.e. the Entity-
Quality (EQ) representation of phenotypes [10]. In the
EQ representation each phenotype is represented with
an entity which is then further described with a qual-
ity, e.g. decreased body weight is composed of the entity
body which is further specified by the quality decreased
weight. This approach is called post-composition of phe-
notype concepts and makes efficient use of existing onto-
logical resources. EQ descriptions have been successfully
applied in a number of studies, focusing on cross-species
phenotype integration [11-13]. Even though EQ represen-
tations are only been used for parts of species-specific
phenotype ontologies, selected experiments have already
demonstrated beneficial results. However, these studies
would certainly profit even more, if more data had been
integrated into this framework.
To date, post-composed phenotype representations
originate mostly from manual curation work which
ensures high quality but is a slow process [14]. Species-
specific pre-composed phenotypes are transformed into
a post-composed representation by applying the Obol
software together with a set of hand-crafted grammar
rules required by Obol [15,16]. This automated step is
then followed bymanual curation step to pick-and-choose
the correct EQ statements from the Obol output as well
as correcting those EQ statements which are incorrectly
formed by Obol. So far, only a subset of the pre-composed
phenotype ontology concepts is available as EQ state-
ments (e.g. 4,783 HPO and 6,579 MP concepts). However,
a higher coverage of concepts is still required (personal
communication with MouseFinder [12] developers) as
well as quality improvements to existing EQ statements
[14].
Furthermore, any ontology is subject to change reflect-
ing the community effort in capturing the domain knowl-
edge. Concepts evolve, become obsolete or change their
representation over time, i.e. the maintenance of the
EQ representations consumes effort and updates are a
very important requirement. Developing an automated
method for the generation of EQ representation from
pre-composed phenotype concept would efficiently sup-
port the manual curation process, improve quality stan-
dards in the maintenance, i.e. reduce curation errors,
and enable a higher pace in the ontology development
cycle.
In this paper, we present a method (called EQ-liser)
that transforms pre-composed phenotype ontologies into
a post-composed representation using EQ. Our prototype
has been applied to MP and HPO concepts to mea-
sure its performance and to identify needs for improve-
ment in the process of automatic transformation of
pre-composed into post-composed phenotype represen-
tations. Our solution not only decomposes pre-composed
phenotype labels, but also discovers inconsistencies in
manually generated EQ statements and in concept labels
from pre-composed phenotype ontologies.
According to our evaluation, our approach generated
correct EQ representation for more than 52% of the MP
concepts from our test set. We could also identify errors
in the existing EQ statements for both HPO and MP, and
label inconsistencies within HPO that caused erroneous
EQ representations in our approach. Our results, informa-
tion about the project and the source code are available
from our project web page [17].
Related work
Our gold standard set of EQ statements allowing cross-
species phenotype comparisons has been produced by
Obol and each EQ statement has been manually curated
thereafter [15,16]. Even though the curated EQ statements
and the Obol software are accessible, the employed gram-
mar rules required to run Obol are not publicly available.
This makes it hard to apply the software to newly created
phenotype statements without contacting the authors.
Furthermore, no data is available on the number of EQ
labels that can correctly be built without the intervention
of a curator.
Köhler et al. 2011 [14] emphasised in their study that
most EQ statements have been generated manually and
pointed out flaws in the existing EQ statements. There-
fore, we suggest and provide an open access software solu-
tion enabling others to perform quality analyses based on
an evaluation file that is generated automatically. We thus
support complete transparency of the automated decom-
position of phenotype representation and also offer new
ways to compare and judge EQ statements from different
resources for their overall improvement.
In a recent study, Groza et al. 2012 [18,19] also sug-
gested the decomposition of pre-composed phenotypes,
but restricted their study to skeletal phenotypes in human
only. The authors use in their approach a corpus of anno-
tated pre-composed phenotype descriptions that contain
entities and qualities. A supervisedmachine learning algo-
rithm is trained on this corpus and afterwards applied
to other pre-composed skeletal phenotypes in order to
identify their entities and qualities. Neither Obol nor
EQ-liser apply machine learning in their algorithm. In
addition, Groza et al.s approach does not comply with
the logical definitions suggested by Mungall et al. and
instead employs a different formalisation to represent
post-composed phenotypes [16,18]. We therefore assume
that in some cases this leads to different entities and qual-
ities used to present a certain phenotype. By contrast, our
EQ-liser method should comply to the definition of enti-
ties and qualities - as suggested in the original study - with
Oellrich et al. Journal of Biomedical Semantics 2013, 4:29 Page 3 of 7
http://www.jbiomedsem.com/content/4/1/29
the goal to evaluate the performance of our algorithmwith
regards to its compliance with the manually assigned EQ
statements.
Results and discussion
Transforming a pre-composed into a post-composed phe-
notype representation requires an analysis of the concept
labels to identify the affected entity and corresponding
qualities relevant to a particular phenotype. The entities
as well as the qualities have to be matched to ontologi-
cal concepts that are provided from other OBO Foundry
ontologies. As use case scenario, we have tested the EQ-
liser method on MP and HPO concept labels. Note that
all decomposition attempts are only executed on structure
and process phenotypes.
EQ-lising the mammalian phenotype ontology
3,549 concept labels (out of 3,761) could be transformed
when processing the concept labels of MPs structure and
process phenotypes. Comparing these to our gold stan-
dard EQ statements shows that 23.7% had been assigned
a correct post-compositional representation by EQ-liser.
Exploiting synonyms in addition, we could improve our
results by 6.7%. If we allow EQ-liser to assign more anno-
tations than a manual curator would do, i.e. we take a
larger number of automatically generated EQ represen-
tation into consideration, we achieve to identify entities
together with their qualities that are correct for 52.2%
of MP concepts. We believe that the relaxing perfor-
mance assessment is reasonable, since all generated EQ
statements will be evaluated by a curator and addition-
ally assigned entities or qualities (apart from the entity
and the quality required to represent the phenotype)
could be removed without much effort, if required. Auto-
matically deriving an EQ representation for more than
half of MPs structure and process phenotypes, is a very
promising achievement for our generalised decomposi-
tion method. Erroneous and thus useless representations
of post-composed phenotype concepts have only been
generated for 5.6% of the concepts. These numbers indi-
cate that the pre-composed concept labels of MP are
already well formed and that the automatic transforma-
tion  with a grain of salt  does generate post-composed
representations that correctly reflect the semantics of the
pre-composed representation.
Mismatches in EQ-lisingMP
We then selected 50MP concepts where the automatically
derived EQ representation and the manually assigned EQ
statements did not match. We manually compared both
EQ representations and identified the reasons for the mis-
match. This lead to the discovery of the following shared
patterns with regards to the three components of the EQ
representations (structure, process, and quality).
A number of mismatches were caused by assigning
wrong PATO annotations due to particular extension
or replacement patterns in the manually designed EQ
statement which cannot yet be picked up with the auto-
mated procedure. For example, the automatically gen-
erated EQ statement quality of increased mitochondrial
proliferation (MP:0006038) corresponds to increased
rate (PATO:0000912) from the manually assigned EQ
statements. However, the automated method chooses
increased (PATO:0000470) as quality for this particu-
lar MP concept. In the same vein, all concept names
containing the phrase increased activity have been anno-
tated with increased rate (PATO:0000912) in the manu-
ally assigned EQ statements which cannot be reproduced
with the automatic method. Furthermore, every pheno-
type concept with the phrase increased ... number in
their label, possesses the quality has extra parts of type
(PATO:0002001) in the manually assigned EQ statement.
The same examples can be found if the term increased
in the concept label is replaced with decreased. All our
examples could be resolved by introducing conditional
replacement rules for PATO concepts, which in return
would lead to a reduction of the contradictory cases and
to an increase in the number of correctly identified EQ
representations.
Furthermismatches resulted frommissed or faulty iden-
tification of the structure entity in the phenotype rep-
resentation, for example when the affected anatomical
structure is named differently in Mouse Anatomy Ontol-
ogy (MA) [20] andMP. Often this is due to singular/plural
divergence, e.g. the MA concept label lumbar verte-
bra (MA:0000312) cannot be automatically attributed
to the MP concept increased lumbar vertebrae number
(MP:0004650) since vertebra and vertebrae differ mor-
phologically. Moreover, mismatches occurred when short
forms for anatomical structures were used, e.g. MP sim-
ply uses coat while MA mentions coat hair. These mis-
matches could be addressed by augmenting the dictionary
in the LingPipe [21,22] MA annotation server or by apply-
ing a stemming to both concept labels and synonyms, and
the underlying annotation dictionary.
The third type of mismatches occurs in the process
entity of the EQ representations. Mismatches partially
resulted from a lack of synonyms in the current GO
annotation server. For example, concept names includ-
ing the process entity salivation were not recognised as
the process saliva secretion contained in GO. In other
cases, different word forms for a concept caused prob-
lems, e.g. smooth muscle contractility and smooth muscle
contraction. Again singular and plural variability caused
mismatches in the process constituent, e.g. MP makes use
of cilia while GO applies cilium representing the plural
and singular of cilium, respectively. The synonym mis-
matches and singular/plural-conflicts can be resolved by
Oellrich et al. Journal of Biomedical Semantics 2013, 4:29 Page 4 of 7
http://www.jbiomedsem.com/content/4/1/29
larger dictionary resources and the integration of stem-
ming prior to the entity recognition step.
In two out of all 50 evaluated concepts, we could iden-
tify an erroneously, manually assigned EQ statement in
our gold standard (corresponds to 4% of the investigated
cases), which have been reported to the curation team for
correction. The errors mainly resulted from older con-
struction patterns in combination with concepts that have
been recently added to the constituent ontologies.
EQ-lising the human phenotype ontology
Then we determined the transformation performance of
our solution on another pre-composed phenotype ontol-
ogy, i.e. we applied EQ-liser to the HPO concept labels.
HPO has been selected, since it serves as ontology for
another mammal species, and we expect that both ontolo-
gies, i.e. HPO and MP, share similar phenotype concepts.
Our analysis was again limited to structural and process
phenotypes only. We used concepts from the Founda-
tional Model of Anatomy (FMA) ontology [23], the Gene
Ontology (GO) [24] and PATO to build post-composed
phenotype representations.
We analysed 3,268 pre-composed concepts, of which
2,731 have obtained an automatically assigned EQ repre-
sentation. Only 231 (8.5%) generated EQ representations
showed an exact match to the manually assigned EQ
statements. If we include synonyms, we can increase the
matching cases to a total of 249 (9.5%). If we then relax
the matching criterion, i.e. allow additionally assigned
entities or qualities in EQ representations, we obtain cor-
rect annotations in 13.3% of the cases. In 25.8% of all
cases, none of the manually assigned entities or qualities
could be reproduced by EQ-liser. Our results demonstrate
that the decomposition of mouse phenotype concepts can
be achieved at a higher rate using lexical features and
synonyms, in contrast to the human counterparts.
Mismatches in EQ-lising HPO
One reason for the mismatches with regards to the qual-
ity in the phenotype representation is again the term
variability in the quality description. For example, HPO
concepts containing either abnormality or abnormali-
ties do not receive the quality abnormal (PATO:0000460)
automatically due to the morphological variability of the
terms. Furthermore, all concepts with reference to abnor-
mality or abnormalities possess the manually assigned
quality quality (PATO:0000001) which cannot be derived
automatically from the pre-composed concept. Moreover,
some terms contained in HPO concept labels are fur-
ther specified in the manually assigned EQ statement.
For example, the term irregular) in Irregular epiphysis
of the middle phalanx of the 4th finger (HP:0009219) is
translated into irregular density (PATO:0002141) in the
manual assignment. Such mismatches can be corrected
by adding special transformation rules in the concept
decomposition step, which would be specific for HPO.
Mismatches in the representation of structure entities
in HPO phenotypes were partially due to diverging nam-
ing conventions in HPO and FMA, e.g. while FMA calls
fingers with a name (index finger or ring finger), HPO
assigns numbers to fingers, such as 2nd finger or fourth
finger. However, HPO does not apply the numbering con-
sistently across all concepts concerned with digits, e.g the
expression thumb is used where the first finger is con-
cerned. Furthermore, HPO is not well standardised with
regards to singular and plural usages of nouns, e.g. (pha-
langes versus phalanx). Mismatches also result from the
introduction of contractions used in HPO concept labels
while FMA uses full descriptions, e.g. premolar instead of
premolar tooth or metatarsal instead of metatarsal bone.
Most of these mismatches can be resolved by augment-
ing the dictionary of the LingPipe FMA annotation server
with additional terms.
Analoguous to mismatches in MP (see section Mis-
matches in EQ-lising MP), mismatches in process enti-
ties were partially due to not supporting synonyms in
the current implementation of the GO server. For exam-
ple, Abnormality of valine metabolism (HP:0010914) does
not obtain the GO annotation valine metabolic pro-
cess (GO:0006573). Such mismatches can be corrected
in future versions of the EQ-liser method by including
synonyms in the current version of the GO annotation
server.
The last type of mismatches occurred rarely and only
when decomposing HPO labels: identical concepts co-
exist in different ontologies, i.e. not all ontologies are
orthogonal although OBO Foundry strives for this goal.
For example, both FMA and GO contain the concept
Chromosome (GO:0005694, FMA:67093) and the devel-
oper of the manually assigned EQ statements is free to
choose either one. This consequently leads to inconsis-
tencies in automated decomposition methods. Another
example for the duplication of a concepts is Anosmia
(HP:0000458, PATO:0000817). These concepts should be
removed during the process of quality assessment through
the OBO Foundry, whereas the decomposition method
may well ignore this aspect. We found this mismatch in
three concepts (6% of the investigated cases). These incon-
sistencies were reported to, confirmed and corrected by
the HPO EQ statement developers and are now available.
Towards a generalised phenotype decomposition
Even though the automated decomposition of HPO con-
cepts lags behind the automated generation of EQ rep-
resentations for MP concepts with the EQ-liser method,
the error analyses for either ontology is similar and
improving the approach would resolve the mismatches
for both ontologies alike. Achieving 52% performance for
Oellrich et al. Journal of Biomedical Semantics 2013, 4:29 Page 5 of 7
http://www.jbiomedsem.com/content/4/1/29
the structural and process phenotypes in MP is a good
start for the automated transformation of pre-composed
labels from a phenotype ontology into a post-composed
representation. However, under the consideration that EQ
statements for MP and HPO have been developed in a
collaborative way and in close range, our method has to
be further validated on other pre-composed phenotype
ontologies. We expect that the performance of our pro-
posed method will increase once the main mismatches
have been addressed and further validation has been per-
formed. We aim to provide a precise automated decom-
position of phenotype labels for all species under the
condition that relevant ontologies for entities and qualities
are available.
Conclusions
EQ-liser generates EQ representations for structural and
process phenotypes from MP and yields correct results in
30% of the cases under strict measures, and 52% under
relaxed measures. In the latter case we assume that we
produce a larger set of annotations under the consid-
eration that a curator will manually assert and approve
the EQ representation before they are used commu-
nity wide, and will remove incorrect assignments. The
decomposition of HPO labels can only be achieved at
a lower rate until solutions for a number of identi-
fied problems have been implemented. Addressing these
problems should also lead the way to a generalised
approach for the automated generation of EQ represen-
tations from pre-composed phenotype labels. Altogether
we will achieve interoperability between species-specific
databases containing phenotypic descriptions of model
organisms.
Apart from decomposing pre-composed phenotype
concept labels, our method is also capable of identifying
inconsistencies in the composition of the pre-composed
labels. While MA and MP follow a rigorous naming
scheme and hence support integration based on concept
labels, FMA and HPO differ in their naming conven-
tions creating obstacles for all data integration efforts.
Furthermore, HPO shows internal inconsistencies in its
naming conventions, which have to be removed for better
interoperability.
Furthermore, we could identify flaws in the manu-
ally assigned EQ statements by systematically compar-
ing them against the automatically generated represen-
tations. We thus improved the quality of the existing
EQ statements and consequently also the performance
of all methods applying these, e.g. PhenomeNET [13] or
MouseFinder [12].
In the future, we aim to cover all phenotypes con-
tained in existing pre-composed phenotype ontologies.
Our solution will be made available to the research com-
munity as a web interface and a command line tool.
Methods
Transforming pre-composed phenotype representations
into post-composed ones requires the identification of
entities and qualities in concept labels. To illustrate the
post-composition of the MP concept abnormal otolithic
membrane (MP:0002895), the manually assigned EQ
statement is provided here:
[Term]
id: MP:0002895 ! abnormal otolithic
membrane
intersection_of: PATO:0000001 ! quality
intersection_of: inheres_in MA:0002842 !
otolithic membrane
intersection_of: qualifier PATO:0000460 !
abnormal
Input data
In the existing, manually derived EQ statements, an entity
is represented with a number of OBO Foundry ontologies
[25] and a quality is always represented using the Pheno-
typic quality And Trait Ontology (PATO) [10,26]. Entity
filling ontologies also differ with the species. Supporting
all ontologies would be beyond the scope of this study.
We therefore limited our approach to two species-specific
ontologies, HPO and MP. More specifically, we only
included phenotype concepts represented in the manu-
ally assigned EQ statements with: the Mouse Anatomy
Ontology (MA) [20], the Gene Ontology (GO) [24], the
Foundational Model of Anatomy Ontology (FMA) [23]
and PATO.We consider this to be corresponding to struc-
tural and process phenotypes. We downloaded a version
of the two phenotype ontologies as .tbl files [27] and
their corresponding EQ statements on the 03.05.2012,
with 9,795 HPO concepts and 9,127 MP concepts. 4,783
HPO and 6,579 MP concepts possess a manual assigned
EQ statement. We note here that our method so far only
supports structure and process phenotypes and therefore
reduced the number of concepts we apply our method
to based on the manually assigned EQ statements. The
reduced data set comprises 3,761 MP and 3,268 HPO
concepts with their corresponding manually assigned EQ
statement.
Deriving PATO cross products
A subset of the PATO concepts constitute a composi-
tion of other PATO concepts. For instance, the concept
decreased depth (PATO:0001472) could be represented
using the PATO concept decreased (PATO:0001997) and
depth (PATO:0001595). To achieve a term-wise compo-
sition of PATO concepts, we downloaded the PATO .tbl
file and applied the filtering and stemming algorithm as
described in section Overview EQ-liser prototype. The
composition of one particular PATO concept corresponds
Oellrich et al. Journal of Biomedical Semantics 2013, 4:29 Page 6 of 7
http://www.jbiomedsem.com/content/4/1/29
to all PATO concepts whose terms form a subset of the
stemmed words contained in the concept name.
After filtering special characters and removing stop
words from the concept names and synonyms, the
remaining textual content was stemmed using a Porter
stemmer [28] provided by Snowball [29]. The stemmer
was applied to all concept names and synonyms. Stemmed
concept labels and synonyms were then pairwise com-
pared and each concept entirely contained in another
(either label or synonym) was recorded. Applying this pro-
cess we retrieved 1,453 PATO concepts (out of 2,290) with
a corresponding cross product.
Overview EQ-liser prototype
Figure 1 shows the processing steps to derive the EQ rep-
resentation from a MP or HPO phenotype concept. Each
of the steps is explained in more detail in the following
paragraphs.
The first step (see Figure 1) in processing the ontol-
ogys downloaded .tbl file was the filtering for special
characters. Therefore, the concept labels contained in
the downloaded .tbl filesa of the ontologies were anal-
ysed for their orthographic correctness [30], i.e. special
Figure 1 EQ-lisers workflow. Shows the individual steps executed
with EQ-liser to decompose a phenotype ontology based on concept
names.
characters, such as e.g. % or -, were excluded. Such
special characters  often special punctuation  poten-
tially cause problems when matching differently punctu-
ated concept labels from several ontologies. Stop words,
such as in or the are part of the common English lan-
guage, considered not to carry any discriminatory infor-
mation and consequently can be removed before analysis
to reduce noise and potential errors resulting from their
inclusion.
After character filtering and stop word removal from all
the concept labels and their synonyms, we used LingPipe
[21] to recognise entities and qualities fromMP and HPO
concepts. The dictionaries for LingPipe were compiled by
using the labels and synonyms provided by the ontology
files for FMA, MA and PATO. For GO, we used an alter-
native approach described in [31] but also implemented
as LingPipe annotation server. A single tagging server has
been established for each ontology. All servers work par-
allel and may assign overlapping annotations which could
potentially result in too many annotations assigned by the
automated method. E.g. in the case of enlarged dorsal root
ganglion (MP:0008490), an MA annotation for dorsal root
ganglion (MA:0000232) and a PATO annotation for dorsal
(PATO:0001233) is assigned. To avoid this behaviour, we
ran a filter process after assigning LingPipe annotations
and removed all annotations that are entirely included in
others. Filtering GO annotations is not yet possible due
to the current implementation of this server but will be
supported in later versions.
In the last step we automatically replaced LingPipes
PATO annotations and combined them into cross prod-
ucts representation where possible (see section Deriv-
ing PATO cross products for further details). We note
here that not all PATO annotations are necessarily com-
bined, only those for which we identified a cross product
before. Consequently, in the before mentioned example
of decreased palatal depth, the two LingPipe annota-
tions would be replaced now with one single annotation
decreased depth. In addition, absent (PATO:0000462) is
replaced in all automated EQ statements with lacks all
parts of type (PATO:0002000) which is commonly used in
the manual assigned EQ descriptions.
Evaluation
To evaluate our results, we introduced a two-step eval-
uation process. We first evaluated the obtained EQ
representation to the available, manually assigned EQ
statements of structural and process phenotypes. In a
second step, we investigated a subset of 50 EQ repre-
sentations of each ontology where automated method
and manual curator do not assign any shared concepts.
Common patterns were identified causing disagreements
in the automatically assigned EQ representation and
are discussed in sections Mismatches in EQ-lising MP
Oellrich et al. Journal of Biomedical Semantics 2013, 4:29 Page 7 of 7
http://www.jbiomedsem.com/content/4/1/29
and Mismatches in EQ-lising HPO, for MP and HPO
respectively.
Endnote
aprovides a tabular view an ontologys data; generated
from .obo files.
Abbreviations
EQ: Entity-quality; FMA: Foundation model of anatomy; GO: Gene ontology;
HPO: Human phenotype ontology; IMPC: International mouse phenotype
consortium; MA: Adult mouse anatomy ontology; MOD: Model organism
databases; MP: Mammalian phenotype pntology; OBO: Open biological and
biomedical ontologies; PATO: Phenotype and trait quality ontology;
WBPhenotype: Worm phenotype ontology.
Competing interests
The authors declare that there are no scientific as well as financial competing
interests.
Authors contributions
AOE outlined the project and designed the experimental set-up. AOE and
CHG implemented the required scripts and server set-up. DRS supervised the
study. All contributed to the manuscript. All authors read and approved the
final manuscript.
Acknowledgements
The authors thank Georgios V. Gkoutos for his close collaboration in analysing
potential errors of the EQ-liser method. He also provided valuable explanations
for patterns contained in the EQ statements of the Mammalian and Human
Phenotype Ontology. In addition, the authors are also grateful to Irina Colgiu
for her fast and reliable implementation of the GO server, used in this study for
annotation purposes. Furthermore, the authors would like to thank Maria
Liakata for valuable input on the draft of this manuscript.
Author details
1European Bioinformatics Institute, Wellcome Trust Genome Campus,
Hinxton, Cambridgeshire, CB10 1SD, UK. 2European Bioinformatics Institute,
Wellcome Trust Genome Campus, Hinxton, Cambridgeshire, CB22 3DQ, UK.
3Intitut für Computerlinguistik, Universität Zürich, Binzmühlestrasse 14, 8050
Zürich, Switzerland.
Received: 20 January 2013 Accepted: 12 April 2013
Published: 16 October 2013
PROCEEDINGS Open Access
Biotea: RDFizing PubMed Central in Support for
the Paper as an Interface to the Web of Data
L J Garcia Castro1, C McLaughlin2, A Garcia2*
From Bio-Ontologies 2012
Long Beach, CA, USA. 13-14 July 2012
* Correspondence:
alexgarciac@gmail.com
2Institute for Digital Information
and Scientific Communication,
College of Communication and
Information, Florida State
University, Tallahassee, Florida,
32306-2651, USA
Abstract
Background: The World Wide Web has become a dissemination platform for scientific
and non-scientific publications. However, most of the information remains locked up in
discrete documents that are not always interconnected or machine-readable. The
connectivity tissue provided by RDF technology has not yet been widely used to
support the generation of self-describing, machine-readable documents.
Results: In this paper, we present our approach to the generation of self-describing
machine-readable scholarly documents. We understand the scientific document as
an entry point and interface to the Web of Data. We have semantically processed
the full-text, open-access subset of PubMed Central. Our RDF model and resulting
dataset make extensive use of existing ontologies and semantic enrichment services.
We expose our model, services, prototype, and datasets at http://biotea.idiginfo.org/
Conclusions: The semantic processing of biomedical literature presented in this paper
embeds documents within the Web of Data and facilitates the execution of concept-
based queries against the entire digital library. Our approach delivers a flexible and
adaptable set of tools for metadata enrichment and semantic processing of biomedical
documents. Our model delivers a semantically rich and highly interconnected dataset
with self-describing content so that software can make effective use of it.
Background
For over 350 years, scientific publications have been fundamental to advancing science.
Since the first scholarly journals, Philosophical Transactions of the Royal Society (of
London) and the Journal de Sçavans, scientific papers have been the primary, formal
means by which scholars have communicated their work, e.g., hypotheses, methods,
results, experiments, etc. [1]. Advances in technology have made it possible for the scienti-
fic article to adopt electronic dissemination channels, from paper-based journals to purely
electronic formats. By the same token, scholarly communication has been complemented
by the adoption of blogs, mailing lists, social networks, and other technologies that
in combination support the tissue, by means of which scholars communicate their work
and establish connections with one another. However, in spite of the advances, scientific
publications remain poorly connected to each other as well as to external resources.
Furthermore, most of the information remains locked up in discrete documents without
machine-processable content. Such interconnectedness and structuring would facilitate
Garcia Castro et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S5
http://www.jbiomedsem.com/content/4/S1/S5 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2013 Garcia Castro et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
interoperability across documents as well as between publications and online resources
resources available online. Scholarly data and documents are of most value when they are
interconnected rather than independent [2].
In an effort to add value to the content of scientific publications, publishers are actively
improving programmatic access to their products. For instance, Nature Publishing Group
(NPG) recently released 20 million Resource Description Framework (RDF) statements,
including primary metadata for more than 450,000 articles published by NPG since 1869.
In this first release, the dataset includes basic citation information (title, author, publica-
tion date, etc.), identifiers, and Medical Subject Headings (MeSH) terms. Their data model
makes use of vocabularies such as the Bibliographic Ontology (BIBO) [3], Dublin Core
Metadata Initiative (DCMI) [4,5], Friend of a Friend (FOAF) [6,7], and the Publishing
Requirements for Industry Standard Metadata (PRISM) [8] as well as ontologies that are
specific to NPG [9]. Similarly, Elsevier provides an Application Programming Interface
(API) that makes it possible for developers to build specialized applications [10].
Semantic Digital Libraries (SDLs) aim at applying semantic technologies in order to
provide uniform access to metadata as well as machine-processable content; in such a
way, SDLs intend to better support information retrieval and classification tasks [11,12].
Within the context of SDLs, ontologies can be used to: (i) organize bibliographic
descriptions, (ii) represent and expose document contents, and (iii) share knowledge
amongst users [12]. Recent efforts such as JeromeDL [11] allow users to semantically
annotate books, papers, and resources. Similarly, the Bricks project [13] aims to integrate
existing digital resources into a shared digital memory. It relies on OWL-DL in order to
support, organize, and manage metadata. Efforts such as DOMEO [14] and the Living
Document [15] illustrate how Semantic and Social Web technologies are being used in
digital libraries within the biomedical domain. DOMEO is a web component developed
using the Google Web Toolkit (GWT) and JavaScript. It allows users to manually or
semi-automatically create unstructured or semi-structured semantic annotations that
can be private, shared within selected groups, or made public. The Living Document
(LD) makes use of the document as an interface to the Web of Data (WoD), a self-
descriptive document fully interoperable with the Web. The LD also acts as a document
router, operating by means of structured and organized social tagging and using existing
ontologies. A desktop tool rather than a digital library, UTOPIA [16] is a Portable Docu-
ment Format (PDF) reader that combines Semantic and Social Web principles with
visualization tools and online content. Similar to DOMEO and LD, UTOPIA also aims
to improve interoperability and user experience.
In this paper, we present our knowledge model for biomedical literature. We aim at deli-
vering interoperable, interlinked, and self-describing documents in the biomedical domain.
We applied our approach to the full-text, open-access subset of PubMed Central (PMC)
[17]. PMC is a free full-text archive of biomedical literature; currently, it includes 1,679
journals. PMC provides an open-access subset; articles in this subset are still protected by
copyright but are also available under the Creative Commons license, i.e., a more liberal
redistribution is allowed. Articles are available as Extensible Markup Language (XML) files
downloadable via File Transfer Protocol (FTP). In our approach, existing ontologies are
brought together in order to facilitate the representation of sections in scientific literature
as well as the identification of biologically meaningful fragments. These are pieces of text
corresponding to proteins, chemicals, drugs, or diseases, among other biological concepts,
Garcia Castro et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S5
http://www.jbiomedsem.com/content/4/S1/S5
Page 2 of 22
within those previously identified sections. By delivering a semantic infrastructure for
scientific publications, i.e., a semantic dataset, we are supporting interoperability as publi-
cations are linked to each other and to biological resources. By embedding biomedical
literature in the WoD it is possible for users and developers to benefit from the advantages
offered by the Linked Open Data (LOD) cloud.
Results
We are RDFizing biomedical literature and providing services in the form of an API. We
define RDFize as a verb, meaning (i) to generate an RDF representation of something
that was originally in a different format and (ii) to convert or transform to RDF. The
Biotea project comprises and makes available (i) a set of RDF files generated from the
open-access subset of PMC and enriched with semantic annotations, (ii) a Web Services
API for querying the RDF dataset, (iii) a SPARQL Protocol and RDF Query Language
(SPARQL) endpoint containing a subset of the RDF files as a proof of concept, (iv) an
article-centric prototype that acts as an interface to the WoD, and (v) an implemented
transformation process from our RDF files to Bio2RDF [18,19]. These services are avail-
able at http://biotea.idiginfo.org. Our dataset comprises 270,834 articles from PMC, dis-
tributed across 2,401 journals. About 40% of these articles correspond to 20 journals;
those are presented in Figure 1.
Our RDFization process orchestrates ontologies such as the Documents Components
Ontology (DoCO) [20], BIBO [3], DCMI [4,5], and FOAF [6,7]; these namespaces have
been added to our SPARQL endpoint so that users do not need to define them as prefixes.
Figure 1 Coverage per journal Coverage per journal; only the top 20, corresponding to approximately
40% of the articles, are presented in this figure.
Garcia Castro et al. Journal of Biomedical Semantics 2013, 4(Suppl 1):S5
http://www.jbiomedsem.com/content/4/S1/S5
Page 3 of 22
Meaningful fragments within sections are automatically marked and enriched by adding
annotations. Such annotations are structured with the Annotation Ontology (AO) [21].
In our model, we follow the four principles proposed by Tim Berners-Lee for publishing
Linked Data [22]: (i) using Uniform Resource Identifiers (URIs) to identify things,
(ii) using Hyper Text Transfer Protocol (HTTP) URIs to enable things to be referenced
and looked up by software agents, (iii) representing things in RDF and providing a
SPARQL endpoint, and (iv) providing links to external URIs in order to facilitate knowl-
edge discovery.
The connectivity tissue supported by our dataset makes it possible to establish networks
of associated concepts across papers (NACAP) [15]. In this way, the retrieved set can be
represented as a graph where nodes are articles and shared terms are edges. This graph-
based navigation allows users to realize how heavily two or more articles are intercon-
nected as well as what terms are shared see section Gene-based search and retrieval, a
first prototype for a complete description of our current implementation. Our semantically
enriched dataset also makes hierarchy-based searching possible. Based on the hierarchy of
classes, retrieval can be widened to direct ascendants or narrowed to direct descendants;
thus, the dataset can be navigated by going up or down in the hierarchy.
RDFized PMC articles
We use BIBO and DCMI Terms to model the bibliographic metadata, DoCO to expli-
citly identify sections, and FOAF to identify authors and organizations. Figure 2 illus-
trates the graph that corresponds to bibliographic data, sections, and content; in this
figure, we present the use of Digital Object Identifiers (DOIs), PubMed Ids, and PMC
Ids as identifiers. Titles and keywords are represented by DCMI Terms. Relations to
other resources representing the same entity are included as owl:sameAs; relations to
webpages are included as rdfs:seeAlso. The abstracts are represented as BIBO elements
and doco:Section. Published data is presented at the top of the figure; authors, which are
found on the right side, are represented as a list of foaf:Person objects. Sections as well
JOURNAL OF
BIOMEDICAL SEMANTICS
Topalis et al. Journal of Biomedical Semantics 2013, 4:16
http://www.jbiomedsem.com/content/4/1/16SHORT REPORT Open AccessIDOMAL: the malaria ontology revisited
Pantelis Topalis1, Elvira Mitraka2, Vicky Dritsou1, Emmanuel Dialynas1 and Christos Louis2,3*Abstract
Background: With about half a billion cases, of which nearly one million fatal ones, malaria constitutes one of the
major infectious diseases worldwide. A recently revived effort to eliminate the disease also focuses on IT resources
for its efficient control, which prominently includes the control of the mosquito vectors that transmit the
Plasmodium pathogens. As part of this effort, IDOMAL has been developed and it is continually being updated.
Findings: In addition to the improvement of IDOMALs structure and the correction of some inaccuracies, there
were some major subdomain additions such as a section on natural products and remedies, and the import, from
other, higher order ontologies, of several terms, which were merged with IDOMAL terms. Effort was put on
rendering IDOMAL fully compatible as an extension of IDO, the Infectious Disease Ontology. The reason for the
difficulties in fully reaching that target were the inherent differences between vector-borne diseases and classical
infectious diseases, which make it necessary to specifically adjust the ontologys architecture in order to comprise
vectors and their populations.
Conclusions: In addition to a higher coverage of domain-specific terms and optimizing its usage by databases and
decision-support systems, the new version of IDOMAL described here allows for more cross-talk between it and
other ontologies, and in particular IDO. The malaria ontology is available for downloading at the OBO Foundry
(http://www.obofoundry.org/cgi-bin/detail.cgi?id=malaria_ontology) and the NCBO BioPortal (http://bioportal.
bioontology.org/ontologies/1311).
Keywords: Malaria, Vector borne disease, IDO, Remedies, OntologyFindings
Background
Although eradicated from most of the non-tropical re-
gions of the world since decades, malaria is still being
considered as one of the major scourges of mankind, af-
fecting hundreds of millions of people in the tropical re-
gions of the world [1]. Recent years have witnessed a
revival of the idea of eradicating the disease, although
this time the prevailing goal is that of elimination, rather
than to completely expunge it [2,3]. To achieve this ob-
jective emphasis has to be put on disease control, aiming
at both the disease as such (prevention, diagnosis and
treatment) and, most importantly, at vector control [1].
On both fronts, different measures have to be chosen
and actions such as vaccine and novel antimalarial drug* Correspondence: louis@imbb.forth.gr
2Institute of Molecular Biology and Biotechnology, Foundation for Research
and Technology-Hellas and Department of Biology, University of Crete,
Heraklion, Crete, Greece
3Centre of Functional Genetics, Medical School, University of Perugia,
Perugia, Italy
Full list of author information is available at the end of the article
© 2013 Topalis et al.; licensee BioMed Central
Commons Attribution License (http://creativec
reproduction in any medium, provided the ordevelopment, innovative strategies for vector control
and vector population monitoring, etc., have to be prom-
inently assisted by approaches based on Information
Technology (IT). It becomes clear that there is a need
for new effective tools that will be able to combine dif-
ferent, yet related datasets covering various aspects of
disease (e.g. epidemiological and entomological data,
intervention efforts, etc.). These tools encompass re-
sources such as smart databases (including decision sup-
port systems), enhanced bioinformatics software and
usage of technologies such as the Internet and mobile
telephony for the fast transfer of data. The latter is espe-
cially crucial, given that malaria usually strikes the
worlds poorest areas, in countries in which general in-
frastructures are often under-developed.
It is now established that ontologies help overcome
several difficulties encountered in the wide usage of IT
resources by achieving enhanced interoperability. This is
the reason why we decided to put emphasis on the de-
velopment of ontologies that cover the domains of both
vector borne diseases, including malaria, and the vectorsLtd. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Topalis et al. Journal of Biomedical Semantics 2013, 4:16 Page 2 of 6
http://www.jbiomedsem.com/content/4/1/16that transmit their pathogens. We have therefore devel-
oped a series of ontologies [4] that describe the anatomy
of mosquitoes and ticks, mosquito insecticide resistance,
as well as malaria as the first disease in this category. It
was early on decided that the latter ontology, IDOMAL
[5], would be built as an extension to IDO, the Infec-
tious Disease Ontology [6]. The first version of
IDOMAL was made publicly available three years ago, at
a time when IDO was still at a very early stage of devel-
opment. This wrong order obviously led to some dis-
crepancies between the two ontologies, which would
have to be reconciled if IDOMAL is to be considered a
bona fide extension of IDO. It is particularly important
to achieve the status as early as possible, since we are in
the process of developing additional ontologies in the
domain of vector borne diseases such as, for example,
IDODEN, a yet unpublished ontology for Dengue fever
[7]. IDODEN follows the same architecture as IDOMAL,
something that proved to be extremely efficient in terms
of the ontology design. We therefore decided to restruc-
ture IDOMAL at this point in order to avoid major fu-
ture repair work on several ontologies. Here, we
describe both the changes introduced in IDOMAL for
this purpose, as well as several other changes that have
been made.
Updating IDOMAL
All significant changes introduced to IDOMAL are indi-
cated in Table 1. In total 222 terms were added, while
another 207 changes of different nature were performed.
All terms and relations that are included in the ontol-
ogies described here are in italics. The first piece of up-
date is the conversion of IDOMAL from the OBO
format to OWL. The advantages and shortcomings of
the one versus the other format are not to be discussed
here. Given, though, the increased spread of the OWL
format among bio-ontologies we decided to proceed
with the conversion, at the same time making both the
OBO and the OWL formatted versions available to the
community. We used the OBOtoOWL script that we
previously described [8] to achieve the conversion.Table 1 List of changes and corrections introduced in IDOMA
Update element Action
Non availability of IDOMAL in OWL Format Used OBOtoOW
Several terms lacked is_a paths to top Completed is_
Multiple (11) root terms Fully adopted B
Availability of terms in other ontologies Imported (merg
Missing terms 146 terms adde
Absence of terms relating to natural products/remedies 76 terms added
Minor problems (typos, individual terms, definitions) Edited/ comple
Discrepancies with IDOThe next change introduced to IDOMAL could be
called technical since it concerned the editing of the
ontology in order to make sure that all terms of the
ontology have complete is_a paths to the root, which,
now, in accordance to BFO (Basic Formal Ontology)
[9-11] is entity. Therefore the term role which was one
of many root-level terms in previous versions, now took
its proper place and is to be found under realizable en-
tity, which is a specifically dependent continuant, which
is a dependent continuant, which is a continuant, which
is an entity  the root term of BFO. Furthermore, condi-
tion, which was also a top-level term, is now a sibling of
role, in accordance to the BFO 1.0 specification [9-11].
Finally, we corrected typos that were found, and added a
small number of terms that had either missed our atten-
tion or their inclusion was deemed necessary given the
latest developments in malaria research.
A further major addition to IDOMAL concerns the in-
clusion of terms from the sub-domain of natural remed-
ies and medicinal plants. Chemotherapy of malaria has
been increasingly hindered by the development of resist-
ance of Plasmodium parasites against antimalarial drugs
[12]. The search for novel antimalarials, therefore, has
now also turned its attention to traditional remedies, in
particular natural products derived from plants. It
should be stressed that, currently, one such product and
its derivatives (artemisinins), against which widespread
resistance had not been developed, are now under such
risk [13]. To cover the domain we used, mostly, a hand-
book [14] that fully covers the domain. We should
underline the fact that we concentrated, obviously, on
terms that described substances and procedures for
which a certain degree of efficacy had been previously
shown. Similar to what is true for conventional anti-
malarials, we do not consider the terms included as
complete, and we are planning to perform more addi-
tions if necessary.
In addition to this important addition we also decided
to perform a wide exchange of terms. The decision
was taken based on the idea that in the now rapidly
expanding field of bio-ontologies the same term is oftenL
Comments
L script to generate Both versions publically available
a paths for all terms
FO hierarchy to top-level entity is now the single root term.
ed) terms from other ontologies
d
(included in those above)
ted
See Table 2 for details
Topalis et al. Journal of Biomedical Semantics 2013, 4:16 Page 3 of 6
http://www.jbiomedsem.com/content/4/1/16defined differently, and is also linked with is_a relations
to terms that are different. For example, querying the
Bioportal [15] one finds that the term symptom is now
described in 15 distinct ontologies. Interestingly, IDO in-
cludes symptom as a quality and has no children terms,
the Ontology for General Medical Science (OGMS) [16]
has entity as its parent and only lists one child (pain)
while, finally, the Influenza ontology (FLU) [17] also has
entity as the parent of symptom (also imported from
OGMS) and has 8 children listed that, though, do not
include pain. It should be noted that FLU is an exten-
sion of IDO. In IDOMAL, symptom (with a large num-
ber of children) is linked to condition of the malaria
host, which is obviously a child of condition. We decided
to replace all children of symptom with those listed by
the Symptom Ontology (SYMP) [18], in which symptom
is the root. By term replacing we actually mean the mer-
ging of the terms from SYMP to those in IDOMAL.
Merging, instead of replacing leaves both IDs intact
within IDOMAL, and therefore if somebody has already
been using IDOMAL there will be no need to perform
any changes in the software that uses the ontology.
The choice of SYMP was made purely on the fact
that alternatives such as OGMS do not list the terms
that we needed. Finally, we should state at this point that
IDO is using the symptom term imported from OGMS
(see below).
IDOMAL and IDO
As mentioned earlier, due to the timing of development
of the two ontologies the published version of IDOMAL
has some features that make it difficult, as such, to be
called an extension of IDO in the latters present form.
The example stated above, i.e. the term symptom being
imported from two different ontologies, exemplifies this
problem while, at the same time, it also shows that the
differences are not necessarily irreconcilable: the easiest
solution for this kind of discrepancy would be to simply
merge the two terms. There are several more examples
of how some differences may be eliminated. Table 2 lists
these, showing in addition the actions taken or to be
taken. For example, while antiparasitic drug is a role in
IDOMAL, a term antiparasitic disposition is found in
IDO, defining antiparasitic material entity as entity,
which bears antiparasitic disposition (IDO contains dis-
positions such as antibacterial, antifungal, antiparasitic,
antimicrobial, antiviral). We could easily reconcile the
difference by accepting that antimalarial drug in
IDOMAL is a role borne by a material entity which has
antiparasitic disposition and is given to a patient to treat
malaria. This would follow the example set in IDO by
antiseptic role (definition: A role borne by a material en-
tity in virtue of the fact that it has an antimicrobial dis-
position and is applied to an anatomical entity of a livingorganism). There are a few more cases of discrepancy
between the two ontologies and for some of them we
have decided to adopt the IDO point of view. For ex-
ample, endemicity that was a disposition in IDOMAL
will be changed to quality of a population, and so
will be the terms holoendemicity, hypoendemicity and
mesoendemicity, which are absent from IDO. We have
also changed resistance from quality to disposition; al-
though a good case was made for the fact that resistance
is a disposition [19], we should nevertheless state here
that resistance is in most, if not all cases a genetic
phenotype. And without going into further discussions,
we simply state that phenotypes are usually considered
to be quality, possibly because of the fact that several
of them are visible (e.g. white eyes, ectopic expres-
sion, etc.). Finally, habitat, a spatial region so far in
IDOMAL, has been changed to site like in IDO.
What also had to be changed to fit the present onto-
logical representation is to define breeding site as a
role carried by material entity (i.e. site).
In spite of the changes made, a series of issues remain
that havent yet been resolved. Some of them, in our
opinion, are relatively secondary and they could be re-
solved easily. For example, terms such as treatment and
refractoriness which we deem to be necessary for an
ontology of vector borne diseases could be carried again
by IDO, from which they were obsoleted some time ago.
The remaining open issues are due to the distinctive
properties of vector borne diseases. These infections are
characterized by the fact that they arise through the bio-
logical interactions between three organisms (patient/
host, vector and pathogen), rather than only two as is
common in the vast majority of infectious diseases.
Thus, an ontology such as IDOMAL has to capture all
three organisms, as well as prominently include terms
on the respective populations. For example, control of
malaria, eventually leading, perhaps, to its elimination, is
predominantly based on vector control. This involves
measures aimed, for example, at reducing mosquito pop-
ulations, possibly using genetic approaches [20] or, as
may be the case in the future if planned strategies suc-
ceed, at replacing vector populations with others that
will simply not be able to transmit the pathogen [21].
Both IDO and IDOMAL use the BFO [9-11] as an
upper level ontology. IDO would then describe the
infectious disease domain, while IDOMAL would ideally
be placed below it. The current structure of IDO,
though, does not allow for a full deployment of the mal-
aria domain. IDOMAL has separated several classes of
terms in three main groups namely the patient/host,
the vector and the pathogen or, to be more precise, into
six groups since populations are treated separately.
The reason for this is obvious: many terms apply to both
patient/host and vector (both being metazoan). Similarly,
Table 2 Differences between IDO and IDOMAL and steps (to be) taken to unify the ontologies
IDO-IDOMAL comparison Action Comments
IDO: symptom imported from OGMS, no children
IDOMAL: native term, 39 children Imported/merged terms from
Symptom Ontology
IDO: endemicity is quality of a population
IDOMAL: endemicity is disposition Changed in IDOMAL, now as in IDO
IDO: habitat is site (material entity)
IDOMAL: habitat is spatial region Changed in IDOMAL, now as in IDO
IDO: breeding site absent
IDOMAL: Anopheles breeding site is a role Changed to Anopheles breeding
site role born by site (material entity)
IDO: epidemiological types [of an infectious
disease] absent
IDOMAL: epidemiological types of malaria present Discuss with IDO developers Needed in IDOMAL
IDO: antiparasitic material entity is material entity
bearing antiparasitic disposition
IDOMAL: antiparasitic drug is a role. Issue solved in IDOMAL: antimalarial
drug is a role born by a material entity
which has antiparasitic disposition
Similar for antibiotic role/ disposition, etc.
IDO: resistance is a disposition
IDOMAL: resistance is quality (of (vector population) Changed in IDOMAL, now as in IDO
IDO: infection is a material entity
IDOMAL: infection is not present; infectious disposition
is not present
No action needed Terms are not needed in IDOMAL
IDO: vector competence is not present
IDOMAL: vector competence is present Discuss with IDO developers Needed in IDOMAL
IDO: infectious disease control is absent
IDOMAL: malaria control is present Discuss with IDO developers Needed in IDOMAL
IDO: No terms for decision support systems, [disease]
and vector control, intervention methods
IDOMAL: Corresponding terms present Discuss with IDO developers Needed in IDOMAL
IDO: refractoriness is obsolete term
IDOMAL: refractoriness is present Discuss with IDO developers Needed in IDOMAL
IDO: treatment is obsolete term in IDO
IDOMAL: treatment is present Discuss with IDO developers Needed in IDOMAL
IDO: holoendemicity / hypoendemicity / mesoendemicity
are missing
IDOMAL: terms are present Discuss with IDO developers Needed in IDOMAL
IDO: quality of infectious disease is absent
IDOMAL: quality of malaria is present Discuss with IDO developers Needed in IDOMAL, acts as a placeholder
for terms relating to malaria-specific
interventions.
IDO: process of infectious disease is absent
IDOMAL: process of malaria is present Discuss with IDO developers
IDO: no differentiation between host/patient and
pathogen
IDOMAL: differentiates between host/patient, vector
and pathogen
Discuss with IDO developers Needed in IDOMAL, coverage of physiological
and pathophysiological processes occurring
in host and/or vector and/or parasite.
Topalis et al. Journal of Biomedical Semantics 2013, 4:16 Page 4 of 6
http://www.jbiomedsem.com/content/4/1/16
Topalis et al. Journal of Biomedical Semantics 2013, 4:16 Page 5 of 6
http://www.jbiomedsem.com/content/4/1/16the latter separation, of course, is due to the fact that
several terms are specific for populations, rather than in-
dividuals; this is especially true for vector control.
Another problem that is not yet resolved is how to
list in a grouped, ontologically correct form terms
such as pathogen specific form of malaria and epidemio-
logical type of malaria. A solution could be to create
sub-ontologies for each one of the different forms of
malaria (i.e. for P. falciparum, P. malariae, P. ovale and
P. vivax); we consider this to be impractical in several
obvious aspects. For the time being we keep the prob-
lematic class quality of malaria and well aim at finding
an appropriate way to describe these features in collab-
oration with the IDO consortium. The class process of
malaria, thus, groups a series of physiological and
pathophysiological processes occurring in the patient/
host and/or the parasite. Finally, IDOMAL has no place-
holders for vector-specific processes (e.g. host seeking)
or qualities (e.g. vector competence). Of course, all of
these terms could be listed as direct children of process
and quality, but we think that a more detailed classifi-
cation would benefit the users of the ontology and,
especially, would make it easier to design other ontol-
ogies for vector-borne diseases. A similar consideration
is valid for malaria prevention and vector control, terms
that need to be included, and are now under the place
holder process of malaria. It should be noted here that
recently the Vector Surveillance and Management
Ontology (VSMO) was published that covers the do-
main of vector control [22]. This development may
make it easier to find a partial solution to the last men-
tioned problem.
Conclusions
It was not unexpected that IDOMAL had to undergo
several updates, partial revisions and expansion during
the three years after it was published, which all are sum-
marized in this report. Not only is it legitimate to always
try to obtain a better product, but also some of the
changes are dictated by the needs of the community (e.g.
remedies and natural products) or recent developments
in the field. As mentioned in the beginning, improved
IT tools are becoming indispensable, especially as high
throughput technology develops and provides more data.
In the case of malaria and other vector borne diseases,
this evolution is obvious. Only about ten years after the
determination of the genome sequence of Anopheles
gambiae [23] tens of genomes of different vectors have
become available [24]. Although so far genes are usually
only annotated with GO terms [25], the day is not
far when they, and other data in genomic/biological
databases, will also be annotated with ontological
terms describing these domains, such as, for example,
VectorBase, the database that covers arthropod diseasevectors [26]. Moreover novel IT tools such as decision
support systems are already making use of ontologies
[27] and, even more, tools are planned that will be able
to direct information to and from ontologies and data
holders [28]. In the domain of vector-borne diseases,
IDOMAL and MIRO, an ontology of Mosquito Insecti-
cide Resistance [29] that has now been fully integrated
in IDOMAL, are used by newly developed Decision Sup-
port Systems for vector-borne diseases [30,31]. Further-
more, VSMO also uses a several terms that have been
imported from IDOMAL [22]. It becomes clear that the
availability of all the new, open bio-medical ontologies
provides ways to achieve enhanced interoperability be-
tween databases and to expand the title of the original
publication of the Gene Ontology [32] to tools for the
unification of bio-medical sciences.
N.B. Both IDOMAL versions are available for
downloading: the OBO version is at the OBO Foundry
and at the NCBO BioPortal, while the OWL version is avail-
able at: http://anobase.vectorbase.org/idomal/.
Abbreviations
BFO: Basic formal ontology; FLU: Influenza ontology; GO: Gene ontology;
IDO: Infectious disease ontology; IDODEN: Infectious disease ontology-
dengue; IDOMAL: Infectious disease ontology-malaria; MIRO: Mosquito
insecticide resistance ontology; NCBO: National center for biomedical
ontology; OGMS: Ontology for general medical science; OBO: Open and
biomedical ontologies; OWL: Web ontology language; SYMP: Symptom
ontology; VMSO: Vector surveillance and management ontology.Competing interests
The authors declare that they have no competing interests.Authors contributions
PT was responsible for the final updating process and oversaw the regular
operations; EM, VD and ED were responsible for individual parts of the
update project; CL researched the domain of natural products and wrote the
first draft of the paper (which all other authors helped finalize) and
coordinated the study. All authors read and approved the final manuscript.Acknowledgements
We would like to dedicate this paper to the memory of an outstanding
malariologist, an excellent geneticist and, first of all, a very dear friend, Prof.
Mario Coluzzi. We thank Dr. Frank Collins for his continuous support. This
study was funded by the National Institutes of Health/National Institute for
Allergy and Infectious Diseases (grant numbers HHSN266200400039C,
HHSN272200900039C) in the frame of the VectorBase project and was also
partially supported by grants 223736 (Transmalariabloc) and 201588
(Evimalar) of the FP7 (HEALTH) programme of the European Commission. CL
was supported, in part, by an i-Move fellowship from the Polo d'Innovazione
di Genetica Genomica e Biologia, Perugia.
Author details
1Institute of Molecular Biology and Biotechnology, Foundation for Research
and Technology-Hellas, Heraklion, Crete, Greece. 2Institute of Molecular
Biology and Biotechnology, Foundation for Research and Technology-Hellas
and Department of Biology, University of Crete, Heraklion, Crete, Greece.
3Centre of Functional Genetics, Medical School, University of Perugia,
Perugia, Italy.
Received: 29 May 2013 Accepted: 27 August 2013
Published: 13 September 2013
Topalis et al. Journal of Biomedical Semantics 2013, 4:16 Page 6 of 6
JOURNAL OF
BIOMEDICAL SEMANTICS
Knüpfer and Beckstein Journal of Biomedical Semantics 2013, 4:24
http://www.jbiomedsem.com/content/4/1/24
RESEARCH Open Access
Function of dynamic models in systems
biology: linking structure to behaviour
Christian Knüpfer* and Clemens Beckstein
Abstract
Background: Dynamic models in Systems Biology are used in computational simulation experiments for addressing
biological questions. The complexity of the modelled biological systems and the growing number and size of the
models calls for computer support for modelling and simulation in Systems Biology. This computer support has to be
based on formal representations of relevant knowledge fragments.
Results: In this paper we describe different functional aspects of dynamic models. This description is conceptually
embedded in our meaning facets framework which systematises the interpretation of dynamic models in structural,
functional and behavioural facets. Here we focus on how function links the structure and the behaviour of a model.
Models play a specific role (teleological function) in the scientific process of finding explanations for dynamic
phenomena. In order to fulfil this role a model has to be used in simulation experiments (pragmatical function).
A simulation experiment always refers to a specific situation and a state of the model and the modelled system
(conditional function). We claim that the function of dynamic models refers to both the simulation experiment
executed by software (intrinsic function) and the biological experiment which produces the phenomena under
investigation (extrinsic function). We use the presented conceptual framework for the function of dynamic models to
review formal accounts for functional aspects of models in Systems Biology, such as checklists, ontologies, and formal
languages. Furthermore, we identify missing formal accounts for some of the functional aspects. In order to fill one of
these gaps we propose an ontology for the teleological function of models.
Conclusion: We have thoroughly analysed the role and use of models in Systems Biology. The resulting conceptual
framework for the function of models is an important first step towards a comprehensive formal representation of the
functional knowledge involved in the modelling and simulation process. Any progress in this area will in turn improve
computer-supported modelling and simulation in Systems Biology.
Background
The modelling of complex biological systems is faced with
huge challenges. New high-throughput experimentation
generates enormous amounts of data which forms the
empirical basis for dynamic models in Systems Biology
(for short called bio-models in this paper). The manage-
ment and analysis of the data iceberg [1] is impossible
without computer support. Modelling and simulation on
the systems level requires the integration of data from dif-
ferent sources on various levels in a collaborative manner
[2] and the incorporation of existing models. See [3] for
a detailed discussion of the challenges for modelling and
simulation in Systems Biology.
*Correspondence: christian.knuepfer@uni-jena.de
Artificial Intelligence Group, University of Jena, Jena, Germany
Bio-models are mathematical descriptions of biological
processes which are used to answer biological questions.
Generally, these questions are causal questions asking for
mechanistic explanations of dynamic biological phenom-
ena. In order to serve as mechanistic explanations it is
necessary that the temporal behaviour of a bio-model
can be simulated by means of computers. Therefore, the
bio-model has to be encoded in an appropriate computer-
understandable format. System-level understanding of
biological phenomena requires the integration of bio-
models from different abstraction levels expressed in dif-
ferent modelling paradigms [4]. Computer support for
modelling and simulation is an important contribution to
meet the challenges in Systems Biology. This computer
support has to be based on formal representations of rel-
evant knowledge fragments. A first step towards such
© 2013 Knüpfer and Beckstein; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the
Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,
distribution, and reproduction in any medium, provided the original work is properly cited.
Knüpfer and Beckstein Journal of Biomedical Semantics 2013, 4:24 Page 2 of 9
http://www.jbiomedsem.com/content/4/1/24
a formal representation is an ontological analysis of the
domain of knowledge in question. The result of such an
analysis will be a conceptual framework, i.e. a set of cat-
egories and relations between them which can be used to
capture the respective knowledge. What we call a con-
ceptual framework is usually called domain model or
conceptual model in computer science. In the context
of this work the use of the traditional terms seems to be
misleading because the omnipresent term model is used
with different meaning.
The function of a bio-model describes its role in the
scientific process of finding mechanistic explanations for
biological phenomena. Beside this teleological aspect of a
models function there are pragmatical aspects: The func-
tion of a bio-model also describes the use of the model in
simulation experiments. In such simulation experiments
behaviours of the model are generated. Thus, function
links a model (structure) to its behaviours. To summarise,
the function of bio-models describes why and how to use
models in simulation experiments:
Models can fulfil many functions as we have seen; but
they generally perform these functions not by being
built, but by being used. Models are not passive
instruments, they must be put to work, used, or
manipulated. ([5], p.32)
Physiology is the branch of biology dedicated to func-
tion of living systems. The notion of function as used
in physiology incorporates two aspects of a biological
entity: (1) The function states a role an entity plays
as a component of an encompassing process. The bio-
logical function is therefore tied to a specific process.
(2) The function characterises the behaviour which the
entity has to exhibit for fulfilling its role. Biological func-
tion links system structure (the entities and relation)
to behaviour. The most famous example is the func-
tion of genes which links the genotype to the pheno-
type. In this paper we transfer this notion of function to
bio-models.
We claim that the function of a bio-model links its struc-
ture to its behaviour. Before we describe the function
of a bio-model in detail we will introduce the mean-
ing facets which provide a framework to systematically
describe the knowledge involved in creating and using
bio-models. In the meaning facets framework the func-
tion of a bio-model mediates between its structure and its
behaviour (cf. Figure 1). The present paper focuses on how
the function plays this mediating role.
In order to do so, we first analyse the function of bio-
models and develop a conceptual framework. Then, the
conceptual framework is used to show whether and how
the function can be formalised. Thereby, we review exist-
ing formal approaches and identifymissing pieces. For one
of the identified gaps we outline a possible filler. Finally,
we compare our conceptual framework with related work.
Meaning facets of bio-models
Generally, a mathematical model establishes a relation
between a system under observation (what Rosen [6] calls
the natural system) and a formal system. Rosen calls this
the Modeling Relation. In order to be useful the mod-
elling relation has to be an isomorphism: the structure
of the formal system, expressed as entities and relations
between them, can be mapped onto the structure of the
natural system, recognised as objects and interactions.
In our analysis of the modelling process in Systems
Biology we make two important observations about bio-
models [7]. First, a bio-model has a dual interpretation:
In order to be used in computer simulations the encoded
model has to be intrinsically interpreted with respect
to the encoding format used. This can be done without
referring to the modelled natural system. Furthermore, a
model has to be related to the natural system (cf. Rosens
modelling relation [6] mentioned above), i.e. it has to
be extrinsically interpreted. Second, dynamic models are
considered on three levels: Models are systems of com-
ponents and relations (model structure). Models are used
in simulation experiments for answering biological ques-
tions (model function). Models exhibit temporal changes
(model behaviour). The three levels of bio-models are
inspired by function modelling in engineering (see, e.g.,
[8]). In function modelling formally represented teleolog-
ical knowledge is used for the design and diagnosis of
engineering artefacts.
We claim that full computer support of the overall mod-
elling process has to be based on a complete description
of a bio-model encompassing all six meaning facets [9],
i.e. the intrinsic and extrinsic sides on each of the three
levels. If we look at the scientific process of modelling
a biological system in order to explain data observed in
experiments we can clearly identify the three levels and
the dual interpretation (see Figure 1). More details about
the six meaning facets are given by Knüpfer et al. [9]
including a complete description of an example model
with respect to the meaning facets. However, in this paper
we will focus on the functional facets of bio-models.
Conceptual framework for the function of
bio-models
A bio-model used in simulation experiments in order
to answer biological question has an intrinsic and an
extrinsic function (see above) which can be understood
in teleological and in pragmatical terms. There is a third
perspective on the function of bio-models, which we call
conditional: the role played by an entity depends on the
context; in different situations or under different condi-
tions an entity may have different roles. In the following
Knüpfer and Beckstein Journal of Biomedical Semantics 2013, 4:24 Page 3 of 9
http://www.jbiomedsem.com/content/4/1/24
computer
reality
intentionmodel dynamics
formal system
target system
results
data
simulation
experiment
m
od
el
lin
g
explanation
Figure 1 Structure, function and behaviour of a bio-model. Structure (blue/left), function (yellow/middle) and behaviour (green/right) of a
bio-model. The model relates the (intrinsic) computer representation with the (extrinsic) biological reality. (1) Structure: The biological target system
is transferred into a model which can in turn be intrinsically interpreted as a formal system. This establishes a modelling relation between the two
systems. If there is a valid mapping between the components of the target system and the formal system, we call the model a competence model.
(2) Function: The intention of the model is its use in simulation experiments for explaining biological phenomena observed in biological
experiments. (3) Behaviour: The simulation experiments produce results which can be interpreted as the dynamics of the model. This dynamics can
be related to the interpreted data of the biological experiments. If the behaviour of the model is similar to the behaviour of the biological system,
we call the model a performance model with respect to the corresponding biological phenomena. Explanation is using a competence model in an
simulation experiment which makes it a performance model with respect to the biological phenomena to explain.
we will investigate the individual functional aspects in
detail. The introduced concepts were distinguished by
writing them in italics throughout the paper. Table 1
summarises the used concepts.
The presented conceptual framework extends the
Minimum Information About a Simulation Experiment
(MIASE, [10]) which is a reporting guideline for simula-
tion experiments performed on bio-models. The rules of
MIASE regard the conditional and pragmatical function
of bio-models. The conceptual framework presented here
also incorporates the teleological function and relates the
description of simulation experiments (intrinsic function)
to the biological counterparts (extrinsic function).
It should be noted that a bio-model can have more than
one function if it be used to answer different biological
questions. The separation of structure and function of
Table 1 Functional aspects of bio-models
Intrinsic Extrinsic
Teleological function Intended use Model use intention
Constraints Assumptions
Conditional function Model instantiation Boundary conditions
Initial values Initial state
Pragmatical function Setup Experimental settings
Post-processing Result calculations
bio-models in our meaning facets framework facilitates
this re-use of the samemodel for different purposes. How-
ever, the model structure can restrict the adequate use of
a model in so far as not every model structure is able to
answer a given question.
Teleological function
Bio-models play an important teleological role in answer-
ing questions about the biological system under investiga-
tion. These questions lead to specific intentions of using
the model in simulation experiments (called model use
intentions in the following). Model use intentions may
be, for example, the explanation of observed behaviours
or the prediction of possible behaviours. What is an
accepted explanation or prediction depends on the sci-
entific field and community [11]. Furthermore, there are
specific assumptions underlying the model use inten-
tions, e.g. the assumption that a specific reaction is very
fast. The extrinsic teleological function of bio-models
refers to the model use intentions and the underlying
assumptions.
The intended use of the model in simulation experi-
ments has to reflect the model use intentions. Depending
on these intentions different types of simulation exper-
iments may be appropriate. Often, different simulation
experiments have to be combined in order to yield the
desired outcome. Constraints which are in line with the
assumptions are imposed on the simulation experiments.
Knüpfer and Beckstein Journal of Biomedical Semantics 2013, 4:24 Page 4 of 9
http://www.jbiomedsem.com/content/4/1/24
Such constraints may include value restrictions, ratios
between values, and conservation rules. The intrinsic tele-
ological function of a bio-model describes its intended use
and imposed constraints.
Conditional function
The questions addressed by the model, in general, assume
certain boundary conditions and a specific initial state
of the experimentally observed biological system. The
boundary conditions determine the environment of the
biological system (e.g. temperature, pH, nutrition) and
may be reflected by corresponding kinetic data. It is also
possible to give plausible ranges for some of the condi-
tional values instead of single values. The extrinsic con-
ditional function of a bio-model is expressed in terms of
boundary conditions and initial states.
A bio-model contains state variables and formal param-
eters. In order to be used in simulation experiments,
concrete values must be assigned to all parameters. This
is called model instantiation. Furthermore, the initial
values for all state variables have to be chosen. The intrin-
sic conditional function of a bio-model makes the model
ready to be used in simulation experiments by means of
model instantiation and choice of initial values.
Pragmatical function
As mentioned above, bio-models explain or predict the
behaviour of the modelled biological system. This tele-
ological function requires a complementary description
of the experimental settings which lead to the observed
behaviour and allow for the verification of the predicted
behaviour. Usually the experimental data is transformed
into the final observations by means of result calcula-
tions. The extrinsic pragmatical function of a bio-model
describes the experimental settings and result calculations
related to the dynamic phenomena under investigation.
Bio-models are used in simulation experiments. The
setup of the simulation experiments precisely describes
the procedure applied to the instantiated model. This
involves the simulation algorithm used and specific set-
tings for this algorithm. In addition, the exact steps,
their order and applied perturbations have to be speci-
fied. Post-processing of the raw data from the simulation
experiments generates the desired outcome. The intrinsic
pragmatical function of a bio-model describes the setup of
the simulation experiments applied to themodel structure
and the post-processing which finally produces the model
behaviour.
Formal approaches to the function of bio-models
In this section we briefly review existing approaches for
formalising the different functional aspects of bio-models.
The aim of this review is to investigate the coverage
of formal representations of functional knowledge as a
basis for computer support. Table 2 provides an overview
of the formal approaches. The classification of the for-
mal approaches in checklists, languages and ontologies is
motivated by [12]. Table 2 can be seen as a detailed view
of the middle column Simulation description of Figure 1
from ([12], p.7) which reviews existing approaches for bio-
models. The formal approaches displayed in Table 2 and
the gaps are discussed in the following sections.
Checklists
To start the formalisation of a specific kind of sci-
entific data, like bio-models, experiment protocols, or
experimental results, the responsible community first
of all has to agree on the information needed. So-
called Minimum Information Checklists state what
information at least has to be described. Checklists
are semi-formal in that they structure the information.
The information, however, is still formulated in natural
language.
The concrete intrinsic functional description of the sim-
ulation experiments is the main focus of MIASE [10].
MIASE requests information about the simulation type
Table 2 Formal approaches for functional aspects of bio-models
Functional aspect Checklists Languages Ontologies
Teleological function
Intrinsic: Intended use MIASE SED-ML MINTENTO
Extrinsic: Model use intention Unknown Unknown MINTENTO
Conditional function
Intrinsic: Model instantiation MIASE SED-ML Not applicable
Extrinsic: Boundary conditions MIBBI SABIO-RK* XCO
Pragmatical function
Intrinsic: Setup MIASE SED-ML KiSAO
Extrinsic: Experimental settings MIBBI FuGE MMO
*The database scheme of SABIO-RK could be seen as a formal language for experimental conditions.
Knüpfer and Beckstein Journal of Biomedical Semantics 2013, 4:24 Page 5 of 9
http://www.jbiomedsem.com/content/4/1/24
(intended use, teleological function), the model instantia-
tion (conditional function), the exact experimental setup,
and the necessary post-processing (pragmatical function).
For the description of the extrinsic function there are
a lot of specific checklists listed in the MIBBI portal
(Minimum Information for Biological and Biomedi-
cal Investigations, [13]), for example, MIAME (Min-
imum Information About a Microarray Experiment,
[14]) and MIFlowCyt (Minimum Information about a
Flow Cytometry Experiment, [15]). The listed check-
lists concern information about the boundary conditions
(conditional function) and the experimental settings
(pragmatical function) for specific types of biologi-
cal experiments. We are not aware of any checklists
for the model use intentions (teleological function) of
bio-models.
Languages
In most cases a checklist can be translated to a spec-
ification of a formal language. Such a formal language
for describing simulation experiments is SED-ML (Sim-
ulation Experiment Description Markup Language, [16])
which allows to specify the type of simulation, i.e. the
intended use (intrinsic teleological function), the model
instantiation and initial values (intrinsic conditional func-
tion) as well as the setup and post-processing of the
simulation experiments (intrinsic pragmatical function).
However, SED-ML can not fully describe the intended
use and the imposed constraints. The Systems Biology
Markup Language (SBML, [17]) used for the encoding
of the model structure is also able to determine parame-
ter values (model instantiation) and initial values. But, in
order to be able to reuse models in different simulation
experiments we suggest to clearly separate descriptions
of models (in SBML) from descriptions of their use
(in SED-ML).
There are languages for describing extrinsic experi-
mental conditions and specific biological experiments.
SABIO-RK (System for the Analysis of Biochemical Path-
ways  Reaction Kinetics, [18]), for example, allows the
representation of experimental and environmental bound-
ary conditions for measurements of the stored kinetic
data. The Functional Genomics ExperimentObjectModel
(FuGE, [19]) describes experimental settings in functional
genomics. We will not go into detail here. We are not
aware of any languages for the model use intentions (tele-
ological function) of bio-models.
Ontologies
Ontologies formalise conceptual knowledge. They can
provide vocabularies for formal languages.
The Kinetic Simulation Algorithm Ontology (KiSAO,
[20]) is employed within SED-ML to precisely specify
the algorithms used for the simulation experiment. Thus,
KiSAO contributes to the description of the intrinsic prag-
matical function of bio-models (setup).
Ontologies for intrinsicmodel instantiation seem not to
be very useful: There is not much conceptual knowledge
involved in assigning values to parameters and variables.
This is not the case for the (extrinsic) boundary condi-
tions. The Experimental Conditions Ontology (XCO) [21]
provides a rich vocabulary of experimental conditions
for phenotype experiments. The Measurement Method
Ontology (MMO) [21] can be used for specifying the
measurement method in descriptions of extrinsic experi-
mental settings. Because XCO and MMO are slightly out
of our scope we will not provide further details.
At the moment there exists no ontology for the teleo-
logical function of bio-models. In the next section we pro-
pose an ontology for both the intrinsic and the extrinsic
teleological function.
A new ontology for intentions of bio-models
In this section we provide first ideas for an ontology for
the teleological function of bio-models. For convenience,
we will call this proposed ontology MINTENTO (Mod-
elling Intention Ontology). Such an ontology would, for
example, facilitate computer support for the choice of
appropriate simulation types andmethods based on inten-
tions. Such an ontology would also allow to place simula-
tion results in the context of the respective intentions. As a
consequence, this will ease the reuse of simulation results.
How could an ontology for the teleological function
of bio-models look like? Here we present first ideas for
MINTENTO. The teleological function of bio-models
comprises of two aspects: the extrinsic model use inten-
tions and the intrinsic intended use of the model fulfilling
the intentions. This reflects the difference between a func-
tion (here: model use intention) and its realisation (here:
intended use) which is also employed for the ontological
analysis of biological function by Burek et al. [22]. There
may exist realisedBy relations between classes of the
two mainMINTENTO branches. A statement of the form
A realisedBy B, where A is a subclass of model use
intention and B is a subclass of intended use, means
for every instance a of A there is an instance b of B (a
process) such that the execution of b realises a. As a
consequence the instance level relation realises reads
as fulfils or contributes to.
In MINTENTO we want to cover both aspects and
appropriate realisedBy relations between them. To
grammatically reflect this distinction we use infinitive
verb forms (e.g. to construct) for model use intentions
and nouns for the realising processes (e.g. construction).
MINTENTO specialises the upper ontology BFO (Basic
Formal Ontology, [23]): model use intention is a subclass
of the BFO concept snap:Function and intended use is
a subclass of the BFO concept span:Process.
Knüpfer and Beckstein Journal of Biomedical Semantics 2013, 4:24 Page 6 of 9
http://www.jbiomedsem.com/content/4/1/24
The model use intentions for the investigation of a bio-
logical system are formulated in natural language. There
is a wide diversity of model use intentions. Morrison and
Morgan describe different high-level (teleological) func-
tions of models [5]. Summarising they state:
[...] models fulfil a wide range of functions in building,
exploring and applying theories; in various
measurement activities; and in the design and
production of technologies for intervention in the
world. ([5], p.24)
The set of ways how models can be used, provided by
Morrison and Morgan [5], is a good starting point for
an upper classification of general model use intentions.
Models are used to construct (a theory), to explore
(a theory), to measure, and to intervene. These four
upper level model use intentions are based on two dis-
tinctions: (1) A model can mediate between theory and
the world ([5], p.11), i.e. the model is either intended
to affect theory or to affect reality. (2) There is a dis-
tinction between using a model to effect (something)
and to learn (something) ([5], p.11). Figure 2 shows the
two distinctions and the resulting upper level model use
intentions.
In this paper we focus on dynamic and computational
bio-models. The intended use of this kind of model is
the simulation of the models dynamic behaviour. There
is also the possibility to mathematically analyse dynamic
properties of a bio-model without explicitly considering
any temporal behaviour. Therefore, the upper level of
the intended use branch of MINTENTO consists of
simulation and analysis. Simulations are further dif-
ferentiated into elementary simulation and combined
simulation (see Figure 3). An elementary simulation cal-
culates the temporal behaviour of the model for a single
model instantiation whereas a combined simulation con-
sists of several elementary simulations for differentmodel
instantiations and integrates the single results into the
final outcome. An example for an elementary simulation
is a uniform time-course simulation, where the state of
the formal system specified by the model is calculated at
discrete equidistant time points. This simulation type is
model use intention
to affect theory
to effect
to construct to intervene
to learn
to explore to measure
to affect reality
Figure 2 Upper level of MINTENTO: model use intention. Upper
level terms for the model use intention branch of MINTENTO. The
edges represent subclass (is-a) relations.
intended use
simulation
elementary simulation combined simulation
analysis
Figure 3 Upper level of MINTENTO: intended use. Upper level
terms for the intended use branch of MINTENTO. The edges
represent subclass (is-a) relations.
covered by SED-ML Level 1 Version 1 [24]. A parame-
ter scan is an example for a combined simulation where
the scanned parameter is varied in the particular elemen-
tary simulations. The next SED-ML version is supposed to
provide mechanisms for defining combined simulations,
like sequential and nested simulations (cf. the website [25]
for current developments).
In order to illustrate such realisedBy relations
betweenmodel use intentions and intended use we look at
some common tasks (model use intentions), a bio-model
is used for: (1) to approximate observed behaviour, (2)
to investigate the variability in behaviour, (3) to demon-
strate the ability for specific kinds of behaviour, and (4)
to explore parameter influence on the behaviour. Each
task requires a different corresponding (realisedBy)
simulation type: (1) time series (eventually including
parameter fitting), (2) bifurcation analysis, (3) stability
analysis, and (4) parameter scan.
Related conceptual frameworks
In this section we will compare our conceptual framework
for the function of bio-models with related conceptuali-
sations from the fields of Artificial Intelligence, modelling
and simulation, functional modelling, and biology.
The dual intrinsic/extrinsic interpretation of bio-
models is rooted in the knowledge representation
hypothesis from Artificial Intelligence:
Any mechanically embodied intelligent process will be
comprised of structural ingredients that a) we as
external observers naturally take to represent a
propositional account of the knowledge that the overall
process exhibits, and b) independent of such external
semantical attribution, play a formal but causal and
essential role in engendering the behavior that
manifests that knowledge. ([26], p.15)
Simon generalises this duality to all kinds of artifacts
which serve as interfaces between an inner and an outer
Knüpfer and Beckstein Journal of Biomedical Semantics 2013, 4:24 Page 7 of 9
http://www.jbiomedsem.com/content/4/1/24
environment [27]. Rosens modelling relation [6] is a con-
gruence between an extrinsic/outer natural system and an
intrinsic/inner formal system established by a model.
As stated above, a conceptual framework provides
categories and relations between them which are
employed to capture relevant knowledge fragments.
There are other conceptual frameworks for modelling
and simulation in general [28,29], and in particular
for bio-modelling [12]. Our meaning facets, how-
ever, provide much more details and are more rigid,
i.e. they provide categories of finer granularity and
define them more precisely. Our meaning facets are
therefore a solid foundation for computer support for
modelling and simulation based on formal knowledge
representation.
Zeiglers notion of an experimental frame [29] resem-
bles the model instantiation (conditional function) and
the experimental setup (pragmatical function) presented
in this paper. His experimental frame is the operational
formulation of the objectives that motivate a modeling
and simulation project ([29], p.27), i.e. it also describes
the teleological function of a model.
The field of functional modelling relates structure,
behaviour and function of engineering artifacts. Erden
et al. reviews the different approaches to formalise func-
tion and its relations to structure and behaviour [8]. Two
different notions of function are employed [8]: On the
one hand, function is mediating between structure and
behaviour and determines the structural behaviours, i.e.
all possible behaviours the model is able to show. This
is essentially what we call conditional and pragmatical
function, respectively. On the other hand, function refers
to the intentions of the modeller and restricts the possi-
ble behaviours to the expected behaviours. This notion
of function as purpose corresponds to our teleological
function. In short, functional modelling addresses the
questions of what the device and its components do or
what the purpose of the device and its components are
([8], p.149). Joining these two sides, function becomes
the bridge between human intention and physical behav-
ior of artifacts ([30], p.271). The distinction between
structural and expected behaviours originates from
Gero [31].
We transfer the notion of function in biology to mod-
elling in systems biology. There are some strong par-
allels between function in biology and function of bio-
models. The idea that function links between structure
and behaviour is deeply rooted in molecular biology, as,
e.g., stated by Lander:
If one such behavior seems useful (to the organism), it
becomes a candidate for explaining why the network
itself was selected, i.e., it is seen as a potential purpose
for the network. ([32], p.0712)
However, we will not discuss the notion of function in
biology further here. Krohs and Kroes compare the notion
of function in biology and technology and examines dis-
analogies and parallels [33].
There are some formal approaches to biological func-
tion. Ontologies like EcoCyc [34] and the Gene Ontology
[35] list molecular functions played by biological enti-
ties. Burek et al. presents an ontology of biological func-
tions which formalises three functional aspects [22]: the
so-called function structure, the realisation and the has-
function relation, which could be related to our teleolog-
ical, pragmatical and conditional function, respectively.
Because of the different knowledge domains of [22] and
our work, the two are not identical. We provide more
details on each functional aspect. However, the similarity
of these conceptualisation and the one presented in this
paper can be seen as further justification for transferring
the notion of biological function to bio-models.
Conclusion
Wehave applied the notion of function to dynamicmodels
in Systems Biology. Function is the link between themodel
structure and the model behaviour. The intrinsic function
of bio-models describes three aspects of the models use:
Why should the model be used in simulation experiments
(teleological function)? Which model instance should be
used in simulation experiments (conditional function)?
How should the model be used in simulation experiments
(pragmatical function)? The extrinsic function of a bio-
model refers to the intentions of using the model in order
to address biological questions.
The presented conceptual framework of the functional
aspects of bio-models was used to systematise and review
corresponding formal accounts. Some functional aspects
are well covered by checklists, languages and associ-
ated ontologies. However, there are no checklists and
languages for the extrinsic teleological function. Closing
these gaps would improve the standardised descriptions
of biological experiments.
At the moment there also exists no ontology for the
model use intentions and the intended use of bio-models.
We outlined such an ontology, the Modelling Intention
Ontology (MINTENTO). Since this is ongoing work we
are only presenting first thoughts about MINTENTO
here. We nevertheless believe that these thoughts provide
an important starting point for developing an ontology for
the teleological function of bio-models. Next steps would
involve the incorporation of lower levels terms and the
specification ofMINTENTO in some standard format like
OWL (Web Ontology Language, [36]).
Our ontological analysis of functional aspects of
dynamic models in Systems Biology and their use in simu-
lation experiments provides an important prerequisite for
formalising the involved knowledge. Ultimately, this will
Knüpfer and Beckstein Journal of Biomedical Semantics 2013, 4:24 Page 8 of 9
http://www.jbiomedsem.com/content/4/1/24
improve any computer-supported research method for
answering biological questions by means of bio-models.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
CK developed the meaning facets framework and analysed the different
functional aspects of bio-models. CB supervised this work. Both authors read
and approved the final manuscript.
Acknowledgements
We acknowledge Johannes Kretzschmar, Uwe Krüger and Christian Schäufler
for valuable discussions about the topic.
Received: 17 January 2013 Accepted: 23 May 2013
Published: 8 October 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Nigam et al. Journal of Biomedical Semantics 2013, 4:36
http://www.jbiomedsem.com/content/4/1/36DATABASE Open AccessRat Strain Ontology: structured controlled
vocabulary designed to facilitate access to strain
data at RGD
Rajni Nigam1*, Diane H Munzenmaier1,2, Elizabeth A Worthey1,3, Melinda R Dwinell1,2, Mary Shimoyama1,4
and Howard J Jacob1,2Abstract
Background: The Rat Genome Database (RGD) (http://rgd.mcw.edu/) is the premier site for comprehensive data on
the different strains of the laboratory rat (Rattus norvegicus). The strain data are collected from various publications,
direct submissions from individual researchers, and rat providers worldwide. Rat strain, substrain designation and
nomenclature follow the Guidelines for Nomenclature of Mouse and Rat Strains, instituted by the International
Committee on Standardized Genetic Nomenclature for Mice. While symbols and names aid in identifying strains
correctly, the flat nature of this information prohibits easy search and retrieval, as well as other data mining
functions. In order to improve these functionalities, particularly in ontology-based tools, the Rat Strain Ontology (RS)
was developed.
Results: The Rat Strain Ontology (RS) reflects the breeding history, parental background, and genetic manipulation
of rat strains. This controlled vocabulary organizes strains by type: inbred, outbred, chromosome altered, congenic,
mutant and so on. In addition, under the chromosome altered category, strains are organized by chromosome, and
further by type of manipulations, such as mutant or congenic. This allows users to easily retrieve strains of interest
with modifications in specific genomic regions. The ontology was developed using the Open Biological and
Biomedical Ontology (OBO) file format, and is organized on the Directed Acyclic Graph (DAG) structure. Rat Strain
Ontology IDs are included as part of the strain report (RS: ######).
Conclusions: As rat researchers are often unaware of the number of substrains or altered strains within a breeding
line, this vocabulary now provides an easy way to retrieve all substrains and accompanying information. Its
usefulness is particularly evident in tools such as the PhenoMiner at RGD, where users can now easily retrieve
phenotype measurement data for related strains, strains with similar backgrounds or those with similar introgressed
regions. This controlled vocabulary also allows better retrieval and filtering for QTLs and in genomic tools such as
the GViewer.
The Rat Strain Ontology has been incorporated into the RGD Ontology Browser (http://rgd.mcw.edu/rgdweb/
ontology/view.html?acc_id=RS:0000457#s) and is available through the National Center for Biomedical Ontology
(http://bioportal.bioontology.org/ontologies/1150) or the RGD ftp site (ftp://rgd.mcw.edu/pub/ontology/rat_strain/).
Keywords: Rat strains, Phylogeny, RGD, Rat genome database* Correspondence: rnigam@mcw.edu
1Human and Molecular Genetics Center, Medical College of Wisconsin, 8701
Watertown Plank Rd, Milwaukee 53226-3548, WI, USA
Full list of author information is available at the end of the article
© 2013 Nigam et al.; licensee BioMed Central Ltd. This is an open access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Nigam et al. Journal of Biomedical Semantics 2013, 4:36 Page 2 of 8
http://www.jbiomedsem.com/content/4/1/36Background
The use of the rat for genetics studies in Europe can be
traced back to the first half of the eighteenth century.
Experimentally, Crampe et al. mated an albino female
to a wild gray male in 1880. In the F1 offspring, three
mutant genes were phenotypically observed: c (albino),
a (non-agouti), and h (hooded) [1]. An early effort to track
new strains and substrains, focused on when rats were
transferred from one lab to another, resulting in new
substrains that could affect animals both phenotypically
and genotypically by the resultant changes in environ-
ment, dietary conditions or breeding strategy, as well as
spontaneous genetic variations. The list of codes used to
designate laboratories developing and maintaining rat
colonies was first published in 1973 [2]. Efforts have
also been made to capture differences in phenotype by
integrating microsatellite markers into the genetic link-
age maps [3] and radiation hybrid maps [4]. In order to
make significant comparisons, determine relationships
amongst strains, and select an appropriate model for
biomedical studies, knowledge of the different rat strains
and their breeding approaches is crucial. The first at-
tempt to create a phylogenetic tree for 13 inbred strains
(homozygous strain produced by brother-sister mating
for at least 20+ generations) using genetic markers was done
by Canzian et al. [5]. This was followed by an enhanced
version comprising 63 inbred strains and 214 substrains
(genetically diverse inbred strains due to separation after
20 generations or separated due to any genetic difference),
which was plotted using the percentage of genotypic
differences [6]. Thomas et al. presented phylogenetic
relationships of 48 inbred strains, using the allele size of
each strain at each microsatellite locus [7]. A phylogenetic
tree is also available at The National BioResource Project
for the Rat in Japan [8], for 132 rat strains. Maximum
parsimony analysis was used to calculate this tree
(http://www.anim.med.kyoto-u.ac.jp/nbr/phylo.aspx). Lever-
aging these efforts to represent relationships amongst
strains, the Rat Genome Database (RGD) has created
standardized data formats for capturing strain background
and breeding variations to represent all registered strains
in a format that is hierarchical and computable.
RGD: a unique resource for registering rat strains
RGD is a universally accessible database that has an exclu-
sive collection of rat genetic and genomic data curated from
current research publications and direct data submission by
rat researchers and rat providers. RGD currently has a cata-
logue of more than 2900 strains and substrains. RGD pro-
vides official assignment of rat strain symbols and names,
and encourages researchers to submit strain data prior
to publication through an online strain registration form
(http://www.rgd.mcw.edu/tu/strains/#StrainRegistration),
to ensure proper identification of their strains in theirmanuscripts. RGD validates the nomenclature of the
submitted strains following the nomenclature guidelines
laid out by the International Committee on Standardized
Genetic Nomenclature for Mouse and the Rat Genome and
Nomenclature Committee [9,10]. The registered symbol
and name of the strain along with a unique identifier, the
RGD ID, are assigned and sent to submitters for reference
in their publication. Strains from major rat resources
such as the PhysGen Program for Genomic Applications
(PhysGen, http://pga.mcw.edu/), Rat Resource and Research
Center (RRRC, http://www.rrrc.us/), National BioResource
Project (NBRP, http://www.anim.med.kyoto-u.ac.jp/nbr/
Default.aspx) in Japan, and commercial rat providers
such as Charles River (CRL, http://www.criver.com),
Harlan Laboratories (http://www.harlan.com/), Sigma
Advanced Genetic Engineering Labs (SAGE, http://www.
sageresearchmodels.com) and Transposagen (http://www.
transposagenbio.com) regularly submit strains to RGD
for nomenclature and ID assignment and the creation of
strain reports. These distributors mention the specific
nomenclature on their websites, reminding researchers
to use the correct nomenclature in their publications so
that the information can be extracted and attached to
the appropriate strain.
Results and discussion
Rat Strain Ontology
The Rat Strain Ontology (RS) is able to preserve the genetic
background and the breeding history of strains based
on their strain types. In order to do this, the strain no-
menclature had to be parsed so that the parental/ancestral
strains could be determined and progeny placed ac-
cordingly. In the latest version (version 5.1) of the RS
Ontology, dated September 2013, there are 3503 terms,
which include 13 different types of first level nodes
depicting the strain types (Figure 1). 62% of the terms have
one parent, including parental strains and placeholder
terms used to organize the strains properly. For example,
all congenic strains (strains in which a chromosomal
segment has been transferred from a donor strain to a
recipient strain but which is otherwise identical to the
original inbred recipient strain) that are derived from
the same parental strains DA and F344 are placed under
the congenic placeholder DA.F344 (RS:0000237). This
is further branched into congenic placeholders DA/
BklArbN.F344/NHsd (RS:0001224) and DA/BklArbNsi.
F344/NHsd (RS:0003211). These second level terms
house the actual congenic strains, for example DA.
F344-(D10Arb21-D10Arb22)/Arb (RS:0000210), DA.F344-
(D10Rat37-D10Arb22)/Arb (RS:0000209). As a result, 1%
of the terms have more than 3 parents. The addition of
congenic substrains (substrains derived from the parental
congenic strains) and mutant strains generated another
layer of complexity in the vocabulary and also justify
Figure 1 RS Ontology as viewed in OBO-Edit2 with rat strain
types displayed as different first level nodes.
Nigam et al. Journal of Biomedical Semantics 2013, 4:36 Page 3 of 8
http://www.jbiomedsem.com/content/4/1/36the fact that breeding techniques are crucial in defining a
rat strain. Hence, it was clear that the strain background
impacts the newly developed strain and its phenotype.
Accordingly, these details need to be captured and rep-
resented in a format that can be used.
Organization of the ontology
The earlier version of this ontology (version 1.0, dated
January 2009) had 2350 terms, with 76% of the terms
having one parent and only three terms having more
than three parents. In this format all the inbred strains
and the different strains types were the main nodes.
This format was not able to differentiate between the
substrains and congenic strains of the same parental strain.
For example, the substrain: ACI/Eur (RS:0000021) and
congenic placeholder: BUF.ACI (RS:0000432) were under
the parental node ACI which was a first level node under
rat strains (Figure 2A). This format was replaced in the
latest version 5.1 where the strain types are first nodes
(Figure 2B). This major version change referred to a global
change in the structure of the ontology. Therefore, ACI is
an inbred strain which is placed under inbred with all
substrains under it, whereas the related congenic strains
are under the node congenic strain. Another node re-
cently added is the chromosome altered (Figure 3A).
This node has all the chromosomes which can be further
divided into the relevant strain types, for example: congenic,
consomic (strains in which a whole chromosome has been
transferred from a donor strain to a recipient strain but isotherwise identical to the original inbred recipient strain)
and mutant (strains in which a gene has been modified or
spontaneous mutant with an altered phenotype). All the
strains in which the chromosomes have been manipulated
are placed under the chromosome altered node, as well as
under their specific categories. This helps in visualizing
strains in which the same chromosome has been intro-
gressed into different background strains making it easier
to predict the genotype to phenotype associations and com-
pare the sequence variations amongst strains.
Since techniques used to alter the chromosomes also
play a crucial role in determining the strains, mutant strains
are further divided. For example, mutants created
by N-ethyl-N-nitrosourea (ENU) [11,12], zinc-finger
nucleases (ZFN) [13] and transcription activator-like
effector nuclease (TALEN) have separate nodes under
the parental strains. The technique used to create the
specific mutant is mentioned in parenthesis. The
mutant strain in which a particular gene is mutated is
placed under the relative chromosome number; for ex-
ample, gene Tgfb1 (transforming growth factor, beta 1)
maps to chromosome 1 in the rat, so the heterozygous
mutant strain SS-Tgfb1em3Mcwi?/+ (RS:0003129) is placed
under SS/JrHsdMcwi Heterozygous (ZFN) mutants
(Figure 3B). This strain, having a mutation in chromosome
1, is also under SS/JrHsdMcwi (ZFN) mutants (chr 1) a
sub-branch of chromosome 1 mutant under chromo-
some altered. These substantial improvements have helped
in making this vocabulary more robust and usable. In
addition, users can now view all the homozygous, heterozy-
gous and wild type strains under a single node.
In OBO-Edit and the RGD Ontology Browser (Figure 4),
the RS Ontology can be viewed in the tree view, which
can be expanded to show the child relationships [14].
Since a term has many relationships to other terms, it
appears several times in the graphic view (Figure 5) or
a tree view (Figure 6).
Searching a strain in the ontology
The RGD Ontology Browser (http://rgd.mcw.edu/rgdweb/
ontology/search.html) can easily be used to search a
strain. When a desired strain symbol; for example, BN is
searched in the browser the result page displays the
number of terms that match the searched term in all
the different ontologies. Clicking on the ontology name,
RS: Rat Strains or on the number of terms displays all
the strains that have the searched term BN in them in
the Rat Strain Ontology. By first clicking on the tree sign
adjacent to BN mutants, then BN/NHsdMcwi mutants
and finally BN/NHsdMcwi (ENU) mutants, a list of all the
strains is generated by this technique using the parental
BN strain. A click on any individual strain takes the user
to the ontology report page, where the term is displayed
in a driller format [15], with the searched term in the
Figure 2 A. Version 1.0 showing substrains and congenics in the same level node. B. Version 4.9 showing the different strain types as first
level nodes.
Nigam et al. Journal of Biomedical Semantics 2013, 4:36 Page 4 of 8
http://www.jbiomedsem.com/content/4/1/36middle column along with siblings, parents in the left
column and children in the right (Figure 4). Clicking
the View Strain Report option takes the user to the
respective RGD strain report page which displays the
RS ID. This option is available for the curated strains
and not for placeholders. If a strain is searched using
the general keyword search in RGD, then the result
page lists all the strains. A click on the strain symbol
goes to the strain report page which has the RS ID of
the strain mentioned as ontology ID. Using the same
example, if BN/NHsdMcwi is searched in keyword
search of RGD, then the report page shows all the
strains that have the searched term BN/NHsdMcwi in
them. A click on the strain symbol takes the user to the
individual strain report page that has the RS: 0000145,
this ID links to the RGD Ontology Browser showing
the different substrains derived from it. The tree
view of the Rat Strain Ontology at NCBO BioPortal
[16,17] also displays the hierarchy of the strains in a
similar fashion.Expanded usage
As RGD has a vast collection of strains, selecting an
appropriate strain from a list of over 2900 strains is not
easy; it is here that the RS Ontology has a vital role.
Users can scroll down the lists of different types of
strains, or restrict their choices by chromosome altered
and then by the different strain types. RGDs robust
usage of the RS Ontology for classifying strains makes it
valuable for biologists using rats in their research, as it
helps them in predicting the genomic contents a particu-
lar strain may have inherited from the parental strains.
The RS Ontology is included in the RGD Ontology
Browser, and the strain report pages, which have com-
prehensive descriptions of characteristics, origin, disease,
phenotype and physiological information, behavior, drug
reactions and reproductive notes make the RS Ontology
annotations an important navigational tool. The rat strains
that are curated from published articles are annotated by
Mammalian Phenotype Ontology [18] and MEDIC disease
ontology [19] which are used to conduct effective searches
Figure 4 RGD Ontology Browser displaying ENU mutants.
Figure 3 Classical tree view showing A. chromosome altered B. ZFN mutant strains.
Nigam et al. Journal of Biomedical Semantics 2013, 4:36 Page 5 of 8
http://www.jbiomedsem.com/content/4/1/36
Figure 5 Graphic view of a congenic substrain.
Figure 6 Classical tree view of the Rat Strain Ontology in OBO-Edit2.
Nigam et al. Journal of Biomedical Semantics 2013, 4:36 Page 6 of 8
http://www.jbiomedsem.com/content/4/1/36
Nigam et al. Journal of Biomedical Semantics 2013, 4:36 Page 7 of 8
http://www.jbiomedsem.com/content/4/1/36for strains based on disease and phenotypes. These an-
notations help in assigning strains to their respective
disease portals.
RGD tools, such as PhenoMiner, display experimental
records associated with phenotypic measurements of rat
strains used in experiments. PhenoMiner has 18580
records with quantified phenotype values attached to
consomic strains, 11524 values attached to inbred strains,
2870 to congenic strains, 2204 to all mutant (ZFN) strains
and 2063 to all mutant (ENU) strains as of September
2013 [20]. These are entered into PhenoMiner by using
the RS Ontology and three other ontologies, namely,
clinical measurement (CMO), measurement method
(MMO), and experimental condition ontologies (XCO)
[21]. All rat QTLs are annotated to the RS Ontology to
facilitate querying, retrieval and filtering of QTL data
[22]. All the congenic [23] and consomic [24], strains
that have an introgressed segment can be visualized in
GViewer which can be accessed from the disease portals.
QTL report pages have a link that leads to a narrower
region which can be visualized by zooming in with
GBrowse [25,26] which displays the congenic and congenic
substrains that have the desired region. As stated earlier,
this information is captured in the chromosome altered
node of the RS Ontology.
Conclusions
The Rat Strain Ontology is a new tool for annotating rat
strains in a standardized manner which reflects the
breeding history and genetic makeup of the strains to
facilitate querying and retrieval, analysis and comparisons
amongst strains. The latest version of the Rat Strain
Ontology has been revised to classify all of the wild type,
heterozygous, and homozygous strains, with the mutants
further grouped under these strain subtypes. As the de-
velopment process continues, new strains are continu-
ally being added and application of this vocabulary is
continually expanding to allow investigators to integrate,
consolidate and compare phenotypic measurement data
from diverse sources.
Methods
Development of the ontology
This ontology is developed using OBO-Edit [27,28], a
Java based tool that uses a graph-oriented approach to
display and edit the ontologies. RGD currently uses OBO-
Edit2 for editing and adding new strain information. In
some instances, strain symbols are used as placeholders
for the graph nodes in order to maintain the relationship
and hierarchical structure of the ontology. For example no
details are known for the parent strain ACI (RS:0000012),
whereas details are known about the substrains ACI/N,
ACI/Kun, ACI/SegHsd etc. So, in these cases, the parent
term ACI was used as a placeholder so that the childrenterms could be added. Textual synonyms including the
RGD ID are entered via Term Editor.
Availability
This ontology is free and available to all users. This can be
viewed in the RGD Ontology Browser at http://rgd.mcw.
edu/rgdweb/ontology/search.html, as well as at the National
Center for Biomedical Ontology (NCBO) BioPortal website
http://bioportal.bioontology.org/ontologies/1150. Systematic
versions can be downloaded from the RGD ftp site ftp://rgd.
mcw.edu/pub/ontology/rat_strain/.
Competing interests
No financial or commercial conflict of interest is declared by the authors.
Authors contributions
RN prepared the manuscript, developed the ontology and curated all the rat
strains. RN and MS designed the ontology. DHM, EAW, MRD, MS, HJJ assisted
in the designing and planning of this project. All authors have read and
approved the manuscript.
Acknowledgements
RGD is funded by the National Heart, Lung, and Blood Institute on behalf of
the National Institutes of Health (HL64541). PhenoMiner is funded by the
National Heart, Lung, and Blood Institute on behalf of the National Institutes
of Health (HL094271).
Author details
1Human and Molecular Genetics Center, Medical College of Wisconsin, 8701
Watertown Plank Rd, Milwaukee 53226-3548, WI, USA. 2Department of
Physiology, Medical College of Wisconsin, 8701 Watertown Plank Rd,
Milwaukee 53226-3548, WI, USA. 3Department of Pediatrics, Medical College
of Wisconsin, 8701 Watertown Plank Rd, Milwaukee 53226-3548, WI, USA.
4Department of Surgery, Medical College of Wisconsin, 8701 Watertown
Plank Rd, Milwaukee 53226-3548, WI, USA.
Received: 31 May 2013 Accepted: 2 October 2013
Published: 22 November 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
de Bono et al. Journal of Biomedical Semantics 2013, 4:22
http://www.jbiomedsem.com/content/4/1/22RESEARCH Open AccessFunctional tissue units and their primary tissue
motifs in multi-scale physiology
Bernard de Bono1,2*, Pierre Grenon3, Richard Baldock4 and Peter Hunter1Abstract
Background: Histology information management relies on complex knowledge derived from morphological tissue
analyses. These approaches have not significantly facilitated the general integration of tissue- and molecular-level
knowledge across the board in support of a systematic classification of tissue function, as well as the coherent
multi-scale study of physiology. Our work aims to support directly these integrative goals.
Results: We describe, for the first time, the precise biophysical and topological characteristics of functional units of
tissue. Such a unit consists of a three-dimensional block of cells centred around a capillary, such that each cell in
this block is within diffusion distance from any other cell in the same block. We refer to this block as a functional
tissue unit. As a means of simplifying the knowledge representation of this unit, and rendering this knowledge
more amenable to automated reasoning and classification, we developed a simple descriptor of its cellular content
and anatomical location, which we refer to as a primary tissue motif. In particular, a primary motif captures the set
of cellular participants of diffusion-mediated interactions brokered by secreted products to create a tissue-level
molecular network.
Conclusions: Multi-organ communication, therefore, may be interpreted in terms of interactions between
molecular networks housed by interconnected functional tissue units. By extension, a functional picture of an
organ, or its tissue components, may be rationally assembled using a collection of these functional tissue units as
building blocks. In our work, we outline the biophysical rationale for a rigorous definition of a unit of functional
tissue organization, and demonstrate the application of primary motifs in tissue classification. In so doing, we
acknowledge (i) the fundamental role of capillaries in directing and radically informing tissue architecture, as well as
(ii) the importance of taking into full account the critical influence of neighbouring cellular environments when
studying complex developmental and pathological phenomena.Background
Current standards and practices in histology informa-
tion management rely heavily on implicitly represented
knowledge derived from morphological analyses of
two-dimensional images captured from tissue samples.
These practices have not significantly facilitated the inte-
gration of tissue- and molecular-level knowledge across
the board in support of a systematic classification of tissue
function, as well as the multi-scale study of physiology.
This work discusses one particular element in an over-
all system of ontology for physiology in multicellular* Correspondence: b.bono@ucl.ac.uk
1Auckland Bioengineering Institute, University of Auckland, Symonds Street,
Auckland 1010, New Zealand
2CHIME Institute, Archway Campus, University College London, London, UK
Full list of author information is available at the end of the article
© 2013 de Bono et al.; licensee BioMed Centra
Commons Attribution License (http://creativec
reproduction in any medium, provided the ororganisms in support of these integrative goals. Specific-
ally, this paper describes for the first time the precise
biophysical and topological characteristics of a generic
functional unit in tissues. Such a unit consists of a
three-dimensional block of cells centred around a capil-
lary, such that each cell in this block is within diffusion
distance from any other cell in the same block. We refer
to this block as a functional tissue unit (FTU). As a
means of simplifying the knowledge representation of an
FTU, and rendering this knowledge more amenable to
automated reasoning, we developed a simple descriptor
of the cellular content and anatomical location of an
FTU, which we refer to as a primary tissue motif
(PTM). In this work we demonstrate the application of
PTM knowledge in tissue classification.l Ltd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
de Bono et al. Journal of Biomedical Semantics 2013, 4:22 Page 2 of 13
http://www.jbiomedsem.com/content/4/1/22Given that the spatial characteristics of an FTU are
constrained to ensure the homogeneous diffusive admix-
ture of extracellular molecules across its entire volume,
including those molecules that enter or leave the FTU
by advective means (e.g. via the blood supply), the rest of
this section will briefly introduce the key biophysical
constraints on molecular transport processes as a means
to explain the central, rate limiting, role that tissues ful-
fill in transport physiology.
Overview of transport physiology
In protein biology, tissues serve two key functions, namely,
to (i) support the synthesis and folding of proteins within
cells, and (ii) to regulate the extent by which proteins are
allowed to meet other molecules across compartments,
thus facilitating or inhibiting the fulfillment of protein
function. A metabolically critical goal for transport physi-
ology is the regulation of the extent by which proteins are
allowed to encounter H+ ions [1], in view of the crucial role
these ions play in the protein folding process. A second
key goal for transport processes is to convey molecule-
mediated communication between cells, through juxtacrine
(e.g. Notch signaling [2]), paracrine (e.g. IGF-1 [3]),
endocrine (e.g. thyroid hormone action [4]) and exo-
crine (e.g. immunological factors passed on via lacta-
tion [5]) modalities. This type of communication serves
to co-ordinate cells on protein production and the
intercompartmental trafficking of molecules.
Given the above physiological goals, the generic archi-
tecture of cellular arrangement within any type of solid
tissue (e.g. skeletal muscle, liver, brain, lung, etc.) has to
ensure that the core processes of (i) metabolism and (ii)
communication achieving these goals are fulfilled.
In this paper, a first step in interpreting tissue struc-
ture in this key is taken by identifying and characterizing
a basic unit of tissue that fulfills the above process goals
(i.e. metabolism and communication). This continuous
block of tissue, known as a functional tissue unit, en-
sures the viability of its cellular content by satisfying the
biophysical constraints for these goals to be reached.
Results
Physiological and biophysical characterisation of an FTU
Given the metabolic goals discussed above, a core biophys-
ical requirement is that cells have to be within diffusion
distance of at least one capillary blood vessel to secure ap-
propriate rates of (i) delivery of supplies (e.g. oxygen, glu-
cose) and (ii) elimination of waste (e.g. carbon dioxide, H+,
urea). As the maximum distance over which diffusion
occurs is approximately 100 ?m, most mammalian cells in
solid tissues are found within 50 ?m of a capillary [6].
The same 100-?m diffusion limit constraint applies to
any local molecular exchange between cells (e.g. electro-
lytes, metabolic intermediates, messenger molecules, etc.).For instance, the paracrine communication modality,
which enables the creation of gene expression regula-
tory networks that span across neighbouring cell types
in the same tissue (e.g. see refs [7,8]), is also bound by
the same distance constraint.
It is therefore possible to identify a unit of solid tissue
consisting of a well-defined heterogeneous set of cells
that is both (i) metabolically dependent on the same ca-
pillary, as well as (ii) the cellular substrate for tissue-
level molecular pathways co-ordinated via paracrine
communication. This block of tissue has a cylindrical
shape whose long axis is that of the feeding capillary on
which it is metabolically dependent (Figure 1A). The
rigid biophysical constraint that within this cylinder no
two cells may be more than 100 ?m apart (dashed line,
Figure 1A) provides a fundamental mathematical limit
to calculate the absolute dimensions of an FTU. As the
application of this limit in isolation gives rise to multiple
solutions, the addition of a second constraint is required
to provide specific values to the diameter and length of
a cylindrical tissue block. For instance, by requiring that
the particular cylinder of choice is one that achieves the
maximal volume (i.e. the volume ensuring the greatest
possible mass of protein metabolic machinery within the
FTU substrate) we have estimated the diameter of that
cylindrical block to be about 80 ?m and the length ap-
proximately 60 ?m.
Representing FTU-derived knowledge as a primary
tissue motif
As three-dimensional (3D) image data about FTUs is an-
ticipated to be complex, it is critical that computation-
ally efficient approaches are developed to represent
salient biological knowledge about FTUs. To this end,
we developed the notion of a primary tissue motif, de-
rived from FTU information, in support of tissue classifi-
cation and cross-scale integrative goals.
If descriptions of FTUs are to tersely provide informa-
tion about cells that are clustered together (Figure 1C)
to locally support a tissue-level molecular pathway, then
the minimal FTU-derived knowledge required should
include:
i. a non-redundant list of distinct cell types
(graphically represented in Figure 1D as a set of five
differently-coloured cells in a box), observed in an
FTUs 3D image data, to indicate the kind of cell
that can contribute extracellular molecules to the
local tissue-level network through diffusion, as
well as
ii. the anatomical region of origin for the sourced
tissue material of the FTU (represented in Figure 1D
through the nesting of the above set of 5 cells within
the colonic region of the body map) as a means to
Figure 1 (See legend on next page.)
de Bono et al. Journal of Biomedical Semantics 2013, 4:22 Page 3 of 13
http://www.jbiomedsem.com/content/4/1/22
(See figure on previous page.)
Figure 1 Example workflow illustrating the acquisition and processing of FTU data from a three-dimensional reconstruction of human
colon tissue. Step 1: The FTU template (A) is prepared according to the biophysical constraints under consideration, such that the long axis of
the resulting cylindrical block of tissue is that of the feeding capillary (CAP) on which it is metabolically dependent. This template is applied to an
appropriate volumetric region in the three-dimensional histology image dataset (B). The various cells within this region (coloured boxes) are
typed and their position recorded (Note: red boxes represent endothelial cells, here shown lining the feeding capillary  CAP  and the
erythrocytes within its lumen). Step 2: The cellular annotations across the full extent of the FTU cylindrical boundaries (C) are stored, together
with the image data and the anatomical provenance of the tissue sample. Step 3: As the resulting primary tissue motif for the above colonic FTU
uses standard reference ontology terms to represent both (i) a non-redundant list of distinct cell types, as well as (ii) the anatomical region of
origin for the sourced tissue material, a terse graphical depiction of the constitution of this FTU may be automatically included in the context of
whole-body anatomy maps, such as the one schematized by the ApiNATOMY tool [9] in (D). In this schematic, the outer boundary of the map
represents the various epithelial surface categories (each individually coloured and labelled), and the inside tiles represent vascular (red) and
neural (purple) structures respectively.
de Bono et al. Journal of Biomedical Semantics 2013, 4:22 Page 4 of 13
http://www.jbiomedsem.com/content/4/1/22identify the advective routes through which body-
level molecular networks can interlink with the local
pathways.
The formal representation of the above minimal know-
ledge, in the form of a PTM, provides a surrogate avenue
to automating and quantifying the comparison of FTUs
as a means to classifying tissues in terms of well-defined
functional criteria.
Part 1 of the Methods Section briefly elicits a number
of requirements for such knowledge representation and
overviews the application of the Basic Formal Ontology
(BFO) [10] to the approach taken. In this section we
present a preliminary specification of the core concepts
and, in the section that follows, draw on these elements
to illustrate how this framework could lead to the classi-
fication of FTUs.
Tissues are made of cells and extracellular matrix. In
the present paper, we leave extracellular matrix constitu-
ents of tissues aside and focus on cellular constitution.
Anatomically, different types of tissues demonstrate re-
gularity in cellular composition and organisation. For a
given tissue, specific patterns of cell types and the organ-
isation of cells of these different types are characteristics.
These patterns are what we call tissue motifs. Without
providing a definition of a specific tissue motif, we can
register that the constraints present two elements: (i) the
types of cell in a tissue, (ii) the organisation of cells (of
different or the same types) in a tissue. When focusing,
in the present paper, on primary tissue motifs we attend
to elements of the first type, that is to say, the list of
types of cells constituting a functional tissue unit.
Definition. FTU u is defined by u is a functional tissue
unit, i.e. a three-dimensional, maximally connected,
block of cells centred around a capillary, such that each
cell in this block is within diffusion distance from any
other cell in the same block. FTU is a subtype of BFOs
material object (we leave open whether it falls more
specifically under one of the subtypes of material ob-
jects, namely: object, aggregate of object or fiat part of
object).Definition. containsCell u v is defined by u is a FTU
which has cell v as a part (containsCell is a specialisation
of the has part relation).
Definition. containsCellType u v is defined by u is a
functional tissue unit such that there is at least one cell of
type v in the containsCell relation to u (containsCellType
is a relation between an instance and a type).
A primary tissue motif can be defined as an extensional
set theoretical abstraction over the containsCellType, i.e. as
the set of cell types entering in the containsCellType rela-
tion with a given FTU. For a given FTU, then, its primary
tissue motif is unique. For convenience, we use the func-
tional notation PTM(u) to denote the primary motif of a
functional tissue unit u. Using set theoretical notation, we
can then write:
PTM uð Þ isdefined by x=containsCellTypeuxf g
Primary tissue motifs are but one aspect of tissue mo-
tifs. Consistently with BFO, we do not include set theor-
etical constructs in the ontology. However, this does not
prevent us from using the formal apparatus of set theory
in order to record PTMs. We leave open whether PTMs
are entities in their own rights or abstractions overs Tis-
sue Motifs.
Finally, we give ourselves the means to formally regis-
ter the anatomical location of an FTU so as to record
the fact that the FTU belongs to, say, liver tissue or any
other anatomical location. To this end, we use the rela-
tion anatomicalLocation linking an FTU to an anatom-
ical location such as represented in the Foundational
Model of Anatomy [11].
The foregoing is enough for the current purpose and
to illustrate the annotation of FTU data in the form of
histological images (as exemplified in part 2 of the
Methods Section). Images are one kind of representation
for FTUs - images constitute imaging data. To these im-
ages, we can attach records of formal descriptions of the
FTUs depicted. Here, such formal descriptions amount
to the annotation of image elements (which depict FTU
parts) with the kind of entity they instantiate. In our
de Bono et al. Journal of Biomedical Semantics 2013, 4:22 Page 5 of 13
http://www.jbiomedsem.com/content/4/1/22case, the image elements are of cells and the kind of en-
tity to which they are associated are cell types (for which
we use the CellType ontology). Annotation links are reg-
istered using the containsCellType relation introduced
above.
On the basis of the foregoing, and with the provision
of data regarding the cellular composition of FTU in-
stances, we can also illustrate the prospective approach
to the classification of FTUs, here on the basis of their
primary tissue motifs. The classification of FTUs con-
sists in fleshing out a taxonomy of FTUs. There are, of
course, many ways of classifying FTUs. The present dis-
cussion is concerned with the classification of FTUs on
the basis of their PTMs.
For two given FTUs, x and y, we can define the follow-
ing relation of PTM-equivalence which holds when the
PTMs of x and y are the same:
ptm?equivalence x; yð Þ isdefined byPTM xð Þ ¼ PTM yð Þ:
If PTMs are handled set-theoretically, the relation holds
whenever the PTMs have the same members: ptm-
equivalence (x,y) if and only if for all z, (containsCellType
x z) is equivalent to (containsCellType y z).
In some cases, only some of the cell types contained in
one FTU are contained in the other. When the first con-
tains no other cell type, we say that x is ptm-subsumed
by y (alternatively, y ptm-subsumes x). When, however,
x contains other cell types than those contained by y, we
say that x and y ptm-overlap.
Using these relations, bearing in mind they can be
read as standard set-theoretic relations, it becomes pos-
sible to consider unions of PTMs and, thus, complex
FTUs that may be broken down into unifying multiple
PTMs. This route leads to the definability of aspects of
tissues, including organs, as aggregates of cells. The out-
come is a theory of parts and wholes among tissues that
may be used in the characterisation of complex tissues
using FTUs and PTMs and the recording of varied tissue
types. Using these tools, it becomes possible to flesh out
PTMs, FTUs, and tissue classification systems. The re-
quired theory remains an item for future work.
In the next section, we exemplify how to proceed with
classification based on semantic similarity between the
elements of PTMs corresponding to recorded FTUs. In
addition, part 2 of the Methods section overviews the
process of 3D image annotation as a means to show how
the above representation can be used to register know-
ledge about FTUs.
Classifying primary tissue motifs
Primary tissue motifs take into account the cellular con-
stituents in an FTU. Consequently, one avenue to ob-
jectively compare FTUs is through the calculation ofpairwise similarity of their primary motif surrogates in
an approach that is, in principle, similar to primary se-
quence comparison in protein structure classification. In
terms of this analogy between pairwise comparisons of
amino acid and cellular constituents of biological struc-
tures, semantic similarity matrices (e.g. [12]) for CellType
ontology terms in PTMs take on the role of amino acid
substitution matrices in primary protein sequence align-
ment (e.g. [13-15]).
In this section, we describe (i) a simple PTM-classification
method we developed based on semantic similarity calcula-
tions over the CellType ontology, and (ii) the results of the
application of this method on an illustrative sample of 5
PTMs as a pre-requisite to highlighting the advantages
and shortcomings of this method in the Discussion
section.
Our method derives a pairwise similarity score s() for
the comparison of cellular constituents in a primary tis-
sue motif. The results of function s() provide the means
to classify FTUs by clustering over this score. The key
steps carried out in this method are exemplified below,
by way of demonstration, and correspondingly illustrated
in Figure 2:
1) Primary motif knowledge for five hypothetical FTUs,
representing hepatic, cardiac, pulmonary, colonic
and gastric tissue, was curated from histology
accounts found in standard textbooks [16,17]
(Figure 2A). The connection between motifs and
well-defined anatomical location remains implicit
here but, formally, the link can be readily registered
using the anatomicalLocation relation introduced in
the section above. The FTUs considered in the
present case have distinct, but overlapping, primary
motifs (in the sense of ptm-overlap, also discussed
above);
2) The cellular constituents of the primary motifs were
mapped to equivalent terms in the CellType
ontology (Figure 2B)  this particular step involved
15 distinct CellType terms;
3) An all-vs-all semantic similarity calculation for the
above 15 terms was carried out over the CellType
ontology graph through the application of the
approach described by Gentleman [18] (the results of
these calculations were courtesy of F. Couto,
University of Lisbon) to create a CellType similarity
matrix c() (Figure 2C);
4) A PTM pairwise alignment algorithm was designed
and implemented to compare the cellular
constituents of two motifs. This step generates a list
p{} of exclusive pairs of CellType terms, such that (i)
a pair consists of a unique combination of two
CellType terms (one from each respective motif )
that have the highest possible score over c(), and (ii)
Figure 2 Step-by-step example illustrating the automation of primary tissue motif comparison. [A] FTU knowledge about 5 distinct
tissues (in this particular example, derived from histology textbooks [16,17]) generated lists of distinct constituent cell types for each of the
corresponding derivative primary tissue motifs. [B] Each distinct cell type in [A] was mapped to the equivalent term from the CellType ontology
and assigned its unique term ID. [C] An all-vs-all pairwise comparison between the primary tissue motifs (ptm) was carried out as follows: (i) for
every unique combination of ptm pairs (such that a pair consists of ptm_X and ptm_Y), an all-vs-all semantic similarity score c() for each unique
combination of CellType term pairs is calculated (such that one CellType term is drawn from ptm_X and another from ptm_Y); (ii) the set p{} of
highest scoring exclusive pairs of CellType terms is identified  exclusivity in a pair ensures that, once a CellType term from one ptm is selected
to match another CellType term from another ptm, neither of these two CellType terms are included in any other pair in p{}; (iii) the sum of c()
scores in p{} are divided by the average number of cell types across ptm_X and ptm_Y to generate s(ptm_X,ptm_Y). [D] The set of ptm elements
is clustered over the pairwise score s(). See also Table 1 for concrete values of c(), p{} and s() involving the 5 distinct tissues referred to in [A].
de Bono et al. Journal of Biomedical Semantics 2013, 4:22 Page 6 of 13
http://www.jbiomedsem.com/content/4/1/22a CellType term from a primary motif is involved in
only one best-matched pair in p{}. The motif
alignments, and the individual c() scores for each
best-matched pair, for every possible pairwise
comparison of motifs described in step 1 are listed
in the last column of Table 1
5) An index of similarity s() between two motifs is then
calculated by dividing (i) the cumulative summation
of the c() scores for best-matched CellType pairs
recorded in p{} by (ii) the average number of
CellType terms across the two motifs. These scores
are also listed in Table 1;
6) The set of 5 primary tissue motifs are subsequently
clustered on the basis of their s() score to produce
the topology of a binary tree illustrated in
Figure 2D.The 5 PTMs were clustered into two tissue pairs and a
hepatic out-group. The basis for the clustering of the
two gastrointestinal PTMs can be found in the relatively
high c() score of 0.52 between pyloric (CL_1000323) and
colonic (CL_1000320) goblet cells, together with a more
modest contribution of 0.33 from the pairing of gastric
(CL_0002182) and colonic (CL_0002071) epithelial cells.
The second cluster paired cardiac and bronchial PTMs
on the strength of the shared presence of musculature
which lead to a c() score contribution of 0.64 through
the matching of smooth (CL_0002598) and cardiac
(CL_0000746) muscle cells.
Discussion
PTMs provide the list of constituent cell types in an
FTU, as well as the anatomical location of these cells.
Table 1 Results from pairwise primary tissue motif calculations in Figure 2
No. ptm ptm Score s() p{} listing closest-matching cell pairs with corresponding c() scores
1. ptm_1 ptm_1 1.0 CL_0000182-CL_0000182:1.0, CL_0000632-CL_0000632:1.0, CL_0000057-CL_0000057:1.0,
CL_0000091-CL_0000091:1.0, CL_0000115-CL_0000115:1.0
2. ptm_1 ptm_2 0.64 CL_0000057-CL_0000057:1.0, CL_0000115-CL_0000115:1.0, CL_0000182-CL_0000746:0.29,
CL_0000632-CL_0002068:0.25
3. ptm_1 ptm_3 0.6 CL_0000057-CL_0000057:1.0, CL_0000115-CL_0000115:1.0, CL_0000182-CL_0000082:0.46,
CL_0000632-CL_0000158:0.3, CL_0000091-CL_0002598:0.23
4. ptm_1 ptm_4 0.64 CL_0000057-CL_0000057:1.0, CL_0000115-CL_0000115:1.0, CL_0000182-CL_0002071:0.3,
CL_0000632-CL_1000320:0.24
5. ptm_1 ptm_5 0.55 CL_0000057-CL_0000057:1.0, CL_0000115-CL_0000115:1.0, CL_0000182-CL_0000508:0.32,
CL_0000632-CL_0002182:0.29, CL_0000091-CL_1000323:0.17
6. ptm_2 ptm_2 1.0 CL_0002068-CL_0002068:1.0, CL_0000057-CL_0000057:1.0, CL_0000746-CL_0000746:1.0,
CL_0000115-CL_0000115:1.0
7. ptm_2 ptm_3 0.72 CL_0000057-CL_0000057:1.0, CL_0000115-CL_0000115:1.0, CL_0002598-CL_0000746:0.64,
CL_0000082-CL_0002068:0.25
8. ptm_2 ptm_4 0.6 CL_0000057-CL_0000057:1.0, CL_0000115-CL_0000115:1.0, CL_0000746-CL_0002071:0.22,
CL_0002068-CL_1000320:0.17
9. ptm_2 ptm_5 0.6 CL_0000057-CL_0000057:1.0, CL_0000115-CL_0000115:1.0, CL_0000508-CL_0000746:0.23,
CL_0002182-CL_0002068:0.2
10. ptm_3 ptm_3 1.0 CL_0000158-CL_0000158:1.0, CL_0000082-CL_0000082:1.0, CL_0000057-CL_0000057:1.0,
CL_0002598-CL_0002598:1.0, CL_0000115-CL_0000115:1.0
11. ptm_3 ptm_4 0.64 CL_0000057-CL_0000057:1.0, CL_0000115-CL_0000115:1.0, CL_0000158-CL_1000320:0.3,
CL_0000082-CL_0002071:0.29
12. ptm_3 ptm_5 0.57 CL_0000057-CL_0000057:1.0, CL_0000115-CL_0000115:1.0, CL_0000158-CL_0002182:0.35,
CL_0000082-CL_0000508:0.31, CL_0002598-CL_1000323:0.21
13. ptm_4 ptm_4 1.0 CL_1000320-CL_1000320:1.0, CL_0002071-CL_0002071:1.0, CL_0000057-CL_0000057:1.0,
CL_0000115-CL_0000115:1.0
14. ptm_4 ptm_5 0.71 CL_0000057-CL_0000057:1.0, CL_0000115-CL_0000115:1.0, CL_1000323-CL_1000320:0.52,
CL_0002182-CL_0002071:0.33
15. ptm_5 ptm_5 1.0 CL_0002182-CL_0002182:1.0, CL_0000508-CL_0000508:1.0, CL_0000057-CL_0000057:1.0,
CL_1000323-CL_1000323:1.0, CL_0000115-CL_0000115:1.0
The unique combinations of primary tissue motif (ptm) pairs, discussed in Figure 2, are listed in the second and third columns. The last column shows the
comma-delimited list of exclusive best-matched pairs in the following format: <CellType term ID from one ptm> - <CellType term ID from the other ptm> : <c()>.
de Bono et al. Journal of Biomedical Semantics 2013, 4:22 Page 7 of 13
http://www.jbiomedsem.com/content/4/1/22The biophysical constraints satisfied by an FTU ensure
that data about molecular secretions and surface recep-
tors associated with the PTMs list of cell types may be
legitimately included in the realistic construction of the
FTUs molecular interaction network. In addition, the
same lists of constituent cells provide a surrogate means
by which FTU structures may be quantifiably compared,
in a manner analogous to the application of primary
amino acid sequence comparison in the comparison of
complex 3D protein entities.
Below we discuss further work to be done in this area
of tissue knowledge representation to address potential
(a) limitations of the underlying theory of FTUs, (b)
shortcomings with the availability of associated empirical
data, as well as (c) oversimplifications in our classifica-
tion approach.
Theory
For a theory of transport physiology to be generally ap-
plicable, the concept of an FTU must factor in thepresence of more than one advective channel in the
organizational unit. In that sense, a theory of FTUs
must be extended to represent different advective sys-
tems to be located within diffusion distance within the
same FTU (e.g. as in epithelial cases such airway, renal,
biliary and intestinal conduits). The vascular compo-
nent of the FTU theory must also encompass the con-
vective circulation of all types of extracellular fluid to
include, for instance, plasma, lymph, tissue fluid, cerebro-
spinal fluid, synovial fluid, ocular humours and peritoneal
fluid. Furthermore, FTUs must also account for two key
properties of capillary networks, namely that:
1) any particular cell may be within diffusive range of
more than one capillary (and therefore may be part
of more than one FTU), and
2) given that, on average, a capillary is about 500 ?m in
length, compared to an approximately 60-?m stretch
in an FTU, it is possible that the constitution of the
FTU (and, therefore, that of its derivative primary
de Bono et al. Journal of Biomedical Semantics 2013, 4:22 Page 8 of 13
http://www.jbiomedsem.com/content/4/1/22motif ) may alter along the course of the capillary
from the arteriolar to the venular end.
Data
The large scale cataloguing of FTU-related data is a key
challenge in providing quantitative measures about the
relative proportion and spatial distribution of cells along
the FTU, as a first step to (a) predicting surface adhesion
interactions that require cells to be in contact with one
another, and (b) developing realistic mathematical simu-
lations of tissue-level processes over a faithful geometric
representation of the organization of cells in space.
Apart from the generation of significant quantities of
volumetric FTU images, two key data shortcomings that
stand in the way of realistic calculations about multi-
organ molecular networks are:
(i) terminological data: given the complexity of cellular
subtyping, current standard reference ontologies for
cell types may not be sufficiently granular to meet
tissue annotational requirements across the board
(e.g. the January 2013 version of CellType ontology
[19,20] (available via http://bioportal.bioontology.
org) does not yet support the functionally critical
distinction [21] between dermal fibroblasts of
papillary and reticular origin);
(ii) biochemical data: crucially, not all substances
suspended in tissue fluid or plasma are able to cross
all types of endothelial lining and underlying
basement membrane (e.g. the bloodbrain barrier
being one extreme case in point). The cataloguing of
tissue-specific biochemical indices [22] for relevant
extracellular substances (e.g. permeability, reflection
co-efficient) would ensure a reliable calculation of
sustainable molecular networks that cross the semi-
permable capillary membrane.
It is anticipated that addressing the formidable obstacles
outlined above will require community-level co-ordinated
contribution to generate the requisite data and knowledge.
Classification
While the overall classification method, applied to the 5
PTMs depicted in Figure 2, verifiably clusters tissues
according to well-known structural and functional cri-
teria (e.g. the shared presence of musculature in heart
and bronchiole, and the closely-related epithelial features
of the two gut mucosae) the overall approach is subject
to limitations in terms of the precise biological meaning
of PTMs and their automated comparison.
For instance, the curation of the 5 PTMs in our ex-
ample may misleadingly suggest that all these PTMs
have exactly the same type of endothelial and fibroblast
cell within their corresponding FTUs (and subsequentlyclassified on this basis). While this is unlikely to be the
case in practice (e.g. it is not likely that the subtype of
endothelial cell in the fenestrated liver circulation will be
the same as that in the heart [23]), the lack of appropri-
ate terms representing endothelial subtypes in the CellType
ontology highlights the limitation of terminological granu-
larity discussed above. In future, the application of semantic
similarity calculations over gross anatomy terms linked to
the PTMs location knowledge may allay this representa-
tional shortcoming.
Conclusion
An FTU is a unit of tissue organization where diffusive
and advective flow transport modalities interlink, thus
connecting molecular processes involving tissues or or-
gans that are far apart (Figure 3). In our work, we outline
the precise biophysical rationale for a rigorous definition
of the FTU. In so doing, we acknowledge (i) the funda-
mental role of capillaries in directing and radically
informing the formation of tissue architecture (e.g. as
discussed in ref [24]), as well as (ii) the importance of tak-
ing into full account the critical influence of neighbouring
cellular environments when studying complex develop-
mental and pathological phenomena (e.g. the effect of
stromal cells on cancer progression [25]).
By analogy to protein domain structure, which exhibits
a backbone of linked alpha carbons tethering amino acid
residues that interact to form a functional peptide unit,
a capillary serves as an organizational backbone to an
interacting cluster of cells that constitutes an FTU.
The primary motif of an FTU coherently maps (i)
terms representing these interacting cells to (ii) terms
representing anatomical location. This mapping ap-
proach squarely addresses a major use case requirement
in mammalian systems biology to computationally inte-
grate data and model resources linked to biological struc-
ture across the divide of tissue scale. Given the increasing
availability of resource metadata that are semantically an-
notated with terms from standard reference ontologies
(e.g. as discussed in [26]), the potential generation of
multi-tissue FTU catalogues opens an avenue for the
functional bridging of cellular-level high throughput
molecular resources with organ-level clinical resources.
The realistic description of molecular processes is at
the core of the notion of FTUs and their derivative mo-
tifs. In future, we anticipate the development of a hier-
archy of tissue motifs (a hierarchy analogous to that of
primary, secondary, tertiary and quaternary protein struc-
ture descriptions) with which to organize the depiction of
molecular processes. In this hierarchy, each motif level in-
crease cumulatively provides more detail about the molecu-
lar interaction network held in the FTU. In this paper, for
instance, we described the types of cell in a primary tissue
motif that contribute secreted molecules and cell-surface
Figure 3 Mock up of whole-body treemap schematic depicting a multi-organ endocrine pathway consiting of interlinked FTU-level
molecular networks. Figure 1D is extended to depict a number of primary tissue motifs representing FTUs involved in the endocrine regulation of
electrolyte and blood pressure levels during exercise. Every individual motif is labeled (in blue  see below for key to label abbreviations). In this
mockup of an ApiNATOMY [9] schematic, nesting of one box within another represents the part_of relation such that: (i) tiles representing motifs are
nested within tiles representing the anatomical region of origin for the tissue material from which the FTU was acquired, and (ii) tiles representing the
constitutent cells of the motif are nested within the corresponding motif tile. The position of nodes in the treegraph overlaid onto the treemap
depicts the location of substances (i.e. molecules or charged atoms), with respect to the motif constituents, as follows: a node within the boundary of
a cell tile represents an intracellular substance; on the boundary of a cell tile represent a molecule tethered to the plasma membrane of that cell;
outside all cell boundaries represents a substance located in the extracellular tissue fluid of the corresponding FTU. TISSUE MOTIF LABELS: [LGIN: Large
Intestine; SMIN: Small Intestine; LIVR: Liver; STMC: Stomach; ADRC: Adrenal Cortices; HART: Heart; ARTL: Arterioles; BLOD: Blood; LUNG: Lungs; KDNY:
Kidneys; ADRM: Adrenal Medullae; CORD: Spinal Cord; MEDL: Medulla Oblongata; PITR: Pituitary; SKLM: Skeletal Muscles; SWET: Sweat Glands]. EDGE
COLOUR: [Black: Molecular Binding; Blue: Intercompartmental Translocation].
de Bono et al. Journal of Biomedical Semantics 2013, 4:22 Page 9 of 13
http://www.jbiomedsem.com/content/4/1/22receptors to this interaction network. It is envisaged that a
secondary tissue motif will further enrich this network by
taking into account those cells that are in direct plasma
membrane contact with one another, enabling surface-
to-surface molecular interaction (e.g. cell adhesion).
Additional interactions brought into an FTUs molecu-
lar network by advective means (e.g. through the blood
supply) would then characterize the contribution of a
tertiary tissue motif.
Methods
Part 1: Representing knowledge about primary tissue motifs
Maintaining a registry, classifying and recording charac-
teristic knowledge about PTMs motivates the specificationof a knowledge representation. We briefly overview the
approach taken and elicit a number of requirements for
such knowledge representation.
We adopt a formal ontological framework, the Basic
Formal Ontology [10], to provide a general formal the-
ory underlying the ontological analysis and formal treat-
ment of FTUs and their derivative PTMs. BFO has been
chosen for its simplicity and clear-cut, general distinctions
such as, for example, between objects and the processes in
which they participate. In practice, such provision allows
for making domain specific ontologies more relevant to
their intended domain (by off-loading generalities) and
more structured (by adopting constraints from the
adopted generic framework). For example, BFO makes
de Bono et al. Journal of Biomedical Semantics 2013, 4:22 Page 10 of 13
http://www.jbiomedsem.com/content/4/1/22provision for property-like entities that depend upon their
bearers - this provision proves useful in conceptualising
motifs, as we will see shortly. Formally, the distinction be-
tween functional tissue units and tissue motifs is enforced
via the BFO distinction between higher-level categories of
object-like entities and property-like entities. In the
present work, the recourse to BFO thus allows us to
secure, albeit here implicitly, the formal treatment of
the general aspects of FTUs, TMs and their relations
(categorical distinction, mereotopological features of
FTUs, existential dependence of PTMs upon FTUs and
so on). Furthermore, as the BFO framework has already
been applied in related areas of the biomedical domain (in
particular, in the context of the Open Biological and
Biomedical Ontologies (OBO) Foundry [27]), this choice
facilitates some degree of integration of the present treat-
ment with related, existing or forthcoming, formalizations
ab initio.
In BFO parlance, the world is made of two main kinds
of things: objects, such as material objects and processes
that involve these objects. We find this high-level di-
chotomy adequate for dealing with FTUs, their deriva-
tive motifs, and their role in physiology processes.
According to this view, tissue motifs are on the side of
objects insofar as these motifs are patterns of structural
organization of possibly complex objects (i.e. tissues).
But tissue motifs are of course not these objects: they
are not tissues; they are repeating patterns of tissue
structure. In BFO, entities such as patterns fall under a
category of so-called Generically Dependent Continuant
(which means that tissue motifs need some other entity
in order to exist). Thus, we separate (i) motifs as entities
in their own right from (ii) the entities (i.e. FTUs) in
which they recur as patterns.
While these considerations solve the question of the
ontological status of tissue motifs, they do little to pro-
vide the formal means for describing and registering the
characteristics of tissue motifs in general and, in particu-
lar, for registering the differential characteristics between
distinct motifs. Certainly, as generically dependent en-
tities, tissue motifs can be characterized as the motifs of
some tissue. This however does little more than secure a
form of bookkeeping and, while it is fundamental for
some purpose to identify and collect the association be-
tween tissues and their motifs, more detail is needed.
One reason why such associations are important is that
tissue motifs give a key to the classification of tissues.
Furthermore, once the description of tissue motifs in-
cludes enough of the physical characteristics from
which to derive characterizations of the physiological
processes, they allow richer characterizations of tissues
to be achieved, including characterizations of physio-
logical processes now occurring at the tissue level of
granularity.The elements of the required tissue framework can
separated into three main kinds according to the repre-
sentation they support:
1) the characterization of tissue motifs through (i) the
type of relationships in which they enter with other
entities such as anatomical location, tissues and
material parts of these tissues (e.g. cells and fluid
compartments), as well as (ii) the way they are
configured in virtue of presenting a given motif;
2) the elicitation of selected aspects of tissue motifs
allowing for deriving the characteristics of the
physiological processes they enable (spatial
relationships and distances, in particular), as well as the
various types of processes in question (e.g. processes of
flow, stress transfer, electrical transmission, etc.);
3) the description of emerging biological properties and
functions that tissues have in virtue of presenting
given motifs or their combinations.
An interesting, and also challenging, aspect of such
knowledge representation is that it brings together,
through the central treatment of tissue motifs, treat-
ments that are traditionally circumscribed to areas of
the biomedical domain but that lack the required articu-
lation to support a multi-scale ontology of transport
physiology (e.g. as discussed in [28]). Given tissue motifs
and their formal account, it is possible to articulate the
description of transport phenomena from scales that
range from the molecular to the organ level. Tissue mo-
tifs, therefore, provide a key bridge for the representa-
tion of transport physiology, which can now be traversed
as a network of connected and interdependent know-
ledge representations.
The presentation of a full-fledged formalisation of the
knowledge representation answering the above require-
ments is beyond the scope of this paper. Furthermore,
our methodology is to adopt a dual bottom-up and top-
down approach grounded in the experimentation with
image annotation and their knowledge management.
Thus a number of particular questions remain open at
this stage and the formalisation outlined here is largely
an item of future work. A preliminary specification of
the core concepts is given in the Results section of this
paper.
Part 2: Acquiring and annotating FTU data
The prototypical histology image acquisition and anno-
tation workflow required for FTU data production is ex-
emplified in the process we specifically carried out to
generate and annotate three-dimensional (3D) image
data from human colon tissue (tissue material provided
by David Harrison, University of St. Andrews). Following
visual inspection of the reconstructed 3D image, by way
de Bono et al. Journal of Biomedical Semantics 2013, 4:22 Page 11 of 13
http://www.jbiomedsem.com/content/4/1/22of demonstration, the following sample annotations were
generated for:
1) the location and course of small-calibre blood
vessels (in view of the distortion caused by the
plastic embedding procedure in this particular
experiment, the positive identification of capillaries
could not be conclusively ascertained). A movie that
cycles through a set of contiguous slides illustrating
the course of three colonic vessels, coloured red,
green and blue, is available through the Additional
file 1: Movie S1;
2) the approximate diffusive field surrounding a blood
vessel was automatically calculated by drawing a 40-?m
radius centred on the vascular lumen, in the plane of
the original histological sections, using the image
processing operation of dilation (as in this particular
example the long axis of the vessel is not absolutely
orthogonal to the plane of these sections, further
refinement of our approach to diffusive field calculations
is necessary). Additional file 2: Movie S2 shows the
traversal - twice, in opposite directions - of anFigure 4 Annotation of the location of cell nuclei within a vascular di
lumen (light blue shading) were identified and annotated onto three broad
dot) and connective tissue (orange dot). The interface shown in this scree
eAtlasViewer_demo/application/TPRDemo/wlz/colonRecon.php].approximately 60-?m stretch along a vessel. This movie
consists of two identical sets of 30 histology images -
the slice thickness between on image and another being
2 ?m. The first 15 secs of the movie shows the location
of the vessel (in dark blue) over the 30 contiguous
slides. The second 15-sec half of the movie traverses the
same 30 slides in the reverse order showing both the
central lumen and a 40-?m dilation that surrounds it;
3) the location of cellular nuclei within the
approximate diffusive field. This step annotated
cellular nuclei onto three broad cellular categories,
namely: endothelial (Figure 4, red dot), epithelial
(green dot) and connective tissue (orange dot). The
full set of annotations over the 30 slides discussed
above is presented in Additional file 3: Movie S3,
and the result of their reconstruction is animated in
3D in Additional file 4: Movie S4.
The full dataset from this work (generated by RB and
BdB) can be navigated at the following URL: [http://
aberlour.hgu.mrc.ac.uk/eAtlasViewer_demo/application/
TPRDemo/wlz/colonRecon.php].ffusive field. Cellular nuclei within 40 ?m of the centre of the vascular
cellular categories, namely: endothelial (red dot), epithelial (green
nshot can be found at: [http://aberlour.hgu.mrc.ac.uk/
de Bono et al. Journal of Biomedical Semantics 2013, 4:22 Page 12 of 13
http://www.jbiomedsem.com/content/4/1/22Part 3: Ethical approval for experimental work on
human tissue
Tissue collection took place in Edinburgh through the
Pathology Department in Lothian University Hospitals.
The laboratory is clinically accredited (under UK Clinical
Pathology Accreditation programme).
The tissue collection was conducted under GLP stan-
dards (UK MHRA inspected) and subject to regulation
and inspection by Scottish Government: Health Im-
provement Scotland (HIS) Team. A range of ethical per-
missions is in place (07/S1102/33: 08/S1103/10: 08/
S1101/41: 10/S1402/33) granted by South East Scotland
Ethics Committee. These permissions allow tissue and
clinical data to be collected with full consent, but also
permit use of unconsented tissue surplus to diagnostic
requirement, providing that the tissue would have been re-
moved anyway and the sample is completely anonymised.
Permission includes use for academic research, teaching
and training, provision to commercial companies and ex-
port overseas, including all microscopy techniques as well
as molecular biology and proteomic uses, on completion of
a simple Material Transfer Agreement. In this context, the
work described in this paper adheres to operational princi-
ples that are consistent with the Oviedo convention and
the Helsinki declaration in its last 2002 amendment. This
ensures that appropriate consent has been obtained for the
envisaged use of data, prior to anonymisation.
Additional files
Additional file 1: Movie S1. Movie that cycles through a set of
contiguous slides (2 microns apart) taken from human colon illustrating
the course of three capillary-like vessels coloured red, green and blue.
Additional file 2: Movie S2. Movie that follows in close-up the
traversal - twice, in opposite directions - of an approximately 60-?m
stretch along the blue colonic vessel shown in file 1.
Additional file 3: Movie S3. Movie showing the location of cellular
nuclei within the approximate diffusive field of a colonic blood vessel.
The nuclei are annotated onto three broad cellular categories, namely:
endothelial (red dot), epithelial (green dot) and connective tissue
(orange dot).
Additional file 4: Movie S4. 3D animation of the annotation
reconstruction for a colonic FTU.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
The work and the redaction of the paper is the result of the authors
collaboration. BdB developed and curated the knowledge of functional
tissue units and associated primary motifs, and carried out classification of
the latter set. PG formulated the required characteristics of the knowledge
representation presented in the Methods section. RB provided colon tissue
image data and associated navigational tools. PH provided expertise in
biophysics and in the quantification of FTU dimensions. BdB produced a first
draft of the paper, which was subsequently collaboratively optimized by the
authors. All authors read and approved the final manuscript.
Acknowledgements
The authors gratefully acknowledge:1. Francisco Couto and João Ferreira, University of Lisbon, for the provision
of calculation results for the semantic distance analysis over the CellType
ontology;
2. David Harrison, University of St. Andrews and NHS, UK for the provision of
tissue material discussed in this work.
This work is supported by the following grant funding:
1. the European Commission, grant agreement number 248502 (RICORDO)
and 223920 (VPH NoE) within the 7th Framework Programme;
2. Innovative Medicines Initiative (IMI) grant 1151562 (DDMoRe).
Author details
1Auckland Bioengineering Institute, University of Auckland, Symonds Street,
Auckland 1010, New Zealand. 2CHIME Institute, Archway Campus, University
College London, London, UK. 3European Bioinformatics Institute, Cambridge,
UK. 4MRC Human Genetics Unit, MRC IGMM, University of Edinburgh,
Edinburgh, UK.
Received: 18 January 2013 Accepted: 24 April 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Segerdell et al. Journal of Biomedical Semantics 2013, 4:31
http://www.jbiomedsem.com/content/4/1/31RESEARCH Open AccessEnhanced XAO: the ontology of Xenopus anatomy
and development underpins more accurate
annotation of gene expression and queries on
Xenbase
Erik Segerdell1*, Virgilio G Ponferrada2, Christina James-Zorn2, Kevin A Burns2, Joshua D Fortriede2,
Wasila M Dahdul3,4, Peter D Vize5 and Aaron M Zorn2Abstract
Background: The African clawed frogs Xenopus laevis and Xenopus tropicalis are prominent animal model
organisms. Xenopus research contributes to the understanding of genetic, developmental and molecular
mechanisms underlying human disease. The Xenopus Anatomy Ontology (XAO) reflects the anatomy and
embryological development of Xenopus. The XAO provides consistent terminology that can be applied to
anatomical feature descriptions along with a set of relationships that indicate how each anatomical entity is related
to others in the embryo, tadpole, or adult frog. The XAO is integral to the functionality of Xenbase (http://www.
xenbase.org), the Xenopus model organism database.
Results: We significantly expanded the XAO in the last five years by adding 612 anatomical terms, 2934
JOURNAL OF
BIOMEDICAL SEMANTICS
Schofield et al. Journal of Biomedical Semantics 2013, 4:18
http://www.jbiomedsem.com/content/4/1/18DATABASE Open AccessThe mouse pathology ontology, MPATH; structure
and applications
Paul N Schofield1,2*, John P Sundberg2, Beth A Sundberg2, Colin McKerlie3 and Georgios V Gkoutos4Abstract
Background: The capture and use of disease-related anatomic pathology data for both model organism phenotyping
and human clinical practice requires a relatively simple nomenclature and coding system that can be integrated into
data collection platforms (such as computerized medical record-keeping systems) to enable the pathologist to rapidly
screen and accurately record observations. The MPATH ontology was originally constructed in 2,000 by a committee of
pathologists for the annotation of rodent histopathology images, but is now widely used for coding and analysis of
disease and phenotype data for rodents, humans and zebrafish.
Construction and content: MPATH is divided into two main branches describing pathological processes and
structures based on traditional histopathological principles. It does not aim to include definitive diagnoses, which
would generally be regarded as disease concepts. It contains 888 core pathology terms in an almost exclusively is_a
hierarchy nine layers deep. Currently, 86% of the terms have textual definitions and contain relationships as well as
logical axioms to other ontologies such the Gene Ontology.
Application and utility: MPATH was originally devised for the annotation of histopathological images from mice but
is now being used much more widely in the recording of diagnostic and phenotypic data from both mice and
humans, and in the construction of logical definitions for phenotype and disease ontologies. We discuss the use of
MPATH to generate cross-products with qualifiers derived from a subset of the Phenotype and Trait Ontology (PATO)
and its application to large-scale high-throughput phenotyping studies. MPATH provides a largely species-agnostic
ontology for the descriptions of anatomic pathology, which can be applied to most amniotes and is now finding
extensive use in species other than mice. It enables investigators to interrogate large datasets at a variety of depths,
use semantic analysis to identify the relations between diseases in different species and integrate pathology data with
other data types, such as pharmacogenomics.
Keywords: Pathology, Ontology, Disease, Mouse, PhenotypeBackground
Since the late eighteenth century when achromatic
lenses and reliable histological stains began to be avail-
able, investigators of anatomic pathology, and particu-
larly in the mid -nineteenth century the innovators of
cellular pathology such as Rudolf Virchow, developed
and applied terminologies to describe their observations
[1,2]. These depended on the school to which the pa-
thologists belonged, but more importantly on the etio-
logic or mechanistic paradigm in which they were* Correspondence: ps@mole.bio.cam.ac.uk
1Department of Physiology, Development and Neuroscience, University of
Cambridge, Downing Street, CB2 3EG, Cambridge, UK
2The Jackson Laboratory, 600, Main Street, Bar Harbor, ME 04609-1500, USA
Full list of author information is available at the end of the article
© 2013 Schofield et al.; licensee BioMed Centr
Commons Attribution License (http://creativec
reproduction in any medium, provided the orworking [3]. One of the great achievements of the nine-
teenth century was the recognition of the universality of
pathological processes and entities and their occurrence
in multiple species as recognisable manifestations of the
same underlying processes [4]. It was, nevertheless, a
century before broadly accepted and rationally struc-
tured pathology terminologies were developed (e.g. [5]).
The development of pathology terminologies has to an
extent occurred independently of disease terminologies
and nosologies, partly as a result of the much longer his-
tory of classifying diseases, and partly due to the
inherited preconceptions of the nature of disease in clin-
ical medicine.
The distinction between pathological and clinical de-
scriptions of disease, disorders and predispositions is stillal Ltd. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Schofield et al. Journal of Biomedical Semantics 2013, 4:18 Page 2 of 8
http://www.jbiomedsem.com/content/4/1/18not satisfactorily resolved. However, in recent years there
have been attempts to rationalise the definitions of these
concepts [6] and their relation to each other as part of a
broadly applicable model of disease other than an un-
structured collection of manifestations or phenotypes
which are found in that class of individuals forming the
basis of a diagnosis. Issues about severity, time course,
organ involvement etc. are beginning to be addressed,
but it is remarkable that even treating diseases as a bag
of phenotypes has been shown to provide a powerful ap-
proach in establishing the relationships between dis-
eases, and the presence of related diseases in different
organisms [7-10]. What has recently been identified as
important, nevertheless, is that the tissue-specific reso-
lution of the recording of lesions, and the ability to rec-
ord the pattern of disease within an individual, has
proved vital for GWAS mapping of predisposing genetic
variants in inbred strains of mice allowing each class of
lesion to be analysed in isolation [11,12].
The discipline of pathology may be broken down into
clinical and anatomic pathology, the former is concerned
with clinical chemistry, hematology, clinical microbiol-
ogy and emerging sub-specialities such as molecular
diagnostics and proteomics. The latter, which forms the
domain of MPATH, deals with the histological, histochem-
ical or immunohistochemical observations of alterations
in tissue composition or architecture. Both branches of
the medical specialty, which are increasingly merging, may
be viewed as aspects of phenotyping, and both provide
subtypes of the clinical signs associated with ongoing dis-
ease processes, the results of developmental abnormalities,
or the historical presence of disease.
Anatomic pathology nomenclature and its applications
The universality of the repertoire of responses to under-
lying genetic or extrinsic insults means that gross and
histopathologically-defined phenotypes are some of the
most useful phenotypes for relating diseases between dif-
ferent species, and constitute some of the most species-
agnostic phenotype descriptors. This makes a pathologic
term-based ontology a crucial tool in experimental and
clinical phenotype data capture [13].
The development of systematic human pathologic no-
menclatures has been driven by the efforts of the American
College of Pathologists, initially with the development of
the pathology specific nomenclature (SNOP) over 40 years
ago [14] to the current SNOMED CT with cross refer-
ences to UMLS, the NCI thesaurus and other terminolo-
gies. The ICD [15], now in its 11th revision and the
associated ICD-O v-3 for cancer, also contains descrip-
tions of many pathological lesions.
The other driver for pathologic terminology standard-
isation has been coding of lesions from toxicopathology.
The American Society of Toxicopathology (STP) workingwith Registry of Industrial Toxicology Animal-data (RITA)
database group in Europe has produced several inter-
nationally accepted nomenclature systems, particularly fo-
cusing on proliferative lesions. Recently, the STP has
undertaken a major harmonization exercise for rodent
pathology  the INHAND (International Harmonization
of Nomenclature and Diagnostic Criteria for Lesions in
Rats and Mice) initiative [16]. So far this group has
reported on the hepaticobiliary, respiratory, nervous and
urinary systems [17-20]. For some time the National Can-
cer Institutes Mouse Models of Human Cancer consor-
tium (MMHCC) has been examining the classification of
tumours in genetically engineered mice. MMHCC has
produced a consensus base terminology for neoplasias of
the major organ systems that have been presented in a
series of papers over the last decade [21].
Despite the huge value of these resources, none is cur-
rently constructed as an ontology with meaningful ax-
ioms to support inference and automated reasoning, and
to that end we developed MPATH to describe lesions
that arise in laboratory mice.
Construction and content
The MPATH ontology was constructed ab initio by a
group of clinical and veterinary pathologists in 2,000 and
has since been revised and augmented by an evolving
group of US and European pathologists on a regular
basis. It is clear from more than a decade of experience
that expert input and manual curation are essential to
generate an accurate and functional resource. One strat-
egy for building the ontology has been to integrate it
into large-scale phenotyping and diagnostic programs so
that the pathologists use it on a daily basis and have
fields to add missing terms or synonyms that they are
more familiar with thereby constantly increasing its
coverage and utilitarian value.
MPATH contains 888 classes in an almost exclusively
is_a hierarchy eight layers deep. Part_of relations are used
only in the case of, for example, adenomatous polyp
(MPATH:490) and intraductal papilloma (MPATH:285)
where pathologists distinguish between a macroscopic le-
sion (osis) and individual lesions which make them up;
both need to be described. The top level distinction in
MPATH is between pathophysiology (pathological pro-
cesses) and anatomic pathology (pathological entities).
One issue met frequently in development was the normal
practice of referring to observation of a physical lesion by
using the process term; for example necrosis or scler-
osis. Thus the noun describing the real-world entity ob-
served is homonymous with the inferred process. This
problem has been addressed through the textual and lo-
gical definitions of terms but is a recurrent source of con-
fusion in formal treatments of pathology nomenclature.
Pathologists using MPATH almost exclusively use the
Schofield et al. Journal of Biomedical Semantics 2013, 4:18 Page 3 of 8
http://www.jbiomedsem.com/content/4/1/18anatomic pathology segment of the ontology with the ex-
ception of describing inflammation or other general pro-
cesses where the process is described using qualifiers such
as acute which are logically process-specific, consistent
with the use of Phenotype And Trait Ontology (PATO)
[22] qualifiers (see below). The upper levels of MPATHs
anatomic pathology branch include six broad domains fa-
miliar to all traditions of pathology training and compre-
hensively covering all known lesions; cell and tissue
damage, circulatory disorder, developmental and structural
abnormality, growth and differentiation defect, healing
and repair structure, and neoplasm, and are as orthogonal
as is feasible given the complexities of pathobiology (see
Figure 1A). The upper levels of MPATHs pathophysiology
branch denote pathological processes that underlie lesions
and include six broad domains; cell and tissue damage
process (see Figure 1C), defective growth and differenti-
ation process, developmental process abnormalities,
healing and repair process, immunopathological process,
and neoplasia. All pathological processes and entities can
be placed within these upper level domains, which will
be familiar to all pathologists and are common to all
amniotes.
Relationship to upper level and other ontologies
MPATH is largely congruent with the upper level Ontol-
ogy for General Medical Science (OGMS) [6], founded
on the Basic Formal Ontology (BFO). Pathological bodily
process (OGMS:0000061) and pathological anatomicalFigure 1 Structure of MPATH. A; overall structure of the top level classes
entity for the major divisions. B; polyhierarchy showing multiple parentage
and morphological classification. C; cell and tissue damage process segmenstructure (OGMS:0000077) are broadly mappable to the
upper levels of MPATH; MPATH:603 (pathological ana-
tomical entity) and MPATH: 596 (pathological process) re-
spectively. However, more detailed mapping is difficult.
For example the MPATH experts view congenital malfor-
mations as pathological anatomical structures, whereas
OGMS views them as distinct, and similarly MPATH
views inflammation as a pathological process whereas
OGMS does not include this as a pathological bodily
process. Until such discrepancies are resolved, integra-
tion of MPATH into the OGMS framework will be
problematical.
From the point of view of application, the most import-
ant mappings for MPATH are to the Human Phenotype
Ontology (125), the Mammalian Phenotype ontology
(111), the Disease Ontology (231), SNOMED-CT (867)
and the NCIt (566), reflecting the emphasis on the domain
of anatomic pathology rather than disease.
Definitions and axiomatic relationships
Currently, 86% of classes have textual definitions. Each
class is in the mouse pathology namespace and is
uniquely identified by a URI of the form: http://purl.
obolibrary.org/OBO/MPATH_n. The main ontology is
available in both the OBO Flatfile Format and the Web
Ontology Language (OWL). MPATH is housed in a sub-
version repository and is made available via OBO registry,
Bioportal (http://purl.bioontology.org/ontology/MPATH)
and on the projects website http://mpath.googlecode.com/.in MPATH showing division into pathological process and physical
for some anatomically predicated tumor classes and their anatomical
t of MPATH showing main pathological process classes in this branch.
Schofield et al. Journal of Biomedical Semantics 2013, 4:18 Page 4 of 8
http://www.jbiomedsem.com/content/4/1/18MPATH contains relationships and other logical axioms to
other ontologies such the Gene Ontology (GO) [23], Cell
Type ontology (CL) [24] and the Phenotype And Trait
Ontology (PATO) [22]. For example, the MPATH term
transitional cell metaplasia (MPATH:172) represents a
metaplastic response of the transitional epithelium, for ex-
ample in the bladder to give squamous metaplasia and
glandular metaplasia. To allow computational access to
these relations, we use the derives-from relation and relate
metaplasia (MPATH:549) (an MPATH term that denotes
an abnormal transformation of a differentiated adult
cell or tissue of one kind into a differentiated tissue of
another kind) with the CL term transitional epithelial
cell (CL:0000244).
Application and utility
A post-composition strategy for pathology coding
Traditionally pathologists have relied on a narrative form
of recording their definitive diagnoses, making use of
morphologic, etiologic, and disease-based terms that col-
lectively provide a diagnosis useful for clinical patient
management. This is particularly important for non
neoplastic lesions where it can be complex to capture
important subtleties of distribution, severity, microscopic
sub-type and anatomical location for example. Whilst
this is the gold standard, it is not possible to compute
on data recorded in this way and it is very difficult to
tabulate and quantitatively analyse the collected infor-
mation. There are strong arguments, mainly from ex-
perience in toxicologic pathology, that a descriptive
(anatomic) rather than diagnostic coding is the most ob-
jective and useful way to code pathology-based observa-
tions. This is particularly relevant to examination of
mutant mice where traditional etiologic or summative
diagnostic terms are simply not available because of the
novelty of the lesion or its presentation. This is particu-
larly the case where mice are manipulated to model hu-
man conditions that have not been previously seen, for
example lung or mammary tumours [11,25,26] which
have not previously been reported to occur spontan-
eously in mice. In many cases, a disease diagnosis im-
plies a particular pathogenesis or etiology based on the
spontaneous disease, which is not appropriate for the
disease caused by genetic and sometimes both genetic
and external challenge combined. This latter issue is of
particular concern to practicing pathologists and in the
development of MPATH we have been urged to include
some diagnostic terms as well as descriptive anatomic
ones.
Many tissue responses are common to multiple ana-
tomical sites and as far as possible the verbosity (ontol-
ogy bloat) of specifying a particular response in
multiple tissues has been avoided, with the additional
topographical or anatomical information for descriptioncoming from an anatomy ontology, generally the MA
[27] or EMAP ontologies [28] for the mouse, however,
there is often an intrinsic anatomical element embedded
in the term or traditional pathology includes information
about the cell type or tissue of origin. This is most fre-
quent with the neoplasias and we felt that such terms
were best included in their familiar form. Figure 1B
shows how anatomically predicated classes such as hepa-
tocellular carcinoma have multiple parents, providing re-
lations in this case to both carcinoma and hepatic tumor
superclasses. Most observations made by pathologists
using MPATH are, nevertheless, cross-products using a
combination of an MPATH term and an anatomical
(MA) or cell type (CL) [24,29] component. This strategy
provides all of the necessary coverage.
In addition to the core terms in MPATH, it is import-
ant to describe organ-specific topography, distribution,
microscopic character, duration/chronicity and severity.
These are not included in MPATH but drawn from
other ontologies. These qualifiers or modifiers are gener-
ally applicable across a wide range of organs and lesions
and so need to be coded separately to the core terms to
allow post-composition as required. The pattern we have
adopted is very close to that recommended by the
INHAND proposals and also includes compound
terms which lie beneath a definitive diagnosis or disease
level of description, but bundle defined sets of descrip-
tive terms, for example nephropathy, alopecia, glom-
erulonephritis which are in common use and well
understood. These qualifiers have been incorporated into
PATO, and some examples are given in Table 1. The
strategy for composing pathology descriptions using the
combination of MPATH, MA and PATO is summarised
in Figure 2.
Implementation of MPATH coding strategy
The strategy adopted was originally designed to describe
histopathology images for the Pathbase mouse pathology
database [30], but lends itself readily to a wide range of
coding applications. The MPATH strategy has been
adopted by two major high-throughput studies. A com-
bination of MPATH and PATO is being used for the
capture of pathology data from the genome-wide mutant
mouse phenotyping project, KOMP2 run as part of the
International Mouse Phenotyping Consortium [31],
where the MPATH approach is being used in the pri-
mary phenotyping pipeline by the Toronto Centre for
Phenogenomics and other centres carrying out histo-
pathology. MPATH has also been adopted for the
MoDIS database [32] to capture and analyse pathology
data from a massive aging study which has systematically
phenotyped 31 of the most important inbred mouse
strains. Complete necropsies of mice were carried out at
12 and 20 months of age (cross-sectional study) and
Figure 2 Post-composition coding strategy. Elements of the compound description are specified by the string of linked pentagons on the left
hand side of the figure and specific examples given for three observations which taken together are indicative of foreign body pneumonia.
Tissue terms are taken from the appropriate anatomy ontology, eg. MA, Process from MPATH, character, topology, distribution and severity from
PATO. The combination of observations defines the disease foreign body pneumonia.
Table 1 Examples of pathology term qualifiers now incorporated into PATO
Qualifier PATO Class name Definition
Severity 0000461 Normal No lesions
0000394 Mild Lesion dependent; often size, number and characteristics.
0000395 Moderate
0000465 Marked
0000396 Severe
Duration 0002387 Per-acute Extremely acute and aggressive
0000389 Acute Beginning abruptly with marked intensity
0002091 Subacute Between acute and chronic
0001863 Chronic Slow progress and long continuance
0002387 Chronic-active Coexistence of chronic process and superimposed acute process
Distribution 0000627 Focal Single well delineated lesion
0002388 Focally extensive Single lesion with expansion into surrounding tissue
0001791 Multifocal Multiple lesions
0002389 Multifocal to coalescing Multiple lesions some interconnecting with each other
0000330 Random No appreciable pattern
0001566 Diffuse Not circumscribed or limited
0000635 Generalized Affecting all regions without specificity of distribution
0000634 Unilateral Confined to one side only
0000618 Bilateral Involving both sides
0002389 Segmental Relating to a segment
Schofield et al. Journal of Biomedical Semantics 2013, 4:18 Page 5 of 8
http://www.jbiomedsem.com/content/4/1/18
Schofield et al. Journal of Biomedical Semantics 2013, 4:18 Page 6 of 8
http://www.jbiomedsem.com/content/4/1/18moribund mice in the life span (longitudinal study).
Nearly 2,000 mice were necropsied, generating more
than 50,000 slides [33]. Lesion incidence and severity
data for all organs is now being applied in highly suc-
cessful GWAS studies of age-associated disease [33].
MPATH has proved to be additionally useful in deal-
ing with the recoding of multi-species legacy data from
non-standard nomenclatures, permitting integration of
otherwise siloed data. Examples are the European Radio-
logical archive (ERA) database where 6,700 human diag-
noses were recoded from ICD-8 and the Klinischer
Diagnosenschleussel [34] to MPATH/FMA [35], and
with the Northwestern University Janus radiobiology data-
base (http://janus.northwestern.edu/janus2/), who have
coded 50,000 individual mouse records to MPATH to link
the two datasets. Recently the ontology has been applied
to zebrafish phenotype data in the Zfin database [36] indi-
cating a useful application of MPATH to non-mammalian
species which could be developed further.
MPATH as a core ontology for PATO-based logical definitions
The PATO framework was built with the intention of
providing an integration platform for phenotype data be-
tween species and between data types [22]. According to
the PATO framework, phenotype data can be described
by utilising species-specific ontologies (such as the vari-
ous anatomy ontologies) or species-agnostic ontologies
such as GO with the various qualities provided by the
PATO ontology in order to describe affected entities in a
phenotype manifestation. PATO can be used for annota-
tion either directly in a so-called post-composed (post-
coordinated) manner or for providing logical definitions
(equivalence axioms) to ontologies containing a set of pre-
composed (pre-coordinated) phenotype terms [22,37-39].
For further discussion see [40].
Rather than using a pre-composed phenotype ontology
such as MP [29] or HPO [41], phenotypes may be de-
scribed using the EntityQuality (EQ) formalism. In the
EQ method, a phenotype is characterized by an affected
Entity and a Quality (from PATO) that specifies how the
entity is affected. The affected entity can either be a bio-
logical function or process such as specified in GO, or
an anatomical entity. The phylogenetic conservation, at
least within the amniotes, of most histopathologic lesions
or processes makes MPATH an important core ontology
in writing logical definitions and we have used it exten-
sively in defining classes in the major pre-composed
phenotype ontologies and MPATH is an important com-
ponent ontology of our recently developed semantic ap-
proaches to comparative phenomics  PhenomeNET and
Mousefinder [8,9].
Composition of logical definitions is a time-consuming
task for which there are currently several approaches
to automation using class label segmentation, entityrecognition and lexical matching to core ontologies.
This approach can be useful for suggesting definitions
where the class label is a composite of for example,
anatomy and process (MA +GO). Automated decom-
position of unilexical terms such as are found in the
neoplasias is much more difficult though approaches
with text mining definitions from other ontologies such
as NCIt for lexically matching labels may be useful to
expert curators in establishing more simple definitions
for these classes.
Conclusions
Whilst MPATH was originally designed to support ro-
dent, and particularly mouse, pathology the extensive
overlap with human pathology means that most of the
terms may be used in a human context and linked to the
foundational model of anatomy (FMA) [42] as the anat-
omy ontology. Extending MPATH to become a mamma-
lian pathology ontology encompassing human pathology
is a major undertaking, but we have established that the
current structure and upper level classes would readily
support the inclusion of human terminology. Initially we
will import terms for neoplasias from the CINEAS codes
(Central Information System for Hereditary Diseases and
Synonyms; http://www.cineas.org/; Prof Rolf Sijmons,
pers, comm). SNOMED-CT, UMLS and ICD-O v3 will be
mined for terms not currently in MPATH which relate to
anatomic pathology. Terms already covered by existing
ontologies such as Disease Ontology (DO) [43] may be
referenced using MIREOT [44]. DO classifies diseases
largely by anatomical site and not by disease process or
class, and overlaps only slightly with MPATH as it is
concerned with summative diagnostic entities for the
main part. For example there is no inflammation super-
class in DO for the tissue specific inflammatory conditions
described.
Use of MPATH to construct logical definitions for DO
classes would potentially add a further dimension to the
richness and applicability of DO.
The power of the description of pathological lesions to
discriminate between diseases and therefore between
models of human disease is substantial. We recently es-
timated the information content (IC) of pre-composed
MP ontology terms used to code phenotypes in the
EUMODIC mouse phenotyping pipeline [45], which in-
cluded or excluded anatomic pathology descriptions,
using their logical definitions. Pathology-related pheno-
types were shown to have a significantly greater discrimi-
natory power than other in vivo assays, strongly supporting
the use of these assays in the development of mouse models
of human diseases [13].
Further development and application of MPATH will
inevitably depend on community engagement and we
encourage anyone with an interest to provide feedback.
Schofield et al. Journal of Biomedical Semantics 2013, 4:18 Page 7 of 8
http://www.jbiomedsem.com/content/4/1/18Abbreviations
CINEAS: Central Information System for Hereditary Diseases and Synonyms;
CL: Cell ontology; DO: Disease ontology; EMAP: EMouse atlas project;
ERA: European radiobiology archive; EUMODIC: European mouse disease
clinic; FMA: Foundational model of anatomy; GWAS: Genome-wide
association study; HPO: Human phenotype ontology; ICD: International
classification of disease; ICD-O: International classification of disease 
oncology; IMPC: International Mouse Phenotyping Consortium;
INHAND: International Harmonization of Nomenclature and Diagnostic
Criteria for Lesions in Rats and Mice; KOMP2: Knockout mouse project 2;
MA: Mouse anatomy ontology; MIREOT: Minimum information to reference
an external ontology term; MMHCC: Mouse Models of Human Cancer
consortium; MPATH: Mouse pathology ontology; MP: Mouse phenotype
ontology; NCIt: National Cancer Institute Thesaurus; OBO: Open biological
ontology; PATO: Phenotype and trait ontology; RITA: Registry of Industrial
Toxicology Animal-data; SNOMED-CT: Systematized nomenclature of
medicine clinical terms; STP: Society for toxicopathology; UMLS: Unified
medical language system.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
All of the authors have made major contributions to the writing and coding
of the ontology and its definitions over many years. The paper was written
by PNS, JPS, and GVG. All authors read and approved the final manuscript.
Acknowledgements
The authors would like to thank those who have contributed to the
development and application of MPATH over the years. This work was funded
by the European Commission. Contract QLRI-1999-00320, the Ellison Medical
Foundation, National Institutes of Health AG25707, for the Shock Aging Center,
CA89713, and AR056635 and AR063781 to JPS, and HG004838-04 to PNS.
Author details
1Department of Physiology, Development and Neuroscience, University of
Cambridge, Downing Street, CB2 3EG, Cambridge, UK. 2The Jackson
Laboratory, 600, Main Street, Bar Harbor, ME 04609-1500, USA. 3Physiology
and Experimental Medicine Research Program, The Hospital for Sick Children,
M5G 1X8, Toronto, Canada. 4Department of Computer Science, University of
Aberystwyth, Old College, King Street, SY23 2AX Ceredigion, Wales.
Received: 4 February 2013 Accepted: 19 August 2013
Published: 13 September 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Machado et al. Journal of Biomedical Semantics 2013, 4:21
http://www.jbiomedsem.com/content/4/1/21
RESEARCH Open Access
Enrichment analysis applied to disease
prognosis
Catia M Machado1,2*, Ana T Freitas2 and Francisco M Couto1
*Correspondence:
cmachado@xldb.di.fc.ul.pt
1LaSIGE, Departamento de
Informática, Faculdade de Ciências,
Universidade de Lisboa, Lisboa,
Portugal
2Instituto de Engenharia de
Sistemas e Computadores/Instituto
Superior Técnico, Lisboa, Portugal
Abstract
Enrichment analysis is well established in the field of transcriptomics, where it is used
to identify relevant biological features that characterize a set of genes obtained in an
experiment.
This article proposes the application of enrichment analysis as a first step in a disease
prognosis methodology, in particular of diseases with a strong genetic component.
With this analysis the objective is to identify clinical and biological features that
characterize groups of patients with a common disease, and that can be used to
distinguish between groups of patients associated with disease-related events. Data
mining methodologies can then be used to exploit those features, and assist medical
doctors in the evaluation of the patients in respect to their predisposition for a specific
event.
In this work the disease hypertrophic cardiomyopathy (HCM) is used as a case-study, as
a first test to assess the feasibility of the application of an enrichment analysis to disease
prognosis. To perform this assessment, two groups of patients have been considered:
patients that have suffered a sudden cardiac death episode and patients that have not.
The results presented were obtained with genetic data and the Gene Ontology, in two
enrichment analyses: an enrichment profiling aiming at characterizing a group of
patients (e.g. that suffered a disease-related event) based on their mutations; and a
differential enrichment aiming at identifying differentiating features between a
sub-group of patients and all the patients with the disease. These analyses correspond
to an adaptation of the standard enrichment analysis, since multiple sets of genes are
being considered, one for each patient.
The preliminary results are promising, as the sets of terms obtained reflect the current
knowledge about the gene functions commonly altered in HCM patients, thus
allowing their characterization. Nevertheless, some factors need to be taken into
consideration before the full potential of the enrichment analysis in the prognosis
methodology can be evaluated. One of such factors is the need to test the enrichment
analysis with clinical data, in addition to genetic data, since both types of data are
expected to be necessary for prognosis purposes.
Background
Enrichment analysis is extensively used for the functional analysis of large lists of genes
identified with high-throughput technologies, such as expression microarrays. It exploits
the use of statistical methods over ontological gene annotations to identify biological fea-
tures that are represented in a gene set under analysis more than would be expected by
© 2013 Machado et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 2 of 17
http://www.jbiomedsem.com/content/4/1/21
chance. Such biological features are said to be enriched, or overrepresented, in the study
set and are then used to formulate a biological interpretation about it.
The ontology most commonly used in these analyses is the Gene Ontology [1-3],
although other resources such asMeSH and KEGG have also been explored [4]. Strategies
based on multiple vocabularies have also been developed, namely in pharmacoge-
nomics considering theHumanDiseaseOntology and the Pharmacogenomics Knowledge
Base [5]. LePendu et al. [6] proposed a method to generate annotations when using
medical vocabularies, testing its feasibility with the Human Disease Ontology.
Enrichment analyses are normally divided in three categories: Singular Enrichment
Analysis (SEA), Gene Set Enrichment Analysis (GSEA) andModular Enrichment Analysis
(MEA). SEA works with a user-selected gene set and iteratively tests the enrichment of
each individual ontology concept in a linear mode. GSEA also evaluates the enrichment
of ontology concepts individually, but considering all the genes in the experiment and not
just a user-selected gene set. MEA works with a user-selected gene set, but incorporates
into the analysis the relationships between concepts represented in the ontologies, thus
evolving from a term-centric approach to a biological module-centric approach [7]. Sev-
eral tools have been developed that implement one or more of these approaches, such as
Ontologizer [8,9], Onto-express [10] and GSEA [11].
This article proposes the application of enrichment analysis for disease prognosis, as the
first component of a prognosis methodology that will assist in the evaluation of patients
in respect to the likelihood of suffering a disease-related event (see Figure 1). By per-
forming an enrichment analysis on the patients data based on controlled vocabularies,
we expect to identify sets of characterizing features that will be used as profiles for the
patients. These profiles will then be explored to evaluate the predisposition of the patients
for the specific event. This evaluation is the second component of the prognosis method-
ology, and can be performed by following a classification or a similarity approach. In
the classification approach, the terms composing the profiles will be added as features to
the patients dataset and analyzed with classification algorithms such as random forests
[12] and Bayesian networks [13]. In the similarity approach, semantic similarity measures
will be used to compute the similarity between patients, based on their profiles. Differ-
ent semantic similarity measures [14] and a relatedness measure [15] can be explored to
compare the patients profiles.
Classification
Ontology
Patient
Data
Similarity
Enrichment
Analysis
Profile Prognosis
Figure 1 Schematic representation of the prognosis methodology. The methodology is composed by
two units: the first (left-side) receives as input data from patients mapped to biomedical ontologies/
controlled vocabularies. It performs an enrichment analysis to identify a list of ontology terms considered to
be enriched, which will be used to create profiles for individual patients. These profiles will then be subjected
to an evaluation step (the second unit, on the right-side) that will result in the evaluation of the prognosis for
the patients. For the implementation of the second unit, both a classification and a similarity approach will be
explored.
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 3 of 17
http://www.jbiomedsem.com/content/4/1/21
The purpose of the prognosis methodology is to assist medical doctors in the definition
of the appropriate treatments and preventive actions for individual patients. Conse-
quently, the datasets to be used with this methodology are collected by biomedical experts
in the context of medical practice. Such datasets are frequently characterized by a small
number of clinical features and a high number of missing values, which difficults their
use for knowledge extraction purposes.We propose that the application of an enrichment
analysis to this type of dataset can result in the extraction of relevant knowledge from
controlled vocabularies to improve the quality of the dataset and, therefore, improve the
quality of the predictions made with it.
The present work focuses in diseases which diagnosis and prognosis are dependent both
on clinical and genetic data. An example of such a disease is hypertrophic cardiomyopa-
thy (HCM), which is used in this work as a case-study. HCM is an autosomal dominant
genetic disease, characterized by a variable clinical presentation and onset, with approxi-
mately 900mutations in more than 30 genes currently known to be associated with it [16].
It has been observed that the presence of a given mutation can correspond to a benign
manifestation in one individual and can result in sudden cardiac death (SCD) in another
[17,18]. This disease is, in fact, the most frequent cause of SCD in apparently healthy
young people and athletes [17,18]. Given the severity of this manifestation of the disease,
SCD is the event evaluated with our prognosis methodology.
This methodology is under development, and in the present article we present the pre-
liminary results obtained when applying the enrichment analysis to the genetic data of
patients with HCM. While standard applications of enrichment analysis analyze a single
set of genes per experiment, the application here proposed analyzes several sets of genes,
one from each patient. Our implementation of the enrichment analysis had thus to be
adapted to accommodate multiple sets of genes.
The Gene Ontology (GO) was the ontology chosen to perform the analysis instead of
other genetic vocabularies since it is themost well studied application of enrichment anal-
ysis. It allows the annotation of biological products with terms describing the molecular
functions they perform, the biological processes in which they are involved, and the cel-
lular components where they are located or of which they are a component. Additionally,
the GO was chosen instead of a clinical vocabulary since it has been extensively used
for annotation purposes, and thus possesses a background set of annotations that can be
promptly used, which is not normally the case for clinical vocabularies.
The following sections present and discuss the results obtainedwith the adapted enrich-
ment analysis, considering the genetic data of the HCM patients and the GO; delineate
the conclusions extracted from the results, as well as how the work will evolve; and explain
in detail the methods followed to obtain the results presented.
Results and discussion
Since all the patients share the same genome, it is through their individual mutations that
we can find differentiating features. However, information regarding a patients muta-
tions, when available, exists only for a few genes. In the case of the HCM patients, the
genetic data used in this analysis is precisely the presence/absence of the mutations in the
genes associated with the disease.
An oversimplified way to define the study set when analyzing, for example, the SCD
patients, would be to consider the list of genes mutated in at least one of these patients.
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 4 of 17
http://www.jbiomedsem.com/content/4/1/21
However, this would only be accurate if all the SCD patients had a mutation in those
genes, which might not be the truth. In order to maximize our use of the available
genetic information, the best option is to consider the set of mutations each patient has,
individually.
Following this approach, we performed two enrichment analyses: a profiling analysis,
where the total number of genesmutated in each group of patients (with SCD andwithout
SCD) was compared with all (protein-coding) genes in the same group of patients; and a
differential analysis, where the total number of genes mutated in each group of patients
(with SCD and without SCD) was compared with the total number of genes mutated in
all the HCM patients.
Enrichment profiling
Terms identified as enriched by the profiling analysis can be used to characterize the
genotype of patients with and without SCD (SCD and no-SCD, respectively), since they
correspond to specific functional aspects that are mutated in the patients. These func-
tional aspects, in turn, correspond to phenotypical traits expected to be altered. While
terms identified as enriched both in SCD and no-SCD patients can be interpreted as
associated with the disease, terms enriched differently can be interpreted as associated
with the occurrence of SCD. This profiling analysis is more directly comparable with
the application of enrichment analysis to gene expression data, where a set of genes (e.g.
overexpressed) can be analyzed against the whole genome.
In the enrichment approach followed in this work, i.e. Single Enrichment Analysis, the
set of genes selected by the user to be evaluated for the existence of enriched ontology
terms is called the study set, and these genes can be the ones overexpressed in a microar-
ray. The reference set of genes is called the population set, and can be the whole set of
genes analyzed in the microarray. In the context of the patients profiling analysis, we can
theorize the existence of a study set and a population set for each individual patient. The
study set contains the genes mutated in the patient, whereas the population set contains
all genes in the patient, either mutated or not. In the HCM dataset we only have muta-
tion information for the genes associated with the disease, and consequently the study set
is exclusively composed by these genes. The genes associated with HCM but not tested
(see the Methods section for an explanation of how the genotyping is performed) have
to be treated as genes without mutations just as happens with the genes not associated
with HCM, and are included in the population set. The enrichment analysis is then per-
formed considering in the study set all the genes mutated in all the patients of a given
group (e.g. with SCD). In turn, the population set includes all the genes in the genome of
all the patients in the same group (see Figure 2 for a representation of how the two sets of
genes are obtained).
For SCD patients, the study set contains 16 mutated genes (total for the 14 SCD
patients) and the population set contains 18,759× 14 genes (the number of GO annotated
protein-coding Human genes multiplied by the number of SCD patients). For no-SCD
patients, the study set contains 100 mutated genes (total for the 69 no-SCD patients) and
the population set contains 18,759 × 69 genes (see Table 1 for a compilation of the num-
ber of genes analyzed in both enrichment analyses). It is important to note that the SCD
patients have mutations in only 4 distinct genes, and the no-SCD in 7 distinct genes that
include the previous 4.
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 5 of 17
http://www.jbiomedsem.com/content/4/1/21


 




Patient 1


 




Patient 2
+ + ...
Figure 2 Representation of the population and study sets in the enrichment profiling analysis. The
two sets of dots represent the genome of two patients, from the same group (e.g. with SCD). The smaller,
yellow set of dots, corresponds to the genes mutated in the patient; the larger, white set of dots, corresponds
to the entire genome of the patient: genes not mutated (outside the yellow set) and genes mutated. In these
sets of genes, blue dots represent genes annotated with a term of interest (t); gray dots represent genes not
annotated with t. In the profiling analysis, the study set is the union of the genes mutated in all the patients;
the population set is the union of the genome of all the patients. The annotation frequency is then calculated
by counting the total number of genes annotated with the term in the study set (study frequency) and in the
population set (population frequency).
As shown in Table 2 (in the column Total), the enrichment profiling analysis identified
the following number of enriched terms (p-value < 0.1): 53 for SCD and 70 for no-SCD,
without multiple-testing correction; 40 for SCD and 62 for no-SCD, with Bonferroni
correction.
Tables 3, 4 and 5 show, respectively, the top 10 enriched biological process (BP), molec-
ular function (MF) and cellular component (CC) terms for the SCD patients. The top 10
enriched terms identified for the no-SCD patients are not shown since they are nearly
identical to those of SCD. The full set of results for SCD and no-SCD is available in the
Additional files 1 and 2, respectively.
Analyzing the enriched terms in detail, we can confirm their relation with HCM.
According to the BP terms enriched, the patients analyzed suffer from cardiac alter-
ations (e.g. regulation of heart rate, adult heart development), in particular in the ventricle
(ventricular cardiac muscle tissue morphogenesis and ventricular cardiac muscle tissue
development), and some of those alterations affect the contraction of striated muscles, in
which group the cardiac muscle is included (e.g. actin-myosin filament sliding and actin-
mediated cell contraction). HCM is indeed a cardiac disease, in which themain anatomical
manifestation is the thickening of the interventricular septum, and the occurrence of
a sudden cardiac arrest can be a consequence of the malfunctioning of the heart con-
traction. Considering the MF terms, several binding terms are enriched, namely myosin
heavy chain binding, titin binding, troponin C and troponin I binding. All of these terms
refer to proteins that participate in the contraction of the filaments that compose striated
muscles, and thus the HCM patients present alterations in the normal function of this
Table 1 Number of genes considered in the profiling and the differential enrichment
analyses
Enrichment test Study set Population set
Enrichment profiling
SCD 16 18,759 × 14
no-SCD 100 18,759 × 69
Differential enrichment
SCD 16 116
no-SCD 100 116
For each enrichment test performed is indicated the number of genes in the study and the population sets.
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 6 of 17
http://www.jbiomedsem.com/content/4/1/21
Table 2 Number of enriched terms in each of the analyses performed
Analysis
Number of enriched terms
Bio. Proc. Mol. Func. Cel. Comp. Total
noCorr Bonf noCorr Bonf noCorr Bonf noCorr Bonf
Profiling
SCD 30 19 13 11 10 10 53 40
no-SCD 39 33 21 19 10 10 70 62
Differential
SCD vs no-SCD 0 0 1 0 0 0 1 0
no-SCD vs SCD 2 0 1 0 2 0 5 0
For each enrichment analysis is indicated the number of terms of each GO type (biological process, molecular function and
cellular component), with p-value below 0.1, when considering no multiple-testing correction (noCorr) and with Bonferroni
correction (Bonf).
type of muscle. Finally, the CC terms confirm the previous observations that the alter-
ations in HCM patients occur at the level of striated muscle functioning, namely through
the following terms: striated muscle myosin thick filament, striated muscle thin filament,
troponin complex, A band (a component of the sarcomere) and C zone (a component of
the A band).
The difference between terms enriched for SCD and for no-SCD consists in a set of
18 terms identified in the latter and not in the former (see Table 6). These terms do not
provide biologically meaningful information, since it cannot be interpreted that when
those functions and processes are altered the patients will not suffer a SCD episode. These
differences in the set of enriched terms can be explained by the fact that the number
of no-SCD patients is considerably larger than the number of SCD patients (69 vs. 14)
and consequently there are more distinct genes mutated (7 vs. 4). Thus, the enrichment
profiling analysis did not identify differentiating aspects between the two groups that can
be used to prognosticate SCD solely based on genetic data.
Differential enrichment
With the differential enrichment analysis, our purpose was to identify the differences
between SCD and no-SCD, and thus compare each, in turn, with the complete set of HCM
patients. Since this set is divided in SCD and no-SCD patients, we are basically comparing
one group with the other. As happened in the enrichment profiling, the study set contains
Table 3 Top 10 enriched biological process terms in the profiling analysis of SCD patients
Acc Name p-value p-Bonf SFreq PFreq
GO:0030049 Muscle filament sliding 7.7E-40 4.1E-38 94% 0.21%
GO:0033275 Actin-myosin filament sliding 7.7E-40 4.1E-38 94% 0.21%
GO:0055010 Ventricular cardiac muscle tissue morphogenesis 7.7E-40 4.1E-38 94% 0.21%
GO:0003229 Ventricular cardiac muscle tissue development 2.4E-39 1.3E-37 94% 0.22%
GO:0070252 Actin-mediated cell contraction 6.8E-39 3.6E-37 94% 0.24%
GO:0002027 Regulation of heart rate 1.3E-31 6.9E-30 81% 0.26%
GO:0007512 Adult heart development 6.8E-25 3.6E-23 56% 0.07%
GO:0032781 Positive regulation of ATPase activity 2.9E-15 1.5E-13 38% 0.09%
GO:0043462 Regulation of ATPase activity 2.6E-14 1.4E-12 38% 0.12%
GO:0032971 Regulation of muscle filament sliding 1.0E-12 5.4E-11 25% 0.02%
The terms shown are the 10 biological process terms with the lowest p-value, obtained in the profiling of SCD patients. For
each term is indicated: GO accession number (Acc), term name, p-value without multiple-testing correction, p-value with
Bonferroni correction (p-Bonf), annotation frequency in the study set (SFreq) and annotation frequency in the population
set (PFreq).
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 7 of 17
http://www.jbiomedsem.com/content/4/1/21
Table 4 Top 10 enrichedmolecular function terms in the profiling analysis of SCD patients
Acc Name p-value p-Bonf SFreq PFreq
GO:0008307 Structural constituent of muscle 2.9E-35 1.6E-33 88% 0.25%
GO:0030898 Actin-dependent ATPase activity 1.1E-26 6.1E-25 56% 0.05%
GO:0000146 Microfilament motor activity 1.8E-23 9.4E-22 56% 0.11%
GO:0032036 Myosin heavy chain binding 3.3E-11 1.8E-09 25% 0.04%
GO:0001671 ATPase activator activity 9.1E-11 4.8E-09 25% 0.05%
GO:0031432 Titin binding 2.1E-10 1.1E-08 25% 0.06%
GO:0060590 ATPase regulator activity 4.0E-10 2.1E-08 25% 0.07%
GO:0017022 Myosin binding 7.6E-09 4.0E-07 25% 0.14%
GO:0030172 Troponin C binding 5.3E-06 2.8E-04 13% 0.02%
GO:0031013 Troponin I binding 8.4E-06 4.4E-04 13% 0.03%
The terms shown are the 10 molecular function terms with the lowest p-value, obtained in the profiling of SCD patients. For
each term is indicated: GO accession number (Acc), term name, p-value without multiple-testing correction, p-value with
Bonferroni correction (p-Bonf), annotation frequency in the study set (SFreq) and annotation frequency in the population
set (PFreq).
16 and 100 genes mutated for SCD and no-SCD, respectively. As for the population set,
in this case it contains 116 mutated genes (the sum of the genes mutated in SCD and in
no-SCD). Given the design of the analysis, terms found enriched correspond to functional
aspects that are mutated more frequently in one group of patients than in the other.
A total of one term for SCD and five terms for no-SCD were identified as enriched
(p-value < 0.1, not considering multiple-testing correction). The SCD term is the
MF structural constituent of muscle (p-value = 0.08). The no-SCD terms are: negative
regulation of ATPase activity (p-value = 0.08) and regulation of ATPase activity
(p-value = 0.09; both BP); striated muscle thin filament (p-value = 0.08) and troponin
complex (p-value = 0.08; both CC); and troponin C binding (p-value = 0.08; MF) (the
complete information regarding these terms is available in the Additional file 3).
For the purpose of prognosis, the most interesting terms are evidently those identified
as enriched in SCD. Thus, the term structural constituent of musclemay have potential for
prognosis, given that it occurs more frequently in SCD patients than in no-SCD patients.
Nevertheless, the fact that the corrected p-value is above the significance level and that
Table 5 Top 10 enriched cellular component terms in the profiling analysis of SCD patients
Acc Name p-value p-Bonf SFreq PFreq
GO:0005859 Muscle myosin complex 2.4E-37 1.3E-35 81% 0.10%
GO:0032982 Myosin filament 2.4E-37 1.3E-35 81% 0.10%
GO:0016460 Myosin II complex 1.1E-35 5.8E-34 81% 0.13%
GO:0001725 Stress fiber 4.1E-20 2.2E-18 56% 0.25%
GO:0032432 Actin filament bundle 7.3E-20 3.8E-18 56% 0.27%
GO:0014705 C zone 9.2E-15 4.9E-13 25% 0.01%
GO:0005863 Striated muscle myosin thick filament 1.0E-12 5.4E-11 25% 0.02%
GO:0031672 A band 4.7E-09 2.5E-07 25% 0.13%
GO:0005861 Troponin complex 2.2E-05 1.1E-03 13% 0.04%
GO:0005865 Striated muscle thin filament 6.6E-05 3.5E-03 13% 0.07%
The terms shown are the 10 cellular component terms with the lowest p-value, obtained in the profiling of SCD patients. For
each term is indicated: GO accession number (Acc), term name, p-value without multiple-testing correction, p-value with
Bonferroni correction (p-Bonf), annotation frequency in the study set (SFreq) and annotation frequency in the population
set (PFreq).
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 8 of 17
http://www.jbiomedsem.com/content/4/1/21
Table 6 Enriched terms in the profiling analysis of no-SCD patients, not identified in the
SCD patients
Acc Name p-value p-Bonf SFreq PFreq
Biological process
GO:0001980 Regulation of systemic arterial blood 6.0E-41 4.4E-39 13% 0.00
pressure by ischemic conditions
GO:0001976 Neurological system process involved in regulation 3.4E-25 2.5E-23 13% 0.00
of systemic arterial blood pressure
GO:0006940 Regulation of smooth muscle contraction 6.6E-19 4.9E-17 13% 0.00
GO:0007522 Visceral muscle development 5.3E-03 3.9E-01 1% 0.00
GO:0042694 Muscle cell fate specification 5.3E-03 3.9E-01 1% 0.00
GO:0055009 Atrial cardiac muscle tissue morphogenesis 2.6E-02 1.9E+00 1% 0.00
GO:0003228 Atrial cardiac muscle tissue development 2.6E-02 1.9E+00 1% 0.00
GO:0048739 Cardiac muscle fiber development 4.2E-02 3.1E+00 1% 0.00
GO:0042693 Muscle cell fate commitment 5.7E-02 4.2E+00 1% 0.00
Molecular function
GO:0031014 Troponin T binding 9.9E-33 7.3E-31 13% 0.00
GO:0019855 Calcium channel inhibitor activity 1.9E-31 1.4E-29 13% 0.00
GO:0008200 Ion channel inhibitor activity 1.4E-23 1.0E-21 13% 0.00
GO:0016248 Channel inhibitor activity 1.4E-23 1.0E-21 13% 0.00
GO:0005246 Calcium channel regulator activity 4.9E-23 3.6E-21 13% 0.00
GO:0048306 Calcium-dependent protein binding 1.1E-19 8.1E-18 13% 0.00
GO:0042805 Actinin binding 5.6E-06 4.2E-04 4% 0.00
GO:0030899 Calcium-dependent ATPase activity 1.6E-02 1 1% 0.00
GO:0003785 Actin monomer binding 7.2E-02 1 1% 0.00
The terms shown are the biological process and molecular function terms identified as enriched in the profiling analysis of
no-SCD patients that were not identified in the profiling of SCD patients. For each term is indicated: GO accession number
(Acc), term name, p-value without multiple-testing correction, p-value with Bonferroni correction (p-Bonf), annotation
frequency in the study set (SFreq) and annotation frequency in the population set (PFreq).
the term is not particularly informative in respect to HCM, limits the confidence with
which this term can be used for that purpose.
Study limitations and future work
The results obtained can be explained by the following factors: a) the genetic data can be
insufficient to prognosticate the occurrence of SCD; b) the genetic data may not be fully
explored; and c) the dataset used may not be the most appropriate to test the method-
ology. The first two factors are related to the HCM case-study. Firstly, and as already
referred, the occurrence of SCD is currently not predictable solely based on genetic data,
and thus an enrichment analysis has to be performed considering the clinical data and
clinical controlled vocabularies before a final evaluation of the methodology can be made.
This analysis will be very important to understand if the inclusion of clinical data is really
imperative. Since the HCM dataset is already mapped to clinical vocabularies (NCIt and
SNOMED-CT), these vocabularies will be tested next. Secondly, in this initial test we only
considered the existence or absence of mutations in the genes, but the type and number of
mutations can also be a useful source of information. For example, it is known that some
mutations are associated with a benign outcome (i.e. no occurrence of SCD) whereas oth-
ers with a malignant outcome. It has also been reported that the occurrence of mutations
in some genes is associated with a higher incidence of SCD than in others [19]. All of these
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 9 of 17
http://www.jbiomedsem.com/content/4/1/21
aspects can be taken into consideration when calculating the frequencies of annotation or
even be added as features to the dataset. It is important to notice, however, that we are not
concerned with pleiotropic effects. It is known that some HCM mutations have different
phenotypic manifestations in different patients, and different manifestations should also
be expected if a patient has multiple mutations. Nevertheless, the goal of this analysis is to
obtain profiles that provide a global characterization of the patients in respect to an event,
and not to perform a precise evaluation of each patient in terms of his mutations. Regard-
ing the genetic enrichment analysis, that global characterization should be in terms of the
functions and processes most frequently affected in the event-positive patients. Finally,
it cannot be overlooked the possibility that to evaluate the true potential of the progno-
sis methodology we might need to test it with other datasets. This is due to the reduced
number of patients in the dataset tested, in particular of SCD patients.
In respect to the methodology itself, there is also one factor that needs to be taken into
account when interpreting the results, that is how the missing values were dealt with. In
terms of the genetic features, missing values are mutations associated with HCM that
were not tested. Due to the sparseness of the dataset, it was not feasible to simply elim-
inate the mutations not tested or the patients with mutations not tested. Consequently,
we considered these mutations as having a negative value, i.e. that they were not present
in the patient. This approach allows us to exploit all the available data and to obtain an
informative characterization of the patients. It is important to stress out that an evalua-
tion of all the patients for all the mutations is almost never done. On the one hand, more
mutations tested might result in an increase in the number of genes analyzed, possibly
leading to an increment in the number of terms tested and, consequently, in the terms
found enriched. On the other hand, it might result in an increase in the frequency of
annotation of the terms in the study set of the enrichment profiling analysis, and in both
the study and the population sets in the differential enrichment analysis. In the profiling
analysis, this increase would result in the strengthening of the confidence in the results
since we would increase the difference of annotation frequency between the study set and
the population set. In the differential analysis, the results might be more strongly altered,
since both sets of annotation frequencies would have to be recalculated.
Another relevant factor in the methodology is that a Singular Enrichment Analysis
does not take into account the existence of relations between the genes. Since a Modular
Enrichment Analysis addresses this issue, we will also test this approach.
By applying a methodology that relies in controlled vocabularies we may have to work
with incomplete annotations, as well as with a set of ontology terms that might not
provide the level of detail necessary to fully characterize the patients. In respect to the
possibility of incomplete annotations, we tried to deal with it by considering all types of
annotation, including inferred from electronic annotation, even with the risk of introduc-
ing some annotation errors. In respect to the possibility of an insufficient level of detail,
it can be overcome by considering more than one vocabulary for the same domain of
knowledge, which we will do when analyzing the clinical features.
The enrichment analysis with the clinical data was not yet performed due to the dif-
ficulty of defining a population set annotated with terms from clinical vocabularies, as
discussed by LePendu et al. [6]. In our analysis, this problem presents itself when con-
sidering the implementation of the equivalent of the profiling analysis, in which a group
of patients would be characterized in terms of their phenotype, based on their clinical
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 10 of 17
http://www.jbiomedsem.com/content/4/1/21
information. Considering the approach suggested by the same authors, and that we are
currently implementing, the problem can be overcome by exploiting the identification of
gene-clinical vocabulary annotations in the PubMed articles that originated the gene-GO
annotations. Although the equivalent of the differential enrichment analysis can be more
readily implemented, its application with the clinical data presents one of the same lim-
itations found with the genetic data, i.e the existence of a great overlap of annotations
between the SCD and no-SCD patients.
Conclusions
In this article we presented the application of enrichment analysis in a prognosis method-
ology. The goal of the enrichment analysis was to identify a set of features that might
assist in the differentiation of patients for whom a disease-specific event occurred from
the patients for whom it did not.
The application of the enrichment analysis was tested with genetic data from patients
with the disease hypertrophic cardiomyopathy (HCM), and using the GeneOntology. The
event under analysis was the occurrence of sudden cardiac death (SCD), which is themost
severe manifestation of HCM.
The implementation of the analysis was adapted to the fact that we were not studying a
single set of genes, but rather several, one from each patient. This adaptation was tested in
two enrichment analysis: an enrichment profiling comparing the genes mutated in SCD
(or no-SCD) patients with all protein-coding genes in the same group of patients; and a
differential enrichment comparing the genes mutated in SCD (or no-SCD) patients with
the genes mutated in all HCM patients.
Overall, the results obtained indicate that the enrichment profiling analysis is useful
for the characterization of patients, as it allowed the identification of meaningful terms
associated with HCM. Notwithstanding, a full evaluation of its potential for prognosis
purposes requires that some aspects are taken into consideration. One of such aspects
is the fact that in this first implementation, only genetic data was analyzed for enrich-
ment. However, the disease used as case-study cannot be prognosticated solely based on
this data, and thus the enrichment analysis has to be performed in both the genetic and
the clinical domains. Another aspect is that more information might be extracted from
the genetic data in addition to the number of genes mutated, namely the type of muta-
tions (i.e. if they are benign or malignant in respect to the event analyzed) or even the
number of mutations per gene. Finally, the methodology itself might need to be tested in
other datasets, due to the characteristics of the one used, such as the reduced number of
patients and the high number of missing values.
Methods
Singular enrichment analysis
The enrichment analysis approach most commonly used is the Singular Enrichment
Analysis (SEA). The statistic test underlying this approach is normally the Fishers exact
test, and the distribution considered when working with small datasets is the hyper-
geometric distribution. This distribution is applied to situations of sampling without
replacement from a finite population when considering that the population elements are
in one of two possible states. Translating this to the enrichment analysis, the goal is to
evaluate if the genes in the population set are annotated with a term t, which means that
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 11 of 17
http://www.jbiomedsem.com/content/4/1/21
the two possible states for a gene are: being annotated with the term, and not being anno-
tated with the term. When drawing a sample of genes from the population (thus forming
the study set), the objective is then to evaluate if the probability of annotation with term
t is higher in this sample than would be expected by chance. The expected frequency of
annotation is given by the knowledge of the population set, and if the frequency of anno-
tation in the sample is higher than in the population, then term t might be used to explain
the study set. In this type of analysis, what is being calculated is the probability of observ-
ing at least n genes in the study set annotated with term t, given the knowledge of: the size
of the study set, the size of the population set, and the number of genes in the population
set annotated with t [2]. For a term to be considered enriched in the study set, the p-value
obtained from the Fishers test has to be lower than a significance level, which is normally
considered to be 0.05 or 0.1.
The terms tested in this manner are not only those that directly annotate the genes, but
also their ancestors. Given the high number of tests that are performed with resources
such as the Gene Ontology (with more than 38,000 terms on January, 2013), a multiple-
testing correction is necessary to reduce the possibility of false-positive results. The most
conservative multiple-testing correction is the Bonferroni correction, which is obtained
simply by multiplying the calculated p-value by the number of tests performed.
Enrichment analysis: from genes to patients
The purpose of the methods here described is to analyze patients suffering from a given
disease in respect to their predisposition to suffer a disease-related event. The disease
and the event are characterizable with clinical and genetic data, and this data is analyzed
in terms of ontology terms enrichment. The clinical data includes features such as symp-
toms and measurements, whereas the genetic data refers to the presence or absence of
mutations.
The methods with which we perform the enrichment analysis with the genetic data
have been adapted from the standard implementation used in existing enrichment anal-
ysis tools. The following two are the main differences between the standard enrichment
analysis and the one described here. In the standard analysis:
 Only one set of genes is analyzed, such as the genome of an organism. In our analysis,
several sets of genes are taken into consideration, exactly one for each patient.
 The frequency of annotation of a term is given by the number of genes annotated
with that term. In our analysis, the frequency of annotation is given by the number of
mutated genes annotated with the term.
Enrichment profiling
The purpose of this analysis is to characterize the genotype of a group of patients (e.g. the
patients positive for a disease-related event), based on the set of mutations the patients
have. Since the knowledge of these mutations is normally not available for the complete
genome of a patient but only for a set of genes associated with the disease under analysis,
the characterization is performed by comparing the information of the genes mutated in
the patients with the complete set of genes in the genome.
Given a group of patients, for each of which is known his/her set of mutations, and
the set of Human protein-coding genes, the enrichment profiling analysis is performed as
follows:
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 12 of 17
http://www.jbiomedsem.com/content/4/1/21
1. Define the population set as the union of the genes in the genome of all the patients.
2. Define the study set as the union of the genes mutated in all the patients.
3. Find all GO terms annotating at least one gene mutated in the patients.
4. Calculate the population set frequency of annotation (PFreq) of term t as follows:
PFreq(t) =
n?
1
count(gene(t))
where n is the total number of patients, and gene(t) is a gene annotated with t (see
Figure 2).
5. Calculate the study set frequency of annotation (SFreq) of term t as follows:
SFreq(t) =
n?
1
count(mut_gene(t))
where mut_gene(t) is a mutated gene annotated with t.
6. Apply Fishers exact test to calculate the probability of enrichment of term t.
7. Perform a multiple-testing correction (e.g. Bonferroni) over the p-values obtained.
8. Consider term t as enriched in the study set if p-value(t) < ? (e.g. 0.05 or 0.1).
Differential enrichment
The purpose of this analysis is to identify differentiating features between a group of
patients with a particular characteristic, for example being positive for a disease-related
event, and all the patients with the disease. This analysis is also based in the set of muta-
tions the patients have, considering the mutations in the study group vs. the mutations in
all the patients.
The implementation of this analysis is very similar to that of the enrichment profiling,
with the differences presented below.
Given a group of patients with a disease, a sub-group of those patients with a study
characteristic, and the set of mutations in each group:
1. Define the population set as the union of the genes mutated in the group of
patients with the disease.
2. Define the study set as the union of the genes mutated in the sub-group of patients
with the study characteristic.
3. Find all GO terms annotating at least one genemutated in the sub-group of patients.
4. Calculate the population set frequency of annotation (PFreq) of term t as follows:
PFreq(t) =
n?
1
count(mut_gene(t))
where n is the total number of patients with the disease, and mut_gene(t) is a
mutated gene annotated with t.
5. Calculate the study set frequency of annotation (SFreq) of term t as follows:
SFreq(t) =
n?
1
count(mut_gene(t))
where n is the number of patients in the sub-group with the study characteristic.
6. to 8. Do as in the enrichment profiling.
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 13 of 17
http://www.jbiomedsem.com/content/4/1/21
Genetic enrichment analysis applied to HCM
HCM dataset
The HCM dataset is composed by clinical and genetic features characterizing 83 patients,
which was previously collected from Portuguese hospitals andmolecular biology research
laboratories. From these 83 patients, 14 are positive for SCD and the remaining 69 are
negative for SCD. Table 7 shows the complete list of clinical features, and Table 8 the
percentage of patients with known values for those features. From the total set of clini-
cal features, the following three were used to define which patients are positive for SCD:
sudden death, resuscitated sudden death, and cardioverter defibrillator. The first two indi-
cate if the patient suffered a sudden cardiac death, either resuscitated or not, whereas the
third indicates if the patient has an implanted cardioverter defibrillator. This device pre-
vents the occurrence of SCD by delivering an electric charge when cardiac arrhythmia is
detected, and it is implanted after a resuscitated sudden death occurred or when there is
a very high risk of SCD occurrence. Patients are then considered positive for SCD if they
are positive for at least one of the three features. Considering the three features instead of
just two resulted in an increase of 4 SCD positive patients.
The genetic features are the mutations associated with the disease, in a total of 569, and
are represented as Boolean variables. From this set of mutations, only 78 were found in
Table 7 Features used for the clinical characterization of the HCM patients
Clinical feature Feature value SCD (%) no-SCD (%)
Sudden death (SD)
True 5 (36) 0
False 9 (64) 69 (100)
Resuscitated SD
True 3 (21) 0
False 8 (57) 69 (100)
Cardioverter defibrillator
True 9 (64) 0
False 2 (14) 69 (100)
Non-sudden death
True 0 0
False 14 (100) 69 (100)
Obstructive HCM
True 4 (29) 8 (12)
False 1 (7) 17 (25)
Non-obstructive HCM
True 1 (7) 17 (25)
False 4 (29) 8 (12)
SD family history
True 3 (21) 1 (1)
False 2 (14) 25 (36)
HCM form
Familial 9 (64) 32 (46)
Sporadic 2 (14) 37 (54)
Blood pressure
Normal 4 (29) 22 (32)
Hypotension 0 1 (1)
Hypertension 0 5 (7)
Gender
Male 6 (43) 41 (59)
Female 5 (36) 25 (36)
Age
[0,20] 0 5 (7)
]20,40] 2 (14) 11 (16)
]40,60] 3 (21) 15 (22)
> 60 3 (21) 10 (14)
For each feature are indicated its possible values, the number of SCD and no-SCD patients that have them, and the
respective percentages. The total number of SCD patients is 14, whereas of no-SCD is 69. (See Table 8 for the percentage of
patients with known values for each of the features).
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 14 of 17
http://www.jbiomedsem.com/content/4/1/21
Table 8 Percentage of SCD and no-SCD patients that have a known value for each clinical
feature
Clinical feature SCD no-SCD
Sudden death (SD) 100 100
Resuscitated SD 79 100
Cardioverter defibrillator 79 100
Non-sudden death 100 100
Obstructive HCM 36 36
Non-obstructive HCM 36 36
SD family history 36 38
HCM form 79 100
Blood pressure 29 41
Gender 79 96
Age 57 61
the HCM patients. These 78 mutations occur in 7 distinct genes (shown in Table 9), all of
which are mutated in at least one patient without SCD (no-SCD). In the case of the SCD
patients, only 4 of those 7 genes are mutated in at least one of the patients: MYBPC3,
MYH7, CSRP3, and TNNT2. The number of mutations identified per patient ranges from
1 to 5, with an average value of 1.8.
The genotyping of the patients was done in two manners: with a microarray able to
detect 508 mutations associated with HCM, and a technique called high-resolution melt-
ing analysis (HRM) [20] followed by sequencing. The HRM analysis was used to analyze
individual exons to indentify the presence of mutations, whereas the sequencing allows
the identification of the exact mutation. Some of the patients were analyzed with both
techniques, whereas others with only one of the techniques. HRM can be used to test
for mutations not present in the microarray and/or to confirm the results obtained with
the microarray. One of the reasons to use HRM instead of the microarray is that when
patients are tested after a family member was diagnosed, only the mutations found in this
one are searched for. Additionaly, the identification of only one mutation is sufficient for
a positive diagnosis, and the overall process is cheaper.
GO annotations
The set of genes in the Human genome was obtained from the GeneCards Database [21],
the set of terms from the Gene Ontology [1] and the set of GO annotations from the GOA
Table 9 Genes used for the genetic characterization of the HCM patients
Gene SCD no-SCD GO annotations
MYBPC3 4 25 202
MYH7 9 36 192
CSRP3 1 4 138
TNNT2 2 20 178
TNNI3 0 13 173
MYL2 0 1 133
MYH6 0 1 251
For each gene in indicated the number of SCD and no-SCD patients with at least one mutation in it, as well as the number of
Gene Ontology (GO) annotations. The total number of SCD patients is 14, whereas of no-SCD is 69.
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 15 of 17
http://www.jbiomedsem.com/content/4/1/21
database [22], as of the releases of October 4th, 2012. From the total set of Human protein-
coding genes, only 18,759 were annotated with GO terms. All types of GO annotations
were considered, including inferred from electronic annotation.
The enrichment analysis was performed for the three types of GO terms: biological
process, molecular function, and cellular component. In order to filter out uninformative
GO terms, we considered only terms with information content (IC) above 60%. The IC of
a term t is given by the expression [23]:
IC(t) = ? log2
f (t)
f (root)
where f(t) is the annotation frequency of the term (i.e. the number of distinct gene prod-
ucts it annotates) and f(root) is the frequency of annotation of the root term of the GO
(which corresponds to the total number of annotated gene products). In this work, we
used the annotations to Human genes to compute the IC, including annotations with all
evidence codes. In order to obtain a normalized IC, we divided the IC values by the scale
maximum (log2 f (root)).
Enrichment analyses
Four enrichment experiments were performed, two enrichment profiling analysis and
two differential enrichment analysis, as presently described in accordance with the steps
previously indicated for each analysis. Enrichment profiling for the group of SCD patients:
1. Population set: 18,759 genes × 14 patients
2. Study set: 16 mutated genes, corresponding to 4 distinct genes (MYBPC3, MYH7,
CSRP3, and TNNT2)
3. GO terms obtained for the previous 4 genes
4. PFreq: number of genes annotated with t in the 14 patients
5. SFreq: number of mutated genes annotated with t in the 14 patients
Enrichment profiling for the group of no-SCD patients:
1. Population set: 18,759 genes × 69 patients
2. Study set: 100 mutated genes, corresponding to 7 distinct genes (MYBPC3, MYH7,
CSRP3, TNNT2, TNNI3, MYL2 and MYH6)
3. GO terms obtained for the previous 7 genes
4. PFreq: number of genes annotated with t in the 69 patients
5. SFreq: number of mutated genes annotated with t in the 69 patients
Differential enrichment for the HCM patients and the sub-group of SCD patients:
1. Population set: 116 mutated genes, corresponding to the 7 distinct genes mutated
in the 83 patients (MYBPC3, MYH7, CSRP3, TNNT2, TNNI3, MYL2 and MYH6)
2. Study set: 16 mutated genes, corresponding to the 4 distinct genes mutated in the
14 SCD patients (MYBPC3, MYH7, CSRP3, and TNNT2)
3. GO terms obtained for the 7 genes
4. PFreq: number of mutated genes annotated with t in the 83 patients
5. SFreq: number of mutated genes annotated with t in the 14 SCD patients
Machado et al. Journal of Biomedical Semantics 2013, 4:21 Page 16 of 17
http://www.jbiomedsem.com/content/4/1/21
Differential enrichment for the HCM patients and the sub-group of no-SCD patients:
1. Population set: 116 mutated genes, corresponding to the 7 distinct genes mutated
in the 83 patients
2. Study set: 100 mutated genes, corresponding to the 7 distinct genes mutated in the
69 no-SCD patients
3. GO terms obtained for the 7 genes
4. PFreq: number of mutated genes annotated with t in the 83 patients
5. SFreq: number of mutated genes annotated with t in the 69 no-SCD patients
In all the analyses a Bonferroni correction was performed, and 0.1 was the confidence
level considered.
Additional files
Additional file 1: Full results of the enrichment profiling for SCD patients. For each term is indicated: GO
accession number (GO acc), term name, p-value without multiple-testing correction (noCorr), p-value with Bonferroni
correction, annotation frequency in the study set, annotation frequency in the population set, and the information
content (IC).
Additional file 2: Full results of the enrichment profiling for no-SCD patients. For each term is indicated: GO
accession number (GO acc), term name, p-value without multiple-testing correction (noCorr), p-value with Bonferroni
correction, annotation frequency in the study set, annotation frequency in the population set, and the information
content (IC).
Additional file 3: Full results of the two differential enrichments: HCM patients and the sub-group of SCD
patients; HCM patients and the sub-group of no-SCD patients. For each term is indicated: GO accession number
(GO acc), term name, p-value without multiple-testing correction (noCorr), p-value with Bonferroni correction,
annotation frequency in the study set, annotation frequency in the population set, and the information content (IC).
Competing interests
The authors declare that they have no competing interests.
Authors contributions
CMM conceived and implemented the methodology, and drafted the manuscript. ATF and FMC participated in the
conception of the methodology, in the draft and revision of the manuscript. All authors read and approved the final
manuscript.
Acknowledgements
This work was supported by the FCT through the Multi-annual Funding Program, the doctoral grant
SFRH/BD/65257/2009, the projects SOMER (PTDC/EIA-EIA/119119/2010) and PEst-OE/EEI/LA0021/2011.
The authors would like to thank to Alexandra R. Fernandes, Susana Santos and Dr. Nuno Cardim for collecting and
providing the dataset, and to Daniel Faria for his participation in discussions related with the methodology and the
analysis of the results.
Received: 22 January 2013 Accepted: 18 April 2013
Published: 8 October 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Hanna et al. Journal of Biomedical Semantics 2013, 4:44
http://www.jbiomedsem.com/content/4/1/44RESEARCH Open AccessBuilding a drug ontology based on RxNorm and
other sources
Josh Hanna*, Eric Joseph, Mathias Brochhausen and William R HoganAbstract
Background: We built the Drug Ontology (DrOn) because we required correct and consistent drug information in
a format for use in semantic web applications, and no existing resource met this requirement or could be altered
to meet it. One of the obstacles we faced when creating DrOn was the difficulty in reusing drug information from
existing sources. The primary external source we have used at this stage in DrOns development is RxNorm, a
standard drug terminology curated by the National Library of Medicine (NLM). To build DrOn, we (1) mined data
from historical releases of RxNorm and (2) mapped many RxNorm entities to Chemical Entities of Biological Interest
(ChEBI) classes, pulling relevant information from ChEBI while doing so.
Results: We built DrOn in a modular fashion to facilitate simpler extension and development of the ontology and
to allow reasoning and construction to scale. Classes derived from each source are serialized in separate modules.
For example, the classes in DrOn that are programmatically derived from RxNorm are stored in a separate module
and subsumed by classes in a manually-curated, realist, upper-level module of DrOn with terms such as clinical
drug role, tablet, capsule, etc.
Conclusions: DrOn is a modular, extensible ontology of drug products, their ingredients, and their biological
activity that avoids many of the fundamental flaws found in other, similar artifacts and meets the requirements of
our comparative-effectiveness research use-case.
Keywords: Drug Ontology, RxNorm, ChEBIBackground
Several researchers have identified use cases for an
ontology of drugs, such as comparative effectiveness re-
search [1], clinical decision support [2-4], and clinical
data warehousing and data integration [2,3,5-7], among
others. We previously analyzed existing terminology and
ontology artifacts that represent some aspect of drugs,
and found that no existing resource was sufficient for
our use cases in these domains [8]. Our requirements in-
cluded (1) a historically comprehensive list of NDCs, (2)
correctness with respect to pharmacology and biomed-
ical science, (3) logically consistent, correct axioms that
do not entail untrue or inconsistent inferences, and (4)
interoperability with other ontologies used in transla-
tional science. Specifically, we analyzed RxNorm, the
National Drug File  Reference Terminology, SNOMED
CT, Chemical Entities of Biological Interest (ChEBI), an* Correspondence: jhanna@uams.edu
Division of Biomedical Informatics, University of Akransas for Medical
Sciences, Little Rock, AR, USA
© 2013 Hanna et al.; licensee BioMed Central
Commons Attribution License (http://creativec
reproduction in any medium, provided the orOWL conversion of the Anatomical and Therapeutic
Chemical classification system, DrugBank, PharmGKB,
and other sources and found that none of them met
these requirements. Minimally, no existing resource con-
tains in its current version a historically comprehensive
list of National Drug Codes (NDCs). We also found
problems with scientific correctness and unintended and
incorrect description logic entailments from artifacts
represented in Web Ontology Language. A key flaw in-
herent in several artifacts that frequently led to scientif-
ically incorrect representations of drugs was assigning
the properties of drug products such as tablets, creams,
ointments, etc. to molecules and vice versa.
We therefore decided to build the Drug Ontology
(DrOn) to meet our requirements. However, rather than
assemble comprehensive historical information about
drug products from scratch, we decided to begin with
resources that (1) had enough quality information about
drugs to begin with, (2) could therefore be restructuredLtd. This is an open access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly cited.
Hanna et al. Journal of Biomedical Semantics 2013, 4:44 Page 2 of 9
http://www.jbiomedsem.com/content/4/1/44in a coherent manner, and (3) were sufficiently open to
allow us to make DrOn publicly available.
RxNorm [5]a standard drug terminology maintained
by the U.S. National Library of Medicine (NLM)in-
cludes normalized names and relationships extracted
from several proprietary drug knowledge bases. Because
RxNorm (1) contains a large amount of drug informa-
tion, (2) is freely available, and (3) has a great deal of
content available under a permissive license that allows
derivative works, it is a good candidate for a source of
information to create a formal drug ontology. In particu-
lar, RxNorm had enough information to allow us to rect-
ify numerous shortcomings of existing artifacts, and its
historical versions collectively contain the largest, openly
available set of historical NDCs, dating back until June
2008 at least (the date of the first version of RxNorm to
contain NDCs).
RxNorm is focused primarily on prescription and
over-the-counter drugs that are currently available in the
United States. It uses Concept Unique Identifiers called
RXCUIs to catalog and relate information.
At this stage of DrOn development, we are interested in
the ability to query for historical NDCs to study pharmacy
claims databases that contain a decade or more of data
(and thus NDCs in use 10 years ago as well as today are re-
quired). The NDC is a unique identifier that the Drug List-
ing Act of 1972 requires companies to report to the Food
and Drug Administration (FDA). RxNorm associates each
NDC with a drug product via the RXCUI. Although our re-
quirement is to have a comprehensive, historical list of
NDCs, RxNorm maintains only currently active NDCs in
its current release. So tracking all NDCs and the RXCUIs
with which they have been associated over historical re-
leases of RxNorm is key to building DrOn, and represents a
key contribution of the present work.
Moreover, NDCs are often lost with no explanation when
an RXCUI is retired, especially in releases of RxNorm prior
to 2009. This situation necessitates careful tracking to en-
sure that all valid NDCs (and, indeed, any useful informa-
tion) associated with a retired RXCUI can be associated
with the most recent RXCUI that refers to the same entity.
In this paper, we describe how we build DrOn from his-
torical releases of RxNorm while navigating these pitfalls.
In addition, during the build process, we map drug ingredi-
ents from RxNorm to the Chemical Entities of Biological
Interest (ChEBI) ontology [9]. For example, we map the
RXCUI for furosemide (4603) to the ChEBI identifier
for furosemide: http://purl.obolibrary.org/obo/CHEBI_47426.
As a result, we import hundreds of ChEBI classes and their
associated URIs, labels, etc. into DrOn.
Methods
The overall workflow of the extraction and translation
process has three main steps:1. Extracting relevant data from RxNorm, including
information found only in older releases.
2. Transforming and loading this data into a
normalized Relational Database Management System
(RDBMS).
3. Translating the normalized RDBMS into an OWL
2.0 artifact.
Each of these three steps is further subdivided into
substeps that we explain in detail below.Extracting information from RxNorm
We first download the raw RxNorm files directly from the
NLM website, specifically the UMLS (or Unified Medical
Language System) Terminology Services (UTS) site [10]
and import them into a locally hosted RDBMS using the
scripts provided by the NLM. Additionally, to support
maintenance of comprehensive information over time, we
created and maintain two additional tables that store all the
information that we extract from each release of RxNorm
(a subset of all the information). We describe these tables
in detail below (sections entitled Extraction of National
Drug Codes (NDCs) and related RXCUIs and Tracking
Provenance).
Currently, we include in DrOn information from every
version of RxNorm released between June, 2008 and Febru-
ary, 2013. The release from June, 2008 marks the first time
that RxNorm-curated NDCs were included.
It should be noted that we use only information curated
within RxNorm and not any information from its sources
directly, and thus our overall process is allowable under the
UMLS license (all content reused in DrOn is marked Level
0, which, per the license, does not prohibit derivative works
as do levels 1 and higher).RxNorm files
The next step is to extract all relevant information from
the files downloaded from the UTS site. RxNorm comes
as a set of nine Rich Release Format (RRF) files, each of
which contains a specific subset of the total information.
However, we process only five of these files in our build
process.
Specifically, we process RXNSAT.RRF, RXNCONSO.RRF,
RXNCUI.RRF, RXNCUICHANGES.RRF, and RXNSAB.
RRF. Table 1 shows the information we extract from each
file.
There are four different term types in RXNCUI.RRF
that are relevant to DrOn. They are: (1) Semantic Clinical
Drug Forms (SCDFs), (2) Semantic Clinical Drugs (SCDs),
(3) Semantic Branded Drugs (SBDs), and (4) Ingredients
(IN). RxNorm treats NDCs as attributes of an SCD or SBD
rather than a separate term type.
Table 1 The RxNorm files and the information extracted
from each
File Extracted information
RXNSAT.RRF NDCs and RXCUIs
RXNCONSO.RRF SCDFs, SCDs, SBDs, and INs and their RXCUIs
RXNCUI.RRF retired RXCUIs with provenance
RXNCUICHANGES.RRF RXCUI provenance
RXNSAB.RRF RxNorm version information
Hanna et al. Journal of Biomedical Semantics 2013, 4:44 Page 3 of 9
http://www.jbiomedsem.com/content/4/1/44RXCUI provenance
Tracking entities within RxNorm requires tracking the
RXCUIs to which they are attached. This can be a diffi-
cult task. Any RXCUIs that have been entered in error
are retired. Additionally, if two RXCUIs refer to the
same entity, they are consolidated and either (1) one of
them is retired while the other remains or (2) a new
RXCUI is created and both older RXCUIs are retired
(Figure 1). Prior to the April 2009 release of RxNorm,
no comprehensive list of retired RXCUIs was included
in RxNorm. The reasons for retirement are not always
well-documented, making it difficult to distinguish be-
tween RXCUIs that have been retired because they are
nonsense and ones that have been replaced or merged.
For instance, as of this writing, there are 40 RXCUIs
with 210 associated NDCs that are no longer contained
in the most recent release of RxNorm, however, there is
no record of why these RXCUIs were removed.
Extraction of National Drug Codes (NDCs) and related
RXCUIs
To facilitate the tracking of NDCs, we have created an
additional table, NDC_COMP, that contains a compre-
hensive list of all RxNorm-curated NDCs from all re-
leases of RxNorm since June 2008 (when they first
appeared) and their corresponding RXCUIs. To generate
this table, we parse the RXNSAT.RRF file in each release
of RxNorm. Any entry in the file whose source is
RxNorm and is annotated as being an NDC is extractedFigure 1 How RXCUIs are de-duplicated. How the National Library of Mfrom the file, along with its associated RXCUI, and
imported into our NDC_COMP table. We also store the
version from which each NDC was mined, which is
parsed from the RXNSAB.RRF file.
Tracking provenance
The second of the two additional tables is a master con-
version table, DEPRECATED_RXCUIS, which we use to
track the current status of each retired RXCUI. This
table contains two fields: old_rxcui and new_rxcui. The
old_rxcui field contains a retired RXCUI, and the new_-
rxcui field contains the current RXCUI to which the re-
tired RXCUIs information is now associated. The
new_rxcui field may also contain a status code if the
retired RXCUIs information is unable to be tracked
because it was entered in error or split into multiple
new RXCUIs. These special status codes are ERROR
for RXCUIs that have been entered in error and
S_RXNCUI for RXCUIs which have been split. Be-
cause RxNorm does not document why an erroneous
RXCUI was entered in error, we are unable to do further
processing on them or their associated information. For
the RXCUIs which are split, it may be possible to track
some of their associated information, but it is not always
clear which information belongs to which child RXCUI
and this issue requires manual intervention at present.
Our DEPRECATED_RXCUIS table is updated with
each release of RxNorm through the following proced-
ure (Figure 2):
1. First, we extract any RXCUIs from the
comprehensive NDC_COMP table, built as
described above (section entitled Extraction of
National Drug Codes (NDCs) and related RXCUIs),
that can no longer be found in the RXNCONSO.
RRF file being imported. We then import these
RXCUIs into the old_rxcui column of our
DEPRECATED_RXCUIS table. Because the
RXNCONSO.RRF file contains all current RXCUIs,edicine handles RXCUI errors within RxNorm.
Figure 2 Workflow for updating the DEPRECATED_RXCUIS table. How we tracked RXCUI provenance.
Hanna et al. Journal of Biomedical Semantics 2013, 4:44 Page 4 of 9
http://www.jbiomedsem.com/content/4/1/44any RXCUIs that meet the above criteria must have
been retired.
2. Next, using the RxNorm-curated RXNCUI table, we
update all entries in the new_rxcui column. The
RXNCUI table contains a cui1 field containing a re-
tired RXCUI, a cui2 field containing the RXCUI into
which the retired RXCUIs information has been
merged, and a cardinality column contains the num-
ber of RXCUIs into which the information has been
merged. Any RXCUI that has been entered in error
is indicated by an entry in which the value of the
cui1 field is equal to the value of the cui2 field. Add-
itionally, any entry with a cardinality greater than 1
indicates that the RXCUI has been split. These are
indicated in our table by setting the new_rxcui entry
to ERROR and S_RXNCUI, respectively. As of
this writing, 768 RXCUIs and 3,484 associated
NDCs are reported by RxNorm to have been en-
tered in error and are therefore not included in
DrOn. Additionally, 187 RXCUIs and 3,126 associ-
ated NDCs have been split. Both these RXCUIs and
NDCs have also been left out of DrOn (for the time
being) due to the difficulty of determining which in-
formation from the parent RXCUI belongs to which
child RXCUI.
3. Finally, we compute the transitive closure,
associating each RXCUI with the latest RXCUI that
refers to the same entity with no intervening steps
in our DEPRECATED_RXCUIS table. Because this
table is updated with each release of RxNorm,
occasionally an RXCUI in the new_rxcui field is
retired. In such situations, the new_rxcui field is
updated as described in Step 2, and a new row in
the table is created with the newly-retired RXCUI
set as the old_rxcui, and the new_rxcui field is set to
match the updated new_rxcui from the original
entry.
Mapping to ChEBI
The process maps ingredients (IN entity type) extracted
from RxNorm to ChEBI Uniform Resource Identifiers
(URIs) where possible. We accomplish this step through
a simple Java console application (that we built) thatcompares the labels of ingredients pulled from RxNorm
with class annotations in ChEBI. We assumed that any
exact matches between the names or synonyms of
RxNorm IN entities and ChEBI annotations meant that
the RxNorm concept and ChEBI class referred to the
same entity, and thus we used the ChEBI URIs in DrOn
for the ingredient. We used three different annotation
types from ChEBI in the mapping process: rdfs:label,
related_synonym, and exact_synonym. To date, we im-
port into DrOn ~750 classes (including URI and rdfs:
label and other annotations) from ChEBI: roughly 500
matches were on rdfs:label, 250 were on related_syno-
nym, and only two were on exact_synonym. Many of the
ingredients found in RxNorm are extracts of various
plants, e.g. ginger extract, which we would not expect to
find in ChEBI. These ingredients are currently all chil-
dren of the class processed material, which we imported
from the Ontology of Biomedical Investigations (OBI).
The process originally mapped somatropin (also
known as somatotroin or human growth hormone) erro-
neously to the ChEBI role growth hormone. Once we
noticed this error, we fixed it. The ingredient is now
mapped to the Protein Ontology URI that represents the
protein molecule somatotropin.
We assigned a DrOn URI to every ingredient that was
not found in ChEBI via this process.Transforming the data into a normalized format
As noted above, there are five RxNorm term types that
we were initially interested in pulling from RxNorm, in-
cluding ingredient, clinical drug form, clinical drug,
branded drug, and national drug code (NDC). Addition-
ally, we wanted to represent a number of dispositions of
ingredients, such as the disposition of metoprolol to
bind beta-adrenergic receptors of cells. Figure 3 shows
these six entity types and the relationships between
them. Note that the entities the NDC classes represent
are not the codes themselves, but instead the packaged
drug products that the NDCs represent. Additionally,
every DrOn entity that corresponds to a RxNorm entity
is annotated with the corresponding RXCUI via an an-
notation property called has_Rxcui.
Figure 3 DrOn Entity Types. The entity types of DrOn and their relationships as stored in the normalized format.
Hanna et al. Journal of Biomedical Semantics 2013, 4:44 Page 5 of 9
http://www.jbiomedsem.com/content/4/1/44Entity types
The ingredient entities represent the types of molecules
that are present in a drug product and have an active
biological role. The URIs of ingredients, where possible,
are taken from the Chemical Entities of Biological Inter-
est (ChEBI) ontology as described above. Examples of in-
gredients include acetaminophen, sulfur, and ephedrine.
There are 7,848 unique ingredients in DrOn.
The disposition entities represent dispositions that
molecules bear (see Hogan et al. [8]) that correspond to
what is typically considered a drugs mechanism of ac-
tion. There are, as of now, six molecular dispositions in
DrOn. They are:
1. non-activating competitive beta-adrenergic receptor
binding disposition (i.e., beta-adrenergic blockade)
2. function-inhibiting hydrogen/potassium adenosine
tri-phosphatase enzyme (H+/K + ATPase) binding
disposition (i.e., proton-pump inhibition)
3. function-inhibiting L-type voltage-gated calcium
channel binding disposition (i.e., the subtype of
calcium-channel blockade found in cardiovacscular
drugs that lower blood pressure and alter heart
rhythm)
4. function-inhibiting vitamin K epoxide reductase
binding disposition (i.e., the type of Vitamin K
antagonism exhibited by warfarin)
5. function-inhibiting Na-K-Cl cotransporter 2 (NKCC2)
binding disposition (i.e., NKCC2 inhibition)
6. function-inhibiting T-type calcium channel binding
disposition (i.e., another subtype of calcium-channel
blockade, which does not have cardiovascular effects)
These six dispositions were chosen based on their bio-
logical importance and relevance to ongoing compara-
tive effectiveness research at the University of Arkansas
for Medical Sciences. There is no direct correspondence
between DrOn dispositions and RxNorm, because bydesign RxNorm lacks information about drugs mechan-
ism of action. Instead, the relationships between DrOn
dispositions and ingredients were mined from ChEBI, al-
though ChEBI treats the same realizable entities that we
represent as roles [8]. Table 2 shows the associated ChEBI
role from which the ingredient relationships for the three
dispositions were mined. Authors WRH and JH manually
curated the other three dispositions not in the table.
Function-inhibiting T-type calcium channel binding
disposition was included because we erroneously associ-
ated ethosuximide and function-inhibiting L-type voltage-
gated calcium channel binding disposition. This error
was not due to any particular oversight of ChEBI but an
artifact caused by the more specific nature of DrOns
dispositions as compared to ChEBIs more general cal-
cium channel blocker. Ethosuximide instead is the bearer
of a function-inhibiting T-type voltage-gated calcium
channel binding disposition, which does not confer any
cardiovascular activity but instead gives it a neurological,
anti-seizure activity.
The Clinical Drug Form (CDF) entities represent
types of drug products at the level of granularity of dose
form (e.g. drug tablet) and often the intended route of
administration (e.g. oral ingestion), without brand or
strength information. They correspond with SCDFs in
RxNorm. Examples of CDFs include estradiol transder-
mal patch, iodine topical solution, and menthol crystals.
There are 14,035 unique CDFs in DrOn.
The Clinical Drug (CD) entities represent drug prod-
ucts at the level of granularity of specific dosage/
strength/form information. They are related to the CDF
by an is-a relationship. For example, every aspirin 325
MG enteric coated tablet (CD) is a aspirin enteric coated
tablet (CDF). DrOn contains 34,560 CDs.
The Branded Drug (BD) entities represent brand-
name drug products with specific dosage/strength/form
information. The drug products that BDs represent are
related to the products that CDs represent by an is-a
Table 2 The ChEBI roles used to mine DrOn disposition-
ingredient relationships
DrOn disposition ChEBI role
non-activating competitive beta-adrenergic
receptor binding disposition
beta-adrenergic
antagonist
function-inhibiting hydrogen/potassium
adenosine triphosphatase enzyme (H+/K +
ATPase) binding disposition
proton pump inhibitor
function-inhibiting L-type voltage-gated calcium
channel binding disposition
calcium channel
blocker
Hanna et al. Journal of Biomedical Semantics 2013, 4:44 Page 6 of 9
http://www.jbiomedsem.com/content/4/1/44relationship (RxNorm uses a tradename of relation-
ship, but drug products are not names). There are
21,248 unique BDs in DrOn.
The National Drug Code (NDC) entities represent a
drug product and its packaging, such as a 100 tablet bot-
tle of acetaminophin 325 mg oral tablets. These entities
are distinct from entities represented by BDs or CDs, in-
stead containing some number of instances of drug
products represented by CDs/BDs, for example a 100-
tablet bottle of aspirin 325 mg tablets. There are 390,813
unique NDC entities in DrOn (Table 3).
RDBMS design
The RDBMS design representing the normalized format
of the entity types described above is simple. There are 5
core tables, one for each entity type. These are as fol-
lows: clinical_drug_form, clinical_drug, branded_drug,
ndc, ingredient, and disposition.
Additionally, there are two tables storing provenance in-
formation from RxNorm, such as the version of RxNorm in
which each RxCUI was found. These are rxcui and rxnorm.
These are completely separate from the core entity tables to
allow for incorporation of other data.
Many-to-many tables representing the relationships
between the various entities are omitted in the interest
of brevity. However, all of the relationships shown in 1
are also represented in RDBMS.
Export into RDBMS system
The export process is done in four major steps:
1. First, we initialize the rxcui and rxnorm tables.
This includes mapping every deprecated RXCUI toTable 3 The associated RxNorm entity type for each DrOn
entity except disposition
DrOn entity type RxNorm entity type
CDF SCDF
CD SCD
BD SBD
Ingredient IN
NDC SCD or SBD attributethe most recent RXCUI that identifies the same
object, either to an RXCUI from the current set or
another deprecated, but not entered in error,
RXCUI.
2. Next, we initialize the ndc table. This primarily
involves copying all the NDCs found in the
extraction process (without the duplication caused
by storing NDCs multiple times during the
process) and associating them with the relevant
RXCUI.
3. Next, we create the ingredients, CDFs, CDs, and
BDs from the associated RxNorm type. This
includes maintaining the proper relationships
between the various entities (e.g. associating the
correct ingredients with each CDF).
4. Finally, we associate each NDC with the appropriate
CD or BD. This primarily involves following the
provenance trail of RXCUIs provided in step.
Creating the OWL 2.0 Artifact
We use the OWLAPI 3.4.3 [11], Scala 2.10 [12], and
Slick 1.0.0 [13] to extract the entities from our internal
representation and transform them into an OWL
artifact. This process is subdivided into the following
steps:
1. Extract the ingredients, using ChEBI URIs where
appropriate.
2. Extract the dispositions and associate them via
the bearer_of relation to the one or more
ingredients.
3. Extract the clinical drug forms and associate them
via the has_proper_part relation to the one or more
ingredients.
4. Extract the clinical drugs and assert they are a
subclass of the appropriate clinical drug form.
5. Extract the branded drugs and assert that they are a
subclass of the appropriate clinical drug.
6. Extract the NDCs and assert that they are related to
one branded drug or one clinical drug via the
has_proper_part relation.
This ordering of the steps is deliberate. Each step de-
pends on one or more previous steps.
Since the RDBMS structure defined above represents
the entities and their relationships already, this process
is fairly straightforward.
Results and discussion
We developed an ontology, DrOn, that contains infor-
mation programmatically derived from three different
sources (RxNorm, ChEBI, and PRO) during its build
process. Because it is derived from general-purpose re-
sources, we believe DrOn can serve many use cases
Hanna et al. Journal of Biomedical Semantics 2013, 4:44 Page 7 of 9
http://www.jbiomedsem.com/content/4/1/44beyond our current ones (although this conjecture re-
quires further research). We plan on adding additional
sources in the future to maintain current information in
DrOn, with more immediate plans to include informa-
tion from Structured Product Labels [14]. As such, we
built our internal representation to maintain provenance
information of the sources separately, ensuring that we
can both track the provenance of the various entities as
the ontology develops and add new sources without ad-
versely affecting the existing ontology.
DrOn follows OBO Foundry guidelines and is cur-
rently listed on the OBO Foundry website as a candidate
ontology. In additional to the information from RxNorm
detailed above, DrOn imports BFO 1.1 and includes
terms MIREOTed from the Relationship Ontology, the
Ontology of Biomedical Investigations, and BFO 2.
DrOn contains a total of 514,268 classes as of this
writing. Of these, 2 are MIREOTed, 51 were imported
using OWLs built in mechanisms, 1,885 were taken
from ChEBI, two were taken from PRO, and the
remaining 512,328 were mined from RxNorm.
The development site and issue tracker for DrOn can
be found at https://bitbucket.org/uamsdbmi/dron. The
permanent URL for DrOn is http://purl.obolibrary.org/
obo/dron.owl. DrOn can also be found in Ontobee [15].
It can be browsed and queried at http://www.ontobee.
org/browser/index.php?o=DRON.
The upper module of DrOn
DrOn is primarily made up of classes created by extract-
ing information from other sources. However, there are
a number of classes that were defined specifically for
DrOn, which we use to give structure to the extracted
data, as well as to handle future work such as represent-
ing dose forms.
We define drug product as a material entity (1) con-
taining at least one scattered molecular aggregate as part
(the active ingredient) and (2) that is the bearer of a clin-
ical drug role. It is currently the superclass to all CDFs
(and thus, all CDs and BDs).
Additionally, we define in DrOn several different sub-
types of drug product, such as drug capsule and drug
tablet. Currently, we do not use these subclasses, but
we will eventually place all the drug products under
them to achieve a better mid-level structure.
As stated above, we imported the processed material
class from the OBI. OBI defines it as a material entity
that is created or changed during material processing. It
is the parent class of all the drug ingredients that we
could not match to a ChEBI class (and thus are not
found within the ChEBI structure). Given that most of
these ingredients are plant extracts created by some
form of processing a plant, it was the most appropriate
choice for the time being.The packaged drug product class is the superclass of
all of the classes that correspond with NDCs, and thus
one of the primary interfaces between the dron-rxnorm
module and the dron-upper module. At present,
RxNorm contains insufficient information to add struc-
ture to what is essentially a flat list of NDC classes
underneath packaged drug product. However, in the fu-
ture, we anticipate using information from Structured
Product Labels to capture things in bottles vs. boxes, for
example, and adding that level of structure underneath
packaged drug product.
Modularization
The ability to incorporate additional sources of informa-
tion has been a key requirement for the build process.
To help facilitate this ability, we developed DrOn in a
modular fashion. Currently, DrOn has five different
modules: dron-full, dron-chebi, dron-rxnorm, dron-
pro, and dron-upper. Figure 4 illustrates the relation-
ships of these modules to key classes in DrOn.
The dron-full module is simply a connector that im-
ports the other modules. It is so named on the assump-
tion that certain subsets of the modules may prove
useful enough to warrant lighter versions of the
ontology.
The dron-chebi module contains all of the annota-
tions for the ingredients mapped to ChEBI (as described
in the section entitled Mapping to ChEBI). It also con-
tains all of the ChEBI superclasses of the ingredients and
their upper level is-a structure in ChEBI.
The dron-rxnorm module contains all of the informa-
tion mined from RxNorm, which at this point of the
ontologys development, is the bulk of DrOns informa-
tion. It includes the NDCs, though we plan to split the
NDCs from the rest of the RxNorm module in future
work.
The dron-pro module includes everything imported
from the Protein Ontology (PRO). At present, it is very
small and only contains the protein and somatotropin
classes from PRO. As stated above, we imported these
classes to represent somatotropin as a drug ingredient,
which previously was erroneously mapped to a role in
ChEBI.
The dron-upper module contains the hand-curated
upper-level ontology onto which the other modules are
mapped [8].
This modularization brings two major benefits: devel-
opment simplicity and increased scalability. By creating
logical divisions and well-defined interfaces between the
modules, we can more easily maintain each module sep-
arately without significantly affecting the other modules.
Additionally, as each module grows in size, we can dis-
tribute the processing and creation of the ontologies to
different servers, making it simpler to scale the process.
Figure 4 DrOn Infrastructure. The relationship of DrOn modules to key DrOn classes.
Table 4 Several ingredients and ingredient dispositions
and the number of NCDs found associated with them in
DrOn
Ingredient or disposition Number of NDCs
Acetaminophen 19,399
Ibuprofen 5,774
function-inhibiting L-type voltage-gated calcium
channel binding disposition
9,650
function-inhibiting vitamin K epoxide reductase
binding disposition
1,893
Hanna et al. Journal of Biomedical Semantics 2013, 4:44 Page 8 of 9
http://www.jbiomedsem.com/content/4/1/44Validation
We validated the design of DrOn by building a web-
based software application that supports our primary,
driving use case. This use case was to enable
comparative-effectiveness research at the University of
Arkansas for Medical Sciences, where researchers
wish to study pharmacy claims datasets. To do so,
they need to pull all claims where the drug product
dispensed meets certain criteria. For example, author
WRH was part of a research team wherein a student
had to manually identify all drug products that con-
tain acetaminophen. We built a web application that
uses DrOn to support this use case; users can search
for all NDCs that either contain a specific ingredient
or contain an ingredient that has a specific dispos-
ition (such as beta-adrenergic receptor blockade). This
web application is accessible at http://ingarden.uams.
edu/ingredients.
Using this application, a user can find nearly 20,000
different packaged drug products that contain acet-
aminophen. Table 4 shows a number of ingredients or
ingredient dispositions along with the number of
NCDs the application finds associated with them by
querying DrOn. In previous work [8], we used this
application to test the results of a manually created
list of acetaminophen NDCs against the NDCs found
in DrOn. We found that DrOn contained every NDC
found in the manually curated list.Future work
Future work includes addressing limitations in the
current process. One of the more egregious examples is
the lack of a link from the various drug products to their
dose forms (e.g., drug capsule). Nearly all of the most
common dose forms are already in the upper level of the
ontology (dron-upper module), but the CDFs are not
properly related to them. This is due to (1) time con-
straints and (2) the dubious ontological nature of some
of the dose forms found in RxNorm. For example, in-
haler does not refer to the form of the drug but instead
to its container (which also serves the role of drug deliv-
ery device). But the form of the drug itself is a solution
or suspension contained in the inhaler. Note that the
presentation form in this case (e.g., solution) differs from
the administration form (e.g., aerosol).
Hanna et al. Journal of Biomedical Semantics 2013, 4:44 Page 9 of 9
http://www.jbiomedsem.com/content/4/1/44Another issue is the lack of a full logical definition for
some of the terms. For instance, only a small subset of
the parts of each drug product is defined. A CDF has in-
formation about its dose form, its route of administra-
tion, and its active ingredients. As of the writing of this
paper, only active ingredients are represented in the
ontology, though dose forms are mostly represented.
Even these, however, are still not fully developed, gener-
ally lacking class restrictions.
The final issue with the process is the need for manual
interaction. Although each step in the process is auto-
mated, they are not tied together in a coherent way. We
expect that some manual intervention will always be
needed as we continue to mine updated information
from these sources, but there is significant room for im-
provement in connecting the various segments of the
overall process flow and fully automating the less onto-
logically nebulous steps.
Since DrOn is already large and will likely increase in
size as we incorporate more sources and as more drug
products are manufactured, we expect that we will run
into difficulties managing generation of, and reasoning
over, the ontology. One potential solution we intend to
investigate is to reason over modules individually and
combine the results. We also intend to create more
manageable subsets of DrOn, which should allow users
to work with only the portions of DrOn that they need
for a particular use case.
Competing interest
The authors have no competing interests.
Authors contributions
Author EJ performed most of the historical analysis of RxNorm, including
nearly all the work described under Mining RxNorm. He also contributed
most of the text of this section. Author WRH contributed to the historical
analysis of RxNorm, guiding much of EJs work. He also built the dron-upper
module, was involved in mapping the other modules to the proper places in
the ontology, and manually curated the three dispositions not taken from
ChEBI. Additionally, he helped draft the manuscript. Author MB provided
ontological guidance in the construction of the ontologies and helped draft
the manuscript. Author JH performed most of ETL process using the data
gathered by EJ, programatically created most of the modules of the ontol-
ogy, incorporated the manually-curated dispostions into the database, and
helped draft the manuscript. All authors read and approved the final
manuscript.
Acknowledgements
This work was supported by award number UL1TR000039 from the National
Center for Advancing Translational Sciences, award R01GM101151 from the
National Institute for General Medical Science, and the Arkansas Biosciences
Institute, the major research component of the Arkansas Tobacco Settlement
Proceeds Act of 2000. This paper does not represent the views of NCATS,
NIGMS, or the National Institutes for Health.
Received: 16 August 2013 Accepted: 12 December 2013
Published: 18 December 2013
JOURNAL OF
BIOMEDICAL SEMANTICS
Marcos et al. Journal of Biomedical Semantics 2013, 4:40
http://www.jbiomedsem.com/content/4/1/40RESEARCH Open AccessThe Ontology of Vaccine Adverse Events (OVAE)
and its usage in representing and analyzing
adverse events associated with US-licensed
human vaccines
Erica Marcos1, Bin Zhao2 and Yongqun He3,4,5*Abstract
Background: Licensed human vaccines can induce various adverse events (AE) in vaccinated patients. Due to the
involvement of the whole immune system and complex immunological reactions after vaccination, it is difficult to
identify the relations among vaccines, adverse events, and human populations in different age groups. Many
known vaccine adverse events (VAEs) have been recorded in the package inserts of US-licensed commercial vaccine
products. To better represent and analyze VAEs, we developed the Ontology of Vaccine Adverse Events (OVAE) as
an extension of the Ontology of Adverse Events (OAE) and the Vaccine Ontology (VO).
Results: Like OAE and VO, OVAE is aligned with the Basic Formal Ontology (BFO). The commercial vaccines and
adverse events in OVAE are imported from VO and OAE, respectively. A new population term human vaccinee
population is generated and used to define VAE occurrence. An OVAE design pattern is developed to link vaccine,
adverse event, vaccinee population, age range, and VAE occurrence. OVAE has been used to represent and classify
the adverse events recorded in package insert documents of commercial vaccines licensed by the USA Food and
Drug Administration (FDA). OVAE currently includes over 1,300 terms, including 87 distinct types of VAEs associated
with 63 human vaccines licensed in the USA. For each vaccine, occurrence rates for every VAE in different age
groups have been logically represented in OVAE. SPARQL scripts were developed to query and analyze the OVAE
knowledge base data. To demonstrate the usage of OVAE, the top 10 vaccines accompanying with the highest
numbers of VAEs and the top 10 VAEs most frequently observed among vaccines were identified and analyzed.
Asserted and inferred ontology hierarchies classify VAEs in different levels of AE groups. Different VAE occurrences
in different age groups were also analyzed.
Conclusions: The ontology-based data representation and integration using the FDA-approved information from
the vaccine package insert documents enables the identification of adverse events from vaccination in relation to
predefined parts of the population (age groups) and certain groups of vaccines. The resulting ontology-based VAE
knowledge base classifies vaccine-specific VAEs and supports better VAE understanding and future rational AE
prevention and treatment.* Correspondence: yongqunh@umich.edu
3Unit for Laboratory Animal Medicine, University of Michigan Medical School,
Ann Arbor, MI 48109, USA
4Department of Microbiology and Immunology, University of Michigan
Medical School, Ann Arbor, MI 48109, USA
Full list of author information is available at the end of the article
© 2013 Marcos et al.; licensee BioMed Central Ltd. This is an open access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Marcos et al. Journal of Biomedical Semantics 2013, 4:40 Page 2 of 10
http://www.jbiomedsem.com/content/4/1/40Background
Many licensed vaccines exist to protect against a variety
of diseases and infections. They are extremely useful in
decreasing infection prevalence in human populations. Due
to the public health benefits of vaccines, their coverage has
been increasing in recent years. However, each vaccine
often induces different types of adverse events (AEs).
As vaccine usage increases, the risk of adverse events
proportionally increases [1]. There is a need to predict
probabilities of different adverse events arising in different
individuals, which can potentially lead to a decline in the
risk of developing an adverse event. Many known vaccine
adverse events (VAEs) at the population level have been
recorded in the package inserts of commercial vaccine
products. The VAE information in the package inserts
may be used for systematic VAE analysis and comparison,
providing a fundamental basis for further individual level
VAE evaluation and prediction.
Two existing ontologies are closely related to the VAE
studies. The Ontology of Adverse Events (OAE) is a
community-based biomedical ontology in the area of
adverse events [2,3]. OAE defines an adverse event as a
pathological bodily process that occurs after a medical
intervention (e.g., vaccination, drug administration). The
OAE adverse event is a subclass of the ontology term
pathological bodily process defined in the Ontology of
General Medicine Science (OGMS) (http://code.google.
com/p/ogms/). To be consistent with most practical uses
of the term, OAE does not assume a causal relation
between an adverse event and a medical intervention.
OAE has defined over 2,000 types of adverse events
that are commonly found in different medical interventions.
The community-based Vaccine Ontology (VO) represents
various vaccines, vaccine components, and vaccinations
[4,5]. Both OAE and VO are OBO Foundry library ontol-
ogies and are developed by following the OBO Foundry
principles [6].
OAE has been shown to significantly increase the
power of analyzing often noisy case report data from the
Vaccine Adverse Event Reporting System (VAERS) [3].
In this study, the adverse events associated with killed
attenuated and live attenuated influenza vaccines were
separately extracted from VAERS, statistically analyzed,
and compared with each other. The AEs annotated and
stored in VAERS were assigned to the Medical Dictionary
for Regulatory Activities (MedDRA) codes [7]. Compared to
MedDRA, OAE was found to be better to classify the
groups of AEs associated with different types of influenza
vaccines, and biologically significant findings were generated
[3]. Due to the lack of randomized, well-controlled studies,
it is often difficult to justify the causality between an adverse
event reported and a vaccine administration using the
VAERS or other clinical case report data. However,
the results cited from the package insert documentsof FDA licensed vaccines were typically generated from
randomized, well-controlled clinical trials. Compared to
the noisy data from clinical VAE case reports, the adverse
events recorded in the official package inserts are known
adverse events specific for individual vaccines. To our
knowledge, there has been no published paper in the
ontological domain to analyze commonly known VAEs
recorded in the FDA package insert documents.
To better represent various VAEs and support vaccine
safety study, we developed the Ontology of Vaccine Adverse
Events (OVAE) as an extension of the biomedical ontologies
OAE and VO. In this paper, we introduce the basic frame-
work of the OVAE and how OVAE is used to represent and
analyze all adverse events reported in the product package
inserts of 63 FDA approved commercial vaccines currently
used in the USA market.
Results
OVAE system design and statistics
The goal of current OVAE development is to generate
an ontology-based VAE knowledge base that represents
known adverse events (AEs) associated with licensed
vaccines. Such a knowledge base incorporates the OAE
terms of AEs together with the vaccine information defined
in the VO. As the primary developer of the OAE and VO,
we argue that OAE is not appropriate or responsible for
representing various AEs specific for any particular medical
intervention including vaccination due to the following
reasons. First, OAE emphasizes the representation of
various AEs general for most medical interventions,
and related topics (e.g., methods for analysis of the
causal relation between AEs and medical interventions,
and factors affecting the causality analysis). Currently
OAE is already large and contains nearly 3,000 terms. It is
expected that many more AE terms will be added to OAE.
Therefore, it is ideal to make OAE focused and as
concise as possible. Secondly, AE researchers related
to specific medical intervention domains may have more
domain-specific demands and requests. For example, VAE
researchers would like to link AEs to different vaccines.
The drug researchers may prefer to associate AEs with
specific drugs. The vaccine (or drug) researchers may
not be interested in drug (or vaccine) specific AEs. As a
relatively independent domain, VAEs have been focuses of
many vaccine researchers and groups. Independent from
drug AEs, clinical VAEs are reported to vaccine-specific
VAERS system in the USA [8]. Meanwhile, the Vaccine
Ontology (VO) is not suitable for representing complex
VAE data. VO has been focused on classification of
various vaccines, including licensed vaccines, vaccines
in clinical trials, and vaccines only verified in laboratory
animal models. VO also represents various types of vaccine
components (e.g., vaccine antigens, adjuvants, and vectors),
vaccine attributes (e.g., vaccine organism viability and
Table 1 Summary of ontology terms in OVAE
Ontology
names
Classes Object properties Data properties Total
OVAE 626 1 1 628
BFO 40 88 0 128
CHEBI 7 0 0 7
OBI 8 8 0 16
PATO 7 0 0 7
IAO 6 0 0 6
NCBOTaxon 81 0 0 81
OAE 119 1 0 120
OGMS 2 0 0 2
RO 0 15 0 15
UBERON 105 15 0 220
VO 188 6 3 197
Total: 1189 124 4 1,327
Marcos et al. Journal of Biomedical Semantics 2013, 4:40 Page 3 of 10
http://www.jbiomedsem.com/content/4/1/40virulence), vaccination methods, and other concise and
closely related vaccine information. The inclusion of
complex and large VAE information to VO would
make VO imbalance and not specific enough. Due to
these reasons, we generated the VAE-specific OVAE,
which is an extension of OAE and VO. OVAE specifies
AEs associated with various vaccines, for example, influenza
vaccine Afluria-associated pain adverse event. The logical
definition of such a VAE requires both the pain AE term
from OAE and the Afluria vaccine term from VO. Such a
term cannot be captured without the OVAE. The OVAE
integration of OAE and VO is also required to link
such a term to related features about the AE and vaccine,
for example, the parent term of pain AE and the patient
age requirement for the vaccine administration. Since
both OAE and VO use the Basic Formal Ontology (BFO)
(http://www.ifomis.org/bfo) as the top level class, the
alignments between OVAE, OAE, and VO are easy and
straightforward.
As an extension of OAE and VO, OVAE targets for not
only importing related terms from these two ontologies but
also including many OVAE-specific terms. The primary
data source for generating vaccine-specific AE ontology
terms in current OVAE is the official vaccine package
inserts available in the USA FDA website [9]. Each official
vaccine package insert document provided by the USA
FDA includes a section called Adverse Reactions. The
results provided in the section were obtained from carefully
designed clinical trials with randomized controls and
worldwide post-marketing experience. Therefore, the VAE
information provides basic known VAEs that are likely to
occur after an administration of a specific vaccine in
a human vaccinee. Based on the officially documented
information, OVAE includes many OVAE-specific terms,
for example, Afluria-associated pain AE to define a pain
AE specific for Afluria-vaccinated patients. As shown in
detail later in the paper, the generation of these new terms
allows the inclusion of more detailed information about
these VAEs, for example, the VAE occurrences in human
vaccinee populations in different age groups.
Table 1 lists the OVAE statistics as of July 1, 2013.
OVAE used the most recent BFO 2.0 Graz version
(http://purl.obolibrary.org/obo/bfo.owl) as the top level
ontology. Since BFO 2.0 is not yet finalized, some relation
terms (e.g., part of  or BFO_0000050) are still used in
OVAE but do not necessarily comply with the most recent
BFO 2.0. During the process of importing many AEs or
vaccine-related terms from OAE and VO to OVAE,
many terms from other existing ontologies, including
OGMS, Ontology for Biomedical Investigation (OBI) [10],
Phenotypic Quality Ontology (PATO) [11], and Information
Artifact Ontology (IAO) (http://code.google.com/p/
information-artifact-ontology/), have also been imported
to OVAE (Table 1). To maintain the ontology asserted andinferred hierarchies and support intact reasoning capabil-
ity, the OntoFox software was used for external term
importing [12]. In summary, OVAE contains 1,327
terms, including 626 OVAE-specific terms (with OVAE_
prefix). In addition, OVAE includes many ontology terms
from external ontologies, for example, all 128 terms from
the BFO version 2.0, 197 VO terms, 120 OAE terms, 16
OBI terms, 6 IAO terms, and 2 OGMS terms (Table 1).
By referencing the vaccine package insert data, OVAE
represents 87 distinct AEs associated with 63 licensed
human vaccines.OVAE design pattern of representing VAE
The general design pattern of representing a VAE in
OVAE is shown in Figure 1. Specifically, a licensed
vaccine, manufactured by a company and having spe-
cific quality (e.g., using inactivated vaccine organism),
is targeted to immunize a human vaccinee against infection
of a microbial pathogen. A particular vaccination route
(e.g., intramuscular route) is specified. A specific VAE
(e.g., Afluria-associated injection-site pain adverse event)
occurs in a human vaccinee and after (preceded_by) a
vaccination. The human vaccinee, having a specific age
(defined via a datatype) at the time of vaccination, is part of
the population of human vaccinees using this vaccine. The
VAE occurrence is defined as a frequency of an adverse
event associated with the administration of a vaccine in a
vaccinee population. The new object property term has
VAE occurrence is defined in OVAE to specify a VAE
occurrence (xsd:decimal datatype) in a human vaccinee
population that has been individually vaccinated with a
specific vaccine during a specific time period. To simplify
the representation of axioms linking vaccine adverse
Figure 1 OVAE design pattern of a human vaccine adverse event.
Marcos et al. Journal of Biomedical Semantics 2013, 4:40 Page 4 of 10
http://www.jbiomedsem.com/content/4/1/40event and human vaccinee population, OVAE generates a
shortcut relation occurs in population (Figure 1).
The vaccine attributes and vaccination details are
imported from VO. Their inclusion in the design pattern
is due to their possible contribution to the VAE determin-
ation. For example, a live attenuated vaccine and a killed
inactivated vaccine may in general induce different
types or levels of VAEs, which can be analyzed by
statistical analysis [3].
One novelty in the design pattern is the generation
and application of the population term human vaccinee
population to define a VAE occurrence. In previous
versions of OAE and VO, only vaccinee and human
vaccinee (i.e., a human being administered with a vaccine)
exist. However, it is incorrect to say that a specific
human vaccinee has a VAE occurrence of some per-
centage (e.g., 10%). An occurrence is defined only for
a population. The generation of the term human vac-
cinee population solves the ontology modeling issue.
Any particular human vaccinee is part of a human
vaccinee population.
There are two different approaches for representing the
relation between a human vaccinee (or human vaccinee
population) and an age (or age range). One approach is to
link a vaccinee to a quality named age, and then link
the age to a datatype using the OBI relation term quality
measured as. Another approach for representing the
relation is to generate a shortcut relation has age
(or specifically has age in year). To make the representa-
tion simpler and reasoning efficient, we have taken
the second choice. The use of the relation has age
will need to specify the data value as well as the unit of
the data (e.g., year). The use of the shortcut relation has
age in year is much simpler, requiring only the data value.
An example is provided below (Figure 2).Generation of OVAE covering FDA package insert
AE information
Based on the design pattern described above, the OVAE
was generated to cover the AE information extracted
from the FDA package insert documents [9]. The FDA
website includes supporting materials for most human
vaccines licensed in the USA [9]. The detailed methods
of how to manually annotate the VAE information and
represent the knowledge in OVAE are described in the
Methods section.
An example of OVAE representation of VAE is shown
in Figure 2. Briefly, Afluria has been associated with nine
different types of AEs, including injection-site pain AE
that has been defined in OAE (Figure 2A and 2B). For
each AE, it is likely that different VAE occurrences
are reported based on the age groups. OVAE uses
two datatype property terms (has age in year and has
VAE occurrence) to link vaccinee population groups
and VAEs associated with particular VAE occurrences
(Figure 2B). The OR clause is used to include vaccinee
populations with different age ranges. The information
matches to the FDA package insert information (Figure 2C).
The FDA package insert citation was also used as a
definition source (annotation property).SPARQL query of OVAE data
The SPARQL Protocol and RDF Query Language (SPARQL)
is a query language for querying and manipulating
data stored in a RDF tripe store. SPARQL is a standard rec-
ommended by the World Wide Web Consortium (W3C),
and is recognized as a key technology of the Semantic
Web. SPARQL 1.1 has been the official version since
March, 2013 [13]. SPARQL queries allow for triple patterns,
conjunctions, disjunctions, and optional patterns.
Figure 2 OVAE representing Afluria VAEs reported in FDA vaccine package insert. (A) The hierarchical structure of Afluria VAEs represented
in OVAE. (B) OVAE axiom representation of Afluria-associated injection-site pain AE based on three age groups. (C) Afluria adverse reactions
recorded in the FDA package insert document. Other VAEs shown in the FDA package inserts are also represented in OVAE. The subfigures
(A) and (B) were screenshots of OVAE using the Protégé OWL editor. The text from (C) comes from the FDA package insert document of the
Afluria vaccine.
Marcos et al. Journal of Biomedical Semantics 2013, 4:40 Page 5 of 10
http://www.jbiomedsem.com/content/4/1/40Figure 3 demonstrates an example of how to use
SPARQL to count the number of specific adverse events
for each vaccine. Figure 3A is a SPARQL script for
querying OVAE in a RDF triple store. In this SPARQL
query, the source of the OVAE ontology is specified
following the FROM keyword. In this script, the
variables ?pclass and ?cclass are two classes with
their labels (rdfs:label) ?plabel and ?clabel, respectively.
The child class ?cclass is a subclass (rdfs:subClassOf) of
the parent class ?pclass. A regular expression (regex)
filter function requires that the string ?plabel include the
words adverse events, for example, Recombivax HB
vaccine adverse event. Another regex filter function
specifies the inclusion of the word associated in the
subclass label ?clable, for example, Recombivax
HB-associated fever AE. These two regex functions
are designed based on the naming convention defined
in OVAE. Specifically, a bottom-level vaccine-specific
adverse event term label always uses the words associated
and AE (instead of adverse event), and its parent
vaccine-specific term label always contains the words
adverse event (instead of the abbreviation version AE).
To display the results, the SELECT function in the
script specifies ?pclass, ?plabel, and the total count
of ?cclass in a decreasing order (DESC) based on
the count. The top eight query results are shown in
Figure 3B.
In this study, different SPARQL scripts were generated
to address different questions as exemplified below.OVAE VAE data analysis results
After all VAEs found in FDA licensed vaccines are
represented in OVAE, the quality of the ontology was
manually annotated, and an ontology reasoner [14] was
used to ensure no logical errors occurring in the ontology
formation. To address scientific questions associated with
different vaccine AEs, the OVAE ontology was queried
using SPARQL. The Protege-OWL editor also provides
user-friendly function to directly visualize OVAE results.
Below we provide examples to illustrate how the analysis
of the OVAE knowledge base can be used to answer
different VAE questions.
First, those vaccines that are associated with the largest
number of VAEs were analyzed (Table 2). It is interesting
that many of these vaccines protect against meningitis, which
can be caused by different pathogens including Haemophilus
influenza type b (Comvax and PedvaxHIB), Neisseria
meningitides (Menactra), and Streptococcus pneumonia
(Prevnar 13). The list also includes three tetanus vaccines
and two Hepatitis B vaccines (Table 2). The relation between
these common diseases/pathogens and the high variety of
VAEs reported is unclear and deserves further investigations.
It is noted that the information does not dictate the severity
of AEs associated with each vaccine, but instead indicates
that those FDA-licensed vaccines display the most variation
in their reported AEs.
Secondly, we evaluated the top VAEs that have been
reported most frequently among all vaccines licensed in
the USA and represented by OVAE (Table 3). Most of
Figure 3 Example SPARQL for OVAE query. This example queries the numbers of specific adverse events associated with individual vaccines.
The SPARQL script is shown at the top. Below the script is the results obtained after execution of the SPARQL query. See the text in the
manuscript for detailed explanation.
Marcos et al. Journal of Biomedical Semantics 2013, 4:40 Page 6 of 10
http://www.jbiomedsem.com/content/4/1/40the top 10 frequently observed VAEs are expected, such as
injection-site pain and redness, fever, and local swelling.
The headache and myalgia (i.e., muscle pain) are two
subtypes of pain. The pain AE, malaise (i.e. uneasiness and
discomfort) AE, and fatigue AE are all subtypes of behavior
and neurological AEs. The frequent occurrence of behavior
and neurological AE is likely associated with the common
intramuscular route used for vaccine administration.
Specific microbial antigen contents may also induce
frequently observed VAEs (e.g., fever). It is noted that
the information does not dictate which VAEs are the
most severe, but indicates which VAEs are commonly
observed in currently licensed vaccines in the USA.
To better understand the top VAEs associated with
licensed human vaccines, the hierarchical structure of
the top 10 VAEs (Table 3) was extracted using the
tool OntoFox and visualized using Protégé ontology
editor (Figure 4). The hierarchical visualization indicatesthat most of the top ranked VAEs belong to the behavior
and neurological AE branch. It is also noted that after rea-
soning, two adverse events (e.g., injection-site pain AE)
were inferred to be subclasses of injection-site adverse
event (Figure 4B). Since OAE does not allow multiple
inheritance, injection-site pain AE cannot be asserted
under both pain AE and injection-site adverse event. In
OAE, injection-site pain AE is asserted under pain AE
which occurs in an injection site. A reasoner will be able
to infer it as a subclass of injection-site adverse event as
well (Figure 4B). The ontology reasoning provides additional
power in VAE classification.
Furthermore, we compared the VAEs and VAE occur-
rences under different age groups. As shown in Figure 2,
the OVAE clearly represents the associations between
VAEs, the VAE occurrence rates, and different ages
(in years) of human vaccinee population. Our analysis
can further identify which age category has a higher
Table 2 Top 10 vaccines with the largest variety of VAEs
reported
Vaccine (disease or pathogen) VO_ID Total # VAE
Recombivax HB (Hepatitis B) VO_0010737 20
Comvax (Hib meningitis, Hepatitis B) VO_0000028 19
Menactra (Neisseria meningitidis) VO_0000071 18
Tetanus and Diphtheria Toxoids VO_0000111 18
Absorbed by MA Biological
(Tetanus, Diphtheria)
Prevnar 13 (Streptococcus pneumonia) VO_0000090 16
PedvaxHIB (H. influenzae type b) VO_0000083 15
RabAvert (Rabies) VO_0000094 14
JE-Vax (Japanese Encephalitis) VO_0000066 13
Tetanus Toxoid (boost only) (Tetanus) VO_0000984 13
Tetanus Toxoid Absorbed (Tetanus) VO_0000048 13
Note: The disease or pathogen name specified next to a vaccine name
indicates the disease or pathogen infection against which the vaccine is used.
The three bacteria listed (shown by italicized words) can all induce meningitis.
The vaccines are sorted based on the VAEs recorded in their package
insert documents.
Marcos et al. Journal of Biomedical Semantics 2013, 4:40 Page 7 of 10
http://www.jbiomedsem.com/content/4/1/40probability of experiencing any specific adverse events. For
example, we found that Salmonella typhi vaccine Typhim
Vi is associated with injection-site tenderness adverse
events with the highest rate of 97.5% at the age group of
1840 years old. Based on the classification of child,
adult, and child-adult described in the Methods section
in the paper, there are 240, 160, and 177 vaccine-specific
AEs in the age categories child, adult, and child-adult,
respectively. It is also found that in general the VAE
occurrences shown in the children are typically higher
than those in adults. This suggests that individuals
under 18 years may be more likely to experience an
adverse reaction after vaccination.
The above examples illustrate the advantages of OVAE
in VAE data integration and analyses. The usage of
OVAE provides an efficient approach to answer differentTable 3 Top 10 most frequently reported VAEs
AE name OAE_ID Total # vaccines %
Injection-site pain AE OAE_0000369 43 68.3
Headache AE OAE_0000377 39 61.9
Fever AE OAE_0000361 34 54.0
Local swelling AE OAE_0001139 30 47.6
Injection-site redness AE OAE_0001546 25 40.7
Irritability AE OAE_0001105 23 36.5
Malaise AE OAE_0000390 21 33.3
Injection-site erythema AE OAE_0000644 20 31.7
Myalgia AE OAE_0000375 19 30.2
Fatigue AE OAE_0000034 18 28.6VAE questions, which would be very difficult to address
without such an ontology.
Discussion
The development of OVAE is aimed to align and reuse
existing ontologies OAE and VO, and systematically repre-
sent and analyze vaccine-specific adverse events (VAEs).
As demonstrated in this report, such a strategy has many
advantages. First, as shown in Figure 2, the ontological
classification is easy for humans to interpret and analyze. A
human can browse the hierarchical tree to quickly under-
stand which VAEs are typically associated with a licensed
vaccine. Secondly, the ontology OWL representation is also
interpretable by computers and software programs. New
programs can be developed to parse and analyze the infor-
mation. Thirdly, the approach of aligning OVAE with exist-
ing ontologies allows efficient integration of data presented
in other ontologies (e.g., VO). Fourthly, the usage of OVAE
and other related ontologies makes it possible to analyze
VAEs with various tools such as VO-based literature
mining [15]. Eventually, an ontology-based linked VAE
data system can be generated.
Furthermore, it is possible to apply the OVAE framework
to analyze clinical VAE data such as those case reports
stored in VAERS [8]. For example, by comparing the
reported vaccine-specific VAE cases in VAERS with the
VAE occurrences reported in the package inserts and
OVAE, it is easy to differentiate known VAEs and possibly
new VAEs associated with the vaccine. Many differences
exist in terms of the data shown in the package inserts and
in VAERS database. While the data in the package inserts
were typically obtained from well controlled clinical trials,
clinical VAE case reports stored in VAERS came from
random reports from physicians, patients, patients parents,
or other sources. The VAERS database does not indicate
the total number of vaccinated human vaccinees in any
given period, making it impossible to calculate exact VAE
occurrences. However, an ontological approach in combin-
ation with a statistical analysis is still useful in VAERS data
analysis as previously demonstrated [3]. Currently the AE
data stored in VAERS are annotated using the Medical
Dictionary for Regulatory Activities (MedDRA), a coding
vocabulary nomenclature commonly used for clinical
adverse event recording and normalization [16]. However,
many disadvantages of MedDRA, including the lack of
term definitions and a well-defined hierarchical and logical
structure, prevent its effective usage in VAE term classifica-
tion. Our previous study showed that a mapping between
MedDRA and OAE terms followed by the application of
OAE hierarchy provided a feasible solution for valid classi-
fication of VAEs detected through statistical analyses of
VAERS data [3]. MedDRA does not have rich axiomatiza-
tion as shown in OAE and OVAE. The richer and verified
ontological axiomatization will facilitate VAE data analysis.
Figure 4 Classification of top 10 AEs associated with licensed human vaccines in the US. These OAE terms have been imported to OVAE
using OntoFox and visualized using Protégé OWL editor. (A) Asserted hierarchy in OAE; (B) Inferred hierarchy after reasoning.
Marcos et al. Journal of Biomedical Semantics 2013, 4:40 Page 8 of 10
http://www.jbiomedsem.com/content/4/1/40As an extension of both OAE and VO, OVAE represents
various VAEs associated with different licensed vaccines.
One future research direction will be to identify novel
ways to better analyze VAE clinical data using OVAE. Indeed,
one effective way is to develop an OVAE-based "Linked
Data" (LD; http://www.w3.org/standards/semanticweb/data)
system specifically for representing and sharing various VAE
clinical and research "instance" data obtained from VAERS
and other resources. Advanced reasoning methods can
then be developed to analyze the large but well-organized
data in the linked data system. Such an strategy is being
designed and implemented in our group.
While many AEs are common, different vaccines
are associated with different AEs with various molecular
mechanisms. The classification of different vaccine-specific
AEs allows us to examine the similarities and difficulties of
molecular interactions and pathways underlying different
types of VAEs. Various Omics and informatics tools can
also be applied. Therefore, the ontology representation
of vaccine-specific AEs is a first step towards refined
deep understanding of vaccine adverse events. The better
understanding of the vaccine-specific AE patterns and the
underlying molecular mechanisms will make it possible to
rationally design practical measures to prevent and treat
VAEs and thus support public health.
In addition to the VAEs associated with USA licensed
vaccines, the OVAE can be used to represent VAEs
associated with vaccines licensed in other countries. It is
also noted that the method of establishing vaccine-specific
OAE extension may likely be applied for developingOAE extensions in other specified domains such as
drug-associated adverse events.Conclusions
The Ontology of Vaccine Adverse Events (OVAE) onto-
logically represents and classifies various identified vac-
cine adverse events (VAEs) associated with human
vaccines licensed for use in the USA. Systematical ana-
lysis of the OVAE data improves the understanding of
vaccine-specific VAEs, making it possible to rationally
design VAE prevention and treatment measures and to
benefit public health.Methods
OVAE ontology generation
Following VO and OAE, OVAE is also edited with the
Web Ontology Language (OWL2) format (http://www.
w3.org/TR/owl-guide/). FDA-licensed human vaccines
represented in VO were imported to OVAE using the
tool OntoFox [12]. Those adverse event terms reported
in the package inserts of FDA licensed human vaccines
were also imported to the OVAE using OntoFox. New
OVAE-specific terms were generated with IDs containing
the prefix of OVAE_ followed by seven auto-incremental
digital numbers and edited using the Protégé 4.2 OWL
ontology editor (http://protege.stanford.edu/). The Java-
based ELK OWL 2 reasoner [14] was used for OVAE
ontology reasoning.
Marcos et al. Journal of Biomedical Semantics 2013, 4:40 Page 9 of 10
http://www.jbiomedsem.com/content/4/1/40Data source of known VAEs
The official FDA website that provides supporting
documents of licensed vaccines was the primary data
source [9]. A PDF version of a package insert docu-
ment is available for almost every vaccine in the data
source. The PDF document includes a section called
Adverse Reactions that contains text descriptions of
known vaccine adverse events associated with the
vaccinated population.
Data collection and formatting to ontology
Based on the OVAE framework and the adverse event
description in the package inserts, a design pattern was first
generated to lay out the relations between different ontol-
ogy classes, properties, terms and data types (Figure 1). The
design pattern was used to form an MS Excel template for
collection of individual adverse events for different vaccines.
The MS Excel template includes the following categories:
vaccine name, vaccine VO ID, VAE location, VAE name in
package insert, VAE name in OAE, OAE ID, age category,
age years, VAE occurrence, and reference. Data for each
category was manually collected from individual vaccine
package inserts and then input into the Excel template. The
VAE location is listed as either injection-site or systemic.
The injection-site location is incorporated as part of the
OAE term, while the systemic AEs are set up as default.
Age categories included child (typically under 18 years old),
adult (above 18 years old), senior (above 65 years old), or
child-adult (all ages). Specific ages are concerted to years
and presented to comply with the OWL format. Each VAE
is referenced by the package insert citation. Following the
manual data collection and annotation, the program
Ontorat (http://ontorat.hegroup.org) was used to transform
the Excel file data to the OVAE ontology format [17].
VAE data analysis
To identify specific OAE or VO hierarchical structure among
a list of terms, OntoFox was first used to extract the input
OAE or VO terms and all associated terms required for
proper hierarchical assertion and inference. The output
OWL files were then visualized using a Protégé OWL editor.
SPARQL scripts were generated to query the OVAE
knowledge base from a RDF triple store that contains
the OVAE RDF triples. As an ontology in the OBO
Foundry ontology library (http://obofoundry.org/), OVAE
is automatically deposited in the Hegroup RDF triple store
[18]. The Hegroup triple store, the default OBO Foundry
library ontology RDF triple store, is used by Ontobee [18]
and can be queried through the Ontobee SPARQL query
interface (http://www.ontobee.org/sparql/). Our SPARQL
scripts were executed using the Ontobee SPARQL query
interface.
To identify specific OAE or VO hierarchical structure
among a list of terms, OntoFox was first used to extractthe input OAE or VO terms and all associated terms
required for proper hierarchical assertion and inference.
The output OWL files were then visualized using a
Protégé OWL editor.
OVAE project site, ontology dissemination, and licensing
The OVAE project website (http://www.violinet.org/ovae) is
located under VIOLIN, a comprehensive vaccine database
and analysis system [19]. OVAE has been deposited
to the BioPortal project of the National Center of Bio-
medical Ontology (NCBO) (http://bioportal.bioontology.
org/ontologies/3227). OVAE is also deposited in the
Ontobee linked data server (http://www.ontobee.org/
browser/index.php?o=OVAE) [18]. The OVAE source
code is available in a Google Code website: http://code.
google.com/p/ovae. The OVAE source is freely available
under the Apache License 2.0.
Abbreviations
AE: Adverse event; FDA: Food and Drug Administration; NCBO: The National
Center for Biomedical Ontology; OAE: Ontology of adverse events;
OBI: Ontology for Biomedical Investigations; OBO: The Open Biological and
Biomedical Ontologies; OGMS: Ontology for General Medical Science;
OVAE: Ontology of Vaccine Adverse Events; OWL: Web Ontology Language;
PATO: Phenotypic Quality Ontology; PHP: Hypertext preprocessor;
RDF: Resource Description Framework; SPARQL: SPARQL Protocol and RDF
Query Language; VAE: Vaccine adverse event; VAERS: Vaccine Adverse Event
Reporting System; VIOLIN: Vaccine Investigation and Online Information
Network; VO: Vaccine ontology.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
EM: Vaccine adverse event data annotation and organization, data analysis,
and manuscript editing. BZ: SPARQL script development, and manuscript
editing. YH: Primary OVAE developer, OVAE design pattern generation, data
analysis, and drafting of manuscript. All authors read and approved the final
manuscript.
Acknowledgements
This work has been primarily supported by grant R01AI081062 from the
National Institute of Allergy and Infectious Diseases (NIH-NIAID). BNs work
in this project was supported by a MCubed project titled Ontology
Development and Applications for Clinical and Translational Science
from the University of Michigan.
Author details
1College of Literature, Science, and the Arts, University of Michigan, Ann
Arbor, MI 48109, USA. 2School of Information, University of Michigan, Ann
Arbor, MI 48109, USA. 3Unit for Laboratory Animal Medicine, University of
Michigan Medical School, Ann Arbor, MI 48109, USA. 4Department of
Microbiology and Immunology, University of Michigan Medical School, Ann
Arbor, MI 48109, USA. 5Center for Computational Medicine and
Bioinformatics, University of Michigan Medical School, Ann Arbor, MI 48109,
USA.
Received: 17 August 2013 Accepted: 4 November 2013
Published: 26 November 2013
