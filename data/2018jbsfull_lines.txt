RESEARCH Open Access
Deep learning meets ontologies:
experiments to anchor the cardiovascular
disease ontology in the biomedical
literature
Mercedes Arguello Casteleiro1, George Demetriou1, Warren Read1, Maria Jesus Fernandez Prieto2, Nava Maroto3,
Diego Maseda Fernandez4, Goran Nenadic1,5, Julie Klein6,7, John Keane1,5 and Robert Stevens1*
Abstract
Background: Automatic identification of term variants or acceptable alternative free-text terms for gene and
protein names from the millions of biomedical publications is a challenging task. Ontologies, such as the
Cardiovascular Disease Ontology (CVDO), capture domain knowledge in a computational form and can provide
context for gene/protein names as written in the literature. This study investigates: 1) if word embeddings from
Deep Learning algorithms can provide a list of term variants for a given gene/protein of interest; and 2) if biological
knowledge from the CVDO can improve such a list without modifying the word embeddings created.
Methods: We have manually annotated 105 gene/protein names from 25 PubMed titles/abstracts and mapped
them to 79 unique UniProtKB entries corresponding to gene and protein classes from the CVDO. Using more than
14 M PubMed articles (titles and available abstracts), word embeddings were generated with CBOW and Skip-gram.
We setup two experiments for a synonym detection task, each with four raters, and 3672 pairs of terms (target term
and candidate term) from the word embeddings created. For Experiment I, the target terms for 64 UniProtKB
entries were those that appear in the titles/abstracts; Experiment II involves 63 UniProtKB entries and the target
terms are a combination of terms from PubMed titles/abstracts with terms (i.e. increased context) from the CVDO
protein class expressions and labels.
Results: In Experiment I, Skip-gram finds term variants (full and/or partial) for 89% of the 64 UniProtKB entries, while
CBOW finds term variants for 67%. In Experiment II (with the aid of the CVDO), Skip-gram finds term variants for
95% of the 63 UniProtKB entries, while CBOW finds term variants for 78%. Combining the results of both
experiments, Skip-gram finds term variants for 97% of the 79 UniProtKB entries, while CBOW finds term
variants for 81%.
Conclusions: This study shows performance improvements for both CBOW and Skip-gram on a gene/protein
synonym detection task by adding knowledge formalised in the CVDO and without modifying the word
embeddings created. Hence, the CVDO supplies context that is effective in inducing term variability for both
CBOW and Skip-gram while reducing ambiguity. Skip-gram outperforms CBOW and finds more pertinent term
variants for gene/protein names annotated from the scientific literature.
Keywords: Semantic deep learning, Ontology, Deep learning, CBOW, Skip-gram, Cardiovascular disease
ontology, PubMed
* Correspondence: Robert.Stevens@manchester.ac.uk
1School of Computer Science, University of Manchester, Manchester, UK
Full list of author information is available at the end of the article
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 
https://doi.org/10.1186/s13326-018-0181-1
Background
The sysVASC project [1] seeks to provide a comprehen-
sive systems medicine approach to elucidate pathological
mechanisms for cardiovascular diseases (CVDs), the
number one cause of death globally according to the
World Health Organisation [2]. SysVASC developed the
CVD ontology (CVDO) to provide the schema to inte-
grate omics data (e.g. genomics, transcriptomics, prote-
omics and metabolomics) that, together with the most
recent scientific papers, are the source of up-to-date
knowledge about the biology of the genes and proteins
underlying CVD. Extracting knowledge about genes and
proteins implicated in CVD for incorporation in the
CVDO is an important task in its maintenance. Recog-
nising those genes and proteins within the literature is a
required function of this task.
Rebholz-Schuhmann et al. [3] distinguish two ap-
proaches to identify gene/protein names from literature:
1. Lexicon based approaches that are based on large
terminological resources, e.g. resources generated
from large databases like the UniProt
Knowledgebase (UniProtKB) [4].
2. Machine Learning (ML) approaches such as
conditional random fields [5] that is used in ABNER
[6] and BANNER [7].
The first approach has the benefit of normalisation
(a.k.a. grounding) [3, 8, 9], i.e. the process of mapping a
biological term (e.g. protein name) into a unique entry
in a database of biological entities such as UniProtKB.
Fundel and Zimmer [10] suggest a limitation that the
overlap of synonyms in different data sources is rather
moderate and thus, terms from other databases, such as
the HUGO Gene Nomenclature Committee database
[11] or Entrez Gene [12], are also needed to develop a
more complete lexicon for gene and protein names. An-
other difficulty is keeping such a lexicon up-to-date, as
new term variants for genes and proteins are produced
every day [8, 9]. Our study takes the second approach
using Deep Learning, an area within ML, to identify suit-
able term variants (i.e. short forms such as abbreviations
or acronyms as well as long forms including phrases) for
protein/gene names from the literature.
While conventional ML techniques are limited in their
ability to process input data in raw natural language
form [13], neural language models from Deep Learning
can associate terms with vectors of real-valued features,
and semantically related terms end up close in the vec-
tor space [13]. The vector representations learnt for the
terms are known as word embeddings (i.e. distributed
word representations). As the performance of conven-
tional ML techniques are heavily dependent on feature
selection [14, 15], a tangible benefit of applying neural
language models is that the semantic features of the
word embeddings learnt are not explicitly present in the
raw natural language input.
This study investigates the suitability of the neural
language models CBOW (Continuous Bag-of-Words)
and Skip-gram of Mikolov et al. [16, 17] to derive a
list of acceptable alternative free-text terms (i.e. term
variants) for genes/proteins mentioned in the
biomedical literature. The study focuses on two re-
search questions:
1. Is it possible to obtain a list of term variants for a
gene/protein from CBOW and Skip-gram word
embeddings?
2. Can an improved list of term variants for a gene/
protein be produced from the word embeddings by
adding knowledge formalised in the CVDO about
genes/proteins (i.e. providing more context to
reduce ambiguity)?
In this study, a term is a combination of one or more
words/tokens, such as Klf7(?/?) with one token and
Annexin A4 with two tokens. Terms referring to a
gene and its gene product (typically a protein) are likely
to appear together as well as separately in the literature.
CBOW and Skip-gram use content windows as context,
i.e. terms appearing together in the textual input.
According to Mikolov et al. [17], CBOW predicts the
current term based on the context, while Skip-gram pre-
dicts surrounding terms given the current term.
The CVDO represents information about genes and
protein from the UniProtKB as a subClassOf axioms (i.e.
class expressions or descriptions). With the aid of the
CVDO ontology, we expect to obtain terms that provide a
more pertinent context to terms from the word embed-
dings, by, for example a) navigating the class expressions
and retrieving the protein name (e.g. ETS translocation
variant 1) for a gene symbol (e.g. ETV1); or b) retrieving
the full protein name (e.g. Annexin A4) from a partial
match (e.g. annexin 4) with the protein class name.
Knowledge within ontologies has been used in two stud-
ies  Pilehvar and Collier [18] and Minarro-Gimenez et al.
[19]  to assess the quality of word embeddings induced
from the literature. As far as we are aware, the use of on-
tologies per se to provide more context (i.e. extra terms)
and improve the list of candidate terms from the word
embeddings has not been investigated. This study intends
to exploit the relationship between genes and proteins for-
mally represented within the CVDO. A difference between
our work and Pilehvar and Colliers work [18] is that the
word embeddings are not modified, i.e. no post-
processing of the term vectors is performed. Hence, the
use of terms that exploits biological knowledge from the
CVDO ontology can be seen as an external intervention.
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 2 of 24
Related work
ML methods learn input-output relations from examples
with the goal of interpreting new inputs; hence, their
performance is heavily dependent on the choice of data
representation (or features) to which they are applied
[20]. Various types of models have been proposed to rep-
resent words as continuous vectors to estimate continu-
ous representation of words and create distributional
semantic models (DSMs). DSMs derive representations
for words in such a way that words occurring in similar
contexts have similar representations, and therefore, the
context needs to be defined.
Traditional DSMs include Latent Semantic Analysis
(LSA) [21], that generally takes an entire document as a
context (i.e. word-document models), and Hyperspace
Analog to Language (HAL), [22] that takes a sliding
word window as a context (i.e. sliding window models).
Random Indexing [23] has emerged as a promising alter-
native to LSA. LSA, HAL and Random Indexing are
spatially motivated DSMs. Examples of probabilistic
DSMs are Probabilistic LSA (PLSA) [24] and Latent
Dirichlet Allocation (LDA) [25]. While spatial DSMs
compare terms using distance metrics in high-
dimensional space [26], probabilistic DSMs such as LDA
or PLSA measure similarity between terms according to
the degree to which they share the same topic distribu-
tions [26]. Most DSMs have high computational and
storage costs associated with building the model or
modifying it due to the huge number of dimensions in-
volved when a large corpus is modelled [26].
This study applies neural language models, i.e. distrib-
uted representation of words learnt by neural networks
(NNs). Although neural models are not new in DSMs,
recent advances in NNs make feasible the derivation of
words from corpora of billions of words, hence the
growing interest in Deep Learning and the neural lan-
guage models CBOW and Skip-gram [16, 17]. CBOW
and Skip-gram have gained popularity to the point of be-
ing the baseline for benchmarking word embeddings
[27] and as baseline models for performance compari-
sons [28]. CBOW and Skip-gram have already been
trained to produce high-quality word embeddings from
English Wikipedia [27, 29].
Pyysalo et al. [30] and Minarro-Gimenez et al. [19]
were the first to apply neural language models to
PubMed corpora. Pyysalo et al. [30] used Skip-gram with
22 M PubMed articles as well as more than 672 K
PubMed Central Open Access full text articles. The
main aim of Pyysalo et al.s work was to make available
word representations (1- to 5-grams) from the literature
that could be reused. Minarro-Gimenez et al. [19] used
smaller datasets from PubMed as well as from other
medical (i.e. Merck Manuals [31], Medscape [32]) and
non-medical sources (i.e. Wikipedia [33]). Many later
studies have created word embeddings with CBOW and
Skip-gram using PubMed corpora.
We describe some of these studies taking into account
four tasks that focus on text words, concepts and their
relations. At the end of this subsection, we include stud-
ies that combine ontologies with word embeddings.
Semantic similarity and relatedness task
Pedersen et al. [34] align with more recent studies (Hill
et al. [35] and Pakhomov et al. [36]) in emphasising the
difference between semantic similarity and semantically
relatedness. Pedersen et al. [34] state: semantically simi-
lar concepts are deemed to be related on the basis of
their likeness. Both Pedersen et al. [34] and Hill et al.
[35] agree with the view of Resnik [37] that semantic
similarity represents a special case of semantic related-
ness. Pedersen et al. [34] advocate semantic similarity
measures based on is-a relations, where concepts within
a hierarchy are linked directly or indirectly. Prior to Pe-
dersen et al. [34], Caviedes and Cimino [38] investigated
conceptual similarity metrics based on the minimum
number of parent links between concepts. Studies by
Caviedes and Cimino [38], Pedersen et al. [34], Hill et al.
[35] and Pakhomov et al. [36] made available their data-
sets of word-pairs together with human judgments of re-
latedness/similarity. Hill et al.s [35] dataset of 999 word-
pairs, like the WordSimilarity-353 Test Collection [39]
(353 word-pairs) and the MEN Test Collection [40] (3 K
word-pairs), are common English words. These datasets
can be regarded as gold standards for the evaluation of
semantic models.
Muneeb [41] et al. applied Skip-gram and CBOW to
1.25 M PubMed articles and evaluated the quality of the
word embeddings using the Pedersen et al. [34] word-
pairs. Muneeb [41] et al. concluded that Skip-gram is
better suited than CBOW for semantic similarity and re-
latedness. Chiu et al. [42] used the Pyysalo et al. [30]
datasets and more than 10 M PubMed English abstracts
from the BioASQ challenge [43] for an intrinsic evalu-
ation of the Skip-gram and CBOW word embeddings
with the Pakhomov et al. [36] word-pairs. Chiu et al.
[42] conclude that Skip-gram shows overall better re-
sults for semantic similarity and relatedness than CBOW
with different pre-processing.
Synonymy detection task
Hill et al. [35] interpret relatedness as association
and the strongest similarity relation is synonymy. Two
well-known datasets for evaluating synonymy are the 80
TOEFL (Test of English as a Foreign Language) syno-
nym questions from [21] and the 50 ESL (English as a
Second Language) synonym questions from [44]. Both
studies [21] and [44] consist of synonym questions with
4 options that require knowledge of common English
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 3 of 24
words. It should be noted that the TOEFL synonym
questions dataset is used in the paper that introduces
LSA [21].
To the best of our knowledge no gold standard of word-
pairs together with human judgments for synonymy de-
tection exists specific to the biomedical domain.
Name entity recognition (NER) and relation extraction tasks
The BioCreative (Critical Assessment of Information Ex-
traction systems in Biology) challenge [45] focuses on
recognition of entities in text (i.e. NER) as well as rela-
tion extraction. For BioCreative II, Smith et al. [46] men-
tion three tasks: gene mention (GM), gene normalisation
(GN), and protein-protein interaction (PPI); the first two
are within the scope of NER, whilst the third is a rela-
tion extraction task that has NER as a subtask [47].
Pyysalo et al. [30] used Skip-gram to create word em-
beddings from three datasets: one based on all 22 M
PubMed articles; a second based on more than 672 K
PubMed Central Open Access full text articles; and a
third combining the previous two. Pyysalo et al. [30]
clustered the word embeddings created using the well-
known K-means clustering algorithm [48] with k = 100.
Pyysalo et al. [30] performed a set of NER experiments
to assess the quality of both the word embeddings and
the clusters created. The NER experiments rely on three
biomedical domain corpora: GM using the BioCreative
II dataset; anatomical entity recognition using the Ana-
tomical Entity Mention corpus [49]; and disease recogni-
tion using the NCBI (National Center for Biotechnology
Information) Disease corpus [50]. More recently Chiu et
al. [42] performed an extrinsic evaluation of word em-
beddings created from CBOW and Skip-gram for NER
using two biomedical domain corpora: GM using the
BioCreative II dataset and the JNLPBA challenge corpus
from Kim et al. [51]. The JNLPBA challenge is a NER
task using an extended version of the GENIA corpus
(version 3.02) [52]. The GENIA corpus is a manually an-
notated corpus of 2 K PubMed/MEDLINE abstracts se-
lected from a search using Medical subject headings
(MeSH) [53] terms human, blood cells, and tran-
scription factors. Chiu et al. [42] conclude that overall
Skip-gram shows better results for NER using the data-
sets from [46, 51] than CBOW with different pre-
processing.
Li et al. [54] used Skip-gram with 5.33 M PubMed
abstracts obtained from a search with protein as the
keyword. Li et al. [54] like Pyysalo et al. [30] applied the
K-means clustering algorithm to cluster word vectors. A
difference to the Pyysalo et al. [30] study is that Li et al.
[54] employed the Brown tree-building algorithm [55],
which is intended for n-gram language models, after ap-
plying K-means clustering. To evaluate the PPI extrac-
tion performed, Li et al. [54] relied on five publically
annotated corpora that has been quantitatively analysed
previously in a study by Pyysalo et al. [56].
Text categorisation (a.k.a. text classification, or topic
spotting)
Sebastiani [15] states that text categorization is "the ac-
tivity of labeling natural language texts with thematic
categories from a predefined set". Therefore, assigning
keywords or key phrases from MeSH to PubMed/MED-
LINE titles or titles-plus-abstracts is a type of text cat-
egorisation known as MeSH indexing. The 2017 BioASQ
challenge comprised three tasks, one is MeSH indexing,
i.e. requesting participants to classify new PubMed arti-
cles before curators manually assign MeSH terms to
them with some help from the Medical Text Indexer
(MTI) [57] from NLM. The MeSHLabeler is an algo-
rithm for MeSH indexing (Liu et al... [58]) that outper-
forms MTI and won the BioASQ challenge for MeSH
indexing in years 2 and 3 of the competition. Both MTI
and the MeSHLabeler [58] employ classic bag-of-words
representations.
Peng et al [59] used more than 1 M PubMed citations
(some downloaded from NLM and some from the
BioASQ Year 3 challenge) and introduced DeepMeSH, a
workflow that exploits CBOW and obtained a slightly
better performance (2% higher micro F-measure) than
the MeSHLabeler. It should be noted that MTI, MeSH-
Labeler, and DeepMeSH employed implementations of
the k-nearest neighbour algorithm.
Word embeddings and ontologies
The neural language models CBOW and Skip-gram rep-
resent each term as a d-dimensional vector of d real
numbers. Taking the vector for a target term and apply-
ing cosine similarity, a list of top ranked terms (highest
cosine value) can be obtained from the created word
embeddings. Minarro-Gimenez et al. [19] and Pilehvar
and Collier [18] employed the knowledge represented
within ontologies together with metrics based on cosine
similarity to evaluate the quality of generated word em-
beddings. We overview the studies as follows:
1. Minarro-Gimenez et al. [19] focused on four
relationships (may_treat; may_prevent; has_PE; and
has_MoA) from the National Drug File - Reference
Terminology (NDF-RT) ontology [60] to assess the
word embeddings created based on the hit rate
(a.k.a. true positive rate or recall). For example, the
number of diseases in a may_treat relationship
with a drug. The hit rate increases if more words for
pertinent diseases are within the list of top ranked
terms from the word embeddings. Hence, the
authors assessed the word embeddings based on a
relation extraction task and benchmark against
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 4 of 24
knowledge within the NDF-RT ontology. This early
study reported a relatively low hit rate; in contrast,
later studies (e.g. Levy et al. [29] and Chiu et al.
[42]) benefit from the effect of various hyperpara-
meter configurations.
2. Pilehvar and Collier [18] used the Human
Phenotype Ontology (HPO) [61] to assess word
embeddings created with Skip-gram from 4B tokens
from PubMed abstracts based on two tasks:
synonym (alternative names to a class name) and
hypernym (X is-a subclass of Y) identification. For
the synonym task, the authors benchmark against
knowledge within the HPO for two annotation
properties; oboInOwl:hasRelatedSynonym and
oboInOwl:hasExactSynonym. For the HPO in OWL,
a class name (rdfs:label) may have synonyms
represented by these two OWL annotation properties.
Based on the position in the list of retrieved terms,
Pilehvar and Collier [18] calculated the mean and
median rank as well as the percentage of phenotypes
(i.e. class names in the HPO) for which the rank was
equal to one (i.e. the first term in the list retrieved has
a synonym in the HPO). Pilehvar and Collier [18]
reported improvements by post-processing, i.e.
recalculating each dimension of the resulting word
vector per phenotype considering a list of weighted
words obtained via Babelfy [62]. The authors state that
for the phenotype flexion contracture of digit a list
of 1.3 K weighted words was obtained via Babelfy.
Methods
This section starts by introducing the three data re-
sources used in two experiments. Next, we describe the
two experiments for a gene/protein synonym detection
task that use the same vector representations learnt for
the terms (i.e. the word embeddings) with CBOW and
Skip-gram. As in the synonym detection task described
by Baroni et al. [63], both experiments consist of a pair
of terms (the target and the candidate) where the cosines
(the normalized dot product) of each candidate term
vector with the target term vector is computed. Finally,
we present the human evaluation performed and the
three metrics applied to assess the performance of
CBOW and Skip-gram in the gene/protein synonym de-
tection task.
Data resources
Creation of a small-annotated corpus of gene/protein
names from 25 PubMed articles
The sysVASC project performed a systematic literature
review that involved a PubMed query with the text: cor-
onary heart disease AND (proteomics OR proteome OR
transcriptomics OR transcriptome OR metabolomics OR
metabolome OR omics) [Julie Klein 2016, personal
communication, 07 June]. The sysVASC review formed
part of the data collection protocol to obtain patients
with chronic and stable vascular (coronary) disease with
exclusion of datasets on acute vascular events or history
of potentially interfering concomitant disease. A collec-
tion of 34 omics studies/articles with different biological
entities of interest (gene, protein, metabolite, miRNA)
fulfilled the eligibility criteria. To create a small-
annotated corpus relevant for sysVASC and useful for
the synonym detection task, we selected 25 of these
omics studies that focuses mainly on genes/proteins and
are available in the MEDLINE/PubMed database [64].
We left out articles that focus on metabolites or miRNA.
The 25 PubMed articles selected were published be-
tween 2004 and 2014.
To find the genes/proteins mentioned within the 25
PubMed titles/abstracts, we followed Jensen et al. [65]
who divided the task into two: first, the recognition of
words that refer to entities and second, the unique
identification of the entities in question. One curator
manually annotated 105 terms related to gene/protein
names from the 25 PubMed abstracts and titles. Corpus
annotation requires at least two annotators and the
development of annotation instructions, and thus, the
small-annotated corpus cannot be considered a gold
standard corpus as only one curator annotated the gene/
protein names and no detailed annotation guidelines
were developed. For unique identification of genes/pro-
teins we use UniProtKB identifiers. In the UniProtKB
each protein entry has two identifiers [66]: 1) an acces-
sion number (AC) that is assigned to each amino acid
sequence upon inclusion into the UniProtKB; and 2) the
Entry name (a.k.a. ID), which often contains biologic-
ally relevant information. Table 1 contains examples of
the manual annotation and normalisation process
performed; Table 1 illustrates the lack of standardisation
for protein names in the literature.
The next two examples illustrate the subtask of assign-
ing UniProtKB identifiers to the genes/proteins anno-
tated within the 25 PubMed articles corpus:
 In the abstract of the PubMed article with ID =
15,249,501 the term heat shock protein-27
(HSP27) is recognised as a gene/protein name, and
subsequently mapped to UniProtKB AC = P04792.
 In the abstract of the PubMed article with ID =
21,938,407 the term heat shock protein 70 KDa is
recognised as a gene/protein name, and
subsequently mapped to UniProtKB AC = P08107.
However, on the 27th May, 2015 this UniProtKB
entry became obsolete (see [67]), and it is now
found with the UniProtKB AC equals P0DMV8 and
P0DMV9. Therefore, the term heat shock protein
70 KDa is mapped to both UniProtKB ACs, i.e.
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 5 of 24
P0DMV8 and P0DMV9. This example can be seen
as a case where some level of ambiguity remains, i.e.
more than one UniProtKB AC is assigned to the
gene/protein term manually annotated.
The current study is limited to 25 PubMed titles and
abstracts, so we acknowledge that some level of ambigu-
ity may remain. We also acknowledge that one straight-
forward way to disambiguate is by reading the full paper
to find the extra information that may aid in uniquely
identifying the gene/protein of interest. For example,
considering the full text of the article with PubMed ID
= 21,938,407, it is clear that the term heat shock protein
70 KDa refers to the protein name Heat shock 70 kDa
protein 1A that has the UniProtKB AC = P0DMV8.
Thus, the full article helps to clarify the ambiguity.
The auxiliary file TermsMapped.xls contains the de-
tails of the normalisation performed, i.e. the correlation
of the 105 terms annotated to the 79 unique UniProtKB
entries, where both the UniProtKB identifiers AC and
ID are shown.
The cardiovascular disease ontology (CVDO)
CVDO provides the schema to integrate the omics data
from multiple biological resources, such as the Uni-
ProtKB, the miRBase [68] from EMBL-EBI, the Human
Metabolome Database [69] and the data gathered from
various scientific publications (e.g. 34 full-paper omics
studies from the sysVASC systematic review and their
auxiliary files).
At the core of CVDO is the Ontology for Biomedical
Investigations [70] along with other reference ontologies
produced by the OBO Consortium, such as the Protein
Ontology (PRO) [71], the Sequence Ontology (SO) [72],
the three Gene Ontology (GO) sub-ontologies [73], the
Chemical Entities of Biological Interest Ontology [74],
the Cell Ontology [75], the Uber Anatomy Ontology [76],
the Phenotypic Quality Ontology [77], and the Relation-
ship Ontology [78].
For a protein, the CVDO takes as its IRIs the PRO
IRIs while also keeping the UniProtKB entry identifiers
(i.e. the AC and ID) by means of annotation properties.
UniProtKB entry updates could mean changes in the
amino acid sequence and/or changes in the GO annota-
tions. The CVDO represents formally the associations
between a protein class and classes from the three GO
sub-ontologies. In the CVDO there are 172,121 Uni-
ProtKB protein classes related to human, and 86,792
UniProtKB protein classes related to mouse. Taking into
account the GO annotations for a protein, so far, a total
of only 8,196 UniProtKB protein classes from mouse
and human have been identified as of potential interest
to sysVASC.
The CVDO incorporates information about genes and
proteins from the UniProtKB, where no alternative
names for genes and proteins are available in the Uni-
ProtKB downloadable files [79]. In terms of knowledge
modelling, the CVDO shares the protein/gene represen-
tation used in the Proteasix Ontology (PxO) [80]. The
SubClassOf axioms for the PxO protein class in OWL
Manchester Syntax [81] are shown in Fig. 1. The axiom
protein SubClassOf (has_gene_template some gene) is a
class expression that conveys an existential restriction
over the object property has_gene_template from the
PRO, where the class protein (PR:000000001) is from
the PRO and the class gene (SO:0000704) is from the
SO. Hence, in the CVDO, as in the PxO, the association
between a gene and a protein (gene product) is formally
represented with the axiom pattern protein SubClassOf
(has_gene_template some gene) and this is the key
Table 1 Exemplifying the identification of genes/proteins mentioned within the 25 PubMed titles/abstracts: Terms from PubMed
abstract/title from the small-annotated corpus (first column) mapped to UniProtKB ACs (second column) and their corresponding
values for skos:altLabel annotation properties of the PxO protein classes (third column)
Term(s) from PubMed abstract/title UniProtKB AC skos:altLabel for PxO protein classes
?(1)-antitrypsin
alpha-1-antitrypsin
P01009 SERPINA1 (P01009; A1AT_HUMAN) Alpha-1-antitrypsin
annexin 4 P09525 ANXA4 (P09525; ANXA4_HUMAN) Annexin A4
superoxide dismutase 3 P08294 SOD3 (P08294; SODE_HUMAN) Extracellular superoxide dismutase [Cu-Zn]
OLR1 P78380 OLR1 (P78380; OLR1_HUMAN) Oxidized low-density lipoprotein receptor 1
glutathione transferase P30711 GSTT1 (P30711; GSTT1_HUMAN) Glutathione S-transferase theta-1
FJX1 Q86VR8 FJX1 (Q86VR8; FJX1_HUMAN) Four-jointed box protein 1
Class: protein
SubClassOf: 
'amino acid chain',
has_gene_template some gene
'located in' some cellular_component,
'participates in' some biological_process,
'has function' some molecular_function
Fig. 1 The SubClassOf axioms for the PxO protein class in OWL
Manchester Syntax
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 6 of 24
knowledge along with the protein and gene names (i.e.
lexical content) that we propose to exploit to provide
more context for the target terms in Experiment II (see
subsection Setup of Experiment I and Experiment II for
a gene/protein synonym detection task for details).
For a CVDO protein class, we can use its UniProtKB
identifier (i.e. AC or ID) to build SPARQL [82] SELECT
queries to retrieve: a) the protein class label; and b) the
gene class associated to the protein class by exploiting the
axiom pattern protein SubClassOf (has_gene_template
some gene). The auxiliary file TermsMapped.xls contains
the gene and protein class labels (i.e. rdfs:label) from the
CVDO for each of the 79 UniProtKB entries that are part
of the small-annotated corpus created.
In the PxO, the annotation property skos:altLabel from
the Simple Knowledge Organization System (SKOS) [83] is
assigned to each protein class that represents a UniProtKB
entry. The string value for this annotation property also
contains the identifiers (UniProtKB AC and ID) that
pinpoint the protein uniquely and has typically the format
gene symbol (UniProtKB AC; UniProtKB ID) protein
name. Hence, in the PxO, the association between a
protein and a gene is modelled at the logical level with a
SubClassOf axiom as well as information attached to the
protein class (UniProtKB entry) with no effect on the
logical aspects of the class. Table 1 shows how the PxO can
provide more context for the terms annotated, e.g. SER-
PINA1 is the gene symbol for the protein name Alpha-1-
antitrypsin.
The 14 M PubMed dataset
We downloaded the MEDLINE/PubMed baseline files
for 2015 and the up-to-date files through 8th June 2016.
To transform the XML PubMed files (see [84] for details
of the XML data elements) into a corpus of suitable text-
ual input for Skip-gram and CBOW, two pre-processing
steps are carried out. For the first step, we created a pro-
cessing pipeline that uses open-source software in Py-
thon, such as Beautiful soup [85] and the open-source
Natural Language Toolkit (NLTK) [86].
When pre-processing the textual input for CBOW and
Skip-gram, it is common practice to transform the text
into lower-case and to remove systematically all num-
bers and punctuation marks. This is, however, unsuitable
when dealing with protein/gene nomenclature and crit-
ical information will be lost if this practice is followed.
Tanabe et al. [87] highlight gene and protein names
often contain hyphens, parentheses, brackets, and other
types of punctuation. Furthermore, capitalisation and
numerals are essential features in symbols or abbrevia-
tions. For instance, for human, non-human primates,
chickens, and domestic species, gene symbols contain
three to six alphanumeric characters that are all in up-
percase (e.g. OLR1), while for mice and rats the first
letter alone is in uppercase (e.g. Olr1). We therefore de-
cided to alter the commonly employed pre-processing
workflow. The Python processing examines the PubMed
XML files, locates the data elements of interest and ex-
tracts information contained within them while preserv-
ing uppercase and punctuation marks within a sentence
as well as numbers.
For the second step, we employed word2phrase within
the word2vec software package [88] to get n-grams. The
title and abstract (if available) of each PubMed publica-
tion are the basis to build the DSMs using Skip-gram
and CBOW.
Meaningful biomedical terms are typically multi-
words; therefore, to obtain better performance titles/ab-
stracts need to be transformed into n-grams. To indicate
that more than one word and/or numbers are part of a
term, white space is replaced by _ indicating that the
multiple words (and/or numbers) constitute a term.
Once pre-processing is complete, we have a biomed-
ical unannotated corpus of 14,056,762 PubMed publica-
tions (titles and available abstracts) with dates of
publication between 2000 and 2016 (termed PubMed
14 M for short). The complete list of PubMed IDs can
be downloaded from [89].
Setup of two experiments for a gene/protein synonym
detection task
This subsection starts by detailing the creation of the
word embeddings with CBOW and Skip-gram using the
14 M PubMed dataset. Next, we detail the setup of two
experiments using a small-annotated corpus of gene/
protein names and we also specify the exact contribution
of the CVDO in Experiment II.
Creation of word embeddings with CBOW and Skip-gram
From CBOW and Skip-gram we typically obtain: 1) a
lexicon (i.e. a list of terms) in textual format that is
constructed from the input data; and 2) the vector
representations learnt for the terms, i.e. the word
embeddings.
The basic Skip-gram formulation uses the softmax
function [17]. The hierarchical softmax is a computa-
tionally efficient approximation of the full softmax. If W
is the number of words in the lexicon, hierarchical soft-
max only needs to evaluate about log2(W) output nodes
to obtain the probability distribution, instead of needing
to evaluate W output nodes. This study uses hierarchical
softmax.
In traditional distributional methods, there are a small
number of variables known as the hyperparameters of
the model. For example, the parameters for the Dirichlet
priors in an LDA model are often referred to as hyper-
parameters. Levy et al. [29] acknowledges that some
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 7 of 24
hyperparameters are tuneable, while others are already
tuned to some extent by the algorithms designers.
Levy et al. [29] distinguish three types of hyperpara-
meters: 1) pre-processing hyperparameters, 2) associ-
ation metric hyperparameters, and 3) post-processing
hyperparameters. As this study does not modify the
resulting term vectors, we present the setup of the pre-
processing and association metric hyperparameters im-
plemented in word2vec. We refer the reader to Levy et
al. [29] and Chiu et al. [42] that study in detail the effect
of various hyperparameter configurations.
Four pre-processing hyperparameters need to be
considered:
 Vector dimension  In word2vec the default value is
100. We setup the dimensional representation of
terms to 300. This value is much lower than Levy et
al. [29] that uses 500.
 Context window size  In word2vec the default value
is 5. We setup the window size to 10, similarly to
Levy et al. [29]. word2vec implements a weighting
scheme where a size-10 window weights its contexts
by 10/ 9, 10/ 10, , 2/ 1, 10/ 10.
 Subsampling  This method dilutes very frequent
words [29]. As recommended by Mikolov et al. [17],
and like Levy et al. [29], we use the value 1e-5. In
word2vec subsampling happens before the textual
input is processed and a value zero means that sub-
sampling is switched off.
 Minimum count (min-count)  Terms that occur
only a few times can be discarded and consequently
some terms will not have vector representations. In
word2vec the default value of min-count is 5, which
is the value taken in this study. Chiu et al. [42] show
that this hyperparameter has a small effect on
performance.
The two association metric hyperparameters are:
 Negative sampling  In word2vec by default negative
sampling is zero (i.e. not used). However, Skip-gram
with negative sampling is acknowledged to provide
state-of-the-art results on various linguistic tasks
[29]. A higher negative sampling means [29]: a)
more data and better estimation; and b) negative
examples are more probable. This study does not use
negative sampling, and therefore, performance gains
for Skip-gram should be relatively easy to obtain if
negative sampling is also applied. In other words, it
can be argued that by no using negative sampling we
are reducing the performance for Skip-gram.
 Learning rate  This is a smoothing technique. In
word2vec the default value of alpha is 0.025, which
is used in this study.
In this study to create word embeddings with Skip-
gram and CBOW, we use a Supermicro with 256GB
RAM and two CPUs Intel Xeon E52630 v4 at 2.20GHz.
For the 14 M PubMed dataset execution time is less
than 1 hour for CBOW and more than 10 hours for
Skip-gram.
Setup of experiment I and experiment II for a gene/protein
synonym detection task
In the small-annotated corpus with 105 terms mapped
to 79 UniProtKB entries, not all the UniProtKB entries
have the same number of terms manually annotated
from the 25 PubMed titles and abstracts. Considering
the origin of the target terms and driven by a pragmatic
approach, the 79 UniProtKB AC are divided into two
sets that participate in each experiment as follows:
 Experiment I: the UniProtKB entries that participate
in this experiment typically have gene/protein terms
manually annotated from the PubMed titles/
abstracts. The target terms for this experiment are
only gene/protein terms manually annotated with
vector representations.
 Experiment II: the UniProtKB entries that participate
in this experiment typically have gene/protein terms
manually annotated from the PubMed titles/
abstracts for which there is not a vector
representation and/or the CVDO can provide more
biological knowledge (e.g. the gene symbol does not
appear among the terms manually annotated for the
protein/gene of interest). The target terms for this
experiment are a combination of: a) gene/protein
terms manually annotated from PubMed titles and/
or abstracts, and b) terms taken from the CVDO
protein and gene class labels. The terms from the
CVDO can provide more context to the terms
manually annotated to take full advantage of the
biological knowledge represented within the CVDO.
The list of acceptable alternative free-text terms (i.e.
candidate terms) for genes/proteins is made of terms
from the word embeddings with the largest cosine value
(the normalized dot product of two vectors) with the
target term. In this study, we limit the list to the twelve
candidate terms with the highest cosine value (i.e. the
top twelve ranked) and we give more importance to the
three candidate terms with the highest cosine value (i.e.
the top three ranked) within the list. We based our deci-
sion in cognitive theories such as that of Novak and
Cañas [90] that states if we give learners 1012 familiar
but unrelated words to memorize in a few seconds, most
will recall only 59 words. If the words are unfamiliar,
such as technical terms introduced for the first time, the
learner may do well to recall correctly two or three of
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 8 of 24
these. Conversely, if the words are familiar and can be
related to knowledge the learner has in her/his cognitive
structure, e.g. months of the year, 12 or more may be eas-
ily recalled.
Taking into account the word embeddings obtained,
the final setup of both experiments is as follows:
 Experiment I: this experiment involves 64
UniProtKB entries and 85 target terms, where
typically multiple target terms were tried for the
same UniProtKB entry. For each target term, a list
of the top twelve ranked candidate terms (highest
cosine similarity) is obtained from the word
embeddings, and thus, this experiment has 1020
pairs of terms (the target and the candidate) to be
assessed by the four raters with CBOW and Skip-
gram.
 Experiment II: this experiment involves 63
UniProtKB entries and 68 target terms, where the
correspondence between target terms and
UniProtKB entries is almost one-to-one. For each
target term, a list of the top twelve ranked candidate
terms (highest cosine similarity) is obtained from
the word embeddings, and thus, this experiment has
816 pairs of terms (the target and the candidate) to
be assessed by the four raters with CBOW and Skip-
gram.
A total of 48 UniProtKB entries participate in both
Experiment I and II. In Experiment I there are 16 Uni-
ProtKB entries that do not participate in Experiment II,
for those that the CVDO cannot provide much more
added value as they already have the protein name or
the protein name and the gene symbol. In Experiment II
there are 15 UniProtKB entries that do not participate in
Experiment I, those typically correspond to terms anno-
tated from PubMed title/abstracts that do not have a
vector and for which the CVDO may supply target terms
for them by taking terms from the CVDO protein class
expressions and labels.
To clarify the similarities and differences between the
two experiments as well as the exact contribution of
CVDO in Experiment II, we introduce a simple categor-
isation that can be applied to: a) the terms from the
small-annotated corpus, which appear separated by the
character | and b) the target terms for the synonym de-
tection task, which appear separated by white space. The
simple categorisation introduced consists of five
categories:
1. Only gene symbol Term is the gene symbol. For
example: OLR1.
2. Gene symbol appears  A combination of terms
among which the gene symbol appears. An example
from the small-annotated corpus is C3|complement
C3. An example from the target terms for the syno-
nym detection task is: oxidized_low-density_lipopro-
tein receptor_ 1 OLR1.
3. Refer protein name  Terms that refer to the protein
name. An example from the small-annotated corpus
is CTRP1|C1q/TNF-related protein 1|adipokine
C1q/TNF-related protein (CTRP). An example from
the target terms for the synonym detection task is
collagen_type_1.
4. Only protein name The exact protein name as it
appears in the UniProtKB. An example from the
target terms for the synonym detection task is
glutathione_S-transferase theta-1.
5. Terms from protein name Terms taken from the
protein name as it appears in the UniProtKB. An
example from the target terms for the synonym
detection task is c1q tumor_necrosis_factor.
Both categories Only protein name and Terms from
protein name are applied only to the target terms and
take into account the protein name as it appears in the
UniProtKB, which is the lexical content from protein
class labels (i.e. rdfs:label) within the CVDO.
Table 2 for Experiment I and Table 3 for Experiment
II apply the simple categorisation proposed to the terms
from the small-annotated corpus (first column in the
Tables); and to the target terms for the synonym detec-
tion task (second column in the Tables). The third col-
umn represents the number of target terms. For
example, in Table 2 for Experiment I the higher number
Table 2 Setup for Experiment I: The simple categorisation
introduced (see Setup of Experiment I and Experiment II for a
gene/protein synonym detection task) has been applied to the
terms from PubMed abstract/title from the small-annotated
corpus (first column) as well as to the target terms (second
column). Each row of the third column contains the number
of target terms for the experiment taking into account the
categories that appear in the first and second column
Simple categorisation introduced
Terms from PubMed titles/abstracts Target terms n
Gene symbol appears Gene symbol appears 5
Gene symbol appears Only gene symbol 13
Gene symbol appears Only protein name 3
Gene symbol appears Refer protein name 2
Gene symbol appears Terms from protein name 2
Only gene symbol Only gene symbol 21
Refer protein name Gene symbol appears 1
Refer protein name Only protein name 16
Refer protein name Refer protein name 18
Refer protein name Terms from protein name 4
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 9 of 24
of target terms corresponds to the category Only gene
symbol with 34 target terms, where 13 of them corres-
pond to terms from the small-annotated corpus belong-
ing to the category Gene symbol appears.
Table 3 for Experiment II has a fourth column to clearly
indicate the origin of the terms added by the CVDO to
the target terms. In Table 3 for Experiment II the higher
number of target terms corresponds to the category Gene
symbol appears with 53 target terms, where 27 of them
correspond to terms from the small-annotated corpus be-
longing to the category Refer protein name. For these 27
target terms, the CVDO added terms from protein name
and gene symbol, and therefore, exploiting the protein
class expressions within the CVDO.
In the rows of the fourth column of Table 3, the
symbol (R) means that the protein class expressions
within the CVDO are used to add terms to the target
terms. Hence, 63 of the 68 target terms (i.e. 93%) exploit
the relationship between genes and proteins modelled in
the CVD ontology. Only 5 target terms (i.e. 7%) exploit
lexical content from protein class labels.
Human evaluation and metrics to assess the performance
of Skip-gram and CBOW in experiment I and II
To assess how many free-text candidate terms within
the list can be actually considered to be term variants
(e.g. synonyms, abbreviations, and variant spellings) we
rely on four domain experts to rate pairs of terms (the
target and the candidate) and assess whether the candi-
date term is a full-term variant (FTV for short), a
partial-term variant (PTV for short), or a non-term vari-
ant (NTV for short, meaning none of the previous
categories). The same four raters (A, B, C, and D)
assessed the 3672 pairs of terms (target term and candi-
date term) in Experiments I and II. Raters A and D are
trained terminologists who work in biomedicine; Raters
B and C are bio-curators, who at the time of the study
worked on biochemical knowledge extraction from text-
ual resources.
We established a strict criterion to mark each pair of
terms (the target and the candidate) from the CBOW
and Skip-gram word embeddings. Following Nenadic et
al. [91], a candidate term is marked as FTV only when
the term falls within the following types of term variation:
a) orthographic, b) morphological, c) lexical, d) structural,
or e) acronyms and abbreviations. Considering the bio-
medical value of phraseological expressions (e.g. ankyrin-
B_gene or CBS_deficiency), they are marked as PTV if
they refer to the same protein/gene of interest.
In order to calculate precision and recall, which are
well-known metrics for evaluating retrieval (classifica-
tion) performance, one set of annotations should be con-
sidered as the gold standard [92]. In this study, we
advocate a voting system as we have four annotators/
raters and two of them are bio-curators. Hence, we do
not follow studies like Thompson et al. [93], which cal-
culate precision and recall, and use F score (i.e. a metric
that combines precision and recall) as a way of calculat-
ing inter-annotator agreement.
When having two raters/coders/annotators, the inter-
annotator agreement is typically calculated using Cohens
Kappa measure [94]. For more than two coders, Fleiss
[95] proposed a coefficient of agreement that calculates
expected agreement based on the cumulative distribution
Table 3 Setup for Experiment II and contribution of the CVDO: The simple categorisation introduced (see Setup of Experiment I and
Experiment II for a gene/protein synonym detection task) has been applied to the terms from PubMed abstract/title from the small-
annotated corpus (first column) as well as to the target terms (second column). Each row of the third column contains the number
of target terms for the experiment taking into account the categories that appear in the first and second column
Simple categorisation introduced
Terms from PubMed titles/abstracts Target terms n Terms added by CVDO to the target terms
Gene symbol appears Gene symbol appears 6 Terms from protein name (R)
Gene symbol appears Only protein name 1 Protein name (R)
Gene symbol appears Refer protein name 1 Terms referring to the protein name (R)
Gene symbol appears Terms from protein name 2 Terms from protein name (R)
Only gene symbol Gene symbol appears 20 Terms from protein name (R)
Only gene symbol Only protein name 4 Protein name (R)
Refer protein name Gene symbol appears 27 Terms from protein name and gene symbol (R)
Refer protein name Only gene symbol 2 Gene symbol (R)
Refer protein name Only protein name 2 Protein name
Refer protein name Refer protein name 1 Terms referring to the protein name
Refer protein name Terms from protein name 2 Terms from protein name
The fourth column indicates the terms added by the CVDO, when the symbol (R) appears it means that the protein class expressions within the CVDO are used
to add terms to the target terms
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 10 of 24
of judgments by all coders [96]. This measure of inter-
annotator agreement is also known as Fleisss multi-? as it
can be interpreted as a generalisation of Scotts ? [97]. It
should be noted that when all disagreements are consid-
ered equal, as in this study, Fleisss multi-? is nearly identi-
cal to Krippendorff s ? [98], which is an agreement
coefficient recommended in computational linguistics for
coding tasks without involving nominal and disjoint cat-
egories [96]. Hence, we adhere to Artstein and Poesio [96]
who state that it is better practice in computational
linguistics to use generalised versions of the coefficients
(e.g. Fleisss multi-?) instead of measuring agreement sep-
arately for each pair of coders (Cohens Kappa measure),
and then report the mean.
In this study three metrics are used to assess the per-
formance of CBOW and Skip-gram for the synonym de-
tection task. The first metric is the area under the
Receiver Operating Characteristics (ROC) curve for a
binary classifier. FTV and PTV can be merged into one
category called term variant or TV for short. Hence, the
multiple class classification problem can be reduced to
three binary classification problems: 1) FTV and non-
FTV; 2) PTV and non-PTV; and 3) TV and non-TV.
This study uses ROC curves instead of precision-recall
curves, as ROC curves do not change if the class distri-
bution is different [99]. The second metric is the median
of the rank that was used by Pilehvar and Collier [18] in
a synonym and hypernym identification tasks with Skip-
gram. The third metric is the number of term variants (i.e.
FTV and/or PTV) found for each of the 79 UniProtKB en-
tries within the small-annotated corpus of gene/protein
names from 25 PubMed articles.
Receiver operating characteristics (ROC) curve and the area
under the ROC curve (AUC)
To compare classifiers, calculating the area under the
ROC curve, the so-called AUC [100102], is a common
method. Fawcett [99] defines the ROC curve as a tech-
nique for visualizing, organizing and selecting classifiers
based on their performance. As Bradley [100] states
when comparing a number of different classification
schemes it is often desirable to obtain a single figure as a
measure of the classifier's performance. The AUC can
be interpreted as a probability of correct ranking as esti-
mated by the Wilcoxon statistic [101]. Furthermore, as
Hand and Till [102] highlight, the AUC is independent of
costs, priors, or (consequently) any classification threshold.
A ROC curve has two dimensions, where typically TP
rate is plotted on the Y axis and FP rate is plotted on the
X axis [99]. TP rate stands for true positive rate (a.k.a.
hit rate or recall or sensitivity) and is the proportion of
positives correctly classified as positives; FP rate stands
for false positive rate (a.k.a. false alarm rate) and is the
proportion of negatives that are incorrectly classified as
positive. For the perfect classifier TP rate = 1 and FP
rate = 0. In the ROC curves, the diagonal line (y = x) is
also plotted which represents random guessing [99] and
acts as the baseline for ROC. A random classifier typic-
ally slides back and forth on the diagonal [99].
As the candidate terms evaluated for the human raters
are ranked (highest cosine value), we have the category
assigned by the rater to each candidate term (FTV, PTV,
or NTV) as well as the position that the candidate term
has in the top twelve ranked list. Firstly, for each experi-
ment and rater, we created a table with twelve rows and
three columns: frequency of FTV, frequency of PTV, and
frequency of NTV. For example, the frequency of FTV
column accounts for the number of times that a rater
assigned FTV for the term in the ith position in the list,
with i = [1,, 12]. Secondly, we calculated the cumula-
tive frequency, and thus, three more columns were
added. The cumulative frequency is calculated in de-
scending order, where the value of the cumulative fre-
quency for the ith position in the list adds to the value
from the frequency column in the ith position, the value
of the cumulative frequency for the (i-1)th position in
the list. Thirdly, we calculated the cumulative rate, and
therefore, three more columns were added. For example,
the cumulative rate of FTV column is calculated by div-
iding the values of the cumulative frequency of FTV col-
umn by the total number of FTV assigned by the rater.
Hence, the last value in any of the cumulative rate col-
umns (12th position) is equal to 1. In the ROC curves,
we plot the cumulative rates obtained. Hence, the ROC
curves for FTV, PTV, and TV end at (1, 1).
The values for the AUC go from zero to one. Random
guessing will have an AUC = 0. 5 and no realistic classi-
fier should have an AUC less than 0. 5 [99]. We plot
ROC curves for FTV, PTV, and TV and calculate the
AUC for each rater and experiment.
The median of the rank per human rater
Based on the domain expert category assigned (FTV,
PTV, or NTV) to each candidate term from the word
embeddings, as well as the position that the candidate
term has in the top twelve ranked list (highest cosine
similarity), we can calculate the median of the rank for
FTV and PTV per rater. A lower median means that the
terms marked as terms variants (full or partial) appear at
the beginning of the list.
Number of UniProtKB ACs and CVDO classes with a term
variant
Based on the 79 unique UniProtKB entries from the
small-annotated corpus we implement a voting system
based on raters judgement and determine for how many
of the 79 UniProtKB entries mapped to CVDO classes,
term variants were found. The voting system takes the
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 11 of 24
domain expert category assigned (FTV, PTV, or NTV)
and considers that a candidate term from the top twelve
ranked list is an FTV if at least one of the four raters
assigned the category FTV once. Likewise, and more
generally, if at least one of the four raters marks a candi-
date term from the top twelve ranked list as FTV or
PTV, the voting system concludes a TV has been found.
Results
We start by illustrating the results obtained in Experi-
ment I and II with CBOW and Skip-gram. Next we re-
port the human inter-annotator agreement and the
results obtained for the three metrics to assess the per-
formance of CBOW and Skip-gram in the gene/protein
synonym detection task.
Exemplifying the results obtained for the gene/protein
synonym detection task in experiment I and II
Each auxiliary file - CBOW.xls and Skip-gram.xls - con-
tains the 1836 pairs of terms (target term and candidate
term) from the word embeddings created, along with the
cosine similarity obtained for each pair of terms. Each
file includes the list of the top twelve ranked candidate
terms (highest cosine similarity) per target term, where
the last four columns have the human judgement (FTV,
PTV, or NTV) by the four raters A-D. Each target term:
a) relates to a UniProtKB entry that has a UniProtKB
identifier (i.e. the UniProtKB AC column) and also a
string value for the annotation property skos:altLabel for
the PxO protein class, b) has a unique identifier in col-
umn nQ that also appears in the auxiliary file Terms-
Mapped_votingSystem.xls, c) contains at least one term
from the small-annotated corpus (Term from the
PubMed titles/abstracts column), and d) participates in
Experiment I (abbreviated as Exp I) or Experiment II
(abbreviated as Exp II) as indicated in the Experiment
column.
We use target terms from the auxiliary files to illus-
trate the ranked list of the top twelve candidate terms
(highest cosine similarity) for gene/protein names ob-
tained from the word embeddings created with CBOW
and Skip-gram for Experiments I and II.
Table 4 shows the list of the top twelve candidate
terms (highest cosine similarity) obtained with CBOW
and Skip-gram word embeddings in Experiment I for the
target term KLF7, which is a gene symbol and appears
as such in the abstract of the PubMed article with ID =
23,468,932. For CBOW, all four raters agree that there is
not a full or partial gene/protein term variant (i.e. FTV
or PTV) among the list of candidate terms; in other
words, all the top twelve ranked candidate terms for
CBOW were marked as NTV by the four raters. For
Skip-gram, all four raters agree that: a) the candidate
term in the second position in the list is an FTV, and b)
the candidate term in the third position in the list is a
PTV. Hence, in Experiment I for the target term KLF7,
CBOW could not find a TV while Skip-gram found an
FTV and also a PTV among the top three ranked candi-
date terms in the list. From a biological point of view,
the target term KLF7 denotes a human gene, while the
candidate term in the second position in the list Klf7
denotes the equivalent gene in mice. The genes KLF7
and Klf7 are orthologs according to the NCBI [103]. The
candidate term in the third position in the list Klf7(?/?)
refers to mice which are homozygous for the Klf7 gene
knockout. Hence, the pre-processing of the 14 M PubMed
dataset that keeps uppercase, punctuation marks, and
numbers, demonstrably preserves valuable biological
information.
The term OLR1, which is a gene symbol, appears as
such in the abstract of the PubMed article with ID =
22,738,689. Using OLR1 as the target term in Experi-
ment I for CBOW and Skip-gram, no candidate terms
from the word embeddings were suitable as FTV or PV
according to all four raters.
In Experiment II, the term oxidized_low-density_lipo-
protein receptor_1 that corresponds to the protein name
is added to the gene symbol OLR1 to create a target
term. Table 5 shows the top twelve ranked candidate
terms obtained by CBOW and Skip-gram word embed-
dings in Experiment II using these two terms oxidize-
d_low-density_lipoprotein receptor_1 OLR1 as the target
term. Therefore, the target contains a term that exploits
knowledge within the CVDO and, more concretely, the
association relationship formally represented between
genes and proteins. As the CVDO provides more con-
text, in Experiment II with both CBOW and Skip-gram,
suitable term variants (FTV as well as PTV) were found
for the protein/gene name.
Tables 4 and 5 show higher cosine values for Skip-
gram than CBOW. As cosine similarity gives an indica-
tion of how strongly semantically related is the pair of
terms (the target and the candidate), it seems natural
that Skip-grams finds more term variants than CBOW.
Table 6 shows the categories FTV, PTV, or NTV
assigned by the four human raters (A-D) to the top
twelve ranked candidate terms obtained for Skip-gram
in Experiment II using two terms oxidized_low-densi-
ty_lipoprotein receptor_1 OLR1 as the target term. This
list of the top twelve ranked candidate terms appears in
the right-hand side of Table 5. The last three columns of
the Table exemplify the voting system (abbreviated as
VS) applied: full term variant (VS: FTV column), full
term variant among the top three (VS: FTV for top three
column), and full and/or partial term variant (VS: TV
column).
Two rows appear with a grey background in Table 6.
They indicate the process of manually assigned categories
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 12 of 24
to be error-prone as Rater C assigned NTV to the candi-
date term in the eighth position in the list oxidized_low-
density_lipoprotein_(ox-LDL) while marking PTV for the
candidate term in the ninth position, oxidized_low-densi-
ty_lipoprotein_(oxLDL). From visual inspection, the only
difference in these two candidate terms is the appearance
of, or lack of, a -. It should be noted that Raters A, B, and
D mark both candidate terms in the list equally, although
they differ in the category assigned. The biological back-
ground knowledge of Raters B and C (curators) and their
impact on the manual categorisation process can be de-
duced from Table 6. Gene OLR1 has a well-known alias
LOX-1, and thus, Raters B and C marked the candidate
terms as FTV if LOX-1 appears alone or PTV if LOX-1
appears in combination with other term(s); however,
Raters A and D marked all the candidate terms as NTV
where LOX-1 appears.
Human evaluation and metrics to assess the performance
of Skip-gram and CBOW in Experiment I and II
We start reporting on the inter-annotator agreement coeffi-
cients for the four raters. For pairwise inter-annotator agree-
ment (the Cohens Kappa measure) per experiment and
model, we refer the reader to auxiliary file pairwiseIAA.xls.
All the inter-annotator agreement coefficients are calculated
with the implementations from the NLTK [86]:
Table 4 Exemplifying results for Experiment I: Top twelve ranked candidate terms (highest cosine similarity) from the word
embeddings created with CBOW and Skip-gram for the target term KLF7 that appears in the abstract of the PubMed article with
ID = 23,468,932
CBOW Skip-gram
Rank Candidate terms from word embeddings Cosine Candidate terms from word embeddings Cosine
1 MoKA 0.376371 Prrx2 0.601920
2 pluripotency-associated_genes 0.335113 Klf7 0.592946
3 Sp1_regulates 0.334092 Klf7(?/?) 0.590523
4 LOC101928923 0.333423 RXRG 0.589875
5 p107_dephosphorylation 0.331689 LOC101928923 0.585979
6 PU_1 0.329925 SOX-17 0.585295
7 histone_demethylase 0.323529 rs820336 0.585094
8 gene_promoter 0.321640 GLI-binding_site 0.581073
9 homeobox_protein 0.319997 Tead2 0.580012
10 histone_arginine 0.315875 hHEX 0.579868
11 transfated 0.314202 ACY-957 0.579542
12 are_unable_to_repress 0.313112 ETS1 0.577272
Table 5 Exemplifying results for Experiment II: Top twelve ranked candidate terms (highest cosine similarity) from the word
embeddings created with CBOW and Skip-gram using two terms as target: OLR1 from the abstract of the PubMed article with ID
= 22,738,689; and oxidized_low-density_lipoprotein receptor_ 1 that is the CVDO protein class name (rdfs:label) for the CVDO class
gene with name (rdfs:label) OLR1. Hence, the target term exploits the protein class expressions within the CVDO
CBOW Skip-gram
Rank Candidate terms from word embeddings Cosine Candidate terms from word embeddings Cosine
1 atherogenesis 0.469405 lectin-like_oxidized_low-density_lipoprotein 0.688603
2 atherosclerosis 0.465861 (LOX-1)_is 0.672042
3 CD36 0.439280 atherosclerosis_we_investigated 0.669050
4 LOX-1 0.424173 receptor-1 0.664891
5 atherosclerotic_lesion_formation 0.416537 lectin-like_oxidized_LDL_receptor-1 0.663988
6 vascular_inflammation 0.414620 lOX-1_is 0.660110
7 inflammatory_genes 0.411186 human_atherosclerotic_lesions 0.657075
8 atherosclerotic_lesions 0.405906 oxidized_low-density_lipoprotein_(ox-LDL) 0.655515
9 monocyte_chemoattractant_protein-1 0.398739 oxidized_low-density_lipoprotein_(oxLDL) 0.654965
10 plaque_destabilization 0.398201 (LOX-1) 0.652099
11 oxidized_low-density_lipoprotein_(oxLDL) 0.397967 proatherosclerotic 0.651571
12 atherosclerosis_atherosclerosis 0.396677 receptor-1_(LOX-1)_is 0.649000
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 13 of 24
 Using data from auxiliary file CBOW.xls, the Fleisss
multi-? for the four raters in Experiment I is
0.763205 and for Experiment II is 0.730869. The
Krippendorff s ? for the four raters in Experiment I
is 0.763211 and for Experiment II is 0.730874.
 Using data from auxiliary file Skip-gram.xls, the
Fleisss multi-? for the four raters in Experiment I is
0.794919 and for Experiment II is 0.673514. The
Krippendorff s ? for the four raters in Experiment I
is 0.794938 and for Experiment II is 0.674181.
As expected, the values obtained for the Fleisss
multi-? and the Krippendorff s ? for the four raters
are nearly identical. The inter-annotator agreement is
lower for Experiment II, which is more challenging in
terms of biological background knowledge. Camon et
al. [104] reports that the chance of curator agreement
is 39% to 43% when annotating proteins in the Uni-
ProtKB with terms from the GO. Hence, inter-
annotator agreement from 0.6734 (lowest value for
Fleisss multi-?) to 0.7949 (highest value for Fleisss
multi-?) appears reasonable.
Receiver operating characteristics (ROC) curve and the area
under the ROC curve (AUC)
Using data from auxiliary files CBOW.xls and Skip-
gram.xls, we plotted the ROC curves. For each Rater
A-D the ROC curves are shown in Figs. 2, 3, 4 and 5 re-
spectively. The ROC curves on the left-hand side plot
FTV, PTV, and TV (i.e. the combination of FTV and
PTV) for CBOW in Experiment I (abbreviated as Exp I)
and Experiment II (abbreviated as Exp II). The ROC
curves on the right-hand side plot FTV, PTV, and TV
for Skip-gram in Experiment I and II.
Looking at the AUC values for FTV, PTV, and TV in
Figs. 2, 3, 4 and 5, it can be observed that for all four
raters:
 The AUC values for FTV, PTV, and TV are always
greater than 0. 5 (i.e. better than random guessing) for
both CBOW and Skip-gram in Experiments I and II.
 The AUC values for TV are always greater in
Experiment II than in Experiment I for both CBOW
and Skip-gram.
 The AUC values for TV are always greater for Skip-
gram than for CBOW in both Experiment I and II.
 The AUC values for PTV are always greater in
Experiment II than in Experiment I for both CBOW
and Skip-gram.
 The higher AUC values are for FTV with both
CBOW and Skip-gram.
 The maximum AUC values are for FTV in
Experiment II with Skip-gram.
The only noticeable discrepancy is that for three
Raters (A, C, and D), CBOW has the higher AUC values
for FTV in Experiment II, and for Rater B the higher
AUC value for CBOW is for FTV in Experiment I.
Table 6 Exemplifying human judgements and voting system for Skip-gram: Categories FTV, PTV, or NTV assigned for the four human
raters (A, B, C, and D) to the top twelve candidate terms for the target term oxidized_low-density_lipoprotein receptor_ 1 OLR1 in
Experiment II using Skip-gram. The last three columns show the voting system (VS) applied for FTV (full term variant), FTV among
the top three, and TV (full and/or partial term variant). The two rows in grey background remark how two almost identical candidate
terms from the word embeddings are marked differently by rater C, and thus, the manual annotation by raters is error-prone
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 14 of 24
Considering the ROC curves and the AUC values, we
conclude that: a) Skip-gram outperforms CBOW in both
Experiments for the binary classification problem TV and
non-TV, b) both CBOW and Skip-gram perform best for
the binary classification problem FTV and non-FTV, c) the
best performance is for Skip-gram in Experiment II for the
binary classification problem FTV and non-FTV.
The median of the rank per human rater
Using data from the auxiliary files CBOW.xls and Skip-
gram.xls, we calculated the median of the rank. Table 7
shows the median of the rank for Raters A-D. From Table 7:
 For CBOW and Skip-gram in Experiment II, the
mean of the median of the rank for an FTV is 3.
 For CBOW and Skip-gram in Experiment I, the
mean of the median of the rank for an FTV is 4.
 For Skip-gram in Experiment I and II, the median of
the rank for a PTV is 6 for all four raters.
 For CBOW in Experiment II, the median of the rank
for a PTV is 5 for all four raters.
 For CBOW in Experiment I, the mean of the
median of the rank for a PTV is 6.
The higher the rank (i.e. lowest number) for an FTV
the better, and thus, results obtained for both CBOW
and Skip-gram indicate that CVDO can slightly improve
the ranking of an FTV from being among the top four
ranked candidate terms in Experiment I (without the aid
of the CVDO) to be among the top three ranked candi-
date terms in Experiment II (with the aid of the CVDO).
Fig. 2 ROC curves for rater A: left-hand side CBOW and right-hand side Skip-gram. Abbreviations: Exp I = Experiment I; Exp II = Experiment II
Fig. 3 ROC curves for rater B: left-hand side CBOW and right-hand side Skip-gram. Abbreviations: Exp I = Experiment I; Exp II = Experiment II
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 15 of 24
Number of UniProtKB entries mapped to CVDO gene and
protein classes with term variants
The auxiliary file TermsMapped_votingSystem.xls
contains the results of the voting system according to
the simple categorisation introduced (see subsection
Setup of Experiment I and Experiment II for a gene/
protein synonym detection task), which has been
applied to the terms from PubMed abstract/title from
the small-annotated corpus (Category for terms from the
title/abstract column) as well as to the target terms (Cat-
egory for target terms column). The file has 153 target
terms, each with a unique identifier in column nQ that
also appears in each auxiliary file under the column nQ.
Of these 153 target terms: 85 target terms for 64 Uni-
ProtKB entries are mapped to CVDO protein and gene
classes in Experiment I (abbreviated as Exp I), and 68
target terms for 63 UniProtKB entries are mapped to
CVDO protein and gene classes in Experiment II (abbre-
viated as Exp II). The last six columns display the presence
(i.e. value equals 1) or absence (i.e. value equals 0) for each
neural language model CBOW and Skip-gram of: full
term variants (i.e. FTV) among the top twelve ranked can-
didate terms for the target term; FTV among the top three
ranked candidate terms for the target term; and term vari-
ants (i.e. FTV and/or PTV) among the top twelve ranked
candidate terms for the target term.
Tables 811 take the data from auxiliary file Terms-
Mapped_votingSystem.xls and summarise the results
obtained.
Table 8 shows the overall performance of CBOW and
Skip-gram in Experiment I and II according to the
voting system, which can be summarised as follows:
Fig. 4 ROC curves for rater C: left-hand side CBOW and right-hand side Skip-gram. Abbreviations: Exp I = Experiment I; Exp II = Experiment II
Fig. 5 ROC curves for rater D: left-hand side CBOW and right-hand side Skip-gram. Abbreviations: Exp I = Experiment I; Exp II = Experiment II
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 16 of 24
 In Experiment I, Skip-gram finds term variants
among the top twelve ranked candidate terms
(Number TV column) for 89% of the 64 unique
UniProtKB entries mapped to CVDO gene and
protein classes, while CBOW finds term variants for
67%. Hence, using as target terms only terms from
PubMed titles/abstracts, the word embeddings
generated with the 14 M PubMed dataset can obtain
a list of term variants for gene/protein names.
 In Experiment II (with the aid of the CVDO), Skip-gram
finds term variants among the top twelve ranked candi-
date terms (Number TV column) for 95% of the 63
unique UniProtKB entries mapped to CVDO gene and
protein classes, while CBOW finds term variants for
78%. Hence, both neural language models Skip-gram
and CBOW provide more term variants (FTVs and/or
PTVs) if the CVDO is used to provide more context for
the target terms, and therefore increasing the chances of
finding suitable term variants for a gene/protein name.
 Combining the results of both experiments, Skip-gram
finds term variants (FTVs and/or PTVs) among the top
twelve ranked candidate terms for 97% of the 79 Uni-
ProtKB entries mapped to CVDO gene and protein
classes, while CBOW finds term variants for 81%.
 The number of term pairs in Experiment I is 1020
while in Experiment II it is 816, however more term
variants are found in Experiment II. Hence,
knowledge from the CVDO (i.e. mostly the protein
class expressions along with lexical content from
protein class labels) to make the term targets more
efficient as fewer term pairs are needed to produce
more term variants.
Table 9 shows the performance of CBOW and Skip-
gram according to the voting system and considers the
number of UniProtKB entries that participate in each ex-
periment. The third column contains the number of target
terms for the experiment considering the number of Uni-
ProtKB entries, where Experiment I has a higher number
of target terms per UniProtKB entry than Experiment II.
In Table 9 there are some rows with a grey background;
they refer to the 48 UniProtKB entries that participate in
both Experiments. There are 16 UniProtKB entries that
participate only in Experiment I and 15 UniProtKB entries
that participate only in Experiment II. Considering each
number of UniProtKB entries in an Experiment, it can be
observed that Skip-gram always outperforms CBOW and
finds more FTVs among the top twelve ranked candidate
terms (Number FTV column); FTVs among the top three
ranked candidate terms (Number FTV for the top three
column); and TVs among the top twelve ranked candidate
terms (Number TV column). By considering only the 48
UniProtKB entries that participate in both Experiments, it
can be observed that:
Table 7 Median of the rank for CBOW and Skip-gram in Experiments I and II for each of the four raters
Experiment Model Rater A Rater B Rater C Rater D
Median FTV Median PTV Median FTV Median PTV Median FTV Median PTV Median FTV Median PTV
I CBOW 4 5 3 7 4 6 4 6
II CBOW 3 5 4 5 3 5 3 5
I Skip-gram 4 6 4 6 4 6 4 6
II Skip-gram 3 6 4 6 3 6 3 6
Table 8 Overall performance of CBOW and Skip-gram according to the voting system: Number of unique UniProtKB entries and
number of term pairs for protein/gene names that are involved in Experiment I, II, and combined (i.e. merging Experiment I and II)
Voting system
Experiment Model Number of terms pairs Number of UniProtKB entries Number
FTV
Number FTV for top three Number TV
(%)
I CBOW 1020 64 31 21 43 (67%)
II CBOW 816 63 29 21 49 (78%)
I and II
combined
CBOW 1836 79 47 37 64 (81%)
I Skip-gram 1020 64 49 37 57 (89%)
II Skip-gram 816 63 56 51 60 (95%)
I and II
combined
Skip-gram 1836 79 71 63 77 (97%)
According to the voting system, for each model the last three columns show: the number of full term variants among the top twelve ranked candidate terms for
the UniProtKB entries (Number FTV column); the number of full term variants among the top three ranked candidate terms for the UniProtKB entries (Number FTV
for top three); and the number and % of term variants (i.e. FTV and/or PTV) among the top twelve ranked candidate terms for the UniProtKB entries (Number
TV column)
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 17 of 24
 CBOW finds TVs (Number TV column) among
the top twelve ranked candidate terms for 30 of
the 48 UniProtKB entries in Experiment I (i.e.
62%) and for 37 of the 48 UniProtKB entries in
Experiment II (i.e. 77%).
 Skip-gram finds TVs (Number TV column) among the
top twelve ranked candidate terms for 42 of the 48
UniProtKB entries in Experiment I (i.e. 87%) and for 45
of the 48 UniProtKB entries in Experiment II (i.e. 93%).
Tables 10 and 11 display the performance of CBOW and
Skip-gram for Experiments I and II respectively according
to the voting system and considering the categorisation in-
troduced (see subsection Setup of Experiment I and Ex-
periment II for a gene/protein synonym detection task)
that has been applied to the terms from PubMed abstract/
title from the small-annotated corpus (first column) as well
as to the target terms (second column). From these two ta-
bles, it can be observed:
 In Table 10, corresponding to Experiment I, the higher
number of target terms corresponds to the category
Only gene symbol (two rows with a grey background)
with a total of 34 target terms. CBOW finds TVs
among the top twelve ranked candidate terms (nTV
column) for 19 of them (i.e. 56%), while Skip-gram
finds TVs among the top twelve ranked candidate
terms (nTV column) for 29 of them (i.e. 85%).
 In Table 11, corresponding to Experiment II, the
higher number of target terms corresponds to the
Table 9 Performance of CBOW and Skip-gram - Experiment I and Experiment II: Number of unique UniProtKB entries mapped to
CVDO gene and protein classes that participated in Experiment I or II
The rows with grey background remark the 48 UniProtKB entries that participate in both Experiment I and II. Each row of the third column contains the number
of target terms for the experiment taking into account the number of UniProtKB entries. According to the voting system, for each model and experiment, the last
three columns show: the number of full term variants among the top twelve ranked candidate terms for the UniProtKB entries (Number FTV column); the number
of full term variants among the top three ranked candidate terms for the UniProtKB entries (Number FTV for top three); and the number of term variants (i.e. FTV
and/or PTV) among the top twelve ranked candidate terms for the UniProtKB entries (Number TV column)
Table 10 Results for Experiment I according to the voting system and the simple categorisation introduced: Results of the voting system
according to the simple categorisation introduced (see Setup of Experiment I and Experiment II for a gene/protein synonym
detection task), which has been applied to the terms from PubMed abstract/title from the small-annotated corpus (first column) as
well as to the target terms (second column)
Abbreviations: n = number of target terms; nFTV= number of target terms that have a FTV among the top twelve candidate terms; nFTVr3= number of target terms that
have a FTV among the top three candidate terms; nTV = number of target terms that have a TV (i.e. FTV and/or PTV) among the top twelve candidate terms
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 18 of 24
category Gene symbol appears (three rows with a
grey background) with a total of 53 target terms.
CBOW finds TVs among the top twelve ranked
candidate terms (nTV column) for 39 of them (i.e.
74%), while Skip-gram finds TVs among the top
twelve ranked candidate terms (nTV column) for 50
of them (i.e. 94%).
 Comparing results of the voting system for CBOW
and Skip-gram, corresponding to both Experiments I
(Table 10) and II (Table 11), Skip-gram always ob-
tains an equal or higher number than CBOW for:
FTVs variants among the top twelve ranked
candidate terms (nFTV column), FTVs among the
top three ranked candidate terms (nFTVr3 column);
and TVs (FTVs and/or PTVs) among the top twelve
ranked candidate terms (nTV column).
Table 8 shows, corresponding to both Experiments I
and II, the number of FTVs among the top twelve
ranked candidate terms (Number FTV column) for
Skip-gram is higher than the number of TVs among the
top twelve ranked candidate terms (Number TV col-
umn) for CBOW. To further illustrate this: a) in Experi-
ment I, CBOW finds 43 TVs while Skip-gram finds 49
FTVs, and b) in Experiment II, CBOW finds 49 TVs
while Skip-gram finds 56 FTVs. Tables 10 and 11 pro-
vide more details based on the categorisation intro-
duced; it can be observed that for both Experiments I
and II, the number of FTVs among the top twelve
ranked candidate terms (nFTV column) for Skip-gram
is always equal to or greater than the number of TVs
among the top twelve ranked candidate terms (nTV col-
umn) for CBOW.
We conclude that: a) Skip-gram outperforms CBOW
in both Experiments and finds more TVs and FTVs; b)
the number of FTVs in both Experiments for Skip-gram
is equal to or greater than the number of TVs for
CBOW; and c) both Skip-gram and CBOW find more
TVs and FTVs in Experiment II (with the aid of the
CVDO) than in Experiment I.
Discussion
The CVDO has a limited lexical content, where each
gene and protein class has only one name (i.e. the value
of the rdfs:label), and thus lacks term variants (e.g. syno-
nyms and acronyms) for genes/proteins. Keeping the
CVDO up-to-date in this respect is a challenge shared
with the typical biologist. As Jensen et al. [65] acknow-
ledge that for the typical biologist, hands-on literature
mining currently means a keyword search in PubMed.
Both biological entity annotations (gene/protein and or-
ganism/species) and molecular interaction annotations
(protein-protein and genetic interactions) of the free-
text scientific literature are needed to support queries
from biologists that may use different names to refer to
the same biological entity. However, identification of
biological entities within the literature has proven diffi-
cult due to term variation and term ambiguity [105], be-
cause a biological entity can be expressed by various
realisations. A large-scale database such as PubMed
contains longer forms including phrases (e.g. serum
amyloid A-1 protein) as well as shorter forms such as
abbreviations or acronyms (e.g. SAA). Finding all term
variants in text is important to improve the results of in-
formation retrieval systems such as PubMed that trad-
itionally rely on keyword-based approaches. Therefore,
Table 11 Results for Experiment II according to the voting system and the simple categorisation introduced: Results of the voting
system according to the simple categorisation introduced (see Setup of Experiment I and Experiment II for a gene/protein synonym
detection task), which has been applied to the terms from PubMed abstract/title from the small-annotated corpus (first column) as
well as to the target terms (second column)
Abbreviations: n ; number of target terms; nFTV ; number of target terms that have a FTV among the top twelve candidate terms; nFTVr3; number of target terms
that have a FTV among the top three candidate terms; nTV; number of target terms that have a TV (i.e. FTV and/or PTV) among the top twelve candidate terms
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 19 of 24
the number of documents retrieved is prone to change
when using acronyms instead of and/or in combination
with full terms [106, 107].
This study investigates to what extent word embed-
dings can contribute to keeping the CVDO up-to-date
with new biomedical publications, and furthermore if
the CVDO itself can aid such update. Experiment I in-
vestigates whether, in taking a gene/protein name from
PubMed titles/articles as a target term, it is possible to
obtain a list of term variants from the word embeddings
created with a 14 M PubMed dataset. The results
obtained for Experiment I confirm that it is feasible and
that Skip-gram finds 22% more term variants than
CBOW using 85 target terms that correspond to 64
UniProtKB entries, which are mapped to CVDO gene
and protein classes. Experiment II investigates if the
same word embeddings used in Experiment I can pro-
duce a better list of term variants (i.e. more term vari-
ants) using as target terms a combination of gene/
protein names from PubMed titles/abstracts with terms
(i.e. more context) from the CVDO protein class expres-
sions and labels. The results obtained for Experiment II
show an improvement in performance of CBOW by 11%
and Skip-gram by 6% using 68 target terms (fewer target
terms than in Experiment I) that corresponds to 63
UniProtKB entries, which are mapped to CVDO gene
and protein classes. In Experiment II (with the aid of the
CVDO), not only is a better list of gene/protein term
variants obtained but also a better ranking, where a full-
term variant is likely to appear among the top three
ranked candidate terms. Hence, the CVDO supplies con-
text that is effective in inducing term variability whilst
reducing ambiguity.
Studies related to semantic similarity and relatedness
tasks employ gold standards specific for the biomedical
domain that have a relatively small number of term
pairs, such as Caviedes and Cimino [38] with 10 term/
concept pairs, Pedersen et al. [34] with 30 term/concept
pairs, and Pakhomov et al. [36] with 724 term pairs. This
study considers a total of 3672 term-pairs from the two
experiments together with human judgments from four
raters. Hence, an outcome of this study is the creation of
a gene/protein names dataset (larger than the MEN Test
Collection [40] with 3 K common English word-pairs)
that can be reused for the evaluation of semantic models
in a gene/protein synonym detection task. However, the
overall setup of the two experiments is unbalanced as a
result of capturing a realistic scenario where: a) some
gene/protein names appearing in PubMed titles/ab-
stracts do not have a vector representation; and b) a
gene and its product (typically a protein) can appear to-
gether in the scientific text, and thus, the biological
knowledge formally represented in the CVDO is already
present.
Considering only the 48 UniProtKB entries mapped to
CVDO gene and protein classes that participate in both
Experiment I and II, the asymmetry between the two ex-
periments can be reduced leading to a smaller gene/pro-
tein names dataset with: a) 660 pairs of terms (target
term and candidate term) taken from the word embed-
dings created with CBOW and Skip-gram (i.e. total of
1320 term pairs) and assessed by four raters in Experi-
ment I; and b) 624 pairs of terms taken from the word
embeddings created with CBOW and Skip-gram (i.e. a
total of 1248 term pairs) and assessed by four raters in
Experiment II. Considering only these 2568 term-pairs
instead of the total of 3672 term-pairs from the two ex-
periments, the performance obtained for CBOW and
Skip-gram is the same as the overall performance re-
ported with Skip-gram outperforming CBOW in both
Experiments; and both CBOW and Skip-gram find more
term variants in Experiment II (with the aid of the
CVDO) than in Experiment I.
Besides the asymmetry between the two experiments
presented, there are certain areas of improvement possible
regarding the data resources. On one hand, the small-
annotated corpus is very narrow in scope with only one
curator performing the gene/protein name annotation for
25 PubMed articles (titles and abstracts). On the other
hand, the 14 M PubMed dataset used to generate the
word embeddings can be arguably larger or include more
recent PubMed articles as it only contains titles and avail-
able abstracts from PubMed articles published between
2000 and 2016 (files up to 8th June 2016).
As of today, data integration remains a challenge in
the life sciences, and therefore, the main curation effort
for the sysVASC project is in normalisation. Rebholz-
Schuhmann et al. [3] emphasises the lack of a complete
solution to normalise proteins and genes (e.g. unique
protein identifier together with protein properties and
alternative names/labels) that facilitates recognising
them from the scientific text. As part of this study, gene/
protein names annotated from PubMed titles and/or ab-
stracts are mapped to UniProtKB entries. Other studies
have also carried out normalisation whilst making no
distinction between genes/proteins. For example, Dogan
et al. [108] annotated genes/proteins of interest and
manually added their corresponding Entrez Gene identi-
fiers. There are, however, studies that have a list of mul-
tiple types of biomedical entities, such as PubTator
[109], and BEST [110]. PubTator considers 5 biomedical
entities and BEST considers 10 biomedical entities. Both
PubTator and BEST perform daily updates of PubMed
content and both have automated identification of bio-
medical entities such as genes. Neither PubTator nor
BEST, however, distinguish between proteins and genes.
The results obtained for Experiment II suggest benefits
in using target terms belonging to the category Gene
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 20 of 24
symbol appears introduced  using terms from protein
class expressions and labels from the CVDO (or the
PxO)  with Skip-gram to automatically obtain the top
three ranked candidate terms for a gene/protein of inter-
est. Although this study does not present a tool, it sug-
gests that the CVDO can provide a better context and
improve the performance of CBOW and Skip-gram
without modifying the word embeddings (i.e. no post-
processing of the term vectors is performed), and this
could be the foundation for building a tool similar to
PubTator or BEST. As the CVDO and the PxO are for-
malised in OWL, it seems natural to envision a tool
based on Semantic Web technologies, such as OWL and
SPARQL. Therefore taking into account two annotation
properties from SKOS, i.e. skos:altLabel and skos:hidden-
Label, we can define the automation for the gene/pro-
tein synonym detection task as: for each CVDO protein,
find term variants for the string values within skos:altLa-
bel and store them in skos:hiddenLabel.
Levy et al. [29] remarks that if different models are
allowed to tune a similar set of hyperparameters, their
performance is largely comparable. The neural language
models CBOW and Skip-gram have a similar set of
hyperparameters, and thus, their performance has been
already compared when accomplishing biomedical tasks
[41, 42]. Muneeb et al. [41] applied different hyperpara-
meter configurations and reported a better performance
for Skip-gram than CBOW in a semantic similarity and
relatedness task for biomedical concepts. Chiu et al. [42]
performed a systematic exploration of different hyper-
parameter configurations and reported an overall better
performance for Skip-gram than CBOW in word simi-
larity and NER tasks using biomedical corpora. This
study also shows a better performance for Skip-gram
than CBOW in a gene/protein synonym detection task
considering two metrics: the AUC for the binary classifi-
cation problem TV and non-TV; and the number of
term variants found for 79 UniProtKB entries. We, how-
ever, used the same hyperparameter configuration for
CBOW and Skip-gram in a study about patient safety
[111] and it was not possible to determine which
(CBOW or Skip-gram) had better performance on an
NER task. This study does not exploit Skip-gram with
negative sampling, which typically improves its perform-
ance [29]. Furthermore, this study does not systematic-
ally explore alternative hyperparameter configurations
that may lead to performance gains.
As far as we are aware, the use of ontologies to pro-
vide more context (i.e. extra terms) for terms selected
from the scientific literature has not previously been in-
vestigated. This paper demonstrates that the CVDO, and
by extension the PxO, can provide better target terms
for a gene/protein synonym detection task without alter-
ing the word embeddings created by Deep Learning
algorithms CBOW and Skip-gram from a 14 M PubMed
dataset. At the time of writing BioPortal [112], an open
repository of biomedical ontologies, has 551 ontologies.
The PxO is re-used by CVDO and is in BioPortal. The
experiments reported here can be replicated, and do not
demand post-processing of the word embeddings cre-
ated with CBOW or Skip-gram to obtain performance
gains. Therefore, other ontologies from BioPortal may
benefit from our proposal to anchor the CVDO in the
biomedical literature.
Conclusion
This study shows performance improvements for both
CBOW and Skip-gram on a gene/protein synonym de-
tection task by adding knowledge formalised in the
CVDO and without modifying the word embeddings
created. Hence, the CVDO supplies context that is ef-
fective in inducing term variability for both CBOW and
Skip-gram while reducing ambiguity. Skip-gram outper-
forms CBOW and finds more pertinent term variants
for gene/protein names annotated from the scientific
literature.
Additional files
Additional file 1: TermsMapped.xls, this file contains the mapping
performed for the 105 terms from 25 PubMed titles/abstracts to 79
UniProtKB identifiers (ACs and IDs) along with the CVDO gene and
protein classes labels. (XLS 34 kb)
Additional file 2: CBOW.xls, this file shows the results for CBOW per
experiment and rater. (XLSX 175 kb)
Additional file 3: Skip-gram.xls, this file shows the results for Skip-gram
per experiment and rater. (XLS 465 kb)
Additional file 4: TermsMapped_votingSystem.xls, this file contains the
details of the voting system for CBOW and Skip-gram per experiment.
(XLS 70 kb)
Additional file 5: pairwiseIAA.xls, this file contains the values of the
Cohens Kappa measure for each pair of raters per experiment and
model, as well as the average mean. (XLS 8 kb)
Acknowledgements
Thanks to Tim Furmston for help with software and e-infrastructure, and to
the anonymous reviewers for their useful comments.
Funding
This work was supported by a grant from the European Union Seventh
Framework Programme (FP7/20072013) for the sysVASC project under grant
agreement number 603288.
Availability of data and materials
All data generated or analysed during this study are included in this
published article and its Additional files 1, 2, 3, 4 and 5.
Authors contributions
All authors contributed to the development of the design of the method
and experiments as well as the writing of the paper. All authors read and
approved the final manuscript.
Competing interest
The authors declare that they have no competing interests.
Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 21 of 24
Ethics approval and consent to participate
The human raters have consented to make their anonymised judgements
publically available.
Consent for publication
Not applicable.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1School of Computer Science, University of Manchester, Manchester, UK.
2Salford Languages, University of Salford, Salford, UK. 3Departamento de
Lingüística Aplicada a la Ciencia y a la Tecnología, Universidad Politécnica de
Madrid, Madrid, Spain. 4Midcheshire Hospital Foundation Trust NHS, Crewe,
England, UK. 5Manchester Institute of Biotechnology, University of
Manchester, Manchester, UK. 6Institut National de la Santé et de la Recherche
Medicale (INSERM) U1048, Toulouse, France. 7Universite Toulouse III Paul
Sabatier, route de Narbonne, Toulouse, France.
Received: 27 September 2017 Accepted: 6 March 2018
Amith and Tao Journal of Biomedical Semantics  (2018) 9:22 
https://doi.org/10.1186/s13326-018-0190-0
RESEARCH Open Access
Representing vaccine misinformation
using ontologies
Muhammad Amith and Cui Tao*
Abstract
Background: In this paper, we discuss the design and development of a formal ontology to describe misinformation
about vaccines. Vaccine misinformation is one of the drivers leading to vaccine hesitancy in patients. While there are
various levels of vaccine hesitancy to combat and specific interventions to address those levels, it is important to have
tools that help researchers understand this problem. With an ontology, not only can we collect and analyze varied
misunderstandings about vaccines, but we can also develop tools that can provide informatics solutions.
Results: We developed the Vaccine Misinformation Ontology (VAXMO) that extends the Misinformation Ontology
and links to the nanopublication Resource Description Framework (RDF) model for false assertions of vaccines.
Preliminary assessment using semiotic evaluation metrics indicated adequate quality for our ontology. We outlined
and demonstrated proposed uses of the ontology to detect and understand anti-vaccine information.
Conclusion: We surmised that VAXMO and its proposed use cases can support tools and technology that can pave
the way for vaccine misinformation detection and analysis. Using an ontology, we can formally structure knowledge
for machines and software to better understand the vaccine misinformation domain.
Keywords: Vaccine, Misinformation, Ontology, Natural language processing, Semantic web, Semantic similarity,
Microattribution
Background
Since their introduction, vaccines have been an impor-
tant breakthrough that has led to the near-eradication of
many infectious diseases. Some of these diseases include
polio, typhoid, and smallpox - all which are now uncom-
mon. But in themodern era, certain sectors of society have
embraced a post-modernist approach that endorses that
science and experts are open to questioning ... put[ting]
greater emphasis on intuition and social relationships and
tends to distrust the scientific method as the best paths to
healing our ills [1]. This, compounded with various other
factors including misinformation about vaccines, has pre-
sented a problem in vaccine uptake into the population.
The effects of this are troublesome, considering in one poll
20% of those surveyed believed that there is a link between
autism and vaccine [2], in a Gallup poll, 58% are either
unsure or actually believe that vaccines cause autism [3],
and 11% presume that vaccines are not necessary and 25%
*Correspondence: cui.tao@uth.tmc.edu
School of Biomedical Informatics, The University of Texas Health Science
Center, 7000 Fannin Street, Suite 600, Houston, TX, USA
presume that autism is a side-effect of vaccines in another
survey of parents [4].
Vaccine skepticism dates back as far as the 19th century,
when the United Kingdom introduced the Vaccination
Act of 1853 requiring compulsory inoculation of children.
Backlash to the law emerged with the formation of the
Anti-Compulsory Vaccination League and ensuing pub-
lications to advocate anti-vaccination beliefs and ideas
[5, 6]. In the 20th century, the retracted study by Andrew
Wakefield that claimed a link between vaccine and autism
had an unfortunate impact on vaccine discourse and the
decline of MMR vaccine rates in certain regions of the
world [7, 8]. Even to this day, Andrew Wakefield is still
propagating the same discredited vaccine claims, and also
has directed a documentary called Vaxxed:From Cover-
Up to Catastrophe that received a special screening at the
Cannes Film Festival [9]. Other figures, like U.S. President
Donald Trump [10], Robert Kennedy, Jr of the Kennedy
family [11], Dr. Robert Sears [12], Alex Jones [13], Bill
Maher [14], Jenny McCarthy [15, 16], etc., have continued
to express distorted claims about vaccines.
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Amith and Tao Journal of Biomedical Semantics  (2018) 9:22 Page 2 of 13
In the information age, the unregulated nature of the
Web has provided free discourse and information shar-
ing to anyone with a computer and Internet access. To
some researchers, the Web is a Pandoras Box that
has both benefits and costs [17, 18], particularly its
impact on health-seeking knowledge. In a Pew Research
poll from 2013 [19], a majority of those surveyed (73%)
sought health-related information with a third of those
(35%) diagnosing themselves as opposed to seeing a doc-
tor. In the same study, of the individuals who sought
vaccine information (17%), 70% made a decision about
vaccination based on the information they found. This
may be troubling, as previous studies have highlighted
that anti-vaccination websites appear highly ranked in
search engine hits [17, 20]. Additionally, social media plat-
forms have a significant impact on vaccination attitudes
[17, 2124]. Overall, the proliferation of vaccine misinfor-
mation is accessible to anyone with a mobile device and
limited time to perform extensive research.
There are previous studies that have looked at the con-
tent of vaccine misinformation and motivation, but none
that have investigated informatics tools that can assist
and automate the analysis of vaccine misinformation to
understand the drivers behind these false notions. The
theoretical benefit of such tools can help process massive
amount of content (i.e. social media posts), and also dis-
cover new knowledge that may not be apparent through
manual human analysis. Numerous previous studies can
help inform the development of tools and technology to
accomplish this objective.
We aimed to use semantic web and ontological technol-
ogy to represent the domain scope of vaccine misinforma-
tion. Also, with ontological representation, we intended
to use this artifact to store various misconceptions about
vaccines. This would eventually assist in a catalogue mis-
information that can be queried and analyzed for future
research. While some vaccines are associated with spe-
cific misinformation, we focused in this study on the
general domain. The Vaccine Misinformation Ontology
(VAXMO) is composed of existing ontologies - Misinfor-
mation Ontology and nanopublications - and is extended
with features pertinent to the anti-vaccine domain. Lastly,
we introduced possible use cases that will involve the vac-
cine misinformation ontology to identify misinformation
for text-mining tasks and other applications.
Semantic web and ontologies
The word ontology has its roots in metaphysical philoso-
phy, extending back to Aristotles Categories, as a nature
of being. In the early 90s, the definition of ontology was
applied in the computer science field as a specification of
a conceptualization. [25]. At the turn of the century, Sir
Tim Berners-Lee described his vision for the next genera-
tion web called the semantic web in Scientific America,
where ontologies would be the foundation for this vision
[26]. Simply, an ontology is a machine-readable artifact
that encodes a logical representation of a domain space
using vocabularies, and their semantic meanings. It is the
output of a knowledge engineering process where tools
and methods are used to build the ontology [27]. Over-
all, ontologies are used for representing information and
knowledge [2830].
In general, knowledge in an ontology is represented
as triple which is information presented in subject >
predicate > object. Essentially, the subject > predicate >
object are concepts that are smallest, unambiguous unit
of thought ... [that are] uniquely identifiable [31]. Each
triple can seamlessly link to another triple to form an
ontological knowledge-base. For this knowledge to be
readable by a machine, we use a computer-based syntax
to encode this knowledge. Once encoded, this artifact can
be shared and distributed for various purposes. More-
over, using Web Ontology Language (OWL) or Resource
Description Framework (RDF), a specific type of web
ontology language syntax for ontologies, we can define
more complex axioms and assertions to fully describe
concepts which provide machine reasoning capabilities.
Nanopublication primer
Semantic web technologies, specifically ontologies, have
had continued impact on research and knowledge shar-
ing, and standardization in the biomedical domain. Some
of what has been described were the benefits of formal-
izing information, information integration, information
reuse, and querying and search, etc. We introduce the use
of nanopublication, which is an ontology-based micro-
publishing format for encoding and distributing singular
units of assertions. Nanopublications have been used pri-
marily in the life sciences, pharma sciences, as well as
genomics and proteomic research data [32]. The benefit
of nanopublications include [32]:
 Improve finding of scientific information
 Connect scientific information from multiple sources
 Organize provenance information of the research
finding
 Verifiable
 Small
The model or structure of a nanopublication involves
a scientific assertion, provenance of the assertion, and
provenance information of the nanopublication itself [33].
The scientific assertion component is the singular atomic
finding that is represented as subject > predicate >
object. An example would be trastuzumab [subject] is
indicated for (treats)[predicate] breast cancer[object].
The other component is the provenance of the assertion,
or the origin or source of something [34], which will
Amith and Tao Journal of Biomedical Semantics  (2018) 9:22 Page 3 of 13
express metadata information, like DOI, authors, research
institution, time and date, experimental method, etc. The
third part is the provenance information about the nanop-
ublication, which generally indicates who created the
nanopublication and when it was created (analogous to
citation metadata).
Provided (Listing 1) is a basic example of a nanopubli-
cation encoding for the research assertion, trastuzumab
is indicated for (treats) breast cancer. Specific discussion
of the encoding is outside the scope of this proposal,
Osborne et al. Journal of Biomedical Semantics  (2018) 9:2 
DOI 10.1186/s13326-017-0173-6
RESEARCH Open Access
CUILESS2016: a clinical corpus applying
compositional normalization of text mentions
John D. Osborne1, Matthew B. Neu1, Maria I. Danila1, Thamar Solorio2 and Steven J. Bethard3*
Abstract
Background: Traditionally text mention normalization corpora have normalized concepts to single ontology
identifiers (pre-coordinated concepts). Less frequently, normalization corpora have used concepts with multiple
identifiers (post-coordinated concepts) but the additional identifiers have been restricted to a defined set of
relationships to the core concept. This approach limits the ability of the normalization process to express semantic
meaning. We generated a freely available corpus using post-coordinated concepts without a defined set of
relationships that we term compositional concepts to evaluate their use in clinical text.
Methods: We annotated 5397 disorder mentions from the ShARe corpus to SNOMED CT that were previously
normalized as CUI-less in the SemEval-2015 Task 14 shared task because they lacked a pre-coordinated mapping.
Unlike the previous normalization method, we do not restrict concept mappings to a particular set of the Unified
Medical Language System (UMLS) semantic types and allow normalization to occur to multiple UMLS Concept
Unique Identifiers (CUIs). We computed annotator agreement and assessed semantic coverage with this method.
Results: We generated the largest clinical text normalization corpus to date with mappings to multiple identifiers
and made it freely available. All but 8 of the 5397 disorder mentions were normalized using this methodology.
Annotator agreement ranged from 52.4% using the strictest metric (exact matching) to 78.2% using a hierarchical
agreement that measures the overlap of shared ancestral nodes.
Conclusion: Our results provide evidence that compositional concepts can increase semantic coverage in clinical
text. To our knowledge we provide the first freely available corpus of compositional concept annotation in clinical text.
Keywords: NLP, Information extraction, Concept normalization, Concept recognition, Fine grained named entity
recognition
Background
Post-coordinated concepts are concepts represented by
combining multiple concepts from an ontology, in con-
trast to pre-coordinated concepts, which are explicitly
predefined and represented in an ontology by a sin-
gle identifier. Post-coordinated concepts have been used
by medical ontological systems such as GALEN [1] and
SNOMED CT [2] to elucidate a broader range of concepts
than is possible with pre-coordinated systems [3, 4] using
descriptive logic. This methodology relies on a restricted
set of pre-defined semantic relationships to avoid or min-
*Correspondence: bethard@email.arizona.edu
3School of Information, University of Arizona, 85721 Tucson, USA
Full list of author information is available at the end of the article
imize semantic ambiguity. This is in contrast to Gene
Ontology [5], which until the recent introduction of anno-
tation extensions [6], assigned multiple annotations to a
single protein without regard to the relationships between
the assigned annotations. Not requiring formal semantic
relationships for all multi-concept annotations may intro-
duce some semantic ambiguity, but allows higher seman-
tic coverage in situations where the source text describes
a concept whose logical description cannot be captured
by the set of pre-existing semantic relationships. Indeed,
the ideal that an ontology of medicine can express all and
only what is medically sensible has been termed unob-
tainable and focusing on all rather than only should
take precedence [7].
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Osborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 2 of 9
In clinical interface systems utilizing SNOMED CT,
complicated clinical concepts are typically created by clin-
icians who select from a set of inter-related atomic con-
cepts with pre-defined relations. However the creation
of a publicly available clinical text corpus with post-
coordinated normalization training data has received less
attention. This is likely due the difficulty and cost of cre-
ating and sharing such a corpus. Moreover, earlier work
[8] comparing normalization between different SNOMED
CT encoding groups that applied post-coordination to
normalize text mentions in case report forms failed to find
any statistically significant semantic agreement.
More recently, post-coordination has been applied in
biomedical corpus construction with the creation of
the NCBI Disease Corpus [9]. During corpus creation,
Do?gan first attempted to normalize disease mentions in
PubMed abstracts to the MEDIC vocabulary using pre-
coordinated concepts, which was successful for 91% of
the disease mentions. For the remaining 9% of disease
mentions, they employed a minimally restricted form of
post-coordination that we term compositional normal-
ization that allowed the use of multiple concepts without
regard to specific relations or slots. They further cat-
egorized these compositional concepts between aggre-
gate or composite concepts that consisted of multiple
self-contained pre-coordinated concepts in the text men-
tion and composed concepts which collectively act to
describe a single concept. The aggregate concepts in this
context are simply concepts linked by logical operators
(AND/OR) since no provision was made for logical oper-
ator usage in the annotation. Examples are shown in
Table 1.
In the NCBI Disease corpus, only 76 such unique com-
positional concepts were normalized (52 aggregate and
24 composed) and annotator agreement for these post-
coordinated concepts was not reported separately.
In contrast to the open-ended nature of Do?gans
compositional concepts, Roberts [10] annotated post-
coordinated concepts for only one predefined relation:
anatomical location. Roberts work includes both a corpus
annotated on medical consumer language and software to
normalize text mentions. However, the corpus contains
only 500 post-coordinated concept instances.
SemEval-2015 Task 14 [11] annotated a corpus of clini-
cal text with post-coordinated concepts, normalizing each
disorder mention to a single SNOMED CT concept, and
restricting further post-coordination to 8 predefined rela-
tions: body locations, which were normalized to UMLS
anatomical concepts, and 7 other small-domain concept
types. We refer to this corpus as SEMEVAL2015. The
SEMEVAL2015 section of Table 2 shows examples of
each predefined relation. However, they report anno-
tator agreement only for disorder mention normaliza-
tion, not the overall normalization annotator agreement
for that mention which would include associated post-
coordinated concepts or slots. They were also unable
to normalize 30% of the disorder mentions (such men-
tions are termed CUI-less) because annotators were
unable to find a single UMLS Concept Unique Iden-
tifier (CUI) for the concept. This suggests that there
are limitations in the annotation process, the ontology
being normalized to (SNOMED CT) or both, which pre-
vent the full semantic capture of clinical text. This is
known as the content completeness problem, first coined
by Elkin [12, 13] but recognized earlier by Rogers and
Rector [14].
In the current study we evaluate the extent to which
compositional annotation, not restricted to a predefined
set of relations, can attenuate the content completeness
problem in clinical text. To address this problem, we gen-
erate the largest corpus to date for this compositional
method. To our knowledge it is the first such composi-
tional corpus in clinical text.
Method
Corpus generation
We generated a novel dataset CUILESS2016 derived
from the part of ShARe corpus used for the SemEval-
2015 Task 14 Shared Task [11], which we term,
SEMEVAL2015. Only a subset of SEMEVAL2015 was
utilized, consisting of those disorder mentions that were
not normalized to SNOMED CT, so called CUI-less
disorders because they lack a Unified Medical Language
System (UMLS) CUI corresponding to a SNOMED CT
concept. Their distribution in the SEMEVAL2015 train-
ing and development datasets is shown in Table 3.
Table 1 Examples of pre-cordinated and post-coordinated concepts from the NCBI disease corpus
Type / Subtype Identifiers Text mention example Concept name/s
Pre-coordinated 1 Bone dysplasia Bone diseases, Developmental
Compositional /
Aggregate (|)
2 Breast or ovarian cancer Breast cancer|Ovarian cancer
Compositional /
Composed (+)
3 Inherited neuromuscular disease Neuromuscular disease + Genetic diseases +
Inborn
Post-coordinated concepts of type (aggregate or composed) have 2 or more identifiers
Osborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 3 of 9
Table 2 CUI-less examples from SEMEVAL2015 and CUILESS2016 annotation of ShARe corpus
Aggregate example Composed example
SEMEVAL2015 Text mention RRW Surgical defect
Negation Yes No*
Subject Patient* Patient*
Uncertainty No* Yes
Course Unmarked* Unmarked*
Severity Unmarked* Unmarked*
Conditional False* False*
Generic False* False*
Body location CUI C0225754 (Both lungs) C1521748 (Entire mastoid)
Disorder CUI CUI-less CUI-less
CUILESS2016 Disorder CUI C0034642 (Rhales) C0543467 (Operative surgery)
C0035508 (Rhonchi) C2004491 (Cicatrix)
C0043144 (Wheezing)
An * indicates the default value for that slot in SEMEVAL2015. Our CUILESS2016 annotators added identifiers to describe the disorder when the Disorder CUI was marked
CUI-less in SEMEVAL2015
We re-annotated only the CUI-less disorder CUI; CUI-
less body locations or other relations are not re-annotated,
as shown in Table 2.
Since test data was not readily available, only disor-
der mentions from the development and training portion
of SEMEVAL2015 were normalized. Approximately 30%
(5397) of disorder mentions fit this CUI-less descrip-
tion from a set of 298 training notes and a set of 133
development notes. The 298 training note set was itself
derived from the notes used in the ShARe/CLEF eHealth
2013 Evaluation Lab Task 1 [15]. Statistics for the input
SEMEVAL2015 corpus are provided in Table 4.
Annotation method
We used an open-ended compositional annotation
methodology similar to that of Do?gan [9] to normal-
ize all 5397 CUI-less disorder mentions as described
in the Annotation Guidelines (Additional file 1). Exam-
ples of our annotations are shown in the CUILESS2016
Table 3 SEMEVAL2015 CUI-less distribution by clinical document
type
Data set Document type CUI-less count Average
CUI-less
by Note
Development Discharge summaries 1929 13.9
Training Discharge summaries 2796 20.6
Training Echocardiogram 331 6.1
Training Electrocardiogram 91 1.7
Training Radiology 250 4.6
Only discharge summaries were available for annotation in the development
document set
section of Table 2. Rules for annotation were similar to
the ShARe/CLEF corpus [15] in that disorders were nor-
malized to UMLS CUIs from SNOMED CT using the
most specific CUI possible, ignoring negation and tempo-
ral modifiers, including acronyms, abbreviations and, to
the fullest extent possible, mentions that are co-referent
or anaphoric. There are some critical differences between
the ShARe/CLEF annotation and our method that allow
us to annotate these additional mentions. They are:
1 One or more identifiers were selected to annotate the
text mention if (and only if) no appropriate single
identifier (pre-coordinated term) is found.
2 All of SNOMED CT was available for mention
normalization.
3 The annotators could use existing SEMEVAL2015
identifiers to create compositional concepts.
For example, if the mention no bowel wall thickening
was annotated, and no CUI in SNOMED CT existed for
bowel wall thickening, but the SEMEVAL2015 annota-
tions include a body location CUI for bowel wall and
the disorder was flagged as negated, then the text men-
tion was normalized using just the CUI for Thickened
Table 4 SEMEVAL2015 and CUILESS2016 document statistics
Set Word count
Clinical note count
Discharge ECG EKG Radiology
Train 182K 136 54 54 54
Development 153K 133 0 0 0
Total 335K 269 54 54 54
Osborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 4 of 9
(fndg), since the other two concepts needed for post-
coordination are already present in the SEMEVAL2015
annotations.
Unlike the work of Do?gan [9], we made no distinction as
to whether the multiple CUIs used to annotate the span
were aggregate or composed concepts. Thus, all of the
CUIs in our mention were space separated and could rep-
resent either aggregation (|) or concatenation (+) per the
operator nomenclature of Do?gan [9].
Calculation of annotator agreement
Annotator agreement between the 2 annotators (MID and
MN) on the development data set was computed in 2
different ways.
1 Exact Agreement - Annotators used exactly the same
set of CUIs to annotate the disorder text mention.
We report only proportional agreement pa for this
task by which we mean the fraction of text mentions
on which the annotators agree. Thus, in Table 5 (in
the Exact agreement row) we count only a single
agreement for both Drug Allergy and Levofloxacin,
not 2 agreements. Proportional agreement can be
defined more formally as pa = m/n where m is the
number of mentions where both annotators agree
and n is the total number of mentions. This should
approximate Cohens ? because agreement due to
chance is expected to be extremely small. This is due
to the UMLS representation of SNOMED CT having
over 320K distinct CUIs and we allow an unbounded
number of CUIs per mention.
2 Hierarchical Agreement - We compute hierarchical
agreement between annotators using the set of
annotated nodes and all their ancestors similar to the
hierarchical precision and recall metric used by
Verspoor [16]. It is calculated as:
1
n
n?
i=1
({? Ai} ? {? Bi})/({? Ai} ? {? Bi}) (1)
where {? Ai} indicates the set of annotated nodes
and their ancestors from annotator A for mention i,
{? B} indicates the set of annotated nodes and their
ancestors from annotator B for mention i and n is
the total number of mentions annotated. In cases
where an annotated CUI mapped to multiple
SNOMED CT identifiers, SNOMED CT ancestors
from all paths were used.
Software and data
Annotations were mapped using BRAT 1.3 software
as shown in Fig. 1 [17]. Annotators SP, ES, MN and
MID normalized the training data to the US Edition
of SNOMED CT (2013_03_01) as represented in UMLS
2013AB. Development data was normalized to SNOMED
CT (2016_03_01) in UMLS 2016AA by annotators MID
and MN. Disorder CUIs found in the training data that
were not present in SNOMED CT 2016_03_01 due to
vocabulary changes or errors in the original annotation
were normalized to SNOMED CT (2016_09_01) by MID
and JDO.
Results
As shown in Table 6 we found the majority of dis-
order mentions had only a single identifier, which
reflects the expanded range of available concepts and
our guidance to use pre-coordinated concepts pref-
erentially as outlined in our annotation guidelines.
However Table 6 under-represents the true disorder
multi-identifier count since disorder CUIs can be post-
coordinated with SEMEVAL2015 annotations that rep-
resent disorder attributes. Thus no bowel wall thicken-
ing would be counted as Single in Table 6 since only
the identifier for Thickened (fndg) was directly anno-
tated; the anatomical CUI and negative polarity were
already present in the linked SEMEVAL2015 attribute
annotations.
Table 7 shows the overall distribution of disorder-
related identifiers both when attributes (non-disorder
identifiers assigned in SEMEVAL2015) are either
included or excluded from consideration. Thus in the
Table 5 CUILESS2016 annotator agreement type examples
Exact
mention
score
Hierarchical men-
tion score
Text mention Annotator 1 Concept/s Annotator 2 Concept/s
1.0 1.0 Allergies Levofloxacin Drug allergy Drug allergy
Levofloxacin Levofloxacin
0.0 0.52 Posturing (O/E) - posturing Posturing behaviour
0.0 0.64 Rightward shift Midline shift of brain Midline shift of brain
To the right
0.0 0.22 Redness Erythema Redness
The computed hierarchical mention score was used instead of annotator judgment in determining an approximate level of agreement
Osborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 5 of 9
Fig. 1 Annotation Workflow. BRAT 1.3 [17] used to normalize concepts to UMLS CUIs from SNOMED CT
Disorder + Attributes column the text mention no
bowel wall thickening was scored as having 3 identifiers,
one for the disorder, one for the anatomical location and
one for negation. Only when including these attributes
are the majority of the concepts in CUILESS2016
post-coordinated.
Annotator agreement on the development set is shown
in Table 8.
Discussion
We have normalized all but 8 of the 5397 original CUI-
less concepts in our corpus indicating that a com-
positional normalization methodology can alleviate the
content completeness problem and increase semantic
coverage in clinical text. All examples where our approach
failed to normalize concepts are shown in Table 9. These
examples fall into 3 general classes, those where the entity
is not really a disease (named entity recognition failure),
those where the text is ambiguous, and those where the
annotators were unable to find a suitable composition
in SNOMED CT. Only the last of these classes repre-
sents a concept that was truly not normalizable under
our methodology. The 3 cases that fall into this class
represent a tiny fraction (0.06%) of the original 5397 men-
tions. Leveraging the existing SEMEVAL2015 annotation
(which specified 8 different semantic modifiers of dis-
orders) and allowing our annotators to normalize using
a general semantic association (without specifying the
exact relationship) allowed us to dramatically increase
semantic coverage. Our corpus should be of interest to
developers of clinical text normalization software inter-
ested in annotating a wider range of disorder annotations.
We make our corpus freely available.
While our methodology is similar to that used by Do?gan
[9] for PubMed abstracts, we provide an order of mag-
nitude more compositional normalization data. With the
exception of some common abbreviations, the majority of
compositional clinical concepts we created are composed
Table 6 Disorder multiple identifier distribution by data set
Disorder CUI type Development count Development proportion Training count Training proportion
CUI-less 1 0.05 7 0.20
Single 1687 87.46 2823 81.40
Double 221 11.46 562 16.21
Triple 18 0.93 73 2.11
Quadruple 2 0.10 3 0.09
Total 1929 100 3468 100
Differences in disorder mention distribution between the development and training data set are likely due to note composition (see Table 3), a larger (4) set of annotators in
the training data and a lack of a consensus process for the training data since each training document is annotated only by a single annotator
Osborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 6 of 9
Table 7 Overall disorder and attribute multiple identifier
distribution
Identifier type Disorder Disorder + Attributes
Count Proportion Count Proportion
CUI-less 8 0.1% 3 0.06%
Single 4502 83.54% 966 17.90%
Double 783 14.53% 2505 46.41%
Triple 91 1.7% 1608 29.79%
Quadruple 5 0.1% 263 4.87%
Pentuple 0 0.0% 50 0.93%
Hextuple 0 0.0% 20 0.04%
Total 5397 100% 5397 100%
The Disorder column shows the count and proportion of disorders annotated with
one or more concepts excluding attributes. The Disorder + Attributes column
includes identifiers from attributes in the count to capture post-coordination with
other identifiers
concepts, not aggregate concepts. This is in sharp con-
trast to Do?gan [9] where the majority of mentions (114)
from PubMed abstracts are aggregates of discrete con-
cepts and only 34 mentions (24 unique) require logical
description. Moreover, a substantial proportion (at least
16%) of the CUI-less clinical concepts required composi-
tional normalization to specify the disorder mention. This
is a higher proportion than is seen previously in PubMed
abstracts [9] and consistent with the greater variability of
clinical text.
Exact annotator agreement
There is a clear need for multi-identifier annotation in the
clinical arena, where multiple identifiers are semantically
critical for diseases such as cancer [18] and peripheral
arterial disease [19]. However, evaluating the annota-
tor agreement of post-coordinated concepts is difficult
because of a lack of a common annotation standard. Pre-
vious studies reported proportionate agreement on exact
matches [8, 15, 20], but the definition of an exact match
can vary.
For example Andrews [8], took research questions
from case report forms and provided them to 3
different coding companies and instructed them to
extract (normalize) core SNOMED CT concepts, using
Table 8 Development dataset annotator agreement
Agreement type Agreement count Proportionate agreement
Exact 1011 52.4
Hierarchical NA 78.2
Total mentions 1929
There is no count for hierarchical agreement since each mention is assigned a value
based on Eq. (1), whereas exact agreement assign every mention as a match (1.0) or
not (0.0)
Table 9 Compositional CUI normalization error analysis
Mention Error Class
Allergies, Calcium Named entity recognition failure
Atrial sensed Named entity recognition failure
Left ventricular inflow pattern Named entity recognition failure
RCIA Ambiguous text
RC one Aneurysm Ambiguous text
Echogenic kidney No composition found
Making grammatical errors No composition found
Tortous aorta No composition found
All 8 mentions where annotators were unable to annotate the disease using the
compositional approach
either pre-coordinated or post-coordinated expressions.
Normalization was measured using proportionate agree-
ment only at the core concept level, which ignored
disagreements resulting from additional identifiers from
modifiers. Even with this restriction, agreement between
all 3 coding companies was calculated to be only 33%,
with 44% agreement between the two most similar anno-
tation sets. Using Krippendorff s ? as their statistic they
concluded there was no significant semantic agreement in
normalization. In contrast, our proportionate exact agree-
ment (our worst performing metric) was 10% higher than
their best inter-annotator agreement although we were
more stringent in including disagreement to extend to
non-core concepts. This may be due to their data set
which was focused on rare diseases in case report forms
(rather than clinical text), differences in the tool selection
and/or annotator medical knowledge.
An alternative measure of annotator normalization
agreement (accuracy) was used in the original annotation
of this corpus [15] instead of Cohens ? and Krippendorf s
?. Annotator normalization agreement was calculated
between annotators and was not separated from the
underlying mention span boundary detection. A relaxed
accuracy calculation where correctness was defined as
any overlapping span where the disorder CUIs matched
yielded an accuracy of 0.776, a strict agreement score
based on exact span matching yielded a much higher
agreement of 0.846. However this high accuracy applies
to single CUI disorder agreement. No annotator agree-
ment was reported including disagreements with CUIs
from the body location attribute or other included iden-
tifiers. While that reported exact agreement is higher
than ours, we expected our agreement to be substantially
lower since our annotation was for CUI-less disorders
that they did not annotate. The original annotation delib-
erately excluded use of the UMLS semantic group finding
for these disorders and reported that this semantic group
was found to be a noisy, catch-all category, and attempts
Osborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 7 of 9
to consistently annotate against it did not succeed in our
preliminary studies.
Non-exact annotator agreement
Our exact agreement calculation cannot determine if a
pre-coordinated concept and a post-coordinated concept
are logically equivalent. Additionally, exact agreement
cannot capture the difference between concepts with
completely different meanings and hyponyms/hypernyms
that have similar meanings. Our hierarchical agreement
measure can account for this distinction. Hierarchical
agreement penalizes distant errors and those at the
higher levels of the hierarchy more severely than finer
misclassifications, similar to hierarchical precision [16].
Unfortunately, the performance of hierarchical agreement
is dependent on the structure of the ontology used. It is
sensitive to the level of branching and assumes a consis-
tent correlation between branch length and semantic dis-
tance. Thus even semantically similar concepts such as the
posturing example seen in Table 5 may not score well, a
consideration given the semantic duplication in SNOMED
CT [21, 22]. We thus asked our annotators to consider the
sets of concepts in each disagreement, and judge whether
they were semantically equivalent, using their knowledge
as medical professionals, rather than the exact structure
of the ontology. The two annotators reached consensus
easily on this task; there was only one case where they
could not reach consensus, and for this, a neurologist was
consulted to resolve the dispute. This process yielded a
semantic agreement level of 71.6%, 19% increase over
our exact agreement and is consistent with Casper [20]
who reported 53% exact agreement and 75% semantic
agreement.
Compositional annotation rules
One unresolved consideration with compositional anno-
tations is which rules or conditions should govern anno-
tation construction. In a previous study [8], the 3 coding
companies mapping to SNOMED CT presumably (not
specified in paper) used the extremely structured and
elaborate SNOMED CT specific post-coordination spec-
ification to compose any post-coordinated diseases they
annotated. However Pradhan [15] took a more general
(but domain specific) approach specifying only 9 per-
missible disorder modifiers. All of these disorder specific
domains (with the exception of body location) had a small
(single digit) range of acceptable values. While core dis-
order concepts annotated in these publications should be
comparable, associated concepts should be expected to
be quite different. The more general annotation approach
taken by Do?gan [9] and this work allowed for any concept
within the target ontology or ontologies. This allows for
more flexibility at the expense of interpretation. For exam-
ple, a body location CUI could refer to the site of disease
finding, an affected organ, or a procedure site related to
the illness. It is an open-ended question whether it is
better to define the set of rules and allowable domains
for post-coordination for each domain or to allow unre-
stricted composition. An enumerated set of possible rela-
tionships make closed world logic operations possible, but
enumerating a complete and useful set of distinct seman-
tic relationships that can be described in natural language
text may not be feasible [7].
Practical applications
A practical application of our work is increasing seman-
tic representation in clinical text. The approximately 70%
coverage of named entities in SemEval-2015 Task 14 is
too low for many practical purposes. Additionally, while
SEMEVAL2015 corpus has themost exhaustive set of rela-
tions or slots for diseases to date, it still does not include
important clinical relationships useful for practical appli-
cations of NLP. For example, metastasis, infection, surgi-
cal procedures or other SNOMED CT specified relations
are relevant for practical clinical use. Additionally, by cre-
ating a corpus that includes clinical compositional anno-
tation, this corpus opens the door to such annotation by
machines that could potentially reduce the clinical coding
burden.
Limitations
We have shown that annotating text from discharge
summaries with compositional concepts from SNOMED
CT is possible with high levels of annotator agreement.
While this approach improves semantic coverage and
is not bound to specific semantic relationship types, it
does introduce a measure of semantic ambiguity since
the relationship between the concepts is unclear. Thus,
our annotations are more useful for information extrac-
tion than for logical reasoning, especially since we do
not annotate logical operators (AND/OR) which would
be useful in distinguishing aggregate from composite
concepts. Future work should be able to make this dis-
tinction and also determine if our results are achiev-
able for other medical text types (e.g., pathology reports)
and other medical ontologies (e.g., the consumer health
vocabulary).
We have shown high annotator agreement for anno-
tating a single text mention with the identifiers of
multiple ontological concepts, though we expect this
agreement is lower than agreement on single identifier
mentions. Unfortunately, we are unable to directly calcu-
late single-identifier agreement because, under our anno-
tation scheme, a mention which has been annotated with
a single identifier may represent either (1) a true single-
identifier disease/disorder where the identifier completely
captures the meaning, or (2) a disease/disorder where a
single identifier captures only part of the meaning but
Osborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 8 of 9
the remaining meaning is captured by linked attributes
(e.g., the body location already identified by the SemEval-
2015 Task 14 annotations).
Conclusions
In conclusion, we extended the SemEval-2015 Task 14
annotations of the ShARe disorder corpus to cover CUI-
less concepts and showed that the compositional anno-
tation approach first used by Do?gan [9] on PubMed
text can function in clinical text to assign semantic
identifiers to named entities and reduce the content
completeness problem [12, 13]. We believe our larger,
freely available corpus is an important resource for
the annotation of CUI-less concepts and that infor-
mation extraction utilizing compositional normalization
can lead to a more complete understanding of clinical
text by complementing annotation approaches using pre-
defined relations or slots such as the original ShareClef
annotation. While annotation of complex clinical con-
cepts using multiple identifiers has been routinely done
by humans in a clinical or research setting, this cor-
pus should aid the development of compositional nor-
malization by machines to supplement manual coding
practises.
Additional file
Additional file 1: Annotation Guidelines for Annotating CUI-less
Concepts in BRAT. (PDF 1050 kb)
Acknowledgements
Shyam Patel and Efe Sahine helped annotate the training corpus. Discussions
with Ken Barker were extremely helpful and his support in the creation of the
annotation guidelines was much appreciated.
Funding
Research reported in this publication was supported by the National Institutes
of Health. Support includes grant award number UL1TR001417 from the
National Center for Advancing Translational Sciences and grant award number
1R01GM114355 from the National Institute of General Medicine Science,
Extended Methods and Software Development for Health NLP. The content
is solely the responsibility of the authors and does not necessarily represent
the official views of the National Institutes of Health.
Availability of data andmaterials
To obtain a copy of the dataset it is required that the original ShARe corpus be
downloaded first and a license agreement signed as described on the ShARe
website (http://alt.qcri.org/semeval2015/task14/index.php?id=data-and-
tools). The CUI-less annotations may then be made downloaded from https://
physionet.org/works/CuilessClinical/. Annotation guidelines are included in
the Additional Files section. An implementation of the hierarchical annotator
agreement calculation for SNOMED CT can found in the
HierarchicalAnnotatorAgreementClient class at https://github.com/ozborn/
jbratuimatools. All other intermediate data is available upon request.
Authors contributions
JDO conceived the project idea, helped design the experiment, analyzed the
data and wrote the first draft. SB and TS helped design the experiment,
analyze the data, and edit the manuscript. MN and MID annotated the corpus,
analyzed the data, and edited the manuscript. All authors read and approved
the final manuscript.
Ethics approval and consent to participate
Not applicable, this study uses previously published de-identified human
subject data that is classified as non-human subject data because of the lack
of identifying information.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1University of Alabama at Birmingham, 7th Ave S, 1720 Birmingham, USA.
2Computer Science Department, University of Houston, Düsternbrooker Weg
20, 24105 Houston, USA. 3School of Information, University of Arizona, 85721
Tucson, USA.
Received: 10 April 2017 Accepted: 26 December 2017
Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 
https://doi.org/10.1186/s13326-018-0192-y
RESEARCH Open Access
Investigating the role of interleukin-1
beta and glutamate in inflammatory bowel
disease and epilepsy using discovery browsing
Thomas C. Rindflesch1, Catherine L. Blake2, Michael J. Cairelli3, Marcelo Fiszman4, Caroline J. Zeiss5
and Halil Kilicoglu6*
Abstract
Background: Structured electronic health records are a rich resource for identifying novel correlations, such as
co-morbidities and adverse drug reactions. For drug development and better understanding of biomedical
phenomena, such correlations need to be supported by viable hypotheses about the mechanisms involved, which
can then form the basis of experimental investigations.
Methods: In this study, we demonstrate the use of discovery browsing, a literature-based discovery method, to
generate plausible hypotheses elucidating correlations identified from structured clinical data. The method is
supported by Semantic MEDLINE web application, which pinpoints interesting concepts and relevant MEDLINE
citations, which are used to build a coherent hypothesis.
Results: Discovery browsing revealed a plausible explanation for the correlation between epilepsy and inflammatory
bowel disease that was found in an earlier population study. The generated hypothesis involves interleukin-1 beta
(IL-1 beta) and glutamate, and suggests that IL-1 beta influence on glutamate levels is involved in the etiology of both
epilepsy and inflammatory bowel disease.
Conclusions: The approach presented in this paper can supplement population-based correlation studies by
enabling the scientist to identify literature that may justify the novel patterns identified in such studies and can
underpin basic biomedical research that can lead to improved treatments and better healthcare outcomes.
Keywords: Literature-based discovery, Discovery browsing, Epilepsy, Inflammatory bowel disease, Interleukin-1 beta,
Glutamate
Background
Information needs in clinical setting and basic research
setting differ significantly. Studies of information needs
in clinical setting have focused on the types of questions
asked by clinicians [1] and have informed the specific
information facets (population, intervention, comparison,
and outcome) that need to be identified in order to address
those clinical questions [2]. In contrast to the clinical set-
ting where there are often multiple studies relevant to
the clinical encounter, scientists operate at the discovery
end of the information synthesis spectrum where there
*Correspondence: kilicogluh@mail.nih.gov
6Lister Hill National Center for Biomedical Communications, U.S. National
Library of Medicine, 8600 Rockville Pike, Bethesda, MD, USA
Full list of author information is available at the end of the article
is less information available, and agreement on how to
evaluate or combine findings from different studies is
still under development [3]. In such an environment, a
scientist begins with what is best described as a hypoth-
esis projection, the purely conjectural proliferation of a
whole gamut of alternative explanatory hypotheses that
are relatively plausible, a proliferation based on guesswork
- though not mere guesswork, but guesswork guided by
a scientifically trained intuition. The aim of this enter-
prise is to identify those hypotheses that merit detailed
scrutiny. [4]
Structured data from electronic health records (EHRs)
are increasingly mined to identify novel correlations, such
as disease co-occurrences or adverse drug reactions [5].
Such studies are sometimes highly localized, relying on
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 2 of 14
data collected from a small set of institutions; thus, they
can violate some of the key assumptions made when using
traditional statistical measures to determine significance,
leading to false positive associations [6]. When performed
with population-level data (e.g., Medicare claims data),
these data mining studies can provide epidemiological
evidence for co-morbidities and other biomedical phe-
nomena; however, they alone are unable to elucidate the
mechanisms involved in such phenomena or offer plausi-
ble explanations. Such epidemiological evidence must be
subjected to further analysis by scientists in order to gen-
erate viable hypotheses about the etiology of the observed
correlations, a critical step for the development of safe and
effective treatments.
In this paper, guided by statistical correlations extracted
from structured EHR data, we show how a literature-
based discovery technique called discovery browsing [7, 8]
can be used to support scientists as they explore hypoth-
esis projections. This study was instigated to generate a
hypothesis of mechanism for the results of a recent ret-
rospective population study that measured the relation-
ship between epilepsy and twelve autoimmune diseases
[9]. That study analyzed health insurance claims data for
2,518,034 patients, both male and female, 65 years or
younger. They reported that the risk of epilepsy was sig-
nificantly heightened among patients with autoimmune
disease. Collectively, individuals with autoimmune dis-
ease accounted for 17.5% of patients with epilepsy in the
study population. This was a significant result that was
not generally anticipated in the clinical community. The
authors made several suggestive observations relevant to
this correlation. Glutamate receptors may be involved in
the etiology of epilepsy and other central nervous sys-
tem disorders (the glutamate hypothesis) [1012]. The
inflammatory component of autoimmune diseases may
be responsible for the occurrence of epilepsy [13, 14].
They did not, however, propose possible mechanisms
underlying their findings and observations. Because there
exists a lack of mechanistic understanding for the rela-
tionship between epilepsy and autoimmune disease, we
view it as a prime candidate for discovery browsing. The
suggestive observations mentioned above combined with
our prior work that investigated the relationship between
major depression and inflammation via cytokines using
discovery browsing [7] indicated to us that focusing on
glutamate and inflammation could be a fruitful avenue for
a mechanistic understanding.
In this study, we focus on epilepsy and one autoimmune
disorder, inflammatory bowel disease (IBD, a broader
term covering both Crohns disease and ulcerative col-
itis as included in Ong et al. [9]). Further, we look at
interleukin-1 beta (IL-1 beta), as one of the principal sub-
stances (along with interleukin-6) involved in inflamma-
tion. We use discovery browsing to generate a hypothesis
about the mechanisms of both IL-1 beta and glutamate,
and suggest that the influence of the former on the lat-
ter is involved in the etiology of both IBD and epilepsy,
thus proposing a mechanism for the observed connection
between these two disorders. This study also investigates
generalizability of previous work on discovery browsing.
Instead of using discovery browsing to elucidate a general
phenomenon that was observed in many different stud-
ies and known anecdotally for years (e.g., obesity paradox
[8]), we apply it to a narrower scope, exploring possible
mechanisms for the results of a single study without such
an obvious presence in the clinical community.
Related work
Discovery browsing
Literature-based discovery (LBD) [15] is a method of
hypothesis generation, the core premise of which is the
so-called ABC paradigm. AB (a relationship between two
terms A and B) and BC (a relationship between B and
C) are both known, but an AC relationship has so far
not been proposed. The method can be used for open
discovery, in which the discovery (or hypothesis) AC is
the result. Alternatively, in closed discovery, AC may be
known (or assumed) and relations AB and BC are sought
to posit B as an explanation for AC (or a mechanistic link
between the two concepts). While LBD research has pre-
dominantly focused on biomedical literature, it has also
been applied to other domains, such as humanities [16],
world wide web [17], as well as technology and social
issues [18].
Wilkowski et al. [7] introduced discovery browsing as a
modification of LBD. They described it as a tool for illumi-
nating under-studied and poorly understood phenomena
rather than necessarily for making discoveries. Discovery
browsing also relies on the ABC paradigm and the rela-
tionships it exhibits; however, the researcher may assume
(or already know) the relationships, but seek to elucidate
the details of these assumptions, hypotheses, or known
relationships. In the current study, we consider the rela-
tionships to be IBD (A)  inflammation (B)  epilepsy (C),
in which AB, BC, and AC have all been proposed. We
then seek to investigate, expand, and elucidate the B rela-
tionship between these two diseases for a finer-grained
understanding of the mechanisms involved.
Wilkowski et al. [7] used discovery browsing to look
at the interaction of melatonin, cytokines, and major
depression. Cairelli et al. [8] formalized the method and
exploited it to investigate why obesity is beneficial in
intensive care, but detrimental otherwise (i.e. obesity
paradox).
SemRep
Semantic predications extracted fromMEDLINE citations
(titles and abstracts) naturally correlate with relations in
the ABC paradigm and underpin this study. A semantic
Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 3 of 14
predication is a formal representation of an assertion
made in text. Such structures provide a type of com-
putable knowledge representing the information in the
text from which they are extracted. For example, the pred-
ication Interleukin-1 beta-CAUSES-Seizures represents
part of the meaning of the sentence, In addition, high IL-
1beta doses induced seizures only in IL-1beta receptor-
expressing mice (mentions relevant to the predication in
bold). Note that it does not (necessarily) summarize an
entire sentence, and in this case does not contain the
information limiting the seizure induction to IL-1 beta
receptor-expressingmice. A semantic predication consists
of a predicate (CAUSES in this example) and arguments
(Interleukin-1 beta and Seizures).
We extract predications using the SemRep natural lan-
guage processing system [19]. SemRep inspects each sen-
tence of input text to identify predications asserted in
each sentence. The system depends on domain knowledge
in the Unified Medical Language System (UMLS) devel-
oped by the U.S. National Library of Medicine [20, 21]. A
SemRep predication has UMLS Metathesaurus concepts
as arguments and a UMLS Semantic Network relation as
predicate.
Extracted predications may be filtered by using auto-
matic abstraction summarization [22] to focus on specific
aspects of biomedicine, such as treatment of diseases or
pharmacogenomics. In this study, we used this process to
focus on predications asserting core relations in molecu-
lar biology [23]. We used the following meta-predications,
where the arguments are represented as general semantic
classes:
 {Substance} ASSOCIATED_WITH OR
PREDISPOSES OR CAUSES
{Pathology}
 {Substance} INTERACTS_WITH OR INHIBITS OR
STIMULATES
{Substance}
 {Substance} AFFECTS OR DISRUPTS OR
AUGMENTS {Anatomy OR Process}
 {Anatomy OR Living Being} LOCATION_OF
{Substance}
 {Anatomy} PART_OF {Anatomy OR Living Being}
 {Process} PROCESS_OF {Living Being}
Semantic MEDLINE
The methodology pursued in this study is implemented
with Semantic MEDLINE [24], a Web application that
integrates PubMed document retrieval, SemRep natural
language processing, automatic abstraction summariza-
tion, and visualization into a single Web portal. Sem-
Rep predications extracted from all MEDLINE citations
are made available from SemMedDB [25] and are sum-
marized according to the meta-predications just noted.
Summarized predications are then presented as a con-
nected interactive graph of semantic relations (Fig. 1).
Subjects and objects are nodes in the graph, while pred-
icates are edges. By clicking on an edge, the user can see
the predication represented. The edge has a link to the
Fig. 1 Illustration of the Semantic MEDLINE web application. The summarized results of a PubMed search are displayed as an interactive graph,
where nodes represent subjects and objects of semantic predications and the edges represent the predicates (right). Edges are linked to the original
MEDLINE citation from which the predication is extracted (top-left). Nodes and edges can be filtered using relation and semantic group filters
(bottom-left)
Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 4 of 14
original MEDLINE citation from which the predication is
extracted.
Semantic MEDLINE supports discovery browsing by
presenting relationships to the user that might not have
been noticed without it. Miller et al. [26] used Seman-
tic MEDLINE to study the effect of the interaction of
testosterone and cortisol on declining sleep quality in
aging men. In related work, Cohen et al. [27] discussed
EpiphaNet, which displays SemRep predications in graph-
ical form for literature-based discovery. Hristovski et al.
[28] described several SemanticMEDLINE-based systems
designed to facilitate discovery.
Other literature-based exploration and discovery sys-
tems have also been proposed to formulate and assess
scientific hypotheses. For example, Arrowsmith [29] links
two sets of articles from biomedical literature using title
words and phrases and statistical information. In a sim-
ilar vein, LitLinker [30] performs open discovery using
UMLS concepts identified by MetaMap [31] as the basis,
grouping and pruning them in conjunction with statis-
tical correlations. Berlanga et al. [32] focus on semantic
integration and visualization from multiple knowledge
and data sources, using named entity recognition to
recognize concepts, exploiting concept taxonomies and
co-occurrence across documents to identify interesting
associations and visualize them for exploration purposes.
HyQue [33] is concerned with semantic integration for
the purpose of hypothesis evaluation and uses Semantic
Web technologies to standardize representation of input,
knowledge sources, data, queries, and outputs. Unlike
SemRep, these systems/tools do not perform explicit rela-
tion extraction, mainly relying on concept co-occurrence
or manually curated relationships.
Inflammation, epilepsy, and inflammatory bowel disease
Before investigating possible mechanistic connections
between IBD and epilepsy, we surveyed MEDLINE
regarding the observations from Ong et al. [9] to deter-
mine how much research has been published concerning
this relationship. We began by looking at IBD, a dis-
order in which the involvement of inflammation is not
controversial.
In order to focus relevant background information,
we used Semantic MEDLINE as an adjunct to PubMed
to query MEDLINE for the primary proinflammatory
cytokines involved in IBD. We issued the Semantic MED-
LINE query inflammatory bowel disease on 03/22/2017
and restricted results to the most recent 500 citations.
4402 predications were extracted and we restricted these
to the core relations in molecular biology. The follow-
ing cytokines appear as nodes in the graph: IL-1 alpha,
IL-1 beta, IL-6, IL-10, IL-17, IL-18, IL-19, and IL-23. We
then used PubMed to determine the amount of research
for each of these substances in association with IBD. In
order to achieve high recall, we combined the search for
IL-1 alpha and IL-1 beta with the query (interleukin-1
AND inflammatory bowel disease). All other cytokines
in this list were queried with the form (<IL-X> AND
inflammatory bowel disease). The query results were:
IL-1: 340MEDLINE citations; IL-6: 779; IL-10: 841; 1L-17:
348; IL-18: 106; IL-19: 10; IL-23: 228.
In order to determine which cytokines are most promi-
nently involved in both IBD and epilepsy, we then
repeated this series of queries to PubMed, substituting
epilepsy for inflammatory bowel disease. The results
for each query were: IL-1: 201 MEDLINE citations;
IL-6: 216; IL-10: 71; IL-17: 19; IL-18: 14; IL-23: 4; and no
citations retrieved for IL-19. Although several cytokines
are prominent in IBD research, only IL-1 (mostly beta)
and interleukin-6 have been much studied with respect
to epilepsy. The inflammatory aspects of both conditions
are accompanied by alterations in a broad array of medi-
ators (see Bevivino and Monteleone [34] and Matin et al.
[35] for reviews on IBD and epilepsy, respectively) whose
interaction in disease etiology is likely to be contextual.
While both IL-1 beta and interleukin-6 were promising
candidates for discovery browsing, we focused on IL-1
beta in this paper, partly to keep the scope of this work
manageable. This procedure could be repeated with other
substances, especially interleukin-6, to potentially reveal
additional insights.
As further background investigation, we used PubMed
to get an overview of research on IL-1 beta and IBD.
We looked at a sample from the 340 citations returned
with the query noted above. IL-1 beta has long been asso-
ciated with gastrointestinal disturbances (e.g. [36]) and
with IBD in particular (e.g. [37]). Subsequent research has
looked at various aspects of that association. For exam-
ple, Casellas et al. [38] investigated the role of IL-1 beta
in chronic ulcerative colitis. Heresbach et al. [39] sought
to elucidate genetic susceptibility to IBD, concentrating
on IL-1beta and IL-1 receptor antagonist (IL-1ra) gene
polymorphisms. Coccia et al. [40] reported on multi-
ple mechanisms through which IL-1 beta contributes to
intestinal pathology. Li et al. [41] exploited biolumines-
cence imaging to determine the location of cells produc-
ing IL-1 beta during intestinal inflammation. Das [42]
hypothesized that the etiology of IBD is due to inade-
quate production of inflammation resolving molecules,
such as lipoxins, resolvins, protectins, maresins and
nitrolipids.
Finally, we queried PubMed to determine whether
there is any research on IL-1 beta and both IBD and
epilepsy. The query (interleukin 1 AND (inflammatory
bowel disease OR colitis) AND (seizure OR epilepsy))
returned only 1 citation [43], a review which states in the
abstract, foreshadowing the conclusions of Ong et al. [9],
that There are reports suggesting more predispositions
Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 5 of 14
to seizures during inflammatory conditions like colitis,
pneumonia and rheumatoid arthritis.
We then moved on to the focus of the paper, which
was twofold: 1) investigate the research on IL-1 beta and
epilepsy, and 2) look at possible mechanistic connections
between IBD and epilepsy involving inflammation (IL-1
beta).
Methods
At the core of the discovery browsing methodology pur-
sued in this study is cooperative reciprocity between the
system and the users domain knowledge. This takes the
form of the user issuing an initial query to Semantic
MEDLINE reflecting an area of interest. All queries were
issued at the end of March, 2017. The graph resulting
from each query was inspected for concepts (either in
the predications or in the abstracts from which they are
extracted) that capture the attention of the researcher
and which may incite the development of a potential
hypothesis regarding the study being pursued. At this
point, PubMed was consulted (using the same query)
to determine whether any citation from which Seman-
tic MEDLINE did not extract a predication supported
the viability of the hypothesis being developed. This
step is performed in part to address recall problems
of SemRep, which may result in missing information
important for hypothesis generation. If the developing
hypothesis was supported, it was pursued with another
query to Semantic MEDLINE incorporating the concept
of interest, and the process was repeated until we were
satisfied with a coherent argumentation chain. Finally,
PubMed was searched to determine whether the hypoth-
esis generated is novel. Figure 2 provides an overview of
the method.
For each query to Semantic MEDLINE, we limited
the abstracts returned to the most recent 500, although
fewer total abstracts were retrieved for some queries. The
predications extracted were then summarized using the
meta-predications given in previous section. For ease of
inspection, we further limited the graph for each query to
a maximum of 50 nodes and 100 edges; nodes are ranked
by frequency and the graph is limited to 50 nodes with the
highest frequency.
Results
IL-1 beta and epilepsy
The initial query to Semantic MEDLINE was (inter-
leukin 1 AND (epilepsy OR seizure)), which returned
2481 predications extracted from 240 citations. In the
summarized graph, several predications were considered
noteworthy as indicating a relationship between IL-1
beta and epilepsy. Interleukin-1 beta-AFFECTS-Seizure
was extracted from Vezzani et al. [44], which reports
that intrahippocampal application of recombinant IL-1ra
inhibits seizures experimentally induced by bicuculline
methiodide in rodents.This study cites previous work [45],
inwhich they found that exogenous application of IL-1 beta
in the rat hippocampus prolongs kainite-induced seizure
by enhancing glutamatergic neurotransmission.
The predication Interleukin-1 beta-CAUSES-Seizures
was extracted from two citations. In one, Ravizza and
Vezzani [46] conducted immunohistochemical analysis of
tissue following acute electrical stimulation in the ventral
hippocampus of rats. They investigated the role of IL-1
beta during resulting epileptic activity, focusing on the
role of IL-1 receptor type 1 (IL-1R1) in rat forebrain. They
suggest that this receptor plays different roles in neurons
and in astrocytes during status epilepticus. Another study
Fig. 2 Overview of discovery browsing. An iterative process that incorporates Semantic MEDLINE help identify interesting concepts, which are used
to build an argumentation chain
Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 6 of 14
[47] looked at the mechanism of seizure by injecting the
right lateral ventricle of rats with both IL-1 beta and gluta-
mate. They conclude that there is an interaction between
IL-1 beta (through the IL-1 receptor) and metabotropic g
lutamate receptors in the onset of epilepsy. Interleukin-1
beta-AUGMENTS-Status Epilepticus was extracted from
Pernot et al. [48], in which the relationship between
neuroinflammation and mesiotemporal lobe epilepsy syn-
drome was explored with immunohistochemical analysis
of tissue after mesiotemporal lobe epilepsy syndrome was
experimentally induced in C57BL/6 adult mice by the uni-
lateral intrahippocampal injection of kainate. They con-
clude that neuroinflammatory pathways are associated
with epileptogenesis.
Opposing results have also been published. One such
study, Claycomb et al. [49] (from which the predication
Interleukin-1 beta-ASSOCIATED_WITH-Seizures was
extracted), reports that IL-1 beta is neuroprotective. This
study was conducted on transgenic mice with targeted
disruption in genes for either the ligand IL-1 beta or its
signaling receptor, IL-lR1. Their claim is based on their
finding that chemoconvulsants administered to IL-1 beta
and IL-1R1 -/- mice produced more acute seizures than in
their respective +/+ littermates. It is not clear that these
results would generalize to animals without such genetic
manipulation. See Table 1 for an overview of our results
on IL-1 beta and epilepsy.
Since we saw considerable research implicating IL-1
beta in epileptogenesis, we next pursued the potential
interaction of IL-1 beta and glutamate in the pathogenesis
of epilepsy and seizures [45, 47]. We began by looking for
research that examined glutamate and epilepsy without
considering IL-1 beta, and then looked at the interaction
of the two in epilepsy and seizure.
Glutamate and epilepsy
The SemanticMEDLINE query (glutamate AND (epilepsy
OR seizure)) extracted 5170 predications from the most
recent 500 citations. After summarization, we examined
several predications which appeared to be relevant to glu-
tamate in the context of seizure or epilepsy. Juhasz et al. [50]
used protonmagnetic resonance spectroscopic imaging to
test glutamate concentration levels in epileptic children
with Sturge-Weber syndrome (which is strongly associ-
ated with epilepsy [51]). They found increased glutamate
in the affected hemisphere, which they interpret as sup-
port for the role of excess glutamate in these patients
(Glutamate-ASSOCIATED_WITH-Seizures). Cavus et al.
[52]measured glutamate levels in epileptic and nonepilep-
tic cortical sites in 79 patients with refractory epilepsy
using high-performance liquid chromatography. They
found elevated extracellular glutamate at epileptogenic
as compared to nonepileptogenic sites (Glutamate-
ASSOCIATED_WITH-Epilepsy).
In considering MEDLINE citations from which Seman-
tic MEDLINE did not extract a predication, one notable
paper discusses research on the mechanisms of gluta-
mate involvement in epilepsy. Perez et al. [53] assume that
excessive glutamate underlies refractory temporal lobe
epilepsy. They investigated the cause by using immuno-
gold electron microscopy to measure glutamate levels in
tissue extracted from the brains of male Sprague-Dawley
rats infused with methionine sulfoximine, which induces
glutamine synthetase efficiency. They conclude that such
deficiency leads to increased extracellular glutamate. The
studies we report on glutamate and epilepsy are summa-
rized in Table 2.
Based on research indicating glutamate involvement in
epilepsy and considering research implicating IL-1 beta
Table 1 Summary of articles discussing IL-1 beta and epilepsy
Study Subjects Method Result/Conclusion
Vezzani et al. [45] Kainite-intoxicated rats Application of IL-1 beta in the
hippocampus
IL-1 beta prolongs experimentally
induced seizures
Vezzani et al. [44] Bicuculline methiodide-intoxicated
rodents
Intrahippocampal application of
recombinant IL-1ra
IL-1ra inhibits experimentally
induced seizures
Ravizza and Vezzani [46] Male Sprague-Dawley rats Immunohistochemical analysis
following acute electrical
stimulation in the ventral
hippocampus
IL-1R1 plays different roles in
neurons and in astrocytes during
status epilepticus
Wang et al. [47] Rats Injection of right lateral ventricle
with both IL-1 beta and glutamate
Interaction between IL-1 beta and
metabotropic glutamate receptors
in the onset of epilepsy
Pernot et al. [48] C57BL/6 adult mice Immunohistochemical analysis of
tissue after mesiotemporal lobe
epilepsy syndrome induced by
intrahippocampal injection of
kainate
Neuroinflammatory pathways are
associated with epileptogenesis
Claycomb et al. [49] IL-1 beta and IL1R1 -/- mice Administration of
chemoconvulsants
Produced more acute seizures
Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 7 of 14
Table 2 Summary of articles discussing glutamate and epilepsy
Study Subjects Method Result/Conclusion
Juhász et al. [50] Epileptic children with Sturge-Weber
syndrome
Proton magnetic resonance spectro-
scopic brain imaging
Increased glutamate concentrations
observed
Cavus et al. [52] Epileptic and nonepileptic corti-
cal sites in patients with refractory
epilepsy
High-performance liquid chromatog-
raphy based on microdialysis probes
Elevated extracellular glutamate
observed at epileptogenic sites
Perez et al. [53] Tissue extracted from brains of male
Sprague Dawley rats infused with
methionine sulfoximine
Glutamate levels measured with
immunogold electron microscopy
Glutamine synthetase deficiency
leads to increased extracellular
glutamate
in epilepsy, we were encouraged to investigate the inter-
action of IL-1 beta and glutamate in the context of this
disorder.
IL-1 beta and glutamate
We issued three queries to Semantic MEDLINE to inves-
tigate the relationship of IL-1 beta and glutamate in the
etiology of epilepsy. One focused on this disorder (and
seizure), another specified the brain (but not the disor-
der), and a third specified neither disorder nor anatomic
location.
And epilepsy
The query (interleukin-1 AND glutamate AND (seizure
OR epilepsy)) retrieved 18 citations and 202 predications,
which were not summarized. Two papers identified in
the graph were relevant. Xiaoqin et al. [54] injected the
cerebral cortex and hippocampus of rats with IL-1 beta
and IL-6. Immunohistochemistry observation revealed
the development of seizures along with increased glu-
tamate and decreased GABA (Interleukin-6-CAUSES-
Seizures). Donnelly et al. [55] analyzed synaptosomes
prepared from the brains of BALB/c female mice, 8-12
weeks old, in which epilepsy-like symptoms had been
induced with glycerol. Synaptosome pellets were then
subjected to a series of in vitro techniques after which
they observed an increase in IL-1 beta levels and a
decrease in glutamate release in hippocampus tissue
(Entire hippocampus-LOCATION_OF-Glutamate).
In the brain
The Semantic MEDLINE query (interleukin 1 AND
glutamate AND brain) returned 1850 predications from
160 citations. After summarization, several predications
were extracted from citations discussing the interaction of
IL-1 beta and glutamate. Interleukin-1 beta-DISRUPTS-
uptake was extracted from an article [56], which reported
that astrocyte uptake of glutamate is neuroprotective dur-
ing brain inflammation. Based on Northern blot analy-
sis and other in vitro techniques performed on primal
human astrocyte cultures subjected to several cytokines
and 3H-glutamate, the authors concluded that proin-
flammatory cytokines inhibit astrocyte glutamate uptake.
Based on intracerebral microdialysis in unanesthetized
rabbits, Huang et al. [57] reported that organum vasculo-
sum laminae terminalis (OVLT) release of glutamate was
induced by intracerebroventricular injection of IL-1beta
(Interleukin-1 beta-STIMULATES-Glutamate).
When inspecting citations from which Semantic MED-
LINE did not extract a predication, we found an earlier
report which concluded that IL-1 beta enhances glu-
tamate. As measured by brain microdialysis in freely
moving male SpragueDawley rats, Mascarucci et al.
[58] found that injection of intraperitoneal IL-1 beta
increased glutamate release in the nucleus tractus
solitarius.
Some studies reported that IL-1 beta inhibits glutamate.
Murray et al. [59] prepared hippocampal synaptosomes
from male Wistar rats, on which in vitro experiments
were conducted. They reported that immunoblotting
with specific antibody revealed that IL-1 beta inhibited
potassium chloride-stimulated glutamate release in tis-
sue from young (4 month) but not older (22 month)
rats, and only in the presence of calcium (Interleukin-1
beta-STIMULATES-Glutamate (although the predica-
tion itself is wrong)). In a study of the influence of IL-1
beta on memory consolidation, Gonzalez et al. [60]
reported that intrahippocampal injection of IL-1 beta in
adult male Wistar rats decreases glutamate release from
dorsal hippocampus synaptosomes after contextual fear
conditioning (Interleukin-1 beta-INTERACTS_WITH-
CRK protein, human). The studies resulting from this
query (including brain) and the previous one (not
including brain) are given in Table 3.
Disorder and location not specified
The Semantic MEDLINE query (interleukin 1 AND glu-
tamate) retrieved 3232 predications from 289 citations.
The research we saw in the summarized graph focused
on neuronal involvement. For example, the predication
Glutamate-COEXISTS_WITH-Interleukin-1 beta was
extracted from Casamenti et al. [61], which looked at
the involvement of inflammation with Alzheimers dis-
ease. IL-1 beta was injected into the nucleus basalis of
adult male Wistar rats. The authors reported a marked
increase in glutamate (revealed through microdialysis).
Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 8 of 14
Table 3 Summary of articles discussing IL-1 beta and glutamate
Study Subjects Method Result/Conclusion
Xiaoqin et al. [54] Cerebral cortex and hippocampus
of rats
Injection of IL-1 beta and IL-6;
immuno-histochemistry
Increased glutamate and decreased
GABA observed
Donnelly et al. [55] Synaptosome pellets prepared
from brains of 8-12 week-old
BALB/c female mice intoxicated
with glycerol
In vitro techniques Report increased IL-1 beta levels
and decreased glutamate release in
hippocampus tissue
Hu et al. [56] Human astrocyte cultures sub-
jected to several cytokines and
3H-glutamate
Northern blot analysis and other in
vitro techniques
Proinflammatory cytokines inhibit
astrocyte glutamate uptake
Huang et al. [57] Intracerebroventricular injection of
IL-1beta in adult male New Zealand
white rabbits
Intracerebral microdialysis Glutamate induced by IL-1 beta
Mascarucci et al. [58] Intraperitoneal injection of IL-1 beta
in freely moving male Sprague
Dawley rats
Brain microdialysis Increased glutamate released in the
nucleus tractus solitarius
Murray et al. [59] Synaptosomes prepared from male
Wistar rats
Immunoblotting with specific anti-
body
IL-1 beta inhibits potassium
chloride-stimulated glutamate
release in tissue from young (4
month), in the presence of calcium
Gonzalez et al. [60] Adult male Wistar rats Intrahippocampal injection of IL-1
beta; preparation of synaptosomes;
in vitro technique to assay gluta-
mate release
IL-1 beta decreases glutamate
release from dorsal hippocampus
synaptosomes after contextual fear
conditioning
The query which retrieved the first two studies used the term brain, and that which retrieved the other studies used the term epilepsy)
Prow and Irani [62] used immunoblotting and immuno-
histochemistry, cytokine assays, and histological analysis
to examine spinal cord tissue extracted from mice chal-
lenged with neuroadapted Sindbis virus. Based on anal-
ysis of levels of astroglial glutamate transporter (which
removes glutamate from the synaptic cleft), IL-1 beta,
and glutamate, they claimed that the increase of IL-1
beta in response to the virus disrupts glutamate home-
ostasis. They concluded (in the abstract) that their data
provide one of the strongest in vivo links between
innate immune responses and the development of exci-
totoxicity demonstrated to date. (Interleukin-1 beta-
INTERACTS_WITH-Glutamates).
In a sample of citations fromwhich SemanticMEDLINE
did not extract a predication, Fogal et al. [63], for example,
investigated the etiology of hypoxic-ischemic brain dam-
age in IL-1R1 null mutant, mGluR1-/-, and wild-type con-
trol mice. From both in vitro and in vivo experiments, they
concluded that IL-1 beta makes a significant contribution
to such neuronal injury and that it increases extracellu-
lar glutamate as part of the mechanism. Yan and Weng
[64] used both in vitro and in vivo techniques to study the
mechanisms by which IL-1 beta interacts with glutamate
in neuropathic pain experimentally induced in young
adult male Sprague-Dawley rats. They concluded that
IL-1 beta uses presynaptic NMDA receptors to enhance
glutamate release from primary afferents in neuropathic
rats. Yan et al. [65] analyzed tissue extracted from young
adultmale Sprague-Dawley rats subjected to partial sciatic
nerve ligation. Based on several in vitro techniques to
determine the mechanisms involved, they concluded that
IL-1 beta contributes to neuropathic pain by suppress-
ing glial glutamate uptake. The research reported in this
section is summarized in Table 4.
After having seen considerable research suggesting that
IL-1 beta may increase glutamate, facilitate its receptors,
or inhibit its uptake by glial cells in the context of epilepsy
and other neuronal disturbances, we turned to glutamate
and IBD.
Glutamate and inflammatory bowel disease
In order to investigate glutamate and gastrointestinal
phenomena, we issued two queries to Semantic MED-
LINE, one focused on anatomy and another on disease.
The anatomy-focused query (glutamate AND (bowel OR
colon OR intestine OR gastrointestinal OR stomach))
retrieved 4718 predications from 500 citations. Many
of these discuss the relevance of normal levels of gluta-
mate and its receptors to gastrointestinal processes. For
example, two recent reviews highlight the prominence
of glutamatergic phenomena underpinning the mech-
anisms of gastrointestinal functions. The predication
Gut-LOCATION_OF-Glutamate was extracted from
Julio-Pieper et al. [66], which states that glutamate is the
main neurotransmitter of the brain-gut axis (realized
in part by the vagus nerve). (Metabotropic) glutamate
receptors occur in the brain as well as throughout
the gastrointestinal tract, from the mouth to the large
Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 9 of 14
Table 4 Summary of articles discussing IL-1 beta and glutamate (disorder and location not specified in query)
Study Subjects Method Result/Conclusion
Casamenti et al. [61] Adult male Wistar rats IL-1 beta injected into nucleus
basalis; microdialysis
Significant increase in glutamate
Prow and Irani [62] Spinal cord tissue extracted from
mice challenged with neuroad-
apted Sindbis virus
Immunoblotting and immunohis-
tochemistry, cytokine assays, and
histological analysis
Increase of IL-1 beta in response
to the virus disrupts glutamate
homeostasis (development of
excitotoxicity)
Fogal et al. [63] IL-1RI null mutant, mGluR1-/-, and
wild-type control mice
Both in vitro and in vivo experi-
ments
IL-1 beta increases extracellular
glutamate as part of the
mechanism of neuronal injury
Yan and Weng [64] Young adult male Sprague-Dawley
rats
In vitro and in vivo techniques to
study experimentally induced neu-
ropathic pain
IL-1 beta enhances glutamate
release from primary afferents
Yan et al. [65] Tissue from young adult male
Sprague-Dawley rats subjected to
partial sciatic nerve ligation
Several in vitro techniques IL-1 beta contributes to
neuropathic pain by suppressing
glial glutamate uptake
intestine, and are relevant to digestion as a whole. These
receptors are involved in several gastrointestinal reflexes,
including swallowing, gastric accommodation, and
emesis [67].
In one of the citations retrieved with PubMed
that did not produce a SemRep predication, Clarke
et al. [68] investigated the kynurenine pathway of
tryptophan degradation in plasma samples from 10
male patients with irritable bowel syndrome (IBS)
and 26 controls. High performance liquid chromatog-
raphy revealed that concentration of the neuropro-
tective metabolite kynurenic acid (an antagonist of
the NMDA glutamate receptor) was decreased in the
IBS subjects.
The disease-focused query (glutamate AND
(inflammatory bowel disease OR colitis)) retrieved
995 predications from 89 citations. In the summarized
graph, Colitis-PROCESS_OF-Rattus norvegicus was
extracted from Varga et al. [69]. In this study, kynurenic
acid, an antagonist of NMDA (a glutamate receptor),
was administered to male Wistar rats after inducing
colonic inflammation with trinitrobenzene sulfonic acid.
Measurements conducted on anesthetized animals as
well as on blood samples and colon biopsies indicated a
significant modulatory effect, including reduced inflam-
matory enzyme activities, decreased intestinal motility,
and increased tone of the colon.
Several of the MEDLINE citations from which Seman-
tic MEDLINE did not extract a predication report an
association between glutamate and intestinal phenom-
ena. For example, Carpanese et al. [70] conducted
a study based on in vitro cell cultures from adult
male rats. Based on immunocytochemistry, they con-
clude that blockade of glutamate receptors (NMDA and
AMPA/kainite) may protect enteric neurons subjected to
in vitro chemically-induced ischemic injury followed by
reperfusion.
We then sought additional information on kynurenic
acid and its potential role in mitigating gastrointesti-
nal disturbances. The PubMed query, ((kynurenic acid
OR kynurenine) AND inflammatory bowel disease)
returned 7 citations. One of these was Forrest et al. [71], in
which serum concentrations of purines and kynurenines
were measured in patients with mild IBD. In noting
increased levels of kynurenic acid compared to controls,
they concluded that kynurenine modulation of glutamate
receptors is involved in the symptoms of IBD, either as a
response to an abnormality or as a primary abnormality
itself. The studies we cite on glutamate and IBD are given
in Table 5.
Finally, we issued two PubMed queries, one disease-
focused and a second anatomy-focused, to look for
research reporting on the interaction of IL-1 beta and
glutamate in IBD. The first, (interleukin-1 AND gluta-
mate AND (inflammatory bowel disease or colitis)),
returned no citations. The second, (interleukin-1 AND
glutamate AND (bowel OR colon OR intestine OR gas-
trointestinal OR stomach)), retrieved three citations, none
of which discuss the interaction of IL-1 beta and gluta-
mate in the etiology of gastrointestinal disorders. Saperas
et al. [72] discuss a possible effect of IL-1 beta on
gastric acid secretion but do not mention the inter-
action of interleukin-1 beta and glutamate. Morrow et
al. [73] investigated the effect of murine IL-1 beta on
gastric contractility, but did not address the relation of
IL-1 beta and glutamate in IBD. Finally, Qu et al. [74]
present a review of epigenetic phenomena in gastric
cancer. As part of the discussion, IL-1 beta and gluta-
mate are mentioned, but their interaction in IBD is not
addressed.
Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 10 of 14
Table 5 Summary of articles discussing glutamate and inflammatory bowel disease
Study Subjects Method Result/Conclusion
Clarke et al. [68] Male patients with irritable bowel
syndrome and healthy controls
High performance liquid chromato-
graphy on plasma samples
Kynurenic acid was decreased in
patients with disease
Varga et al. [69] Male Wistar rats after inducing
colonic inflammation with TNBS
Measurements on anesthetized ani-
mals and on blood samples and
colon biopsies after administration
of kynurenic acid
Reduced inflammatory enzyme
activities, decreased intestinal
motility, and increased tone of the
colon
Carpanese et al. [70] Enteric neuron cultures from adult
male rats
In vitro ischemic injury; reperfusion;
blockage of glutamate receptors;
Immuno-cytochemistry to measure
cytotoxicity
Blockade of glutamate receptors
(NMDA and AMPA/kainite) may be
neuroprotective
Forrest et al. [71] Patients with mild IBD Measured serum concentrations of
purines and kynurenines
Kynurenine modulation of gluta-
mate receptors is involved in the
symptoms of IBD
Discussion
The iterative approach used during discovery browsing
mirrors the iteration observed in studies of complex prob-
lem solving [75] and is recognized more generally as an
inherent part of the information seeking process [7678],
characterized as six stages: task initiation, selection,
exploration, focus formulation, collection, and presen-
tation1. This work shows how predicates provided by
SemRep and the interactive Semantic MEDLINE inter-
face supports a user as they iterate between selection,
exploration, and focus formulation steps.
Based on the results of this study, the statistical corre-
lation found in clinical data [9] can be explained by an
increase in glutamate due to IL-1 beta which is involved
in the etiology of both IBD and epilepsy. To recapitulate
the research that supports that claim, we first looked at
IL-1 beta involvement with IBD and epilepsy individu-
ally. It is widely accepted that IL-1 beta is etiologically
associated with IBD. Regarding epilepsy, there is con-
siderable research suggesting that IL-1 beta is crucially
involved in the mechanism of that disorder, and, further,
that excess glutamate, being excitotoxic, also contributes
to the etiology of epilepsy and seizure.
We next looked at research investigating the interac-
tion of IL-1 beta and glutamate in epilepsy. Several studies
based on both animal and human in vitro and in vivo
studies suggest that IL-1 beta induces glutamate activ-
ity increasing glutamate, facilitating its receptors, and
inhibiting its uptake by glial cells, although some stud-
ies report that IL-1 beta inhibits glutamate under their
research conditions.
Glutamate plays an important role in several normal
gastrointestinal functions, and some research suggests
that excessive levels of glutamate contribute to distur-
bances. The strongest evidence for this is that the NMDA
antagonist kynurenic acid has demonstrated therapeutic
value in IBD models. Although it is widely accepted that
IL-1 beta plays a crucial role in IBD, we did not find any
studies investigating the interaction of IL-1 beta and glu-
tamate in IBD. In the context of the rest of our findings,
this would seem to be a potentially valuable direction to
pursue. Figure 3 shows an overview of the hypothesis and
supporting research.
Finally, we queried PubMed in an effort to deter-
mine whether the hypothesis that elevated glutamate
levels due to IL-1 beta are part of the mechanism
of both IBD and epilepsy is novel. Despite this sup-
porting evidence, the PubMed query, (interleukin
1 AND glutamate AND (inflammatory bowel
disease OR colitis) AND (seizure OR epilepsy))
returned no citations, indicating the novelty of this
hypothesis.
There are some limitations to our study. First, discov-
ery browsing is a means of generating hypotheses, not
of determining evidence. Therefore, hypotheses derived
must be subjected to experimental investigation to deter-
mine their value and significance. Secondly, SemRep
is not perfectly accurate; its precision is estimated to
be about 75% (lower for predications involving cellu-
lar/molecular interactions) and its recall is lower (esti-
mated to be approximately 50% [25]). Note, however, that
automatic summarization filters out some of the precision
errors, and also that a precision error can still be use-
ful in pinpointing MEDLINE citations that merit closer
scrutiny (as shown in one of the examples above). On
the other hand, by issuing queries directly to PubMed,
in addition to Semantic MEDLINE, we aimed to mit-
igate the effect of recall errors. Lastly, our methodol-
ogy relies solely on semantic predications and manual
inspection of MEDLINE citations that they are extracted
from. There is a wealth of taxonomic and relational
knowledge which can be mined directly from UMLS and
biomedical ontologies (e.g., Gene Ontology) and incorpo-
rated into discovery browsing to pinpoint other interest-
ing associations. We plan to explore this integration in
future work.
Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 11 of 14
Fig. 3 Overview of the hypothesis and supporting research. Increase in glutamate due to IL-1 beta may be involved in the etiology of both IBD and
epilepsy
Conclusion
Data-mining studies using population-scale data have the
potential to identify novel correlations; however, they gen-
erally do not provide plausible explanations for these
correlations. Although there is no single experiment that
demonstrates the direct connection between interleukin
and glutamate and inflammatory bowel disease or coli-
tis, the discovery browsing approach used in this paper
demonstrates that there is evidence available to sup-
port the mechanistic connection between the observation
made in a population study [9] that epilepsy and IBD often
co-occur and that inflammation is likely involved. We
followed cooperative reciprocity in the discovery brows-
ing methodology, which involves a complementary inter-
action between the user and Semantic MEDLINE and
PubMed, with the former suggesting ideas to pursue and
the latter two providing support or disconfirmation. Based
on the results of this method, we proposed the hypothesis
that IL-1 beta influence on glutamate levels is involved in
the etiology of both epilepsy and IBD. This hypothesis can
underpin the development of more effective therapeutic
approaches for both epilepsy and IBD.
We conclude by observing that semantics-based discov-
ery browsing is complementary to population-based cor-
relation studies. The former provides depth (mechanism),
while the latter provide breadth. It is possible to start
with either and use the other for support. In this study
we started with a correlation-based study and used dis-
covery browsing to elucidate a mechanism. Hypotheses
suggested by discovery browsing could also be supported
with population studies.
Endnote
1 Prior work that studied biomedical researchers as they
systematically reviewed the literature also found that this
process holds, with one additional step (synthesis) that
occurs between the collection and presentation stages
[79, 80].
Abbreviations
EHR: Electronic health record IBD: Inflammatory bowel disease IBS: Irritable
bowel syndrome IL: Interleukin IL-1R1: Interleukin-1 receptor type 1 LBD:
Literature-based discovery OVLT: OrganumRrganum vasculosum laminae
terminalis UMLS: Unified medical language system
Acknowledgements
We acknowledge Kenneth Mandl and Mei-Sing Ong, whose research provided
the impetus for our investigation.
Funding
This research was supported in part by the Intramural Research Program of the
National Institutes of Health, National Library of Medicine.
Availability of data andmaterials
SemMedDB data used in this study is publicly available at https://skr3.nlm.nih.
gov/SemMedDB/download/download.html.
Authors contributions
TCR conceived of the study, performed the searches, and wrote the first draft
of the manuscript. CLB, MJC, MF, CJZ, and HK contributed to discussions about
the generated hypothesis. HK finalized the manuscript. All authors read,
edited, and approved the manuscript.
Ethics approval and consent to participate
Not applicable.
Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 12 of 14
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Retired, Washington, DC, USA. 2School of Information Sciences, University of
Illinois at Urbana-Champaign, 501 E Daniel Street, 61820 Champaign, IL, USA.
3Kaiser Permanente Southern California, 11975 El Camino Real, 92103, San
Diego, CA, USA. 4Independent researcher, Rio de Janeiro, Brazil. 5Department
of Comparative Medicine, Yale School of Medicine, 06520 New Haven, CT,
USA. 6Lister Hill National Center for Biomedical Communications, U.S. National
Library of Medicine, 8600 Rockville Pike, Bethesda, MD, USA.
Received: 20 July 2018 Accepted: 16 November 2018
RESEARCH Open Access
DMTO: a realistic ontology for standard
diabetes mellitus treatment
Shaker El-Sappagh1*, Daehan Kwak2, Farman Ali3 and Kyung-Sup Kwak3*
Abstract
Background: Treatment of type 2 diabetes mellitus (T2DM) is a complex problem. A clinical decision support system
(CDSS) based on massive and distributed electronic health record data can facilitate the automation of this process and
enhance its accuracy. The most important component of any CDSS is its knowledge base. This knowledge base can be
formulated using ontologies. The formal description logic of ontology supports the inference of hidden knowledge.
Building a complete, coherent, consistent, interoperable, and sharable ontology is a challenge.
Results: This paper introduces the first version of the newly constructed Diabetes Mellitus Treatment Ontology (DMTO)
as a basis for shared-semantics, domain-specific, standard, machine-readable, and interoperable knowledge relevant to
T2DM treatment. It is a comprehensive ontology and provides the highest coverage and the most complete picture of
coded knowledge about T2DM patients current conditions, previous profiles, and T2DM-related aspects, including
complications, symptoms, lab tests, interactions, treatment plan (TP) frameworks, and glucose-related diseases and
medications. It adheres to the design principles recommended by the Open Biomedical Ontologies Foundry and is
based on ontological realism that follows the principles of the Basic Formal Ontology and the Ontology for General
Medical Science. DMTO is implemented under Protégé 5.0 in Web Ontology Language (OWL) 2 format and is publicly
available through the National Center for Biomedical Ontologys BioPortal at http://bioportal.bioontology.org/ontologies/
DMTO. The current version of DMTO includes more than 10,700 classes, 277 relations, 39,425 annotations, 214 semantic
rules, and 62,974 axioms. We provide proof of concept for this approach to modeling TPs.
Conclusion: The ontology is able to collect and analyze most features of T2DM as well as customize chronic TPs with
the most appropriate drugs, foods, and physical exercises. DMTO is ready to be used as a knowledge base for
semantically intelligent and distributed CDSS systems.
Keywords: Clinical decision support system, Treatment plan, Ontology, Knowledge modeling, Diabetes mellitus
Background
Diabetes is a complex and potentially debilitating chronic
disease [1]. It affects many individuals, and represents a
global health burden with a financial impact on national
healthcare systems [2]. Diabetes has two main clinical cat-
egories: type 1 diabetes mellitus (T1DM) and type 2 dia-
betes mellitus (T2DM). T2DM accounts for 9095% of
new cases. In both conditions, continuous medical care is
required to minimize the risk of acute and long-term
complications. T1DM can only be treated with insulin,
whereas patients with T2DM have a wide range of
therapeutic options available, including lifestyle changes
and administration of multiple oral and/or injectable
anti-diabetes drugs, including insulin [3, 4]. This study
concentrates on the non-insulin medications for T2DM,
which is a risk factor for cardiovascular diseases and
microvascular complications [5].
Lifestyle changes, including a healthy diet, weight loss,
increased physical activity, self-monitoring of blood
glucose, and diabetes self-management education, can
help a patients efforts at controlling hyperglycemia.
However, they may not be adequate for controlling the
disease in the long term, and most patients will require
pharmacotherapy intervention to achieve and maintain
glycemic control [6]. Individualized choices of medica-
tions for patients are a challenge, because the number
* Correspondence: shaker_elsapagh@yahoo.com; kskwak@inha.ac.kr
1Information Systems Department, Faculty of Computers and Informatics,
Benha University, Banha Mansura Road, Meit Ghamr - Benha, Banha, Al
Qalyubia Governorate 3000-104, Egypt
3Department of Information and Communication Engineering, Inha
University, 100 Inharo, Nam-gu, Incheon 22212, South Korea
Full list of author information is available at the end of the article
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
El-Sappagh et al. Journal of Biomedical Semantics  (2018) 9:8 
DOI 10.1186/s13326-018-0176-y
of medications used to treat diabetes has dramatically
increased in the past few years. T2DM patients are usu-
ally treated with multiple drugs, and the choice differs
according to each patients profile [57].
The most recent T2DM clinical practice guidelines
(CPGs), including those from the American Diabetes
Association (ADA) [5], Diabetes Canada (formerly the
Canadian Diabetes Association) [8], and the European
Association for the Study of Diabetes (EASD), recom-
mend patient-centered and individualized diabetes ther-
apy goals based on life expectancy, duration of diabetes,
presence of comorbidities, potential for hypoglycemia
or other adverse events, and other profile features [9, 10].
The tailored therapy decision for a specific patient is
complex, because these decisions include checking many
interrelated symptoms, and choosing from various medica-
tions and lifestyle plans [11]. T2DM patients usually take
more than one drug, and drug interactions may occur. The
risk of harmful drug interactions that can cause hypergly-
cemia, hypoglycemia, nephropathy, retinopathy, gastropar-
esis, and sexual dysfunction (among other deleterious
effects) increases exponentially as the number of medica-
tions in a patients regimen increases [12, 13]. Interactions
can occur between different T2DM drugs, between drugs
and complications from diabetes, between drugs and foods,
or between drugs and exercise [1315]. In addition, the
T2DM pathophysiology involves at least seven organs and
tissues, including the pancreas, the liver, skeletal muscle,
adipose tissue, the brain, the gastrointestinal tract, and the
kidneys. Many treatment agents affect the seven organs in-
volved in the pathogenesis of T2DM. Each agent has a
mechanism of action on these organs, and each has ad-
verse effects and contraindications. Not every patient with
T2DM will respond the same way to a given treatment.
The reason might be that physicians do not take all of the
patients characteristics under consideration, including
Judkins et al. Journal of Biomedical Semantics  (2018) 9:15 
https://doi.org/10.1186/s13326-018-0183-z
RESEARCH Open Access
Extending the DIDEO ontology to include
entities from the natural product drug
interaction domain of discourse
John Judkins1, Jessica Tay-Sontheimer2, Richard D. Boyce3 and Mathias Brochhausen1*
Abstract
Background: Prompted by the frequency of concomitant use of prescription drugs with natural products, and the
lack of knowledge regarding the impact of pharmacokinetic-based natural product-drug interactions (PK-NPDIs), the
United States National Center for Complementary and Integrative Health has established a center of excellence for
PK-NPDI. The Center is creating a public database to help researchers (primarly pharmacologists and medicinal
chemists) to share and access data, results, and methods from PK-NPDI studies. In order to represent the semantics of
the data and foster interoperability, we are extending the Drug-Drug Interaction and Evidence Ontology (DIDEO) to
include definitions for terms used by the data repository. This is feasible due to a number of similarities between
pharmacokinetic drug-drug interactions and PK-NPDIs.
Methods: To achieve this, we set up an iterative domain analysis in the following steps. In Step 1 PK-NPDI domain
experts produce a list of terms and definitions based on data from PK-NPDI studies, in Step 2 an ontology expert
creates ontologically appropriate classes and definitions from the list along with class axioms, in Step 3 there is an
iterative editing process during which the domain experts and the ontology experts review, assess, and amend class
labels and definitions and in Step 4 the ontology expert implements the new classes in the DIDEO development
branch. This workflow often results in different labels and definitions for the new classes in DIDEO than the domain
experts initially provided; the latter are preserved in DIDEO as separate annotations.
Results: Step 1 resulted in a list of 344 terms. During Step 2 we found that 9 of these terms already existed in DIDEO,
and 6 existed in other OBO Foundry ontologies. These 6 were imported into DIDEO; additional terms from multiple
OBO Foundry ontologies were also imported, either to serve as superclasses for new terms in the initial list or to build
axioms for these terms. At the time of writing, 7 terms have definitions ready for review (Step 2), 64 are ready for
implementation (Step 3) and 112 have been pushed to DIDEO (Step 4). Step 2 also suggested that 26 terms of the
original list were redundant and did not need implementation; the domain experts agreed to remove them. Step 4
resulted in many terms being added to DIDEO that help to provide an additional layer of granularity in describing
experimental conditions and results, e.g. transfected cultured cells used in metabolism studies and chemical reactions
used in measuring enzyme activity. These terms also were integrated into the NaPDI repository.
Conclusion: We found DIDEO to provide a sound foundation for semantic representation of PK-NPDI terms, and we
have shown the novelty of the project in that DIDEO is the only ontology in which NPDI terms are formally defined.
Keywords: Biomedical ontologies, OWL, Pharmaceuticals, Pharmacokinetics, Drug-drug interactions, Natural
product-drug interactions
*Correspondence: mbrochhausen@uams.edu
1Department of Biomedical Informatics, University of Arkansas for Medical
Sciences, Little Rock, AR, USA
Full list of author information is available at the end of the article
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Judkins et al. Journal of Biomedical Semantics  (2018) 9:15 Page 2 of 9
Background
Concomitant use of prescription drugs and natural prod-
ucts, including vitamin, mineral, or herbal supplements,
is a frequent occurrence. The high prevalence of natural
products (NP) usage raises concerns about the poten-
tial impact on drug effectiveness and toxicity from nat-
ural product drug interactions. Pharmacokinetic-based
natural product-drug interactions (PK-NPDIs) are of par-
ticular concern because their potential impact on drug
effectiveness or toxicity is often unknown.
To provide evidence-based information regarding pur-
ported PK-NPDIs, a new Center of Excellence for PK-
NPDI Research was established by the United States
National Center for Complementary and Integrative
Health (grant number U54 AT008909). The Center is
creating a publicly accessible database where researchers
can access scientific results, raw data and recommended
approaches to optimally assess the clinical significance
of PK-NPDIs. One of the requirements of the reposi-
tory is that it represent data in a semantically rich and
interoperable way.
There have been previous efforts to provide ontolog-
ical representation of this domain. We reviewed exist-
ing ontologies to consider re-use. Searching the NCBO
Bioportal [1] retrieves the Natural Product Ontology
(NATPRO) [2], which we consider a potentially relevant
resource for our project. According to an article by the
person whose name appears as contact on the Bioportal
site for NATPRO [3], it seems that the use case under
which NATPRO was developed was mining information
about natural products to bring new ideas to drug devel-
opment, which is similar to our own goals. NATPRO
was submitted to the NCBO Bioportal in 2012 and, at
the time of writing, there are no reported updates. The
NCBO BioPortal landing page for NATPRO does not pro-
vide additional documentation or links to resources (e.g. a
code repository). The ontology contains 9465 classes and
22012 individuals, is based on the BioTop ontology [4], a
top-domain ontology for the life sciences, re-uses classes
from Chemical Entities of Biomedical Interest (ChEBI) [5]
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 
https://doi.org/10.1186/s13326-018-0177-x
RESEARCH Open Access
Supporting shared hypothesis testing in
the biomedical domain
Asan Agibetov1,6 , Ernesto Jiménez-Ruiz2, Marta Ondrésik3,4, Alessandro Solimando5, Imon Banerjee1,7,
Giovanna Guerrini5*, Chiara E. Catalano1, Joaquim M. Oliveira3,4, Giuseppe Patanè1,
Rui L. Reis3,4 and Michela Spagnuolo1
Abstract
Background: Pathogenesis of inflammatory diseases can be tracked by studying the causality relationships among
the factors contributing to its development. We could, for instance, hypothesize on the connections of the
pathogenesis outcomes to the observed conditions. And to prove such causal hypotheses we would need to have
the full understanding of the causal relationships, and we would have to provide all the necessary evidences to
support our claims. In practice, however, we might not possess all the background knowledge on the causality
relationships, and we might be unable to collect all the evidence to prove our hypotheses.
Results: In this work we propose a methodology for the translation of biological knowledge on causality
relationships of biological processes and their effects on conditions to a computational framework for hypothesis
testing. The methodology consists of two main points: hypothesis graph construction from the formalization of the
background knowledge on causality relationships, and confidence measurement in a causality hypothesis as a
normalized weighted path computation in the hypothesis graph. In this framework, we can simulate collection of
evidences and assess confidence in a causality hypothesis by measuring it proportionally to the amount of available
knowledge and collected evidences.
Conclusions: We evaluate our methodology on a hypothesis graph that represents both contributing factors which
may cause cartilage degradation and the factors which might be caused by the cartilage degradation during
osteoarthritis. Hypothesis graph construction has proven to be robust to the addition of potentially contradictory
information on the simultaneously positive and negative effects. The obtained confidence measures for the specific
causality hypotheses have been validated by our domain experts, and, correspond closely to their subjective
assessments of confidences in investigated hypotheses. Overall, our methodology for a shared hypothesis testing
framework exhibits important properties that researchers will find useful in literature review for their experimental
studies, planning and prioritizing evidence collection acquisition procedures, and testing their hypotheses with
different depths of knowledge on causal dependencies of biological processes and their effects on the observed
conditions.
Keywords: Biomedical ontology, Ontology mapings, Network analysis, Hypothesis testing, Incomplete knowledge
Background
Diseases and pathologies may be evidenced across mul-
tiple biological scales (e.g., cellular, molecular, organic,
behavioural) as a set of factors, linked among each other
via causal relationships, which constitute the multi-scale
pathological cascade reactions. To study the underlying
*Correspondence: giovanna.guerrini@unige.it
5University of Genoa, Genoa, Italy
Full list of author information is available at the end of the article
causation mechanism of a certain disease, life science
researchers rely on various sources, such as (i) current
knowledge (e.g. previously published studies from the
field), (ii) their data deduced from empirical analysis
of laboratory experiments (e.g., gene analysis, immuno-
assays, cell viability assays, histology) or other tests (i.e.
mechanical tests, imaging, gait analysis), as well as on (iii)
consultations with other fields (i.e. related research areas,
hospitals). To effectively make and test (prove or reject) a
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 2 of 22
causality hypothesis life science research studies face two
challenges: i) the information used in research processes
comes from various sources and is heterogeneous, which
makes it hard to organize, analyze, and assess their rele-
vance in the overall disease process, ii) researchers from
different fields (i.e. molecular biologist, mechanobiologist,
orthopaedists etc.) investigate the same pathological event
from different aspects (biological scales), and might not
be aware of the overlaps and the impact of their individ-
ual findings in a joint venture of understanding causality
mechanisms of pathologies and diseases.
To better convey the idea of causality hypothesis test-
ing we will focus on knee articular cartilage degeneration
during the onset of osteoarthritis (OA) to present our use-
case scenario. OA is a joint degenerative disease and can
be caused due to several factors, such as genetic predis-
position, joint overuse, previous injury to the joint. The
effect of these factors is hallmarked with a complete joint
breakdown and dysfunction, causing a lot of pain [1, 2].
Based on common knowledge, performed experiments,
and diagnosis the causality relation of certain factors to
the development of OA might have different degrees of
confidence. On the one hand, the degeneration of carti-
lage, synovial thickening, osteophyte formation and joint
space narrowing, are known to be as the most marked
features of OA [36]. On the other hand, for some fac-
tors we may have lower degrees of confidence in their
causality relationship to OA. For instance, while being
common in patients with OA, the exact causality rela-
tion of inflammation to OA is not completely understood
[7, 8]. To handle such scenarios of causality hypothe-
sis testing, we propose to translate what we observe in
the biology into a computational framework, which sup-
ports the researchers in their hypothesis testing. In such
a framework we systematically translate our background
knowledge on causality relationships into the represen-
tations suitable for the computation, and we quantify
confidences in our hypothesis with respect to the amount
of evidences that we can supply to the framework.
Hypothesis testing
Schematically, the causality relationships between the fac-
tors of diseases can be represented as directed causal-
ity networks H0...n, where factors fi are represented as
nodes and the causality relationships as arcs (fi, fj). For
instance, our hypothesis H0 can state that inflammation
contributes to the development of OA, where the inflam-
mation is the cause of biological processes which lead to
cartilage degradation (factor f2, Fig. 1) and finally mani-
fest in joint deformation condition (factor f3, Fig. 1). To
prove such a causality hypothesis we need to evidence the
instances of all the participating factors. For example, the
factors f2, f3 are evidenced as the results of diagnosis of
OA done by radiologists and orthopaedists using imag-
ing techniques (i.e. magnetic resonance-MRI, X-ray). By
studying the literature we can discover that the inflamma-
tion can be characterized by the detection of high levels
of pro-inflammatory factors in the synovial cavity, and in
particular tumor necrosis factor alpha (TNF?) (factor f1
in Fig. 1), was demonstrated to be present in excess dur-
ing OA [9]. A justification or evidence for the factor f1
(evidence of f1 in Fig. 1) can be obtained with molecu-
lar biological techniques screening the biomarkers of the
synovial fluid. Given our knowledge of the participating
Fig. 1 Causality hypothesis of TNF alpha overproduction leading to cartilage degeneration and provoking joint deformation
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 3 of 22
biological processes (hypothesis H0) and the supporting
evidences (evidences for factors f1, f2, f3) we have a cer-
tain level of confidence that the synovial inflammation has
been the cause of the development of OA. However, is
our hypothesisH0 complete enough, and are the evidences
for factors (f1, f2, f3) enough to support our hypothesis?
Have we missed other factors? Have we been complete
enough in our characterization of all the participating
factors which support the hypothesis that the synovial
inflammation has been the cause of cartilage degradation?
Is the joint deformation the only consequence of such a
pathological cascaded of reactions?
Studying further the causality mechanism of OA, we
can refine our initial hypothesis H0. In particular, cel-
lular biological studies observed that TNF? facilitates
the catabolic processes of the chondrocytes, including
the production of matrix metalloproteinases (MMPs),
and the production of aggrecanases (members of the
ADAMTs family) [10, 11]. The MMPs, especially MMP-
13 and aggrecanases are proteases responsible for the
degradation of collagen macromolecules and proteogly-
cans respectively, as evidenced in literature [12]. Collagens
and proteoglycans are the main building blocks of articu-
lar cartilage. Accordingly, the excess of TNF? in the joint
space can be associated to the disruption of biochemical
balance in the cartilage. Factors: Loss of collagen and pro-
teoglycan molecules (factors f4, f5 in Fig. 2), are caused
by the action of matrix degrading proteases, and can be
attached to higher scales in the OA processes, such as the
mechanical functioning of cartilage. These factors can be
evidenced on the tissue level by histology and immuno-
histochemistry (evidences of f4, f5 in Fig. 2). Collabora-
tions with mechano-biological fields allow the detection
of the changes in cartilage mechanical properties due
to the effect of high levels of MMPs and aggrecanses
[13, 14]. It has been shown previously that once the carti-
lage suffers collagen loss, it is no longer able to withstand
the mechanical forces in the knee [15, 16]. Consequently,
the cartilage, the trabecular bone beneath it, and all sur-
rounding tissue components suffer damage, which can be
evidenced by imaging [17, 18]. Damage to the joint com-
ponents, will cause pain, joint deformation and loss of
function, which is a subject of behavioural scales and can
be evidenced by gait analysis [19].
The relationship between inflammation and OA is even
more complex, than the example brought above. Nonethe-
less, collaborations among medical doctors and bench
researchers of various fields can reveal the connections
between molecular evidence and those observed on organ
scale. Accordingly, we can refine our hypothesis by adding
new causal relationships.
Shared hypothesis testing framework
In this work we propose a methodology for the translation
of biological knowledge on causality relationships of
biological processes and their effects on conditions to a com-
putational framework for hypothesis testing. The method-
ology consists of two main points: hypothesis graph
Fig. 2 Refined causality hypothesis of pro-inflammatory factors leading to loss of building blocks of articular cartilage  collagen and proteoglycan ,
which in turn lead to cartilage degeneration and provoking joint deformation
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 4 of 22
construction from the formalization of the background
knowledge on causality relationships, and confidence
measurement in a causality hypothesis as a normalized
weighted path computation in the hypothesis graph. In
this framework, we can simulate collection of evidences
and assess confidence in a causality hypothesis by mea-
suring it proportionally to the amount of available knowl-
edge and collected evidences. We evaluate our method
on an example causality hypothesis of factors which cause
and, in turn, may be caused by cartilage degeneration
during osteoarthritis. The results of the evaluation and
the feedback from the domain experts allow us to con-
clude that our methodology may simulate the execution
of evidence collection, and can be used as a means of
measuring the confidence in a causality hypothesis with
respect to the amount of knowledge on causality rela-
tionships among participating factors. Such simulation
supports the researchers in the planning and in the pri-
oritization of their next studies by identifying impor-
tant factors in a causality hypothesis. Our methodology
demonstrates robustness towards the addition of poten-
tially inconsistent knowledge by separately representing
opposite causality possibilities for complementary biolog-
ical scenarios.
We would like to emphasize that the contribution of
this work is the methodology to extract the causality
information from the input ontologies into a hypothesis
graph, and perform hypothesis testing on the obtained
hypothesis graph. The ontologies and the ontology map-
pings discussed and provided are created together with
the domain experts, and in the context of this work are
only meant to serve as proof of concept.
Related work
To the best of our knowledge the proposed method-
ology to test a causality hypothesis in a collaborative
setting with respect to the amount of knowledge avail-
able for the framework does not have an equivalent
methodology or an implemented system to test against,
in its entirety. However, once decomposed, our method-
ology can be compared on specific steps and modelling
choices.
Formalization of background knowledge on a causal
hypothesis as ontologies. Our methodology for causal-
ity hypothesis testing relies on the formalization of the
background knowledge on a hypothesis with ontologies.
Indeed, to facilitate knowledge sharing and increase
understanding of the method in use, it is common to
employ already existing ontologies that are well agreed
on in the biomedical community (e.g., Gene Ontology
[20]). The most widely used ontology modeling language
is the (OWL 2) [21], based on formal logic [22]. The main
advantage of using logic over alternative representation
mechanisms is that logic provides an unambiguous mean-
ing to ontologies. We assume that the input ontologies
to our framework focus on (biological) processes and
findings (i.e., laboratory tests) that are or may be linked
via a causality relationship, and other (material) entities
that (actively or passively) participate in the process or
finding. In this work we assume that the input ontologies
follow good practices and relevant ontology classes are
either subsumed by or annotated with, for example,
the concept Biological_process (key concept in
the Gene Ontology [20]) or Finding (e.g., common
semantic type in the UMLS semantic network [23]). We
expect the following (object) properties or its poten-
tial subproperties as source for causality relationships:
causes, results in, regulates, positively
regulates, negatively regulates, increases
levels of and decreases levels of. Most
of these properties are available in the Relations ontology
[24] and are extensively used in biomedical ontologies.
We reuse the domain independent categories Continuant
and Occurrent, which are commonly used in the literature
(e.g., River Flow Model of Diseases (RFM) [25]) and in
upper ontologies (e.g., DOLCE [26] and BFO [27]). For
example, processes and findings are typically classified as
occurrents, while material entities as continuants.
Graph projection of OWL ontologies. The hypothesis
graph construction heavily relies on the graph projection
of OWL ontologies. This procedure, at its core, trans-
forms an OWL ontology into its graph representation,
by studying the axiomatic structure of the ontology and
identifying nodes and edges (arcs) of its equivalent graph
representation. Implicitly, Lembo et al. [28] use graph pro-
jections of OWL QL to propose ontology classification
algorithm, which transforms OWL QL ontologies into
directed graphs, and computes subsumption relations via
transitive closure computation. Analogously, Seidenberg
et al. [29] use graph representation of ontologies to pro-
pose a segmentation algorithm based on subgraph extrac-
tion procedure. Some of the proposed methodologies for
graph projection of OWL ontologies draw their inspira-
tions from Social Network Analysis (SNA) [30] for the
representation of the encoded semantic information in an
OWL ontology. SNA is the process of investigating social
structures of connected information/knowledge entities
through the use of network and graph theories. SNA
techniques application to ontology analysis has been pio-
neered by Hoser et al. [31], where standards in SNA
community graph metrics based on: node degree, node
betweenness and on eigenanalysis of the adjacencymatrix,
were used to study properties of ontologies. The con-
nection between SNA and ontology analysis have also
been studied in a highly cited paper by Mika [32], bridg-
ing Social Networks and Semantics. Network partitioning
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 5 of 22
algorithms have been used by Stuckenschmidt et al. [33]
to identify islands of ontology, a notion comparable to a
module of ontology (as used by the graph-based modu-
lar extraction community), with the applications to Visual
Analytics. Grontocrawler [34] transforms OWL-EL [35]
ontologies into networks by defining a rule-based edge
production procedure, which takes into account exis-
tential and values restrictions on object relations. For-
mal treatment of rule-based graph projection procedures
and their connection to the logical entailment prob-
lem for OWL 2 ontologies have been recently proposed
[3638]. In our work we use Grontocrawler [34] for graph-
based ontology projection, enriched with the projection of
advanced OWL 2 axioms, as suggested in Soylu et al. [38].
Rule-based reasoning with incomplete knowledge in
the biomedical domain. Similarly to previous works
[39, 40], we focus on graph-based reasoning with incom-
plete knowledge, by analyzing OWL ontologies, to sup-
port researchers in the biomedical domain. In particular,
Larson et al. [39] propose a method for rule-based reason-
ing with a multi-scale neuroanatomical ontology, where
the authors conclude that OWL is an important technol-
ogy formerging disparate data and performingmulti-scale
reasoning. They demonstrate how OWL-based ontolo-
gies and rule-based reasoning help infer novel facts about
brain connectivity at large scale from the existence of
synapses at a micro scale. Oberkampf et al. [40] pro-
pose a methodology for interpreting patient clinical data
(medical images and reports), semantically annotated by
concepts from large medical ontologies. They introduce
an ontology containing lymphoma-related diseases and
symptoms as well as their relations and use it to infer likely
diseases of patients based on annotations.
In contrast to Larson et al. [39] our graph-based rea-
soning method relies on network analysis of the final
hypothesis graph, which presents an advantage of a full
overview of all possible conclusions with the quantifica-
tion of the confidence measure induced by the number
of evidences that have been collected and the final topol-
ogy of the hypothesis graph. Oberkampf et al. [40] focus
on the problem of inferring likely diseases in the presence
of patient-specific evidences, represented as symptoms,
and the similarity of the diseases is then ranked based on
their distances to the symptoms. The focus of our work
and the methodology are different. We tailor our causality
hypotheses to a single diseases and study causality rela-
tionships among the factors, the findings obtained with
our methodology may have impact not only in the clin-
ical, patient-specific setting, but can be used in general
research. Technically, our methodology for graph pro-
jections employs a rich set of OWL 2 axioms, and go
beyond the usual taxonomical relationships which can be
extracted from the ontologies.
Probabilistic methodologies for reasoning with
incomplete knowledge and causality inference, with
applications in the biomedical domain. In a more gen-
eral setting, not necessarily connected to the biomedical
domain, there are examples of general theoretical frame-
works which marry formal methods (e.g., First-Order
Logic) and probabilistic models (e.g., stochastic processes)
[4143]. Application of those methodologies in biology is
studied in Ciocchetta et al. [44] who tune the Stochastic
Process Algebra language PEPA [43] to model biological
pathways and complex biological networks, involving
stochastic processes. This line of works bridge uncer-
tainty and formal methods for general frameworks for
reasoning with incomplete knowledge in biology, and
differently with our methodology is not compatible with
OWL ontologies, and thus cannot benefit from OWL
reasoning tasks (e.g., classification, alignment).
Our work is perhaps similar in spirit to that of Pearl
et al. [45, 46], where the authors advocate for a paradig-
matic shift that must be undertaken in moving from tradi-
tional statistical analysis to causal analysis of multivariate
data [45, 46]. Pearl et al. propose a formal treatment and
a unified methodology for the graphical representation of
joint probability distributions along with rules for infer-
ring causality directly from such graphical representa-
tions. In particular, the directed graphs are introduced as
a compact way of representing conditional independence
restrictions for complex multidimensional probability dis-
tributions. In contrast, in our work we do not stress the
existence of joint probability distributions between the
factors of a hypothesis. Rather, we rely on expert knowl-
edge of causality relationship between the factors, already
known to the community, such as knowledge graphs
which can be obtained from literature sources, and/or can
be formalized in an OWL ontology by the domain experts.
Methods
Herein we assume that there exists a universal causal-
ity hypothesis H that can be represented as a network
of factors with causality relationships, which we call
a hypothesis graph. The background knowledge on the
hypothesis graphH is formalized in an ontologyO, which,
for instance, may define factors as biological processes and
conditions, and the causality relationships may indicate
the connections between them. Moreover, we assume that
different experts formalize the background knowledge on
H in ontologiesOi=1...n, such that eachOi highlights a cer-
tain subpart of this hypothesis graph H. Consider O1 =
?RboxO1 , TboxO1?, O2 = ?RboxO2 , TboxO2? in Fig. 3, the
examples of formalization of the the causality relation-
ships among biological processes that participate in OA
pathogenesis, from two different points of view.
The overlaps among the ontologies Oi may or may
not exist and, as the number of ontologies increases,
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 6 of 22
Fig. 3 Formalization of knowledge on OA pathogenesis processes
we assume that it is possible to assemble (align) these
ontologies. The assembled ontology
?n
i Oi = O repre-
sents the iteratively gathered and formalized biological
and biomedical knowledge on the hypothesis graph H.
Finally, the causality hypothesis graph H  the network of
factors interconnected with causality relationships  can
be extracted from the assembled ontology O at any given
point in time ti (Ht0 , . . . ,Htn ). As a consequence, the shape
of the causality hypothesis Hti depends on the amount
of background knowledge formalized in O at ti. Finally,
the hypothesis graph construction from ontologies is per-
formed in a three-step process: (1) projection of OWL 2
ontologiesO1, . . . ,On into ontology graphsG1, . . . ,Gn, (2)
assembly of the ontology graph G from G1, . . . ,Gn, and
(3) normalization of the graph G to obtain the hypothesis
graph H (Fig. 4).
Graph-based ontology projections
The nodes of the ontology-graph are unary predicates and
edges are labelled with possible relations between such
elements, that is, binary predicates. The key property of
this ontology-graph is that every X-labelled edge e = (v,w)
is justified by one or more axioms entailed by the ontology
which semantically relates v to w via X. For exam-
ple, edges e of the form A broader????? B are justified by
the OWL 2 axiom: B SubClassOf: A . We rely on
the OWL 2 reasoner HermiT [47] to build the ontology
graph (e.g., extraction of classification) to consider both
explicit and implicit knowledge defined in the ontology
O. In the following, A,Asup,Asub,B,Bi represent classes,
while R, S, Si,R? represent object properties. Edges e of
the form A R?? B are justified by the following OWL 2
axioms:
(i) A SubClassOf: R restriction B, where restriction
is one of the following: some (existential
restriction), only (universal restriction),min x
(minimum cardinality),max x (maximum
cardinality) and exactly x (exact cardinality).
Note that axioms with an union of classes in the
restriction (e.g. A SubClassOf: R restriction
B1 or . . . or Bn) or an intersection of classes in the
restriction (e.g. A SubClassOf: R restriction
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 7 of 22
Fig. 4 Our methodology defines a pipeline to transform background knowledge into a hypothesis graph via sequential application of processing
steps: projection of input Oi ontologies into ontology graphs Gi , assembly of an ontology graph G with input ontology mappingsmi , normalization
of the ontology graph G into a final hypothesis graph H
B1 and . . . and Bn) also justify edges of the form
A R?? Bi with 1 ? i ? n.
(ii) Nesting (one level) with the same object property:
A SubClassOf: R restriction (R restriction B),
being R transitive.
(iii) Nesting (one level) with different properties:
A SubClassOf: R restriction (S restriction B),
and the role chain axiom of the form:
R ? S SubPropertyOf: R.
(iv) A combination of range and domain axioms of the
form: R Domain: A and R Range: B.
(v) Role chain axiom of the form: S0 ? · · · ? Sn
SubPropertyOf: R when the ontology graph
already includes the edges A S0?? C1 . . .Cn Sn?? B.
(vi) R InverseOf: R? when the ontology graph already
includes the edge B R
??? A.
(vii) Top-down propagation of restrictions:
A SubClassOf: Asup when the ontology graph
already includes the edge Asup
R?? B.
(viii) Entailment among restrictions:
Bsub SubClassOf: B when the ontology graph
already includes the edge A R?? Bsub.
Assembly of ontology graphs
The ontologies formalizing the hypothesis graph may
be created by different group of experts with dif-
ferent modelling (e.g., defining relationships between
occurrents, or between ocurrents and continuants)
and naming conventions. For example, a group may
use the concept Cartilage degradation (occur-
rent) from SNOMED-CT [48] while another may pre-
fer to use the concept negative regulation of
cartilage development (occurrent) from the GO
[20]. Furthermore, other groups would rather use the con-
cept Cartilage (continuant) and push the semantics of
degradation into the ontology property.
Ontology alignment will enable the integration and
assembly of the (sub-)ontology graphs in a larger ontol-
ogy graph. An ontology alignment is composed by
a set of ontology mappings. An ontology mapping
m between two concepts C1,C2 from the vocabulary
of two different ontologies O1,O2 can be defined as
follows: m = ?C1,C2, r?, where r is the relation
between C1 and C2 and, using SKOS vocabulary, it
can be of one of the following types: skos:exactMatch,
skos:closeMatch, skos:relatedMatch, skos:narrowMatch or
skos:broadMatch.
Mappings to guide the assembly (i.e., link factors
from different hypothesis) can be discovered in online
resources like UMLS Metathesaurus [49] and BioPortal
[50, 51], or using state of the art ontology alignment
systems like LogMap [52] and AML [53]. Mappings
in UMLS Metathesaurus or BioPortal typically repre-
sent correspondences of the type skos:exactMatch and
skos:closeMatch,1 while the output provided by automatic
systems will typically provided mappings of diverse type
and quality.
If a mapping exists to link two factors f1 and f
?
1 from
two different (sub-)ontology graphs, then these two fac-
tors are merged into one. The weight of the merged
factor will be according to the type of the ontology map-
ping. In our setting, we assume the following weight
values w (ranging from 0 to 1) depending on the map-
ping type: (1) skos:exactMatch mappings are associated
with a weight value 1.0, (2) skos:closeMatch mappings
with 0.75, while (3) skos:relatedMatch, skos:narrowMatch
and skos:broadMatch with a weight of 0.5. The weight
associated to each (merged) factor will play a key role
in our methodology for confidence measurement in a
hypothesis.
Normalization of the assembled graph
The final step of hypothesis graph construction is the
normalization of the assembled hypothesis graph, which
pushes the rich semantics of causality relationships (e.g.,
edges of the type A R?? B ) into, possibly newly created,
nodes. Generally speaking, the normalization procedure
leads to a simplified representation of all the available
facts on causality relationships as a directed graph with
specific constraints on the types of nodes and edges.
Specifically, we aim to build a 1-mode network where all
the nodes represent the same fundamental metaphysical
type (occurrent), and all the edges represent the simplified
causality relationship defined between two occurrents.
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 8 of 22
This is necessary because the general graph projection
step of our pipeline might produce semantic networks of
concepts where the concepts and the edges may have dif-
ferent types. For instance, the ontology graph may contain
edges representing causality relationships involving both
an occurrent and a continuant  two fundamentally dif-
ferent metaphysical types of concepts. Additionally, the
semantics of causality relations may reflect complemen-
tary effect when we consider causal chains in the hypothe-
sis graph, for instance negative and positive regulations of
biological processes. The hypothesis graph normalization
consists in iterative rewriting of the graph, where we fil-
ter all edges and rewrite them according to the following
patterns:
(i) Occurrent R?? Occurrent where R represent the
property results in or causes justifies the edge in the
hypothesis graph Occurrent ? Occurrent. For
example, if the ontology contains the axiom,
Chondrocyte catabolism SubClassOf:
results in some Collagen degradation
the ontology graph will include the edge
Chondrocytes catabolism
results in??????
Collagen degradation and the hypothesis
graph will contain the causality relationship
Chondrocytes catabolism ? Collagen
degradation.
(ii) Occurrent R?? Occurrent where R represent the
property positively regulates or
negatively regulates. In this case the
positive or negative semantics of the property are
pushed to a fresh ocurrent concept. For example, if
the ontology projection contains the edge
Chondrocytes anabolism
positively regulates???????????
Collagen production, we will add the causal
relationship Chondrocyte anabolism ?
Positive regulation of Collagen
production.
(iii) Occurrent R?? Continuant where R represent the
property positively regulates,
negatively regulates, increases
levels of or decreases levels of. For
example if the ontology graph includes the edge TNF
alpha overproduction
decreases levels of???????????
Collagen the hypothesis graph will include the
fresh term Decreased levels of Collagen
(or Loss of Collagen) and the causal
relationship TNF alpha overproduction ?
Decreased levels of Collagen.
In Fig. 5 we illustrate the whole pipeline of construct-
ing a hypothesis graph H from the two input ontologies
O1,O2, defined in Fig. 3. The two ontology graphs G1,G2
represent the individual extent of background knowledge
of the two specialists on causality relationships of fac-
tors between synovial inflammation and cartilage degra-
dation (obtained by projecting ontologies O1,O2). The
assembly of the graphs takes as input the ontology map-
pings m1 and m2 (see Table 1), which have been manu-
ally created by the domain experts, to merge the graphs
G1,G2. Overall, the graph projection and the graph assem-
bly steps of the pipeline work in couple to entail new
causal links among the factors, which we represent in
the assembled graph G. For instance, once we align the
two graphs we entail the circular causality relationship,
which states that Synovial inflammation may be,
simultaneously, the cause and the effect of Cartilage
degradation. Notice that before the alignment the two
specialists were not aware of this circular relationship. The
normalization of the assembled graphG splits the two bio-
logical scenarios of chondrocytes anabolic and catabolic
activities, such that the resulting hypothesis graph H con-
tains only unambiguous causality relations among the
factors.
Measuring confidence in a hypothesis
Once we obtain the hypothesis graph H, we are ready
to form the causality hypothesis and perform evidence-
based hypothesis testing. Before we delve into this topic,
we briefly introduce the notation that we use for the
hypothesis graphs throughout this work.
Notation for hypothesis graphs. LetH = (N ,A) be a dir
ected graph, which we call hypothesis graph, with ni ? N
set of nodes. And A is a set of ordered pairs of (s, t) in N,
called arcs, where s denote the source of the arc, and t the
target of the arc [54]. A path ?(s, t) from source node s
to the target node t is denoted as ?i(s, t) = (s, ni, . . . , t).
We write (s, t) to denote all possible simple paths in
the hypothesis graph from node s to the node t. A sim-
ple path is a path which does not have repeating nodes.
And we use I(s, t) = {ni|ni ? ?i, ??i(s, t) ? (s, t)} to
refer to all the interior nodes which appear in all paths
from s to t.
Causality hypothesis. A causal hypothesis asks a ques-
tion whether some factor (s) has caused another factor (t).
There might be a direct causality relationship from s to
t, or there might exist an indirect causality relationship,
such that s has caused t through some intermediate fac-
tors, which might have participated actively or passively
to the causality chain from s to t. These causal chains
from s to t represent different possibilities of how smight
have caused t. We use the notation for hypothesis graph
H to represent factors as nodes fi ? N , direct causality
relationships as arcs (fi, fj) ? E, and causality chains as
paths (s, t).
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 9 of 22
Fig. 5 Schematic representation of the three-step pipeline for the hypothesis graph H creation from the two input ontologies O1,O2: i) use graph
projection rules to transform each ontology Oi into its graph representation, ii) assemble the hypothesis graph H from two ontology graphs by
merging concepts for which we have ontology mappingsmi , and finally iii) normalize the hypothesis graph H by extracting only the relevant
information of causality relationships among the occurrents
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 10 of 22
Table 1 Ontology mappings created manually by the domain experts
Mapping mi O1 : C1 O2 : C2 r c
m1 O1:Synovial
inflammation
O2:Synovial capsule
inflammation
skos:closeMatch 0.75
m2 O1:Biochemical
imbalance
O2:Disruption of biochemical
balance
skos:relatedMatch 0.5
Consider an example causality hypothesis that pos-
tulates that s = Positive regulation of TNF
alpha overproduction caused t = Synovial
inflammation in Fig. 6. In our example, we do not have
a direct causality relationship between these two factors,
however there exist 6 different causal chains, i.e., 6 dif-
ferent ways in which s might have caused t. In Fig. 6 we
present two possible chains of factors (Path 1, Path 2)
starting from s and leading to t.
We are confident in our causality hypothesis  within
the domain of the known facts  when we are able to
provide evidences to all the factors that participate in
causality chains from s to t. I(s, t) represents the set of
nodes in the hypothesis graph H, which correspond to
the factors that need to be evidenced, E is an indicator
set which denotes factors evidenced so far, and C(s, t, E)
be the confidence function. Intuitively, confidence in a
hypothesis should grow with the number of factors that
we are able to evidence, more factors we evidence, more
confident we are that s did indeed cause t. Since, we
might have several possibilities of s causing t we, first,
propose to measure confidence of each causality possibil-
ity separately, and then, we propose to measure overall
causality hypothesis as a sum of the confidences of all the
known possibilities (Eq. 1). To this end, our confidence
in a causality hypothesis depends on three parameters: i)
source of the causality (s), ii) target of the causality (t), and
iii) set of evidenced factors (E).
Cts (E) =
?
??(s,t)
?
f??
F(f ), (1)
Measuring confidence in a causality hypothesis propor-
tionally to the number of evidenced factors might not be
correct, there are two sources of uncertainty that might
negatively effect our confidence in the hypothesis, even
if we collect all the evidences, and should be reflected
in the way we measure confidence in the hypothesis: i)
the quality of the evidences, i.e., we can surely state that
Fig. 6 Two possible paths from the factor Positive regulation of TNF alpha overproduction to the factor Synovial
inflammation
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 11 of 22
the evidence is not due to errors, and ii) quality of our
modelling of the hypothesis. The first source of uncer-
tainty comes from the fact that during our experiments
or literature search for the justifications of evidences we
might face errors. And the second source of uncertainty
comes from the waywemodel our hypothesis as an assem-
bly of sub-hypotheses, which relies on ontology mappings
to merge formalizations of the background knowledge of
the hypothesis. During this process we might introduce
uncertainty for the matched concepts representing factors
of the hypothesis.
To this end, we introduce two functions defined on the
nodes of the hypothesis graph, ? : N ?[ 0 . . . 1] that
associates weights of the confidence in the ontology map-
ping to every factor, and represents our confidence in the
hypothesis modelling, and ? : N ?[ 0 . . . 1] associates
weights of the confidence in evidence for each factor.
Equation 2 represents the contribution function for the
hypothesis factors.
F(f ) =
?
?????
?????
0 f ? E
factor f not evidenced
?(f )?(f ) f ? E
weighted contribution
if f evidenced
(2)
Properties of the confidence function. Confidence in
causality hypothesis is defined as a sum of weighted con-
tributions of factors, that participate in causality possibil-
ities. The contributions of factors is a weighted, and most
importantly a non-negative, function (Eq. 1), thus thus as
we add more evidenced factors the value of the function,
can only grow. Confidence depends on the evidenced fac-
tors, it has its minimum value (Cts = 0) when we have no
evidences (E = ?), and it has its maximum value when
all the factors have been evidenced (argmaxCtswhen E =
I(s, t)). To this end, we can normalize our confidence
function to the maximum possible confidence value we
can obtain, when all the factors have been evidenced, such
that the confidence is always measured in the [ 0 . . . 1]
range (Eq. 3).
0 = C
t
s (E = ?)
Cts (E = I)
? C
t
s (E ? I)
Cts (E = I)
<
Cts (E = I)
Cts (E = I)
= 1. (3)
Results
With the help of our domain experts in biology and
biomechanical engineering (multi-disciplinary consor-
tium of the EU FP7 MultiScaleHuman project [55]) we
have been formalizing the background knowledge around
factors participating in the process of cartilage degrada-
tion, which can be evidenced across different biological
scales. This background knowledge has been captured, as
a proof of concept, in an OWL 2 ontology O and has been
iteratively validated with our domain experts. This ontol-
ogy has been designed to contain a significant amount
of axioms which go beyond the usual taxonomical rela-
tionships in the biomedical ontologies, and instead, model
causality relationships with rich ontology concept con-
struction operators including nested OWL restrictions
and property chains. During our interviews (t1, . . . , tn)
with the domain experts we have been updating the back-
ground knowledge formalization (Ot1 , . . . ,Otn ), either
with the help of our domain experts or by translating
discovered causality relationships from the literature our-
selves. Each snapshot of the background knowledge Oti
has been presented as the results of our methodology of
hypothesis graph constructionHti for validation and feed-
back. To report our results we fix our attention to two
specific snapshots of the causality hypothesis, and we refer
to them asHsub andHbroader .Hsub has been extracted from
the state of the ontology Oti , which corresponds to the
extent of knowledge of the molecular biologist on causal-
ity relationships between the biological processes which
lead to cartilage degradation with a focus on cellular and
molecular biological scales (Hsub is an equivalent hypothe-
sis graph to what we presented as a normalized hypothesis
graph in the Methods section). Hbroader was extracted
from the ontology Otj at time point tj, which corresponds
to the ontology Oti updated with more knowledge about
factors that lead to cartilage degradation, from organ and
behavior biological scales. Table 2 summarizes Oti ,Otj
with ontology metrics and descriptions, computed with
the Protégé ontology editor.
In Fig. 7 we notice that Hsub = ?Nsub,Asub? is a sub-
graph of Hbroader = ?Nbroader ,Abroader?, such that Nsub ?
Nbroader and Asub ? Abroader . The additional knowledge
(Hbroader/Hsub) is not present in the formalization by the
molecular biologist, meaning that he might not be aware
about alternative factors that concur during osteoarthri-
tis and might have played a significant role in the causality
hypothesis (Fig. 7). The subsequent experiments demon-
strate how our methodology supports hypothesis testing
by quantifying confidence in a causality hypothesis
with incomplete evidences, and provides means to
compare confidence measures with different depths of
knowledge.
Table 2 Oti ,Otj ontology metrics
Ontology metric Oti Otj
Axioms 66 151
Logical axiom count 39 92
Declaration axiom count 18 34
Class count 14 30
Object property count 4 4
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 12 of 22
Fig. 7 Bold contours show the normalized hypothesis graph known to the molecular biologist Hsub , whereas the dotted contours delineate the
additional knowledge of which the biologist is not aware Hbroader
Robustness of the system in presence of complementary
causality relationships
Our methodology is capable of adequately tracking
two complementary biological scenarios, where one
factor might stand as a cause of two opposite effects.
We tested our methodology for hypothesis graph con-
struction with small increments in our knowledge which
might lead to big changes in the shape of the causality
hypothesis, and what we can understand from it. In
particular, at the time point ti the knowledge on the
hypothesis contained causality path from Mechanical
loading factor to the Chondrocytes catabolism
factor. Indeed, the positive regulation of chondrocytes
catabolism by mechanical loading has been demonstrated
in the literature [56]. However, it is also known that
the mechanical loading can also have positive effect
on the chondrocytes anabolism (the opposite
biological process of catabolism), and thus facilitate
proteoglycan and collagen production [57]. Based on
the complementary causality effects of mechanical
loading on the biochemical balance in cartilage, we can
thus hypothesize that mechanical loading might result
in both beneficial and detrimental conditions of the
joint cartilage. This additional knowledge is reflected
in the way our methodology constructs the hypothesis
graph. In particular, the normalization patterns (intro-
duced in the Methodology section) split the causality
chains starting in mechanical loading, that span two
complementary causality possibilities of benign and
malign effect on articular joint (Fig. 7). Validly, all the
possibilities of mechanical loading leading cartilage
degradation pass through the factor positive
regulation of chondrocytes catabolism
and we do not have a situation where mechanical
loading leads to cartilage degradation
by passing through positive regulation of
chondrocytes anabolism. Conversely all the causality
chains which lead from mechanical loading to
collagen or proteoglycan production pass through
chondrocytes anabolism factor.
Relative confidence measurement
This experiment demonstrates how molecular objectives
can measure his confidence in the causality hypothe-
sis according to his knowledge on causality relationships
(Hsub) and can compare it to the confidencemeasure when
we add more knowledge Hbroader . We simulate the case
where the molecular biologist wants to test a hypothesis
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 13 of 22
that s = Synovial inflammation has caused t =
Cartilage degradation. We treat Hbroader as a
coarse approximation of our universal knowledge on all
possible causalities which lead from s to t, and Hsub as a
personal view of that universal knowledge by the molecu-
lar biologist.
Table 3 summarizes network statistics of the two graphs.
In particular, in the universal hypothesis graph Hbroader
there are 24 possible causal chains which lead from s to
t, whereas in the subgraph Hsub we have only 6 possible
causal chains, which means that the molecular biologist
is missing a significant amount of knowledge about the
causalities that he is studying. Moreover, in the universal
knowledge of causality hypothesis we have 12 (|IHbroader | =
12) factors that can potentially be evidenced and would
contribute positively to the overall confidence of the
hypothesis, whereas in the restricted knowledge case we
are aware of only 9 (|IHsub | = 9) factors which need to
be evidenced to obtain the maximum confidence in the
same hypothesis that s has caused t. To study the behav-
ior of the confidence function Cts in these two cases we
perform the following tests: i) study the evolution of the
confidence function separately for two graphs, ii) normal-
ize the confidence function with the maximum possible
confidence for individual graphs, iii) normalize the two
confidence functions with themaximum confidence in the
universal graph. Note that, the parameter for the confi-
dence function is the set of evidenced nodes, where each
node may have different importance value, as defined by
the weighting functionF . To take into account all the pos-
sible variability of the confidence function we compute
the distributions of the confidence values for a gradually
increasing number of evidences. That is, we start with
the case where the evidence set is empty, correspond-
ing to the initial phase of hypothesis testing and where
our confidence is 0. Then, we compute the distribution
of confidences for all evidence sets of size (cardinality) 1,
corresponding to different choices of choosing one fac-
tor to evidence. For instance, for the universal hypothesis
graph Hbroader we have 12 ways to to prove hypothesis by
evidencing only one factor (out of 12 possible), whereas
for Hsub we have 9 factors to choose from. We continue
computing confidence distributions until we reach the full
evidence set.
Figure 8 represents the distribution of confidences com-
puted with Cts (Eq. 1) for gradually increasing sizes of
Table 3 Statistics of the graphs
Statistic Hsub Hbroader
Number of nodes |N| 15 30
Number of arcs |A| 19 57
Number of possible causal chains from s to t 6 24
Number of possible factors to evidence |I | 9 12
evidence sets, with a trivial weighting function of factors
F = const 1  where every factor has equal contribution
to the causality chains. The mean values of the con-
fidence distributions grow linearly as we increase the
number of evidences, as expected, the maximum confi-
dence value obtained in the universal case is bigger than
in the restricted case because we take into account more
possibilities in the universal case. We now use the individ-
ual maximum mean confidence values for each graph to
scale our distributions, such that they always stay in the
0..1 range.
Figure 9 shows the normalized version of the confidence
distributions, namely Cts = C
t
s
max(Cts )
for Hsub and Hbroader .
In particular, it shows that a molecular biologist, relative
to his extent of knowledge, obtains the 100% confidence
in his causality hypothesis by evidencing all the possible
factors which contribute to all the possible ways in which
s might have caused t, however, with the same amount
of evidence, but taking into account universal knowledge
about the causality possibilities, his confidence is less than
100%, which shows that he has missed some important
causality possibilities. To quantify this uncertainty, which
is proportionate to the amount of missed causality pos-
sibilities, we scale both confidence distributions by the
maximum confidence value that we may obtain in the
universal case.
Figure 10 demonstrates the relative confidence of the
molecular biologist to the universal causality hypothesis
for the same evidenced sets. The x-axis is truncated to
evidence sets of size 9, since molecular biologist is only
aware of 9 factors which need to be evidenced to prove
his hypothesis. If we collect the mean values of the con-
fidence distributions in two vectors x1, x2 then we can
quantify the error as their Euclidean distance ?x1 ? x2?.
In Table 4 we summarize the errors which quantify the
uncertainty in obtained confidence measures with respect
to the universal case for different weighting functions Fi.
These weighting functions were chosen as follows: i) F1
trivial weighting of importance of factors, ii) F2 random
weighting of importance of each factor, iii) F3 gives more
importance to factors which molecular biologist is aware
of, whereas those that he is not aware of are given less
importance, iv) F4 opposite to F3, we give more impor-
tance to factors that molecular biologist is not aware of
and we decrease the importance of factors that he is
aware of. The error variation is intuitive, if we evidence
the most important factors, even if we miss other fac-
tors and other causality chains, but whose importance
to the overall hypothesis is significantly smaller, then we
are more confident even with a restricted knowledge of
the causality possibilities. Vice-versa, if we evidence less
important factors and we miss the important ones, then
our confidence is much more compromised.
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 14 of 22
Fig. 8 Confidence distributions for gradually increasing sizes of evidence sets for the two graphs Hsub ,Hbroader , with a trivial weighting function
F (f ) = 1
Local importance of factors
Importance of the factors for a causality hypothesis can
be deduced from our confidence measure defined on the
hypothesis graph. The factors ranked as the most important
may help the researchers prioritize their next experi-
ments, studies, andmay help in the discovery of the poten-
tial collaborations with other scientists. Analogously, the
factors that are identified as the least important for a spe-
cific causality hypothesis hint on the lack of knowledge
about the possibly missing causality relationships, and
might represent an opportunity to focus on an underre-
searched topic. In particular, Cts measures our confidence
in the causality hypothesis that factor s caused t with a
given set of evidenced nodes E . This function accumu-
lates the weighted contribution of all evidenced nodes in
each causality possibility leading from s to t. When we first
start proving our hypothesis we do not have any evidence
and we have a choice of I to evidence from. However,
do we need to evidence all the factors in the interior of
the causality hypothesis I? What if we can only obtain
an incomplete set of evidences, which factors should we
choose? Intuitively, we should first focus on evidencing
factors which are most important in our causality hypoth-
esis. But how can we assess the importance of each factor
in the causality hypothesis? In this experiment, we pro-
pose a general approach to assessing the local importance
Fig. 9 Confidence distributions for gradually increasing sizes of evidence sets for the two graphs Hsub ,Hbroader , normalized by its maximum possible
confidence value
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 15 of 22
Fig. 10 Confidence distributions for gradually increasing sizes of evidence sets for the two graphs Hsub ,Hbroader , normalized by the maximum
possible confidence value in the universal case
of factors, independently of the weighting function F . To
do so we start with a case where we do not have any evi-
dence E = ?, we then rank each factor fi in the causality
hypothesis by its potential contribution to the confidence
in the causality hypothesis if it was evidenced |Cts (E ? fi)?
Cts (E = ?)|.
Figure 11 depicts the variation of potential contribu-
tions to the overall confidence measure Cts for each fac-
tor fi. In particular, we can observe that in both cases:
Hsub restricted personal view of the hypothesis, and
Hbroader universal causality hypothesis themost important
factors are: Positive regulation of TNF alpha
overproduction, s =Synovial inflammation,
t =Cartilage degeneration and Biochemical
imbalance. Indeed, to prove that s has resulted in
t our best strategy is to focus on evidencing those
two factors, however, given our knowledge of causal-
ity relationships, we might choose to evidence alterna-
tive factors to obtain the same overall confidence in
the validity of our causality hypothesis. We also observe
that by extracting more knowledge on causality relation-
ships more important factors to our causality hypoth-
esis emerge, i.e., the factors which we did not know
about before. For instance, Decrease of cartilage
elasticity and Water content increase in
Table 4 Mean squared error between the confidence
distributions for different weighting functions F
Weighting function Fi Error
F1(f ) = 1 2.17
F2(f ) = random(0, 1) 2.09
F3(f ) = 1 if f ? IHsub , otherwise 0.1 1.95
F4(f ) = 1 if f ? IHbroader , otherwise 0.1 2.96
cartilage have relatively low potential confidence con-
tributions (< 0.04) and thus our unawareness of the
contribution to causality hypothesis of these factors is not
so penalizing. Yet, Diminution of load bearing
capacity of cartilage is capable of contribut-
ing more than 10% of the overall confidence measure
Cts . It is also interesting to observe that adding knowl-
edge (Hbroader) reduces the importance of Biochemical
imbalance factor to the point that it is no longer one of
the most important factors in the causality hypothesis.
Generalization of the hypothesis configuration
In the previous experiment we identified the most impor-
tant factors, such that evidencing them would maximize
our confidence in the causality hypothesis that s resulted
in t. We can use the local importance of factors to the
hypothesis configuration to target our evidence collec-
tion. Suppose we managed to evidence the four most
important factors for the hypothesis graphHsub, which we
summarize in Table 5.
For the same evidence set Esub we obtain the normal-
ized confidence of Cts = 0.66 for Hsub and Cts = 0.53
for Hbroader . Now, we ask ourselves a question with the
same evidence set what other causalities can we prove
(with the same confidence)?. If we keep the same evidence
set Esub we are able to prove causalities with a confidence
>60% as depicted in Table 6. These causalities correspond
to very similar causality chains, as our initial causality
hypothesis that Synovial inflammation has results
in Cartilage degradation.
Intuitively, Table 7 demonstrates that for the same evi-
dence set, as we add more knowledge (Hbroader) we are
able to prove more causality relationships, with a good
confidence (>50%).
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 16 of 22
Fig. 11 Contributions of the interior factors of the hypothesis s caused t for two hypothesis graphs Hsub ,Hbroader with two different depths of
knowledge
Generalization of the hypothesis configuration leads
to the scenarios where the seemingly wrong causality
relationships, might actually be explained with plausible
interpretations. One such example scenario is when we
obtain the significant confidence (0.60) in a causality
hypothesis that Cartilage calcification might
result in Positive regulation of TNF alpha
overproduction (line 1 in Table 7). First, it is tempt-
ing to say that this is a wrong hypothesis, and is due
to the error in the formalization of the background
knowledge on causality relationships. Partly, because
calcification of cartilage entails cell apoptosis and thus
should cause the decrease of levels of TNF alpha cytokine
cells. However, we get the high confidence score in this
causality due to the presence of a path from Cartilage
calcification to Positive regulation of
TNF alpha overproduction (see Fig. 7). This path
represents our knowledge that calcified cartilage will
result in degeneration of cartilage tissue, which will
Table 5 4 Most important factors for Hsub in the two hypothesis
graphs and their relative confidence values in both Hsub and
Hbroader
Evidence set Esub Importance for Hbroader Importance for Hsub
Biochemical imbalance 0.10 0.16
Cartilage degeneration 0.14 0.16
Positive regulation of TNF
alpha overproduction
0.14 0.16
Synovial inflammation 0.14 0.16
Cts(Esub) for Hbroader Cts(Esub) for Hsub
0.53 0.66
provoke synovial inflammation, and we hypothesized that
synovial inflammation will result in positive regulation of
TNF alpha. After a discussion with our domain experts
we reached the conclusion that, although this causality
relationship between calcified cartilage and positive
regulation of TNF alpha might seem contradictory, there
actually might be a plausible explanation. Namely, while
the calcification causes tissue death in cartilage, it does so
only in a specific region of cartilage. The calcified region,
however, will induce the diminution of the load bearing
properties of the whole cartilage, and this will provoke
the synovial inflammation, which, in turn, will result in
excessive levels of TNF alpha in the neighbouring regions
of the cartilage (neighbouring to the calcified region).
Prototype
We implemented a prototype (Fig. 12) to interactively
apply and present the proposed methodology for causality
hypothesis testing on the obtained hypothesis graphs. The
demo of the prototype is available at http://hypothtest.
plumdeq.xyz/test/. Source code for the hypothesis test-
ing of the prototype and proof of concept ontologies, as
well as the Jupyter Notebooks (reproducible experiments
presented in this manuscript) are available on GitHub at
https://github.com/plumdeq/hypothtest (see Availability
of data and materials subsection).
The interface of the prototype is divided into 4 logical
blocks, labeled a, b, c, d in Fig. 12.
(A) Control over the hypothesis configuration. The
users can change the hypothesis configuration in two
modes - i) identifying the boundary nodes s, t, ii) selecting
the evidenced nodes E . Each mode is triggered by clicking
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 17 of 22
Table 6 Other causalities we can prove (>60% confidence) with the same evidence set Esub
Source s Target t Cts(Esub) for Hbroader Cts(Esub) for Hsub
Cartilage Biochemical imbalance 0.66 0.66
degeneration Negative regulation of Collagen production 0.75 0.75
Positive regulation of TNF alpha overproduction 1.00 1.00
Loss of Positive regulation of TNF alpha overproduction 0.62 0.80
collagen
Loss of Positive regulation of TNF alpha overproduction 0.62 0.80
proteoglycan
Synovial Negative regulation of Chondrocytes anabolic activity 0.66 0.66
inflammation
Negative regulation of Collagen production 0.66 0.66
Negative regulation of Proteoglycan production 0.66 0.66
Positive regulation of Chondrocytes catabolic activity 0.66 0.66
Positive regulation of TNF alpha overproduction 1.00 1.00
Table 7 Causalities we can prove (>50% confidence), as we add more knowledge, and which we cannot prove with our restricted
knowledge of causality relationships
Source s Target t Cts(Esub) for Hbroader Cts(Esub) for Hsub
Cartilage calcification Positive regulation of TNF 0.60 0.0
alpha overproduction
Diminution of load bearing Biochemical imbalance 0.57 0.0
capacity of cartilage Negative regulation of 0.60 0.0
Chondrocytes anabolic activity
Negative regulation of 0.60 0.0
Collagen production
Negative regulation of 0.60 0.0
Proteoglycan production
Positive regulation of Chondrocytes 0.60 0.0
catabolic activity
Positive regulation of TNF 0.75 0.0
alpha overproduction
Synovial inflammation 0.66 0.0
Meniscal tear Biochemical imbalance 0.57 0.0
Negative regulation of 0.60 0.0
Collagen production
Negative regulation of 0.60 0.0
Proteoglycan production
Positive regulation of Chondrocytes 0.60 0.0
catabolic activity
Positive regulation of TNF 0.75 0.0
alpha overproduction
Water content increase Positive regulation of TNF 0.60 0.0
in cartilage alpha overproduction
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 18 of 22
Fig. 12 The interface of the prototype is divided into 4 logical blocks: a) control over the hypothesis configuration h, b) hypothesis summary, c) local
importance of nodes in the hypothesis and d) visualization of the hypothesis graph
on an associated button (see Fig. 12a), and then selecting
the specific nodes in the hypothesis graph (Fig. 12d).
(B) Hypothesis summary. A textual summary of a cur-
rent hypothesis configuration (see Fig. 12b).
(C) Local importance of nodes in the hypothesis. Local
importance of each node with respect to the hypothesis
configuration.
(D) Visualisation of the hypothesis graph. Interactive
network visualisation with the force directed layout [58]
of the hypothesis graph H. The users can interactively
click on the nodes and drag them for a visually better spa-
tial distribution of the network. The boundary nodes are
visually distinguished as completely opaque nodes in the
hypothesis graph (Fig. 12), while all other nodes are semi-
opaque. Evidenced nodes are visually distinguished as
green nodes. Consequently, if a node ni is both evidenced
and either a source or a target of the confidence evalua-
tion, then it will be opaque green. The backend (server)
of the prototype constructs hypothesis graphs, computes
importance measures on each node of the graph, and
evaluates confidence in the hypothesis configuration. The
frontend (client) is responsible for the interactive visu-
alisation of the hypothesis graph, and serves as a user
interface. In particular the user can interactively assign the
boundary nodes, and mark nodes as evidenced. The user
input is then transmitted to the backend via custom data
exchange protocol, based on JSON files. Each time the
user changes the configuration of the hypothesis (i.e., evi-
dences/unevidences node or assigns new source or target
nodes of the confidence evaluation the hypothesis confi-
dence is reevaluated and the results are sent back to the
client.
Discussion
We evaluated our methodology on a hypothesis graph
which covers our use-case scenario of cartilage degrada-
tion during osteoarthritis. The obtained hypothesis graph
represents both contributing factors which may cause
cartilage degradation and the factors which might be
caused by the cartilage degradation. Hypothesis graph
construction (see Robustness of the system in presence
of complementary causality relationships section) has
proven to be robust to the addition of potentially con-
tradictory information on the simultaneously positive and
negative effects, by adequately separating two comple-
mentary causality scenarios. By evaluating our method-
ology for relative confidence measurement (see Relative
confidence measurement section) we have observed the
following: i) the more evidences we are able to provide
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 19 of 22
(as E ? I) the bigger is our overall confidence function
(confidence grows Cts ?), ii) our relative confidence to the
universal knowledge of the hypothesis (i.e., the difference
in confidences) is proportionate to how much knowledge
on causal possibilities we lack with respect to the uni-
versal causality hypothesis, the less causality possibilities
we take into account in our formalization the smaller is
our confidence in the causality hypothesis with respect
to the universal knowledge of the causality hypothesis,
iii) our confidence in the causality hypothesis increases
when we evidence more factors favored byF with respect
to the universal formalization of the causality hypothe-
sis, even if we do not have full knowledge of the causality
possibilities. The domain experts found that our compu-
tational methodology for assessing confidence in a causal-
ity hypothesis proportionally to the amount of available
knowledge, corresponds to their subjective assessments
of confidences in an investigated hypothesis. Moreover,
the obtained confidence measures for the specific causal-
ity hypotheses have been validated by our domain experts,
and, in some cases, have led to new interpretations of the
already known causality connections (see Generalization
of the hypothesis configuration section).
Limits, assumptions and dependencies of methodology.
Overall our framework is dependant on the validity, qual-
ity and the richness of the modelling, which will induce
the final shape and topology of the hypothesis graph
and the way the confidence is assessed by using our
methodology for confidence assessment. Of course, our
methodology has its limits and has its assumptions and
dependencies. Main assumptions and dependencies of the
methodology for hypothesis testing rely on: i) ontologi-
cal commitment of the input ontologies Oi that formalize
background biological knowledge on causality relation-
ships, ii) biological validity and logical consistency of
the formalized knowledge - input to the framework, iii)
weighting scheme of factors of the hypothesis that mea-
sure the quality of the ontology matching of concepts used
to assemble the final ontology, and the confidence of the
obtained evidence for a specific factor fi. Ontological com-
mitment of the modelled realities representing causality
relationships among the factors should follow the good
design patterns for modelling causalities, for both con-
cepts and relationships that interrelate those concepts. In
particular, we consider the processual perspective of a dis-
ease as a causal chain structure as in River Flow Model of
Diseases [25] as opposed to an object-like perspective of
a whole constituting a disease as in Ontology of General
Medical Sciences (OGMS) [59]. As has been argued by
Rovetto and Mizgouchi [25], the causality in OGMS is
unstated, implicit or stated indirectly. The general account
of disease in OGMS draws ideas from Scheuermann
et al. [60], and distinguishes diseases from disease courses.
Diseases in OGMS are treated as dispositions potentially
realizable via pathological processes, and have some dis-
orders as their physical basis. In our work, we focus on
causality relationships which constitute a disease course,
and reason on these relationships by relying on graph
analysis techniques. Due to this modelling choice we
expect the input ontologies to follow the RFM account
of disease as a causal chain structure. Specifically, our
methodology for hypothesis graph construction extracts
causality relationships from the assembled ontology such
that the final hypothesis graph contains nodes as occur-
rents, either biological processes, as exemplary modelled
in the Gene Ontology [20], or as conditions (abnormal
states), according to the guidelines of the RFM. The
causality relationships should be compliant with the Rela-
tion Ontology [24], which, among other types, covers con-
current and overlapping causality relationships between
the occurrent entities, relying on Allen interval algebra
calculus for temporal logic [61]. Strategies toward harmo-
nization between disease accounts in OGMS and RFM are
brought up in Rovetto and Mizgouchi [25]. Hypothesis
graph creation with input ontologies following the OGMS
modelling of disease could represent a promising future
direction for the community.
Weighting scheme for the factors of the hypothesis
graph will largely depend on the context (e.g., studied
disease), the quality of the ontology mappings, and the
confidence of the obtained evidence. Mappings to guide
the assembly (i.e., link factors from different hypothe-
sis) can be discovered in online resources like UMLS
Metathesaurus [49] or BioPortal [50, 51], or using state
of the art ontology alignment systems like LogMap [52]
or AML [53]. Confidence in the obtained evidence will
depend on the methodology of the experiment and should
be assessed by the executioner of the experiment, which
might entail subjective importance weight of the factor
and might have subjective consequences on the computa-
tion of the overall confidence in the causality hypothesis
with our framework.
Conclusions
We have presented a promising and nascent method-
ology for the translation of biological knowledge on
causality relationships of biological processes and their
effects on conditions to a computational framework for
shared hypothesis testing. Furthermore, we have defined
a knowledge-driven, and evidenced-based way of mea-
suring confidence in a causality hypothesis proportionally
to the amount of available knowledge and collected evi-
dences. The methodology resumes in two points: hypoth-
esis graph construction from the formalizations of the
background knowledge on causality relationships, and
confidence measurement in a causality hypothesis as a
normalized weighted path computation in the hypothesis
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 20 of 22
graph. Lastly, we have made the source code and mate-
rials available to the community on GitHub at https://
github.com/plumdeq/hypothtest (see Availability of data
and materials subsection).
Herein we took advantage of our domain experts to
build a simplified and a tractable version of a causal-
ity hypothesis graph of cartilage degradation during to
osteoarthritis, and to validate our methodology for confi-
dence assessment of causality hypothesis. The evaluation
results, the feedback from our experts, and the lessons
learnt from this overall experience allow us to conclude
that a methodology for shared hypothesis testing could
be incorporated as an invaluable asset to the online bio-
logical knowledge graph mining services. In particular,
our hypothesis graph construction methodology could
be used routinely to enrich biological knowledge graphs
(e.g., Knowledge Bio [62]) and online databases (e.g.,
Gene Wiki [63]) by extracting the causality relation-
ships information from OWL 2 ontologies. Of course,
the proposed set of patterns for the normalization of
the hypothesis graph will have to be augmented and
tuned for a specific studied context. We, for instance,
defined graph rewriting normalization patterns to deal
with complementary biological scenarios of simultane-
ously positive and negative regulations of biological pro-
cesses (see Robustness of the system in presence of
complementary causality relationships section). In fact,
the graph rewriting patterns is a general paradigm for
the transformation of formalized knowledge on a spe-
cific biological pattern into its equivalent graph rep-
resentation and might open an opportunity for more
research and practical contributions from the biomedical
community.
Shared hypothesis testing services built on top of
the confidence measurement (see Relative confidence
measurement section), and the inference procedures
it induces (see Generalization of the hypothesis con-
figuration section), will enhance the biological knowl-
edge graphs with advanced simulation functionalities
for continuous research. These services could support
researchers in literature review for their experimental
studies, planning and prioritizing evidence collection
acquisition procedures, and testing their hypotheses with
different depths of knowledge on causal dependencies
of biological processes and their effects on the observed
conditions. Measuring confidence in a causality hypothe-
sis relatively to the already discovered causality relation-
ships might serve in the assessment of the fairness of the
obtained results, and its significance to the already known
results. We believe that the shared hypothesis testing
could serve as an important asset for the costless re-
enactment of the experiments, and might eventually con-
tribute to the future, purely computational benchmarks
for the validation of the experiments.
Endnote
1 See https://www.bioontology.org/wiki/index.php/Bio-
Portal_Mappings
Abbreviations
ABox: Description logics ground sentences stating where in the hierarchy
individuals belong; DL: Description logics; FMA: Foundational model of
anatomy ontology; MRI: Magnetic resonance image; OA: Osteoarthritis; OWL:
Web ontology language; RDF: Resource description framework; SNA: Social
network analysis
Acknowledgments
We would like to thank the reviewers for their valuable comments and
suggestions which helped us to substantially improve the paper.
Funding
This work was partially funded by the EU Marie Curie, ITN MultiScaleHuman
(FP7-PEOPLE-2011-ITN, Grant agreement no.: 289897), the CNR project
DIT.AD009.006 Modelling and Analysis of anatomical shapes for computer
assisted diagnosis, the BIGMED project (IKT 259055), the HealthInsight project
(IKT 247784), the SIRIUS Centre for Scalable Data Access (Research Council of
Norway, project no.: 237889), the program Investigador of the Portuguese
Foundation of Science and Technology (FCT, IF/00423/2012), the EU project
Optique (FP7-ICT-318338), and the EPSRC projects ED3 and DBOnto.
Availability of data andmaterials
We implemented a prototype to apply the proposed methodology for
hypothesis testing on an example hypothesis graph. The demo of the
prototype is available at http://hypothtest.plumdeq.xyz/test/. Source code for
the hypothesis testing of the prototype and proof of concept ontologies, as
well as the Jupyter Notebooks (reproducible experiments presented in this
manuscript) are available on GitHub at https://github.com/plumdeq/
hypothtest.
Authors contributions
AA defined most of the theory and did the implementation work. EJR
extended the graph projection methodology to a richer subset of OWL axioms
and proposed the hypothesis graph normalization methodology. MO
composed and validated the biological use-case scenario of cartilage
degradation causality hypothesis. All authors contributed to the definition of
the framework, and to the writing of the manuscript. All authors read and
approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Italian National Research Council, Via De Marini 6, 16149 Genoa, Italy.
2University of Oslo, Oslo, Norway. 33Bs Research Group, Biomaterials,
Biodegradables and Biomimetics, Headquarters of the European Institute of
Excellence on Tissue Engineering and Regenerative Medicine, University of
Minho, Caldas das Taipas, Portugal. 4ICVS/3Bs - PT Government Associate
Laboratory, Braga/Guimarães, Portugal. 5University of Genoa, Genoa, Italy.
6Center for Medical Statistics, Informatics, and Intelligent Systems, Institute for
Artificial Intelligence and Decision Support, Medical University of Vienna,
Spitalgasse 23, 1090 Vienna, Austria. 7Department of Biomedical Data Science,
Stanford University School of Medicine, Stanford 94305, California, USA.
Received: 1 June 2016 Accepted: 18 January 2018
Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 21 of 22
Wang et al. Journal of Biomedical Semantics  (2018) 9:19 
https://doi.org/10.1186/s13326-018-0184-y
RESEARCH Open Access
Adverse event detection by integrating
twitter data and VAERS
Junxiang Wang1, Liang Zhao1, Yanfang Ye4,5 and Yuji Zhang2,3*
Abstract
Background: Vaccine has been one of the most successful public health interventions to date. However, vaccines
are pharmaceutical products that carry risks so that many adverse events (AEs) are reported after receiving vaccines.
Traditional adverse event reporting systems suffer from several crucial challenges including poor timeliness. This
motivates increasing social media-based detection systems, which demonstrate successful capability to capture
timely and prevalent disease information. Despite these advantages, social media-based AE detection suffers from
serious challenges such as labor-intensive labeling and class imbalance of the training data.
Results: To tackle both challenges from traditional reporting systems and social media, we exploit their complementary
strength and develop a combinatorial classification approach by integrating Twitter data and the Vaccine Adverse
Event Reporting System (VAERS) information aiming to identify potential AEs after influenza vaccine. Specifically, we
combine formal reports which have accurately predefined labels with social media data to reduce the cost of manual
labeling; in order to combat the class imbalance problem, a max-rule based multi-instance learning method is
proposed to bias positive users. Various experiments were conducted to validate our model compared with other
baselines. We observed that (1) multi-instance learning methods outperformed baselines when only Twitter data
were used; (2) formal reports helped improve the performance metrics of our multi-instance learning methods
consistently while affecting the performance of other baselines negatively; (3) the effect of formal reports was more
obvious when the training size was smaller. Case studies show that our model labeled users and tweets accurately.
Conclusions: We have developed a framework to detect vaccine AEs by combining formal reports with social media
data. We demonstrate the power of formal reports on the performance improvement of AE detection when the
amount of social media data was small. Various experiments and case studies show the effectiveness of our model.
Keywords: Formal reports, Social media, Multi-instance learning, Vaccine adverse event detection
Background
Vaccine has been one of the most successful public health
interventions to date. Most vaccine-preventable diseases
have declined in the United States by at least 9599%
[1, 2]. However, vaccines are pharmaceutical products
that carry risks. They interact with the human immune
systems and can permanently alter gene molecular struc-
tures. For instance, 7538 adverse event reports were
received between November 2009 and March 2010 in
the Netherlands with respect to two pandemic vaccines,
*Correspondence: Yuzhang@som.umaryland.edu
2Department of Epidemiology & Public Health, University of Maryland School
of Medicine, Baltimore, MD, USA
3Division of Biostatistics and Bioinformatics, University of Maryland Marlene
and Stewart Greenebaum Comprehensive Cancer Center, Baltimore, MD, USA
Full list of author information is available at the end of the article
Focetria and Pandemrix [3]. Serious adverse reactions
may even lead to death. For example, a woman died of
multi-organ failure and respiratory distress, which was
then verified to be caused by a yellow fever vaccina-
tion in Spain on October 24, 2004 [4]. Aiming to build
a nationwide spontaneous post-marketing safety surveil-
lance mechanism, the US Centers for Disease Control and
Prevention (CDC) and the Food and Drug Administration
(FDA) co-sponsored the Vaccine Adverse Event Report-
ing System (VAERS) since 1990, which currently contains
more than 500,000 reports in total. However, such report-
ing systems bear several analytical challenges, such as
underreporting, false-causability issues, and various qual-
ity of information. In addition, formal reports are records
of symptom descriptions caused by vaccine adverse events
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Wang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 2 of 10
(AEs) and need time-consuming administrative process-
ing. As a result, the release of formal reports lags behind
disease trends. For example, the VARES usually releases
newly-collected report data every three months. A real-
time monitoring system to identify potential AEs after
vaccination can serve as complementary surveillance pur-
pose aside from VAERS.
In recent decades, information extraction from social
media data such as Twitter data has demonstrated suc-
cessful capability to capture timely and prevalent dis-
ease information. These advantages effectively address the
drawbacks of existing reporting systems such as VAERS.
However, very little work has been done on the detec-
tion of AEs after vaccinations using social media data.
There are mainly two challenges of the detection of AEs
on social media. (1) The costly labeling process: in prin-
ciple, it is compulsory to check message by message in
order to label user accurately. Labeling millions of users
is labor-intensive. For instance, if a user has about 100
tweets each month, labeling 1,000,000 such users will
need labeling 100,000,000 tweets, which cannot be com-
pleted manually. (2) The class imbalance: in practice, the
proportion of positive users, whose messages indicated
symptom descriptions of AEs, is much lower than that of
negative users. As a result, a classifier biases toward the
negative user class due to its sample majority, causing a
high false negative rate.
To tackle both challenges, we propose to develop a com-
binatorial classification approach by integrating Twitter
data and VAERS information aiming to identify Twit-
ter users suffering from side effects after receiving flu
vaccination. Specifically, in order to reduce the cost of
manual labeling, we combined formal reports which are
accurately labeled with social media data to form a
training set. A max rule based multi-instance learning
approach was developed to address the class imbalance
problem. Various experiments were conducted to vali-
date our model: we first collected and processed data
from Twitter users who received flu shots through Twit-
ter APIs and AE formal reports from VAERS. Then, we
applied a series of baselines and multi-instance learn-
ing methods including our model to investigate whether
formal reports can help improve the classification per-
formance in the Twitter setting. We investigated how the
change of the formal report size influenced the classifica-
tion performance of our multi-instance learning methods
as well as other baselines. We observed that (1) multi-
instance learning methods outperformed baselines when
only Twitter data were used because baselines need to
sum multiple tweets up, most of which are irrelevant to
vaccine adverse events; (2) formal reports helped improve
the performance metrics of our multi-instance learning
methods consistently while affecting the performance of
other baselines negatively; (3) the effect of formal reports
was more obvious when the training size was smaller.
The reason behind the findings (2) and (3) is related to
the proportion changes of positive users against negative
users.
Related work
In this section, several research fields related to our paper
are summarized as follows.
AE detection in social media. Recently, social media
have been considered as popular platforms for health-
care applications because they can capture timely and
rich information from ubiquitous users. Sarker et al. con-
ducted a systematic overview of AE detection in social
media [5]. Some literatures are related to adverse drug
event detection. For example, Yates et al. collected con-
sumer reviews on various social media site to iden-
tify unreported adverse drug reactions [6]; Segura et al.
applied a multi-linguistic text analysis engine to detect
drug AEs from Spanish posts [7]; Liu et al. combined dif-
ferent classifiers based on feature selection for adverse
drug events extraction [8]; OConnor et al. studied the
value of Twitter data for pharmacovigilance by assessing
the value of 74 drugs [9]; Bian et al. analyzed the con-
tent of drug users to build the Support Vector Machine
(SVM) classifiers [10]. Others dwell on flu surveillance.
For instance, Lee et al. built a real-time system to mon-
itor flu and cancer [11]; Chen et al. proposed temporal
topic models to capture hidden states of a user based on
his tweets and aggregated states in geographical dimen-
sion [12]; Polgreen et al. kept track of public concerns with
regard to h1n1 or flu [13]. However, to the best of our
knowledge, there exists no work which has attempted to
detect AEs on vaccines.
Multi-instance learning. In the past twenty years,
multi-instance learning models have attracted the atten-
tion of researchers due to a wide range of applications.
In the multi-instance learning problem, a data point, or a
bag, is composed of many instances. For example, in the
vaccine AE detection problem on Twitter data, a user and
tweets posted by this user are considered as a bag and
instances, respectively. Generally, multi-instance learning
models are classified as either instance-level or bag-level.
Instance-level multi-instance learning classifiers predict
instance label rather than bag label. For example, Kumar
et al. conducted audio event detection task from a col-
lection of audio recordings [14]. Bag-level multi-instance
learning algorithms are more common than instance-
level. For instance, Dietterich et al. evaluated binding
strength of a drug by the shape of drug molecules [15].
Andrews et al. applied Support VectorMachines (SVM) to
both instance-level and bag-level formulations [16]. Zhou
et al. treated instances as independently and identi-
cally distributed and predicted bag labels based on
graph theories [17]. Mandel et al. utilized multi-instance
Wang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 3 of 10
learning approaches to label music tags using many 10-
second song clips [18].
Methods
In this section, we first describe the data resources and
preprocessing processes in this work. Then we introduce
our multi-instance learning method and present all steps
of the MILR, as shown in Fig. 1. All experiments were
analyzed in compliance with Twitter policies1. They were
conducted on a 64-bit machine with Intel(R) core(TM)
quad-core processor (i3-3217U CPU@ 1.80GHZ) and
4.0GB memory.
Feature set and dataset
Feature set: The feature set consists of 234 common key-
words related to AEs which were prepared by domain
experts. These keywords forming different tenses were
common words to describe adverse events and side effects
in both formal reports and social media messages. The
choice of keywords is very important because the ter-
minology used in formal reports and tweets are differ-
ent. Table 1 illustrates the terminology usage difference
between formal reports and tweets. Keywords are high-
lighted in bold types. Specifically, formal reports tend
to use professional terms for symptom descriptions like
BENADRYL and hydrocortisone, while simple words
are more likely used in social media messages. One exam-
ple of flu and shot is presented in Table 1. Fortunately,
there are keyword overlaps between formal reports and
social media messages such as swollen shown in Table 1.
Twitter dataset: Twitter data used in this paper were
obtained from the Twitter API in the following process:
firstly, we queried the Twitter API to obtain the tweets
that were related to flu shots by 113 keywords including
flu,h1n1 and vaccine. Totally, 11,993,211,616 tweets
between Jan 1, 2011 and Apr 15, 2015 in the United States
Table 1 A formal report and tweet example, respectively
Formal report Tweet
T-dap 2 days ago arm As soon as I walk
developed itchy and swollen. in my apartment,
BENADRYL and 2.5% my swollen arm
hydrocortisone should be seen decides to remind me
by allergist referral sent. I got a flu shot today.
Keywords are shown in bold types
were obtained. Second, among these tweets, the users
who had been received flu shots were identified by their
tweets using the LibShortText classifier that was trained
on 10,000 positive tweets and 10,000 negative tweets
[19, 20]. The accuracy of the LibShortText classifier was
92% by 3-fold cross-validation. The full text representa-
tions were used as features for the LibShortText classi-
fier. Then, we collected all tweets within 60 days after
users had been received flu shots identified by the sec-
ond step. The collected tweets formed our dataset in this
paper, which consisted of a total of 41,537 tweets from
1572 users. The labels of users were manually curated
by domain experts. among them 506 were positive users
which were indicative of AEs by their tweets and the other
1066 were negative users.
VAERS dataset: We downloaded all raw data from
VAERS for the year 2016 in the comma-separated value
(CSV) format. The data consisted of 29 columns includ-
ing VAERS ID, report date, sex, age and symptom text.
We extracted 2500 observations of symptom texts, each
of which was considered as a formal report indicative of
an AE.
Multi-instance logistic regression
The scheme of the proposed framework is illustrated in
Fig. 1. As an auxiliary data source, formal reports are
combined with social media data to enhance the classi-
fication generalization. The training dataset consists of
Fig. 1 Overview of the proposed framework. VAERS: Vaccine Adverse Event Reporting System. MILR: Multi-instance Logistic Regression
Wang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 4 of 10
Twitter training data and formal reports from VAERS,
which provide a comprehensive positive labeled dataset
to tackle limited sample challenge of social media. The
scheme of the proposed framework is illustrated in Figure
As an auxiliary data source, formal reports are combined
with Twitter data to enhance the classification general-
ization. The training dataset consists of Twitter training
data and formal reports from VAERS, which provides an
abundance of positive labeled data to reduce the cost of
manual labeling. The test data are Twitter test data only.
They are converted into vectors where each element is
the count of a keyword. Then the Multi-instance Logistic
Regression (MILR) is applied to train the model. The idea
of MILR is to build a mapping from users to tweets. The
relation between users and tweets is summarized by the
max rule: if at least a tweet from a user indicates an AE,
this user is labeled as positive; otherwise, this user is neg-
ative. The max rule for classification is asymmetric from
users to tweets: as for positive users, we only need a tweet
that indicates an AE; but for negative users, none of their
tweets indicates an AE. In reality, a minority of users are
affected by AEs, whereas the remaining users are labeled
as negative. The asymmetric property of the max rule
biases toward positive users and diminishes the influence
of the major negative user class. Therefore, the classifier
treats the positive and negative user class equally. Besides,
the max rule is resistant to feature noise because tweets
selected by the max rule are determined by all candidate
tweets rather than a certain tweet. In this experiment, the
logistic regression with 1 regularization is applied to train
the classifier.
Comparison methods
Two types of classifiers which were applied to this work,
namely baselines and multi-instance learning methods,
are introduced in this subsection.
Baselines
For baselines, the vector was summed by column for each
user, with each column representing a count of keyword
for this user.
1. Support Vector Machines (SVM). The idea of SVM
is to maximize the margin between two classes [21]. The
solver was set to be Sequential Minimal Optimization
(SMO) [22]. We chose three different kernels for compari-
son: the linear kernel (linear), the polynomial kernel (poly)
and the radial basis kernel (rbf ).
2. Logistic Regression with 1-regularization (LR).
Logistic regression is amethodwhichmodels the outcome
as a probability. We implemented this approach by the
LIBLINEAR library [23].
3. Neural Network (NN). The idea of the Neural Net-
work is to simulate a biological brain based on many
neural units [24]. The Neural Network consists of the
input layer, 10 hidden layers and the output layer. Each
layer has 3 nodes. The sigmoid function is used for the
output. The layers are fully connected layers, where each
node in one layer connects the nodes in neighboring
layers.
Multi-instance learningmethods
4. Multi-instance Learning based on the Vector of
Locally Aggregated Descriptors representation(miVLAD)
[25]. In the multi-instance learning problem, a bag is
used to represent a set consisting of many instances. To
make the learning process efficient, all the instances for
each bag were mapped into a high-dimensional vector
by the Vector of Locally Aggregated Descriptors (VLAD)
representation. In other words, VLAD representation
compressed each bag into a vector and hence improved
the computational efficiency. Then a SVM was applied on
these vectors to train the model.
5. Multi-instance Learning based on the Fisher Vec-
tor representation (miFV) [25]. The miFV was similar to
miVLAD except that each bag was represented instead by
a Fisher Vector (FV) representation.
Metrics
In this experiment, our task was to detect flu shot AEs
based on Twitter data and VAERS information. The
evaluation was based on 5-fold cross-validation. Several
metrics were utilized to measure classifier performance.
Suppose TP, FP, TN and FN denote true positive, false pos-
itive, true negative and false negative, respectively, these
metrics are calculated as:
Accuracy (ACC) = (TP+TN)/(TP+FP+TN+FN)
Precision (PR) = TN/(TN+FP)
Recall (RE) = TN/(TN+FN)
F-score (FS) = 2*PR*RE/(PR+RE).
The Receiver Operating Characteristic (ROC) curve
measures the classification ability of a model as discrimi-
nation thresholds vary. The Area Under ROC (AUC) is an
important measurement of the ROC curve.
Results
In this section, experimental results are presented in
detail. We found that (1) multi-instance learning meth-
ods outperformed baselines when only Twitter data were
used; (2) formal reports improved the performance met-
rics of multi-instance learning methods consistently while
affected the performance of baselines negatively; (3) the
effect of formal reports was more obvious when the train-
ing size was smaller.
Performance comparison between baselines and
multi-instance learning methods
We compared model performance between multi-
instance learning methods and baselines, which is shown
Wang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 5 of 10
in Table 2. The results demonstrated that the MILR per-
formed better than any other comparison method when
no formal report was available. The MILR exceeded 0.86
in the AUC, while none of other classifiers attained more
than 0.84. The ACC of the MILR was 0.8034, 0.15 higher
than the SVM with the polynomial kernel. When it came
to the FS, theMILR achieved the result that was 0.6 higher
than the SVM with the radial basis kernel. It surpassed
0.78 in the PR metric, whereas the PR of the LR was only
0.6765. As for the RE, the performance of the MILR was
0.57 better than the SVM with the radial basis kernel.
The ACCs of the miFV and miVLAD were around 0.77
and their AUCs reached over 0.83, which were superior
to any other baseline. The AUCs of the NN and LR were
competitive among baselines, reaching 0.8196 and 0.7524,
respectively. As for the SVM, the kernel choice made a big
difference. The linear kernel and the radial basis kernel
were superior to the polynomial kernel in almost every
metric: the ACCs and the AUCs of these two kernels were
over 0.65 and 0.79, respectively, whereas these of the poly-
nomial kernel were only 0.6412 and 0.5697, respectively.
The PR, RE and FS of the linear kernel were 0.01, 0.25 and
0.36 better than the polynomial kernel, respectively.
Figure 2 illustrates ROC curves for adding different
number of formal reports. X axis and Y axis denote False
Positive Rate (FPR) and True Positive Rate (TPR), respec-
tively. Overall, multi-instance learning methods outper-
formed baselines, which was consistent with the Table 2.
The MILR performed the best however many formal
reports were added in the training set, with ROC curves
covering the largest area above the X axis. The miVLAD
also performed well in Fig. 2a and c while inferior to the
MILR in four other figures. The miFV was inferior to the
miVLAD and the MILR, when the FPR was greater than
0.2. When it came to baseline classifiers, the performance
of the SVM with the polynomial kernel was a random
guess in Fig. 2a, b and c. As more formal reports were
added, its performance was improved, as shown in Fig. 2d,
e and f. The NN and LR were the worst among all meth-
ods when no less than 1500 formal reports were added.
The SVMwith the linear kernel and the radial basis kernel
achieved a competitive performance among all baselines.
The reason behind the superiority of multi-instance
learning methods over baselines is that vector compres-
sion by summation for each user which serve as the input
of baselines lose important information. In reality, only
a few tweets are related to vaccines, and the summation
includes many AE-irrelevant tweets, which usually results
in a noisy data input.
Performance comparison for different formal report
numbers
To examine the effect of formal reports on classification
performance, we made a comparison between no formal
report and 2500 formal reports. It indicated from Table 2
that most multi-instance learningmethods were benefited
Table 2 Model performance between no formal report and 2500 formal report based on five metrics (the highest value for each
metric is highlighted in bold type): multi-instance learning methods outperformed baselines
Method Formal ACC PR RE FS AUC
#Report
SVM(linear) 0 0.7793 0.7309 0.6100 0.6644 0.7916
2500 0.7296 0.6241 0.6370 0.6294 0.7234
SVM(poly) 0 0.6412 0.7231 0.3611 0.3069 0.5697
2500 0.5478 0.5311 0.5497 0.4443 0.6416
SVM(rbf) 0 0.6507 0.6948 0.0572 0.1035 0.8069
2500 0.5897 0.4652 0.9344 0.6210 0.7754
LR 0 0.7665 0.6765 0.6641 0.6700 0.7524
2500 0.7322 0.6209 0.6576 0.6384 0.7303
NN 0 0.7924 0.7408 0.6273 0.6790 0.8196
2500 0.7411 0.6414 0.6396 0.6394 0.7366
miFV 0 0.7818 0.7269 0.6352 0.6775 0.8348
2500 0.7856 0.7331 0.6403 0.6833 0.8361
miVLAD 0 0.7691 0.7261 0.5832 0.6461 0.8390
2500 0.7863 0.7055 0.6999 0.7018 0.8201
MILR 0 0.8034 0.7858 0.6231 0.6947 0.8676
2500 0.8054 0.7871 0.6291 0.6984 0.8902
Wang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 6 of 10
Fig. 2 Receiver operating characteristic (ROC) curves adding different formal reports: multi-instance learning methods outperformed baselines no
matter how many formal reports were added. a No formal report, b 500 formal reports, c 1000 formal reports, d 1500 formal reports, e 2000 formal
reports, f 2500 formal reports
from 2500 formal reports. The AUCs of the MILR and the
miFV were improved by 0.025 and 0.002, respectively. The
miVLAD was only an exception because its AUC declined
by 0.02. However, most baselines were affected nega-
tively by formal reports in the AUC, while other metrics
remained stable. For example, after 2500 formal reports
were added into the training set, the AUCs of the NN and
the SVM with the linear kernel were dropped drastically
by 0.07 and 0.08, respectively. Compared with these con-
siderable tumbles, the AUCs of the LR and the SVM with
the radial basis kernel dropped slightly, which was about
0.02, whereas the AUC of the SVM with the polynomial
kernel increased by 0.07.
Figure 3 shows tendencies of five metrics on differ-
ent number of formal reports. Overall, formal reports
improved the performance of multi-instance learning
methods whereas leading to decline of baselines. All
methods were categorized as three classes. The perfor-
mance of the SVM with the linear kernel, LR and NN
was deteriorated by adding more formal reports: their
AUCs dropped from 0.79, 0.75 and 0.82 to 0.73, 0.73 and
0.75, respectively. Trends of their ACCs, PRs and FSes
were similar while their REs improved significantly with
more formal reports. The SVM with the radial basis ker-
nel and miFV were independent of the change of formal
reports. The remaining classifiers, namely, the SVM with
Fig. 3Metric trends of all classifiers adding different formal reports: formal reports improved the performance metrics of multi-instance learning
methods consistently while affected the performance of baselines negatively. a SVM(linear), b SVM(poly), c SVM(rbf), d LR, e NN, fmiFV, gmiVLAD,
hMILR
Wang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 7 of 10
the polynomial kernel, miFVLAD and the MILR, bene-
fited from the introduction of formal reports: the AUC of
the SVM with the polynomial kernel was below 0.6 while
this result increased to 0.65 with 1500 formal reports; the
RE of the miVLAD first elevated from 0.58 to 0.75, then
declined smoothly to 0.7; there was a slight increase from
0.87 to 0.89 in the AUC of the MILR.
The huge performance discrepancy between baselines
and multi-instance learning methods after the inclusion
of formal reports came from the proportion of positive
users against negative users. For instance, for baselines,
the proportion of positive users was 32% (i.e., 506/1572)
in the Twitter data only. However, the ratio increased dra-
matically to 73.82% (i.e., 3006/4072) after we added 2500
formal reports. In other words, since formal reports (i.e.,
positive users) were introduced into the dataset, the pro-
portion of positive users surpassed that of negative users,
and baselines predicted most users as positive. However,
negative users greatly outnumber positive users in our
dataset. Different from baselines, multi-instance learning
methods focused on the mappings from tweet labels to
user labels. Since tweet labels were unavailable, assum-
ing the predictions of the MILR were accurate, the pro-
portion of tweets related to positive users was 4% (i.e.,
1545/39037), while this ratio changed slightly to 9.73%
(i.e., 4045/41537) after we added 2500 formal reports.
Therefore, the introduction of formal reports benefited
multi-instance learning methods by providing enough
positive user samples and avoiding the label proportion
change problem.
MILR performance with small training sizes
Table 3 shows the effect of the size of the Twitter train-
ing data on model performance using MILR. Overall,
formal reports have a more obvious effect on model per-
formance when the training size of the Twitter data was
small.When the training size was 314, 786, 1048 and 1179,
the corresponding AUC improvement by adding formal
Table 3 Model performance using MILR with smaller training sizes (the highest value for each metric is highlighted in bold type): the
effect of formal reports was more obvious when the training size was smaller
Twitter data Formal ACC PR RE FS AUC
#Training #Report
314 (20%) 0 0.7731 0.7278 0.5923 0.6525 0.8446
500 0.7812 0.7323 0.6212 0.6713 0.8539
1000 0.8112 0.7993 0.6356 0.7076 0.8888
1500 0.8136 0.7935 0.6524 0.7151 0.8923
2000 0.8114 0.7812 0.6612 0.7156 0.8916
2500 0.8112 0.7824 0.6590 0.7147 0.8904
786 (50%) 0 0.7939 0.7689 0.6141 0.6816 0.8646
500 0.7920 0.7651 0.6125 0.6790 0.8684
1000 0.8041 0.7682 0.6567 0.7064 0.8834
1500 0.8034 0.7720 0.6482 0.7031 0.8834
2000 0.8092 0.7968 0.6312 0.7044 0.8897
2500 0.8066 0.7711 0.6615 0.7108 0.8866
1048 (67%) 0 0.7952 0.7841 0.5953 0.6767 0.8646
500 0.7850 0.7615 0.5915 0.6645 0.8653
1000 0.7983 0.7948 0.5937 0.6795 0.8843
1500 0.7996 0.7944 0.5992 0.6830 0.8880
2000 0.8034 0.7984 0.6080 0.6903 0.8899
2500 0.8060 0.8016 0.6133 0.6949 0.8910
1179 (75%) 0 0.7952 0.7845 0.5927 0.6752 0.8664
500 0.7933 0.7695 0.6010 0.6743 0.8846
1000 0.8034 0.7881 0.6172 0.6915 0.8948
1500 0.8041 0.7913 0.6154 0.6915 0.8963
2000 0.8041 0.7940 0.6119 0.6901 0.8983
2500 0.8041 0.7940 0.6119 0.6901 0.8985
Wang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 8 of 10
reports was 0.0477, 0.0251, 0.0264 and 0.015, respectively.
The same trend was applied to the PR, RE and FS. For
example, the FS improvement with 314 training samples
was 0.0622, while that with 1179 training samples was only
0.0149. Different from other metrics, the ACCwas around
0.8 no matter how the size of the Twitter training data
and formal reports changed. The label proportion changes
mentioned in the previous section can account for why
the effect of formal reports is more obvious with smaller
Twitter training data.
Keyword frequencies
In this section, to illustrate the effect of formal reports on
the keyword set, we compare the semantic patterns of AE
tweets between no formal report and 2500 formal reports
implemented by MILR, as shown by Fig. 4. In each word
cloud, the frequencies of keywords in each set of tweets
were in proportion to their sizes. Keywords headache,
sore, sick, arm and pain were the largest keywords
in Fig. 4a and b. The keyword cheeks became more fre-
quent while the keyword vaccines was much smaller after
adding 2500 formal reports. To conclude, most frequent
keywords remained stable after the introduction of 2500
formal reports.
Case studies
We found that most users were accurately labeled by our
proposed approach. For example, Table 4 gives two exam-
ple users and their corresponding tweets. Keywords are
displayed in bold types. For the first user labeled as pos-
itive, the first tweet showed that he/she received a flu
shot. Then a headache happened indicated by the sec-
ond tweet. The third tweet was irrelevant to AEs. When it
came to the second positive user, none of three tweets was
AE-irrelevant. Our approach correctly labeled both users
and selected the tweet accurately by the max rule. There-
fore, the effectiveness of our model was validated by these
two users.
Discussions
Traditional AE reporting systems bear several analytic
challenges, which lead to the rise of information extrac-
tion from social media. However, the costly labeling pro-
cess and class imbalance problem put barriers to the
application of social media on the AE detection. To tackle
these challenges, we developed a combinatorial classifica-
tion approach to identify AEs by integrating Twitter data
and VAERS information. Note that the difference of data
collection timeframe between Twitter data and VAERS
data was not considered in our approach. Our findings
indicated that multi-instance learning methods benefited
from the introduction of formal reports and outperformed
baselines. In addition, the performance improvement of
multi-instance on the formal reports was more obvious
with smaller training sizes. The integration of social media
data and formal reports is a promising approach to iden-
tify AEs in the near future.
Conclusion
In this paper, we propose a combinatorial classification
approach by integrating Twitter data and VAERS informa-
tion to identify potential AEs after influenza vaccines. Our
results indicated that (1) multi-instance learning meth-
ods outperformed baselines when only Twitter data were
used; (2) formal reports improved the performance met-
rics of our multi-instance learning methods consistently
while affected the performance of other baselines neg-
atively; (3) the effect of formal report was more obvi-
ous when the training size was smaller. To the best of
our knowledge, this is the first time that formal reports
are integrated into social media data to detect AEs.
Formal reports provide abundant positive user samples
and improve classification performance of multi-instance
learning methods.
In this work, we omitted the differences between social
media and formal reports, which introduced may extra
bias to the dataset. In the future, a domain adaptation
Fig. 4 Keyword frequencies of tweets which indicated AEs between no formal report and 2500 formal reports: frequent keywords remained stable.
a No formal report, b 2500 formal reports
Wang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 9 of 10
Table 4 Two users and their corresponding tweets
User Id Corresponding tweets Indicative or not
246090881 Got my annual employer-paid flu shot
today.
Not
Now I have a headache. ARGH. Indicative
Starting to yawn. Might be sleepy. GOOD! I
need sleep!
Not
206180021 Getting a flu shot, I realized how amazing
the CDC is even though most people are
completely unaware of all theways they help
us.
Not
Or Gamera! Gamera flies through the air like
a spinning firework. Anyone who hates
Gamera is dead to me.
Not
Personally, I dont like something about the
sound of The Tower Heist movie. Yup,
something about that makes me nervous.
Not
Keywords are displayed in bold types
method can be considered to address this issue. We also
need to deal with other limitations of social media. For
example, it is difficult to differentiate a new AE from pre-
vious AEs for the same Twitter user. Moreover, identifying
serious AEs is very challenging because scarce serious
AE cases lead to severe class imbalance problem, i.e.,
the proportion of serious AEs is far lower than that of
general AEs.
Endnote
1 https://dev.twitter.com/overview/terms/agreement-
and-policy
Funding
This project was supported by the National Cancer Institute grant P30 CA
134274 to the University of Maryland Baltimore.
Availability of data andmaterials
The experimental data and source codes are accessible.
Authors contributions
JW led the experimental design and analysis and drafted the manuscript. LZ
and YZ participated the design, provided support and manuscript editing. YY
conducted the data acquisition. All authors read and approved the final
manuscript.
Ethics approval and consent to participate
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Department of Information Science and Technology, George Mason
University, Fairfax, VA, USA. 2Department of Epidemiology & Public Health,
University of Maryland School of Medicine, Baltimore, MD, USA. 3Division of
Biostatistics and Bioinformatics, University of Maryland Marlene and Stewart
Greenebaum Comprehensive Cancer Center, Baltimore, MD, USA. 4Lane
Department of Computer Science and Electrical Engineering, West Virginia
University, Morgantown, WV, USA. 5Benjamin M. Statler College of Engineering
and Mineral Resources, West Virginia University, Morgantown, WV, USA.
Received: 2 February 2018 Accepted: 10 May 2018
RESEARCH Open Access
Using predicate and provenance
information from a knowledge graph for
drug efficacy screening
Wytze J. Vlietstra1* , Rein Vos1,2, Anneke M. Sijbers3, Erik M. van Mulligen1 and Jan A. Kors1
Abstract
Background: Biomedical knowledge graphs have become important tools to computationally analyse the
comprehensive body of biomedical knowledge. They represent knowledge as subject-predicate-object triples, in
which the predicate indicates the relationship between subject and object. A triple can also contain provenance
Kolyvakis et al. Journal of Biomedical Semantics  (2018) 9:21 
https://doi.org/10.1186/s13326-018-0187-8
RESEARCH Open Access
Biomedical ontology alignment: an
approach based on representation learning
Prodromos Kolyvakis1* , Alexandros Kalousis2, Barry Smith3 and Dimitris Kiritsis1
Abstract
Background: While representation learning techniques have shown great promise in application to a number of
different NLP tasks, they have had little impact on the problem of ontology matching. Unlike past work that has
focused on feature engineering, we present a novel representation learning approach that is tailored to the ontology
matching task. Our approach is based on embedding ontological terms in a high-dimensional Euclidean space. This
embedding is derived on the basis of a novel phrase retrofitting strategy through which semantic similarity
information becomes inscribed onto fields of pre-trained word vectors. The resulting framework also incorporates a
novel outlier detection mechanism based on a denoising autoencoder that is shown to improve performance.
Results: An ontology matching system derived using the proposed framework achieved an F-score of 94% on an
alignment scenario involving the Adult Mouse Anatomical Dictionary and the Foundational Model of Anatomy
ontology (FMA) as targets. This compares favorably with the best performing systems on the Ontology Alignment
Evaluation Initiative anatomy challenge. We performed additional experiments on aligning FMA to NCI Thesaurus and
to SNOMED CT based on a reference alignment extracted from the UMLS Metathesaurus. Our system obtained overall
F-scores of 93.2% and 89.2% for these experiments, thus achieving state-of-the-art results.
Conclusions: Our proposed representation learning approach leverages terminological embeddings to capture
semantic similarity. Our results provide evidence that the approach produces embeddings that are especially well
tailored to the ontology matching task, demonstrating a novel pathway for the problem.
Keywords: Ontology matching, Semantic similarity, Sentence embeddings, Word embeddings, Denoising
autoencoder, Outlier detection
Background
Ontologies seek to alleviate the Tower of Babel effect
by providing standardized specifications of the intended
meanings of the terms used in given domains. Formally, an
ontology is a representational artifact, comprising a tax-
onomy as proper part, whose representations are intended
to designate some combinations of universals, defined
classes and certain relations between them [1]. Ideally,
in order to achieve a unique specification for each term,
ontologies would be built in such a way as to be non-
overlapping in their content. In many cases, however,
domains have been represented by multiple ontologies
and there thus arises the task of ontology matching, which
*Correspondence: prodromos.kolyvakis@epfl.ch
1École Polytechnique Fédérale de Lausanne (EPFL), Route Cantonale, 1015
Lausanne, Switzerland
Full list of author information is available at the end of the article
consists in identifying correspondences among entities
(types, classes, relations) across ontologies with overlap-
ping content.
Different ontological representations draw on the dif-
ferent sets of natural language terms used by different
groups of human experts [2]. In this way, different and
sometimes incommensurable terminologies are used to
describe the same entities in reality. This issue, known as
the human idiosyncrasy problem [1], constitutes the main
challenge to discovering equivalence relations between
terms in different ontologies.
Ontological terms are typically common nouns or noun
phrases. According to whether they do or do not include
prepositional clauses [3], the latter may be either com-
posite (for example Neck of femur) or simple (for example
First tarsometatarsal joint or just Joint). Such grammati-
cal complexity of ontology terms needs to be taken into
account in identifying semantic similarity. But account
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Kolyvakis et al. Journal of Biomedical Semantics  (2018) 9:21 Page 2 of 20
must be taken also of the ontologys axioms and defini-
tions, and also of the position of the terms in the ontol-
ogy graph formed when we view these terms as linked
together through the is_a (subtype), part_of and other
relations used by the ontology.
The primary challenge to identification of semantic sim-
ilarity lies in the difficulty we face in distinguishing true
cases of similarity from cases where terms are merely
descriptively associated1. As a concrete example, the
word harness is descriptively associated with the word
horse because a harness is often used on horses [4].
Yet the two expressions are not semantically similar. The
sorts of large ontologies that are the typical targets of
semantic similarity identification contain a huge number
of such descriptively associated term pairs. This difficulty
in distinguishing similarity from descriptive association is
a well-studied problem in both cognitive science [5] and
NLP [6].
Traditionally, feature engineering has been the predom-
inant way to approach the ontology matching problem
[7]. In machine learning, a feature is an individual mea-
surable property of a phenomenon in the domain being
observed [8]. Here we are interested in features of terms,
for instance the number of incoming edges when a term
is represented as the vertex of an ontology graph; or a
termss tf-idf value  which is a statistical measure of
the frequency of a terms use in a corpus [9]. Feature
engineering consists in crafting features of the data that
can be used by machine learning algorithms in order to
achieve specific tasks. Unfortunately determining which
hand-crafted features will be valuable for a given task can
be highly time consuming. To make matters worse, as
Cheatham and Hitzler have recently shown, the perfor-
mance of ontology matching based on such engineered
features varies greatly with the domain described by the
ontologies [10].
As a complement to feature engineering, attempts have
been made to develop machine-learning strategies for
ontology matching based on binary classification [11].
This means a classifier is trained on a set of align-
ments between ontologies in which correct and incor-
rect mappings are identified with the goal of using
the trained classifier to predict whether an assertion
of semantic equivalence between two terms is or is
not true. In general, the number of true alignments
between two ontologies is several orders of magni-
tude smaller than the number of all possible mappings,
and this introduces a serious class imbalance prob-
lem [12]. This abundance of negative examples hinders
the learning process, as most data mining algorithms
assume balanced data sets and so the classifier runs the
risk of degenerating into a series of predictions to the
effect that every alignment comes to be marked as a
misalignment.
Both standard approaches thus fail: feature engineer-
ing because of the failure of generalization of the engi-
neered features, and supervised learning because of the
class imbalance problem. Our proposal is to address
these limitations through the exploitation of unsuper-
vised learning approaches for ontology matching drawing
on the recent rise of distributed neural representations
(DNRs), in which for example words and sentences are
embedded in a high-dimensional Euclidean space [1317]
in order to provide a means of capturing lexical and
sentence meaning in an unsupervised manner. The way
this works is that the machine learns a mapping from
words to high-dimensional vectors which take account
of the contexts in which words appear in a plurality
of corpora. Vectors of words that appear in the same
sorts of context will then be closer together when mea-
sured by a similarity function. That the approach can
work without supervision stems from the fact that mean-
ing capture is merely a positive externality of context
identification, a task that is unrelated to the meaning
discovery task.
Traditionally, corpus driven approaches were based on
the distributional hypothesis, i.e. the assumption that
semantically similar or related words appear in simi-
lar contexts [18]. This meant that they tended to learn
embeddings that capture both similarity (horse, stallion)
and relatedness (horse, harness) reasonably well, but do
very well on neither [6, 19]. In an effort to correct
for these biases a number of pre-trained word vector
refining techniques were introduced [6, 20, 21]. These
techniques are however restricted to retrofitting single
words and do not easily generalize to the sorts of nom-
inal phrases that appear in ontologies. Wieting et al.
[22, 23] make one step towards addressing the task of
tailoring phrase vectors to the achievement of high per-
formance on the semantic similarity task by focusing on
the task of paraphrase detection. A paraphrase is a restate-
ment of a given phrase that use different words while
preserving meaning. Leveraging what are called univer-
sal compositional phrase vectors [24] for the purposes
of paraphrase detection provides training data for the
task of semantic similarity detection which extends the
approach from single words to phrases. Unfortunately,
the result still fails as regards the problem of distin-
guishing semantic similarity and descriptive association
on rare phrases [22]  constantly appearing on ontolo-
gies  which thus again harms performance in ontology
matching tasks.
In this work, we tackle the aforementioned challenges
and introduce a new framework for representation learn-
ing based ontology matching. Our ontology matching
algorithm is structured as follows: To represent the nouns
and noun-phrases in an ontology, we exploit the con-
text information that accompanies the corresponding
Kolyvakis et al. Journal of Biomedical Semantics  (2018) 9:21 Page 3 of 20
expressions when they are used both inside and out-
side the ontology. More specifically, we create vectors
for ontology terms on the basis of information extracted
not only from natural language corpora but also from
terminological and lexical resources and we join this
with information captured both explicitly and implicitly
from the ontologies themselves. Thus we capture con-
texts in which words are used in definitions and in state-
ments of synonym relations. We also draw inferences
from the ontological resources themselves, for exam-
ple to derive statements of descriptive association  the
absence of a synonymous statement between two terms
with closely similar vectors is taken to imply that as
a statement of descriptive association obtains between
them. We then cast the problem of ontology matching
as an instance of the Stable Marriage problem [25] dis-
covering in that way terminological mappings in which
there is no pair of terms that would rather be matched
to each other than their current matched terms. In
REVIEW Open Access
The eXtensible ontology development
(XOD) principles and tool implementation
to support ontology interoperability
Yongqun He1* , Zuoshuang Xiang1, Jie Zheng2, Yu Lin3, James A. Overton4 and Edison Ong5
Abstract
Ontologies are critical to data/metadata and knowledge standardization, sharing, and analysis. With hundreds of
biological and biomedical ontologies developed, it has become critical to ensure ontology interoperability and the
usage of interoperable ontologies for standardized data representation and integration. The suite of web-based
Ontoanimal tools (e.g., Ontofox, Ontorat, and Ontobee) support different aspects of extensible ontology development. By
summarizing the common features of Ontoanimal and other similar tools, we identified and proposed an eXtensible
Ontology Development (XOD) strategy and its associated four principles. These XOD principles reuse existing terms and
semantic relations from reliable ontologies, develop and apply well-established ontology design patterns (ODPs), and
involve community efforts to support new ontology development, promoting standardized and interoperable data and
knowledge representation and integration. The adoption of the XOD strategy, together with robust XOD tool
development, will greatly support ontology interoperability and robust ontology applications to support data to be
Findable, Accessible, Interoperable and Reusable (i.e., FAIR).
Keywords: Ontology, Interoperability, eXtensible ontology development, Software, Ontoanimal tools, Ontofox, Ontobee,
Ontorat, Semantic alignment, And ontology design pattern
Background
In informatics, an ontology is a set of computer- and
human-interpretable terms and relations that represent
entities and their relations in a specific domain of the
world. Hundreds of biological/ biomedical ontologies
have been developed in the last two decades. The Open
Biological and Biomedical Ontologies (OBO) Foundry is
a collaborative initiative aimed at establishing a set of
ontology development principles and incorporating
ontologies following these principles in an evolving non-
redundant and interoperable suite [1]. The OBO library
currently includes over 160 ontologies covering >3 mil-
lion terms in biological and clinical domains. NCBO
BioPortal [2] has over 400 ontologies including both
OBO and non-OBO ontologies. Given hundreds of
ontologies developed, a critical issue is the lack of ontol-
ogy interoperability, preventing the seamless under-
standing and exchange of semantic information between
different resources.
Ontologies are widely used in different areas [3, 4],
including: (1) Naming things; (2) Knowledge base con-
struction, e.g., the Ontology of Vaccine Adverse Events
(OVAE) representing the knowledge of adverse events
induced by FDA-licensed vaccines [5]; (3) Data ex-
change, e.g., BioPAX for representing molecular and cel-
lular pathways and facilitating the exchange of biological
pathway data [6]; (4) Data integration, e.g., the Ontology for
Biomedical Investigations (OBI) [7] for integrative
representations of data in various areas of life-science and
clinical investigations; (5) Data analysis, as exemplified by
the wide usage of the Gene Ontology (GO) [8] to support
high-throughput gene expression data analyses; (6) Natural
language processing [9, 10]; (7) Metadata standard gener-
ation [1113]. (8) Information retrieval and new knowledge
discovery [1416].
* Correspondence: yongqunh@med.umich.edu
1Unit for Laboratory Animal Medicine, Department of Microbiology and
Immunology, Center for Computational Medicine and Bioinformatics,
University of Michigan Medical School, Ann Arbor, MI, USA
Full list of author information is available at the end of the article
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
He et al. Journal of Biomedical Semantics  (2018) 9:3 
DOI 10.1186/s13326-017-0169-2
To support various needs in ontology development and
applications, different software programs have been devel-
oped. The Protégé OWL editor [17] is likely the most
popular tool for manual processing and editing of ontology
OWL documents. However, manual ontology development
is typically tedious and inefficient, especially when the
structure of ontology is enormous. Over the years, we have
developed a collection of web-based Ontoanimal tools
including Ontofox [18], Ontodog [19], Ontorat [20], Onto-
bee [21], Ontobeep [22], Ontobull [23], Ontokiwi [24], and
Ontobat [20]. Each Ontoanimal tool has its specific func-
tions, and the collective use of these tools enables users to
cover the full development of ontology and linked
data (i.e., data published on the Web that it is expli-
citly defined, machine-readable, and interlinked with
external data sets [25]), including: extracting ontology
subsets for term reuse and semantic alignment, pro-
viding ontology community views, adding and editing
multiple ontology terms, visualizing and comparing
ontology terms, supporting community editing and
discussion, and creating ontology-based linked data.
The back-end He group RDF triple store serves as
the default ontology RDF triple store for the OBO
Foundry ontologies [21]. Complementary to Protégé,
Ontoanimal tools are widely used for efficient and
flexible ontology development without requiring pro-
gramming skills. For example, according to Google
Analytics and Google Scholar, Ontobee has been used
by over 77,000 users from 181 countries, Ontofox has
been used by over 17,000 users from 147 countries,
and Ontoanimal tools have been cited by >400 publi-
cations in the last 5 years.
The Ontoanimal tools and other similar tools have
significantly enhanced the speed and quality of ontology
development and improved ontology interoperability.
Given an increasing number of these tools, it would be
important to identify the common features of these tools.
After retrospective examination and careful summary of
these tools, we realized the most common feature of these
tools being their support for extensible ontology develop-
ment. Such extensibility is crucial to increase the interoper-
ability among the ever increasing number of ontologies.
However, a systematic view of extensible ontology devel-
opment is not available. Thus, we propose the eXtensible
Ontology Development (XOD) strategy and four XOD
principles in this paper. Such an XOD strategy is comple-
mentary to the OBO principles and the OBO goal of
achieving interoperable ontology suite [1], and it is also
complementary to the ten simple rules proposed for bio-
medical ontology development [26]. We also believe that
the adoption of the XOD strategy and principles support
the FAIR Guiding Principles proposal that all research data
should be Findable, Accessible, Interoperable and Reusable
(FAIR) for both machine and human users [27].
XOD: eXtensible ontology development
In information technology, extensible describes some-
thing (e.g., a program or protocol) that is designed so
that users/developers can expand or add to its capabil-
ities with no or minimal change in the systems internal
structure and data flow. For example, extensibility is a
primary feature of the eXtensible Markup Language
(XML) system. Being eXtensible, XOD contains four
key principles that are extensible at different levels of
ontology development (Fig. 1):
(i) Ontology term reuse. Instead of reinventing the
wheel when generating new ontologies, XOD
emphasizes the reuse of terms from existing reliable
ontologies that are well constructed and commonly
used by the ontology community [1, 28, 29].
(ii) Ontology semantic alignment. For ontology
interoperability, it is important to align imported
terms from existing ontologies and newly added
terms with the same semantics.
(iii) ODP usage for new term generation and existing
term editing. Instead of adding one term at a time,
XOD emphasizes the addition or editing of a group
of terms based on ontology design patterns (ODPs).
(iv) Community extensibility. While the development of
an ontology might be initiated by a small group with
one or a few use cases, the ontology should be
co-developed and applied to more use cases by more
people in a broader community.
Ontoanimal tools (Fig. 1) and many other programs
support XOD principles (Table 1). In the following
sections, different principles and associated tools are
described with details.
XOD 1: Ontology term reuse
Reusing terms from reliable reference ontologies is
better than reinventing the wheel to generate new terms
in an ontology [18, 30]. The reference ontology should
be registered in an ontology library (e.g., OBO Foundry)
to make these terms more findable, accessible, and
reusable. To improve reusability and extensibility, ontol-
ogy terms in the reference ontologies should be expres-
sive and generalizable and endure consistency checking
and evaluation. To maintain ontology interoperability,
ontology term mapping is often used to map terms from
different ontologies with the same meaning [31]. Com-
pared to ontology term reuse, term mapping is less ideal
since it is time-consuming, often inaccurate, redundant,
and increases maintenance cost and confusion. Given
multiple ontologies without using the ontology term
reuse strategy, ontology mapping becomes a core task
for ontology interoperability [32]. The wide usage of the
He et al. Journal of Biomedical Semantics  (2018) 9:3 Page 2 of 10
term reuse principle would make the mapping among
different ontologies unneeded.
An initial method of ontology term reuse was to import a
full ontology, which was not ideal since it might import too
many unrelated terms. Instead of importing external ontol-
ogies as a whole, the Minimum Information to Reference
an External Ontology Term (MIREOT) strategy, introduced
by OBI developers [30], proposes the usage of the minimal
information of an external ontology term that is of direct
interest to a target ontology [30]. Specifically, MIREOT sug-
gests the following minimal set: (1) source ontology URI;
(2) source term URI; and (3) target direct superclass URI.
With the set of information, the source ontology term can
be extracted to under the target direct superclass. Since it is
often hard to maintain semantic consistency among ontol-
ogies, the popular MIREOT strategy provides a simple solu-
tion with possible semantics loss.
Ontofox, Ontodog, and Ontobull support term reuse.
Originally named OntoFox, Ontofox was the first web
tool to support the MIREOT strategy (Fig. 2) [18].
Ontofox is able to quickly and easily fetch user-specified
terms and their annotations from source ontologies and
assign them under defined superclass(es) in target ontol-
ogies (Fig. 2a and b). Ontofox also extends MIEROT by
retrieving semantical axioms with different options (see
next section). Ontodog is also able to extract a subset of
ontology terms and axioms [19]. Unlike plain text defin-
ition in Ontofox, Ontodog uses Excel input files to iden-
tify terms to retrieve. To match possible updates of
source ontologies, e.g., the upper-level Basic Formal
Ontology (BFO) [33], Ontobull is developed for auto-
matic conversion and updating [23].
Several other tools also support ontology term reuse
(Table 1). The Protégé MIREOT plugin [34] and
Table 1 XOD principles and supporting software programs
XOD principle # XOD principle names Tool name
XOD 1 Ontology term reuse Ontodog, Ontofox, OntoMATON, Protégé MIREOT plugin, ROBOT
XOD 2 Semantic alignment Ontobeep, Ontofox, ROBOT
XOD 3 ODP usage MappingMaster, Ontorat, Populous, ROBOT, TermGenie, Webulous
XOD 4 Community extensibility Ontodog, Ontokiwi/Ontobedia, WebProtege
Fig. 1 Summary of Ontoanimal tools and their features. Ontofox supports ontology reuse by extracting terms and axioms. Ontodog provides
ontology community views by allowing community-preferred annotations. Ontorat automatically generates new ontology terms and edits existing
terms based on ontology design patterns. Ontobee is an ontology linked data server for OBO library ontologies and many non-OBO ontologies. The
Ontobee-based Ontobeep program supports ontology comparison and identification of redundant terms. Ontokiwi is a Wiki-like ontology editing and
discussion program. Ontobedia is an application of Ontokiwi. Ontobat supports ontology-based data processing (e.g., conversion from Excel to OWL)
and analysis. These tools support different XOD principles
He et al. Journal of Biomedical Semantics  (2018) 9:3 Page 3 of 10
OntoMaton [35] support term reuse as a plugin of the
Protégé OWL editor or Google Spreadsheets, respect-
ively. ROBOT is a command-line Java tool supporting
the extraction of ontology terms and subsets [36].
ROBOT also has many other features and supports mul-
tiple XOD principles (Table 1) as described below [36].
To better support ontology reuse and community-
based ontology development, it would also be valuable
to have the authors of the source ontology know of the
reuse of a term in their ontology. The Ontobee program
[21] includes a feature in the web page of an ontology
term that shows all the other ontologies reusing the
term, which supports ontology interoperability.
XOD 2: Ontology semantic alignment
The XOD 2 principle proposes to align imported ontol-
ogy terms and newly added terms with the same or
compatible semantics. Such semantic alignment has two
specific meanings. First, in addition to the term reuse in
XOD 1, the semantic relations among reused terms
should also be reused and aligned. If different relation
types (i.e., object properties) mean the same thing, they
should be merged. Correspondingly, the axioms of
reused terms and any additional terms specified in the
axioms should also be retrieved and imported. Second,
the semantics related to newly developed terms should
be aligned and compatible with imported semantics, and
the same or compatible relations be used in the new
ontology. If a well-defined relation already exists, we
should reuse the relation instead of defining another
relation with the same meaning. Such semantic align-
ments support ontology semantic interoperability.
Ontofox and Ontodog support ontology semantic
alignment. Ontofox and Ontodog extract semantic ax-
ioms and terms related to user-specified terms from the
source ontologies. Given different options, Ontofox al-
lows the computation and extraction of (i) intermediate
terms that are the shared parent terms of multiple low
level terms (Fig. 2c), or (ii) all intermediate terms be-
tween the required terms and a top level term (Fig. 2d).
These subset semantic axioms and terms can then be
retrieved and become a part of the new ontology. Note
that manual intervention and judgment may still be
needed now to ensure the semantic alignment between
retrieved subset and target ontology semantics [37]. It
will also be important to have computer-supported
Fig. 2 Ontofox retrieval of an NCBITaxon subset. Input data includes 3 species of organisms (human, mouse, and rat) and Ontofox settings. The input
data and settings can be entered via web-based forms (a). The Ontofox results can be shown using Protégé (b-d). Different results may appear based
on different settings: The setting IncludeNoIntermediates implements MIREOT (b). The setting includeComputedIntermediates extracts computed
intermediates which that are closest ancestors of more than one low level source terms (c). The setting includeAllIntermediates outputs all
possible intermediates
He et al. Journal of Biomedical Semantics  (2018) 9:3 Page 4 of 10
semantic capture and synchronization of ontology evolu-
tion and updates. To foster reliability, an overall formal
evaluation and consistency checking would be needed.
XOD 3: ODP-based ontology development
An Ontology Design Pattern (ODP) represents a
reusable solution to solve a recurrent modeling problem
in the context of ontology engineering [20, 38, 39].
ODPs provide extensible representations of entities and
relations, make ontologies more maintainable, and im-
prove ontology quality. This XOD 3 principle requires
an ODP-based strategy to develop and edit new terms,
annotations, and relations. This principle extends XOD
1 and XOD 2 and provides a specific, feasible, and
robust mechanism to achieve interoperable ontology
term generation/annotation and semantic consistency.
The Ontorat program (http://ontorat.hegroup.org)
supports ODP-based creation of new ontology terms,
annotations and logical axioms [20]. Fig. 3 illustrates
an example of using Ontorat to add new terms, anno-
tations, and axioms to the Ontology of Adverse
Events (OAE) [40]. Ontorat uses reusable ODPs
(Fig. 4a) to automatically generate and edit ontology
terms and axioms and provides term annotations. A
specific ODP can be used to derive an Excel template
of different terms/annotations and a set of rules that
define the relations among those terms/annotations
(Fig. 3b). The Ontorat template, similar to a QTT
(Quick Term Template) originated by OBI developers
[41], can be populated with specific terms or annota-
tions to define or annotate specific ontology terms, or
generate axioms illustrating logic relations between
ontology terms. With the support of the Ontorat
settings (Fig. 3c), the populated template spreadsheet
can then be converted into an OWL file with newly
generated ontology terms and axioms (Fig. 3d and e).
The setting and template files can also be saved and
reused.
Fig. 3 New OAE term generation and annotation using Ontorat. First an ODP was identified to define new AE terms (a). The ODP guided the
generation of an Excel template and Ontorat settings. The template file was populated with detailed contents (one row for one new term; only
two rows shown in this example) (b). The Ontorat settings were matched to the Excel data format (c). The settings and populated Excel file were
then used as Ontorat inputs to generate an OWL format output file containing newly created ontology terms together with their annotations.
The output could be displayed using the Protégé OWL editor (d). After merging the output file to existing OAE file, the detailed information of
imported ontology terms (e.g., discomfort AE OAE_000081) seen in (d) will be obtained from and aligned to existing OAE (e)
He et al. Journal of Biomedical Semantics  (2018) 9:3 Page 5 of 10
Other ODP-based XOD tools include MappingMaster
[42], TermGenie [43], Populous [44], Webulous [45],
and ROBOT [36] (Table 1). Developed as a Protégé plu-
gin, MappingMaster can only be used with old version
Protégé 3.4 and is not available for newer Protégé 4 and 5
[42]. Targeting domain experts, TermGenie provides a
web application that supports new GO term generation
based on predefined patterns [43]. Populous requires soft-
ware installation but provides a user-friendly interface
[44]. Webulous is Google Add-On application usable with
Google Spreadsheets [45]. ROBOT also has a template
system for converting spreadsheets of terms to OWL files.
XOD 4: Community extensibility
The communitys involvement during the developing
phase of an ontology is the key for wide adoption of the
ontology in the future. However, this step is often a
bottleneck for ontology development, since the wider
the community is, the more difficult it is to reach agree-
ments on term definitions and classifications. In the
reality, an ontology is often initiated by a small group
and often driven by one or more use cases. To enhance
its quality and broad recognition, XOD 4 recommends
that a broader community with more developers and
users participate in the ontology development and
applications. This XOD community extensibility
principle emphasizes the community participation to
further extend, develop, and apply an ontology. With the
principle of community extensibility, one ontology can
be extended to cover different use cases in the same pro-
ject and different projects from a wide range of research
communities. The nature of such a practice will require
more people to participate, make the ontology commu-
nity bigger, and achieve better data interoperability.
WebProtege [46] and Ontokiwi [24] support community-
based ontology development. WebProtege is a web-based
ontology editor that supports collaborative OWL ontology
development [46]. WebProtege has been used by many
groups. It includes full change tracking and revision history,
and many community collaboration features such as
sharing and permissions, threaded notes and discussions,
watches and email notifications. Ontokiwi is the user-
friendly Wiki-like web program that supports community-
wide ontology editing, annotation, discussion, and distribu-
tion [24]. Ontobedia is an Ontokiwi application preloaded
with existing biomedical ontologies [24]. The Wiki-
like addition and editing of text that is not part of
the ontology makes Ontokiwi/Ontobedia a unique platform
for community-wide ontology discussion and distribution.
For community-wide ontology development and applica-
tions, tools to support ontology query, comparison, and
evaluations are also needed. NCBO BioPortal [2], OLS [47],
Ontobee [21], and AberOWL [48] are commonly used
ontology registry and repositories that also provide ontol-
ogy visualization, queries and analysis features, which facili-
tates the community involvement principle. The SPOT
ontology toolkit (http://www.ebi.ac.uk/spot/ontology/) also
provides a list of community-driven open source ontology
tools. For example, Ontobee (http://www.ontobee.org) is an
ontology browser and a linked ontology data server for
dereferencing ontology terms [21]. Ontobeep is an ontology
comparison program that compares ontologies and identi-
fies common terms existing in two or three ontologies by
aligning 23 ontologies from the roots of these ontologies
[22]. Ontobeep also detects inconsistency and term duplica-
tion in one or more ontologies.
Demonstrations of XOD implementation for interoperable
ontology development
Figure 4 outlines a simple pipeline of how the XOD
principles can be used together for productive ontology
development. Basically, a new ontology can be initiated
by reusing existing terms from different ontologies
(XOD 1) and aligning these terms in a semantic
Fig. 4 A general ontology development pipeline using XOD principles. To initiate a new ontology, needed terms from existing ontologies are
imported and reused (XOD 1) and aligned together with other ontology terms in a consistent semantic framework (XOD 2). To add more terms
and semantics afterwards, we can use the same XOD 1/2 methods to add terms from existing ontologies, and for new terms, we can either use
ODP-based term generation strategy (XOD 3) and manually align and add terms to the new ontology. Community extensibility (XOD 4) should
be considered and applied during the whole ontology development pipeline
He et al. Journal of Biomedical Semantics  (2018) 9:3 Page 6 of 10
framework (XOD 2), and new terms can be added by ex-
tending the semantic framework (XOD 2) and if ODPs
identifiable, applying ODP-based approach (XOD 3).
Ontology development often uses top-down and
bottom-up approaches simultaneously [49]. The ontol-
ogy initiation step is usually achieved by the top-down
approach, i.e., developing the top level semantic frame-
work by reusing and aligning upper level terms and
semantics from existing ontologies (XOD 1/2). The same
top-down approach can also be used to generate the
upper level new terms commonly identified in the new
ontology. Meanwhile, the bottom-up approach is use
case driven and focuses on adding new terms to address
specific use cases. For the bottom-up approach, XOD 1
3 principles are all important, and if possible, ODP-
based design and term generation (XOD 3) is often crit-
ical to ensure development efficiency and consistency.
Here we will demonstrate our pipeline by using the
complete use case of developing the community-based
Vaccine Ontology (VO) [10, 50, 51]. As outlined in the
extensive ontology development pipeline (Fig. 4), at the
early stage of the VO development, we performed ontol-
ogy survey and reused terms from several existing ontol-
ogies including BFO [33], OBI [7], GO [8], and the
Information Artifact Ontology (IAO) [52]. The original
VO version reported in 2009 included ~1000 imported
terms from 10 existing ontologies and ~1000 VO-
specific terms [50]. Since then more terms have been
added to VO. As of November 20, 2017, out of 6541
terms in VO, approximately 1600 terms were imported
and reused from approximately 30 ontologies (http://
www.ontobee.org/ontostat/VO).
Many VO-specific terms were added to VO by seman-
tically alignment with the upper BFO ontology or middle
level ontologies (e.g., OBI) (XOD 2). For example, VO
term vaccine (VO_0000001) is asserted as a subclass of
OBI term processed material (OBI_0000047). This
assertion means that any non-processed material (e.g.,
an infectious bacterium that exists in the air) that causes
an infection in human and eventual immune responses
and protection in the human is not counted as a vaccine.
Similarly, the VO term vaccination (VO_0000002) is
asserted as a subclass of OBI term administering
substance in vivo (OBI_0600007). The alignment with
administering substance in vivo differentiates VO
vaccination (i.e., administering a vaccine to in vivo) from
immunization (i.e., to make one immune to something).
In comparison, vaccination is considered as the synonym
of immunization in MedDRA, a controlled terminology
system commonly used for representation of regulatory
activities [53].
In many cases, we can generate a number of new
terms simultaneously by developing and following spe-
cific ODPs (XOD 3). For example, the VO developers
retrieved from the US Department of Agriculture
(USDA) and other public databases the information of
approximately 800 licensed animal vaccines. Manually
adding these animal vaccines to VO would be time
consuming. To speed up the inclusion of the large
number of licensed animal vaccines to VO, an ODP was
developed to include different entities (e.g., vaccine
name, manufacturer, animal species, animal pathogen,
and disease), annotations, and the semantic relations
among these entities. Such an ODP was further used to
design an Excel template which was then applied to
include the categorized information of these vaccines.
Ontorat was finally used to automatically transfer the
ODP and the information recorded in the Excel file to
an OWL file and then imported to VO [20]. Further-
more, the same ODP could be used later to add new
animal vaccines to VO. The Ontorat use case of ODP-
based VO addition of veterinary vaccines was first
presented in the 2012 International Conference for
Biomedical Ontology (ICBO) [54]. Since then other
ODPs were also developed for further VO development
[51]. Meanwhile, it is noted that not all new terms can
be fit under identifiable common design patterns. In this
case, we can generate the term by aligning it with exist-
ing framework (XOD 2) (Fig. 4).
As a community-based open source ontology, the VO
development has involved the broader community in its
continuous development (XOD 4). The community
participation helps further extend the VO and its inter-
operability with other biomedical ontologies. For example,
according to BioPortal and Ontobee, the VO term vaccine
(VO_0000001) has been reused by more than ten other on-
tologies such as OBI and Apollo Structured Vocabulary
(https://github.com/apollodev/), and the VO term vaccin-
ation (VO_0000002) has been reused by ten other ontologies
such as the Prescription of Drugs Ontology (https://github.-
com/OpenLHS/PDRO). In addition, the VO community in-
volvement makes it achieve better data interoperability with
other ontologies. Meanwhile, the community involvement
extends the applications of VO, such as vaccine-related Tcell
and B cell response analysis and queries [55], epitope data
management [56], vaccine-related literature mining [10], and
vaccine-related network analysis [57, 58].
In addition to VO, many other ontologies, e.g., Beta Cell
Genomics Ontology (BCGO) [37], MicrO ontology for
representing microorganism phenotypic and metabolic
characters [59], and BioAssay Ontology (BAO) [60], have
been developed using the same or similar strategies.
Discussion and perspectives
The XOD strategy and principles reflect the growing ma-
turity of biological and biomedical ontology development.
When only a small number of ontologies were developed,
such XOD strategy was not needed. However, with
He et al. Journal of Biomedical Semantics  (2018) 9:3 Page 7 of 10
hundreds of ontologies developed now, it is critical to
ensure ontology interoperability, and the XOD principles
provide a practical solution. Given the importance of
ontologies in the integration, sharing, and analysis of the
increasing large and heterogeneous data/metadata and
knowledge, the XOD strategy is very significant and
critical to meet the challenges in the current big data era.
Among the four XOD principles, the first three princi-
ples emphasize the requirements to reuse ontology
terms, extend and align semantic structures, and build
new terms and semantics among terms using design pat-
terns. XOD 2 is a more general principle which covers
the semantic interoperability among terms including
terms from the target ontology and terms newly gener-
ated or imported from source ontology. Extending XOD
1 and 2, XOD 3 provides a more specific mechanism
(i.e., ODP-based term generation and editing) to achieve
consistent ontology term generation and annotation.
While the first three principles provide more technical
guidance, XOD 4 emphasizes the community collabor-
ation and involvement in new ontology development.
XOD is complementary to the OBO principles [1] and
the ten simple rules proposed for ontology development
[26]. The OBO principles (e.g., open, common format, ver-
sioning, scope, relations, users, collaboration, and locus of
authority) provide general principles for the development
of an ontology (http://obofoundry.org/principles/fp-000-
summary.html). The ten simple rules proposed by Malone
et al. include ontology term reuse, design patterns, and
community engagement [26], which are directly associated
with XOD principles. The other 7 rules (e.g., scope, license,
versioning) are not directly related. In comparison to the
OBO principles and the ten simple rules, the XOD princi-
ples address the single important point of ontology extensi-
bility and emphasize different scales of extensible relations
among ontologies, with the aim to achieve ontology inter-
operability. Since different ontologies extend and are
aligned with existing reliable ontologies, applying the XOD
principles will support the OBO aim of establishing non-
redundant and interoperable suite of ontologies.
The XOD strategy supports the FAIR Guiding Princi-
ples, which propose that various data be Findable,
Accessible, Interoperable and Reusable [27]. Ontologies
lay out the basic foundation for the data FAIRness.
Adopting the XOD strategy will lead to the development
of extensible ontologies and the generation of ontology-
extended data and metadata representations. Such
ontology-supported data sharing and integration will
result in natural data access, interoperability, and usabil-
ity, query, and advanced analysis. For example, the
KaBOB knowledge base uses the OBO ontologies to se-
mantically integrate data from 18 prominent biomedical
databases [61]. Millions of RDF triples were also gener-
ated in KaBOB, enabling findable, accessible,
interoperable, and reusable queries of the underlying
data from these databases. Therefore, the XOD strategy
supports the eventual achievement of the FAIRness of
data.
Many challenges exist in adopting and achieving the
goals defined in the XOD strategy and principles. First,
the interoperability among current hundreds of ontol-
ogies is still limited and challenging [32, 62, 63]. Term
redundancy among ontologies cannot be solved easily,
leading to issues of achieving data FAIRness. Second,
only a small amount of data resources (including a large
number of databases) adopt ontology-guided strategy,
which restricts data interoperability and analysis. Third,
while many linked data systems [25] standardize data
using ontologies, the ontologies underlying linked data are
often non-interoperable, making linked data systems
become individual silos and difficult to integrate [64, 65].
To address these challenges, it is important to adopt the
XOD strategy and XOD principles. Active ontology train-
ing and outreach will be beneficial.
The suite of Ontoanimal tools has provided different
features to address several real issues in ontology devel-
opment. Each of these tools focuses on one or more
primary tasks, and all together they are combined to
strongly support XOD principles. Given the complexity
of these tools, there are concerns about their usability
and sustainability. Since these tools are more about
research in ontology development, it is important to
have a strong evaluation system to be used to better
understand the strengths and limitation of each tool.
While Ontoanimal tools and other similar XOD tools
have already supported the XOD strategy, existing tools
require further improvements, and more user-friendly
integrative tools are needed. Tools are critical to make
more efficient extensible ontology development. For
example, although it was recognized that term reuse was
a better strategy, the term reuse principle was not widely
implemented until Ontofox and other tools were devel-
oped. Currently XOD tool usage often requires extensive
training. More easy-to-use and integrative XOD tools
are desired for ontology developers and users with no or
limited programming background. We believe that the
adoption of the XOD principles together with robust
XOD tools would greatly support interoperable ontology
development and data FAIRness.
Conclusion
Our examination of Ontoanimal tools and similar programs
discovered their shared features of extensibility. We pro-
posed the eXtensible Ontology Development (XOD) strat-
egy and four XOD principles to support extensible ontology
development and usage. We propose to adopt these XOD
principles for active development and usage of extensible
ontologies and tools, leading to better data FAIRness.
He et al. Journal of Biomedical Semantics  (2018) 9:3 Page 8 of 10
Abbreviations
BFO: Basic formal ontology; GO: Gene ontology; MIREOT: Minimum
Information to reference an external ontology term; NCBO: National Center
for Biomedical Ontology; OAE: Ontology of adverse events; OBO: Open
biological and biomedical ontologies; ODP: Ontology design pattern;
OWL: Web ontology language; RDF: Resource description framework;
XML: eXtensible markup language; XOD: eXtensible ontology development
Acknowledgements
We acknowledge the OBO Foundry consortium and OBO ontology
developers for their support and collaborations.
Funding
The Ontoanimal tool research was partly supported by the NIH National
Institute of Allergy and Infectious Diseases grant 1R01AI081062.
Availability of data and materials
Not applicable.
Authors contributions
YH Ontoanimal tool project design and management, initial XOD concept
proposer, preparation of the first manuscript draft; ZX Primary developers of
Ontoanimal tools Ontofox, early version of Ontobee, Ontobeep, Ontodog,
Ontorat, and Ontobat; JZ Co-developer of Ontodog, Ontorat, and Ontobee;
YL Ontoanimal tool testing, discussion, and evaluation; JO ROBOT tool
developer, Ontoanimal tool testing and evaluation; EQ Primary developers of
Ontokiwi/Ontobedia, later version of Ontobee, Ontobull, and Ontoanimal
tool maintenance. All authors participated in discussion, manuscript
preparation and editing, and approved the manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Unit for Laboratory Animal Medicine, Department of Microbiology and
Immunology, Center for Computational Medicine and Bioinformatics,
University of Michigan Medical School, Ann Arbor, MI, USA. 2Department of
Genetics, University of Pennsylvania Perelman School of Medicine,
Philadelphia, PA 19104, USA. 3Center for Computational Science, University of
Miami, Coral Gables, FL, USA. 4Knocean Inc., Toronto, ON, Canada.
5Department of Computational Medicine and Bioinformatics, University of
Michigan Medical School, Ann Arbor, MI, USA.
Received: 1 May 2017 Accepted: 7 December 2017
RESEARCH Open Access
GGDonto ontology as a knowledge-base
for genetic diseases and disorders of
glycan metabolism and their causative
genes
Elena Solovieva1, Toshihide Shikanai1,2, Noriaki Fujita1,2 and Hisashi Narimatsu1,2*
Abstract
Background: Inherited mutations in glyco-related genes can affect the biosynthesis and degradation of glycans
and result in severe genetic diseases and disorders. The Glyco-Disease Genes Database (GDGDB), which provides
information about these diseases and disorders as well as their causative genes, has been developed by the Research
Center for Medical Glycoscience (RCMG) and released in April 2010. GDGDB currently provides information on about 80
genetic diseases and disorders caused by single-gene mutations in glyco-related genes. Many biomedical resources
provide information about genetic disorders and genes involved in their pathogenesis, but resources focused on genetic
disorders known to be related to glycan metabolism are lacking. With the aim of providing more comprehensive
knowledge on genetic diseases and disorders of glycan biosynthesis and degradation, we enriched the content of the
GDGDB database and improved the methods for data representation.
Results: We developed the Genetic Glyco-Diseases Ontology (GGDonto) and a RDF/SPARQL-based user interface using
Semantic Web technologies. In particular, we represented the GGDonto content using Semantic Web languages, such as
RDF, RDFS, SKOS, and OWL, and created an interactive user interface based on SPARQL queries. This user interface
provides features to browse the hierarchy of the ontology, view detailed information on diseases and related genes,
and find relevant background information. Moreover, it provides the ability to filter and search information by faceted
and keyword searches.
Conclusions: Focused on the molecular etiology, pathogenesis, and clinical manifestations of genetic diseases and
disorders of glycan metabolism and developed as a knowledge-base for this scientific field, GGDonto provides
comprehensive information on various topics, including links to aid the integration with other scientific resources.
The availability and accessibility of this knowledge will help users better understand how genetic defects impact
the metabolism of glycans as well as how this impaired metabolism affects various biological functions and human
health. In this way, GGDonto will be useful in fields related to glycoscience, including cell biology, biotechnology, and
biomedical, and pharmaceutical research.
Keywords: Semantic web technologies, Ontology, RDF/SPARQL-based user interface, Glycan metabolism, Genetic
diseases and disorders
* Correspondence: h.narimatsu@aist.go.jp
1Glycoscience and Glycotechnology Research Group, National Institute of
Advanced Industrial Science and Technology (AIST), Tsukuba, Japan
2GlycoBiomarker Leading Innovation Co. Ltd. (GL-i), Tsukuba, Japan
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Solovieva et al. Journal of Biomedical Semantics  (2018) 9:14 
https://doi.org/10.1186/s13326-018-0182-0
Background
Glycoscience refers to the study of the structures and
functions of glycans and glycoconjugates and covers a
wide range of topics, including the role of carbohydrates
in disease development. Glycans play essential roles in
many cellular functions and biological processes, and
abnormalities in their metabolism lead to impaired func-
tions of multiple organ systems and eventually result in
the development of diseases and disorders. Inherited
mutations in glyco-related genes can affect the biosynthesis
of glycans as well as their degradation, and this impaired
metabolism can result in severe genetic diseases and
disorders, such as congenital disorders of glycosylation
(CDG) and lysosomal storage diseases (LSD).
Genetic diseases and disorders of glycan metabolism
are currently the subject of research in many scientific
fields, including glycoscience and biomedical research.
Many relevant information resources and databases are
now publicly available, including the Online Mendelian
Inheritance in Man (OMIM) [1], the Genetics Home
Reference [2], the Genetic and Rare Diseases Information
Center (GARD) resources [3], the Genetic Testing Registry
(GTR) [4], and the Orphanet portal [5], among others.
These resources provide information on a wide range of
known hereditary diseases and disorders as well as their
causative genes, and, moreover, their contents are not
restricted to certain types of metabolic products. In
contrast to these broadly focused information resources,
the Glyco-Disease Genes Database (GDGDB) [6] has been
created by the Research Center for Medical Glycoscience
(RCMG) to provide information about hereditary diseases
and disorders caused by defects of single glyco-related
genes. The GDGDB provides detailed information about
inborn errors of glycan metabolism in the context of their
responsible genes and pathogenetic processes. It is a
relational database with a web-based user interface and
is available in English and Japanese.
Semantic Web technologies allow us to describe and
organize scientific information, to create a knowledge-
base for a particular scientific field, to share information
across services, and to build links between related informa-
tion resources. They have been widely applied to biomedical
and life sciences information in recent years.
As a part of the Life-Science Database Integration
Project of the National Bioscience Database Center
(NBDC) sponsored by the Japan Science and Technology
Agency (JST), we developed a knowledge-base of genetic
diseases and disorders of glycan metabolism and their
causative genes. We created an ontology named the
Genetic Glyco-Diseases Ontology (GGDonto) [7]. For
its development, we performed the following steps: 1)
we created the schema and main content of GGDonto;
2) organized the GDGDB information, RDFized it, and
integrated it into GGDonto; 3) added information about
other genetic diseases and disorders sharing a similar
etiology; 4) created a classification scheme for the
diseases recorded in GGDonto; and 5) enriched the
content of this ontology by linking it to other biomedical
resources and integrating the information into GGDonto.
Our GGDonto ontology is based on Semantic Web
technologies and is thus represented in Resource Descrip-
tion Framework (RDF) format using the RDF Schema
(RDFS), Simple Knowledge Organization System (SKOS),
and Web Ontology Language (OWL) vocabularies. We also
developed a RDF/SPARQL-based user interface, which
allows users to navigate the ontology, perform searches,
and view detailed information in a user-friendly manner.
These ontology and RDF/SPARQL-based user interface
are available at http://acgg.asia/db/diseases/.
In this article, we introduce the topics of GGDonto,
describe the structure of this ontology, and explain the
functionality of its user interface, including browsing,
searching, and filtering functionalities.
Current knowledge on genetic diseases and
disorders of glycan metabolism and their
causative genes
Over the past several decades, associations between many
human genetic disorders and mutations in genes involved
in the biosynthesis and degradation of glycans have been
newly identified [811].
Current knowledge on genetic diseases and disorders of
glycan biosynthesis
Glycosylation is the enzymatic process by which glycans
are created, altered, and attached to proteins and lipids
[11]. Genetic defects in glycosylation lead to a variety of
inherited metabolic disorders known as CDG [10, 1214].
In the last decade, several disorders of N-linked protein
glycosylation, O-linked protein glycosylation, and glyco-
lipid and glycosaminoglycan biosynthesis have been newly
described, and this process of discovery is just beginning
[11, 1317]. While many glycosylation disorders are
caused by defects in the N-glycosylation pathway, others
result from defects in the O-glycosylation pathway, both
the N- and the O-glycosylation pathways, or other path-
ways. However, defects in C-glycosylation have not yet been
reported [13, 15]. Obviously, the discovery and detailed
descriptions of novel CDG provide new knowledge about
the role of glycans in human physiology and health and
demonstrate a wide range of biological processes that are
dependent on proper glycosylation [16].
The traditional classification of CDG was proposed at
the First International Workshop on CDG and Related
Disorders in Leuven in 1999; it is based on the serum
transferrin pattern obtained by the isoelectric focusing
test [11, 15, 18, 19]. In this classification, CDG are divided
into two groups, I and II, and lowercase letters are used to
Solovieva et al. Journal of Biomedical Semantics  (2018) 9:14 Page 2 of 14
indicate subtypes of disorders in chronological order of
their discovery. In this classification, the CDG-I group
includes disorders characterized by the under-occupancy of
N-glycosylation sites, and the CDG-II group includes disor-
ders caused by defects of N-glycan trimming and elongation
[10, 13, 19, 20]. Based on the transferrin isoelectric focusing
analyses, the traditional nomenclature has included only
N-glycosylation disorders. O-Glycosylation and glycolipid
biosynthesis disorders have not been included in this classi-
fication, and they have been assigned trivial or biochemical
names not associated with the family of CDG [18, 19].
As the number of CDG disorders increased, the trad-
itional classification became more complex and difficult.
Moreover, this classification does not indicate the defective
genes and enzymes that are responsible for the development
of disorders [14, 15, 18]. In 2008, Jaeken et al. proposed a
new nomenclature for all types of CDG [18], and in 2009,
the traditional classification of CDG was revised [15]. In the
new nomenclature, the name Congenital Disorders of
Glycosylation (CDG) is used not only for all types of
protein glycosylation disorders, but also for lipid glyco-
sylation disorders [15, 18, 21]. In this nomenclature,
glycosylation disorders are named by the official symbol
of defective genes, followed by the abbreviation CDG. If
the disorder had a letter-based name in the previous
classification, this name is also used and follows in paren-
theses. For example, in the new nomenclature, CDG type
Ia is named PMM2-CDG (CDG-Ia) [12, 13, 15, 18].
Disorders that are caused by defects of glycosylation
have broad, diverse, and severe clinical phenotypes and
underline the significant roles of glycosylation in human
cells and tissues [1114]. These clinical features are highly
variable, ranging from infantile lethality to moderate intel-
lectual disabilities in adults [14, 17, 19, 20]. Impaired glyco-
sylation usually has a pathophysiological impact in multiple
organ systems, but is particularly likely to affect brain devel-
opment and functions of the nervous, musculoskeletal,
digestive, and immune systems [10, 19]. Clinically, most
patients with these disorders have a general failure to
thrive, developmental delay, psychomotor retardation,
neurological and neuromuscular impairments, and variable
features, like musculoskeletal and eye abnormalities, coagu-
lopathies, hormone dysfunction, and many other symptoms
[10, 17, 19, 20]. However, the phenotypes of some glycosyla-
tion disorders are not completely understood and their
clinical descriptions are limited owing to the small
number of reported cases [12, 19].
Patients with each of these disorders are characterized
by diverse clinical phenotypes, and therefore the clinical
features cannot be used to identify a mutated gene [13, 19].
However, similar clinical features can point to common
causes and common underlying mechanisms, and pheno-
typic similarities may be helpful for the identification of
additional genes related to glycan biosynthesis and
modifications [16]. Moreover, the discovery of novel glyco-
sylation disorders with unexpected clinical findings as well
as the description of clinical variability with additional find-
ings for known disorders may help to understand the func-
tions of glyco-related genes [13, 16]. Certainly, an increased
knowledge of glycosylation disorders will contribute to a
better understanding of the mechanism and impact of
genetic defects on glycosylation and also the impact of
glycosylation on various biological functions and human
health [13, 14, 16, 20].
Current knowledge on genetic diseases and disorders of
glycan degradation
Inherited genetic defects that result in the absence or
deficiency of specific lysosomal hydrolases cause LSD.
Missing or deficient enzymes cause the accumulation
and storage of intermediate compounds in cells, tissues,
and organs, and the macromolecules that are not prop-
erly degraded lead to cellular damage and the onset of
symptoms [9, 2224].
Most glycans are degraded and recycled in lysosomes
[9, 23, 25]. In this manuscript, we discuss LSD that are
associated with the impaired degradation of glycans and
glycoconjugates. These LSD are generally classified accord-
ing to the type of glycoconjugate whose catabolism is
impaired, and they are usually divided into three groups
[9, 23]. The first group contains diseases caused by genetic
defects in enzymes that are involved in the lysosomal deg-
radation of oligosaccharide chains of glycoproteins. These
diseases are also called oligosaccharidoses. The second
group contains diseases associated with genetic deficiencies
in lysosomal enzymes that degrade the polysaccharide
chain (glycosaminoglycan) of proteoglycans. These types of
diseases are called mucopolysaccharidoses (MPS). The
third group contains diseases associated with genetic
defects in enzymes that are involved in glycolipid degrad-
ation. These types of diseases are called sphingolipidoses
when they are associated with the catabolism of sphingoli-
pids and other lipid storage diseases when they are
generally not classified as sphingolipidoses, such as Wolman
disease, which affects the metabolism of cholesteryl esters
and triglycerides [9, 23, 26].
Glycan-related LSD are characterized by a progressive
multisystem pathology and a variety of progressive phys-
ical impairments and mental deterioration [22, 24]. The
phenotypes associated with these LSD include the follow-
ing clinical characteristics: brain pathology with central
nervous system manifestations, hepatomegaly, splenomeg-
aly, skeletal abnormalities, and heart and lung pathology
[2, 24]. For many LSD, different degrees of severity may
be present. If genetic defects result in the complete oblit-
eration of enzyme activity, affected individuals tend to
have earlier onsets of symptoms. If genetic defects result
in the significant reduction of enzyme activity, clinical
Solovieva et al. Journal of Biomedical Semantics  (2018) 9:14 Page 3 of 14
signs and symptoms may manifest later in childhood, ado-
lescence, or adult life, and patients with these juvenile,
childhood, or adult onsets usually display more moderate
symptoms than those of patients with earlier onsets
[9, 22]. Moreover, the organ systems involved may differ
with respect to the time of onset [9].
Inherited genetic defects in the lysosomal catabolism
of glycans show the importance of the balance between
glycan synthesis and degradation for proper biological
functions of cells and tissues [25, 27]. The lysosomal
degradation of glycans and glycoconjugates are ordered
and highly specific processes and many new insights in
our understanding of these complex pathways have been
obtained in studies of LSD [9].
Methods
As described above, both glycan biosynthesis and lysosomal
catabolism are very complicated and highly regulated pro-
cesses, playing important roles in tissue homeostasis. For a
better understanding of the etiology and clinical character-
istics of these diseases and disorders, more comprehensive
information is needed, and this is the aim of GGDonto.
Designing the structure and content of the GGDonto
ontology
To provide comprehensive information about genetic
diseases and disorders of glycan metabolism, we designed
the structure and content of GGDonto.
Most of the illnesses may be described by various
characteristics, including their names, etiology, pathogen-
esis, clinical manifestations, nosological classifications, the
descriptions and codes from coding systems, and identifiers
from biomedical sources. We used the same approach to
describe the diseases and disorders included in GGDonto.
Later in this section, we have described the types of infor-
mation included in the content of GGDonto, the classes
and properties used for data representation, and the
sources that were used to obtain the information for
ontology creation.
As the metabolism of glycans is very complicated, for
the development of a knowledge-base of genetic diseases
and disorders caused by defects in this metabolism, it
was necessary to describe these impairments in detail in
the context of metabolic pathways. Moreover, because these
impairments lead to significant clinical manifestations, it
was important to organize phenotype-related information
in a way that is easy to navigate and understand. For this
purpose, we decided to create classifications of the
GGDonto diseases and disorders.
In nosology (the branch of medical science dealing
with the classification of diseases), diseases are usually
classified by etiology (cause), pathogenesis (mechanism by
which the disease is caused), the presenting symptoms, or
the organ systems involved. We also designed and created
similar types of classifications of the genetic diseases and
disorders of glycan metabolism.
The causes of all diseases and disorders included in
GGDonto are single-gene mutations in glyco-related
genes that result in the absence or deficiency of enzymes
involved in the metabolism of glycans. As a conse-
quence, the mechanism underlying these diseases and
disorders is associated with impaired glycan metabolism,
which leads to glycan deficiency or accumulation. As
the cause and mechanism are both associated with the
metabolic pathways of glycans, we decided to design the
Pathway classifications that represent the etiological and
pathogenetic aspects of the GGDonto diseases and disor-
ders. These Pathway classifications are the main advantage
of our ontology. We designed this type of classifications on
the basis of scientific literature, and to our knowledge,
this is the first time classifications with such a structure
and content, including all subcategories, are being pre-
sented in an ontology. The details of the structure of our
Pathway classifications are presented in Additional file 1.
Along with Pathway classifications, we designed
Phenotype classifications that grouped diseases and
disorders by their symptoms and involved organ systems.
All of these classifications have been described in the
subsection Classifications of the GGDonto diseases and
disorders.
To provide more detailed knowledge about the cause and
mechanism, symptoms, and phenotypes of the GGDonto
diseases and disorders, we decided to add the corresponding
information from related biomedical resources, such as
various descriptions of causative genes and corresponding
enzymes, and definitions of diseases and phenotypic char-
acteristics. In the subsection Linking GGDonto to related
biomedical resources, we have explained how these
resources were integrated into the GGDonto ontology.
Application of semantic web technologies for the
development of the GGDonto ontology
The GGDonto ontology was developed for the particular
subject domain using the Semantic Web standards, such
as RDF, RDFS, and SKOS in combination with OWL.
OWL was used to represent the semantic structure and
semantic relations of this ontology. To describe the
information in GGDonto, we used many elements from
the OWL vocabulary as well as our self-defined classes
and properties that were also defined by rdf:type property
as instances of owl:Class or owl:ObjectProperty and owl:
DatatypeProperty.
To organize distinct concepts (ideas or meanings) of
the GGDonto ontology into concept schemes (a set of
concepts that may include hierarchical, associative, and
semantic relationships between them) we used the SKOS
vocabulary. As it is described in the SKOS Reference
(https://www.w3.org/TR/skos-reference/) and SKOS Primer
Solovieva et al. Journal of Biomedical Semantics  (2018) 9:14 Page 4 of 14
(https://www.w3.org/TR/skos-primer/), the notion of a
concept scheme is useful when dealing with two or more
different knowledge organization systems, such as thesauri,
taxonomies, classification scheme, and subject heading
systems. We created the instance of skos:ConceptScheme
class for each of our classifications, in accordance with the
fact that the origins of their structure are different informa-
tion sources, including the Medical Subject Headings
(MeSH), UMLS Metathesaurus, and scientific literature that
describes the classifications of CDG and LSD. Moreover,
the information from other sources, such as the GDGDB
database and some of the NCBI databases, is also included
in the content of our system. For each of these, we also
created the instance of the skos:ConceptScheme class.
We defined the main elements (meanings) of our ontol-
ogy, such as the diseases, symptoms, or terms in the disease
classifications as instances of skos:Concept class, and their
semantics as instances of our self-defined classes. Using the
skos:inScheme property we linked each of these concepts
with one or more of our concept schemes. Because in our
content each of the diseases or disorders is included in two
(Pathway and Phenotype for LSD) or three (traditional
and new Pathway and Phenotype for CDG) classifica-
tions, each disease concept is linked with two or three of
our concept schemes. As it is described in the SKOS Primer,
for the SKOS concepts, it is possible to participate in several
concept schemes at the same time. Moreover, by using the
skos:broader, skos:narrower, and skos:related properties,
as well as their subproperties defined in GGDonto, we
aggregated the set of concepts from each of our schemes
into distinct structures with their own hierarchical, associ-
ated, and semantic relationships.
As we organized our information into multiple concept
schemes, it was necessary to indicate how these concepts
are related to each other. For this purpose, along with using
semantic relation properties defined in our ontology, we
used the SKOS mapping properties, such as skos:close-
Match and skos:exactMatch. As it is described in the SKOS
Primer, the SKOS mapping properties helped us to indicate
that two concepts from different schemes have comparable
meanings, and to specify these meanings. Linking concepts
by mappings (skos:closeMatch and skos:exactMatch) and
the possibility of a concept to participate in different concept
schemes (skos:inScheme) are the important features
and key advantages of SKOS. This approach helps avoid
the taxonomical conflicts that may be occurring through
the integration different information sources. For example,
CDG are classified in our ontology by the traditional and
new Pathway classifications at the same time.
Creating the schema and main content of the GGDonto
ontology
To provide comprehensive information about genetic
diseases and disorders of glycan metabolism, defined
these genetic diseases and disorders as the main elements
(concepts) of our ontology. The GGDonto ontology was
developed on the basis of the scientific literature and
Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 
DOI 10.1186/s13326-017-0168-3
RESEARCH Open Access
Exploiting graph kernels for high
performance biomedical relation extraction
Nagesh C. Panyam, Karin Verspoor*, Trevor Cohn and Kotagiri Ramamohanarao
Abstract
Background: Relation extraction from biomedical publications is an important task in the area of semantic mining of
text. Kernel methods for supervised relation extraction are often preferred over manual feature engineering methods,
when classifying highly ordered structures such as trees and graphs obtained from syntactic parsing of a sentence.
Tree kernels such as the Subset Tree Kernel and Partial Tree Kernel have been shown to be effective for classifying
constituency parse trees and basic dependency parse graphs of a sentence. Graph kernels such as the All Path Graph
kernel (APG) and Approximate Subgraph Matching (ASM) kernel have been shown to be suitable for classifying
general graphs with cycles, such as the enhanced dependency parse graph of a sentence.
In this work, we present a high performance Chemical-Induced Disease (CID) relation extraction system. We present a
comparative study of kernel methods for the CID task and also extend our study to the Protein-Protein Interaction (PPI)
extraction task, an important biomedical relation extraction task. We discuss novel modifications to the ASM kernel to
boost its performance and a method to apply graph kernels for extracting relations expressed in multiple sentences.
Results: Our system for CID relation extraction attains an F-score of 60%, without using external knowledge sources
or task specific heuristic or rules. In comparison, the state of the art Chemical-Disease Relation Extraction system
achieves an F-score of 56% using an ensemble of multiple machine learning methods, which is then boosted to 61%
with a rule based system employing task specific post processing rules. For the CID task, graph kernels outperform tree
kernels substantially, and the best performance is obtained with APG kernel that attains an F-score of 60%, followed by
the ASM kernel at 57%. The performance difference between the ASM and APG kernels for CID sentence level relation
extraction is not significant. In our evaluation of ASM for the PPI task, ASM performed better than APG kernel for the
BioInfer dataset, in the Area Under Curve (AUC) measure (74% vs 69%). However, for all the other PPI datasets, namely
AIMed, HPRD50, IEPA and LLL, ASM is substantially outperformed by the APG kernel in F-score and AUC measures.
Conclusions: We demonstrate a high performance Chemical Induced Disease relation extraction, without
employing external knowledge sources or task specific heuristics. Our work shows that graph kernels are effective in
extracting relations that are expressed in multiple sentences. We also show that the graph kernels, namely the ASM
and APG kernels, substantially outperform the tree kernels. Among the graph kernels, we showed the ASM kernel as
effective for biomedical relation extraction, with comparable performance to the APG kernel for datasets such as the
CID-sentence level relation extraction and BioInfer in PPI. Overall, the APG kernel is shown to be significantly more
accurate than the ASM kernel, achieving better performance on most datasets.
Keywords: Relation extraction, Graph kernels, APG kernel, ASM kernel
*Correspondence: karin.verspoor@unimelb.edu.au
School of Computing and Information Systems, University of Melbourne,
Melbourne, Australia
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 2 of 11
Background
Automated text mining has emerged as an important
research topic for effective comprehension of the fast
growing body of biomedical publications [1]. Within this
topic, relation extraction refers to the goal of automated
extraction of relations between well known entities, from
unstructured text. Chemical-induced-Disease (CID) rela-
tion extraction is motivated by critical applications such
as toxicology studies and drug discovery. The impor-
tance of CID relations is evident from a recent study of
Pubmed search logs [2], that observed that Chemicals,
Diseases and their relations are the most popular search
topics.
Relation extraction: sentence vs non-sentence level
A large corpus of annotated Pubmed abstracts for CID
relation extraction is now available from BioCreative-V
[3] for furthering research and comparison of different
methods. This is known as the Chemical-Disease Rela-
tions (CDR) corpus. The main objective of the CID rela-
tion extraction task defined by BioCreative-V CDR task
[3], is to infer Chemical-Disease relations expressed by a
Pubmed document (Title and Abstract only). A sample
annotated article from this corpus is illustrated in Table 1.
More generally, relation extraction from text refers to
the task of inferring a relationship between two entities
mentioned in the text.
Within this corpus, many relations may be inferred by
analyzing a single sentence that bears the mentions of the
relevant entities (Chemical and Disease). We refer to such
relations as sentence level relations. For example, the rela-
tion between Propylthiouracil and hepatic damage can
be inferred by analyzing the single sentence in the title.
non-sentence level relations, such as the relation between
propylthiouracil and chronic active hepatitis, are those
in which the entity mentions are separated by one or more
sentence boundaries. These relations cannot be inferred
Table 1 Illustration of an annotated Pubmed abstract from the
CDR corpus
Title Propylthiouracil-induced hepatic damage
Abstract Two cases of propylthiouracil-induced liver damage
have been observed. The first case is of an acute type of
damage, proven by rechallenge; the second presents a
clinical and histologic picture resembling chronic active
hepatitis, with spontaneous remission.
Entity D011441, Chemical, Propylthiouracil, 0-16
Entity D011441, Chemical, propylthiouracil, 54-70
Entity D056486, Disease, hepatic damage, 25-39
Entity D056486, Disease, liver damage, 79-91
Entity D006521, Disease, chronic active hepatis, 246-270
Relation (CID) D011441 - D006521
Relation (CID) D011441 - D056486
by analyzing a single sentence. We refer to such relations
as the non-sentence level relations.
Prior research has shown that relation extraction can
be addressed effectively as a supervised classification
problem [4], by treating sentences as objects for classi-
fication and relation types as classification labels. Clas-
sifiers such as Support Vector Machines (SVMs) are
typically used for high performance classification by first
transforming a sentence into a flat feature vector or
directly designing a similarity score (implemented as a
kernel function) between two sentences. Kernel meth-
ods allow us to directly compute a valid kernel score
(a similarity measure) between two complex objects,
while implicitly evaluating a high dimensional feature
space.
The approach of using a kernel is favored for work-
ing with syntactic parses of a sentence which are highly
structured objects such as trees or graphs. Tree or graph
kernels are known to be efficient in exploring very high
dimensional feature spaces via algorithmic techniques.
Deep learning [5, 6] based efforts are other alternatives,
whose goal is to enable discovery of features (represen-
tation learning) with little or no manual intervention.
However, we limit our scope in this work, to exploring ker-
nel methods for CID relation extraction.We first illustrate
parse structures and then describe the kernels developed
for using these parse structures.
Parse trees vs parse graphs
Simple approaches that use a bag of words model for
a sentence, ignore the inherent order within a sentence.
However, a sentence can be mapped to an ordered object
such as a tree or a graph by using a syntactic parser [7].
We illustrate the syntactic parse structures of a sample
sentence in Fig. 1. A constituency parse tree, encodes a
sentence as a hierarchical tree, as determined by the con-
stituency grammar. The internal nodes of this tree carry
grammatical labels such as noun phrase (NP) and verb
phrase (VP) and the leaf nodes have as labels the words
or tokens in the sentence. In contrast, a dependency graph
expresses grammatical relationships such as noun sub-
ject (nsubj) and verb modifier (vmod) , as directed
and labelled edges between the tokens in the sentence.
The nodes of this graph correspond one-to-one with the
tokens of the sentence. The undirected version of a depen-
dency graph, obtained by dropping edge directions, may
or may not result in a cycle free graph. For example,
the basic version of dependency graphs produced by the
Stanford Parser [7] is guaranteed to be cycle free, in
its undirected form. However, the enhanced dependency
parses produced by the Stanford Parser may contain
cycles in its undirected form. In the example illustrated
in Fig. 1, note the cycle between the nodes caused and
fatigue in the enhanced dependency graph.
Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 3 of 11
Fig. 1 Illustration of different parse structures for the sentence :Seizures were caused by Alcohol and Fatigue
Kernels
In NLP, tree kernels such as the Subset Tree Kernel (SSTK)
[8, 9] and Partial Tree Kernel (PTK) [10] have been used
effectively for related tasks such as sentence classification
[11]. Tree kernels are applied over syntactic parses such as
constituency parse or basic dependency parses [12]. These
tree kernels cannot handle edge labels directly and therefore
transform the original dependency trees to special trees
without edge labels, referred to as the Location Centered
Tree [13]. A further limitation is that other forms of parses
such as enhanced dependency parses which are arbitrary
graphs with cycles, cannot be used with tree kernels.
This limitation is overcome with graph kernels such as
the All Path Graph (APG) [14] kernel that can work with
arbitrary graph structures. However, APG kernel is pri-
marily designed to work with edge weighted graphs and
requires special transformation of the dependency graphs
output by the parser. APG kernel requires the conversion
of edge labels into special vertices and it assigns a heuris-
tically determined weight value to the edges. In contrast,
the Approximate Subgraph Matching (ASM) kernel is
designed to work directly with edge labels in the graph.
We present a detailed discussion of the APG and the ASM
graph kernels in APG kernel and ASM kernel sections.
Relation to prior work
In this section, we relate and contrast the contributions
of this paper with closely related prior work. In our prior
work, we proposed a graph kernel based on approxi-
mate subgraph matching (ASM) [15]. ASM kernel adopts
an approach to graph similarity that is derived from
a subgraph isomorphism based event extraction system
[16] developed for biomedical relation extraction [17].
In the first step, ASM seeks to match vertices between
the two input graphs. Then, the set of all pair shortest
paths from the two input graphs are compared, based on
the matched vertices. The similarity estimation is based
on the counts of edge labels along the shortest path.
In our previous work [18], we evaluated the effective-
ness of Subtree (STK) and Subset-tree kernels (SSTK)
Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 4 of 11
[8, 19] with constituency parse trees for the CID relation
extraction task.
In the current work, we introduce a modified form of
ASM kernel that incorporates edge weights in the graph.
Note that the ASM kernel as presented in prior work [15]
considered edge-unweighted graphs only. This ability to
incorporate edge weights enables the ASM kernel to posi-
tively discriminate between the shortest dependency path
between the entities and other paths in the graph, there-
fore boosting its performance further. For instance, the
CID sentence level relation extraction with ASM kernel as
reported in [15] is 58%, but improved to 63% in current
work. Secondly, we have extended the evaluation for the
CID task with other tree kernels namely the Partial Tree
Kernel (PTK) [10] and graph kernels ASM and APG [20]
with dependency parse graphs.
Contributions
A summary of the main contributions of this paper are :
 We demonstrate a high performance CID relation
extraction system, reaching an F-score of 60.3%.
This performance is achieved using an effective
method for non-sentence relation extraction, by
combining multiple sentence level parse structures
into larger units, and then applying the kernel
methods on the aggregate parse structures. Our
system compares favorably with prior art [21], where
an ensemble of machine learning methods was used
to achieve an F-score of 56% and then boosted to
61.3% using task specific post-processing rules. In
contrast, our system is a general purpose relation
extraction system, that does not employ any task or
domain specific rules.
 We present a novel graph kernel, namely the ASM
kernel with modifications to incorporate edge
weights in the graph. We provide a comparative
study of the performance of the ASM kernel with the
state of the art tree and graph kernels, over two
important biomedical relation extraction tasks, the
Chemical-Induced Disease (CID) and the Protein-
Protein Interaction (PPI) tasks. We demonstrate that
the ASM kernel is effective for biomedical relation
extraction, with comparable performance to the
state of the art APG kernel on several datasets such
as CID-sentence level relations and BioInfer in PPI.
 All software for reproducing the experiments in this
paper, including our implementation of the APG and
the ASM graph kernels in the Java based Kelp [22]
framework, is available in the public domain1.
Methods
In this section, we describe the 3 main kernel methods
that are studied in this paper, namely the Tree Kernels
[10, 19, 23], the All Path Graph (APG) Kernel and the
Approximate Subgraph Matching (ASM) Kernel [15].
Tree kernels
Tree kernels [8] using constituency parse or dependency
parse trees have been widely applied for several rela-
tion extraction tasks [13, 18, 24]. They estimate simi-
larity by counting the number of common substructures
between two trees. Owing to the recursive nature of trees,
the computation of the common subtrees can be effi-
ciently addressed using dynamic programming. Efficient
linear time algorithms for computing tree kernels are
discussed in [10].
Different variants of tree kernels can be obtained, based
on the definition of a tree fragment, namely subtree, sub-
set tree and partial tree. A subtree satisfies the constraint
that if a node is included in the subtree, then all its descen-
dents are also included in the subtree. A subset tree only
requires, that for each node included in the subset tree,
either all of its children are included or none is included
in the subtree. A partial tree is the most general tree frag-
ment, which allows for partial expansion of a node, i.e
for a given node in the partial tree fragment, any subset
of its children nodes may be included in the fragment.
Subset trees are most relevant with constituency parse
trees, where the inner nodes refer to grammatical produc-
tion rules. Partial expansion of a grammatical production
rule leads to inconsistent grammatical structures. As such,
subset trees restrict the expansion of a node to include
all of its children or none. For dependency parse trees
with no such grammatical constraints, partial trees are
more suitable to explore a wider set of possible tree frag-
ments. We experiment with subset tree kernels (SSTK)
with constituency parses and partial tree kernels (PTK)
with dependency parses and report the results on both.
We illustrate the constituency parse tree for a sample
sentence in Fig. 1.
Here, we present the formal definition of tree kernels.
Let T1 and T2 denote two trees and let F = {f1, f2, . . .}
denote the set of all possible tree fragments. Let Ii(n) be an
indicator function that evaluates to 1 when the fragment
fi is rooted at node n and 0 otherwise. The unnormalized
kernel score is given by:
K(T1,T2) =
?
n1?NT1
?
n2?NT2
(n1, n2) (1)
where NT1 and NT2 are the sets of nodes of T1 and T2
respectively and (n1, n2) = ?|F|i=1 Ii(n1)Ii(n2).
Efficient algorithms for computing tree kernels in linear
time in the average case are presented in [10].We used the
implementation of tree kernels provided in Kelp [22].
Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 5 of 11
APG kernel
The APG kernel [14] is designed to work with edge
weighted graphs. A given dependency graph G needs to
be first modified, to remove edge labels and introduce
edge weights. Let e = l(a, b) denote an edge e with label
l, from the vertex a to vertex b. For every such edge in
the original graph, we introduce a new node with label l
and two unlabeled edges (a, l) and (l, b) in the new graph.
The APG kernel recommends a edge weight of 0.3 as
a default setting for all edges. To accord greater impor-
tance to the entities in the graph, the edges along the
shortest path between the two entities are given a larger
weight of 0.9. This constitutes the subgraph derived from
the dependency graph of a sentence. Another subgraph
derived from the linear order of the tokens in the sentence
is constructed. In this subgraph, n vertices are created to
represent the n tokens in the sentence. The lemma of a
token is set as the label of the corresponding node. These
vertices are connected by n ? 1 edges, for the n tokens
from left to right. That is, edges are introduced between
token i and token i+1. These two disconnected subgraphs
together form the final edge weighed graph over which the
APG kernel operates.
Let A denote the adjacency matrix of the combined
graph. Let connectivity of a path refer to the product of
edge weights along the path. Intuitively, longer paths or
paths with lesser edge weights, have connectivity closer
to 0 and shorter paths or paths with greater edge weights
have a connectivity closer to 1. Note that the matrix Ai
represents the sum of connectivity of all paths of length
i, between any two vertices. The matrix W is defined as
the sum of the powers of A, I.e W = ??i=1 Ai. It is effi-
ciently computed asW = (I?A)?1. Therefore,W denotes
the sum of connectivity over all paths. Any contribution
to connectivity from self loops is eliminated by setting
W = W ? I. Finally, the APG kernel computes the matrix
Gm = LWLT , where L is the label allocation matrix, such
that L[ i, j]= 1 if the label li is present in the vertex vj and
0 otherwise. The resultant matrix Gm represents the sum
total of connectivity in the given graphG between any two
labels. Let Gm1 and Gm2 denote the matrices constructed as
described above, for the two input graphs G1 and G2. The
APG kernel score is then defined as :
K(G1,G2) =
|L|?
i=1
|L|?
j=1
Gm1
[
li, lj
] × Gm2
[
li, lj
]
(2)
Impact of linear subgraph
Wenoticed substantially lower performance with the APG
kernel when the labels marking the relative position of
the tokens with respect to the entities, i.e. labels such as
before, middle and after in the linear subgraph are
left out. For example, the F-score for AIMed in PPI task
drops by 8 points, from 42 to 34%, when these labels are
left out. This highlights the importance of the information
contained in the linear order of the sentence, in addition
to the dependency parse graph.
ASM kernel
The ASM kernel [15] is based on the principles of graph
isomorphism. Given two graphs G1 = (V1,E1) and G2 =
(V2,E2), graph isomorphism seeks a bijective mapping
of nodes M : V1 ? V2 such that, for every edge e
between two vertices vi, vj ? G1, there exists an edge
between the matched nodes M(vi),M(vj) ? G2 and vice
versa. The ASM kernel though, seeks an approximate
measure of graph isomorphism between the two graphs,
that is described below. Let L be the vocabulary of node
labels. In the first step, ASM seeks a bijective mapping
M1 : L ? V1, between the vocabulary and the nodes,
such that M1(li) = vj, vj ? V1 when the vertex vj has the
node label li. To enable this, all nodes in the graph are
assumed to have distinct labels. For every missing label li
in the vocabulary, a special disconnected (dummy) node
vj with the label li is introduced. Next, ASM does not seek
matching edges between matching node pairs. Instead, it
evaluates the similarity of the shortest path between them.
Consider two labels li, lj. Let x, y be the vertices in the
first graph with these labels respectively. I.e M1 (li) =
x,M1
(
lj
) = yandx, y ? V1. Let P1x,y be the shortest path
between the vertices x and y in the graph G1. Similarly, let
x?, y? denote the matching vertices in the second graph. I.e
M2 (li) = x?,M2
(
lj
) = y?andx?, y? ? V2. Let P2x?,y? denote
the shortest path between the vertices x? and y? in the
graph G2. The feature map ? that maps a shortest path
P into a feature vector is described following the ASM
kernel definition below.
The ASM kernel score is computed as:
K(G1,G2) =
|L|?
i=1
|L|?
j=1
?
(
P1x,y
)
· ?
(
P2x?,y?
)
s.tM1(li) = x,M1(lj) = y and x, y ? V1
andM2(li) = x?,M2(lj) = y? and x?, y? ? V2
(3)
Feature space
The feature space of ASM kernel is revealed by examin-
ing the feature map ? that is evaluated for each short-
est path P. ASM kernel explores path similarity along 3
aspects, namely structural, directionality and edge labels,
as described below. We use the notationWe to denote the
weight of an edge e. An indicator function Ile is used to
indicate if an an edge e has an edge label l. Similar to the
APG graph, we set the edge weights to 0.9 for edges on the
shortest dependency path between two entities and 0.3 for
the others.
Structural similarity is estimated by comparing path
lengths. Note that similar graphs or approximately
Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 6 of 11
isomorphic graphs are expected to have similar path
lengths for matching shortest paths. Therefore, a single
feature ?distance(P) = ?e?P We , is computed to incorpo-
rate structural similarity, where We denotes the weight of
an edge e in the path P.
Directional similarity is computed like structural sim-
ilarity, but unlike structural similarity, edge directions
are considered. ASM kernel computes two features,
?forward edges(P) = ?f?P Wf and ?backward edges(P) =?
b?P Wb, where f and b denote a forward facing and
backward facing edge respectively, in the path P.
Edge directions may themselves be regarded as special
edge labels of type forward or backward. Edge label
similarity generalizes the above notion to an arbitrary
vocabulary of edge labels E. In particular, E is the set of
dependency types or edge labels generated by the syntac-
tic parser. For each such edge label l ? E, ASM kernel
computes the feature ?l(P) =
?
e?P W
Ilee , where Ile denotes
an indicator function that takes a value 1 when the edge e
has a label l and 0 otherwise.
The full feature map ?(P) is the concatenation of the
above described features for structural, directionality and
edge label similarity. We illustrate this feature map for a
sample enhanced dependency graph illustrated in Fig. 1.
For the label pair seizures, fatigue, the shortest path P is
through the single intermediate vertex caused. For this
path, the non-zero features are : ?(P) = {?distance =
(0.9)2,?forward edge = 0.9,?backward edge = 0.9,?nsubj =
0.9,?nmod:by = 0.9, }.
Implementation details
We implemented the APG and ASM kernel in the Java
based Kelp framework [22]. The Kelp framework pro-
vides several tree kernels and an SVM classifier that we
used for our experiments. We did not perform tuning
for the regularization parameter for SVM, and used the
default settings (C-Value= 1) in Kelp. Dependency parses
were generated using Stanford CoreNLP [7] for the CDR
dataset. For the Protein-Protein-Interaction task, we used
the pre-converted corpora available from [14]. The cor-
pus contains the dependency parse graphs derived from
Charniak-Lease Parser, which was used as input for our
graph kernels. All software implemented by us for repro-
ducing the experiments in this paper, including the graph
kernels APG and ASM implementations are available in a
public repository.
Results
We evaluate the performance of the ASM and APG ker-
nels. We first describe our experimental setup and then
discuss the results of our evaluation of the different ker-
nels for relation extraction.
CID relation extraction
This experiment follows the Chemical-Induced Disease
Relation Extraction subtask of [3]. The CDR corpus made
available by [3] contains three datasets, namely the train-
ing set, development set and the test set. Each dataset
contains 500 PubMed documents (title and abstract only)
with gold standard entity annotations. More details about
this corpus is available at [3]. A sample Pubmed document
is illustrated in Table 1.
Classifier setup
We build separate relation extraction subsystems for sen-
tence level relations and non-sentence level relations.
That is, for any relation (C,D) in a document (where C,D
denotes a chemical and disease identifier respectively), we
search for any single sentence that bears mentions to both
the relevant entities C,D. If such a sentence is found, it is
added as an example into sentence level relation extrac-
tion subsystem. When no such sentence can be found,
such a (C,D) pair is regarded as a non-sentence relation.
For these relations, we retrieve all sentences bearing a
mention to either C or D. All such sentences are paired to
form examples for the relation (C,D). That is, an example
for a non-sentence relation (C,D) is a pair of sentences,
one containing the mention of entity C and the second
containing the mention of entity D.
Entity focus
Note that a single sentence can carry multiple entity
pair mentions, with different relations between then. For
example, the sentence The hypotensive effect of alpha
methyldopa was reversed by naloxone, carries two entity
pair mentions, namely alpha methyldopa, hypotensive
and naloxone, hypotensive. The first entity pair is related
(alpha methyldopa causes hypotension) whereas the sec-
ond entity pair is unrelated. Therefore, the above sen-
tence should be suitably processed to extract two different
training or testing examples for classification, that serve
two different entity-pairs, namely alpha methyldopa,
hypotensive and naloxone, hypotensive. To distinguish
between the two cases, we attach special vertices with the
labels Entity1 and Entity2, that are connected to the
entity-pair in focus, in the given graph.
Examples for the classifier
For sentence level relations, we transform each sentence
into tree or graph by retrieving its constituency parse
tree or dependency parse graph. For non-sentence rela-
tion examples, we first retrieve the underlying pair of
sentences representing the example and transform each
sentence to a tree or graph. The resultant pair of trees
or graphs are then connected at the root node, with a
special edge labelled Sentence Boundary, to result in a
single tree or a graph, that can then be input to a tree or
Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 7 of 11
graph kernel based classifier. The relations retrieved from
the two subsystems for sentence and non-sentence level
relations are merged (union) together, to form the final
set of retrieved Chemical-Disease relations for the whole
PubMed document.
Results for the CID task
The CID Relation extraction performance of the differ-
ent kernels is characterized by measuring the Precision,
Recall and F1measures. These are presented in Table 2 for
the CDR dataset. All the measurements listed in Table 2
are based on relation extraction with gold standard entity
annotation. Further, we have provided the performance
measurements for sentence-level relations only and non-
sentence level relations only, which characterizes the per-
formance of our two relation extraction subsystems. The
column All Relations represents the performance of the
final relation extraction system over the full CDR test data,
that corresponds to the subtask of BioCreative-V [3].
Comparisonwith prior art
A suitable comparison from prior art is the CID relation
extraction system by [21]. Similar to our system, they use
gold standard entity annotations and do not employ any
external knowledge source or knowledge base. This prior
work by [21] consists of a hybrid system or an ensemble
of classifiers based on feature-based model, a tree kernel-
based model and a neural network model. Their system
is designed for sentence level relations only and ignores
non-sentence relations. The F-score of this hybrid system
is reported to be 56%. To further boost the performance,
the authors in [21], propose the use of custom or CID
task specific post processing rules, such as associating the
Chemical mentioned in the title with the Diseases men-
tioned in the abstract. These heuristics were found to help
boost the performance of their system to 61.3%.
In our work, we do not employ any custom heuris-
tics and instead rely on machine learning techniques
only. Interestingly, when we removed our subsystem for
non-sentence level relation extraction, we observed that
our final CID relation extraction performance, drops to
55.7% and 54.0% respectively, for the APG and ASM
kernel based systems. In other words, our final perfor-
mance of 60.3%, is due to the substantial contribution
(+5% points in F-score), from the non-sentence relation
extraction.
To summarize, our main findings from the CID relation
extraction task are:
 The APG and ASM graph kernels substantially
outperform the tree kernels for relation extraction.
 APG kernel offers the best performance, with an
F-score of 65% for sentence level relation extraction,
45% for non-sentence level relation extraction and
60% for the full CID test relations.
 ASM kernel is effective for relation extraction and
its performance approaches that of the state of the
art APG kernel, with an F-score of 63% for sentence
level relation extraction, 37% for non-sentence
relations and 57% for the full CID test relations.
 Our system achieves a close to state of the art
performance for CID relation extraction (60% vs 61%),
without employing heuristics or task specific rules.
 Effective non-sentence level relation extraction
provides a substantial boost (+5 points) to the final
F-score for our CID relation extraction task.
Protein-protein interaction extraction
The Protein-Protein Interaction (PPI) extraction task,
involves extracting Protein-pairs that interact with each
other, from Biomedical Literature. We used the PPI cor-
pora from [25], that consists of 5 datasets, namely AIMed,
BioInfer, HPRD50, IEPA and LLL. These are collections
of sentences sourced from biomedical publications about
protein interactions. The goal of the PPI task is to ana-
lyze these sentences, such as Isolation of human delta-
catenin and its binding specificity with presenilin 1 and
extract interacting Protein-pairs such as (delta-catenin,
presenilin 1). We used the derived version of the PPI
corpora [25], that contains sentences together with their
Table 2 Performance measurements for chemical induced disease relation extraction
Method Sent-Rel. only Non-Sent-Rel. only All relations
P R F P R F P R F
SSTK with CP-Tree 43.1 73.7 54.4 36.9 14.2 20.5 42.5 56.0 48.3
PTK with LCT 42.2 75.3 54.1 30.5 40.1 34.6 39.5 64.8 49.0
APG with Dep. Graphs 54.7 80.6 65.1 47.8 43.8 45.7 53.2 69.7 60.3
ASM with Dep. Graphs 51.6 80.8 63.0 38.8 36.0 37.3 49.0 67.4 56.8
Hybrid (Prior art [21]) - - - - - - 64.9 49.2 56.0
Hybrid+Rules (Prior art [21]) - - - - - - 55.6 68.4 61.3
(Key: P,R,F denotes Precision, Recall and F1 score respectively. Sent-Rel. and Non-Sent-Rel. denotes sentence level relations and Non-Sentence level relations respectively.
CP-Tree and LCT denote constituency parse tree and location centered tree. Dep. Graph denotes dependency graph. The best performance is highlighted in italicized font)
Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 8 of 11
Charniak-Lease Parser based tokenization, part of speech
tagging and dependency parse in a standardized XML
format. The corpus contains the list of protein-pairs in
each sentence with a label True for interacting pairs
and False otherwise. We used the dependency parses
in the corpus to produce graphs that serve as inputs
for our graph kernel with SVM based classification. We
experiment with graph kernels, specifically the APG and
ASM kernels. From prior work [26], we know that APG
kernel substantially outperforms tree kernels for the PPI
task. Therefore, our main objective in this experiment is
to characterize the performance of the ASM and APG
(our implementation) kernels for the PPI task, and con-
trast these to the state of the art APG kernel based PPI
performance.
Results for the PPI task
We evaluate our implementation of the APG and ASM
kernels in the cross-learning setting, that involves group-
ing 4 out of the 5 datasets into one training unit and
testing on the one remaining dataset. These results are
presented in Table 3.We have also listed the state of the art
performance measurements for PPI with the APG kernel,
as reported in prior art (see Table 3 of [26]).
Comparisonwith prior art
The PPI task, is characterized by the measures Precision,
Recall and F-score and the AUC or the Area Under the
ROC Curve. As indicated in prior art [26], AUC is invari-
ant to the class distribution in the dataset and is therefore
regarded as an important measurement to characterize
the PPI extraction performance.
To summarize, our findings from the PPI experiment
are:
 We expect the AUC measurements for our APG
implementation to match that of the APG
implementation in prior art (Table 3 of [14]). The
AUC measurements are nearly equal for the larger
datasets, AIMed and BioInfer, but differ noticeably
for the smaller datasets, HPRD50 and LLL. A likely
cause for this variation is the differing classifier
frameworks (SVMs vs Regularized Least Squares)
used in these two experiments.
 Our APG implementation varies substantially with
the prior art, in Precision and Recall and moderately
in F-score. These measurements are known to be
sensitive to parameter setting of the classifier and
less dependent on the kernel characteristics itself.
However, due to computational costs, we have not
performed any parameter tuning in this work.
 ASM kernel outperforms APG for BioInfer (AUC of
74.1 vs 69.6), which is a large dataset. However, APG
kernel outperforms ASM by a substantial difference
for all the remaining datasets, namely AIMed,
HPRD50, IEPA and LLL. We conclude that ASM is
outperformed by APG for the full PPI task.
Statistical significance testing
The CID and PPI relation extraction tasks, considered
different measurements, such as F-score and AUC, that
are considered relevant for relation extraction task. In
terms of classification accuracy, a better comparison of
the two kernels can be performed with the McNemars
test [27]. McNemars test estimates the statistical signifi-
cance for the null hypothesis that the two classifiers are
equally accurate. The P-values for the null hypothesis, cor-
responding to different classification tasks, are listed in
Table 4. The datasets for which the null hypothesis can be
rejected (P-value < 0.01) are highlighted. This test con-
firms that the APG and ASM kernels are significantly dif-
ferent in classification accuracy, over several large datasets
such as AIMed, BioInfer and CID non-sentence relations.
Discussion
In this section, we present a detailed comparison of the
two graph kernels, namely ASM and APG kernel. We
focus our study on the graph kernels only as we saw
above for CID relation extraction, that they substantially
Table 3 Performance measurements for protein-protein interaction extraction
Method
AIMed BioInfer HPRD50
P R F A P R F A P R F A
SOA 30.5 77.5 43.8 77.6 58.1 29.4 39.1 69.6 64.2 76.1 69.7 84.0
APG 28.6 81.6 42.3 76.8 68.6 28.6 40.4 69.7 62.3 69.9 65.9 79.7
ASM 26.3 78.0 39.3 72.9 67.2 22.6 33.8 74.1 66.0 58.3 61.9 76.2
IEPA LLL
P R F A P R F A
SOA. 78.5 48.1 59.6 82.4 86.4 62.2 72.3 86.4
APG 78.2 41.8 54.5 80.2 84.7 57.3 68.3 83.4
ASM 82.8 17.3 28.6 77.7 79.3 28.0 41.4 75.3
(Key: P,R, F and A denotes Precision, Recall, F score and area under curve respectively. SOA denotes State of the art performance with APG as reported in [26]). The best
performance is highlighted in italicized font)
Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 9 of 11
Table 4 Statistical significance (McNemars) tests for the ASM and APG classifiers, for the null-hypothesis being that the two classifiers
are equally accurate and a significance threshold of 0.01
Dataset
Number of examples Accuracy
P-value
Training Testing APG ASM
AIMed 11,246 5,834 58.6 53.1 3.8e-7
BioInfer 7,414 9,666 77.8 76.8 0.0011
HPRD50 16,647 433 70.9 68.1 0.999
IEPA 16,263 817 73.6 66.5 3.9e-6
LLL 16,750 330 75.4 65.1 2.2e-6
CID: Sentence level relations. 9,913 5,099 72.2 71.2 0.0969
CID: Non Sentence level relations 21,656 11,562 84.9 84.1 0.0002
P-values less than the threshold are shown in italicized font
outperform the tree kernels. Interestingly, both kernels
follow the approach of comparing node pairs between
two graphs to estimate graph similarity. However, the
key difference between the two kernels is their treat-
ment of edge labels in the graph. In APG, edge labels
in the graph are transformed into intervening nodes.
Therefore, the node label vocabulary in APG is a het-
erogeneous set which is the union of the vocabulary of
word lemmas, V in the corpus and the vocabulary of
edge labels D defined by the dependency parser [7]. That
is, the set of node labels considered by APG kernels is
L = V ? D. The features explored by the APG ker-
nel can be indexed by pairs of node labels, of the form
(V ? D) × (V ? D).
In ASM kernel, edge labels (dependency types D) and
node labels (word lemmas V ) are treated separately.
ASM associates a node label pair with a rich feature
vector, where each feature is a function of the edge labels
along the shortest path between the nodes. Therefore,
its feature space can be indexed by triplets of the form
(V × V × D). This is an important difference from
the APG kernel, which associates a single scalar (graph
connectivity) value with a node label pair. The higher
feature space dimensionality for the ASM kernel is a
likely cause for its lower performance than the APG
kernel. The other main difference between the two ker-
nels is that the APG kernel considers all possible paths
between a pair of nodes, whereas ASM kernel considers
only the shortest path. This is another likely factor, that
is disadvantageous to ASM kernel in comparison to
APG kernel.
Error analysis
We manually examined a few error samples to identify
the likely causes of errors by APG and ASM kernel in
CID and PPI relation extraction tasks. We noticed that
an important characteristic of the CDR dataset, which is
the presence of many entity pairs in a single sentence, to
be a likely cause for the high false positive rate. Consider
the example: Susceptibility to seizures produced by
pilocarpine in rats after microinjection of isoniazid or
gamma-vinyl-GABA into the substantia nigra. Here,
pilocarpine and seizures are positively related, which
is correctly recognized by our classifiers. However, our
classifiers also associate the disease seizures with the
chemicals isoniazid and gamma-vinyl-GABA. The graph
examples corresponding to different entity pairs arising
out of the above sentence, share many common subgraph
and are likely to be close enough in the feature space of
the classifiers. We hypothesize that a sentence simplifi-
cation step, that trims the sentences into shorter phrases
specific to entity-pairs, or a specific treatment of coordi-
nation structure in the sentences [28], is likely to reduce
the error rates.
Another source of errors is in preprocessing. Consider
the following sentence from the PPI corpora: We took
advantage of previously collected data during a random-
ized double-blind, placebo-controlled clinical trial to con-
duct a secondary analysis of the RBP/TTR ratio and its
relationship to infection andVA status.. In cases like these,
the tokenization offered as part of the PPI corpora recog-
nizes the string RBP/TTR as a single token. This error
in preprocessing causes the corresponding dependency
graph to have a single node with the label RBP/TTR,
instead of two different nodes , corresponding to the pro-
teins RBP and TTR. Improving preprocessing accuracy
is likely to improve the relation extraction performance
for PPI.
Future work
Enriching edge labels
The main strength of ASM kernel is that it handles
edge labels distinctly from node labels in the graph. This
strength can be exploited by designing informative fea-
tures for edges or paths, that are representative of the
corresponding sub-phrase in the sentence, for example,
phrase level measurements of sentiment polarity, nega-
tion and hedging [29].
Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 10 of 11
Custom edge similarity
ASM computes the similarity of shortest paths, based on
their edge label composition. As the dependency edge
labels have well defined semantics, designing custom sim-
ilarity measures between these edge labels is likely to
improve performance. These edge labels are grouped in
a well defined hierarchical fashion, which the similarity
function can exploit. For example, the edge labels vmod
(verb modifier) and advmod (adverbial modifier) are
more closely related to each other than to the edge label
nsubj (nominal subject).
Semantic matching
ASM relies on comparing shortest paths between two
input graphs, whose start and end nodes have identical
labels. Currently, node labels are set to be word lemmas
instead of tokens, to improve generalization and address
minor variations such as cured and curing. In future,
we aim to explore setting node labels to word classes that
group words with similar meanings together. For example,
node labels may be set to cluster ids, post word cluster-
ing. Semantic matching of lemmas using distributional
similarity [30], may allow matching different lemmas with
similar meanings (For example, lemmas such as cure and
improve). Similar approaches to tree kernels [31] has
been shown to improve performance.
Conclusion
We demonstrated a method for extracting relations that
are expressed in multiple sentences, to achieve a high
performance Chemical-Induced Disease relation extrac-
tion, without using external knowledge sources or task
specific heuristics. We studied the performance of state
of the art tree kernels and graph kernels for two impor-
tant biomedical relation extraction tasks, namely the
Chemical-Induced Disease (CID) relation extraction and
Protein-Protein-Interaction (PPI) task. We showed that
the Approximate Subgraph Matching (ASM) kernel is
effective and comparable to the state of the art All Path
Graph (APG) kernel, for CID sentence level relation
extraction and PPI extraction from BioInfer dataset. The
difference in performance between the two kernels is
not significant for CID sentence level relation extraction.
However, for the full CID relation extraction and most
other datasets in PPI, ASM is substantially outperformed
by the APG kernel.
Endnote
1 https://bitbucket.org/readbiomed/asm-kernel
Abbreviations
ASM Kernel: Approximate subgraph matching kernel; APG Kernel: All path
graph kernel; CID: Chemical-Induced-Disease; PTK: Partial tree kernel; SSTK:
Subset tree kernel; SVM: Support vector machine
Acknowledgements
We thank the anonymous reviewers for their valuable suggestions.
Funding
This work is supported by the ARC Discovery Project DP15010155. Nagesh C.
Panyam is a Graduate research student at the University of Melbourne,
Melbourne, Australia and is supported by the Australian Government Research
Training Program Scholarship. This research was supported by use of the
Nectar Research Cloud, a collaborative Australian research platform supported
by the National Collaborative Research Infrastructure Strategy (NCRIS).
Availability of data andmaterials
The CID corpus used in our experiments is made available from the
BioCreative-V [3] at the following url: http://www.biocreative.org/tasks/
biocreative-v/track-3-cdr. Stanford dependency parser [7] can be installed
from http://stanfordnlp.github.io/CoreNLP/. The Java based KeLP [22]
framework was used for custom graph kernel implementation and can be
downloaded from https://github.com/SAG-KeLP. All software developed for
this paper, including our implementation of the APG and the ASM graph
kernels in the Java based Kelp [22] framework, can be accessed from https://
bitbucket.org/readbiomed/asm-kernel.
Authors contributions
All authors contributed to the analysis and design of experiments. NCP
implemented the experiment scripts and kernel implementations and wrote
the manuscripts. All authors read and approved the final manuscript.
Ethics approval and consent to participate
Not Applicable.
Consent for publication
Not Applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Received: 15 February 2017 Accepted: 1 December 2017
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 
DOI 10.1186/s13326-017-0172-7
REVIEW Open Access
MIRO: guidelines for minimum
information for the reporting of an ontology
Nicolas Matentzoglu1*, James Malone2, Chris Mungall3 and Robert Stevens1
Abstract
Background: Creation and use of ontologies has become a mainstream activity in many disciplines, in particular, the
biomedical domain. Ontology developers often disseminate information about these ontologies in peer-reviewed
ontology description reports. There appears to be, however, a high degree of variability in the content of these
reports. Often, important details are omitted such that it is difficult to gain a sufficient understanding of the ontology,
its content and method of creation.
Results: We propose theMinimum Information for Reporting an Ontology (MIRO) guidelines as a means to facilitate a
higher degree of completeness and consistency between ontology documentation, including published papers, and
ultimately a higher standard of report quality. A draft of the MIRO guidelines was circulated for public comment in the
form of a questionnaire, and we subsequently collected 110 responses from ontology authors, developers, users and
reviewers. We report on the feedback of this consultation, including comments on each guideline, and present our
analysis on the relative importance of each MIRO information item. These results were used to update the MIRO
guidelines, mainly by providing more detailed operational definitions of the individual items and assigning degrees of
importance. Based on our revised version of MIRO, we conducted a review of 15 recently published ontology
description reports from three important journals in the Semantic Web and Biomedical domain and analysed them for
compliance with the MIRO guidelines. We found that only 41.38% of the information items were covered by the
majority of the papers (and deemed important by the survey respondents) and a large number of important items are
not covered at all, like those related to testing and versioning policies.
Conclusions: We believe that the community-reviewed MIRO guidelines can contribute to improving significantly
the quality of ontology description reports and other documentation, in particular by increasing consistent reporting
of important ontology features that are otherwise often neglected.
Keywords: Ontologies, Reporting guidelines, Minimum information, Ontology reporting
Background
The need for a common understanding of the entities in
a field of interest has led to the widespread adoption of
ontologies as a means of representing knowledge [1]. This
is particularly true in biology, medicine and healthcare
[1, 2]. We also see the use of semantic technologies,
including ontologies, increasing outside research in areas
such as business and commerce; see, for example, the list
of PoolParty customers [3]. Ontologies attempt to rep-
resent our knowledge such that inclusion of an entity in
*Correspondence: nicolas.matentzoglu@manchester.ac.uk
1School of Computer Science, University of Manchester, Oxford Road,
Manchester, UK
Full list of author information is available at the end of the article
a category can be recognised by both humans and com-
puters, for example by using automated reasoners. The
definitions and descriptions of every entity in a category
may be done in the form of natural language or logical
axioms that describe the relationship of one category of
objects to objects in another category [4]. Groups of data
annotators use ontologies to describe entities; commit-
ting to use that ontology seeks to facilitate a common
understanding of entities across data sources [1].
Several journals regularly publish ontology description
reports (ODR), for example, the Semantic Web Journal
(SWJ), the Journal of Web Semantics (JWS) and the Jour-
nal of Biomedical Semantics (JBMS). An ODR, in the
sense of the current focus of our work, is a published,
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 2 of 13
peer-reviewed report regarding the development of a
single ontology represented in a formal language such as
the Web Ontology Language (OWL) [5]. Descriptions of
an ontology need not, however, be restricted to tradi-
tional papers. The original motivation for developing the
MIRO guidelines comes from the perceived inadequacy
of ODR in the form of published papers, but the use of
the MIRO guidelines need not be restricted to traditional
ODR. The traditional paper is currently the main route
for reporting about an ontology. This should not be the
case and, just as research publishing is diversifying in its
form, such as through the growth of preprint archives
and moving beyond facsimiles of printed paper publish-
ing [6], so should the documentation of an ontology. An
ontology itself can and should be the vehicle that dis-
seminates information about its development and status.
Indeed, it is plausible that at least some of the report-
ing could be automated and incorporated in the form
of annotations of the ontology. An ontology could, for
instance, carry its own descriptions as part of its anno-
tations, and other types of documentation should also
carry descriptions of the ontology. This is already the case
for datasets that choose to use the W3Cs Vocabulary of
Interlinked Datasets (VoID) ) [7] which describes meta-
data about RDF datasets and can be published alongside
those datasets to act as a bridge between the publishers
and users of data.
While ODRs certainly vary in scope, there is also a
high degree of commonality with respect to the pro-
cess of ontology engineering. Some commonly recur-
ring aspects of the engineering process are, for example,
the necessity for some form of knowledge elicitation,
formalisation and evaluation. Moreover, there are some
commonly shared attributes of the ontology itself: Every
ontology has a size, a degree of coverage and a name.
As the goal of ontology authors is usually to publish the
ontology for community use, another important branch
of information items relates to the publishing process,
such as licenses, details about the versioning and location
on the web.
Unfortunately, and perhaps surprisingly, given that
ontologies are a shared conceptualisation about a domain,
there is no standard or common understanding about
what should be reported in a good, meaningful ODR or
other documentation. As a consequence, the contents of
published articles appear highly inconsistent. This can be
a problem for a variety of reasons. Reviewers for journals
and conferences faced with large numbers of ODRs have
no reliable guidelines to help them assess the quality of the
reports making consistent reviewing between reviewers
harder.
There are a large number of (domain-specific) agree-
ments for example when it comes to novel algorithmic
contributions (the necessity of reporting a performance
benchmark, etc.) but it is still largely unclear how to dis-
tinguish reliably between Here ismy ontology ODRs and
substantial contributions that constitute an advancement,
such as in modelling, usage or scope, to their respec-
tive community. Another problem concerns the ability of
a potential consumer in understanding what an ontol-
ogy is intended to capture. Since traditional ODRs often
serve as the main documentation of an ontology, they are
often important in building trust among potential users
and outlining the intended applicability of an ontology, as
merely looking at the ontology may be misleading for a
variety of reasons (e.g. intended use, degree of complete-
ness, etc.).
In ontology building, reproducibility is probably a some-
what unrealistic goal; given the same motivation and
community, it is unlikely the exact same ontology be pro-
duced as a result. Knowing themotivation, the community
of interest, the requirements gathered for the ontology,
whence the knowledge came to put in the ontology, the
axiom patterns used, testing, evaluation, and so on would,
however, appear a priori to be reasonable features to know
about an ontologys development, along with aspects such
as numbers of classes and so on.
Languages such as OWL have annotation properties
that support some aspects of ontology description. Edi-
tors such as Protégé [8] enable vocabularies such as
Dublin Core [9] to be imported so that the ontology
and its entities may have dates, creators, descriptions and
so on supplied as part of the ontology. Such metadata
are, however, insufficient to report on an ontology. Out-
side research articles describing ontologies, prior work
in describing ontologies has been in the area of ontol-
ogy libraries (registries, repositories and so on) [1012]
wheremetadata is primarily used for discovery and associ-
ated characterisation. TheOntologyMetadata Vocabulary
(OMV) [10] and the Metadata for Ontology Description
and Publication (MOD) [11] are both ontologies that cap-
ture aspects of reporting about an ontology. Both seek to
promote ontology discovery and re-use. MOD incorpo-
rates many aspects about how an ontology was developed
within its metadata, such as method, principal classes,
and so on and used an open-ended questionnaire to
gather material. MOD is more extensive than the OMV,
though its primary purpose is still discovery motivated by
promoting re-use.
The Minimum Information for the Reporting of an
Ontology (MIRO) guidelines aim to guide that which
is reported in narratives reporting an ontology such as
ODR, as well as other documentation. As such, MIRO
is likely to be more extensive than these vocabularies for
ontology metadata, despite covering some of the same
topics.
We take a broader perspective than discovery, looking
at what needs to be reported about an ontology such that
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 3 of 13
its development and status can be understood. Just as
the MOD provides more than the OMV, we expect that
the MIRO extends the content of the MOD. Our objec-
tive is not to create a new ontology, but to establish a set
of guidelines for that which should be reported about an
ontology. These guidelines may be captured in an ontol-
ogy itself or other documentation, but the main purpose
of MIRO is for use to guide authors and reviewers of
papers about ontologies.
The main contributions of this paper are as follows:
 The Minimum Information for the Reporting of an
Ontology (MIRO) guidelines, which have the aim of
improving the quality and consistency of the
information content of ontology descriptions.
 A survey with more than 100 respondents to evaluate
and refine the guidelines. We present the results of
this survey and use participant ratings to prioritise
information items by importance.
 A systematic review of the compliance of recent,
high-quality ODR published as papers with the
MIRO guidelines.
Given the prominence of ontologies in the bio-health
community, the MIRO guidelines are of particular inter-
est; they are, however, of general applicability to any
ontology description.
Materials andmethods
MIRO guideline development
The first version of the MIRO was written by a team
of three ontology experts (authors of this paper, except
Matentzoglu). All three experts have extensive knowl-
edge of building ontologies, reviewing and authoring
ODRs, managing ontology collections such as the OBO
foundry [13] and organising international ontology-
related conferences and workshops such as Semantic
Web Applications and tools for the Life Sciences, the
International Conference on Biomedical Ontology and
the Bio-Ontologies SIG at the Intelligent Systems for
Molecular Biology conference. One of the motivations
for producing the guidelines stemmed from the difficulty
of setting reviewing standards when acting as conference
chairs.
Apart from extensive expert knowledge in the ontology
domain, the original MIRO information items also took
input from reviews of existing ontology metadata vocabu-
laries, the OBO principles [14], and fruitful discussions at
events, such as the yearly Ontology Summit [15] and the
UK Ontology Network [16].
After gathering an informal list of best practices on
reporting, the three experts reviewed each of them
internally and organised them into cognate sections,
which resulted in the first MIRO draft.
Survey on the importance of ontology reporting
information items
The first draft of the MIRO guidelines was offered to
a broad community of ontology paper authors, review-
ers, developers and users via a typeform survey [17].
Broadly, the survey had seven sections: (1) basic ontol-
ogy facts such as the name and URL, (2) motivation
for why the ontology was being developed, (3) scope,
requirement and community for which the ontology was
being developed, (4) knowledge elicitation around how
the knowledge included was extracted, (5) ontology con-
tent describing technical facts of the ontology such as
number of classes, properties, (6) managing change on
how the ontology is maintained, and, (7) quality assurance
around testing and evaluation.
In this survey, we presented (1) the information item,
such as Ontology name or Ontology evaluation, (2) a
Likert scale between 1 (unimportant) and 5 (very impor-
tant) to rate the subjective relative importance of each
item to the respondent and (3) a comment field. Impor-
tantly, we did not provide a detailed operationalisation
of the MIRO items, i.e. details on how we envisioned
a particular information item to be realised in a given
ODR or other documentation. We did this for two rea-
sons: (1) We wanted to provide an opportunity for the
community to present their position on how a particular
item should be realised without too much upfront bias;
(2) Some of the items, such as Testing and Evaluation
can mean significantly different things across cases. As
we wanted to avoid the impression that we only spec-
ified certain cases, we did not include descriptions of
how a guideline would be operationalised. In addition
to comment fields on each item, we asked, for every
section of MIRO, which items or aspects of items were
of particular importance to the respondent. Our goal
was to create a more detailed characterisation of impor-
tant items and to use this information to further specify
the operationalisation of information items in the final
MIRO guidelines, in particular, to emphasise important
details in the item description. Towards the end of the
survey, we asked the respondents for the single most
important criterion when deciding on whether to use an
ontology.
Participants viewed ontologies and ontology papers
from a variety of perspectives for which different infor-
mation items may be important to different degrees. To
account for these differences in our analysis, we asked
participants to indicate their main roles (multiple roles
were permitted), i.e. ontology developer, ontology user,
reader of papers on ontologies and reviewer of papers on
ontologies. We furthermore collected information on the
respondents professional background, i.e. whether they
were student, academic employee, public sector/not-for-
profit employee, private sector employee or Other.
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 4 of 13
The questionnaire was sent to email lists read by a wide
variety of actors involved with ontologies that would have
an interest in how ontologies are reported. Email lists were
not limited to only those used by biological and medi-
cal ontology developers and users, but to a range of lists
reaching a range of domains and technologies. The lists
used were:
 The Protég´e User email list
protege-user@mailman.stanford.edu.
 The Open Biomedical Ontologies Discussion list
obo-discuss@lists.sourceforge.net.
 The Health Care and Life Sciences Semantic Web
Discussion list public-semweb-lifesci@w3.org.
 The Semantic Web email list semantic-web@w3.org.
 The Web Ontology Language email list
public-owled@w3.org.
 The UK Ontology Network email list
ontology-uk@googlegroups.com.
 The European Ontology Network email list
euon@googlegroups.com.
 The European Bioinformatics Institute ontology
mailing list ontology@ebi.ac.uk.
After the survey had closed, we analysed the results in
the following way:
 We calculated descriptive statistics for the
importance ratings given to each item.
 We coded the comments for each item to determine
information items of particular interest to
respondents and elicit information items potentially
beyond the current coverage of MIRO. The
comments were coded in a bottom-up fashion, by
first collecting the information items mentioned in
each comment in a list, then reconciling potentially
redundant terminology and finally grouping the
comments into categories.
 We analysed the responses to our question for the
single most important criterion in the same way as
the comments.
Systematic review of MIRO compliance
To determine to what extent current high-quality ODRs
would have adhered to the MIRO guidelines, we per-
formed a systematic review of MIRO compliance [18].
We selected three important journals that regularly pub-
lish high-quality ODR AS : the Semantic Web Journal
(SWJ), the Journal of Web Semantics (JWS) and the Jour-
nal of Biomedical Semantics (JBMS). We decided to focus
on recently published work and restricted our search to
papers published between March 2015 and May 2016.
This time frame was chosen for convenience, to ensure
that our sample contained between 15 and 20 relevant
ODRs.
First, we retrieved all research papers (168) published
by the three journals within the time frame and had three
independent researchers filter out obviously unrelated
titles according to the following inclusion criteria: (1)
ontology description paper and (2) primarily about an
ontology and its development (where 1 simply describes
an ontology and 2 extends this scope with a description of
its development and use) and according to the following
exclusion criteria: (i) primarily system USING ontology,
(ii) review about ontologies, (iii) primarily a use case
description (study on how the ontology generated value),
(iv) an update or extension of an existing ontology. At this
stage, we considered all those papers that were thought
potentially relevant by at least one reviewer (36). In the
second phase, three independent researchers reviewed
the abstracts of the remaining papers, after which 19
papers remained. In the last phase, two independent
researchers reviewed the remaining papers in depth,
which resulted in the exclusion of another 4 papers. The
final set of 15 papers was coded according to the 35
information items of MIRO. All codes except for ontology
name and ontology owner, which were coded on a three-
point Likert scale (absent, mentioned, explicit)here,
explicit means that the description of an information
item in the paper was present in the narrative with
explicit indicators such as the motivation for developing
this ontology was . . . , were coded simply with absent
and present. Note that many information items such
as coverage or need can be addressed in a variety
of ways and to varying levels of detail. The goal of this
review was not to determine the quality of the papers,
which would require a coding granularity covering these
aspects, but merely to see whether certain items are
covered at all.
Ontology development reporting guidelines
In the following, we call MIRO the document that
describes the guidelines, information item a particular
item in the guidelines such as Ontology name or Ontol-
ogy coverage and section a block of items that belong to
a single cognate category such as Quality Assurance or
Motivation. An information item consists of a (1) label,
such as Ontology name, (2) a description with a defini-
tion and details on the operationalisation, (3) a level of
importance using the RFC 2119 keywords often used by
the W3C [19] MUST, SHOULD and OPTIONAL and (4)
an example or a reference to an example.
The guidelines are divided into reporting areas, each
with a list of guidelines. The MIRO guidelines in its cur-
rent state, 5 March 2017, are presented below. For space
reasons, we omit the example text here. It can be found in
the official guidelines on GitHub [20]. Information items
markedwith an asterisk were introduced as a consequence
of the survey responses.
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 5 of 13
A. The basics
A.1 Ontology name (MUST): The full name of the
ontology, including the acronym and the version
number referred to in the report.
A.2 Ontology owner (MUST): The names,
affiliations (where appropriate) and contact details of
the person, people or consortium that manage the
development of the ontology.
A.3 Ontology license (MUST): The licence which
governs the permissions surrounding the ontology.
A.4 Ontology URL (MUST): The web location
where the ontology file is available.
A.5Ontology repository (MUST): The web location
(URL) of the version control system where current
and previous versions of the ontology can be found.
A.6 Methodological framework* (MUST):
A name or description of the steps taken to develop
the ontology. This should describe the overall
organisation of the ontology development process.
B. Motivation
B.1 Need (MUST): Justification of why the ontology
is required.
B.2 Competition (MUST): The names and citations
for other ontology or ontologies in the same general
area as the one being reported upon, together with a
description on why the one being reported is needed
instead or in addition to the others.
B.3 Target audience (MUST): The community or
organisation performing some task or use for which
the ontology was developed.
C. Scope, requirements, development community (SRD)
C.1 Scope and coverage (MUST): The domain or
field of interest for the ontology and the boundaries,
granularity of representation and coverage of the
ontology. State the requirements of the ontology,
such as the competency questions it should satisfy. A
visualisation or tabular representation is optional, but
often helpful to illustrate the scope.
C.2 Development community (MUST): The
person, group of people or organisation that actually
creates the content of the ontology. This is distinct
from the Ontology Owner (above) that is concerned
with the management of the ontologys development.
C.3 Communication (MUST): Location, usually
URL, of the email list and/or the issue tracking
systems used for development and managing feature
requests for the ontology.
D. Knowledge acquisition (KA)
D.1 Knowledge acquisition method (MUST): How
the knowledge in the ontology was gathered, sorted,
verified, etc.
D.2 Source knowledge location (SHOULD); The
location of the source whence the knowledge was
gathered.
D.3 Content selection (SHOULD): The
prioritisation of entities to be represented in the
ontology and how that prioritisation was achieved.
Some knowledge is more important or of greater
priority to be in the ontology to support the
requirements of that ontology.
E. Ontology content
E.1 Knowledge Representation language (MUST):
the knowledge representation language used and why
it was used. For a language like OWL, indicate the
OWL profile and expressivity.
E.2 Development environment (OPTIONAL): The
tool(s) used in developing the ontology.
E.3 Ontology metrics (SHOULD): Number of
classes, properties, axioms and types of axioms, rules
and individuals in the ontology.
E.4 Incorporation of other ontologies (MUST):
The names, versions and citations of external
ontologies imported into the ontology and where
they are placed in the host ontology.
E.5 Entity naming convention (MUST): The
naming scheme for the entities in the ontology,
capturing orthography, organisation rules, acronyms,
and so on.
E.6 Identifier generation policy (MUST): What is
the scheme used for creating identifiers for entities in
the ontology. State whether identifiers are
semantic-free or meaningful.
E.7 Entity metadata policy (MUST): What
metadata for each entity is to be present. This could
include, but not be limited to: A natural language
definition, editor, edit history, examples, entity label
and synonyms, etc.
E.8 Upper ontology (MUST): If an upper ontology
is used, which one is used and why is it used? If not
used, then why not?
E.9 Ontology relationships (MUST): The
relationships or properties used in the ontology,
which were used and why? Were new relationships
required? Why?
E.10 Axiom pattern (MUST): An axiom pattern is a
regular design of axioms or a template for axioms
used to represent a category of entities or common
aspects of a variety of types of entities. An axiom
pattern may comprise both asserted and inferred
axioms. The aim of a pattern is to achieve a
consistent style of representation. An important
family of axiom patterns are Ontology Design pattern
(ODP) which are commonly used solutions for issues
in representation.
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 6 of 13
E.11 Dereferencable IRI* (OPTIONAL): State
whether or not the IRI used are dereferenceable to a
Web resource. Provide any standard prefix (CURIE).
F. Managing change
F.1 Sustainability plan (MUST): State whether the
ontology will be actively maintained and developed.
Describe a plan for how the ontology will be kept up
to date.
F.2 Entity deprecation strategy (MUST): Describe
the procedures for managing entities that become
removed, split or redefined.
F.3 Versioning policy (MUST): State or make
reference to the policy that governs when new
versions of the ontology are created and released.
G. Quality Assurance (QA)
G.1 Testing (MUST): Description of the procedure
used to judge whether the ontology achieves the
claims made for the ontology. State, for example,
whether the ontology is logically consistent, answers
the queries it claims to answer, and whether it can
answer them in a time that is reasonable for the
projected use case scenario (benchmarking).
G.2 Evaluation (MUST): A determination of
whether the ontology is of value and significance.
An evaluation should show that the motivation is
justified and that the objectives of the ontologys
development are met effectively and satisfactorily.
Describe whether or not the ontology meets its
stated requirements, competency questions and
goals.
G.3 Examples of use (MUST): An illustration of the
ontology in use in its an application setting or use
case.
G.4 Institutional endorsement* (OPTIONAL);
State whether the ontology is endorsed by the W3C,
the OBO foundry or some organisation representing
a community.
G.5 Evidence of use* (MUST): An illustration of
active projects and applications that use the ontology.
Results
We sent our first call for participation in the survey on
the MIRO guidelines proposal on 11th April 2016 and
closed the survey on 12th May 2016. After two weeks from
the announcement, reminder emails were sent out to the
selected email lists. There were 110 responses in total to
the survey. This large number of responses gives us a good
level of confidence of a reasonable representation from the
ontology community. The R analysis documentation for
the survey data can be found at [21].
Demographics of responders
Figure 1 shows the jobs responders declared. The high-
est responders were academic employees (76 out of 110)
with the second largest group being public sector/not-for-
profit employees (12). From the top level domains (TLD)
of the email addresses given by the responders, we created
a geographical profile (Fig. 1, right). We witnessed a broad
spread of TLDs, which indicates that our advertisement
strategymade the survey widely visible. Figure 2 shows the
roles declared by responders. Almost half of the respon-
dents (44%) reported to act in all 5 roles, with another
12% acting in all roles except reviewer of ontology papers.
From the correlation matrix on the right, there appear to
be roughly threemajor groups of responders: (1) users and
readers, (2) paper authors, reviewers and readers and (3)
developers and authors.
Importance of MIRO information items
Figure 3 shows the mean importance rating given to each
MIRO information item. Inspection of the figure shows
three major step changes in the importance that we have
mapped to categories of importance: features that must
be given; those that should be given; and those that are
optional as to whether or not they are given. The major-
ity of the MIRO information items are deemed to be
mandatory: Only the editor used for creating the ontology
(optional), the location of the source knowledge (should),
the ontology content selection (should) and the basic
ontology metrics (should) were not. That ontology met-
rics were thought of as having lower importance was
Fig. 1 Demographics of respondents. Left: Jobs of respondents, overall counts. One job per respondent. Right: Institutional spread of respondents,
overall counts of email top level domain. One email per respondent
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 7 of 13
Fig. 2 Demographics of respondents. Left: Roles of respondents, overall counts. Multiple roles per respondent. Right: Correlation matrix for roles of
users. The darker, the more highly correlated
surprising to us, as metrics are a relatively simple mech-
anism for communicating (for example) the scale of the
ontology, and complexity of the modelling (for example in
the form of a breakdown of axiom type counts, or simple
OWL 2 profile memberships) and can usually be auto-
matically computed. Another low priority item was the
explanations of how the content was selected, i.e. how
the entities and classes were chosen that should be part
of the ontology. In practice, they are often implicit in the
requirements of the ontology and are perhaps therefore
deemed of lower importance.
Table 1 shows the descriptive statistics of all 30 original
information items, sorted by standard deviation. The
standard deviation can be seen as a measure of dis-
agreement: the lower it is, the more respondents agreed
on a rating. It is notable that the standard deviation is
strongly negatively correlated (-0.85) with the mean: The
higher the average rating, the lower the disagreement. For
example, basics such as the URL and the ontology name
have very high mean ratings and the lowest standard
deviations among all items. For items like ontology met-
rics, such as class and property counts, and the editing
tool with which the ontology was built, receive low mean
ratings, and have the highest standard deviations; they are
very important to a handful of people but unimportant
to others.
Fig. 3Mean rating for each information item. Vertical lines correspond to importance level (optional, should, must)
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 8 of 13
Table 1 Descriptive statistics of all information items in MIRO
(mean, median, standard deviation)
MIRO item Rank Mean Med SD
Basics: Ontology URL 1 4.72 5 0.68
Basics: Ontology name 2 4.71 5 0.70
Basics: Ontology license 4 4.50 5 0.79
SRD: Scope and coverage 6 4.15 4 0.84
SRD: Development community 25 3.77 4 0.86
Basics: Ontology owner 3 4.53 5 0.87
Content: Ontology relationships 7 4.13 4 0.88
Content: Incorporation of other ontologies 9 4.09 4 0.95
Motivation: Target audience 13 3.94 4 0.96
Content: Axiom patterns 24 3.80 4 0.96
QA: Examples of use 5 4.19 5 0.99
KA: Knowledge acquisition methodology 14 3.93 4 0.99
Content: Entity metadata policy 16 3.89 4 1.02
Content: KR language 8 4.11 4 1.03
Content: Upper ontology 17 3.88 4 1.03
Change: Versioning policy 23 3.80 4 1.03
QA: Testing 18 3.87 4 1.04
KA: Content selection 28 3.38 4 1.04
Content: Entity naming convention 26 3.74 4 1.04
Basics: Ontology repository 10 4.01 4 1.04
Change: Entity deprecation strategy 21 3.83 4 1.07
Motivation: Competition 12 3.96 4 1.07
Motivation: Need 20 3.85 4 1.08
Content: Identifier generation policy 19 3.86 4 1.08
QA: Evaluation 11 3.99 4 1.08
SRD: Communication 22 3.80 4 1.09
Change: Sustainability plan 15 3.89 4 1.09
KA: Source knowledge location 29 3.36 3 1.09
Content: Ontology metrics 27 3.42 3 1.18
Content: Development environment 30 2.88 3 1.30
Abbreviations: SRD Scope, requirements, development community, QA Quality
assurance, KA Knowledge acquisition, medMedian, sd standard deviation
Data is sorted by standard deviation (sd) in order to highlight the items that had the
largest disagreement
We have ranked the information items shown in Fig. 3
from 1 to 30, with 1 being the most important feature
(i.e. the one that received the highest mean rating) and
30 being the least important. Apart from the overall rank-
ing (see Table 1), we computed the ranking for each user
role separately, to find differences in relative importance.
We will report this difference in what follows by the dif-
ference in rank compared to the overall rank, mentioning
only those items that deviate by at least 4 positions in the
ranking. For example, authors of ontology papers are less
interested in the knowledge acquisition methodology (-7)
than the mean. Indeed, if only the scores of respondents
that are ontology authors are considered, the MIRO item
knowledge acquisition methodology would fall 7 places
in rank order (from position 14 to 21).
Depending on the role of the respondents, some rat-
ings differed markedly in their overall rank. Apart from
the above-mentioned uninterest in the knowledge acqui-
sition methodology, authors of ontology papers are less
interested in the identifier generation policy (-6) and the
reference to the repository holding the ontology (-4) com-
pared to the overall ranking. On the other hand, they are
more interested in upper ontologies (6) and the commu-
nity that is being engaged to develop the ontology (6) than
all of the other groups.
Developers are less interested in reporting about test-
ing (-6), while they care more about the sustainability
plan (5) and an entity deprecation strategy (6) than the
mean. That ontology developers rank testing so low (rank
24 of 30 items on the developers ranking) is, at least
to us, worrying. Testing should be a critical part of the
development lifecycle, and reporting on the results of this
testing is crucial for increasing confidence in potential
users. Reviewers, like authors, find the knowledge acqui-
sition methodology less important than the mean (-6),
while they, perhaps surprisingly given their role, ascribed
considerably more importance to information on why the
ontology is needed (5). Ontology users care less than the
rest about which upper ontologies are used (-4) but rank
the entity deprecation strategy (7) higher than the mean.
Lastly, readers of ontology papers are less interested to
learn about the knowledge acquisition methodology (-6)
as well as details on the entity metadata policy (-5) com-
pared to themean, while caringmore than themean about
the sustainability plan (5) and the entity deprecation strat-
egy (5) - the latter two items both associated broadly with
considerations for planning and risk.
Analysis of comments
The MIRO as presented in the survey was restricted to
very short descriptions, often only a simple label, and did
not provide details on the individual information items.
In the draft, the authors had provided some operational-
isation for the information items. These were omitted
from the survey to encourage as many comments about
the items as possible. As a result of this feedback, we
added four new information items to the original draft
(for details, see Ontology development reporting guide-
lines section):
 Methodological framework
 Dereferenceable IRIs
 Institutional endorsement
 Evidence of use
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 9 of 13
Furthermore, we used the feedback to improve the oper-
ationalisation of the MIRO items, as well as perform some
minor changes to the item labels.
In the following, we present our somewhat ancillary
analysis of the most frequently discussed topics in the
comment fields (see Materials and methods for details).
Note that these categories do not always correspond to
MIRO items. The reason for that is that we did not want
to deviate too much from the labelling of the responders.
Our goal was to capture areas of personal concern to the
responders and establish a secondary metric of impor-
tance that is orthogonal to our main metric (the ratings)
and provides further insight.
As can be seen in Table 2, the information item thought
to be most important was that of coverage and scope. Put
crudely, this is what does the ontology claim to cover?
and in what detail is it covered by the ontology?. This
aspect of ontology reporting directly corresponds to the
MIRO item SRD: Scope and coverage, which is placed
sixth in the main ranking (see Table 1); the highest rank-
ing, just after the basics: ontology URL, name, owner and
license and QA: Example of use. The latter is also the
information item that correspondsmost closely to the sec-
ond most important topic, Use Case, with 18 mentions.
Responders were very interested for the report to reveal
exactly the scenario for which the ontology was designed,
to decide whether it is likely to fit their own. This inter-
est was further reflected by the topic Evidence of use,
which subsequently received its own MIRO item; people
seem to just want to be reassured that the ontology was
not merely developed for its own sake (or purely academic
reasons) before they give it their trust and apply it to their
own scenario.
The active community topic (third most frequently
mentioned topic) corresponds most closely to the MIRO
Table 2 Analysis of the comments on what is most important
Topic Count
Scope and coverage 23
Use case 18
Active community 16
Content 11
Publishing and life cycle 10
Interoperability 9
Metadata and documentation 8
Representation 8
Evidence for use 7
Usability 5
Other 4
All comments were coded and grouped into topics. The counts on the right are the
total number participants mentioning an item belonging to the group
item SRD: Development community, which is ranked
25th according to importance. The discrepancy here can
be explained by the sentiment of the comments mention-
ing the active community: the emphasis of the com-
ments was on the community being active, rather than a
description of the creators of the community. The impor-
tance of the community being active was perhaps not so
much regarding the communitys role as an information
item in a report, rather than a key selection criterion for
whether or not to use the ontology reported upon [22].We
feel that most of this ancillary comment analysis should be
viewed in the light of this: often what should be reported
and what should be the case. These are two very different
questions in practice but are perhaps interpreted as the
same by our survey responders.
The remaining, slightly less important topics of inter-
est do not exactly correspond to individual MIRO items.
Publishing and life cycle for example relates to aspects
of the MIRO Managing change section, as well as other
information items such as the URL, the communication
infrastructure and the repository. However, they all relate
to practical considerations: pieces of information that help
users to decide whether or not to employ the ontology.
Is it represented in a format I can use (representation)?
How easy is it to use in my scenario (Usability)? How
good is the ontology document (metadata and documen-
tation)? The topic of content, the fourth most frequently
mentioned after active community is very vague, but
most probably expresses the sentiment of the responders
that it is important to them what is actually in the ontol-
ogy; in terms of our issue of what is important to report
on, this means that good descriptions of what is repre-
sented in the ontology are critical, andperhapsoften
neglected.
Compliance of existing papers with MIRO guidelines
Table 3 shows the 15 papers that were selected for inclu-
sion into the review process (for methodological details,
see Materials and methods).
Compliance is defined as the number of papers thatmen-
tion a MIRO item divided by the overall number of papers.
We define the following compliance level categories: If the
compliance is
 <20%, we consider it very low (V),
 =20% and <50%, we consider it low (L),
 =50% and <80%, we consider it medium (M),
 >=80% we consider it high (H).
The rating levels optional (O), should (S) and must
(M) are defined in Importance of MIRO information
items section. Table 4 shows, for each of the MIRO
information items the compliance contrasted with the rat-
ings from the ontology survey and the compliance-ratings
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 10 of 13
Table 3 Reviewed papers
Title Journal Year
LOTED2: An Ontology of European Public
Procurement Notices [27]
SWJ 2016
PPROC, an Ontology for Transparency in
Public Procurement [28]
SWJ 2016
Overview of the MPEG-21 Media Contract
Ontology [29]
SWJ 2016
The Document Components Ontology
(DoCO) [30]
SWJ 2016
The Data Mining OPtimization Ontol-
ogy [31]
JWS 2015
My Corporis Fabrica Embryo: An ontology-
based 3D spatio-temporal modeling of
human embryo development [32]
JBMS 2015
Development of an Ontology for Peri-
odontitis [33]
JBMS 2015
Developing VISO: Vaccine Information
Statement Ontology for patient educa-
tion [34]
JBMS 2015
Development and application of an inter-
action network ontology for literature
mining of vaccine-associated gene-gene
interactions [35]
JBMS 2015
The cellular microscopy phenotype ontol-
ogy [36]
JBMS 2016
The Non-Coding RNA Ontology (NCRO):
a comprehensive resource for the unifica-
tion of non-coding RNA biology [37]
JBMS 2016
OBIB-a novel ontology for biobanking JBMS 2016
VICO: Ontology-based representation
and integrative analysis of Vaccination
Informed Consent forms [38]
JBMS 2016
MicrO: an ontology of phenotypic and
metabolic characters, assays, and culture
media found in prokaryotic taxonomic
descriptions [39]
JBMS 2016
Representing vision and blindness JBMS 2016
Towards exergaming commons: compos-
ing the exergame ontology for publishing
open game data [40]
JBMS 2016
An ontology for major histocompatibility
restriction [41]
JBMS 2016
Journals are Semantic Web Journal (SWJ), Journal of Biomedical Semantics (JBMS)
and Journal of Web Semantics (JWS)
factor (CRF). The compliance-rating factor comprises two
letters, the first of which corresponds to the rating level
and the second to the compliance level. For example, MH
stands for a must (M) rating with high (H) compliance.
The first observation to be made is that a large proportion
of MIRO items fall under the MH category (13 out of 30,
43.33%). We need to remember, however, that the survey
only assessed whether an item was covered at all, so no
conclusions can be derived on how well these items were
Table 4 MIRO items ordered by compliance (COM), including
the rating (RAT) from the ontology survey
MIRO item RAT COM CRF
SRD: Scope and coverage 4.15 100.00 MH
Content: KR language 4.11 100.00 MH
Motivation: Target audience 3.94 100.00 MH
Motivation: Need 3.85 100.00 MH
Content: Axiom patterns 3.80 100.00 MH
Basics: Ontology URL 4.72 93.33 MH
Content: Ontology relationships 4.13 93.33 MH
SRD: Development community 3.77 93.33 MH
Basics: Ontology name 4.71 90.00 MH
QA: Examples of usage 4.19 86.67 MH
Content: Incorporation of other ontologies 4.09 86.67 MH
Motivation: Competition 3.96 80.00 MH
KA: Knowledge acqu. methodology 3.93 80.00 MH
Content: Ontology metrics 3.42 80.00 SH
Content: Development environment 2.88 73.33 OM
QA: Evaluation 3.99 66.67 MM
Content: Upper ontology 3.88 66.67 MM
KA: Content selection 3.38 66.67 SM
Basics: Ontology owner 4.53 53.33 MM
Basics: Ontology repository 4.01 53.33 MM
SRD: Communication 3.80 40.00 ML
Content: Entity metadata policy 3.89 33.33 ML
Basics: Ontology license 4.50 26.67 ML
QA: Testing 3.87 26.67 ML
Content: Entity naming conventions 3.74 26.67 ML
KA: Source knowledge location 3.36 26.67 SL
Content: Identifier generation policy 3.86 6.67 MV
Change: Versioning policy 3.80 6.67 MV
Change: Sustainability plan 3.89 0.00 MV
Change: Entity deprecation strategy 3.83 0.00 MV
The compliance-rating factor (CRF) is described in Compliance of existing papers
with MIRO guidelines section
covered by the original papers. The second most impor-
tant category is ML (must rating, low coverage) with
5 out of 30 items (16.67%), followed by MM (medium
coverage) and MV (very low coverage) with 4 items
(13.33%).
We believe the ML and MV categories to be the most
important ones to consider, as they represent the high-
est discrepancy between what readers wish to see in a
paper compared to what they would actually find. The
four items in the MV category, the identifier generation
policy, the versioning policy, the sustainability plan and
the entity deprecation strategy all concern aspects of the
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 11 of 13
ontology lifecycle. It is perhaps less surprising that the
identifier generation policy and the deprecation strategy
are rarely mentioned at all: they may either be taken for
granted (perhaps implicitly by referring to the compliance
to OBO principles) or simply not be applicable, for exam-
ple in cases where the scope of the ontology is small and
well-defined, which would render the use of identifiers
unnecessary, as it would a bespoke deprecation strategy.
The other two items, however, versioning and sustainabil-
ity plan, are applicable to all ontologies, and neglecting
to give the reader a sense of them can easily lead to the
impression that the development of the ontology is a one-
off, zero maintenance, in some cases even throw-away
prototype case study. In our opinion, this is a wide-spread
problem even beyond the scope of this review, and finds
another confirmation in the fact that just about half of the
reviewed papers explicitly referred to a versioned repos-
itory such as GitHub (53%) and less than half mention
something like issue tracking or email lists (40%).
The items in the ML category are the entity meta-
data policy (33.33% coverage), an explicit mention of the
license under which the ontology may be used (26.67%),
means of communication such as email lists and issue
tracking (40%), an explicit naming convention for entities
(26.67%) and an explicit testing strategy (26.67%). Again,
most of these metrics concern the management of the
ontology life-cycle.
Noteworthy is the low compliance on the testing item.
Testing differs from an evaluation in that it is not con-
cerned with the question of whether the ontology in
principle does its job (this would be the evaluation, for
example through a use case study), but a systematic
attempt to capture non-functional aspects of the ontol-
ogy, such as performance (for example classification time
when reasoning is required) or correctness of the hierar-
chy after modelling or mapping with other ontologies, etc.
At the very least, we feel, it should be stated whether or
not the ontology is parseable by the usual tools, like the
Jena API [23] or the OWL API [24], Protégé or OBO-Edit.
Discussion
In this paper, we addressed the problem of what to report
upon in ontology description reports and potentially
other documentation, including in the ontology itself.
There are several actors in this scenario, for example: (1)
paper or documentation authors writing up the report
that need guidance on what aspects of the ontology
development process to cover, (2) ontology users (which,
among our survey respondents, frequently coincide with
readers of ontology papers) that need guidance for the
ontology selection process and (3) ontology developers
that are just about to start development that require
a checklist for recording the forthcoming development.
Many of the respondents to our survey adopt a broad
range of ontology-related roles. How ontologies are
reported needs to satisfy actors playing all of these roles.
We were gratified by the large number of responses
we received to our survey in a relatively small period of
time. Even after the survey was formally closed, we kept
receiving responses, which suggests that the issue of what
should be reported about an ontology is of significant
interest in the community. With the 110 responses used
in this study, we think the survey is representative of the
community; indeed, the number of responses approxi-
mates the number of people attending bio-ontology meet-
ings such as the Bio-Ontologies COSI at the ISMB and the
International Conference on Biomedical Ontology.
The vast majority of the MIRO guidelines have the
importance designation of must. This may appear oner-
ous, but the MIRO guidelines are a minimal list of
that which should be reported. Being minimal indicates
that the MIRO information items are intrinsically those
that are most important. Thus, a claim of compliance with
the MIRO guidelines should mean that the ontology is
reported well. Besides, our importance designations are
driven by the data supplied in the survey; irrespective of
any possible response biases [25] and we have trusted the
data.
A methodological problem we faced during the paper
coding was to judge whether a code was sufficiently cov-
ered when it is only implicitly mentioned. For example,
items such as scope and need are very hard to not cover at
all. That is why the compliance of the MIRO items SRD:
Scope and coverage and Motivation: Need was 100%
(remember that this does not mean they were covered
well, only that they were at least mentioned). Coding such
items was, however, sometimes challenging as they were
not explicitlymentioned, for example by saying The need
of this ontology emerged from. . .  or The ontology covers
all categories of. . . . As reviewers, we would have liked such
explicit statements, and it is likely that readers of ontol-
ogy papers would also benefit from clarity resulting from
stating information items explicitly.
The most important categories were around ontology
scope and coverage, and this is perhaps unsurprising.
Apart from this, a category of very high concern to
the community (as reflected for example by Analysis of
comments section) was the area of publishing and ontol-
ogy life-cycle related issues. Such issues touch on some
MIRO items such as the sustainability plan, versioning
policy and repository location. We found that this area
is frequently absent in high-end ODR; the three items
with the lowest compliance all fall under this category.
We believe that in some cases, this may point to the
intention of the ontology developers to produce a one-
off product rather than produce a continuously main-
tained knowledge artefact. Our recommendation from
this work is that authors should make these parts explicit
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 12 of 13
in the report. If an ODR suggests that it provides an
important service, especially if positioning as a refer-
ence to be widely adopted by a community, a description
of the sustainability plan must be included. However,
it is also possible that ontology developers are simply
not entirely conscious about how important such aspects
are for deciding whether or not to employ an ontology.
Most ODR focus on ontology content and knowledge
acquisition rather than aspects of the development life-
cycle that become relevant only after the first draft of
the ontology is published. The MIRO guidelines and the
analysis we presented should help to improve awareness
regarding such information items and their importance
to users.
Community feedback was an integral part of the devel-
opment process of theMIRO guidelines. Not only were we
able to derive categories of importance from the ratings;
we were also able to identify four new categories that were
not covered by the firstMIRO draft.We further used com-
munity feedback to improve the definitions and labels of
the information items. For theMIRO guidelines to have an
impact on the quality of ontology papers, we believe that
MIRO and projects similar to it should be community-
driven to reach the highest degree of consensus possible,
in much the same way as the ontology community has
developed some of the most popular ontologies.
In the future, it should also be possible to extend MIRO
beyond guidelines for reporting in text to a more struc-
tured form. Such a form would enable the metadata
reportingMIRO to be accessed programmatically inmuch
the same way as approaches such as VoID [7] have suc-
cessfully taken. Besides, a W3C working group, similar to
that pursued by the VoID authors, to further the estab-
lishment of a structured form of MIRO could help MIRO
become a more widely adopted method used for publish-
ing ontologies in literature and on the web. Those involved
in developing and using ontologies should be involved
in such an effort, but an important additional partici-
pant would be the maintainers of ontology libraries and
repositories such as the OBO Library, BioPortal and the
Ontology Lookup Service (OLS). Adoption and publishing
of MIRO alongside ontologies in these repositories would
be a valuable asset when considering the suitability of an
ontology for use.
As well as structured, computationally amenable report-
ing, it should also be possible to derive some aspects
of the MIRO guidelines programmatically. Obvious cases
include numbers of entities in an ontology, relationships
used, location, licence and location etc. Programmatically
extracting axiom patterns is more difficult, but attempts
have been made such as extracting syntactic regulari-
ties from ontologies as proxies for axiom patterns [26],
which finds syntactic regularities in ontologies. With
such computational support, creating sound, up-to-date
descriptions of an ontology in accordance with the MIRO
guidelines becomes easier.
Conclusions
Appropriate reporting of ontologies and ontology devel-
opment processes is important for the understanding of
those ontologies. To this end, we have created a set of min-
imum information guidelines for ontology reports upon
which we have gathered input from the ontology com-
munity. The method we have used to develop the MIRO
guidelines give confidence that they are well supported.
We learned which information items are of particular
importance to the community, and we learned where the
current reporting is lacking. TheMIRO guidelines need to
be an evolving reporting standard, especially with respect
to how each of the reporting items is operationalised; we
welcome continuous input on the MIRO guidelines [20].
We recommend the MIRO guidelines to both ontology
users, authors and reviewers in the ontology community
to improve the presentation of their work.
Abbreviations
CRF: Compliance-ratings factor; IRI: International resource identifier; JBMS:
Journal of biomedical semantics; KA: Knowledge acquisition; MIRO: Minimum
information for reporting an ontology; MOD: Metadata for ontology
description and publication; OWL: Web ontology language; ODP: Ontology
description paper; OMV: Ontology metadata vocabulary; OBO: Open
biomedical ontologies; QA: Quality assurance; SWJ: Semantic web Journal;
SRD: Scope, requirements, development community; TLD: Top level domain;
VOID: Vocabulary of interlinked datasets; WSJ: Journal of web semantics; W3C:
World Wide web consortium
Acknowledgements
The authors would like to thank the ontology community for the time they
took to respond to our extensive questionnaire and their valuable feedback.
The participation of RS and NM in this work has been funded by the EPSRC
project:WhatIf: Answering What if. . .  questions for Ontology Authoring, EPSRC
reference EP/J014176/1.
Funding
NM was funded by the EPSRC project:WhatIf: Answering What if. . .  Questions
for Ontology Authoring, reference EP/J014176/1.
Availability of data andmaterial
The datasets generated and/or analysed during the current study are available
in the MIRO GitHub repository, https://github.com/owlcs/miro/tree/master/
supplementary, as well as on Zenodo, https://doi.org/10.5281/zenodo.398804
and http://rpubs.com/matentzn/miro.
Authors contributions
JM, CM and RS conceived the study and drafted the guidelines. NM did the
analysis of the data and led the writing supported by the other authors. All
authors read and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 13 of 13
Author details
1School of Computer Science, University of Manchester, Oxford Road,
Manchester, UK. 2FactBio, Innovation Centre, Cambridge Science Park, CB4 0EY
Cambridge, UK. 3Lawrence Berkeley National Laboratory, Berkeley, USA.
Received: 25 March 2017 Accepted: 22 December 2017
Névéol et al. Journal of Biomedical Semantics  (2018) 9:12 
https://doi.org/10.1186/s13326-018-0179-8
REVIEW Open Access
Clinical Natural Language Processing in
languages other than English: opportunities
and challenges
Aurélie Névéol1 *, Hercules Dalianis2, Sumithra Velupillai3,4, Guergana Savova5 and Pierre Zweigenbaum1
Abstract
Background: Natural language processing applied to clinical text or aimed at a clinical outcome has been thriving in
recent years. This paper offers the first broad overview of clinical Natural Language Processing (NLP) for languages
other than English. Recent studies are summarized to offer insights and outline opportunities in this area.
Main Body: We envision three groups of intended readers: (1) NLP researchers leveraging experience gained in other
languages, (2) NLP researchers faced with establishing clinical text processing in a language other than English, and
(3) clinical informatics researchers and practitioners looking for resources in their languages in order to apply NLP
techniques and tools to clinical practice and/or investigation. We review work in clinical NLP in languages other than
English. We classify these studies into three groups: (i) studies describing the development of new NLP systems or
components de novo, (ii) studies describing the adaptation of NLP architectures developed for English to another
language, and (iii) studies focusing on a particular clinical application.
Conclusion: We show the advantages and drawbacks of each method, and highlight the appropriate application
context. Finally, we identify major challenges and opportunities that will affect the impact of NLP on clinical practice
and public health studies in a context that encompasses English as well as other languages.
Keywords: Natural Language Processing, Clinical Decision-Making, Languages other than English
Background
Clinical research in a global context
Healthcare is a top priority for every country. The
goal of clinical research is to address diseases with
efforts matching the relative burden [1]. Compu-
tational methods enable clinical research and have
shown great success in advancing clinical research in
areas such as drug repositioning [2]. Much clinica
l information is currently contained in the free text of sci-
entific publications and clinical records. For this reason,
Natural Language Processing (NLP) has been increasingly
impacting biomedical research [35]. Prime clinical appli-
cations for NLP include assisting healthcare professionals
with retrospective studies and clinical decision making
*Correspondence: aurelie.neveol@limsi.fr
1LIMSI, CNRS, Université Paris Saclay, Rue John von Neumann, F-91405 Orsay
Paris, France
Full list of author information is available at the end of the article
[6, 7]. There have been a number of success stories in
various biomedical NLP applications in English [819].
The ability to analyze clinical text in languages other than
English opens access to important medical data concern-
ing cohorts of patients who are treated in countries where
English is not the official language, or in generating global
cohorts especially for rare diseases. One such example
is the Phelan-McDermid Syndrome Foundation (PMSF),
which is leading a Patient Powered Research Network
project (part of the Patient Centered Outcome Research
Institute, PCORI [20] on a very rare disease. PMSF par-
ents, together with researchers and advisors, launched an
international patient registry, the PMSIR, that is directed,
governed, and implemented by patient families. There are
a total of 900 cases of this rare disease in the entire world.
Each patient contributed their EHR and genomics data
to enable phenotype/genotype studies. Recently, Kohane
et al. have shown that methods allowing an aggregated
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Névéol et al. Journal of Biomedical Semantics  (2018) 9:12 Page 2 of 13
exploitation of clinical data from multiple healthcare cen-
ters could contribute to make headway in the understand-
ing of autism spectrum disorders [21]. Cross-lingual text
mining of newswires in thirteen languages was shown
to be helpful for automated health surveillance of dis-
ease outbreaks, and was routinely implemented in the
BioCaster portal [22].
In this context, data extracted from clinical text and clin-
ically relevant texts in languages other than English adds
another dimension to data aggregation. TheWorld Health
Organization (WHO) is taking advantage of this oppor-
tunity with the development of IRIS [23], a free software
tool for interactively coding causes of death from clini-
cal documents in seven languages. The system comprises
language-dependent modules for processing death certifi-
cates in each of the supported languages. The result of
language processing is standardized coding of causes of
death in the form of ICD10 codes, independent of the
languages and countries of origin.
Objective and Scope
This paper follows-up on a panel discussion at the 2014
American Medical Informatics Association (AMIA) Fall
Symposium [24]. Following the definition of the Interna-
tional Medical Informatics Association (IMIA) Yearbook
[25, 26], clinical NLP is a sub-field of NLP applied to clini-
cal texts or aimed at a clinical outcome. This encompasses
NLP applied to texts in Electronic Health Records (EHRs),
but also extends to the development of resources for
clinical NLP systems, and to clinically relevant research
addressing biomedical information retrieval or the analy-
sis of patient-authored text for public health or diagnostic
purposes. We survey studies conducted over the past
decade and seek to provide insight on the major develop-
ments in the clinical NLP field for languages other than
English. We outline efforts describing (i) building new
NLP systems or components from scratch, (ii) adapting
NLP architectures developed for English to another lan-
guage, and (iii) applying NLP approaches to clinical use
cases in a language other than English.
Finally, we identify major NLP challenges and opportu-
nities with impact on clinical practice and public health
studies accounting for language diversity.
Main Text
Reviewmethod and selection criteria
Conducting a comprehensive survey of clinical NLP work
for languages other than English is not a straightforward
task because relevant studies are scattered across the lit-
erature of multiple fields, including medical informatics,
NLP and computer science. In addition, the language
addressed in these studies is not always listed in the title
or abstract of articles, making it difficult to build search
queries with high sensitivity and specificity.
In order to approximate the publication trends in the
field, we used very broad queries. A Pubmed query for
Natural Language Processing returns 4,486 results (as of
January 13, 2017). Table 1 shows an overview of clinical
NLP publications on languages other than English, which
amount to almost 10% of the total.
We are showing the results of this query as an imperfect
proxy for estimating the scale of the biomedical literature
relevant to NLP research, as some publications addressing
clinical NLP may not appear in PubMed, and some publi-
cations referenced in PubMedmay bemissed by the query.
As described below, our selection of studies reviewed
herein extends to articles not retrieved by the query.
Figure 1 shows the evolution of the number of NLP
publications in PubMed for the top five languages other
than English over the past decade. We can see that French
benefits from a historical but sustained and steady inter-
est. Chinese and Spanish have recently attracted sustained
efforts. Japanese and German seem to receive plateauing
attention.
This work is not a systematic review of the clinical NLP
literature, but rather aims at presenting a selection of
studies covering a representative (albeit not exhaustive)
number of languages, topics and methods. We browsed
the results of broad queries for clinical NLP in MEDLINE
and ACL anthology [26], as well as the table of contents
of the recent issues of key journals. We also leveraged our
own knowledge of the literature in clinical NLP in lan-
guages other than English. Finally, we solicited additional
RESEARCH Open Access
Ontology-based literature mining and class
effect analysis of adverse drug reactions
associated with neuropathy-inducing drugs
Junguk Hur1* , Arzucan Özgür2 and Yongqun He3,4,5,6*
Abstract
Background: Adverse drug reactions (ADRs), also called as drug adverse events (AEs), are reported in the FDA drug
labels; however, it is a big challenge to properly retrieve and analyze the ADRs and their potential relationships
from textual data. Previously, we identified and ontologically modeled over 240 drugs that can induce peripheral
neuropathy through mining public drug-related databases and drug labels. However, the ADR mechanisms of these
drugs are still unclear. In this study, we aimed to develop an ontology-based literature mining system to identify
ADRs from drug labels and to elucidate potential mechanisms of the neuropathy-inducing drugs (NIDs).
Results: We developed and applied an ontology-based SciMiner literature mining strategy to mine ADRs from the
drug labels provided in the Text Analysis Conference (TAC) 2017, which included drug labels for 53 neuropathy-
inducing drugs (NIDs). We identified an average of 243 ADRs per NID and constructed an ADR-ADR network, which
consists of 29 ADR nodes and 149 edges, including only those ADR-ADR pairs found in at least 50% of NIDs.
Comparison to the ADR-ADR network of non-NIDs revealed that the ADRs such as pruritus, pyrexia,
thrombocytopenia, nervousness, asthenia, acute lymphocytic leukaemia were highly enriched in the NID network.
Our ChEBI-based ontology analysis identified three benzimidazole NIDs (i.e., lansoprazole, omeprazole, and
pantoprazole), which were associated with 43 ADRs. Based on ontology-based drug class effect definition, the
benzimidazole drug group has a drug class effect on all of these 43 ADRs. Many of these 43 ADRs also exist in the
enriched NID ADR network. Our Ontology of Adverse Events (OAE) classification further found that these 43
benzimidazole-related ADRs were distributed in many systems, primarily in behavioral and neurological, digestive,
skin, and immune systems.
Conclusions: Our study demonstrates that ontology-based literature mining and network analysis can efficiently
identify and study specific group of drugs and their associated ADRs. Furthermore, our analysis of drug class effects
identified 3 benzimidazole drugs sharing 43 ADRs, leading to new hypothesis generation and possible mechanism
understanding of drug-induced peripheral neuropathy.
Background
While drugs have been widely and successfully used to
treat various diseases, most drugs cause different adverse
events (AEs), commonly called adverse drug reactions
(ADRs). These ADRs are sometimes severe and signifi-
cantly affect public health. Indeed, ADRs are listed as
the fourth killer after heart disease, cancer, and stroke
[1]. Therefore, it is critical to carefully study the ADRs
and underlying mechanisms.
Multiple studies have been conducted to automatically
identify ADRs in text using Natural Language Processing
(NLP) techniques. Different types of data sources such
as electronic health records [2], scientific publications,
and social media data have been used to extract ADRs.
A lexicon of ADR-related terms and concepts was com-
piled from different sources such as the Unified Medical
Language System (UMLS) [3] and the side effect re-
source (SIDER) [4] and was used to match the ADR
* Correspondence: junguk.hur@med.und.edu; yongqunh@med.umich.edu
1Department of Department of Biomedical Sciences, University of North
Dakota School of Medicine and Health Sciences, Grand Forks, ND 58202, USA
3Unit for Laboratory Animal Medicine, Department of Microbiology and
Immunology, University of Michigan Medical School, Ann Arbor, MI 48109,
USA
Full list of author information is available at the end of the article
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Hur et al. Journal of Biomedical Semantics  (2018) 9:17 
https://doi.org/10.1186/s13326-018-0185-x
mentions in user comments retrieved from Daily-
Strength (http://www.dailystrength.org) by Leaman et al.
[5]. Nikfarjam and Gonzalez used the same user com-
ment data set and developed an association rule mining
approach to tag ADR mentions [6]. Similarly to Leaman
et al., Gurulingappa et al. [7] also developed a
lexicon-based matching approach to identify ADRs in
text using the lexicon created based on the Medical
Dictionary for Regulatory Activities (MedDRA) [8] and
DrugBank [9]. However, rather than using user com-
ments from social media, Gurulingappa et al. used the
abstracts of case reports as their data source. Product la-
bels have also been used as data sources to extract ADRs
and create knowledge bases of known ADRs [10, 11]. A
review of recent techniques on ADR extraction from
text from various data sources is available in [12].
An important group of ADRs is neuropathy. Using
FDA reported package insert documents and drug safety
records, our previous studies identified 242
neuropathy-inducing drugs (NIDs) through mining vari-
ous public resources and drug labels [13, 14]. We have
previously developed an Ontology of Drug Neuropathy
Adverse Events (ODNAE) that ontologically represents
214 NIDs, corresponding chemicals of these drugs,
chemical function, adverse events associated with these
drugs, and various other chemical characteristics [14].
Our study also showed that ODNAE provides an ideal
platform to systematically represent and analyze AEs
associated with neuropathy-inducing drugs and generate
new scientific insights and hypotheses [14]. One weak-
ness of the ODNAE study is that ODNAE only collects
neuropathy-related ADRs commonly found in drug
package insert documents and misses the collection of
non-neuropathy ADRs from different sources.
In addition to enhanced literature mining, ontology
can also be used for advanced class effect analysis.
Specifically, an AE-specific drug class effect is defined to
exist when all the drugs in a specific drug class (or drug
group) are associated with an AE. In a recent study on
cardiovascular drug-associated AEs, a proportional
class-level ratio (PCR) value was defined and used to
identify drug class effect on different AEs [15]. Specific-
ally, when the PCR value equals to 1, it means that a
class effect of a group of drugs on a specific AE exists.
Previous PCR-based heatmap analyses identified many
important drug class effects on different AEs [15].
In addition to the official FDA drug package insert
documents, FDA also collects large amounts of spontan-
eous ADR case reports. To better understand these case
report data, it is critical to use standardized termin-
ologies or ontologies to identify drugs, ADRs, and
associated data from the text reports. Therefore,
ontology-based literature mining becomes critical. Previ-
ously, we applied the Vaccine Ontology (VO) [16] to
enhance our literature mining of interferon-gamma re-
lated [17], Brucella-related [18], and fever-related [19]
gene interaction networks in the context of vaccines and
vaccinations. In these studies, we used and expanded Sci-
Miner [20], a literature mining program with a focus on
scientific article mining. SciMiner uses both dictionary-
and rule-based strategies for literature mining [20].
To better study biological interaction networks, we
have also developed a literature mining strategy
CONDL, or Centrality and Ontology-based Network
Discovery using Literature data [19]. The centrality ana-
lysis here refers to the application of different centrality
measures to calculate the most important genes (i.e.,
hub genes) of the resulting gene-gene interaction net-
work out of biomedical literature mining. Centrality
measures, including degree, eigenvector, closeness, and
betweenness, have been studied [19, 21]. The CONDL
strategy was applied to extract and analyze IFN-? and
vaccine-related gene interaction network [21] and
vaccine and fever-related gene interaction network [19],
and our results showed that centrality analyses could
identify important genes and raise novel hypotheses
based on literature mined gene interaction networks.
The main purpose of this study was to develop a
CONDL method for literature mining of all ADRs asso-
ciated with neuropathy inducing drugs (NIDs) and used
the mined results for systematic network and class effect
analyses. Using MedDRA [8], ODNAE [14], Chemical
Entities of Biological Interest (ChEBI) [22], and Ontol-
ogy of Adverse Events (OAE) [23], we developed an
ontology-based ADR-SciMiner tool for identifying ADRs
from drug labels and applied it to NIDs to ontologically
model their ADR-associated characteristics. The litera-
ture mined results were then used for ontology-based
class effect analysis, leading to new scientific discoveries.
Methods
The overall workflow of our ontology-based literature
mining approach for the study of neuropathy-inducing
drugs (NIDs) is illustrated in Fig. 1. Briefly, our approach
included development of ADR-SciMiner platform that
identifies ADRs from drug labels using the terms in
MedDRA and OAE. Various term expansion, name
matching, and filtering rules have been implemented.
The mining performance was evaluated using manually
curated drug labels. The final version of ADR-SciMiner
was applied to the NID labels and the results were
examined using the ADR-ADR interaction network and
the OAE hierarchical structure.
NID drug labels
In the present study, we used a collection of
XML-structured drug labels that are applied for the Text
Analysis Conference (TAC) Adverse Drug Reaction
Hur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 2 of 10
Extraction from Drug Labels track (https://tac.nist.gov/
2017/). This data set includes the adverse event sections
from a total of 2308 US FDA drug labels, which were
split into two sets: Training set and Unannotated set,
each containing 101 and 2207 drug labels. The Training
set contained manually curated ADRs provided by the
TAC organizing committee. Among 2207 drug labels in
the Unannotated set, TAC provided 99 labels with manu-
ally curated ADRs, which were used for performance evalu-
ation of ADR-SciMiner. Figure 2 illustrates an example of
XML-formatted drug-label from the Training set.
NIDs were collected from our previous two studies:
one examining the systems pharmacological aspects of
NIDs [13] and another focusing on ontology-based col-
lection, representation and analysis of drug-associated
neuropathy adverse events [14].
SciMiner tagging of ADR and drug terms
SciMiner was originally developed as a web-based litera-
ture mining platform, designed for identification of
human genes and proteins in a context-specific corpus
[20]. Later, SciMiner was updated to identify bacterial
genes and various biomedical ontologies such as Vaccine
Ontology (VO) and Interaction Network Ontology
(INO), developed by our groups, resulting in specific
variations of SciMiner: INO-SciMiner [24], VO-SciMiner
[18], and E-coli-SciMiner [25]. In this study, we devel-
oped another version of SciMiner, specializing in the
identification and analysis of ADRs from the US FDA
drug labels.
MedDRA, or Medical Dictionary for Regulatory
Activities, is a clinically validated standardized medical
terminology dictionary (and thesaurus), consisting of five
levels of hierarchy. MedDRA has been widely used for
supporting ADR reporting in clinical trials [8, 26].
MedDRA release version 20 (https://www.meddra.org/)
and the OAE ontology were used as the source of the
ADR terms, which have been incorporated into SciMiner
dictionary for ADR term identification. Perl package
Lingua::EN was used to expand the ADR dictionary
allowing the inclusion of additional plural or singular
forms where only one form is included in the dictionary.
For example, peripheral neuropathy has been expanded
to include peripheral neuropathies. Besides, various
term variation and filtering rules were implemented to
improve the accuracy of ADR term tagging. For ex-
ample, MedDRA terms ID 10003481 has preferred name
of Aspartate aminotransferase increased. ADR-SciMiner
was designed to properly identify variations of this pre-
ferred name such as increased AST, AST elevated, and
high AST. To reduce false positives, any matching ADR
terms from section or table headers of drug labels were
excluded.
Performance evaluation of ADR-SciMiner
The TAC dataset included 200 manually curated labels
(101 in the Training and 99 in the Unannotated sets)
and the details have been recently published [27]. Briefly,
four annotators, including two medical doctors, one
medical librarian and one biomedical informatics
Fig. 1 Project workflow. This figure illustrates our overall workflow in the present study. US FDA drug labels were analyzed to identify ADRs and
normalized them through MedDRA v20 and OAE using ADR-SciMiner. A network of ADR-ADR based on the ADRs reported to have been caused
by NIDs was constructed. The most central ADRs in the network were analyzed. The characteristics of NID-associated ADRs were further explored
using the ontological structures in OAE
Hur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 3 of 10
researcher, participated in the manual annotation process
of these 200 drug labels. These annotators were all trained
biomedical annotation and the drug labels were annotated
independently by these annotators. Any disagreements
were reconciled in pairs or collectively resolved by all four
annotators. The mining performance of ADR-SciMiner
was evaluated using the 99 drug labels in the Unannotated
set. The evaluation was done at the level of normalized
MedDRA Preferred Terms (PTs) for each drug. Recall,
Precision, and F-Score were calculated.
Generation of ADR-ADR network and its analysis
NID and non-NID associated ADR-ADR networks were
constructed in our study. ADRs were represented as the
nodes of the network. Two nodes were connected by an
edge if they are associated with the same drug. In order
to obtain highly prevalent NID and non-NID specific
ADRs, an edge weight threshold of 50% was set. In other
words, two ADRs were connected by an edge if they
co-occur together as ADRs of at least 50% of the NID or
non-NID drugs. Centrality analysis was performed on
Fig. 2 XML-formatted drug label. This figure illustrates an example of XML-formatted drug labels (adcetris) from the training set. The content has
been reduced and simplified to fit into a figure for demonstration purpose. Typical XML-formatted labels from the training set include three main
sections: Text containing the texts from ADR-relevant sections from drug labels; Mentions containing the manually curated ADRs; and
Reactions containing normalized ADRs in terms of MedDRA terms
Hur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 4 of 10
the ADR-ADR networks using the Cytoscape plug-in
CentiScaPe [28] to identify the most salient NID and
non-NID associated ADRs. Degree centrality and eigen-
vector centrality were computed. Degree centrality cor-
responds to the number of neighbors a node has. Each
neighbor contributes equally to the centrality of the
node. On the other hand, in eigenvector centrality the
contribution of each neighbor is proportional to its own
centrality.
ChEBI and OAE-based ontological analyses of three
neuropathy-inducing drugs and associated ADRs
The drugs were mapped to ChEBI [22] terms, which are
also imported and used in the ODNAE. The identified
ADRs were mapped to OAE terms, and the OAE struc-
ture was used to classify and analyze the ADR structure.
To extract the associated drugs, AEs, and their related
terms, the Ontofox tool [29] was used. The Protégé
OWL editor [30] was used to visualize the hierarchical
structure of these extracted terms.
Ontology-based analysis of drug class effects on AEs
ChEBI was used to classify NIDs into different
higher-level classes or groups. For each high or inter-
mediate level class, we calculated the drug class effect
on AEs. Specifically, all the identified 53 NIDs were clas-
sified into different categories using ChEBI. The AEs as-
sociated with each NID were identified in the previous
studies. Based on these results, we were able to identify
the common AEs associated with all NIDs under a spe-
cific class (e.g., benzimidazole drugs). Based on the class
effect definition, these results indicate that there exists a
class effect of the specific class on the common AEs (i.e.,
the PCR value =1) [15]. All the common AEs were then
classified based on OAE using the Ontofox tool [29].
Results
NID drug labels
From our two published studies on neuropathy-inducing
drugs [13, 14], we collected a total of 242 NIDs. We also
obtained a collection of XML-structured drug labels that
are used for the 2017 Text Analysis Conference (TAC)
Adverse Drug Reaction Extraction from Drug Labels
track. This data set contains the adverse event sections
of a total of 2308 US FDA drug labels in two subsets:
Training set with 101 labels and Unannotated set with
2207 labels, which corresponded to a total of 1883
unique drugs. There were 299 unique drug names, each
of which included two or more labels, because a drug in
our study refers to a generic drug name or an active
drug ingredient which can have multiple brands with
different labels. Among the 2308 labels, there were 69
labels corresponding to 53 NIDs, which served as the
dataset in the present study.
SciMiner tagging of ADR and drug terms and
performance evaluation
ADR-SciMiner has been developed to include the
dictionary of ADRs based on MedDRA release 20 and
the current version of OAE. The ADR term diction-
ary is expanded to include variations such as plural
vs singular nouns to increase the coverage. The per-
formance of current version of ADR-SciMiner was
evaluated based on the ADRs from 99 labels. These
labels included 5158 MedDRA PT terms, while
ADR-SciMiner reported 5360 PT terms collectively.
ADR-SciMiner correctly identified 4198 of these 5158
PTs in the TAC data: a recall of 0.81, a precision of
0.75, and an F-Score of 0.77 was obtained.
MedDRA representation of ADRs
Table 1 summarizes the numbers of identified ADRs
from the 53 NIDs. These NIDs are a subset of the total
NIDs identified in our previous studies [13, 14]. We did
not use all the over 200 NIDs because only these 53
NIDs have corresponding ADR text data in the FDA
TAC 2017 dataset. Briefly, ADR-SciMiner identified
approximately an average of 243 ADRs per drug (114
unique ADRs per drug). Antidepressant medicine Venla-
faxine had the most ADRs of 433, while glucocorticoid
triamcinolone has the least ADRs of 9 (Table 1).
Literature mining statistics and ADR-ADR network
Figure 3 is a NID-associated ADR network based on the
cutoff of co-occurrence of two ADRs connected in at
least 50% (i.e., 27 out of 53) of the NIDs. The NID spe-
cific ADR-ADR network shown in Fig. 3 contains 29
nodes and 149 edges. The common ADRs are located at
the center of the network, including terms like headache,
vomiting, pyrexia, nausea, dizziness, etc. More specific
analysis of the network is reported below.
Centrality analysis of ADR-ADR network
The eigenvector and degree centrality scores of the 29
ADRs found using NIDs are shown in Table 2. The same
approach was used to construct a non-NID specific
ADR-ADR network, where two ADRs are connected by
an edge if they co-occur in at least 50% of the remaining
(i.e., non-NID drugs). This resulted in a network
containing only six ADRs, namely headache, vomiting,
diarrhoea, rash, nausea, and dizziness. Although these are
also among the most central ADRs in the NID specific
network, they are not NID specific, since they are also
prevalent and commonly occur together in the non-NID
case. Some notable ADRs central in the NID-specific net-
work but not parts of the non-NID specific network in-
clude pruritus, pyrexia, thrombocytopenia, nervousness,
asthenia, acute lymphocytic leukaemia, decreased appetite,
Hur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 5 of 10
insomnia, and depression. Degree and eigenvector central-
ity produced the same ranking (Table 2).
Ontology-based analysis of benzimidazole NID drugs and
their associated ADR types
Out of the 53 drugs, we used the ChEBI chemical
ontology structure to examine the chemical classifica-
tion of these 53 drugs and their associated
upper-level hierarchies. One interesting group of che-
micals becomes interesting to us, which is the group
of benzimidazole, a colorless heterocyclic aromatic
organic compound that consists of the fusion of ben-
zene and imidazole [31]. Benzimidazole drugs are
structural isosteres of naturally-occurring nucleotides,
allowing them to interact with the biopolymers of liv-
ing systems and become an important group of drugs
with antimicrobial, anti-inflammatory, and anticancer
activities. The three benzimidazole NIDs identified in
our study include lansoprazole, omeprazole, and pan-
toprazole (Fig. 4), which are all proton-pump inhibi-
tors that inhibit gastric acid secretion [32]. These
three drugs can all be used for relief of symptoms of
gastroesophageal reflux disease, gastric and duodenal
ulcer disease, and eradication of Helicobacter pylori
infection [32]. Their shared and different ADR pro-
files have not been studied.
In our study, lansoprazole, omeprazole, and panto-
prazole are associated with 389 (273 are unique), 298
(165 are unique), and 166 (74) ADRs, respectively.
We identified 43 ADRs associated with all three
drugs. Based on our drug class effect definition [15],
these 43 ADRs are all categorized as AEs out of the
class effect of the benzimidazole drug class. Further-
more, we applied the OAE to generate a subset view
of these ADRs in the OAE framework (Fig. 5). As
shown in this figure, these 43 ADRs are focused on
behavioral and neurological ADRs, digestive ADRs,
and skin ADRs. There are also many ADRs in the
hematopoietic system, homeostasis system, immune
system, and muscular system.
Discussion
The contributions of this study are multiple fold.
First, we developed and applied an ontology-based
SciMiner literature mining approach, which was then
used to mine the FDA TAC 2017 dataset. It is a huge
challenge to identify all ADRs using textual descrip-
tion of ADR case reports. Our MedDRA/OAE-based
SciMiner literature mining approach was successfully
used to mine the FDA TAC 2017 dataset with a spe-
cial focus on 53 neuropathy-inducing drugs (NIDs).
Our study demonstrates the important role of the
MedDRA controlled terminology and ontologies (e.g.,
ChEBI, OAE, and ODNAE) in the literature mining
Table 1 Identified ADRs from 53 NIDs drug labels
Color highlight was used to visualize difference among the number of ADRs
across NIDs
Hur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 6 of 10
and further ADR analysis. Second, we constructed an
ADR-ADR network and applied centrality analysis to
identify the hub ADRs in the network. Third, among
the 53 NIDs, our ChEBI-based analysis found three
benzimidazole drugs, which formed a drug class effect
on 43 ADRs. An OAE analysis of these ADRs further
identified many enriched ADR categories. Based on the
results, we can hypothesize that the proton-pump inhib-
ition role, common to all the three benzimidazole drugs,
might participate in different pathways leading to these
ADRs. To our knowledge, our study represents the first of
such literature mining-derived ontology-based drug class
effect analysis.
The present study is based on a subset of US FDA
drug labels, which was included in the 2017 Text
Analysis Conference (TAC) Adverse Drug Reaction
Extraction from Drug Labels track. We used this data
set as a proof of concept as well as to develop a proto-
type version of ADR-SciMiner. We assumed that if an
ADR is mentioned in the file of a drug, it is associated
with the drug. However, it is likely that the ADR occurs
within a negation or speculation statement such as
depression was not observed as an ADR of the drug or
depression might be an ADR of the drug. Therefore,
more semantic oriented NLP analysis techniques may be
developed to identify whether an ADR is really associ-
ated with a drug or not.
To identify the most salient ADRs associated with
NIDs, we created ADR-ADR networks both specific
to NIDs and non-NIDs using a threshold of 50% for
association. In other words, two ADRs were
connected by an edge, if they co-occur in at least
50% of the NIDs or non-NIDs. Six of the central
ADRs in the NID specific network were also in-
cluded in the non-NID specific network, showing
that these are prevalent and commonly occur to-
gether both in NID and non-NID cases. The other
ADRs in Table 2 are central only in the NID associ-
ated network, which might reveal that they are more
NID specific. As future work, we plan to extend the
network analysis by including the specific drugs to
the network as well and creating bipartite drug-ADR
networks. The types of relations between drugs and
ADRs can be identified by using the Interaction Net-
work Ontology (INO) [24].
Our study identified three benzimidazole drugs (i.e.
lansoprazole, pantoprazole, and omeprazole) that
induce similar profiles of ADRs. Overall these three
drugs have been found safe in terms of their associ-
ated ADR reports [3335]. For example, a previous
study with 10,008 users of lansoprazole in daily prac-
tice indicated that the most frequently reported lanso-
prazole ADRs were diarrhoea, headache, nausea, skin
disorders, dizziness, and generalized abdominal pain/
cramps, but no evidence of rare ADRs were found
[33]. Current study found many ADRs associated with
each of these three drugs, and all these three drugs
are associated with 43 ADRs, commonly behavioral
and neurological, digestive, muscular, and skin ADRs.
A common reason for stopping pantoprazole usage
was found to be the diarrhea ADR [34], which is also
listed as one of the 43 ADRs.
Fig. 3 NID associated ADR network. Two ADRs are connected by an edge if they co-occur in over 50% of the NIDs. Node sizes are proportional
to the degrees of the nodes. Edge thickness corresponds to the number of drugs having two ADRs
Hur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 7 of 10
A previous study suggested that these three drugs have
similar profiles to interact with other drugs (most com-
monly vitamin K antagonist), suggesting a class effect
[36]. According to the ODNAE records [14], lansopra-
zole, omeprazole, and pantoprazole are all associated
with neuropathy adverse events. Our study found 43
AEs commonly shared with these three benzimidazole
drugs. Interestingly, many of these AEs are also found to
be the hubs of the highly enriched NID network from
our literature mining data centrality analysis. It is likely
that these three benzimidazole drugs, which function as
proton-pump inhibitors, use the same or similar path-
ways to induce neuropathy adverse events.
It is noted that the ontology-based drug class ef-
fect study is novel in many aspects compared to its
original report [15]. First, compared to the previous
report using the drug package insert information,
our study uses the data generated from literature
mining of FDA provided case report data. Second,
given the large size of AE data for each vaccine, we
were able to identify many AEs commonly used by a
class of drugs, in our case, 43 AEs associated with
the three benzimidazole drugs. Our OAE-based ana-
lysis was able to further identify the common pat-
terns among these AEs. Such a high throughput
study was not reported in the previous package in-
sert document-based studies.
The ADR identification performance is not yet op-
timal and there is still much room for improvement.
The majority of falsely identified ADR terms by
SciMiner could be grouped into three types: (1)
incorrect mapping of acronyms to ADRs (e.g., all, as
in all patients, mapped to acute lymphocytic leukae-
mia); (2) ADR that may not be caused by the
current drug (e.g., caution is needed in patients with
diabetes); and (3) ADRs that occur as discontinuous
entities in text (e.g., corneal ulceration is an ADR,
but does not occur as a continuous text fragment in
corneal exposure and ulceration). Integration of
other dictionaries such as SNOMED CT [37] into
ADR-SciMiner will be explored to possibly expand
the ADR dictionary thus to improve the recall. Iden-
tifying whether a term is an acronym for an ADR or
not, determining whether an ADR that occurs in a
drug label is really caused by that drug, and detect-
ing ADRs that occur as discontinuous text fragments
Table 2 The centrality scores of the ADRs in the NID specific
ADR-ADR network
ADR Degree Eigenvector
nausea 27 0.311
headache 26 0.310
vomiting 23 0.301
diarrhoea 23 0.301
pruritus 19 0.270
dizziness 16 0.245
pyrexia 14 0.231
rash 14 0.231
thrombocytopenia 14 0.228
nervousness 13 0.222
asthenia 13 0.214
acute lymphocytic leukaemia 10 0.187
decreased appetite 10 0.177
insomnia 8 0.149
depression 8 0.148
urticaria 7 0.139
hypersensitivity 7 0.138
leukopenia 7 0.137
abdominal pain 6 0.122
dyspepsia 6 0.118
constipation 6 0.114
neuropathy peripheral 5 0.102
seizure 4 0.086
somnolence 4 0.086
paraesthesia 2 0.044
myalgia 2 0.044
arthralgia 2 0.041
alopecia 1 0.022
hyperhidrosis 1 0.022
Two centrality measures (degree and eigenvector) were calculated using
Cytoscape app CentiScaPe
Fig. 4 Identification of three benzimidazole drugs associated with
neuropathy adverse events. The three drugs were grouped by ChEBI
under the benzimidazoles chemical group. The hierarchical structure
of the benzimidazoles chemical group is also laid out
Hur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 8 of 10
in text require deeper semantic understanding of the
sentences by considering the context information
(i.e., the surrounding words) of an ADR in text. Our
current method is a dictionary and rule-based
method, which does not consider the context of an
ADR occurrence in text. These challenges can be
tackled by using machine learning methods with fea-
tures that capture context information and utilize
the syntactic analysis of the sentences such as their
dependency parses.
As future work, we plan to develop machine learning
based methods to improve the accuracy of ADR tagging
as well as the detection of the associations between
ADRs and drugs. We will also extend our approach to
include all available structured drug labels in the
DailyMed database, maintained by National Institute of
Health. DailyMed currently contains listings of 95,513
drugs submitted to the US FDA, about 28,000 of which
are prescription drugs for human. Our ontological study
of NIDs will be extended using this larger drug label
dataset.
Conclusions
In this study we developed an MedDRA and
ontology-based SciMiner literature mining pipeline, ap-
plied the pipeline to mine a FDA text set for ADRs asso-
ciated with neuropathy-inducing drugs, performed
centrality network analysis, and drug class effect studies.
Our approach identified scientific insights regarding
these drug-specific ADRs. Our study demonstrates the
feasibility of using ontology-based literature mining,
network analysis, and drug class effect classification to
efficiently identify and study specific drugs and their
associated ADRs.
Abbreviations
ADR: Adverse Drug Reaction; ChEBI: Chemical Entities of Biological Interest;
CONDL: Centrality and Ontology-based Network Discovery using Literature
data; INO: Interaction Network Ontology; MedDRA: Medical Dictionary for
Regulatory Activities; NID: Neuropathy Inducing Drug; NLP: Natural Language
Processing; OAE: Ontology of Adverse Events; ODNAE: Ontology of Drug
Neuropathy Adverse Events; PCR: Proportional Class level Ratio; TAC: Text
Analysis Conference; VO: Vaccine Ontology
Acknowledgements
The authors thank the participants of the 6th International Workshop on
Vaccine and Drug Ontology Studies (VDOS) 2017 for their valuable feedback.
Funding
This work was partially supported by the BAGEP Award of the Science
Academy (to AO).
Availability of data and materials
All data generated or analyzed during this study are included in this
published article.
Authors contributions
JH developed the ontology-based literature mining pipeline and applied the
pipeline to generate literature mining results. AO performed the centrality-
based network analysis. YH performed ontology-based result analyses. JH,
AO, and YH all participated in the project design, result interpretation, and
manuscript writing. All authors read and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Fig. 5 Hierarchical display of 43 ADRs associated with three benzimidazoles drugs. The OAE IDs corresponding to the 43 ADRs were identified,
and Ontofox was used to these terms and their associated hierarchical terms using the IncludeComputedIntermediate condition
Hur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 9 of 10
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Department of Department of Biomedical Sciences, University of North
Dakota School of Medicine and Health Sciences, Grand Forks, ND 58202,
USA. 2Department of Computer Engineering, Bogazici University, 34342
Istanbul, Turkey. 3Unit for Laboratory Animal Medicine, Department of
Microbiology and Immunology, University of Michigan Medical School, Ann
Arbor, MI 48109, USA. 4Department of Microbiology and Immunology,
University of Michigan Medical School, Ann Arbor, MI 48109, USA. 5Center for
Computational Medicine and Bioinformatics, University of Michigan Medical
School, Ann Arbor, MI 48109, USA. 6Comprehensive Cancer Center, University
of Michigan Medical School, Ann Arbor, MI 48109, USA.
Received: 6 February 2018 Accepted: 18 May 2018
Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 
https://doi.org/10.1186/s13326-018-0178-9
RESEARCH Open Access
Matching biomedical ontologies based
on formal concept analysis
Mengyi Zhao1,2, Songmao Zhang1*, Weizhuo Li1,2 and Guowei Chen1,2
Abstract
Background: The goal of ontology matching is to identify correspondences between entities from different yet
overlapping ontologies so as to facilitate semantic integration, reuse and interoperability. As a well developed
mathematical model for analyzing individuals and structuring concepts, Formal Concept Analysis (FCA) has been
applied to ontology matching (OM) tasks since the beginning of OM research, whereas ontological knowledge
exploited in FCA-based methods is limited. This motivates the study in this paper, i.e., to empower FCA with as much
as ontological knowledge as possible for identifying mappings across ontologies.
Methods: We propose a method based on Formal Concept Analysis to identify and validate mappings across
ontologies, including one-to-one mappings, complex mappings and correspondences between object properties.
Our method, called FCA-Map, incrementally generates a total of five types of formal contexts and extracts mappings
from the lattices derived. First, the token-based formal context describes how class names, labels and synonyms share
lexical tokens, leading to lexical mappings (anchors) across ontologies. Second, the relation-based formal context
describes how classes are in taxonomic, partonomic and disjoint relationships with the anchors, leading to positive
and negative structural evidence for validating the lexical matching. Third, the positive relation-based context can be
used to discover structural mappings. Afterwards, the property-based formal context describes how object properties
are used in axioms to connect anchor classes across ontologies, leading to property mappings. Last, the
restriction-based formal context describes co-occurrence of classes across ontologies in anonymous ancestors of
anchors, from which extended structural mappings and complex mappings can be identified.
Results: Evaluation on the Anatomy, the Large Biomedical Ontologies, and the Disease and Phenotype track of the
2016 Ontology Alignment Evaluation Initiative campaign demonstrates the effectiveness of FCA-Map and its
competitiveness with the top-ranked systems. FCA-Map can achieve a better balance between precision and recall for
large-scale domain ontologies through constructing multiple FCA structures, whereas it performs unsatisfactorily for
smaller-sized ontologies with less lexical and semantic expressions.
Conclusions: Compared with other FCA-based OM systems, the study in this paper is more comprehensive as an
attempt to push the envelope of the Formal Concept Analysis formalism in ontology matching tasks. Five types of
formal contexts are constructed incrementally, and their derived concept lattices are used to cluster the
commonalities among classes at lexical and structural level, respectively. Experiments on large, real-world domain
ontologies show promising results and reveal the power of FCA.
Keywords: Ontology matching, Formal concept analysis, Concept lattice
*Correspondence: smzhang@math.ac.cn
1MADIS, Academy of Mathematics and Systems Science, Chinese Academy of
Sciences, Beijing, Peoples Republic of China
Full list of author information is available at the end of the article
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 2 of 27
Background
Ontologies aim to model domain conceptualizations so
that applications built upon them can interoperate with
each other by sharing the same meanings. Such knowl-
edge sharing and reuse can be severely hindered by the
fact that ontologies for the same domain are often devel-
oped for various purposes, differing in coverage, granular-
ity, naming, structure and many other aspects. Ontology
matching (OM) techniques aim to alleviate the hetero-
geneity by identifying correspondences across ontologies.
Ontology matching can be performed at the element level
and the structure level [1]. The former considers ontology
classes and their instances independently, such as string-
based and language-based techniques, whereas the latter
exploits relations among entities, including graph-based
and taxonomy-based techniques. Most ontology match-
ing systems [28] adopt both element and structure level
techniques to achieve better performance.
Life sciences is one of the most successful applica-
tion areas of the Semantic Web technology, and many
biomedical ontologies have been developed and utilized
in real-world applications. These ontologies cover dif-
ferent yet overlapping domains and are often of large
scale, including, for example, the Foundational Model of
Anatomy (FMA) [9] and Adult Mouse Anatomy (MA)
[10] for anatomy, National Cancer Institute Thesaurus
(NCI) [11] for disease, and Systematized Nomenclature of
Medicine-Clinical Terms (SNOMED-CT) [12] for clinical
medicine. Moreover, efforts such as the Unified Medical
Language System (UMLS) [13] integrate various biomed-
ical systems so as to enhance their reuse and interoper-
ability. For such biomedical domain ontologies, the annual
Ontology Evaluation Alignment Initiative (OAEI) [14] sets
three competition tracks, the Anatomy, the Large Biomed-
ical Ontologies, and the Disease and Phenotype, which
have attracted many state-of-the-art ontology matching
systems [24, 7, 8] to challenge.
Among the first batch of OM algorithms and tools pro-
posed in the early 2000s, FCA-Merge [15] distinguished
in using the Formal Concept Analysis (FCA) formalism
to derive mappings from classes sharing textual docu-
ments as their individuals. Proposed by Rudolf Wille [16],
FCA is a well developed mathematical model for ana-
lyzing individuals and structuring concepts. FCA starts
with a formal context consisting of a set of objects, a
set of attributes, and their binary relations. Concept lat-
tice, or Galois lattice, can be computed based on formal
context, where each node represents a formal concept
composed of a subset of objects (extent) with their com-
mon attributes (intent). The extent and the intent of a
formal concept uniquely determine each other in the lat-
tice. Moreover, the lattice represents a concept hierarchy
where one formal concept becomes sub-concept of the
other if its objects are contained in the latter.
Both ontologies and FCA aim at modeling concepts
in hierarchical structures. The purpose of an ontology is
to represent a shared understanding of the domain of
interest [17] that can be queried and reasoned upon in an
automated way. On the other hand, FCA is a conceptual
clustering technique with solidmathematical foundations,
allowing to derive concept hierarchies from datasets.
Ontologies and FCA can complement each other, as ana-
lyzed in [18] from an application point of view. FCA can
naturally be applied to constructing ontologies in ontol-
ogy engineering [1921], and is also widely used in data
analysis, information retrieval, and knowledge discovery.
Following the steps of FCA-Merge, several OM sys-
tems continued to use FCA as well as its alternative for-
malisms, exploiting different entities as the sets of objects
and attributes for constructing formal contexts [2226].
FCA-OntMerge [23], for example, utilizes the classes of
ontologies and their attributes to form its formal con-
text, whereas in [22] the formal context is composed of
ontology classes as objects and terms of a domain-specific
thesaurus as attributes. Different types of formal contexts
decide the information used for ontology matching, and
we observed that some intrinsic and essential knowledge
of ontology has not been involved yet, including both tex-
tual information within classes (e.g., class labels and syn-
onyms) and relationships among classes (e.g., ISA, sibling,
disjointedness relations, and properties and axioms).
This motivated the study in this paper, i.e., empow-
ering FCA with as much as ontological information as
possible for identifying and validating mappings across
ontologies. Our method, called FCA-Map, incrementally
generates a total of five types of formal contexts and
extracts mappings from the lattices derived. First, the
token-based formal context describes how class names,
labels and synonyms share lexical tokens, leading to lex-
ical mappings (anchors) across ontologies. Second, the
relation-based formal context describes how classes are
in taxonomic, partonomic and disjoint relationships with
the anchors, leading to positive and negative structural
evidence for validating the lexical matching. Third, after
conflict repairing, the positive relation-based context can
be used to discover structural mappings. Afterwards,
the property-based formal context describes how object
properties are used in axioms to connect anchor classes
across ontologies, leading to property mappings. Last, the
restriction-based formal context describes co-occurrence
of classes across ontologies in anonymous ancestors
of anchors, from which extended structural one-to-one
mappings and complex mappings can be identified.
We participated in the three OAEI 2016 tracks related
to the biomedical domain, and the results demonstrate
the effectiveness of FCA-Map and its competitiveness
with the OAEI top-ranked OM systems. FCA-Map is
one of the three winners of the Disease and Phenotype
Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 3 of 27
track of the OAEI 2016 campaign. Our method is suit-
able for aligning large-scale domain ontologies with rich
lexical and structural knowledge, due to a comprehen-
sive construction of multiple FCA structures using names,
hierarchies, properties, and axioms. This requires that
ontologies provide meaningful lexical symbols and terms
for classes, deep taxonomic hierarchies, and a large num-
ber of classes and expressive logical axioms specifying
restrictions on properties linking classes. Such conditions
can be satisfied by many ontologies in the biomedi-
cal domain, for which FCA-Map is effective and suc-
ceeds in discovering mappings that are missed by other
OM systems.
The rest of the paper is organized as follows. We first
introduce the basic definitions and characteristics of FCA.
An overview of the FCA-Map method is presented, fol-
lowed by five sections describing the five types of formal
contexts and the derivation of mappings in detail. The
evaluation section presents a comprehensive group of
experiments, including the respective empirical results of
the five steps as well as step-wise comparisons with coun-
terparts. The evaluation also includes comparisons with
OAEI 2016 top-ranked systems and previous FCA-based
OM systems. Finally, we analyze in-depth the advantages
and limitations of FCA-Map in contrast with other OM
systems and FCA-based systems, and discuss the future
work, followed by a conclusion.
Preliminaries
Formal Concept Analysis (FCA) is a mathematical theory
of data analysis based on applied lattice and order the-
ory. FCA constructs formal contexts for objects and their
attributes, and then derives concept hierarchical struc-
tures which constitute lattices. Formal context is defined
as a triple K := (G,M, I), where G is a set of objects, M
a set of attributes, and I a binary relation between G and
M in which gIm holds, i.e., (g,m) ? I, reads: object g has
attribute m [27]. Formal contexts are often illustrated in
binary tables, as exemplified by Table 1, where rows cor-
respond to objects, columns to attributes, and a cell is
marked with × if the object in its row has the attribute
in its column. In Table 1, the marked cell represents that
the animal listed in the row possesses the corresponding
feature in the column.
Table 1 An example formal contextKe
Vertebrate Mammal Flying Aquatic Carnivorous
Elephant × ×
Dolphin × × × ×
Porpoise × × × ×
Hawk × × ×
Octopus × ×
Definition 1 [27] For subsets of objects and attributes
A ? G and B ? M, derivation operators are defined as
follows:
A? = {m ? M | gIm for all g ? A}
B? = {g ? G | gIm for all m ? B}
A? denotes the set of attributes common to the objects
in A; B? denotes the set of objects which have all the
attributes in B.
A formal concept of contextK is a pair (A,B) consisting
of extent A ? G and intent B ? M such that A = B? and
B = A?. B(K) denotes the set of all formal concepts of
contextK. The partial order relation, namely subconcept-
superconcept-relation, is defined as:
(A1,B1) ? (A2,B2) :? A1 ? A2(? B1 ? B2)
Relation ? is called a hierarchical order of formal con-
cepts. B(K) ordered in this way is exactly a complete
lattice, called the concept lattice and denoted byB(K).
For an object g ? G, its object concept ? g := ({g}??, {g}?)
is the smallest concept in B(K) whose extent contains
g. In other words, object g can generate formal concept
? g. Symmetrically, for an attribute m ? M, its attribute
concept ?m := ({m}?, {m}??) is the greatest concept in
B(K) whose intent contains m. In other words, attribute
m can generate formal concept ?m. For a formal concept
(A,B), its simplified extent (simplified intent), denoted by
Kex (Kin), is a minimal description of the concept. Each
object (attribute) in Kex (Kin) can generate the formal con-
cept (A,B). As a matter of fact, Kex dose not occur in any
descendant of (A,B) in B(K) and Kin dose not occur in
any ancestor of (A,B) inB(K). Figure 1 shows the concept
lattice of context Ke in Table 1. In the concept lattice dia-
grams in this paper, each node represents a formal concept
labeled by its simplified intent and simplified extent, the
latter being given in italics. A line connecting two nodes
represents that the lower formal concept is a subconcept
of the upper concept. The node at the top represents
suprema whose extent is the set of all objects, whereas the
node at the bottom is infima whose intent is the set of all
attributes.
Methods
Given two ontologies, FCA-Map builds formal contexts
and uses the derived concept lattices to cluster the com-
monalities among ontology entities including classes and
object properties, at lexical level and structural level,
respectively. Concretely, FCA-Map performs step-by-step
as follows, where a total of five types of contexts are
constructed.
Step 1 Acquiring anchors lexically. Based on class
names, labels and synonyms, the token-based
formal context is constructed, and from its
Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 4 of 27
Fig. 1 Concept latticeB(Ke) with simplified labeling for the example formal context in Table 1
derived concept lattice, a group of lexical
mappings between classes across ontologies can
be extracted, called lexical anchorsA0class.
Step 2 Validating anchors structurally. Based on
A0class, ISA and PART-OF hierarchies and
disjointness axioms, the relation-based formal
context is constructed, and from its derived
concept lattice, positive and negative structural
evidence of anchors can be extracted. Moreover,
an enhanced alignmentA1class without any
conflicts among anchors is obtained.
Step 3 Discovering structural matches. Based onA1class
and ISA and PART-OF hierarchies, the positive
relation-based formal context is constructed, and
from its derived concept lattice, structural
matches among classes across ontologies can be
identified, augmentingA1class to alignmentA2class.
Step 4 Acquiring property mappings. Based onA2class
and axioms specifying that object properties hold
between instances of class mappings, the
property-based formal context is constructed, and
from its derived concept lattice, a group of
mappings among properties across ontologies
Aproperty can be extracted.
Step 5 Identifying extended and complex mappings.
Based onAproperty,A2class and axioms specifying
restrictions on how to use properties with respect
to classes, the restriction-based formal context is
constructed, and from its derived concept lattice,
extended structural mappings among classes
across ontologiesA3class can be extracted,
including one-to-one mappings and complex
mappings where a class is identified to correspond
to a semantic expression composed of classes and
properties in another ontology.
To illustrate every step of FCA-Map, we use parts of
fourmatching tasks from the Anatomy track and the Large
Biomedical Ontologies track of OAEI 2016, shown in
Table 2, as running examples in the subsequent sections.
MA, NCI, FMA, and SNOMED-CT are all real-world,
biomedical ontologies and the versions used are the OWL
files provided by OAEI. These matching tasks use small
fragments of the corresponding ontologies, whose pro-
portions are listed in Table 2.
Constructing the token-based formal context to
acquire lexical anchors
Most OM systems rely on lexical matching as initiation
due to the fact that classes sharing names across ontolo-
gies quite likely represent the same entity in the domain of
interest. FCA-Map, rather than using lexical and linguis-
tic analyzing techniques, generates a formal context at the
lexical level and obtains mappings from the lattice derived
from the context. Concretely, names of ontology classes
as well as their labels and synonyms, when available,
are exploited after normalization that includes inflection,
tokenization, stop word elimination1, and punctuation
elimination. The token-based formal context for ontology
matching is defined as follows.
Definition 2 The token-based formal context for ontol-
ogy matching is a triple Klex := (Glex,Mlex, Ilex), where
objects Glex is the set of strings each corresponding to a
name, label, or synonym of classes in two source ontolo-
gies, attributes Mlex is the set of tokens in these strings, and
binary relation (g,m) ? Ilex holds when string g contains
token m, or a synonym or lexical variation of m.
Table 2 Matching tasks of fragment ontologies of the OAEI 2016
Anatomy track and the Large Biomedical Ontologies track
Matching task Number of classes in O1 Number of classes in O2
MA-NCI 2744 (100% of MA) 3304 (5% of NCI)
FMA-NCI 3696 (5% of FMA) 6488 (10% of NCI)
FMA-SNOMED 10157 (13% of FMA) 13412 (5% of SNOMED)
SNOMED-NCI 51128 (17% of SNOMED) 23958 (36% of NCI)
Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 5 of 27
We use the UMLS Sub-Term Mapping Tools [28] to
access synonyms and the UMLS SPECIALIST Lexicon
[29] to access lexical variations of biomedical terms.
Table 3 shows Klex of a small part of MA and NCI,
and its derived concept lattice is displayed in Fig. 2. For
each formal concept derived, in addition to the strings
in its extent, we are also interested in the classes that
these strings come from, and we call them class-origin
extent. For example, in Fig. 2, the class-origin extent
of formal concept by node 8 is {MA:mammary gland
fluid/secretion, NCI:Breast Fluid or Secretion} since in
NCI, Mammary Gland Fluids and Secretions is a syn-
onym of class NCI:Breast Fluid or Secretion.
An essential feature of FCA is the duality between a
set of objects and their attributes. The more attributes
demanded, the fewer objects can meet the requirements.
In the case of the token-based formal concept, the more
common tokens occurring in its intent, the fewer strings
the extent contains, and the more possibly for the classes
in class-origin extent to bematched. This is to say that car-
dinality of the extent can reflect how similar the strings
are, thus classes from different source ontologies in a
smaller-sized class-origin extent can be considered as a
mapping with higher confidence. Practically, we restrict
our attention to formal concepts whose simplified extent
or class-origin extent contains exactly two strings or
classes across ontologies, and extract two types of lexi-
cal anchors, namely Type I anchor for the exact match,
and Type II anchor for the partial match, respectively. On
the other hand, note that cardinality of the intent cannot
be used to measure the similarity of strings. For example,
MA:nerve and NCI:Nerve, which is a match, share only
one token, whereas MA:left lung respiratory bronchiole
and NCI:Right Lung Respiratory Bronchiole, not a match,
share three tokens.
Type I anchor. Simplified extent Kex of the for-
mal concept contains exactly two strings from classes
across ontologies. This indicates that the two strings
are composed of the same or synonymous tokens, thus
the corresponding classes are extracted to be a match,
as exemplified by ?MA:mammary gland fluid/secretion,
NCI:Breast Fluid or Secretion? through formal concept of
node 8 in Fig. 2 whose Kex has two strings, one from MA
and the other NCI.
Type II anchor. The class-origin extent of the formal
concept contains exactly two classes across ontologies
and simplified extent Kex contains strings from at most
one source ontology. Here the strings share tokens in
the intent rather than composed of the same or syn-
onymous tokens. For example, ?MA:adrenal gland zona
fasciculata, NCI:Fasciculata Zone? is extracted from node
3 in Fig. 2, due to the common token fasciculata which
exists solely in these two classes. And ?MA:palatine gland,
NCI:Palatine Salivary Gland? is identified as an anchor
from node 7, due to the common tokens palatine and
gland which co-exist solely in these two classes.
Constructing the relation-based formal context to
validate lexical anchors
Structural relationships of ontologies are exploited to val-
idate the matches obtained at the lexical level. One of our
previous studies [30] proposed using positive and nega-
tive structural evidence among anchors for the purpose
of validation. More precisely, classes of one anchor shar-
ing relationships to classes in another anchor can be seen
as their respective positive evidence. On the other hand,
negative structural evidence refers to the conflict based on
the disjointedness relationships between classes. In FCA-
Map, we build the relation-based formal context, defined
as follows, to obtain both positive and negative structural
evidence for lexical anchors. Specifically, we exploit the
taxonomic, partonomic and disjoint relationships which
are common in biomedical ontologies. Both explicitly rep-
resented and inferred semantic relations are used in our
method.
Definition 3 The relation-based formal context for
ontology matching is a tripleKrel := (Grel,Mrel, Irel), where
objects Grel is the set of all classes in two source ontologies,
and attributes Mlex is the lexical anchors prefixed with
Table 3 Token-based formal contextKlex of a small part of MA and NCI
Gland Adrenal Zona Zone Fasciculata Reticularis Salivary Palatine Mammary Secretion Fluid
MA:palatine gland × ×
MA:adrenal gland zona fasciculata × × × ×
MA:adrenal gland zona reticularis × × × ×
MA:mammary gland fluid/secretion × × × ×
NCI:Palatine Salivary Gland × × ×
NCI:Fasciculata Zone × ×
NCI:Reticularis Zone × ×
NCI:Mammary Gland Fluids and Secretions × × × ×
Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 6 of 27
Fig. 2 Concept lattice with simplified labeling derived fromKlex in Table 3
four kinds of relationships, i.e., ISA, SIBLING-WITH,
PART-OF, and DISJOINT-WITH, labeled by (ISA),
(SIB), (PAT), and (I-D) (or (D-I)), respectively.
Binary relation (g,m) ? Irel holds if g in its ontology
has the relationship ISA, SIBLING-WITH, PART-OF, or
DISJOINT-WITH (as in the prefix of m) with the class in
anchor m.
The relation-based formal context Krel of a small part
of MA and NCI is displayed in Table 4. For instance,
MA:periodontal ligament and NCI:Periodontium are sub-
classes of MA:ligament and NCI:Ligament, respectively,
thus (MA:periodontal ligament, (ISA)?MA:ligament,
NCI:Ligament?) ? Irel and (NCI:Periodontium, (ISA)
?MA: ligament, NCI:Ligament?) ? Irel hold. Moreover,
MA:adipose tissue is a subclass of MA:organ system
whereas NCI:Adipose Tissue is disjoint with NCI:Organ
System, thus (MA:adipose tissue, (I-D)?MA:organ system,
NCI:Organ system?) ? Irel and (NCI:Adipose Tissue,
(I-D)?MA:organ system, NCI:Organ system?) ? Irel hold.
The derived concept lattice Krel of a small part of MA
and NCI is illustrated in Fig. 3. Formal concepts whose
extents include both classes in an anchor indicate struc-
tural evidence, defined as follows.
Table 4 Relation-based formal contextKrel of a small part of MA and NCI
(ISA)
?MA:ligament,
NCI:Ligament?
(I-D)
?MA:organ system,
NCI:Organ System?
(SIB)
?MA:adipose tissue,
NCI:Adipose Tissue?
(SIB)
?MA:larynx ligament,
NCI:Laryngeal Ligament?
(PAT)
?MA:larynx,NCI:Larynx?
MA:ligament × ×
MA:periodontal ligament × × ×
MA:auricular ligament × × ×
MA:adipose tissue ×
MA:larynx ligament × × ×
NCI:Ligament ×
NCI:Periodontium × × ×
NCI:Broad Ligament × × ×
NCI:Adipose Tissue ×
NCI:Laryngeal Ligament × × ×
Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 7 of 27
Fig. 3 Concept lattice ofKrel with simplified labeling
Definition 4 In the derived concept lattice of the
relation-based formal context Krel, if a formal concept
(A,B) satisfies that its extent A includes both classes in the
same anchor a, then for anchors in its intent B with label
(ISA), (SIB) or (PAT) , a is a positive evidence; and for
anchors in its intent B with label (I-D) or (D-I), a is a
negative evidence.
For example, in the extent of node 3 in Fig. 3,
?MA:periodontal ligament, NCI:Periodontium? and
?MA:larynx ligament, NCI:Laryngeal Ligament?, two
anchors acquired lexically, are positive evidences to
anchor ?MA:ligament, NCI:Ligament? with label (ISA)
in the intent, and negative evidences to anchor ?MA:organ
system, NCI:Organ System? with label (I-D). We use
P(a) and N(a) to denote the sets of positive and negative
structural evidence of anchor a, respectively, whose cardi-
nalities are called the support degree and conflict degree of
anchor a. FCA-Map utilizes all the positive evidence sets
P and negative evidence sets N to eliminate incorrect
lexical anchors and retain the correct ones, as follows.
Conflict repairing. The negative evidence leads to con-
flicts among anchors, for which FCA-Map repairs in a
greedy way, i.e., eliminating the conflict-causing anchors
iteratively until N becomes empty. At each iteration,
anchor a having the least negative evidence set, i.e., the
smallest conflict degree, is selected. For every anchor b in
N(a), if conflict degree of b is greater than a, eliminate
b; otherwise, compare the support degree of a and b, and
eliminate the one with smaller support degree.
Anchor screening. Anchors having no positive structural
evidence according to the updated P are either caused by
the structural isolation of classes, or simply mismatches.
FCA-Map screens anchors based on both lexical and
structural evidence, and Type II anchors without positive
evidence are eliminated.
Constructing the positive relation-based formal
context to discover structural matches
After conflict repairing and screening, anchors retained
are those supported both lexically and structurally. Based
on the enhanced alignment, FCA-Map goes further to
build the positive relation-based formal context aim-
ing to identify new, structural mappings. The way posi-
tive relation-based formal context KposRel constructed is
similar to Krel, i,e., using classes in two source ontolo-
gies as object set and anchors prefixed with relationship
labels as attribute set. Concretely, five kinds of relation-
ships are considered, ISA, SUPERCLASS-OF, SIBLING-
WITH, PART-OF, and HAS-PART, where disjointedness
relationship is no longer necessary. For the derived for-
mal concepts, we restrict our attention to those with
classes across ontologies in the simplified extent, and
both one-to-one mappings and complex mappings can
be identified.
One-to-one structural mappings are extracted from
the formal concepts whose simplified extent exactly con-
tains two classes across ontologies. Although most of the
mappings extracted this way have already been identi-
fied at the lexical level, new additional matches emerge,
as exemplified by ?MA:hindlimb bone, NCI:Bone of the
Lower Extremity?.
Complex mappings are traced from the formal con-
cepts whose simplified extents contain more than two
classes from different source ontologies. It means that
these classes share the same structural relationships to
anchors in the intent. Such classes may compose a com-
plex mapping, as elaborated in the following.
Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 8 of 27
1 One-to-group mappings. The simplified extent
contains only one class from one source ontology
and multiple classes from the other source ontology.
For example, MA:inferior suprarenal vein can be
mapped to the group of concepts {NCI:Left
Suprarenal Vein, NCI:Right Suprarenal Vein} as the
three concepts are contained within one simplified
extent that has no more classes. This one-to-group
mapping comes from the difference in granularity
between MA and NCI.
2 Group-to-group mappings. The simplified extent
contains multiple classes from different source
ontologies, respectively. For example, two groups of
concepts {MA: sacral vertebra 1, MA:sacral vertebra
2, MA:sacral vertebra 3, MA:sacral vertebra 4} and
{NCI:S1 Vertebra, NCI:S2 Vertebra, NCI:S3 Vertebra,
NCI:S4 Vertebra, NCI:S5 Vertebra} can be mapped as
these classes are contained in one simplified extent
that has no more classes. This group-to-group
mapping represents the difference between mouse
and human anatomy.
In all the four matching tasks of Table 2, such complex
mappings can be identified, as shown in Table 5, where
the classes within one mapping are of the same type, thus
the logical constructor used in the semantic expressions is
disjunction. Note that no extra operations are needed in
FCA-Map for identifying such complex mappings as they
and the one-to-one mappings are implied similarly in the
formal concepts derived from the positive relation-based
formal context.
Constructing the property-based formal context to
acquire property mappings
Properties across ontologies tend to differ greatly in
names, even for ontologies of the same domain [30].
Thus, we utilize the structural rather than lexical infor-
mation to obtain property mappings. Axioms specifying
what properties are used to link the individuals of anchors
in respective ontologies are the core for identifying the
commonalities among properties.
Definition 5 The property-based formal context for
ontology matching is a triple Kpro := (Gpro,Mpro, Ipro),
where objects Gpro is the set of all object properties in two
source ontologies, and attributes Mpro is the pairs of one-
to-one class mappings. Binary relation (g,m) ? Ipro holds
where m =< (CAi,CBi), (CAj,CBj) >, i 
= j, if axiom
CAi  ?g.CAj or CAi  ?g.CAj (CBi  ?g.CBj or CBi 
?g.CBj) is asserted or can be inferred within one source
ontology.
The property-based formal context Kpro of a small part
of SNOMED and NCI is displayed in Table 6. Take the
second column of Table 6 for example. The two cells
are marked because axioms Benign neoplasm of buccal
mucosa  ?Finding site.Buccal mucosa and Benign Buc-
cal Mucosa Neoplasm  ?Disease Has Primary Anatomic
Table 5 Some one-to-group and group-to-group mappings discovered by the positive relation-based formal contexts
Classes Semantic expressions
MA Inferior suprarenal vein Inferior suprarenal vein
NCI Left Suprarenal Vein, Right Suprarenal Vein (Left Suprarenal Vein unionsq Right Suprarenal Vein)
FMA T helper cell type 1, T helper cell type 2 (T helper cell type 1 unionsq T helper cell type 2)
SNOMED T helper subset 1 cell, T helper subset 2 cell (T helper subset 1 cell unionsq T helper subset 2 cell)
FMA First sacral spinal ganglion, (First sacral spinal ganglion
Second sacral spinal ganglion, unionsq Second sacral spinal ganglion
Third sacral spinal ganglion unionsq Third sacral spinal ganglion)
SNOMED S1 spinal ganglion, (S1 spinal ganglion
S2 spinal ganglion, unionsq S2 spinal ganglion
S3 spinal ganglion unionsq S3 spinal ganglion)
SNOMED Simian foamy virus, (Simian foamy virus
Chimpanzee foamy virus, unionsq Chimpanzee foamy virus
Chimpanzee foamy virus human isolate unionsq Chimpanzee foamy virus human isolate)
NCI Foamy Retrovirus Foamy Retrovirus
SNOMED Malignant teratoma of undescended testis Malignant teratoma of undescended testis
NCI Stage I Immature Testicular Te ratoma, (Stage I Immature Testicular Te ratoma
Stage II Immature Testicular Teratoma unionsq Stage II Immature Testicular Teratoma
Stage III Immature Testicular Teratoma, unionsq Stage III Immature Testicular Teratoma)
Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 9 of 27
Ta
b
le
6
Pr
op
er
ty
-b
as
ed
fo
rm
al
co
nt
ex
tK
pr
o
of
a
sm
al
lp
ar
to
fS
N
O
M
ED
an
d
N
C
I
<
?SN
O
M
ED
:B
en
ig
n
ne
op
la
sm
of
bu
cc
al
m
uc
os
a,
N
C
I:B
en
ig
n
Bu
cc
al
M
uc
os
a
N
eo
pl
as
m
?,
?SN
O
M
ED
:B
uc
ca
lm
uc
os
a,
N
C
I:B
uc
ca
lM
uc
os
a?
>
<
?SN
O
M
ED
:S
yn
ov
io
m
a
be
ni
gn
,N
C
I:B
en
ig
n
Sy
no
vi
al
N
eo
pl
as
m
?,
?SN
O
M
ED
:S
of
t
tis
su
es
,N
C
I:S
of
tT
iss
ue
?>
<
?SN
O
M
ED
:P
ho
co
m
el
ia
of
up
pe
r
lim
b
N
O
S,
N
C
I:P
ho
co
m
el
ia
of
th
e
U
pp
er
Li
m
b?,
?SN
O
M
ED
:U
pp
er
ex
tr
em
ity
pa
rt
,N
C
I:U
pp
er
Ex
tr
em
ity
Pa
rt
?>
<
?SN
O
M
ED
:B
ow
en
oi
d
pa
pu
-
lo
sis
,N
C
I:B
ow
en
oi
d
Pa
pu
lo
sis
?,
?SN
O
M
ED
:H
um
an
pa
pi
llo
m
a
vi
ru
si
nf
ec
tio
n,
N
C
I:H
um
an
Pa
pi
llo
m
a
Vi
ru
sI
nf
ec
tio
n?
>
<
?SN
O
M
ED
:In
su
lin
co
m
a,
N
C
I:I
ns
ul
in
Co
m
a?,
?SN
O
M
ED
:H
yp
og
ly
ce
m
ia
,
N
C
I:H
yp
og
ly
ce
m
ia
?>
<
?SN
O
M
ED
:L
ap
ar
os
co
py
,
N
C
I:L
ap
ar
os
co
py
?,
?SN
O
M
ED
: E
nd
os
co
pe
de
vi
ce
,N
C
I:E
nd
os
co
pe
?>
SN
O
M
ED
:F
in
di
ng
sit
e
×
×
×
SN
O
M
ED
:D
ue
to
×
×
SN
O
M
ED
:U
sin
g
de
vi
ce
×
N
C
I:D
ise
as
e
H
as
Pr
im
ar
y
An
at
om
ic
Si
te
×
×
N
C
I:D
ise
as
e
M
ay
H
av
e
As
so
ci
at
ed
D
ise
as
e
×
Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 10 of 27
Site.BuccalMucosa can be inferred in SNOMED andNCI,
respectively.
The derived concept lattice of Kpro of a small part of
SNOMED and NCI is illustrated in Fig. 4. We can extract
property mappings from the formal concepts whose
extents contain exactly two properties across ontologies.
This means that they are used to connect the same
pairs of mappings. For example, ?SNOMED:Finding site,
NCI:Disease Has Primary Anatomic Site? is extracted from
node 4 in Fig. 4.
Constructing the restriction-based formal context
to acquire extended and complexmappings
With the availability of property mappings, we can start
exploiting anonymous classes in ontologies, i.e., restric-
tions on how to use properties with respect to classes.
An axiom with a named class at the left-hand side and
a restriction at the right-hand actually defines a neces-
sary condition for the class, and the condition becomes
necessary and sufficient in equivalent axioms. When two
classes in an anchor have necessary conditions (restric-
tions) described by the same property, the two classes
specified in the restrictions, i.e., fillers of the property,
could possibly be a match across ontologies. We illus-
trate this by a validated anchor ?SNOMED:Hemangioma
of liver, NCI:Hepatic Hemangioma?. All the anonymous
ancestors of these two classes in SNOMED and NCI,
respectively, are listed in Table 7. They are either asserted
or inferred, as shown in Fig. 5. Since ?SNOMED:Finding
site, NCI:Disease Has Associated Anatomic Site? is a
property mapping, one can see that the fillers of the prop-
erties imply some correspondences across two ontolo-
gies. We pair fillers in anonymous ancestors of the
two classes in anchor, denoted as FP . In the case of
anchor ?SNOMED:Hemangioma of liver, NCI:Hepatic
Hemangioma?, 16 such pairs can be generated. We utilize
these potential matches to construct a FCA formal context
so as to confirm the correct mappings.
Definition 6 The restriction-based formal context for
ontology matching is a triple Kres := (Gres,Mres, Ires),
where objects Gres is the set of all classes in one source
ontology, and attributes Mres is the set of all classes in the
other source ontology. Binary relation (g,m) ? Ires holds
if (g,m) ? FP , where FP denotes the set of pairs (D,E)
from axiom CA  ?g.D (or CA  ?g.D) in one ontology
and axiom CB  ?h.E (or CA  ?h.E) in the other ontol-
ogy where ?CA,CB? is a class mapping and ?g, h? a property
mapping.
Table 8 showsKres of a small part of SNOMED andNCI,
where the gray area corresponds to Table 7. The derived
concept lattice of Kres of a small part of SNOMED and
NCI is illustrated in Fig. 6. Mappings can be extracted
from the formal concepts according to the simplified
extent Kex and simplified intent Kin.
For a formal concept (A,B) with non-empty simpli-
fied intent and simplified extent, Kin represents the
attributes uniquely introduced by (A,B) compared with
all its ancestors in the lattice. Similarly, Kex is the set of
objects uniquely introduced by (A,B) compared with all
its descendants. Hence, Kin and Kex are introduced by
formal concept (A,B) at the same time, in other words,
the objects in Kex specifically embody the attributes in
Kin; and the attributes in Kin describe the most particular
characteristics of the objects in Kex. In the case of the
restriction-based concept lattice, if both Kex and Kin
of a formal concept contain exactly one class, then it
means that these two classes always occur at the same
time as fillers of the same properties in anonymous
ancestors of anchors. They are more likely a match than
other filler pairs in FP that are also present across
the intent and extent of the same formal concept. For
example, node 7 in Fig. 6 represents a formal concept
with intent {NCI:Cardiovascular System, NCI:Heart,
NCI:Epicardium} and extent {SNOMED:Structure
of visceral pericardium, SNOMED:Heart structure,
Fig. 4 Concept lattice ofKpro with simplified labeling
Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 11 of 27
Table 7 Anonymous ancestors of SNOMED:Hemangioma of liver
and NCI:Hepatic Hemangioma
Classes in an anchor Anonymous ancestors
SNOMED:Hemangioma of liver ? Finding site.Structure of cardiovascular
system
? Finding site.Blood vessel structure
? Finding site.Vascular structure of liver
? Finding site.Liver structure
NCI:Hepatic Hemangioma ? Disease has associated anatomic
site.Cardiovascular system
? Disease has associated anatomic
site.Vascular system
? Disease has associated anatomic
Site.Blood vessel
? Disease has associated anatomic
Site.Liver
SNOMED:Structure of cardiovascular system}. Its simpli-
fied intent is {NCI:Epicardium} and its simplified extent
{SNOMED:Structure of visceral pericardium}, indicating
that these two classes are always used as fillers at the same
time, i.e., in the restrictions about the same properties for
the same anchor classes across ontologies. For other pairs
of classes across the intent and extent of node 7 in Fig. 6,
their two classes may occur as fillers at the same time but
not always. Thus ?SNOMED:Structure of visceral peri-
cardiumis,NCI:Epicardium? is extracted to be a match.
Similarly, node 6 in Fig. 6 yields match ?SNOMED:Atrial
structure,NCI:Cardiac Atrium?.
There are formal concepts in the restriction-based lat-
tice that have an empty simplified intent (extent) and a
non-empty simplified extent (intent), indicating the dif-
ference in class hierarchies and expressions of axioms
across two ontologies. Rather than one-to-one mappings,
complex mappings might be implied in such cases. For
example, node 8 in Fig. 6 has an empty Kin whereas its
Kex is {SNOMED:Vascular structure of liver}. Instead of
one class, there may be a complex combination of NCI
classes in the complete intent of node 8 that corresponds
to {SNOMED:Vascular structure of liver}. Under a manual
review, a complex mapping is determined, as illustrated
in Fig. 7.
Results
To evaluate the effectiveness of FCA-Map, we conduct
experiments on three OAEI 2016 biomedical tracks,
the Anatomy, the Large Biomedical Ontologies, and the
Disease and Phenotype. Additionally, we run FCA-Map
on the Conference track to test its performance on a rel-
atively general-purpose domain. The versions used are
the OWL files of the ontologies provided by OAEI 2016,
and the precision, recall and F-measure values listed in
the subsequent subsections are computed based on the
reference alignments provided by OAEI. In the Large
Oliveira and Pesquita Journal of Biomedical Semantics  (2018) 9:1 
DOI 10.1186/s13326-017-0171-8
RESEARCH Open Access
Improving the interoperability of
biomedical ontologies with compound
alignments
Daniela Oliveira1,2* and Catia Pesquita2
Abstract
Background: Ontologies are commonly used to annotate and help process life sciences data. Although their original
goal is to facilitate integration and interoperability among heterogeneous data sources, when these sources are annotated
with distinct ontologies, bridging this gap can be challenging. In the last decade, ontology matching systems have
been evolving and are now capable of producing high-quality mappings for life sciences ontologies, usually limited to
the equivalence between two ontologies. However, life sciences research is becoming increasingly transdisciplinary
and integrative, fostering the need to develop matching strategies that are able to handle multiple ontologies and
more complex relations between their concepts.
Results: Wehavedevelopedontologymatching algorithms that are able to find compoundmappings betweenmultiple
biomedical ontologies, in the form of ternary mappings, finding for instance that aortic valve stenosis(HP:0001650) is
equivalent to the intersection between aortic valve(FMA:7236) and constricted (PATO:0001847). The algorithms
take advantage of search space filtering based on partial mappings between ontology pairs, to be able to handle the
increased computational demands. The evaluation of the algorithms has shown that they are able to produce meaningful
results, with precision in the range of 60-92% for new mappings. The algorithms were also applied to the potential
extension of logical definitions of the OBO and the matching of several plant-related ontologies.
Conclusions: This work is a first step towards finding more complex relations between multiple ontologies. The
evaluation shows that the results produced are significant and that the algorithms could satisfy specific integration
needs.
Keywords: Biomedical ontologies, Ontology alignment, Algorithms
Background
Life sciences research is becoming increasingly integra-
tive, with research areas such as Systems Biology and
Translational Medicine bridging distinct domains to pro-
vide novel insights. The need for data integration across
domains coupled with the massive amounts of data being
produced both by biological and clinical domains poses
new challenges. A common strategy to deal with this
data deluge involves linking the information to ontolo-
gies, making it easier to search through databases and
to develop algorithms to process information. Ontologies
*Correspondence: daniela.oliveira@insight-centre.org
1Insight Centre for Data Analytics, NUI Galway, Galway Business Park, Dangan,
Galway, H91 AEX4, Ireland
Full list of author information is available at the end of the article
have been remarkably successful in the life sciences, espe-
cially in the biomedical domain, where the Gene Ontology
[1] is the most notable success case. BioPortal1, a por-
tal for life sciences ontologies, lists over 400 ontologies
dedicated to diverse domains ranging from molecules to
phenotypes.
However, when data is annotated with different ontolo-
gies, to allow data interoperability the ontologies them-
selves need to become interoperable. This can be achieved
through a process called ontology matching [2], whereby
meaningful links are established between semantically
related concepts. The matching of biomedical ontolo-
gies poses specific computational challenges due to their
large size and vocabulary complexity [3], and also by their
increasing semantic richness in the form of new kinds
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Oliveira and Pesquita Journal of Biomedical Semantics  (2018) 9:1 Page 2 of 13
of relations between classes and complex axioms. These
open challenges have attracted the interest of the com-
munity and spurred the inclusion of specific tracks dedi-
cated to biomedical ontologies in the Ontology Alignment
Evaluation Initiative [4].
Currently, ontology matching techniques and systems
are mostly devoted to finding links between two equiva-
lent entities from two distinct ontologies, but when data
crosses domains, the need arises for matching techniques
that go beyond this and allow linking more than two
ontologies through more complex relations. Compound
ontology matching [5] allows the matching of several
ontologies with distinct but related domains through the
establishment of compoundmappings that involve several
entities. A specific case is the ternary compound mapping
whereby two classes are related to form a class expression
that is then mapped to a third class. For instance, the class
HP:0000337 labelled broad forehead is equivalent to
an axiom obtained by relating the classes PATO:0000600
(increased width) and FMA:63864 (forehead) via an
intersection. Such mappings allow a fuller semantic inte-
gration of multidimensional semantic spaces, supporting
more complex data analysis and knowledge discovery
tasks.
In this paper, we present a set of new algorithms
which are able to create ternary compound alignments for
large biomedical ontologies. The algorithms were evalu-
ated against reference ontology alignments and applied
to potentially extend ontology logical definitions and to
match plant ontologies.
Related work
Ontology matching can be defined as a function f that
returns an alignment between the classes of a pair of
ontologies O and O? [2]. An alignment consists of a
set of correspondences (mappings) between semanti-
cally related entities of different ontologies. This pro-
cess can be extended by using other parameters and
resources, e.g., weights, thresholds, and even external
knowledge. Most ontology matching systems usually
include three main types of components: (1) loading
and pre-processing, where ontology files are loaded and
other procedures are employed such as normalization
of labels; (2) matching, where pairs of mapped ontol-
ogy entities are given a score reflecting their close-
ness; (3) refinement, where the list of mappings is fil-
tered to adhere to quality, cardinality and consistency
requirements among others. Typically, ontology match-
ing corresponds to binary mappings between classes,
properties or instances. However, more complex kinds of
ontology matching that extend the definition have been
proposed.
One of the first steps in this direction was the defini-
tion of complex ontology matching, which is commonly
described as a correspondence between two classes from
two different ontologies, where one of them is a com-
plex concept or property description. It involves only
two ontologies, but each mapping relates to more than
two entities in those ontologies. An example of a com-
plex mapping could be the alignment of the concept
AcceptedPaper in one ontology, to the entity Paper
in a second ontology, which has the associated property
Accepted [6]. Ritze et al. [7] developed a pattern-based
approach to finding these mappings, where they present
correspondence patterns and define matching conditions
for each of them.
The CGLUE [8] and the iMAP systems [9] were both
developed to find complex matches. CGLUE applies a rule
learning process and iMAP uses several searchers, each
considering a meaningful subset of the space, to find com-
plex mappings. They both apply a beam search to control
the search through the space of candidate matches, given
its large size.
Partial matching has also been investigated. Dhom-
bres and Bodenreider [10] employed lexical and logical
approaches to derive partial mappings for theHP ontology
and SNOMED CT.
A related, but more complex approach, is compound
matching [5] which is the process of identifying com-
pound mappings, i.e. matches between class or prop-
erty expressions involving more than two ontologies.
This means that a ternary compound mapping is a
tuple <X,Y,Z,R,M>, where X, Y and Z are classes from
three distinct ontologies, R is a relationship established
between Y and Z to generate a class expression that
is mapped to X via a mapping relation M. The ontol-
ogy to which X belongs is considered to be the source
ontology, and the ontologies that define Y and Z are con-
sidered as the target ontologies. In this particular case,
the relation R is always an intersection (regardless of
any qualifier) and the mapping M an equivalence. The
concept of compound alignment is defined as a set of
mappings between classes from a source ontology Os
and class expressions obtained by combining two other
classes, each belonging to a different target ontology
Ot1 and Ot2.
To the best of our knowledge, there are currently no
ontology matching systems capable of generating such
mappings. However, a preliminary approach was tested
by [5] that first matched the source ontology to each
of the target ontologies individually, using an anchor-
based word matching algorithm, and then matched all
pairs of target classes that map individually to the same
source class. Despite the reduced search space, they
could not test their algorithm in larger sets of ontolo-
gies and evaluated only the MP-PATO-CL and MP-
PATO-NBO alignments, obtaining recall values of 30
and 11% respectively, but precision values below 1%.
Oliveira and Pesquita Journal of Biomedical Semantics  (2018) 9:1 Page 3 of 13
This work was the starting point for the develop-
ment of our novel approach (with preliminary results
presented in [11]).
Methods
The design, development and implementation of com-
pound matching algorithms involved three stages: (i) an
exploratory stage, which consisted in a pattern anal-
ysis of a representative set of biomedical ontologies
to devise strategies and explore the challenges of the
development of compound matching algorithms; (ii) the
adaptation and extension of existing classical match-
ing algorithms to compound ontology matching, which
were (iii) implemented in a state-of-the-art ontology
matching system.
Pattern analysis
The first stage of this work had an exploratory nature
and aimed to understand the mappings between source
and target ontologies and to seek new strategies to apply
to ternary compound matching. We used the ontologies
for which we were able to create a reference alignment
for compound matching from logical definitions of OBO
ontologies (see Reference alignments section).
Table 1 presents these biomedical ontologies with the
number of different classes and names (labels and syn-
onyms) that each one had at the time of the download.
Using a source ontology and a single target ontology as
input, several binary alignments were created by apply-
ing AMLs Word Matcher and String Matcher (see the
AgreementMakerLight section). The mappings of those
alignments were manually analysed to uncover the fol-
lowing patterns: (1) addition, where the source or target
class label had one or more extra words; (2) variation,
which had labels with the same number of words, but
one word did not match; (3) combination with map-
pings that combined the previous patterns; and, (4) full
match, which had terms that match completely, but can
sometimes have words in a different order. The reference
alignments were also split into pairs to form binary align-
ments and a manual search for the previously defined
patterns was performed. This search led to the discov-
ery of a new pattern which is the occurrence of synonyms
between the two classes that are being matched. Table 2
shows one example mapping for each of the situations
described.
The analysis of all the alignments led to the conclusion
that the majority of the mappings fit in at least one of the
pattern categories. Most, however, are a combination of
the first two patterns, with the addition pattern being the
more prevalent one (see Table 3).
The mappings that were classified with the addition
pattern are mostly partial mappings, i.e., only some
words matched between the labels of the classes mapped.
Dhombres and Bodenreider [10] worked on a method
to identify partial lexical matches between HP and
SNOMED CT. The authors used existing matching tech-
niques and extended them to find partial mappings.
Their approach identified 7358 partial lexical matches
and 82% of them had an inferred logical mapping. Com-
paring with the 14,535 mappings analysed approximately
82% fit the addition pattern and can be considered a
partial match.
These findings served as the conceptual foundation
for the development of the compound matching algo-
rithms. For instance, the prevalence of theaddition pat-
tern indicated that a bag-of-words approach could be
an efficient solution. The existence of mapped classes
with different word order, however, can change the
meaning of a concept in a class and this situation
would be overlooked by the bag-of-words approach.
This approach would also not directly handle the syn-
onym pattern. Finally, the variation pattern led to the
use of a popular word stemmer, the Snowball stemmer2,
which was applied to the words in each label of all
the classes.
Table 1 Biomedical ontologies downloaded from the OBO Foundry in May 2015 (http://obo.sourceforge.net)
Ontology Acronym Classes Names Reference
Cell type CL 4775 4375 [29]
Foundational model of anatomy FMA 78977 126190 [30]
Gene ontology (biological process domain) GO 43048 276577 [1]
Human phenotype HP 28621 18431 [31]
Mammalian phenotype MP 28643 29592 [32]
Neuro behaviour ontology NBO 116710 1168 [33]
Phenotypic quality PATO 2497 3378 [34]
Uber anatomy ontology UBERON 18322 50713 [35]
Caenorhabditis elegans phenotype WBP 2290 2739 [36]
Oliveira and Pesquita Journal of Biomedical Semantics  (2018) 9:1 Page 4 of 13
Table 2 Examples of the patterns found in a manual analysis of binary alignments
Pattern Source URI and label Target URI and label
Addition WBP:0001911 axon regeneration defective GO:0031103 axon regeneration
Variation MP:0002269 muscular atrophy GO:0014889 muscle atrophy
Combination MP:0013527 absent conjunctiva goblet cells CL:2000084 conjunctiva goblet cell
Full match MP:0002119 dipsosis NBO:0000541 dipsosis
Synonym HP:0010108 aplasia of the hallux FMA:25047 big toe
None MP:0002229 neurodegeneration GO:0070657 neuromast regeneration
Algorithm implementation
The CompoundMatching algorithm has three main steps:
Step 1 - First-pass recall selection.
The algorithm performs a pairwise mapping of the
labels of the source ontology with the labels of the target
ontology to match first (target 1). Each word is weighted
by its Evidence Content (EC) [12]. The EC is the inverse
logarithm of the frequency of a word and reflects the usage
of that word within the ontology. The similarity is then
calculated by finding the ratio between the sum of the EC
Table 3 Distributions of mappings fitting lexical patterns 1 or 2
Matcher Ontology Addition Variation Size
String Matcher
MP-CL 26 7 34
MP-GO 287 210 501
MP-NBO 354 205 594
MP-UBERON 58 11 71
WBP-GO 182 137 322
HP-FMA 272 23 304
MP-PATO 18 1 29
WBP-PATO 28 2 41
HP-PATO 12 1 25
Word Matcher
MP-CL 4 1 5
MP-GO 32 25 65
MP-NBO 118 44 219
MP-UBERON 42 5 50
WBP-GO 183 33 219
HP-FMA 158 44 252
MP-PATO 33 21 59
WBP-PATO 19 1 25
HP-PATO 6 0 12
Reference
MP-CL 439 12 474
MP-GO 805 83 944
MP-NBO 177 24 219
MP-UBERON 1693 126 1999
WBP-GO 256 39 325
HP-FMA 1691 66 1893
MP-PATO 3096 35 3636
HP-PATO 1710 8 1893
WBP-PATO 302 4 325
Total 12001 1168 14535
Oliveira and Pesquita Journal of Biomedical Semantics  (2018) 9:1 Page 5 of 13
of the words shared by the source label (ls) and the target
1 label (lt1), and the sum of the EC of the words in lt1.
sim1 (ls, lt1) =
?
EC (word ? (ls ? lt1))
?
EC (word ? lt1) (1)
Step 2 - Search space reduction.
The algorithm filters out all mappings with similar-
ity below a given threshold and removes all the source
classes which were not mapped to any target 1 classes. It
also reduces the number of words of the source labels by
removing from themapped classes all the words that had a
match with a word from a target 1 class. Taking the exam-
ple of Aortic valve stenosis (HP:0001650), after matching
HP with FMA, which would capture the mapping for aor-
tic valve (FMA:7236), HPs class label would be reduced
to stenosis.
Step 3 - Longest match precision selection.
For each of the remaining mappings, the algorithm per-
forms a pairwise mapping of the reduced source labels
against the labels of the last target (target 2). In this step,
however, the denominator corresponds to the sum of EC
of the words in the longer label, to ensure a complete
match.
sim2 (ls, lt2) =
?
EC (word ? (ls? ? lt2))
?
EC
(
word ? longest (ls, lt2)
) (2)
The final similarity between the matched labels is com-
puted as the average between the similarities computed in
steps 1 and 2. Mappings with an average below the second
threshold are filtered out.
The resulting alignment is a list of all mappings above
the selected threshold, without any consideration for
cardinality. To ensure proper cardinality, refinement (or
selection) strategies need to be employed. The reference
alignments have a cardinality of 1, meaning that for each
source class there is a single compound mapping. How-
ever, given the potential for conflicts, it was also desirable
to investigate the option of allowing two mappings for the
same source class. To this end, both a top-one and top-two
ranked selectors were implemented.
Both are greedy algorithms that select mappings based
on their similarity. They start by sorting the mappings
in the compound alignment in descending order of their
similarity values. When there are competing mappings
with equal similarity values, the top-one selector chooses
a single mapping taking the one that was sorted as first,
whereas the top-two selector, chooses the two first sorted
mappings.
AgreementMakerLight
The AgreementMakerLight (AML) ontology matching
system [13] focuses on the efficient matching of very large
ontologies and is one of the most successful systems for
aligning ontologies [14]. AML has three main modules:
(1) ontology loading, (2) ontology matching and (3) align-
ment selection and repair. When an ontology is loaded
into AML, a Lexicon is built with all class labels and syn-
onyms. AML has several matchers that explore lexical and
structural information. The selection and repair module
ensures that the final alignment has the desired cardinality
and removes mappings causing logical inconsistencies.
In this work, we adapted the loading module to han-
dle three ontologies. The implementation of our matching
algorithm takes advantage of the data structures AML
builds for its Word Matcher. The Word Matcher uses a
bag-of-words approach and creates a new Lexicon with
every word frequency and EC. The similarity between
classes of different ontologies is then based on a weighted
Jaccard index. We also made use of AMLs selection
strategies, which were adapted to work over compound
mappings.
Evaluation
The compound alignments were evaluated with classifi-
cation metrics automatically against reference alignments
and also manually, to better understand the results and
point towards possible improvements.
Reference alignments
The technique for the construction of the compound ref-
erence alignments used in the evaluation originated from
the work of [5], where ternary compound mappings were
derived from logical definitions of OBO ontologies to be
used as a gold-standard.
Logical definitions are applied to classes and use genus-
differentia constructs of the form X is a G that D, where
X is the defined class, G is the genus and D the differentia.
The genus is a more general class than X and D discrim-
inates instances of X from other instances of G [15]. The
following text shows an example of a logical definition:
id: MP:0000216 ! absent erythroid
progenitor cell
intersection_of: PATO:0002000 ! lacks
all parts of type
intersection_of: inheres_in CL:0000038
! erythroid progenitor cell
OBO ontologies with over 100 logical definitions that
had a class expression intersected by two classes from two
other ontologies were selected (see the example above).
Following these rules, we created six reference align-
ments, which determined the sets of biomedical ontolo-
gies used throughout this work.
Precision, recall and F-measure
The automatic evaluation of the algorithms was per-
formed based on the classification metrics precision,
Oliveira and Pesquita Journal of Biomedical Semantics  (2018) 9:1 Page 6 of 13
recall and F-measure, which are defined in this context as
follows.
Pr(A) = | {m|m ? A ? m ? R} ||A|
Rec(A) = | {m|m ? A ? m ? R} ||R| (3)
F-measure(A) = 2 · Pr(A) · Rec(A)Pr(A) + Rec(A) (4)
where A is an alignment resulting from the algorithms
developed, Pr(A) and Rec(A) are the precision and recall,
respectively, of the alignment. m is a mapping in an
alignment. R is the reference alignment to which A is
compared.
Thresholds
The thresholds used in the evaluation process were
defined through a series of tests aimed to find consistent
values across all six sets of ontologies, which returned the
best metrics.
The evaluation process involved testing the algorithms
with different thresholds for the first and third algorithm
steps and checking which were the two optimal values
to use throughout the evaluation process. These values
had to return good precision or recall but also needed to
have a reasonable runtime with a considerable amount of
mappings found3.
The first-pass recall selection needs to return a high
recall so that the search space is not too narrowed for
the subsequent steps of the algorithm while still providing
good filtering of irrelevant mappings. For this to happen
the first threshold (T1) needs to be low to return a high
recall, at expense of the precision. To determine the best
T1 the compound reference were reduced to binary ref-
erences by removing the second target from each class.
Barros et al. Journal of Biomedical Semantics  (2018) 9:18 
https://doi.org/10.1186/s13326-018-0186-9
RESEARCH Open Access
Disease mentions in airport and hospital
geolocations expose dominance of news
events for disease concerns
Joana M. Barros1* , Jim Duggan2 and Dietrich Rebholz-Schuhmann1,3
Abstract
Background: In recent years, Twitter has been applied to monitor diseases through its facility to monitor users
comments and concerns in real-time. The analysis of tweets for disease mentions should reflect not only user specific
concerns but also disease outbreaks. This requires the use of standard terminological resources and can be focused
on selected geographic locations. In our study, we differentiate between hospital and airport locations to better
distinguish disease outbreaks from background mentions of disease concerns.
Results: Our analysis covers all geolocated tweets over a 6 months time period, uses SNOMED-CT as a standard
medical terminology, and explores language patterns (as well as MetaMap) to identify mentions of diseases in
reference to the geolocation of tweets. Contrary to our expectation, hospital and airport geolocations are not suitable
to collect significant portions of tweets concerned with disease outcomes. Overall, geolocated tweets exposed a large
number of messages commenting on disease-related news articles. Furthermore, the geolocated messages exposed
an over-representation of non-communicable diseases in contrast to infectious diseases.
Conclusions: Our findings suggest that disease mentions on Twitter not only serve the purpose to share personal
statements but also to share concerns about news articles. In particular, our assumption about the relevance of
hospital and airport geolocations for an increased frequency of diseases mentions has not been met. To further
address the linguistic cues, we propose the study of health forums to understand how a change in medium affects the
language applied by the users. Finally, our research on the language use may provide essential clues to distinguish
complementary trends in the use of language in Twitter when analysing health-related topics.
Keywords: Social media, Disease surveillance, SNOMED-CT, MetaMap, Part-of-speech tagging, Geolocation
Background
The increase in life expectancy through better health of
the world population has mainly been achieved through
advancements in the fields of medicine, biology and
microbiology [1]. However, it becomes increasingly cru-
cial to public health research to detect, monitor, treat and
avoid threats to population health [2]. Thus, public health
has benefited from the use of surveillance [3] which has
been crucial for the detection of disease outbreaks and its
counter-actions in our modern information society. This
*Correspondence: joana.barros@insight-centre.org
1Insight Centre for Data Analytics, Data Science Institute, NUI Galway, Lower
Dangan, Galway, Ireland
Full list of author information is available at the end of the article
has become a key issue for public health and has led to the
application of new sources of valuable health information.
Modern sources of data such as search engine queries
[4] and online news [5, 6] can provide near real-time,
government independent information through different
channels, and have been harnessed in the health domain.
In recent years, social media networks have moved into
the focus of the research; this medium fosters the shar-
ing of health-related content (e.g. personal experiences
and opinions), thus, being the preferred platform for shar-
ing information [7]. One of such platforms is Twitter
[8]. This resource is being used by over 310 million
users worldwide [9] who publish their messages to the
public (i.e. tweets) possibly in combination with the
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 2 of 11
location of the individual; thus, it supplies a continu-
ous stream of data useful to monitor public health con-
cerns such as disease spread. Twitter has been exploited
to monitor disease awareness and surveillance [1013],
suggesting its usefulness for evaluating the health state of
a population. The available location information helped
to identify global movement patterns [14] and has
been integrated into specific applications in the health
domain [1517].
Given the richness of this source, we take advantage of
the rapid availability of data, textual features and geoloca-
tion provided from Twitter. We focus on the full range of
illnesses, including infectious and non-communicable dis-
eases, to determine the scope of disease mentions in social
media. The origin of the tweets is given special attention
to contrast hospital geolocated tweets with those from
airports. Furthermore, we address the linguistic cues,
provided by the users, when health is discussed. By com-
paring both infectious and non-infectious diseases, we
hope to discover if language and/or locations features can
be used to uniquely characterise these categories. This
research is based on the hypothesis that large-scale social
media data can provide new insights about the health state
of the population through the analysis of language and
with a focus on location.
Our research is based on the following assumptions:
1 Twitter is a prime news medium where a wide range
of illnesses are discussed. This enables the detection
of different patterns in the discussion of selected
diseases, and as a consequence allows linking of
worldwide events with such disease mentions.
2 Considering the location plays an important role in
determining the relevant health mentions and in
monitoring specific areas for their distribution of
health mentions. As a primary assumption, we
expected that a hospital location would inflate the
number of disease mentions, given the purpose of the
location.
3 Different language styles could be predominant when
communicating different illnesses; knowing the
language patterns could help to identify non-explicit
mentions of a given disease. Furthermore, different
language patterns may be attributed to different
locations.
Related work
In the health domain, there is an increased interest in
the use of social media analytics. The first exploita-
tion of Twitter in this regard was performed by [18]
to improve market predictions based on external infor-
mation, in this case, using the public belief concerning
the likelihood that H1N1 (i.e. swine flu) would turn into
a pandemic. For the case of specific diseases and with
the focus on the health state of the population, Twit-
ter was initially tested for the case of influenza (i.e. flu)
in the areas of surveillance and prediction [12, 1921].
This illness was comprehensively researched due to the
availability of well documented and historic gold stan-
dard data, its seasonality, and its ease in infecting
others [22]. In this case study, more attention was initially
given to specific words (i.e. keywords) or individual words
(i.e. unigrams) present in a tweet to select potentially
relevant messages [12, 1921]. However, further devel-
opments led to machine learning approaches which take
advantage of additional features such as n-grams [11, 23],
regular expressions [13], user behaviour [24], and part
of speech [25] to further filter relevant messages. With
the results achieved for influenza, other diseases such as
Ebola [26], food-borne illnesses [27], respiratory illnesses
[28], and mental health diseases [29], were researched
following comparable methodologies. Geolocation has
been harnessed to focus on specific cities, regions, and
countries [12, 17, 30, 31] and to study disease diffusion
networks [15, 16, 23, 27].
Given this, there is still research to be conducted regard-
ing how the proximity to disease-prone locations influ-
ences Twitter users, especially at a language level. In
addition, these findings could elucidate on what and how
information is shared. There is also a lack of research
regarding the identification of multiple diseases from
tweets, although, this is partially addressed by topic mod-
elling approaches [32, 33].
Following the same principles described above, the
analysis of language could be used to distinguish non-
communicable disease mentions in tweets from infectious
disease mentions. For example, it is expected that users
apply a different language when being concerned about
cancer and food poisoning. So far, Twitter has been used
to analyse specific disease outbreaks, which required to
capture specific mentions of disease. By contrast, we
observe in this study the full spectrum of disease terms
to better analyse the language use of disease mentions on
Twitter for the outbreak or trend development of different
disease types.
Furthermore, we use the geolocation to differentiate
Twitter use for hospital visitors in contrast to airports. In
our primary assumption, we expected that Twitter use in
hospitals is focused on specific disease mentions, whereas
the use of Twitter at airports could form an indicator
for the early detection of communicable diseases. Cer-
tainly, the geolocation restricts the amount of Twitter data
and the use of medical terminology for the identification
of disease mentions may not necessarily reflect the men-
tion of diseases in the daily common language in the use
of Twitter. However, monitoring the full amount of data
published through Twitter should give sufficient input to
analyse the questions addressed above.
Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 3 of 11
Methods
Data
The collected Twitter data amounts to 58751297 tweets
gathered between the 26th of October 2016 and the 27th
of March 2017. This was performed using the Twitter
application program interface (API) [34] by collecting only
tweets containing latitude and longitude coordinates (i.e.
geolocated tweets) and written in English, using the API
language filter. These messages were stored using Mon-
goDB [35] due to its document-oriented construction,
efficiency in querying large quantities of documents and
scalability [36]. To improve the signal-to-noise ratio, we
applied regular expressions to remove job advertisements
(e.g.Were #hiring! Read about our latest #job opening
here: St. Louis Trauma Hospital Seeking Multiple Special-
ties) and predefined location sharing messages (e.g.Im
at Terrabela Zona Sul in Porto Alegre, RS). Subsequently,
the data set was filtered according to the proximity of
tweets to airports (airport collection) and hospitals (hos-
pital collection). We chose an area within a 3 km distance
surrounding the airports, motivated by the large size of an
airport, and a 0.2 km radius surrounding hospitals. The
remaining set of messages constitutes the geolocated col-
lection. The airport coordinates were retrieved from the
OurAirports [37] database; after considering only large
airports the result consisted of 575 locations. Regarding
the hospitals, to gather a large sample we utilise Open
Street Map [38] to automatically collect 77989 locations
worldwide.
With this partition of the data, we can focus on identi-
fying the differences that location poses on the frequency
and language for distinct illnesses. In particular, the tar-
geted locations that constitute disease hot spots due to
their nature. Increases in international travel are raising
concerns regarding travel-associated illnesses [39], and
hospitals are inherently prone to have a high frequency of
sick people.
Disease terms
Clinical terms have been collected from the Systematized
Nomenclature of Medicine  Clinical Terms [40], the ref-
erence source for a comprehensive and precise coverage
of clinical terms. These terms, referred to as disease
terms in this manuscript, have been selected from the
Disease class of the terminology, as depicted in Fig. 1. The
retrieval was performed through the Bioportals API [41]
and, to achieve a broader scope, for each class the corre-
sponding sub-classes and synonyms have been collected.
The synonyms enable the normalisation of the diseases
names, i.e.laymans terms are considered as well as proper
medical terminology. Due to the limits in Twitters mes-
sage length (in characters) and due to the complexity
of specific disease terms, we decided to remove names
which are composed of more than three terms without
Fig. 1 Disease terms retrieval. The disease terms were collected from
the Disease class and its sub-classes. In the example acute idiopathic
thrombocytopenic purpura is a sub-class of idiopathic thrombocytopenic
purpura, and this disease is a sub-class of idiopathic disease
considering determiners, conjunctions and prepositions
(i.e. of, from, the, a, and, to), which were fre-
quently observed in the list of disease terms. The remain-
ing disease terms and synonyms were utilised for the
selection of relevant tweets. The search for the retrieved
terms and synonyms was performed using the complete
set of terms (e.g. muscle atrophy was searched as a strict
term in contrast to combinatorial variants of muscle and
atrophy), followed by a search using the synonym list.
Part-of-speech tagging
Tweets contain a variety of special characters, therefore,
we applied five modifications to increase the perfor-
mance of the pattern identification: (1) the disease term, if
present, was normalised into DISEASE; (2) usernames,
defined by words immediately preceded by the symbol @,
were replaced by @username; (3) URLs were replaced
by URL; (4) the @ symbol was replaced by at when not
succeeded or preceded by a word; and (5) the remaining
punctuation symbols have been removed. Subsequently,
each tweet was tokenized and a Part-of-Speech (POS)
tag was assigned to each token. These steps were per-
formed using the TweetTokenizer and the POS tagger
from Pythons Natural Language Tool Kit (NLTK) pack-
age [42], as well as the Penn Treebank tag set for the
POS tagger. It was decided to focus on POS due to their
ability to provide a general grammatical tag based on a
word definition and its context. NLTK was chosen given
its widespread use and good performance. To produce
the POS patterns, a rule-based approach, exemplified in
Fig. 2, was followed. This approach permitted to focus on
POS tags surrounding the disease term which we hypoth-
esise being related to the semantics of the disease term in
the tweet.
Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 4 of 11
Fig. 2 POS patterns example. Patterns are created around the disease
term. When it is not possible to have an equal number of POS tags on
each side, the pattern stops. For this example, four patterns were
created
Named entity recognition
The disease terminology collected for this research
includes polysemous terms mainly due to the presence
of synonyms for some of the illnesses. Although the con-
sideration of synonyms allows for a more appropriate
representation of the layman language used on Twitter,
it can also lead to the dubious semantics of the terms,
rendering the tweet unusable for monitoring the health
status of the population sample. To address this issue, we
apply Named Entity Recognition (NER) techniques. With
this, we try to classify entities (i.e. disease names is this
case) contained in each tweet. Given that disease terms
are not common entities in NER tools provided by popu-
lar services such as Stanford NER [43] and GATEs TwitIE
[44], we decided to use MetaMap [45]. MetaMap utilises
the unified medical language system (UMLS) metathe-
saurus to identify concepts referenced in the presented
text, the relevance is given through theMetaMap Indexing
(MMI) score which has a maximum score of 1000 cor-
responding to highest relevance. Although specialised to
biomedical text, we chose this tool given its suitability for
the purposed task.
Results and discussion
Data exploration
The retrieval and filtering of the clinical terms amounted
to 21080 disease names and 19813 synonyms. Both
numbers differ due to the lack of synonyms for some
disease terms.
The full analysis of all 58751297 with regards to dis-
ease mentions and geolocation produced the following
result: 242 messages are within the 3 km radius of air-
ports, 132 occur near hospitals, and the remaining 10242
are assigned to the geolocated collection. From the 132
messages within the 0.2 radius from hospitals, 3 are simul-
taneous within 3 km of airports. In total, 10613 poten-
tially relevant tweets have been identified, i.e. containing
a disease mention that could be normalised to the disease
terms.
Given the contrast between the number of hospital
locations and airport locations, the smaller number of
retrieved messages occurring near hospitals suggest that:
(1) the proximity to hospitals does not induce a signifi-
cant number tweets covering disease mentions; and (2)
there seems to be only a small number of users (with
geolocation activated) tweeting near hospitals.
Data statistics
Further analysing the complete set of 10613 tweets, 493
disease terms and synonyms have been identified in the
data set, with 302 present in more than 1 message. As a
first step, we utilised tweets in each collection to deter-
mine the distribution of the disease terms. For the hos-
pital and airport tweets, results are shown in Figs. 3
and 4 where terms with a frequency lower than 1 and 2,
respectively, were omitted. The term distribution for the
geolocated collection is present in Fig. 5. In this case, it
was decided to only show the terms with a frequency
higher or above 30. The decision to omit certain terms
is due to simplicity and improved readability of the
presented figures.
The results show that 21 disease terms are common
to all collections, which includeviral hepatitis, always
sleepy, heart failure, kidney stone, brain damage,
mosquito bite, among others. Knocked out, Lassa
fever, heart disease, substance abuse, mental disor-
der, and culture shock are present in high frequency
in both collections. The term human immunodeficiency
virus (HIV), the most frequent in the airport collec-
tion, is absent in the geolocated and hospital collections.
Upon further inspection, it was found that these messages
were created by an organisation, which raises awareness
of stigma, to share information about HIV related news.
Additionally, we compare the least frequent terms in all
collections. For the geolocated sample, 383 terms appear
in less than 10 messages. In the airport collection, 50
terms have a frequency below 10. The hospital collection
has 55 terms with a frequency lower than 5. Considering
the three samples, multiple myeloma is the only term
common to all. Upon further inspection of the low fre-
quency tweets, it was clear that personal statements were
vastly more common than the presence of news titles,
especially in the geolocated collection. For the airport and
hospital collections this occurs to a lesser extent.
These results suggest that less mentioned terminology,
in our dataset, is correlated with an increase in personal
messages, which are useful to monitor the populations
health, and more frequent term occurrences are inflated
due to the repetitiveness of news article titles. However,
term specificity is another important factor. Disease terms
such as acute laryngopharyngitis, metabolic acidosis,
Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 5 of 11
Fig. 3 Terms distribution for the hospital collection. A total of 55 messages containing terms with a frequency of 1 were omitted for simplicity. In
total, 61 disease terms are present in this collection
segmental vitiligo, and plantar fasciitis use specialised
terminology of which the vast majority of the popula-
tion is unaware. This may suggest the frequency of 1
for these terms. On contrary, terms such as bee sting
and mosquito bite, although appearing in 5 messages,
use informal terminology more characteristic of Twitters
users.
For the second step, we consider all collections as a sin-
gle dataset. For the remaining of the paper, we will focus
on terms with an occurrence frequency above 199; this
subset corresponds to? 50% of the Twitter data collection
hence we believe it to be appropriate for further analysis
and it provides a reasonable amount of messages for each
disease mention. Additionally, we utilised this threshold
given the high volume our data and to guarantee readabil-
ity throughout the paper. All the terms found correspond
to the diseases clinical terminology, i.e. the terms are not
synonyms. The filtering of terms with more than three
words reduced the complexity hence their presence in the
data set, given that we expect simple English to be pref-
erentially used in Twitter. The disease terms range from
clear health-related terms (e.g. heart disease, mental
disorder, brain damage) to clinically less relevant terms
(e.g. knocked out). All messages with a disease term
Fig. 4 Terms distribution for the airport collection. Terms with a frequency of 1 (n = 26) were omitted for simplicity. In total, 29 disease terms are
present
Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 6 of 11
Fig. 5 Terms distribution for the geolocated collection. Due to their high frequency and for simplicity, terms appearing in less than 30 messages
were omitted (n = 432). The total number of disease terms present is 56
have been further analysed to determine the distribution
of terms and the use of language in the tweets. The results
of this analysis are partly presented in Table 1.
Considering the disease terms knocked out and cul-
ture shock, the messages may use the terms in a non-
clinical sense. The first term can be interpreted and often
be used in the sense of elimination (e.g. Novak Djokovic
knocked out of Australian Open by 117th-ranked Denis
Istomin to hand Andy Murray a huge boost URL.), and
culture shock is mostly used as a company name (e.g.
Cool new socks! Made in Chicago USA! at Culture Shock
- Clothing and Records URL). Similarly, cat ear clini-
cally corresponds to a malformation of the inner ear [46];
however, in the tweets, the term is used to express a cloth-
ing item (e.g. I dont like Halloween I just like being able
to wear cat ears again URL). Similar results, although to
a lesser extent, can be identified with brain damage and
cardiac arrest which are used in song names (e.g. Brain
Damage by Pink Floyd is #nowplaying in Veras On The
Drive, Vancouver., #PalmillaBeach pool is #nowplaying
Cardiac Arrest by #BadSuns #cubevenue).
Our analysis and findings reveal that there is a strong
presence and influence of news articles and their distri-
bution on the use of medical language when properly
analysing Twitter feeds. The majority of disease terms
occur frequently in tweets with reference to specific news
articles or explicitly repeat the article title. As a conclu-
sion, newsmedia are the source explicit disease termmen-
tions and their frequency form a systematic bias to disease
mentions and have to be excluded when analysing Twitter
feeds for surveillance. This also shows that Twitter users
give high relevance to news media; this phenomenon
could receive particular relevance when determining the
impact of campaigns (exposed as specific news events)
Table 1 Disease terms message analysis
Message content
Lassa fever The term is only applied in the context of
news-related messages.
Heart disease The term is used to express health concerns in
news reports, raise awareness and for
personal statements.
Substance abuse The messages contain news article titles,
personal tweets and awareness tweets. These
messages also include job advertisements
which were not filtered by the previous steps.
Mental disorder The term is used in personal messages and in
tweets related to awareness.
Eating disorder The messages are related to news stories and
personal opinions.
Kidney disease The term is mostly used in news stories, to a
lesser extent it is applied in personal tweets.
Female genital mutilation All messages correspond to news articles.
Heart failure The majority of the messages correspond
to news articles and job advertisements
unfiltered by the previous steps. The
remaining messages are personal statements
from the users.
Brain damage The messages include news stories and
personal tweets in which the disease term is
applied in a clinical sense. The remaining mes-
sages use a term with a non-clinical meaning.
Spinal cord injury The term is mostly applied in the context
of news-related messages and job
advertisements. The remaining tweets
correspond to personal statements.
Content analysis for knocked out and culture shock is present in text
Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 7 of 11
that target given diseases (e.g. awareness campaigns). Fur-
thermore, a short-term peak in the frequency of a disease
term aligned with the increase in related news articles can
be exploited as an indicator for changes in public con-
cerns, perceptions, and opinions for an illness, or could be
removed as an obvious distraction from the surveillance
analysis.
Geographic distribution
For all 10613 messages (see Fig. 6), we show the world-
wide geographic distribution of the tweets assigned to
each collection. The presence of English written tweets
occurring in countries where English is not the native
language occurs due to the high presence of news arti-
cle titles written in English. With regards to the subset
selected above, we present in Fig. 7 the geographic dis-
tribution of each disease term given the location of the
messages. Overall, a large portion of tweets originates in
the United States of America, since this country has the
highest number of Twitter users [47].
As an exception, messages about Lassa fever origi-
nate from Nigeria and nearby regions, and are entirely
constituted by news related tweets. We use this case
to further explore the correlation between our Twitter
dataset and other sources of health information, in an out-
break context. For this, we gathered past information from
the World Health Organisation (WHO) website [48] and
Google News [49] (using Lassa fever as a search term).
Given the high presence of URLs in our Twitter dataset
we hypothesise that news articles released at a given
time-stamp could be the source of the high frequency of
tweets. We then compared outbreak reports or news with
spikes in the frequency of tweets mentioning Lassa fever.
WHO provided scarce information, including the absence
of reports from October 2016 to January 2017. In addi-
tion, we found no clear association between Google News
and the news mentioned in the tweets. This can be due
to the lack of sources from the major affected countries,
in Google News. Lassa fever is considered endemic in
regions of sub-developed countries where internet access
is not widely available. However, this data can be leveraged
to extract outbreak information from local news sources.
These findings suggest that attributing a location may
reduce the capability of identifying disease outbreaks
mainly relating to the high volume of news media content.
In addition, the presence of news media hinders the con-
tent analysis of Twitter with the goal of monitoring the
health state of a population.
Part of speech
Disease term part of speech
Our POS analysis with regards to the disease mentions
shows that the most frequent POS patterns are singu-
lar nouns (NN, 4390 terms), and singular proper nouns
(NNP, 4201 terms). To a lesser degree, we identified adjec-
tives (JJ, 350 terms), verbs in base form (VB, 339 terms),
and prepositions or subordinating conjunctions (IN, 289
terms). For themajority of the disease terms, the preferen-
tial POS is an NNP or NN, a few exceptions are knocked
out which is assigned an IN, as the second most frequent
term, and kidney disease which is attributed a VB also
as the second most frequent term. Due to the nature of
the disease term, it was expected a high presence of nouns
or proper nouns for selected diseases (e.g. Lassa fever),
hence, the results suggest that the term could be used with
its clinical meaning. Furthermore, in previous sections, we
a
b c
Fig. 6 Geographic distribution for the dataset collections. a Hospital, b Airport, c Geolocated
Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 8 of 11
a b c
d e f
g h i
j k l
Fig. 7 Disease terms geographic distribution. a Knocked out, b Lassa fever, c Heart Disease, d Substance abuse, eMental Disorder, f Culture shock,
g Eating disorder, h Kidney disease, i Female genital mutilation, j Heart failure, k Brain damage, l Spinal cord injury
addressed the high presence of news articles which is a
likely indicator that the term is used in a clinical context.
However, it is applied to give reports regarding the disease
and not to express personal statements from the users.
Part of speech patterns
Considering the POS patterns generated through the pre-
viously mentioned method, overall, the results suggest a
strong presence of NNP. This predominance of nouns has
already been addressed in the literature [50] and it was
linked to the distinct vocabulary used on Twitter. These
findings also suggest a relaxed use of regular grammar
constructs such as the random capitalisation of words.
In Tables 2, 3 and 4 we present a selection of the most
frequent patterns for the hospital, airport and geolocated
collections. In addition to the presence of NNP, the col-
lections also contain IN and coordinating conjunctions
(CC). Focusing on the patterns assigned to each disease,
the strong presence of NNP is again present.
The frequency of each pattern is correlated with the
number of messages related to news articles. These tweets
contain the same news title, thus, the high frequency of
given patterns in disease terms such as Lassa fever and
female genital mutilation. A similar behaviour can be
seen with culture shock, albeit, in this case, it is related
to the standard pattern used for advertising the prod-
uct of a company. In contrast, terms such as substance
abuse, brain damage and spinal cord injury are present
in patterns with similar frequencies. These are similarly
linked to news articles, however, the titles and news stories
contain more diversity and are also repetitive, therefore,
contributing to the frequency of the patterns. In addition,
the results also suggest that similar patterns, within each
disease term, tend to occur with similar frequencies.
Named entity recognition
MetaMap was able to identify disease terms in 3489
tweets from the initial 10618 messages containing the ter-
minology. In the hospital collection, Metamap reached a
score of 4.04 and identified terms in 44 messages from
Table 2 POS patterns for the hospital collection
Frequency
DISEASE,NNP 14
IN,DISEASE,NNP 6
JJ,DISEASE,IN 6
The table presents the top 3 patterns with a frequency higher than 5
Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 9 of 11
Table 3 POS patterns for the airport collection
Frequency
NNP,DISEASE,NNP 36
NNP,NNP,DISEASE,NNP 16
NNP,DISEASE,NNP,NNP 11
IN,DISEASE,CC 10
The table presents the top 4 patterns with a frequency higher than 9
the total of 132 messages. A total of 45 terms such as
eating disorder, brain damage, and substance abuse
were not identified. Considering the airport collection,
MetaMap achieved an average relevance score of 25.49
and was able to identify the terms contained in 183 mes-
sages out of 242 messages. A total of 23 terms were not
identified, these include terms such as bee sting, shell
shock, Lassa fever and bipolar disorder. In the geolo-
cated collection, the average relevance score was 3.36 and
MetaMap was able to identify the terms in 3272 messages
of a total of 10242. Similarly to the airport and hospi-
tal collections, issues regarding the identification of terms
in the tweets were verified. For example, despite repre-
senting the most frequent term knocked out was also
the term most difficult to identify to MetaMap. The same
occurs with Lassa fever and culture shock. Consider-
ing the agreement betweenMetaMaps term identification
and the actual terminology, the score is of 0.62 (the terms
contained in 2044 tweets are representative of the termi-
nology present in the 3272 messages in which MetaMap
can identify a UMLS concept). As an example, we detail
the specific case of the term knocked out. This term is
present in 103 of the 3272 tweets identified by MetaMap,
however, it is never identified by MetaMap as knocked
out; the algorithm identifies other disease terms in these
103 messages.
The results suggest a trade-off between the use of com-
plex and colloquial terminology. Although more frequent
in Twitter, health-related layman terms can pose signif-
icant challenges and require the application of domain-
specific semantic disambiguation tools. In addition, we
suspect the characteristics of the messages (e.g. short
length and possible disregard for grammar rules) may
have increased the difficulty for MetaMaps algorithm.
Table 4 POS patterns for the geolocated collection
Frequency
IN,DISEASE 592
DISEASE,NNP 589
IN,DISEASE,NNP 418
NNP,DISEASE,NNP 401
In this table it is represented the top 4 patterns with a frequency higher than 300
This is further supported by cases where the same ter-
minology was found in some tweets and not in others,
despite being referenced in both.
Conclusion
In this paper, we tested an approach to determine the
presence of linguistic patterns associated with diseases
and we explored the representation value of a geolocated
Twitter sample. We analysed the full body of Twitter
feeds with geolocation over a period of 5 months. Using
SNOMED clinical terms, we verified a higher presence of
non-communicable diseases compared to infectious dis-
eases. The division of the data showed that hospital and
airport locations do not contribute to the increase in the
number of disease mentions, contrary to our expectation.
We also identified questionable interpretations for
selected disease terms, exposing non-medical interpre-
tations of the medical term (e.g. knocked out, culture
shock). The findings originating from our data explo-
ration suggest a high presence of news article titles or
mentions in the messages which indicate that current
events have a strong influence on the diseases frequency.
An example is Lassa fever, the majority of the mes-
sages correspond to news stories originating from regions
affected by an outbreak in late 2016/early 2017. This
finding provides an alternative way to explore the news
content provided by Twitter, mainly through the deter-
mination of the degree of concern of users and through
the gathering of outbreak information from local news
providers, thus, suggesting its utility for disease monitor-
ing. However, our findings also suggest that Twitter is not
only used as a medium to share personal statements but
also to disseminate news articles. Furthermore, it suggests
that users give high relevance and interest to news media.
Regarding the POS tagging, the majority tweets expose
noun or proper noun use of the disease term which cor-
responds to our expectations and previous findings in the
scientific literature. For the POS patterns, the high pres-
ence of news articles may have hindered the identification
of linguistic patterns as the ones identified may solely
relate to the news articles and not individual statements
regarding health conditions. To address this, an additional
step to remove tweets related to news articles could be
implemented to exclusively analyse personal tweets. The
results from the NER suggest that despite being useful
to identify tweets containing the correct clinical termi-
nology and providing semantic disambiguation, further
developments are needed to better handle the unique
style of the Twitter messages. To further address the lin-
guistic cues, we propose the study of health forums to
understand how a change in medium affects the language
applied by the users. Furthermore, these insights can pro-
vide new information on the complexity of language when
discussing health.
Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 10 of 11
Using a collection of more than 58 million tweets, we
used and determined language patterns, and contrasted
the use of Twitter between airport locations (with a larger
number of feeds) against the hospital location (with a
small number of feeds). As an outcome, we determined
that these locations are not suitable for the collection
of significant portions of tweets concerned with disease
outcomes. Additionally, we verified a high presence of dis-
cussion regarding non-communicable diseases.This study
is based on the premise that users utilise Twitter as a
medium to share concerns regarding illnesses and that
hospital and airport locations would be preferential for the
discussion of certain diseases or diseases in general. This
was not verified, in contrast, we found the predominance
and influence of news articles. To closely monitor dis-
ease outbreaks, personal statements mentioning illnesses
or symptoms are desired. However, our findings can also
be applied to measure the degree of concern expressed by
the users, although not strictly indicative of an outbreak
it can be used use to determine if additional public health
measures should be implemented.
Funding
This research has been funded by Science Foundation Ireland (SFI) under
Grant Number SFI/12/RC/2289.
Availability of data andmaterials
Please contact author for data requests.
Authors contributions
JMB designed the study, developed the methodology, collected the data,
performed the analysis, and wrote the manuscript. JD provided general
guidance. DRS contributed for the study design, provided general guidance
and revised the manuscript. All authors read and approved the final
manuscript.
Ethics approval and consent to participate
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Insight Centre for Data Analytics, Data Science Institute, NUI Galway, Lower
Dangan, Galway, Ireland. 2School of Computer Science, NUI Galway, University
Road, Galway, Ireland. 3ZB MED, University Cologne, Gleueler Str. 60, 50931
Cologne, Germany.
Received: 17 August 2017 Accepted: 25 May 2018
Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 
https://doi.org/10.1186/s13326-018-0180-2
RESEARCH Open Access
Querying archetype-based EHRs by
search ontology-based XPath engineering
Stefan Kropf1* , Alexandr Uciteli1, Katrin Schierle2, Peter Krücken2, Kerstin Denecke3 and Heinrich Herre1
Abstract
Background: Legacy data and new structured data can be stored in a standardized format as XML-based EHRs on
XML databases. Querying documents on these databases is crucial for answering research questions. Instead of using
free text searches, that lead to false positive results, the precision can be increased by constraining the search to
certain parts of documents.
Methods: A search ontology-based specification of queries on XML documents defines search concepts and relates
them to parts in the XML document structure. Such query specification method is practically introduced and evaluated
by applying concrete research questions formulated in natural language on a data collection for information retrieval
purposes. The search is performed by search ontology-based XPath engineering that reuses ontologies and XML-related
W3C standards.
Results: The key result is that the specification of research questions can be supported by the usage of search
ontology-based XPath engineering. A deeper recognition of entities and a semantic understanding of the content is
necessary for a further improvement of precision and recall. Key limitation is that the application of the introduced
process requires skills in ontology and software development. In future, the time consuming ontology development
could be overcome by implementing a new clinical role: the clinical ontologist.
Conclusion: The introduced Search Ontology XML extension connects Search Terms to certain parts in XML
documents and enables an ontology-based definition of queries. Search ontology-based XPath engineering can
support research question answering by the specification of complex XPath expressions without deep syntax
knowledge about XPaths.
Keywords: Electronic health records, Medical informatics applications, Search ontology, Information retrieval,
EHR query, Pathology electronic health records, Query engineering
Background
Precise questions on semi-structured medical records
Since clinicians prefer narratives and dictated speech over
rigid entry forms [1], Electronic Health Records (EHRs)
are often stored as free text. This information type is
referred to by the term semi-structured, preassumed the
documents are structured by headers and keywords man-
ually assigned by the physicians. This structure is usually
not technically implemented. Queries on such data can
not be very precise because there is no semantic informa-
tion explicitly available as markup in the free text.
*Correspondence: stefan.kropf@imise.uni-leipzig.de
1Institute for Medical Informatics, Statistics and Epidemiology (IMISE), Leipzig
University, Härtelstraße 16-18, 04107 Leipzig, Germany
Full list of author information is available at the end of the article
In order to specify precise queries on semi-structured
health records, a transformation of semi-structured health
records into Structured EHRs is required as well as meth-
ods for Querying on Structured EHRs.
Structured EHRs
A well written patient history may be a narrative or
structured document.[...] There is a drive to structure
and/or code all clinically relevant information in EHRs to
benefit from computability of information [2]. Not only
machines, also physicians are benefiting by structured
documents, because it seems that having an expectation
of what to find under a certain heading makes for a
faster interpretation of the text [3]. Anyway, there are
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 2 of 14
narrative as well as structured EHRs; and when the physi-
cians structure their information using certain keywords
and headers in the narratives, it is possible to transfer
free text based medical records into standardized and
section-structured XML EHRs [4]. Querying EHRs by
keywords in certain sections requires that the sections
are recognized by Section Boundary Detection (SBD) and
stored in an appropriate format. In previous work [4], we
showed, that such a transfer is possible: A set of pathol-
ogy reports has been automatically transformed into
archetype-based Pathology Electronic Health Records
(PEHRs). The standard openEHR was exploited for this
transformation.
Querying structured EHRs
After the transformation process, queries can be applied
to specific sections instead of the entire document. This
can reduce false positive results. There is a need for an
ontology-based way for the generation of XPath expres-
sions. This method, referred to as search ontology-based
XPath engineering, will be introduced in this work. More
specifically, the suggested approach [5] will be proven
in a real world scenario by real Research Questions
(RQs) on a real data set. One hypothesis of this paper
is: when the PEHRs are structured into sections by SBD
and stored in an XML database, the sections can be used
for Research Question Answering (RQA).
Related work
Related work can be distinguished in EHR Query Lan-
guages on Data Marts, and Ontology-based Queries.
EHR Query Languages on Data Marts Particularly in
health care, secondary use and mining on EHRs is still
challenging [6]. There are already well defined query lan-
guages for archetype based EHRs [7, 8]. These query
languages define an abstract language, which borrows
keywords from StructuredQuery Language (SQL) [9], and
combines them with archetype path expressions, which
are similar to XPaths [10]. Another prominent SQL based
approach is the usage of the i2b2 [11] data mart for query-
ing EHRs. Precondition for that is an Extract Transform
Load (ETL) transformation process into the i2b2 Star
Schema [12].
Ontology-based queries When the data is stored on
a structured relational database, semantic searches can
be applied for answering different kinds of RQs [13].
The PONTE platform [14] enables querying on a global
EHR ontology using SPARQL statements [15]. A simi-
lar approach uses ontology-based mediation and Object
Query Language (OQL) for query formulation [16].
The XOntoRank system [17] enables semantic search
by inferring semantic relationships between the query
keywords and the terms in the documents (based on
domain ontologies like Systematized Nomenclature of
Medicine (SNOMED)). A promising approach is the
SPARQL2XQuery framework [18], which enables both,
transformation between XML and ontologies, and the
query translation of SPARQL to XQuery [19].
Reducing ETL processes All in all, for answering RQs
by structured query languages like SQL or SPARQL
time consuming ETL processes are necessary. In essence,
EHRs have to be transformed into data marts like i2b2
or an ontology for enabling SPARQL. Moreover, the
transformation into data marts or ontologies requires
structured data, but again, many EHRs consist of free
text. We can skip these time consuming processes
when queries are directly applied to PEHRs (using SBD
and XPaths).
Demarcation to Question Answering (QA) systems
Researching QA systems was an early explored research
field in computer science [20]. Nowadays the topic of
semantic QA systems is a comprehensive and active
research field with many different approaches [21].
Nevertheless the approach of this paper can sup-
port experts during RQA by ontology-based query
formulation and query generation, we distance this
approach from general QA systems, because QA sys-
tems directly return answers, rather than documents
containing answers, in response to a natural language
question [22].
Other limitations The category Ontology-based Queries
is promising a higher precision than queries by keywords
in certain sections, because SPARQL queries on OWL
based patient data would be more powerful than XPath
expressions on XML; but a comprehensive and long term
persistence storage of pathology data within semantic web
technologies is only partially solved. A deep semantic
understanding of free text based EHRs is an open research
topic, but in the near future especially the time consuming
manual review process could be supported by methods of
Named Entity Recognition (NER) and ontology extraction
(? Discussion section).
Generally speaking, the approach of this paper is inher-
ent independent from the underlying XML structure
and belongs to the category of Ontology-based Queries.
We suggest the usage of an ontology, which is strongly
bound to the used XML structure for the generation of
XPath expressions. This strong binding on a structure is
only meaningful when standardized XML-based EHRs
are used.
Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 3 of 14
Fig. 1 Use case overview: search ontology-based XPath generation
Approach and paper overview
We consider in this work RQs from the pathology domain
as a concrete example (? M1. Questions by a domain
expert section) which have to be answered by a set
of PEHRs. These PEHRs are stored after applying SBD
to the (free) text on an XML database (? M2. struc
tured PEHRs section). After that, XPath expressions
can address certain parts of the XML documents (?
Querying PEHRs using XPaths section). The devel-
opment of such XPaths is time consuming for domain
experts, but also for computer scientists. We suggest to
use ontologies to support experts for answering RQs by
search ontology-based XPath engineering (? I. SO-based
XPath engineering section) using the Search Ontology
XML extension (SOX). For answering clinical RQs or
for searching similar cases, XPaths can be generated
automatically out of this ontology (? II. Automatic
XPath Generation section), which in turn can be applied
to document corpora on XML database systems.
Figure 1 gives an overview of the idea of this paper. In
the middle of the search process is a domain expert. On
the left hand side of Fig. 1 it is illustrated, that the agent
uses Protégé, the ontology editor of the Stanford Univer-
sity [23] for modeling the query using the SO (? Search
ontology section) and SOX (? Search ontology XML
extension section). On the right hand side of Fig. 1 the
agent interacts with the XML database; by using XPaths
(? Querying PEHRs using XPaths section) the agent
can retrieve relevant XML documents. In summary, focus
of this work is the evaluation of the SOX-approach by try-
ing to support RQA. Themain contribution is a tool which
is able to generate XPaths expressions out of the SOX
(? Search Ontology XML Extenstion XPath Generator
(SOXPathGen) section). The tool is tested on sample
PEHRs files (? Simple Test Files (Pathology Electronic
Health Records section) by applying five real-world RQs
(? Table 1).
Material
M1. Questions by a domain expert
Table 1 lists the questions in Natural Language (NL), that
are asked by a pathologist, which we will try to solve by
applying SOX. In this paper, the Question 1 (Q1) will be
picked as continuous example, which will be referenced in
the following sections. In Q1 the pathologist is interested
in the average flake weight, that occurs when prostate
cancer is diagnosed. More precisely:
(1) Query for answering Q1 in NL (formulated by a
computer scientist)We search for all PEHRs, where in
the Macroscopy section occurs a prostate flake
weight, intersected with all PEHRs where a prostate
cancer diagnosis occurs. These are PEHRs, which
contain certain terms in the Overall
Interpretation section, or they have certain
classification strings in the Typification and
Localisation section. For a better precision,
PEHRs which have blister related terms in Material
have to be excluded.
Q1 is in principle a simple question, but it shows that
processing NL questions is difficult to understand for
humans as well as for machines. Because of that we are
convinced: there is a demand of an ontological-based
query formulation.
M2. structured PEHRs
In this article, we will concentrate on the special domain
of pathology, where a lot of semi-structured information
Table 1 NL description of the queries (? Search
Ontology-based Pathology Questions (OWL) section)
Q Question
Q0a PEHRs which contains T2 as primary tumor classification
and defined phrases of excised skinmaterial
Q1 Prostatic carcinomas are found starting from how many
grams of flake tissue?
Q2 Prostatic carcinomas are found starting from how many
capsules? What influence has the processing method
(with/without remainder)?
Q3 How large are the leiomyomas of the uterus in the entry
material?
Q4 How many lymph node metastasis occur at colon cancer
in stage pT2?
Q5 In how many esophageal biopsies is a barret mucosa found?
Exclude a certain negation expressionb (cave).
aQ0 is only for proofing the concept [5]
bohne Nachweis einer Barrett-Schleimhaut (en: without evidence of barrett
mucosa)
Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 4 of 14
occurs in terms of pathology reports. In fact, pathol-
ogy reports are based on certain section patterns and
section-introducing keywords, like material, macroscopy
or microscopy. We verified manually, that keywords like
Material, Makroskopie or Mikroskopiewere con-
stantly used for section tagging of pathology reports
of the Institute of Pathology of Leipzig. Therefore, the
reports can be section-structured very precisely into an
archetype-based Pathology Patient Information Model
(PPIM) by the application of methods like SBD and
openEHR [4]. As a result of this previous work, 68,583
openEHR-based PEHRs are stored on an XML database,
ready for answering RQs. For a better understanding,
we publish herewith some test files (? Simple Test
Files (Pathology Electronic Health Records) section).
The corresponding XML of one sample PEHR is listed
in Fig. 2.
Methods
Querying PEHRs using XPaths
When EHRs are stored in XML, another query language
is more suitable than classical free text retrieval meth-
ods such as Lucene [24]. XPath expressions are following
the structure of the EHRs and are a W3C standardized
method for addressing parts in XML documents [10]. An
example XPath expression regarding Q1 is shown in Fig. 3.
XPath functions are used for matching the German word
stems. E.g. when florid(\w)* is used as matching
pattern, we will also find any variation like floride or
florides. Of course, irregular words needs to be treated
bymultiple disjunct specifications. For the combination of
words, the expression ([\w]*\s){0,2} can be useful,
which implies that a maximum of two words is allowed to
match the pattern, which is similar to Lucene Proximity
Searches [24].
Ontologies
Top level ontology General Formal Ontology (GFO)
The GFO introduces a top level ontology [25], useful for
conceptual modeling. The GFO classes Concept and
Symbolic_structure and the property has_part
have been reused during the introduction of the SO and
SOX classes and properties (summarized in Fig. 4).
Search ontology The development, management and
reuse of search concepts is a complex task, that can
be supported by the SO [26]. The SO has been devel-
oped to support full text search on documents; it can
be used for Information Retrieval (IR) in any domain
by extending it by the corresponding domain ontol-
ogy. The representation of the knowledge in the SO is
similar to knowledge-based IR, where Hierarchical Con-
cept Graphs (HCGs) constitute hierarchical thesauri as
an useful knowledge representation [27]. In the SO we
distinguish Search_Concepts from Search_Terms,
disaggregating the latter into Simple_Terms and
Composite_Terms. Composite_Terms are made up
of Simple_Terms, related by the Object Property
Fig. 2 Simplified XML-based pathology EHR snippet, containing a specimen, an overall interpretation and a macroscopic findings part
Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 5 of 14
Fig. 3 One simple XPath example
has_part, and Composite_Terms are constrained
by the additional data property max_distance, which
defines the word distance between Simple_Terms,
where max_distance=0 represents, that one word
immediately follows another word. Writing variations,
synonyms, abbreviations as well as term phrases can be
handled by the assignment of multiple labels to the con-
crete individuals of a Simple_Term. The SO is illus-
trated and described in detail in Fig. 5.
Search ontology XML extension We extended already
the SO in a way that allows querying structured data
stored as XML documents [5]. By extending the SO,
XPaths are automatically producible out of the ontology,
which can be executed on XML documents by integrat-
ing them into XSLT or XQueries. The extension of the SO
is summarized in Figs. 4 and 6. On the top level of the
ontology the class XML_Structure was added, which
subclass structure represents the XML structure. Figure 6
shows that Search_Concepts are described_by
Search_Terms. Search_Terms belong to certain
parts in the XML_Structure, linked by the added in
relation. Namespaces and tag names of the XML docu-
ment are defined within the class IRI. For a combination
of multiple Search_Concepts, we enhance the SO by
a new class, the Search_Query (? I.5 Combining
Search_Concepts to Search_Queries section). Further, an
additional annotation property xpath is adhered during
the XPath generation process (? II. Automatic XPath
Generation section).
Fig. 4 SO ? SOX
Engineering and generation process overview
Figure 7 is important for understanding the overall pro-
cess, in which the ontologymethods are used. Prerequisite
for the query engineering is a concrete RQ (M1) and
structured PEHRs (M2), which are stored on an XML
database. The process illustrated in Fig. 7 is described in
the following subsections (I.-IV.).
I. SO-based XPath engineering
The modelling of the queries has to be done manually and
consists of the following sub-steps:
I.1 Defining the XML_Structure
I.2 Understanding and Formalization of the Questions
I.3 Preparing the Search_Terms
I.4 Describing the Search_Concept and linking them
to the XML_Structure
I.5 Combining Search_Concepts to
Search_Queries
The process order is not strict. In practice, it is also
useful to describe the Search_Concept (I.4) before the
definition of the Search_Terms (I.3). Practical query
engineering is a cyclic process (? Refinement circles
section), which will be explained in the following by a
practical example.
I.1 Defining the XML_Structure The definition of the
XML_Structure in a HCG is conditional, because
Search_Termshave to be bound to the XML_Structure
in a later sub-step. Namespace declarations are directly
Fig. 5 Overview search ontology
Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 6 of 14
Fig. 6 Search ontology XML extension
used in the IRI. Figure 8 illustrates the XML_Structure,
which is based on the PEHRs and required for answering
the questions of Table 1.
I.2 Understanding and formalization of the questions
In this preparation step, all questions of Table 1 can be
formalized like suggested in Table 2. Another approach
would be the usage of NL, as long as it is clear and
complete.
I.3 Preparing the Search_Terms Based on the latter
sub-step (Table 2) the Search_Term classes, more
precisely Simple_Terms and Composite_Terms,
were defined. Firstly Simple_Terms classes and
instances were defined; multiple labels can be created,
which can contain regular expressions. Figure 9 illus-
trates the defined Search_Term classes and labels
regarding Q1. After defining the Simple_Terms,
Composite_Terms can be constructed by linking them
to the Simple_Terms by the has_part relation.
I.4 Describing the Search_Concept Search_Concepts
are primitive classes, which are described by the following
someValueFrom restriction:
described_by some (Search_Term and (in
some XML_Structure))
For instance (Q1), to refine a Search_Concept
to a class which represents, that certain adeno-
carcinoma Search_Terms are expected in an
Overall_interpratation section, the following
class description is used.
Adenocarcinoma_in_Interpretation:
described_by some (Adenocarcinoma
and (in some pim:Overall_interpretation/
pim:value/oe:value))
I.5 Combining Search_Concepts to Search_Queries
It became clear during the engineering process of
this practical use case, that an additional concept is
Fig. 7 Overall process overview
Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 7 of 14
Fig. 8 XML_Structure tree
needed for connecting multiple Search_Concepts
together by Boolean expressions. The following class
description represents the combination of multiple
Search_Concepts regarding Q1.
(2) Q1 class description (Boolean connected)
G_Unit_in_Makro and (
Adenocarcinoma_in_Interpretation or
(ICD-O-C-61_in_Localisation and
ICD-O-M-8140/3_in_Typification))
and No_Blister_in_Material
There is an improved readability when we compare (1)
Query for answering Q1 in NL with the latter (2) Q1 class
description.
II. Automatic XPath generation
The latter ontological query engineering yields an
OWL file, that holds all necessary data for the auto-
matic generation of the XPath expressions. During
that generation, each Search_Query concept gets an
XPath annotation. These annotations are generated by
a program fetch, that interprets the class descriptions
and labels by the usage of the Jena API [28]. The
algorithm dissolves each Search_Concept contained
in the Boolean expression of each Search_Query.
When the Search_Concept is described_by a
Simple_Term, a disjunction is generated, that contains
for every instance label of the Simple_Term an XPath
expression; the generation is based on the labels of
the Simple_Term instances and is based on the path
of the referenced XML_Structure node. Otherwise,
when the Search_Concept is described_by a
Composite_Term, a disjunction of a constructed cross
product of the referenced Simple_Terms is generated.
III. Fetching EHR snippets
The generated XPath expressions are integrated in
XQueries, which are applied on an XML database for
Table 2 DL-based-description of the queries
Q Question
Q0a (HE_Shapes in Macroscopy AND
T2_Term in Overall_staging) [5]
Q1 G_Unit inMacroscopy AND ¬ Blister
in Interpretation AND
(Adenocarcinoma in Interpretation OR
(ICD-O-C-61 in Localisation AND
ICD-O-M-8140/3 in Typification))
Q2 (without residual) K_No_Rest inMacroscopy AND
¬ Blister in Interpretation AND (Adenocarcinoma
in Interpretation OR (ICD-O-C-61 in Localisation AND
ICD-O-M-8140/3 in Typification))
AND (ProstateFlake inMacroskopy)
OR ProstateFlake in Interpretation)
Q2 (with residual) K_Rest inMacroscopy AND ¬ Blister
in Interpretation AND
(Adenocarcinoma in Interpretation
OR (ICD-O-C-61 in Localisation AND
ICD-O-M-8140/3 in Typification)) AND
(ProstateFlake inMacroskopy OR
ProstateFlake in Interpretation)
Q3 CM_Unit in Interpretation AND Leiomyom in
Interpretation AND Uterus inMaterial
Q4 (C18 in Localisation or Colon inMaterial) AND T2 in
Overall_staging AND TNM_Sub_pN in staging
Q5 (numerator) BarrettsMucosa in Overall_interpratation AND
NO_Exclusion_Cave in Interpretation
Q5 (denominator) EsohagusBiopsy inMaterial
aQ0 is only for proofing the concept [5]
The in relation was introduced in SOX. X in Y means that at least one instance of the
Search_Term class X (bold) should occur in the section representing class Y
Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 8 of 14
Fig. 9 Class Quest1_ProstateCancerGramCorrelation
Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 9 of 14
retrieving relevant XML snippets. After that, the relevant
PEHR snippets are stored on the local file system, ready
for the manual review.
IV. Manual review
During the manual review process, the retrieved PEHRs
snippets have to be evaluated and interpreted. Ideally after
that step, the initial RQ can be answered. In practice cir-
cles occur, which means that the question has often to be
refined during the manual review.
Results
The main contribution of this work, the introduced
method SO-based XPath engineering, has been evaluated
by the application of the described process by an ontol-
ogist, where five RQs have been processed. Each process
yields interim results, that will be presented in the follow-
ing. Based on these interim results, which are OWLs and
PEHR snippets, a short interpretation of the RQA indi-
cates the practical usefulness of the presented approach.
I. SO-based XPath engineering and automatic XPath
generation
The OWL class descriptions (which relate to Q1) are
verbosely listed in Fig. 10. For a better understanding, we
published the resulting OWL files containing
 the generated XPath expressions for the five RQs (?
Search Ontology-based Pathology Questions
(OWL) section),
 as well as the binary of the XPath generation tool (?
Search Ontology XML Extenstion XPath Generator
(SOXPathGen) section).
II. Fetched PEHR snippets andmanual review
The XPaths have been applied within XQueries for fetch-
ing the relevant PEHR snippets. The second column
of the Table 3 shows the amount of retrieved XML
snippets for each of the five questions. These PEHR
snippets are used for RQA during the manual review,
where each PEHR snippet has to be evaluated to pre-
vent false positives in the query result. After removing
the false positives, the PEHR snippets are ready for the
interpretation.
III. Interpretation
Table 3 summarizes the amount of retrieved PEHRs and
indicates the counts of cases of enumerated content. In
the result set, about ? 64% of the PEHRs contained enu-
meration lists. Moreover, all RQs of Table 1 could been
answered in Table 4. In particular, the amount of results
retrieved for Q1, Q3, and Q5 are useful for answering the
corresponding RQs:
Q1 The average weight of flakes ? 18.26 g seems to be
reasonable.
Q3 Especially the relatively high amount of 93 cases
indicates, that the average maximum diameter of
leiomyomas of ? 2.76 cm could be a plausible answer.
Q5 The high amount of cases indicates, that in about 8 of
10 cases a barret mucosa has been found during an
esophageal biopsy. This value is a characteristic
quality factor, usable for a comparison of clinicians as
well as institutes.
All questions could be better evaluated by a bigger
amount of PEHRs in the database.
Fig. 10 OWL Class Quest1_ProstateCancerGramCorrelation
Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 10 of 14
Table 3 Overview on the evaluation results
Question |PEHR| |PEHR| |PEHR| |PEHR|
(partly) enumerated ECRI false PQCRI false
positives positives
Q0a 12 9 n/ab n/ac
Q1 36 5 1 0
Q2 (without residual) 18 6 0 0
Q2 (with residual) 9 2 0 0
Q3 153 67 1 60
Q4 4 4 n/ab n/ac
Q5 (denominator) 902 632 n/ad n/ac
Q5 (numerator) 756e skipf n/ad n/ac
Sumg 1134 725 2 60
aQ0 is only for proofing the concept [5]
bnot structured by an enumeration list, TNM classification codes are used
cPQCRI can not occur because no units are used in this query
dECRI can not occur because in this type of PEHR the specimen tissue section was
not structured by an enumeration list
enot part of the column sum because Q5 (denominator) contains the Q5
(numerator) records
fevaluation can be skipped because Q5 (denominator) contains already the Q5
(numerator) records
gwithout Q5 (numerator)
In the second column is the amount of the retrieved PEHRs, in the third column is
the amount of numbered content, in the fourth column is the amount of false
positives which occur because of the ECRI, and in the fifth column is the amount of
false positives which occur because of the PQCRI
Discussion
We introduced an extension of the Search Ontology to
support querying XML documents. The SOX approach
can simplify the generation of a big pool of XPath expres-
sions. During the practical evaluation of the approach,
Table 4 Answers of the NL Questions based on the dataset of
68,583 PEHRs, interpretated by the ontologist
Q Answer
Q1 The least weight was 3 g, the maximum weight
was 38 g, were prostate carcinomas have been
found. The average weight was
? 18.26 g , ? ? 10.18 g.
Q2 (without residual) At least 2, at most 26 capsules were took without
rest. In average 9.28 capsules were took,
? ? 4.78 capsules.
Q2 (with residual) At least 6, at most 10 capsules were took with rest.
In average ? 9.55 capsules were took,
? ? 0.15 capsules.
Q3 ? 2.76 cm is the maximum diameter of leiomyomas
in average, ? ? 1.42 cm.
Q4 In four found casesa 0.5metastasis occur at colon
cancer in stage pT2 in average.
Q5 In 83.81% of the esophageal biopsies a barret
mucosa has been found.
a(1/1), (1/1), (0/41), (0/19)
difficulties regarding NL arose, which will be discussed in
the following.
Uncertainty of NLs
Uncertainty of NL questions Q1 can be interpreted in
different ways: (1) The pathologist wants to know the
minimum known flake weight, were prostate carcinoma
could be diagnosed. (2) The pathologist wants to know an
avarage value. (3) The pathologist wants to know a value
range. We solved this uncertainty by offering answers of
all of these variations in Table 4.
Uncertainty in the material During the manual review
process, we recognized a frequent occurrence of certain
types of false positives in the result set: (1) Enumera-
tion Coreference Resolution Issue (ECRI) and (2) Physical
Quantity Coreference Resolution Issue (PQCRI).
(1) ECRI In essence, an enumerated PEHR consists usu-
ally of different material items: mat1, . . . , mati, matn;
and then, the macroscopy section could also have an enu-
meration list mac1, . . . , macj, macn. Imagine we found a
PEHR, where matx contains one related search term (e.g.
adenocarcinoma), andmacy contains e.g. the weight con-
cept. Everything is fine when x = y, which e.g. means
that the weight concept belongs to the adenocarcinoma
material. But when x = y we found a false positive,
RESEARCH Open Access
Using OWL reasoning to support the
generation of novel gene sets for
enrichment analysis
David J. Osumi-Sutherland1*, Enrico Ponta2, Melanie Courtot1, Helen Parkinson1 and Laura Badi2
Abstract
Background: The Gene Ontology (GO) consists of over 40,000 terms for biological processes, cell components and
gene product activities linked into a graph structure by over 90,000 relationships. It has been used to annotate the
functions and cellular locations of several million gene products. The graph structure is used by a variety of tools to
group annotated genes into sets whose products share function or location. These gene sets are widely used to
interpret the results of genomics experiments by assessing which sets are significantly over- or under-represented in
results lists. F Hoffmann-La Roche Ltd. has developed a bespoke, manually maintained controlled vocabulary (RCV) for
use in over-representation analysis. Many terms in this vocabulary group GO terms in novel ways that cannot easily be
derived using the graph structure of the GO. For example, some RCV terms group GO terms by the cell, chemical or
tissue type they refer to. Recent improvements in the content and formal structure of the GO make it possible to use
logical queries in Web Ontology Language (OWL) to automatically map these cross-cutting classifications to sets of
GO terms. We used this approach to automate mapping between RCV and GO, largely replacing the increasingly
unsustainable manual mapping process. We then tested the utility of the resulting groupings for over-representation
analysis.
Results: We successfully mapped 85% of RCV terms to logical OWL definitions and showed that these could be used
to recapitulate and extend manual mappings between RCV terms and the sets of GO terms subsumed by them. We
also show that gene sets derived from the resulting GO terms sets can be used to detect the signatures of cell and
tissue types in whole genome expression data.
Conclusions: The rich formal structure of the GO makes it possible to use reasoning to dynamically generate novel,
biologically relevant groupings of GO terms. GO term groupings generated with this approach can be used in.
over-representation analysis to detect cell and tissue type signatures in whole genome expression data.
Keywords: OWL, EL, gene ontology, GO, gene set enrichment analysis, enrichment, over-representation analysis,
ontology mapping
Background
The Gene Ontology (GO) consists of almost 40,000 terms
and has been used to annotate millions of gene products
to record their subcellular location (e.g., lysosome), their
molecular function (e.g., kinase activity) and their wider
role in cellular, developmental and physiological processes
(e.g., signal transduction) [1]. In its original form, the GO
was conceived of as a directed acyclic graph in which
terms referring to classes are nodes and edges record rela-
tionships between classes including classification (is a)
and partonomy (part of). This graph structure is com-
monly used to group genes annotated with related terms
in user-facing tools such as QuickGO [2] and AmiGO [3]
and to generate gene sets for enrichment (over- represen-
tation) analyses [4] to interpret the results of genomics
experiments. For example, an experiment to assess the
how treatment of liver cells with a particular drug effects
the transcriptome (genome-wide gene expression profile)
in liver cells may result in a list of all genes whose expres-
sion is increased by the drug treatment. The GO graph
* Correspondence: davidos@ebi.ac.uk
1European Bioinformatics Institute (EMBL-EBI), Wellcome Trust Genome
Campus, Cambridge CB10 1SD, UK
Full list of author information is available at the end of the article
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 
DOI 10.1186/s13326-018-0175-z
structure can be used to group all genes in the relevant
genome into sets sharing function or location. One can
then ask which gene sets are statistically over or under-
represented in the gene list compared to the expected
number of of genes from that set in an equivalent length
list generated by random sampling from the set of all
genes in the genome.
In recent years, the GO has developed into a richly
axiomatised formal ontology specified using Web Ontol-
ogy Language (OWL) [5, 6] and defining GO terms with
reference to terms from other ontologies. For example,
the GO now records the chemical participants in over
12,000 processes and functions via axioms referencing
chemical entities defined by the Chemical Entities of
Biological Interest (ChEBI) ontology [7, 8]. Over 8000
GO classes have some direct or indirect logical link to a
term from the Cell Ontology (CL) [9] or the Uber anat-
omy ontology (Uberon) [10]. These record, for example,
the location of cellular components (the acrosome and
its parts are present only in sperm), cell types that are
the sole location of some process (natural killer cell de-
granulation only occurs in natural killer cells), and the
products of developmental processes (bone is a product
of bone morphogen- esis). There are also over 2500
logical axioms recording the functions of cellular
components via links to molecular function and bio-
logical process terms.
When combined with standard OWL reasoning tech-
nologies, this improved axiomatisation opens up new
possibilities for grouping terms and their xannotations
in biologically meaningful and semantically precise ways
that are potentially useful in enrichment analyses. For
example, we can use OWL reasoning queries to group
processes occurring in T-cells or in the pancreas, or
processes involving nitric oxide or collagen fibers.
The results of enrichment analyses using gene sets for
all GO terms can be difficult and slow to interpret due
to high levels of overlap between gene sets. There are a
number of sources of overlap: grouping via class and
part hierarchies means that gene sets derived from anno-
tation to a class subsumes the gene sets of its subclasses
and subparts; one GO class can sit in multiple branches
of the hierarchy; a single gene product may be annotated
to terms in multiple branches. For this reason, many en-
richment analyses rely on a more limited number of
gene sets, corresponding to grouping under a limited
number of high or intermediate level GO terms
commonly referred to as a slim.
Rather than use a slim of GO terms, F. Hoffmann-La
Roche Ltd. (Roche), maintains an internal controlled
vocabulary (referred to hereafter as the RCV) for use in
enrichment analyses. The RCV consists of around 360
terms, each of which is mapped to a set of terms from
the GO. It is tailored to the research interests of Roche,
and its terms were chosen with the aim of achieving
gene set composition descriptive and broad enough to
allow robust and statistically significant results though
not so broad and redundant in composition that it pre-
vents easy interpretation of results. Detecting enrich-
ment to gene products involved in anatomy, organ or
cell-specific processes or components can be critical for
pharmacological research, especially when working with
complex tissues where there is a need to tease apart
events occurring in specific tissue compartments or cell
types. To support this, many RCV terms group GO
terms in ways that are out of scope for classes in the
GO, including groupings of GO terms related to specific
cell, tissue or molecule types.
Here we describe the development and testing of a
dynamic, computable mapping between RCV terms and
the GO that makes use of OWL reasoning. We show
that RCV groupings of GO terms related to specific cell,
and molecule types can be used to identify the transcrip-
tomes of those cell types via enrichment analyses.
Methods
As the RCV is a flat list and includes classifications that
are orthogonal to the classification schemes used by the
GO, it is not amenable to mapping via ontology align-
ment techniques that use ontology structure [11]. Given
the small size of the RCV, it is viable to manually map
each RCV term to an OWL class expression (DL query),
which can then be used in conjunction with an OWL
reasoner to generate lists of GO terms. The RCV does
not include textual definitions to clarify meaning, so for
each RCV term we attempted to find a class expression
(a mapping query) that reflected the intended meaning
of the RCV term, as judged by the RCV term name,
manual mappings and discussion with RCV developers.
Query strategy
We manually mapped each RCV term to an OWL class
expression (a mapping query) and used a standard OWL
reasoner to generate a combined list of classes equiva-
lent to and classes subsumed by the class expression.
We tested classification and query answering using the
GO with imports of CHEBI, CL and Uberon on a
2.9 GHz Intel Core i7 Mac laptop, assigning 10Gb of
RAM to the JVM. Classification with the OWL 2 EL rea-
soner ELK [12] com- pleted in under 6 s and used less
than 4Gb of RAM. Subsequent queries of the classified
ontology took 10-100 ms milliseconds. In contrast, clas-
sification using the HermiT reasoner [13], which
supports OWL 2 DL took ~70 min and used 7.5Gb of
RAM. Subsequent query answering was very slow, with
some test queries timing out. To ensure speed and scal-
ability, we therefore chose to restrict mapping queries to
the EL profile of OWL 2 and use the ELK reasoner. The
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 2 of 10
expressiveness of the GO and of imported ontologies is
almost entirely within the OWL 2 EL profile (the only
exception is a handful of inverse property assertions), so
while some incompleteness in query answering is
possible, we dont expect it to be common.
In order to keep the mapping process simple, only a
single GO, CL, Uberon or ChEBI mapping class was
specified for each mapping query.
To compensate partially for the lack of disjunction
(OR) in OWL-EL, we developed a hierarchy of high level
object properties for use in queries. For example, we de-
fine occurs in OR has participant as a grouping relation
allowing queries for processes that occur in a specified
cell, or have that cell as a participant. Many RCV terms
group processes in which a specified chemical or cell
participates with processes regulating those in which it
participates (see Table 1 for example). To support such
groupings, we used an OWL property chain axiom [5] to
define a relation, regulates o has participant, which can
be used to query for processes that regulate a process in
which some specified entity is a participant. We then de-
fined a super-property, participant OR reg participant,
for this new relation and has participant:
regulates o has participant
. subPropertyOf participant OR reg participant
. . subPropertyOf regulates o has participant
. . subPropertyOf has participant
These new, high-level object properties are difficult to
name in a way that communicates the meanings of map-
ping queries clearly. In order to compensate for this, we
used scripting to generate human readable descriptions
for each mapping query. Compare, for example, the
mapping query for the RCV term cannabinoid with its
description:
Mapping query: participant OR reg participant
some cannabinoid.
Description: A process in which a cannabinoid
participates, or that regulates a process in which a
cannabinoid participates.
Ontologies used
We used a fully expressive release version of the GO [14],
release version 201501-30, supplemented with the bespoke
relations described above (21 relations). This resulting
ontology includes over 40,000 GO classes and over 13,000
imported classes from the Cell Ontology, ChEBI, Uberon,
the Sequence Ontology the Protein ontology and over 130
object properties imported from the OBO Relations
Ontology [15]. The DL expressiveness is SRI. For a sum-
mary of owl entity and axioms counts please see Table 2.
Pipeline
Mapping queries were run using the ELK OWL 2 rea-
soner [12] via calls to the OWL- API [16]. The query
and results processing pipeline was written in Jython
[17]. All code, mapping tables and results were main-
tained in a GitHub repository [18]. The mapping was
specified using a single tab separated values (TSV) file in
which each line maps an RCV term to an OWL-EL
mapping query that includes a term from GO, ChEBI,
CL, Uberon or NCBI taxonomy [19]. Query results were
used to generate a TSV file, allowing direct comparison
of manual and automated mappings (see Table 1 for an
example). We used the GitHub API to generate tickets
for each mapping, linked to the relevant TSV results file,
which GitHub renders as a table. This allowed easy
manual review and editing by RCV curators at Roche
who used the linked tickets to discuss mapping issues
and record the approval status of all mappings.
Mapping queries were selected, tested and the
results reviewed against manual mappings to decide
which patterns were most appropriate. Once a map-
ping query was chosen, corrections and/or additions
to the GO were made where results were wrong or
incomplete. At this point, any clear errors in the
manual mapping we blacklisted. Review of automated
mappings was then passed to Roche who approved or
blacklisted individual classes (see Table 1 for an
example). When satisfied with the results, the corre-
sponding GitHub ticket was closed, thereby indicating
the mapping as approved. Results approved by Roche
Table 1 Results table for RCV cannabinoid
GO name GO ID manual auto checked black listed is obsolete
regulation of endocannabinoid signaling pathway GO 2000124 1 1 1 0 0
cannabinoid signaling pathway GO 0038171 1 1 1 0 0
endocannabinoid signaling pathway GO 0071926 1 0 1 0 0
cannabinoid receptor activity GO 0004949 0 1 1 0 0
cannabinoid biosynthetic process GO 1901696 0 1 1 0 0
The table shows the mapping of an RCV termcannabinoid to a set of GO terms, comparing manual mapping (manual column) with automated mapping (auto column).
The automated mapping results from an OWL query for processes in which a cannabinoid participates, or that regulates a process in which a cannabinoid participates. The
automated mapping found three additional GO terms compared to the manual mapping. In this case, no manually mapped terms were obsolete in GO and all automated
mappings were approved
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 3 of 10
were combined to produce a new RCV mapping table
(available from [20]).
Over-representation analysis
For each RCV term we generated a gene set consisting
of all human genes directly annotated to each mapped
GO term (retrieved from NCBI/entrez [21]). These are
referred to in the following text and figures using the
RCV term name name suffixed with rcv.
We additionally generated 155 gene sets that are
enriched in specific tissue types. These were generated
from three datasets: the Neurocrine Biosciences (NB)
CNS dataset [22], the GNF Gene Expression Atlas [23]
(both based on the Affymetrix microarray technology),
and sequencing-based RNASeq Atlas [24]). The Gini
index was used to identify tissue-enriched genes in each
dataset [25] by selecting genes with Gini coefficient > 0.7
for the specified tissue. The list of tissue specific genes
from each dataset was combined in a non-redundant
tissue signature list. These are referred to in the follow-
ing text and figures by the tissue name suffixed with ts.
Using tissue type expression data from the Genotype-
Tissue Expression project (GTEx) [26], we calculated the
average level of expression (mean reads per kilobase of
transcript per million mapped reads (RPKM) signal)
across all samples available for a given tissue type and
used this to construct a gene vector (a rank order list of
genes by expression level) for each tissue type. We used
the same approach to process a publicly available set of
immune-cell type expression data [27, 28] for enrich-
ment analysis.
For each geneset and each tissue-type or cell-type gene
vector we calculated an enrichment score, defined as the
-log10 (p-Value) of the Wilcoxon test [29] applied using
the entire geneset collection as universe. We present en-
richment by using the resulting Z-scores to generate
heat maps showing over- or under-representation of
each gene set in each tissue, using euclidean distance
clustering to cluster similar results on both the X axis
(GTEx tissue type) and Y axis (gene set). Heat maps are
a standard way to represent this type of analysis, in part
because they make clustering of similar results easily
visible as blocks of similar patterns.
Results
Mapping results
We developed successful mapping queries (owl class ex-
pressions) for 308 out of 364 RCV terms and used OWL
reasoning to find all classes equivalent to or subsumed
by the mapping query for each RCV term (only 72 RCV
classes had equivalent classes as well as subsumed clas-
ses in GO).
Over a third (104) of the mapping queries were sufficient
to recapitulate all manual mappings. A further 40% of the
mappings (148) had 10 or fewer additional manual map-
pings (Fig. 1a) and most of these (114) had fewer than 5.
Mapping queries identified many GO terms that were
not in the manual mapping (Fig. 1b). In some cases (e.g.,
leukocyte activation), over 1000 new mappings were
found. On manual review, only 8 out of several thousand
automated mappings were flagged as unsuitable by Roche,
56 terms were not mapped. Some were judged to be se-
mantically equivalent to other RCV terms. The rest were
rejected as currently not mappable due to the lack of suit-
able terms or axiomatisation within the GO. For example,
RCV has terms for aerobic and anaerobic metabolic pro-
cesses, but the GO currently has no terms for these pro-
cesses, and no axiomatization that allows them to be
queried for. Further axiomatization of the GO is likely to
improve the number of RCV terms that can be mapped.
Improvements to the GO
While the GO has extensive axiomatization linking pro-
cesses to cells, anatomical structures and chemicals, this
is not always complete. In mapping from the RCV to the
GO we found and corrected over 200 omissions in the
axiomatization including links from processes to partici-
pant cell types, anatomical structures and chemicals. Ex-
amples include linking GO process terms referring to
the aggregation of immune cell types such as lympho-
cytes and thymocytes to the relevant cell type terms in
the cell ontology.
We also found and corrected a number of errors, in-
cluding errors in axiomatization of developmental pro-
cesses that led to incorrect inferences for RCV anatomy
terms. For example, we uncovered and fixed errors in
axiomatisation of epidermis development that tangled
together classification of terms referring to animal epi-
dermal structures (e.g. skin and its parts) with those
from plants (such as stomatal guard cells).
Table 2 Ontology metrics: Counts of OWL entity and axioms
types in the ontology used for mapping
entity/axiom type Count
Logical axioms 142,894
Classes 53,799
Object properties 153
SubClassOf axioms 113,104
EquivalentClasses 29,386
DisjointClasses 148
GCI 6910
SubObjectPropertyOf 164
InverseObjectProperties 28
TransitiveObjectProperty 16
ReflexiveObjectProperty 1
SubPropertyChainOf 46
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 4 of 10
Assessing the utility of RCV for over-representation
analysis.
We assessed the ability of gene sets derived from RCV
terms to identify tissue and cell types in enrichment
studies. The RCV contains a number of terms for immune
cell types defined using a standard pattern that groups
terms related to immune cells ina variety of ways. e.g.:
T_cells
Some part of a T cell, or some process in which: a T cell
participates or that occur in a T cell or which results in the
developmental progression of a cell that will form a Tcell.
Using immune cell-type expression profiles from [27, 28]
we calculated how over-or under-represented each RCV
immune cell type gene set was in each cell-type expression
profile. The results are displayed in Fig. 2, with cell-types
(X-axis) clustered according to similarity of enrichment
profile across gene sets. All RCV immune cell gene sets
are highly enriched in whole blood transcriptomes. The
over- representation profiles for RCV immune cell-type
gene sets matched immune cell types. NK cells rcv and B
cells rcv and monocytes rcv were all enriched only in
matching cell-type transcriptomes. CD8 is expressed in T
cells and a subset of Natural Killer cells [30, 31]. Consist-
ent with this, enrichment to the T cells rcv and NK cells
rcv gene sets is seen in CD8 expressing cells. CD4 is
expressed in subset of T-cells [30]. Consistent with this, a
low level of enrichment is seen for the RCV T-cell gene
sets. To test the ability of the RCV to match tissue types
more broadly, we used tissue-type expression profiles
from GTEx [26], a publicly available data set with expres-
sion profiles of 47 different tissues. We used this to com-
pare enrichment to RCV terms to enrichment to a set of
tissue-derived gene sets. The complete results are available
in Additional file 1 as a heat map showing over and under
representation of each gene set (Y-axis) for each GTEx
tissue-type (X-axis), with both axes clustered for similarity.
Distinct enrichment clusters generated by this analysis in-
clude clusters identifying tissues rich in immune cells
(Fig. 3), clusters indentifying brain tissue (Fig. 4 and clus-
ters identifying skin (Additional file 1).
Figure 3 shows enrichment analysis for genesets re-
lated to immune cells. RCV immune cell genesets form
a co-cluster with immune cell enriched genesets - show-
ing over-representation in tissues rich in immune cells
such as blood, lungs and gut mucosa as well as in trans-
formed lymphocytes.
Comparable ts and rcv gene sets have very little over-
lap: all comparisons between equivalent rcv and ts
genesets have a Jaccard index of less than 0.1 (Table 3).
They therefore provide complementary sets of signatures
for detecting the presence of immune cells in samples
for which transcriptomic data is available.
Also included in the cluster are immune-system
related groupings such as immune response rcv, inflam-
mation rcv and leukocyte activation rcv, showing that
the RCV provides a semantically richer picture than sim-
ply detecting cell-types.
Figure 4 shows a similar cluster of enrichment for
brain tissue samples. Gene sets derived from annotation
to processes involving glial cells (glial rcv) and more
specifically astrocytes (astrocyte rcv) are sufficient to
distinguish brain tissue types and nerves from other
tissue types in GTEx. The cluster also contains RCV
gene sets for novel grouping terms defined with re-
spect to molecules (dopamine, cAMP, neurotransmit-
ter) and cell components (synapse) with definitions
following the patterns:
Fig. 1 Summary of mapping results a. Distribution of manual
mappings not found by automated mapping X axis = number of
manual-only mappings. Y axis = Number of RCV terms. Over 80% of
mappings are completely automated or require less than 10 manual
mappings. b. Distribution of automated mappings not present in the
original manual mapping. X axis = number of auto-only mappings.
Y axis = Number of RCV terms. Many new mappings were uncovered
by automation
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 5 of 10
Fig. 2 Use of RCV-derived gene sets to identify immune cell types. RCV-derived gene sets (Y-axis); immune cell type transcriptomes (X-axis); over-
representation is indicated in red; under-representation in blue. Cell-type transcriptomes are clustered based on similarity of enrichment profile
across gene sets
Fig. 3 Comparison of RCV derived gene sets and tissue derived gene sets for identification of immune-cell rich tissues Over-representation of
RCV-derived gene sets (Y-axis) in tissue-type transcriptomes (X-axis) is indicated in red, under-representation in blue. Tissue-type transcriptomes
are clustered based on similarity of enrichment profile across gene sets (X-axis) and gene sets are clustered by similarity of enrichment profile
across tissues (Y-axis). Only the immune-rich tissue cluster of gene sets is shown in this figure. For the fully enrichment analysis please see
Additional file 1
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 6 of 10
Neurotransmitter
A process in which some substance with neurotransmit-
ter biological role participates, or that regulates a
process in which substance with neurotransmitter bio-
logical role participates.
Synapse
A synapse OR part of a synapse OR a process that re-
sults in organisation of a synapse OR that has a synapse
as a participant.
These enrichments make sense given what is known
about the biology of neural tissue, as do a set of RCV
terms that map to conventional GO terms: ion transport;
cell cell signaling, glutamate metabolism and nervous
system development terms.
Discussion
Mapping of RCV terms to set of GO terms is now fully
dynamic, allowing RCV to be automatically kept up to
date as the GO. Where new terms follow mapping query
patterns that are already used, they can be added simply
by specifying an additional line in the mapping file.
48% of mapped RCV terms have 10 or fewer manual
mappings. We are reviewing all of these cases to decide
whether to drop manual mappings or whether complete
automation might be achieved by a different query strat-
egy. In some cases, a more complete mapping could be
achieved by a disjunctive query. For example, all RCV
terms referring to metabolism of some specified
Fig. 4 Comparison of RCV derived gene sets and tissue derived gene sets for identification of brain derived tissues. Over-representation of
RCV-derived gene sets (Y-axis) in tissue-type transcriptomes (X-axis) is indicated in red, under-representation in blue. Tissue-type transcriptomes
are clustered based on similarity of enrichment profile across gene sets (X-axis) and gene sets are clustered by similarity of enrichment profile
across tissues (Y-axis). Only the brain tissue cluster gene sets is shown in this figure. For the full enrichment analysis please see Additional file 1
Table 3 Overlap between cell-specific gene sets derived from
RCV and cell expression data is low
Gene sets Jaccard Index
B cells rcv vs Lymphocyte B FOLL ts 0.064
NK cells rcv vs Lymphocytes NK ts 0.000
T cells rcv vs Lymphocytes T various tsa 0.025a
T helper rcv vs Lymphocytes T H ts 0.032
dendritic cell rcv vs Dendritic cells ts 0.000
granulocyte rcv vs Granulocyte INFL ts 0.082
lymphocyte rcv vs Lymphocytes NK ts 0.071
macrophage rcv vs Macrophage PB ts 0.033
mast cell rcv vs Mast cell PB ts 0.045
Column one lists the two gene sets compared. Column 2 lists the Jaccard similarity
coefficient comparing the two gene sets (0 = no overlap, 1 = full overlap.) aIn the
case of T cells the average of a range of T-cell expression datasets is shown
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 7 of 10
chemical are mapped to GO terms referring to meta-
bolic and transport processes in which the specified
chemical is a participant. (This is consistent with some
medical use of the term metabolism.) A more complete
mapping could be achieved using a disjunctive query
with an OWL2 DL reasoner such as HermiT. This
approach was found to be prohibitively slow but new
generation reasoners such as MoRE [32] which combine
ELK with DL reasoners such as HermiT may turn out to
be useful in this approach. A simple, if potentially in-
complete alternative would be to simply run two EL
mappings using ELK and generate a union of the results.
56 terms were not mapped. Some were rejected from
the pipeline as they were judged to be too close in
meaning to other RCV terms. The rest were rejected as
currently un-mappable due to the lack of suitable terms
or axiomatisation within the GO at this time. For
example, GO currently has no formal way to group
aerobic or anaerobic metabolic processes, although it
does reflect the aerobic or anaerobic nature of many
metabolic processes in their names and textual
definitions.
Making novel groupings of GO terms generally accessible
The approach described here could be used to provide a
view of the GO that groups terms in ways defined with
reference to the complete range of cell-types, chemical
types and anatomical structures referenced by the GO
and all of their ancestor classes. This is already reflected
in some of the newer functionalities of the GO browsing
tool AMIGO, which now displays inferred annotations
to cell-types based on axioms in the GO recording
where processes occur [33]. An extended version of the
GO with extended axiomatisation and imported terms
from external ontologies includin CL, Uberon, ChEBI is
available from [34].
The system described bears some relationship to
TermGenie [35] which is already used to generate 80%
of new GO terms. One possible approach to fulfilling
the needs of external groups for types of classification
not included in the GO would be to offer a TermGenie-
like system to create bespoke terms.
Conclusions
Our work demonstrates how the logical structure of the
GO can be used to achieve biologically meaningful map-
pings between concepts in external controlled vocabu-
laries and corresponding sets of GO terms, even when
there is no concept in the GO that is directly equivalent
the term to be mapped. This is possible as long as the
concept can be mapped to an OWL class expression
referencing classes and relations in the full version of
the GO. These classes may come from the GO, or from
ontologies from which the GO imports classes such as
ChEBI, CL and Uberon. The resulting mapping is
dynamic and so can easily be kept up to date as the GO
evolves.
While OWL 2 DL profile queries could be used for
these mappings, this would make mapping software slow
to run, resource intensive, and may not be sustainable as
the GO becomes still larger and more complex [6]. The
mapping system we describe uses class expressions
restricted to the OWL 2 EL profile to ensure that map-
ping is fast and scaleable. It also demonstrates how
OWL property chains and property hierarchy can be
used to partially overcome the absence of disjunction
(OR) in OWL 2 EL.
The RCV includes many terms that group GO terms
in novel ways by their rela- tionship to some type of cell,
molecule, tissue or cell component. One possible usage
of these terms is to provide a mechanism for detecting
the signatures of particular cell or tissue types in tran-
scriptomic data. We demonstrate the effectiveness of
this by showing how gene sets derived from RCV terms
for cell types can be used to identify specific immune
cell types and how gene sets derived from RCV terms
for cell, molecular and cell component types can be used
to identify tissue types. In some cases, gene sets derived
directly from transcriptomic data may be used for the
same purpose (see Figs 3 and 4). In these cases the RCV
term sets provide an alternative method using gene sets
with very little overlap to those derived from differential
expression (Fig. 3). Unlike gene sets derived from
transcriptomic data, those derived from the GO and its
annotations are not limited by the ability to experimen-
tally isolate suitable biological samples and can include
broad groupings of cell or tissue types that are unlikely
to be isolated together (e.g. all glial cells (Fig. 4), or all
epithelia).
Additional file
Additional file 1: A complete over-representation analysis for RCV gene
sets against GTEx tissue type transcriptomes. The analysis is displayed as
a heat map with RCV on the Y-axis, GTEx on the X-axis, over-respresentation
in blue and under-respresentation in red. Both axes are clustered for similarity
(see Methods for details). (PDF 91 kb)
Acknowledgements
The initial version of this paper was published in the Proceedings of the 8th
International Conference on Semantic Web Applications and Tools for Life
Sciences [36].
Funding
This work was supported by direct funding from F. Hoffmann-La Roche Ltd. The
Gene Ontology Consortium is supported by a P41 grant from the National Human
Genome Research Institute (NHGRI) [grant 5U41HG00227314].
Availability of data and materials
All files used in this mapping (mapping tables, OWL files, pattern documents)
and the resulting mappings are available on a dedicated GitHub repository [20].
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 8 of 10
Authors contributions
DOS designed and implemented the GO to RCV mapping strategy and
contributed to all other aspects of the work described here. LB developed
the original Roche controlled vocabulary and collaboratively developed the
query-based mapping to GO with the DOS. She also devised and executed
out all of the enrichment analysis in this paper. EP manually inspected all
mapping results and provided feedback on their accuracy. MC contributed
to writing the manuscript and to framing and shaping its arguments. HP su-
pervised the work of DOS and contributed to writing the manuscript. All au-
thors read and approved the final manuscript.
Ethics approval and consent to participate
Not applicable
Consent for publication
Not applicable
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1European Bioinformatics Institute (EMBL-EBI), Wellcome Trust Genome
Campus, Cambridge CB10 1SD, UK. 2Roche Pharma Research and Early
Development, Pharmaceutical Sciences, Roche Innovation Center Basel, F.
Hoffmann-La Roche Ltd, Grenzacherstrasse 124, -4070 Basel, CH, Switzerland.
Received: 16 May 2016 Accepted: 3 January 2018
Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 
https://doi.org/10.1186/s13326-018-0191-z
SOFTWARE Open Access
Ontoserver: a syndicated terminology
server
Alejandro Metke-Jimenez* , Jim Steel, David Hansen and Michael Lawley
Abstract
Background: Even though several high-quality clinical terminologies, such as SNOMED CT and LOINC, are readily
available, uptake in clinical systems has been slow and many continue to capture information in plain text or using
custom terminologies. This paper discusses some of the challenges behind this slow uptake and describes a clinical
terminology server implementation that aims to overcome these obstacles and contribute to the widespread
adoption of standardised clinical terminologies.
Results: Ontoserver is a clinical terminology server based on the Fast Health Interoperability Resources (FHIR)
standard. Some of its key features include: out-of-the-box support for SNOMED CT, LOINC and OWL ontologies, such
as the Human Phenotype Ontology (HPO); a fast, prefix-based search algorithm to ensure users can easily find content
and are not discouraged from entering coded data; a syndication mechanism to facilitate keeping terminologies up to
date; and a full implementation of SNOMED CTs Expression Constraint Language (ECL), which enables sophisticated
data analytics.
Conclusions: Ontoserver has been designed to overcome some of the challenges that have hindered adoption of
standardised clinical terminologies and is used in several organisations throughout Australia. Increasing adoption is an
important goal because it will help improve the quality of clinical data, which can lead to better clinical decision
support and ultimately to better patient outcomes.
Keywords: Clinical terminologies, Interoperability, SNOMED CT, FHIR
Background
The problem of sharing and reusing knowledge in soft-
ware systems is common across many domains. In the
area of health there have been several efforts to create clin-
ical ontologies to address this issue, such as SNOMEDCT,
considered the most comprehensive clinical terminol-
ogy currently available, with more than 300,000 medical
concepts. However, building and maintaining clinical ter-
minologies is considered a hard problem [1, 2]. Despite
the availability of SNOMED CT1 and other standardised
clinical terminologies, many clinical information systems
still capture information in plain text or using custom
code lists.
There are several challenges that have hindered the
widespread adoption of clinical terminologies. The
heterogeneity and complexity of specifications results
*Correspondence: alejandro.metke@csiro.au
The Australian e-Health Research Centre, CSIRO, Level 5 UQ Health Sciences
Building, Royal Brisbane and Womens Hospital, QLD 4029 Herston, Australia
in a significant effort for implementors. For exam-
ple, SNOMED CT is distributed in Release Format 2
(RF2) [3], a table-based format that is non-trivial to pro-
cess (the SNOMED CT implementation guide is over 700
pages long). Other clinical terminologies are modelled
in completely different formats. For example, LOINC is
distributed in comma-separated values (CSV) files and
spreadsheets. There are also many ontologies available
natively in OWL format. All of this is compounded by
a lack of expertise in the area, which usually requires
knowledge in a wide range of technologies, including pro-
gramming, description logics and ontology reasoners. A
terminology server should be able to provide access to
any clinical terminology in a standardised manner and
thus shield implementers and end users from all of the
underlying complexity.
Some terminologies include a vast amount of content
which makes finding a specific concept challenging. In
order to drive their adoption, it is of fundamental impor-
tance to provide a search mechanism that is effective and
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 2 of 10
responsive. It should also be possible to define subsets
for specific contexts, for example a subset of concepts for
use in an emergency department, in order to improve the
quality of the results and reduce the search space.
Another important difficulty when dealing with clini-
cal terminologies is keeping them up to date. Accessing
the content of a clinical terminology usually involves an
indexing process that can be time consuming and compu-
tationally expensive. As releases become more frequent,
this can impose a heavy burden on downstream servers.
A terminology server should include a distribution mech-
anism that facilitates keeping clinical terminologies up to
date in a straightforward manner.
Finally, when an existing clinical system does not sup-
port coded data or uses proprietary code lists, updating
it to use a standardised terminology requires a significant
amount of effort. Therefore, stakeholders need to be able
to clearly see the value of such an investment. Data analyt-
ics is one of the benefits of adopting a standardised clinical
terminology that generates significant value. Therefore, a
terminology server should be able to provide advanced
concept querying capabilities.
In this paper we describe Ontoserver [4], a clinical ter-
minology server based on the HL7s Fast Health Interop-
erability Resources (FHIR) standard, which was designed
to overcome the challenges mentioned before and there-
fore contribute to the widespread adoption of clinical
terminology.
Implementation
Ontoserver is implemented as a Java application and its
high level architecture is shown in Fig. 1. There are four
main features that drove the development of this version
of Ontoserver: adopting a terminology service standard,
supporting several key clinical terminologies out of the
box, designing a mechanism to easily keep the terminolo-
gies up to date and providing effective concept search. The
following sections describe the implementation of these
features in detail.
Terminology service standard
Ontoserver was implemented based on the terminol-
ogy subset of the Fast Health Interoperability Resources
(FHIR) standard [5, 6]. Other standards, such as the Com-
mon Terminology Services 2 (CTS2) [7], could have also
been used for this purpose, but FHIR was chosen because
of its developer-focused development approach. Adopt-
ing the FHIR specification allows Ontoserver to provide
a unified API to access any clinical terminology, includ-
ing LOINC, SNOMED CT and any local extensions with
bespoke terms, in a simple and well-defined manner. This
also allows clients to easily switch to other FHIR-based
implementations. The following is a brief overview of the
main resources involved in implementing a FHIR termi-
nology server2.
A code system represents a set of codes from a system.
Each clinical terminology of interest is represented by a
code system resource within a FHIR server. Ontoserver
provides out-of-the-box support for SNOMED CT and
LOINC. It also supports any OWL ontology through an
external transformation service. Users can also create and
upload custom code systems.
The main operations defined by this FHIR resource are
lookup and subsumes. The lookup operation retrieves
details about a concept, such as properties and additional
labels (designations). The subsumes operation determines
what subsumption relationship holds between the spec-
ified codes, if any. More details about the code system
resource can be found in the FHIR documentation avail-
able at http://hl7.org/fhir/codesystem.html.
Fig. 1 High-level architecture of Ontoserver. Ontoserver is a RESTful server that provides three APIs: a FHIR API, implemented using the HAPI FHIR
library, to support terminology functionality, an administration API used to implement functionality that is not defined in the FHIR specification,
such as uploading a SNOMED CT code system in RF2 format, and a syndication API that is used to consume and expose syndicated terminology
resources. It uses a Postgres database to store FHIR resources and a Lucene index to support searching. The application is deployed using Docker
Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 3 of 10
A value set represents a subset of codes drawn from
one or more code systems. A code system usually has a
canonical value set that represents all of its codes. Value
sets can be implicitly or explicitly defined. Implicit value
sets may be defined for a specific code system (such as
SNOMEDCT or LOINC), based on its underlying seman-
tics. Table 1 shows some examples of implicit value sets in
SNOMED CT.
Explicit value sets can be defined by using a combi-
nation of include and exclude statements. Within these,
the users can refer to codes explicitly, by using filters,
or by importing other value sets. The FHIR specifica-
tion defines a set of filters that is common to all code
systems. Each code system can also define filters spe-
cific to it. For example, the specification defines a filter,
constraint, for SNOMED CT that uses the Expression
Constraint Language (ECL), a language developed specif-
ically for SNOMED CT to define subsets of concepts that
satisfy certain criteria [8]. This filter is not used with other
code systems because the ECL is specific to SNOMEDCT.
The expand operation on a value set resolves its mem-
bers. When a value set is defined as a list of codes, the
result of this operation is trivial. However, complex value
sets that use filters or import other value sets need to be
evaluated at runtime to produce the expansion. The other
operation offered by the value set resource is validate-
code which indicates if a code is part of a value set.
More details about the value set resource can be found in
the FHIR documentation available at http://hl7.org/fhir/
valueset.html.
A concept map defines the relationships between two
sets of codes, i.e., it defines the relationships between a
source and a target value set. One of the most impor-
tant operations supported by the concept map resource is
translate which returns a mapping between a code from
a source value set to a code in a target value set, if such a
mapping exists. The mapping includes the type of equiva-
lence between the codes. More details about the concept
map resource can be found in the FHIR documentation at
http://hl7.org/fhir/conceptmap.html.
Clinical terminology support
Ontoserver supports several clinical terminologies out of
the box. In order to expose them through the FHIR API,
Table 1 Examples of SNOMED CT implicit value sets
Description URL
All concepts http://snomed.info/sct?fhir_vs
All concepts subsumed by the
clinical finding concept (i.e. all
clinical findings)
http://snomed.info/sct?
fhir_vs=isa/404684003
All concepts that belong to the
emergency department reference
set
http://snomed.info/
sct?fhir_vs=refset/
171881000036108
Ontoserver needs to be able to import and index con-
tent in each native format. The following sections describe
how this support is implemented for SNOMED CT,
LOINC and OWL ontologies.
SNOMED CT
SNOMED CT is currently distributed as a collection
of RF2 files. The set of core files is used to represent
concepts, descriptions and relationships, which form the
primary content of the distribution. In addition to this,
there is also an extensible pattern, referred to as ref-
erence sets, that is used to provide additional informa-
tion. The most important of these additional files is the
Module Dependency Reference Set (MDRS), which rep-
resents dependencies between different SNOMED CT
modules, for example, between the Australian extension
and the international version, as well as dependencies
between different module versions. All of these files can
be uploaded to Ontoserver in order to support import-
ing and indexing multiple versions of SNOMED CT
modules.
An extension to the RF2 specification that allows rep-
resenting concrete domains is used in Australia to model
the AustralianMedicines Terminology. Concrete domains
allowmodelling properties that have concrete values, such
as strings or integers, using predicates such as = or ?.
SNOMED CT was originally built using a subset of the
OWLEL profile [9] that did not include concrete domains.
Even thoughmost of its content can be correctly modelled
using this subset, some concepts cannot be fully modelled
without concrete domains. For example, it is impossible to
fully represent aHydrochlorothiazide 50mg tablet without
using a data literal to represent the quantity of the active
ingredient. To overcome this limitation, the Australian
Digital Health Agency (ADHA) proposed a mechanism
to represent a limited form of concrete domains using
RF2 reference sets. This mechanism allows maintain-
ing compatibility with existing RF2 processing tools that
do not support concrete domains but adds significant
complexity for developers that want to support the new
functionality in their tools. Ontoserver also supports this
extension.
SNOMED CT also defines the Expression Constraint
Language (ECL) which allows building expressions that
select sets of concepts. FHIR defines a mechanism to use
ECL expressions as filters in value set definitions and the
search operators also allow referring to value set mem-
bership. Therefore, when systems have data coded with
SNOMED CT, ECL can be used to create complex queries
through the use of value sets. Ontoserver provides a com-
plete implementation of ECL. More details about the
features available for the SNOMED CT code system can
be found in the FHIR documentation at http://hl7.org/
fhir/snomedct.html.
Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 4 of 10
LOINC
LOINC is a database and universal set of test identifiers
for medical laboratory observations and other health data
[10]. It is currently distributed as a collection of CSV
files and spreadsheets packaged in a ZIP file. The LOINC
importer extracts the required files from the archive and
the LOINC indexer reads them and creates a Lucene
index. Each version of LOINC is distributed as a separate
archive, so the handling of versions is much simpler than
with SNOMED CT. More details about the features avail-
able for the LOINC code system can be found in the FHIR
documentation at http://hl7.org/fhir/loinc.html.
OWL ontologies
OWL ontologies are supported by providing an exter-
nal service that transforms OWL files into FHIR code
system resources. Implementing an external transforma-
tion has several advantages over implementing a custom
FHIR importer. First, even though a set of sensible defaults
needs to be defined for the result of the transformation,
the resulting code systems can be easily tweaked by the
users before uploading them to the server. Also, an exter-
nal transformation allows storing the resulting code sys-
tem in any capable FHIR server, not just Ontoserver. Note
that an external transformation service is not suitable for
ontologies such as SNOMED CT because these include
elements such as implicit value sets that are impractical to
create externally.
Figure 2 shows the high level steps involved in this
transformation. First, the OWL ontology is loaded and
classified using any ontology reasoner that supports the
OWL-API [11]. Our implementation supports the OWL
EL profile but other profiles are supported as long as a
reasoner for that profile exists and implements the OWL-
API. Then, a FHIR code system is created and its attributes
are populated based on certain values in the ontology.
These values are shown in detail in Table 2. Most of these
are straightforward mappings between attributes in the
source ontology and attributes in the target code sys-
tem. However, the system provides configuration options
to override these default mappings when more than one
source attribute can be used.
The generated code system also includes a set of prop-
erties that are mainly used to provide an easy way for
clients to access some of the most important information
about the concepts. Table 3 shows these properties and
how their values are calculated.
The main challenge in this process is the mismatch in
terms of modularity supported by OWL ontologies and
FHIR code systems. FHIR code systems do not support
modularity, while OWL ontologies support it through
an import mechanism. Therefore, once the target code
system is created, the system iterates over all the ontolo-
gies referenced by the main ontology, including itself. For
example, the Human Phenotype Ontology (HPO) imports
11 additional ontologies, some of which also import other
ontologies, so this process would iterate over 12 different
ontologies, HPO and the 11 additional ontologies refer-
enced by it. All the concepts from every ontology in the
imports closure are added to the target code system.
In addition to this, a value set is created for the main
ontology (excluding its imports) and for every imported
ontology. This allows preserving the correct hierarchy in
the code system (because some elements in the hierar-
chy might come from the imported ontologies) and at
the same time allows restricting search to just the main
ontology (or any of the imported ontologies) by using the
value sets.
Effective concept search
Finding a concept in a clinical terminology is a key
operation in many contexts, such as entering coded
Fig. 2 High level view of OWL to FHIR transformation. OWL ontologies are supported in Ontoserver through a service that transforms an OWL file
into a FHIR Code System. The process involves classifying the ontology, merging all imported ontologies into a single code system and creating
value sets for all imported ontologies
Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 5 of 10
Table 2 Code system elements and their corresponding
elements in OWL ontologies
Code
System
OWL Comments
Id - The id of the resource is local so it can be
passed in as a parameter to the
transformation.
Url Ontology
IRI
The IRI is optional in an OWL ontology. If
it is not present then the transformation
stops.
Version Ontology
version
If the ontology has no version, then the
version is set to NA. The user can mod-
ify this or remove the version altogether.
Name rdfs:label,
Ontology
IRI
If the ontology has been annotated with
an RDFS label, then the first occurrence
is used as the code systems name.
Otherwise the ontologys IRI is used.
Publisher Publisher This element can be configured (the
default value is http://purl.org/dc/
elements/1.1/publisher).
Description Subject,
rdfs:comment
These elements can be configured
(defaults values are http://purl.org/
dc/elements/1.1/subject and http://
www.w3.org/2000/01/rdf-schema#
comment).
Status - Always set to ACTIVE.
ValueSet Ontology
IRI
Set by default to the same URI as the
code system.
Hierarchy
Meaning
- Always set to SUBSUMES.
data in a clinical system. In many cases entering struc-
tured data is an additional burden on the end user, who
would typically prefer to capture information using natu-
ral language. Therefore it is important that users are able
to quickly locate a concept of interest with minimal effort
which means the system should return a good ranking of
potential matches and should also do it quickly. The use
of value sets is useful in this context because it constrains
the search to a more manageable set. However, in many
cases the search space can still be very large. Also, users
are unlikely to formulate entire queries as they would do
Table 3 Properties added to the code system
Code
System
OWL Comments
Parent - This property is used to provide an easy
way for clients to access the direct parents
of a concept. This is useful when creating
a graphical view of the code system. It is
calculated using the reasoner.
Root - This property is used to inform clients if a
concept is a root. It is calculated using the
reasoner.
Deprecated Annotation In OWL a class is marked as deprecated by
annotating it with a deprecation
annotation.
in a web search scenario but rather just start typing and
expect autocomplete-style results.
There have been few studies on this type of search on
large clinical terminologies. The most relevant one is the
work by Sevenster, van Ommering and Qian [12], where
a user experiment was conducted to evaluate two auto-
completion algorithms, standard breadth-first (SBF) and
multi-prefix matching (MPM), on a large medical vocabu-
lary. The former extends the string the user is typing to the
right. For example, if the user searches for acute append in
SNOMED CT, the algorithm could yield the strings acute
appendicitis and acute appendicitis with peritonitis, for
example, but not the string acute focal appendicitis. The
latter matches the terms whose prefixes match the string
typed by the user. For example, the search string ac app
would match acute appendicitis, acute appendicitis with
peritonitis and acute focal appendicitis. The user exper-
iment concluded that the MPM algorithm performed
better, requiring fewer keystrokes to obtain a certain tar-
get concept. One of the key aspects of theMPM algorithm
is how to rank the matches. The authors propose the
following scoring function
?(F) = 1n
m?
i=1
|qi|
|F(qi)| (1)
where qi is a query prefix, F(qi) is a word in the label that
matches the prefix, m is the number of query prefixes,
and n is the number of words in the matching label. Note
that the order of the query prefixes has no impact on the
ranking score.
Searching for a concept in a clinical terminology corre-
sponds to a value set expansion with a filter parameter in
FHIR, which is one of the most important and frequently
used operations when implementing search interfaces on
top of a FHIR server. The FHIR specification does not
mandate how the filters should be interpreted so each
server is free to implement this functionality however it
sees fit. We implement the MPM algorithm in Ontoserver
using Lucene with some slight modifications. These are
required because the original algorithm does not han-
dle certain cases properly, such as duplicate prefixes. One
example of this is pne pne, which is a search string that
could be reasonable to expect from a user searching for
the concept Pneumococcal pneumonia.
Clinical terminologymaintenance
One of the key features of Ontoserver is its ability to act
as a syndication client and server. The syndication func-
tionality is implemented using the Atom standard [13].
Figure 3 shows how Ontoserver can act both as a client
and a server and therefore create a chain of syndication
servers. The idea behind the syndication functionality is
to provide an easy mechanism for terminology servers to
Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 6 of 10
Fig. 3 Ontoservers syndication model. Ontoserver can consume and publish terminology resources to syndicate, allowing the creation of a
syndication chain. In Australia, the Australian Digital Health Agency published an Atom feed that can be used as the first node in the chain
keep up to date with new releases of clinical terminologies.
In Australia, for example, the Australian Digital Health
Agency (ADHA) maintains an Atom feed that contains
all the releases of SNOMED CT-AU, the local version of
SNOMED CT. When an instance of Ontoserver is con-
figured to point at this feed, it can easily check if a new
version of SNOMED CT-AU has been published and in
that case retrieve it and install it locally.
If an organisation produces its own version of a clinical
terminology, for example, an extension of SNOMED CT-
AU, they can use their instance of Ontoserver to distribute
it downstream. The syndication API includes operations
to turn an instance of Ontoserver into a syndication
server, and allows selecting which content is to be exposed
in the Atom feed it generates.
The syndication mechanism supports both source files
and Ontoserver binary indexes. The binary index format
can only be processed by Ontoserver but the source files
are standard RF2 and can be processed by any client.
For example, the ADHA consumes the feed to generate a
web page that allows downloading the SNOMED CT-AU
releases by a human user.
When a user requests to index a code system in
Ontoserver, the system first looks for a compatible binary
index in the syndication feed. If it doesnt find one, then
it looks for the source files and builds the index locally. If
the source files are also unavailable then the request fails.
This functionality was designed in this manner because
building an index locally can be computationally expen-
sive and therefore it is preferable to download a prebuilt
binary index if available.
Results and discussion
As mentioned in the Effective concept search section,
the search algorithm implemented by Ontoserver is a
modified version of an algorithm available in the pub-
lished literature [12] that has already been evaluated
against other algorithms in terms of quality of the search
results. The modifications only deal with edge cases and
therefore the conclusions from the user study are still
valid. Another aspect of the evaluation that is important
to consider is the performance of the implementation,
because the value set expansion operation is likely to be
called constantly by user interfaces doing concept search.
Having a responsive search widget is essential to providing
a good user experience. However, the FHIR specification
does not mandate which search algorithm to use. A fair
comparison should be performed in practice, balancing
speed and success rate, because simpler search algorithms
are likely to perform better than more complex ones, but
the quality of the search results will be very different.
We refer the readers to our VSTool, available at https://
ontoserver.csiro.au/vstool/, which allows doing interac-
tive searches across different FHIR terminology servers
and looking both at the results returned as well as the
response times. Figure 4 shows a screenshot of the tool
with the results of the pne pne query run across several
publicly available FHIR terminology servers, including
Ontoserver.
One of the main reasons for the relatively low adoption
of clinical terminologies is the perception of stakehold-
ers that the cost of migrating to a standardised clin-
ical terminology might be too high compared to the
Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 7 of 10
Fig. 4 The FHIR ValueSet $expand Comparison Tool. The VSTool can be used to interactively compare the results and response times of different
FHIR terminology servers
value it generates. Many terminology servers have been
implemented in the past, such as the VOSER vocabu-
lary server [14], the UMLS Knowledge Source Server
[15] and the GALEN terminology server [16], among
many others. One of the disadvantages of implementing
a proprietary terminology server is that clients will have
to write different code to interact with it. This is the
main advantage of implementing a FHIR-compliant ter-
minology server and is the main reason why Ontoserver
was migrated to use the FHIR standard in its latest
version. Using the FHIR standard mitigates the cost of
adoption and also prevents vendor lock-in because other
FHIR-compliant implementations can be used as drop-in
replacements.
There are additional challenges that have hindered the
adoption of standardised clinical terminologies, including
the technical complexity of the formats used by different
terminologies, the difficulties in keeping them up to date
and the complexities of locating concepts in large termi-
nologies. Ontoserver addresses the technical complexity
of terminologies by providing out-of-the-box support for
SNOMED CT and LOINC, and also implementing an
OWL to FHIR transformation service. To our knowl-
edge, no other FHIR terminology server has implemented
support for OWL ontologies.
The clinical domain is far from static and clinical ter-
minologies change often. A key challenge when using a
sever is keeping it up to date. This is important because
it gives users access to the latest content, which might
include new concepts and bug fixes, and also because
some licenses impose limitations on the use of older ver-
sions. This challenge is addressed by implementing an
open syndication mechanism. The National Clinical Ter-
minology Service (NCTS), developed in Australia by the
ADHA in collaboration with our research group, devel-
oped a syndication API standard3 that is implemented
by Ontoserver. Having the possibility of acting as both a
server and a client for syndication of terminology content
enables a straightforward mechanism to maintain multi-
ple instances up to date with the most recent version of
the terminologies of interest.
Finding the right concept is also a key challenge,
especially when using very large terminologies. This is
addressed by implementing a smart search algorithm
based on the published literature and modifying it slightly
to account for certain edge cases that are present in
SNOMED CT.
Once a terminology server is adopted and data is avail-
able in coded form, Ontoserver generates value by pro-
viding the building blocks for doing more sophisticated
analytics. FHIR supports a wide range of search operators
that can be used as the basis for this type of functional-
ity, for example, the modifiers above and below for coded
types, which allow users to search not only for a specific
Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 8 of 10
code but also for codes that subsume or are subsumed by
a code. For example the query:
http://myhost.com/fhir/Observation?code:below=
http://snomed.info/sct|118188004
will return all the observations that refer to findings of
neonates, that is, all the observations types that are sub-
sumed by the concept 118188004 | Finding of neonate.
Ontoserver is currently in use in several projects.
Shrimp is a terminology browser that uses Ontoserver
to implement fast concept search and display a graph-
ical view of the search results when the terminology
is hierarchical4. A screenshot with the pne pne search
example discussed previously is shown in Fig. 5.
Escargot is a quality assurance tool that uses
Ontoservers validation capabilities to find code labels
that have been changed by end users and attempts to
automatically identify problematic cases. Some clinical
systems allow modifying the description of a code once
it is entered. This can lead to undesirable user behaviour
such as selecting a concept that seems close enough and
modifying its display text. This can seriously impact
data quality and could even have serious consequences
in the context of a clinical decision support system.
Escargot retrieves data from a FHIR server and uses the
value set validate-code operation to determine which
code descriptions have been modified and no longer
match any of the original labels. It then uses patterns
to automatically flag potentially problematic changes.
Figure 6 shows a screenshot of the application.
Finally, an example of the use of Ontoservers ECL
implementation can be found in a tool called SNOMap,
which was developed as a way to automatically map
SNOMED CT codes into ICD10-AM codes. This is
important because many organisations still rely on ICD
codes to support Activity Based Funding so migrating a
system to use SNOMED CT, which is clinically-focused
andmore granular, can impact this model. The tool is now
in use in several hospitals in Australia.
Conclusion
Despite the availability of many high-quality clinical ter-
minologies such as SNOMED CT and LOINC, there
has been a slow uptake of terminology in clinical sys-
tems. In this paper we identify several key challenges that
have hindered adoption and show how Ontoserver has
been designed to help overcome them. This is important
because adoption of standardised clinical terminologies
and the use of coded data is key to improving the quality
of clinical data, which can lead to better clinical decision
support and ultimately to better patient outcomes. We
also show examples where Ontoserver is currently being
used.
There are threemain features that are planned for future
versions of Ontoserver. The first one is boosting the rank-
ing of search results based on a value set. This is useful
when constraining the results to a value set is too restric-
tive and it is still desirable to give the user the possibility
of selecting a concept from a bigger set. The second one
is support for post-coordination. FHIR defines a closure
operation that can be used to maintain a client-side clo-
sure table based on the terminological logic in the server
that is built incrementally. When a client encounters a
Fig. 5 The Shrimp terminology browser Shrimp is a browser. for hierarchical terminologies like SNOMED CT and AMT. It is HTML5, SVG, and
Javascript based, and runs in most modern web browsers
Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 9 of 10
Fig. 6 The Escargot data validation tool. Escargot is a web tool that retrieves data from a FHIR server and uses the value set validate-code operation
to determine which code descriptions have been modified and no longer match any of the original labels
code and needs to run searches that involve accessing its
hierarchy, it requests the data needed to complete the clo-
sure table from the server. One of the most interesting
features of the closure operation is that it can also work
with post-coordinated concepts, i.e., concepts that can be
built on the fly if they dont exist as pre-coordinated con-
cepts in the terminology. Future work will focus on adding
post-coordination support to the closure operation, which
is currently implemented in Ontoserver but does not
support post-coordination. Finally, when building concept
maps, it is often necessary to search for potential matches
for an entire string, not just a prefix. We refer to this type
of search as automap because it can be used to produce
an initial set of candidates when building a map between
two code systems. The current implementation is very
simple and future work will explore more sophisticated
approaches with the goal of producingmuch better quality
maps automatically.
Availability of data andmaterials
Project name: Ontoserver. Project home page: http://
ontoserver.csiro.au/. Operating system(s): Platform
independent. Programming language: Java. Other
requirements: Docker. License: Free to use in Australia -
sublicences available from the Australian Digital Health
Agency after 1 July 2016. Any restrictions to use by
non-academics: Licence required for commercial use
outside Australia. A public Ontoserver instance is avail-
able worldwide for free for research purposes at https://
ontoserver.csiro.au/stu3-latest/.
Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 10 of 10
Endnotes
1 SNOMED CT is freely available to all members coun-
tries of SNOMED International (formerly the Inter-
national Health Terminology Standards Development
Organisation (IHTSDO)).
2 The descriptions in this section correspond to ver-
sion 3.0.0 of the FHIR specification, which was used by
Ontoserver v5.0.0 at the time of writing. FHIR is an
evolving standard and some of these resources might
have changed in more recent versions. A directory of
published versions can be found at http://hl7.org/fhir/
directory.html.
3NCTS Conformant Server Applications Technical
Services Specification, which can be found here: https://
www.healthterminologies.gov.au/ncts/#/learn
4Available at https://ontoserver.csiro.au/shrimp-fhir/.
Abbreviations
ADHA: Australian digital health agency; API: Application programming
interface; CSV: Comma separated values; CTS2: Common terminology services
2; ECL: Expression constraint language; FHIR: Fast health interoperability
resources; HPO: Human phenotype ontology; IHTSDO: International health
terminology standards development organisation; IRI: Internationalised
resource identifier; MDRS: Module dependency reference set; MPM: Multi-
prefix matching; OWL: Web ontology language; RDFS: Resource description
framework schema; RF2: Release format 2; SBF: Standard breath-first
Authors contributions
DH wrote the first version of Ontoserver. AM, ML and JS were involved in the
design and implementation of the current version of Ontoserver (described in
this paper). AM wrote the initial version of the manuscript. JS, ML and DH
revised the manuscript. All authors read and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Received: 13 August 2017 Accepted: 27 August 2018
RESEARCH Open Access
EAPB: entropy-aware path-based metric for
ontology quality
Ying Shen1, Daoyuan Chen1, Buzhou Tang2, Min Yang3 and Kai Lei1*
Abstract
Background: Entropy has become increasingly popular in computer science and information theory because it can
be used to measure the predictability and redundancy of knowledge bases, especially ontologies. However, current
entropy applications that evaluate ontologies consider only single-point connectivity rather than path connectivity,
and they assign equal weights to each entity and path.
Results: We propose an Entropy-Aware Path-Based (EAPB) metric for ontology quality by considering the path
information between different vertices and textual information included in the path to calculate the connectivity path
of the whole network and dynamic weights between different nodes. The information obtained from structure-based
embedding and text-based embedding is multiplied by the connectivity matrix of the entropy computation. EAPB is
analytically evaluated against the state-of-the-art criteria. We have performed empirical analysis on real-world medical
ontologies and a synthetic ontology based on the following three aspects: ontology statistical information (data
quantity), entropy evaluation (data quality), and a case study (ontology structure and text visualization). These aspects
mutually demonstrate the reliability of the proposed metric. The experimental results show that the proposed EAPB
can effectively evaluate ontologies, especially those in the medical informatics field.
Conclusions: We leverage path information and textual information to enrich the network representational learning
and aid in entropy computation. The analytics and assessments of semantic web can benefit from the structure
information but also the text information. We believe that EAPB is helpful for managing ontology development and
evaluation projects. Our results are reproducible and we will release the source code and ontology of this work after
publication. (Source code and ontology: https://github.com/AnonymousResearcher1/ontologyEvaluate).
Keywords: Ontology evaluation, Ontology modeling, Entropy-based metric, Knowledge representation, Big data and
semantics
Background
The term ontology refers to a representation and definition
of the categories, properties, and relations of the concepts,
data, and entities that substantiate one, many, or all do-
mains. [1] Ontology has attracted increasing attention re-
cently due to its broad applications such as information
retrieval, relation extraction, and question answering. Sig-
nificant progress has been made in the ontology construc-
tion [2]. However, the ontology evaluation is still a relatively
new territory and under-explored. As a result, there are few
commonly agreed-upon methodologies and metrics for
ontology evaluation.
Considering each ontology as a graph or a network,
entropy can be used as a measure of the complexity and
redundancy of the graph. Ontologies may contain data
and concepts redundancy that could be removed for the
sake of consolidation and conciseness without changing
the overall meaning. The information density is opera-
tionalized based on the normalized entropy measured
between all concept pairs in the ontology [3]. States of
lower entropy occur when ontology become organized.
In the literature, the entropy evaluation of the lexical in-
formation included in the ontology has been studied in
the last decade and have been proved to be helpful for
ontology evaluation [4].
* Correspondence: leik@pkusz.edu.cn
Ying Shen and Daoyuan Chen contributed equally to this work.
1Shenzhen Key Lab for Information Centric Networking & Blockchain
Technology (ICNLAB), School of Electronics and Computer Engineering,
Peking University Shenzhen Graduate School, 518055 Shenzhen, Peoples
Republic of China
Full list of author information is available at the end of the article
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Shen et al. Journal of Biomedical Semantics  (2018) 9:20 
https://doi.org/10.1186/s13326-018-0188-7
Despite the effectiveness of previous studies, current
entropy applications used to evaluate ontology have
three limitations, in that they (1) Exclusively consider
single point connectivity rather than paths [5], which ne-
glects information pertaining to non-adjacent nodes. (2)
Assign equal weights to edges and paths [6], which in-
duces a loss of diversity. (3) Assume vertices are static,
which ignores the various aspects of vertices when inter-
acting with different neighboring vertices [7].
To address these three limitations, this article de-
scribes an Entropy-Aware Path-Eased quality metric for
ontologies (EAPB) by comparing their information dens-
ities to those of other ontologies. We consider the path
information between different vertices in ontology as
well as the textual information included in the path to
calculate the dynamic weight between different nodes
and the connectivity path of the entire network. Specific-
ally, we first apply CNN to learn the structure-based em-
bedding and text-based embedding to capture both the
ontology network structures and their encapsulated text-
ual information. Subsequently, the information gain
which is in the form of a matrix obtained by a cosine
similarity calculation of the relevancy between nodes u
and v, is multiplied by the connectivity matrix of entropy
computations. Finally, we validate the effectiveness and
robust superiority of our model on four real-world
ontologies.
Three infectious disease-relevant ontologies, i.e. Infec-
tious Disease Ontology1 (IDO), Infectious Disease Ontol-
ogy for Dengue2 (IDODEN), and Disease Ontology3 (DO)
are adopted as baselines. Our material includes an
in-house ontology that is used to develop an
ontology-driven clinical decision support system for infec-
tious disease diagnosis and antibiotic prescription
(IDDAP) [8]. To demonstrate the applicability and gener-
ality of our quality metric for ontologies, we conduct eval-
uations on real-world ontologies with different structures
and different textual information. To verify whether our
quality metric can make a significant performance boost
by incorporating textual information into the EAPB archi-
tecture, we assess ontologies with the same structures but
different textual information, as well as report the ablation
tests in terms of discarding the textual information. The
textual attention visualization and ontology statistical in-
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 
DOI 10.1186/s13326-017-0174-5
RESEARCH Open Access
OpenBiodiv-O: ontology of the
OpenBiodiv knowledge management system
Viktor Senderov1,2* , Kiril Simov3, Nico Franz4, Pavel Stoev1,7, Terry Catapano5, Donat Agosti5,
Guido Sautter5, Robert A. Morris6 and Lyubomir Penev1,2
Abstract
Background: The biodiversity domain, and in particular biological taxonomy, is moving in the direction of
semantization of its research outputs. The present work introduces OpenBiodiv-O, the ontology that serves as the basis
of the OpenBiodiv Knowledge Management System. Our intent is to provide an ontology that fills the gaps between
ontologies for biodiversity resources, such as DarwinCore-based ontologies, and semantic publishing ontologies, such
as the SPAR Ontologies. We bridge this gap by providing an ontology focusing on biological taxonomy.
Results: OpenBiodiv-O introduces classes, properties, and axioms in the domains of scholarly biodiversity publishing
and biological taxonomy and aligns them with several important domain ontologies (FaBiO, DoCO, DwC, Darwin-SW,
NOMEN, ENVO). By doing so, it bridges the ontological gap across scholarly biodiversity publishing and biological
taxonomy and allows for the creation of a Linked Open Dataset (LOD) of biodiversity information (a biodiversity
knowledge graph) and enables the creation of the OpenBiodiv Knowledge Management System.
A key feature of the ontology is that it is an ontology of the scientific process of biological taxonomy and not of any
particular state of knowledge. This feature allows it to express a multiplicity of scientific opinions. The resulting
OpenBiodiv knowledge system may gain a high level of trust in the scientific community as it does not force a
scientific opinion on its users (e.g. practicing taxonomists, library researchers, etc.), but rather provides the tools for
experts to encode different views as science progresses.
Conclusions: OpenBiodiv-O provides a conceptual model of the structure of a biodiversity publication and the
development of related taxonomic concepts. It also serves as the basis for the OpenBiodiv Knowledge Management
System.
Keywords: Biodiversity, Biodiversity informatics, Semantic web, Semantic publishing, Ontology, Knowledge
management, Linked open data, RDF, OWL, Taxonomy, Concept taxonomy, Biological systematics, Data modeling
Background
The desire for an integrated information system serving
the needs of the biodiversity community dates at least as
far back as 1985 when the Taxonomy Database Working
Group (TDWG)later renamed to Biodiversity Informatics
Standardswas established [1]. In 1999, the Global
Biodiversity Information Facility (GBIF) was created
after the Organization for Economic Cooperation and
Development (OECD) had arrived at the conclusion that
*Correspondence: vsenderov@gmail.com
1Pensoft Publishers, Prof. Georgi Zlatarski 12, 1700 Sofia, Bulgaria
2Institute of Biodiversity and Ecosystems Research, Bulgarian Academy of
Sciences, Sofia, Bulgaria
Full list of author information is available at the end of the article
an international mechanism is needed to make biodi-
versity data and information accessible worldwide [2].
The Bouchout declaration [3] crowned the results of
the pro-iBiosphere project (2012 - 2014) [4] dedicated
to the task of creating an integrated biodiversity infor-
mation system. The Bouchout declaration proposes to
make scholarly biodiversity knowledge freely available as
Linked Open Data. A parallel process in the U.S.A. started
even earlier with the establishment of the Global Names
Architecture [5, 6].
The specification and design of a semantic system,
the Open Biodiversity Knowledge Management System
(OBKMS, later simply OpenBiodiv), implementing the
objectives of the Bouchout Declaration by focusing
© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 2 of 15
on knowledge extraction from academic journals and
research databases, were outlined amongst others in [7, 8].
In this publication we present the OpenBiodiv Ontology
(OpenBiodiv-O)the knowledge and inferencing model
of OpenBiodiv [9]. OpenBiodiv-O provides a conceptual
model of the structure of a biodiversity publication and
the development of related taxonomic concepts.
Previous work
In the biomedical domain there are well-established
efforts to extract information and discover knowledge
from literature [1012]. The biodiversity domain, and
in particular biological systematics and taxonomy (from
here on in this paper referred to as taxonomy), is also
moving in the direction of semantization of its research
outputs [1315]. The publishing domain has been mod-
eled through the Semantic Publishing and Referencing
Ontologies (SPAR Ontologies) [16]. The SPAR Ontologies
are a collection of ontologies incorporatingamongst
othersFaBiO, the FRBR-aligned Bibliographic Ontology
[17], and DoCO, the Document Component Ontology
[18]. The SPAR Ontologies provide a set of classes and
properties for the description of general-purpose jour-
nal articles, their components, and related publishing
resources. Taxonomic articles and their components, on
the other hand, have been modeled through the TaxPub
XML Document Type Definition (DTD) (also referred
to loosely as XML schema) and the Treatment Ontolo-
gies [19, 20]. While TaxPub is the XML-schema of taxo-
nomic publishing for several important taxonomic jour-
nals (e.g. ZooKeys, Biodiversity Data Journal), the Treat-
ment Ontologies are still in development and have served
as a conceptual template for OpenBiodiv-O. In fact, they
share many of the same authors.
Taxonomic nomenclature is a discipline with a very
long tradition. It transitioned to its modern form with
the publication of the Linnaean System [21]. Already by
the beginning of the last century, there were hundreds of
vocabulary terms (e.g. types) [22]. At present the naming
of organismal groups is governed by by the International
Code of Zoological Nomenclature (ICZN) [23] and by
the International Code of Nomenclature for algae, fungi,
and plants (Melbourne Code) [24]. Due to their com-
plexity (e.g. ICZN has 18 chapters and 3 appendices), it
proved challenging to create a top-down ontology of bio-
logical nomenclature. Example attempts include the rela-
tively complete NOMEN ontology [25] and the somewhat
less complete Taxonomic Nomenclatural Status Terms
(TNSS) [26].
There are several projects that are aimed at model-
ing the broader biodiversity domain conceptually. Darwin
Semantic Web (Darwin-SW) [27] adapts the previously
existing Darwin Core (DwC) terms [28] as RDF. These
models deal primarily with organismal occurrence data.
Modeling and formalization of the strictly taxonomic
domain has been discussed by Berendsohn [29] and later,
e.g., in [30, 31]. Noteworthy efforts are the XML-based
Taxonomic Concept Transfer Schema [32] and a now
defunct Taxon Concept ontology [33].
Aims
The present work introduces OpenBiodiv-O, which serves
as the basis of OpenBiodiv. By developing an ontology
focusing on biological taxonomy, our intent is to provide
an ontology that fills in the gaps between ontologies for
biodiversity resources such as Darwin-SW and semantic
publishing ontologies such as the ontologies comprising
the SPAROntologies. Moreover, we take the view that it is
advantageous tomodel the taxonomic process itself rather
than any particular state of knowledge.
OpenBiodiv [8] lifts biodiversity information from
scholarly publications and academic databases into a comput-
able semantic form. The implementation of the system will
be treated in future works. In this contribution, we discuss
OpenBiodiv-O by first introducing the modeled domain
conceptually and then formalizing it in Results section.
Domain description
Biological taxonomy is a very old discipline dating back
possibly to Aristotle, whose fundamental insight was to
group living things in a hierarchy [34]. The discipline took
its modern form after Carl Linnaeus (1707 - 1778) [34]. In
his Systema Naturae Linnaeus proposed to group organ-
isms into kingdoms, classes, orders, genera, and species
bearing latinized scientific names with a strictly pre-
scribed syntax. Linnaeus listed possible alternative names
and gave a characteristic description of the groups [21].
These groups are called taxa, which is a Greek word for
arrangement. The hierarchy that taxa form is called tax-
onomy. The etymology of the word is Greek and roughly
translates tomethod of arranging. Note the polysemy here:
the science of biological taxonomy is called taxonomy as
is the arrangement of taxa itself. We believe, however,
that it is sufficiently clear from context what is meant by
taxonomy in any particular usage throughout this paper.
Even though Linnaeus and his colleagues may have
hoped to describe life on Earth during their lifetimes, we
now know that there are millions of species still undiscov-
ered and undescribed [35]. On the other hand, our under-
standing of species and higher-rank taxonomic concepts
changes as evolutionary biology advances [36]. Therefore,
an accurate and evolutionarily reliable description of life
on Earth is a perpetual process and cannot be completed
with a single project that can be converted into an ontol-
ogy. Thus, our aim is not to create an ontology capturing a
fixed view of biological taxonomy, but to create an ontol-
ogy of the taxonomic process. The ongoing use of this
ontology will enable the formal description of taxonomic
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 3 of 15
biodiversity knowledge at any given point in time. In the
following paragraphs, we introduce what the taxonomic
process entails and reflect on the resources that need
modeling.
An examination of the taxonomic process reveals that
taxonomy works by employing the scientific method:
researchers examine specimens and, based on the phe-
notypic and genetic variation that they observe, form a
hypothesis [37]. This hypothesis may be called a taxo-
nomic concept, a potential taxon, a species hypothesis
[29], or an operational taxonomic unit (OTU) [38] in the
case of a numerically delimited taxon.
A taxonomic concept describes the allowable pheno-
typic, genomic, or other variation within a taxon by
designating type specimens and describing characters
explicitly. It is a valid falsifiable scientific claim as it needs
to fulfill certain verifiable evolutionary requirements. For
example, a species-rank taxonomic hypothesis needs to
fit our current understanding of species (species con-
cept, [36]). More generally, the aspiration is that species
concepts are adequate and give certain tangible criteria
for species delimitation. However, valid scientific discus-
sions continue about concept adequacy. The discussions
are nuanced because they often draw on different con-
ceptions of the relative weight of certain evolutionary
phenomena. This leads to having quite a few differ-
ent species conceptsmorphological, ecological, phylo-
genetic, genomic, biological, etc. [36]. Nevertheless, if we
fix a species conceptlet us say we take the biological
species conceptwe can falsify any given species-rank
taxonomic hypothesis against our fixed species concept.
Similarly, hypotheses of higher rank (representing upper
levels of the taxonomic hierarchy) also need to fulfill cer-
tain evolutionary requirements. For example, a modern
genus concept requires all species assigned to it to be
descendants of a separate lineage and to form a mono-
phyletic clade.
The ranks (taxonomy hierarchy levels) are not com-
pletely fixed. The usage of lower ranks (species, genus,
family, order) is governed by international Codes [23, 24].
In the example of Linnaeus ranks, each organism is first a
member of its species, then genus, then order, then class,
and finally kingdom. Which specific ranks a given taxo-
nomic study employs is dependent on the field (e.g. botany
vs. zoology), on the particular author, on the level of tax-
onomic resolution required, as well as on the history of
classifying in that particular group.
Once the researchers have formed their concept, it
must be published in a scientific outlet (journal or book).
The biological Codes put some requirements and recom-
mendations aimed at ensuring the quality of published
research but ultimately it is a democratic process guar-
anteeing that everyone may publish taxonomic concepts
provided they follow the rules of the Codes. This means
that in order to create a knowledge base of biodiversity, we
need to be able to mine taxonomic papers from legacy and
modern journals and books.
As a first good approximation, a taxonomic concept is
based on a number of specimens or occurrences that are
listed in a section usually called Materials Examined. In
general terms, we can say that a sighting of a living thing,
i.e. an organism, at a given location and at a given time is
referred to as an occurrence, and a voucher for this occur-
rence (e.g. the sampling of the organism itself ) is referred
to as a specimen [27]. Moreover, a taxonomic article may
include other specialized sections such as the Checklist
section, where one may list all taxa (in fact: the taxonomic
concepts for those taxa) for organisms observed in a given
region.
Typically, the information content of a treatment con-
sists of several units. First, we have the aforementioned
nomenclatural information that pertains to the scien-
tific nameits authorship, etymology, related names, etc.
Then, we have the taxonomic concept information that
can be considered to have two components, as well: the
first one is the intensional component of the taxonomic
concept made up mostly of traits or characters. Traits
are an explicit definition of the allowable variation (e.g.
phenotypic, genomic, or ecological) of the organisms that
make up the taxon. For example, we can define the order
of spiders, Araneae, to be the class of organisms that have
specialized appendages used for sperm transfer called
pedipalps [39]. Knowledge of this kind is found in the
Diagnosis, Description, Distribution and other subsec-
tions of the treatment.
Non-traditionally delimited taxonomic hypotheses are
called operational taxonomic units (OTUs). In the case
of genomic delimitation, sometimes the concepts are
published directly as database entries and not as Code-
compliant taxonomic articles [40]. A genomic delimita-
tion can, for example, be based on a barcode sequence and
on a statistical clustering algorithm specifying the allow-
able sequence variability that an organism can possess
in order to be considered part of the barcode sequence-
bearing operational taxonomic unit. However, as, in the
general case, we dont have a Linnaean name or a mor-
phological description for an operational taxonomic unit,
we refer to it as a dark taxon [40]. The term dark is,
however, usually reserved for concepts at lower ranks.
Operational taxonomic units are published, for example,
in the form of barcode identification numbers (BINs) in
the Barcode of Life Data Systems (BOLD) [41], or as
species hypotheses in Unified system for the DNA based
fungal species linked to the classification (UNITE) [42].
The second part of the information content of a
taxonomic concept is the ostensive component: a list-
ing of some (but not necessarily all) of the organisms
that belong to the taxonomic concept. This information
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 4 of 15
is found in the Materials Examined subsection of the
treatment.
Finally, the relationships between taxonomic concepts
simple hierarchical (is a) or more fine-grained Region
Connection Calculus 5 (RCC-5) [30, 43]can be both
intensionally defined in the nomenclature section or
ostensively inferred from the Materials Examined. How-
ever, given the customary idiosyncrasies of biological
descriptions, providing an initial set of RCC-5 relation-
ships for a machine reasoner to work with often requires
expert assessment and cannot be easily lifted from the
text.
Thus, in order to model the taxonomic process, our
ontology models scholarly taxonomic papers, database
entries, agents responsible for their creation, treatments,
taxonomic concepts, scientific names, occurrence and
specimen information, other entities (e.g. ecological, geo-
graphical) part-taking in the taxonomic process, as well as
relationships among these.
Methods
OpenBiodiv-O is expressed in Resource Description
Framework (RDF). At the onset of the project [8], a con-
sideration was made to use RDF in favor of a more com-
plex data model such as Neo4Js. The choice of RDF was
made in order to be able to incorporate the multitude of
existing domain ontologies into the overall model.
To develop the conceptualization of the taxonomic pro-
cess and then the ontology we utilized the following pro-
cess: (1) domain analysis and identification of important
resources and their relationships; (2) analysis of existing
data models and ontologies and identification of missing
classes and properties for the successful formalization of
the domain.
The formal structure of the ontology is specified by
employing the RDF Schema (RDFS) and the Web Ontol-
ogy Language (OWL). It is encoded as a part of a lit-
erate programming [44] document titled OpenBiodiv
Ontology and Guide [45]. The structure has been
extracted from that file via knitr and provided here
as Additional file 1. It is also possible to request the
ontology via Curl from the endpoint with the indi-
cation of content-type: application/rdf+xml.
The vocabularies can be found as more additional
files: Taxonomic Statuses (Additional file 2) and RCC-5
(Additional file 3), on the website [9], and on the GitHub
page [46] (under ontology/).
A partial dataset from Pensofts journals has been gener-
ated with OpenBiodiv-O and can be found at the SPARQL
Endpoint <http://graph.openbiodiv.net/>, select reposi-
tory obkms_i6. The endpoint is also accessible from the
website, <http://openbiodiv.net/>, under SPARQL End-
point. Demos are available as Saved Queries from the
workbench.
Results
We understand OpenBiodiv-O to be the shared formal
specification of the conceptualization [4749] that we have
introduced in Background. OpenBiodiv-O describes the
structure of this conceptualization, not any particular
state of it.
There are several domains in which the modeled
resources fall. The first one is the scholarly biodiversity
publishing domain. The second domain is that of taxo-
nomic nomenclature. The third domain is that of broader
taxonomic (biodiversity) resources (e.g. taxonomic con-
cepts and their relationships, species occurrences, traits).
To combine such disparate resources together we rely on
SKOS [50]. Unless otherwise noted, the default names-
pace of the classes and properties for this paper is <http://
openbiodiv.net/>. The prefixes discussed in this paper are
listed in Additional file 1, at the beginning of the ontology.
Semantic modeling of the biodiversity publishing domain
An article as suchmay be represented by a set of metadata,
while its content consists of article components such as
sections, tables, figures and so on [51].
To accommodate the specific needs of scholarly biodi-
versity publishing, we introduce a new class for taxonomic
articles, Taxonomic Article (:TaxonomicArticle),
new classes for specific subsections of the taxonomic arti-
cle such as Taxonomic Treatment, Taxonomic Key, and
Taxonomic Checklist, and a new class, Taxonomic Name
Usage (:TaxonomicNameUsage), for the mentioning
of a taxonomic name (see next subsection) in an article.
These new classes are summarized in Table 1.
The classes from this subsection are based on the Tax-
Pub XML Document Type Definition (DTD) [19] (also
Table 1 New biodiversity publishing classes introduced
Class QName Comment
:Treatment Section of a taxonomic
article
:NomenclatureSection Subsection of Treatment
:NomenclatureHeading Contains a nomenclatural
act
:NomenclatureCitationList List of citations of related
concepts
:MaterialsExamined List of examined
specimens
:BiologySection Subsection of Treatment
:DescriptionSection Subsection of Treatment
:TaxonomicKey Section with an
identification key
:TaxonomicChecklist Section with a list of taxa
for a region
:TaxonomicNameUsage Mention of a taxonomic
name
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 5 of 15
referred to loosely as XML schema), on the structure of
Biodiversity Data Journals taxonomic paper [52], and and
on the Treatment Ontologies [20].
Furthermore, we introduce two properties: contains
(:contains) and mentions (:mentions). Contains is
used to link parts of the article together andmentions links
parts of the article to other concepts.
A graphical representation of the relationships between
instances of the publishing-related classes that OpenBio-
div introduces is to be found in the diagram in Fig. 1.
Semantics, alignment, and usage
Our bibliographic model has the Semantic Publishing and
Referencing Ontologies (SPAROntologies) at its core with
a few extensions that we have written to accommodate
for taxonomic elements. The SPAR Ontologies FRBR-
aligned Bibliographic Ontology (FaBiO) uses the Func-
tional Requirements for Bibliographic Records (FRBR)
[53] model to separate publishable items into less or more
abstract classes. We deal primarily with the Work class,
i.e. the conceptual idea behind a publishable item (e.g. the
story of War and Peace as thought up by Leo Tolstoy),
and the Expression class, i.e. a version of record of a Work
(e.g. War and Peace, paperback edition by Wordsworth
Classics).
Taxonomic Article is a subclass of FaBiOs Journal Arti-
cle. Furthermore Journal Article is a FRBR Expression.
This implies that taxonomic articles are FRBR expres-
sions as well. This has important implications later on
when discussing taxonomic concept labels. Also, it means
that we separate the abstract properties of an article (in a
FaBiO Research Paper instance, which is aWork) from the
version of record (in a Taxonomic Article, an Expression).
The taxonomic-specific section and subsection
classes are introduced as subclasses of Discourse
Element Ontologys (DEO) Discourse Element
(deo:DiscourseElement, [18]). So is the class
Mention (:Mention), meant to represent an area
of a document that can be considered a mention of
something. This class, and the corresponding property,
mentions, are inspired by pext:Mention and its corre-
sponding property from PROTON [54]. The redefinition
is necessary by the fact in OpenBiodiv-O they possess a
slightly different semantics and a different placement in
the upper-level hierarchy. We then introduce Taxonomic
Name Usage as a subclass of Mention.
This placement of the document component classes
that weve introduced in Discourse Element means that
they ought to be used exactly in the same way as one
would use the other discourse elements from DEO and
DoCO (analogous to e.g. deo:Introduction). Note:
DEO is imported by DoCO. Figures 2 and 3 give exam-
ple usage in Turtle illustrating these ideas. A caveat here
is that while the SPAR Ontologies use po:contains in
their examples, we use contains, which is a subproperty
of po:contains with the additional property of being
transitive. We believe this definition is sensible as surely a
sub-subcomponent is contained in a component. All other
Fig. 1 Taxonomic article diagram. A graphical representation of the relationships between instances of the publishing-related classes that
OpenBiodiv introduces
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 6 of 15
Fig. 2 Example article metadata. This example shows how to express the metadata of a taxonomic article with the SPAR Ontologies model and the
classes that OpenBiodiv defines. The code is in Turtle
aspects of expressing a taxonomic article in RDF accord-
ing to OpenBiodiv-O are exactly the same as according to
the SPAR Ontologies.
Semantic modeling of biological nomenclature
While NOMEN and TNSS (introduced in subsection
Previous work) take a top-down approach of modeling
the nomenclatural Codes, OpenBiodiv-O takes a bottom-
up approach of modeling the use of taxonomic names in
articles. Where possible we align OpenBiodiv-O classes to
NOMEN.
Based on the need to accommodate taxonomic con-
cepts, we have defined the class hierarchy of tax-
onomic names found in Fig. 4. Furthermore, we
have introduced the class Taxonomic Name Usage
(:TaxonomicNameUsage). Taxonomic name usages
have been discussed widely in the community (e.g. in
[55]); however, the meaning of term remains vague. The
abbreviation TNU is used interchangeably for taxon
name usage and for taxonomic name usage. In
OpenBiodiv-O, a taxonomic name usage is the mention-
ing of a taxonomic name in the text, optionally followed
by a taxonomic status.
For example, Heser stoevi Deltschev 2016, sp. n. is a
taxonomic name usage. The cursive text followed by the
author and year of the original species description is the
latinized scientific name. The abbreviation sp. n. stands
for the Latin species novum, indicating the discovery of a
new taxon.
We also introduce the class Taxonomic Concept Label
(:TaxonomicConceptLabel). A taxonomic concept
label (TCL) is a Linnaean name plus a reference to a pub-
lication, where the discussed taxon is circumscribed. The
link is via the keyword sec. (Latin for secundum) [29].
An example would be "Andropogon virginicus var. tenuis-
patheus sec. Blomquist (1948)". Here, Blomquist (1948) is
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 7 of 15
Fig. 3 Example article structure. This examples shows how to express the article structure with the help of :contains. The code is in Turtle
Fig. 4 Taxonomic name class hierarchy diagram. We created this class hierarchy to accommodate both traditional taxonomic name usages and the
usage of taxonomic concept labels and operational taxonomic units
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 8 of 15
Table 2 OpenBiodiv taxonomic status vocabulary
Vocabulary instance QName Example abbrev Comment
:TaxonomicUncertainty incertae sedis Taxonomic uncertainty
:TaxonDiscovery sp. n. Taxonomic discovery
:ReplacementName comb. n. Replacement name
:UnavailableName nomen dubium Unavailable name
:AvailableName stat. rev. Available name
:TypeSpecimenDesignation lectotype designation Type specimen designation
:TypeSpeciesDesignation type species Type species designation
:NewOccurrenceRecord new country record New occurrence record (for region)
a reference to [56], the publication where the concept is
circumscribed.
We extracted taxonomic status abbreviations from
about 4000 articles across four taxonomic journals
(ZooKeys, Biodiversity Data Journal, PhytoKeys, and
MycoKeys) in order to create a taxonomic status vocabu-
lary (Additional file 2) that covers the eight most common
cases (Table 2). The Latin abbreviations that have been
classified into these classes can be found on the OpenBiodiv
GitHub page [46] (See Methods section for more details).
Based on our analysis of taxonomic statuses, we have
identified two Code-compliant patterns of relationship
between latinized scientific names (Fig. 5). The pat-
tern replacement name, implemented via the property
:replacementName, indicates that a certain Linnaean
name should be used instead of another Linnaean name.
It covers a wide variety of cases in the Codes, such as,
for example, the placement of one species taxon in a new
genus (comb. n.), the correction of a name for nomen-
clatural reasons (nomen novum), or the application of
the Principle of Priority for the discovery of synonyms
(syn. nov.) [23].
The other pattern is that of related names
(:relatedName). It is a broader pattern, indicating
that two names are somehow related. For example, they
may be synonyms, with one replacing the other, or they
may point to taxonomically related taxonomic concepts.
For example, Harmonia manillana (Mulsant, 1866) is
related to Caria manillana Mulsant 1866 since, as per
Fig. 5 Scientific name patterns diagram. Chains of replacement names
can be followed to find the currently used name. Related name
indicates that two names are related somehow, but not which one is
preferable
[57], a name-bearing type (lectotype) ofHarmonia manil-
lana (Mulsant, 1866) sec. Poorani [57] is named Caria
manillanaMulsant 1866.
Semantics, alignment and usage
As evident from Fig. 4, OpenBiodiv-O taxonomic names
are aligned to NOMEN names.
The linking between text and taxonomic names must
pass through the intermediary class Taxonomic Name
Usage. As parts of the manuscript, taxonomic name
usages link document components to taxonomic names.
Taxonomic name usages are contained in sections such as
Treatment, and mention a taxonomic name as illustrated
in the example in Fig. 6.
Semantic modeling of the taxonomic concepts
In OpenBiodiv-O taxonomic names are not the car-
riers of semantic information about taxa. This task
is accomplished by a new class, Taxonomic Concept
(:TaxonomicConcept). A taxonomic concept is the
theory that a taxonomist forms about a taxon in
a scholarly biological taxonomic publication and thus
always has a taxonomic concept label. We also intro-
duce a more general class, Operational Taxonomic Unit
(:OperationalTaxonomicUnit) that can be used for
all kinds of taxonomic hypotheses, including ones that
dont have a proper taxonomic concept label. The class
hierarchy has been illustrated in Fig. 7.
Taxonomic concepts are related to taxonomic names
including taxonomic concept labelsvia the property
has taxonomic name (:taxonomicName) and its sub-
properties mimicking in their range the hierarchy of
taxonomic names that we introduced earlier. We have
defined a property specifically to link taxonomic concepts
to taxonomic concept labels, has taxonomic concept label
(:taxonomicConceptLabel). The property hierarchy
diagram is shown in Fig. 8.
There are two ways to relate taxonomic concepts to
each other (Fig. 9). As we pointed out earlier, histori-
cally taxonomic concepts form the hierarchy known as
biological taxonomy. To express such simple semantic
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 9 of 15
Fig. 6 Example taxonomic name usage. This examples shows how taxonomic name usages link document components to taxonomic names. The
code is in Turtle
relations, it is fully sufficient to use the SKOS semantic
vocabulary [50].
However, these simple relationships are not well suited
for machine reasoning. This is why Franz and Peet [30]
suggested, building on previous work by e.g. [58], to
use the RCC-5 language to express relationships between
taxonomic concepts. Furthermore, the Euler [59] program
was developed, which uses Answer Set Programming
(ASP) to reason over RCC-5 taxonomic relationships. An
answer set reasoner is not part of OpenBiodiv as this task
can be accomplished by Euler; however, we have provided
an RCC-5 dictionary class (:RCC5Dictionary), an
Fig. 7 Taxonomic concept diagram. A taxonomic concept is a skos:Concept, a frbr:Work, a dwc:Taxon and has at least one taxonomic
concept label
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 10 of 15
Fig. 8 Taxonomic name property hierarchy diagram. Property hierarchy is aligned with the taxonomic name class hierarchy and with DarwinCore
RCC-5 relation term class (:RCC5Relation), a vocab-
ulary of such terms to express the RCC-5 relationships
in RDF (Additional file 3), as well as a class and prop-
erties to express RCC-5 statements (:RCC5Statement,
:rcc5Property, and subproperties).
Semantics and alignment
We introduce Taxonomic Concept as equivalent
(owl:equivalentClass) to the DwC term Taxon
(dwc:Taxon) [60]. However, by including concept in
the class name, we highlight the fact that the semantics it
carries reflect the scientific theory of a given author about
a taxon in nature. As we mentioned earlier, our ontology
models the ongoing still unfinished process of taxonomic
discovery. For this reason, we also derive Taxonomic
Concept from Work. This derivation fits the definition
of Work in FRBR/FaBiO, which is a distinct intellectual
or artistic creation. Finally, as we use SKOS to connect
Fig. 9 Taxonomic concept relationships diagram. In order to express an RCC-5 relationship between concepts, create an :RCC5Sgtatement and
use the corresponding properties to link two taxonomic concepts via it. Further, taxonomic concepts are linked to traits (e.g. ecology in ENVO),
occurrences (e.g. Darwin-SW) and realize treatments
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 11 of 15
taxonomic concepts to each other, we derive Taxonomic
Concept from SKOS Concept.
As with other semantic publishing-related aspects of the
ontology, the creation of the RCC-5 vocabulary follows
the SPAR Ontologies model. Thus OpenBiodiv RCC-5
Vocabulary (:RCC5RelationshipTerms) is a SKOS
concept scheme and every RCC-5 Relation is a SKOS
concept. This allows to seamlessly share this vocabulary
with other publishers of biodiversity information that also
follow the SPAR Ontologies model.
It is important to note that we have aligned the
subproperty of has taxonomic name, has scientific
name (:scientificName), to the DwC property
dwciri:scientificName. The difference is that
while the DwC property is unbound and provides more
flexibility, the OpenBiodiv-O property has the domain
Taxonomic Concept and the range Scientific Name and
provides for inference. Furthermore, has taxonomic con-
cept label is an inverse-functional property with the
domain Taxonomic Concept. This means that a given tax-
onomic concept label uniquely determines its taxonomic
concept. This is accomplished by a minimum cardinality
restriction on the property.
Together with the declaration of has taxonomic concept
label to be an inverse functional property, we can now
list what types of relationships between names and taxo-
nomic concepts are allowed: (1) The relationship between
a taxonomic concept and a name that is not a taxonomic
concept label is many-to-manyi.e. one Linnaean name
can be amention of multiple taxonomic concepts, and one
taxonomic concept may have multiple Linnaean names.
(2) The relationship between a taxonomic concept and
a taxonomic concept label is one-to-many: while a taxo-
nomic concept may have more than one (at least one is
needed) labels, every label uniquely identifies a concept.
These logical restrictions make taxonomic concept labels
into unique identifiers to taxonomic concepts, something
that Linnaean names are not.
Usage
For an example of linking two taxonomic concepts to
each other, let us look at the species-rank concept
Casuarinicola australis Taylor, 2010 sec. Thorpe [61]. It
is a narrower concept than the genus-rank concept of
Casuarinicola Taylor, 2010 sec. Taylor [62]. As we have
aligned our concepts to SKOS, we can use its vocabulary
to express this statement as seen in the example in Fig. 10.
A further example of how to utilize the OpenBiodiv RCC-
5 vocabulary is found in Fig. 11.
Furthermore, thanks to the alignment to DwC, we treat
instances of our class Taxonomic Concept as function-
ally equivalent to DwC Taxa. This makes linking to other
biodiversity ontologies possible. For example, the Open
Biomedical Ontologies (OBO) Population and Commu-
nity Ontology (PCO) [63] has a class collection of organ-
isms (http://purl.obolibrary.org/obo/PCO_0000000) that
can be considered a superclass of DwC Taxon. There-
fore, every taxonomic concept is a collection of organ-
isms and the application of OBO properties on it is
allowed.
In the paper that inspired our Casuarinicola exam-
ple [61], we read: On 26 February 2013, the species
was found to be fairly common on Casuarina trees at
Thomas Bloodworth Park, Auckland. This statement can
be interpreted (in ENVO) as meaning that the taxo-
nomic concept that the author formulated implies that it
includes the habitat forest biome (http://purl.obolibrary.
org/obo/RO_0002303). The RDF example is shown in
Fig. 12.
As we pointed out earlier, taxonomic concepts have
an intensional component (traits or characters) and an
ostensive component (a list of occurrences belonging to
the concept). The ostensive component can be expressed
by linking occurrences to the taxonomic concepts via
Darwin-SW. This is possible as we have aligned the Taxon
Concept class to DwC Taxon used by Darwin-SW. For an
example refer to [27].
Lastly, describing traits is an active area of ontolog-
ical research [64]. Due to the very complex language
used to describemorphological characteristics, the Ontol-
ogy Term Organizer (OTO) [64] software was developed
to allow for user-created vocabularies. We will rely on
such external efforts for expressing traits and trait equiv-
alences (in the taxonomic sense) during the population
of OpenBiodiv with triples. We are tightly working with
the developers of OTO to integrate their efforts into
OpenBiodiv [65].
Further, the interpretation of Taxonomic Concepts as
Work means that they are realized by taxonomic treat-
ments (e.g. Fig. 13).
Fig. 10 Example simple taxonomic concept relationships. We can use SKOS semantic properties to illustrate simple relationships between
taxonomic concepts
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 12 of 15
Fig. 11 Example of RCC-5 taxonomic concept relationships. In order to express an RCC-5 relationship between concepts, create an
:RCC5Sgtatement and use the corresponding properties to link two taxonomic concepts via it. SKOS relations relate concepts directly
Discussion
OpenBiodiv-O istogether with the Treatment Ontolo-
gies [20]the first effort to model taxonomic articles as
RDF. It introduces classes and properties in the domains
of biodiversity publishing and biological taxonomy and
aligns them with the SPAR Ontologies, the Treatment
Ontologies, the Open Biomedical Ontologies (OBO), Tax-
Pub, NOMEN, and DarwinCore. We believe this intro-
duction bridges the ontological gap that we had outlined
in our aims and allows for the creation of a Linked Open
Dataset (LOD) of biodiversity information (biodiversity
knowledge graph [8, 66]).
Furthermore, this biodiversity knowledge graph,
together with this ontology, additional semantic rules,
and user software will form the OpenBiodiv Knowledge
Management System. This system, as any taxonomic
information system should, has taxonomic names as a key
building block. For any given taxonomic name, the user
will be able to rely on two patternsreplacement name
and related nameto get answers to two questions of
high importance to the working taxonomist. First: what
is the current and historical usage of any given Linnaean
name? Second: given a particular name, what other
related names ought to be considered in a taxonomic
discussion?
Both may be useful in building semantic search appli-
cations and the latter, in particular, is actively being
researched by a group at the National Center for Text
Mining in the UK (NaCTeM) [67]. OpenBiodiv-O proper
does not include a mechanism for inferring replacement
names and related names; however, such mechanisms are
part of the OpenBiodiv knowledge system via SPARQL
rules using information encoded in the document struc-
ture (Nomenclature section). Another way to infer related
names is via a machine learning approach to obtain fea-
ture vectors of taxonomic names. Note that the ontology
can describe related names independent of the process of
their generation and will enable the comparison of both
approaches in a future work.
On the other hand, by using OpenBiodiv-O, a
knowledge-based system does not have to have a back-
bone name-based taxonomy. A backbone taxonomy is a
single, monolithic hierarchy in which any and all conflicts
or ambiguities have been pragmatically (socially, algorith-
mically) resolved, even if there is no clear consensus in the
greater taxonomic domain. Such backbone taxonomies
are used in systems that rely solely on taxonomic names
(and not concepts) as bearers of information. They are
needed as it is impossible, in such a system, to express two
different sets of statements for a single name.
In OpenBiodiv, however, multiple hierarchies of taxo-
nomic concepts may exist. For example, large synthetic
taxonomies such as GBIFs backbone taxonomy [68] or
Catalogue of Life [69] may not agree or may have some
issues [70]. With OpenBiodiv-O, we may, in fact, incorpo-
rate both these taxonomies at the same time! It is possible
according to the ontology to have two sets of taxonomic
concepts (even with the same taxonomic names) with a
different hierarchical arrangement. By allowing this, we
leave some room for human interpretation as an addi-
tional architectural layer. Thus, we delay the decision of
which hierarchy to use to the user of the system (e.g. a
practicing taxonomist) and not to the systems architect.
Due to this design feature, it is likely that our system
stands a better chance to be trusted as a science process-
enabling platform as the system architects dont force a
taxonomic opinion on the practicing taxonomist.
It should be noted that a successful concept-based sys-
tem exists for the taxonomic order Aves (birds) [71]. The
main issue that we will face is to develop tools to enable
expert users to annotate taxonomic concepts with the
proper relationships as only recently individual articles
utilizing concept taxonomy in addition to nomenclature
have been published [43, 72, 73]. We do believe that their
Fig. 12 Example of combining ENVO with OpenBiodiv-O. We create a shortcut for has habitat and instance of the forest biome and link them to
our taxonomic concept in order to express the fact that specimens of it have been found to live in Casuarina trees
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 13 of 15
Fig. 13 Example connection between a treatment and a taxonomic concept. A treatment is the realization of a taxonomic concept
numbers will rise driven by the realization that there are
some problems with relying solely on Linnaean names for
the identification of taxonomic concepts [5, 74, 75]. Con-
cept taxonomy may, in fact, become even more important
in the future as conservation efforts face challenges due to
unresolved taxonomies [76]. Properly aligning taxonomic
concepts to nomenclature across revisions [77] may be the
solution.
Together with taxonomic information, the ontology
allows modeling the source information in a knowledge
base. This will be useful for metastudies, for the purposes
of reproducible research, and other scholarly purposes.
Moreover, it will be an expert system as the knowledge
extracted will come from scholarly publications. We envi-
sion the system to be able to address a wide variety of
taxonomic competency questions raised by researchers
during pro-iBiosphere [78]. Examples include: Is X a valid
taxonomic name (in a nomenclatorial sense)? Which
treatments use different names for the same taxon con-
cepts? Which treatments are nomenclatorially linked
(including homonyms!) to another treatment?
Out immediate next efforts will be concentrated on pop-
ulating the ontology with triples extracted from prospec-
tively published Pensoft journals [79], legacy journals text-
mined by Plazi [80], as well as databases such as GBIF
and Bioimages [81]. Special effort will be made to link the
dataset to the Linked Open Data cloud via resources such
as geographic or institution names. In terms of extend-
ing the ontological model, more research needs to go
into modeling the taxonomic concept circumscription
creating ontologies for morphological, genomic, or eco-
logical traits. Also possibly refining the RCC-5 statements
informed by the actual implementation. A study will be
carried out to investigate the usefulness of the ontology
once the LOD dataset had been created in a real-world
scenario.
Conclusions
The paper provides an informal conceptualization of the
taxonomic process and a formalization in OpenBiodiv-O.
It introduces classes and properties in the domains of bio-
diversity publishing and biological systematics and aligns
them with the important domain-specific ontologies. By
bridging the ontological gap between the publishing and
the biodiversity domains, it will enable the creation of
Open Biodiversity Knowledge Management System, con-
sisting of (1) the ontology itself; (2) a Linked Open
Dataset (LOD) of biodiversity information (biodiversity
knowledge graph); and (3) user interface components
aimed at searching, browsing and discovering knowledge
in big corpora of previously dispersed scholarly publica-
tions. Through the usage of taxonomic concepts, we have
includedmechanisms for democratization of the scholarly
process and not forcing a taxonomic opinion on the users.
Additional files
Additional file 1: Ontology is a plain text file containing statements in the
Turtle syntax forming OpenBiodiv-O. It can be edited in a text (e.g. Sublime
Text, Emacs, etc.) or in an ontology editor (e.g. Protégé). It can be loaded it
into a triple store (e.g. GraphDB). The prefixes that are used throughout this
manuscript are defined at the beginning. This file corresponds to <http://
openbiodiv.net/openbiodivo-20171103>. (TXT 22 kb)
Additional file 2: Vocabulary of Taxonomic Statuses is a plain text file
containing statements in the Turtle syntax forming the OpenBiodiv
Vocabulary of Taxonomic Statuses. Like the ontology [Additional file 1] it
can be edited in a text or ontology editor or loaded in a triple store. Make
sure you also load the ontology first. (TXT 7 kb)
Additional file 3: RCC-5 Vocabulary is a plain text file containing
statements in the Turtle syntax forming the OpenBiodiv RCC-5 Vocabulary.
Like the ontology [Additional file 1] it can be edited in a text or ontology
editor or loaded in a triple store. Make sure you also load the ontology first.
(TXT 5 kb)
Abbreviations
LOD: Linked open data; OWL: W3C web ontology language; RCC-5: Region
connection calculus 5; RDF: Resource description framework; RDFS: RDF
schema; SPARQL: SPARQL protocol and RDF query language; XML: Extensible
markup language
Acknowledgements
We acknowledge É. Ó Tuama and D. Mietchen for the many helpful
discussions that lead to theoretical contributions. We also acknowledge the
programming team at Pensoft and in particular Georgi Zhelezov for
web-development in PHP and JavaScript.
Funding
Research financed through the European Unions Horizon 2020 research and
innovation program under the Marie Sklodowska-Curie grant agreement No.
642241.
Availability of data andmaterials
A partial dataset from Pensofts journals has been generated with
OpenBiodiv-O and can be found at the SPARQL Endpoint <http://213.145.125.
72:7777/>, select repository obkms_i6. The endpoint is also accessible from
the website, <http://openbiodiv.net/>. Demos are available as Saved Queries
from the workbench and from the website.
Authors contributions
VS: Marie-Sklodowska-Curie Ph.D. student, whose main project is OpenBiodiv.
LP: principal investigator, the main academic supervisor of VS, supported each
step of the way. KS and NF are co-advisors. PS consulted on the taxonomic
process and on the development of the Taxonomic Status Vocabulary. KS
consulted on ontological development, proof-read and improved the
manuscript. NF consulted on concept taxonomy and the vision of the system,
also proof-read and improved the manuscript. TC and RAM are the main
authors of the Treatment Ontologies, which serve as a conceptual template
for OpenBiodiv-O. They also provided proof-reading and improvements to the
text. DA and GS provided many insights into biodiversity publishing. All
authors read and approved the final manuscript.
Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 14 of 15
Ethics approval and consent to participate
Not applicable
Consent for publication
Not applicable
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Pensoft Publishers, Prof. Georgi Zlatarski 12, 1700 Sofia, Bulgaria. 2Institute of
Biodiversity and Ecosystems Research, Bulgarian Academy of Sciences, Sofia,
Bulgaria. 3Institute of Information and Communication Technologies,
Bulgarian Academy of Sciences, Sofia, Bulgaria. 4Arizona State University,
School of Life Sciences, Tempe Campus, 4501 Tempe, AZ, USA. 5Plazi, Bern,
Switzerland. 6University of Massachusetts at Boston, Boston, USA. 7National
Museum of Natural History, 1 Tsar Osvoboditel Blvd., Sofia 1000, Bulgaria.
Received: 18 August 2017 Accepted: 28 December 2017
Faria et al. Journal of Biomedical Semantics  (2018) 9:4 
DOI 10.1186/s13326-017-0170-9
RESEARCH Open Access
Tackling the challenges of matching
biomedical ontologies
Daniel Faria1* , Catia Pesquita2, Isabela Mott2, Catarina Martins3, Francisco M. Couto2 and Isabel F. Cruz4
Abstract
Background: Biomedical ontologies pose several challenges to ontology matching due both to the complexity of
the biomedical domain and to the characteristics of the ontologies themselves. The biomedical tracks in the Ontology
Matching Evaluation Initiative (OAEI) have spurred the development of matching systems able to tackle these
challenges, and benchmarked their general performance. In this study, we dissect the strategies employed by
matching systems to tackle the challenges of matching biomedical ontologies and gauge the impact of the challenges
themselves on matching performance, using the AgreementMakerLight (AML) system as the platform for this study.
Results: We demonstrate that the linear complexity of the hash-based searching strategy implemented by most
state-of-the-art ontology matching systems is essential for matching large biomedical ontologies efficiently. We show
that accounting for all lexical annotations (e.g., labels and synonyms) in biomedical ontologies leads to a substantial
improvement in F-measure over using only the primary name, and that accounting for the reliability of different types
